{
  "metadata": {
    "title": "The Data Warehouse Toolkit 3rd Edition",
    "author": "Ralph Kimball and Margy Ross",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 601,
    "conversion_date": "2025-12-25T18:20:23.876582",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "The Data Warehouse Toolkit 3rd Edition.pdf",
    "extraction_method": "PyMuPDF_fallback (Unstructured failed)"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Segment 1 (pages 3-12)",
      "start_page": 3,
      "end_page": 12,
      "detection_method": "topic_boundary",
      "content": "The Data Warehouse Toolkit: The Deﬁ nitive Guide to Dimensional Modeling, Third Edition\nPublished by\nJohn Wiley & Sons, Inc.\n10475 Crosspoint Boulevard\nIndianapolis, IN 46256\nwww.wiley.com\nCopyright © 2013 by Ralph Kimball and Margy Ross\nPublished by John Wiley & Sons, Inc., Indianapolis, Indiana\nPublished simultaneously in Canada\nISBN: 978-1-118-53080-1\nISBN: 978-1-118-53077-1 (ebk)\nISBN: 978-1-118-73228-1 (ebk)\nISBN: 978-1-118-73219-9 (ebk)\nManufactured in the United States of America\n10 9 8 7 6 5 4 3 2 1\nNo part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or \nby any means, electronic, mechanical, photocopying, recording, scanning or otherwise, except as permit-\nted under Sections 107 or 108 of the 1976 United States Copyright Act, without either the prior written \npermission of the Publisher, or authorization through payment of the appropriate per-copy fee to the \nCopyright Clearance Center, 222 Rosewood Drive, Danvers, MA 01923, (978) 750-8400, fax (978) 646-\n8600. Requests to the Publisher for permission should be addressed to the Permissions Department, John \nWiley & Sons, Inc., 111 River Street, Hoboken, NJ  07030, (201) 748-6011, fax (201) 748-6008, or online \nat http://www.wiley.com/go/permissions.\nLimit of Liability/Disclaimer of Warranty: The publisher and the author make no representations or war-\nranties with respect to the accuracy or completeness of the contents of this work and speciﬁ cally disclaim all \nwarranties, including without limitation warranties of ﬁ tness for a particular purpose. No warranty may be \ncreated or extended by sales or promotional materials. The advice and strategies contained herein may not \nbe suitable for every situation. This work is sold with the understanding that the publisher is not engaged in \nrendering legal, accounting, or other professional services. If professional assistance is required, the services \nof a competent professional person should be sought. Neither the publisher nor the author shall be liable for \ndamages arising herefrom. The fact that an organization or Web site is referred to in this work as a citation \nand/or a potential source of further information does not mean that the author or the publisher endorses \nthe information the organization or website may provide or recommendations it may make. Further, readers \nshould be aware that Internet websites listed in this work may have changed or disappeared between when \nthis work was written and when it is read.\nFor general information on our other products and services please contact our Customer Care \nDepartment within the United States at (877) 762-2974, outside the United States at (317) 572-3993 or fax \n(317) 572-4002.\nWiley publishes in a variety of print and electronic formats and by print-on-demand. Some material \nincluded with standard print versions of this book may not be included in e-books or in print-on-\ndemand. If this book refers to media such as a CD or DVD that is not included in the version you \npurchased, you may download this material at http://booksupport.wiley.com. For more informa-\ntion about Wiley products, visit www.wiley.com.\nLibrary of Congress Control Number: 2013936841\nTrademarks: Wiley and the Wiley logo are trademarks or registered trademarks of John Wiley & Sons, \nInc. and/or its affi  liates, in the United States and other countries, and may not be used without written per-\nmission. All other trademarks are the property of their respective owners. John Wiley & Sons, Inc. is not \nassociated with any product or vendor mentioned in this book.\n\n\nRalph Kimball founded the Kimball Group. Since the mid-1980s, he has been the \ndata warehouse and business intelligence industry’s thought leader on the dimen-\nsional approach. He has educated tens of thousands of IT professionals. The Toolkit \nbooks written by Ralph and his colleagues have been the industry’s best sellers \nsince 1996. Prior to working at Metaphor and founding Red Brick Systems, Ralph \ncoinvented the Star workstation, the ﬁ rst commercial product with windows, icons, \nand a mouse, at Xerox’s Palo Alto Research Center (PARC). Ralph has a PhD in \nelectrical engineering from Stanford University.\nMargy Ross is president of the Kimball Group. She has focused exclusively on data \nwarehousing and business intelligence since 1982 with an emphasis on business \nrequirements and dimensional modeling. Like Ralph, Margy has taught the dimen-\nsional best practices to thousands of students; she also coauthored ﬁ ve Toolkit books \nwith Ralph. Margy previously worked at Metaphor and cofounded DecisionWorks \nConsulting. She graduated with a BS in industrial engineering from Northwestern \nUniversity.\nAbout the Authors\n\n\nExecutive Editor\nRobert Elliott\nProject Editor\nMaureen Spears\nSenior Production Editor\nKathleen Wisor\nCopy Editor\nApostrophe Editing Services\nEditorial Manager\nMary Beth Wakeﬁ eld \nFreelancer Editorial Manager\nRosemarie Graham\nAssociate Director of Marketing\nDavid Mayhew\nMarketing Manager\nAshley Zurcher\nBusiness Manager\nAmy Knies\nProduction Manager\nTim Tate\nVice President and Executive Group \nPublisher\nRichard Swadley\nVice President and Executive Publisher\nNeil Edde\nAssociate Publisher\nJim Minatel\nProject Coordinator, Cover\nKatie Crocker\nProofreader\nWord One, New York\nIndexer\nJohnna VanHoose Dinse\nCover Image\niStockphoto.com / teekid\nCover Designer\nRyan Sneed\nCredits\n\n\nF\nirst, thanks to the hundreds of thousands who have read our Toolkit books, \nattended our courses, and engaged us in consulting projects. We have learned as \nmuch from you as we have taught. Collectively, you have had a profoundly positive \nimpact on the data warehousing and business intelligence industry. Congratulations!\nOur Kimball Group colleagues, Bob Becker, Joy Mundy, and Warren Thornthwaite, \nhave worked with us to apply the techniques described in this book literally thou-\nsands of times, over nearly 30 years of working together. Every technique in this \nbook has been thoroughly vetted by practice in the real world. We appreciate their \ninput and feedback on this book—and more important, the years we have shared \nas business partners, along with Julie Kimball. \nBob Elliott, our executive editor at John Wiley & Sons, project editor Maureen \nSpears, and the rest of the Wiley team have supported this project with skill and \nenthusiasm. As always, it has been a pleasure to work with them.\nTo our families, thank you for your unconditional support throughout our \ncareers. Spouses Julie Kimball and Scott Ross and children Sara Hayden Smith, \nBrian Kimball, and Katie Ross all contributed in countless ways to this book.\nAcknowledgments\n\n\nContents\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .xxvii\n 1  Data Warehousing, Business Intelligence, and Dimensional \nModeling Primer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\nDifferent Worlds of Data Capture and Data Analysis . . . . . . . . . . . . . . . . . . .2\nGoals of Data Warehousing and Business Intelligence . . . . . . . . . . . . . . . . . .3\nPublishing Metaphor for DW/BI Managers . . . . . . . . . . . . . . . . . . . . . . .5\nDimensional Modeling Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .7\nStar Schemas Versus OLAP Cubes  . . . . . . . . . . . . . . . . . . . . . . . . . . . . .8\nFact Tables for Measurements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\nDimension Tables for Descriptive Context . . . . . . . . . . . . . . . . . . . . . . 13\nFacts and Dimensions Joined in a Star Schema . . . . . . . . . . . . . . . . . . . 16\nKimball’s DW/BI Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\nOperational Source Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\nExtract, Transformation, and Load System . . . . . . . . . . . . . . . . . . . . . . 19\nPresentation Area to Support Business Intelligence. . . . . . . . . . . . . . . .21\nBusiness Intelligence Applications  . . . . . . . . . . . . . . . . . . . . . . . . . . . .22\nRestaurant Metaphor for the Kimball Architecture . . . . . . . . . . . . . . . .23\nAlternative DW/BI Architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .26\nIndependent Data Mart Architecture . . . . . . . . . . . . . . . . . . . . . . . . . .26\nHub-and-Spoke Corporate Information Factory Inmon Architecture  . .28\nHybrid Hub-and-Spoke and Kimball Architecture . . . . . . . . . . . . . . . . .29\nDimensional Modeling Myths. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .30\nMyth 1: Dimensional Models are Only for Summary Data . . . . . . . . . .30\nMyth 2: Dimensional Models are Departmental, Not Enterprise  . . . . .31\nMyth 3: Dimensional Models are Not Scalable . . . . . . . . . . . . . . . . . . .31\nMyth 4: Dimensional Models are Only for Predictable Usage . . . . . . . .31\nMyth 5: Dimensional Models Can’t Be Integrated . . . . . . . . . . . . . . . .32\nMore Reasons to Think Dimensionally  . . . . . . . . . . . . . . . . . . . . . . . . . . . .32\nAgile Considerations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .34\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .35\n\n\nContents\nx\n 2 Kimball Dimensional Modeling Techniques Overview . . . . . . . . . 37\nFundamental Concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\nGather Business Requirements and Data Realities . . . . . . . . . . . . . . . . . 37\nCollaborative Dimensional Modeling Workshops . . . . . . . . . . . . . . . . .38\nFour-Step Dimensional Design Process . . . . . . . . . . . . . . . . . . . . . . . . .38\nBusiness Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .39\nGrain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .39\nDimensions for Descriptive Context . . . . . . . . . . . . . . . . . . . . . . . . . . .40\nFacts for Measurements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .40\nStar Schemas and OLAP Cubes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .40\nGraceful Extensions to Dimensional Models . . . . . . . . . . . . . . . . . . . . . 41\nBasic Fact Table Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\nFact Table Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\nAdditive, Semi-Additive, Non-Additive Facts . . . . . . . . . . . . . . . . . . . .42\nNulls in Fact Tables  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .42\nConformed Facts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .42\nTransaction Fact Tables  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\nPeriodic Snapshot Fact Tables  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\nAccumulating Snapshot Fact Tables . . . . . . . . . . . . . . . . . . . . . . . . . . .44\nFactless Fact Tables  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .44\nAggregate Fact Tables or OLAP Cubes . . . . . . . . . . . . . . . . . . . . . . . . .45\nConsolidated Fact Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .45\nBasic Dimension Table Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .46\nDimension Table Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .46\nDimension Surrogate Keys . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .46\nNatural, Durable, and Supernatural Keys . . . . . . . . . . . . . . . . . . . . . . .46\nDrilling Down  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\nDegenerate Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\nDenormalized Flattened Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . 47\nMultiple Hierarchies in Dimensions  . . . . . . . . . . . . . . . . . . . . . . . . . . .48\nFlags and Indicators as Textual Attributes . . . . . . . . . . . . . . . . . . . . . . .48\nNull Attributes in Dimensions  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .48\nCalendar Date Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .48\nRole-Playing Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .49\nJunk Dimensions  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .49\n\n\nContents\nxi\nSnowﬂ aked Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .50\nOutrigger Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .50\nIntegration via Conformed Dimensions  . . . . . . . . . . . . . . . . . . . . . . . . . . .50\nConformed Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .51\nShrunken Dimensions  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .51\nDrilling Across . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .51\nValue Chain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .52\nEnterprise Data Warehouse Bus Architecture . . . . . . . . . . . . . . . . . . . .52\nEnterprise Data Warehouse Bus Matrix . . . . . . . . . . . . . . . . . . . . . . . . .52\nDetailed Implementation Bus Matrix . . . . . . . . . . . . . . . . . . . . . . . . . .53\nOpportunity/Stakeholder Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . .53\nDealing with Slowly Changing Dimension Attributes . . . . . . . . . . . . . . . . .53\nType 0: Retain Original  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .54\nType 1: Overwrite  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .54\nType 2: Add New Row . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .54\nType 3: Add New Attribute  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .55\nType 4: Add Mini-Dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .55\nType 5: Add Mini-Dimension and Type 1 Outrigger . . . . . . . . . . . . . . .55\nType 6: Add Type 1 Attributes to Type 2 Dimension. . . . . . . . . . . . . . .56\nType 7: Dual Type 1 and Type 2 Dimensions . . . . . . . . . . . . . . . . . . . .56\nDealing with Dimension Hierarchies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .56\nFixed Depth Positional Hierarchies . . . . . . . . . . . . . . . . . . . . . . . . . . . .56\nSlightly Ragged/Variable Depth Hierarchies . . . . . . . . . . . . . . . . . . . . .57\nRagged/Variable Depth Hierarchies with Hierarchy Bridge Tables  . . . .57\nRagged/Variable Depth Hierarchies with Pathstring Attributes . . . . . . .57\nAdvanced Fact Table Techniques  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .58\nFact Table Surrogate Keys. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .58\nCentipede Fact Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .58\nNumeric Values as Attributes or Facts  . . . . . . . . . . . . . . . . . . . . . . . . .59\nLag/Duration Facts. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .59\nHeader/Line Fact Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .59\nAllocated Facts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .60\nProﬁ t and Loss Fact Tables Using Allocations . . . . . . . . . . . . . . . . . . . .60\nMultiple Currency Facts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .60\nMultiple Units of Measure Facts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .61\n\n\nContents\nxii\nYear-to-Date Facts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .61\nMultipass SQL to Avoid Fact-to-Fact Table Joins . . . . . . . . . . . . . . . . . .61\nTimespan Tracking in Fact Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . .62\nLate Arriving Facts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .62\nAdvanced Dimension Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .62\nDimension-to-Dimension Table Joins . . . . . . . . . . . . . . . . . . . . . . . . . .62\nMultivalued Dimensions and Bridge Tables . . . . . . . . . . . . . . . . . . . . .63\nTime Varying Multivalued Bridge Tables  . . . . . . . . . . . . . . . . . . . . . . .63\nBehavior Tag Time Series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .63\nBehavior Study Groups  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .64\nAggregated Facts as Dimension Attributes . . . . . . . . . . . . . . . . . . . . . .64\nDynamic Value Bands  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .64\nText Comments Dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .65\nMultiple Time Zones . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .65\nMeasure Type Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .65\nStep Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .65\nHot Swappable Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .66\nAbstract Generic Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .66\nAudit Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .66\nLate Arriving Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .67\nSpecial Purpose Schemas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .67\nSupertype and Subtype Schemas for Heterogeneous Products  . . . . . .67\nReal-Time Fact Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .68\nError Event Schemas  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .68\n 3 Retail Sales . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69\nFour-Step Dimensional Design Process . . . . . . . . . . . . . . . . . . . . . . . . . . . .70\nStep 1: Select the Business Process . . . . . . . . . . . . . . . . . . . . . . . . . . . .70\nStep 2: Declare the Grain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .71\nStep 3: Identify the Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .72\nStep 4: Identify the Facts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .72\nRetail Case Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .72\nStep 1: Select the Business Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74\nStep 2: Declare the Grain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74\nStep 3: Identify the Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76\n\n\nContents xiii\nStep 4: Identify the Facts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76\nDimension Table Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .79\nDate Dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .79\nProduct Dimension  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .83\nStore Dimension  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .87\nPromotion Dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .89\nOther Retail Sales Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .92\nDegenerate Dimensions for Transaction Numbers . . . . . . . . . . . . . . . .93\nRetail Schema in Action . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .94\nRetail Schema Extensibility  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .95\nFactless Fact Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .97\nDimension and Fact Table Keys . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .98\nDimension Table Surrogate Keys  . . . . . . . . . . . . . . . . . . . . . . . . . . . . .98\nDimension Natural and Durable Supernatural Keys . . . . . . . . . . . . . .100\nDegenerate Dimension Surrogate Keys  . . . . . . . . . . . . . . . . . . . . . . . 101\nDate Dimension Smart Keys . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101\nFact Table Surrogate Keys. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102\nResisting Normalization Urges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .104\nSnowﬂ ake Schemas with Normalized Dimensions . . . . . . . . . . . . . . .104\nOutriggers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .106\nCentipede Fact Tables with Too Many Dimensions . . . . . . . . . . . . . . .108\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .109\n 4 Inventory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  111\nValue Chain Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\nInventory Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112\nInventory Periodic Snapshot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113\nInventory Transactions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116\nInventory Accumulating Snapshot . . . . . . . . . . . . . . . . . . . . . . . . . . . 118\nFact Table Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119\nTransaction Fact Tables  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120\nPeriodic Snapshot Fact Tables  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120\nAccumulating Snapshot Fact Tables . . . . . . . . . . . . . . . . . . . . . . . . . . 121\nComplementary Fact Table Types  . . . . . . . . . . . . . . . . . . . . . . . . . . . 122\n\n\nContents\nxiv\nValue Chain Integration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122\nEnterprise Data Warehouse Bus Architecture . . . . . . . . . . . . . . . . . . . . . . . 123\nUnderstanding the Bus Architecture  . . . . . . . . . . . . . . . . . . . . . . . . . 124\nEnterprise Data Warehouse Bus Matrix . . . . . . . . . . . . . . . . . . . . . . . . 125\nConformed Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130\nDrilling Across Fact Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130\nIdentical Conformed Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131\nShrunken Rollup Conformed Dimension with Attribute Subset  . . . . . 132\nShrunken Conformed Dimension with Row Subset  . . . . . . . . . . . . . . 132\nShrunken Conformed Dimensions on the Bus Matrix . . . . . . . . . . . . . 134\nLimited Conformity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135\nImportance of Data Governance and Stewardship . . . . . . . . . . . . . . . 135\nConformed Dimensions and the Agile Movement . . . . . . . . . . . . . . . 137\nConformed Facts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139\n 5 Procurement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  141\nProcurement Case Study  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141\nProcurement Transactions and Bus Matrix  . . . . . . . . . . . . . . . . . . . . . . . . 142\nSingle Versus Multiple Transaction Fact Tables . . . . . . . . . . . . . . . . . . 143\nComplementary Procurement Snapshot. . . . . . . . . . . . . . . . . . . . . . . 147\nSlowly Changing Dimension Basics  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147\nType 0: Retain Original  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148\nType 1: Overwrite  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149\nType 2: Add New Row . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150\nType 3: Add New Attribute  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154\nType 4: Add Mini-Dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156\nHybrid Slowly Changing Dimension Techniques . . . . . . . . . . . . . . . . . . . . 159\nType 5: Mini-Dimension and Type 1 Outrigger  . . . . . . . . . . . . . . . . . 160\nType 6: Add Type 1 Attributes to Type 2 Dimension. . . . . . . . . . . . . . 160\nType 7: Dual Type 1 and Type 2 Dimensions . . . . . . . . . . . . . . . . . . . 162\nSlowly Changing Dimension Recap  . . . . . . . . . . . . . . . . . . . . . . . . . . . . .164\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165\n",
      "page_number": 3
    },
    {
      "number": 2,
      "title": "Segment 2 (pages 13-20)",
      "start_page": 13,
      "end_page": 20,
      "detection_method": "topic_boundary",
      "content": "Contents\nxv\n 6 Order Management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  167\nOrder Management Bus Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168\nOrder Transactions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168\nFact Normalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169\nDimension Role Playing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170\nProduct Dimension Revisited . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172\nCustomer Dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174\nDeal Dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 177\nDegenerate Dimension for Order Number . . . . . . . . . . . . . . . . . . . . . 178\nJunk Dimensions  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179\nHeader/Line Pattern to Avoid . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181\nMultiple Currencies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182\nTransaction Facts at Different Granularity  . . . . . . . . . . . . . . . . . . . . .184\nAnother Header/Line Pattern to Avoid . . . . . . . . . . . . . . . . . . . . . . . . 186\nInvoice Transactions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187\nService Level Performance as Facts, Dimensions, or Both . . . . . . . . . . 188\nProﬁ t and Loss Facts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189\nAudit Dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192\nAccumulating Snapshot for Order Fulﬁ llment Pipeline  . . . . . . . . . . . . . . . 194\nLag Calculations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196\nMultiple Units of Measure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197\nBeyond the Rearview Mirror  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199\n 7 Accounting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201\nAccounting Case Study and Bus Matrix  . . . . . . . . . . . . . . . . . . . . . . . . . .202\nGeneral Ledger Data  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .203\nGeneral Ledger Periodic Snapshot . . . . . . . . . . . . . . . . . . . . . . . . . . .203\nChart of Accounts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .203\nPeriod Close . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .204\nYear-to-Date Facts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .206\nMultiple Currencies Revisited . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .206\nGeneral Ledger Journal Transactions  . . . . . . . . . . . . . . . . . . . . . . . . .206\n\n\nContents\nxvi\nMultiple Fiscal Accounting Calendars . . . . . . . . . . . . . . . . . . . . . . . . .208\nDrilling Down Through a Multilevel Hierarchy . . . . . . . . . . . . . . . . . .209\nFinancial Statements  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .209\nBudgeting Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 210\nDimension Attribute Hierarchies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 214\nFixed Depth Positional Hierarchies . . . . . . . . . . . . . . . . . . . . . . . . . . . 214\nSlightly Ragged Variable Depth Hierarchies . . . . . . . . . . . . . . . . . . . . 214\nRagged Variable Depth Hierarchies . . . . . . . . . . . . . . . . . . . . . . . . . . 215\nShared Ownership in a Ragged Hierarchy . . . . . . . . . . . . . . . . . . . . . 219\nTime Varying Ragged Hierarchies  . . . . . . . . . . . . . . . . . . . . . . . . . . .220\nModifying Ragged Hierarchies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .220\nAlternative Ragged Hierarchy Modeling Approaches . . . . . . . . . . . . .221\nAdvantages of the Bridge Table Approach for Ragged Hierarchies . . .223\nConsolidated Fact Tables  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .224\nRole of OLAP and Packaged Analytic Solutions . . . . . . . . . . . . . . . . . . . . .226\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .227\n 8 Customer Relationship Management . . . . . . . . . . . . . . . . . . . . 229\nCRM Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .230\nOperational and Analytic CRM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 231\nCustomer Dimension Attributes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233\nName and Address Parsing  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233\nInternational Name and Address Considerations . . . . . . . . . . . . . . . .236\nCustomer-Centric Dates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .238\nAggregated Facts as Dimension Attributes . . . . . . . . . . . . . . . . . . . . .239\nSegmentation Attributes and Scores  . . . . . . . . . . . . . . . . . . . . . . . . .240\nCounts with Type 2 Dimension Changes . . . . . . . . . . . . . . . . . . . . . . 243\nOutrigger for Low Cardinality Attribute Set . . . . . . . . . . . . . . . . . . . . 243\nCustomer Hierarchy Considerations . . . . . . . . . . . . . . . . . . . . . . . . . .244\nBridge Tables for Multivalued Dimensions  . . . . . . . . . . . . . . . . . . . . . . . . 245\nBridge Table for Sparse Attributes  . . . . . . . . . . . . . . . . . . . . . . . . . . . 247\nBridge Table for Multiple Customer Contacts . . . . . . . . . . . . . . . . . . .248\nComplex Customer Behavior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249\nBehavior Study Groups for Cohorts . . . . . . . . . . . . . . . . . . . . . . . . . . 249\n\n\nContents xvii\nStep Dimension for Sequential Behavior . . . . . . . . . . . . . . . . . . . . . . .251\nTimespan Fact Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .252\nTagging Fact Tables with Satisfaction Indicators . . . . . . . . . . . . . . . . .254\nTagging Fact Tables with Abnormal Scenario Indicators . . . . . . . . . . .255\nCustomer Data Integration Approaches . . . . . . . . . . . . . . . . . . . . . . . . . .256\nMaster Data Management Creating a Single Customer Dimension  . .256\nPartial Conformity of Multiple Customer Dimensions . . . . . . . . . . . . .258\nAvoiding Fact-to-Fact Table Joins . . . . . . . . . . . . . . . . . . . . . . . . . . . .259\nLow Latency Reality Check  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .260\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 261\n 9 Human Resources Management . . . . . . . . . . . . . . . . . . . . . . . . 263\nEmployee Proﬁ le Tracking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263\nPrecise Effective and Expiration Timespans  . . . . . . . . . . . . . . . . . . . .265\nDimension Change Reason Tracking  . . . . . . . . . . . . . . . . . . . . . . . . .266\nProﬁ le Changes as Type 2 Attributes or Fact Events . . . . . . . . . . . . . . 267\nHeadcount Periodic Snapshot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267\nBus Matrix for HR Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .268\nPackaged Analytic Solutions and Data Models . . . . . . . . . . . . . . . . . . . . .270\nRecursive Employee Hierarchies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .271\nChange Tracking on Embedded Manager Key . . . . . . . . . . . . . . . . . .272\nDrilling Up and Down Management Hierarchies . . . . . . . . . . . . . . . .273\nMultivalued Skill Keyword Attributes  . . . . . . . . . . . . . . . . . . . . . . . . . . . . 274\nSkill Keyword Bridge  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275\nSkill Keyword Text String . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 276\nSurvey Questionnaire Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .277\nText Comments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .278\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .279\n 10 Financial Services . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 281\nBanking Case Study and Bus Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . .282\nDimension Triage to Avoid Too Few Dimensions . . . . . . . . . . . . . . . . . . . .283\nHousehold Dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .286\nMultivalued Dimensions and Weighting Factors . . . . . . . . . . . . . . . . . 287\n\n\nContents\nxviii\nMini-Dimensions Revisited . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .289\nAdding a Mini-Dimension to a Bridge Table . . . . . . . . . . . . . . . . . . . .290\nDynamic Value Banding of Facts  . . . . . . . . . . . . . . . . . . . . . . . . . . . .291\nSupertype and Subtype Schemas for Heterogeneous Products . . . . . . . . .293\nSupertype and Subtype Products with Common Facts  . . . . . . . . . . .295\nHot Swappable Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .296\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .296\n 11 Telecommunications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 297\nTelecommunications Case Study and Bus Matrix  . . . . . . . . . . . . . . . . . . .297\nGeneral Design Review Considerations . . . . . . . . . . . . . . . . . . . . . . . . . . .299\nBalance Business Requirements and Source Realities  . . . . . . . . . . . . .300\nFocus on Business Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .300\nGranularity  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .300\nSingle Granularity for Facts  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .301\nDimension Granularity and Hierarchies . . . . . . . . . . . . . . . . . . . . . . .301\nDate Dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .302\nDegenerate Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .303\nSurrogate Keys . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .303\nDimension Decodes and Descriptions . . . . . . . . . . . . . . . . . . . . . . . .303\nConformity Commitment  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .304\nDesign Review Guidelines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .304\nDraft Design Exercise Discussion  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .306\nRemodeling Existing Data Structures . . . . . . . . . . . . . . . . . . . . . . . . . . . .309\nGeographic Location Dimension  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 310\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 310\n 12 Transportation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  311\nAirline Case Study and Bus Matrix  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 311\nMultiple Fact Table Granularities . . . . . . . . . . . . . . . . . . . . . . . . . . . . 312\nLinking Segments into Trips . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 315\nRelated Fact Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 316\nExtensions to Other Industries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317\nCargo Shipper . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317\nTravel Services . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317\n\n\nContents xix\nCombining Correlated Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 318\nClass of Service  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 319\nOrigin and Destination  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .320\nMore Date and Time Considerations  . . . . . . . . . . . . . . . . . . . . . . . . . . . . 321\nCountry-Speciﬁ c Calendars as Outriggers  . . . . . . . . . . . . . . . . . . . . . 321\nDate and Time in Multiple Time Zones  . . . . . . . . . . . . . . . . . . . . . . .323\nLocalization Recap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 324\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 324\n 13 Education  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 325\nUniversity Case Study and Bus Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . .325\nAccumulating Snapshot Fact Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . .326\nApplicant Pipeline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .326\nResearch Grant Proposal Pipeline . . . . . . . . . . . . . . . . . . . . . . . . . . . .329\nFactless Fact Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .329\nAdmissions Events . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .330\nCourse Registrations  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .330\nFacility Utilization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .334\nStudent Attendance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335\nMore Educational Analytic Opportunities . . . . . . . . . . . . . . . . . . . . . . . . . 336\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 336\n 14 Healthcare . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 339\nHealthcare Case Study and Bus Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . 339\nClaims Billing and Payments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .342\nDate Dimension Role Playing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .345\nMultivalued Diagnoses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .345\nSupertypes and Subtypes for Charges . . . . . . . . . . . . . . . . . . . . . . . .347\nElectronic Medical Records . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .348\nMeasure Type Dimension for Sparse Facts . . . . . . . . . . . . . . . . . . . . .349\nFreeform Text Comments  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .350\nImages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .350\nFacility/Equipment Inventory Utilization . . . . . . . . . . . . . . . . . . . . . . . . . . 351\nDealing with Retroactive Changes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 351\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 352\n\n\nContents\nxx\n 15 Electronic Commerce . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 353\nClickstream Source Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 353\nClickstream Data Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .354\nClickstream Dimensional Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 357\nPage Dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .358\nEvent Dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 359\nSession Dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 359\nReferral Dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .360\nClickstream Session Fact Table . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .361\nClickstream Page Event Fact Table . . . . . . . . . . . . . . . . . . . . . . . . . . .363\nStep Dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .366\nAggregate Clickstream Fact Tables . . . . . . . . . . . . . . . . . . . . . . . . . . .366\nGoogle Analytics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .367\nIntegrating Clickstream into Web Retailer’s Bus Matrix . . . . . . . . . . . . . . .368\nProﬁ tability Across Channels Including Web . . . . . . . . . . . . . . . . . . . . . . . 370\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 373\n 16 Insurance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  375\nInsurance Case Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 376\nInsurance Value Chain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 377\nDraft Bus Matrix  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 378\nPolicy Transactions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 379\nDimension Role Playing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .380\nSlowly Changing Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .380\nMini-Dimensions for Large or Rapidly Changing Dimensions . . . . . . .381\nMultivalued Dimension Attributes . . . . . . . . . . . . . . . . . . . . . . . . . . .382\nNumeric Attributes as Facts or Dimensions  . . . . . . . . . . . . . . . . . . . .382\nDegenerate Dimension  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .383\nLow Cardinality Dimension Tables . . . . . . . . . . . . . . . . . . . . . . . . . . .383\nAudit Dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .383\nPolicy Transaction Fact Table . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .383\nHeterogeneous Supertype and Subtype Products  . . . . . . . . . . . . . . .384\nComplementary Policy Accumulating Snapshot . . . . . . . . . . . . . . . . .384\nPremium Periodic Snapshot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .385\nConformed Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .386\nConformed Facts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .386\n\n\nContents xxi\nPay-in-Advance Facts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .386\nHeterogeneous Supertypes and Subtypes Revisited . . . . . . . . . . . . . .387\nMultivalued Dimensions Revisited . . . . . . . . . . . . . . . . . . . . . . . . . . .388\nMore Insurance Case Study Background . . . . . . . . . . . . . . . . . . . . . . . . . .388\nUpdated Insurance Bus Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .389\nDetailed Implementation Bus Matrix . . . . . . . . . . . . . . . . . . . . . . . . .390\nClaim Transactions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .390\nTransaction Versus Proﬁ le Junk Dimensions  . . . . . . . . . . . . . . . . . . . . 392\nClaim Accumulating Snapshot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 392\nAccumulating Snapshot for Complex Workﬂ ows . . . . . . . . . . . . . . . . 393\nTimespan Accumulating Snapshot . . . . . . . . . . . . . . . . . . . . . . . . . . .394\nPeriodic Instead of Accumulating Snapshot  . . . . . . . . . . . . . . . . . . . .395\nPolicy/Claim Consolidated Periodic Snapshot . . . . . . . . . . . . . . . . . . . . . .395\nFactless Accident Events . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .396\nCommon Dimensional Modeling Mistakes to Avoid . . . . . . . . . . . . . . . . . 397\nMistake 10: Place Text Attributes in a Fact Table . . . . . . . . . . . . . . . . . 397\nMistake 9: Limit Verbose Descriptors to Save Space . . . . . . . . . . . . . .398\nMistake 8: Split Hierarchies into Multiple Dimensions  . . . . . . . . . . . .398\nMistake 7: Ignore the Need to Track Dimension Changes  . . . . . . . . .398\nMistake 6: Solve All Performance Problems with More Hardware . . . .399\nMistake 5: Use Operational Keys to Join Dimensions and Facts . . . . . .399\nMistake 4: Neglect to Declare and Comply with the Fact Grain . . . . .399\nMistake 3: Use a Report to Design the Dimensional Model  . . . . . . . .400\nMistake 2: Expect Users to Query Normalized Atomic Data . . . . . . . .400\nMistake 1: Fail to Conform Facts and Dimensions  . . . . . . . . . . . . . . .400\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .401\n 17 Kimball DW/BI Lifecycle Overview . . . . . . . . . . . . . . . . . . . . . . 403\nLifecycle Roadmap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .404\nRoadmap Mile Markers  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .405\nLifecycle Launch Activities  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .406\nProgram/Project Planning and Management . . . . . . . . . . . . . . . . . . .406\nBusiness Requirements Deﬁ nition  . . . . . . . . . . . . . . . . . . . . . . . . . . . 410\nLifecycle Technology Track . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 416\nTechnical Architecture Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 416\nProduct Selection and Installation . . . . . . . . . . . . . . . . . . . . . . . . . . . 418\n\n\nContents\nxxii\nLifecycle Data Track . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .420\nDimensional Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .420\nPhysical Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .420\nETL Design and Development . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .422\nLifecycle BI Applications Track  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .422\nBI Application Speciﬁ cation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .423\nBI Application Development  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .423\nLifecycle Wrap-up Activities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 424\nDeployment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 424\nMaintenance and Growth . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .425\nCommon Pitfalls to Avoid . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 426\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .427\n 18 Dimensional Modeling Process and Tasks . . . . . . . . . . . . . . . . . 429\nModeling Process Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .429\nGet Organized . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 431\nIdentify Participants, Especially Business Representatives . . . . . . . . . . 431\nReview the Business Requirements . . . . . . . . . . . . . . . . . . . . . . . . . . . 432\nLeverage a Modeling Tool . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 432\nLeverage a Data Proﬁ ling Tool . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 433\nLeverage or Establish Naming Conventions . . . . . . . . . . . . . . . . . . . . 433\nCoordinate Calendars and Facilities . . . . . . . . . . . . . . . . . . . . . . . . . . 433\nDesign the Dimensional Model  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .434\nReach Consensus on High-Level Bubble Chart . . . . . . . . . . . . . . . . . . 435\nDevelop the Detailed Dimensional Model . . . . . . . . . . . . . . . . . . . . . 436\nReview and Validate the Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 439\nFinalize the Design Documentation . . . . . . . . . . . . . . . . . . . . . . . . . .441\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .441\n 19 ETL Subsystems and Techniques  . . . . . . . . . . . . . . . . . . . . . . . 443\nRound Up the Requirements. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .444\nBusiness Needs  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .444\nCompliance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .445\nData Quality  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .445\nSecurity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .446\nData Integration  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .446\n",
      "page_number": 13
    },
    {
      "number": 3,
      "title": "Segment 3 (pages 21-32)",
      "start_page": 21,
      "end_page": 32,
      "detection_method": "topic_boundary",
      "content": "Contents xxiii\nData Latency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .447\nArchiving and Lineage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .447\nBI Delivery Interfaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .448\nAvailable Skills . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .448\nLegacy Licenses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .449\nThe 34 Subsystems of ETL  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .449\nExtracting: Getting Data into the Data Warehouse . . . . . . . . . . . . . . . . . .450\nSubsystem 1: Data Proﬁ ling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .450\nSubsystem 2: Change Data Capture System . . . . . . . . . . . . . . . . . . . . 451\nSubsystem 3: Extract System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 453\nCleaning and Conforming Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 455\nImproving Data Quality Culture and Processes . . . . . . . . . . . . . . . . . . 455\nSubsystem 4: Data Cleansing System . . . . . . . . . . . . . . . . . . . . . . . . .456\nSubsystem 5: Error Event Schema  . . . . . . . . . . . . . . . . . . . . . . . . . . .458\nSubsystem 6: Audit Dimension Assembler . . . . . . . . . . . . . . . . . . . . .460\nSubsystem 7: Deduplication System . . . . . . . . . . . . . . . . . . . . . . . . . .460\nSubsystem 8: Conforming System . . . . . . . . . . . . . . . . . . . . . . . . . . .461\nDelivering: Prepare for Presentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .463\nSubsystem 9: Slowly Changing Dimension Manager . . . . . . . . . . . . .464\nSubsystem 10: Surrogate Key Generator  . . . . . . . . . . . . . . . . . . . . . .469\nSubsystem 11: Hierarchy Manager . . . . . . . . . . . . . . . . . . . . . . . . . . . 470\nSubsystem 12: Special Dimensions Manager . . . . . . . . . . . . . . . . . . . 470\nSubsystem 13: Fact Table Builders  . . . . . . . . . . . . . . . . . . . . . . . . . . . 473\nSubsystem 14: Surrogate Key Pipeline . . . . . . . . . . . . . . . . . . . . . . . . 475\nSubsystem 15: Multivalued Dimension Bridge Table Builder . . . . . . . . 477\nSubsystem 16: Late Arriving Data Handler . . . . . . . . . . . . . . . . . . . . . 478\nSubsystem 17: Dimension Manager System . . . . . . . . . . . . . . . . . . . . 479\nSubsystem 18: Fact Provider System . . . . . . . . . . . . . . . . . . . . . . . . . .480\nSubsystem 19: Aggregate Builder . . . . . . . . . . . . . . . . . . . . . . . . . . . .481\nSubsystem 20: OLAP Cube Builder . . . . . . . . . . . . . . . . . . . . . . . . . . .481\nSubsystem 21: Data Propagation Manager . . . . . . . . . . . . . . . . . . . . .482\nManaging the ETL Environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .483\nSubsystem 22: Job Scheduler . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .483\nSubsystem 23: Backup System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .485\nSubsystem 24: Recovery and Restart System  . . . . . . . . . . . . . . . . . . .486\n\n\nContents\nxxiv\nSubsystem 25: Version Control System  . . . . . . . . . . . . . . . . . . . . . . .488\nSubsystem 26: Version Migration System . . . . . . . . . . . . . . . . . . . . . .488\nSubsystem 27: Workﬂ ow Monitor  . . . . . . . . . . . . . . . . . . . . . . . . . . .489\nSubsystem 28: Sorting System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .490\nSubsystem 29: Lineage and Dependency Analyzer . . . . . . . . . . . . . . .490\nSubsystem 30: Problem Escalation System . . . . . . . . . . . . . . . . . . . . .491\nSubsystem 31: Parallelizing/Pipelining System . . . . . . . . . . . . . . . . . .492\nSubsystem 32: Security System  . . . . . . . . . . . . . . . . . . . . . . . . . . . . .492\nSubsystem 33: Compliance Manager . . . . . . . . . . . . . . . . . . . . . . . . . 493\nSubsystem 34: Metadata Repository Manager  . . . . . . . . . . . . . . . . .495\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .496\n 20 ETL System Design and Development Process and Tasks  . . . . . 497\nETL Process Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 497\nDevelop the ETL Plan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .498\nStep 1: Draw the High-Level Plan . . . . . . . . . . . . . . . . . . . . . . . . . . . .498\nStep 2: Choose an ETL Tool . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .499\nStep 3: Develop Default Strategies . . . . . . . . . . . . . . . . . . . . . . . . . . .500\nStep 4: Drill Down by Target Table . . . . . . . . . . . . . . . . . . . . . . . . . . .500\nDevelop the ETL Speciﬁ cation Document  . . . . . . . . . . . . . . . . . . . . .502\nDevelop One-Time Historic Load Processing . . . . . . . . . . . . . . . . . . . . . . . 503\nStep 5: Populate Dimension Tables with Historic Data . . . . . . . . . . . . 503\nStep 6: Perform the Fact Table Historic Load . . . . . . . . . . . . . . . . . . .508\nDevelop Incremental ETL Processing. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 512\nStep 7: Dimension Table Incremental Processing . . . . . . . . . . . . . . . . 512\nStep 8: Fact Table Incremental Processing . . . . . . . . . . . . . . . . . . . . . 515\nStep 9: Aggregate Table and OLAP Loads  . . . . . . . . . . . . . . . . . . . . . 519\nStep 10: ETL System Operation and Automation . . . . . . . . . . . . . . . . 519\nReal-Time Implications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .520\nReal-Time Triage  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 521\nReal-Time Architecture Trade-Offs . . . . . . . . . . . . . . . . . . . . . . . . . . .522\nReal-Time Partitions in the Presentation Server. . . . . . . . . . . . . . . . . . 524\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 526\n\n\nContents xxv\n 21 Big Data Analytics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 527\nBig Data Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .527\nExtended RDBMS Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .529\nMapReduce/Hadoop Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . .530\nComparison of Big Data Architectures . . . . . . . . . . . . . . . . . . . . . . . .530\nRecommended Best Practices for Big Data . . . . . . . . . . . . . . . . . . . . . . . . 531\nManagement Best Practices for Big Data . . . . . . . . . . . . . . . . . . . . . . 531\nArchitecture Best Practices for Big Data . . . . . . . . . . . . . . . . . . . . . . . 533\nData Modeling Best Practices for Big Data . . . . . . . . . . . . . . . . . . . . .538\nData Governance Best Practices for Big Data . . . . . . . . . . . . . . . . . . .541\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .542\n \n Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 543\n\n\nT\nhe data warehousing and business intelligence (DW/BI) industry certainly has \nmatured since Ralph Kimball published the ﬁ rst edition of The Data Warehouse \nToolkit (Wiley) in 1996. Although large corporate early adopters paved the way, DW/\nBI has since been embraced by organizations of all sizes. The industry has built \nthousands of DW/BI systems. The volume of data continues to grow as warehouses \nare populated with increasingly atomic data and updated with greater frequency. \nOver the course of our careers, we have seen databases grow from megabytes to \ngigabytes to terabytes to petabytes, yet the basic challenge of DW/BI systems has \nremained remarkably constant. Our job is to marshal an organization’s data and \nbring it to business users for their decision making. Collectively, you’ve delivered \non this objective; business professionals everywhere are making better decisions \nand generating payback on their DW/BI investments.\nSince the ﬁ rst edition of The Data Warehouse Toolkit was published, dimensional \nmodeling has been broadly accepted as the dominant technique for DW/BI presenta-\ntion. Practitioners and pundits alike have recognized that the presentation of data \nmust be grounded in simplicity if it is to stand any chance of success. Simplicity is \nthe fundamental key that allows users to easily understand databases and software \nto effi  ciently navigate databases. In many ways, dimensional modeling amounts \nto holding the fort against assaults on simplicity. By consistently returning to a \nbusiness-driven perspective and by refusing to compromise on the goals of user \nunderstandability and query performance, you establish a coherent design that \nserves the organization’s analytic needs. This dimensionally modeled framework \nbecomes the platform for BI. Based on our experience and the overwhelming feed-\nback from numerous practitioners from companies like your own, we believe that \ndimensional modeling is absolutely critical to a successful DW/BI initiative.\nDimensional modeling also has emerged as the leading architecture for building \nintegrated DW/BI systems. When you use the conformed dimensions and con-\nformed facts of a set of dimensional models, you have a practical and predictable \nframework for incrementally building complex DW/BI systems that are inherently \ndistributed.\nFor all that has changed in our industry, the core dimensional modeling tech-\nniques that Ralph Kimball published 17 years ago have withstood the test of time. \nConcepts such as conformed dimensions, slowly changing dimensions, heteroge-\nneous products, factless fact tables, and the enterprise data warehouse bus matrix \nIntroduction\n\n\nIntroduction\nxxviii\ncontinue to be discussed in design workshops around the globe. The original con-\ncepts have been embellished and enhanced by new and complementary techniques. \nWe decided to publish this third edition of Kimball’s seminal work because we felt \nthat it would be useful to summarize our collective dimensional modeling experi-\nence under a single cover. We have each focused exclusively on decision support, \ndata warehousing, and business intelligence for more than three decades. We want \nto share the dimensional modeling patterns that have emerged repeatedly during \nthe course of our careers. This book is loaded with speciﬁ c, practical design recom-\nmendations based on real-world scenarios.\nThe goal of this book is to provide a one-stop shop for dimensional modeling \ntechniques. True to its title, it is a toolkit of dimensional design principles and \ntechniques. We address the needs of those just starting in dimensional DW/BI and \nwe describe advanced concepts for those of you who have been at this a while. We \nbelieve that this book stands alone in its depth of coverage on the topic of dimen-\nsional modeling. It’s the deﬁ nitive guide.\nIntended Audience\nThis book is intended for data warehouse and business intelligence designers, imple-\nmenters, and managers. In addition, business analysts and data stewards who are \nactive participants in a DW/BI initiative will ﬁ nd the content useful.\nEven if you’re not directly responsible for the dimensional model, we believe it \nis important for all members of a project team to be comfortable with dimensional \nmodeling concepts. The dimensional model has an impact on most aspects of a \nDW/BI implementation, beginning with the translation of business requirements, \nthrough the extract, transformation and load (ETL) processes, and ﬁ nally, to the \nunveiling of a data warehouse through business intelligence applications. Due to the \nbroad implications, you need to be conversant in dimensional modeling regardless \nof whether you are responsible primarily for project management, business analysis, \ndata architecture, database design, ETL, BI applications, or education and support. \nWe’ve written this book so it is accessible to a broad audience.\nFor those of you who have read the earlier editions of this book, some of the \nfamiliar case studies will reappear in this edition; however, they have been updated \nsigniﬁ cantly and ﬂ eshed out with richer content, including sample enterprise data \nwarehouse bus matrices for nearly every case study. We have developed vignettes \nfor new subject areas, including big data analytics.\nThe content in this book is somewhat technical. We primarily discuss dimen-\nsional modeling in the context of a relational database with nuances for online \n\n\nIntroduction xxix\nanalytical processing (OLAP) cubes noted where appropriate. We presume you \nhave basic knowledge of relational database concepts such as tables, rows, keys, \nand joins. Given we will be discussing dimensional models in a nondenominational \nmanner, we won’t dive into speciﬁ c physical design and tuning guidance for any \ngiven database management systems.\nChapter Preview\nThe book is organized around a series of business vignettes or case studies. We \nbelieve developing the design techniques by example is an extremely eff ective \napproach because it allows us to share very tangible guidance and the beneﬁ ts of \nreal world experience. Although not intended to be full-scale application or indus-\ntry solutions, these examples serve as a framework to discuss the patterns that \nemerge in dimensional modeling. In our experience, it is often easier to grasp the \nmain elements of a design technique by stepping away from the all-too-familiar \ncomplexities of one’s own business. Readers of the earlier editions have responded \nvery favorably to this approach.\nBe forewarned that we deviate from the case study approach in Chapter 2: Kimball \nDimensional Modeling Techniques Overview. Given the broad industry acceptance \nof the dimensional modeling techniques invented by the Kimball Group, we have \nconsolidated the offi  cial listing of our techniques, along with concise descriptions \nand pointers to more detailed coverage and illustrations of these techniques in \nsubsequent chapters. Although not intended to be read from start to ﬁ nish like the \nother chapters, we feel this technique-centric chapter is a useful reference and can \neven serve as a professional checklist for DW/BI designers.\nWith the exception of Chapter 2, the other chapters of this book build on one \nanother. We start with basic concepts and introduce more advanced content as the \nbook unfolds. The chapters should be read in order by every reader. For example, it \nmight be diffi  cult to comprehend Chapter 16: Insurance, unless you have read the \npreceding chapters on retailing, procurement, order management, and customer \nrelationship management.\nThose of you who have read the last edition may be tempted to skip the ﬁ rst \nfew chapters. Although some of the early fact and dimension grounding may be \nfamiliar turf, we don’t want you to sprint too far ahead. You’ll miss out on updates \nto fundamental concepts if you skip ahead too quickly.\nNOTE \nThis book is laced with tips (like this note), key concept listings, and \nchapter pointers to make it more useful and easily referenced in the future.\n\n\nIntroduction\nxxx\nChapter 1: Data Warehousing, Business Intelligence, \nand Dimensional Modeling Primer\nThe book begins with a primer on data warehousing, business intelligence, and \ndimensional modeling. We explore the components of the overall DW/BI archi-\ntecture and establish the core vocabulary used during the remainder of the book. \nSome of the myths and misconceptions about dimensional modeling are dispelled.\nChapter 2: Kimball Dimensional Modeling \nTechniques Overview\nThis chapter describes more than 75 dimensional modeling techniques and pat-\nterns. This offi  cial listing of the Kimball techniques includes forward pointers to \nsubsequent chapters where the techniques are brought to life in case study vignettes. \nChapter 3: Retail Sales\nRetailing is the classic example used to illustrate dimensional modeling. We start \nwith the classic because it is one that we all understand. Hopefully, you won’t need \nto think very hard about the industry because we want you to focus on core dimen-\nsional modeling concepts instead. We begin by discussing the four-step process for \ndesigning dimensional models. We explore dimension tables in depth, including \nthe date dimension that will be reused repeatedly throughout the book. We also \ndiscuss degenerate dimensions, snowﬂ aking, and surrogate keys. Even if you’re not \na retailer, this chapter is required reading because it is chock full of fundamentals.\nChapter 4: Inventory\nWe remain within the retail industry for the second case study but turn your atten-\ntion to another business process. This chapter introduces the enterprise data ware-\nhouse bus architecture and the bus matrix with conformed dimensions. These \nconcepts are critical to anyone looking to construct a DW/BI architecture that is \nintegrated and extensible. We also compare the three fundamental types of fact \ntables: transaction, periodic snapshot, and accumulating snapshot.\nChapter 5: Procurement\nThis chapter reinforces the importance of looking at your organization’s value chain \nas you plot your DW/BI environment. We also explore a series of basic and advanced \ntechniques for handling slowly changing dimension attributes; we’ve built on the \nlong-standing foundation of type 1 (overwrite), type 2 (add a row), and type 3 (add \na column) as we introduce readers to type 0 and types 4 through 7. \n\n\nIntroduction xxxi\nChapter 6: Order Management\nIn this case study, we look at the business processes that are often the ﬁ rst to be \nimplemented in DW/BI systems as they supply core business performance met-\nrics—what are we selling to which customers at what price? We discuss dimensions \nthat play multiple roles within a schema. We also explore the common challenges \nmodelers face when dealing with order management information, such as header/\nline item considerations, multiple currencies or units of measure, and junk dimen-\nsions with miscellaneous transaction indicators. \nChapter 7: Accounting\nWe discuss the modeling of general ledger information for the data warehouse in \nthis chapter. We describe the appropriate handling of year-to-date facts and multiple \nﬁ scal calendars, as well as consolidated fact tables that combine data from mul-\ntiple business processes. We also provide detailed guidance on dimension attribute \nhierarchies, from simple denormalized ﬁ xed depth hierarchies to bridge tables for \nnavigating more complex ragged, variable depth hierarchies.\nChapter 8: Customer Relationship Management\nNumerous DW/BI systems have been built on the premise that you need to better \nunderstand and service your customers. This chapter discusses the customer dimen-\nsion, including address standardization and bridge tables for multivalued dimension \nattributes. We also describe complex customer behavior modeling patterns, as well \nas the consolidation of customer data from multiple sources.\nChapter 9: Human Resources Management\nThis chapter explores several unique aspects of human resources dimensional \nmodels, including the situation in which a dimension table begins to behave like a \nfact table. We discuss packaged analytic solutions, the handling of recursive man-\nagement hierarchies, and survey questionnaires. Several techniques for handling \nmultivalued skill keyword attributes are compared.\nChapter 10: Financial Services\nThe banking case study explores the concept of supertype and subtype schemas \nfor heterogeneous products in which each line of business has unique descriptive \nattributes and performance metrics. Obviously, the need to handle heterogeneous \nproducts is not unique to ﬁ nancial services. We also discuss the complicated rela-\ntionships among accounts, customers, and households.\n\n\nIntroduction\nxxxii\nChapter 11: Telecommunications\nThis chapter is structured somewhat diff erently to encourage you to think critically \nwhen performing a dimensional model design review. We start with a dimensional \ndesign that looks plausible at ﬁ rst glance. Can you ﬁ nd the problems? In addition, \nwe explore the idiosyncrasies of geographic location dimensions.\nChapter 12: Transportation\nIn this case study we look at related fact tables at diff erent levels of granularity \nwhile pointing out the unique characteristics of fact tables describing segments in \na journey or network. We take a closer look at date and time dimensions, covering \ncountry-speciﬁ c calendars and synchronization across multiple time zones.\nChapter 13: Education\nWe look at several factless fact tables in this chapter. In addition, we explore accu-\nmulating snapshot fact tables to handle the student application and research grant \nproposal pipelines. This chapter gives you an appreciation for the diversity of busi-\nness processes in an educational institution.\nChapter 14: Healthcare\nSome of the most complex models that we have ever worked with are from the \nhealthcare industry. This chapter illustrates the handling of such complexities, \nincluding the use of a bridge table to model the multiple diagnoses and providers \nassociated with patient treatment events.\nChapter 15: Electronic Commerce\nThis chapter focuses on the nuances of clickstream web data, including its unique \ndimensionality. We also introduce the step dimension that’s used to better under-\nstand any process that consists of sequential steps.\nChapter 16: Insurance\nThe ﬁ nal case study reinforces many of the patterns we discussed earlier in the book \nin a single set of interrelated schemas. It can be viewed as a pulling-it-all-together \nchapter because the modeling techniques are layered on top of one another.\n\n\nIntroduction xxxiii\nChapter 17: Kimball Lifecycle Overview\nNow that you are comfortable designing dimensional models, we provide a high-\nlevel overview of the activities encountered during the life of a typical DW/BI proj-\nect. This chapter is a lightning tour of The Data Warehouse Lifecycle Toolkit, Second \nEdition (Wiley, 2008) that we coauthored with Bob Becker, Joy Mundy, and Warren \nThornthwaite.\nChapter 18: Dimensional Modeling Process and Tasks\nThis chapter outlines speciﬁ c recommendations for tackling the dimensional mod-\neling tasks within the Kimball Lifecycle. The ﬁ rst 16 chapters of this book cover \ndimensional modeling techniques and design patterns; this chapter describes \nresponsibilities, how-tos, and deliverables for the dimensional modeling design \nactivity.\nChapter 19: ETL Subsystems and Techniques\nThe extract, transformation, and load system consumes a disproportionate share \nof the time and eff ort required to build a DW/BI environment. Careful consider-\nation of best practices has revealed 34 subsystems found in almost every dimen-\nsional data warehouse back room. This chapter starts with the requirements and \nconstraints that must be considered before designing the ETL system and then \ndescribes the 34 extraction, cleaning, conforming, delivery, and management \nsubsystems.\nChapter 20: ETL System Design and Development \nProcess and Tasks\nThis chapter delves into speciﬁ c, tactical dos and don’ts surrounding the ETL \ndesign and development activities. It is required reading for anyone tasked with \nETL responsibilities.\nChapter 21: Big Data Analytics\nWe focus on the popular topic of big data in the ﬁ nal chapter. Our perspective \nis that big data is a natural extension of your DW/BI responsibilities. We begin \nwith an overview of several architectural alternatives, including MapReduce and \n\n\nIntroduction\nxxxiv\nHadoop, and describe how these alternatives can coexist with your current DW/BI \narchitecture. We then explore the management, architecture, data modeling, and \ndata governance best practices for big data.\nWebsite Resources\nThe Kimball Group’s website is loaded with complementary dimensional modeling \ncontent and resources:\n \n■Register for Kimball Design Tips to receive practical guidance about dimen-\nsional modeling and DW/BI topics.\n \n■Access the archive of more than 300 Design Tips and articles.\n \n■Learn about public and onsite Kimball University classes for quality, vendor-\nindependent education consistent with our experiences and writings.\n \n■Learn about the Kimball Group’s consulting services to leverage our decades \nof DW/BI expertise.\n \n■Pose questions to other dimensionally aware participants on the Kimball \nForum.\nSummary\nThe goal of this book is to communicate the offi  cial dimensional design and devel-\nopment techniques based on the authors’ more than 60 years of experience and \nhard won lessons in real business environments. DW/BI systems must be driven \nfrom the needs of business users, and therefore are designed and presented from a \nsimple dimensional perspective. We are conﬁ dent you will be one giant step closer \nto DW/BI success if you buy into this premise.\nNow that you know where you are headed, it is time to dive into the details. We’ll \nbegin with a primer on DW/BI and dimensional modeling in Chapter 1 to ensure that \neveryone is on the same page regarding key terminology and architectural concepts. \n\n\nData Warehousing, \nBusiness Intelligence, \nand Dimensional \nModeling Primer\nT\nhis first chapter lays the groundwork for the following chapters. We begin by \nconsidering data warehousing and business intelligence (DW/BI) systems from \na high-level perspective. You may be disappointed to learn that we don’t start with \ntechnology and tools—first and foremost, the DW/BI system must consider the \nneeds of the business. With the business needs firmly in hand, we work backwards \nthrough the logical  and then physical designs, along with decisions about technol-\nogy and tools.\nWe drive stakes in the ground regarding the goals of data warehousing and busi-\nness intelligence in this chapter, while observing the uncanny similarities between \nthe responsibilities of a DW/BI manager and those of a publisher. \nWith this big picture perspective, we explore dimensional modeling core concepts \nand establish fundamental vocabulary. From there, this chapter discusses the major \ncomponents of the Kimball DW/BI architecture, along with a comparison of alterna-\ntive architectural approaches; fortunately, there’s a role for dimensional modeling \nregardless of your architectural persuasion. Finally, we review common dimensional \nmodeling myths. By the end of this chapter, you’ll have an appreciation for the need \nto be one-half DBA (database administrator) and one-half MBA (business analyst) \nas you tackle your DW/BI project.\nChapter 1 discusses the following concepts:\n \n■Business-driven goals of data warehousing and business intelligence\n \n■Publishing metaphor for DW/BI systems\n \n■Dimensional modeling core concepts and vocabulary, including fact and \ndimension tables\n \n■Kimball DW/BI architecture’s components and tenets\n \n■Comparison of alternative DW/BI architectures, and the role of dimensional \nmodeling within each\n \n■Misunderstandings about dimensional modeling\n1\n",
      "page_number": 21
    },
    {
      "number": 4,
      "title": "Segment 4 (pages 33-40)",
      "start_page": 33,
      "end_page": 40,
      "detection_method": "topic_boundary",
      "content": "Chapter 1\n2\nDifferent Worlds of Data Capture and \nData Analysis\nOne of the most important assets of any organization is its information. This asset \nis almost always used for two purposes: operational record keeping and analytical \ndecision making. Simply speaking, the operational systems are where you put the \ndata in, and the DW/BI system is where you get the data out.\nUsers of an  operational system turn the wheels of the organization. They take \norders, sign up new customers, monitor the status of operational activities, and log \ncomplaints. The operational systems are optimized to process transactions quickly. \nThese systems almost always deal with one transaction record at a time. They predict-\nably perform the same operational tasks over and over, executing the organization’s \nbusiness processes. Given this execution focus, operational systems typically do not \nmaintain history, but rather update data to reﬂ ect the most current state.\nUsers of a DW/BI  system, on the other hand, watch the wheels of the organiza-\ntion turn to evaluate performance. They count the new orders and compare them \nwith last week’s orders, and ask why the new customers signed up, and what the \ncustomers complained about. They worry about whether operational processes are \nworking correctly. Although they need detailed data to support their constantly \nchanging questions, DW/BI users almost never deal with one transaction at a time. \nThese systems are optimized for high-performance queries as users’ questions often \nrequire that hundreds or hundreds of thousands of transactions be searched and \ncompressed into an answer set. To further complicate matters, users of a DW/BI \nsystem typically demand that historical context be preserved to accurately evaluate \nthe organization’s performance over time.\nIn the ﬁ rst edition of The Data Warehouse Toolkit (Wiley, 1996), Ralph Kimball \n devoted an entire chapter to describe the dichotomy between the worlds of opera-\ntional processing  and data warehousing. At this time, it is widely recognized that \nthe DW/BI system has profoundly diff erent needs, clients, structures, and rhythms \nthan the operational systems of record. Unfortunately, we still encounter supposed \nDW/BI systems that are mere copies of the operational systems of record stored on \na separate hardware platform. Although these environments may address the need \nto isolate the operational and analytical environments for performance reasons, \nthey do nothing to address the other inherent diff erences between the two types \nof systems. Business users are underwhelmed by the usability and performance \nprovided by these pseudo data warehouses; these imposters do a disservice to DW/\nBI because they don’t acknowledge their users have drastically diff erent needs than \noperational system users.\n\n\nData Warehousing, Business Intelligence, and Dimensional Modeling Primer \n3\nGoals of Data Warehousing and \nBusiness Intelligence\nBefore we delve into the details of dimensional modeling, it is helpful to focus on \nthe fundamental goals of data warehousing and business intelligence. The goals  can \nbe readily developed by walking through the halls of any organization and listening \nto business management. These recurring themes have existed for more than three \ndecades:\n \n■“We collect tons of data, but we can’t access it.”\n \n■“We need to slice and dice the data every which way.”\n \n■“Business people need to get at the data easily.”\n \n■“Just show me what is important.”\n \n■“We spend entire meetings arguing about who has the right numbers rather \nthan making decisions.”\n \n■“We want people to use information to support more fact-based decision \nmaking.”\nBased on our experience, these concerns are still so universal that they drive the \nbedrock requirements for the DW/BI system. Now turn these business management \nquotations into requirements.\n \n■The DW/BI system must make information easily accessible. The contents \nof the DW/BI system must be understandable. The  data must be intuitive and \nobvious to the business user, not merely the developer. The data’s structures \nand labels should mimic the business users’ thought processes and vocabu-\nlary. Business users want to separate and combine analytic data in endless \ncombinations. The business intelligence tools and applications that access \nthe data must be simple and easy to use. They also must return query results \nto the user with minimal wait times. We can summarize this requirement by \nsimply saying simple and fast.\n \n■The DW/BI system must present information consistently. The data in the \nDW/BI system must be credible. Data must be carefully assembled from a \nvariety of sources, cleansed, quality  assured, and released only when it is ﬁ t \nfor user consumption. Consistency also implies common labels and deﬁ ni-\ntions for the DW/BI system’s contents are used across data sources. If two \nperformance measures have the same name, they must mean the same thing. \nConversely, if two measures don’t mean the same thing, they should be labeled \ndiff erently.\n\n\nChapter 1\n4\n \n■The DW/BI system must adapt to change. User needs, business conditions, \ndata,  and technology are all subject to change. The DW/BI system must be \ndesigned to handle this inevitable change gracefully so that it doesn’t invali-\ndate existing data or applications. Existing data and applications should not \nbe changed or disrupted when the business community asks new questions \nor new data is added to the warehouse. Finally, if descriptive data in the DW/\nBI system must be modiﬁ ed, you must appropriately account for the changes \nand make these changes transparent to the users.\n \n■The DW/BI system must present information in a timely way. As the DW/\nBI system is  used more intensively for operational decisions, raw data may \nneed to be converted into actionable information within hours, minutes, \nor even seconds. The DW/BI team and business users need to have realistic \nexpectations for what it means to deliver data when there is little time to \nclean or validate it.\n \n■The DW/BI system must be a secure bastion that protects the information \nassets. An  organization’s informational crown jewels are stored in the data \nwarehouse. At a minimum, the warehouse likely contains information about \nwhat you’re selling to whom at what price—potentially harmful details in the \nhands of the wrong people. The DW/BI system must eff ectively control access \nto the organization’s conﬁ dential information.\n \n■The DW/BI system must serve as the authoritative and trustworthy foun-\ndation for improved decision making. The data warehouse must have the \nright data to support decision making. The most important  outputs from a \nDW/BI system are the decisions that are made based on the analytic evidence \npresented; these decisions deliver the business impact and value attributable \nto the DW/BI system. The original label that predates DW/BI is still the best \ndescription of what you are designing: a decision support system.\n \n■The business community must accept the DW/BI system to deem it successful. \nIt doesn’t matter that you built an elegant solution using best-of-breed products \nand platforms. If the business community does not embrace the DW/BI environ-\nment and actively use it, you have failed the acceptance test. Unlike an opera-\ntional system implementation where business users have no choice but to use \nthe new system, DW/BI usage is sometimes optional. Business users will embrace \nthe DW/BI system if it is the “simple and fast” source for actionable information.\nAlthough each requirement on this list is important, the ﬁ nal two are the most \ncritical, and unfortunately, often the most overlooked. Successful data warehousing \nand business intelligence demands more than being a stellar architect, technician, \nmodeler, or database administrator. With a DW/BI initiative, you have one foot \nin your information technology (IT) comfort zone while your other foot is on the \n\n\nData Warehousing, Business Intelligence, and Dimensional Modeling Primer \n5\nunfamiliar turf of business users. You must straddle the two, modifying some tried-\nand-true skills to adapt to the unique demands of DW/BI. Clearly, you need to bring \na spectrum of skills to the party to behave like you’re a hybrid DBA/MBA.\n Publishing Metaphor for DW/BI Managers\nWith the  goals of DW/BI as a backdrop, let’s compare the responsibilities of DW/BI \nmanagers with those of a publishing editor-in-chief. As the editor of a high-quality \nmagazine, you would have broad latitude to manage the magazine’s content, style, \nand delivery. Anyone with this job title would likely tackle the following activities:\n \n■Understand the readers:\n \n■Identify their demographic characteristics.\n \n■Find out what readers want in this kind of magazine.\n \n■Identify the “best” readers who will renew their subscriptions and buy \nproducts from the magazine’s advertisers.\n \n■Find potential new readers and make them aware of the magazine.\n \n■Ensure the magazine appeals to the readers:\n \n■Choose interesting and compelling magazine content.\n \n■Make layout and rendering decisions that maximize the readers’ \npleasure.\n \n■Uphold high-quality writing and editing standards while adopting a \nconsistent presentation style.\n \n■Continuously monitor the accuracy of the articles and advertisers’ \nclaims.\n \n■Adapt to changing reader profiles and the availability of new input \nfrom a network of writers and contributors.\n \n■Sustain the publication:\n \n■Attract advertisers and run the magazine profitably.\n \n■Publish the magazine on a regular basis.\n \n■Maintain the readers’ trust.\n \n■Keep the business owners happy.\nYou also can identify items that should be non-goals for the magazine’s editor-\nin-chief, such as building the magazine around a particular printing technology \nor exclusively putting management’s energy into operational effi  ciencies, such as \nimposing a technical writing style that readers don’t easily understand, or creating \nan intricate and crowded layout that is diffi  cult to read.\nBy building the publishing business on a foundation of serving the readers eff ec-\ntively, the magazine is likely to be successful. Conversely, go through the list and \nimagine what happens if you omit any single item; ultimately, the magazine would \nhave serious problems.\n\n\nChapter 1\n6\nThere are strong parallels that can be drawn between being a conventional pub-\nlisher and being a DW/BI manager. Driven by the needs of the business, DW/BI \nmanagers must publish data that has been collected from a variety of sources and \nedited for quality and consistency. The main responsibility is to serve the readers, \notherwise known as business users. The publishing metaphor underscores the need \nto focus outward on your customers rather than merely focusing inward on prod-\nucts and processes. Although you use technology to deliver the DW/BI system, the \ntechnology is at best a means to an end. As such, the technology and techniques \nused to build the system should not appear directly in your top job responsibilities.\nNow recast the magazine publisher’s responsibilities as DW/BI manager \nresponsibilities:\n \n■Understand the business users:\n \n■Understand their job responsibilities, goals, and objectives.\n \n■Determine the decisions that the business users want to make with the \nhelp of the DW/BI system.\n \n■Identify the “best” users who make effective, high-impact decisions.\n \n■Find potential new users and make them aware of the DW/BI system’s \ncapabilities.\n \n■Deliver high-quality, relevant, and accessible information and analytics to \nthe business users:\n \n■Choose the most robust, actionable data to present in the DW/BI sys-\ntem, carefully selected from the vast universe of possible data sources \nin your organization.\n \n■Make the user interfaces and applications simple and template-driven, \nexplicitly matched to the users’ cognitive processing profiles.\n \n■Make sure the data is accurate and can be trusted, labeling it consis-\ntently across the enterprise.\n \n■Continuously monitor the accuracy of the data and analyses.\n \n■Adapt to changing user profiles, requirements, and business priorities, \nalong with the availability of new data sources. \n \n■Sustain the DW/BI environment:\n \n■Take a portion of the credit for the business decisions made using the \nDW/BI system, and use these successes to justify staffing and ongoing \nexpenditures.\n \n■Update the DW/BI system on a regular basis.\n \n■Maintain the business users’ trust.\n \n■Keep the business users, executive sponsors, and IT management \nhappy.\n\n\nData Warehousing, Business Intelligence, and Dimensional Modeling Primer \n7\nIf you do a good job with all these responsibilities, you will be a great DW/BI \nmanager! Conversely, go through the list and imagine what happens if you omit \nany single item. Ultimately, the environment would have serious problems. Now \ncontrast this view of a DW/BI manager’s job with your own job description. Chances \nare the preceding list is more oriented toward user and business issues and may not \neven sound like a job in IT. In our opinion, this is what makes data warehousing \nand business intelligence interesting.\nDimensional Modeling Introduction\nNow that  you understand the DW/BI system’s goals, let’s consider the basics of dimen-\nsional modeling. Dimensional modeling is widely accepted as the preferred technique \nfor presenting analytic data because it addresses two simultaneous requirements:\n \n■Deliver data that’s understandable to the business users.\n \n■Deliver fast query performance.\nDimensional modeling is a longstanding technique for making databases simple. \nIn case after case, for more than ﬁ ve decades, IT organizations, consultants, and \nbusiness users have naturally gravitated to a simple dimensional structure to match \nthe fundamental human need for simplicity. Simplicity is critical because it ensures \nthat users can easily understand the data, as well as allows software to navigate and \ndeliver results quickly and effi  ciently.\nImagine an executive who describes her business as, “We sell products in various \nmarkets and measure our performance over time.” Dimensional designers listen \ncarefully to the emphasis on product, market, and time. Most people ﬁ nd it intui-\ntive to think of such a business as a cube of data, with the edges labeled product, \nmarket, and time. Imagine slicing and dicing along each of these dimensions. Points \ninside the cube are where the measurements, such as sales volume or proﬁ t, for \nthat combination of product, market, and time are stored. The ability to visualize \nsomething as abstract as a set of data in a concrete and tangible way is the secret \nof understandability. If this perspective seems too simple, good! A data model that \nstarts simple has a chance of remaining simple at the end of the design. A model \nthat starts complicated surely will be overly complicated at the end, resulting in \nslow query performance and business user rejection. Albert Einstein captured the \nbasic philosophy driving dimensional design when he said, “Make everything as \nsimple as possible, but not simpler.”\nAlthough  dimensional models are often instantiated in relational database man-\nagement systems, they are quite diff erent from third normal form (3NF) models which \n\n\nChapter 1\n8\nseek to remove data redundancies. Normalized 3NF structures divide data into \nmany discrete entities, each of which becomes a relational table. A database of sales \norders might start with a record for each order line but turn into a complex spider \nweb diagram as a 3NF model, perhaps consisting of hundreds of normalized tables.\nThe industry  sometimes refers to 3NF models as entity-relationship (ER) \nmodels. Entity-relationship diagrams (ER diagrams or ERDs) are drawings that com-\nmunicate the relationships between tables. Both 3NF and dimensional models can \nbe represented in ERDs because both consist of joined relational tables; the key \ndiff erence between 3NF and dimensional models is the degree of normalization. \nBecause both model types can be presented as ERDs, we refrain from referring to \n3NF models as ER models; instead, we call  them normalized models to minimize \nconfusion.\nNormalized 3NF structures  are immensely useful in operational processing \nbecause an update or insert transaction touches the database in only one place. \nNormalized models, however, are too complicated for BI queries. Users can’t under-\nstand, navigate, or remember normalized models that resemble a map of the Los \nAngeles freeway system. Likewise, most relational database management systems \ncan’t effi  ciently query a normalized model; the complexity of users’ unpredictable \nqueries overwhelms the database optimizers, resulting in disastrous query perfor-\nmance. The use of normalized modeling in the DW/BI presentation area defeats the \nintuitive and high-performance retrieval of data. Fortunately, dimensional modeling \naddresses the problem of overly complex schemas in the presentation area. \nNOTE \nA dimensional model contains the same information as a normalized \nmodel, but packages the data in a format that delivers user understandability, query \nperformance, and resilience to change.\n Star Schemas Versus OLAP Cubes\nDimensional  models implemented in relational database management systems are \nreferred to as star schemas because of their resemblance to a star-like structure. \nDimensional models implemented in multidimensional database environments are \nreferred to as online analytical processing (OLAP) cubes, as illustrated in Figure 1-1. \nIf your DW/BI environment includes either star schemas or OLAP cubes, it lever-\nages dimensional concepts. Both stars and cubes have a common logical design with \nrecognizable dimensions; however, the physical implementation diff ers.\nWhen  data is loaded into an OLAP cube, it is stored and indexed using formats \nand techniques that are designed for dimensional data. Performance aggregations \nor precalculated summary tables are often created and managed by the OLAP cube \nengine. Consequently, cubes deliver superior query performance because of the \n\n\nData Warehousing, Business Intelligence, and Dimensional Modeling Primer \n9\nprecalculations, indexing strategies, and other optimizations. Business users can \ndrill down or up by adding or removing attributes from their analyses with excellent \nperformance without issuing new queries. OLAP cubes also provide more analyti-\ncally robust functions that exceed those available with SQL. The downside is that you \npay a load performance price for these capabilities, especially with large data sets.\nDate\nDimension\nMarket\nDimension\nProduct\nDimension\nMarket\nProduct\nDate\nSales\nFacts\nFigure 1-1: Star schema versus OLAP cube.\nFortunately, most of the recommendations in this book pertain regardless of the \nrelational versus multidimensional database platform. Although the capabilities \nof OLAP technology are continuously improving, we generally recommend that \ndetailed, atomic information be loaded into a star schema; optional OLAP cubes are \nthen populated from the star schema. For this reason, most dimensional modeling \ntechniques in this book are couched in terms of a relational star schema.\nOLAP Deployment Considerations\nHere are some  things to keep in mind if you deploy data into OLAP cubes:\n \n■A star schema hosted in a relational database is a good physical foundation \nfor building an OLAP cube, and is generally regarded as a more stable basis \nfor backup and recovery.\n \n■OLAP cubes have traditionally been noted for extreme performance advan-\ntages over RDBMSs, but that distinction has become less important with \nadvances in computer hardware, such as appliances and in-memory databases, \nand RDBMS software, such as columnar databases.\n \n■OLAP cube data structures are more variable across diff erent vendors than \nrelational DBMSs, thus the ﬁ nal deployment details often depend on which \nOLAP vendor is chosen. It is typically more diffi  cult to port BI applications \nbetween diff erent OLAP tools than to port BI applications across diff erent \nrelational databases.\n",
      "page_number": 33
    },
    {
      "number": 5,
      "title": "Segment 5 (pages 41-49)",
      "start_page": 41,
      "end_page": 49,
      "detection_method": "topic_boundary",
      "content": "Chapter 1\n10\n \n■OLAP cubes typically off er more sophisticated security options than RDBMSs, \nsuch as limiting access to detailed data but providing more open access to \nsummary data.\n \n■OLAP cubes off er signiﬁ cantly richer analysis capabilities than RDBMSs, \nwhich are saddled by the constraints of SQL. This may be the main justiﬁ ca-\ntion for using an OLAP product.\n \n■OLAP cubes gracefully support slowly changing dimension type 2 changes \n(which are discussed in Chapter 5: Procurement), but cubes often need to be \nreprocessed partially or totally whenever data is overwritten using alternative \nslowly changing dimension techniques.\n \n■OLAP cubes gracefully support transaction and periodic snapshot fact tables, \nbut do not handle accumulating snapshot fact tables because of the limitations \non overwriting data described in the previous point.\n \n■OLAP cubes typically support complex ragged hierarchies of indeterminate \ndepth, such as organization charts or bills of material, using native query \nsyntax that is superior to the approaches required for RDBMSs.\n \n■OLAP cubes may impose detailed constraints on the structure of dimension \nkeys that implement drill-down hierarchies compared to relational databases.\n \n■Some OLAP products do not enable dimensional roles or aliases, thus requir-\ning separate physical dimensions to be deﬁ ned.\nWe’ll return to the world of dimensional modeling in a relational platform as we \nconsider the two key components of a star schema.\n Fact Tables for Measurements\nThe fact table in  a dimensional model stores the performance measurements result-\ning from an organization’s business process events. You should strive to store the \nlow-level measurement data resulting from a business process in a single dimen-\nsional model. Because measurement data is overwhelmingly the largest set of data, \nit should not be replicated in multiple places for multiple organizational functions \naround the enterprise. Allowing business users from multiple organizations to access \na single centralized repository for each set of measurement data ensures the use of \nconsistent data throughout the enterprise.\nThe term fact represents a business measure. Imagine standing in the marketplace \nwatching products being sold and writing down the unit quantity and dollar sales \namount for each product in each sales transaction. These measurements are captured \nas products are scanned at the register, as illustrated in Figure 1-2.\nEach  row in a fact table corresponds to a measurement event. The data on each \nrow is at a speciﬁ c level of detail, referred to as the grain, such as one row per product \n\n\nData Warehousing, Business Intelligence, and Dimensional Modeling Primer 11\nsold on a sales transaction. One of the core tenets of dimensional modeling is that \nall the measurement rows in a fact table must be at the same grain. Having the dis-\ncipline to create fact tables with a single level of detail ensures that measurements \naren’t inappropriately double-counted.\nTranslates into\nRetail Sales Facts\nDate Key (FK)\nProduct Key (FK)\nStore Key (FK)\nPromotion Key (FK)\nCustomer Key (FK)\nClerk Key (FK)\nTransaction #\nSales Dollars\nSales Units\nFigure 1-2: Business process measurement events translate into fact tables.\nNOTE \nThe idea that a measurement event in the physical world has a one-to-one \nrelationship to a single row in the corresponding fact table is a bedrock principle \nfor dimensional modeling. Everything else builds from this foundation.\nThe most useful facts  are numeric and additive, such as dollar sales amount. \nThroughout this book we will use dollars as the standard currency to make the \ncase study examples more tangible—you can substitute your own local currency \nif it isn’t dollars.\nAdditivity is crucial because BI applications rarely retrieve a single fact table \nrow. Rather, they bring back hundreds, thousands, or even millions of fact rows at \na time, and the most useful thing to do with so many rows is to add them up. No \nmatter how the user slices the data in Figure 1-2, the sales units and dollars sum \nto a valid total. You will see that facts are sometimes semi-additive or even non-\nadditive. Semi-additive facts, such as account balances, cannot be summed across \nthe time dimension. Non-additive facts, such as unit prices, can never be added. You \nare forced to use counts and averages or are reduced to printing out the fact rows \none at a time—an impractical exercise with a billion-row fact table.\nFacts are often described as continuously valued to help sort out what is a fact \nversus a dimension attribute. The dollar sales amount fact is continuously valued in \nthis example because it can take on virtually any value within a broad range. As an \n\n\nChapter 1\n12\nobserver, you must stand out in the marketplace and wait for the measurement before \nyou have any idea what the value will be.\nIt is  theoretically possible for a measured fact to be textual; however, the condition \nrarely arises. In most cases, a textual measurement is a description of something \nand is drawn from a discrete list of values. The designer should make every eff ort to \nput textual data into dimensions where they can be correlated more eff ectively with \nthe other textual dimension attributes and consume much less space. You should \nnot store redundant textual information in fact tables. Unless the text is unique \nfor every row in the fact table, it belongs in the dimension table. A true text fact is \nrare because the unpredictable content of a text fact, like a freeform text comment, \nmakes it nearly impossible to analyze.\nReferring  to the sample fact table in Figure 1-2, if there is no sales activity for a \ngiven product, you don’t put any rows in the table. It is important that you do not \ntry to ﬁ ll the fact table with zeros representing no activity because these zeros would \noverwhelm most fact tables. By including only true activity, fact tables tend to be \nquite sparse. Despite their sparsity, fact tables usually make up 90 percent or more \nof the total space consumed by a dimensional model. Fact tables tend to be deep in \nterms of the number of rows, but narrow in terms of the number of columns. Given \ntheir size, you should be judicious about fact table space utilization.\nAs  examples are developed throughout this book, you will see that all fact table \ngrains fall into one of three categories: transaction, periodic snapshot, and accu-\nmulating snapshot. Transaction grain fact tables are the most common. We will \nintroduce transaction fact tables in Chapter 3: Retail Sales, and both periodic and \naccumulating snapshots in Chapter 4: Inventory.\n All fact tables have  two or more foreign keys (refer to the FK notation in Figure 1-2) \nthat connect to the dimension tables’ primary keys. For example, the product key in \nthe fact table always matches a speciﬁ c product key in the product dimension table. \nWhen all the keys in the fact table correctly match their respective primary keys in \nthe corresponding dimension tables, the tables satisfy referential integrity. You access \nthe fact table via the dimension tables joined to it.\nThe fact table  generally has its own primary key composed of a subset of the for-\neign keys. This key is often called a composite key. Every table that has a composite \nkey is a fact table. Fact tables express many-to-many relationships. All others are \ndimension tables.\nThere are usually a handful of dimensions that together uniquely identify each \nfact table row. After this subset of the overall dimension list has been identiﬁ ed, the \nrest of the dimensions take on a single value in the context of the fact table row’s \nprimary key. In other words, they go along for the ride.\n\n\nData Warehousing, Business Intelligence, and Dimensional Modeling Primer 13\n Dimension Tables for Descriptive Context\nDimension tables  are integral companions to a fact table. The dimension tables con-\ntain the textual context associated with a business process measurement event. They \ndescribe the “who, what, where, when, how, and why” associated with the event.\nAs illustrated in Figure 1-3, dimension tables often have many columns or \nattributes. It is not uncommon for a dimension table to have 50 to 100 attributes; \nalthough, some dimension tables naturally have only a handful of attributes. \nDimension tables tend to have fewer rows than fact tables, but can be wide with \nmany large text columns. Each dimension is deﬁ ned by a single primary key (refer \nto the PK notation in Figure 1-3), which serves as the basis for referential integrity \nwith any given fact table to which it is joined.\nProduct Key (PK)\nSKU Number (Natural Key)\nProduct Description\nBrand Name\nCategory Name\nDepartment Name\nPackage Type\nPackage Size\nAbrasive Indicator\nWeight\nWeight Unit of Measure\nStorage Type\nShelf Life Type\nShelf Width\nShelf Height\nShelf Depth\n...\nProduct Dimension\nFigure 1-3: Dimension tables contain descriptive characteristics of business \nprocess nouns.\nDimension  attributes serve as the primary source of query constraints, group-\nings, and report labels. In a query or report request, attributes are identiﬁ ed as the \nby words. For example, when a user wants to see dollar sales by brand, brand must \nbe available as a dimension attribute.\nDimension table attributes play a vital role in the DW/BI system. Because they \nare the source of virtually all constraints and report labels, dimension attributes are \ncritical to making the DW/BI system usable and understandable. Attributes should \nconsist of real words rather than cryptic abbreviations. You should strive to mini-\nmize the use of codes in dimension tables by replacing them with more verbose \n\n\nChapter 1\n14\ntextual attributes. You may have already trained the business users to memorize \noperational codes, but going forward, minimize their reliance on miniature notes \nattached to their monitor for code translations. You should make standard decodes \nfor the operational codes available as dimension attributes to provide consistent \nlabeling on queries, reports, and BI applications. The decode values should never be \nburied in the reporting applications where inconsistency is inevitable.\nSometimes operational codes or identiﬁ ers have legitimate business signiﬁ cance \nto users or are required to communicate back to the operational world. In these \ncases, the codes should appear as explicit dimension attributes, in addition to the \ncorresponding user-friendly textual descriptors. Operational codes sometimes have \nintelligence embedded in them. For example, the ﬁ rst two digits may identify the \nline of business, whereas the next two digits may identify the global region. Rather \nthan forcing users to interrogate or ﬁ lter on substrings within the operational codes, \npull out the embedded meanings and present them to users as separate dimension \nattributes that can easily be ﬁ ltered, grouped, or reported.\nIn many ways, the data warehouse is only as good as the dimension attributes; the \nanalytic power of the DW/BI environment is directly proportional to the quality and \ndepth of the dimension attributes. The more time spent providing attributes with \nverbose business terminology, the better. The more time spent populating the domain \nvalues in an attribute column, the better. The more time spent ensuring the quality \nof the values in an attribute column, the better. Robust dimension attributes deliver \nrobust analytic slicing-and-dicing capabilities.\nNOTE \nDimensions provide the entry points to the data, and the ﬁ nal labels and \ngroupings on all DW/BI analyses.\nWhen triaging operational source data, it is sometimes unclear whether a \nnumeric data element is a fact or dimension attribute. You often make the decision \nby asking whether the column is a measurement that takes on lots of values and \nparticipates in calculations (making it a fact) or is a discretely valued description \nthat is more or less constant and participates in constraints and row labels (making \nit a dimensional attribute). For example, the standard cost for a product seems like \na constant attribute of the product but may be changed so often that you decide it \nis more like a measured fact. Occasionally, you can’t be certain of the classiﬁ cation; \nit is possible to model the data element either way (or both ways) as a matter of the \ndesigner’s prerogative.\nNOTE \nThe designer’s dilemma of whether a numeric quantity is a fact or a \ndimension attribute is rarely a diffi  cult decision. Continuously valued numeric \n\n\nData Warehousing, Business Intelligence, and Dimensional Modeling Primer 15\nobservations are almost always facts; discrete numeric observations drawn from a \nsmall list are almost always dimension attributes.\nFigure 1-4  shows that dimension tables often represent hierarchical relation-\nships. For example, products roll up into brands and then into categories. For each \nrow in the product dimension, you should store the associated brand and category \ndescription. The hierarchical descriptive information is stored redundantly in the \nspirit of ease of use and query performance. You should resist the perhaps habitual \nurge to normalize data by storing only the brand code in the product dimension and \ncreating a separate brand lookup table, and likewise for the category description in a \nseparate category lookup table. This normalization is called snowﬂ aking. Instead of \nthird normal form, dimension tables typically are highly denormalized with ﬂ attened \nmany-to-one relationships within a single dimension table. Because dimension tables \ntypically are geometrically smaller than fact tables, improving storage effi  ciency by \nnormalizing or snowﬂ aking has virtually no impact on the overall database size. You \nshould almost always trade off  dimension table space for simplicity and accessibility.\n1\n2\n3 \n4 \n5 \n6 \n7 \n8 \n9 \n10 \n11\nProduct Key\nPowerAll 20 oz\nPowerAll 32 oz\nPowerAll 48 oz\nPowerAll 64 oz\nZipAll 20 oz\nZipAll 32 oz\nZipAll 48 oz\nShiny 20 oz\nShiny 32 oz\nZipGlass 20 oz\nZipGlass 32 oz\nPowerClean\nPowerClean\nPowerClean\nPowerClean\nZippy\nZippy\nZippy\nClean Fast\nClean Fast\nZippy\nZippy\nAll Purpose Cleaner\nAll Purpose Cleaner\nAll Purpose Cleaner\nAll Purpose Cleaner\nAll Purpose Cleaner\nAll Purpose Cleaner\nAll Purpose Cleaner\nGlass Cleaner\nGlass Cleaner\nGlass Cleaner\nGlass Cleaner\nProduct Description\nBrand Name\nCategory Name\nFigure 1-4: Sample rows from a dimension table with denormalized hierarchies.\nContrary to popular folklore, Ralph Kimball didn’t invent the terms  fact and \ndimension. As best as can be determined, the dimension and fact terminology \noriginated from a joint research project conducted by General Mills and Dartmouth \nUniversity in the 1960s. In the 1970s, both AC Nielsen and IRI used the terms con-\nsistently to describe their syndicated data off erings and gravitated to dimensional \nmodels for simplifying the presentation of their analytic information. They under-\nstood that their data wouldn’t be used unless it was packaged simply. It is probably \naccurate to say that no single person invented the dimensional approach. It is an \nirresistible force in designing databases that always results when the designer places \nunderstandability and performance as the highest goals.\n\n\nChapter 1\n16\nFacts and Dimensions Joined in a Star Schema\nNow  that you understand fact and dimension tables, it’s time to bring the building blocks \ntogether in a dimensional model, as shown in Figure 1-5. Each business process is repre-\nsented by a dimensional model that consists of a fact table containing the event’s numeric \nmeasurements surrounded by a halo of dimension tables that contain the textual context \nthat was true at the moment the event occurred. This characteristic star-like structure \nis often called a star join, a term dating back to the earliest days of relational databases.\nRetail Sales Fact\nDate Key (FK)\nProduct Key (FK)\nStore Key (FK)\nPromotion Key (FK)\nCustomer Key (FK)\nClerk Key (FK)\nTransaction #\nSales Dollars\nSales Units\nDate Dimension\nProduct Dimension\nPromotion Dimension\nClerk Dimension\nStore Dimension\nCustomer Dimension\nFigure 1-5: Fact and dimension tables in a dimensional model.\nThe ﬁ rst thing to notice about the dimensional schema is its simplicity and \nsymmetry. Obviously, business users beneﬁ t from the simplicity because the data \nis easier to understand and navigate. The charm of the design in Figure 1-5 is that \nit is highly recognizable to business users. We have observed literally hundreds of \ninstances in which users immediately agree that the dimensional model is their \nbusiness. Furthermore, the reduced number of tables and use of meaningful busi-\nness descriptors make it easy to navigate and less likely that mistakes will occur.\nThe  simplicity of a dimensional model also has performance beneﬁ ts. Database \noptimizers process these simple schemas with fewer joins more effi  ciently. A data-\nbase engine can make strong assumptions about ﬁ rst constraining the heavily \nindexed dimension tables, and then attacking the fact table all at once with the \nCartesian product of the dimension table keys satisfying the user’s constraints. \nAmazingly, using this approach, the optimizer can evaluate arbitrary n-way joins \nto a fact table in a single pass through the fact table’s index.\nFinally,  dimensional models are gracefully extensible to accommodate change. \nThe predictable framework of a dimensional model withstands unexpected changes \nin user behavior. Every dimension is equivalent; all dimensions are symmetrically-\nequal entry points into the fact table. The dimensional model has no built-in bias \nregarding expected query patterns. There are no preferences for the business ques-\ntions asked this month versus the questions asked next month. You certainly don’t \nwant to adjust schemas if business users suggest new ways to analyze their business. \n\n\nData Warehousing, Business Intelligence, and Dimensional Modeling Primer 17\nThis  book illustrates repeatedly that the most granular or atomic data has the \nmost dimensionality. Atomic data that has not been aggregated is the most expres-\nsive data; this atomic data should be the foundation for every fact table design to \nwithstand business users’ ad hoc attacks in which they pose unexpected queries. \nWith dimensional models, you can add completely new dimensions to the schema \nas long as a single value of that dimension is deﬁ ned for each existing fact row. \nLikewise, you can add new facts to the fact table, assuming that the level of detail \nis consistent with the existing fact table. You can supplement preexisting dimen-\nsion tables with new, unanticipated attributes. In each case, existing tables can be \nchanged in place either by simply adding new data rows in the table or by executing \nan SQL ALTER TABLE command. Data would not need to be reloaded, and existing BI \napplications would continue to run without yielding diff erent results. We examine \nthis graceful extensibility of dimensional models more fully in Chapter 3.\nAnother  way to think about the complementary nature of fact and dimension \ntables is to see them translated into a report. As illustrated in Figure 1-6, dimension \nattributes supply the report ﬁ lters and labeling, whereas the fact tables supply the \nreport’s numeric values. \nProduct Dimension\nDate Dimension\nStore Dimension\nSales Fact\nProduct Key (PK)\nSKU Number (Natural Key)\nProduct Description\nPackage Type\nPackage Size\nBrand Name\nCategory Name\n... and more\nDate Key (FK)\nProduct Key (FK)\nStore Key (FK)\n...\nTransaction #\nSales Dollars\nSales Units\nStore Key (PK)\nStore Number\nStore Name\nStore State\nStore ZIP\nDistrict\nRegion\n... and more\nDate Key (PK)\nDate\nDay of Week\nMonth\nYear\n...and more\nFilter\nSum\nGroup by\nGroup by\nSales Activity for June 2013\nDistrict\nAtherton\nAtherton\nBelmont\nBelmont\nBrand Name\nPowerClean\nZippy\nClean Fast\nZippy\nSales Dollars\n2,035\n707\n2,330\n527\nFigure 1-6: Dimensional attributes and facts form a simple report.\n\n\nChapter 1\n18\nYou can easily envision the SQL that’s written (or more likely generated by a BI \ntool) to create this report:\nSELECT\n     store.district_name,\n     product.brand,\n     sum(sales_facts.sales_dollars) AS \"Sales Dollars\"\nFROM\n     store,\n     product,\n     date,\n     sales_facts\nWHERE\n     date.month_name=\"January\" AND\n     date.year=2013 AND\n     store.store_key = sales_facts.store_key AND \n     product.product_key = sales_facts.product_key AND\n     date.date_key = sales_facts.date_key\nGROUP BY\n     store.district_name,\n     product.brand\nIf  you study this code snippet line-by-line, the ﬁ rst two lines under the SELECT \nstatement identify the dimension attributes in the report, followed by the aggre-\ngated metric from the fact table. The FROM clause identiﬁ es all the tables involved \nin the query. The ﬁ rst two lines in the WHERE clause declare the report’s ﬁ lter, and \nthe remainder declare the joins between the dimension and fact tables. Finally, the \nGROUP BY clause establishes the aggregation within the report.\nKimball’s DW/BI Architecture\nLet’s build on your understanding of DW/BI systems and dimensional modeling \nfundamentals by investigating the components of a DW/BI environment based on \nthe Kimball architecture. You need to learn the strategic signiﬁ cance of each com-\nponent to avoid confusing their role and function.\nAs  illustrated in Figure 1-7, there are four separate and distinct components to \nconsider in the DW/BI environment: operational source systems, ETL system, data \npresentation area, and business intelligence applications. \nOperational Source Systems\nThese  are the operational systems of record that capture the business’s transactions. \nThink of the source systems as outside the data warehouse because presumably you \nhave little or no control over the content and format of the data in these operational \nsystems. The main priorities of the source systems are processing performance and avail-\nability. Operational queries against source systems are narrow, one-record-at-a-time \n",
      "page_number": 41
    },
    {
      "number": 6,
      "title": "Segment 6 (pages 50-58)",
      "start_page": 50,
      "end_page": 58,
      "detection_method": "topic_boundary",
      "content": "Data Warehousing, Business Intelligence, and Dimensional Modeling Primer 19\nqueries that are part of the normal transaction ﬂ ow and severely restricted in their \ndemands on the operational system. It is safe to assume that source systems are not \nqueried in the broad and unexpected ways that DW/BI systems typically are queried. \nSource systems maintain little historical data; a good data warehouse can relieve \nthe source systems of much of the responsibility for representing the past. In many \ncases, the source systems are special purpose applications without any commitment \nto sharing common data such as product, customer, geography, or calendar with other \noperational systems in the organization. Of course, a broadly adopted cross-application \nenterprise resource planning (ERP) system or operational master data management \nsystem could help address these shortcomings.\nSource\nTransactions\nBack Room\nETL System:\n•  Transform from\n \nsource-to-target\n• Conform\n \ndimensions\n• Normalization\n \noptional\n•  No user query\n \nsupport\nDesign Goals:\n•  Throughput\n• Integrity and\n \nconsistency\nPresentation Area:\n•  Dimensional (star\n \nschema or OLAP\n \ncube)\n• Atomic and\n \nsummary data\n• Organized by\n \nbusiness process\n•  Uses conformed\n \ndimensions\nDesign Goals:\n•  Ease-of-use\n• Query performance\nBI Applications:\n•  Ad hoc queries\n• Standard reports\n• Analytic apps\n•  Data mining and\n \nmodels\nEnterprise DW Bus\nArchitecture\nFront Room\nFigure 1-7: Core elements of the Kimball DW/BI architecture.\nExtract, Transformation, and Load System\nThe extract, transformation, and load (ETL)  system of the DW/BI environment consists \nof a work area, instantiated data structures, and a set of processes. The ETL system \nis everything between the operational source systems and the DW/BI presentation \narea. We elaborate on the architecture of ETL systems and associated techniques \nin Chapter 19: ETL Subsystems and Techniques, but we want to introduce this \nfundamental piece of the overall DW/BI system puzzle.\nExtraction  is the ﬁ rst step in the process of getting data into the data warehouse \nenvironment. Extracting means reading and understanding the source data and \ncopying the data needed into the ETL system for further manipulation. At this \npoint, the data belongs to the data warehouse.\nAfter the data is extracted to the ETL system, there are numerous potential trans-\nformations, such as cleansing the data (correcting misspellings, resolving domain \n\n\nChapter 1\n20\nconﬂ icts, dealing with missing elements, or parsing into standard formats), com-\nbining data from multiple sources, and de-duplicating data. The ETL system adds \nvalue to the data with these cleansing and conforming tasks by changing the data \nand enhancing it. In addition, these activities can be architected to create diagnos-\ntic metadata, eventually leading to business process reengineering to improve data \nquality in the source systems over time.\nThe ﬁ nal step of the ETL process is the physical structuring and loading of data \ninto the presentation area’s target dimensional models. Because the primary mis-\nsion of the ETL system is to hand off  the dimension and fact tables in the delivery \nstep, these subsystems are critical. Many of these deﬁ ned subsystems focus on \ndimension table processing, such as surrogate key assignments, code lookups to \nprovide appropriate descriptions, splitting, or combining columns to present the \nappropriate data values, or joining underlying third normal form table structures \ninto ﬂ attened denormalized dimensions. In contrast, fact tables are typically large \nand time consuming to load, but preparing them for the presentation area is typically \nstraightforward. When the dimension and fact tables in a dimensional model have \nbeen updated, indexed, supplied with appropriate aggregates, and further quality \nassured, the business community is notiﬁ ed that the new data has been published.\nThere remains industry consternation about whether the data in the ETL system \nshould be repurposed into physical normalized structures prior to loading into the \npresentation area’s dimensional structures for querying and reporting. The ETL \nsystem is typically dominated by the simple activities of sorting and sequential \nprocessing. In many cases, the ETL system is not based on relational technology but \ninstead may rely on a system of ﬂ at ﬁ les. After validating the data for conformance \nwith the deﬁ ned one-to-one and many-to-one business rules, it may be pointless to \ntake the ﬁ nal step of building a 3NF physical database, just before transforming the \ndata once again into denormalized structures for the BI presentation area.\nHowever, there are cases in which the data arrives at the doorstep of the ETL \nsystem in a 3NF relational format. In these situations, the ETL system develop-\ners may be more comfortable performing the cleansing and transformation tasks \nusing normalized structures. Although a normalized database for ETL processing \nis acceptable, we have some reservations about this approach. The creation of both \nnormalized structures for the ETL and dimensional structures for presentation \nmeans that the data is potentially extracted, transformed, and loaded twice—once \ninto the normalized database and then again when you load the dimensional model. \nObviously, this two-step process requires more time and investment for the develop-\nment, more time for the periodic loading or updating of data, and more capacity to \nstore the multiple copies of the data. At the bottom line, this typically translates into \nthe need for larger development, ongoing support, and hardware platform budgets. \n\n\nData Warehousing, Business Intelligence, and Dimensional Modeling Primer 21\nUnfortunately, some DW/BI initiatives have failed miserably because they focused \nall their energy and resources on constructing the normalized structures rather \nthan allocating time to developing a dimensional presentation area that supports \nimproved business decision making. Although enterprise-wide data consistency is a \nfundamental goal of the DW/BI environment, there may be eff ective and less costly \napproaches than physically creating normalized tables in the ETL system, if these \nstructures don’t already exist.\nNOTE \nIt is acceptable to create a normalized database to support the ETL \nprocesses; however, this is not the end goal. The normalized structures must be \noff -limits to user queries because they defeat the twin goals of understandability \nand performance.\n Presentation Area to Support Business Intelligence\nThe DW/BI  presentation area is where data is organized, stored, and made available \nfor direct querying by users, report writers, and other analytical BI applications. \nBecause the back room ETL system is off -limits, the presentation area is the DW/BI \nenvironment as far as the business community is concerned; it is all the business \nsees and touches via their access tools and BI applications. The original pre-release \nworking title for the ﬁ rst edition of The Data Warehouse Toolkit was Getting the Data \nOut. This is what the presentation area with its dimensional models is all about.\nWe have several strong opinions about the presentation area. First of all, we insist \nthat the data be presented, stored, and accessed in dimensional schemas, either \nrelational star schemas or OLAP cubes. Fortunately, the industry has matured to the \npoint where we’re no longer debating this approach; it has concluded that dimen-\nsional modeling is the most viable technique for delivering data to DW/BI users.\nOur second stake in the ground about the presentation area is that it must \ncontain detailed, atomic data. Atomic data is required to withstand assaults from \nunpredictable ad hoc user queries. Although the presentation area also may contain \nperformance-enhancing aggregated data, it is not suffi  cient to deliver these sum-\nmaries without the underlying granular data in a dimensional form. In other words, \nit is completely unacceptable to store only summary data in dimensional models \nwhile the atomic data is locked up in normalized models. It is impractical to expect \na user to drill down through dimensional data almost to the most granular level and \nthen lose the beneﬁ ts of a dimensional presentation at the ﬁ nal step. Although DW/\nBI users and applications may look infrequently at a single line item on an order, \nthey may be very interested in last week’s orders for products of a given size (or \nﬂ avor, package type, or manufacturer) for customers who ﬁ rst purchased within \n\n\nChapter 1\n22\nthe last 6 months (or reside in a given state or have certain credit terms). The most \nﬁ nely grained data must be available in the presentation area so that users can ask \nthe most precise questions possible. Because users’ requirements are unpredictable \nand constantly changing, you must provide access to the exquisite details so they \ncan roll up to address the questions of the moment.\nThe presentation data area should be structured around business process mea-\nsurement events. This approach naturally aligns with the operational source data \ncapture systems. Dimensional models should correspond to physical data capture \nevents; they should not be designed to deliver the report-of-the-day. An enterprise’s \nbusiness processes cross the boundaries of organizational departments and func-\ntions. In other words, you should construct a single fact table for atomic sales metrics \nrather than populating separate similar, but slightly diff erent, databases containing \nsales metrics for the sales, marketing, logistics, and ﬁ nance teams.\nAll  the dimensional structures must be built using common, conformed dimen-\nsions. This is the basis of the enterprise data warehouse bus architecture described \nin Chapter 4. Adherence to the bus architecture is the ﬁ nal stake in the ground \nfor the presentation area. Without shared, conformed dimensions, a dimensional \nmodel becomes a standalone application. Isolated stovepipe data sets that cannot be \ntied together are the bane of the DW/BI movement as they perpetuate incompatible \nviews of the enterprise. If you have any hope of building a robust and integrated \nDW/BI environment, you must commit to the enterprise bus architecture. When \ndimensional models have been designed with conformed dimensions, they can be \nreadily combined and used together. The presentation area in a large enterprise \nDW/BI solution ultimately consists of dozens of dimensional models with many of \nthe associated dimension tables shared across fact tables.\nUsing the bus architecture is the secret to building distributed DW/BI systems. \nWhen the bus architecture is used as a framework, you can develop the enterprise \ndata warehouse in an agile, decentralized, realistically scoped, iterative manner.\nNOTE \nData in the queryable presentation area of the DW/BI system must be \ndimensional, atomic (complemented by performance-enhancing aggregates), busi-\nness process-centric, and adhere to the enterprise data warehouse bus architecture. \nThe data must not be structured according to individual departments’ interpreta-\ntion of the data.\nBusiness Intelligence Applications\nThe  ﬁ nal major component of the Kimball DW/BI architecture is the business intelligence \n(BI) application. The term BI application loosely refers to the range of capabilities pro-\nvided to business users to leverage the presentation area for analytic decision making. \n\n\nData Warehousing, Business Intelligence, and Dimensional Modeling Primer 23\nBy definition, all BI applications query the data in the DW/BI presentation area. \nQuerying, obviously, is the whole point of using data for improved decision making.\nA BI application can be as simple as an ad hoc query tool or as complex as a sophis-\nticated data mining or modeling application. Ad hoc query tools, as powerful as they \nare, can be understood and used eff ectively by only a small percentage of the potential \nDW/BI business user population. Most business users will likely access the data via \nprebuilt parameter-driven applications and templates that do not require users to con-\nstruct queries directly. Some of the more sophisticated applications, such as modeling \nor forecasting tools, may upload results back into the operational source systems, ETL \nsystem, or presentation area.\nRestaurant Metaphor for the Kimball Architecture\nOne of  our favorite metaphors reinforces the importance of separating the overall \nDW/BI environment into distinct components. In this case, we’ll consider the simi-\nlarities between a restaurant and the DW/BI environment.\nETL in the Back Room Kitchen\nThe ETL system is analogous to the kitchen of a restaurant. The restaurant’s kitchen \nis a world unto itself. Talented chefs take raw materials and transform them into \nappetizing, delicious meals for the restaurant’s diners. But long before a commercial \nkitchen swings into operation, a signiﬁ cant amount of planning goes into designing \nthe workspace layout and components.\nThe kitchen is organized with several design goals in mind. First, the layout must \nbe highly effi  cient. Restaurant managers want high kitchen throughput. When the \nrestaurant is packed and everyone is hungry, there is no time for wasted movement. \nDelivering consistent quality from the restaurant’s kitchen is the second important \ngoal. The establishment is doomed if the plates coming out of the kitchen repeat-\nedly fail to meet expectations. To achieve consistency, chefs create their special \nsauces once in the kitchen, rather than sending ingredients out to the table where \nvariations will inevitably occur. Finally, the kitchen’s output, the meals delivered \nto restaurant customers, must also be of high integrity. You wouldn’t want someone \nto get food poisoning from dining at your restaurant. Consequently, kitchens are \ndesigned with integrity in mind; salad preparation doesn’t happen on the same \nsurfaces where raw chicken is handled.\nJust as quality, consistency, and integrity are major considerations when designing \nthe restaurant’s kitchen, they are also ongoing concerns for everyday management \nof the restaurant. Chefs strive to obtain the best raw materials possible. Procured \nproducts must meet quality standards and are rejected if they don’t meet minimum \nstandards. Most ﬁ ne restaurants modify their menus based on the availability of \nquality ingredients.\n\n\nChapter 1\n24\nThe restaurant staff s its kitchen with skilled professionals wielding the tools of \ntheir trade. Cooks manipulate razor-sharp knives with incredible conﬁ dence and \nease. They operate powerful equipment and work around extremely hot surfaces \nwithout incident.\nGiven the dangerous surroundings, the back room kitchen is off  limits to res-\ntaurant patrons. Things happen in the kitchen that customers just shouldn’t see. It \nsimply isn’t safe. Professional cooks handling sharp knives shouldn’t be distracted \nby diners’ inquiries. You also wouldn’t want patrons entering the kitchen to dip their \nﬁ ngers into a sauce to see whether they want to order an entree. To prevent these \nintrusions, most restaurants have a closed door that separates the kitchen from the \narea where diners are served. Even restaurants that boast an open kitchen format \ntypically have a barrier, such as a partial wall of glass, separating the two environ-\nments. Diners are invited to watch but can’t wander into the kitchen. Although part \nof the kitchen may be visible, there are always out-of-view back rooms where the \nless visually desirable preparation occurs. \nThe data warehouse’s ETL system resembles the restaurant’s kitchen. Source data \nis magically transformed into meaningful, presentable information. The back room \nETL system must be laid out and architected long before any data is extracted from \nthe source. Like the kitchen, the ETL system is designed to ensure throughput. \nIt must transform raw source data into the target model effi  ciently, minimizing \nunnecessary movement.\nObviously, the ETL system is also highly concerned about data quality, integrity, and \nconsistency. Incoming data is checked for reasonable quality as it enters. Conditions \nare continually monitored to ensure ETL outputs are of high integrity. Business rules \nto consistently derive value-add metrics and attributes are applied once by skilled \nprofessionals in the ETL system rather than relying on each patron to develop them \nindependently. Yes, that puts extra burden on the ETL team, but it’s done to deliver a \nbetter, more consistent product to the DW/BI patrons.\nNOTE \nA properly designed DW/BI environment trades off  work in the front \nroom BI applications in favor of work in the back room ETL system. Front room \nwork must be done over and over by business users, whereas back room work is \ndone once by the ETL staff .\nFinally, ETL system should be off  limits to the business users and BI application \ndevelopers. Just as you don’t want restaurant patrons wandering into the kitchen \nand potentially consuming semi-cooked food, you don’t want busy ETL profession-\nals distracted by unpredictable inquiries from BI users. The consequences might \nbe highly unpleasant if users dip their ﬁ ngers into interim staging pots while data \npreparation is still in process. As with the restaurant kitchen, activities occur in \n\n\nData Warehousing, Business Intelligence, and Dimensional Modeling Primer 25\nthe ETL system that the DW/BI patrons shouldn’t see. When the data is ready and \nquality checked for user consumption, it’s brought through the doorway into the \nDW/BI presentation area.\nData Presentation and BI in the Front Dining Room\nNow turn your attention to the restaurant’s dining room. What are the key fac-\ntors that diff erentiate restaurants? According to the popular restaurant ratings and \nreviews, restaurants are typically scored on four distinct qualities:\n \n■Food (quality, taste, and presentation)\n \n■Decor (appealing, comfortable surroundings for the patrons) \n \n■Service (prompt food delivery, attentive support staff , and food received \nas ordered) \n \n■Cost\nMost patrons focus initially on the food score when they’re evaluating dining \noptions. First and foremost, does the restaurant serve good food? That’s the res-\ntaurant’s primary deliverable. However, the decor, service, and cost factors also \naff ect the patrons’ overall dining experience and are considerations when evaluating \nwhether to eat at a restaurant. \nOf course, the primary deliverable from the DW/BI kitchen is the data in \nthe presentation area. What data is available? Like the restaurant, the DW/BI \nsystem provides “menus” to describe what’s available via metadata, published \nreports, and parameterized analytic applications. The DW/BI patrons expect con-\nsistency and high quality. The presentation area’s data must be properly prepared \nand safe to consume.\nThe presentation area’s decor should be organized for the patrons’ comfort. It \nmust be designed based on the preferences of the BI diners, not the development \nstaff . Service is also critical in the DW/BI system. Data must be delivered, as ordered, \npromptly in a form that is appealing to the business user or BI application developer.\nFinally, cost is a factor for the DW/BI system. The kitchen staff  may be dream-\ning up elaborate, expensive meals, but if there’s no market at that price point, the \nrestaurant won’t survive.\nIf restaurant patrons like their dining experience, then everything is rosy for \nthe restaurant manager. The dining room is always busy; sometimes there’s even \na waiting list. The restaurant manager’s performance metrics are all promising: \nhigh numbers of diners, table turnovers, and nightly revenue and proﬁ t, while staff  \nturnover is low. Things look so good that the restaurant’s owner is considering an \nexpansion site to handle the traffi  c. On the other hand, if the restaurant’s diners \naren’t happy, things go downhill in a hurry. With a limited number of patrons, \nthe restaurant isn’t making enough money to cover its expenses, and the staff  isn’t \nmaking any tips. In a relatively short time, the restaurant closes.\n\n\nChapter 1\n26\nRestaurant managers often proactively check on their diners’ satisfaction with \nthe food and dining experience. If a patron is unhappy, they take immediate action \nto rectify the situation. Similarly, DW/BI managers should proactively monitor sat-\nisfaction. You can’t aff ord to wait to hear complaints. Often, people will abandon \na restaurant without even voicing their concerns. Over time, managers notice that \ndiner counts have dropped but may not even know why.\nInevitably, the prior DW/BI patrons will locate another “restaurant” that bet-\nter suits their needs and preferences, wasting the millions of dollars invested to \ndesign, build, and staff  the DW/BI system. Of course, you can prevent this unhappy \nending by managing the restaurant proactively; make sure the kitchen is properly \norganized and utilized to deliver as needed to the presentation area’s food, decor, \nservice, and cost.\nAlternative DW/BI Architectures\nHaving  just described the Kimball architecture, let’s discuss several other DW/BI \narchitectural approaches. We’ll quickly review the two dominant alternatives to the \nKimball architecture, highlighting the similarities and diff erences. We’ll then close \nthis section by focusing on a hybrid approach that combines alternatives.\nFortunately, over the past few decades, the diff erences between the Kimball \narchitecture and the alternatives have softened. Even more fortunate, there’s a role \nfor dimensional modeling regardless of your architectural predisposition.\nWe acknowledge that organizations have successfully constructed DW/BI systems \nbased on the approaches advocated by others. We strongly believe that rather than \nencouraging more consternation over our philosophical diff erences, the industry \nwould be far better off  devoting energy to ensure that our DW/BI deliverables are \nbroadly accepted by the business to make better, more informed decisions. The \narchitecture should merely be a means to this objective.\nIndependent Data Mart Architecture\nWith  this approach, analytic data is deployed on a departmental basis without con-\ncern to sharing and integrating information across the enterprise, as illustrated in \nFigure 1-8. Typically, a single department identiﬁ es requirements for data from an \noperational source system. The department works with IT staff  or outside consul-\ntants to construct a database that satisﬁ es their departmental needs, reﬂ ecting their \nbusiness rules and preferred labeling. Working in isolation, this departmental data \nmart addresses the department’s analytic requirements.\nMeanwhile, another department is interested in the same source data. It’s extremely \ncommon for multiple departments to be interested in the same performance met-\nrics resulting from an organization’s core business process events. But because this \n\n\nData Warehousing, Business Intelligence, and Dimensional Modeling Primer 27\ndepartment doesn’t have access to the data mart initially constructed by the other \ndepartment, it proceeds down a similar path on its own, obtaining resources and \nbuilding a departmental solution that contains similar, but slightly diff erent data. \nWhen business users from these two departments discuss organizational perfor-\nmance based on reports from their respective repositories, not surprisingly, none of \nthe numbers match because of the diff erences in business rules and labeling.\nSource\nTransactions\nBack Room\nBI Applications for\nDepartment #1\nData Mart for\nDepartment #1\nData Mart for\nDepartment #2\nData Mart for\nDepartment #3\nBI Applications for\nDepartment #2\nBI Applications for\nDepartment #3\nFront Room\nETL\nETL\nETL\nETL\nETL\nFigure 1-8: Simpliﬁ ed illustration of the independent data mart “architecture.”\nThese standalone analytic silos represent a DW/BI “architecture” that’s essen-\ntially un-architected. Although no industry leaders advocate these independent \ndata marts, this approach is prevalent, especially in large organizations. It mirrors \nthe way many organizations fund IT projects, plus it requires zero cross-organi-\nzational data governance and coordination. It’s the path of least resistance for fast \ndevelopment at relatively low cost, at least in the short run. Of course, multiple \nuncoordinated extracts from the same operational sources and redundant storage \nof analytic data are ineffi  cient and wasteful in the long run. Without any enterprise \nperspective, this independent approach results in myriad standalone point solutions \nthat perpetuate incompatible views of the organization’s performance, resulting in \nunnecessary organizational debate and reconciliation.\nWe strongly discourage the independent data mart approach. However, often \nthese independent data marts have embraced dimensional modeling because they’re \ninterested in delivering data that’s easy for the business to understand and highly \nresponsive to queries. So our concepts of dimensional modeling are often applied \nin this architecture, despite the complete disregard for some of our core tenets, such \nas focusing on atomic details, building by business process instead of department, \nand leveraging conformed dimensions for enterprise consistency and integration.\n",
      "page_number": 50
    },
    {
      "number": 7,
      "title": "Segment 7 (pages 59-68)",
      "start_page": 59,
      "end_page": 68,
      "detection_method": "topic_boundary",
      "content": "Chapter 1\n28\nHub-and-Spoke Corporate Information Factory \nInmon Architecture\nThe  hub-and-spoke Corporate Information Factory (CIF) approach is advocated \nby Bill Inmon and others in the industry. Figure 1-9 illustrates a simpliﬁ ed version \nof the CIF, focusing on the core elements and concepts that warrant discussion.\nSource\nTransactions\nBack Room\nEnterprise Data\nWarehouse (EDW)\n•  Normalized\n \ntables (3NF)\n• Atomic data\n• User queryable\nFront Room\nD\na\nt\na\n \nA\nc\nq\nu\ni\ns\ni\nt\ni\no\nn\nD\na\nt\na\n \nD\ne\nl\ni\nv\ne\nr\ny\nB\nI\n \nA\np\np\nl\ni\nc\na\nt\ni\no\nn\ns\nData Marts:\n•  Dimensional\n• Often\n \nsummarized\n• Often\n \ndepartmental\nFigure 1-9: Simpliﬁ ed illustration of the hub-and-spoke Corporate Information Factory \narchitecture.\nWith the CIF, data is extracted from the operational source systems and processed \nthrough an ETL system sometimes referred to as data acquisition. The atomic data \nthat results from this processing lands in a 3NF database; this normalized, atomic \nrepository is referred to as the Enterprise Data Warehouse (EDW) within the CIF \narchitecture. Although the Kimball architecture enables optional normalization to \nsupport ETL processing, the normalized EDW is a mandatory construct in the CIF. \nLike the Kimball approach, the CIF advocates enterprise data coordination and inte-\ngration. The CIF says the normalized EDW ﬁ lls this role, whereas the Kimball archi-\ntecture stresses the importance of an enterprise bus with conformed dimensions.\nNOTE \nThe process of normalization does not technically speak to integration. \nNormalization  simply creates physical tables that implement many-to-one rela-\ntionships. Integration, on the other hand, requires that inconsistencies arising \nfrom separate sources be resolved. Separate incompatible database sources can be \nnormalized to the hilt without addressing integration. The Kimball architecture \n\n\nData Warehousing, Business Intelligence, and Dimensional Modeling Primer 29\nbased on conformed dimensions reverses this logic and focuses on resolving data \ninconsistencies without explicitly requiring normalization.\nOrganizations who have adopted the CIF approach often have business users \naccessing the EDW repository due to its level of detail or data availability timeli-\nness. However, subsequent ETL data delivery processes also populate downstream \nreporting and analytic environments to support business users. Although often \ndimensionally structured, the resultant analytic databases typically diff er from \nstructures in the Kimball architecture’s presentation area in that they’re frequently \ndepartmentally-centric (rather than organized around business processes) and popu-\nlated with aggregated data (rather than atomic details). If the data delivery ETL \nprocesses apply business rules beyond basic summarization, such as departmental \nrenaming of columns or alternative calculations, it may be diffi  cult to tie these \nanalytic databases to the EDW’s atomic repository.\nNOTE \nThe most extreme form of a pure CIF architecture is unworkable as a data \nwarehouse, in our opinion. Such an architecture locks the atomic data in diffi  cult-\nto-query normalized structures, while delivering departmentally incompatible data \nmarts to diff erent groups of business users. But before being too depressed by this \nview, stay tuned for the next section.\nHybrid Hub-and-Spoke and Kimball Architecture\nThe  ﬁ nal architecture warranting discussion is the marriage of the Kimball and \nInmon CIF architectures. As illustrated in Figure 1-10, this architecture populates \na CIF-centric EDW that is completely off -limits to business users for analysis and \nreporting. It’s merely the source to populate a Kimball-esque presentation area \nin which the data is dimensional, atomic (complemented by aggregates), process-\ncentric, and conforms to the enterprise data warehouse bus architecture.\nSome proponents of this blended approach claim it’s the best of both worlds. Yes, it \nblends the two enterprise-oriented approaches. It may leverage a preexisting invest-\nment in an integrated repository, while addressing the performance and usability \nissues associated with the 3NF EDW by offl  oading queries to the dimensional presen-\ntation area. And because the end deliverable to the business users and BI applications \nis constructed based on Kimball tenets, who can argue with the approach?\nIf you’ve already invested in the creation of a 3NF EDW, but it’s not delivering \non the users’ expectations of fast and ﬂ exible reporting and analysis, this hybrid \napproach might be appropriate for your organization. If you’re starting with a blank \nsheet of paper, the hybrid approach will likely cost more time and money, both dur-\ning development and ongoing operation, given the multiple movements of data and \n\n\nChapter 1\n30\nredundant storage of atomic details. If you have the appetite, the perceived need, and \nperhaps most important, the budget and organizational patience to fully normalize \nand instantiate your data before loading it into dimensional structures that are well \ndesigned according to the Kimball methods, go for it.\nSource\nTransactions\nBack Room\nETL\nETL\nPresentation Area:\n•  Dimensional (star\n \nschema or OLAP\n \ncube)\n• Atomic and\n \nsummary data\n• Organized by\n \nbusiness process\n•  Uses conformed\n \ndimensions\nEnterprise DW Bus\nArchitecture\nFront Room\nEnterprise Data\nWarehouse (EDW)\n•  Normalized\n \ntables (3NF)\n• Atomic data\nB\nI\n \nA\np\np\nl\ni\nc\na\nt\ni\no\nn\ns\nFigure 1-10: Hybrid architecture with 3NF structures and dimensional Kimball \npresentation area.\nDimensional Modeling Myths\nDespite  the widespread acceptance of dimensional modeling, some misperceptions \npersist in the industry. These false assertions are a distraction, especially when you \nwant to align your team around common best practices. If folks in your organiza-\ntion continually lob criticisms about dimensional modeling, this section should \nbe on their recommended reading list; their perceptions may be clouded by these \ncommon misunderstandings.\nMyth 1: Dimensional Models are Only \nfor Summary Data\nThis  ﬁ rst myth is frequently the root cause of ill-designed dimensional models. \nBecause you can’t possibly predict all the questions asked by business users, you \nneed to provide them with queryable access to the most detailed data so they can \nroll it up based on the business question. Data at the lowest level of detail is practi-\ncally impervious to surprises or changes. Summary data should complement the \n\n\nData Warehousing, Business Intelligence, and Dimensional Modeling Primer 31\ngranular detail solely to provide improved performance for common queries, but \nnot replace the details.\nA related corollary to this ﬁ rst myth is that only a limited amount of historical \ndata should be stored in dimensional structures. Nothing about a dimensional model \nprohibits storing substantial history. The amount of history available in dimensional \nmodels must only be driven by the business’s requirements.\nMyth 2: Dimensional Models are Departmental, \nNot Enterprise\nRather  than drawing boundaries based on organizational departments, dimensional \nmodels should be organized around business processes, such as orders, invoices, and \nservice calls. Multiple business functions often want to analyze the same metrics \nresulting from a single business process. Multiple extracts of the same source data \nthat create multiple, inconsistent analytic databases should be avoided.\nMyth 3: Dimensional Models are Not Scalable\nDimensional  models are extremely scalable. Fact tables frequently have billions of \nrows; fact tables containing 2 trillion rows have been reported. The database ven-\ndors have wholeheartedly embraced DW/BI and continue to incorporate capabilities \ninto their products to optimize dimensional models’ scalability and performance.\nBoth normalized and dimensional models contain the same information and data \nrelationships; the logical content is identical. Every data relationship expressed in \none model can be accurately expressed in the other. Both normalized and dimen-\nsional models can answer exactly the same questions, albeit with varying diffi  culty.\nMyth 4: Dimensional Models are Only \nfor Predictable Usage\nDimensional  models should not be designed by focusing on predeﬁ ned reports \nor analyses; the design should center on measurement processes. Obviously, it’s \nimportant to consider the BI application’s ﬁ ltering and labeling requirements. But \nyou shouldn’t design for a top ten list of reports in a vacuum because this list is \nbound to change, making the dimensional model a moving target. The key is to \nfocus on the organization’s measurement events that are typically stable, unlike \nanalyses that are constantly evolving.\nA related corollary is that dimensional models aren’t responsive to changing busi-\nness needs. On the contrary, because of their symmetry, dimensional structures are \nextremely ﬂ exible and adaptive to change. The secret to query ﬂ exibility is building \n\n\nChapter 1\n32\nfact tables at the most granular level. Dimensional models that deliver only summary \ndata are bound to be problematic; users run into analytic brick walls when they try \nto drill down into details not available in the summary tables. Developers also run \ninto brick walls because they can’t easily accommodate new dimensions, attributes, \nor facts with these prematurely summarized tables. The correct starting point for \nyour dimensional models is to express data at the lowest detail possible for maxi-\nmum ﬂ exibility and extensibility. Remember, when you pre-suppose the business \nquestion, you’ll likely pre-summarize the data, which can be fatal in the long run.\nAs the architect Mies van der Rohe is credited with saying, “God is in the details.” \nDelivering dimensional models populated with the most detailed data possible ensures \nmaximum ﬂ exibility and extensibility. Delivering anything less in your dimensional \nmodels undermines the foundation necessary for robust business intelligence.\nMyth 5: Dimensional Models Can’t Be Integrated\nDimensional  models most certainly can be integrated if they conform to the enterprise \ndata warehouse bus architecture. Conformed dimensions are built and maintained \nas centralized, persistent master data in the ETL system and then reused across \ndimensional models to enable data integration and ensure semantic consistency. Data \nintegration depends on standardized labels, values, and deﬁ nitions. It is hard work \nto reach organizational consensus and then implement the corresponding ETL rules, \nbut you can’t dodge the eff ort, regardless of whether you’re populating normalized \nor dimensional models.\nPresentation area databases that don’t adhere to the bus architecture \nwith shared conformed dimensions lead to standalone solutions. You can’t hold \ndimensional modeling responsible for organizations’ failure to embrace one of its \nfundamental tenets.\nMore Reasons to Think Dimensionally\nThe majority  of this book focuses on dimensional modeling for designing databases \nin the DW/BI presentation area. But dimensional modeling concepts go beyond the \ndesign of simple and fast data structures. You should think dimensionally at other \ncritical junctures of a DW/BI project.\nWhen gathering requirements for a DW/BI initiative, you need to listen for and \nthen synthesize the ﬁ ndings around business processes. Sometimes teams get lulled \ninto focusing on a set of required reports or dashboard gauges. Instead you should \nconstantly ask yourself about the business process measurement events producing \nthe report or dashboard metrics. When specifying the project’s scope, you must stand \n\n\nData Warehousing, Business Intelligence, and Dimensional Modeling Primer 33\nﬁ rm to focus on a single business process per project and not sign up to deploy a \ndashboard that covers a handful of them in a single iteration.\nAlthough it’s critical that the DW/BI team concentrates on business processes, it’s \nequally important to get IT and business management on the same wavelength. Due \nto historical IT funding policies, the business may be more familiar with depart-\nmental data deployments. You need to shift their mindset about the DW/BI rollout \nto a process perspective. When prioritizing opportunities and developing the DW/\nBI roadmap, business processes are the unit of work. Fortunately, business man-\nagement typically embraces this approach because it mirrors their thinking about \nkey performance indicators. Plus, they’ve lived with the inconsistencies, incessant \ndebates, and never ending reconciliations caused by the departmental approach, so \nthey’re ready for a fresh tactic. Working with business leadership partners, rank each \nbusiness process on business value and feasibility, then tackle processes with the \nhighest impact and feasibility scores ﬁ rst. Although prioritization is a joint activity \nwith the business, your underlying understanding of the organization’s business \nprocesses is essential to its eff ectiveness and subsequent actionability.\nIf tasked with drafting the DW/BI system’s data architecture, you need to wrap \nyour head around the organization’s processes, along with the associated master \ndescriptive dimension data. The prime deliverable for this activity, the enterprise \ndata warehouse bus matrix, will be fully vetted in Chapter 4. The matrix also serves \nas a useful tool for touting the potential beneﬁ ts of a more rigorous master data \nmanagement platform.\nData stewardship or governance programs should focus ﬁ rst on the major dimen-\nsions. Depending on the industry, the list might include date, customer, product, \nemployee, facility, provider, student, faculty, account, and so on. Thinking about \nthe central nouns used to describe the business translates into a list of data gov-\nernance eff orts to be led by subject matter experts from the business community. \nEstablishing data governance responsibilities for these nouns is the key to eventually \ndeploying dimensions that deliver consistency and address the business’s needs for \nanalytic ﬁ ltering, grouping, and labeling. Robust dimensions translate into robust \nDW/BI systems.\nAs you can see, the fundamental motivation for dimensional modeling is front and \ncenter long before you design star schemas or OLAP cubes. Likewise, the dimen-\nsional model will remain in the forefront during the subsequent ETL system and BI \napplication designs. Dimensional modeling concepts link the business and technical \ncommunities together as they jointly design the DW/BI deliverables. We’ll elaborate \non these ideas in Chapter 17: Kimball DW/BI Lifecycle Overview and Chapter 18: \nDimensional Modeling Process and Tasks, but wanted to plant the seeds early so \nthey have time to germinate.\n\n\nChapter 1\n34\nAgile Considerations\nCurrently,  there’s signiﬁ cant interest within the DW/BI industry on agile development \npractices. At the risk of oversimpliﬁ cation, agile methodologies focus on manage-\nably sized increments of work that can be completed within reasonable timeframes \nmeasured in weeks, rather than tackling a much larger scoped (and hence riskier) \nproject with deliverables promised in months or years. Sounds good, doesn’t it?\nMany of the core tenets of agile methodologies align with Kimball best practices, \nincluding\n \n■Focus on delivering business value. This has been the Kimball mantra for \ndecades.\n \n■Value collaboration between the development team and business stakehold-\ners. Like the agile camp, we strongly encourage a close partnership with the \nbusiness.\n \n■Stress ongoing face-to-face communication, feedback, and prioritization with \nthe business stakeholders.\n \n■Adapt quickly to inevitably evolving requirements.\n \n■Tackle development in an iterative, incremental manner. \nAlthough this list is compelling, a common criticism of the agile approaches is the \nlack of planning and architecture, coupled with ongoing governance challenges. The \nenterprise data warehouse bus matrix is a powerful tool to address these shortcom-\nings. The bus matrix provides a framework and master plan for agile development, \nplus identiﬁ es the reusable common descriptive dimensions that provide both data \nconsistency and reduced time-to-market delivery. With the right collaborative mix \nof business and IT stakeholders in a room, the enterprise data warehouse bus matrix \ncan be produced in relatively short order. Incremental development work can produce \ncomponents of the framework until suffi  cient functionality is available and then \nreleased to the business community.\nSome clients and students lament that although they want to deliver consistently \ndeﬁ ned conformed dimensions in their DW/BI environments, it’s “just not feasible.” \nThey explain that they would if they could, but with the focus on agile development \ntechniques, it’s “impossible” to take the time to get organizational agreement on \nconformed dimensions. We argue that conformed dimensions enable agile DW/BI \ndevelopment, along with agile decision making. As you ﬂ esh out the portfolio of mas-\nter conformed dimensions, the development crank starts turning faster and faster. \nThe time-to-market for a new business process data source shrinks as developers \nreuse existing conformed dimensions. Ultimately, new ETL development focuses \nalmost exclusively on delivering more fact tables because the associated dimension \ntables are already sitting on the shelf ready to go.\n\n\nData Warehousing, Business Intelligence, and Dimensional Modeling Primer 35\nWithout a framework like the enterprise data warehouse bus matrix, some DW/\nBI teams have fallen into the trap of using agile techniques to create analytic or \nreporting solutions in a vacuum. In most situations, the team worked with a small \nset of users to extract a limited set of source data and make it available to solve \ntheir unique problems. The outcome is often a standalone data stovepipe that others \ncan’t leverage, or worse yet, delivers data that doesn’t tie to the organization’s other \nanalytic information. We encourage agility, when appropriate, however building \nisolated data sets should be avoided. As with most things in life, moderation and \nbalance between extremes is almost always prudent.\nSummary\nIn this chapter we discussed the overriding goals for DW/BI systems and the fun-\ndamental concepts of dimensional modeling. The Kimball DW/BI architecture and \nseveral alternatives were compared. We closed out the chapter by identifying com-\nmon misunderstandings that some still hold about dimensional modeling, despite \nits widespread acceptance across the industry, and challenged you to think dimen-\nsionally beyond data modeling. In the next chapter, you get a turbocharged tour \nof dimensional modeling patterns and techniques, and then begin putting these \nconcepts into action in your ﬁ rst case study in Chapter 3.\n\n\nKimball Dimensional \nModeling \nTechniques \nOverview\nS\ntarting with the first edition of The Data Warehouse Toolkit (Wiley, 1996), the \nKimball Group has defined the complete set of techniques for modeling data \nin a dimensional way. In the first two editions of this book, we felt the techniques \nneeded to be introduced through familiar use cases drawn from various industries. \nAlthough we still feel business use cases are an essential pedagogical approach, the \ntechniques have become so standardized that some dimensional modelers reverse \nthe logic by starting with the technique and then proceeding to the use case for \ncontext. All of this is good news!\nThe Kimball techniques have been accepted as industry best practices. \nAs evidence, some former Kimball University students have published their own \ndimensional modeling books. These books usually explain the Kimball techniques \naccurately, but it is a sign of our techniques’ resilience that alternative books have \nnot extended the library of techniques in signiﬁ cant ways or off ered conﬂ icting \nguidance.\nThis chapter is the “offi  cial” list of Kimball Dimensional Modeling Techniques \nfrom the inventors of these design patterns. We don’t expect you to read this chapter \nfrom beginning to end at ﬁ rst. But we intend the chapter to be a reference for our \ntechniques. With each technique, we’ve included pointers to subsequent chapters \nfor further explanation and illustrations based on the motivating use cases. \nFundamental Concepts\nThe techniques in this section must be considered during every dimensional \ndesign. Nearly every chapter in the book references or illustrates the concepts in \nthis section.\nGather Business Requirements and Data Realities\nBefore  launching a dimensional modeling eff ort, the team needs to understand the \nneeds of the business, as well as the realities of the underlying source data. You \n2\n\n\nChapter 2\n38\nuncover the requirements via sessions with business representatives to understand \ntheir objectives based on key performance indicators, compelling business issues, \ndecision-making processes, and supporting analytic needs. At the same time, data \nrealities are uncovered by meeting with source system experts and doing high-level \ndata proﬁ ling to assess data feasibilities.\nChapter 1 \nDW/BI and Dimensional Modeling Primer , p 5\nChapter 3 \nRetail Sales , p 70\nChapter 11 Telecommunications , p 297\nChapter 17 Lifecycle Overview , p 412 \nChapter 18 Dimensional Modeling Process and Tasks , p 431\nChapter 19 ETL Subsystems and Techniques ,p 444\nCollaborative Dimensional Modeling Workshops\nDimensional  models should be designed in collaboration with subject matter experts \nand data governance representatives from the business. The data modeler is in \ncharge, but the model should unfold via a series of highly interactive workshops \nwith business representatives. These workshops provide another opportunity to \nﬂ esh out the requirements with the business. Dimensional models should not be \ndesigned in isolation by folks who don’t fully understand the business and their \nneeds; collaboration is critical!\nChapter 3 \nRetail Sales , p 70\nChapter 4 \nInventory , p 135\nChapter 18 Dimensional Modeling Process and Tasks , p 429\nFour-Step Dimensional Design Process\nThe  four key decisions made during the design of a dimensional model include:\n \n1. Select the business process.\n 2. Declare the grain.\n \n3. Identify the dimensions.\n \n4. Identify the facts.\nThe answers to these questions are determined by considering the needs of the \nbusiness along with the realities of the underlying source data during the collab-\norative modeling sessions. Following the business process, grain, dimension, and \nfact declarations, the design team determines the table and column names, sample \ndomain values, and business rules. Business data governance representatives must \nparticipate in this detailed design activity to ensure business buy-in.\n",
      "page_number": 59
    },
    {
      "number": 8,
      "title": "Segment 8 (pages 69-77)",
      "start_page": 69,
      "end_page": 77,
      "detection_method": "topic_boundary",
      "content": "Kimball Dimensional Modeling Techniques Overview 39\nChapter 3 \nRetail Sales , p 70\nChapter 11 Telecommunications , p 300\nChapter 18 Dimensional Modeling Process and Tasks , p 434\nBusiness Processes\nBusiness processes  are the operational activities performed by your organization, \nsuch as taking an order, processing an insurance claim, registering students for a \nclass, or snapshotting every account each month. Business process events generate \nor capture performance metrics that translate into facts in a fact table. Most fact \ntables focus on the results of a single business process. Choosing the process is \nimportant because it deﬁ nes a speciﬁ c design target and allows the grain, dimen-\nsions, and facts to be declared. Each business process corresponds to a row in the \nenterprise data warehouse bus matrix.\nChapter 1 \nDW/BI and Dimensional Modeling Primer , p 10\nChapter 3 \nRetail Sales , p 70\nChapter 17 Lifecycle Overview , p 414\nChapter 18 Dimensional Modeling Process and Tasks , p 435\nGrain\nDeclaring  the grain is the pivotal step in a dimensional design. The grain establishes \nexactly what a single fact table row represents. The grain declaration becomes a bind-\ning contract on the design. The grain must be declared before choosing dimensions \nor facts because every candidate dimension or fact must be consistent with the grain. \nThis consistency enforces a uniformity on all dimensional designs that is critical to \nBI application performance and ease of use. Atomic grain refers to the lowest level at \nwhich data is captured by a given business process. We strongly encourage you to start \nby focusing on atomic-grained data because it withstands the assault of unpredictable \nuser queries; rolled-up summary grains are important for performance tuning, but they \npre-suppose the business’s common questions. Each proposed fact table grain results \nin a separate physical table; diff erent grains must not be mixed in the same fact table.\nChapter 1 \nDW/BI and Dimensional Modeling Primer, p 30\nChapter 3 \nRetail Sales , p 71\nChapter 4 \nInventory , p 112\nChapter 6 \nOrder Management, p 184\nChapter 11 Telecommunications , p 300\nChapter 12 Transportation , p 312\nChapter 18 Dimensional Modeling Process and Tasks , p 435\n\n\nChapter 2\n40\nDimensions for Descriptive Context\nDimensions  provide the “who, what, where, when, why, and how” context surround-\ning a business process event. Dimension tables contain the descriptive attributes \nused by BI applications for ﬁ ltering and grouping the facts. With the grain of a fact \ntable ﬁ rmly in mind, all the possible dimensions can be identiﬁ ed. Whenever pos-\nsible, a dimension should be single valued when associated with a given fact row. \nDimension tables are sometimes called the “soul” of the data warehouse because \nthey contain the entry points and descriptive labels that enable the DW/BI system \nto be leveraged for business analysis. A disproportionate amount of eff ort is put \ninto the data governance and development of dimension tables because they are \nthe drivers of the user’s BI experience. \nChapter 1 \nDW/BI and Dimensional Modeling Primer , p 13\nChapter 3 \nRetail Sales , p 72\nChapter 11 Telecommunications , p 301\nChapter 18 Dimensional Modeling Process and Tasks , p 437\nChapter 19 ETL Subsystems and Techniques , p 463\nFacts for Measurements\nFacts  are the measurements that result from a business process event and are almost \nalways numeric. A single fact table row has a one-to-one relationship to a measurement \nevent as described by the fact table’s grain. Thus a fact table corresponds to a physi-\ncal observable event, and not to the demands of a particular report. Within a fact \ntable, only facts consistent with the declared grain are allowed. For example, in a \nretail sales transaction, the quantity of a product sold and its extended price are \ngood facts, whereas the store manager’s salary is disallowed. \nChapter 1 \nDW/BI and Dimensional Modeling Primer , p 10\nChapter 3 \nRetail Sales , p 72\nChapter 4 \nInventory , p 112\nChapter 18 Dimensional Modeling Process and Tasks , p 437\nStar Schemas and OLAP Cubes\nStar schemas  are dimensional structures deployed in a relational database management \nsystem (RDBMS). They characteristically consist of fact tables linked to associated \ndimension tables via primary/foreign key relationships. An online analytical processing \n(OLAP) cube is a dimensional structure implemented in a multidimensional database; \nit can be equivalent in content to, or more often derived from, a relational star schema. \nAn OLAP cube contains dimensional attributes and facts, but it is accessed through \nlanguages with more analytic capabilities than SQL, such as XMLA and MDX. OLAP \n\n\nKimball Dimensional Modeling Techniques Overview 41\ncubes are included in this list of basic techniques because an OLAP cube is often \nthe ﬁ nal step in the deployment of a dimensional DW/BI system, or may exist as an \naggregate structure based on a more atomic relational star schema. \nChapter 1 \nDW/BI and Dimensional Modeling Primer , p 8\nChapter 3 \nRetail Sales , p 94\nChapter 5 \nProcurement , p 149\nChapter 6 \nOrder Management , p 170\nChapter 7 \nAccounting , p 226\nChapter 9 \nHuman Resources Management , p 273\nChapter 13 Education , p 335\nChapter 19 ETL Subsystems and Techniques , p 481\nChapter 20 ETL System Process and Tasks , p 519\nGraceful Extensions to Dimensional Models\nDimensional  models are resilient when data relationships change. All the following \nchanges can be implemented without altering any existing BI query or application, \nand without any change in query results.\n \n■Facts consistent with the grain of an existing fact table can be added by creat-\ning new columns.\n \n■Dimensions can be added to an existing fact table by creating new foreign key \ncolumns, presuming they don’t alter the fact table’s grain.\n \n■Attributes can be added to an existing dimension table by creating new \ncolumns.\n \n■The grain of a fact table can be made more atomic by adding attributes to an exist-\ning dimension table, and then restating the fact table at the lower grain, being \ncareful to preserve the existing column names in the fact and dimension tables.\nChapter 3 \nRetail Sales , p 95\nBasic Fact Table Techniques\nThe techniques in this section apply to all fact tables. There are illustrations of fact \ntables in nearly every chapter.\nFact Table Structure\nA fact table  contains the numeric measures produced by an operational measure-\nment event in the real world. At the lowest grain, a fact table row corresponds to a \nmeasurement event and vice versa. Thus the fundamental design of a fact table is \nentirely based on a physical activity and is not inﬂ uenced by the eventual reports \n\n\nChapter 2\n42\nthat may be produced. In addition to numeric measures, a fact table always contains \nforeign keys for each of its associated dimensions, as well as optional degenerate \ndimension keys and date/time stamps. Fact tables are the primary target of compu-\ntations and dynamic aggregations arising from queries.\nChapter 1 \nDW/BI and Dimensional Modeling Primer , p 10\nChapter 3 \nRetail Sales , p 76\nChapter 5 \nProcurement, p 143\nChapter 6 \nOrder Management, p 169\nAdditive, Semi-Additive, Non-Additive Facts\nThe  numeric measures in a fact table fall into three categories. The most ﬂ exible and \nuseful facts are fully additive; additive measures can be summed across any of the \ndimensions associated with the fact table. Semi-additive measures can be summed \nacross some dimensions, but not all; balance amounts are common semi-additive facts \nbecause they are additive across all dimensions except time. Finally, some measures \nare completely non-additive, such as ratios. A good approach for non-additive facts is, \nwhere possible, to store the fully additive components of the non-additive measure \nand sum these components into the ﬁ nal answer set before calculating the ﬁ nal \nnon-additive fact. This ﬁ nal calculation is often done in the BI layer or OLAP cube. \nChapter 1 \nDW/BI and Dimensional Modeling Primer , p 10\nChapter 3 \nRetail Sales , p 76\nChapter 4 \nInventory , p 114\nChapter 7 \nAccounting , p 204\nNulls in Fact Tables\nNull-valued measurements behave gracefully in fact tables. The aggregate functions \n(SUM, COUNT, MIN, MAX, and AVG) all do the “right thing” with null facts. However, \nnulls must be avoided in the fact table’s foreign keys because these nulls would \nautomatically cause a referential integrity  violation. Rather than a null foreign key, \nthe associated dimension table must have a default row (and surrogate key) repre-\nsenting the unknown or not applicable condition.\nChapter 3 \nRetail Sales , p 92\nChapter 20 ETL System Process and Tasks , p 509\nConformed Facts\nIf  the same measurement appears in separate fact tables, care must be taken to make \nsure the technical deﬁ nitions of the facts are identical if they are to be compared \n\n\nKimball Dimensional Modeling Techniques Overview 43\nor computed together. If the separate fact deﬁ nitions are consistent, the conformed \nfacts should be identically named; but if they are incompatible, they should be dif-\nferently named to alert the business users and BI applications. \nChapter 4 \nInventory , p 138\nChapter 16 Insurance , p 386\nTransaction Fact Tables\nA  row in a transaction fact table corresponds to a measurement event at a point in \nspace and time. Atomic transaction grain fact tables are the most dimensional and \nexpressive fact tables; this robust dimensionality enables the maximum slicing \nand dicing of transaction data. Transaction fact tables may be dense or sparse \nbecause rows exist only if measurements take place. These fact tables always con-\ntain a foreign key for each associated dimension, and optionally contain precise \ntime stamps and degenerate dimension keys. The measured numeric facts must be \nconsistent with the transaction grain.\nChapter 3 \nRetail Sales , p 79\nChapter 4 \nInventory , p 116\nChapter 5 \nProcurement , p 142\nChapter 6 \nOrder Management , p 168\nChapter 7 \nAccounting , p 206\nChapter 11 Telecommunications , p 306\nChapter 12 Transportation , p 312\nChapter 14 Healthcare , p 351\nChapter 15 Electronic Commerce , p 363\nChapter 16 Insurance , p 379\nChapter 19 ETL Subsystems and Techniques , p 473\nPeriodic Snapshot Fact Tables\nA  row in a periodic snapshot fact table summarizes many measurement events occur-\nring over a standard period, such as a day, a week, or a month. The grain is the \nperiod, not the individual transaction. Periodic snapshot fact tables often contain \nmany facts because any measurement event consistent with the fact table grain is \npermissible. These fact tables are uniformly dense in their foreign keys because \neven if no activity takes place during the period, a row is typically inserted in the \nfact table containing a zero or null for each fact. \n\n\nChapter 2\n44\nChapter 4 \nInventory , p 113\nChapter 7 \nAccounting , p 204\nChapter 9 \nHuman Resources Management , p 267\nChapter 10 Financial Services , p 283\nChapter 13 Education , p 333\nChapter 14 Healthcare, p 351\nChapter 16 Insurance , p 385\nChapter 19 ETL Subsystems and Techniques , p 474\nAccumulating Snapshot Fact Tables\nA  row in an accumulating snapshot fact table summarizes the measurement events \noccurring at predictable steps between the beginning and the end of a process. \nPipeline or workﬂ ow processes, such as order fulﬁ llment or claim processing, that \nhave a deﬁ ned start point, standard intermediate steps, and deﬁ ned end point can be \nmodeled with this type of fact table. There is a date foreign key in the fact table for \neach critical milestone in the process. An individual row in an accumulating snap-\nshot fact table, corresponding for instance to a line on an order, is initially inserted \nwhen the order line is created. As pipeline progress occurs, the accumulating fact \ntable row is revisited and updated. This consistent updating of accumulating snap-\nshot fact rows is unique among the three types of fact tables. In addition to the date \nforeign keys associated with each critical process step, accumulating snapshot fact \ntables contain foreign keys for other dimensions and optionally contain degener-\nate dimensions. They often include numeric lag measurements consistent with the \ngrain, along with milestone completion counters. \nChapter 4 \nInventory , p 118\nChapter 5 \nProcurement , p 147\nChapter 6 \nOrder Management , p 194\nChapter 13 Education , p 326\nChapter 14 Healthcare , p 342\nChapter 16 Insurance , p 392\nChapter 19 ETL Subsystems and Techniques , p 475\nFactless Fact Tables\nAlthough  most measurement events capture numerical results, it is possible that \nthe event merely records a set of dimensional entities coming together at a moment \nin time. For example, an event of a student attending a class on a given day may \nnot have a recorded numeric fact, but a fact row with foreign keys for calendar day, \nstudent, teacher, location, and class is well-deﬁ ned. Likewise, customer communi-\ncations are events, but there may be no associated metrics. Factless fact tables can \n\n\nKimball Dimensional Modeling Techniques Overview 45\nalso be used to analyze what didn’t happen. These queries always have two parts: a \nfactless coverage table that contains all the possibilities of events that might happen \nand an activity table that contains the events that did happen. When the activity \nis subtracted from the coverage, the result is the set of events that did not happen. \nChapter 3 \nRetail Sales , p 97\nChapter 6 \nOrder Management , p 176\nChapter 13 Education , p 329\nChapter 16 Insurance , p 396\nAggregate Fact Tables or OLAP Cubes\nAggregate fact tables  are simple numeric rollups of atomic fact table data built solely \nto accelerate query performance. These aggregate fact tables should be available to \nthe BI layer at the same time as the atomic fact tables so that BI tools smoothly \nchoose the appropriate aggregate level at query time. This process, known as \naggregate navigation, must be open so that every report writer, query tool, and BI \napplication harvests the same performance beneﬁ ts. A properly designed set of \naggregates should behave like database indexes, which accelerate query perfor-\nmance but are not encountered directly by the BI applications or business users. \nAggregate fact tables contain foreign keys to shrunken conformed dimensions, as \nwell as aggregated facts created by summing measures from more atomic fact tables. \nFinally, aggregate OLAP cubes with summarized measures are frequently built in \nthe same way as relational aggregates, but the OLAP cubes are meant to be accessed \ndirectly by the business users. \nChapter 15 Electronic Commerce , p 366\nChapter 19 ETL Subsystems and Techniques , p 481\nChapter 20 ETL System Process and Tasks , p 519\nConsolidated Fact Tables\nIt  is often convenient to combine facts from multiple processes together into a single \nconsolidated fact table if they can be expressed at the same grain. For example, sales \nactuals can be consolidated with sales forecasts in a single fact table to make the task \nof analyzing actuals versus forecasts simple and fast, as compared to assembling a \ndrill-across application using separate fact tables. Consolidated fact tables add bur-\nden to the ETL processing, but ease the analytic burden on the BI applications. They \nshould be considered for cross-process metrics that are frequently analyzed together.\nChapter 7 \nAccounting , p 224\nChapter 16 Insurance , p 395\n\n\nChapter 2\n46\nBasic Dimension Table Techniques\nThe techniques in this section apply to all dimension tables. Dimension tables are \ndiscussed and illustrated in every chapter.\nDimension Table Structure\nEvery  dimension table has a single primary key column. This primary key is embedded \nas a foreign key in any associated fact table where the dimension row’s descriptive \ncontext is exactly correct for that fact table row. Dimension tables are usually wide, ﬂ at \ndenormalized tables with many low-cardinality text attributes. While operational codes \nand indicators can be treated as attributes, the most powerful dimension attributes \nare populated with verbose descriptions. Dimension table attributes are the primary \ntarget of constraints and grouping speciﬁ cations from queries and BI applications. The \ndescriptive labels on reports are typically dimension attribute domain values.\nChapter 1 \nDW/BI and Dimensional Modeling Primer , p 13\nChapter 3 \nRetail Sales , p 79\nChapter 11 Telecommunications , p 301\nDimension Surrogate Keys\nA  dimension table is designed with one column serving as a unique primary key. \nThis primary key cannot be the operational system’s natural key because there will \nbe multiple dimension rows for that natural key when changes are tracked over time. \nIn addition, natural keys for a dimension may be created by more than one source \nsystem, and these natural keys may be incompatible or poorly administered. The \nDW/BI system needs to claim control of the primary keys of all dimensions; rather \nthan using explicit natural keys or natural keys with appended dates, you should \ncreate anonymous integer primary keys for every dimension. These dimension sur-\nrogate keys are simple integers, assigned in sequence, starting with the value 1, \nevery time a new key is needed. The date dimension is exempt from the surrogate \nkey rule; this highly predictable and stable dimension can use a more meaningful \nprimary key. See the section “Calendar Date Dimensions.”\nChapter 3 \nRetail Sales , p 98\nChapter 19 ETL Subsystems and Techniques , p 469\nChapter 20 ETL System Process and Tasks , p 506\nNatural, Durable, and Supernatural Keys\nNatural keys  created by  operational source systems are subject to business rules outside \nthe control of the DW/BI system. For instance, an employee number (natural key) may \n\n\nKimball Dimensional Modeling Techniques Overview 47\nbe changed if the employee resigns and then is rehired. When the data warehouse \nwants to have a single key for that employee, a new durable key must be created that is \npersistent and does not change in this situation. This key is sometimes referred to as \na durable supernatural key. The best durable keys have a format that is independent of \nthe original business process and thus should be simple integers assigned in sequence \nbeginning with 1. While multiple surrogate keys may be associated with an employee \nover time as their proﬁ le changes, the durable key never changes. \nChapter 3 \nRetail Sales , p 100\nChapter 20 ETL System Process and Tasks , p 510\nChapter 21 Big Data Analytics, p 539\nDrilling Down\nDrilling down  is the most fundamental way data is analyzed by business users. Drilling \ndown simply means adding a row header to an existing query; the new row header \nis a dimension attribute appended to the GROUP BY expression in an SQL query. The \nattribute can come from any dimension attached to the fact table in the query. Drilling \ndown does not require the deﬁ nition of predetermined hierarchies or drill-down paths. \nSee the section “Drilling Across.”\nChapter 3 \nRetail Sales , p 86\nDegenerate Dimensions\nSometimes  a dimension is deﬁ ned that has no content except for its primary key. \nFor example, when an invoice has multiple line items, the line item fact rows inherit \nall the descriptive dimension foreign keys of the invoice, and the invoice is left with \nno unique content. But the invoice number remains a valid dimension key for fact \ntables at the line item level. This degenerate dimension is placed in the fact table with \nthe explicit acknowledgment that there is no associated dimension table. Degenerate \ndimensions are most common with transaction and accumulating snapshot fact tables.\nChapter 3 \nRetail Sales , p 93\nChapter 6 \nOrder Management , p 178\nChapter 11 Telecommunications , p 303\nChapter 16 Insurance , p 383\nDenormalized Flattened Dimensions\nIn  general, dimensional designers must resist the normalization urges caused by years \nof operational database designs and instead denormalize the many-to-one ﬁ xed depth \n",
      "page_number": 69
    },
    {
      "number": 9,
      "title": "Segment 9 (pages 78-87)",
      "start_page": 78,
      "end_page": 87,
      "detection_method": "topic_boundary",
      "content": "Chapter 2\n48\nhierarchies into separate attributes on a ﬂ attened dimension row. Dimension denor-\nmalization supports dimensional modeling’s twin objectives of simplicity and speed.\nChapter 1 \nDW/BI and Dimensional Modeling Primer , p 13\nChapter 3 \nRetail Sales , p 84\nMultiple Hierarchies in Dimensions\nMany  dimensions contain more than one natural hierarchy. For example, calendar \ndate dimensions may have a day to week to ﬁ scal period hierarchy, as well as a \nday to month to year hierarchy. Location intensive dimensions may have multiple \ngeographic hierarchies. In all of these cases, the separate hierarchies can gracefully \ncoexist in the same dimension table. \nChapter 3 \nRetail Sales , p 88\nChapter 19 ETL Subsystems and Techniques , p 470\nFlags and Indicators as Textual Attributes\nCryptic  abbreviations, true/false ﬂ ags, and operational indicators should be sup-\nplemented in dimension tables with full text words that have meaning when \nindependently viewed. Operational codes with embedded meaning within the \ncode value should be broken down with each part of the code expanded into its \nown separate descriptive dimension attribute.\nChapter 3 \nRetail Sales , p 82\nChapter 11 Telecommunications, p 301 \nChapter 16 Insurance , p 383\nNull Attributes in Dimensions\nNull-valued  dimension attributes result when a given dimension row has not been \nfully populated, or when there are attributes that are not applicable to all the dimen-\nsion’s rows. In both cases, we recommend substituting a descriptive string, such as \nUnknown or Not Applicable in place of the null value. Nulls in dimension attributes \nshould be avoided because diff erent databases handle grouping and constraining \non nulls inconsistently.\nChapter 3 \nRetail Sales , p 92\nCalendar Date Dimensions\nCalendar date dimensions  are attached to virtually every fact table to allow navigation \nof the fact table through familiar dates, months, ﬁ scal periods, and special days on \n\n\nKimball Dimensional Modeling Techniques Overview 49\nthe calendar. You would never want to compute Easter in SQL, but rather want to \nlook it up in the calendar date dimension. The calendar date dimension typically \nhas many attributes describing characteristics such as week number, month name, \nﬁ scal period, and national holiday indicator. To facilitate partitioning, the primary \nkey of a date dimension can be more meaningful, such as an integer representing \nYYYYMMDD, instead of a sequentially-assigned surrogate key. However, the date \ndimension table needs a special row to represent unknown or to-be-determined \ndates. When further precision is needed, a separate date/time stamp can be added \nto the fact table. The date/time stamp is not a foreign key to a dimension table, but \nrather is a standalone column. If business users constrain or group on time-of-day \nattributes, such as day part grouping or shift number, then you would add a separate \ntime-of-day dimension foreign key to the fact table.\nChapter 3 \nRetail Sales , p 79\nChapter 7 \nAccounting , p 208\nChapter 8 \nCustomer Relationship Management , p 238\nChapter 12 Transportation , p 321\nChapter 19 ETL Subsystems and Techniques , p 470\nRole-Playing Dimensions\nA  single physical dimension can be referenced multiple times in a fact table, with \neach reference linking to a logically distinct role for the dimension. For instance, a \nfact table can have several dates, each of which is represented by a foreign key to the \ndate dimension. It is essential that each foreign key refers to a separate view of \nthe date dimension so that the references are independent. These separate dimen-\nsion views (with unique attribute column names) are called roles. \nChapter 6 \nOrder Management , p 170\nChapter 12 Transportation , p 312\nChapter 14 Healthcare , p 345\nChapter 16 Insurance , p 380\nJunk Dimensions\nTransactional  business processes typically produce a number of miscellaneous, low-\ncardinality ﬂ ags and indicators. Rather than making separate dimensions for each \nﬂ ag and attribute, you can create a single junk dimension combining them together. \nThis dimension, frequently labeled as a transaction proﬁ le dimension in a schema, \ndoes not need to be the Cartesian product of all the attributes’ possible values, but \nshould only contain the combination of values that actually occur in the source data. \n\n\nChapter 2\n50\nChapter 6 \nOrder Management , p 179\nChapter 12 Transportation , p 318\nChapter 16 Insurance , p 392\nChapter 19 ETL Subsystems and Techniques , p 470\nSnowﬂ aked Dimensions\nWhen  a hierarchical relationship in a dimension table is normalized, low-cardinal-\nity attributes appear as secondary tables connected to the base dimension table by \nan attribute key. When this process is repeated with all the dimension table’s hier-\narchies, a characteristic multilevel structure is created that is called a snowﬂ ake. \nAlthough the snowﬂ ake represents hierarchical data accurately, you should avoid \nsnowﬂ akes because it is diffi  cult for business users to understand and navigate \nsnowﬂ akes. They can also negatively impact query performance. A ﬂ attened denor-\nmalized dimension table contains exactly the same information as a snowﬂ aked \ndimension. \nChapter 3 \nRetail Sales , p 104\nChapter 11 Telecommunications , p 301\nChapter 20 ETL System Process and Tasks , p 504\nOutrigger Dimensions\nA  dimension can contain a reference to another dimension table. For instance, a \nbank account dimension can reference a separate dimension representing the date \nthe account was opened. These secondary dimension references are called outrigger \ndimensions. Outrigger dimensions are permissible, but should be used sparingly. In \nmost cases, the correlations between dimensions should be demoted to a fact table, \nwhere both dimensions are represented as separate foreign keys. \nChapter 3 \nRetail Sales , p 106\nChapter 5 \nProcurement , p 160\nChapter 8 \nCustomer Relationship Management , p 243\nChapter 12 Transportation , p 321\nIntegration via Conformed Dimensions\nOne of the marquee successes of the dimensional modeling approach has been to \ndeﬁ ne a simple but powerful recipe for integrating data from diff erent business \nprocesses.\n\n\nKimball Dimensional Modeling Techniques Overview 51\nConformed Dimensions\nDimension  tables conform when attributes in separate dimension tables have the \nsame column names and domain contents. Information from separate fact tables \ncan be combined in a single report by using conformed dimension attributes that \nare associated with each fact table. When a conformed attribute is used as the \nrow header (that is, the grouping column in the SQL query), the results from the \nseparate fact tables can be aligned on the same rows in a drill-across report. This \nis the essence of integration in an enterprise DW/BI system. Conformed dimen-\nsions, deﬁ ned once in collaboration with the business’s data governance represen-\ntatives, are reused across fact tables; they deliver both analytic consistency and \nreduced future development costs because the wheel is not repeatedly re-created.\nChapter 4 \nInventory , p 130\nChapter 8 \nCustomer Relationship Management , p 256\nChapter 11 Telecommunications , p 304\nChapter 16 Insurance , p 386\nChapter 18 Dimensional Modeling Process and Tasks , p 431\nChapter 19 ETL Subsystems and Techniques , p 461\nShrunken Dimensions\nShrunken dimensions  are conformed dimensions that are a subset of rows and/or \ncolumns of a base dimension. Shrunken rollup dimensions are required when con-\nstructing aggregate fact tables. They are also necessary for business processes that \nnaturally capture data at a higher level of granularity, such as a forecast by month \nand brand (instead of the more atomic date and product associated with sales data). \nAnother case of conformed dimension subsetting occurs when two dimensions are \nat the same level of detail, but one represents only a subset of rows.\nChapter 4 \nInventory , p 132\nChapter 19 ETL Subsystems and Techniques , p 472\nChapter 20 ETL System Process and Tasks , p 504\nDrilling Across\nDrilling across  simply means making separate queries against two or more fact tables \nwhere the row headers of each query consist of identical conformed attributes. The \nanswer sets from the two queries are aligned by performing a sort-merge opera-\ntion on the common dimension attribute row headers. BI tool vendors refer to this \nfunctionality by various names, including stitch and multipass query.\nChapter 4 \nInventory , p 130\n\n\nChapter 2\n52\nValue Chain\nA value chain  identiﬁ es the natural ﬂ ow of an organization’s primary business \nprocesses. For example, a retailer’s value chain may consist of purchasing to ware-\nhousing to retail sales. A general ledger value chain may consist of budgeting to \ncommitments to payments. Operational source systems typically produce transac-\ntions or snapshots at each step of the value chain. Because each process produces \nunique metrics at unique time intervals with unique granularity and dimensionality, \neach process typically spawns at least one atomic fact table. \nChapter 4 \nInventory , p 111\nChapter 7 \nAccounting , p 210\nChapter 16 Insurance , p 377\nEnterprise Data Warehouse Bus Architecture\nThe enterprise data warehouse bus architecture provides an incremental approach \nto building the enterprise DW/BI system. This architecture decomposes the DW/\nBI planning process into manageable pieces by focusing on business processes, \nwhile delivering integration via standardized conformed dimensions that are reused \nacross processes. It provides an architectural framework, while also decomposing \nthe program to encourage manageable agile implementations corresponding to the \nrows on the enterprise data warehouse bus matrix. The bus architecture is tech-\nnology and database platform independent; both relational and OLAP dimensional \nstructures can participate. \nChapter 1 \nDW/BI and Dimensional Modeling Primer , p 21\nChapter 4 \nInventory , p 123\nEnterprise Data Warehouse Bus Matrix\nThe enterprise data warehouse bus matrix is the essential tool for designing and com-\nmunicating the enterprise data warehouse bus architecture. The rows of the matrix \nare business processes and the columns are dimensions. The shaded cells of the \nmatrix indicate whether a dimension is associated with a given business process. The \ndesign team scans each row to test whether a candidate dimension is well-deﬁ ned for \nthe business process and also scans each column to see where a dimension should be \nconformed across multiple business processes. Besides the technical design consid-\nerations, the bus matrix is used as input to prioritize DW/BI projects with business \nmanagement as teams should implement one row of the matrix at a time. \n\n\nKimball Dimensional Modeling Techniques Overview 53\nChapter 4 \nInventory , p 125\nChapter 5 \nProcurement , p 143\nChapter 6 \nOrder Management , p 168\nChapter 7 \nAccounting, p 202\nChapter 9 \nHuman Resources Management , p 268\nChapter 10 Financial Services , p 282\nChapter 11 Telecommunications , p 297\nChapter 12 Transportation , p 311\nChapter 13 Education , p 325\nChapter 14 Healthcare , p 339\nChapter 15 Electronic Commerce , p 368\nChapter 16 Insurance , p 389\nDetailed Implementation Bus Matrix\nThe detailed implementation bus matrix is a more granular bus matrix where each \nbusiness process row has been expanded to show speciﬁ c fact tables or OLAP cubes. \nAt this level of detail, the precise grain statement and list of facts can be documented.\nChapter 5 \nProcurement , p 143\nChapter 16 Insurance , p 390\nOpportunity/Stakeholder Matrix\nAfter  the enterprise data warehouse bus matrix rows have been identiﬁ ed, you can \ndraft a diff erent matrix by replacing the dimension columns with business func-\ntions, such as marketing, sales, and ﬁ nance, and then shading the matrix cells to \nindicate which business functions are interested in which business process rows. \nThe opportunity/stakeholder matrix helps identify which business groups should be \ninvited to the collaborative design sessions for each process-centric row. \nChapter 4 \nInventory , p 127\nDealing with Slowly Changing Dimension Attributes\nThe  following section describes the fundamental approaches for dealing with slowly \nchanging dimension (SCD) attributes. It is quite common to have attributes in the \nsame dimension table that are handled with diff erent change tracking techniques.\n\n\nChapter 2\n54\nType 0: Retain Original\nWith type 0,  the dimension attribute value never changes, so facts are always grouped \nby this original value. Type 0 is appropriate for any attribute labeled “original,” such \nas a customer’s original credit score or a durable identiﬁ er. It also applies to most \nattributes in a date dimension. \nChapter 5 \nProcurement , p 148\nType 1: Overwrite\nWith type 1,  the old attribute value in the dimension row is overwritten with the new \nvalue; type 1 attributes always reﬂ ects the most recent assignment, and therefore \nthis technique destroys history. Although this approach is easy to implement and \ndoes not create additional dimension rows, you must be careful that aggregate fact \ntables and OLAP cubes aff ected by this change are recomputed. \nChapter 5 \nProcurement , p 149\nChapter 16 Insurance , p 380\nChapter 19 ETL Subsystems and Techniques , p 465\nType 2: Add New Row\nType 2  changes add a new row in the dimension with the updated attribute values. \nThis requires generalizing the primary key of the dimension beyond the natural or \ndurable key because there will potentially be multiple rows describing each member. \nWhen a new row is created for a dimension member, a new primary surrogate key is \nassigned and used as a foreign key in all fact tables from the moment of the update \nuntil a subsequent change creates a new dimension key and updated dimension row.\nA minimum of three additional columns should be added to the dimension row \nwith type 2 changes: 1) row eff ective date or date/time stamp; 2) row expiration \ndate or date/time stamp; and 3) current row indicator. \nChapter 5 \nProcurement , p 150\nChapter 8 \nCustomer Relationship Management , p 243\nChapter 9 \nHuman Resources Management , p 263\nChapter 16 Insurance , p 380\nChapter 19 ETL Subsystems and Techniques , p 465\nChapter 20 ETL System Process and Tasks , p 507\n\n\nKimball Dimensional Modeling Techniques Overview 55\nType 3: Add New Attribute\nType 3  changes add a new attribute in the dimension to preserve the old attribute \nvalue; the new value overwrites the main attribute as in a type 1 change. This kind of \ntype 3 change is sometimes called an alternate reality. A business user can group and \nﬁ lter fact data by either the current value or alternate reality. This slowly changing \ndimension technique is used relatively infrequently.\nChapter 5 \nProcurement , p 154\nChapter 16 Insurance , p 380\nChapter 19 ETL Subsystems and Techniques , p 467\nType 4: Add Mini-Dimension\nThe type 4  technique is used when a group of attributes in a dimension rapidly \nchanges and is split off  to a mini-dimension. This situation is sometimes called a \nrapidly changing monster dimension. Frequently used attributes in multimillion-row \ndimension tables are mini-dimension design candidates, even if they don’t fre-\nquently change. The type 4 mini-dimension requires its own unique primary key; \nthe primary keys of both the base dimension and mini-dimension are captured in \nthe associated fact tables. \nChapter 5 \nProcurement , p 156\nChapter 10 Financial Services , p 289\nChapter 16 Insurance , p 381\nChapter 19 ETL Subsystems and Techniques , p 467\nType 5: Add Mini-Dimension and Type 1 Outrigger\nThe type 5  technique is used to accurately preserve historical attribute values, \nplus report historical facts according to current attribute values. Type 5 builds on \nthe type 4 mini-dimension by also embedding a current type 1 reference to the \nmini-dimension in the base dimension. This enables the currently-assigned mini-\ndimension attributes to be accessed along with the others in the base dimension \nwithout linking through a fact table. Logically, you’d represent the base dimension \nand mini-dimension outrigger as a single table in the presentation area. The ETL \nteam must overwrite this type 1 mini-dimension reference whenever the current \nmini-dimension assignment changes. \nChapter 5 \nProcurement , p 160\nChapter 19 ETL Subsystems and Techniques , p 468\n\n\nChapter 2\n56\nType 6: Add Type 1 Attributes to Type 2 Dimension\nLike  type 5, type 6 also delivers both historical and current dimension attribute \nvalues. Type 6 builds on the type 2 technique by also embedding current type \n1 versions of the same attributes in the dimension row so that fact rows can be \nﬁ ltered or grouped by either the type 2 attribute value in eff ect when the measure-\nment occurred or the attribute’s current value. In this case, the type 1 attribute is \nsystematically overwritten on all rows associated with a particular durable key \nwhenever the attribute is updated. \nChapter 5 \nProcurement , p 160\nChapter 19 ETL Subsystems and Techniques , p 468\nType 7: Dual Type 1 and Type 2 Dimensions\nType 7  is the ﬁ nal hybrid technique used to support both as-was and as-is report-\ning. A fact table can be accessed through a dimension modeled both as a type 1 \ndimension showing only the most current attribute values, or as a type 2 dimen-\nsion showing correct contemporary historical proﬁ les. The same dimension table \nenables both perspectives. Both the durable key and primary surrogate key of the \ndimension are placed in the fact table. For the type 1 perspective, the current ﬂ ag \nin the dimension is constrained to be current, and the fact table is joined via the \ndurable key. For the type 2 perspective, the current ﬂ ag is not constrained, and the \nfact table is joined via the surrogate primary key. These two perspectives would be \ndeployed as separate views to the BI applications. \nChapter 5 \nProcurement , p 162\nChapter 19 ETL Subsystems and Techniques , p 468\nDealing with Dimension Hierarchies\nDimensional hierarchies are commonplace. This section describes approaches for \ndealing with hierarchies, starting with the most basic.\nFixed Depth Positional Hierarchies\nA ﬁ xed depth hierarchy is a series of many-to-one relationships, such as product \nto brand to category to department. When a ﬁ xed depth hierarchy is deﬁ ned and \nthe hierarchy levels have agreed upon names, the hierarchy levels should appear \nas separate positional attributes in a dimension table. A ﬁ xed depth hierarchy is \nby far the easiest to understand and navigate as long as the above criteria are met. \nIt also delivers predictable and fast query performance. When the hierarchy is not \na series of many-to-one relationships or the number of levels varies such that the \n\n\nKimball Dimensional Modeling Techniques Overview 57\nlevels do not have agreed upon names, a ragged hierarchy technique, described \nbelow, must be used. \nChapter 3 \nRetail Sales , p 84\nChapter 7 \nAccounting , p 214\nChapter 19 ETL Subsystems and Techniques , p 470\nChapter 20 ETL System Process and Tasks , p 501\nSlightly Ragged/Variable Depth Hierarchies\nSlightly ragged  hierarchies don’t have a ﬁ xed number of levels, but the range in depth \nis small. Geographic hierarchies often range in depth from perhaps three levels to \nsix levels. Rather than using the complex machinery for unpredictably variable \nhierarchies, you can force-ﬁ t slightly ragged hierarchies into a ﬁ xed depth positional \ndesign with separate dimension attributes for the maximum number of levels, and \nthen populate the attribute value based on rules from the business. \nChapter 7 \nAccounting , p 214\nRagged/Variable Depth Hierarchies with Hierarchy Bridge Tables\nRagged hierarchies  of indeterminate depth are diffi  cult to model and query in a \nrelational database. Although SQL extensions and OLAP access languages provide \nsome support for recursive parent/child relationships, these approaches have limita-\ntions. With SQL extensions, alternative ragged hierarchies cannot be substituted at \nquery time, shared ownership structures are not supported, and time varying ragged \nhierarchies are not supported. All these objections can be overcome in relational \ndatabases by modeling a ragged hierarchy with a specially constructed bridge table. \nThis bridge table contains a row for every possible path in the ragged hierarchy \nand enables all forms of hierarchy traversal to be accomplished with standard SQL \nrather than using special language extensions. \nChapter 7 \nAccounting , p 215\nChapter 9 \nHuman Resources Management , p 273\nRagged/Variable Depth Hierarchies with Pathstring Attributes\nThe  use of a bridge table for ragged variable depth hierarchies can be avoided by \nimplementing a pathstring attribute in the dimension. For each row in the dimen-\nsion, the pathstring attribute contains a specially encoded text string containing \nthe complete path description from the supreme node of a hierarchy down to the \nnode described by the particular dimension row. Many of the standard hierarchy \n",
      "page_number": 78
    },
    {
      "number": 10,
      "title": "Segment 10 (pages 88-95)",
      "start_page": 88,
      "end_page": 95,
      "detection_method": "topic_boundary",
      "content": "Chapter 2\n58\nanalysis requests can then be handled by standard SQL, without resorting to SQL \nlanguage extensions. However, the pathstring approach does not enable rapid sub-\nstitution of alternative hierarchies or shared ownership hierarchies. The pathstring \napproach may also be vulnerable to structure changes in the ragged hierarchy that \ncould force the entire hierarchy to be relabeled. \nChapter 7 \nAccounting , p 221\nAdvanced Fact Table Techniques\nThe techniques in this section refer to less common fact table patterns. \nFact Table Surrogate Keys\nSurrogate  keys are used to implement the primary keys of almost all dimension \ntables. In addition, single column surrogate fact keys can be useful, albeit not \nrequired. Fact table surrogate keys, which are not associated with any dimension, \nare assigned sequentially during the ETL load process and are used 1) as the single \ncolumn primary key of the fact table; 2) to serve as an immediate identiﬁ er of a fact \ntable row without navigating multiple dimensions for ETL purposes; 3) to allow an \ninterrupted load process to either back out or resume; 4) to allow fact table update \noperations to be decomposed into less risky inserts plus deletes. \nChapter 3 \nRetail Sales , p 102\nChapter 19 ETL Subsystems and Techniques , p 486\nChapter 20 ETL System Process and Tasks , p 520\nCentipede Fact Tables\nSome  designers create separate normalized dimensions for each level of a many-to-\none hierarchy, such as a date dimension, month dimension, quarter dimension, and \nyear dimension, and then include all these foreign keys in a fact table. This results \nin a centipede fact table with dozens of hierarchically related dimensions. Centipede \nfact tables should be avoided. All these ﬁ xed depth, many-to-one hierarchically \nrelated dimensions should be collapsed back to their unique lowest grains, such as \nthe date for the example mentioned. Centipede fact tables also result when design-\ners embed numerous foreign keys to individual low-cardinality dimension tables \nrather than creating a junk dimension. \nChapter 3 \nRetail Sales , p 108\n\n\nKimball Dimensional Modeling Techniques Overview 59\nNumeric Values as Attributes or Facts\nDesigners  sometimes encounter numeric values that don’t clearly fall into either \nthe fact or dimension attribute categories. A classic example is a product’s standard \nlist price. If the numeric value is used primarily for calculation purposes, it likely \nbelongs in the fact table. If a stable numeric value is used predominantly for ﬁ ltering \nand grouping, it should be treated as a dimension attribute; the discrete numeric \nvalues can be supplemented with value band attributes (such as $0-50). In some \ncases, it is useful to model the numeric value as both a fact and dimension attribute, \nsuch as a quantitative on-time delivery metric and qualitative textual descriptor. \nChapter 3 \nRetail Sales , p 85\nChapter 6 \nOrder Management , p 188\nChapter 8 \nCustomer Relationship Management , p 254\nChapter 16 Insurance , p 382\nLag/Duration Facts\nAccumulating  snapshot fact tables capture multiple process milestones, each with a \ndate foreign key and possibly a date/time stamp. Business users often want to analyze \nthe lags or durations between these milestones; sometimes these lags are just the \ndiff erences between dates, but other times the lags are based on more complicated \nbusiness rules. If there are dozens of steps in a pipeline, there could be hundreds \nof possible lags. Rather than forcing the user’s query to calculate each possible lag \nfrom the date/time stamps or date dimension foreign keys, just one time lag can be \nstored for each step measured against the process’s start point. Then every possible \nlag between two steps can be calculated as a simple subtraction between the two \nlags stored in the fact table. \nChapter 6 \nOrder Management , p 196\nChapter 16 Insurance , p 393\nHeader/Line Fact Tables\nOperational  transaction systems often consist of a transaction header row that’s \nassociated with multiple transaction lines. With header/line schemas (also known \nas parent/child schemas), all the header-level dimension foreign keys and degenerate \ndimensions should be included on the line-level fact table.\nChapter 6 \nOrder Management , p 181\nChapter 12 Transportation , p 315\nChapter 15 Electronic Commerce , p 363\n\n\nChapter 2\n60\nAllocated Facts\nIt  is quite common in header/line transaction data to encounter facts of diff er-\ning granularity, such as a header freight charge. You should strive to allocate \nthe header facts down to the line level based on rules provided by the business, so the \nallocated facts can be sliced and rolled up by all the dimensions. In many cases, you \ncan avoid creating a header-level fact table, unless this aggregation delivers query \nperformance advantages.\nChapter 6 \nOrder Management , p 184\nProﬁ t and Loss Fact Tables Using Allocations\nFact  tables that expose the full equation of proﬁ t are among the most powerful deliv-\nerables of an enterprise DW/BI system. The equation of proﬁ t is (revenue) – (costs) = \n(proﬁ t). Fact tables ideally implement the proﬁ t equation at the grain of the atomic \nrevenue transaction and contain many components of cost. Because these tables are \nat the atomic grain, numerous rollups are possible, including customer proﬁ tabil-\nity, product proﬁ tability, promotion proﬁ tability, channel proﬁ tability, and others. \nHowever, these fact tables are diffi  cult to build because the cost components must \nbe allocated from their original sources to the fact table’s grain. This allocation step \nis often a major ETL subsystem and is a politically charged step that requires high-\nlevel executive support. For these reasons, proﬁ t and loss fact tables are typically \nnot tackled during the early implementation phases of a DW/BI program.\nChapter 6 \nOrder Management , p 189\nChapter 15 Electronic Commerce , p 370\nMultiple Currency Facts\nFact  tables that record ﬁ nancial transactions in multiple currencies should contain \na pair of columns for every ﬁ nancial fact in the row. One column contains the fact \nexpressed in the true currency of the transaction, and the other contains the same \nfact expressed in a single standard currency that is used throughout the fact table. \nThe standard currency value is created in an ETL process according to an approved \nbusiness rule for currency conversion. This fact table also must have a currency \ndimension to identify the transaction’s true currency. \nChapter 6 \nOrder Management , p 182\nChapter 7 \nAccounting , p 206\n\n\nKimball Dimensional Modeling Techniques Overview 61\nMultiple Units of Measure Facts\nSome  business processes require facts to be stated simultaneously in several units \nof measure. For example, depending on the perspective of the business user, a \nsupply chain may need to report the same facts as pallets, ship cases, retail cases, \nor individual scan units. If the fact table contains a large number of facts, each of \nwhich must be expressed in all units of measure, a convenient technique is to store \nthe facts once in the table at an agreed standard unit of measure, but also simulta-\nneously store conversion factors between the standard measure and all the others. \nThis fact table could be deployed through views to each user constituency, using \nan appropriate selected conversion factor. The conversion factors must reside in the \nunderlying fact table row to ensure the view calculation is simple and correct, while \nminimizing query complexity.\nChapter 6 \nOrder Management , p 197\nYear-to-Date Facts\nBusiness  users often request year-to-date (YTD) values in a fact table. It is hard to \nargue against a single request, but YTD requests can easily morph into “YTD at the \nclose of the ﬁ scal period” or “ﬁ scal period to date.” A more reliable, extensible way \nto handle these assorted requests is to calculate the YTD metrics in the BI applica-\ntions or OLAP cube rather than storing YTD facts in the fact table.\nChapter 7 \nAccounting , p 206\nMultipass SQL to Avoid Fact-to-Fact Table Joins\nA  BI application must never issue SQL that joins two fact tables together across the \nfact table’s foreign keys. It is impossible to control the cardinality of the answer set \nof such a join in a relational database, and incorrect results will be returned to the \nBI tool. For instance, if two fact tables contain customer’s product shipments and \nreturns, these two fact tables must not be joined directly across the customer \nand product foreign keys. Instead, the technique of drilling across two fact tables \nshould be used, where the answer sets from shipments and returns are separately \ncreated, and the results sort-merged on the common row header attribute values to \nproduce the correct result. \nChapter 4 \nInventory , p 130\nChapter 8 \nCustomer Relationship Management , p 259\n\n\nChapter 2\n62\nTimespan Tracking in Fact Tables\nThere  are three basic fact table grains: transaction, periodic snapshot, and accu-\nmulating snapshot. In isolated cases, it is useful to add a row eff ective date, row \nexpiration date, and current row indicator to the fact table, much like you do with \ntype 2 slowly changing dimensions, to capture a timespan when the fact row was \neff ective. Although an unusual pattern, this pattern addresses scenarios such as \nslowly changing inventory balances where a frequent periodic snapshot would load \nidentical rows with each snapshot. \nChapter 8 \nCustomer Relationship Management , p 252\nChapter 16 Insurance , p 394\nLate Arriving Facts\nA  fact row is late arriving if the most current dimensional context for new fact rows \ndoes not match the incoming row. This happens when the fact row is delayed. In \nthis case, the relevant dimensions must be searched to ﬁ nd the dimension keys that \nwere eff ective when the late arriving measurement event occurred.\nChapter 14 Healthcare , p 351\nChapter 19 ETL Subsystems and Techniques , p 478\nAdvanced Dimension Techniques\nThe techniques in this section refer to more advanced dimension table patterns.\nDimension-to-Dimension Table Joins\nDimensions  can contain references to other dimensions. Although these relation-\nships can be modeled with outrigger dimensions, in some cases, the existence of a \nforeign key to the outrigger dimension in the base dimension can result in explosive \ngrowth of the base dimension because type 2 changes in the outrigger force cor-\nresponding type 2 processing in the base dimension. This explosive growth can \noften be avoided if you demote the correlation between dimensions by placing the \nforeign key of the outrigger in the fact table rather than in the base dimension. This \nmeans the correlation between the dimensions can be discovered only by traversing \nthe fact table, but this may be acceptable, especially if the fact table is a periodic \nsnapshot where all the keys for all the dimensions are guaranteed to be present for \neach reporting period.\nChapter 6 \nOrder Management , p 175\n\n\nKimball Dimensional Modeling Techniques Overview 63\nMultivalued Dimensions and Bridge Tables\nIn  a classic dimensional schema, each dimension attached to a fact table has a single \nvalue consistent with the fact table’s grain. But there are a number of situations in \nwhich a dimension is legitimately multivalued. For example, a patient receiving a \nhealthcare treatment may have multiple simultaneous diagnoses. In these cases, the \nmultivalued dimension must be attached to the fact table through a group dimen-\nsion key to a bridge table with one row for each simultaneous diagnosis in a group.\nChapter 8 \nCustomer Relationship Management , p 245\nChapter 9 \nHuman Resources Management , p 275\nChapter 10 Financial Services , p 287\nChapter 13 Education , p 333\nChapter 14 Healthcare , p 345\nChapter 16 Insurance , p 382\nChapter 19 ETL Subsystems and Techniques , p 477\nTime Varying Multivalued Bridge Tables\nA multivalued bridge table may need to be based on a type 2 slowly changing dimen-\nsion. For example, the bridge table that implements the many-to-many relationship \nbetween bank accounts and individual customers usually must be based on type \n2 account and customer dimensions. In this case, to prevent incorrect linkages \nbetween accounts and customers, the bridge table must include eff ective and expi-\nration date/time stamps, and the requesting application must constrain the bridge \ntable to a speciﬁ c moment in time to produce a consistent snapshot. \nChapter 7 \nAccounting , p 220\nChapter 10 Financial Services , p 286\nBehavior Tag Time Series\nAlmost  all text in a data warehouse is descriptive text in dimension tables. Data \nmining customer cluster analyses typically results in textual behavior tags, often \nidentiﬁ ed on a periodic basis. In this case, the customers’ behavior measurements \nover time become a sequence of these behavior tags; this time series should be \nstored as positional attributes in the customer dimension, along with an optional \ntext string for the complete sequence of tags. The behavior tags are modeled in a \npositional design because the behavior tags are the target of complex simultaneous \nqueries rather than numeric computations. \nChapter 8 \nCustomer Relationship Management , p 240\n\n\nChapter 2\n64\nBehavior Study Groups\nComplex  customer behavior can sometimes be discovered only by running lengthy \niterative analyses. In these cases, it is impractical to embed the behavior analyses \ninside every BI application that wants to constrain all the members of the customer \ndimension who exhibit the complex behavior. The results of the complex behavior \nanalyses, however, can be captured in a simple table, called a study group, consisting \nonly of the customers’ durable keys. This static table can then be used as a kind of \nﬁ lter on any dimensional schema with a customer dimension by constraining the \nstudy group column to the customer dimension’s durable key in the target schema \nat query time. Multiple study groups can be deﬁ ned and derivative study groups \ncan be created with intersections, unions, and set diff erences. \nChapter 8 \nCustomer Relationship Management , p 249\nAggregated Facts as Dimension Attributes\nBusiness  users are often interested in constraining the customer dimension based \non aggregated performance metrics, such as ﬁ ltering on all customers who spent \nover a certain dollar amount during last year or perhaps over the customer’s lifetime. \nSelected aggregated facts can be placed in a dimension as targets for constraining and \nas row labels for reporting. The metrics are often presented as banded ranges in the \ndimension table. Dimension attributes representing aggregated performance metrics \nadd burden to the ETL processing, but ease the analytic burden in the BI layer.\nChapter 8 \nCustomer Relationship Management , p 239\nDynamic Value Bands\nA dynamic value banding report is organized as a series of report row headers that \ndeﬁ ne a progressive set of varying-sized ranges of a target numeric fact. For instance, \na common value banding report in a bank has many rows with labels such as \n“Balance from 0 to $10,” “Balance from $10.01 to $25,” and so on. This kind of \nreport is dynamic because the speciﬁ c row headers are deﬁ ned at query time, not \nduring the ETL processing. The row deﬁ nitions can be implemented in a small value \nbanding dimension table that is joined via greater-than/less-than joins to the fact \ntable, or the deﬁ nitions can exist only in an SQL CASE statement. The value band-\ning dimension approach is probably higher performing, especially in a columnar \ndatabase, because the CASE statement approach involves an almost unconstrained \nrelation scan of the fact table. \nChapter 10 Financial Services , p 291\n\n\nKimball Dimensional Modeling Techniques Overview 65\nText Comments Dimension\nRather  than treating freeform comments as textual metrics in a fact table, they \nshould be stored outside the fact table in a separate comments dimension (or as \nattributes in a dimension with one row per transaction if the comments’ cardinal-\nity matches the number of unique transactions) with a corresponding foreign key \nin the fact table. \nChapter 9 \nHuman Resources Management , p 278\nChapter 14 Healthcare , p 350\nMultiple Time Zones\nTo  capture both universal standard time, as well as local times in multi-time zone \napplications, dual foreign keys should be placed in the aff ected fact tables that join \nto two role-playing date (and potentially time-of-day) dimension tables. \nChapter 12 Transportation , p 323\nChapter 15 Electronic Commerce , p 361\nMeasure Type Dimensions\nSometimes  when a fact table has a long list of facts that is sparsely populated in any \nindividual row, it is tempting to create a measure type dimension that collapses the \nfact table row down to a single generic fact identiﬁ ed by the measure type dimen-\nsion. We generally do not recommend this approach. Although it removes all the \nempty fact columns, it multiplies the size of the fact table by the average number \nof occupied columns in each row, and it makes intra-column computations much \nmore diffi  cult. This technique is acceptable when the number of potential facts is \nextreme (in the hundreds), but less than a handful would be applicable to any given \nfact table row. \nChapter 6 \nOrder Management , p 169\nChapter 14 Healthcare , p 349\nStep Dimensions\nSequential  processes, such as web page events, normally have a separate row in a \ntransaction fact table for each step in a process. To tell where the individual step \nﬁ ts into the overall session, a step dimension is used that shows what step number \nis represented by the current step and how many more steps were required to com-\nplete the session. \n",
      "page_number": 88
    },
    {
      "number": 11,
      "title": "Segment 11 (pages 96-103)",
      "start_page": 96,
      "end_page": 103,
      "detection_method": "topic_boundary",
      "content": "Chapter 2\n66\nChapter 8 \nCustomer Relationship Management , p 251\nChapter 15 Electronic Commerce , p 366\nHot Swappable Dimensions\nHot swappable dimensions  are used when the same fact table is alternatively paired \nwith diff erent copies of the same dimension. For example, a single fact table con-\ntaining stock ticker quotes could be simultaneously exposed to multiple separate \ninvestors, each of whom has unique and proprietary attributes assigned to diff erent \nstocks.\nChapter 10 Financial Services , p 296\nAbstract Generic Dimensions\nSome  modelers are attracted to abstract generic dimensions. For example, their \nschemas include a single generic location dimension rather than embedded geo-\ngraphic attributes in the store, warehouse, and customer dimensions. Similarly, \ntheir person dimension includes rows for employees, customers, and vendor \ncontacts because they are all human beings, regardless that signiﬁ cantly diff erent \nattributes are collected for each type. Abstract generic dimensions should be avoided \nin dimensional models. The attribute sets associated with each type often diff er. If \nthe attributes are common, such as a geographic state, then they should be uniquely \nlabeled to distinguish a store’s state from a customer’s. Finally, dumping all variet-\nies of locations, people, or products into a single dimension invariably results in \na larger dimension table. Data abstraction may be appropriate in the operational \nsource system or ETL processing, but it negatively impacts query performance and \nlegibility in the dimensional model.\nChapter 9 \nHuman Resources Management , p 270\nChapter 11 Telecommunications , p 310\nAudit Dimensions\nWhen  a fact table row is created in the ETL back room, it is helpful to create \nan audit dimension containing the ETL processing metadata known at the time. \nA simple audit dimension row could contain one or more basic indicators of data \nquality, perhaps derived from examining an error event schema that records \ndata quality violations encountered while processing the data. Other useful audit \ndimension attributes could include environment variables describing the versions \nof ETL code used to create the fact rows or the ETL process execution time stamps. \n\n\nKimball Dimensional Modeling Techniques Overview 67\nThese environment variables are especially useful for compliance and auditing \npurposes because they enable BI tools to drill down to determine which rows were \ncreated with what versions of the ETL software. \nChapter 6 \nOrder Management , p 192\nChapter 16 Insurance , p 383\nChapter 19 ETL Subsystems and Techniques , p 460\nChapter 20 ETL System Process and Tasks , p 511\nLate Arriving Dimensions\nSometimes  the facts from an operational business process arrive minutes, hours, \ndays, or weeks before the associated dimension context. For example, in a real-time \ndata delivery situation, an inventory depletion row may arrive showing the natural \nkey of a customer committing to purchase a particular product. In a real-time ETL \nsystem, this row must be posted to the BI layer, even if the identity of the customer \nor product cannot be immediately determined. In these cases, special dimension \nrows are created with the unresolved natural keys as attributes. Of course, these \ndimension rows must contain generic unknown values for most of the descriptive \ncolumns; presumably the proper dimensional context will follow from the source at \na later time. When this dimensional context is eventually supplied, the placeholder \ndimension rows are updated with type 1 overwrites. Late arriving dimension data \nalso occurs when retroactive changes are made to type 2 dimension attributes. \nIn this case, a new row needs to be inserted in the dimension table, and then the \nassociated fact rows must be restated.\nChapter 14 Healthcare , p 351\nChapter 19 ETL Subsystems and Techniques , p 478\nChapter 20 ETL System Process and Tasks , p 523\nSpecial Purpose Schemas\nThe following design patterns are needed for speciﬁ c use cases.\nSupertype and Subtype Schemas for Heterogeneous Products\nFinancial  services and other businesses frequently off er a wide variety of products \nin disparate lines of business. For example, a retail bank may off er dozens of types \nof accounts ranging from checking accounts to mortgages to business loans, but all \nare examples of an account. Attempts to build a single, consolidated fact table with \nthe union of all possible facts, linked to dimension tables with all possible attributes \n\n\nChapter 2\n68\nof these divergent products, will fail because there can be hundreds of incompatible \nfacts and attributes. The solution is to build a single supertype fact table that has the \nintersection of the facts from all the account types (along with a supertype dimen-\nsion table containing the common attributes), and then systematically build separate \nfact tables (and associated dimension tables) for each of the subtypes. Supertype and \nsubtype fact tables are also called core and custom fact tables. \nChapter 10 Financial Services , p 293\nChapter 14 Healthcare , p 347\nChapter 16 Insurance , p 384\nReal-Time Fact Tables\nReal-time fact tables  need to be updated more frequently than the more traditional \nnightly batch process. There are many techniques for supporting this requirement, \ndepending on the capabilities of the DBMS or OLAP cube used for ﬁ nal deployment \nto the BI reporting layer. For example, a “hot partition” can be deﬁ ned on a fact table \nthat is pinned in physical memory. Aggregations and indexes are deliberately not \nbuilt on this partition. Other DBMSs or OLAP cubes may support deferred updat-\ning that allows existing queries to run to completion but then perform the updates. \nChapter 8 \nCustomer Relationship Management, p 260 \nChapter 20 ETL System Process and Tasks , p 520\nError Event Schemas\nManaging  data quality in a data warehouse requires a comprehensive system of \ndata quality screens or ﬁ lters that test the data as it ﬂ ows from the source sys-\ntems to the BI platform. When a data quality screen detects an error, this event \nis recorded in a special dimensional schema that is available only in the ETL \nback room. This schema consists of an error event fact table whose grain is the \nindividual error event and an associated error event detail fact table whose grain \nis each column in each table that participates in an error event.\nChapter 19 ETL Subsystems and Techniques , p  458\n\n\nRetail Sales\nT\nhe best way to understand the principles of dimensional modeling is to work \nthrough a series of tangible examples. By visualizing real cases, you hold the \nparticular design challenges and solutions in your mind more effectively than if they \nare presented abstractly. This book uses case studies from a range of businesses to \nhelp move past the idiosyncrasies of your own environment and reinforce dimen-\nsional modeling best practices.\nTo learn dimensional modeling, please read all the chapters in this book, even \nif you don’t manage a retail store or work for a telecommunications company. The \nchapters are not intended to be full-scale solutions for a given industry or business \nfunction. Each chapter covers a set of dimensional modeling patterns that comes \nup in nearly every kind of business. Universities, insurance companies, banks, and \nairlines alike surely need the techniques developed in this retail chapter. Besides, \nthinking about someone else’s business is refreshing. It is too easy to let historical \ncomplexities derail you when dealing with data from your company. By stepping out-\nside your organization and then returning with a well-understood design principle \n(or two), it is easier to remember the spirit of the design principles as you descend \ninto the intricate details of your business.\nChapter 3 discusses the following concepts:\n \n■Four-step process for designing dimensional models\n \n■Fact table granularity\n \n■Transaction fact tables\n \n■Additive, non-additive, and derived facts\n \n■Dimension attributes, including indicators, numeric descriptors, and multiple \nhierarchies\n \n■Calendar date dimensions, plus time-of-day\n \n■Causal dimensions, such as promotion\n \n■Degenerate dimensions, such as the transaction receipt number\n3\n\n\nChapter 3\n70\n \n■Nulls in a dimensional model\n \n■Extensibility of dimension models\n \n■Factless fact tables\n \n■Surrogate, natural, and durable keys\n \n■Snowﬂ aked dimension attributes\n \n■Centipede fact tables with “too many dimensions”\n Four-Step Dimensional Design Process\nThroughout this book, we will approach the design of a dimensional model by \nconsistently considering four steps, as the following sections discuss in more detail.\n Step 1: Select the Business Process\nA business process is a low-level activity performed by an organization, such as taking \norders, invoicing, receiving payments, handling service calls, registering students, \nperforming a medical procedure, or processing claims. To identify your organiza-\ntion’s business processes, it’s helpful to understand several common characteristics: \n \n■Business processes are frequently expressed as action verbs because they repre-\nsent activities that the business performs. The companion dimensions describe \ndescriptive context associated with each business process event.\n \n■Business processes are typically supported by an operational system, such as \nthe billing or purchasing system.\n \n■Business processes generate or capture key performance metrics. Sometimes \nthe metrics are a direct result of the business process; the measurements are \nderivations at other times. Analysts invariably want to scrutinize and evaluate \nthese metrics by a seemingly limitless combination of ﬁ lters and constraints.\n \n■Business processes are usually triggered by an input and result in output \nmetrics. In many organizations, there’s a series of processes in which the \noutputs from one process become the inputs to the next. In the parlance of a \ndimensional modeler, this series of processes results in a series of fact tables.\nYou need to listen carefully to the business to identify the organization’s business \nprocesses because business users can’t readily answer the question, “What busi-\nness process are you interested in?” The performance measurements users want to \nanalyze in the DW/BI system result from business process events. \nSometimes  business users talk about strategic business initiatives instead of busi-\nness processes. These initiatives are typically broad enterprise plans championed \nby executive leadership to deliver competitive advantage. In order to tie a business \ninitiative to a business process representing a project-sized unit of work for the \n\n\nRetail Sales 71\nDW/BI team, you need to decompose the business initiative into the underlying \nprocesses. This means digging a bit deeper to understand the data and operational \nsystems that support the initiative’s analytic requirements. \nIt’s also worth noting what a business process is not. Organizational business \ndepartments or functions do not equate to business processes. By focusing on pro-\ncesses, rather than on functional departments, consistent information is delivered \nmore economically throughout the organization. If you design departmentally bound \ndimensional models, you inevitably duplicate data with diff erent labels and data \nvalues. The best way to ensure consistency is to publish the data once.\n Step 2: Declare the Grain\nDeclaring the grain means specifying exactly what an individual fact table row \nrepresents. The grain conveys the level of detail associated with the fact table \nmeasurements. It provides the answer to the question, “How do you describe a \nsingle row in the fact table?” The grain is determined by the physical realities of \nthe operational system that captures the business process’s events.\nExample grain declarations include:\n \n■One row per scan of an individual product on a customer’s sales transaction \n \n■One row per line item on a bill from a doctor\n \n■One row per individual boarding pass scanned at an airport gate\n \n■One row per daily snapshot of the inventory levels for each item in a warehouse\n \n■One row per bank account each month\nThese grain declarations are expressed in business terms. Perhaps you were \nexpecting the grain to be a traditional declaration of the fact table’s primary key. \nAlthough the grain ultimately is equivalent to the primary key, it’s a mistake to list \na set of dimensions and then assume this list is the grain declaration. Whenever \npossible, you should express the grain in business terms. \nDimensional modelers sometimes try to bypass this seemingly unnecessary step \nof the four-step design process. Please don’t! Declaring the grain is a critical step that \ncan’t be taken lightly. In debugging thousands of dimensional designs over the years, \nthe most frequent error is not declaring the grain of the fact table at the beginning \nof the design process. If the grain isn’t clearly deﬁ ned, the whole design rests on \nquicksand; discussions about candidate dimensions go around in circles, and rogue \nfacts sneak into the design. An inappropriate grain haunts a DW/BI implementation! \nIt is extremely important that everyone on the design team reaches agreement on \nthe fact table’s granularity. Having said this, you may discover in steps 3 or 4 of the \ndesign process that the grain statement is wrong. This is okay, but then you must \nreturn to step 2, restate the grain correctly, and revisit steps 3 and 4  again.\n\n\nChapter 3\n72\n Step 3: Identify the Dimensions\nDimensions  fall out of the question, “How do business people describe the data \nresulting from the business process measurement events?” You need to decorate \nfact tables with a robust set of dimensions representing all possible descriptions \nthat take on single values in the context of each measurement. If you are clear about \nthe grain, the dimensions typically can easily be identiﬁ ed as they represent the \n“who, what, where, when, why, and how” associated with the event. Examples of \ncommon dimensions include date, product, customer, employee, and facility. With \nthe choice of each dimension, you then list all the discrete, text-like attributes that \nﬂ esh out each dimension table. \n Step 4: Identify the Facts\nFacts are determined by answering the question, “What is the process measuring?” \nBusiness users are keenly interested in analyzing these performance metrics. All \ncandidate facts in a design must be true to the grain deﬁ ned in step 2. Facts that \nclearly belong to a diff erent grain must be in a separate fact table. Typical facts are \nnumeric additive ﬁ gures, such as quantity ordered or dollar cost amount.\nYou need to consider both your business users’ requirements and the realities \nof your source data in tandem to make decisions regarding the four steps, as illus-\ntrated in Figure 3-1. We strongly encourage you to resist the temptation to model \nthe data by looking at source data alone. It may be less intimidating to dive into the \ndata rather than interview a business person; however, the data is no substitute for \nbusiness user input. Unfortunately, many organizations have attempted this path-\nof-least-resistance data-driven approach but without much  success.\nDimensional Model\nBusiness Process\nGrain\nDimensions\nFacts\nData\nRealities\nBusiness\nRequirements\nFigure 3-1: Key input to the four-step dimensional design process. \nRetail Case Study\nLet’s  start with a brief description of the retail business used in this case study. We \nbegin with this industry simply because it is one we are all familiar with. But the \npatterns discussed in the context of this case study are relevant to virtually every \ndimensional model regardless of the industry.\n\n\nRetail Sales 73\nImagine you work in the headquarters of a large grocery chain. The business has \n100 grocery stores spread across ﬁ ve states. Each store has a full complement of \ndepartments, including grocery, frozen foods, dairy, meat, produce, bakery, ﬂ oral, \nand health/beauty aids. Each store has approximately 60,000 individual products, \ncalled stock keeping units (SKUs), on its  shelves. \nData  is collected at several interesting places in a grocery store. Some of the most \nuseful data is collected at the cash registers as customers purchase products. The point-\nof-sale (POS) system scans product barcodes at the cash register, measuring consumer \ntakeaway at the front door of the grocery store, as illustrated in Figure 3-2’s cash register \nreceipt. Other data is captured at the store’s back door where vendors make  deliveries. \nAllstar Grocery\n123 Loon Street\nGreen Prairie, MN 55555\n(952) 555-1212\nStore: 0022\nCashier: 00245409/Alan\n2.50\n4.99\n1.99\n3.19\n12.67\n12.67\n4/15/2013 10:56 AM\n4\n0030503347 Baked Well Multigrain Muffins\n2840201912 SoySoy Milk Quart\nTOTAL\nAMOUNT TENDERED\nCASH\nTransaction: 649\n0064900220415201300245409\nThank you for shopping at Allstar\nITEM COUNT:\n2120201195 Diet Cola 12-pack\n \nSaved $.50 off $5.49\n0070806048 Sparkly Toothpaste\n \nCoupon $.30 off $2.29\nFigure 3-2: Sample cash register receipt.\nAt the grocery store, management is concerned with the logistics of ordering, \nstocking, and selling products while maximizing proﬁ t. The proﬁ t ultimately comes \n",
      "page_number": 96
    },
    {
      "number": 12,
      "title": "Segment 12 (pages 104-111)",
      "start_page": 104,
      "end_page": 111,
      "detection_method": "topic_boundary",
      "content": "Chapter 3\n74\nfrom charging as much as possible for each product, lowering costs for product \nacquisition and overhead, and at the same time attracting as many customers as \npossible in a highly competitive environment. Some of the most signiﬁ cant manage-\nment decisions have to do with pricing and promotions. Both store management \nand headquarters marketing spend a great deal of time tinkering with pricing and \npromotions. Promotions in a grocery store include temporary price reductions, ads \nin newspapers and newspaper inserts, displays in the grocery store, and coupons. \nThe most direct and eff ective way to create a surge in the volume of product sold \nis to lower the price dramatically. A 50-cent reduction in the price of paper towels, \nespecially when coupled with an ad and display, can cause the sale of the paper \ntowels to jump by a factor of 10. Unfortunately, such a big price reduction usually \nis not sustainable because the towels probably are being sold at a loss. As a result of \nthese issues, the visibility of all forms of promotion is an important part of analyz-\ning the operations of a grocery store.\nNow that we have described our business case study, we’ll begin to design the \ndimensional model.\nStep 1: Select the Business Process\nThe  ﬁ rst step in the design is to decide what business process to model by combin-\ning an understanding of the business requirements with an understanding of the \navailable source data.\nNOTE \nThe ﬁ rst DW/BI project should focus on the business process that is \nboth the most critical to the business users, as well as the most feasible. Feasibility \ncovers a range of considerations, including data availability and quality, as well as \norganizational readiness.\nIn our retail case study, management wants to better understand customer pur-\nchases as captured by the POS system. Thus the business process you’re modeling \nis POS retail sales transactions. This data enables the business users to analyze \nwhich products are selling in which stores on which days under what promotional \nconditions in which transactions. \nStep 2: Declare the Grain\nAfter  the business process has been identiﬁ ed, the design team faces a serious deci-\nsion about the granularity. What level of data detail should be made available in \nthe dimensional model? \nTackling  data at its lowest atomic grain makes sense for many reasons. Atomic \ndata is highly dimensional. The more detailed and atomic the fact measurement, \n\n\nRetail Sales 75\nthe more things you know for sure. All those things you know for sure translate \ninto dimensions. In this regard, atomic data is a perfect match for the dimensional \napproach.\nAtomic data provides maximum analytic ﬂ exibility because it can be con-\nstrained and rolled up in every way possible. Detailed data in a dimensional model \nis poised and ready for the ad hoc attack by business users.\nNOTE \nYou should develop dimensional models representing the most detailed, \natomic information captured by a business process. \nOf course, you could declare a more summarized granularity representing an \naggregation of the atomic data. However, as soon as you select a higher level grain, \nyou limit yourself to fewer and/or potentially less detailed dimensions. The less \ngranular model is immediately vulnerable to unexpected user requests to drill down \ninto the details. Users inevitably run into an analytic wall when not given access to \nthe atomic data. Although aggregated data plays an important role for performance \ntuning, it is not a substitute for giving users access to the lowest level details; users \ncan easily summarize atomic data, but it’s impossible to create details from sum-\nmary data. Unfortunately, some industry pundits remain confused about this point. \nThey claim dimensional models are only appropriate for summarized data and then \ncriticize the dimensional modeling approach for its supposed need to anticipate the \nbusiness question. This misunderstanding goes away when detailed, atomic data is \nmade available in a dimensional model.\nIn our case study, the most granular data is an individual product on a POS transac-\ntion, assuming the POS system rolls up all sales for a given product within a shopping \ncart into a single line item. Although users probably are not interested in analyzing \nsingle items associated with a speciﬁ c POS transaction, you can’t predict all the ways \nthey’ll want to cull through that data. For example, they may want to understand the \ndiff erence in sales on Monday versus Sunday. Or they may want to assess whether it’s \nworthwhile to stock so many individual sizes of certain brands. Or they may want \nto understand how many shoppers took advantage of the 50-cents-off  promotion on \nshampoo. Or they may want to determine the impact of decreased sales when a com-\npetitive diet soda product was promoted heavily. Although none of these queries calls \nfor data from one speciﬁ c transaction, they are broad questions that require detailed \ndata sliced in precise ways. None of them could have been answered if you elected to \nprovide access only to summarized data.\nNOTE \nA DW/BI system almost always demands data expressed at the lowest \npossible grain, not because queries want to see individual rows but because queries \nneed to cut through the details in very precise ways.\n\n\nChapter 3\n76\nStep 3: Identify the Dimensions\nAfter  the grain of the fact table has been chosen, the choice of dimensions is straight-\nforward. The product and transaction fall out immediately. Within the framework \nof the primary dimensions, you can ask whether other dimensions can be attributed \nto the POS measurements, such as the date of the sale, the store where the sale \noccurred, the promotion under which the product is sold, the cashier who handled \nthe sale, and potentially the method of payment. We express this as another design \nprinciple.\nNOTE \nA careful grain statement determines the primary dimensionality of the \nfact table. You then add more dimensions to the fact table if these additional dimen-\nsions naturally take on only one value under each combination of the primary \ndimensions. If the additional dimension violates the grain by causing additional \nfact rows to be generated, the dimension needs to be disqualiﬁ ed or the grain state-\nment needs to be revisited.\nThe following descriptive dimensions apply to the case: date, product, store, \npromotion, cashier, and method of payment. In addition, the POS transaction ticket \nnumber is included as a special dimension, as described in the section “Degenerate \nDimensions for Transaction Numbers” later in this chapter.\nBefore ﬂ eshing out the dimension tables with descriptive attributes, let’s complete \nthe ﬁ nal step of the four-step process. You don’t want to lose sight of the forest for \nthe trees at this stage of the  design.\n Step 4: Identify the Facts\nThe  fourth and ﬁ nal step in the design is to make a careful determination of which \nfacts will appear in the fact table. Again, the grain declaration helps anchor your \nthinking. Simply put, the facts must be true to the grain: the individual product \nline item on the POS transaction in this case. When considering potential facts, \nyou may again discover adjustments need to be made to either your earlier grain \nassumptions or choice of dimensions.\nThe facts collected by the POS system include the sales quantity (for example, \nthe number of cans of chicken noodle soup), per unit regular, discount, and net \npaid prices, and extended discount and sales dollar amounts. The extended sales \ndollar amount equals the sales quantity multiplied by the net unit price. Likewise, \nthe extended discount dollar amount is the sales quantity multiplied by the unit \ndiscount amount. Some sophisticated POS systems also provide a standard dollar \ncost for the product as delivered to the store by the vendor. Presuming this cost \nfact is readily available and doesn’t require a heroic activity-based costing initiative, \n\n\nRetail Sales 77\nyou can include the extended cost amount in the fact table. The fact table begins \nto take shape in Figure 3-3.\nRetail Sales Fact\nDate Key (FK)\nProduct Key (FK)\nStore Key (FK)\nPromotion Key (FK)\nCashier Key (FK)\nPayment Method Key (FK)\nPOS Transaction # (DD)\nSales Quantity\nRegular Unit Price\nDiscount Unit Price\nNet Unit Price\nExtended Discount Dollar Amount\nExtended Sales Dollar Amount\nExtended Cost Dollar Amount\nExtended Gross Profit Dollar Amount\nDate Dimension\nProduct Dimension\nPromotion Dimension\nPayment Method Dimension\nStore Dimension\nCashier Dimension\nFigure 3-3: Measured facts in retail sales schema.\nFour of the facts, sales quantity and the extended discount, sales, and cost dollar \namounts, are beautifully additive across all the dimensions. You can slice and dice \nthe fact table by the dimension attributes with impunity, and every sum of these \nfour facts is valid and correct.\nDerived Facts\nYou  can compute the gross proﬁ t by subtracting the extended cost dollar amount \nfrom the extended sales dollar amount, or revenue. Although computed, gross proﬁ t \nis also perfectly additive across all the dimensions; you can calculate the gross \nproﬁ t of any combination of products sold in any set of stores on any set of days. \nDimensional modelers sometimes question whether a calculated derived fact should \nbe stored in the database. We generally recommend it be stored physically. In this \ncase study, the gross proﬁ t calculation is straightforward, but storing it means it’s \ncomputed consistently in the ETL process, eliminating the possibility of user cal-\nculation errors. The cost of a user incorrectly representing gross proﬁ t overwhelms \nthe minor incremental storage cost. Storing it also ensures all users and BI reporting \napplications refer to gross proﬁ t consistently. Because gross proﬁ t can be calculated \nfrom adjacent data within a single fact table row, some would argue that you should \nperform the calculation in a view that is indistinguishable from the table. This is \na reasonable approach if all users access the data via the view and no users with \nad hoc query tools can sneak around the view to get at the physical table. Views \nare a reasonable way to minimize user error while saving on storage, but the DBA \n\n\nChapter 3\n78\nmust allow no exceptions to accessing the data through the view. Likewise, some \norganizations want to perform the calculation in the BI tool. Again, this works if all \nusers access the data using a common tool, which is seldom the case in our expe-\nrience. However, sometimes non-additive metrics on a report such as percentages \nor ratios must be computed in the BI application because the calculation cannot \nbe precalculated and stored in a fact table. OLAP cubes excel in these situations.\nNon-Additive Facts\nGross  margin can be calculated by dividing the gross proﬁ t by the extended sales \ndollar revenue. Gross margin is a non-additive fact because it can’t be summarized \nalong any dimension. You can calculate the gross margin of any set of products, \nstores, or days by remembering to sum the revenues and costs respectively before \ndividing. \nNOTE \nPercentages and ratios, such as gross margin, are non-additive. The \nnumerator and denominator should be stored in the fact table. The ratio can then \nbe calculated in a BI tool for any slice of the fact table by remembering to calculate \nthe ratio of the sums, not the sum of the ratios.\nUnit  price is another non-additive fact. Unlike the extended amounts in the fact \ntable, summing unit price across any of the dimensions results in a meaningless, \nnonsensical number. Consider this simple example: You sold one widget at a unit \nprice of $1.00 and four widgets at a unit price of 50 cents each. You could sum \nthe sales quantity to determine that ﬁ ve widgets were sold. Likewise, you could \nsum the sales dollar amounts ($1.00 and $2.00) to arrive at a total sales amount \nof $3.00. However, you can’t sum the unit prices ($1.00 and 50 cents) and declare \nthat the total unit price is $1.50. Similarly, you shouldn’t announce that the average \nunit price is 75 cents. The properly weighted average unit price should be calcu-\nlated by taking the total sales amount ($3.00) and dividing by the total quantity \n(ﬁ ve widgets) to arrive at a 60 cent average unit price. You’d never arrive at this \nconclusion by looking at the unit price for each transaction line in isolation. To \nanalyze the average price, you must add up the sales dollars and sales quantities \nbefore dividing the total dollars by the total quantity sold. Fortunately, many BI \ntools perform this function correctly. Some question whether non-additive facts \nshould be physically stored in a fact table. This is a legitimate question given \ntheir limited analytic value, aside from printing individual values on a report or \napplying a ﬁ lter directly on the fact, which are both atypical. In some situations, \na fundamentally non-additive fact such as a temperature is supplied by the source \nsystem. These non-additive facts may be averaged carefully over many records, if \nthe business analysts agree that this makes  sense.\n\n\nRetail Sales 79\nTransaction Fact Tables\nTransactional business processes are the most common. The fact tables representing \nthese processes share several characteristics:\n \n■The grain of atomic transaction fact tables can be succinctly expressed in the \ncontext of the transaction, such as one row per transaction or one row per \ntransaction line.\n \n■Because these fact tables record a transactional event, they are often sparsely \npopulated. In our case study, we certainly wouldn’t sell every product in \nevery shopping cart.\n \n■Even though transaction fact tables are unpredictably and sparsely populated, \nthey can be truly enormous. Most billion and trillion row tables in a data \nwarehouse are transaction fact tables.\n \n■Transaction fact tables tend to be highly dimensional.\n \n■The metrics resulting from transactional events are typically additive as long \nas they have been extended by the quantity amount, rather than capturing \nper unit metrics.\nAt this early stage of the design, it is often helpful to estimate the number of rows \nin your largest table, the fact table. In this case study, it simply may be a matter of \ntalking with a source system expert to understand how many POS transaction line \nitems are generated on a periodic basis. Retail traffi  c ﬂ uctuates signiﬁ cantly from \nday to day, so you need to understand the transaction activity over a reasonable \nperiod of time. Alternatively, you could estimate the number of rows added to the \nfact table annually by dividing the chain’s annual gross revenue by the average item \nselling price. Assuming that gross revenues are $4 billion per year and that the aver-\nage price of an item on a customer ticket is $2.00, you can calculate that there are \napproximately 2 billion transaction line items per year. This is a typical engineer’s \nestimate that gets you surprisingly close to sizing a design directly from your arm-\nchair. As designers, you always should be triangulating to determine whether your \ncalculations are  reasonable.\n Dimension Table Details\nNow that we’ve walked through the four-step process, let’s return to the dimension \ntables and focus on populating them with robust attributes.\n Date Dimension\nThe  date dimension is a special dimension because it is the one dimension nearly \nguaranteed to be in every dimensional model since virtually every business process \n\n\nChapter 3\n80\ncaptures a time series of performance metrics. In fact, date is usually the ﬁ rst dimen-\nsion in the underlying partitioning scheme of the database so that the successive \ntime interval data loads are placed into virgin territory on the disk.\nFor  readers of the ﬁ rst edition of The Data Warehouse Toolkit (Wiley, 1996), this \ndimension was referred to as the time dimension. However, for more than a decade, \nwe’ve used the “date dimension” to mean a daily grained dimension table. This helps \ndistinguish between date and time-of-day dimensions.\nUnlike most of the other dimensions, you can build the date dimension table in \nadvance. You may put 10 or 20 years of rows representing individual days in the table, \nso you can cover the history you have stored, as well as several years in the future. \nEven 20 years’ worth of days is only approximately 7,300 rows, which is a relatively \nsmall dimension table. For a daily date dimension table in a retail environment, we \nrecommend the partial list of columns shown in Figure 3-4.\nDate Dimension\nDate Key (PK)\nDate\nFull Date Description\nDay of Week\nDay Number in Calendar Month\nDay Number in Calendar Year\nDay Number in Fiscal Month\nDay Number in Fiscal Year\nLast Day in Month Indicator\nCalendar Week Ending Date\nCalendar Week Number in Year\nCalendar Month Name\nCalendar Month Number in Year\nCalendar Year-Month (YYYY-MM)\nCalendar Quarter\nCalendar Year-Quarter\nCalendar Year\nFiscal Week\nFiscal Week Number in Year\nFiscal Month\nFiscal Month Number in Year\nFiscal Year-Month\nFiscal Quarter\nFiscal Year-Quarter\nFiscal Half Year\nFiscal Year\nHoliday Indicator\nWeekday Indicator\nSQL Date Stamp\n...\nFigure 3-4: Date dimension table.\n\n\nRetail Sales 81\nEach column in the date dimension table is deﬁ ned by the particular day that the \nrow represents. The day-of-week column contains the day’s name, such as Monday. \nThis column would be used to create reports comparing Monday business with \nSunday business. The day number in calendar month column starts with 1 at the \nbeginning of each month and runs to 28, 29, 30, or 31 depending on the month. \nThis column is useful for comparing the same day each month. Similarly, you could \nhave a month number in year (1, . . ., 12). All these integers support simple date \narithmetic across year and month boundaries. \nFor reporting, you should include both long and abbreviated labels. For exam-\nple, you would want a month name attribute with values such as January. In \naddition, a year-month (YYYY-MM) column is useful as a report column header. \nYou likely also want a quarter number (Q1, . . ., Q4), as well as a year-quarter, \nsuch as 2013-Q1. You would include similar columns for the ﬁ scal periods if \nthey diff er from calendar periods. Sample rows containing several date dimen-\nsion columns are illustrated in Figure 3-5.\nJanuary 1, 2013\nJanuary 2, 2013\nJanuary 3, 2013\nJanuary 4, 2013\nJanuary 5, 2013\nJanuary 6, 2013\nJanuary 7, 2013\nJanuary 8, 2013\nJanuary\nJanuary\nJanuary\nJanuary\nJanuary\nJanuary\nJanuary\nJanuary\nQ1\nQ1\nQ1\nQ1\nQ1\nQ1\nQ1\nQ1\n2013\n2013\n2013\n2013\n2013\n2013\n2013\n2013\nF2013-01\nF2013-01\nF2013-01\nF2013-01\nF2013-01\nF2013-01\nF2013-01\nF2013-01\nHoliday\nNon-Holiday\nNon-Holiday\nNon-Holiday\nNon-Holiday\nNon-Holiday\nNon-Holiday\nNon-Holiday\nWeekday\nWeekday\nWeekday\nWeekday\nWeekday\nWeekday\nWeekday\nWeekday\nTuesday\nWednesday\nThursday\nFriday\nSaturday\nSunday\nMonday\nTuesday\n20130101\n20130102\n20130103\n20130104\n20130105\n20130106\n20130107\n20130108\n01/01/2013\n01/02/2013\n01/03/2013\n01/04/2013\n01/05/2013\n01/06/2013\n01/07/2013\n01/08/2013\nDate Key\nDate\nFull Date\nDescription\nDay of\nWeek\nCalendar\nMonth\nCalendar\nQuarter\nCalendar\nYear\nFiscal Year-\nMonth\nHoliday\nIndicator\nWeekday\nIndicator\nFigure 3-5: Date dimension sample rows.\nNOTE \nA sample date dimension is available at www.kimballgroup.com under \nthe Tools and Utilities tab for this book title.\nSome designers pause at this point to ask why an explicit date dimension table is \nneeded. They reason that if the date key in the fact table is a date type column, then \nany SQL query can directly constrain on the fact table date key and use natural SQL \ndate semantics to ﬁ lter on month or year while avoiding a supposedly expensive \njoin. This reasoning falls apart for several reasons. First, if your relational database \ncan’t handle an effi  cient join to the date dimension table, you’re in deep trouble. \nMost database optimizers are quite effi  cient at resolving dimensional queries; it is \nnot necessary to avoid joins like the plague. \nSince the average business user is not versed in SQL date semantics, he would \nbe unable to request typical calendar groupings. SQL date functions do not support \n",
      "page_number": 104
    },
    {
      "number": 13,
      "title": "Segment 13 (pages 112-119)",
      "start_page": 112,
      "end_page": 119,
      "detection_method": "topic_boundary",
      "content": "Chapter 3\n82\nﬁ ltering by attributes such as weekdays versus weekends, holidays, ﬁ scal periods, \nor seasons. Presuming the business needs to slice data by these nonstandard date \nattributes, then an explicit date dimension table is essential. Calendar logic belongs \nin a dimension table, not in the application code. \nNOTE \nDimensional models always need an explicit date dimension table. There \nare many date attributes not supported by the SQL date function, including week \nnumbers, ﬁ scal periods, seasons, holidays, and weekends. Rather than attempting \nto determine these nonstandard calendar calculations in a query, you should look \nthem up in a date dimension table.\n Flags and Indicators as Textual Attributes\nLike  many operational ﬂ ags and indicators, the date dimension’s holiday indicator \nis a simple indicator with two potential values. Because dimension table attributes \nserve as report labels and values in pull-down query ﬁ lter lists, this indicator should \nbe populated with meaningful values such as Holiday or Non-holiday instead of \nthe cryptic Y/N, 1/0, or True/False. As illustrated in Figure 3-6, imagine a report \ncomparing holiday versus non-holiday sales for a product. More meaningful domain \nvalues for this indicator translate into a more meaningful, self-explanatory report. \nRather than decoding ﬂ ags into understandable labels in the BI application, we prefer \nthat decoded values be stored in the database so they’re consistently available to all \nusers regardless of their BI reporting environment or tools.\nMonthly Sales\nExtended Sales\nDollar Amount\nHoliday\nIndicator\nHoliday\nIndicator\nOR\nPeriod:\nProduct\nJune 2013\nBaked Well Sourdough\nN\nY\n1,009\n6,298\nMonthly Sales\nExtended Sales\nDollar Amount\nPeriod:\nProduct\nJune 2013\nBaked Well Sourdough\nHoliday\nNon-holiday\n6,298\n1,009\nFigure 3-6: Sample reports with cryptic versus textual indicators.\nA  similar argument holds true for the weekday indicator that would have a value \nof Weekday or Weekend. Saturdays and Sundays obviously would be assigned the \nweekend value. Of course, multiple date table attributes can be jointly constrained, \nso you can easily compare weekday holidays with weekend  holidays.\nCurrent and Relative Date Attributes\nMost  date dimension attributes are not subject to updates. June 1, 2013 will always \nroll up to June, Calendar Q2, and 2013. However, there are attributes you can add \n\n\nRetail Sales 83\nto the basic date dimension that will change over time, including IsCurrentDay, \nIsCurrentMonth, IsPrior60Days, and so on. IsCurrentDay obviously must be updated \neach day; the attribute is useful for generating reports that always run for today. A \nnuance to consider is the day that IsCurrentDay refers to. Most data warehouses \nload data daily, so IsCurrentDay would refer to yesterday (or more accurately, the \nmost recent day loaded). You might also add attributes to the date dimension that \nare unique to your corporate calendar, such as IsFiscalMonthEnd.\nSome date dimensions include updated lag attributes. The lag day column would \ntake the value 0 for today, –1 for yesterday, +1 for tomorrow, and so on. This attribute \ncould easily be a computed column rather than physically stored. It might be useful \nto set up similar structures for month, quarter, and year. Many BI tools include func-\ntionality to do prior period calculations, so these lag columns may be unnecessary.\nTime-of-Day as a Dimension or Fact\nAlthough date and time are comingled in an operational date/time stamp, time-of-\nday is typically separated from the date dimension to avoid a row count explosion \nin the date dimension. As noted earlier, a date dimension with 20 years of history \ncontains approximately 7,300 rows. If you changed the grain of this dimension to \none row per minute in a day, you’d end up with over 10 million rows to accommodate \nthe 1,440 minutes per day. If you tracked time to the second, you’d have more than \n31 million rows per year! Because the date dimension is likely the most frequently \nconstrained dimension in a schema, it should be kept as small and manageable as \npossible.\nIf  you want to ﬁ lter or roll up time periods based on summarized day part group-\nings, such as activity during 15-minute intervals, hours, shifts, lunch hour, or prime \ntime, time-of-day would be treated as a full-ﬂ edged dimension table with one row per \ndiscrete time period, such as one row per minute within a 24-hour period resulting \nin a dimension with 1,440 rows. \nIf there’s no need to roll up or ﬁ lter on time-of-day groupings, time-of-day should \nbe handled as a simple date/time fact in the fact table. By the way, business users \nare often more interested in time lags, such as the transaction’s duration, rather \nthan discreet start and stop times. Time lags can easily be computed by taking the \ndiff erence between date/time stamps. These date/time stamps also allow an applica-\ntion to determine the time gap between two transactions of interest, even if these \ntransactions exist in diff erent days, months, or  years.\nProduct Dimension \nThe  product dimension describes every SKU in the grocery store. Although a typi-\ncal store may stock 60,000 SKUs, when you account for diff erent merchandising \nschemes and historical products that are no longer available, the product dimension \n\n\nChapter 3\n84\nmay have 300,000 or more rows. The product dimension is almost always sourced \nfrom the operational product master ﬁ le. Most retailers administer their product \nmaster ﬁ le at headquarters and download a subset to each store’s POS system at \nfrequent intervals. It is headquarters’ responsibility to deﬁ ne the appropriate product \nmaster record (and unique SKU number) for each new product. \n Flatten Many-to-One Hierarchies\nThe  product dimension represents the many descriptive attributes of each SKU. The \nmerchandise hierarchy is an important group of attributes. Typically, individual \nSKUs roll up to brands, brands roll up to categories, and categories roll up to depart-\nments. Each of these is a many-to-one relationship. This merchandise hierarchy and \nadditional attributes are shown for a subset of products in Figure 3-7.\nBaked Well\nFluffy\nFluffy\nLight\nColdpack\nFreshlike\nFrigid\nIcy\nIcy\nBread\nBread\nBread\nSweeten Bread\nFrozen Desserts\nFrozen Desserts\nFrozen Desserts\nFrozen Desserts\nFrozen Desserts\nBakery\nBakery\nBakery\nBakery\nFrozen Foods\nFrozen Foods\nFrozen Foods\nFrozen Foods\nFrozen Foods\nReduced Fat\nRegular Fat\nReduced Fat\nNon-Fat\nNon-Fat\nReduced Fat\nRegular Fat\nRegular Fat\nRegular Fat\nFresh\nPre-Packaged\nPre-Packaged\nPre-Packaged\nIce Cream\nIce Cream\nIce Cream\nIce Cream\nNovelties\n1 \n2 \n3 \n4 \n5 \n6 \n7 \n8 \n9\nBaked Well Light Sourdough Fresh Bread\nFluffy Sliced Whole Wheat\nFluffy Light Sliced Whole Wheat\nLight Mini Cinnamon Rolls\nDiet Lovers Vanilla 2 Gallon\nLight and Creamy Butter Pecan 1 Pint\nChocolate Lovers 1/2 Gallon\nStrawberry Ice Creamy 1 Pint\nIcy Ice Cream Sandwiches\nProduct\nKey\nProduct Description\nBrand\nDescription\nSubcategory\nDescription\nCategory\nDescription\nDepartment\nDescription\nFat Content\nFigure 3-7: Product dimension sample rows.\nFor each SKU, all levels of the merchandise hierarchy are well deﬁ ned. Some \nattributes, such as the SKU description, are unique. In this case, there are 300,000 \ndiff erent values in the SKU description column. At the other extreme, there are only \nperhaps 50 distinct values of the department attribute. Thus, on average, there are \n6,000 repetitions of each unique value in the department attribute. This is perfectly \nacceptable! You do not need to separate these repeated values into a second nor-\nmalized table to save space. Remember dimension table space requirements pale in \ncomparison with fact table space considerations.\nNOTE \nKeeping the repeated low cardinality values in the primary dimension \ntable is a fundamental dimensional modeling technique. Normalizing these values \ninto separate tables defeats the primary goals of simplicity and performance, as \ndiscussed in “Resisting Normalization Urges” later in this chapter.\nMany of the attributes in the product dimension table are not part of the mer-\nchandise hierarchy. The package type attribute might have values such as Bottle, \nBag, Box, or Can. Any SKU in any department could have one of these values. \n\n\nRetail Sales 85\nIt often makes sense to combine a constraint on this attribute with a constraint \non a merchandise hierarchy attribute. For example, you could look at all the SKUs \nin the Cereal category packaged in Bags. Put another way, you can browse among \ndimension attributes regardless of whether they belong to the merchandise hier-\narchy. Product dimension tables typically have more than one explicit hierarchy. \nA recommended partial product dimension for a retail grocery dimensional model \nis shown in Figure 3-8.\nProduct Key (PK)\nSKU Number (NK)\nProduct Description\nBrand Description\nSubcategory Description\nCategory Description\nDepartment Number\nDepartment Description\nPackage Type Description\nPackage Size\nFat Content\nDiet Type\nWeight\nWeight Unit of Measure\nStorage Type\nShelf Life Type\nShelf Width\nShelf Height\nShelf Depth\n...\nProduct Dimension\nFigure 3-8: Product dimension table.\nAttributes with Embedded Meaning\nOften  operational product codes, identiﬁ ed in the dimension table by the NK notation \nfor natural key, have embedded meaning with diff erent parts of the code representing \nsigniﬁ cant characteristics of the product. In this case, the multipart attribute should \nbe both preserved in its entirety within the dimension table, as well as broken down \ninto its component parts, which are handled as separate attributes. For example, if \nthe ﬁ fth through ninth characters in the operational code identify the manufacturer, \nthe manufacturer’s name should also be included as a dimension table attribute.\n Numeric Values as Attributes or Facts\nYou  will sometimes encounter numeric values that don’t clearly fall into either the \nfact or dimension attribute categories. A classic example is the standard list price \n\n\nChapter 3\n86\nfor a product. It’s deﬁ nitely a numeric value, so the initial instinct is to place it in \nthe fact table. But typically the standard price changes infrequently, unlike most \nfacts that are often diff erently valued on every measurement event.\nIf the numeric value is used primarily for calculation purposes, it likely belongs \nin the fact table. Because standard price is non-additive, you might multiply it by \nthe quantity for an extended amount which would be additive. Alternatively, if the \nstandard price is used primarily for price variance analysis, perhaps the variance \nmetric should be stored in the fact table instead. If the stable numeric value is used \npredominantly for ﬁ ltering and grouping, it should be treated as a product dimen-\nsion attribute.\nSometimes numeric values serve both calculation and ﬁ ltering/grouping func-\ntions. In these cases, you should store the value in both the fact and dimension \ntables. Perhaps the standard price in the fact table represents the valuation at the \ntime of the sales transaction, whereas the dimension attribute is labeled to indicate \nit’s the current standard price.\nNOTE \nData elements that are used both for fact calculations and dimension \nconstraining, grouping, and labeling should be stored in both locations, even \nthough a clever programmer could write applications that access these data \nelements from a single location. It is important that dimensional models be as \nconsistent as possible and application development be predictably simple. Data \ninvolved in calculations should be in fact tables and data involved in constraints, \ngroups and labels should be in dimension tables.\n Drilling Down on Dimension Attributes\nA  reasonable product dimension table can have 50 or more descriptive attributes. \nEach attribute is a rich source for constraining and constructing row header labels. \nDrilling down is nothing more than asking for a row header from a dimension that \nprovides more information. \nLet’s say you have a simple report summarizing the sales dollar amount by depart-\nment. As illustrated in Figure 3-9, if you want to drill down, you can drag any \nother attribute, such as brand, from the product dimension into the report next to \ndepartment, and you can automatically drill down to this next level of detail. You \ncould drill down by the fat content attribute, even though it isn’t in the merchandise \nhierarchy rollup.\nNOTE \nDrilling down in a dimensional model is nothing more than adding row \nheader attributes from the dimension tables. Drilling up is removing row headers. \nYou can drill down or up on attributes from more than one explicit hierarchy and \nwith attributes that are part of no hierarchy.\n\n\nRetail Sales 87\nDepartment\nName\nSales Dollar\nAmount\nBakery\nFrozen Foods\n12,331\n31,776\nDrill down by brand name:\nBakery\nBakery\nBakery\nFrozen Foods\nFrozen Foods\nFrozen Foods\nFrozen Foods\nFrozen Foods\nBaked Well\nFluffy\nLight\nColdpack\nFreshlike\nFrigid\nIcy\nQuickFreeze\n3,009 \n3,024 \n6,298 \n5,321 \n10,476 \n7,328 \n2,184 \n6,467\nDepartment\nName\nBrand\nName\nSales Dollar\nAmount\nOr drill down by fat content:\nBakery\nBakery\nBakery\nFrozen Foods\nFrozen Foods\nFrozen Foods\nNonfat\nReduced fat\nRegular fat\nNonfat\nReduced fat\nRegular fat\n6,298 \n5,027 \n1,006 \n5,321 \n10,476 \n15,979\nDepartment\nName\nFat\nContent\nSales Dollar\nAmount\nFigure 3-9: Drilling down on dimension attributes.\nThe product dimension is a common dimension in many dimensional models. \nGreat care should be taken to ﬁ ll this dimension with as many descriptive attributes \nas possible. A robust and complete set of dimension attributes translates into robust \nand complete analysis capabilities for the business users. We’ll further explore the \nproduct dimension in Chapter 5: Procurement where we’ll also discuss the handling \nof product attribute changes.\nStore Dimension \nThe  store dimension describes every store in the grocery chain. Unlike the product \nmaster ﬁ le that is almost guaranteed to be available in every large grocery business, \nthere may not be a comprehensive store master ﬁ le. POS systems may simply sup-\nply a store number on the transaction records. In these cases, project teams must \nassemble the necessary components of the store dimension from multiple opera-\ntional sources. Often there will be a store real estate department at headquarters \nwho will help deﬁ ne a detailed store master ﬁ le.\n\n\nChapter 3\n88\n Multiple Hierarchies in Dimension Tables\nThe  store dimension is the case study’s primary geographic dimension. Each store \ncan be thought of as a location. You can roll stores up to any geographic attribute, \nsuch as ZIP code, county, and state in the United States. Contrary to popular \nbelief, cities and states within the United States are not a hierarchy. Since many \nstates have identically named cities, you’ll want to include a City-State attribute \nin the store dimension.\nStores likely also roll up an internal organization hierarchy consisting of store \ndistricts and regions. These two diff erent store hierarchies are both easily repre-\nsented in the dimension because both the geographic and organizational hierarchies \nare well deﬁ ned for a single store row. \nNOTE \nIt is not uncommon to represent multiple hierarchies in a dimension \ntable. The attribute names and values should be unique across the multiple \nhierarchies.\nA recommended retail store dimension table is shown in Figure 3-10.\nStore Key (PK)\nStore Number (NK)\nStore Name\nStore Street Address\nStore City\nStore County\nStore City-State\nStore State\nStore Zip Code\nStore Manager\nStore District\nStore Region\nFloor Plan Type\nPhoto Processing Type\nFinancial Service Type\nSelling Square Footage\nTotal Square Footage\nFirst Open Date\nLast Remodel Date\n...\nStore Dimension\nFigure 3-10: Store dimension table.\nThe ﬂ oor plan type, photo processing type, and ﬁ nance services type are all short \ntext descriptors that describe the particular store. These should not be one-character \ncodes but rather should be 10- to 20-character descriptors that make sense when \nviewed in a pull-down ﬁ lter list or used as a report label.\n\n\nRetail Sales 89\nThe column describing selling square footage is numeric and theoretically addi-\ntive across stores. You might be tempted to place it in the fact table. However, it is \nclearly a constant attribute of a store and is used as a constraint or label more often \nthan it is used as an additive element in a summation. For these reasons, selling \nsquare footage belongs in the store dimension table.\nDates Within Dimension Tables\nThe ﬁ rst open date and last remodel date in the store dimension could be date type \ncolumns. However, if users want to group and constrain on nonstandard calendar \nattributes (like the open date’s ﬁ scal period), then they are typically join keys to \ncopies of the date dimension table. These date dimension copies are declared in SQL \nby the view construct and are semantically distinct from the primary date dimen-\nsion. The view declaration would look like the following:\ncreate view first_open_date (first_open_day_number, first_open_month, \n...)\n    as select day_number, month, ... \n    from date\nNow  the system acts as if there is another physical copy of the date dimension \ntable called FIRST_OPEN_DATE. Constraints on this new date table have nothing to \ndo with constraints on the primary date dimension joined to the fact table. The ﬁ rst \nopen date view is a permissible outrigger to the store dimension; outriggers will be \ndescribed in more detail later in this chapter. Notice we have carefully relabeled all \nthe columns in the view so they cannot be confused with columns from the primary \ndate dimension. These distinct logical views on a single physical date dimension are \nan example of dimension role playing, which we’ll discuss more fully in Chapter 6: \nOrder  Management.\nPromotion Dimension\nThe  promotion dimension is potentially the most interesting dimension in the \nretail sales schema. The promotion dimension describes the promotion condi-\ntions under which a product is sold. Promotion conditions include temporary \nprice reductions, end aisle displays, newspaper ads, and coupons. This dimension \nis often called a causal dimension because it describes factors thought to cause a \nchange in product sales.\nBusiness analysts at both headquarters and the stores are interested in determin-\ning whether a promotion is eff ective. Promotions are judged on one or more of the \nfollowing factors:\n \n■Whether  the products under promotion experienced a gain in sales, called \nlift, during the promotional period. The lift can be measured only if the store \ncan agree on what the baseline sales of the promoted products would have \n",
      "page_number": 112
    },
    {
      "number": 14,
      "title": "Segment 14 (pages 120-127)",
      "start_page": 120,
      "end_page": 127,
      "detection_method": "topic_boundary",
      "content": "Chapter 3\n90\nbeen without the promotion. Baseline values can be estimated from prior sales \nhistory and, in some cases, with the help of sophisticated models.\n \n■Whether  the products under promotion showed a drop in sales just prior to \nor after the promotion, canceling the gain in sales during the promotion (time \nshifting). In other words, did you transfer sales from regularly priced products \nto temporarily reduced priced products?\n \n■Whether  the products under promotion showed a gain in sales but other \nproducts nearby on the shelf showed a corresponding sales decrease \n(cannibalization).\n \n■Whether  all the products in the promoted category of products experienced a \nnet overall gain in sales taking into account the time periods before, during, \nand after the promotion (market growth).\n \n■Whether the promotion was proﬁ table. Usually the proﬁ t of a promotion is \ntaken to be the incremental gain in proﬁ t of the promoted category over the \nbaseline sales taking into account time shifting and cannibalization, as well \nas the costs of the promotion.\nThe causal conditions potentially aff ecting a sale are not necessarily tracked \ndirectly by the POS system. The transaction system keeps track of price reduc-\ntions and markdowns. The presence of coupons also typically is captured with \nthe transaction because the customer either presents coupons at the time of sale \nor does not. Ads and in-store display conditions may need to be linked from other \nsources. \nThe various possible causal conditions are highly correlated. A temporary price \nreduction usually is associated with an ad and perhaps an end aisle display. For \nthis reason, it makes sense to create one row in the promotion dimension for each \ncombination of promotion conditions that occurs. Over the course of a year, there \nmay be 1,000 ads, 5,000 temporary price reductions, and 1,000 end aisle displays, \nbut there may be only 10,000 combinations of these three conditions aff ecting any \nparticular product. For example, in a given promotion, most of the stores would run \nall three promotion mechanisms simultaneously, but a few of the stores may not \ndeploy the end aisle displays. In this case, two separate promotion condition rows \nwould be needed, one for the normal price reduction plus ad plus display and one \nfor the price reduction plus ad only. A recommended promotion dimension table \nis shown in Figure 3-11.\nFrom a purely logical point of view, you could record similar information about \nthe promotions by separating the four causal mechanisms (price reductions, ads, \ndisplays, and coupons) into separate dimensions rather than combining them into \none dimension. Ultimately, this choice is the designer’s prerogative. The trade-off s \nin favor of keeping the four dimensions together include the following:\n\n\nRetail Sales 91\n \n■If the four causal mechanisms are highly correlated, the combined single \ndimension is not much larger than any one of the separated dimensions \nwould be.\n \n■The combined single dimension can be browsed effi  ciently to see how the vari-\nous price reductions, ads, displays, and coupons are used together. However, \nthis browsing only shows the possible promotion combinations. Browsing in \nthe dimension table does not reveal which stores or products were aff ected \nby the promotion; this information is found in the fact table.\nPromotion Key (PK)\nPromotion Code\nPromotion Name\nPrice Reduction Type\nPromotion Media Type\nAd Type\nDisplay Type\nCoupon Type\nAd Media Name\nDisplay Provider\nPromotion Cost\nPromotion Begin Date\nPromotion End Date\n...\nPromotion Dimension\nFigure 3-11: Promotion dimension table.\nThe trade-off s in favor of separating the causal mechanisms into four distinct \ndimension tables include the following:\n \n■The separated dimensions may be more understandable to the business com-\nmunity if users think of these mechanisms separately. This would be revealed \nduring the business requirement interviews.\n \n■Administration of the separate dimensions may be more straightforward than \nadministering a combined dimension.\nKeep in mind there is no diff erence in the content between these two choices.\nNOTE \nThe inclusion of promotion cost attribute in the promotion dimension \nshould be done with careful thought. This attribute can be used for constraining \nand grouping. However, this cost should not appear in the POS transaction fact \ntable representing individual product sales because it is at the wrong grain; this \ncost would have to reside in a fact table whose grain is the overall promotion.\n\n\nChapter 3\n92\n Null Foreign Keys, Attributes, and Facts\nTypically,  many sales transactions include products that are not being promoted. \nHopefully, consumers aren’t just ﬁ lling their shopping cart with promoted products; \nyou want them paying full price for some products in their cart! The promotion \ndimension must include a row, with a unique key such as 0 or –1, to identify this \nno promotion condition and avoid a null promotion key in the fact table. Referential \nintegrity is violated if you put a null in a fact table column declared as a foreign key \nto a dimension table. In addition to the referential integrity alarms, null keys are \nthe source of great confusion to users because they can’t join on null keys.\nWARNING \nYou must avoid null keys in the fact table. A proper design includes \na row in the corresponding dimension table to identify that the dimension is not \napplicable to the measurement.\nWe  sometimes encounter nulls as dimension attribute values. These usually result \nwhen a given dimension row has not been fully populated, or when there are attri-\nbutes that are not applicable to all the dimension’s rows. In either case, we recom-\nmend substituting a descriptive string, such as Unknown or Not Applicable, in place \nof the null value. Null values essentially disappear in pull-down menus of possible \nattribute values or in report groupings; special syntax is required to identify them. \nIf users sum up facts by grouping on a fully populated dimension attribute, and then \nalternatively, sum by grouping on a dimension attribute with null values, they’ll get \ndiff erent query results. And you’ll get a phone call because the data doesn’t appear to \nbe consistent. Rather than leaving the attribute null, or substituting a blank space or \na period, it’s best to label the condition; users can then purposely decide to exclude \nthe Unknown or Not Applicable from their query. It’s worth noting that some OLAP \nproducts prohibit null attribute values, so this is one more reason to avoid them.\nFinally, we can also encounter nulls as metrics in the fact table. We generally \nleave these null so that they’re properly handled in aggregate functions such as SUM, \nMIN, MAX, COUNT, and AVG which do the “right thing” with nulls. Substituting a zero \ninstead would improperly skew these aggregated calculations.\nData  mining tools may use diff erent techniques for tracking nulls. You may need \nto do some additional transformation work beyond the above recommendations if \ncreating an observation set for data  mining.\nOther Retail Sales Dimensions\nAny  descriptive attribute that takes on a single value in the presence of a fact table \nmeasurement event is a good candidate to be added to an existing dimension or \n\n\nRetail Sales 93\nbe its own dimension. The decision whether a dimension should be associated \nwith a fact table should be a binary yes/no based on the fact table’s declared \ngrain. For example, there’s probably a cashier identiﬁ ed for each transaction. The \ncorresponding cashier dimension would likely contain a small subset of non-\nprivate employee attributes. Like the promotion dimension, the cashier dimension \nwill likely have a No Cashier row for transactions that are processed through \nself-service registers.\nA  trickier situation unfolds for the payment method. Perhaps the store has rigid \nrules and only accepts one payment method per transaction. This would make \nyour life as a dimensional modeler easier because you’d attach a simple payment \nmethod dimension to the sales schema that would likely include a payment method \ndescription, along with perhaps a grouping of payment methods into either cash \nequivalent or credit payment types.\nIn real life, payment methods often present a more complicated scenario. If \nmultiple payment methods are accepted on a single POS transaction, the payment \nmethod does not take on a single value at the declared grain. Rather than altering \nthe declared grain to be something unnatural such as one row per payment method \nper product on a POS transaction, you would likely capture the payment method in \na separate fact table with a granularity of either one row per transaction (then the \nvarious payment method options would appear as separate facts) or one row per \npayment method per transaction (which would require a separate payment method \ndimension to associate with each  row).\n Degenerate Dimensions for Transaction Numbers\nThe  retail sales fact table includes the POS transaction number on every line item \nrow. In an operational parent/child database, the POS transaction number would \nbe the key to the transaction header record, containing all the information valid \nfor the transaction as a whole, such as the transaction date and store identiﬁ er. \nHowever, in the dimensional model, you have already extracted this interesting \nheader information into other dimensions. The POS transaction number is still \nuseful because it serves as the grouping key for pulling together all the products \npurchased in a single market basket transaction. It also potentially enables you to \nlink back to the operational system.\nAlthough the POS transaction number looks like a dimension key in the fact \ntable, the descriptive items that might otherwise fall in a POS transaction dimension \nhave been stripped off . Because the resulting dimension is empty, we refer to the \nPOS transaction number as a degenerate dimension (identiﬁ ed by the DD notation \n\n\nChapter 3\n94\nin this book’s ﬁ gures). The natural operational ticket number, such as the POS \ntransaction number, sits by itself in the fact table without joining to a dimension \ntable. Degenerate dimensions are very common when the grain of a fact table rep-\nresents a single transaction or transaction line because the degenerate dimension \nrepresents the unique identiﬁ er of the parent. Order numbers, invoice numbers, \nand bill-of-lading numbers almost always appear as degenerate dimensions in a \ndimensional model.\nDegenerate dimensions often play an integral role in the fact table’s primary \nkey. In our case study, the primary key of the retail sales fact table consists of the \ndegenerate POS transaction number and product key, assuming scans of identical \nproducts in the market basket are grouped together as a single line item. \nNOTE \nOperational transaction control numbers such as order numbers, invoice \nnumbers, and bill-of-lading numbers usually give rise to empty dimensions and are \nrepresented as degenerate dimensions in transaction fact tables. The degenerate \ndimension is a dimension key without a corresponding dimension table.\nIf, for some reason, one or more attributes are legitimately left over after all the \nother dimensions have been created and seem to belong to this header entity, you \nwould simply create a normal dimension row with a normal join. However, you would \nno longer have a degenerate dimension.\n Retail Schema in Action\nWith  our retail POS schema designed, let’s illustrate how it would be put to use in \na query environment. A business user might be interested in better understanding \nweekly sales dollar volume by promotion for the snacks category during January \n2013 for stores in the Boston district. As illustrated in Figure 3-12, you would place \nquery constraints on month and year in the date dimension, district in the store \ndimension, and category in the product dimension.\nIf the query tool summed the sales dollar amount grouped by week ending \ndate and promotion, the SQL query results would look similar to those below in \nFigure 3-13. You can plainly see the relationship between the dimensional model \nand the associated query. High-quality dimension attributes are crucial because they \nare the source of query constraints and report labels. If you use a BI tool with more \nfunctionality, the results would likely appear as a cross-tabular “pivoted” report, \nwhich may be more appealing to business users than the columnar data resulting \nfrom an SQL  statement.\n\n\nRetail Sales 95\nDate Key (PK)\nDate\nDay of Week\nCalendar Month\nCalendar Quarter\nCalendar Year\n...\nDate Key (FK)\nProduct Key (FK)\nStore Key (FK)\nPromotion Key (FK)\nCashier Key (FK)\nPayment Method Key (FK)\nPOS Transaction # (DD)\nSales Quantity\nRegular Unit Price\nDiscount Unit Price\nNet Unit Price\nExtended Discount Dollar Amount\nExtended Sales Dollar Amount\nExtended Cost Dollar Amount\nExtended Gross Profit Dollar Amount\nProduct Key (PK)\nSKU Number (NK)\nProduct Description\nBrand Description\nCategory Description\n...\nPromotion Key (PK)\nPromotion Code (NK)\nPromotion Name\nPromotion Media Type\nPromotion Begin Date\n...\nSnacks\nJanuary\n2013\nBoston\nPayment Method Key (PK)\nPayment Method Description\nPayment Method Group\nStore Key (PK)\nStore Number (NK)\nStore Name\nStore District\nStore Region\n...\nCashier Key (PK)\nCashier Employee ID (NK)\nCashier Name\n...\nDate Dimension\nRetail Sales Facts\nProduct Dimension\nPromotion Dimension\nPayment Method Dimension\nStore Dimension\nCashier Dimension\nFigure 3-12: Querying the retail sales schema.\nCalendar Week\nEnding Date\nJanuary 6, 2013 \nJanuary 13, 2013 \nJanuary 20, 2013 \nJanuary 27, 2013\nNo Promotion\nNo Promotion\nSuper Bowl Promotion\nSuper Bowl Promotion\n2,647 \n4,851 \n7,248 \n13,798\nExtended Sales\nDollar Amount\nPromotion Name\nDepartment\nName\nNo Promotion\nExtended Sales\nDollar Amount\nSuper Bowl Promotion\nExtended Sales\nDollar Amount\nJanuary 6, 2013 \nJanuary 13, 2013 \nJanuary 20, 2013 \nJanuary 27, 2013\n2,647 \n4,851 \n0 \n0\n0 \n0 \n7,248 \n13,798\nFigure 3-13: Query results and cross-tabular report.\n Retail Schema Extensibility \nLet’s  turn our attention to extending the initial dimensional design. Several years \nafter the rollout of the retail sales schema, the retailer implements a frequent shop-\nper program. Rather than knowing an unidentiﬁ ed shopper purchased 26 items on \n\n\nChapter 3\n96\na cash register receipt, you can now identify the speciﬁ c shopper. Just imagine the \nbusiness users’ interest in analyzing shopping patterns by a multitude of geographic, \ndemographic, behavioral, and other diff erentiating shopper characteristics.\nThe  handling of this new frequent shopper information is relatively straightfor-\nward. You’d create a frequent shopper dimension table and add another foreign key \nin the fact table. Because you can’t ask shoppers to bring in all their old cash register \nreceipts to tag their historical sales transactions with their new frequent shopper \nnumber, you’d substitute a default shopper dimension surrogate key, corresponding \nto a Prior to Frequent Shopper Program dimension row, on the historical fact table \nrows. Likewise, not everyone who shops at the grocery store will have a frequent \nshopper card, so you’d also want to include a Frequent Shopper Not Identiﬁ ed row \nin the shopper dimension. As we discussed earlier with the promotion dimension, \nyou can’t have a null frequent shopper key in the fact table.\nOur original schema gracefully extends to accommodate this new dimension \nlargely because the POS transaction data was initially modeled at its most granular \nlevel. The addition of dimensions applicable at that granularity did not alter the \nexisting dimension keys or facts; all existing BI applications continue to run without \nany changes. If the grain was originally declared as daily retail sales (transactions \nsummarized by day, store, product, and promotion) rather than the transaction line \ndetail, you would not have been able to incorporate the frequent shopper dimen-\nsion. Premature summarization or aggregation inherently limits your ability to add \nsupplemental dimensions because the additional dimensions often don’t apply at \nthe higher grain.\nThe predictable symmetry of dimensional models enable them to absorb some \nrather signiﬁ cant changes in source data and/or modeling assumptions without \ninvalidating existing BI applications, including:\n \n■New dimension attributes. If you discover new textual descriptors of a dimen-\nsion, you can add these attributes as new columns. All existing applications \nwill be oblivious to the new attributes and continue to function. If the new \nattributes are available only after a speciﬁ c point in time, then Not Available \nor its equivalent should be populated in the old dimension rows. Be fore-\nwarned that this scenario is more complicated if the business users want to \ntrack historical changes to this newly identiﬁ ed attribute. If this is the case, \npay close attention to the slowly changing dimension coverage in Chapter 5.\n \n■New dimensions. As we just discussed, you can add a dimension to an exist-\ning fact table by adding a new foreign key column and populating it correctly \nwith values of the primary key from the new dimension.\n\n\nRetail Sales 97\n \n■New measured facts. If new measured facts become available, you can add \nthem gracefully to the fact table. The simplest case is when the new facts are \navailable in the same measurement event and at the same grain as the existing \nfacts. In this case, the fact table is altered to add the new columns, and the \nvalues are populated into the table. If the new facts are only available from \na point in time forward, null values need to be placed in the older fact rows. \nA more complex situation arises when new measured facts occur naturally \nat a diff erent grain. If the new facts cannot be allocated or assigned to the \noriginal grain of the fact table, the new facts belong in their own fact table \nbecause it’s a mistake to mix grains in the same fact table.\n Factless Fact Tables\nThere  is one important question that cannot be answered by the previous retail sales \nschema: What products were on promotion but did not sell? The sales fact table \nrecords only the SKUs actually sold. There are no fact table rows with zero facts \nfor SKUs that didn’t sell because doing so would enlarge the fact table enormously. \nIn the relational world, a promotion coverage or event fact table is needed to \nanswer the question concerning what didn’t happen. The promotion coverage fact \ntable keys would be date, product, store, and promotion in this case study. This \nobviously looks similar to the sales fact table you just designed; however, the grain \nwould be signiﬁ cantly diff erent. In the case of the promotion coverage fact table, \nyou’d load one row for each product on promotion in a store each day (or week, if \nretail promotions are a week in duration) regardless of whether the product sold. \nThis fact table enables you to see the relationship between the keys as deﬁ ned by a \npromotion, independent of other events, such as actual product sales. We refer to it \nas a factless fact table because it has no measurement metrics; it merely captures the \nrelationship between the involved keys, as illustrated in Figure 3-14. To facilitate \ncounting, you can include a dummy fact, such as promotion count in this example, \nwhich always contains the constant value of 1; this is a cosmetic enhancement that \nenables the BI application to avoid counting one of the foreign keys.\nTo  determine what products were on promotion but didn’t sell requires a two-\nstep process. First, you’d query the promotion factless fact table to determine the \nuniverse of products that were on promotion on a given day. You’d then determine \nwhat products sold from the POS sales fact table. The answer to our original ques-\ntion is the set diff erence between these two lists of products. If you work with data \n",
      "page_number": 120
    },
    {
      "number": 15,
      "title": "Segment 15 (pages 128-139)",
      "start_page": 128,
      "end_page": 139,
      "detection_method": "topic_boundary",
      "content": "Chapter 3\n98\nin an OLAP cube, it is often easier to answer the “what didn’t happen” question \nbecause the cube typically contains explicit cells for nonbehavior.\nDate Key (PK)\nDate\nDay of Week\nCalendar Month\nCalendar Quarter\nCalendar Year\n...\nStore Key (PK)\nStore Number (NK)\nStore Name\nStore District\nStore Region\n...\nDate Key (FK)\nProduct Key (FK)\nStore Key (FK)\nPromotion Key (FK)\nPromotion Count (=1)\nProduct Key (PK)\nSKU Number (NK)\nProduct Description\nBrand Description\nCategory Description\n...\nPromotion Key (PK)\nPromotion Code (NK)\nPromotion Name\nPromotion Media Type\nPromotion Begin Date\n...\nDate Dimension\nStore Dimension\nPromotion Coverage Facts\nProduct Dimension\nPromotion Dimension\nFigure 3-14: Promotion coverage factless fact table.\nDimension and Fact Table Keys\nNow that the schemas have been designed, we’ll focus on the dimension and fact \ntables’ primary keys, along with other row identiﬁ ers.\n Dimension Table Surrogate Keys\nThe  unique primary key of a dimension table should be a surrogate key rather than \nrelying on the operational system identiﬁ er, known as the natural key. Surrogate keys \ngo by many other aliases: meaningless keys, integer keys, non-natural keys, artiﬁ -\ncial keys, and synthetic keys. Surrogate keys are simply integers that are assigned \nsequentially as needed to populate a dimension. The ﬁ rst product row is assigned a \nproduct surrogate key with the value of 1; the next product row is assigned product \nkey 2; and so forth. The actual surrogate key value has no business signiﬁ cance. The \nsurrogate keys merely serve to join the dimension tables to the fact table. Throughout \nthis book, column names with a Key suffi  x, identiﬁ ed as a primary key (PK) or \nforeign key (FK), imply a surrogate.\nModelers sometimes are reluctant to relinquish the natural keys because they \nwant to navigate the fact table based on the operational code while avoiding a join \nto the dimension table. They also don’t want to lose the embedded intelligence \nthat’s often part of a natural multipart key. However, you should avoid relying on \n\n\nRetail Sales 99\nintelligent dimension keys because any assumptions you make eventually may be \ninvalidated. Likewise, queries and data access applications should not have any \nbuilt-in dependency on the keys because the logic also would be vulnerable to \ninvalidation. Even if the natural keys appear to be stable and devoid of meaning, \ndon’t be tempted to use them as the dimension table’s primary key.\nNOTE \nEvery join between dimension and fact tables in the data warehouse \nshould be based on meaningless integer surrogate keys. You should avoid using a \nnatural key as the dimension table’s primary key.\nInitially,  it may be faster to implement a dimensional model using operational \nnatural keys, but surrogate keys pay off  in the long run. We sometimes think of \nthem as being similar to a ﬂ u shot for the data warehouse—like an immunization, \nthere’s a small amount of pain to initiate and administer surrogate keys, but the long \nrun beneﬁ ts are substantial, especially considering the reduced risk of substantial \nrework. Here are several advantages:\n \n■Buff er the data warehouse from operational changes. Surrogate keys enable \nthe warehouse team to maintain control of the DW/BI environment rather \nthan being whipsawed by operational rules for generating, updating, deleting, \nrecycling, and reusing production codes. In many organizations, historical \noperational codes, such as inactive account numbers or obsolete product \ncodes, get reassigned after a period of dormancy. If account numbers get \nrecycled following 12 months of inactivity, the operational systems don’t miss \na beat because their business rules prohibit data from hanging around for that \nlong. But the DW/BI system may retain data for years. Surrogate keys provide \nthe warehouse with a mechanism to diff erentiate these two separate instances \nof the same operational account number. If you rely solely on operational \ncodes, you might also be vulnerable to key overlaps in the case of an acquisi-\ntion or consolidation of data.\n \n■Integrate multiple source systems. Surrogate keys enable the data warehouse \nteam to integrate data from multiple operational source systems, even if they \nlack consistent source keys by using a back room cross-reference mapping \ntable to link the multiple natural keys to a common surrogate.\n \n■Improve performance. The surrogate key is as small an integer as possible \nwhile ensuring it will comfortably accommodate the future anticipated car-\ndinality (number of rows in the dimension). Often the operational code is a \nbulky alphanumeric character string or even a group of ﬁ elds. The smaller \nsurrogate key translates into smaller fact tables, smaller fact table indexes, \nand more fact table rows per block input-output operation. Typically, a 4-byte \n\n\nChapter 3\n100\ninteger is suffi  cient to handle most dimensions. A 4-byte integer is a single \ninteger, not four decimal digits. It has 32 bits and therefore can handle approx-\nimately 2 billion positive values (232) or 4 billion total positive and negative \nvalues (–232 to +232). This is more than enough for just about any dimension. \nRemember, if you have a large fact table with 1 billion rows of data, every byte \nin each fact table row translates into another gigabyte of storage.\n \n■Handle null or unknown conditions. As mentioned earlier, special surrogate \nkey values are used to record dimension conditions that may not have an \noperational code, such as the No Promotion condition or the anonymous \ncustomer. You can assign a surrogate key to identify these despite the lack of \noperational coding. Similarly, fact tables sometimes have dates that are yet \nto be determined. There is no SQL date type value for Date to Be Determined \nor Date Not Applicable. \n \n■Support dimension attribute change tracking. One of the primary techniques \nfor handling changes to dimension attributes relies on surrogate keys to handle \nthe multiple proﬁ les for a single natural key. This is actually one of the most \nimportant reasons to use surrogate keys, which we’ll describe in Chapter 5. \nA pseudo surrogate key created by simply gluing together the natural key \nwith a time stamp is perilous. You need to avoid multiple joins between the \ndimension and fact tables, sometimes referred to as double-barreled joins, due \nto their adverse impact on performance and ease of use.\nOf course, some eff ort is required to assign and administer surrogate keys, but \nit’s not nearly as intimidating as many people imagine. You need to establish and \nmaintain a cross-reference table in the ETL system that will be used to substitute the \nappropriate surrogate key on each fact and dimension table row. We lay out a process \nfor administering surrogate keys in Chapter 19: ETL Subsystems and Techniques.\n Dimension Natural and Durable Supernatural Keys\nLike surrogate keys, the natural keys assigned and used by operational source sys-\ntems go by other names, such as business keys, production keys, and operational \nkeys. They are identiﬁ ed with the NK notation in the book’s ﬁ gures. The natural \nkey is often modeled as an attribute in the dimension table. If the natural key comes \nfrom multiple sources, you might use a character data type that prepends a source \ncode, such as SAP|43251 or CRM|6539152. If the same entity is represented in both \noperational source systems, then you’d likely have two natural key attributes in \nthe dimension corresponding to both sources. Operational natural keys are often \ncomposed of meaningful constituent parts, such as the product’s line of business \nor country of origin; these components should be split apart and made available as \nseparate attributes.\n\n\nRetail Sales 101\nIn a  dimension table with attribute change tracking, it’s important to have an iden-\ntiﬁ er that uniquely and reliably identiﬁ es the dimension entity across its attribute \nchanges. Although the operational natural key may seem to ﬁ t this bill, sometimes \nthe natural key changes due to unexpected business rules (like an organizational \nmerger) or to handle either duplicate entries or data integration from multiple \nsources. If the dimension’s natural keys are not absolutely protected and preserved \nover time, the ETL system needs to assign permanent durable identiﬁ ers, also known \nas supernatural keys. A persistent durable supernatural key is controlled by the DW/\nBI system and remains immutable for the life of the system. Like the dimension \nsurrogate key, it’s a simple integer sequentially assigned. And like the natural keys \ndiscussed earlier, the durable supernatural key is handled as a dimension attribute; \nit’s not a replacement for the dimension table’s surrogate primary key. Chapter 19 \nalso discusses the ETL system’s responsibility for these durable identiﬁ ers.\nDegenerate Dimension Surrogate Keys\nAlthough  surrogate keys aren’t typically assigned to degenerate dimensions, each \nsituation needs to be evaluated to determine if one is required. A surrogate key is \nnecessary if the transaction control numbers are not unique across locations or get \nreused. For example, the retailer’s POS system may not assign unique transaction \nnumbers across stores. The system may wrap back to zero and reuse previous con-\ntrol numbers when its maximum has been reached. Also, the transaction control \nnumber may be a bulky 24-byte alphanumeric column. Finally, depending on the \ncapabilities of the BI tool, you may need to assign a surrogate key (and create an \nassociated dimension table) to drill across on the transaction number. Obviously, \ncontrol number dimensions modeled in this way with corresponding dimension \ntables are no longer degenerate.\nDate Dimension Smart Keys\nAs  we’ve noted, the date dimension has unique characteristics and requirements. \nCalendar dates are ﬁ xed and predetermined; you never need to worry about deleting \ndates or handling new, unexpected dates on the calendar. Because of its predict-\nability, you can use a more intelligent key for the date dimension.\nIf  a sequential integer serves as the primary key of the date dimension, it should \nbe chronologically assigned. In other words, January 1 of the ﬁ rst year would \nbe assigned surrogate key value 1, January 2 would be assigned surrogate key 2, \nFebruary 1 would be assigned surrogate key 32, a nd so on.\nMore commonly, the primary key of the date dimension is a meaningful integer \nformatted as yyyymmdd. The yyyymmdd key is not intended to provide business \nusers and their BI applications with an intelligent key so they can bypass the date \ndimension and directly query the fact table. Filtering on the fact table’s yyyymmdd \n\n\nChapter 3\n102\nkey would have a detrimental impact on usability and performance. Filtering and \ngrouping on calendar attributes should occur in a dimension table, not in the BI \napplication’s code.\nHowever,  the yyyymmdd key is useful for partitioning fact tables. Partitioning \nenables a table to be segmented into smaller tables under the covers. Partitioning \na large fact table on the basis of date is eff ective because it allows old data to be \nremoved gracefully and new data to be loaded and indexed in the current parti-\ntion without disturbing the rest of the fact table. It reduces the time required for \nloads, backups, archiving, and query response. Programmatically updating and \nmaintaining partitions is straightforward if the date key is an ordered integer: year \nincrements by 1 up to the number of years wanted, month increments by 1 up to \n12, and so on. Using a smart yyyymmdd key provides the beneﬁ ts of a surrogate, \nplus the advantages of easier partition management.\nAlthough the yyyymmdd integer is the most common approach for date dimen-\nsion keys, some relational database optimizers prefer a true date type column for \npartitioning. In these cases, the optimizer knows there are 31 values between \nMarch 1 and April 1, as opposed to the apparent 100 values between 20130301 and \n20130401. Likewise, it understands there are 31 values between December 1 and \nJanuary 1, as opposed to the 8,900 integer values between 20121201 and 20130101. \nThis intelligence can impact the query strategy chosen by the optimizer and further \nreduce query times. If the optimizer incorporates date type intelligence, it should \nbe considered for the date key. If the only rationale for a date type key is simpliﬁ ed \nadministration for the DBA, then you can feel less compelled.\nWith more intelligent date keys, whether chronologically assigned or a more \nmeaningful yyyymmdd integer or date type column, you need to reserve a special \ndate key value for the situation in which the date is unknown when the fact row is \ninitially loaded.\n Fact Table Surrogate Keys\nAlthough  we’re adamant about using surrogate keys for dimension tables, we’re less \ndemanding about a surrogate key for fact tables. Fact table surrogate keys typically \nonly make sense for back room ETL processing. As we mentioned, the primary \nkey of a fact table typically consists of a subset of the table’s foreign keys and/or \ndegenerate dimension. However, single column surrogate keys for fact tables have \nsome interesting back room beneﬁ ts.\nLike its dimensional counterpart, a fact table surrogate key is a simple integer, \ndevoid of any business content, that is assigned in sequence as fact table rows are \ngenerated. Although the fact table surrogate key is unlikely to deliver query perfor-\nmance advantages, it does have the following beneﬁ ts:\n\n\nRetail Sales 103\n \n■Immediate unique identiﬁ cation. A single fact table row is immediately iden-\ntiﬁ ed by the key. During ETL processing, a speciﬁ c row can be identiﬁ ed \nwithout navigating multiple dimensions.\n \n■Backing out or resuming a bulk load. If a large number of rows are being \nloaded with sequentially assigned surrogate keys, and the process halts before \ncompletion, the DBA can determine exactly where the process stopped by \nﬁ nding the maximum key in the table. The DBA could back out the complete \nload by specifying the range of keys just loaded or perhaps could resume the \nload from exactly the correct point.\n \n■Replacing updates with inserts plus deletes. The fact table surrogate key \nbecomes the true physical key of the fact table. No longer is the key of the \nfact table determined by a set of dimensional foreign keys, at least as far as \nthe RDBMS is concerned. Thus it becomes possible to replace a fact table \nupdate operation with an insert followed by a delete. The ﬁ rst step is to \nplace the new row into the database with all the same business foreign keys \nas the row it is to replace. This is now possible because the key enforce-\nment depends only on the surrogate key, and the replacement row has a \nnew surrogate key. Then the second step deletes the original row, thereby \naccomplishing the update. For a large set of updates, this sequence is more \neffi  cient than a set of true update operations. The insertions can be pro-\ncessed with the ability to back out or resume the insertions as described in \nthe previous bullet. These insertions do not need to be protected with full \ntransaction machinery. Then the ﬁ nal deletion step can be performed safely \nbecause the insertions have run to completion.\n \n■Using the fact table surrogate key as a parent in a parent/child schema. In \nthose cases in which one fact table contains rows that are parents of those in \na lower grain fact table, the fact table surrogate key in the parent table is also \nexposed in the child table. The argument of using the fact table surrogate \nkey in this case rather than a natural parent key is similar to the argument \nfor using surrogate keys in dimension tables. Natural keys are messy and \nunpredictable, whereas surrogate keys are clean integers and are assigned by \nthe ETL system, not the source system. Of course, in addition to including \nthe parent fact table’s surrogate key, the lower grained fact table should also \ninclude the parent’s dimension foreign keys so the child facts can be sliced \nand diced without traversing the parent fact table’s surrogate key. And as we’ll \ndiscuss in Chapter 4: Inventory, you should never join fact tables directly to \nother fact tables.\n\n\nChapter 3\n104\nResisting Normalization Urges\nIn this section, let’s directly confront several of the natural urges that tempt model-\ners coming from a more normalized background. We’ve been consciously breaking \nsome traditional modeling rules because we’re focused on delivering value through \nease of use and performance, not on transaction processing effi  ciencies.\n Snowﬂ ake Schemas with Normalized Dimensions\nThe  ﬂ attened, denormalized dimension tables with repeating textual values make \ndata modelers from the operational world uncomfortable. Let’s revisit the case study \nproduct dimension table. The 300,000 products roll up into 50 distinct depart-\nments. Rather than redundantly storing the 20-byte department description in the \nproduct dimension table, modelers with a normalized upbringing want to store a \n2-byte department code and then create a new department dimension for the depart-\nment decodes. In fact, they would feel more comfortable if all the descriptors in the \noriginal design were normalized into separate dimension tables. They argue this \ndesign saves space because the 300,000-row dimension table only contains codes, \nnot lengthy descriptors.\nIn addition, some modelers contend that more normalized dimension tables are \neasier to maintain. If a department description changes, they’d need to update only \nthe one occurrence in the department dimension rather than the 6,000 repetitions \nin the original product dimension. Maintenance often is addressed by normaliza-\ntion disciplines, but all this happens back in the ETL system long before the data \nis loaded into a presentation area’s dimensional schema.\nDimension table normalization is referred to as snowﬂ aking. Redundant attributes \nare removed from the ﬂ at, denormalized dimension table and placed in separate \nnormalized dimension tables. Figure 3-15 illustrates the partial snowﬂ aking of the \nproduct dimension into third normal form. The contrast between Figure 3-15 and \nFigure 3-8 is startling. The plethora of snowﬂ aked tables (even in our simplistic \nexample) is overwhelming. Imagine the impact on Figure 3-12 if all the schema’s \nhierarchies were normalized.\nSnowﬂ aking is a legal extension of the dimensional model, however, we encour-\nage you to resist the urge to snowﬂ ake given the two primary design drivers: ease \nof use and performance.\n\n\nRetail Sales 105\nProduct Key (PK)\nSKU Number (Natural Key)\nProduct Description\nBrand Key (FK)\nPackage Type Key (FK)\nFat Content\nWeight\nWeight Unit of Measure\nStorage Type Key (FK)\nShelf Width\nShelf Height\nShelf Depth\n...\nBrand Key (PK)\nBrand Description\nCategory Key (FK)\nCategory Key (PK)\nCategory Description\nDepartment Key (FK)\nDepartment Key (PK)\nDepartment Number\nDepartment Description\nShelf Life Type Key (PK)\nShelf Life Type Description\nPackage Type Key (PK)\nPackage Type Description\nStorage Type Key (PK)\nStorage Type Description\nShelf Life Type Key (FK)\nProduct Dimension\nBrand Dimension\nCategory Dimension\nShelf Life Type Dimension\nDepartment Dimension\nPackage Type Dimension\nStorage Type Dimension\nFigure 3-15: Snowﬂ aked product dimension.\n \n■The multitude of snowﬂ aked tables makes for a much more complex presen-\ntation. Business users inevitably will struggle with the complexity; simplicity \nis one of the primary objectives of a dimensional model.\n \n■Most database optimizers also struggle with the snowﬂ aked schema’s complex-\nity. Numerous tables and joins usually translate into slower query performance. \nThe complexities of the resulting join speciﬁ cations increase the chances that \nthe optimizer will get sidetracked and choose a poor strategy.\n \n■The minor disk space savings associated with snowﬂ aked dimension tables \nare insigniﬁ cant. If you replace the 20-byte department description in the \n300,000 row product dimension table with a 2-byte code, you’d save a whop-\nping 5.4 MB (300,000 x 18 bytes); meanwhile, you may have a 10 GB fact \ntable! Dimension tables are almost always geometrically smaller than fact \ntables. Eff orts to normalize dimension tables to save disk space are usually \na waste of time.\n \n■Snowﬂ aking negatively impacts the users’ ability to browse within a dimen-\nsion. Browsing often involves constraining one or more dimension attributes \nand looking at the distinct values of another attribute in the presence of these \nconstraints. Browsing allows users to understand the relationship between \ndimension attribute values.\n\n\nChapter 3\n106\n \n■Obviously, a snowﬂ aked product dimension table responds well if you just \nwant a list of the category descriptions. However, if you want to see all the \nbrands within a category, you need to traverse the brand and category dimen-\nsions. If you want to also list the package types for each brand in a category, \nyou’d be traversing even more tables. The SQL needed to perform these seem-\ningly simple queries is complex, and you haven’t touched the other dimensions \nor fact table.\n \n■Finally, snowﬂ aking defeats the use of bitmap indexes. Bitmap indexes are \nuseful when indexing low-cardinality columns, such as the category and \ndepartment attributes in the product dimension table. They greatly speed \nthe performance of a query or constraint on the single column in question. \nSnowﬂ aking inevitably would interfere with your ability to leverage this per-\nformance tuning technique.\nNOTE \nFixed depth hierarchies should be flattened in dimension tables. \nNormalized, snowﬂ aked dimension tables penalize cross-attribute browsing and \nprohibit the use of bitmapped indexes. Disk space savings gained by normalizing \nthe dimension tables typically are less than 1 percent of the total disk space needed \nfor the overall schema. You should knowingly sacriﬁ ce this dimension table space \nin the spirit of performance and ease of use advantages.\nSome database vendors argue their platform has the horsepower to query a fully \nnormalized dimensional model without performance penalties. If you can achieve \nsatisfactory performance without physically denormalizing the dimension tables, \nthat’s ﬁ ne. However, you’ll still want to implement a logical dimensional model with \ndenormalized dimensions to present an easily understood schema to the business \nusers and their BI applications.\nIn the past, some BI tools indicated a preference for snowﬂ ake schemas; snowﬂ ak-\ning to address the idiosyncratic requirements of a BI tool is acceptable. Likewise, if \nall the data is delivered to business users via an OLAP cube (where the snowﬂ aked \ndimensions are used to populate the cube but are never visible to the users), then \nsnowﬂ aking is acceptable. However, in these situations, you need to consider the \nimpact on users of alternative BI tools and the ﬂ exibility to migrate to alternatives \nin the future.\n Outriggers\nAlthough  we generally do not recommend snowﬂ aking, there are situations in which \nit is permissible to build an outrigger dimension that attaches to a dimension within \n\n\nRetail Sales 107\nthe fact table’s immediate halo, as illustrated in Figure 3-16. In this example, the \n“once removed” outrigger is a date dimension snowﬂ aked off  a primary dimension. \nThe outrigger date attributes are descriptively and uniquely labeled to distinguish \nthem from the other dates associated with the business process. It only makes \nsense to outrigger a primary dimension table’s date attribute if the business wants \nto ﬁ lter and group this date by nonstandard calendar attributes, such as the ﬁ scal \nperiod, business day indicator, or holiday period. Otherwise, you could just treat \nthe date attribute as a standard date type column in the product dimension. If a date \noutrigger is used, be careful that the outrigger dates fall within the range stored in \nthe standard date dimension table.\nProduct Key (PK)\nSKU Number (NK)\nProduct Description\nBrand Description\nSubcategory Description\nCategory Description\nDepartment Number\nDepartment Description\nPackage Type Description\nPackage Size\nProduct Introduction Date Key (FK)\n...\nProduct Dimension\nProduct Introduction Date Key (PK)\nProduct Introduction Date\nProduct Introduction Calendar Month\nProduct Introduction Calendar Year\nProduct Introduction Fiscal Month\nProduct Introduction Fiscal Quarter\nProduct Introduction Fiscal Year\nProduct Introduction Holiday Period Indicator\n...\nProduct Introduction Date Dimension\nFigure 3-16: Example of a permissible outrigger.\nYou’ll encounter more outrigger examples later in the book, such as the han-\ndling of customers’ county-level demographic attributes in Chapter 8: Customer \nRelationship Management.\nAlthough outriggers may save space and ensure the same attributes are referenced \nconsistently, there are downsides. Outriggers introduce more joins, which can nega-\ntively impact performance. More important, outriggers can negatively impact the \nlegibility for business users and hamper their ability to browse among attributes \nwithin a single dimension.\nWARNING \nThough outriggers are permissible, a dimensional model should \nnot be littered with outriggers given the potentially negative impact. Outriggers \nshould be the exception rather than the rule.\n\n\nChapter 3\n108\n Centipede Fact Tables with Too Many Dimensions\nThe  fact table in a dimensional schema is naturally highly normalized and compact. \nThere is no way to further normalize the extremely complex many-to-many relation-\nships among the keys in the fact table because the dimensions are not correlated \nwith each other. Every store is open every day. Sooner or later, almost every product \nis sold on promotion in most or all of our stores.\nInterestingly, while uncomfortable with denormalized dimension tables, some \nmodelers are tempted to denormalize the fact table. They have an uncontrollable \nurge to normalize dimension hierarchies but know snowﬂ aking is highly discour-\naged, so the normalized tables end up joined to the fact table instead. Rather than \nhaving a single product foreign key on the fact table, they include foreign keys for \nthe frequently analyzed elements on the product hierarchy, such as brand, category, \nand department. Likewise, the date key suddenly turns into a series of keys joining \nto separate week, month, quarter, and year dimension tables. Before you know it, \nyour compact fact table has turned into an unruly monster that joins to literally \ndozens of dimension tables. We aff ectionately refer to these designs as centipede \nfact tables because they appear to have nearly 100 legs, as shown in Figure 3-17. \nPOS Retail Sales Transaction Fact\nDate Key (FK)\nWeek Key (FK)\nMonth Key (FK)\nQuarter Key (FK)\nYear Key (FK)\nFiscal Year Key (FK)\nFiscal Month Key (FK)\nProduct Key (FK)\nBrand Key (FK)\nCategory Key (FK)\nDepartment Key (FK)\nPackage Type Key (FK)\nStore Key (FK)\nStore County Key (FK)\nStore State Key (FK)\nStore District Key (FK)\nStore Region Key (FK)\nStore Floor Plan Key (FK)\nPromotion Key (FK)\nPromotion Reduction Type Key (FK)\nPromotion Media Type Key (FK)\nPOS Transaction Number (DD)\nSales Quantity\nExtended Discount Dollar Amount\nExtended Sales Dollar Amount\nExtended Cost Dollar Amount\nDate Dimension\nBrand Dimension\nCategory Dimension\nDepartment Dimension\nPackage Type Dimension\nPromotion Dimension\nPromotion Reduction Type Dimension\nPromotion Media Type Dimension\nWeek Dimension\nMonth Dimension\nQuarter Dimension\nYear Dimension\nFiscal Year Dimension\nFiscal Month Dimension\nStore Dimension\nStore County Dimension\nStore State Dimension\nStore District Dimension\nStore Region Dimension\nStore Floor Plan Dimension\nProduct Dimension\nFigure 3-17: Centipede fact table with too many normalized dimensions.\n\n\nRetail Sales 109\nEven with its tight format, the fact table is the behemoth in a dimensional model. \nDesigning a fact table with too many dimensions leads to signiﬁ cantly increased fact \ntable disk space requirements. Although denormalized dimension tables consume \nextra space, fact table space consumption is a concern because it is your largest \ntable by orders of magnitude. There is no way to index the enormous multipart \nkey eff ectively in the centipede example. The numerous joins are an issue for both \nusability and query performance.\nMost business processes can be represented with less than 20 dimensions in the \nfact table. If a design has 25 or more dimensions, you should look for ways to com-\nbine correlated dimensions into a single dimension. Perfectly correlated attributes, \nsuch as the levels of a hierarchy, as well as attributes with a reasonable statistical \ncorrelation, should be part of the same dimension. It’s a good decision to combine \ndimensions when the resulting new single dimension is noticeably smaller than the \nCartesian product of the separate dimensions.\nNOTE \nA very large number of dimensions typically are a sign that several \ndimensions are not completely independent and should be combined into a \nsingle dimension. It is a dimensional modeling mistake to represent elements of \na single hierarchy as separate dimensions in the fact table.\nDevelopments with columnar databases may reduce the query and storage penal-\nties associated with wide centipede fact table designs. Rather than storing each table \nrow, a columnar database stores each table column as a contiguous object that is \nheavily indexed for access. Even though the underlying physical storage is colum-\nnar, at the query level, the table appears to be made up of familiar rows. But when \nqueried, only the named columns are actually retrieved from the disk, rather than \nthe entire row in a more conventional row-oriented relational database. Columnar \ndatabases are much more tolerant of the centipede fact tables just described; how-\never, the ability to browse across hierarchically related dimension attributes may \nbe compromised.\nSummary\nThis chapter was your ﬁ rst exposure to designing a dimensional model. Regardless \nof the industry, we strongly encourage the four-step process for tackling dimensional \nmodel designs. Remember it is especially important to clearly state the grain associ-\nated with a dimensional schema. Loading the fact table with atomic data provides \nthe greatest ﬂ exibility because the data can be summarized “every which way.” As \n",
      "page_number": 128
    },
    {
      "number": 16,
      "title": "Segment 16 (pages 140-155)",
      "start_page": 140,
      "end_page": 155,
      "detection_method": "topic_boundary",
      "content": "Chapter 3\n110\nsoon as the fact table is restricted to more aggregated information, you run into \nwalls when the summarization assumptions prove to be invalid. Also it is vitally \nimportant to populate your dimension tables with verbose, robust descriptive attri-\nbutes for analytic ﬁ ltering and labeling.\nIn the next chapter we’ll remain within the retail industry to discuss techniques \nfor tackling a second business process within the organization, ensuring your earlier \neff orts are leveraged while avoiding stovepipes.\n\n\nInventory\nI\nn Chapter 3: Retail Sales, we developed a dimensional model for the sales transac-\ntions in a large grocery chain. We remain within the same industry in this chapter \nbut move up the value chain to tackle the inventory process. The designs developed \nin this chapter apply to a broad set of inventory pipelines both inside and outside \nthe retail industry.\nMore important, this chapter provides a thorough discussion of the enterprise \ndata warehouse bus architecture. The bus architecture is essential to creating an \nintegrated DW/BI system. It provides a framework for planning the overall environ-\nment, even though it will be built incrementally. We will underscore the importance \nof using common conformed dimensions and facts across dimensional models, and \nwill close by encouraging the adoption of an enterprise data governance program.\nChapter 4 discusses the following concepts:\n \n■Representing organizational value chains via a series of dimensional models\n \n■Semi-additive facts\n \n■Three fact table types: periodic snapshots, transaction, and accumulating \nsnapshots \n \n■Enterprise data warehouse bus architecture and bus matrix\n \n■Opportunity/stakeholder matrix\n \n■Conformed dimensions and facts, and their impact on agile methods\n \n■Importance of data governance\n Value Chain Introduction\nMost  organizations have an underlying value chain of key business processes. The \nvalue chain identiﬁ es the natural, logical ﬂ ow of an organization’s primary activi-\nties. For example, a retailer issues purchase orders to product manufacturers. The \nproducts are delivered to the retailer’s warehouse, where they are held in inven-\ntory. A delivery is then made to an individual store, where again the products sit in \n4\n\n\nChapter 4\n112\ninventory until a consumer makes a purchase. Figure 4-1 illustrates this subset of a \nretailer’s value chain.  Obviously, products sourced from manufacturers that deliver \ndirectly to the retail store would bypass the warehousing processes.\nIssue Purchase\nOrder to\nManufacturer\nReceive\nWarehouse\nDeliveries\nWarehouse\nProduct\nInventory\nReceive\nStore\nDeliveries\nStore\nProduct\nInventory\nRetail\nSales\nFigure 4-1: Subset of a retailer’s value chain.\nOperational source systems typically produce transactions or snapshots at each \nstep of the value chain. The primary objective of most analytic DW/BI systems is \nto monitor the performance results of these key processes. Because each process \nproduces unique metrics at unique time intervals with unique granularity and \ndimensionality, each process typically spawns one or more fact tables. To this end, \nthe value chain provides high-level insight into the overall data architecture for an \nenterprise DW/BI environment. We’ll devote more time to this topic in the “Value \nChain Integration” section later in this chapter.\n Inventory Models\nIn  the meantime, we’ll discuss several complementary inventory models. The ﬁ rst \nis the inventory periodic snapshot where product inventory levels are measured at \nregular intervals and placed as separate rows in a fact table. These periodic snapshot \nrows appear over time as a series of data layers in the dimensional model, much like \ngeologic layers represent the accumulation of sediment over long periods of time. \nWe’ll then discuss a second inventory model where every transaction that impacts \n\n\nInventory 113\ninventory levels as products move through the warehouse is recorded. Finally, in the \nthird model, we’ll describe the inventory accumulating snapshot where a fact table \nrow is inserted for each product delivery and then the row is updated as the product \nmoves through the warehouse. Each model tells a diff erent story. For some analytic \nrequirements, two or even all three models may be appropriate simultaneously.\n Inventory Periodic Snapshot\nLet’s  return to our retail case study. Optimized inventory levels in the stores can have \na major impact on chain proﬁ tability. Making sure the right product is in the right \nstore at the right time minimizes out-of-stocks (where the product isn’t available \non the shelf to be sold) and reduces overall inventory carrying costs. The retailer \nwants to analyze daily quantity-on-hand inventory levels by product and store.\nIt is time to put the four-step dimensional design process to work again. The \nbusiness process we’re interested in analyzing is the periodic snapshotting of retail \nstore inventory. The most atomic level of detail provided by the operational inven-\ntory system is a daily inventory for each product in each store. The dimensions \nimmediately fall out of this grain declaration: date, product, and store. This often \nhappens with periodic snapshot fact tables where you cannot express the granular-\nity in the context of a transaction, so a list of dimensions is needed instead. In this \ncase study, there are no additional descriptive dimensions at this granularity. For \nexample, promotion dimensions are typically associated with product movement, \nsuch as when the product is ordered, received, or sold, but not with inventory.\nThe simplest view of inventory involves only a single fact: quantity on hand. \nThis leads to an exceptionally clean dimensional design, as shown in Figure 4-2.\nDate Dimension\nStore Inventory Snapshot Fact\nDate Key (PK)\n...\nDate Key (FK)\nProduct Key (FK)\nStore Key (FK)\nQuantity on Hand\nProduct Key (PK)\nStorage Requirement Type\n...\nStore Key (PK)\n...\nProduct Dimension\nStore Dimension\nFigure 4-2: Store inventory periodic snapshot schema.\nThe date dimension table in this case study is identical to the table developed \nin Chapter 3 for retail store sales. The product and store dimensions may be deco-\nrated with additional attributes that would be useful for inventory analysis. For \nexample, the product dimension could be enhanced with columns such as the \nminimum reorder quantity or the storage requirement, assuming they are constant \nand discrete descriptors of each product. If the minimum reorder quantity varies for \n\n\nChapter 4\n114\na product by store, it couldn’t be included as a product dimension attribute. In the \nstore dimension, you might include attributes to identify the frozen and refrigerated \nstorage square footages. \nEven a schema as simple as Figure 4-2 can be very useful. Numerous insights can \nbe derived if inventory levels are measured frequently for many products in many \nlocations. However, this periodic snapshot fact table faces a serious challenge that \nChapter 3’s sales transaction fact table did not. The sales fact table was reasonably \nsparse because you don’t sell every product in every shopping cart. Inventory, on \nthe other hand, generates dense snapshot tables. Because the retailer strives to \navoid out-of-stock situations in which the product is not available, there may be \na row in the fact table for every product in every store every day. In that case you \nwould include the zero out-of-stock measurements as explicit rows. For the grocery \nretailer with 60,000 products stocked in 100 stores, approximately 6 million rows \n(60,000 products x 100 stores) would be inserted with each nightly fact table load. \nHowever, because the row width is just 14 bytes, the fact table would grow by only \n84 MB with each load. \nAlthough the data volumes in this case are manageable, the denseness of some \nperiodic snapshots may mandate compromises. Perhaps the most obvious is to \nreduce the snapshot frequencies over time. It may be acceptable to keep the last 60 \ndays of inventory at the daily level and then revert to less granular weekly snap-\nshots for historical data. In this way, instead of retaining 1,095 snapshots during \na 3-year period, the number could be reduced to 208 total snapshots; the 60 daily \nand 148 weekly snapshots should be stored in two separate fact tables given their \nunique periodicity.\n Semi-Additive Facts\nWe  stressed the importance of fact additivity in Chapter 3. In the inventory snap-\nshot schema, the quantity on hand can be summarized across products or stores \nand result in a valid total. Inventory levels, however, are not additive across dates \nbecause they represent snapshots of a level or balance at one point in time. Because \ninventory levels (and all forms of ﬁ nancial account balances) are additive across \nsome dimensions but not all, we refer to them as semi-additive facts.\nThe semi-additive nature of inventory balance facts is even more understand-\nable if you think about your checking account balances. On Monday, presume \nthat you have $50 in your account. On Tuesday, the balance remains unchanged. \nOn Wednesday, you deposit another $50 so the balance is now $100. The account \nhas no further activity through the end of the week. On Friday, you can’t merely \nadd up the daily balances during the week and declare that the ending balance is \n$400 (based on $50 + $50 + $100 + $100 + $100). The most useful way to combine \n\n\nInventory 115\naccount balances and inventory levels across dates is to average them (resulting in \nan $80 average balance in the checking example). You are probably familiar with \nyour bank referring to the average daily balance on a monthly account summary.\nNOTE \nAll measures that record a static level (inventory levels, ﬁ nancial account \nbalances, and measures of intensity such as room temperatures) are inherently \nnon-additive across the date dimension and possibly other dimensions. In these \ncases, the measure may be aggregated across dates by averaging over the number \nof time periods.\nUnfortunately, you cannot use the SQL AVG function to calculate the average \nover time. This function averages over all the rows received by the query, not just \nthe number of dates. For example, if a query requested the average inventory for \na cluster of three products in four stores across seven dates (e.g., the average daily \ninventory of a brand in a geographic region during a week), the SQL AVG function \nwould divide the summed inventory value by 84 (3 products × 4 stores × 7 dates). \nObviously, the correct answer is to divide the summed inventory value by 7, which \nis the number of daily time periods.\nOLAP products provide the capability to deﬁ ne aggregation rules within the \ncube, so semi-additive measures like balances are less problematic if the data is \ndeployed via OLAP cubes.\nEnhanced Inventory Facts\nThe  simplistic view in the periodic inventory snapshot fact table enables you to see \na time series of inventory levels. For most inventory analysis, quantity on hand isn’t \nenough. Quantity on hand needs to be used in conjunction with additional facts to \nmeasure the velocity of inventory movement and develop other interesting metrics \nsuch as the number of turns and number of days’ supply.\nIf quantity sold (or equivalently, quantity shipped for a warehouse location) was \nadded to each fact row, you could calculate the number of turns and days’ supply. \nFor daily inventory snapshots, the number of turns measured each day is calculated \nas the quantity sold divided by the quantity on hand. For an extended time span, \nsuch as a year, the number of turns is the total quantity sold divided by the daily \naverage quantity on hand. The number of days’ supply is a similar calculation. Over \na time span, the number of days’ supply is the ﬁ nal quantity on hand divided by \nthe average quantity sold.\nIn addition to the quantity sold, inventory analysts are also interested in the \nextended value of the inventory at cost, as well as the value at the latest selling price. \nThe initial periodic snapshot is embellished in Figure 4-3.\n\n\nChapter 4\n116\nDate Dimension\nStore Inventory Snapshot Fact\nDate Key (PK)\n...\nDate Key (FK)\nProduct Key (FK)\nStore Key (FK)\nQuantity on Hand\nQuantity Sold\nInventory Dollar Value at Cost\nInventory Dollar Value at Latest Selling Price\nProduct Key (PK)\n...\nStore Key (PK)\n...\nProduct Dimension\nStore Dimension\nFigure 4-3: Enhanced inventory periodic snapshot.\nNotice that quantity on hand is semi-additive, but the other measures in the \nenhanced periodic snapshot are all fully additive. The quantity sold amount has been \nrolled up to the snapshot’s daily granularity. The valuation columns are extended, \nadditive amounts. In some periodic snapshot inventory schemas, it is useful to \nstore the beginning balance, the inventory change or delta, along with the ending \nbalance. In this scenario, the balances are again semi-additive, whereas the deltas \nare fully additive across all the dimensions.\nThe periodic snapshot is the most common inventory schema. We’ll brieﬂ y dis-\ncuss two alternative perspectives that complement the inventory snapshot just \ndesigned. For a change of pace, rather than describing these models in the context \nof the retail store inventory, we’ll move up the value chain to discuss the inventory \nlocated in the warehouses.\n Inventory Transactions\nA  second way to model an inventory business process is to record every transac-\ntion that aff ects inventory. Inventory transactions at the warehouse might include \nthe following:\n \n■Receive product.\n \n■Place product into inspection hold.\n \n■Release product from inspection hold.\n \n■Return product to vendor due to inspection failure.\n \n■Place product in bin.\n \n■Pick product from bin.\n \n■Package product for shipment.\n \n■Ship product to customer.\n \n■Receive product from customer.\n \n■Return product to inventory from customer return.\n \n■Remove product from inventory.\n\n\nInventory 117\nEach inventory transaction identiﬁ es the date, product, warehouse, vendor, trans-\naction type, and in most cases, a single amount representing the inventory quantity \nimpact caused by the transaction. Assuming the granularity of the fact table is one \nrow per inventory transaction, the resulting schema is illustrated in Figure 4-4.\nDate Dimension\nWarehouse Inventory Transaction Fact\nDate Key (FK)\nProduct Key (FK)\nWarehouse Key (FK)\nInventory Transaction Type Key (FK)\nInventory Transaction Number (DD)\nInventory Transaction Dollar Amount\nInventory Transaction Type Key (PK)\nInventory Transaction Type Description\nInventory Transaction Type Group\nWarehouse Key (PK)\nWarehouse Number (NK)\nWarehouse Name\nWarehouse Address\nWarehouse City\nWarehouse City-State\nWarehouse State\nWarehouse ZIP\nWarehouse Zone\nWarehouse Total Square Footage\n...\nProduct Dimension\nInventory Transaction Type Dimension\nWarehouse Dimension\nFigure 4-4: Warehouse inventory transaction model.\nEven though the transaction fact table is simple, it contains detailed information \nthat mirrors individual inventory manipulations. The transaction fact table is use-\nful for measuring the frequency and timing of speciﬁ c transaction types to answer \nquestions that couldn’t be answered by the less granular periodic snapshot.\nEven so, it is impractical to use the transaction fact table as the sole basis for ana-\nlyzing inventory performance. Although it is theoretically possible to reconstruct the \nexact inventory position at any moment in time by rolling all possible transactions \nforward from a known inventory position, it is too cumbersome and impractical \nfor broad analytic questions that span dates, products, warehouses, or vendors.\nNOTE \nRemember there’s more to life than transactions alone. Some form of a \nsnapshot table to give a more cumulative view of a process often complements \na transaction fact table. \nBefore leaving the transaction fact table, our example presumes each type of \ntransaction impacting inventory levels positively or negatively has consistent dimen-\nsionality: date, product, warehouse, vendor, and transaction type. We recognize \nsome transaction types may have varied dimensionality in the real world. For \nexample, a shipper may be associated with the warehouse receipts and shipments; \ncustomer information is likely associated with shipments and customer returns. If the \n\n\nChapter 4\n118\ntransactions’ dimensionality varies by event, then a series of related fact tables should \nbe designed rather than capturing all inventory transactions in a single fact table.\nNOTE \nIf performance measurements have different natural granularity or \ndimensionality, they likely result from separate processes that should be modeled \nas separate fact tables.\n Inventory Accumulating Snapshot\nThe  ﬁ nal inventory model is the accumulating snapshot. Accumulating snapshot \nfact tables are used for processes that have a deﬁ nite beginning, deﬁ nite end, and \nidentiﬁ able milestones in between. In this inventory model, one row is placed in the \nfact table when a particular product is received at the warehouse. The disposition \nof the product is tracked on this single fact row until it leaves the warehouse. In \nthis example, the accumulating snapshot model is only possible if you can reliably \ndistinguish products received in one shipment from those received at a later time; \nit is also appropriate if you track product movement by product serial number or \nlot number.\nNow assume that inventory levels for a product lot captured a series of well-\ndeﬁ ned events or milestones as it moves through the warehouse, such as receiving, \ninspection, bin placement, and shipping. As illustrated in Figure 4-5, the inventory \naccumulating snapshot fact table with its multitude of dates and facts looks quite \ndiff erent from the transaction or periodic snapshot schemas.\nInventory Receipt Accumulating Fact\nProduct Lot Receipt Number (DD)\nDate Received Key (FK)\nDate Inspected Key (FK)\nDate Bin Placement Key (FK)\nDate Initial Shipment Key (FK)\nDate Last Shipment Key (FK)\nProduct Key (FK)\nWarehouse Key (FK)\nVendor Key (FK)\nQuantity Received\nQuantity Inspected\nQuantity Returned to Vendor\nQuantity Placed in Bin\nQuantity Shipped to Customer\nQuantity Returned by Customer\nQuantity Returned to Inventory\nQuantity Damaged\nReceipt to Inspected Lag\nReceipt to Bin Placement Lag\nReceipt to Initial Shipment Lag\nInitial to Last Shipment Lag\nDate Received Dimension\nProduct Dimension\nWarehouse Dimension\nVendor Dimension\nDate Inspected Dimension\nDate Bin Placement Dimension\nDate Initial Shipment Dimension\nDate Last Shipment Dimension\nFigure 4-5: Warehouse inventory accumulating snapshot.\n\n\nInventory 119\nThe accumulating snapshot fact table provides an updated status of the lot as it \nmoves through standard milestones represented by multiple date-valued foreign \nkeys. Each accumulating snapshot fact table row is updated repeatedly until the \nproducts received in a lot are completely depleted from the warehouse, as shown \nin Figure 4-6.\nDate Inspected\nKey\nReceipt to\nInspected Lag\nReceipt to Bin\nPlacement Lag\nDate Bin\nPlacement Key\nProduct\nKey\nQuantity\nReceived\nLot Receipt\nNumber\nDate Received\nKey\n101\n20130101\n0\n0\n1\n100\nFact row inserted when lot received:\nDate Inspected\nKey\nReceipt to\nInspected Lag\nReceipt to Bin\nPlacement Lag\nDate Bin\nPlacement Key\nProduct\nKey\nQuantity\nReceived\nLot Receipt\nNumber\nDate Received\nKey\n101\n20130101\n20130103\n0\n1\n100\n2\nFact row updated when lot inspected:\nDate Inspected\nKey\nReceipt to\nInspected Lag\nReceipt to Bin\nPlacement Lag\nDate Bin\nPlacement Key\nProduct\nKey\nQuantity\nReceived\nLot Receipt\nNumber\nDate Received\nKey\n101\n20130101\n20130103\n20130104\n1\n100\n2\n3\nFact row updated when lot placed in bin:\nFigure 4-6: Evolution of an accumulating snapshot fact row.\nFact Table Types\nThere are just three fundamental types of fact tables: transaction, periodic snapshot, \nand accumulating snapshot. Amazingly, this simple pattern holds true regardless \nof the industry. All three types serve a useful purpose; you often need two comple-\nmentary fact tables to get a complete picture of the business, yet the administration \nand rhythm of the three fact tables are quite diff erent. Figure 4-7 compares and \ncontrasts the variations.\nTransaction\nDiscrete transaction point\nin time\nRecurring snapshots at\nregular, predictable intervals\n1 row per transaction or\ntransaction line \n1 row per snapshot period\nplus other dimensions\nSnapshot date\nCumulative performance\nfor time interval\nPredictably dense\nTransaction date\nTransaction performance\nSparse or dense, depending\non activity\nNo updates, unless error\ncorrection\nNo updates, unless error\ncorrection\nIndeterminate time span for\nevolving pipeline/workflow\n1 row per pipeline\noccurrence\nMultiple dates for pipeline’s\nkey milestones\nPerformance for pipeline\noccurrence\nSparse or dense, depending\non pipeline occurrence\nUpdated whenever pipeline\nactivity occurs\nPeriodicity\nGrain\nDate dimension(s)\nFacts\nFact table sparsity\nFact table updates\nPeriodic Snapshot\nAccumulating Snapshot\nFigure 4-7: Fact table type comparisons.\n\n\nChapter 4\n120\nTransaction Fact Tables\nThe  most fundamental view of the business’s operations is at the individual transac-\ntion or transaction line level. These fact tables represent an event that occurred at \nan instantaneous point in time. A row exists in the fact table for a given customer \nor product only if a transaction event occurred. Conversely, a given customer or \nproduct likely is linked to multiple rows in the fact table because hopefully the \ncustomer or product is involved in more than one transaction.\nTransaction data ﬁ ts easily into a dimensional framework. Atomic transaction \ndata is the most naturally dimensional data, enabling you to analyze behavior in \nextreme detail. After a transaction has been posted in the fact table, you typically \ndon’t revisit it.\nHaving made a solid case for the charm of transaction detail, you may be think-\ning that all you need is a big, fast server to handle the gory transaction minutiae, \nand your job is over. Unfortunately, even with transaction level data, there are busi-\nness questions that are impractical to answer using only these details. As indicated \nearlier, you cannot survive on transactions  alone.\nPeriodic Snapshot Fact Tables\nPeriodic  snapshots are needed to see the cumulative performance of the business \nat regular, predictable time intervals. Unlike the transaction fact table where a row \nis loaded for each event occurrence, with the periodic snapshot, you take a picture \n(hence the snapshot terminology) of the activity at the end of a day, week, or month, \nthen another picture at the end of the next period, and so on. The periodic snap-\nshots are stacked consecutively into the fact table. The periodic snapshot fact table \noften is the only place to easily retrieve a regular, predictable view of longitudinal \nperformance trends.\nWhen transactions equate to little pieces of revenue, you can move easily from \nindividual transactions to a daily snapshot merely by adding up the transactions. \nIn this situation, the periodic snapshot represents an aggregation of the transac-\ntional activity that occurred during a time period; you would build the snapshot \nonly if needed for performance reasons. The design of the snapshot table is closely \nrelated to the design of its companion transaction table in this case. The fact tables \nshare many dimension tables; the snapshot usually has fewer dimensions overall. \nConversely, there are usually more facts in a summarized periodic snapshot table \nthan in a transactional table because any activity that happens during the period \nis fair game for a metric in a periodic snapshot.\nIn many businesses, however, transaction details are not easily summarized to \npresent management performance metrics. As you saw in this inventory case study, \n\n\nInventory 121\ncrawling through the transactions would be extremely time-consuming, plus the \nlogic required to interpret the eff ect of diff erent kinds of transactions on inventory \nlevels could be horrendously complicated, presuming you even have access to the \nrequired historical data. The periodic snapshot again comes to the rescue to provide \nmanagement with a quick, ﬂ exible view of inventory levels. Hopefully, the data for \nthis snapshot schema is sourced directly from an operational system that handles \nthese complex calculations. If not, the ETL system must also implement this com-\nplex logic to correctly interpret the impact of each transaction type.\nAccumulating Snapshot Fact Tables\nLast,  but not least, the third type of fact table is the accumulating snapshot. Although \nperhaps not as common as the other two fact table types, accumulating snapshots \ncan be very insightful. Accumulating snapshots represent processes that have a \ndeﬁ nite beginning and deﬁ nite end together with a standard set of intermediate \nprocess steps. Accumulating snapshots are most appropriate when business users \nwant to perform workﬂ ow or pipeline analysis.\nAccumulating snapshots always have multiple date foreign keys, representing the \npredictable major events or process milestones; sometimes there’s an additional date \ncolumn that indicates when the snapshot row was last updated. As we’ll discuss in \nChapter 6: Order Management, these dates are each handled by a role-playing date \ndimension. Because most of these dates are not known when the fact row is ﬁ rst \nloaded, a default surrogate date key is used for the undeﬁ ned  dates.\nLags Between Milestones and Milestone Counts\nBecause  accumulating snapshots often represent the effi  ciency and elapsed time of \na workﬂ ow or pipeline, the fact table typically contains metrics representing the \ndurations or lags between key milestones. It would be diffi  cult to answer duration \nquestions using a transaction fact table because you would need to correlate rows \nto calculate time lapses. Sometimes the lag metrics are simply the raw diff erence \nbetween the milestone dates or date/time stamps. In other situations, the lag calcula-\ntion is made more complicated by taking workdays and holidays into consideration.\nAccumulating snapshot fact tables sometimes include milestone completion coun-\nters, valued as either 0 or 1. Finally, accumulating snapshots often have a foreign \nkey to a status dimension, which is updated to reﬂ ect the pipeline’s latest status.\nAccumulating Snapshot Updates and OLAP Cubes\nIn  sharp contrast to the other fact table types, you purposely revisit accumulating \nsnapshot fact table rows to update them. Unlike the periodic snapshot where the \nprior snapshots are preserved, the accumulating snapshot merely reﬂ ects the most \n\n\nChapter 4\n122\ncurrent status and metrics. Accumulating snapshots do not attempt to accommodate \ncomplex scenarios that occur infrequently. The analysis of these outliers can always \nbe done with the transaction fact table.\nIt is worth noting that accumulating snapshots are typically problematic for \nOLAP cubes. Because updates to an accumulating snapshot force both facts and \ndimension foreign keys to change, much of the cube would need to be reprocessed \nwith updates to these snapshots, unless the fact row is only loaded once the pipeline \noccurrence is complete.\nComplementary Fact Table Types\nSometimes accumulating and periodic snapshots work in conjunction with one \nanother, such as when you incrementally build the monthly snapshot by adding the \neff ect of each day’s transactions to a rolling accumulating snapshot while also storing \n36 months of historical data in a periodic snapshot. Ideally, when the last day of the \nmonth has been reached, the accumulating snapshot simply becomes the new regular \nmonth in the time series, and a new accumulating snapshot is started the next day.\nTransactions and snapshots are the yin and yang of dimensional designs. Used \ntogether, companion transaction and snapshot fact tables provide a complete view \nof the business. Both are needed because there is often no simple way to combine \nthese two contrasting perspectives in a single fact table. Although there is some \ntheoretical data redundancy between transaction and snapshot tables, you don’t \nobject to such redundancy because as DW/BI publishers, your mission is to publish \ndata so that the organization can eff ectively analyze it. These separate types of fact \ntables each provide diff erent vantage points on the same story. Amazingly, these \nthree types of fact tables turn out to be all the fact table types needed for the use \ncases described in this  book.\nValue Chain Integration\nNow  that we’ve completed the design of three inventory models, let’s revisit our ear-\nlier discussion about the retailer’s value chain. Both business and IT organizations \nare typically interested in value chain integration. Business management needs to \nlook across the business’s processes to better evaluate performance. For example, \nnumerous DW/BI projects have focused on better understanding customer behavior \nfrom an end-to-end perspective. Obviously, this requires the ability to consistently \nlook at customer information across processes, such as quotes, orders, invoicing, \npayments, and customer service. Similarly, organizations want to analyze their \nproducts across processes, or their employees, students, vendors, and so on.\n\n\nInventory 123\nIT managers recognize integration is needed to deliver on the promises of data \nwarehousing and business intelligence. Many consider it their ﬁ duciary respon-\nsibility to manage the organization’s information assets. They know they’re not \nfulﬁ lling their responsibilities if they allow standalone, nonintegrated databases \nto proliferate. In addition to addressing the business’s needs, IT also beneﬁ ts from \nintegration because it allows the organization to better leverage scarce resources \nand gain effi  ciencies through the use of reusable components.\nFortunately, the senior managers who typically are most interested in integration \nalso have the necessary organizational inﬂ uence and economic willpower to make \nit happen. If they don’t place a high value on integration, you face a much more \nserious organizational challenge, or put more bluntly, your integration project will \nprobably fail. It shouldn’t be the sole responsibility of the DW/BI manager to garner \norganizational consensus for integration across the value chain. The political sup-\nport of senior management is important; it takes the DW/BI manager off  the hook \nand places the burden on senior leadership’s shoulders where it belongs. \nIn Chapters 3 and 4, we modeled data from several processes of the retailer’s value \nchain. Although separate fact tables in separate dimensional schemas represent the \ndata from each process, the models share several common business dimensions: \ndate, product, and store. We’ve logically represented this dimension sharing in \nFigure 4-8. Using shared, common dimensions is absolutely critical to designing \ndimensional models that can be integrated.\nStore Dimension\nDate Dimension\nRetail Sales\nTransaction Facts\nRetail Inventory\nSnapshot Facts\nWarehouse Inventory\nTransaction Facts\nPromotion Dimension\nProduct Dimension\nWarehouse Dimension\nFigure 4-8: Sharing dimensions among business processes.\nEnterprise Data Warehouse Bus Architecture\nObviously,  building the enterprise’s DW/BI system in one galactic eff ort is too daunt-\ning, yet building it as isolated pieces defeats the overriding goal of consistency. For \nlong-term DW/BI success, you need to use an architected, incremental approach to \nbuild the enterprise’s warehouse. The approach we advocate is the enterprise data \nwarehouse bus architecture.\n\n\nChapter 4\n124\nUnderstanding the Bus Architecture\nContrary  to popular belief, the word bus is not shorthand for business; it’s an old \nterm from the electrical power industry that is now used in the computer industry. \nA bus is a common structure to which everything connects and from which every-\nthing derives power. The bus in a computer is a standard interface speciﬁ cation \nthat enables you to plug in a disk drive, DVD, or any number of other specialized \ncards or devices. Because of the computer’s bus standard, these peripheral devices \nwork together and usefully coexist, even though they were manufactured at diff er-\nent times by diff erent vendors.\nNOTE \nBy deﬁ ning a standard bus interface for the DW/BI environment, separate \ndimensional models can be implemented by diff erent groups at diff erent times. \nThe separate business process subject areas plug together and usefully coexist if \nthey adhere to the standard.\nIf you refer back to the value chain diagram in Figure 4-1, you can envision many \nbusiness processes plugging into the enterprise data warehouse bus, as illustrated \nin Figure 4-9. Ultimately, all the processes of an organization’s value chain create \na family of dimensional models that share a comprehensive set of common, con-\nformed dimensions.\nStore Sales\nStore Inventory\nPurchase Orders\nDate\nProduct\nStore\nPromotion\nWarehouse\nVendor\nShipper\nFigure 4-9: Enterprise data warehouse bus with shared dimensions.\nThe enterprise data warehouse bus architecture provides a rational approach to \ndecomposing the enterprise DW/BI planning task. The master suite of standard-\nized dimensions and facts has a uniform interpretation across the enterprise. This \nestablishes the data architecture framework. You can then tackle the implementation \nof separate process-centric dimensional models, with each implementation closely \n\n\nInventory 125\nadhering to the architecture. As the separate dimensional models become available, \nthey ﬁ t together like the pieces of a puzzle. At some point, enough dimensional models \nexist to make good on the promise of an integrated enterprise DW/BI environment.\nThe bus architecture enables DW/BI managers to get the best of both worlds. \nThey have an architectural framework guiding the overall design, but the problem \nhas been divided into bite-sized business process chunks that can be implemented \nin realistic time frames. Separate development teams follow the architecture while \nworking fairly independently and asynchronously.\nThe bus architecture is independent of technology and database platforms. All \nﬂ avors of relational and OLAP-based dimensional models can be full participants \nin the enterprise data warehouse bus if they are designed around conformed dimen-\nsions and facts. DW/BI systems inevitably consist of separate machines with diff erent \noperating systems and database management systems. Designed coherently, they \nshare a common architecture of conformed dimensions and facts, allowing them \nto be fused into an integrated whole.\n Enterprise Data Warehouse Bus Matrix\nWe  recommend using an enterprise data warehouse bus matrix to document and com-\nmunicate the bus architecture, as illustrated in Figure 4-10. Others have renamed the \nbus matrix, such as the conformance or event matrix, but these are merely synonyms \nfor this fundamental Kimball concept ﬁ rst introduced in the 1990s.\nIssue Purchase Orders\nReceive Warehouse Deliveries\nWarehouse Inventory\nReceive Store Deliveries\nStore Inventory\nRetail Sales\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nRetail Sales Forecast\nX\nX\nRetail Promotion Tracking\nX\nX\nX\nX\nX\nCustomer Returns\nX\nReturns to Vendor\nX\nX\nX\nX\nX\nX\nFrequent Shopper Sign-Ups\nBUSINESS PROCESSES\nCOMMON DIMENSIONS\nPromotion\nStore\nX\nX\nX\nX\nX\nX\nX\nX\nX\nEmployee\nCustomer\nWarehouse\nProduct\nDate\nFigure 4-10: Sample enterprise data warehouse bus matrix for a retailer.\n",
      "page_number": 140
    },
    {
      "number": 17,
      "title": "Segment 17 (pages 156-164)",
      "start_page": 156,
      "end_page": 164,
      "detection_method": "topic_boundary",
      "content": "Chapter 4\n126\nWorking in a tabular fashion, the organization’s business processes are repre-\nsented as matrix rows. It is important to remember you are identifying business \nprocesses, not the organization’s business departments. The matrix rows translate \ninto dimensional models representing the organization’s primary activities and \nevents, which are often recognizable by their operational source. When it’s time to \ntackle a DW/BI development project, start with a single business process matrix row \nbecause that minimizes the risk of signing up for an overly ambitious implementa-\ntion. Most implementation risk comes from biting off  too much ETL system design \nand development. Focusing on the results of a single process, often captured by a \nsingle underlying source system, reduces the ETL development risk.\nAfter individual business processes are enumerated, you sometimes identify more \ncomplex consolidated processes. Although dimensional models that cross processes \ncan be immensely beneﬁ cial in terms of both query performance and ease of use, \nthey are typically more diffi  cult to implement because the ETL eff ort grows with \neach additional major source integrated into a single dimensional model. It is pru-\ndent to focus on the individual processes as building blocks before tackling the task \nof consolidating. Proﬁ tability is a classic example of a consolidated process in which \nseparate revenue and cost factors are combined from diff erent processes to provide a \ncomplete view of proﬁ tability. Although a granular proﬁ tability dimensional model \nis exciting, it is deﬁ nitely not the ﬁ rst dimensional model you should attempt to \nimplement; you could easily drown while trying to wrangle all the revenue and \ncost components.\nThe columns of the bus matrix represent the common dimensions used across \nthe enterprise. It is often helpful to create a list of core dimensions before ﬁ lling \nin the matrix to assess whether a given dimension should be associated with a busi-\nness process. The number of bus matrix rows and columns varies by organization. \nFor many, the matrix is surprisingly square with approximately 25 to 50 rows and \na comparable number of columns. In other industries, like insurance, there tend to \nbe more columns than rows.\nAfter the core processes and dimensions are identiﬁ ed, you shade or “X” the \nmatrix cells to indicate which columns are related to each row. Presto! You can \nimmediately see the logical relationships and interplay between the organization’s \nconformed dimensions and key business  processes.\nMultiple Matrix Uses\nCreating  the enterprise data warehouse bus matrix is one of the most important \nDW/BI implementation deliverables. It is a hybrid resource that serves multiple \npurposes, including architecture planning, database design, data governance \ncoordination, project estimating, and organizational communication.\n\n\nInventory 127\nAlthough it is relatively straightforward to lay out the rows and columns, the \nenterprise bus matrix deﬁ nes the overall data architecture for the DW/BI system. \nThe matrix delivers the big picture perspective, regardless of database or technol-\nogy preferences.\nThe matrix’s columns address the demands of master data management and \ndata integration head-on. As core dimensions participating in multiple dimensional \nmodels are deﬁ ned by folks with data governance responsibilities and built by the \nDW/BI team, you can envision their use across processes rather than designing in \na vacuum based on the needs of a single process, or even worse, a single depart-\nment. Shared dimensions supply potent integration glue, allowing the business to \ndrill across processes.\nEach business process-centric implementation project incrementally builds out \nthe overall architecture. Multiple development teams can work on components \nof the matrix independently and asynchronously, with conﬁ dence they’ll ﬁ t together. \nProject managers can look across the process rows to see the dimensionality of \neach dimensional model at a glance. This vantage point is useful as they’re gauging \nthe magnitude of the project’s eff ort. A project focused on a business process with \nfewer dimensions usually requires less eff ort, especially if the politically charged \ndimensions are already sitting on the shelf.\nThe matrix enables you to communicate effectively within and across data \ngovernance and DW/BI teams. Even more important, you can use the matrix to \ncommunicate upward and outward throughout the organization. The matrix is a \nsuccinct deliverable that visually conveys the master plan. IT management needs \nto understand this perspective to coordinate across project teams and resist the \norganizational urge to deploy more departmental solutions quickly. IT management \nmust also ensure that distributed DW/BI development teams are committed to the \nbus architecture. Business management needs to also appreciate the holistic plan; \nyou want them to understand the staging of the DW/BI rollout by business process. \nIn addition, the matrix illustrates the importance of identifying experts from the \nbusiness to serve as data governance leaders for the common dimensions. It is a \ntribute to its simplicity that the matrix can be used eff ectively to communicate \nwith developers, architects, modelers, and project managers, as well as senior IT \nand business management.\n Opportunity/Stakeholder Matrix\nYou can  draft a diff erent matrix that leverages the same business process rows, \nbut replaces the dimension columns with business functions, such as merchandis-\ning, marketing, store operations, and ﬁ nance. Based on each function’s requirements, \nthe matrix cells are shaded to indicate which business functions are interested in \n\n\nChapter 4\n128\nwhich business processes (and projects), as illustrated in Figure 4-11’s opportunity/\nstakeholder matrix variation. It also identiﬁ es which groups need to be invited to \nthe detailed requirements, dimensional modeling, and BI application speciﬁ cation \nparties after a process-centric row is queued up as a project.\nIssue Purchase Orders\nReceive Warehouse Deliveries\nWarehouse Inventory\nReceive Store Deliveries\nStore Inventory\nRetail Sales\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nRetail Sales Forecast\nX\nX\nX\nX\nRetail Promotion Tracking\nX\nX\nX\nX\nX\nX\nCustomer Returns\nX\nX\nReturns to Vendor\nX\nX\nX\nX\nFrequent Shopper Sign-Ups\nX\nX\nX\nBUSINESS PROCESSES\nSTAKEHOLDERS\nFinance\nLogistics\nStore Operations\nMarketing\nMerchandising\nFigure 4-11: Opportunity/stakeholder matrix.\nCommon Bus Matrix Mistakes\nWhen  drafting a bus matrix, people sometimes struggle with the level of detail \nexpressed by each row, resulting in the following missteps:\n \n■Departmental  or overly encompassing rows. The matrix rows shouldn’t cor-\nrespond to the boxes on a corporate organization chart representing functional \ngroups. Some departments may be responsible or acutely interested in a single \nbusiness process, but the matrix rows shouldn’t look like a list of the CEO’s \ndirect reports.\n \n■Report-centric  or too narrowly deﬁ ned rows. At the opposite extreme, the \nbus matrix shouldn’t resemble a laundry list of requested reports. A single \nbusiness process supports numerous analyses; the matrix row should refer-\nence the business process, not the derivative reports or analytics.\nWhen deﬁ ning the matrix columns, architects naturally fall into the similar traps \nof deﬁ ning columns that are either too broad or too narrow:\n\n\nInventory 129\n \n■Overly  generalized columns. A “person” column on the bus matrix may refer \nto a wide variety of people, from internal employees to external suppliers \nand customer contacts. Because there’s virtually zero overlap between these \npopulations, it adds confusion to lump them into a single, generic dimension. \nSimilarly, it’s not beneﬁ cial to put internal and external addresses referring \nto corporate facilities, employee addresses, and customer sites into a generic \nlocation column in the matrix.\n \n■Separate  columns for each level of a hierarchy. The columns of the bus \nmatrix should refer to dimensions at their most granular level. Some \nbusiness process rows may require an aggregated version of the detailed \ndimension, such as inventory snapshot metrics at the weekly level. Rather \nthan creating separate matrix columns for each level of the calendar hierarchy, \nuse a single column for dates. To express levels of detail above a daily grain, \nyou can denote the granularity within the matrix cell; alternatively, you can \nsubdivide the date column to indicate the hierarchical level associated with \neach business process row. It’s important to retain the overarching identiﬁ ca-\ntion of common dimensions deployed at diff erent levels of granularity. Some \nindustry pundits advocate matrices that treat every dimension table attribute \nas a separate, independent column; this defeats the concept of dimensions \nand results in a completely unruly matrix.\nRetroﬁ tting Existing Models to a Bus Matrix\nIt  is unacceptable to build separate dimensional models that ignore a framework \ntying them together. Isolated, independent dimensional models are worse than \nsimply a lost opportunity for analysis. They deliver access to irreconcilable views \nof the organization and further enshrine the reports that cannot be compared with \none another. Independent dimensional models become legacy implementations \nin their own right; by their existence, they block the development of a coherent \nDW/BI environment.\nSo what happens if you’re not starting with a blank slate? Perhaps several dimen-\nsional models have been constructed without regard to an architecture using \nconformed dimensions. Can you rescue your stovepipes and convert them to the \nbus architecture? To answer this question, you should start ﬁ rst with an honest \nappraisal of your existing non-integrated dimensional structures. This typically \nentails meetings with the separate teams (including the clandestine pseudo IT \nteams within business organizations) to determine the gap between the current \nenvironment and the organization’s architected goal. When the gap is understood, \nyou need to develop an incremental plan to convert the standalone dimensional \nmodels to the enterprise architecture. The plan needs to be internally sold. Senior \nIT and business management must understand the current state of data chaos, the \n\n\nChapter 4\n130\nrisks of doing nothing, and the beneﬁ ts of moving forward according to your game \nplan. Management also needs to appreciate that the conversion will require a sig-\nniﬁ cant commitment of support, resources, and funding.\nIf an existing dimensional model is based on a sound dimensional design, per-\nhaps you can map an existing dimension to a standardized version. The original \ndimension table would be rebuilt using a cross-reference map. Likewise, the fact \ntable would need to be reprocessed to replace the original dimension keys with the \nconformed dimension keys. Of course, if the original and conformed dimension \ntables contain diff erent attributes, rework of the preexisting BI applications and \nqueries is inevitable.\nMore typically, existing dimensional models are riddled with dimensional model-\ning errors beyond the lack of adherence to standardized dimensions. In some cases, \nthe stovepipe dimensional model has outlived its useful life. Isolated dimensional \nmodels often are built for a speciﬁ c functional area. When others try to leverage \nthe data, they typically discover that the dimensional model was implemented at \nan inappropriate level of granularity and is missing key dimensionality. The eff ort \nrequired to retroﬁ t these dimensional models into the enterprise DW/BI architec-\nture may exceed the eff ort to start over from scratch. As diffi  cult as it is to admit, \nstovepipe dimensional models often have to be shut down and rebuilt in the proper \nbus architecture framework.\n Conformed Dimensions\nNow  that you understand the importance of the enterprise bus architecture, let’s fur-\nther explore the standardized conformed dimensions that serve as the cornerstone \nof the bus because they’re shared across business process fact tables. Conformed \ndimensions go by many other aliases: common dimensions, master dimensions, ref-\nerence dimensions, and shared dimensions. Conformed dimensions should be built \nonce in the ETL system and then replicated either logically or physically throughout \nthe enterprise DW/BI environment. When built, it’s extremely important that the \nDW/BI development teams take the pledge to use these dimensions. It’s a policy \ndecision that is critical to making the enterprise DW/BI system function; their usage \nshould be mandated by the organization’s CIO.\n Drilling Across Fact Tables\nIn  addition to consistency and reusability, conformed dimensions enable you to com-\nbine performance measurements from diff erent business processes in a single report, \nas illustrated in Figure 4-12. You can use multipass SQL to query each dimensional \n\n\nInventory 131\nmodel separately and then outer-join the query results based on a common dimen-\nsion attribute, such as Figure 4-12’s product name. The full outer-join ensures all \nrows are included in the combined report, even if they only appear in one set of \nquery results. This linkage, often referred to as drill across, is straightforward if the \ndimension table attribute values are identical.\nProduct Description\nBaked Well Sourdough\nFluffy Light Sliced White\nFluffy Sliced Whole Wheat\n1,201\n1,472\n846\n935\n801\n513\n1,042\n922\n368\nOpen Orders Qty\nInventory Qty\nSales Qty\nFigure 4-12: Drilling across fact tables with conformed dimension attributes.\nDrilling across is supported by many BI products and platforms. Their implemen-\ntations diff er on whether the results are joined in temporary tables, the application \nserver, or the report. The vendors also use diff erent terms to describe this technique, \nincluding multipass, multi-select, multi-fact, or stitch queries. Because metrics from \ndiff erent fact tables are brought together with a drill-across query, often any cross-\nfact calculations must be done in the BI application after the separate conformed \nresults have been returned.\nConformed dimensions come in several diff erent ﬂ avors, as described in the \nfollowing sections.\nIdentical Conformed Dimensions\nAt  the most basic level, conformed dimensions mean the same thing with every pos-\nsible fact table to which they are joined. The date dimension table connected to the \nsales facts is identical to the date dimension table connected to the inventory facts. \nIdentical conformed dimensions have consistent dimension keys, attribute column \nnames, attribute deﬁ nitions, and attribute values (which translate into consistent \nreport labels and groupings). Dimension attributes don’t conform if they’re called \nMonth in one dimension and Month Name in another; likewise, they don’t conform \nif the attribute value is “July” in one dimension and “JULY” in another. Identical \nconformed dimensions in two dimensional models may be the same physical table \nwithin the database. However, given the typical complexity of the DW/BI system’s \ntechnical environment with multiple database platforms, it is more likely that the \ndimension is built once in the ETL system and then duplicated synchronously out-\nward to each dimensional model. In either case, the conformed date dimensions in \nboth dimensional models have the same number of rows, same key values, same \nattribute labels, same attribute data deﬁ nitions, and same attribute values. Attribute \ncolumn names should be uniquely labeled across dimensions.\n\n\nChapter 4\n132\nMost  conformed dimensions are deﬁ ned naturally at the most granular level \npossible. The product dimension’s grain will be the individual product; the date \ndimension’s grain will be the individual day. However, sometimes dimensions at the \nsame level of granularity do not fully conform. For example, there might be product \nand store attributes needed for inventory analysis, but they aren’t appropriate for \nanalyzing retail sales data. The dimension tables still conform if the keys and com-\nmon columns are identical, but the supplemental attributes used by the inventory \nschema are not conformed. It is physically impossible to drill across processes using \nthese add-on attributes.\n Shrunken Rollup Conformed Dimension \nwith Attribute Subset\nDimensions  also conform when they contain a subset of attributes from a more \ngranular dimension. Shrunken rollup dimensions are required when a fact table \ncaptures performance metrics at a higher level of granularity than the atomic \nbase dimension. This would be the case if you had a weekly inventory snapshot in \naddition to the daily snapshot. In other situations, facts are generated by another \nbusiness process at a higher level of granularity. For example, the retail sales pro-\ncess captures data at the atomic product level, whereas forecasting generates data \nat the brand level. You couldn’t share a single product dimension table across the \ntwo business process schemas because the granularity is diff erent. The product \nand brand dimensions still conform if the brand table attributes are a strict subset \nof the atomic product table’s attributes. Attributes that are common to both the \ndetailed and rolled-up dimension tables, such as the brand and category descrip-\ntions, should be labeled, deﬁ ned, and identically valued in both tables, as illustrated \nin Figure 4-13. However, the primary keys of the detailed and rollup dimension \ntables are separate.\nNOTE \nShrunken rollup dimensions conform to the base atomic dimension if \nthe attributes are a strict subset of the atomic dimension’s attributes.\nShrunken Conformed Dimension with Row Subset\nAnother  case of conformed dimension subsetting occurs when two dimensions are \nat the same level of detail, but one represents only a subset of rows. For example, a \ncorporate product dimension contains rows for the full portfolio of products across \nmultiple disparate lines of business, as illustrated in Figure 4-14. Analysts in the \n\n\nInventory 133\nseparate businesses may want to view only their subset of the corporate dimension, \nrestricted to the product rows for their business. By using a subset of rows, they \naren’t encumbered with the corporation’s entire product set. Of course, the fact table \njoined to this subsetted dimension must be limited to the same subset of products. \nIf a user attempts to use a shrunken subset dimension while accessing a fact table \nconsisting of the complete product set, they may encounter unexpected query results \nbecause referential integrity would be violated. You need to be cognizant of the \npotential opportunity for user confusion or error with dimension row subsetting. \nWe will further elaborate on dimension subsets when we discuss supertype and \nsubtype dimensions in Chapter 10: Financial Services.\nProduct Key (PK)\nProduct Description\nSKU Number (Natural Key)\nBrand Description\nSubcategory Description\nCategory Description\nDepartment Description\nPackage Type Description\nPackage Size\nFat Content Description\nDiet Type Description\nWeight\nWeight Units of Measure\n...\nBrand Key (PK)\nBrand Description\nSubcategory Description\nCategory Description\nDepartment Description\nMonth Key (PK)\nCalendar Month Name\nCalendar Month Number\nCalendar YYYY-MM\nCalendar Year\nDate Key (PK)\nDate\nFull Date Description\nDay of Week\nDay Number in Month\nCalendar Month Name\nCalendar Month Number\nCalendar YYYY-MM\nCalendar Year\nFiscal Week\nFiscal Month\n...\nProduct Dimension\nDate Dimension\nBrand Dimension\nMonth Dimension\nConforms\nConforms\nFigure 4-13: Conforming shrunken rollup dimensions.\n\n\nChapter 4\n134\nAppliance\nProducts\nApparel\nProducts\nCorporate\nProduct Dimension\nDrilling across requires common conformed attributes.\nFigure 4-14: Conforming dimension subsets at the same granularity.\nConformed date and month dimensions are a unique example of both row \nand column dimension subsetting. Obviously, you can’t simply use the same date \ndimension table for daily and monthly fact tables because of the diff erence in rollup \ngranularity. However, the month dimension may consist of the month-end daily \ndate table rows with the exclusion of all columns that don’t apply at the monthly \ngranularity, such as the weekday/weekend indicator, week ending date, holiday \nindicator, day number within year, and others. Sometimes a month-end indicator on \nthe daily date dimension is used to facilitate creation of this month dimension  table. \nShrunken Conformed Dimensions on the Bus Matrix\nThe  bus matrix identiﬁ es the reuse of common dimensions across business processes. \nTypically, the shaded cells of the matrix indicate that the atomic dimension is \nassociated with a given process. When shrunken rollup or subset dimensions are \ninvolved, you want to reinforce their conformance with the atomic dimensions. \nTherefore, you don’t want to create a new, unrelated column on the bus matrix. \nInstead, there are two viable approaches to represent the shrunken dimensions within \nthe matrix, as illustrated in Figure 4-15:\n \n■Mark the cell for the atomic dimension, but then textually document the \nrollup or row subset granularity within the cell.\n \n■Subdivide the dimension column to indicate the common rollup or subset \ngranularities, such as day and month if processes collect data at both of these \ngrains.\n",
      "page_number": 156
    },
    {
      "number": 18,
      "title": "Segment 18 (pages 165-174)",
      "start_page": 165,
      "end_page": 174,
      "detection_method": "topic_boundary",
      "content": "Inventory 135\nDate\nOR\nDate\nDay\nMonth\nIssue Purchase Orders\nReceive Deliveries\nInventory\nRetail Sales\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nMonth\nRetail Sales Forecast\nFigure 4-15: Alternatives for identifying shrunken dimensions on the bus matrix. \nLimited Conformity\nNow that we’ve preached about the importance of conformed dimensions, we’ll \ndiscuss the situation in which it may not be realistic or necessary to establish con-\nformed dimensions for the organization. If a conglomerate has subsidiaries spanning \nwidely varied industries, there may be little point in trying to integrate. If each line \nof business has unique customers and unique products and there’s no interest in \ncross-selling across lines, it may not make sense to attempt an enterprise archi-\ntecture because there likely isn’t much perceived business value. The willingness \nto seek a common deﬁ nition for product, customer, or other core dimensions is a \nmajor litmus test for an organization theoretically intent on building an enterprise \nDW/BI system. If the organization is unwilling to agree on common deﬁ nitions, the \norganization shouldn’t attempt to build an enterprise DW/BI environment. It would \nbe better to build separate, self-contained data warehouses for each subsidiary. But \nthen don’t complain when someone asks for “enterprise performance” without going \nthrough this logic.\nAlthough organizations may ﬁ nd it diffi  cult to combine data across disparate lines \nof business, some degree of integration is typically an ultimate goal. Rather than \nthrowing your hands in the air and declaring it can’t possibly be done, you should \nstart down the path toward conformity. Perhaps there are a handful of attributes that \ncan be conformed across lines of business. Even if it is merely a product description, \ncategory, and line of business attribute that is common to all businesses, this least-\ncommon-denominator approach is still a step in the right direction. You don’t need \nto get everyone to agree on everything related to a dimension before  proceeding.\n Importance of Data Governance and Stewardship\nWe’ve  touted the importance of conformed dimensions, but we also need to acknowl-\nedge a key challenge: reaching enterprise consensus on dimension attribute names \n\n\nChapter 4\n136\nand contents (and the handling of content changes which we’ll discuss in Chapter 5: \nProcurement). In many organizations, business rules and data deﬁ nitions have \ntraditionally been established departmentally. The consequences of this commonly \nencountered lack of data governance and control are the ubiquitous departmental \ndata silos that perpetuate similar but slightly diff erent versions of the truth. Business \nand IT management need to recognize the importance of addressing this shortfall \nif you stand any chance of bringing order to the chaos; if management is reluctant \nto drive change, the project will never achieve its goals.\nOnce the data governance issues and opportunities are acknowledged by senior \nleadership, resources need to be identiﬁ ed to spearhead the eff ort. IT is often tempted \nto try leading the charge. They are frustrated by the isolated projects re-creating \ndata around the organization, consuming countless IT and outside resources while \ndelivering inconsistent solutions that ultimately just increase the complexity of \nthe organization’s data architecture at signiﬁ cant cost. Although IT can facilitate \nthe deﬁ nition of conformed dimensions, it is seldom successful as the sole driver, \neven if it’s a temporary assignment. IT simply lacks the organizational authority to \nmake things happen. \nBusiness-Driven Governance\nTo  boost the likelihood of business acceptance, subject matter experts from the \nbusiness need to lead the initiative. Leading a cross-organizational governance \nprogram is not for the faint of heart. The governance resources identiﬁ ed by busi-\nness leadership should have the following characteristics:\n \n■Respect from the organization\n \n■Broad knowledge of the enterprise’s operations\n \n■Ability to balance organizational needs against departmental requirements\n \n■Gravitas and authority to challenge the status quo and enforce policies\n \n■Strong communication skills\n \n■Politically savvy negotiation and consensus building skills\nClearly, not everyone is cut out for the job! Typically those tapped to spearhead \nthe governance program are highly valued and in demand. It takes the right skills, \nexperience, and conﬁ dence to rationalize diverse business perspectives and drive \nthe design of common reference data, together with the necessary organizational \ncompromises. Over the years, some have criticized conformed dimensions as being \ntoo hard. Yes, it’s diffi  cult to get people in diff erent corners of the business to agree \non common attribute names, deﬁ nitions, and values, but that’s the crux of uniﬁ ed, \nintegrated data. If everyone demands their own labels and business rules, there’s \nno chance of delivering on the promises made to establish a single version of the \n\n\nInventory 137\ntruth. The data governance program is critical in facilitating a culture shift away \nfrom the typical siloed environment in which each department retains control of \ntheir data and analytics to one where information is shared and leveraged across \nthe organization.\nGovernance Objectives\nOne of the key objectives of the data governance function is to reach agreement on \ndata deﬁ nitions, labels, and domain values so that everyone is speaking the same \nlanguage. Otherwise, the same words may describe diff erent things; diff erent words \nmay describe the same thing; and the same value may have diff erent meaning. \nEstablishing common master data is often a politically charged issue; the chal-\nlenges are cultural and geopolitical rather than technical. Deﬁ ning a foundation of \nmaster descriptive conformed dimensions requires eff ort. But after it’s agreed upon, \nsubsequent DW/BI eff orts can leverage the work, both ensuring consistency and \nreducing the implementation’s delivery cycle time.\nIn addition to tackling data deﬁ nitions and contents, the data governance func-\ntion also establishes policies and responsibilities for data quality and accuracy, as \nwell as data security and access controls.\nHistorically,  DW/BI teams created the “recipes” for conformed dimensions and \nmanaged the data cleansing and integration mapping in the ETL system; the opera-\ntional systems focused on accurately capturing performance metrics, but there was \noften little eff ort to ensure consistent common reference data. Enterprise resource \nplanning (ERP) systems promised to ﬁ ll the void, but many organizations still \nrely on separate best-of-breed point solutions for niche requirements. Recently, \noperational master data management (MDM) solutions have addressed the need \nfor centralized master data at the source where the transactions are captured. \nAlthough technology can encourage data integration, it doesn’t ﬁ x the problem. \nA strong data governance function is a necessary prerequisite for conforming infor-\nmation regardless of technical  approach.\nConformed Dimensions and the Agile Movement\nSome  lament that although they want to deliver and share consistently deﬁ ned \nmaster conformed dimensions in their DW/BI environments, it’s “just not feasible.” \nThey explain they would if they could, but with senior management focused on \nusing agile development techniques, it’s “impossible” to take the time to get organi-\nzational agreement on conformed dimensions. You can turn this argument upside \ndown by challenging that conformed dimensions enable agile DW/BI development, \nalong with agile decision making.\n\n\nChapter 4\n138\nConformed dimensions allow a dimension table to be built and maintained once \nrather than re-creating slightly diff erent versions during each development cycle. \nReusing conformed dimensions across projects is where you get the leverage for \nmore agile DW/BI development. As you ﬂ esh out the portfolio of master conformed \ndimensions, the development crank starts turning faster and faster. The time-to-\nmarket for a new business process data source shrinks as developers reuse existing \nconformed dimensions. Ultimately, new ETL development focuses almost exclusively \non delivering more fact tables because the associated dimension tables are already \nsitting on the shelf ready to go.\nDeﬁ ning a conformed dimension requires organizational consensus and com-\nmitment to data stewardship. But you don’t need to get everyone to agree on every \nattribute in every dimension table. At a minimum, you should identify a subset \nof attributes that have signiﬁ cance across the enterprise. These commonly referenced \ndescriptive characteristics become the starter set of conformed attributes, enabling \ndrill-across integration. Even just a single attribute, such as enterprise product \ncategory, is a viable starting point for the integration eff ort. Over time, you can \niteratively expand from this minimalist starting point by adding attributes. These \ndimensions could be tackled during architectural agile sprints. When a series of \nsprint deliverables combine to deliver suffi  cient value, they constitute a release to \nthe business users.\nIf you fail to focus on conformed dimensions because you’re under pressure to \ndeliver something yesterday, the departmental analytic data silos will likely have \ninconsistent categorizations and labels. Even more troubling, data sets may look \nlike they can be compared and integrated due to similar labels, but the underlying \nbusiness rules may be slightly diff erent. Business users waste inordinate amounts \nof time trying to reconcile and resolve these data inconsistencies, which negatively \nimpact their ability to be agile decision makers.\nThe senior IT managers who are demanding agile systems development practices \nshould be exerting even greater organizational pressure, in conjunction with their \npeers in the business, on the development of consistent conformed dimensions if \nthey’re interested in both long-term development effi  ciencies and long-term decision-\nmaking eff ectiveness across the enterprise.\n Conformed Facts\nThus  far we have considered the central task of setting up conformed dimensions to \ntie dimensional models together. This is 95 percent or more of the data architecture \neff ort. The remaining 5 percent of the eff ort goes into establishing conformed fact \ndeﬁ nitions.\n\n\nInventory 139\nRevenue,  proﬁ t, standard prices and costs, measures of quality and customer \nsatisfaction, and other key performance indicators (KPIs) are facts that must also \nconform. If facts live in more than one dimensional model, the underlying deﬁ ni-\ntions and equations for these facts must be the same if they are to be called the \nsame thing. If they are labeled identically, they need to be deﬁ ned in the same \ndimensional context and with the same units of measure from dimensional model \nto dimensional model. For example, if several business processes report revenue, \nthen these separate revenue metrics can be added and compared only if they have \nthe same ﬁ nancial deﬁ nitions. If there are deﬁ nitional diff erences, then it is essential \nthat the revenue facts be labeled uniquely.\nNOTE \nYou must be disciplined in your data naming practices. If it is impos-\nsible to conform a fact exactly, you should give diff erent names to the diff erent \ninterpretations so that business users do not combine these incompatible facts in \ncalculations. \nSometimes a fact has a natural unit of measure in one fact table and another natu-\nral unit of measure in another fact table. For example, the ﬂ ow of product down the \nretail value chain may best be measured in shipping cases at the warehouse but in \nscanned units at the store. Even if all the dimensional considerations have been cor-\nrectly taken into account, it would be diffi  cult to use these two incompatible units of \nmeasure in one drill-across report. The usual solution to this kind of problem is to \nrefer the user to a conversion factor buried in the product dimension table and hope \nthat the user can ﬁ nd the conversion factor and correctly use it. This is unacceptable \nfor both overhead and vulnerability to error. The correct solution is to carry the fact \nin both units of measure, so a report can easily glide down the value chain, picking \noff  comparable facts. Chapter 6: Order Management talks more about multiple units \nof  measure.\nSummary\nIn this chapter we developed dimensional models for the three complementary \nviews of inventory. The periodic snapshot is a good choice for long-running, con-\ntinuously replenished inventory scenarios. The accumulating snapshot is a good \nchoice for ﬁ nite inventory pipeline situations with a deﬁ nite beginning and end. \nFinally, most inventory analysis will require a transactional schema to augment \nthese snapshot models.\nWe introduced key concepts surrounding the enterprise data warehouse bus \narchitecture and matrix. Each business process of the value chain, supported by a \n\n\nChapter 4\n140\nprimary source system, translates into a row in the bus matrix, and eventually, a \ndimensional model. The matrix rows share a surprising number of standardized, \nconformed dimensions. Developing and adhering to the enterprise bus architecture \nis an absolute must if you intend to build a DW/BI system compose d of an integrated \nset of dimensional models.\n\n\nProcurement\nW\ne explore procurement processes in this chapter. This subject area has \nobvious cross-industry appeal because it is applicable to any organization \nthat acquires products or services for either use or resale.\nIn addition to developing several purchasing models, this chapter provides \nin-depth coverage of the techniques for handling dimension table attribute value \nchanges. Although descriptive attributes in dimension tables are relatively static, \nthey are subject to change over time. Product lines are restructured, causing product \nhierarchies to change. Customers move, causing their geographic information to \nchange. We’ll describe several approaches to deal with these inevitable dimension \ntable changes. Followers of the Kimball methods will recognize the type 1, 2, and \n3 techniques. Continuing in this tradition, we’ve expanded the slowly changing \ndimension technique line-up with types 0, 4, 5, 6, and 7.\nChapter 5 discusses the following concepts:\n \n■Bus matrix snippet for procurement processes\n \n■Blended versus separate transaction schemas \n \n■Slowly changing dimension technique types 0 through 7, covering both basic \nand advanced hybrid scenarios \nProcurement Case Study\nThus far  we have studied downstream sales and inventory processes in the retailer’s \nvalue chain. We explained the importance of mapping out the enterprise data ware-\nhouse bus architecture where conformed dimensions are used across process-centric \nfact tables. In this chapter we’ll extend these concepts as we work our way further \nup the value chain to the procurement processes.\n5\n\n\nChapter 5\n142\nFor many companies, procurement is a critical business activity. Eff ective pro-\ncurement of products at the right price for resale is obviously important to retail-\ners and distributors. Procurement also has strong bottom line implications for any \norganization that buys products as raw materials for manufacturing. Signiﬁ cant \ncost savings opportunities are associated with reducing the number of suppliers \nand negotiating agreements with preferred suppliers.\nDemand  planning drives effi  cient materials management. After demand is fore-\ncasted, procurement’s goal is to source the appropriate materials or products in \nthe most economical manner. Procurement involves a wide range of activities from \nnegotiating contracts to issuing purchase requisitions and purchase orders (POs) \nto tracking receipts and authorizing payments. The following list gives you a better \nsense of a procurement organization’s common analytic requirements:\n \n■Which materials or products are most frequently purchased? How many ven-\ndors supply these products? At what prices? Looking at demand across the \nenterprise (rather than at a single physical location), are there opportunities \nto negotiate favorable pricing by consolidating suppliers, single sourcing, or \nmaking guaranteed buys?\n \n■Are your employees purchasing from the preferred vendors or skirting the \nnegotiated vendor agreements with maverick spending?\n \n■Are you receiving the negotiated pricing from your vendors or is there vendor \ncontract purchase price variance?\n \n■How are your vendors performing? What is the vendor’s ﬁ ll rate? On-time \ndelivery performance? Late deliveries outstanding? Percent back ordered? \nRejection rate based on receipt inspection?\n Procurement Transactions and Bus Matrix\nAs  you begin working through the four-step dimensional design process, you deter-\nmine that procurement is the business process to be modeled. In studying the \nprocess, you observe a ﬂ urry of procurement transactions, such as purchase requisi-\ntions, purchase orders, shipping notiﬁ cations, receipts, and payments. Similar to the \napproach taken in Chapter 4: Inventory, you could initially design a fact table with \nthe grain of one row per procurement transaction with transaction date, product, \nvendor, contract terms, and procurement transaction type as key dimensions. The \nprocurement transaction quantity and dollar amount are the facts. The resulting \ndesign is shown in Figure 5-1.\n\n\nProcurement 143\nDate Dimension\nProcurement Transaction Fact\nProduct Dimension\nContract Terms Dimension\nProcurement Transaction Type Dimension\nVendor Dimension\nVendor Key (PK)\nVendor Name\nVendor Street Address\nVendor City\nVendor City-State\nVendor ZIP-Postal Code\nVendor State-Province\nVendor Country\nVendor Status\nVendor Minority Ownership Flag\nVendor Corporate Parent\n...\nProcurement Transaction Date Key (FK)\nProduct Key (FK)\nVendor Key (FK)\nContract Terms Key (FK)\nProcurement Transaction Type Key (FK)\nContract Number (DD)\nProcurement Transaction Quantity\nProcurement Transaction Dollar Amount\nContract Terms Key (PK)\nContract Terms Description\nContract Terms Type\nProcurement Transaction Type Key (PK)\nProcurement Transaction Type Description\nProcurement Transaction Type Category\nFigure 5-1: Procurement fact table with multiple transaction types.\nIf you work for the same grocery retailer from the earlier case studies, the trans-\naction date and product dimensions are the same conformed dimensions developed \noriginally in Chapter 3: Retail Sales. If you work with manufacturing procurement, \nthe raw materials products likely are located in a separate raw materials dimen-\nsion table rather than included in the product dimension for salable products. The \nvendor, contract terms, and procurement transaction type dimensions are new \nto this schema. The vendor dimension contains one row for each vendor, along \nwith interesting descriptive attributes to support a variety of vendor analyses. The \ncontract terms dimension contains one row for each generalized set of negotiated \nterms, similar to the promotion dimension in Chapter 3. The procurement trans-\naction type dimension enables grouping or ﬁ ltering on transaction types, such as \npurchase orders. The contract number is a degenerate dimension; it could be used \nto determine the volume of business conducted under each negotiated contract.\n Single Versus Multiple Transaction Fact Tables\nAs  you review the initial procurement schema design with business users, you learn \nseveral new details. First, the business users describe the various procurement trans-\nactions diff erently. To the business, purchase orders, shipping notices, warehouse \nreceipts, and vendor payments are all viewed as separate and unique processes.\nSeveral of the procurement transactions come from diff erent source systems. \nThere is a purchasing system that provides purchase requisitions and purchase \norders, a warehousing system that provides shipping notices and warehouse receipts, \nand an accounts payable system that deals with vendor payments.\n\n\nChapter 5\n144\nYou further discover that several transaction types have diff erent dimensionality. \nFor example, discounts taken are applicable to vendor payments but not to the other \ntransaction types. Similarly, the name of the employee who received the goods at \nthe warehouse applies to receipts but doesn’t make sense elsewhere.\nThere are also a variety of interesting control numbers, such as purchase order \nand payment check numbers, created at various steps in the procurement pipeline. \nThese control numbers are perfect candidates for degenerate dimensions. For certain \ntransaction types, more than one control number may apply.\nAs you sort through these new details, you are faced with a design decision. \nShould you build a blended transaction fact table with a transaction type dimension \nto view all procurement transactions together, or do you build separate fact tables \nfor each transaction type? This is a common design quandary that surfaces in many \ntransactional situations, not just procurement.\nAs dimensional modelers, you need to make design decisions based on a thor-\nough understanding of the business requirements weighed against the realities of \nthe underlying source data. There is no simple formula to make the deﬁ nite deter-\nmination of whether to use a single fact table or multiple fact tables. A single fact \ntable may be the most appropriate solution in some situations, whereas multiple \nfact tables are most appropriate in others. When faced with this design decision, \nthe following considerations help sort out the options:\n \n■What are the users’ analytic requirements? The goal is to reduce complexity \nby presenting the data in the most eff ective form for business users. How will \nthe business users most commonly analyze this data? Which approach most \nnaturally aligns with their business-centric perspective? \n \n■Are there really multiple unique business processes? In the procurement \nexample, it seems buying products (purchase orders) is distinctly diff erent \nfrom receiving products (receipts). The existence of separate control num-\nbers for each step in the process is a clue that you are dealing with separate \nprocesses. Given this situation, you would lean toward separate fact tables. By \ncontrast, in Chapter 4’s inventory example, the varied inventory transactions \nwere part of a single inventory process resulting in a single fact table design.\n \n■Are multiple source systems capturing metrics with unique granularities? \nThere are three separate source systems in this case study: purchasing, ware-\nhousing, and accounts payable. This would suggest separate fact tables. \n \n■What is the dimensionality of the facts? In this procurement example, several \ndimensions are applicable to some transaction types but not to others. This \nwould again lead you to separate fact tables.\n",
      "page_number": 165
    },
    {
      "number": 19,
      "title": "Segment 19 (pages 175-183)",
      "start_page": 175,
      "end_page": 183,
      "detection_method": "topic_boundary",
      "content": "Procurement 145\nA simple way to consider these trade-off s is to draft a bus matrix. As illustrated in \nFigure 5-2, you can include two additional columns identifying the atomic granular-\nity and metrics for each row. These matrix embellishments cause it to more closely \nresemble the detailed implementation bus matrix, which we’ll more thoroughly \ndiscuss in Chapter 16: Insurance.\nBusiness Processes\nAtomic Granularity\nMetrics\nPurchase Requisitions\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nDate\nProduct\nVendor\nContract Terms\nEmployee\nWarehouse\nCarrier\nPurchase Orders\nShipping Notifications\nWarehouse Receipts\nVendor Invoices\nVendor Payments\n1 row per requisition line\n1 row per PO line\n1 row per shipping notice line\n1 row per receipt line\n1 row per invoice line\n1 row per payment\nRequisition Quantity & Dollars\nPO Quantity & Dollars\nShipped Quantity\nReceived Quantity\nInvoice Quantity & Dollars\nInvoice, Discount & Net\nPayment Dollars\nFigure 5-2: Sample bus matrix rows for procurement processes.\nBased on the bus matrix for this hypothetical case study, multiple transaction \nfact tables would be implemented, as illustrated in Figure 5-3. In this example, there \nare separate fact tables for purchase requisitions, purchase orders, shipping notices, \nwarehouse receipts, and vendor payments. This decision was reached because users \nview these activities as separate and distinct business processes, the data comes \nfrom diff erent source systems, and there is unique dimensionality for the various \ntransaction types. Multiple fact tables enable richer, more descriptive dimensions \nand attributes. The single fact table approach would have required generalized \nlabeling for some dimensions. For example, purchase order date and receipt date \nwould likely have been generalized to simply transaction date. Likewise, purchasing \nagent and receiving clerk would become employee. This generalization reduces the \nlegibility of the resulting dimensional model. Also, with separate fact tables as you \nprogress from purchase requisitions to payments, the fact tables inherit dimensions \nfrom the previous steps.\nMultiple fact tables may require more time to manage and administer because \nthere are more tables to load, index, and aggregate. Some would argue this approach \nincreases the complexity of the ETL processes. Actually, it may simplify the ETL \nactivities. Loading the operational data from separate source systems into separate \nfact tables likely requires less complex ETL processing than attempting to integrate \ndata from the multiple sources into a single fact table.\n\n\nChapter 5\n146\nDate Dimension\nVendor Dimension\nEmployee Dimension\nCarrier Dimension\nProduct Dimension\nContract Terms Dimension\nWarehouse Dimension\nPurchase Requisition Date Key (FK)\nProduct Key (FK)\nVendor Key (FK)\nContract Terms Key (FK)\nEmployee Requested By Key (FK)\nContract Number (DD)\nPurchase Requisition Number (DD)\nPurchase Requisition Quantity\nPurchase Requisition Dollar Amount\nPurchase Order Date Key (FK)\nRequested By Date Key (FK)\nProduct Key (FK)\nVendor Key (FK)\nContract Terms Key (FK)\nWarehouse Key (FK)\nCarrier Key (FK)\nEmployee Ordered By Key (FK)\nEmployee Purchase Agent Key (FK)\nContract Number (DD)\nPurchase Requisition Number (DD)\nPurchase Order Number (DD)\nPurchase Order Quantity\nPurchase Order Dollar Amount\nPurchase Order Fact\nPurchase Requisition Fact\nShipping Notification Date Key (FK)\nEstimated Arrival Date Key (FK)\nRequested By Date Key (FK)\nProduct Key (FK)\nVendor Key (FK)\nWarehouse Key (FK)\nCarrier Key (FK)\nEmployee Ordered By Key (FK)\nPurchase Order Number (DD)\nShipping Notification Number (DD)\nShipped Quantity\nShipping Notices Fact\nWarehouse Receipt Date Key (FK)\nRequested By Date Key (FK)\nProduct Key (FK)\nVendor Key (FK)\nWarehouse Key (FK)\nCarrier Key (FK)\nEmployee Ordered By Key (FK)\nEmployee Received By Key (FK)\nPurchase Order Number (DD)\nShipping Notification Number (DD)\nWarehouse Receipt Number (DD)\nReceived Quantity\nWarehouse Receipts Fact\nVendor Payment Date Key (FK)\nProduct Key (FK)\nVendor Key (FK)\nWarehouse Key (FK)\nContract Terms Key (FK)\nContract Number (DD)\nPayment Check Number (DD)\nVendor Invoice Dollar Amount\nVendor Discount Dollar Amount\nVendor Net Payment Dollar Amount\nVendor Payment Fact\nFigure 5-3: Multiple fact tables for procurement processes.\n\n\nProcurement 147\n Complementary Procurement Snapshot\nApart  from the decision regarding multiple procurement transaction fact tables, \nyou may also need to develop a snapshot fact table to fully address the business’s \nneeds. As suggested in Chapter 4, an accumulating snapshot such as Figure 5-4 that \ncrosses processes would be extremely useful if the business is interested in monitor-\ning product movement as it proceeds through the procurement pipeline (including \nthe duration of each stage). Remember that an accumulating snapshot is meant to \nmodel processes with well-deﬁ ned milestones. If the process is a continuous ﬂ ow \nthat never really ends, it is not a good candidate for an accumulating snapshot.\nPurchase Order Date Key (FK)\nRequested By Date Key (FK)\nWarehouse Receipt Date Key (FK)\nVendor Invoice Date Key (FK)\nVendor Payment Date Key (FK)\nProduct Key (FK)\nVendor Key (FK)\nContract Terms Key (FK)\nEmployee Ordered By Key (FK)\nWarehouse Key (FK)\nCarrier Key (FK)\nContract Number (DD)\nPurchase Order Number (DD)\nWarehouse Receipt Number (DD)\nVendor Invoice Number (DD)\nPayment Check Number (DD)\nPurchase Order Quantity\nPurchase Order Dollar Amount\nShipped Quantity\nReceived Quantity\nVendor Invoice Dollar Amount\nVendor Discount Dollar Amount\nVendor Net Payment Dollar Amount\nPO to Requested By Date Lag\nPO to Receipt Date Lag\nRequested By to Receipt Date Lag\nReceipt to Payment Date Lag\nInvoice to Payment Date Lag\nProcurement Pipeline Fact\nRequested By Date Dimension\nVendor Invoice Date Dimension\nProduct Dimension\nContract Terms Dimension\nWarehouse Dimension\nPurchase Order Date Dimension\nWarehouse Receipt Date Dimension\nVendor Payment Date Dimension\nVendor Dimension\nEmployee Dimension\nCarrier Dimension\nFigure 5-4: Procurement pipeline accumulating snapshot schema.\nSlowly Changing Dimension Basics\nTo  this point, we have pretended dimensions are independent of time. Unfortunately, \nthis is not the case in the real world. Although dimension table attributes are relatively \nstatic, they aren’t ﬁ xed forever; attribute values change, albeit rather slowly, over time. \n\n\nChapter 5\n148\nDimensional designers must proactively work with the business’s data governance \nrepresentatives to determine the appropriate change-handling strategy. You shouldn’t \nsimply jump to the conclusion that the business doesn’t care about dimension changes \njust because they weren’t mentioned during the requirements gathering. Although \nIT may assume accurate change tracking is unnecessary, business users may assume \nthe DW/BI system will allow them to see the impact of every attribute value change. \nIt is obviously better to get on the same page sooner rather than later.\nNOTE \nThe business’s data governance and stewardship representatives must be \nactively involved in decisions regarding the handling of slowly changing dimension \nattributes; IT shouldn’t make determinations on its own.\nWhen  change tracking is needed, it might be tempting to put every changing \nattribute into the fact table on the assumption that dimension tables are static. This \nis unacceptable and unrealistic. Instead you need strategies to deal with slowly \nchanging attributes within dimension tables. Since Ralph Kimball ﬁ rst introduced \nthe notion of slowly changing dimensions in 1995, some IT professionals in a never-\nending quest to speak in acronym-ese termed them SCDs. The acronym stuck.\nFor each dimension table attribute, you must specify a strategy to handle change. \nIn other words, when an attribute value changes in the operational world, how will \nyou respond to the change in the dimensional model? In the following sections, we \ndescribe several basic techniques for dealing with attribute changes, followed by \nmore advanced options. You may need to employ a combination of these techniques \nwithin a single dimension table.\nKimball  method followers are likely already familiar with SCD types 1, 2, and 3. \nBecause legibility is part of our mantra, we sometimes wish we had given these tech-\nniques more descriptive names in the ﬁ rst place, such as “overwrite.” But after nearly \ntwo decades, the “type numbers” are squarely part of the DW/BI vernacular. As you’ll \nsee in the following sections, we’ve decided to expand the theme by assigning new \nSCD type numbers to techniques that have been described, but less precisely labeled, \nin the past; our hope is that assigning speciﬁ c numbers facilitates clearer communica-\ntion among team members.\nType 0: Retain Original\nThis  technique hasn’t been given a type number in the past, but it’s been around \nsince the beginning of SCDs. With type 0, the dimension attribute value never \nchanges, so facts are always grouped by this original value. Type 0 is appropriate \nfor any attribute labeled “original,” such as customer original credit score. It also \napplies to most attributes in a date dimension.\n\n\nProcurement 149\nAs we staunchly advocated in Chapter 3, the dimension table’s primary key is \na surrogate key rather than relying on the natural operational key. Although we \ndemoted the natural key to being an ordinary dimension attribute, it still has special \nsigniﬁ cance. Presuming it’s durable, it would remain inviolate. Persistent durable \nkeys are always type 0 attributes. Unless otherwise noted, throughout this chapter’s \nSCD discussion, the durable supernatural key is assumed to remain constant, as \ndescribed in Chapter 3.\n Type 1: Overwrite\nWith  the slowly changing dimension type 1 response, you overwrite the old attri-\nbute value in the dimension row, replacing it with the current value; the attribute \nalways reﬂ ects the most recent assignment.\nAssume you work for an electronics retailer where products roll up into the retail \nstore’s departments. One of the products is IntelliKidz software. The existing row in \nthe product dimension table for IntelliKidz looks like the top half of Figure 5-5. Of \ncourse, there would be additional descriptive attributes in the product dimension, \nbut we’ve abbreviated the attribute listing for clarity. \nIntelliKidz\nEducation\n12345 ABC922-Z\nProduct\nKey\nSKU (NK)\nProduct\nDescription\nDepartment\nName\nIntelliKidz\nStrategy\n12345 ABC922-Z\nUpdated row in Product dimension:\nOriginal row in Product dimension:\nProduct\nKey\nSKU (NK)\nProduct\nDescription\nDepartment\nName\nFigure 5-5: SCD type 1 sample rows.\nSuppose a new merchandising person decides IntelliKidz software should be \nmoved from the Education department to the Strategy department on February 1, \n2013 to boost sales. With a type 1 response, you’d simply update the existing row \nin the dimension table with the new department description, as illustrated in the \nupdated row of Figure 5-5.\nIn this case, no dimension or fact table keys were modiﬁ ed when IntelliKidz’s \ndepartment changed. The fact table rows still reference product key 12345, regardless \nof IntelliKidz’s departmental location. When sales take off  following the move to the \nStrategy department, you have no information to explain the performance improve-\nment because the historical and more recent facts both appear as if IntelliKidz \nalways rolled up into Strategy.\n\n\nChapter 5\n150\nThe type 1 response is the simplest approach for dimension attribute changes. In \nthe dimension table, you merely overwrite the preexisting value with the current \nassignment. The fact table is untouched. The problem with a type 1 response is that \nyou lose all history of attribute changes. Because overwriting obliterates historical \nattribute values, you’re left solely with the attribute values as they exist today. A type \n1 response is appropriate if the attribute change is an insigniﬁ cant correction. It also \nmay be appropriate if there is no value in keeping the old description. However, too \noften DW/BI teams use a type 1 response as the default for dealing with slowly chang-\ning dimensions and end up totally missing the mark if the business needs to track \nhistorical changes accurately. After you implement a type 1, it’s diffi  cult to change \ncourse in the future.\nNOTE \nThe type 1 response is easy to implement, but it does not maintain any \nhistory of prior attribute values.\nBefore we leave the topic of type 1 changes, be forewarned that the same BI \napplications can produce diff erent results before versus after the type 1 attribute \nchange. When the dimension attribute’s type 1 overwrite occurs, the fact rows are \nassociated with the new descriptive context. Business users who rolled up sales by \ndepartment on January 31 will get diff erent department totals when they run the \nsame report on February 1 following the type 1 overwrite.\nThere’s another easily overlooked catch to be aware of. With a type 1 response \nto deal with the relocation of IntelliKidz, any preexisting aggregations based on the \ndepartment value need to be rebuilt. The aggregated summary data must continue \nto tie to the detailed atomic data, where it now appears that IntelliKidz has always \nrolled up into the Strategy department.\nFinally, if a dimensional model is deployed via an OLAP cube and the type 1 \nattribute is a hierarchical rollup attribute, like the product’s department in our \nexample, the cube likely needs to be reprocessed when the type 1 attribute changes. \nAt a minimum, similar to the relational environment, the cube’s performance aggre-\ngations need to be recalculated. \nWARNING \nEven though type 1 changes appear the easiest to implement, \nremember they invalidate relational tables and OLAP cubes that have aggregated \ndata over the aff ected attribute.\n Type 2: Add New Row\nIn  Chapter 1: Data Warehousing, Business Intelligence, and Dimensional Modeling \nPrimer, we stated one of the DW/BI system’s goals was to correctly represent history. \n\n\nProcurement 151\nA type 2 response is the predominant technique for supporting this requirement \nwhen it comes to slowly changing dimension attributes.\nUsing the type 2 approach, when IntelliKidz’s department changed on February \n1, 2013, a new product dimension row for IntelliKidz is inserted to reﬂ ect the new \ndepartment attribute value. There are two product dimension rows for IntelliKidz, \nas illustrated in Figure 5-6. Each row contains a version of IntelliKidz’s attribute \nproﬁ le that was true for a span of time.\nOriginal row in Product dimension:\nProduct\nKey\nSKU (NK)\nProduct\nDescription\nDepartment\nName\n…\nRow\nEffective \nDate\nRow\nExpiration \nDate\nCurrent Row\nIndicator\n12345 ABC922-Z\nIntelliKidz\nEducation\n…\n2012-01-01\n9999-12-31\nCurrent\nRows in Product dimension following department reassignment:\nProduct\nKey\nSKU (NK)\nProduct\nDescription\nDepartment\nName\n...\nRow\nEffective \nDate\nRow\nExpiration\nDate\nCurrent Row\nIndicator\n12345\n25984\nABC922-Z\nABC922-Z\nIntelliKidz\nIntelliKidz\nEducation\nStrategy\n... \n...\n2012-01-01\n2013-02-01\n2013-01-31\n9999-12-31\nExpired \nCurrent\nFigure 5-6: SCD type 2 sample rows.\nWith type 2 changes, the fact table is again untouched; you don’t go back to \nthe historical fact table rows to modify the product key. In the fact table, rows for \nIntelliKidz prior to February 1, 2013, would reference product key 12345 when the \nproduct rolled up to the Education department. After February 1, new IntelliKidz \nfact rows would have product key 25984 to reﬂ ect the move to the Strategy depart-\nment. This is why we say type 2 responses perfectly partition or segment history to \naccount for the change. Reports summarizing pre-February 1 facts look identical \nwhether the report is generated before or after the type 2 change. \nWe want to reinforce that reported results may diff er depending on whether \nattribute changes are handled as a type 1 or type 2. Let’s presume the electronic \nretailer sells $500 of IntelliKidz software during January 2013, followed by a $100 \nsale in February 2013. If the department attribute is a type 1, the results from a \nquery reporting January and February sales would indicate $600 under Strategy. \nConversely, if the department name attribute is a type 2, the sales would be reported \nas $500 for the Education department and $100 for the Strategy department.\nUnlike the type 1 approach, there is no need to revisit preexisting aggregation \ntables when using the type 2 technique. Likewise, OLAP cubes do not need to be \nreprocessed if hierarchical attributes are handled as type 2.\nIf you constrain on the department attribute, the two product proﬁ les are diff er-\nentiated. If you constrain on the product description, the query automatically fetches \nboth IntelliKidz product dimension rows and automatically joins to the fact table for \n\n\nChapter 5\n152\nthe complete product history. If you need to count the number of products correctly, \nthen you would just use the SKU natural key attribute as the basis of the distinct \ncount rather than the surrogate key; the natural key column becomes the glue that \nholds the separate type 2 rows for a single product together. \nNOTE \nThe type 2 response is the primary workhorse technique for accurately \ntracking slowly changing dimension attributes. It is extremely powerful because \nthe new dimension row automatically partitions history in the fact table.\nType 2 is the safest response if the business is not absolutely certain about the \nSCD business rules for an attribute. As we’ll discuss in the “Type 6: Add Type 1 \nAttributes to Type 2 Dimension” and “Type 7: Dual Type 1 and Type 2 Dimensions” \nsections later in the chapter, you can provide the illusion of a type 1 overwrite when \nan attribute has been handled with the type 2 response. The converse is not true. If \nyou treat an attribute as type 1, reverting to type 2 retroactively requires signiﬁ cant \neff ort to create new dimension rows and then appropriately rekey the fact table.\nType 2 Effective and Expiration Dates\nWhen  a dimension table includes type 2 attributes, you should include several \nadministrative columns on each row, as shown in Figure 5-6. The eff ective and \nexpiration dates refer to the moment when the row’s attribute values become valid \nor invalid. Eff ective and expiration dates or date/time stamps are necessary in the \nETL system because it needs to know which surrogate key is valid when loading \nhistorical fact rows. The eff ective and expiration dates support precise time slic-\ning of the dimension; however, there is no need to constrain on these dates in the \ndimension table to get the right answer from the fact table. The row eff ective date \nis the ﬁ rst date the descriptive proﬁ le is valid. When a new product is ﬁ rst loaded \nin the dimension table, the expiration date is set to December 31, 9999. By avoiding \na null in the expiration date, you can reliably use a BETWEEN command to ﬁ nd the \ndimension rows that were in eff ect on a certain date. \nWhen a new proﬁ le row is added to the dimension to capture a type 2 attribute \nchange, the previous row is expired. We typically suggest the end date on the \nold row should be just prior to the eff ective date of the new row leaving no gaps \nbetween these eff ective and expiration dates. The deﬁ nition of “just prior” depends \non the grain of the changes being tracked. Typically, the eff ective and expiration \ndates represent changes that occur during a day; if you’re tracking more granular \nchanges, you’d use a date/time stamp instead. In this case, you may elect to apply \ndiff erent business rules, such as setting the row expiration date exactly equal to the \n\n\nProcurement 153\neff ective date of the next row. This would require logic such as “>= eff ective date \nand < expiration date” constraints, invalidating the use of BETWEEN.\nSome argue that a single eff ective date is adequate, but this makes for more \ncomplicated searches to locate the dimension row with the latest eff ective date \nthat is less than or equal to a date ﬁ lter. Storing an explicit second date simpli-\nﬁ es the query processing. Likewise, a current row indicator is another useful \nadministrative dimension attribute to quickly constrain queries to only the cur-\nrent proﬁ les.\nThe type 2 response to slowly changing dimensions requires the use of surrogate \nkeys, but you’re already using them anyhow, right? You certainly can’t use the opera-\ntional natural key because there are multiple proﬁ le versions for the same natural key. \nIt is not suffi  cient to use the natural key with two or three version digits because you’d \nbe vulnerable to the entire list of potential operational issues discussed in Chapter 3. \nLikewise, it is inadvisable to append an eff ective date to the otherwise primary key \nof the dimension table to uniquely identify each version. With the type 2 response, \nyou create a new dimension row with a new single-column primary key to uniquely \nidentify the new product proﬁ le. This single-column primary key establishes the link-\nage between the fact and dimension tables for a given set of product characteristics. \nThere’s no need to create a confusing secondary join based on the dimension row’s \neff ective or expiration dates.\nWe recognize some of you may be concerned about the administration of surro-\ngate keys to support type 2 changes. In Chapter 19: ETL Subsystems and Techniques \nand Chapter 20: ETL System Design and Development Process and Tasks, we’ll dis-\ncuss a workﬂ ow for managing surrogate keys and accommodating type 2 changes \nin more detail. \nType 1 Attributes in Type 2 Dimensions\nIt  is not uncommon to mix multiple slowly changing dimension techniques within \nthe same dimension. When type 1 and type 2 are both used in a dimension, some-\ntimes a type 1 attribute change necessitates updating multiple dimension rows. Let’s \npresume the dimension table includes a product introduction date. If this attribute \nis corrected using type 1 logic after a type 2 change to another attribute occurs, \nthe introduction date should probably be updated on both versions of IntelliKidz’s \nproﬁ le, as illustrated in Figure 5-7.\nThe data stewards need to be involved in deﬁ ning the ETL business rules in \nscenarios like this. Although the DW/BI team can facilitate discussion regarding \nproper update handling, the business’s data stewards should make the ﬁ nal deter-\nmination, not the DW/BI  team.\n",
      "page_number": 175
    },
    {
      "number": 20,
      "title": "Segment 20 (pages 184-192)",
      "start_page": 184,
      "end_page": 192,
      "detection_method": "topic_boundary",
      "content": "Chapter 5\n154\nOriginal row in Product dimension:\nProduct\nKey\nSKU (NK)\nProduct\nDescription\nDepartment\nName\nIntroduction\nDate\nIntroduction\nDate\n…\nRow\nEffective \nDate\nRow\nExpiration \nDate\nCurrent Row\nIndicator\n12345 ABC922-Z\nIntelliKidz\nEducation\n2012-12-15\n…\n2012-01-01\n9999-12-31\nCurrent\nRows in Product dimension following type 2 change to Department Name and type 1 change to Introduction Date:\nProduct\nKey\nSKU (NK)\nProduct\nDescription\nDepartment\nName\n...\nRow\nEffective \nDate\nRow\nExpiration \nDate\nCurrent Row\nIndicator\n12345\n25984\nABC922-Z\nABC922-Z\nIntelliKidz\nIntelliKidz\nEducation\nStrategy\n2012-01-01 \n2012-01-01\n... \n...\n2012-01-01\n2013-02-01\n2013-01-31\n9999-12-31\nExpired \nCurrent\nFigure 5-7: Type 1 updates in a dimension with type 2 attributes sample rows.\n Type 3: Add New Attribute\nAlthough  the type 2 response partitions history, it does not enable you to associ-\nate the new attribute value with old fact history or vice versa. With the type 2 \nresponse, when you constrain the department attribute to Strategy, you see only \nIntelliKidz facts from after February 1, 2013. In most cases, this is exactly what \nyou want.\nHowever, sometimes you want to see fact data as if the change never occurred. \nThis happens most frequently with sales force reorganizations. District boundaries \nmay be redrawn, but some users still want the ability to roll up recent sales for the \nprior districts just to see how they would have done under the old organizational \nstructure. For a few transitional months, there may be a need to track history for \nthe new districts and conversely to track new fact data in terms of old district \nboundaries. A type 2 response won’t support this requirement, but type 3 comes \nto the rescue.\nIn our software example, let’s assume there is a legitimate business need to \ntrack both the new and prior values of the department attribute for a period of \ntime around the February 1 change. With a type 3 response, you do not issue a \nnew dimension row, but rather add a new column to capture the attribute change, \nas illustrated in Figure 5-8. You would alter the product dimension table to add \na prior department attribute, and populate this new column with the existing \ndepartment value (Education). The original department attribute is treated as a \ntype 1 where you overwrite to reﬂ ect the current value (Strategy). All existing \nreports and queries immediately switch over to the new department description, \nbut you can still report on the old department value by querying on the prior \ndepartment attribute.\n\n\nProcurement 155\nIntelliKidz\nEducation\n12345 ABC922-Z\nProduct\nKey\nSKU (NK)\nProduct\nDescription\nDepartment\nName\nIntelliKidz\nStrategy\nEducation\n12345 ABC922-Z\nUpdated row in Product dimension:\nOriginal row in Product dimension:\nProduct\nKey\nSKU (NK)\nProduct\nDescription\nDepartment\nName\nPrior\nDepartment\nName\nFigure 5-8: SCD type 3 sample rows.\nDon’t be fooled into thinking the higher type number associated with type 3 \nindicates it is the preferred approach; the techniques have not been presented in \ngood, better, and best practice sequence. Frankly, type 3 is infrequently used. It is \nappropriate when there’s a strong need to support two views of the world simulta-\nneously. Type 3 is distinguished from type 2 because the pair of current and prior \nattribute values are regarded as true at the same time. \nNOTE \nThe type 3 slowly changing dimension technique enables you to see \nnew and historical fact data by either the new or prior attribute values, sometimes \ncalled alternate realities.\nType 3 is not useful for attributes that change unpredictably, such as a customer’s \nhome state. There would be no beneﬁ t in reporting facts based on a prior home state \nattribute that reﬂ ects a change from 10 days ago for some customers or 10 years \nago for others. These unpredictable changes are typically handled best with type \n2 instead.\nType 3 is most appropriate when there’s a signiﬁ cant change impacting many \nrows in the dimension table, such as a product line or sales force reorganization. \nThese en masse changes are prime candidates because business users often want \nthe ability to analyze performance metrics using either the pre- or post-hierarchy \nreorganization for a period of time. With type 3 changes, the prior column is labeled \nto distinctly represent the prechanged grouping, such as 2012 department or pre-\nmerger department. These column names provide clarity, but there may be unwanted \nripples in the BI layer.\nFinally, if the type 3 attribute represents a hierarchical rollup level within the \ndimension, then as discussed with type 1, the type 3 update and additional column \nwould likely cause OLAP cubes to be reprocessed.\n\n\nChapter 5\n156\nMultiple Type 3 Attributes\nIf a dimension attribute changes with a predictable rhythm, sometimes the business \nwants to summarize performance metrics based on any of the historic attribute \nvalues. Imagine the product line is recategorized at the start of every year and the \nbusiness wants to look at multiple years of historic facts based on the department \nassignment for the current year or any prior year.\nIn this case, we take advantage of the regular, predictable nature of these changes \nby generalizing the type 3 approach to a series of type 3 dimension attributes, as \nillustrated in Figure 5-9. On every dimension row, there is a current department \nattribute that is overwritten, plus attributes for each annual designation, such as \n2012 department. Business users can roll up the facts with any of the department \nassignments. If a product were introduced in 2013, the department attributes for \n2012 and 2011 would contain Not Applicable values.\nIntelliKidz\nStrategy\nEducation\nNot Applicable\n12345 ABC922-Z\nUpdated row in Product dimension:\nProduct\nKey\nSKU (NK)\nProduct\nDescription\nCurrent\nDepartment\nName\n2012\nDepartment\nName\n2011\nDepartment\nName\nFigure 5-9: Dimension table with multiple SCD type 3 attributes.\nThe most recent assignment column should be identiﬁ ed as the current depart-\nment. This attribute will be used most frequently; you don’t want to modify existing \nqueries and reports to accommodate next year’s change. When the departments are \nreassigned in January 2014, you’d alter the table to add a 2013 department attribute, \npopulate this column with the current department values, and then overwrite the \ncurrent attribute with the 2014 department  assignment. \n Type 4: Add Mini-Dimension\nThus  far we’ve focused on slow evolutionary changes to dimension tables. What \nhappens when the rate of change speeds up, especially within a large multimillion-\nrow dimension table? Large dimensions present two challenges that warrant special \ntreatment. The size of these dimensions can negatively impact browsing and query \nﬁ ltering performance. Plus our tried-and-true type 2 technique for change tracking \nis unappealing because we don’t want to add more rows to a dimension that already \nhas millions of rows, particularly if changes happen frequently.\nFortunately, a single technique comes to the rescue to address both the browsing \nperformance and change tracking challenges. The solution is to break off  frequently \nanalyzed or frequently changing attributes into a separate dimension, referred to \nas a mini-dimension. For example, you could create a mini-dimension for a group \n\n\nProcurement 157\nof more volatile customer demographic attributes, such as age, purchase frequency \nscore, and income level, presuming these columns are used extensively and changes \nto these attributes are important to the business. There would be one row in the \nmini-dimension for each unique combination of age, purchase frequency score, \nand income level encountered in the data, not one row per customer. With this \napproach, the mini-dimension becomes a set of demographic proﬁ les. Although the \nnumber of rows in the customer dimension may be in the millions, the number of \nmini-dimension rows should be a signiﬁ cantly smaller. You leave behind the more \nconstant attributes in the original multimillion-row customer table.\nSample rows for a demographic mini-dimension are illustrated in Figure 5-10. \nWhen creating the mini-dimension, continuously variable attributes, such as income, \nare converted to banded ranges. In other words, the attributes in the mini-dimension \nare typically forced to take on a relatively small number of discrete values. Although \nthis restricts use to a set of predeﬁ ned bands, it drastically reduces the number of \ncombinations in the mini-dimension. If you stored income at a speciﬁ c dollar and \ncents value in the mini-dimension, when combined with the other demographic \nattributes, you could end up with as many rows in the mini-dimension as in the \ncustomer dimension itself. The use of band ranges is probably the most signiﬁ cant \ncompromise associated with the mini-dimension technique. Although grouping \nfacts from multiple band values is viable, changing to more discreet bands (such \nas $30,000-34,999) at a later time is diffi  cult. If users insist on access to a speciﬁ c \nraw data value, such as a credit bureau score that is updated monthly, it should be \nincluded in the fact table, in addition to being value banded in the demographic \nmini-dimension. In Chapter 10: Financial Services, we’ll discuss dynamic value \nbanding of facts; however, such queries are much less effi  cient than constraining \nthe value band in a mini-dimension table.\nLow\nMedium\nHigh\nLow\nMedium\nHigh\n... \nLow\nMedium\nHigh \n...\n<$30,000\n<$30,000\n<$30,000\n$30,000-39,999\n$30,000-39,999\n$30,000-39,999 \n... \n<$30,000\n<$30,000\n<$30,000\n...\n1 \n2 \n3 \n4 \n5 \n6 \n... \n142 \n143 \n144 \n...\n21-25 \n21-25 \n21-25 \n21-25 \n21-25 \n21-25 \n... \n26-30 \n26-30 \n26-30 \n...\nDemographics\nKey\nAge Band\nIncome Level\nPurchase\nFrequency\nScore\nFigure 5-10: SCD type 4 mini-dimension sample rows.\n\n\nChapter 5\n158\nEvery  time a fact table row is built, two foreign keys related to the customer would \nbe included: the customer dimension key and the mini-dimension demographics \nkey in eff ect at the time of the event, as shown in Figure 5-11. The mini-dimension \ndelivers performance beneﬁ ts by providing a smaller point of entry to the facts. \nQueries can avoid the huge customer dimension table unless attributes from that \ntable are constrained or used as report labels.\nCustomer Dimension\nCustomer Key (PK)\nCustomer ID (NK)\nCustomer Name\nCustomer Address\nCustomer City-State\nCustomer State\nCustomer ZIP-Postal Code\nCustomer Date of Birth\nDemographics Dimension\nDemographics Key (PK)\nAge Band\nPurchase Frequency Score\nIncome Level\nFact Table\nDate Key (FK)\nCustomer Key (FK)\nDemographics Key (FK)\nMore FKs...\nFacts...\nFigure 5-11: Type 4 mini-dimension with customer dimension.\nWhen  the mini-dimension key participates as a foreign key in the fact table, another \nbeneﬁ t is that the fact table captures the demographic proﬁ le changes. Let’s presume \nwe are loading data into a periodic snapshot fact table on a monthly basis. Referring \nback to our sample demographic mini-dimension sample rows in Figure 5-10, if one \nof our customers, John Smith, were 25 years old with a low purchase frequency score \nand an income of $25,000, you’d begin by assigning demographics key 1 when loading \nthe fact table. If John has a birthday several weeks later and turns 26 years old, you’d \nassign demographics key 142 when the fact table was next loaded; the demographics \nkey on John’s earlier fact table rows would not be changed. In this manner, the fact \ntable tracks the age change. You’d continue to assign demographics key 142 when \nthe fact table is loaded until there’s another change in John’s demographic proﬁ le. If \nJohn receives a raise to $32,000 several months later, a new demographics key would \nbe reﬂ ected in the next fact table load. Again, the earlier rows would be unchanged. \nOLAP cubes also readily accommodate type 4 mini-dimensions.\nCustomer  dimensions are somewhat unique in that customer attributes frequently \nare queried independently from the fact table. For example, users may want to know \nhow many customers live in Dade County by age bracket for segmentation and proﬁ l-\ning. Rather than forcing any analysis that combines customer and demographic data \nto link through the fact table, the most recent value of the demographics key also \ncan exist as a foreign key on the customer dimension table. We’ll further describe \nthis customer demographic outrigger as an SCD type 5 in the next section.\n\n\nProcurement 159\nThe  demographic dimension cannot be allowed to grow too large. If you have \nﬁ ve demographic attributes, each with 10 possible values, then the demographics \ndimension could have 100,000 (105) rows. This is a reasonable upper limit for the \nnumber of rows in a mini-dimension if you build out all the possible combina-\ntions in advance. An alternate ETL approach is to build only the mini-dimension \nrows that actually occur in the data. However, there are certainly cases where even \nthis approach doesn’t help and you need to support more than ﬁ ve demographic \nattributes with 10 values each. We’ll discuss the use of multiple mini-dimensions \nassociated with a single fact table in Chapter 10.\nDemographic proﬁ le changes sometimes occur outside a business event, such \nas when a customer’s proﬁ le is updated in the absence of a sales transaction. If the \nbusiness requires accurate point-in-time proﬁ ling, a supplemental factless fact table \nwith eff ective and expiration dates can capture every relationship change between \nthe customer and demographics dimensions.\nHybrid Slowly Changing Dimension \nTechniques\nIn  this ﬁ nal section, we’ll discuss hybrid approaches that combine the basic SCD \ntechniques. Designers sometimes become enamored with these hybrids because they \nseem to provide the best of all worlds. However, the price paid for greater analytic \nﬂ exibility is often greater complexity. Although IT professionals may be impressed \nby elegant ﬂ exibility, business users may be just as easily turned off  by complexity. \nYou should not pursue these options unless the business agrees they are needed to \naddress their requirements.\nThese ﬁ nal approaches are most relevant if you’ve been asked to preserve the \nhistorically accurate dimension attribute associated with a fact event, while sup-\nporting the option to report historical facts according to the current attribute values. \nThe basic slowly changing dimension techniques do not enable this requirement \neasily on their own.\nWe’ll start by considering a technique that combines type 4 with a type 1 outrig-\nger; because 4 + 1 = 5, we’re calling this type 5. Next, we’ll describe type 6, which \ncombines types 1 through 3 for a single dimension attribute; it’s aptly named type \n6 because 2 + 3 + 1 or 2 × 3 × 1 both equal 6. Finally, we’ll ﬁ nish up with type 7, \nwhich just happens to be the next available sequence number; there is no underly-\ning mathematical signiﬁ cance to this  label.\n\n\nChapter 5\n160\n Type 5: Mini-Dimension and Type 1 Outrigger\nLet’s return to the type 4 mini-dimension. An embellishment to this technique is to \nadd a current mini-dimension key as an attribute in the primary dimension. This \nmini-dimension key reference is a type 1 attribute, overwritten with every proﬁ le \nchange. You wouldn’t want to track this attribute as a type 2 because then you’d be \ncapturing volatile changes within the large multimillion-row dimension and avoid-\ning this explosive growth was one of the original motivations for type 4.\nThe type 5 technique is useful if you want a current proﬁ le count in the absence \nof fact table metrics or want to roll up historical facts based on the customer’s cur-\nrent proﬁ le. You’d logically represent the primary dimension and mini-dimension \noutrigger as a single table in the presentation area, as shown in Figure 5-12. To \nminimize user confusion and potential error, the current attributes in this role-\nplaying dimension should have distinct column names distinguishing them, such \nas current age band. Even with unique labeling, be aware that presenting users with \ntwo avenues for accessing demographic data, through either the mini-dimension \nor outrigger, can deliver more functionality and complexity than some can  handle.\nCurrent Demographics Dimension\nCurrent Demographics Key (PK)\nCurrent Age Band\nCurrent Purchase Frequency Score\nCurrent Income Level\nLogical representation to the BI tools:\nView of Demographics Dimension\nFact Table\nDate Key (FK)\nCustomer Key (FK)\nDemographics Key (FK)\nMore FKs...\nFacts\nCustomer Dimension\nCustomer Key (PK)\nCustomer ID (NK)\nCustomer Name\n...\nCurrent Demographics Key (FK)\nDemographics Dimension\nDemographics Key (PK)\nAge Band\nPurchase Frequency Score\nIncome Level\nFact Table\nDate Key (FK)\nCustomer Key (FK)\nDemographics Key (FK)\nMore FKs...\nFacts\nCustomer Dimension\nCustomer Key (PK)\nCustomer ID (NK)\nCustomer Name\n...\nCurrent Age Band\nCurrent Purchase Frequency Score\nCurrent Income Level\nDemographics Dimension\nDemographics Key (PK)\nAge Band\nPurchase Frequency Score\nIncome Level\nFigure 5-12: Type 4 mini-dimension with type 1 outrigger in customer dimension.\nNOTE \nThe type 4 mini-dimension terminology refers to when the demograph-\nics key is part of the fact table composite key. If the demographics key is a foreign \nkey in the customer dimension, it is referred to as an outrigger.\n Type 6: Add Type 1 Attributes to Type 2 Dimension\nLet’s  return to the electronics retailer’s product dimension. With type 6, you would \nhave two department attributes on each row. The current department column \n\n\nProcurement 161\nrepresents the current assignment; the historic department column is a type 2 \nattribute representing the historically accurate department value. \nWhen IntelliKidz software is introduced, the product dimension row would look \nlike the ﬁ rst scenario in Figure 5-13.\nOriginal row in Product dimension:\nProduct\nKey\nSKU (NK)\nProduct\nDescription\nHistoric\nDepartment\nName\nCurrent\nDepartment\nName\n…\nRow\nEffective \nDate\nRow\nExpiration \nDate\nCurrent Row\nIndicator\n12345 ABC922-Z\nIntelliKidz\nEducation\nEducation\n…\n2012-01-01\n9999-12-31\nCurrent\nRows in Product dimension following first department reassignment:\nProduct\nKey\nSKU (NK)\nProduct\nDescription\nHistoric\nDepartment\nName\nCurrent\nDepartment\nName\n...\nRow\nEffective \nDate\nRow\nExpiration \nDate\nCurrent Row\nIndicator\n12345\n25984\nABC922-Z\nABC922-Z\nIntelliKidz\nIntelliKidz\nEducation\nStrategy\nStrategy \nStrategy\n... \n...\n2012-01-01\n2013-02-01\n2013-01-31\n9999-12-31\nExpired \nCurrent\nRows in Product dimension following second department reassignment:\nProduct\nKey\nSKU (NK)\nProduct\nDescription\nHistoric\nDepartment\nName\nCurrent\nDepartment\nName\n...\nRow\nEffective \nDate\nRow\nExpiration \nDate\nCurrent Row\nIndicator\n12345\n25984\n31726\nABC922-Z\nABC922-Z\nABC922-Z\nIntelliKidz\nIntelliKidz\nIntelliKidz\nEducation\nStrategy\nCritical Thinking\nCritical Thinking\nCritical Thinking\nCritical Thinking\n... \n...\n...\n2012-01-01\n2013-02-01\n2013-07-01\n2013-01-31\n2013-06-30\n9999-12-31\nExpired\nExpired \nCurrent\nFigure 5-13: SCD type 6 sample rows.\nWhen the departments are restructured and IntelliKidz is moved to the Strategy \ndepartment, you’d use a type 2 response to capture the attribute change by issu-\ning a new row. In this new IntelliKidz dimension row, the current department will \nbe identical to the historical department. For all previous instances of IntelliKidz \ndimension rows, the current department attribute will be overwritten to reﬂ ect the \ncurrent structure. Both IntelliKidz rows would identify the Strategy department as \nthe current department (refer to the second scenario in Figure 5-13).\nIn this manner you can use the historic attribute to group facts based on the attribute \nvalue that was in eff ect when the facts occurred. Meanwhile, the current attri-\nbute rolls up all the historical fact data for both product keys 12345 and 25984 into \nthe current department assignment. If IntelliKidz were then moved into the Critical \nThinking software department, the product table would look like Figure 5-13’s ﬁ nal \nset of rows. The current column groups all facts by the current assignment, while \nthe historic column preserves the historic assignments accurately and segments the \nfacts accordingly.\nWith this hybrid approach, you issue a new row to capture the change (type 2) \nand add a new column to track the current assignment (type 3), where subsequent \nchanges are handled as a type 1 response. An engineer at a technology company \n\n\nChapter 5\n162\nsuggested we refer to this combo approach as type 6 because both the sum and \nproduct of 1, 2, and 3 equals 6. \nAgain, although this technique may be naturally appealing to some, it is impor-\ntant to always consider the business users’ perspective as you strive to arrive at a \nreasonable balance between ﬂ exibility and complexity. You may want to limit which \ncolumns are exposed to some users so they’re not overwhelmed by choices.\n Type 7: Dual Type 1 and Type 2 Dimensions\nWhen  we ﬁ rst described type 6, someone asked if the technique would be appropri-\nate for supporting both current and historic perspectives for 150 attributes in a large \ndimension table. That question sent us back to the drawing board.\n In  this ﬁ nal hybrid technique, the dimension natural key (assuming it’s durable) \nis included as a fact table foreign key, in addition to the surrogate key for type 2 \ntracking, as illustrated in Figure 5-14. If the natural key is unwieldy or ever reas-\nsigned, you should use a separate durable supernatural key instead. The type 2 \ndimension contains historically accurate attributes for ﬁ ltering and grouping based \non the eff ective values when the fact event occurred. The durable key joins to a \ndimension with just the current type 1 values. Again, the column labels in this table \nshould be prefaced with “current” to reduce the risk of user confusion. You can use \nthese dimension attributes to summarize or ﬁ lter facts based on the current proﬁ le, \nregardless of the attribute values in eff ect when the fact event occurred. \nFact Table\nDate Key (FK)\nProduct Key (FK)\nDurable Product Key (FK)\nMore FKs...\nFacts\nProduct Dimension\nProduct Key (PK)\nDurable Product Key (DK)\nProduct Description\nDepartment Name\n... \nRow Effective Date\nRow Expiration Date\nCurrent Row Indicator\nCurrent Product Dimension\nDurable Product Key (PK)\nCurrent Product Description\nCurrent Department Name\n...\nView of Product Dimension\n(where Current Row Indicator=Current)\nFigure 5-14: Type 7 with dual foreign keys for dual type 1 and type 2 dimension tables.\nThis approach delivers the same functionality as type 6. Although the type 6 \nresponse spawns more attribute columns in a single dimension table, this approach \nrelies on two foreign keys in the fact table. Type 7 invariably requires less ETL eff ort \nbecause the current type 1 attribute table could easily be delivered via a view of \nthe type 2 dimension table, limited to the most current rows. The incremental cost \nof this ﬁ nal technique is the additional column carried in the fact table; however, \n",
      "page_number": 184
    },
    {
      "number": 21,
      "title": "Segment 21 (pages 193-202)",
      "start_page": 193,
      "end_page": 202,
      "detection_method": "topic_boundary",
      "content": "Procurement 163\nqueries based on current attribute values would be ﬁ ltering on a smaller dimension \ntable than previously described with type 6.\nOf course, you could avoid storing the durable key in the fact table by joining the \ntype 1 view containing current attributes to the durable key in the type 2 dimension \ntable itself. In this case, however, queries that are only interested in current rollups \nwould need to traverse from the type 1 outrigger through the more voluminous \ntype 2 dimension before ﬁ nally reaching the facts, which would likely negatively \nimpact query performance for current reporting.\nA variation of this dual type 1 and type 2 dimension table approach again relies \non a view to deliver current type 1 attributes. However, in this case, the view associ-\nates the current attribute values with all the durable key’s type 2 rows, as illustrated \nin Figure 5-15.\nFact Table\nDate Key (FK)\nProduct Key (FK)\nMore FKs...\nFacts\nProduct Dimension\nProduct Key (PK)\nDurable Product Key\nProduct Description\nDepartment Name\n... \nRow Effective Date\nRow Expiration Date\nCurrent Row Indicator\nCurrent Product Dimension\nProduct Key (PK)\nDurable Product Key\nCurrent Product Description\nCurrent Department Name\n...\nView of Product Dimension\nFigure 5-15: Type 7 variation with single surrogate key for dual type 1 and type 2 \ndimension tables.\nBoth dimension tables in Figure 5-15 have the same number of rows, but the \ncontents of the tables are diff erent, as shown in Figure 5-16.\nRows in Product dimension:\nProduct\nKey\nSKU (NK)\nDurable\nProduct\nKey\nProduct\nDescription\nDepartment\nName\n...\nRow Effective \nDate\nRow\nExpiration \nDate\nCurrent Row\nIndicator\n12345\n25984\n31726\nABC922-Z\nABC922-Z\nABC922-Z\n12345\n12345\n12345\nIntelliKidz\nIntelliKidz\nIntelliKidz\nEducation\nStrategy\nCritical Thinking\n... \n...\n...\n2012-01-01\n2013-02-01\n2013-07-01\n2013-01-31\n2013-06-30\n9999-12-31\nExpired\nExpired \nCurrent\nRows in Product dimension’s current view:\nProduct\nKey\nSKU (NK)\nDurable\nProduct\nKey\nCurrent\nProduct\nDescription\nCurrent\nDepartment\nName\n...\n12345\n25984\n31726\nABC922-Z\nABC922-Z\nABC922-Z\n12345 \n12345 \n12345\nIntelliKidz\nIntelliKidz\nIntelliKidz\nCritical Thinking\nCritical Thinking\nCritical Thinking\n... \n...\n...\nFigure 5-16: SCD type 7 variation sample rows.\n\n\nChapter 5\n164\nType 7 for Random “As Of” Reporting\nFinally,  although it’s uncommon, you might be asked to roll up historical facts \nbased on any speciﬁ c point-in-time proﬁ le, in addition to reporting by the attribute \nvalues in eff ect when the fact event occurred or by the attribute’s current values. \nFor example, perhaps the business wants to report three years of historical metrics \nbased on the hierarchy in eff ect on December 1 of last year. In this case, you can \nuse the dual dimension keys in the fact table to your advantage. First ﬁ lter on the \ntype 2 dimension row eff ective and expiration dates to locate the rows in eff ect on \nDecember 1 of last year. With this constraint, a single row for each durable key \nin the type 2 dimension is identiﬁ ed. Then join this ﬁ ltered set to the durable key in \nthe fact table to roll up any facts based on the point-in-time attribute values. It’s as \nif you’re deﬁ ning the meaning of “current” on-the-ﬂ y. Obviously, you must ﬁ lter \non the row eff ective and expiration dates, or you’ll have multiple type 2 rows for \neach durable key. Finally, only unveil this capability to a limited, highly analytic \naudience; this embellishment is not for the timid.\nSlowly Changing Dimension Recap\nWe’ve summarized the techniques for tracking dimension attribute changes in \nFigure 5-17. This chart highlights the implications of each slowly changing dimen-\nsion technique on the analysis of performance metrics in the fact table.\nSCD Type\nDimension Table Action\nImpact on Fact Analysis\nType 0\nNo change to attribute value.\nFacts associated with attribute’s original value.\nType 1\nOverwrite attribute value.\nFacts associated with attribute’s current value.\nType 2\nType 3\nType 4\nType 5\nType 6\nAdd new dimension row for profile\nwith new attribute value.\nAdd new column to preserve attribute’s\ncurrent and prior values.\nAdd mini-dimension table containing\nrapidly changing attributes.\nAdd type 4 mini-dimension, along with\noverwritten type 1 mini-dimension key\nin base dimension.\nAdd type 1 overwritten attributes to\ntype 2 dimension row, and overwrite\nall prior dimension rows.\nFacts associated with attribute value in effect when\nfact occured.\nFacts associated with both current and prior attribute\nalternative values.\nFacts associated with rapidly changing attributes in\neffect when fact occured.\nFacts associated with rapidly changing attributes in\neffect when fact occurred, plus current rapidly changing\nattribute values.\nFacts associated with attribute value in effect when fact\noccurred, plus current values.\nFacts associated with attribute value in effect when fact\noccurred, plus current values.\nType 7\nAdd type 2 dimension row with new\nattribute value, plus view limited to\ncurrent rows and/or attribute values.\nFigure 5-17: Slowly changing dimension techniques summary.\n\n\nProcurement 165\nSummary\nIn this chapter we discussed several approaches to handling procurement data. \nEff ectively managing procurement performance can have a major impact on an \norganization’s bottom line.\nWe also introduced techniques to deal with changes to dimension attribute \nvalues. The slowly changing responses range from doing nothing (type 0) to \noverwriting the value (type 1) to complicated hybrid approaches (such as types 5 \nthrough 7) which combine techniques to support requirements for both historic \nattribute preservation and current attribute reporting. You’ll undoubtedly need to \nre-read this section as you consider slowly changing dimension attribute strategies \nfor your DW/BI system.\n\n\nOrder Management\nO\nrder  management consists of several critical business processes, including \norder, shipment, and invoice processing. These processes spawn metrics, \nsuch as sales volume and invoice revenue, that are key performance indicators for \nany organization that sells products or services to others. In fact, these foundation \nmetrics are so crucial that DW/BI teams frequently tackle one of the order manage-\nment processes for their initial implementation. Clearly, the topics in this case study \ntranscend industry boundaries.\nIn this chapter we’ll explore several diff erent order management transactions, \nincluding the common characteristics and complications encountered when \ndimensionally modeling these transactions. We’ll further develop the concept of \nan accumulating snapshot to analyze the order fulﬁ llment pipeline from initial \norder to invoicing.\nChapter 6 discusses the following concepts:\n \n■Bus matrix snippet for order management processes\n \n■Orders transaction schema\n \n■Fact table normalization considerations\n \n■Role-playing dimensions\n \n■Ship-to/bill-to customer dimension considerations\n \n■Factors to determine if single or multiple dimensions\n \n■Junk dimensions for miscellaneous ﬂ ags and indicators versus alternative \ndesigns\n \n■More on degenerate dimensions\n \n■Multiple currencies and units of measure\n \n■Handling of facts with diff erent granularity\n \n■Patterns to avoid with header and line item transactions\n \n■Invoicing transaction schema with proﬁ t and loss facts\n \n■Audit dimension\n6\n\n\nChapter 6\n168\n \n■Quantitative measures and qualitative descriptors of service level performance\n \n■Order fulﬁ llment pipeline as accumulating snapshot schema\n \n■Lag calculations\n Order Management Bus Matrix\nThe  order management function is composed of a series of business processes. In \nits most simplistic form, you can envision a subset of the enterprise data warehouse \nbus matrix that resembles Figure 6-1.\nQuoting\nOrdering\nShipping to Customer\nShipment Invoicing\nReceiving Payments\nCustomer Returns\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nDate\nCustomer\nProduct\nSales Rep\nDeal\nWarehouse\nShipper\nFigure 6-1: Bus matrix rows for order management processes.\nAs described in earlier chapters, the bus matrix closely corresponds to the orga-\nnization’s value chain. In this chapter we’ll focus on the order and invoice rows \nof the matrix. We’ll also describe an accumulating snapshot fact table to evaluate \nperformance across multiple stages of the overall order fulﬁ llment process.\n Order Transactions\nThe  natural granularity for an order transaction fact table is one row for each line \nitem on an order. The dimensions associated with the orders business process are \norder date, requested ship date, product, customer, sales rep, and deal. The facts \ninclude the order quantity and extended order line gross, discount, and net (equal \nto the gross amount less discount) dollar amounts. The resulting schema would \nlook similar to Figure 6-2.\n\n\nOrder Management 169\nOrder Date Key (FK)\nRequested Ship Date Key (FK)\nCustomer Key (FK)\nProduct Key (FK)\nSales Rep Key (FK)\nDeal Key (FK)\nOrder Number (DD)\nOrder Line Number (DD)\nOrder Line Quantity\nExtended Order Line Gross Dollar Amount\nExtended Order Line Discount Dollar Amount\nExtended Order Line Net Dollar Amount\nCustomer Dimension\nProduct Dimension\nDeal Dimension\nOrder Date Dimension\nOrder Line Transaction Fact\nRequested Ship Date Dimension\nSales Rep Dimension\nFigure 6-2: Order transaction fact table.\n Fact Normalization\nRather  than storing the list of facts in Figure 6-2, some designers want to further nor-\nmalize the fact table so there’s a single, generic fact amount along with a dimension \nthat identiﬁ es the type of measurement. In this scenario, the fact table granularity is \none row per measurement per order line, instead of the more natural one row per order \nline event. The measurement type dimension would indicate whether the fact is the \ngross order amount, order discount amount, or some other measure. This technique \nmay make sense when the set of facts is extremely lengthy, but sparsely populated \nfor a given fact row, and no computations are made between facts. You could use this \ntechnique to deal with manufacturing quality test data where the facts vary widely \ndepending on the test conducted.\nHowever, you should generally resist the urge to normalize the fact table in this \nway. Facts usually are not sparsely populated within a row. In the order transaction \nschema, if you were to normalize the facts, you’d be multiplying the number of rows \nin the fact table by the number of fact types. For example, assume you started with \n10 million order line fact table rows, each with six keys and four facts. If the fact \nrows were normalized, you’d end up with 40 million fact rows, each with seven \nkeys and one fact. In addition, if any arithmetic function is performed between \nthe facts (such as discount amount as a percentage of gross order amount), it is \nfar easier if the facts are in the same row in a relational star schema because SQL \nmakes it diffi  cult to perform a ratio or diff erence between facts in diff erent rows. \nIn Chapter 14: Healthcare, we’ll explore a situation where a measurement type \ndimension makes more sense. This pattern is also more appropriate if the primary \nplatform supporting BI applications is an OLAP cube; the cube enables computations \n\n\nChapter 6\n170\nthat cut the cube along any dimension, regardless if it’s a date, product, customer, \nor measurement type.\n Dimension Role Playing\nBy  now you know to expect a date dimension in every fact table because you’re \nalways looking at performance over time. In a transaction fact table, the primary date \ncolumn is the transaction date, such as the order date. Sometimes you discover other \ndates associated with each transaction, such as the requested ship date for the order.\nEach  of the dates should be a foreign key in the fact table, as shown in Figure 6-3. \nHowever, you cannot simply join these two foreign keys to the same date dimension \ntable. SQL would interpret this two-way simultaneous join as requiring both the \ndates to be identical, which isn’t very likely.\nLogical views or aliases of the\nsingle physical date dimension\nOrder Date Key (FK)\nRequested Ship Date Key (FK)\nCustomer Key (FK)\nProduct Key (FK)\nSales Rep Key (FK)\nDeal Key (FK)\nOrder Number (DD)\nOrder Line Number (DD)\nOrder Quantity\nExtended Order Line Gross Dollar Amount\nExtended Order Line Discount Dollar Amount\nExtended Order Line Net Dollar Amount\nOrder Line Transaction Fact\nDate Key\nDate\nDay of Week\nMonth\nQuarter\n...\nDate Dimension\nOrder Date Key\nOrder Date\nOrder Day of Week\nOrder Month\nOrder Quarter\n...\nRequested Ship Date Key\nRequested Ship Date\nRequested Ship Day of Week\nRequested Ship Month\nRequested Ship Quarter\n...\nOrder Date Dimension\nRequested Ship Date Dimension\nFigure 6-3: Role-playing date dimensions.\nEven though you cannot literally join to a single date dimension table, you can \nbuild and administer a single physical date dimension table. You then create the \nillusion of two independent date dimensions by using views or aliases. Be careful to \nuniquely label the columns in each of the views or aliases. For example, the order \nmonth attribute should be uniquely labeled to distinguish it from the requested \nship month. If you don’t establish unique column names, you wouldn’t be able to \ntell the columns apart when both are dragged into a report.\nAs we brieﬂ y described in Chapter 3: Retail Sales, we would deﬁ ne the order date \nand requested order date views as follows:\ncreate view order_date\n  (order_date_key, order_day_of_week, order_month, ...) \n  as select date_key, day_of_week, month, ... from date\n\n\nOrder Management 171\nand\ncreate view req_ship_date\n  (req_ship_date_key, req_ship_day_of_week, req_ship_month, ...) \n  as select date_key, day_of_week, month, ... from date\nAlternatively,  SQL supports the concept of aliasing. Many BI tools also enable \naliasing within their semantic layer. However, we caution against this approach \nif multiple BI tools, along with direct SQL-based access, are used within the \norganization.\nRegardless  of the implementation approach, you now have two unique logical date \ndimensions that can be used as if they were independent with completely unrelated \nconstraints. This is referred to as role playing because the date dimension simultane-\nously serves diff erent roles in a single fact table. You’ll see additional examples of \ndimension role playing sprinkled throughout this book.\nNOTE \nRole playing in a dimensional model occurs when a single dimension \nsimultaneously appears several times in the same fact table. The underlying dimen-\nsion may exist as a single physical table, but each of the roles should be presented \nto the BI tools as a separately labeled view.\nIt’s worth noting that some OLAP products do not support multiple roles of the \nsame dimension; in this scenario, you’d need to create two separate dimensions for \nthe two roles. In addition, some OLAP products that enable multiple roles do not \nenable attribute renaming for each role. In the end, OLAP environments may be \nlittered with a plethora of separate dimensions, which are treated simply as roles \nin the relational star schema.\nTo  handle the multiple dates, some designers are tempted to create a single date \ntable with a key for each unique order date and requested ship date combination. \nThis approach falls apart on several fronts. First, the clean and simple daily date \ntable with approximately 365 rows per year would balloon in size if it needed to \nhandle all the date combinations. Second, a combination date table would no longer \nconform to the other frequently used daily, weekly, and monthly date dimensions.\nRole Playing and the Bus Matrix\nThe  most common technique to document role playing on the bus matrix is to \nindicate the multiple roles within a single cell, as illustrated in Figure 6-4. We used \na similar approach in Chapter 4: Inventory for documenting shrunken conformed \ndimensions. This method is especially appropriate for the date dimension on the \nbus matrix given its numerous logical roles. Alternatively, if the number of roles is \nlimited and frequently reused across processes, you can create subcolumns within \na single conformed dimension column on the matrix.\n\n\nChapter 6\n172\nQuoting\nOrdering\nShipping to Customer\nShipment Invoicing\nReceiving Payments\nCustomer Returns\nQuote Date\nOrder Date\nRequested Ship Date\nShipment Date\nInvoice Date\nPayment Receipt Date\nReturn Date\nDate\nFigure 6-4: Communicating role-playing dimensions on the bus matrix.\nProduct Dimension Revisited\nEach  of the case study vignettes presented so far has included a product dimen-\nsion. The product dimension is one of the most common and most important \ndimension tables. It describes the complete portfolio of products sold by a com-\npany. In many cases, the number of products in the portfolio turns out to be sur-\nprisingly large, at least from an outsider’s perspective. For example, a prominent \nU.S. manufacturer of dog and cat food tracks more than 25,000 manufacturing \nvariations of its products, including retail products everyone (or every dog and \ncat) is familiar with, as well as numerous specialized products sold through com-\nmercial and veterinary channels. Some durable goods manufacturers, such as \nwindow companies, sell millions of unique product conﬁ gurations.\nMost  product dimension tables share the following characteristics:\n \n■Numerous verbose, descriptive columns. For manufacturers, it’s not unusual \nto maintain 100 or more descriptors about the products they sell. Dimension \ntable attributes naturally describe the dimension row, do not vary because \nof the inﬂ uence of another dimension, and are virtually constant over time, \nalthough some attributes do change slowly over time.\n \n■One or more attribute hierarchies, plus non-hierarchical attributes. Products \ntypically roll up according to multiple deﬁ ned hierarchies. The many-to-one \nﬁ xed depth hierarchical data should be presented in a single ﬂ attened, denor-\nmalized product dimension table. You should resist creating normalized snow-\nﬂ aked sub-tables; the costs of a more complicated presentation and slower \nintra-dimension browsing performance outweigh the minimal storage savings \nbeneﬁ ts. Product dimension tables can have thousands of entries. With so many \n\n\nOrder Management 173\nrows, it is not too useful to request a pull-down list of the product descriptions. \nIt is essential to have the ability to constrain on one attribute, such as ﬂ avor, \nand then another attribute, such as package type, before attempting to display \nthe product descriptions. Any attributes, regardless of whether they belong to \na single hierarchy, should be used freely for browsing and drilling up or down. \nMany product dimension attributes are standalone low-cardinality attributes, \nnot part of explicit hierarchies.\nThe  existence of an operational product master helps create and maintain the \nproduct dimension, but a number of transformations and administrative steps must \noccur to convert the operational master ﬁ le into the dimension table, including the \nfollowing:\n \n■Remap the operational product code to a surrogate key. As we discussed in \nChapter 3, this meaningless surrogate primary key is needed to avoid havoc \ncaused by duplicate use of an operational product code over time. It also \nmight be necessary to integrate product information sourced from diff erent \noperational systems. Finally, as you just learned in Chapter 5: Procurement, \nthe surrogate key is needed to track type 2 product attribute changes. \n \n■Add descriptive attribute values to augment or replace operational codes. \nYou shouldn’t accept the excuse that the business users are familiar with the \noperational codes. The only reason business users are familiar with codes is that \nthey have been forced to use them! The columns in a product dimension are \nthe sole source of query constraints and report labels, so the contents must be \nlegible. Cryptic abbreviations are as bad as outright numeric codes; they also \nshould be augmented or replaced with readable text. Multiple abbreviated codes \nin a single column should be expanded and separated into distinct attributes.\n \n■Quality check the attribute values to ensure no misspellings, impossible \nvalues, or multiple variations. BI applications and reports rely on the precise \ncontents of the dimension attributes. SQL will produce another line in a report \nif the attribute value varies in any way based on trivial punctuation or spell-\ning diff erences. You should ensure that the attribute values are completely \npopulated because missing values easily cause misinterpretations. Incomplete \nor poorly administered textual dimension attributes lead to incomplete or \npoorly produced reports.\n \n■Document the attribute deﬁ nitions, interpretations, and origins in the \nmetadata. Remember that the metadata is analogous to the DW/BI encyclo-\npedia. You must be vigilant about populating and maintaining the metadata \n repository.\n",
      "page_number": 193
    },
    {
      "number": 22,
      "title": "Segment 22 (pages 203-214)",
      "start_page": 203,
      "end_page": 214,
      "detection_method": "topic_boundary",
      "content": "Chapter 6\n174\nCustomer Dimension\nThe  customer dimension contains one row for each discrete location to which you \nship a product. Customer dimension tables can range from moderately sized (thou-\nsands of rows) to extremely large (millions of rows) depending on the nature of the \nbusiness. A typical customer dimension is shown in Figure 6-5.\nCustomer Key (PK)\nCustomer ID (Natural Key)\nCustomer Name\nCustomer Ship To Address\nCustomer Ship To City\nCustomer Ship To County\nCustomer Ship To City-State\nCustomer Ship To State\nCustomer Ship To ZIP\nCustomer Ship To ZIP Region\nCustomer Ship To ZIP Sectional Center\nCustomer Bill To Name\nCustomer Bill To Address\nCustomer Organization Name\nCustomer Corporate Parent Name\nCustomer Credit Rating\nCustomer Dimension\nFigure 6-5: Sample customer dimension.\nSeveral  independent hierarchies typically coexist in a customer dimension. The natu-\nral geographic hierarchy is clearly deﬁ ned by the ship-to location. Because the ship-to \nlocation is a point in space, any number of geographic hierarchies may be deﬁ ned by \nnesting more expansive geographic entities around the point. In the United States, \nthe usual geographic hierarchy is city, county, and state. It is often useful to include a \ncity-state attribute because the same city name exists in multiple states. The ZIP code \nidentiﬁ es a secondary geographic breakdown. The ﬁ rst digit of the ZIP code identiﬁ es \na geographic region of the United States (for example, 0 for the Northeast and 9 for \ncertain western states), whereas the ﬁ rst three digits of the ZIP code identify a mailing \nsectional center.\nAlthough these geographic characteristics may be captured and managed in a \nsingle master data management system, you should embed the attributes within \nthe respective dimensions rather than relying on an abstract, generic geography/\nlocation dimension that includes one row for every point in space independent of \nthe dimensions. We’ll talk more about this in Chapter 11: Telecommunications.\nAnother common hierarchy is the customer’s organizational hierarchy, assuming \nthe customer is a corporate entity. For each customer ship-to address, you might \nhave a customer bill-to and customer parent corporation. For every row in the \n\n\nOrder Management 175\ncustomer dimension, both the physical geographies and organizational affi  liation \nare well deﬁ ned, even though the hierarchies roll up diff erently.\nNOTE \nIt is natural and common, especially for customer-oriented dimensions, \nfor a dimension to simultaneously support multiple independent hierarchies. The \nhierarchies may have diff erent numbers of levels. Drilling up and drilling down \nwithin each of these hierarchies must be supported in a dimensional model.\nThe alert reader may have a concern with the implied assumption that multiple \nship-tos roll up to a single bill-to in a many-to-one relationship. The real world may \nnot be quite this clean and simple. There are always a few exceptions involving \nship-to addresses that are associated with more than one bill-to. Obviously, this \nbreaks the simple hierarchical relationship assumed in Figure 6-5. If this is a rare \noccurrence, it would be reasonable to generalize the customer dimension so that the \ngrain of the dimension is each unique ship-to/bill-to combination. In this scenario, \nif there are two sets of bill-to information associated with a given ship-to location, \nthen there would be two rows in the dimension, one for each combination. On the \nother hand, if many of the ship-tos are associated with many bill-tos in a robust \nmany-to-many relationship, then the ship-to and bill-to customers probably need to \nbe handled as separate dimensions that are linked together by the fact table. With \neither approach, exactly the same information is preserved. We’ll spend more time \non organizational hierarchies, including the handling of variable depth recursive \nrelationships, in Chapter 7: Accounting.\n Single Versus Multiple Dimension Tables\nAnother  potential hierarchy in the customer dimension might be the manufacturer’s \nsales organization. Designers sometimes question whether sales organization attri-\nbutes should be modeled as a separate dimension or added to the customer dimension. \nIf sales reps are highly correlated with customers in a one-to-one or many-to-one rela-\ntionship, combining the sales organization attributes with the customer attributes in \na single dimension is a viable approach. The resulting dimension is only as big as the \nlarger of the two dimensions. The relationships between sales teams and customers \ncan be browsed effi  ciently in the single dimension without traversing the fact table.\nHowever, sometimes the relationship between sales organization and customer \nis more complicated. The following factors must be taken into consideration:\n \n■Is the one-to-one or many-to-one relationship actually a many-to-many? \nAs we discussed earlier, if the many-to-many relationship is an exceptional \ncondition, then you may still be tempted to combine the sales rep attributes \ninto the customer dimension, knowing multiple surrogate keys are needed to \nhandle these rare many-to-many occurrences. However, if the many-to-many \n\n\nChapter 6\n176\nrelationship is the norm, you should handle the sales rep and customer as \nseparate dimensions.\n \n■Does the sales rep and customer relationship vary over time or under the \ninﬂ uence of another dimension? If so, you’d likely create separate dimensions \nfor the rep and customer.\n \n■Is the customer dimension extremely large? If there are millions of customer \nrows, you’d be more likely to treat the sales rep as a separate dimension rather \nthan forcing all sales rep analysis through a voluminous customer dimension.\n \n■Do the sales rep and customer dimensions participate independently in \nother fact tables? Again, you’d likely keep the dimensions separate. Creating a \nsingle customer dimension with sales rep attributes exclusively around order \ndata may cause users to be confused when they’re analyzing other processes \ninvolving sales reps.\n \n■Does the business think about the sales rep and customer as separate \nthings? This factor may be tough to discern and impossible to quantify. But \nthere’s no sense forcing two critical dimensions into a single blended dimen-\nsion if this runs counter to the business’s perspectives.\nWhen entities have a ﬁ xed, time-invariant, strongly correlated relationship, they \nshould be modeled as a single dimension. In most other cases, the design likely will \nbe simpler and more manageable when the entities are separated into two dimen-\nsions (while remembering the general guidelines concerning too many dimensions). \nIf you’ve already identiﬁ ed 25 dimensions in your schema, you should consider \ncombining dimensions, if possible.\nWhen the dimensions are separate, some designers want to create a little table \nwith just the two dimension keys to show the correlation without using the order \nfact table. In many scenarios, this two-dimension table is unnecessary. There is no \nreason to avoid the fact table to respond to this relationship inquiry. Fact tables are \nincredibly effi  cient because they contain only dimension keys and measurements, \nalong with the occasional degenerate dimension. The fact table is created speciﬁ cally \nto represent the correlations and many-to-many relationships between dimensions.\nAs we discussed in Chapter 5, you could capture the customer’s currently assigned \nsales rep by including the relevant descriptors as type 1 attributes. Alternatively, you \ncould use the slowly changing dimension (SCD) type 5 technique by embedding a \ntype 1 foreign key to a sales rep dimension outrigger within the customer dimen-\nsion; the current values could be presented as if they’re included on the customer \ndimension via a view declaration.\n Factless Fact Table for Customer/Rep Assignments\nBefore  we leave the topic of sales rep assignments to customers, users sometimes \nwant the ability to analyze the complex assignment of sales reps to customers over \n\n\nOrder Management 177\ntime, even if no order activity has occurred. In this case, you could construct a \nfactless fact table, as illustrated in Figure 6-6, to capture the sales rep coverage. \nThe coverage table would provide a complete map of the historical assignments of \nsales reps to customers, even if some of the assignments never resulted in a sale. \nThis factless fact table contains dual date keys for the eff ective and expiration dates \nof each assignment. The expiration date on the current rep assignment row would \nreference a special date dimension row that identiﬁ es a future, undetermined date.\nAssignment Effective Date Key (FK)\nAssignment Expiration Date Key (FK)\nSales Rep Key (FK)\nCustomer Key (FK)\nCustomer Assignment Counter (=1)\nSales Rep-Customer Assignment Fact\nDate Dimension (views for 2 roles)\nSales Rep Dimension\nCustomer Dimension\nFigure 6-6: Factless fact table for sales rep assignments to customers.\nYou may want to compare the assignments fact table with the order transactions \nfact table to identify rep assignments that have not yet resulted in order activity. You \nwould do so by leveraging SQL’s capabilities to perform set operations (for example, \nselecting all the reps in the coverage table and subtracting all the reps in the orders \ntable) or by writing a correlated subquery.\nDeal Dimension\nThe  deal dimension is similar to the promotion dimension from Chapter 3. The deal \ndimension describes the incentives off ered to customers that theoretically aff ect the \ncustomers’ desire to purchase products. This dimension is also sometimes referred \nto as the contract. As shown in Figure 6-7, the deal dimension describes the full \ncombination of terms, allowances, and incentives that pertain to the particular \norder line item.\nDeal Key (PK)\nDeal ID (NK)\nDeal Description\nDeal Terms Description\nDeal Terms Type Description\nAllowance Description\nAllowance Type Description\nSpecial Incentive Description\nSpecial Incentive Type Description\nLocal Budget Indicator\nDeal Dimension\nFigure 6-7: Sample deal dimension.\n\n\nChapter 6\n178\nThe same issues you faced in the retail promotion dimension also arise with this \ndeal dimension. If the terms, allowances, and incentives are usefully correlated, it \nmakes sense to package them into a single deal dimension. If the terms, allowances, \nand incentives are quite uncorrelated and you end up generating the Cartesian \nproduct of these factors in the dimension, it probably makes sense to split the deal \ndimension into its separate components. Again, this is not an issue of gaining or \nlosing information because the schema contains the same information in both cases. \nThe issues of user convenience and administrative complexity determine whether \nto represent these deal factors as multiple dimensions. In a very large fact table, \nwith hundreds of millions or billions of rows, the desire to reduce the number of \nkeys in the fact table composite key favors treating the deal attributes as a single \ndimension, assuming this meshes with the business users’ perspectives. Certainly \nany deal dimension smaller than 100,000 rows would be tractable in this design.\n Degenerate Dimension for Order Number\nEach  line item row in the order fact table includes the order number as a degenerate \ndimension. Unlike an operational header/line or parent/child database, the order \nnumber in a dimensional model is typically not tied to an order header table. You \ncan triage all the interesting details from the order header into separate dimensions \nsuch as the order date and customer ship-to. The order number is still useful for \nseveral reasons. It enables you to group the separate line items on the order and \nanswer questions such as “What is the average number of line items on an order?” \nThe order number is occasionally used to link the data warehouse back to the \noperational world. It may also play a role in the fact table’s primary key. Because \nthe order number sits in the fact table without joining to a dimension table, it is a \ndegenerate dimension.\nNOTE \nDegenerate dimensions typically are reserved for operational transaction \nidentiﬁ ers. They should not be used as an excuse to stick cryptic codes in the fact \ntable without joining to dimension tables for descriptive decodes.\nAlthough there is likely no analytic purpose for the order transaction line num-\nber, it may be included in the fact table as a second degenerate dimension given its \npotential role in the primary key, along with the linkage to the operational system \nof record. In this case, the primary key for the line item grain fact table would be \nthe order number and line number.\nSometimes data elements belong to the order itself and do not naturally fall into \nother dimension tables. In this situation, the order number is no longer a degenerate \ndimension but is a standard dimension with its own surrogate key and attributes. \n\n\nOrder Management 179\nHowever, designers with a strong operational background should resist the urge to \nsimply dump the traditional order header information into an order dimension. In \nalmost all cases, the header information belongs in other analytic dimensions that \ncan be associated with the line item grain fact table rather than merely being cast \noff  into a dimension that closely resembles the operational order header record.\n Junk Dimensions\nWhen  modeling complex transactional source data, you often encounter a number \nof miscellaneous indicators and ﬂ ags that are populated with a small range of dis-\ncrete values. You have several rather unappealing options for handling these low \ncardinality ﬂ ags and indicators, including:\n \n■Ignore the ﬂ ags and indicators. You can ask the obligatory question about \neliminating these miscellaneous ﬂ ags because they seem rather insigniﬁ cant, \nbut this notion is often vetoed quickly because someone occasionally needs \nthem. If the indicators are incomprehensible or inconsistently populated, \nperhaps they should be left out.\n \n■Leave the ﬂ ags and indicators unchanged on the fact row. You don’t want \nto store illegible cryptic indicators in the fact table. Likewise, you don’t \nwant to store bulky descriptors on the fact row, which would cause the \ntable to swell alarmingly. It would be a shame to leave a handful of textual \nindicators on the row.\n \n■Make each ﬂ ag and indicator into its own dimension. Adding separate foreign \nkeys to the fact table is acceptable if the resulting number of foreign keys is \nstill reasonable (no more than 20 or so). However, if the list of foreign keys \nis already lengthy, you should avoid adding more clutter to the fact table.\n \n■Store the ﬂ ags and indicators in an order header dimension. Rather than \ntreating the order number as a degenerate dimension, you could make it a \nregular dimension with the low cardinality ﬂ ags and indicators as attributes. \nAlthough this approach accurately represents the data relationships, it is ill-\nadvised, as described below.\nAn  appropriate alternative approach for tackling these ﬂ ags and indicators is to \nstudy them carefully and then pack them into one or more junk dimensions. A junk \ndimension is akin to the junk drawer in your kitchen. The kitchen junk drawer is a \ndumping ground for miscellaneous household items, such as rubber bands, paper \nclips, batteries, and tape. Although it may be easier to locate the rubber bands if \na separate kitchen drawer is dedicated to them, you don’t have adequate storage \ncapacity to do so. Besides, you don’t have enough stray rubber bands, nor do you \nneed them frequently, to warrant the allocation of a single-purpose storage space. \n\n\nChapter 6\n180\nThe junk drawer provides you with satisfactory access while still retaining storage \nspace for the more critical and frequently accessed dishes and silverware. In the \ndimensional modeling world, the junk dimension nomenclature is reserved for DW/\nBI professionals. We typically refer to the junk dimension as a transaction indicator \nor transaction proﬁ le dimension when talking with the business users.\nNOTE \nA junk dimension is a grouping of low-cardinality ﬂ ags and indicators. \nBy creating a junk dimension, you remove the ﬂ ags from the fact table and place \nthem into a useful dimensional framework.\nIf a single junk dimension has 10 two-value indicators, such as cash versus credit \npayment type, there would be a maximum of 1,024 (210) rows. It probably isn’t \ninteresting to browse among these ﬂ ags within the dimension because every ﬂ ag \nmay occur with every other ﬂ ag. However, the junk dimension is a useful holding \nplace for constraining or reporting on these ﬂ ags. The fact table would have a single, \nsmall surrogate key for the junk dimension.\nOn the other hand, if you have highly uncorrelated attributes that take on more \nnumerous values, it may not make sense to lump them together into a single junk \ndimension. Unfortunately, the decision is not entirely formulaic. If you have ﬁ ve \nindicators that each take on only three values, a single junk dimension is the best \nroute for these attributes because the dimension has only 243 (35) possible rows. \nHowever, if the ﬁ ve uncorrelated indicators each have 100 possible values, we’d \nsuggest creating separate dimensions because there are now 100 million (1005) \npossible combinations.\nFigure 6-8 illustrates sample rows from an order indicator dimension. A subtle \nissue regarding junk dimensions is whether you should create rows for the full \nCartesian product of all the combinations beforehand or create junk dimension \nrows for the combinations as you encounter them in the data. The answer depends \non how many possible combinations you expect and what the maximum number \ncould be. Generally, when the number of theoretical combinations is high and you \ndon’t expect to encounter them all, you build a junk dimension row at extract time \nwhenever you encounter a new combination of ﬂ ags or indicators.\nNow that junk dimensions have been explained, contrast them to the handling \nof the ﬂ ags and indicators as attributes in an order header dimension. If you want \nto analyze order facts where the order type is Inbound (refer to Figure 6-8’s junk \ndimension rows), the fact table would be constrained to order indicator key equals \n1, 2, 5, 6, 9, 10, and probably a few others. On the other hand, if these attributes \nwere stored in an order header dimension, the constraint on the fact table would be \nan enormous list of all order numbers with an inbound order type.\n\n\nOrder Management 181\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n12\n11\nCash\nCash\nCash\nCash\nVisa\nVisa\nVisa\nVisa\nMasterCard\nMasterCard\nMasterCard\nMasterCard\nCash\nCash\nCash\nCash\nCredit\nCredit\nCredit\nCredit\nCredit\nCredit\nCredit\nCredit\nInbound\nInbound\nOutbound\nOutbound\nInbound\nInbound\nOutbound\nOutbound\nInbound\nInbound\nOutbound\nOutbound\nCommissionable\nNon-Commissionable\nCommissionable\nNon-Commissionable\nCommissionable\nNon-Commissionable\nCommissionable\nNon-Commissionable\nCommissionable\nNon-Commissionable\nCommissionable\nNon-Commissionable\nOrder\nIndicator Key\nPayment Type\nDescription\nPayment Type\nGroup\nOrder Type\nCommission Credit\nIndicator\nFigure 6-8: Sample rows of order indicator junk dimension.\n Header/Line Pattern to Avoid\nThere  are two common design mistakes to avoid when you model header/line data \ndimensionally. Unfortunately, both of these patterns still accurately represent the \ndata relationships, so they don’t stick out like a sore thumb. Perhaps equally unfor-\ntunate is that both patterns often feel more comfortable to data modelers and ETL \nteam members with signiﬁ cant transaction processing experience than the patterns \nwe advocate. We’ll discuss the ﬁ rst common mistake here; the other is covered in \nthe section “Another Header/Line Pattern to Avoid.”\nFigure 6-9 illustrates a header/line modeling pattern we frequently observe when \nconducting design reviews. In this example, the operational order header is virtually \nreplicated in the dimensional model as a dimension. The header dimension contains \nall the data from its operational equivalent. The natural key for this dimension is \nthe order number. The grain of the fact table is one row per order line item, but \nthere’s not much dimensionality associated with it because most descriptive context \nis embedded in the order header dimension.\nAlthough  this design accurately represents the header/line relationship, there are \nobvious ﬂ aws. The order header dimension is likely very large, especially relative to \nthe fact table itself. If there are typically ﬁ ve line items per order, the dimension is \n20 percent as large as the fact table; there should be orders of magnitude diff erences \nbetween the size of a fact table and its associated dimensions. Also, dimension tables \ndon’t normally grow at nearly the same rate as the fact table. With this design, you \nwould add one row to the dimension table and an average of ﬁ ve rows to the fact \ntable for every new order. Any analysis of the order’s interesting characteristics, \n\n\nChapter 6\n182\nsuch as the customer, sales rep, or deal involved, would need to traverse this large \ndimension  table.\nOrder Number (PK)\nOrder Date\nOrder Month\n...\nRequested Ship Date\nRequested Ship Month\n...\nCustomer ID\nCustomer Name\n...\nSales Rep Number\nSales Rep Name\n...\nDeal ID\nDeal Description\n...\nOrder Number (FK)\nProduct Key (FK)\nOrder Line Number (DD)\nOrder Line Quantity\nExtended Order Line Gross Dollar Amount\nExtended Order Line Discount Dollar Amount\nExtended Order Line Net Dollar Amount\n1 row per Order Header\nOrder Line Transaction Fact\nProduct Dimension\nOrder Header Dimension\n1 row per Order Line\nFigure 6-9: Pattern to avoid: treating transaction header as a dimension.\n Multiple Currencies\nSuppose  you track the orders of a large multinational U.S.-based company with sales \noffi  ces around the world. You may be capturing order transactions in more than \n15 diff erent currencies. You certainly wouldn’t want to include columns in the fact \ntable for each currency.\nThe most common analytic requirement is that order transactions be expressed \nin both the local transaction currency and the standardized corporate currency, \nsuch as U.S. dollars in this example. To satisfy this need, each order fact would be \nreplaced with a pair of facts, one for the applicable local currency and another for \nthe equivalent standard corporate currency, as illustrated in Figure 6-10. The con-\nversion rate used to construct each fact row with the dual metrics would depend \non the business’s requirements. It might be the rate at the moment the order was \ncaptured, an end of day rate, or some other rate based on deﬁ ned business rules. This \ntechnique would preserve the transactional metrics, plus allow all transactions to \neasily roll up to the corporate currency without complicated reporting application \ncoding. The metrics in standard currency would be fully additive. The local currency \nmetrics would be additive only for a single speciﬁ ed currency; otherwise, you’d be \ntrying to sum Japanese yen, Thai bhat, and British pounds. You’d also supplement \n\n\nOrder Management 183\nthe fact table with a currency dimension to identify the currency type associated \nwith the local currency facts; a currency dimension is needed even if the location \nof the transaction is otherwise known because the location does not necessarily \nguarantee which currency was used.\nOrder Date Key (FK)\nRequested Ship Date Key (FK)\nCustomer Key (FK)\nProduct Key (FK)\nSales Rep Key (FK)\nDeal Key (FK)\nLocal Currency Dimension Key (FK) \nOrder Number (DD)\nOrder Line Number (DD)\nOrder Line Quantity\nExtended Order Line Gross USD Amount\nExtended Order Line Discount USD Amount\nExtended Order Line Net USD Amount\nExtended Order Line Gross Local Currency Amount\nExtended Order Line Discount Local Currency Amount\nExtended Order Line Net Local Currency Amount\nLocal Currency Key (PK)\nLocal Currency Name\nLocal Currency Abbreviation\nLocal Currency Dimension\nOrder Line Transaction Fact\nFigure 6-10: Metrics in multiple currencies within the fact table.\nThis technique can be expanded to support other relatively common examples. \nIf the business’s sales offi  ces roll up into a handful of regional centers, you could \nsupplement the fact table with a third set of metrics representing the transactional \namounts converted into the appropriate regional currency. Likewise, the fact table \ncolumns could represent currencies for the customer ship-to and customer bill-to, \nor the currencies as quoted and shipped.\nIn each of the scenarios, the fact table could physically contain a full set of metrics \nin one currency, along with the appropriate currency conversion rate(s) for that row. \nRather than burdening the business users with appropriately multiplying or divid-\ning by the stored rate, the intra-row extrapolation should be done in a view behind \nthe scenes; all reporting applications would access the facts via this logical layer.\nSometimes the multi-currency support requirements are more complicated than \njust described. You may need to allow a manager in any country to see order volume \nin any currency. In this case, you can embellish the initial design with an additional \ncurrency conversion fact table, as shown in Figure 6-11. The dimensions in this \nfact table represent currencies, not countries, because the relationship between \ncurrencies and countries is not one-to-one. The more common needs of the local \nsales rep and sales management in headquarters would be met simply by querying \nthe orders fact table, but those with less predictable requirements would use the \n\n\nChapter 6\n184\ncurrency conversion table in a specially crafted query. Navigating the currency \nconversion table is obviously more complicated than using the converted metrics \non the orders fact table.\nCurrency Conversion Fact\nConversion Date Key (FK)\nSource Currency Key (FK)\nDestination Currency Key (FK)\nSource-Destination Exchange Rate\nDestination-Source Exchange Rate\nFigure 6-11: Tracking multiple currencies with daily currency exchange fact table.\nWithin each currency conversion fact table row, the amount expressed in local \ncurrency is absolutely accurate because the sale occurred in that currency on that \nday. The equivalent U.S. dollar value would be based on a conversion rate to U.S. \ndollars for that day. The conversion rate table contains the combinations of rel-\nevant currency exchange rates going in both directions because the symmetric \nrates between two currencies are not equal. It is unlikely this conversion fact table \nneeds to include the full Cartesian product of all possible currency combinations. \nAlthough there are approximately 100 unique currencies globally, there wouldn’t \nneed to be 10,000 daily rows in this currency fact table as there’s not a meaningful \nmarket for every possible pair; likewise, all theoretical combinations are probably \noverkill for the business users.\nThe use of a currency conversion table may also be required to support the busi-\nness’s need for multiple rates, such as an end of month or end of quarter close rate, \nwhich may not be deﬁ ned until long after the transactions have been loaded into \nthe orders fact table.\n Transaction Facts at Different Granularity\nIt  is quite common in header/line operational data to encounter facts of diff ering \ngranularity. On an order, there may be a shipping charge that applies to the entire \norder. The designer’s ﬁ rst response should be to try to force all the facts down to \nthe lowest level, as illustrated in Figure 6-12. This procedure is broadly referred \nto as allocating. Allocating the parent order facts to the child line item level is \ncritical if you want the ability to slice and dice and roll up all order facts by all \ndimensions, including product.\nUnfortunately,  allocating header-level facts down to the line item level may entail \na political wrestling match. It is wonderful if the entire allocation issue is handled by \nthe ﬁ nance department, not by the DW/BI team. Getting organizational agreement \non allocation rules is often a controversial and complicated process. The DW/BI team \n\n\nOrder Management 185\nshouldn’t be distracted and delayed by the inevitable organizational negotiation. \nFortunately, in many companies, the need to rationally allocate costs has already \nbeen recognized. A task force, independent of the DW/BI project, already may have \nestablished activity-based costing measures. This is just another name for allocating.\nAllocated\nOrder Header Transaction Fact\nOrder Date Key (FK)\nRequested Ship Date Key (FK)\nCustomer Key (FK)\nSales Rep Key (FK)\nDeal Key (FK)\nOrder Number (PK)\nOrder Shipping Charges Dollar Amount\nOrder Line Transaction Fact\nOrder Date Key (FK)\nRequested Ship Date Key (FK)\nCustomer Key (FK)\nProduct Key (FK)\nSales Rep Key (FK)\nDeal Key (FK)\nOrder Number (DD)\nOrder Line Number (DD)\nOrder Line Quantity\nExtended Order Line Gross Dollar Amount\nExtended Order Line Discount Dollar Amount\nExtended Order Line Net Dollar Amount\nAllocated Order Line Shipping Charges Dollar Amount\nFigure 6-12: Allocating header facts to line items.\nIf the shipping charges and other header-level facts cannot be successfully allo-\ncated, they must be presented in an aggregate table for the overall order. We clearly \nprefer the allocation approach, if possible, because the separate higher-level fact \ntable has some inherent usability issues. Without allocations, you cannot explore \nheader facts by product because the product isn’t identiﬁ ed in a header-grain fact \ntable. If you are successful in allocating facts down to the lowest level, the problem \ngoes away.\nWARNING \nYou shouldn’t mix fact granularities such as order header and order \nline facts within a single fact table. Instead, either allocate the higher-level facts \nto a more detailed level or create two separate fact tables to handle the diff erently \ngrained facts. Allocation is the preferred approach. \nOptimally, the business data stewards obtain enterprise consensus on the allocation \nrules. But sometimes organizations refuse to agree. For example, the ﬁ nance depart-\nment may want to allocate the header freight charged based on the extended gross \norder amount on each line; meanwhile, the logistics group wants the freight charge \nto be allocated based on the weight of the line’s products. In this case, you would \nhave two allocated freight charges on every order line fact table row; the uniquely \ncalculated metrics would also be uniquely labeled. Obviously, agreeing on a single, \nstandard allocation scheme is preferable.\n",
      "page_number": 203
    },
    {
      "number": 23,
      "title": "Segment 23 (pages 215-225)",
      "start_page": 215,
      "end_page": 225,
      "detection_method": "topic_boundary",
      "content": "Chapter 6\n186\nDesign teams sometimes attempt to devise alternative techniques for handling \nheader/line facts at diff erent granularity, including the following:\n \n■Repeat the unallocated header fact on every line. This approach is fraught \nwith peril given the risk of overstating the header amount when it’s summed \non every line.\n \n■Store the unallocated amount on the transaction’s ﬁ rst or last line. This tac-\ntic eliminates the risk of overcounting, but if the ﬁ rst or last lines are excluded \nfrom the query results due to a ﬁ lter constraint on the product dimension, it \nappears there were no header facts associated with this transaction.\n \n■Set up a special product key for the header fact. Teams who adopt this \napproach sometimes recycle an existing line fact column. For example, if \nproduct key = 99999, then the gross order metric is a header fact, like the \nfreight charge. Dimensional models should be straightforward and legible. You \ndon’t want to embed complexities requiring a business user to wear a special \ndecoder ring to navigate the dimensional model successfully.\nAnother Header/Line Pattern to Avoid\n The  second header/line pattern to avoid is illustrated in Figure 6-13. In this example, \nthe order header is no longer treated as a monolithic dimension but as a fact table \ninstead. The header’s associated descriptive information is grouped into dimen-\nsions surrounding the order fact. The line item fact table (identical in structure and \ngranularity as the ﬁ rst diagram) joins to the header fact based on the order number.\nOrder Date Dimension\nRequested Ship Date Dimension\nDeal Dimension\nOrder Header Transaction Fact\nCustomer Dimension\nSales Rep Dimension\nProduct Dimension\nOrder Date Key (FK)\nRequested Ship Date Key (FK)\nCustomer Key (FK)\nSales Rep Key (FK)\nDeal Key (FK)\nOrder Number (PK)\nExtended Order Total Gross Dollar Amount\nExtended Order Total Discount Dollar Amount\nExtended Order Total Net Dollar Amount\nOrder Total Shipping Charges Dollar Amount\nOrder Line Transaction Fact\nOrder Number (FK)\nOrder Line Number (DD)\nProduct Key (FK)\nOrder Line Quantity\nExtended Order Line Gross Dollar Amount\nExtended Order Line Discount Dollar Amount\nExtended Order Line Net Dollar Amount\nFigure 6-13: Pattern to avoid: not inheriting header dimensionality in line facts.\n\n\nOrder Management 187\nAgain, this design accurately represents the parent/child relationship of the order \nheader and line items, but there are still ﬂ aws. Every time the user wants to slice \nand dice the line facts by any of the header attributes, a large header fact table needs \nto be associated with an even larger line fact table. \nInvoice Transactions\nIn  a manufacturing company, invoicing typically occurs when products are shipped \nfrom your facility to the customer. Visualize shipments at the loading dock as boxes \nof product are placed into a truck destined for a particular customer address. The \ninvoice associated with the shipment is created at this time. The invoice has mul-\ntiple line items, each corresponding to a particular product being shipped. Various \nprices, discounts, and allowances are associated with each line item. The extended \nnet amount for each line item is also available.\nAlthough you don’t show it on the invoice to the customer, a number of other \ninteresting facts are potentially known about each product at the time of shipment. \nYou certainly know list prices; manufacturing and distribution costs may be avail-\nable as well. Thus you know a lot about the state of your business at the moment \nof customer invoicing.\nIn  the invoice fact table, you can see all the company’s products, customers, \ncontracts and deals, off -invoice discounts and allowances, revenue generated by \ncustomers, variable and ﬁ xed costs associated with manufacturing and delivering \nproducts (if available), money left over after delivery of product (proﬁ t contribution), \nand customer satisfaction metrics such as on-time shipment.\nNOTE \nFor any company that ships products to customers or bills customers \nfor services rendered, the optimal place to start a DW/BI project typically is with \ninvoices. We often refer to invoicing as the most powerful data because it combines \nthe company’s customers, products, and components of proﬁ tability.\nYou should choose the grain of the invoice fact table to be the individual invoice \nline item. A sample invoice fact table associated with manufacturer shipments is \nillustrated in Figure 6-14.\nAs expected, the invoice fact table contains a number of dimensions from earlier \nin this chapter. The conformed date dimension table again would play multiple \nroles in the fact table. The customer, product, and deal dimensions also would \nconform, so you can drill across fact tables using common attributes. If a single \norder number is associated with each invoice line item, it would be included as a \nsecond degenerate dimension.\n\n\nChapter 6\n188\nDate Dimension (views for 3 roles)\nProduct Dimension\nDeal Dimension\nShipper Dimension\nShipment Invoice Line Transaction Fact\nCustomer Dimension\nSales Rep Dimension\nWarehouse Dimension\nService Level Dimension\nInvoice Date Key (FK)\nRequested Ship Date Key (FK)\nActual Ship Date Key (FK)\nCustomer Key (FK)\nProduct Key (FK)\nSales Rep Key (FK)\nDeal Key (FK)\nWarehouse Key (FK)\nShipper Key (FK)\nService Level Key (FK)\nInvoice Number (DD)\nInvoice Line Number (DD)\nInvoice Line Quantity\nExtended Invoice Line Gross Dollar Amount\nExtended Invoice Line Allowance Dollar Amount\nExtended Invoice Line Discount Dollar Amount\nExtended Invoice Line Net Dollar Amount\nExtended Invoice Line Fixed Mfg Cost Dollar Amount\nExtended Invoice Line Variable Mfg Cost Dollar Amount\nExtended Invoice Line Storage Cost Dollar Amount\nExtended Invoice Line Distribution Cost Dollar Amount\nExtended Invoice Line Contribution Dollar Amount\nShipment On-Time Counter\nRequested to Actual Ship Lag\nFigure 6-14: Shipment invoice fact table.\nThe  shipment invoice fact table also contains some interesting new dimensions. \nThe warehouse dimension contains one row for each manufacturer warehouse loca-\ntion. This is a relatively simple dimension with name, address, contact person, and \nstorage facility type. The attributes are somewhat reminiscent of the store dimension \nfrom Chapter 3. The shipper dimension describes the method and carrier by which \nthe product was shipped from the manufacturer to the customer. \n Service Level Performance as Facts, \nDimensions, or Both\nThe  fact table in Figure 6-14 includes several critical dates intended to capture \nshipment service levels. All these dates are known when the operational invoicing \nprocess occurs. Delivering the multiple event dates in the invoicing fact table with \ncorresponding role-playing date dimensions allows business users to ﬁ lter, group, \nand trend on any of these dates. But sometimes the business requirements are more \ndemanding.\nYou could include an additional on-time counter in the fact table that’s set to an \nadditive zero or one depending on whether the line shipped on time. Likewise, you \ncould include lag metrics representing the number of days, positive or negative, \nbetween the requested and actual ship dates. As described later in this chapter, the \nlag calculation may be more sophisticated than the simple diff erence between dates.\n\n\nOrder Management 189\nIn addition to the quantitative service metrics, you could also include a qualita-\ntive assessment of performance by adding either a new dimension or adding more \ncolumns to the junk dimension. Either way, the attribute values might look similar \nto those shown in Figure 6-15.\nOn-time\nEarly\nEarly\nEarly\nToo early\nLate\nLate\nLate\nToo late\n1 \n2 \n3 \n4 \n5 \n6\n7 \n8 \n9\nOn-time\n1 day early\n2 days early\n3 days early\n> 3 days early\n1 day late\n2 days late\n3 days late\n> 3 days late\nService Level\nKey\nService Level\nDescription\nService Level\nGroup\nFigure 6-15: Sample qualitative service level descriptors.\nIf service level performance at the invoice line is closely watched by business \nusers, you may embrace all the patterns just described, since quantitative metrics \nwith qualitative text provide diff erent perspectives on the same performance.\n Proﬁ t and Loss Facts\nIf  your organization has tackled activity-based costing or implemented a robust \nenterprise resource planning (ERP) system, you might be in a position to identify \nmany of the incremental revenues and costs associated with shipping ﬁ nished prod-\nucts to the customer. It is traditional to arrange these revenues and costs in sequence \nfrom the top line, which represents the undiscounted value of the products shipped \nto the customer, down to the bottom line, which represents the money left over after \ndiscounts, allowances, and costs. This list of revenues and costs is referred to as a \nproﬁ t and loss (P&L) statement. You typically don’t attempt to carry it all the way \nto a complete view of company proﬁ t including general and administrative costs. \nFor this reason, the bottom line in the P&L statement is referred to as contribution.\nKeeping in mind that each row in the invoice fact table represents a single line \nitem on the invoice, the elements of the P&L statement shown in Figure 6-14 have \nthe following interpretations:\n \n■Quantity shipped: Number of cases of the particular line item’s product. \nThe use of multiple equivalent quantities with diff erent units of measure is \ndiscussed in the section “Multiple Units of Measure.”\n \n■Extended gross amount:  Also known as extended list price because it is the \nquantity shipped multiplied by the list unit price. This and all subsequent \n\n\nChapter 6\n190\ndollar values are extended amounts or, in other words, unit rates multiplied by \nthe quantity shipped. This insistence on additive values simpliﬁ es most access \nand reporting applications. It is relatively rare for a business user to ask for \nthe unit price from a single fact table row. When the user wants an average \nprice drawn from many rows, the extended prices are ﬁ rst added, and then \nthe result is divided by the sum of the quantities.\n \n■Extended allowance amount:  Amount subtracted from the invoice line gross \namount for deal-related allowances. The allowances are described in the \nadjoined deal dimension. The allowance amount is often called an off -invoice \nallowance. The actual invoice may have several allowances for a given line \nitem; the allowances are combined together in this simpliﬁ ed example. If \nthe allowances need to be tracked separately and there are potentially many \nsimultaneous allowances on a given line item, an allowance detail fact table \ncould augment the invoice line fact table, serving as a drill-down for details \non the allowance total in the invoice line fact table.\n \n■Extended discount amount:  Amount subtracted for volume or payment term \ndiscounts. The discount descriptions are found in the deal dimension. As \ndiscussed earlier regarding the deal dimension, the decision to describe the \nallowances and discount types together is the designer’s prerogative. It makes \nsense to do this if allowances and discounts are correlated and business users \nwant to browse within the deal dimension to study the relationships between \nallowances and discounts. \nAll allowances and discounts in this fact table are represented at the line \nitem level. As discussed earlier, some allowances and discounts may be cal-\nculated operationally at the invoice level, not at the line item level. An effort \nshould be made to allocate them down to the line item. An invoice P&L state-\nment that does not include the product dimension poses a serious limitation \non your ability to present meaningful contribution slices of the business.\n \n■Extended net amount:  Amount the customer is expected to pay for this line \nitem before tax. It is equal to the gross invoice amount less the allowances \nand discounts.\nThe facts described so far likely would be displayed to the customer on the invoice \ndocument. The following cost amounts, leading to a bottom line contribution, are \nfor internal consumption only.\n \n■Extended ﬁ xed manufacturing cost:  Amount identiﬁ ed by manufacturing as \nthe pro rata ﬁ xed manufacturing cost of the invoice line’s product.\n \n■Extended variable manufacturing cost:  Amount identiﬁ ed by manufacturing \nas the variable manufacturing cost of the product on the invoice line. This \namount may be more or less activity-based, reﬂ ecting the actual location and \n\n\nOrder Management 191\ntime of the manufacturing run that produced the product being shipped to \nthe customer. Conversely, this number may be a standard value set by a com-\nmittee. If the manufacturing costs or any of the other storage and distribution \ncosts are averages of averages, the detailed P&Ls may become meaningless. \nThe DW/BI system may illuminate this problem and accelerate the adoption \nof activity-based costing methods.\n \n■Extended storage cost:  Cost charged to the invoice line for storage prior to \nbeing shipped to the customer.\n \n■Extended distribution cost:  Cost charged to the invoice line for transportation \nfrom the point of manufacture to the point of shipment. This cost is notori-\nous for not being activity-based. The distribution cost possibly can include \nfreight to the customer if the company pays the freight, or the freight cost \ncan be presented as a separate line item in the P&L.\n \n■Contribution amount:  Extended net invoice less all the costs just discussed. This \nis not the true bottom line of the overall company because general and admin-\nistrative expenses and other ﬁ nancial adjustments have not been made, but it is \nimportant nonetheless. This column sometimes has alternative labels, such as \nmargin, depending on the company culture.\nYou should step back and admire the robust dimensional model you just built. \nYou constructed a detailed P&L view of your business, showing all the activity-based \nelements of revenue and costs. You have a full equation of proﬁ tability. However, \nwhat makes this design so compelling is that the P&L view sits inside a rich dimen-\nsional framework of dates, customers, products, and causal factors. Do you want \nto see customer proﬁ tability? Just constrain and group on the customer dimension \nand bring the components of the P&L into the report. Do you want to see product \nproﬁ tability? Do you want to see deal proﬁ tability? All these analyses are equally \neasy and take the same analytic form in the BI applications. Somewhat tongue in \ncheek, we recommend you not deliver this dimensional model too early in your \ncareer because you will get promoted and won’t be able to work directly on any \nmore DW/BI systems!\nProﬁ tability Words of Warning\nWe  must balance the last paragraph with a more sober note and pass along some \ncautionary words of warning. It goes without saying that most of the business users \nprobably are very interested in granular P&L data that can be rolled up to analyze \ncustomer and product proﬁ tability. The reality is that delivering these detailed \nP&L statements often is easier said than done. The problems arise with the cost \nfacts. Even with advanced ERP implementations, it is fairly common to be unable \nto capture the cost facts at this atomic level of granularity. You will face a complex \nprocess of mapping or allocating the original cost data down to the invoice line \n\n\nChapter 6\n192\nlevel. Furthermore, each type of cost may require a separate extraction from a \nsource system. Ten cost facts may mean 10 diff erent extract and transformation \nprograms. Before signing up for mission impossible, be certain to perform a detailed \nassessment of what is available and feasible from the source systems. You certainly \ndon’t want the DW/BI team saddled with driving the organization to consensus on \nactivity-based costing as a side project, on top of managing a number of parallel \nextract implementations. If time and organization patience permits, proﬁ tability is \noften tackled as a consolidated dimensional model after the components of revenue \nand cost have been sourced and delivered separately to business users in the DW/\nBI environment.\n Audit Dimension\nAs  mentioned, Figure 6-14’s invoice line item design is one of the most powerful \nbecause it provides a detailed look at customers, products, revenues, costs, and \nbottom line proﬁ t in one schema. During the building of rows for this fact table, \na wealth of interesting back room metadata is generated, including data quality \nindicators, unusual processing requirements, and environment version numbers \nthat identify how the data was processed during the ETL. Although this metadata is \nfrequently of interest to ETL developers and IT management, there are times when \nit can be interesting to the business users, too. For instance, business users might \nwant to ask the following:\n \n■What is my conﬁ dence in these reported numbers?\n \n■Were there any anomalous values encountered while processing this source \ndata?\n \n■What version of the cost allocation logic was used when calculating the costs?\n \n■What version of the foreign currency conversion rules was used when calcu-\nlating the revenues?\nThese kinds of questions are often hard to answer because the metadata required \nis not readily available. However, if you anticipate these kinds of questions, you can \ninclude an audit dimension with any fact table to expose the metadata context that \nwas true when the fact table rows were built. Figure 6-16 illustrates an example \naudit dimension.\nThe audit dimension is added to the fact table by including an audit dimension \nforeign key. The audit dimension itself contains the metadata conditions encountered \nwhen processing fact table rows. It is best to start with a modest audit dimension \ndesign, such as shown in Figure 6-16, both to keep the ETL processing from getting \ntoo complicated and to limit the number of possible audit dimension rows. The ﬁ rst \nthree attributes (quality indicator, out of bounds indicator, and amount adjusted ﬂ ag) \nare all sourced from a special ETL processing table called the error event table, which \n\n\nOrder Management 193\nis discussed in Chapter 19: ETL Subsystems and Techniques. The cost allocation and \nforeign currency versions are environmental variables that should be available in an \nETL back room status table.\nDate Dimension (views for 3 roles)\nProduct Dimension\nDeal Dimension\nShipper Dimension\nShipment Invoice Line Transaction Fact\nCustomer Dimension\nSales Rep Dimension\nWarehouse Dimension\nService Level Dimension\nInvoice Date Key (FK)\nRequested Ship Date Key (FK)\nActual Ship Date Key (FK)\nCustomer Key (FK)\nProduct Key (FK)\nSales Rep Key (FK)\nDeal Key (FK)\nWarehouse Key (FK)\nShipper Key (FK)\nService Level Key (FK)\nAudit Key (FK)\nInvoice Number (DD)\nInvoice Line Number (DD)\nInvoice Line Quantity\nExtended Invoice Line Gross Dollar Amount\n...\nAudit Dimension\nAudit Key (PK)\nQuality Indicator\nOut of Bounds Indicator\nAmount Adjusted Flag\nCost Allocation Version\nForeign Currency Version\nFigure 6-16: Sample audit dimension included on invoice fact table.\nArmed with the audit dimension, some powerful queries can be performed. You \nmight want to take this morning’s invoice report and ask if any of the reported \nnumbers were based on out-of-bounds measures. Because the audit dimension is \nnow just an ordinary dimension, you can just add the out-of-bounds indicator to \nyour standard report. In the resulting “instrumented” report shown in Figure 6-17, \nyou see multiple rows showing normal and abnormal out-of-bounds results.\nStandard Report:\nAxon\nAxon\nEast\nWest\n1,438 \n2,249\n235,000 \n480,000\nProduct\nWarehouse\nInvoice Line\nQuantity\nExtended Invoice Line\nGross Amount\nInstrumented Reported (with Out of Bounds Indicator added):\nAxon\nAxon\nAxon\nAxon\nEast\nEast\nWest\nWest\nAbnormal\nNormal\nAbnormal\nNormal\n14 \n1,424 \n675 \n1,574\nProduct\nWarehouse\nOut of Bounds\nIndicator\nInvoice Line\nQuantity\n2,350 \n232,650 \n144,000 \n336,000\nExtended Invoice Line\nGross Amount\nFigure 6-17: Audit dimension attribute included on standard report.\n\n\nChapter 6\n194\n Accumulating Snapshot for Order \nFulﬁ llment Pipeline\nThe  order management process can be thought of as a pipeline, especially in a \nbuild-to-order manufacturing business, as illustrated in Figure 6-18. Customers \nplace an order that goes into the backlog until it is released to manufacturing to be \nbuilt. The manufactured products are placed in ﬁ nished goods inventory and then \nshipped to the customers and invoiced. Unique transactions are generated at each \nspigot of the pipeline. Thus far we’ve considered each of these pipeline activities as \na separate transaction fact table. Doing so allows you to decorate the detailed facts \ngenerated by each process with the greatest number of detailed dimensions. It also \nallows you to isolate analysis to the performance of a single business process, which \nis often precisely what the business users want.\nOrders\nBacklog\nShipment\nInvoicing\nRelease to\nManufacturing\nFinished\nGoods\nInventory\nFigure 6-18: Order fulﬁ llment pipeline diagram.\nHowever, there are times when business users want to analyze the entire order \nfulﬁ llment pipeline. They want to better understand product velocity, or how quickly \nproducts move through the pipeline. The accumulating snapshot fact table provides \nthis perspective of the business, as illustrated in Figure 6-19. It enables you to see \nan updated status and ultimately the ﬁ nal disposition of each order.\nThe  accumulating snapshot complements alternative schemas’ perspectives of \nthe pipeline. If you’re interested in understanding the amount of product ﬂ owing \nthrough the pipeline, such as the quantity ordered, produced, or shipped, transac-\ntion schemas monitor each of the pipeline’s major events. Periodic snapshots would \nprovide insight into the amount of product sitting in the pipeline, such as the \nbackorder or ﬁ nished goods inventories, or the amount of product ﬂ owing through \na pipeline spigot during a predeﬁ ned interval. The accumulating snapshot helps \nyou better understand the current state of an order, as well as product movement \nvelocities to identify pipeline bottlenecks and ineffi  ciencies. If you only captured \nperformance in transaction event fact tables, it would be wildly diffi  cult to calculate \nthe average number of days to move between milestones.\nThe  accumulating snapshot looks different from the transaction fact tables \ndesigned thus far in this chapter. The reuse of conformed dimensions is to be \nexpected, but the number of date and fact columns is larger. Each date represents \na major milestone of the fulﬁ llment pipeline. The dates are handled as dimension \n\n\nOrder Management 195\nroles by creating either physically distinct tables or logically distinct views. The date \ndimension needs to have a row for Unknown or To Be Determined because many of \nthese fact table dates are unknown when a pipeline row is initially loaded. Obviously, \nyou don’t need to declare all the date columns in the fact table’s primary key.\nOrder Date Key (FK)\nBacklog Date Key (FK)\nRelease to Manufacturing Date Key (FK)\nFinished Inventory Placement Date Key (FK)\nRequested Ship Date Key (FK)\nScheduled Ship Date Key (FK)\nActual Ship Date Key (FK)\nArrival Date Key (FK)\nInvoice Date Key (FK)\nProduct Key (FK)\nCustomer Key (FK)\nSales Rep Key (FK)\nDeal Key (FK)\nManufacturing Facility Key (FK)\nWarehouse Key (FK)\nShipper Key (FK)\nOrder Number (DD)\nOrder Line Number (DD)\nInvoice Number (DD)\nOrder Quantity\nExtended Order Line Dollar Amount\nRelease to Manufacturing Quantity\nManufacturing Pass Inspection Quantity\nManufacturing Fail Inspection Quantity\nFinished Goods Inventory Quantity\nAuthorized to Sell Quantity\nShipment Quantity\nShipment Damage Quantity\nCustomer Return Quantity\nInvoice Quantity\nExtended Invoice Dollar Amount\nOrder to Manufacturing Release Lag\nManufacturing Release to Inventory Lag\nInventory to Shipment Lag\nOrder to Shipment Lag\nCustomer Dimension\nShipper  Dimension\nManufacturing Facility Dimension\nProduct Dimension\nDeal Dimension\nWarehouse Dimension\nOrder Fulfillment Accumulating Fact\nSales Rep Dimension\nDate Dimension (views for 9 roles)\nFigure 6-19: Order fulﬁ llment accumulating snapshot fact table.\nThe fundamental diff erence between accumulating snapshots and other fact tables \nis that you can revisit and update existing fact table rows as more information becomes \navailable. The grain of an accumulating snapshot fact table in Figure 6-19 is one row \nper order line item. However, unlike the order transaction fact table illustrated in \nFigure 6-2 with the same granularity, accumulating snapshot fact rows are modiﬁ ed \nwhile the order moves through the pipeline as more information is collected from \nevery stage of the cycle.\n\n\nChapter 6\n196\nNOTE \nAccumulating snapshot fact tables typically have multiple dates repre-\nsenting the major milestones of the process. However, just because a fact table \nhas several dates doesn’t dictate that it is an accumulating snapshot. The primary \ndiff erentiator of an accumulating snapshot is that you revisit the fact rows as \nactivity occurs.\nThe accumulating snapshot technique is especially useful when the product mov-\ning through the pipeline is uniquely identiﬁ ed, such as an automobile with a vehicle \nidentiﬁ cation number, electronics equipment with a serial number, lab specimens \nwith an identiﬁ cation number, or process manufacturing batches with a lot num-\nber. The accumulating snapshot helps you understand throughput and yield. If the \ngranularity of an accumulating snapshot is at the serial or lot number, you can see \nthe disposition of a discrete product as it moves through the manufacturing and test \npipeline. The accumulating snapshot ﬁ ts most naturally with short-lived processes \nwith a deﬁ nite beginning and end. Long-lived processes, such as bank accounts, \nare typically better modeled with periodic snapshot fact tables.\nAccumulating Snapshots and Type 2 Dimensions\nAccumulating  snapshots present the latest state of a workﬂ ow or pipeline. If the \ndimensions associated with an accumulating snapshot contain type 2 attributes, \nthe fact table should be updated to reference the most current surrogate dimension \nkey for active pipelines. When a single fact table pipeline row is complete, the row \nis typically not revisited to reﬂ ect future type 2 changes.\n Lag Calculations\nThe  lengthy list of date columns captures the spans of time over which the order is \nprocessed through the fulﬁ llment pipeline. The numerical diff erence between any \ntwo of these dates is a number that can be usefully averaged over all the dimensions. \nThese date lag calculations represent basic measures of fulﬁ llment effi  ciency. You \ncould build a view on this fact table that calculated a large number of these date \ndiff erences and presented them as if they were stored in the underlying table. These \nview columns could include metrics such as orders to manufacturing release lag, \nmanufacturing release to ﬁ nished goods lag, and order to shipment lag, depending \non the date spans monitored by the organization.\nRather than calculating a simple diff erence between two dates via a view, the \nETL system may calculate elapsed times that incorporate more intelligence, such \nas workday lags that account for weekends and holidays rather than just the raw \nnumber of days between milestone dates. The lag metrics may also be calculated \nby the ETL system at a lower level of granularity (such as the number of hours or \n",
      "page_number": 215
    },
    {
      "number": 24,
      "title": "Segment 24 (pages 226-234)",
      "start_page": 226,
      "end_page": 234,
      "detection_method": "topic_boundary",
      "content": "Order Management 197\nminutes between milestone events based on operational timestamps) for short-lived \nand closely monitored processes.\n Multiple Units of Measure\nSometimes,  diff erent functional organizations within the business want to see the \nsame performance metrics expressed in diff erent units of measure. For instance, \nmanufacturing managers may want to see the product ﬂ ow in terms of pallets or \nshipping cases. Sales and marketing managers, on the other hand, may want to see \nthe quantities in retail cases, scan units (sales packs), or equivalized consumer units \n(such as individual cans of soda).\nDesigners are tempted to bury the unit-of-measure conversion factors, such as \nship case factor, in the product dimension. Business users are then required to \nappropriately multiply (or was it divide?) the order quantity by the conversion factor. \nObviously, this approach places a burden on users, in addition to being susceptible \nto calculation errors. The situation is further complicated because the conversion \nfactors may change over time, so users would also need to determine which factor \nis applicable at a speciﬁ c point in time.\nRather than risk miscalculating the equivalent quantities by placing conversion \nfactors in a dimension table, they should be stored in the fact table instead. In the \norders pipeline fact table, assume you have 10 basic fundamental quantity facts, in \naddition to ﬁ ve units of measure. If you physically store all the facts expressed in \nthe diff erent units of measure, you end up with 50 (10 × 5) facts in each fact row. \nInstead, you can compromise by building an underlying physical row with 10 quan-\ntity facts and 4 unit-of-measure conversion factors. You need only four conversion \nfactors rather than ﬁ ve because the base facts are already expressed in one of the \nunits of measure. The physical design now has 14 quantity-related facts (10 + 4), as \nshown in Figure 6-20. With this design, you can see performance across the value \nchain based on diff erent units of measure.\nOf course, you would deliver this fact table to the business users through one \nor more views. The extra computation involved in multiplying quantities by con-\nversion factors is negligible; intra-row computations are very effi  cient. The most \ncomprehensive view could show all 50 facts expressed in every unit of measure, \nbut the view could be simpliﬁ ed to deliver only a subset of the quantities in units \nof measure relevant to a user. Obviously, each unit of measures’ metrics should be \nuniquely labeled.\nNOTE \nPackaging all the facts and conversion factors together in the same fact \ntable row provides the safest guarantee that these factors will be used correctly. \nThe converted facts are presented in a view(s) to the users.\n\n\nChapter 6\n198\nDate Keys (FKs)\nProduct Key (FK)\nMore FKs...\nOrder Quantity Shipping Cases\nRelease to Manufacturing Quantity Shipping Cases\nManufacturing Pass Inspection Quantity Shipping Cases\nManufacturing Fail Inspection Quantity Shipping Cases\nFinished Goods Inventory Quantity Shipping Cases\nAuthorized to Sell Quantity Shipping Cases\nShipment Quantity Shipping Cases\nShipment Damage Quantity Shipping Cases\nCustomer Return Quantity Shipping Cases\nInvoice Quantity Shipping Cases\nPallet Conversion Factor\nRetail Cases Conversion Factor\nScan Units Conversion Factor\nEquivalized Consumer Units Conversion Factor\nOrder Fulfillment Accumulating Fact\nFigure 6-20: Physical fact table supporting multiple units of measure with conversion \nfactors.\nFinally, another side beneﬁ t of storing these factors in the fact table is it reduces \nthe pressure on the product dimension table to issue new product rows to reﬂ ect \nminor conversion factor modiﬁ cations. These factors, especially if they evolve rou-\ntinely over time, behave more like facts than dimension attributes.\nBeyond the Rearview Mirror\nMuch  of what we’ve discussed in this chapter focuses on eff ective ways to analyze \nhistorical product movement performance. People sometimes refer to these as rear-\nview mirror metrics because they enable you to look backward and see where you’ve \nbeen. As the brokerage industry reminds people, past performance is no guarantee of \nfuture results. Many organizations want to supplement these historical performance \nmetrics with facts from other processes to help project what lies ahead. For example, \nrather than focusing on the pipeline at the time an order is received, organizations \nare analyzing the key drivers impacting the creation of an order. In a sales organiza-\ntion, drivers such as prospecting or quoting activity can be extrapolated to provide \nvisibility to the expected order activity volume. Many organizations do a better job \ncollecting the rearview mirror information than they do the early indicators. As these \nfront window leading indicators are captured, they can be added gracefully to the \nDW/BI environment. They’re just more rows on the enterprise data warehouse bus \nmatrix sharing common dimensions.\n\n\nOrder Management 199\nSummary\nThis chapter covered a lengthy laundry list of topics in the context of the order \nmanagement process. Multiples were discussed on several fronts: multiple references \nto the same dimension in a fact table (role-playing dimensions), multiple equivalent \nunits of measure, and multiple currencies. We explored several of the common chal-\nlenges encountered when modeling header/line transaction data, including facts at \ndiff erent levels of granularity and junk dimensions, plus design patterns to avoid. \nWe also explored the rich set of facts associated with invoice transactions. Finally, \nthe order fulﬁ llment pipeline illustrated the power of accumulating snapshot fact \ntables where you can see the updated status of a speciﬁ c product or order as it moves \nthrough a ﬁ nite pi peline.\n\n\nAccounting\nF\ninancial analysis spans a variety of accounting applications, including the gen-\neral ledger, as well as detailed subledgers for purchasing and accounts payable, \ninvoicing and accounts receivable, and fixed assets. Because we’ve already touched \nupon purchase orders and invoices earlier in this book, we’ll focus on the general \nledger in this chapter. Given the need for accurate handling of a company’s financial \nrecords, general ledgers were one of the first applications to be computerized decades \nago. Perhaps some of you are still running your business on a 20-year-old ledger \nsystem. In this chapter, we’ll discuss the data collected by the general ledger, both \nin terms of journal entry transactions and snapshots at the close of an accounting \nperiod. We’ll also talk about the budgeting process.\nChapter 7 discusses the following concepts:\n \n■Bus matrix snippet for accounting processes\n \n■General ledger periodic snapshots and journal transactions\n \n■Chart of accounts\n \n■Period close\n \n■Year-to-date facts\n \n■Multiple ﬁ scal accounting calendars\n \n■Drilling down through a multi-ledger hierarchy\n \n■Budgeting chain and associated processes \n \n■Fixed depth position hierarchies\n \n■Slightly ragged, variable depth hierarchies\n \n■Totally ragged hierarchies of indeterminate depth using a bridge table and \nalternative modeling techniques\n \n■Shared ownership in a ragged hierarchy\n \n■Time varying ragged hierarchies\n \n■Consolidated fact tables that combine metrics from multiple business processes\n \n■Role of OLAP and packaged analytic ﬁ nancial solutions\n7\n\n\nChapter 7\n202\nAccounting Case Study and Bus Matrix\nBecause  ﬁ nance was an early adopter of technology, it comes as no surprise that \nearly decision support solutions focused on the analysis of ﬁ nancial data. Financial \nanalysts are some of the most data-literate and spreadsheet-savvy individuals. Often \ntheir analysis is disseminated or leveraged by many others in the organization. \nManagers at all levels need timely access to key ﬁ nancial metrics. In addition to \nreceiving standard reports, they need the ability to analyze performance trends, vari-\nances, and anomalies with relative speed and minimal eff ort. Like many operational \nsource systems, the data in the general ledger is likely scattered among hundreds of \ntables. Gaining access to ﬁ nancial data and/or creating ad hoc reports may require \na decoder ring to navigate through the maze of screens. This runs counter to many \norganizations’ objective to push ﬁ scal responsibility and accountability to the line \nmanagers.\nThe DW/BI system can provide a single source of usable, understandable ﬁ nan-\ncial information, ensuring everyone is working off  the same data with common \ndeﬁ nitions and common tools. The audience for ﬁ nancial data is quite diverse in \nmany organizations, ranging from analysts to operational managers to executives. \nFor each group, you need to determine which subset of corporate ﬁ nancial data is \nneeded, in which format, and with what frequency. Analysts and managers want to \nview information at a high level and then drill to the journal entries for more detail. \nFor executives, ﬁ nancial data from the DW/BI system often feeds their dashboard or \nscorecard of key performance indicators. Armed with direct access to information, \nmanagers can obtain answers to questions more readily than when forced to work \nthrough a middleman. Meanwhile, ﬁ nance can turn their attention to information \ndissemination and value-added analysis, rather than focusing on report creation.\nImproved  access to accounting data allows you to focus on opportunities to better \nmanage risk, streamline operations, and identify potential cost savings. Although it \nhas cross-organization impact, many businesses focus their initial DW/BI implemen-\ntation on strategic, revenue-generating opportunities. Consequently, accounting data \nis often not the ﬁ rst subject area tackled by the DW/BI team. Given its proﬁ ciency \nwith technology, the ﬁ nance department has often already performed magic with \nspreadsheets and desktop databases to create workaround analytic solutions, per-\nhaps to its short-term detriment, as these imperfect interim ﬁ xes are likely stressed \nto their limits.\nFigure 7-1 illustrates an accounting-focused excerpt from an organization’s bus \nmatrix. The dimensions associated with accounting processes, such as the general \nledger account or organizational cost center, are frequently used solely by these \nprocesses, unlike the core customer, product, and employee dimensions which are \nused repeatedly across many diverse business processes.\n\n\nAccounting 203\nGeneral Ledger Transactions\nGeneral Ledger Snapshot\nBudget\nCommitment\nPayments\nActual-Budget Variance\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nDate\nLedger\nAccount\nOrganization\nBudget Line\nCommitment\nProfile\nPayment\nProfile\nFigure 7-1: Bus matrix rows for accounting processes.\nGeneral Ledger Data\nThe general ledger (G/L) is a core foundation ﬁ nancial system that ties together the \ndetailed information collected by subledgers or separate systems for purchasing, \npayables (what you owe to others), and receivables (what others owe you). As we \nwork through a basic design for G/L data, you’ll discover the need for two comple-\nmentary schemas with periodic snapshot and transaction fact tables.\nGeneral Ledger Periodic Snapshot\nWe’ll  begin by delving into a snapshot of the general ledger accounts at the end of \neach ﬁ scal period (or month if the ﬁ scal accounting periods align with calendar \nmonths). Referring back to our four-step process for designing dimensional models \n(see Chapter 3: Retail Sales), the business process is the general ledger. The grain \nof this periodic snapshot is one row per accounting period for the most granular \nlevel in the general ledger’s chart of accounts.\nChart of Accounts\nThe  cornerstone of the general ledger is the chart of accounts. The ledger’s chart of \naccounts is the epitome of an intelligent key because it usually consists of a series of \nidentiﬁ ers. For example, the ﬁ rst set of digits may identify the account, account type \n(for example, asset, liability, equity, income, or expense), and other account rollups. \nSometimes intelligence is embedded in the account numbering scheme. For example, \naccount numbers from 1,000 through 1,999 might be asset accounts, whereas account \nnumbers ranging from 2,000 to 2,999 may identify liabilities. Obviously, in the data \n\n\nChapter 7\n204\nwarehouse, you’d include the account type as a dimension attribute rather than forc-\ning users to ﬁ lter on the ﬁ rst digit of the account number.\nThe chart of accounts likely associates the organization cost center with the \naccount. Typically, the organization attributes provide a complete rollup from cost \ncenter to department to division, for example. If the corporate general ledger com-\nbines data across multiple business units, the chart of accounts would also indicate \nthe business unit or subsidiary company.\nObviously, charts of accounts vary from organization to organization. They’re \noften extremely complicated, with hundreds or even thousands of cost centers in \nlarge organizations. In this case study vignette, the chart of accounts naturally \ndecomposes into two dimensions. One dimension represents accounts in the general \nledger, whereas the other represents the organization rollup.\nThe organization rollup may be a ﬁ xed depth hierarchy, which would be handled \nas separate hierarchical attributes in the cost center dimension. If the organization \nhierarchy is ragged with an unbalanced rollup structure, you need the more power-\nful variable depth hierarchy techniques described in the section “Ragged Variable \nDepth Hierarchies.”\nIf  you are tasked with building a comprehensive general ledger spanning multiple \norganizations in the DW/BI system, you should try to conform the chart of accounts \nso the account types mean the same thing across organizations. At the data level, \nthis means the master conformed account dimension contains carefully deﬁ ned \naccount names. Capital Expenditures and Offi  ce Supplies need to have the same \nﬁ nancial meaning across organizations. Of course, this kind of conformed dimen-\nsion has an old and familiar name in ﬁ nancial circles: the uniform chart of accounts.\nThe G/L sometimes tracks ﬁ nancial results for multiple sets of books or sub-\nledgers to support diff erent requirements, such as taxation or regulatory agency \nreporting. You can treat this as a separate dimension because it’s such a fundamen-\ntal ﬁ lter, but we alert you to carefully read the cautionary note in the next section.\n Period Close\nAt  the end of each accounting period, the ﬁ nance organization is responsible for \nﬁ nalizing the ﬁ nancial results so that they can be offi  cially reported internally \nand externally. It typically takes several days at the end of each period to recon-\ncile and balance the books before they can be closed with ﬁ nance’s offi  cial stamp \nof approval. From there, ﬁ nance’s focus turns to reporting and interpreting the \nresults. It often produces countless reports and responds to countless variations \non the same questions each month.\nFinancial analysts are constantly looking to streamline the processes for period-\nend closing, reconciliation, and reporting of general ledger results. Although \n\n\nAccounting 205\noperational general ledger systems often support these requisite capabilities, they \nmay be cumbersome, especially if you’re not dealing with a modern G/L. This chap-\nter focuses on easily analyzing the closed ﬁ nancial results, rather than facilitating \nthe close. However, in many organizations, general ledger trial balances are loaded \ninto the DW/BI system leveraging the capabilities of the DW/BI presentation area \nto ﬁ nd the needles in the general ledger haystack, and then making the appropriate \noperational adjustments before the period ends.\nThe sample schema in Figure 7-2 shows general ledger account balances at the \nend of each accounting period which would be very useful for many kinds of ﬁ nan-\ncial analyses, such as account rankings, trending patterns, and period-to-period \ncomparisons.\nAccounting Period Key (FK)\nLedger Key (FK)\nAccount Key (FK)\nOrganization Key (FK)\nPeriod End Balance Amount\nPeriod Debit Amount\nPeriod Credit Amount\nPeriod Net Change Amount\nAccount Key (PK)\nAccount Name\nAccount Category\nAccount Type\nAccounting Period Key (PK)\nAccounting Period Number\nAccounting Period Description\nAccounting Period Fiscal Year\nLedger Key (PK)\nLedger Book Name\nOrganization Key (PK)\nCost Center Name\nCost Center Number\nDepartment Name\nDepartment Number\nDivision Name\nBusiness Unit Name\nCompany Name\nAccounting Period Dimension\nGeneral Ledger Snapshot Fact\nLedger Dimension\nOrganization Dimension\nAccount Dimension\nFigure 7-2: General ledger periodic snapshot.\nFor the moment, we’re just representing actual ledger facts in the Figure 7-2 \nschema; we’ll expand our view to cover budget data in the section “Budgeting \nProcess.” In this table, the balance amount is a semi-additive fact. Although the \nbalance doesn’t represent G/L activity, we include the fact in the design because \nit is so useful. Otherwise, you would need to go back to the beginning of time to \ncalculate an accurate end-of-period balance.\nWARNING \nThe ledger dimension is a convenient and intuitive dimension \nthat enables multiple ledgers to be stored in the same fact table. However, every \nquery that accesses this fact table must constrain the ledger dimension to a single \nvalue (for example, Final Approved Domestic Ledger) or the queries will double \ncount values from the various ledgers in this table. The best way to deploy this \nschema is to release separate views to the business users with the ledger dimension \npre-constrained to a single value.\n\n\nChapter 7\n206\nThe two most important dimensions in the proposed general ledger design are \naccount and organization. The account dimension is carefully derived from the \nuniform chart of accounts in the enterprise. The organization dimension describes \nthe ﬁ nancial reporting entities in the enterprise. Unfortunately, these two crucial \ndimensions almost never conform to operational dimensions such as customer, \nproduct, service, or facility. This leads to a characteristic but unavoidable business \nuser frustration that the “GL doesn’t tie to my operational reports.” It is best to gently \nexplain this to the business users in the interview process, rather than promising \nto ﬁ x it because this is a deep seated issue in the underlying data.\n Year-to-Date Facts\nDesigners  are often tempted to store “to-date” columns in fact tables. They think \nit would be helpful to store quarter-to-date or year-to-date additive totals on each \nfact row so they don’t need to calculate them. Remember that numeric facts must \nbe consistent with the grain. To-date facts are not true to the grain and are fraught \nwith peril. When fact rows are queried and summarized in arbitrary ways, these \nuntrue-to-the-grain facts produce nonsensical, overstated results. They should be \nleft out of the relational schema design and calculated in the BI reporting application \ninstead. It’s worth noting that OLAP cubes handle to-date metrics more gracefully.\nNOTE \nIn general, “to-date” totals should be calculated, not stored in the \nfact table.\n Multiple Currencies Revisited\nIf  the general ledger consolidates data that has been captured in multiple curren-\ncies, you would handle it much as we discussed in Chapter 6: Order Management. \nWith ﬁ nancial data, you typically want to represent the facts both in terms of the \nlocal currency, as well as a standardized corporate currency. In this case, each \nfact table row would represent one set of fact amounts expressed in local currency \nand a separate set of fact amounts on the same row expressed in the equivalent \ncorporate currency. Doing so allows you to easily summarize the facts in a com-\nmon corporate currency without jumping through hoops in the BI applications. \nOf course, you’d also add a currency dimension as a foreign key in the fact table \nto identify the local currency  type.\n General Ledger Journal Transactions\nWhile  the end-of-period snapshot addresses a multitude of ﬁ nancial analyses, many \nusers need to dive into the underlying details. If an anomaly is identiﬁ ed at the \n",
      "page_number": 226
    },
    {
      "number": 25,
      "title": "Segment 25 (pages 235-243)",
      "start_page": 235,
      "end_page": 243,
      "detection_method": "topic_boundary",
      "content": "Accounting 207\nsummary level, analysts want to look at the detailed transactions to sort through \nthe issue. Others need access to the details because the summarized monthly bal-\nances may obscure large disparities at the granular transaction level. Again, you \ncan complement the periodic snapshot with a detailed journal entry transaction \nschema. Of course, the accounts payable and receivable subledgers may contain \ntransactions at progressively lower levels of detail, which would be captured in \nseparate fact tables with additional dimensionality.\nThe grain of the fact table is now one row for every general ledger journal entry \ntransaction. The journal entry transaction identiﬁ es the G/L account and the appli-\ncable debit or credit amount. As illustrated in Figure 7-3, several dimensions from \nthe last schema are reused, including the account and organization. If the ledger \ntracks multiple sets of books, you’d also include the ledger/book dimension. You \nwould normally capture journal entry transactions by transaction posting date, \nso use a daily-grained date table in this schema. Depending on the business rules \nassociated with the source data, you may need a second role-playing date dimension \nto distinguish the posting date from the eff ective accounting date.\nPost Date Key (FK)\nJournal Entry Effective Date/Time\nLedger Key (FK)\nAccount Key (FK)\nOrganization Key (FK)\nDebit-Credit Indicator Key (FK)\nJournal Entry Number (DD)\nJournal Entry Amount\nDebit-Credit Indicator Key (PK)\nDebit-Credit Indicator Description\nGeneral Ledger Journal Entry Fact\nLedger Dimension\nOrganization Dimension\nPost Date Dimension\nAccount Dimension\nDebit-Credit Indicator Dimension\nFigure 7-3: General ledger journal entry transactions.\nThe journal entry number is likely a degenerate dimension with no linkage to \nan associated dimension table. If the journal entry numbers from the source are \nordered, then this degenerate dimension can be used to order the journal entries \nbecause the calendar date dimension on this fact table is too coarse to provide this \nsorting. If the journal entry numbers do not easily support the sort, then an eff ective \ndate/time stamp must be added to the fact table. Depending on the source data, you \nmay have a journal entry transaction type and even a description. In this situation, \nyou would create a separate journal entry transaction proﬁ le dimension (not shown). \nAssuming the descriptions are not just freeform text, this dimension would have \nsigniﬁ cantly fewer rows than the fact table, which would have one row per journal \nentry line. The speciﬁ c journal entry number would still be treated as degenerate.\nEach row in the journal entry fact table is identiﬁ ed as either a credit or a debit. \nThe debit/credit indicator takes on two, and only two, values.\n\n\nChapter 7\n208\n Multiple Fiscal Accounting Calendars\nIn Figure 7-3, the data is captured by posting date, but users may also want to \nsummarize the data by ﬁ scal account period. Unfortunately, ﬁ scal accounting peri-\nods often do not align with standard Gregorian calendar months. For example, a \ncompany may have 13 4-week accounting periods in a ﬁ scal year that begins on \nSeptember 1 rather than 12 monthly periods beginning on January 1. If you deal \nwith a single ﬁ scal calendar, then each day in a year corresponds to a single calendar \nmonth, as well as a single accounting period. Given these relationships, the calendar \nand accounting periods are merely hierarchical attributes on the daily date dimen-\nsion. The daily date dimension table would simultaneously conform to a calendar \nmonth dimension table, as well as to a ﬁ scal accounting period dimension table.\nIn other situations, you may deal with multiple ﬁ scal accounting calendars that \nvary by subsidiary or line of business. If the number of unique ﬁ scal calendars is a \nﬁ xed, low number, then you can include each set of uniquely labeled ﬁ scal calendar \nattributes on a single date dimension. A given row in the daily date dimension would \nbe identiﬁ ed as belonging to accounting period 1 for subsidiary A but accounting \nperiod 7 for subsidiary B.\nIn a more complex situation with a large number of diff erent ﬁ scal calendars, \nyou could identify the offi  cial corporate ﬁ scal calendar in the date dimension. You \nthen have several options to address the subsidiary-speciﬁ c ﬁ scal calendars. The \nmost common approach is to create a date dimension outrigger with a multipart key \nconsisting of the date and subsidiary keys. There would be one row in this table for \neach day for each subsidiary. The attributes in this outrigger would consist of ﬁ s-\ncal groupings (such as ﬁ scal week end date and ﬁ scal period end date). You would \nneed a mechanism for ﬁ ltering on a speciﬁ c subsidiary in the outrigger. Doing so \nthrough a view would then allow the outrigger to be presented as if it were logically \npart of the date dimension table. \nA  second approach for tackling the subsidiary-speciﬁ c calendars would be to \ncreate separate physical date dimensions for each subsidiary calendar, using a \ncommon set of surrogate date keys. This option would likely be used if the fact \ndata were decentralized by subsidiary. Depending on the BI tool’s capabilities, it \nmay be easier to either ﬁ lter on the subsidiary outrigger as described in option \n1 or ensure usage of the appropriate subsidiary-speciﬁ c physical date dimension \ntable (option 2). Finally, you could allocate another foreign key in the fact table to \na subsidiary ﬁ scal period dimension table. The number of rows in this table would \nbe the number of ﬁ scal periods (approximately 36 for 3 years) times the number of \nunique calendars. This approach simpliﬁ es user access but puts additional strain \non the ETL system because it must insert the appropriate ﬁ scal period key during \nthe transformation  process.\n\n\nAccounting 209\nDrilling Down Through a Multilevel Hierarchy\nVery  large enterprises or government agencies may have multiple ledgers arranged \nin an ascending hierarchy, perhaps by enterprise, division, and department. At the \nlowest level, department ledger entries may be consolidated to roll up to a single \ndivision ledger entry. Then the division ledger entries may be consolidated to the \nenterprise level. This would be particularly common for the periodic snapshot grain \nof these ledgers. One way to model this hierarchy is by introducing the parent snap-\nshot’s fact table surrogate key in the fact table, as shown in Figure 7-4. In this case, \nbecause you deﬁ ne a parent/child relationship between rows, you add an explicit \nfact table surrogate key, a single column numeric identiﬁ er incremented as you add \nrows to the fact table.\nFact Table Surrogate Key (PK)\nAccounting Period Key (FK)\nLedger Key (FK)\nAccount Key (FK)\nOrganization Key (FK)\nParent Snapshot Key (FK)\nPeriod End Balance Amount\nPeriod Debit Amount\nPeriod Credit Amount\nPeriod Net Change Amount\nGeneral Ledger Snapshot Fact\nLedger Dimension\nOrganization Dimension\nAccounting Period Dimension\nAccount Dimension\nFigure 7-4: Design for drilling down through multiple ledgers.\nYou can use the parent snapshot surrogate key to drill down in your multilayer \ngeneral ledger. Suppose that you detect a large travel amount at the top level of the \nledger. You grab the surrogate key for that high-level entry and then fetch all the entries \nwhose parent snapshot key equals that key. This exposes the entries at the next lower \nlevel that contribute to the original high-level record of interest. The SQL would look \nsomething like  this:\nSelect * from GL_Fact where Parent_Snapshot_key =\n  (select fact_table_surrogate_key from GL_Fact f, Account a\n   where <joins> and a.Account = 'Travel' and f.Amount > 1000)\nFinancial Statements\nOne  of the primary functions of a general ledger system is to produce the organiza-\ntion’s offi  cial ﬁ nancial reports, such as the balance sheet and income statement. The \noperational system typically handles the production of these reports. You wouldn’t \nwant the DW/BI system to attempt to replace the reports published by the opera-\ntional ﬁ nancial systems.\n\n\nChapter 7\n210\nHowever, DW/BI teams sometimes create complementary aggregated data that \nprovides simpliﬁ ed access to report information that can be more widely dissemi-\nnated throughout the organization. Dimensions in the ﬁ nancial statement schema \nwould include the accounting period and cost center. Rather than looking at general \nledger account level data, the fact data would be aggregated and tagged with the \nappropriate ﬁ nancial statement line number and label. In this manner, managers \ncould easily look at performance trends for a given line in the ﬁ nancial statement \nover time for their organization. Similarly, key performance indicators and ﬁ nancial \nratios may be made available at the same level of detail.\n Budgeting Process\nMost  modern general ledger systems include the capability to integrate budget data \ninto the general ledger. However, if the G/L either lacks this capability or it has not \nbeen implemented, you need to provide an alternative mechanism for supporting \nthe budgeting process and variance comparisons.\nWithin most organizations, the budgeting process can be viewed as a series of \nevents. Prior to the start of a ﬁ scal year, each cost center manager typically creates a \nbudget, broken down by budget line items, which is then approved. In reality, bud-\ngeting is seldom simply a once-per-year event. Budgets are becoming more dynamic \nbecause there are budget adjustments as the year progresses, reﬂ ecting changes in \nbusiness conditions or the realities of actual spending versus the original budget. \nManagers want to see the current budget’s status, as well as how the budget has \nbeen altered since the ﬁ rst approved version. As the year unfolds, commitments to \nspend the budgeted monies are made. Finally, payments are processed.\nAs a dimensional modeler, you can view the budgeting chain as a series of fact \ntables, as shown in Figure 7-5. This chain consists of a budget fact table, commit-\nments fact table, and payments fact table, where there is a logical ﬂ ow that starts \nwith a budget being established for each organization and each account. Then dur-\ning the operational period, commitments are made against the budgets, and ﬁ nally \npayments are made against those commitments.\nWe’ll  begin with the budget fact table. For an expense budget line item, each row \nidentiﬁ es what an organization in the company is allowed to spend for what purpose \nduring a given time frame. Similarly, if the line item reﬂ ects an income forecast, \nwhich is just another variation of a budget, it would identify what an organization \nintends to earn from what source during a time frame.\nYou  could further identify the grain to be a snapshot of the current status of \neach line item in each budget each month. Although this grain has a familiar ring \nto it (because it feels like a management report), it is a poor choice as the fact table \n\n\nAccounting 211\ngrain. The facts in such a “status report” are all semi-additive balances, rather than \nfully additive facts. Also, this grain makes it diffi  cult to determine how much has \nchanged since the previous month or quarter because you must obtain the rows \nfrom several time periods and then subtract them from each other. Finally, this grain \nchoice would require the fact table to contain many duplicated rows when nothing \nchanges in successive months for a given line item.\nMonth Key (FK)\nOrganization Key (FK)\nAccount Key (FK)\nBudget Key (FK)\nBudget Amount\nMonth Key (FK)\nOrganization Key (FK)\nAccount Key (FK)\nBudget Key (FK)\nCommitment Key (FK)\nCommitment Amount\nMonth Key (FK)\nOrganization Key (FK)\nAccount Key (FK)\nBudget Key (FK)\nCommitment Key (FK)\nPayment Key (FK)\nPayment Amount\nBudget Key (PK)\nBudget Name\nBudget Version\nBudget Approval Date\nCommitment Key (PK)\nCommitment Description\nCommitment Party\nPayment Key (PK)\nPayment Description\nPayment Party\nOrganization Dimension\nBudget Dimension\nOrganization Dimension\nBudget Dimension\nOrganization Dimension\nBudget Dimension\nPayment Dimension\nPayment Fact\nAccount Dimension\nMonth Dimension\nAccount Dimension\nCommitment Dimension\nMonth Dimension\nAccount Dimension\nCommitment Dimension\nMonth Dimension\nBudget Fact\nCommitment Fact\nFigure 7-5: Chain of budget processes.\nInstead, the grain you’re interested in is the net change of the budget line item \nin an organizational cost center that occurred during the month. Although this \nsuffi  ces for budget reporting purposes, the accountants eventually need to tie the \nbudget line item back to a speciﬁ c general ledger account that’s aff ected, so you’ll \nalso go down to the G/L account level.\n\n\nChapter 7\n212\nGiven the grain, the associated budget dimensions would include eff ective \nmonth, organization cost center, budget line item, and G/L account, as illustrated \nin Figure 7-6. The organization is identical to the dimension used earlier with the \ngeneral ledger data. The account dimension is also a reused dimension. The only \ncomplication regarding the account dimension is that sometimes a single budget \nline item impacts more than one G/L account. In that case, you would need to \nallocate the budget line to the individual G/L accounts. Because the grain of the \nbudget fact table is by G/L account, a single budget line for a cost center may be \nrepresented as several rows in the fact table.\nEffective Date Dimension\nAccount Dimension\nOrganization Dimension\nBudget Line Item Dimension\nBudget Effective Date Key (PK)\nBudget Effective Date Month\nBudget Effective Date Year\n...\nBudget Effective Date Key (FK)\nBudget Line Item Key (FK)\nAccount Key (FK)\nOrganization Key (FK)\nBudget Amount\nBudget Line Item Key (PK)\nBudget Name\nBudget Version\nBudget Line Description\nBudget Year\nBudget Line Subcategory Description\nBudget Line Category Description\nBudget Fact\nFigure 7-6: Budget schema.\nThe budget line item identiﬁ es the purpose of the proposed spending, such as \nemployee wages or offi  ce supplies. There are typically several levels of summariza-\ntion categories associated with a budget line item. All the budget line items may not \nhave the same number of levels in their summarization hierarchy, such as when some \nonly have a category rollup, but not a subcategory. In this case, you may populate the \ndimension attributes by replicating the category name in the subcategory column to \navoid having line items roll up to a Not Applicable subcategory bucket. The budget \nline item dimension would also identify the budget year and/or budget version.\nThe eff ective month is the month during which the budget changes are posted. \nThe ﬁ rst entries for a given budget year would show the eff ective month when the \nbudget is ﬁ rst approved. If the budget is updated or modiﬁ ed as the budget year \ngets underway, the eff ective months would occur during the budget year. If you \ndon’t adjust a budget throughout the year, then the only entries would be the ﬁ rst \nones when the budget is initially approved. This is what is meant when the grain \nis speciﬁ ed to be the net change. It’s critical that you understand this point, or you \nwon’t understand what is in this budget fact table or how it’s used.\nSometimes budgets are created as annual spending plans; other times, they’re \nbroken down by month or quarter. Figure 7-6 assumes the budget is an annual \namount, with the budget year identiﬁ ed in the budget line item dimension. If you \nneed to express the budget data by spending month, you would need to include a \nsecond month dimension table that plays the role of spending month.\n\n\nAccounting 213\nThe budget fact table has a single budget amount fact that is fully additive. If you \nbudget for a multinational organization, the budget amount may be tagged with the \nexpected currency conversion factor for planning purposes. If the budget amount \nfor a given budget line and account is modiﬁ ed during the year, an additional row \nis added to the budget fact table representing the net change. For example, if the \noriginal budget were $200,000, you might have another row in June for a $40,000 \nincrease and then another in October for a negative $25,000 as you tighten your \nbelt going into year-end.\nWhen the budget year begins, managers make commitments to spend the budget \nthrough purchase orders, work orders, or other forms of contracts. Managers are \nkeenly interested in monitoring their commitments and comparing them to the \nannual budget to manage their spending. We can envision a second fact table for \nthe commitments (refer to Figure 7-5) that shares the same dimensions, in addi-\ntion to dimensions identifying the speciﬁ c commitment document (purchase order, \nwork order, or contract) and commitment party. In this case, the fact would be the \ncommitted amount.\nFinally, payments are made as monies are transferred to the party named in the \ncommitment. From a practical point of view, the money is no longer available in \nthe budget when the commitment is made. But the ﬁ nance department is interested \nin the relationship between commitments and payments because it manages the \ncompany’s cash. The dimensions associated with the payments fact table would \ninclude the commitment fact table dimensions, plus a payment dimension to identify \nthe type of payment, as well as the payee to whom the payment was actually made. \nReferring the budgeting chain shown in Figure 7-5, the list of dimensions expands \nas you move from the budget to commitments to payments.\nWith this design, you can create a number of interesting analyses. To look at \nthe current budgeted amount by department and line item, you can constrain \non all dates up to the present, adding the amounts by department and line item. \nBecause the grain is the net change of the line items, adding up all the entries \nover time does exactly the right thing. You end up with the current approved \nbudget amount, and you get exactly those line items in the given departments \nthat have a budget.\nTo ask for all the changes to the budget for various line items, simply constrain \non a single month. You’ll report only those line items that experienced a change \nduring the month.\nTo compare current commitments to the current budget, separately sum the \ncommitment amounts and budget amounts from the beginning of time to the cur-\nrent date (or any date of interest). Then combine the two answer sets on the row \nheaders. This is a standard drill-across application using multipass SQL. Similarly, \nyou could drill across commitments and payments.\n\n\nChapter 7\n214\nDimension Attribute Hierarchies\nAlthough the budget chain use case described in this chapter is reasonably simple, \nit contains a number of hierarchies, along with a number of choices for the designer. \nRemember a hierarchy is deﬁ ned by a series of many-to-one relationships. You likely \nhave at least four hierarchies: calendar levels, account levels, geographic levels, and \norganization levels.\n Fixed Depth Positional Hierarchies\nIn  the budget chain, the calendar levels are familiar ﬁ xed depth position hierarchies. \nAs the name suggests, a ﬁ xed position hierarchy has a ﬁ xed set of levels, all with \nmeaningful labels. Think of these levels as rollups. One calendar hierarchy may \nbe day ➪ ﬁ scal period ➪ year. Another could be day ➪ month ➪ year. These two \nhierarchies may be diff erent if there is no simple relationship between ﬁ scal periods \nand months. For example, some organizations have 5-4-4 ﬁ scal periods, consisting \nof a 5-week span followed by two 4-week spans. A single calendar date dimension \ncan comfortably represent these two hierarches at the same time in sets of parallel \nattributes since the grain of the date dimension is the individual day.\nThe account dimension may also have a ﬁ xed many-to-one hierarchy such as \nexecutive level, director level, and manager level accounts. The grain of the dimen-\nsion is the manager level account, but the detailed accounts at the lowest grain roll \nup to the director and executive levels.\nIn a ﬁ xed position hierarchy, it is important that each level have a speciﬁ c name. \nThat way the business user knows how to constrain and interpret each  level.\nWARNING \nAvoid ﬁ xed position hierarchies with abstract names such as Level-1, \nLevel-2, and so on. This is a cheap way to avoid correctly modeling a ragged hierar-\nchy. When the levels have abstract names, the business user has no way of knowing \nwhere to place a constraint, or what the attribute values in a level mean in a report. \nIf a ragged hierarchy attempts to hide within a ﬁ xed position hierarchy with abstract \nnames, the individual levels are essentially meaningless.\n Slightly Ragged Variable Depth Hierarchies\nGeographic  hierarchies present an interesting challenge. Figure 7-7 shows three \npossibilities. The simple location has four levels: address, city, state, and country. \nThe medium complex location adds a zone level, and the complex location adds \nboth district and zone levels. If you need to represent all three types of locations \n\n\nAccounting 215\nin a single geographic hierarchy, you have a slightly variable hierarchy. You can \ncombine all three types if you are willing to make a compromise. For the medium \nlocation that has no concept of district, you can propagate the city name down into \nthe district attribute. For the simple location that has no concept of either district or \nzone, you can propagate the city name down into both these attributes. The business \ndata governance representatives may instead decide to propagate labels upward or \neven populate the empty levels with Not Applicable. The business representatives \nneed to visualize the appropriate row label values on a report if the attribute is \ngrouped on. Regardless of the business rules applied, you have the advantage of a \nclean positional design with attribute names that make reasonable sense across all \nthree geographies. The key to this compromise is the narrow range of geographic \nhierarchies, ranging from four levels to only six levels. If the data ranged from \nfour levels to eight or ten or even more, this design compromise would not work. \nRemember the attribute names need to make sense.\nSimple Loc\nLoc Key (PK)\nAddress+\nCity\nCity\nCity\nState\nCountry\n...\nMedium Loc\nLoc Key (PK)\nAddress+\nCity\nCity\nZone\nState\nCountry\n...\nComplex Loc\nLoc Key (PK)\nAddress+\nCity\nDistrict\nZone\nState\nCountry\n...\nFigure 7-7: Sample data values exist simultaneously in a single location dimension \ncontaining simple, intermediate, and complex hierarchies.\n Ragged Variable Depth Hierarchies\nIn  the budget use case, the organization structure is an excellent example of a ragged \nhierarchy of indeterminate depth. In this chapter, we often refer to the hierarchical struc-\nture as a “tree” and the individual organizations in that tree as “nodes.” Imagine your \nenterprise consists of 13 organizations with the rollup structure shown in Figure 7-8. \nEach of these organizations has its own budget, commitments, and payments.\nFor a single organization, you can request a speciﬁ c budget for an account with a \nsimple join from the organization dimension to the fact table, as shown in Figure 7-9. \nBut you also want to roll up the budget across portions of the tree or even all the tree. \nFigure 7-9 contains no information about the organizational rollup.\n",
      "page_number": 235
    },
    {
      "number": 26,
      "title": "Segment 26 (pages 244-251)",
      "start_page": 244,
      "end_page": 251,
      "detection_method": "topic_boundary",
      "content": "Chapter 7\n216\n1\n2\n3\n4\n5\n6\n10\n13\n11\n12\n8\n9\n7\nFigure 7-8: Organization rollup structure.\nOrganization Dimension\nPosting Date Key (FK)\nOrganization Key (FK)\nAccount Key (FK)\nTransaction Key (FK)\nLedger Key (FK)\nTransaction ID (DD)\nAmount\nBalance\nOrganization Key (PK)\nOrganization Name\n...\nGeneral Ledger Fact\nFigure 7-9: Organization dimension joined to fact table.\nThe  classic way to represent a parent/child tree structure is by placing recur-\nsive pointers in the organization dimension from each row to its parent, as shown \nin Figure 7-10. The original deﬁ nition of SQL did not provide a way to evaluate \nthese recursive pointers. Oracle implemented a CONNECT BY function that traversed \nthese pointers in a downward fashion starting at a high-level parent in the tree \nand progressively enumerated all the child nodes in lower levels until the tree \nwas exhausted. But the problem with Oracle CONNECT BY and other more general \napproaches, such as SQL Server’s recursive common table expressions, is that the \nrepresentation of the tree is entangled with the organization dimension because \nthese approaches depend on the recursive pointer embedded in the data. It is imprac-\ntical to switch from one rollup structure to another because many of the recursive \npointers would have to be destructively modiﬁ ed. It is also impractical to maintain \norganizations as type 2 slowly changing dimension attributes because changing the \nkey for a high-level node would ripple key changes down to the bottom of the tree.\nThe solution to the problem of representing arbitrary rollup structures is to build \na special kind of bridge table that is independent from the primary dimension table \nand contains all the information about the rollup. The grain of this bridge table is \n\n\nAccounting 217\neach path in the tree from a parent to all the children below that parent, as shown \nin Figure 7-11. The ﬁ rst column in the map table is the primary key of the parent, \nand the second column is the primary key of the child. A row must be constructed \nfrom each possible parent to each possible child, including a row that connects the \nparent to itself. \nOrganization Dimension\nOrganization Key (PK)\nOrganization Name\n...\nOrganization Parent Key (FK)\nRecursive\nPointer\nFigure 7-10: Classic parent/child recursive design.\nThe example tree depicted in Figure 7-8 results in 43 rows in Figure 7-11. There \nare 13 paths from node number 1, 5 paths from node number 2, one path from node \nnumber 3 to itself, as so on.\nThe highest parent ﬂ ag in the map table means the particular path comes from \nthe highest parent in the tree. The lowest child ﬂ ag means the particular path ends \nin a “leaf node” of the tree.\nIf you constrain the organization dimension table to a single row, you can join \nthe dimension table to the map table to the fact table, as shown in Figure 7-12. For \nexample, if you constrain the organization table to node number 1 and simply fetch \nan additive fact from the fact table, you get 13 hits on the fact table, which traverses \nthe entire tree in a single query. If you perform the same query except constrain the \nmap table lowest child ﬂ ag to true, then you fetch only the additive fact from the six \nleaf nodes, numbers 3, 5, 6, 8, 10, and 11. Again, this answer was computed without \ntraversing the tree at query time!\nNOTE \nThe article “Building Hierarchy Bridge Tables” (available at www\n.kimballgroup.com under the Tools and Utilities tab for this book title) provides \na code example for building the hierarchy bridge table described in this section.\nYou must be careful when using the map bridge table to constrain the organization \ndimension to a single row, or else you risk overcounting the children and grandchil-\ndren in the tree. For example, if instead of a constraint such as “Node Organization \nNumber = 1” you constrain on “Node Organization Location = California”, you \nwould have this problem. In this case you need to craft a custom query, rather than \na simple join, with the following constraint:\nGLfact.orgkey in (select distinct bridge.childkey\n              from innerorgdim, bridge\n              where innerorgdim.state = 'California' and\n              innerorgdim.orgkey = bridge.parentkey)\n\n\nChapter 7\n218\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n2\n2\n2\n2\n2\n3\n4\n4\n4\n5\n6\n7\n7\n8\n7\n7\n7\n7\n7\n9\n9\n9\n9\n9\n10\n10\n10\n11\n12\n13\nParent\nOrganization\nKey\nDepth from\nParent\nHighest\nParent\nFlag\nLowest\nChild\nFlag\nChild\nOrganization\nKey\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n2\n3\n4\n5\n6\n3\n4\n5\n6\n5\n6\n7\n8\n8\n9\n10\n11\n12\n13\n9\n10\n11\n12\n13\n10\n11\n12\n11\n12\n13\n0\n1\n2\n2\n3\n3\n1\n2\n2\n3\n4\n4\n3\n0\n1\n1\n2\n2\n0\n0\n1\n1\n0\n0\n0\n1\n0\n1\n2\n3\n3\n2\n0\n1\n2\n2\n1\n0\n1\n1\n0\n0\n0\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nTRUE\nFALSE\nTRUE\nTRUE\nFALSE\nTRUE\nFALSE\nFALSE\nTRUE\nTRUE\nTRUE\nFALSE\nTRUE\nFALSE\nTRUE\nTRUE\nTRUE\nFALSE\nTRUE\nTRUE\nTRUE\nTRUE\nFALSE\nTRUE\nTRUE\nFALSE\nFALSE\nTRUE\nTRUE\nTRUE\nFALSE\nFALSE\nTRUE\nTRUE\nTRUE\nFALSE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nOrganization Map Bridge\nParent Organization Key (FK)\nChild Organization Key (FK)\nDepth from Parent\nHighest Parent Flag\nLowest Child Flag\nSample Organization Map bridge table rows for Figure 7-8:\nFigure 7-11: Organization map bridge table sample rows.\n\n\nAccounting 219\nOrganization Map Bridge\nParent Organization Key (FK)\nChild Organization Key (FK)\nDepth from Parent\nHighest Parent Flag\nLowest Child Flag\nOrganization Dimension\nOrganization Key (PK)\nOrganization Name\n...\nPosting Date Key (FK)\nOrganization Key (FK)\nAccount Key (FK)\nTransaction Profile Key (FK)\nLedger Version Key (FK)\nTransaction ID (DD)\nAmount\nBalance\nGeneral Ledger Fact\nFIGURE 7-12: Joining organization map bridge table to fact table.\nShared Ownership in a Ragged Hierarchy\nThe  map table can represent partial or shared ownership, as shown in Figure 7-13. \nFor instance, suppose node 10 is 50 percent owned by node 6 and 50 percent \nowned by node 11. In this case, any budget or commitment or payment attributed \nto node 10 ﬂ ows upward through node 6 with a 50 percent weighting and also \nupward through node 11 with a 50 percent weighting. You now need to add extra \npath rows to the original 43 rows to accommodate the connection of node 10 up \nto node 6 and its parents. All the relevant path rows ending in node 10 now need \na 50 percent weighting in the ownership percentage column in the map table. \nOther path rows not ending in node 10 do not have their ownership percentage \ncolumn  changed.\nOrganization Map Bridge\nParent Organization Key (FK)\nChild Organization Key (FK)\nDepth from Parent\nHighest Parent Flag\nLowest Child Flag\nPercent Ownership\nOrganization Dimension\nOrganization Key (PK)\nOrganization Name\n...\nPosting Date Key (FK)\nOrganization Key (FK)\nAccount Key (FK)\nTransaction Profile Key (FK)\nLedger Version Key (FK)\nTransaction ID (DD)\nAmount\nBalance\nGeneral Ledger Fact\nFIGURE 7-13: Bridge table for ragged hierarchy with shared ownership.\n\n\nChapter 7\n220\n Time Varying Ragged Hierarchies\nThe  ragged hierarchy bridge table can accommodate slowly changing hierarchies \nwith the addition of two date/time stamps, as shown in Figure 7-14. When a given \nnode no longer is a child of another node, the end eff ective date/time of the old \nrelationship must be set to the date/time of the change, and new path rows inserted \ninto the bridge table with the correct begin eff ective date/time.\nOrganization Map Bridge\nParent Organization Key (FK)\nChild Organization Key (FK)\nDepth from Parent\nHighest Parent Flag\nLowest Child Flag\nBegin Effective Date/Time\nEnd Effective Date/Time\nOrganization Dimension\nOrganization Key (PK)\nOrganization Name\n...\nPosting Date Key (FK)\nOrganization Key (FK)\nAccount Key (FK)\nTransaction Profile Key (FK)\nLedger Version Key (FK)\nTransaction ID (DD)\nAmount\nBalance\nGeneral Ledger Fact\nFigure 7-14: Bridge table for time varying ragged hierarchies.\nWARNING \nWhen using the bridge table in Figure 7-14, the query must always \nconstrain to a single date/time to “freeze” the bridge table to a single consistent \nview of the hierarchy. Failing to constrain in this way otherwise would result in \nmultiple paths being fetched that could not exist at the same time.\nModifying Ragged Hierarchies\nThe  organization map bridge table can easily be modiﬁ ed. Suppose you want to \nmove nodes 4, 5, and 6 from their original location reporting up to node 2 to a new \nlocation reporting up to node 9, as shown in Figure 7-15.\nIn the static case in which the bridge table only reﬂ ects the current rollup struc-\nture, you merely delete the higher level paths in the tree pointing into the group \nof nodes 4, 5, and 6. Then you attach nodes 4, 5, and 6 into the parents 1, 7, and 9. \nHere is the static SQL:\nDelete from Org_Map where child_org in (4, 5,6) and\n  parent_org not in (4,5,6)\nInsert into Org_Map (parent_org, child_org)\n  select parent_org, 4 from Org_Map where parent_org in (1, 7, 9)\nInsert into Org_Map (parent_org, child_org)\n  select parent_org, 5 from Org_Map where parent_org in (1, 7, 9)\nInsert into Org_Map (parent_org, child_org)\n  select parent_org, 6 from Org_Map where parent_org in (1, 7, 9)\n\n\nAccounting 221\n11\n12\n5\n6\n10\n13\n4\n3\n8\n9\n2\n7\n1\nFigure 7-15: Changes to Figure 7-8’s organization structure.\nIn the time varying case in which the bridge table has the pair of date/time \nstamps, the logic is similar. You can ﬁ nd the higher level paths in the tree point-\ning into the group of nodes 4, 5, and 6 and set their end eff ective date/times to the \nmoment of the change. Then you attach nodes 4, 5, and 6 into the parents 1, 7, and \n9 with the appropriate date/times. Here is the time varying SQL:\nUpdate Org_Map set end_eff_date = #December 31, 2012#\n  where child_org in (4, 5,6) and parent_org not in (4,5,6)\n  and #Jan 1, 2013# between begin_eff_date and end_eff_date\nInsert into Org_Map\n  (parent_org, child_org, begin_eff_date, end_eff_date)\n  values (1, 4, #Jan 1, 2013#, #Dec 31, 9999#)\nInsert into Org_Map\n  (parent_org, child_org, begin_eff_date, end_eff_date)\n  values (7, 4, #Jan 1, 2013#, #Dec 31, 9999#) \nInsert into Org_Map\n  (parent_org, child_org, begin_eff_date, end_eff_date)\n  values (9, 4, #Jan 1, 2013#, #Dec 31, 9999#) \nIdentical insert statements for nodes 5 and 6 …\nThis simple recipe for changing the bridge table avoids nightmarish scenarios \nwhen changing other types of hierarchical models. In the bridge table, only the \npaths directly involved in the change are aff ected. All other paths are untouched. \nIn most other schemes with clever node labels, a change in the tree structure can \naff ect many or even all the nodes in the tree, as shown in the next  section.\n Alternative Ragged Hierarchy Modeling Approaches\nIn  addition to using recursive pointers in the organization dimension, there are at \nleast two other ways to model a ragged hierarchy, both involving clever columns \nplaced in the organization dimension. There are two disadvantages to these schemes \n\n\nChapter 7\n222\ncompared to the bridge table approach. First, the deﬁ nition of the hierarchy is locked \ninto the dimension and cannot easily be replaced. Second, both of these schemes are \nvulnerable to a relabeling disaster in which a large part of the tree must be relabeled \ndue to a single small change. Textbooks (like this one!) usually show a tiny example, \nbut you need to tread cautiously if there are thousands of nodes in your tree.\nOne scheme adds a pathstring attribute to the organization dimension table, \nas shown in Figure 7-16. The values of the pathstring attribute are shown within \neach node. In this scenario, there is no bridge table. At each level, the pathstring \nstarts with the full pathstring of the parent and then adds the letters A, B, C, and \nso on, from left to right under that parent. The ﬁ nal character is a “+” if the node \nhas children and is a period if the node has no children. The tree can be navigated \nby using wild cards in constraints against the pathstring, for example,\n \n■A* retrieves the whole tree where the asterisk is a variable length wild card.\n \n■*. retrieves only the leaf nodes.\n \n■?+ retrieves the topmost node where the question mark is a single character \nwild card.\nAA+\nAAA.\nAAB+\nAABA.\nAABB.\nABBA+\nABBB.\nABBAA.\nABBAB.\nABA.\nABB+\nAB+\nA+\nFigure 7-16: Alternate ragged hierarchy design using pathstring attribute.\nThe pathstring approach is fairly sensitive to relabeling ripples caused by orga-\nnization changes; if a new node is inserted somewhere in the tree, all the nodes to \nthe right of that node under the same parent must be relabeled.\nAnother similar scheme, known to computer scientists as the modiﬁ ed preordered \ntree traversal approach, numbers the tree as shown in Figure 7-17. Every node has a \npair of numbers that identiﬁ es all the nodes below that point. The whole tree can be \nenumerated by using the node numbers in the topmost node. If the values in each node \nhave the names Left and Right, then all the nodes in the example tree can be found with \n\n\nAccounting 223\nthe constraint “Left between 1 and 26.” Leaf nodes can be found where Left and Right \ndiff er by 1, meaning there aren’t any children. This approach is even more vulnerable \nto the relabeling disaster than the pathstring approach because the entire tree must \nbe carefully numbered in sequence, top to bottom and left to right. Any change to the \ntree causes the entire rest of the tree to the right to be relabeled.\n2,11\n3,4\n5,10\n6,7\n8,9\n16,21\n22,23\n17,18\n19,20\n13,14\n15,24\n12,25\n1,26\nFigure 7-17: Alternative ragged hierarchy design using the modiﬁ ed preordered tree \ntraversal approach.\nAdvantages of the Bridge Table Approach for Ragged \nHierarchies\nAlthough  the bridge table requires more ETL work to set up and more work when \nquerying, it off ers exceptional ﬂ exibility for analyzing ragged hierarches of inde-\nterminate depth. In particular, the bridge table allows\n \n■Alternative rollup structures to be selected at query time\n \n■Shared ownership rollups\n \n■Time varying ragged hierarchies\n \n■Limited impact when nodes undergo slowly changing dimension (SCD) \ntype 2 changes\n \n■Limited impact when the tree structure is changed\nYou can use the organization hierarchy bridge table to fetch a fact across all three \nfact tables in the budget chain. Figure 7-18 shows how an organization map table \ncan connect to the three budget chain fact tables. This would allow a drill-across \nreport such as ﬁ nding all the travel budgets, commitments, and payments made by \nall the lowest leaf nodes in a complex organizational  structure.\n",
      "page_number": 244
    },
    {
      "number": 27,
      "title": "Segment 27 (pages 252-259)",
      "start_page": 252,
      "end_page": 259,
      "detection_method": "topic_boundary",
      "content": "Chapter 7\n224\nOrganization Map\nParent Organization Key (FK)\nChild Organization Key (FK)\nDepth from Parent\nHighest Parent Flag\nLowest Child Flag\nOrganization Dimension\nOrganization Key\nCost Center Number\nCost Center Name\n...\nMonth Key (FK)\nOrganization Key (FK)\nAccount Key (FK)\nBudget Key (FK)\nBudget Amount\nMonth Key (FK)\nOrganization Key (FK)\nAccount Key (FK)\nBudget Key (FK)\nCommitment Key (FK)\nCommitment Amount\nMonth Key (FK)\nOrganization Key (FK)\nAccount Key (FK)\nBudget Key (FK)\nCommitment Key (FK)\nPayment Key (FK)\nPayment Amount\nPayment Fact\nBudget Fact\nCommitment Fact\nOrganization Dimension\nOrganization Dimension\nOrganization Dimension\nFigure 7-18: Drilling across and rolling up the budget chain.\n Consolidated Fact Tables\nIn  the last section, we discussed comparing metrics generated by separate business \nprocesses by drilling across fact tables, such as budget and commitments. If this \ntype of drill-across analysis is extremely common in the user community, it likely \nmakes sense to create a single fact table that combines the metrics once rather than \nrelying on business users or their BI reporting applications to stitch together result \nsets, especially given the inherent issues of complexity, accuracy, tool capabilities, \nand performance.\nMost typically, business managers are interested in comparing actual to budget \nvariances. At this point, you can presume the annual budgets and/or forecasts have \nbeen broken down by accounting period. Figure 7-19 shows the actual and budget \namounts, as well as the variance (which is a calculated diff erence) by the common \ndimensions.\n\n\nAccounting 225\nAccounting Period Key (FK)\nAccount Key (FK)\nOrganization Key (FK)\nAccounting Period Actual Amount\nAccounting Period Budget Amount\nAccounting Period Budget Variance\nBudget Variance Fact\nOrganization Dimension\nAccounting Period Dimension\nAccount Dimension\nFigure 7-19: Actual versus budget consolidated fact table.\nAgain, in a multinational organization, you would likely see the actual amounts \nin both local and the equivalent standard currency, based on the eff ective conversion \nrate. In addition, you may convert the actual results based on the planned currency \nconversion factor. Given the unpredictable nature of currency ﬂ uctuations, it is \nuseful to monitor performance based on both the eff ective and planned conversion \nrates. In this manner, remote managers aren’t penalized for currency rate changes \noutside their control. Likewise, ﬁ nance can better understand the big picture impact \nof unexpected currency conversion ﬂ uctuations on the organization’s annual plan.\nFact tables that combine metrics from multiple business processes at a com-\nmon granularity are referred to as consolidated fact tables. Although consolidated \nfact tables can be useful, both in terms of performance and usability, they often \nrepresent a dimensionality compromise as they consolidate facts at the “least \ncommon denominator” of dimensionality. One potential risk associated with \nconsolidated fact tables is that project teams sometimes base designs solely on \nthe granularity of the consolidated table, while failing to meet user requirements \nthat demand the ability to dive into more granular data. These schemas run into \nserious problems if project teams attempt to force a one-to-one correspondence \nto combine data with diff erent granularity or dimensionality.\nNOTE \nWhen facts from multiple business processes are combined in a consoli-\ndated fact table, they must live at the same level of granularity and dimensionality. \nBecause the separate facts seldom naturally live at a common grain, you are forced \nto eliminate or aggregate some dimensions to support the one-to-one correspon-\ndence, while retaining the atomic data in separate fact tables. Project teams should \nnot create artiﬁ cial facts or dimensions in an attempt to force-ﬁ t the consolidation \nof diff erently grained fact data.\n\n\nChapter 7\n226\n Role of OLAP and Packaged Analytic \nSolutions\nWhile discussing ﬁ nancial dimensional models in the context of relational data-\nbases, it is worth noting that multidimensional OLAP vendors have long played a \nrole in this arena. OLAP products have been used extensively for ﬁ nancial reporting, \nbudgeting, and consolidation applications. Relational dimensional models often feed \nﬁ nancial OLAP cubes. OLAP cubes can deliver fast query performance that is critical \nfor executive usage. The data volumes, especially for general ledger balances or ﬁ nan-\ncial statement aggregates, do not typically overwhelm the practical size constraints \nof a multidimensional product. OLAP is well suited to handle complicated organiza-\ntional rollups, as well as complex calculations, including inter-row manipulations. \nMost OLAP vendors provide ﬁ nance-speciﬁ c capabilities, such as ﬁ nancial functions \n(for example, net present value or compound growth), the appropriate handling of \nﬁ nancial statement data (in the expected sequential order such as income before \nexpenses), and the proper treatment of debits and credits depending on the account \ntype, as well as more advanced functions such as ﬁ nancial consolidation. OLAP \ncubes often also readily support complex security models, such as limiting access \nto detailed data while providing more open access to summary metrics.\nGiven the standard nature of general ledger processing, purchasing a general \nledger package rather than attempting to build one from scratch has been a popu-\nlar route for years. Nearly all the operational packages also off er a complementary \nanalytic solution, sometimes in partnership with an OLAP vendor. In many cases, \nprecanned solutions based on the vendor’s cumulative experience are a sound way \nto jump start a ﬁ nancial DW/BI implementation with potentially reduced cost and \nrisk. The analytic solutions often have tools to assist with the extraction and staging \nof the operational data, as well as tools to assist with analysis and interpretation. \nHowever, when leveraging packaged solutions, you need to be cautious in order to \navoid stovepipe applications. You could easily end up with separate ﬁ nancial, CRM, \nhuman resources, and ERP packaged analytic solutions from as many diff erent \nvendors, none of which integrate with other internal data. You need to conform \ndimensions across the entire DW/BI environment, regardless of whether you build \na solution or implement packages. Packaged analytic solutions can turbocharge a \nDW/BI implementation; however, they do not alleviate the need for conformance. \nMost organizations inevitably rely on a combination of building, buying, and inte-\ngrating for a complete  solution.\n\n\nAccounting 227\nSummary\nIn this chapter, we focused primarily on ﬁ nancial data in the general ledger, both in \nterms of periodic snapshots as well as journal entry transactions. We discussed the \nhandling of common G/L data challenges, including multiple currencies, multiple \nﬁ scal years, unbalanced organizational trees, and the urge to create to-date totals.\nWe used the familiar organization rollup structure to show how to model complex \nragged hierarchies of indeterminate depth. We introduced a special bridge table for \nthese hierarchies, and compared this approach to others.\nWe explored the series of events in a budgeting process chain. We described the \nuse of “net change” granularity in this situation rather than creating snapshots of \nthe budget data totals. We also discussed the concept of consolidated fact tables \nthat combine the results of separate business processes when they are frequently \nanalyzed together.\nFinally, we discussed the natural ﬁ t of OLAP products for ﬁ nancial analysis. We \nalso stressed the importance of integrating analytic packages into the overall DW/\nBI environment through the use of conformed dimensions .\n\n\nCustomer \nRelationship \nManagement\nL\nong  before the customer relationship management (CRM) buzzword existed, \norganizations were designing and developing customer-centric dimensional \nmodels to better understand their customers’ behavior. For decades, these models \nwere used to respond to management’s inquiries about which customers were solic-\nited, who responded, and what was the magnitude of their response. The business \nvalue of understanding the full spectrum of customers’ interactions and transactions \nhas propelled CRM to the top of the charts. CRM not only includes familiar resi-\ndential and commercial customers, but also citizens, patients, students, and many \nother categories of people and organizations whose behavior and preferences are \nimportant. CRM is a mission-critical business strategy that many view as essential \nto an organization’s survival. \nIn this chapter we start with a CRM overview, including its operational and ana-\nlytic roles. We then introduce the basic design of the customer dimension, including \ncommon attributes such as dates, segmentation attributes, repeated contact roles, \nand aggregated facts. We discuss customer name and address parsing, along with \ninternational considerations. We remind you of the challenges of modeling complex \nhierarchies when we describe various kinds of customer hierarchies.\nChapter 8 discusses the following concepts:\n \n■CRM overview\n \n■Customer name and address parsing, including international considerations\n \n■Handling of dates, aggregated facts, and segmentation behavior attributes and \nscores in a customer dimension\n \n■Outriggers for low cardinality attributes\n \n■Bridge tables for sparse attributes, along with trade-off s of bridge tables versus \na positional design\n \n■Bridge tables for multiple customer contacts\n \n■Behavior study groups to capture customer cohort groups\n8\n\n\nChapter 8\n230\n \n■Step dimensions to analyze sequential customer behavior\n \n■Timespan fact tables with eff ective and expiration dates\n \n■Embellishing fact tables with dimensions for satisfaction or abnormal scenarios\n \n■Integrating customer data via master data management or partial conformity \nduring the downstream ETL processing\n \n■Warnings about fact-to-fact table joins\n \n■Reality check on real time, low latency requirements\nBecause this chapter’s customer-centric modeling issues and patterns are relevant \nacross industries and functional areas, we have not included a bus matrix.\nCRM Overview\nRegardless  of the industry, organizations have ﬂ ocked to the concept of CRM. \nThey’ve jumped on the bandwagon in an attempt to migrate from a product-centric \norientation to one that is driven by customer needs. Although all-encompassing \nterms such as customer relationship management sometimes seem ambiguous and/\nor overly ambitious, the premise behind CRM is far from rocket science. It’s based \non the simple notion that the better you know your customers, the better you can \nmaintain long-lasting, valuable relationships with them. The goal of CRM is to \nmaximize relationships with your customers over their lifetime. It entails focus-\ning all aspects of the business, from marketing, sales, operations, and service, on \nestablishing and sustaining mutually beneﬁ cial customer relations. To do so, the \norganization must develop a single, integrated view of each customer.\nCRM promises signiﬁ cant returns for organizations that embrace it, both for \nincreased revenue and operational effi  ciencies. Switching to a customer-driven \nperspective can lead to increased sales eff ectiveness and closure rates, revenue \ngrowth, enhanced sales productivity at reduced cost, improved customer proﬁ t-\nability margins, higher customer satisfaction, and increased customer retention. \nUltimately, every organization wants more loyal, more proﬁ table customers. As it \noften requires a sizeable investment to attract new customers, you can’t aff ord to \nhave the proﬁ table ones leave.\nIn many organizations, the view of the customer varies depending on the product \nline business unit, business function, and/or geographic location. Each group may \nuse diff erent customer data in diff erent ways with diff erent results. The evolution \nfrom the existing silos to a more integrated perspective obviously requires organi-\nzational commitment. CRM is like a stick of dynamite that knocks down the silo \nwalls. It requires the right integration of business processes, people resources, and \napplication technology to be eff ective.\nOver  the past decade, the explosive growth of social media, location tracking tech-\nnology, network usage monitoring, multimedia applications, and sensor networks \n\n\nCustomer Relationship Management 231\nhas provided an ocean of customer behavioral data that even Main Street enterprises \nrecognize as providing actionable insights. Although much of this data lies outside \nthe comfort zone of relational databases, the new “big data” techniques can bring \nthis data back into the DW/BI fold. Chapter 21: Big Data Analytics discusses the \nbest practices for bringing this new kind of big data into the DW/BI environment. \nBut setting aside the purely technological challenges, the real message is the need \nfor profound integration. You must step up to the challenge of integrating as many \nas 100 customer-facing data sources, most of which are external. These data sources \nare at diff erent grains, have incompatible customer attributes, and are not under \nyour control. Any questions?\nBecause it is human nature to resist change, it comes as no surprise that people-\nrelated issues often challenge CRM implementations. CRM involves brand new \nways of interacting with customers and often entails radical changes to the sales \nchannels. CRM requires new information ﬂ ows based on the complete acquisition \nand dissemination of customer “touch point” data. Often organization structures and \nincentive systems are dramatically altered. \nIn Chapter 17: Kimball DW/BI Lifecycle Overview, we’ll stress the importance of \nhaving support from both senior business and IT management for a DW/BI initiative. \nThis advice also applies to a CRM implementation because of its cross-functional \nfocus. CRM requires a clear business vision. Without business strategy, buy-in, and \nauthorization to change, CRM becomes an exercise in futility. Neither IT nor the \nbusiness community can successfully implement CRM on its own; CRM demands \na joint commitment of support.\nOperational and Analytic CRM\nIt  could be said that CRM suff ers from a split personality syndrome because it needs \nto address both operational and analytic requirements. Eff ective CRM relies on the \ncollection of data at every interaction you have with a customer and then leveraging \nthat breadth of data through analysis. \nOn the operational front, CRM calls for the synchronization of customer-facing \nprocesses. Often operational systems must either be updated or supplemented to coor-\ndinate across sales, marketing, operations, and service. Think about all the customer \ninteractions that occur during the purchase and usage of a product or service, from \nthe initial prospect contact, quote generation, purchase transaction, fulﬁ llment, pay-\nment transaction, and on-going customer service. Rather than thinking about these \nprocesses as independent silos (or multiple silos varying by product line), the CRM \nmindset is to integrate these customer activities. Key customer metrics and charac-\nteristics are collected at each touch point and made available to the others.\nAs data is created on the operational side of the CRM equation, you obviously \nneed to store and analyze the historical metrics resulting from the customer \n\n\nChapter 8\n232\ninteraction and transaction systems. Sounds familiar, doesn’t it? The DW/BI system \nsits at the core of CRM. It serves as the repository to collect and integrate the \nbreadth of customer information found in the operational systems, as well as from \nexternal sources. The data warehouse is the foundation that supports the panoramic \n360-degree view of your customers. \nAnalytic CRM is enabled via accurate, integrated, and accessible customer data \nin the DW/BI system. You can measure the eff ectiveness of decisions made in the \npast to optimize future interactions. Customer data can be leveraged to better iden-\ntify up-sell and cross-sell opportunities, pinpoint ineffi  ciencies, generate demand, \nand improve retention. In addition, the historical, integrated data can be leveraged \nto generate models or scores that close the loop back to the operational world. \nRecalling the major components of a DW/BI environment from Chapter 1: Data \nWarehousing, Business Intelligence, and Dimensional Modeling Primer, you can \nenvision the model results pushed back to where the relationship is operationally \nmanaged (such as the rep, call center, or website), as illustrated in Figure 8-1. The \nmodel output can translate into speciﬁ c proactive or reactive tactics recommended \nfor the next point of customer contact, such as the appropriate next product off er or \nanti-attrition response. The model results are also retained in the DW/BI environ-\nment for subsequent analysis.\nIntegrate\n(ETL)\nStore\n(Data Presentation)\nAnalyze and\nReport\n(BI Applications)\nModel\n(BI Applications)\nCollect\n(Operational\nSource System)\nFigure 8-1: Closed loop analytic CRM.\n",
      "page_number": 252
    },
    {
      "number": 28,
      "title": "Segment 28 (pages 260-269)",
      "start_page": 260,
      "end_page": 269,
      "detection_method": "topic_boundary",
      "content": "Customer Relationship Management 233\nIn  other situations, information must feed back to the operational website or call \ncenter systems on a more real-time basis. In this case, the closed loop is much tighter \nthan Figure 8-1 because it’s a matter of collection and storage, and then feedback \nto the collection system. Today’s operational processes must combine the current \nview with a historical view, so a decision maker can decide, for example, whether \nto grant credit to a customer in real time, while considering the customer’s lifetime \nhistory. But generally, the integration requirements for operational CRM are not as \nfar reaching as for analytic CRM. \nObviously, as the organization becomes more centered on the customer, so must \nthe DW/BI system. CRM will inevitably drive change in the data warehouse. DW/BI \nenvironments will grow even more rapidly as you collect more and more informa-\ntion about your customers. ETL processes will grow more complicated as you match \nand integrate data from multiple sources. Most important, the need for a conformed \ncustomer dimension becomes even more paramount. \nCustomer Dimension Attributes\nThe  conformed customer dimension is a critical element for eff ective CRM. A well-\nmaintained, well-deployed conformed customer dimension is the cornerstone of \nsound CRM analysis.\nThe customer dimension is typically the most challenging dimension for any \nDW/BI system. In a large organization, the customer dimension can be extremely \ndeep (with many millions of rows), extremely wide (with dozens or even hundreds \nof attributes), and sometimes subject to rapid change. The biggest retailers, credit \ncard companies, and government agencies have monster customer dimensions whose \nsize exceeds 100 million rows. To further complicate matters, the customer dimen-\nsion often represents an amalgamation of data from multiple internal and external \nsource systems. \nIn this next section, we focus on numerous customer dimension design con-\nsiderations. We’ll begin with name/address parsing and other common customer \nattributes, including coverage of dimension outriggers, and then move on to other \ninteresting customer attributes. Of course, the list of customer attributes is typically \nquite lengthy. The more descriptive information you capture about your customers, \nthe more robust the customer dimension, and the more interesting the analyses.\nName and Address Parsing\nRegardless  of whether you deal with individual human beings or commercial enti-\nties, customers’ name and address attributes are typically captured. The operational \nhandling of name and address information is usually too simplistic to be very useful \n\n\nChapter 8\n234\nin the DW/BI system. Many designers feel a liberal design of general purpose col-\numns for names and addresses, such as Name-1 through Name-3 and Address-1 \nthrough Address-6, can handle any situation. Unfortunately, these catchall columns \nare virtually worthless when it comes to better understanding and segmenting \nthe customer base. Designing the name and location columns in a generic way \ncan actually contribute to data quality problems. Consider the sample design in \nFigure 8-2 with general purpose columns.\nMs. R. Jane Smith, Atty\n123 Main Rd, North West, Ste 100A\nPO Box 2348\nKensington\nArk.\n88887-2348 \n888-555-3333 x776 main, 555-4444 fax\nName\nAddress 1 \nAddress 2 \nCity \nState\nZIP Code \nPhone Number\nColumn\nSample Data Value\nFigure 8-2: Sample customer name/address data in overly general columns.\nIn this design, the name column is far too limited. There is no consistent mecha-\nnism for handling salutations, titles, or suffi  xes. You can’t identify what the person’s \nﬁ rst name is, or how she should be addressed in a personalized greeting. If you \nlook at additional sample data from this operational system, you would potentially \nﬁ nd multiple customers listed in a single name attribute. You might also ﬁ nd addi-\ntional descriptive information in the name column, such as Conﬁ dential, Trustee, \nor UGMA (Uniform Gift to Minors Act).\nIn the sample address attributes, inconsistent abbreviations are used in various \nplaces. The address columns may contain enough room for any address, but there \nis no discipline imposed by the columns that can guarantee conformance with \npostal authority regulations or support address matching and latitude/longitude \nidentiﬁ cation.\nInstead of using a few, general purpose columns, the name and location attributes \nshould be broken down into as many elemental parts as possible. The extract process \nneeds to perform signiﬁ cant parsing on the original dirty names and addresses. After \nthe attributes have been parsed, they can be standardized. For example, Rd would \nbecome Road and Ste would become Suite. The attributes can also be veriﬁ ed, such \nas verifying the ZIP code and associated state combination is correct. Fortunately, \nthere are name and address data cleansing and scrubbing tools available in the \nmarket to assist with parsing, standardization, and veriﬁ cation.\n\n\nCustomer Relationship Management 235\nA sample set of name and location attributes for individuals in the United States is \nshown in Figure 8-3. Every attribute is ﬁ lled in with sample data to make the design \nclearer, but no single real instance would look like this. Of course, the business data \ngovernance representatives should be involved in determining the analytic value of \nthese parsed data elements in the customer dimension.\nMs.\nJane\nMs. Smith\nR. Jane\nSmith\nJr.\nEnglish\nAttorney \n123 \nMain\nRoad\nNorth West\nKensington\nCornwall\nBerkeleyshire\nArkansas\nSouth\nUnited States\nNorth America\n88887 \n2348 \nUnited States\n1 \n888 \n5553333 \n776 \n1 \n509 \n5554444 \nRJSmith@ABCGenIntl.com\nwww.ABCGenIntl.com\nX.509 \nVerisign\n7346531\nSalutation\nInformal Greeting Name\nFormal Greeting Name\nFirst and Middle Names\nSurname\nSuffix\nEthnicity\nTitle\nStreet Number\nStreet Name\nStreet Type\nStreet Direction\nCity\nDistrict\nSecond District\nState\nRegion\nCountry\nContinent\nPrimary Postal Code\nSecondary Postal Code\nPostal Code Type\nOffice Telephone Country Code\nOffice Telephone Area Code\nOffice Telephone Number\nOffice Extension\nMobile Telephone Country Code\nMobile Telephone Area Code\nMobile Telephone Number\nE-mail\nWeb Site\nPublic Key Authentication\nCertificate Authority\nUnique Individual Identifier\nColumn\nSample Data Value\nFigure 8-3: Sample customer name/address data with parsed name and address \nelements.\n\n\nChapter 8\n236\nCommercial customers typically have multiple addresses, such as physical and \nshipping addresses; each of these addresses would follow much the same logic as \nthe address structure shown in Figure 8-3.\nInternational Name and Address Considerations\nInternational  display and printing typically requires representing foreign characters, \nincluding not just the accented characters from western European alphabets, but \nalso Cyrillic, Arabic, Japanese, Chinese, and dozens of other less familiar writing \nsystems. It is important to understand this is not a font problem. This is a character \nset problem. A font is simply an artist’s rendering of a set of characters. There are \nhundreds of fonts available for standard English, but standard English has a rela-\ntively small character set that is enough for anyone’s use unless you are a professional \ntypographer. This small character set is usually encoded in American Standard Code \nfor Information Interchange (ASCII), which is an 8-bit encoding that has a maximum \nof 255 possible characters. Only approximately 100 of these 255 characters have a \nstandard interpretation that can be invoked from a normal English keyboard, but \nthis is usually enough for English speaking computer users. It should be clear that \nASCII is woefully inadequate for representing the thousands of characters needed \nfor non-English writing systems.\nAn  international body of system architects, the Unicode Consortium, deﬁ ned a \nstandard known as Unicode for representing characters and alphabets in almost all \nthe world’s languages and cultures. Their work can be accessed on the web at www.\nunicode.org. The Unicode Standard, version 6.2.0 has deﬁ ned speciﬁ c interpreta-\ntions for 110,182 possible characters and now covers the principal written languages \nof the Americas, Europe, the Middle East, Africa, India, Asia, and Paciﬁ ca. Unicode \nis the foundation you must use for addressing international character sets. \nBut it is important to understand that implementing Unicode solutions is done in \nthe foundation layers of your systems. First, the operating system must be Unicode-\ncompliant. Fortunately, the most current releases of all the major operating systems \nare Unicode-compliant.\nAbove the operating system, all the devices that capture, store, transmit, and \nprint characters must be Unicode-compliant. Data warehouse back room tools must \nbe Unicode-compliant, including sort packages, programming languages, and auto-\nmated ETL packages. Finally, the DW/BI applications, including database engines, \nBI application servers and their report writers and query tools, web servers, and \nbrowsers must all be Unicode-compliant. The DW/BI architect should not only talk \nto the vendors of each package in the data pipeline, but also should conduct various \nend-to-end tests. Capture some names and addresses with Unicode characters at \nthe data capture screens of one of the legacy applications, and send them through the \nsystem. Get them to print out of a ﬁ nal report or a ﬁ nal browser window from \n\n\nCustomer Relationship Management 237\nthe DW/BI system and see if the special characters are still there. That simple \ntest will cut through a lot of the confusion. Note that even when you do this, the \nsame character, such as an a-umlaut, sorts diff erently in diff erent countries such as \nNorway and Germany. Even though you can’t solve all the variations in international \ncollating sequences, at least both the Norwegians and the Germans will agree that \nthe character is an a-umlaut.\nCustomer geographic attributes become more complicated if you deal with cus-\ntomers from multiple countries. Even if you don’t have international customers, you \nmay need to contend with international names and addresses somewhere in the \nDW/BI system for international suppliers and human resources personnel records.\nNOTE \nCustomer dimensions sometimes include a full address block attribute. \nThis is a specially crafted column that assembles a postally-valid address for the \ncustomer including mail stop, ZIP code, and other attributes needed to satisfy postal \nauthorities. This attribute is useful for international locations where addresses \nhave local idiosyncrasies.\nInternational DW/BI Goals\nAfter  committing to a Unicode foundation, you need to keep the following goals in \nmind, in addition to the name and address parsing requirements discussed earlier:\n \n■Universal and consistent. As they say, in for a penny, in for a pound. If you \nare going to design a system for international use, you want it to work around \nthe world. You need to think carefully if BI tools are to produce translated ver-\nsions of reports in many languages. It may be tempting to provide translated \nversions of dimensions for each language, but translated dimensions give rise \nto some subtle problems. \n \n■Sorting  sequences will be different, so either the reports will be sorted \ndifferently or all reports except those in the “root” language will appear \nto be unsorted. \n \n■If the attribute cardinalities are not faithfully preserved across lan-\nguages, then either group totals will not be the same across reports, or \nsome groups in various languages will contain duplicated row headers \nthat look like mistakes. To avoid the worst of these problems, you \nshould translate dimensions after the report is run; the report first \nneeds to be produced in a single root language, and then the report \nface needs to be translated into the intended target languages. \n \n■All  the BI tool messages and prompts need to be translated for the \nbenefit of the business user. This process is known as localization and \nis further discussed in Chapter 12: Transportation. \n\n\nChapter 8\n238\n \n■End-to-end data quality and downstream compatibility. The data warehouse \ncannot be the only step in the data pipeline that worries about the integrity \nof international names and addresses. A proper design requires support from \nthe ﬁ rst step of capturing the name and the address, through the data cleaning \nand storage steps, to the ﬁ nal steps of performing geographic and demographic \nanalysis and printing reports.\n \n■Cultural correctness. In many cases, foreign customers and partners will see \nthe results from your DW/BI system in some form. If we don’t understand \nwhich name is a ﬁ rst name and which is a last name, and if you don’t under-\nstand how to refer to a person, you run the risk of insulting these individuals, \nor at the very least, looking stupid. When outputs are punctuated improperly, or \nmisspelled, your foreign customers and partners will wish they were doing \nbusiness with a local company, rather than you.\n \n■Real-time  customer response. DW/BI systems can play an operational role \nby supporting real-time customer response systems. A customer service rep-\nresentative may answer the telephone and may have 5 seconds or less to wait \nfor a greeting to appear on the screen that the data warehouse recommends \nusing with the customer. The greeting may include a proper salutation and \na proper use of the customer’s title and name. This greeting represents an \nexcellent use of a hot response cache that contains precalculated responses \nfor each customer.\n \n■Other kinds of addresses. We are in the midst of a revolution in communication \nand networking. If you are designing a system for identifying international \nnames and addresses, you must anticipate the need to store electronic names, \nsecurity tokens, and internet addresses.\nSimilar to international addresses, telephone numbers must be presented \ndiff erently depending on where the phone call originates. You need to provide \nattributes to represent the complete foreign dialing sequence, complete domestic \ndialing sequence, and local dialing sequence. Unfortunately, complete foreign dial-\ning sequences vary by origin country. \n Customer-Centric Dates\nCustomer  dimensions often contains dates, such as the date of the ﬁ rst purchase, \ndate of last purchase, and date of birth. Although these dates initially may be SQL \ndate type columns, if you want to summarize these dates by your unique calen-\ndar attributes, such as seasons, quarters, and ﬁ scal periods, the dates should be \nchanged to foreign key references to the date dimension. You need to be careful \nthat all such dates fall within the span of the corporate date dimension. These date \ndimension roles are declared as semantically distinct views, such as a First Purchase \n\n\nCustomer Relationship Management 239\nDate dimension table with unique column labels. The system behaves as if there \nis another physical date table. Constraints on any of these tables have nothing to \ndo with constraints on the primary date dimension table. This design, as shown \nin Figure 8-4, is an example of a dimension outrigger, which is discussed in the \nsection “Outrigger for Low Cardinality Attribute Set.”\nDate of 1st Purchase Key (PK)\nDate of 1st Purchase\nDate of 1st Purchase Month\nDate of 1st Purchase Year\nDate of 1st Purchase Fiscal Month\nDate of 1st Purchase Fiscal Quarter\nDate of 1st Purchase Fiscal Year\nDate of 1st Purchase Season\n…\nDate of 1st Purchase Dimension\nCustomer Key (PK)\nCustomer ID (Natural Key)\nCustomer Salutation\nCustomer First Name\nCustomer Surname\nCustomer City\nCustomer State\n…\nDate of 1st Purchase Key (FK)\nCustomer Dimension\nTransaction Date Key (FK)\nCustomer Key (FK)\nMore FKs …\nFacts …\nFact Table\nFigure 8-4: Date dimension outrigger.\n Aggregated Facts as Dimension Attributes\nBusiness  users are often interested in constraining the customer dimension based on \naggregated performance metrics, such as ﬁ ltering on all customers who spent more \nthan a certain dollar amount during last year. Or to make matters worse, perhaps \nthey want to constrain based on how much the customer has purchased in a lifetime. \nProviding aggregated facts as dimension attributes is sure to be a crowd-pleaser with \nthe business users. They could issue a query to identify all customers who satisﬁ ed the \nspending criteria and then issue another fact query to analyze the behavior for \nthat customer dimension subset. But rather than all that, you can instead store \nan aggregated fact as a dimension attribute. This allows business users to simply \nconstrain on the spending attribute just like they might on a geographic attribute. \nThese attributes are meant to be used for constraining and labeling; they’re not to be \nused in numeric calculations. Although there are query usability and performance \nadvantages of storing these attributes, the main burden falls on the back room ETL \nprocesses to ensure the attributes are accurate, up-to-date, and consistent with the \nactual fact rows. These attributes can require signiﬁ cant care and feeding. If you \nopt to include some aggregated facts as dimension attributes, be certain to focus on \nthose that will be frequently used. Also strive to minimize the frequency with which \nthese attributes need to be updated. For example, an attribute for last year’s spending \nwould require much less maintenance than one providing year-to-date behavior. \nRather than storing attributes down to the speciﬁ c dollar value, they are sometimes \n\n\nChapter 8\n240\nreplaced (or supplemented) with more meaningful descriptive values, such as High \nSpender as discussed in the next section. These descriptive values minimize your \nvulnerability that the numeric attributes might not tie back to the appropriate fact \ntables. In addition, they ensure that all users have a consistent deﬁ nition for high \nspenders, for example, rather than resorting to their own individual business rules.\nSegmentation Attributes and Scores\nSome  of the most powerful attributes in a customer dimension are segmentation \nclassiﬁ cations. These attributes obviously vary greatly by business context. For an \nindividual customer, they may include:\n \n■Gender\n \n■Ethnicity\n \n■Age or other life stage classiﬁ cations\n \n■Income or other lifestyle classiﬁ cations\n \n■Status (such as new, active, inactive, and closed)\n \n■Referring source\n \n■Business-speciﬁ c market segment (such as a preferred customer identiﬁ er)\nSimilarly,  many organizations score their customers to characterize them. \nStatistical segmentation models typically generate these scores which cluster cus-\ntomers in a variety of ways, such as based on their purchase behavior, payment \nbehavior, propensity to churn, or probability to default. Each customer is tagged \nwith a resultant score.\n Behavior Tag Time Series \nOne  popular approach for scoring and proﬁ ling customers looks at the recency (R), \nfrequency (F), and intensity (I) of the customer’s behavior. These are known as the \nRFI measures; sometimes intensity is replaced with monetary (M), so it’s also known \nas RFM. Recency is how many days has it been since the customer last ordered \nor visited your site. Frequency is how many times the customer has ordered or \nvisited, typically in the past year. And intensity is how much money the customer \nhas spent over the same time period. When dealing with a large customer base, \nevery customer’s behavior can be modeled as a point in an RFI cube, as depicted \nin Figure 8-5. In this ﬁ gure, the scales along each axis are quintiles, from 1 to 5, \nwhich spread the actual values into even groups.\nIf you have millions of points in the cube, it becomes diffi  cult to see meaning-\nful clusters of these points. This is a good time to ask a data mining professional \nwhere the meaningful clusters are. The data mining professional may come back \nwith a list of behavior tags like the following, which is drawn from a slightly more \ncomplicated scenario that includes credit behavior and returns:\n\n\nCustomer Relationship Management 241\nA: High volume repeat customer, good credit, few product returns\nB: High volume repeat customer, good credit, many product returns\nC: Recent new customer, no established credit pattern\nD: Occasional customer, good credit\nE: Occasional customer, poor credit\nF: Former good customer, not seen recently\nG: Frequent window shopper, mostly unproductive\nH: Other\n5\nHighest\nLowest\nHighest\nLowest\nHighest\nLowest\nRecency\nFrequency\nIntensity\n4\n3\n2\n1\n1\n1\n2\n3\n4\n5\n2\n3\n4\n5\nFigure 8-5: Recency, frequency, intensity (RFI) cube.\nNow you can look at the customers’ time series data and associate each customer \nin each reporting period with the nearest cluster. The data miner can help do this. \nThus, the last 10 observations of a customer named John Doe could look like:\nJohn Doe: C C C D D A A A B B\nThis time series of behavior tags is unusual because although it comes from \na regular periodic measurement process, the observed “values” are textual. The \nbehavior tags are not numeric and cannot be computed or averaged, but they can \nbe queried. For example, you may want to ﬁ nd all the customers who were an A \nsometime in the ﬁ fth, fourth, or third prior period and were a B in the second or ﬁ rst \nprior period. Perhaps you are concerned by progressions like this and fear losing a \nvaluable customer because of the increasing number of returns.\nBehavior  tags should not be stored as regular facts. The main use of behavior tags \nis formulating complex query patterns like the example in the previous paragraph. \nIf the behavior tags were stored in separate fact rows, such querying would be \nextremely diffi  cult, requiring a cascade of correlated subqueries. The recommended \nway to handle behavior tags is to build an explicit time series of attributes in the \ncustomer dimension. This is another example of a positional design. BI interfaces \n\n\nChapter 8\n242\nare simple because the columns are in the same table, and performance is good \nbecause you can build bitmapped indexes on them.\nIn addition to the separate columns for each behavior tag time period, it would \nbe a good idea to create a single attribute with all the behavior tags concatenated \ntogether, such as CCCDDAAABB. This column would support wild card searches \nfor exotic patterns, such as “D followed by a B.” \nNOTE \nIn addition to the customer dimension’s time series of behavior tags, it \nwould be reasonable to include the contemporary behavior tag value in a mini-\ndimension to analyze facts by the behavior tag in eff ect when the fact row was \nloaded.\nRelationship Between Data Mining and DW/BI System\nThe  data mining team can be a great client of the data warehouse, and especially \ngreat users of customer behavior data. However, there can be a mismatch between \nthe velocity that the data warehouse can deliver data and the velocity that the data \nminers can consume data. For example, a decision tree tool can process hundreds \nof records per second, but a big drill-across report that produces “customer observa-\ntions” can never deliver data at such speeds. Consider the following seven-way drill \nacross a report that might produce millions of customer observations from census, \ndemographic, external credit, internal credit, purchases, returns, and website data:\nSELECT Customer Identifier, Census Tract, City, County, State,\n  Postal Code, Demographic Cluster, Age, Sex, Marital Status,\n  Years of Residency, Number of Dependents, Employment Profile,\n  Education Profile, Sports Magazine Reader Flag,\n  Personal Computer Owner Flag, Cellular Telephone Owner Flag,\n  Current Credit Rating, Worst Historical Credit Rating,\n  Best Historical Credit Rating, Date First Purchase,\n  Date Last Purchase, Number Purchases Last Year,\n  Change in Number Purchases vs. Previous Year,\n  Total Number Purchases Lifetime, Total Value Purchases Lifetime,\n  Number Returned Purchases Lifetime, Maximum Debt,\n  Average Age Customer's Debt Lifetime, Number Late Payments,\n  Number Fully Paid, Times Visited Web Site,\n  Change in Frequency of Web Site Access,\n  Number of Pages Visited Per Session,\n  Average Dwell Time Per Session, Number Web Product Orders,\n  Value Web Product Orders, Number Web Site Visits to Partner Web \n  Sites, Change in Partner Web Site Visits\nFROM *** WHERE *** ORDER BY *** GROUP BY ***\n",
      "page_number": 260
    },
    {
      "number": 29,
      "title": "Segment 29 (pages 270-278)",
      "start_page": 270,
      "end_page": 278,
      "detection_method": "topic_boundary",
      "content": "Customer Relationship Management 243\nData mining teams would love this data! For example a big ﬁ le of millions of these \nobservations could be analyzed by a decision tree tool where the tool is “aimed” \nat the Total Value Purchases Lifetime column, which is highlighted above. In this \nanalysis, the decision tree tool would determine which of the other columns “predict \nthe variance” of the target ﬁ eld. Maybe the answer is Best Historical Credit Rating \nand Number of Dependents. Armed with this answer, the enterprise now has a \nsimple way to predict who is going to be a good lifetime customer, without needing \nto know all the other data content.\nBut the data mining team wants to use these observations over and over for \ndiff erent kinds of analyses perhaps with neural networks or case-based reasoning \ntools. Rather than producing this answer set on demand as a big, expensive query, \nthis answer set should be written to a ﬁ le and given to the data mining team to \nanalyze on its servers.\n Counts with Type 2 Dimension Changes\nBusinesses  frequently want to count customers based on their attributes without \njoining to a fact table. If you used type 2 to track customer dimension changes, \nyou need to be careful to avoid overcounting because you may have multiple rows \nin the customer dimension for the same individual. Doing a COUNT DISTINCT on a \nunique customer identiﬁ er is a possibility, assuming the attribute is indeed unique \nand durable. A current row indicator in the customer dimension is also helpful to \ndo counts based on the most up-to-date descriptive values for a customer. \nThings get more complicated if you need to do a customer count at a given histori-\ncal point in time using eff ective and expiration dates in the customer dimension. For \nexample, if you need to know the number of customers you had at the beginning of \n2013, you could constrain the row eff ective date <= ‘1/1/2013’ and row expiration \ndate >= ‘1/1/2013’ to restrict the result set to only those rows that were valid on \n1/1/2013. Note the comparison operators are dependent on the business rules used \nto set the row eff ective/expiration dates. In this example, the row expiration date on \nthe no longer valid customer row is 1 day less than the eff ective date on the new  row.\n Outrigger for Low Cardinality Attribute Set\nIn  Chapter 3: Retail Sales, we encouraged designers to avoid snowﬂ aking where low \ncardinality columns in the dimension are removed to separate normalized tables, \nwhich then link back into the original dimension table. Generally, snowﬂ aking is \nnot recommended in a DW/BI environment because it almost always makes the \nuser presentation more complex, in addition to negatively impacting browsing per-\nformance. In spite of this prohibition against snowﬂ aking, there are some special \n\n\nChapter 8\n244\nsituations in which it is permissible to build a dimension outrigger that begins to \nlook like a snowﬂ aked table.\nIn Figure 8-6, the dimension outrigger is a set of data from an external data pro-\nvider consisting of 150 demographic and socio-economic attributes regarding the \ncustomers’ county of residence. The data for all customers residing in a given county \nis identical. Rather than repeating this large block of data for every customer within \na county, opt to model it as an outrigger. There are several reasons for bending the \n“no snowﬂ ake” rule. First, the demographic data is available at a signiﬁ cantly dif-\nferent grain than the primary dimension data and it’s not as analytically valuable. \nIt is loaded at diff erent times than the rest of the data in the customer dimension. \nAlso, you do save signiﬁ cant space in this case if the underlying customer dimen-\nsion is large. If you have a query tool that insists on a classic star schema with no \nsnowﬂ akes, the outrigger can be hidden under a view declaration.\nCountry Demographics Outrigger Dimension\nCounty Demographics Key (PK)\nTotal Population\nPopulation under 5 Years\n% Population under 5 Years\nPopulation under 18 Years\n% Population under 18 Years\nPopulation 65 Years and Older\n% Population 65 Years and Older\nFemale Population\n% Female Population\nMale Population\n% Male Population\nNumber of High School Graduates\nNumber of College Graduates\nNumber of Housing Units\nHome Ownership Rate\n...\nCustomer Dimension\nCustomer Key (PK)\nCustomer ID (Natural Key)\nCustomer Salutation\nCustomer First Name\nCustomer Surname\nCustomer City\nCustomer County\nCounty Demographics Key (FK)\nCustomer State\n...\nFact Table\nCustomer Key (FK)\nMore FKs ...\nFacts ...\nFigure 8-6: Dimension outrigger for cluster of low cardinality attributes.\nWARNING \nDimension outriggers are permissible, but they should be the \nexception rather than the rule. A red warning ﬂ ag should go up if your design \nis riddled with outriggers; you may have succumbed to the temptation to overly \nnormalize the design.\nCustomer Hierarchy Considerations\nOne  of the most challenging aspects of dealing with commercial customers is mod-\neling their internal organizational hierarchy. Commercial customers often have a \n\n\nCustomer Relationship Management 245\nnested hierarchy of entities ranging from individual locations or organizations up \nthrough regional offi  ces, business unit headquarters, and ultimate parent companies. \nThese hierarchical relationships may change frequently as customers reorganize \nthemselves internally or are involved in acquisitions and divestitures. \nNOTE \nIn Chapter 7: Accounting, we described how to handle ﬁ xed hierar-\nchies, slightly variable hierarchies, and ragged hierarchies of indeterminate depth. \nChapter 7 focuses on ﬁ nancial cost center rollups, but the techniques are exactly \ntransferrable to customer hierarchies. If you skipped Chapter 7, you need to back-\ntrack to read that chapter to make sense of the following recommendations.\nAlthough relatively uncommon, the lucky ones amongst us sometimes are \nconfronted with a customer hierarchy that has a highly predictable ﬁ xed number \nof levels. Suppose you track a maximum of three rollup levels, such as the ultimate \ncorporate parent, business unit headquarters, and regional headquarters. In this \ncase, you have three distinct attributes in the customer dimension corresponding \nto these three levels. For commercial customers with complicated organizational \nhierarchies, you’d populate all three levels to appropriately represent the three diff er-\nent entities involved at each rollup level. This is the ﬁ xed depth hierarchy approach \nfrom Chapter 7.\nBy contrast, if another customer had a mixture of one, two, and three level \norganizations, you’d duplicate the lower-level value to populate the higher-level attri-\nbutes. In this way, all regional headquarters would sum to the sum of all business \nunit headquarters, which would sum to the sum of all ultimate corporate parents. \nYou can report by any level of the hierarchy and see the complete customer base \nrepresented. This is the slightly variable hierarchy approach.\nBut in many cases, complex commercial customer hierarchies are ragged hier-\narchies with an indeterminate depth, so you must use a ragged hierarchy modeling \ntechnique, as described in Chapter 7. For example, if a utility company is devising a \ncustom rate plan for all the utility consumers that are part of a huge customer with \nmany levels of offi  ces, branch locations, manufacturing locations, and sales loca-\ntions, you cannot use a ﬁ xed hierarchy. As pointed out in Chapter 7, the worst design \nis a set of generic levels named such as Level-1, Level-2, and so on. This makes for \nan unusable customer dimension because you don’t know how to constrain against \nthese levels when you have a ragged hierarchy of indeterminate depth.\n Bridge Tables for Multivalued Dimensions\nA  fundamental tenet of dimensional modeling is to decide on the grain of the fact \ntable, and then carefully add dimensions and facts to the design that are true to \nthe grain. For example, if you record customer purchase transactions, the grain of \n\n\nChapter 8\n246\nthe individual purchase is natural and physically compelling. You do not want to \nchange that grain. Thus you normally require any dimension attached to this fact \ntable to take on a single value because then there’s a clean single foreign key in the \nfact table that identiﬁ es a single member of the dimension. Dimensions such as \nthe customer, location, product or service, and time are always single valued. But \nyou may have some “problem” dimensions that take on multiple values at the grain \nof the individual transaction. Common examples of these multivalued dimensions \ninclude:\n \n■Demographic descriptors drawn from a multiplicity of sources\n \n■Contact addresses for a commercial customer \n \n■Professional skills of a job applicant\n \n■Hobbies of an individual\n \n■Diagnoses or symptoms of a patient\n \n■Optional features for an automobile or truck\n \n■Joint account holders in a bank account\n \n■Tenants in a rental property\nWhen faced with a multivalued dimension, there are two basic choices: a posi-\ntional design or bridge table design. Positional designs are very attractive because \nthe multivalued dimension is spread out into named columns that are easy to query. \nFor example, if modeling the hobbies of an individual as previously mentioned, \nyou could have a hobby dimension with named columns for all the hobbies gath-\nered from your customers, including stamp collecting, coin collecting, astronomy, \nphotography, and many others! Immediately you can see the problem. The posi-\ntional design approach isn’t very scalable. You can easily run out of columns in \nyour database, and it is awkward to add new columns. Also if you have a column \nfor every possible hobby, then any single individual’s hobby dimension row will \ncontain mostly null values.\nThe bridge table approach to multivalued dimensions is powerful but comes \nwith a big compromise. The bridge table removes the scalability and null value \nobjections because rows in the bridge table exist only if they are actually needed, \nand you can add hundreds or even thousands of hobbies in the previous example. \nBut the resulting table design requires a complex query that must be hidden from \ndirect view by the business users.\nWARNING \nBe aware that complex queries using bridge tables may require SQL \nthat is beyond the normal reach of BI tools. \n\n\nCustomer Relationship Management 247\nIn the next two sections, we illustrate multivalued bridge table designs that \nﬁ t with the customer-centric topics of this chapter. We will revisit multivalued \nbridges in Chapter 9: Human Resources Management, Chapter 10: Financial \nServices, Chapter 13: Education, Chapter 14:  Healthcare, and Chapter 16: Insurance. \nWe’ll then describe how to build these bridges in Chapter 19: ETL Subsystems and \nTechniques.\nBridge Table for Sparse Attributes\nOrganizations  are increasingly collecting demographics and status information \nabout their customers, but the traditional ﬁ xed column modeling approach for \nhandling these attributes becomes diffi  cult to scale with hundreds of attributes.\nPositional designs have a named column for each attribute. BI tool interfaces are \neasy to construct for positional attributes because the named columns are easily \npresented in the tool. Because many columns contain low cardinality contents, the \nquery performance using these attributes can be very good if bitmapped indexes \nare placed on each column. Positional designs can be scaled up to perhaps 100 or \nso columns before the databases and user interfaces become awkward or hard to \nmaintain. Columnar databases are well suited to these kinds of designs because \nnew columns can be easily added with minimal disruption to the internal storage \nof the data, and the low-cardinality columns containing only a few discrete values \nare dramatically compressed.\nWhen the number of diff erent attributes grows beyond your comfort zone, and \nif new attributes are added frequently, a bridge table is recommended. Ultimately, \nwhen you have a very large and expanding set of demographics indicators, using \noutriggers or mini-dimensions simply does not gracefully scale. For example, you \nmay collect loan application information as a set of open ended name-value pairs, \nas shown in Figure 8-7. Name-value pair data is interesting because the values can \nbe numeric, textual, a ﬁ le pointer, a URL, or even a recursive reference to enclosed \nname-value pair data.\nOver a period of time, you could collect hundreds or even thousands of diff erent \nloan application variables. For a true name-value pair data source, the value ﬁ eld \nitself can be stored as a text string to handle the open-ended modality of the val-\nues, which is interpreted by the analysis application. In these situations whenever \nthe number of variables is open-ended and unpredictable, a bridge table design is \nappropriate, as shown in Figure  8-8.\n\n\nChapter 8\n248\nLoan Application Name-Value Pair Data\nPhotograph: <image>\nPrimary Income: $72345 \nOther Taxable Income: $2345 \nTax-Free Income: $3456\nLong Term Gains: $2367 \nGarnished Wages: $789 \nPending Judgment Potential: $555 \nAlimony: $666 \nJointly Owned Real Estate Appraised Value: $123456 \nJointly Owned Real Estate Image: <image>\nJointly Owned Real Estate MLS Listing: <URL>\nPercentage Ownership Real Estate: 50 \nNumber Dependents: 4 \nPre-existing Medical Disability: Back Injury\nNumber of Weeks Lost to Disability: 6\nEmployer Disability Support Statement: <document archive>\nPrevious Bankruptcy Declaration Type: 11 \nYears Since Bankruptcy: 8 \nSpouse Financial Disclosure: <name-value pair>\n... 100 more name-value pairs...\nFigure 8-7: Loan application name-value pair data.\nLoan Application Fact\nApplication Date Key (FK)\nApplicant Key (FK)\nLoan Type Key (FK)\nApplication ID (DD)\nLoan Officer Key (FK)\nUnderwriter Key (FK)\nBranch Key (FK)\nStatus Key (FK)\nApplication Disclosure Key (FK)\nApplication Disclosure Dimension\nApplication Disclosure Key (PK)\nApplication Disclosure Description\nApplication Disclosure Bridge\nApplication Disclosure Key (FK)\nDisclosure Item Key (FK)\nDisclosure Item Dimension\nDisclosure Item Key (PK)\nItem Name\nItem Value Type\nItem Value Text String\nFigure 8-8: Bridge table for wide and sparse name-value pair data set.\nBridge Table for Multiple Customer Contacts\nLarge  commercial customers have many points of contact, including decision mak-\ners, purchasing agents, department heads, and user liaisons; each point of contact is \nassociated with a speciﬁ c role. Because the number of contacts is unpredictable but \npossibly large, a bridge table design is a convenient way to handle this situation, as \nshown in Figure 8-9. Some care should be taken not to overdo the contact dimen-\nsion and make it a dumping ground for every employee or citizen or salesperson or \nhuman being the organization interacts with. Restrict the dimension for this use \ncase of contacts as part of the customer relationship.\n\n\nCustomer Relationship Management 249\nCustomer Dimension\nCustomer Key (PK)\nCustomer Name \nCustomer Type\nCustomer Contact Group (FK)\nDate of First Contact\n...\nContact Group Dimension\nContact Group Key (PK)\nContact Group Name\nContact Group Bridge\nContact Group Key (FK)\nContact Key (FK)\nContact Role\nContact Dimension\nContact Key (PK)\nContact Name\nContact Street Address\n...\nFigure 8-9: Bridge table design for multiple contacts.\nComplex Customer Behavior\nCustomer behavior can be very complex. In this section, we’ll discuss the han-\ndling of customer cohort groups and capturing sequential behavior. We’ll also cover \nprecise timespan fact tables and tagging fact events with indicators of customer \nsatisfaction or abnormal scenarios.\n Behavior Study Groups for Cohorts\nWith  customer analysis, simple queries such as how much was sold to custom-\ners in this geographic area in the past year rapidly evolve to much more complex \ninquiries, such as how many customers bought more this past month than their \naverage monthly purchase amount from last year. The latter question is too complex \nfor business users to express in a single SQL request. Some BI tool vendors allow \nembedded subqueries, whereas others have implemented drill-across capabilities \nin which complex requests are broken into multiple select statements and then \ncombined in a subsequent pass.\nIn  other situations, you may want to capture the set of customers from a query or \nexception report, such as the top 100 customers from last year, customers who spent \nmore than $1,000 last month, or customers who received a speciﬁ c test solicitation, and \nthen use that group of customers, called a behavior study group, for subsequent analyses \nwithout reprocessing to identify the initial condition. To create a behavior study group, \nrun a query (or series of queries) to identify the set of customers you want to further \nanalyze, and then capture the customer durable keys of the identiﬁ ed set as an actual \nphysical table consisting of a single customer key column. By leveraging the custom-\ners’ durable keys, the study group dimension is impervious to type 2 changes to the \ncustomer dimension which may occur after the study group members are identiﬁ ed.\nNOTE \nThe secret to building complex behavioral study group queries is to \ncapture the keys of the customers or products whose behavior you are tracking. \nYou then use the captured keys to subsequently constrain other fact tables without \nhaving to rerun the original behavior analysis.\n\n\nChapter 8\n250\nYou can now use this special behavior study group dimension table of customer \nkeys whenever you want to constrain any analysis on any table to that set of spe-\ncially deﬁ ned customers. The only requirement is that the fact table contains a \ncustomer key reference. The use of the behavior study group dimension is shown \nin Figure 8-10.\nCustomer Behavior Study\nGroup Dimension\nCustomer ID (Durable Key)\nCustomer Dimension\nCustomer Key (PK)\nCustomer ID (Durable Key)\n...\nPOS Retail Sales Transaction Fact\nDate Key (FK)\nCustomer Key (FK)\nMore FKs ...\nSales Quantity\nSales Dollar Amount\nFigure 8-10: Behavior study group dimension joined to customer dimension’s \ndurable key.\nThe behavior study group dimension is attached with an equijoin to the customer \ndimension’s durable key (refer to Customer ID in Figure 8-10). This can even be \ndone in a view that hides the explicit join to the behavior dimension. In this way, \nthe resulting dimensional model looks and behaves like an uncomplicated star. If the \nspecial dimension table is hidden under a view, it should be labeled to uniquely \nidentify it as being associated with the top 100 customers, for example. Virtually \nany BI tool can now analyze this specially restricted schema without paying syn-\ntax or user-interface penalties for the complex processing that deﬁ ned the original \nsubset of customers.\nNOTE \nThe exceptional simplicity of study group tables allows them to be com-\nbined with union, intersection, and set diff erence operations. For example, a set of \nproblem customers this month can be intersected with the set of problem custom-\ners from last month to identify customers who were problems for two consecutive \nmonths.\nStudy groups can be made even more powerful by including an occurrence date \nas a second column correlated with each durable key. For example, a panel study \nof consumer purchases can be conducted where consumers enter the study when \nthey exhibit some behavior such as switching brands of peanut butter. Then fur-\nther purchases can be tracked after the event to see if they switched brands again. \nTo get this right, these purchase events must be tracked with the right time stamps to \nget the behavior in the right sequence.\nLike many design decisions, this one represents certain compromises. First, \nthis approach requires a user interface for capturing, creating, and administering \n\n\nCustomer Relationship Management 251\nreal physical behavior study group tables in the data warehouse. After a complex \nexception report has been deﬁ ned, you need the ability to capture the resulting keys \ninto an applet to create the special behavior study group dimension. These study \ngroup tables must live in the same space as the primary fact table because they are \ngoing to be joined directly to the customer dimension table. This obviously aff ects \nthe DBA’s responsibilities. \n Step Dimension for Sequential Behavior\nMost  DW/BI systems do not have good examples of sequential processes. Usually \nmeasurements are taken at a particular place watching the stream of customers or \nproducts going by. Sequential measurements, by contrast, need to follow a customer \nor a product through a series of steps, often measured by diff erent data capture \nsystems. Perhaps the most familiar example of a sequential process comes from \nweb events where a session is constructed by collecting individual page events on \nmultiple web servers tied together via a customer’s cookie. Understanding where \nan individual step ﬁ ts in the overall sequence is a major challenge when analyzing \nsequential processes.\nBy introducing a step dimension, you can place an individual step into the context \nof an overall session, as shown in Figure 8-11. \nTransaction Fact\nTransaction Date Key (FK)\nCustomer Key (FK)\nSession Key (FK)\nTransaction Number (DD)\nSession Step Key (FK)\nPurchase Step Key (FK)\nAbandon Step Key (FK)\nMore FKs ...\nFacts...\nStep Dimension (3 Roles)\nStep Key (PK)\nTotal Number Steps\nThis Step Number\nSteps Until End\nSample Step Dimension Rows:\nStep\nKey\nTotal\nNumber\nSteps\nThis\nStep\nNumber\nSteps\nUntil\nEnd\n1 \n2 \n3 \n4 \n5 \n6 \n7 \n8 \n9 \n10\n1 \n2 \n2 \n3 \n3 \n3 \n4 \n4 \n4 \n4\n1 \n1 \n2 \n1 \n2 \n3 \n1 \n2 \n3 \n4\n0 \n1 \n0 \n2 \n1 \n0 \n3 \n2 \n1 \n0\nFigure 8-11: Step dimension to capture sequential activities.\nThe step dimension is an abstract dimension deﬁ ned in advance. The ﬁ rst row in \nthe dimension is used only for one-step sessions, where the current step is the ﬁ rst \nstep and there are no more steps remaining. The next two rows in the step dimension \nare used for two-step sessions. The ﬁ rst row (Step Key = 2) is for step number 1 where \nthere is one more step to go, and the next row (Step Key = 3) is for step number 2 \n",
      "page_number": 270
    },
    {
      "number": 30,
      "title": "Segment 30 (pages 279-286)",
      "start_page": 279,
      "end_page": 286,
      "detection_method": "topic_boundary",
      "content": "Chapter 8\n252\nwhere there are no more steps. The step dimension can be prebuilt to accommodate \nsessions of at least 100 steps. In Figure 8-11 you see the step dimension can be asso-\nciated with a transaction fact table whose grain is the individual page event. In this \nexample, the step dimension has three roles. The ﬁ rst role is the overall session. The \nsecond role is a successful purchase subsession, where a sequence of page events \nleads to a conﬁ rmed purchase. The third role is the abandoned shopping cart, where \nthe sequence of page events is terminated without a purchase.\nUsing the step dimension, a speciﬁ c page can immediately be placed into one or \nmore understandable contexts (overall session, successful purchase, and abandoned \nshopping cart). But even more interestingly, a query can constrain exclusively only \nto the ﬁ rst page of successful purchases. This is a classic web event query, where \nthe “attractant” page of successful sessions is identiﬁ ed. Conversely, a query could \nconstrain exclusively to the last page of abandoned shopping carts, where the cus-\ntomer is about to decide to go elsewhere.\nAnother approach for modeling sequential behavior takes advantage of speciﬁ c \nﬁ xed codes for each possible step. If you track customer product purchases in a \nretail environment, and if each product can be encoded, for instance, as a 5 digit \nnumber, then you can create a single wide text column for each customer with the \nsequence of product codes. You separate the codes with a unique non-numeric \ncharacter. Such a sequence might look like\n11254|45882|53340|74934|21399|93636|36217|87952|…etc.\nNow using wild cards you can search for speciﬁ c products bought sequentially, \nor bought with other products intervening, or situations in which one product was \nbought but another was never bought. Modern relational DBMSs can store and \nprocess wide text ﬁ elds of 64,000 characters or more with wild card searches.\n Timespan Fact Tables\nIn more  operational applications, you may want to retrieve the exact status of a cus-\ntomer at some arbitrary instant in the past. Was the customer on fraud alert when \ndenied an extension of credit? How long had he been on fraud alert? How many \ntimes in the past two years has he been on fraud alert? How many customers were on \nfraud alert at some point in the past two years? All these questions can be addressed \nif you carefully manage the transaction fact table containing all customer events. The \nkey modeling step is to include a pair of date/time stamps, as shown in Figure 8-12. \nThe ﬁ rst date/time stamp is the precise moment of the transaction, and the second \ndate/time stamp is the exact moment of the next transaction. If this is done correctly, \nthen the time history of customer transactions maintains an unbroken sequence \nof date/time stamps with no gaps. Each actual transaction enables you to associate \n\n\nCustomer Relationship Management 253\nboth demographics and status with the customer. Dense transaction fact tables are \ninteresting because you potentially can change the demographics and especially \nthe status each time a transaction occurs.\nDate Dimension\nDemographics Dimension\nCustomer Dimension\nStatus Dimension\nCustomer Transaction Fact\nTransaction Date Key (FK)\nCustomer Key (FK)\nDemographics Key (FK)\nStatus Key (FK)\nTransaction Number (DD)\nMore FKs ...\nBegin Effective Date/Time\nEnd Effective Date/Time\nAmount\nFigure 8-12: Twin date/time stamps in a timespan fact table.\nThe critical insight is that the pair of date/time stamps on a given transaction \ndeﬁ nes a span of time in which the demographics and the status are constant. \nQueries can take advantage of this “quiet” span of time. Thus if you want to know \nwhat the status of the customer “Jane Smith” was on July 18, 2013 at 6:33 am, you \ncan issue the following query:\nSelect Customer.Customer_Name, Status\nFrom Transaction_Fact, Customer_dim, Status_dim\nWhere Transaction_Fact_Customer_Key = Customer_dim.Customer_key\n    And Transaction_Fact.Status_key = Status_dim.Status_key\n    And Customer_dim.Customer_Name = 'Jane Smith' \n    And #July 18, 2013 6:33:00# >= Transaction_Fact.Begin_Eff_\nDateTime\n    And #July 18, 2013 6:33:00# < Transaction_Fact.End_Eff_DateTime\nThese date/time stamps can be used to perform tricky queries on your customer \nbase. If you want to ﬁ nd all the customers who were on fraud alert sometime in the \nyear 2013, issue the following query:\nSelect Customer.Customer_Name\nFrom Transaction_Fact, Customer_dim, Status_dim\nWhere <joins>\n   And Status_dim Status_Description = 'Fraud Alert'\n   And Transaction_Fact.Begin_Eff_DateTime <= 12/31/2013:23:59:59\n   And Transaction_Fact.End_Eff_DateTime >= 1/1/2013:0:0:0\nAmazingly, this one query handles all the possible cases of begin and end eff ec-\ntive date/times straddling the beginning or end of 2013, being entirely contained \nwith 2013, or completely straddling 2013.\n\n\nChapter 8\n254\nYou can even count the number of days each customer was on fraud alert in 2013:\nSelect Customer.Customer_Name,\n    sum( least(12/31/2013:23:59:59, Transaction_Fact.End_Eff_\nDateTime) \n       - greatest(1/1/2013:0:0:0, Transaction_Fact.Begin_Eff_\nDateTime))\nFrom Transaction_Fact, Customer_dim, Status_dim\nWhere <joins>\n   And Status_dim Status_Description = 'Fraud Alert'\n   And Transaction_Fact.Begin_Eff_DateTime <= 12/31/2013:23:59:59\n   And Transaction_Fact.End_Eff_DateTime >= 1/1/2013:0:0:0\nGroup By Customer.Customer_Name\nBack Room Administration of Dual Date/Time Stamps\nFor a given customer, the date/time stamps on the sequence of transactions must \nform a perfect unbroken sequence with no gaps. It is tempting to make the end \neff ective date/time stamp be one “tick” less than the beginning eff ective date/time \nstamp of the next transaction, so the query SQL can use the BETWEEN syntax \nrather than the uglier constraints shown above. However, in many situations the \nlittle gap deﬁ ned by that tick could be signiﬁ cant if a transaction could fall within \nthe gap. By making the end eff ective date/time exactly equal to the begin date time \nof the next transaction, you eliminate this risk.\nUsing the pair of date/time stamps requires a two-step process whenever a new \ntransaction row is entered. In the ﬁ rst step, the end eff ective date/time stamp of \nthe most current transaction must be set to a ﬁ ctitious date/time far in the future. \nAlthough it would be semantically correct to insert NULL for this date/time, nulls \nbecome a headache when you encounter them in constraints because they can cause \na database error when you ask if the ﬁ eld is equal to a speciﬁ c value. By using a \nﬁ ctitious date/time far in the future, this problem is avoided.\nIn the second step, after the new transaction is entered into the database, the ETL \nprocess must retrieve the previous transaction and set its end eff ective date/time to \nthe date/time of the newly entered transaction. Although this two-step process is a \nnoticeable cost of this twin date/time approach, it is a classic and desirable trade-\noff  between extra ETL overhead in the back room and reduced query complexity \nin the front  room.\n Tagging Fact Tables with Satisfaction Indicators\nAlthough  proﬁ tability might be the most important key performance indicator in \nmany organizations, customer satisfaction is a close second. And in organizations \nwithout proﬁ t metrics, such as government agencies, satisfaction is (or should be) \nnumber one.\n\n\nCustomer Relationship Management 255\nSatisfaction, like profitability, requires integration across many sources. \nVirtually every customer facing process is a potential source of satisfaction infor-\nmation, whether the source is sales, returns, customer support, billing, website \nactivity, social media, or even geopositioning data.\nSatisfaction data can be either numeric or textual. In the Chapter 6: Order \nManagement, you saw how classic measures of customer satisfaction could be \nmodeled both ways simultaneously. The on-time measures could be both additive \nnumeric facts as well as textual attributes in a service level dimension. Other purely \nnumeric measures of satisfaction include numbers of product returns, numbers \nof lost customers, numbers of support calls, and product attitude metrics from \nsocial media.\nFigure 8-13 illustrates a frequent ﬂ yer satisfaction dimension that could be added \nto the ﬂ ight activity fact tables described in Chapter 12. Textual satisfaction data is \ngenerally modeled in two ways, depending on the number of satisfaction attributes \nand the sparsity of the incoming data. When the list of satisfaction attributes is \nbounded and reasonably stable, a positional design is very eff ective, as shown in \nFigure 8-13. \nSatisfaction Dimension\nSatisfaction Key (PK)\nDelayed Arrival Indicator\nDiversion to Other Airport Indicator\nLost Luggage Indicator\nFailure to Get Upgrade Indicator\nMiddle Seat Indicator\nPersonnel Problem Indicator\nFigure 8-13: Positional satisfaction dimension for airline frequent ﬂ yers.\nTagging Fact Tables with Abnormal \nScenario Indicators\nAccumulating  snapshot fact tables depend on a series of dates that implement the \n“standard scenario” for the pipeline process. For order fulﬁ llment, you may have \nthe steps of order created, order shipped, order delivered, order paid, and order \nreturned as standard steps in the order scenario. This kind of design is successful \nwhen 90 percent or more of the orders progress through these steps (hopefully \nwithout the return) without any unusual exceptions.\nBut if an occasional situation deviates from the standard scenario, you don’t \nhave a good way to reveal what happened. For example, maybe when the order \n\n\nChapter 8\n256\nwas shipped, the delivery truck had a ﬂ at tire. A decision was made to unload the \ndelivery to another truck, but unfortunately it began to rain and the shipment was \nwater damaged. Then it was refused by the customer, and ultimately there was a \nlawsuit. None of these unusual steps are modeled in the standard scenario in the \naccumulating snapshot. Nor should they be!\nThe way to describe unusual departures from the standard scenario is to add \na delivery status dimension to the accumulating snapshot fact table. For the case \nof the weird delivery scenario, you tag this order fulﬁ llment row with the status \nWeird. Then if the analyst wants to see the complete story, the analyst can join to a \ncompanion transaction fact table through the order number and line number that \nhas every step of the story. The transaction fact table joins to a transaction dimen-\nsion, which indeed has Flat Tire, Damaged Shipment, and Lawsuit as transactions. \nEven though this transaction dimension will grow over time with unusual entries, \nit is well bounded and stable.\n Customer Data Integration Approaches\nIn  typical environments with many customer facing processes, you need to choose \nbetween two approaches: a single customer dimension derived from all the versions \nof customer source system records or multiple customer dimensions tied together \nby conformed attributes.\nMaster Data Management Creating a Single \nCustomer Dimension\nIn  some cases, you can build a single customer dimension that is the “best of breed” \nchoice among a number of available customer data sources. It is likely that such \na conformed customer dimension is a distillation of data from several operational \nsystems within your organization. But it would be typical for a unique customer to \nhave multiple identiﬁ ers in multiple touch point systems. To make matters worse, \ndata entry systems often don’t incorporate adequate validation rules. Obviously, an \noperational CRM objective is to create a unique customer identiﬁ er and restrict the \ncreation of unnecessary identiﬁ ers. In the meantime, the DW/BI team will likely \nbe responsible for sorting out and integrating the disparate sources of customer \ninformation.\nSome  organizations are lucky enough to have a centralized master data manage-\nment (MDM) system that takes responsibility for creating and controlling the single \nenterprise-wide customer entity. But such centralization is rare in the real world. \nMore frequently, the data warehouse extracts multiple incompatible customer data \n\n\nCustomer Relationship Management 257\nﬁ les and builds a “downstream” MDM system. These two styles of MDM are illus-\ntrated in Figure 8-14.\nEnterprise\nMDM\nDownstream\nMDM\nOperational\nApp #1\nOperational\nApp #2\nOperational\nApp #3\nOperational\nApp #1\nOperational\nApp #2\nOperational\nApp #3\nEDW\nEDW\nFigure 8-14: Two styles of master data management.\nUnfortunately,  there’s no secret weapon for tackling this data consolidation. The \nattributes in the customer dimension should represent the “best” source available \nin the enterprise. A national change of address (NCOA) process should be integrated \nto ensure address changes are captured. Much of the heavy lifting associated with \ncustomer data consolidation demands customer matching or deduplicating logic. \nRemoving duplicates or invalid addresses from large customer lists is critical to \neliminate the costs associated with redundant, misdirected, or undeliverable com-\nmunication, avoid misleading customer counts, and improve customer satisfaction \nthrough higher quality communication.\nThe  science of customer matching is more sophisticated than it might ﬁ rst appear. \nIt involves fuzzy logic, address parsing algorithms, and enormous look-up direc-\ntories to validate address elements and postal codes, which vary signiﬁ cantly by \ncountry. There are specialized, commercially available software and service off erings \nthat perform individual customer or commercial entity matching with remarkable \naccuracy. Often these products match the address components to standardized \ncensus codes, such as state codes, country codes, census tracts, block groups, metro-\npolitan statistical areas (MSAs), and latitude/longitude, which facilitate the merging \n\n\nChapter 8\n258\nof external data. As discussed in Chapter 10, there are also householding capabilities \nto group or link customers sharing similar name and/or address information. Rather \nthan merely performing intraﬁ le matching, some services maintain an enormous \nexternal reference ﬁ le of everyone in the United States to match against. Although \nthese products and services are expensive and/or complex, it’s worthwhile to make \nthe investment if customer matching is strategic to the organization. In the end, \neff ective consolidation of customer data depends on a balance of capturing the data \nas accurately as possible in the source systems, coupled with powerful data cleans-\ning/merging tools in the ETL process.\nPartial Conformity of Multiple Customer Dimensions\nEnterprises  today build customer knowledge stores that collect all the internal and \nexternal customer-facing data sources they can ﬁ nd. A large organization could \nhave as many as 20 internal data sources and 50 or more external data sources, \nall of which relate in some way to the customer. These sources can vary wildly in \ngranularity and consistency. Of course, there is no guaranteed high-quality customer \nkey deﬁ ned across all these data sources and no consistent attributes. You don’t have \nany control over these sources. It seems like a hopeless mess.\nIn Chapter 4: Inventory, we laid the groundwork for conformed dimensions, \nwhich are the required glue for achieving integration across separate data sources. \nIn the ideal case, you examine all the data sources and deﬁ ne a single compre-\nhensive dimension which you attach to all the data sources, either simultaneously \nwithin a single tablespace or by replicating across multiple tablespaces. Such a \nsingle comprehensive conformed dimension becomes a wonderful driver for creating \nintegrated queries, analyses, and reports by making consistent row labels available \nfor drill-across queries.\nBut in the extreme integration world with dozens of customer-related dimensions \nof diff erent granularity and diff erent quality, such a single comprehensive customer \ndimension is impossible to build. Fortunately, you can implement a lighter weight \nkind of conformed customer dimension. Remember the essential requirement for \ntwo dimensions to be conformed is they share one or more specially administered \nattributes that have the same column names and data values. Instead of requiring \ndozens of customer-related dimensions to be identical, you only require they share \nthe specially administered conformed attributes.\nNot  only have you taken the pressure off  the data warehouse by relaxing the \nrequirement that all the customer dimensions in your environment be equal from \ntop to bottom, but in addition you can proceed in an incremental and agile way \nto plant the specially administered conformed attributes in each of the customer-\nrelated dimensions. For example, suppose you start by deﬁ ning a fairly high-level \n\n\nCustomer Relationship Management 259\ncategorization of customers called customer category. You can proceed methodically \nacross all the customer-related dimensions, planting this attribute in each dimension \nwithout changing the grain of any target dimension and without invalidating any \nexisting applications that depend on those dimensions. Over a period of time, you \ngradually increase the scope of integration as you add the special attributes to the \nseparate customer dimensions attached to diff erent sources. At any point in time, \nyou can stop and perform drill-across reports using the dimensions where you have \ninserted the customer category attribute.\nWhen the customer category attribute has been inserted into as many of the \ncustomer-related dimensions as possible, you can then deﬁ ne more conformed attri-\nbutes. Geographic attributes such as city, county, state, and country should be even \neasier than the customer category. Over a period of time, the scope and power of the \nconformed customer dimensions let you do increasingly sophisticated analyses. This \nincremental development with its closely spaced deliverables ﬁ ts an agile approach.\n Avoiding Fact-to-Fact Table Joins\nDW/BI  systems should be built process-by-process, not department-by-department, \non a foundation of conformed dimensions to support integration. You can imagine \nquerying the sales or support fact tables to better understand a customers’ purchase \nor service history. \nBecause  the sales and support tables both contain a customer foreign key, you \ncan further imagine joining both fact tables to a common customer dimension to \nsimultaneously summarize sales facts along with support facts for a given customer. \nUnfortunately, the many-to-one-to-many join will return the wrong answer in a \nrelational environment due to the diff erences in fact table cardinality, even when \nthe relational database is working perfectly. There is no combination of inner, outer, \nleft, or right joins that produces the desired answer when the two fact tables have \nincompatible cardinalities.\nConsider the case in which you have a fact table of customer solicitations, \nand another fact table with the customer responses to solicitations, as shown in \nFigure 8-15. There is a one-to-many relationship between customer and solicita-\ntion, and another one-to-many relationship between customer and response. The \nsolicitation and response fact tables have diff erent cardinalities; in other words, not \nevery solicitation results in a response (unfortunately for the marketing department) \nand some responses are received for which there is no solicitation. Simultaneously \njoining the solicitations fact table to the customer dimension, which is, in turn, \njoined to the responses fact table, does not return the correct answer in a relational \nDBMS due to the cardinality diff erences. Fortunately, this problem is easily avoided. \nYou simply issue the drill-across technique explained in Chapter 4 to query the \n",
      "page_number": 279
    },
    {
      "number": 31,
      "title": "Segment 31 (pages 287-296)",
      "start_page": 287,
      "end_page": 296,
      "detection_method": "topic_boundary",
      "content": "Chapter 8\n260\nsolicitations table and responses table in separate queries and then outer join the two \nanswer sets. The drill-across approach has additional beneﬁ ts for better controlling \nperformance parameters, in addition to supporting queries that combine data from \nfact tables in diff erent physical locations.\nCustomer Solicitation Fact\nSolicitation Date Key (FK)\nCustomer Key (FK)\nMore FKs ...\nSolicitation Facts ...\nCustomer Response Fact\nResponse Date Key (FK)\nCustomer Key (FK)\nMore FKs ...\nResponse Facts ...\nCustomer Dimension\nCustomer Key (PK)\nCustomer ID (Natural Key)\n...\nFigure 8-15: Many-to-one-to-many joined tables should not be queried with a single \nSELECT statement.\nWARNING \nBe very careful when simultaneously joining a single dimension \ntable to two fact tables of diff erent cardinality. In many cases, relational engines \nreturn the “wrong” answer.\nIf business users are frequently combining data from multiple business pro-\ncesses, a ﬁ nal approach is to deﬁ ne an additional fact table that combines the data \nonce into a consolidated fact table rather than relying on users to consistently \nand accurately combine the data on their own, as described in Chapter 7. Merely \nusing SQL to drill across fact tables to combine the results makes more sense when \nthe underlying processes are less closely correlated. Of course, when constructing the \nconsolidated fact table, you still need to establish business rules to deal with \nthe diff ering cardinality. For example, does the consolidated fact table include all the \nsolicitations and responses or only those where both a solicitation and response \noccurred?\nLow Latency Reality Check \nThe  behavior of a customer in the last few hours or minutes can be extremely inter-\nesting. You may even want to make decisions while dealing with the customer in \nreal time. But you need to be thoughtful in recognizing the costs and limitations \nof low latency data. Generally, data quality suff ers as the data is delivered closer \nto real time.\nBusiness users may automatically think that the faster the information arrives in \nthe DW/BI system, the better. But decreasing the latency increases the data quality \n\n\nCustomer Relationship Management 261\nproblems. Figure 20-6 summarizes the issues that arise as data is delivered faster. \nIn the conventional batch world, perhaps downloading a batch ﬁ le once each 24 \nhours, you typically get complete transaction sets. For example, if a commercial \ncustomer places an order, they may have to pass a credit check and verify the ﬁ nal \ncommitment. The batch download includes orders only where all these steps have \ntaken place. In addition, because the batch download is processed just once each 24 \nhours, the ETL team has the time to run the full spectrum of data quality checks, \nas we’ll describe in Chapter 19: ETL Subsystems and Techniques.\nIf the data is extracted many times per day, then the guarantee of complete \ntransaction sets may have to be relinquished. The customer may have placed the \norder but has not passed the credit check. Thus there is the possibility that results \nmay have to be adjusted after the fact. You also may not run the full spectrum of \ndata quality checks because you don’t have time for extensive multitable lookups. \nFinally, you may have to post data into the data warehouse when all the keys have \nnot been resolved. In this case, temporary dimensional entries may need to be used \nwhile waiting for additional data feeds.\nFinally, if you deliver data instantaneously, you may be getting only transaction \nfragments, and you may not have time to perform any data quality checks or other \nprocessing of the data.\nLow latency data delivery can be very valuable, but the business users need to \nbe informed about these trade-off s. An interesting hybrid approach is to provide \nlow latency intraday delivery but then revert to a batch extract at night, thereby \ncorrecting various data problems that could not be addressed during the day. We \ndiscuss the impact of low latency requirements on the ETL system in Chapter 20: \nETL System Design and Development Process and Tasks.\nSummary\nIn this chapter, we focused exclusively on the customer, beginning with an over-\nview of customer relationship management (CRM) basics. We then delved into \ndesign issues surrounding the customer dimension table. We discussed name and \naddress parsing where operational ﬁ elds are decomposed to their basic elements \nso that they can be standardized and validated. We explored several other types \nof common customer dimension attributes, such as dates, segmentation attributes, \nand aggregated facts. Dimension outriggers that contain a large block of relatively \nlow-cardinality attributes were described.\nThis chapter introduced the use of bridge tables to handle unpredictable, sparsely \npopulated dimension attributes, as well as multivalued dimension attributes. \n\n\nChapter 8\n262\nWe also explored several complex customer behavior scenarios, including sequential \nactivities, timespan fact tables, and tagging fact events with indicators to identify \nabnormal situations.\nWe closed the chapter by discussing alternative approaches for consistently iden-\ntifying customers and consolidating a rich set of characteristics from the source \ndata, either via operational master data management or downstream processing in \nthe ETL back room with potentially partial conformity. Fi nally, we touched on the \nchallenges of low latency data requirements.\n\n\nHuman Resources \nManagement\nT\nhis  chapter, which focuses on human resources (HR) data, is the last in the series \ndealing with cross-industry business applications. Similar to the accounting \nand finance data described in Chapter 7: Accounting, HR information is dissemi-\nnated broadly throughout the organization. Organizations want to better understand \ntheir employees’ demographics, skills, earnings, and performance to maximize their \nimpact. In this chapter we’ll explore several dimensional modeling techniques in the \ncontext of HR data.\nChapter 9 discusses the following concepts:\n \n■Dimension tables to track employee proﬁ le changes\n \n■Periodic headcount snapshots\n \n■Bus matrix for a snippet of HR-centric processes\n \n■Pros and cons of packaged DW/BI solutions or data models\n \n■Recursive employee hierarchies\n \n■Multivalued skill keyword attributes handled via dimension attributes, out-\nriggers, or bridges\n \n■Survey questionnaire data\n \n■Text comments\n Employee Proﬁ le Tracking\nThus  far the dimensional models we have designed closely resemble each other; \nthe fact tables contain key performance metrics that typically can be added across \nall the dimensions. It is easy for dimensional modelers to get lulled into a kind of \nadditive complacency. In most cases, this is exactly how it is supposed to work. \nHowever, with HR employee data, a robust employee dimension supports numerous \nmetrics required by the business on its own.\n9\n\n\nChapter 9\n264\nTo frame the problem with a business vignette, let’s assume you work in the HR \ndepartment of a large enterprise. Each employee has a detailed HR proﬁ le with \nat least 100 attributes, including hire date, job grade, salary, review dates, review \noutcomes, vacation entitlement, organization, education, address, insurance plan, \nand many others. Employees are constantly hired, transferred, and promoted, as \nwell as adjusting their proﬁ les in a variety of ways.\nA high-priority business requirement is to accurately track and analyze \nemployee proﬁ le changes. You might immediately visualize a schema in which \neach employee proﬁ le change event is captured in a transaction-grained fact table, \nas depicted in Figure 9-1. The granularity of this somewhat generalized fact \ntable would be one row per employee proﬁ le transaction. Because no numeric \nmetrics are associated with changes made to employee proﬁ les, such as a new \naddress or job grade promotion, the fact table is factless.\nEmployee Key (PK)\nEmployee ID (NK)\n...\nTransaction Date Key (FK)\nTransaction Date/Time\nEmployee Key (FK)\nEmployee Transaction Type Key (FK)\nEmployee Dimension\nEmployee Transaction Type Key (PK)\nEmployee Transaction Type Description\n1 row per employee profile transaction\nEmployee Transaction Type Dimension\nEmployee Transaction Fact\nTransaction Date Dimension\nFigure 9-1: Initial draft schema for tracking employees’ proﬁ le changes.\nIn this draft schema, the dimensions include the transaction date, transaction \ntype, and employee. The transaction type dimension refers to the reason code that \ncaused the creation of this particular row, such as a promotion or address change. \nThe employee dimension is extremely wide with many attribute columns.\nWe envision using the type 2 slowly changing dimension technique for tracking \nchanged proﬁ le attributes within the employee dimension. Consequently, with every \nemployee proﬁ le transaction in the Figure 9-1 fact table, you would also create a \nnew type 2 row in the employee dimension that represents the employee’s proﬁ le as \na result of the proﬁ le change event. This new row continues to accurately describe \nthe employee until the next employee transaction occurs at some indeterminate \ntime in the future. The alert reader is quick to point out that the employee proﬁ le \ntransaction fact table and type 2 employee dimension table have the same number of \nrows; plus they are almost always joined to one another. At this point dimensional \nmodeling alarms should be going off . You certainly don’t want to have as many rows \nin a fact table as you do in a related dimension table.\nInstead of using the initial schema, you can simplify the design by embellish-\ning the employee dimension table to make it more powerful and thereby doing \n\n\nHuman Resources Management 265\naway with the proﬁ le transaction event fact table. As depicted in Figure 9-2, the \nemployee dimension contains a snapshot of the employee proﬁ le characteristics \nfollowing the employee’s proﬁ le change. The transaction type description becomes \na change reason attribute in the employee dimension to track the cause for the pro-\nﬁ le change. In some cases, the aff ected characteristics are numeric. If the numeric \nattributes are summarized rather than simply constrained upon, they belong in a \nfact table instead.\nEmployee Key (PK)\nEmployee ID (NK)\nEmployee Name ...\nEmployee Address ...\nJob Grade ...\nSalary ...\nEducation ...\nOriginal Hire Date (FK)\nLast Review Date (FK)\nAppraisal Rating ...\nHealth Insurance Plan ...\nVacation Plan ...\nChange Reason Code\nChange Reason Description\nRow Effective Date/Time\nRow Expiration Date/Time\nCurrent Row Indicator\nEmployee Dimension\nFigure 9-2: Employee dimension with proﬁ le characteristics.\nAs you’d expect, the surrogate employee key is the primary key of the dimen-\nsion table; the durable natural employee ID used in the HR operational system to \npersistently identify an employee is included as a dimension attribute.\nPrecise Effective and Expiration Timespans\nAs  discussed in Chapter 5: Procurement with the coverage of slowly changing \ndimension techniques, you should include two columns on the employee dimension \nto capture when a speciﬁ c row is eff ective and then expired. These columns deﬁ ne \na precise timespan during which the employee’s proﬁ le is accurate. Historically, \nwhen daily data latency was the norm, the eff ective and expiration columns were \ndates. However, if you load data from any business process on a more frequent basis, \nthe columns should be date/time stamps so that you can associate the appropriate \nemployee proﬁ le row, which may diff er between 9 a.m. and 9 p.m. on the same day, \nto operational events.\n\n\nChapter 9\n266\nThe  expiration attribute for the current row is set to a future date. When the row \nneeds to be expired because the ETL system has detected a new proﬁ le of attributes, \nthe expiration attribute is typically set to “just before” the new row’s eff ective stamp, \nmeaning either the prior day, minute, or second.\nIf the employee’s proﬁ le is accurately changed for a period of time, then the \nemployee reverts back to an earlier set of characteristics, a new employee dimen-\nsion row is inserted. You should resist the urge to simply revisit the earlier proﬁ le \nrow and modify the expiration date because multiple dimension rows would be \neff ective at the same time.\nThe current row indicator enables the most recent status of any employee to be \nretrieved quickly. If a new proﬁ le row occurs for this employee, the indicator in the \nformer proﬁ le row needs to be updated to indicate it is no longer the current proﬁ le.\nOn its own, a date/time stamped type 2 employee dimension answers a number \nof interesting HR inquiries. You can choose an exact historical point in time and ask \nhow many employees you have and what their detailed proﬁ les were at that speciﬁ c \nmoment by constraining the date/time to be equal to or greater than the eff ective \ndate/time and strictly less than the expiration date/time. The query can perform \ncounts and constraints against all the rows returned from these constraints.\nDimension Change Reason Tracking\nWhen  a dimension row contains type 2 attributes, you can embellish it with a change \nreason. In this way, some ETL-centric metadata is embedded with the actual data. \nThe change reason attribute could contain a two-character abbreviation for each \nchanged attribute on a dimension row. For example, the change reason attribute \nvalue for a last name change could be LN or a more legible value, such as Last \nName, depending on the intended usage and audience. If someone asks how many \npeople changed ZIP codes last year, the SELECT statement would include a LIKE \noperator and wild cards, such as \"WHERE ChangeReason LIKE '%ZIP%’\".\nBecause multiple dimension attributes may change concurrently and be repre-\nsented by a single new row in the dimension, the change reason would be multi-\nvalued. As we’ll explore later in the chapter when discussing employee skills, the \nmultiple reason codes could be handled as a single text string attribute, such as \n“|Last Name|ZIP|” or via a multivalued bridge table.\nNOTE \nThe eff ective and expiration date/time stamps, along with a reason code \ndescription, on each row of a type 2 slowly changing dimension allows very precise \ntime slicing of the dimension by itself.\nFinally, employee proﬁ le changes may be captured in the underlying source \nsystem by a set of micro-transactions corresponding to each individual employee \n\n\nHuman Resources Management 267\nattribute change. In the DW/BI system, you may want to encapsulate the series of \nmicro-transactions from the source system and treat them as a super transaction, \nsuch as an employee promotion because it would be silly to treat these artiﬁ cial \nmicro-transactions as separate type 2 changes. The new type 2 employee dimen-\nsion row would reﬂ ect all the relevant changed attributes in one step. Identifying \nthese super transactions may be tricky. Obviously the best way to identify them is \nto ensure the HR operational application captures the higher level action.\nProﬁ le Changes as Type 2 Attributes or Fact Events\nWe just described the handling of employee attribute changes as slowly changing \ndimension type 2 attributes with proﬁ le eff ective and expiration dates within the \nemployee dimension. Designers sometimes wholeheartedly embrace this pattern \nand try to leverage it to capture every employee-centric change. This results in a \ndimension table with potentially hundreds of attributes and millions of rows for \na 100,000-employee organization given the attributes’ volatility.\nTracking changes within the employee dimension table enables you to easily \nassociate the employee’s accurate proﬁ le with multiple business processes. You \nsimply load these fact tables with the employee key in eff ect when the fact event \noccurred, and ﬁ lter and group based on the full spectrum of employee attributes.\nBut the pendulum can swing too far. You probably shouldn’t use the employee \ndimension to track every employee review event, every beneﬁ t participation event, \nor every professional development event. As illustrated in Figure 9-4’s bus matrix \nin the next section, many of these events involve other dimensions, like an event \ndate, organization, beneﬁ t description, reviewer, approver, exit interviewer, separa-\ntion reasons, and the list goes on. Consequently, most of them should be handled \nas separate process-centric fact tables. Although many human resources events are \nfactless, capturing them within a fact table enables business users to easily count \nor trend by time periods and all the other associated dimensions.\nIt’s certainly common to include the outcome of these HR events, like the job \ngrade resulting from a promotion, as an attribute on the employee dimension. But \ndesigners sometimes err by including lots of foreign keys to outriggers for the \nreviewer, beneﬁ t, separation reason and other dimensions within the employee \ndimension, resulting in an overloaded dimension that’s diffi  cult to  navigate.\n Headcount Periodic Snapshot\nIn  addition to proﬁ ling employees in HR, you also want to report statuses of the \nemployees on a regular basis. Business managers are interested in counts, statistics, \nand totals, including number of employees, salary paid, vacation days taken, vaca-\ntion days accrued, number of new hires, and number of promotions. They want \n\n\nChapter 9\n268\nto analyze the data by all possible slices, including time and organization, plus \nemployee characteristics.\nAs shown in Figure 9-3, the employee headcount periodic snapshot consists of an \nordinary looking fact table with three dimensions: month, employee, and organiza-\ntion. The month dimension table contains the usual descriptors for the corporate cal-\nendar at the month grain. The employee key corresponds to the employee dimension \nrow in eff ect at the end of the last day of the given reporting month to guarantee the \nmonth-end report is a correct depiction of the employees’ proﬁ les. The organization \ndimension contains a description of the organization to which the employee belongs \nat the close of the relevant month.\n1 row per employee per month\nMonth Key (FK)\nOrganization Key (FK)\nEmployee Key (FK)\nEmployee Count\nNew Hire Count\nTransfer Count\nPromotion Count\nSalary Paid\nOvertime Paid\nRetirement Fund Paid\nRetirement Fund Employee Contribution\nVacation Days Accrued\nVacation Days Taken\nVacation Days Balance\nEmployee Headcount Snapshot Fact\nMonth Dimension\nEmployee Dimension\nOrganization Dimension\nFigure 9-3: Employee headcount periodic snapshot.\nThe facts in this headcount snapshot consist of monthly numeric metrics and \ncounts that may be diffi  cult to calculate from the employee dimension table alone. \nThese monthly counts and metrics are additive across all the dimensions or dimen-\nsion attributes, except for any facts labeled as balances. These balances, like all \nbalances, are semi-additive and must be averaged across the month dimension after \nadding across the other dimensions.\n Bus Matrix for HR Processes\nAlthough  an employee dimension with precise type 2 slowly changing dimension \ntracking coupled with a monthly periodic snapshot of core HR performance metrics \nis a good start, they just scratch the surface when it comes to tracking HR data. \n\n\nHuman Resources Management 269\nFigure 9-4 illustrates other processes that HR professionals and functional manag-\ners are likely keen to analyze. We’ve embellished this preliminary bus matrix with \nthe type of fact table that might be used for each process; however, your source \ndata realities and business requirements may warrant a diff erent or complementary \ntreatment.\nEmployee Position Snapshot\nEmployee Requisition Pipeline\nEmployee Performance Review Pipeline\nEmployee Disciplinary Action Pipeline\nEmployee Separations\nEmployee Performance Review\nEmployee Prof Dev Completed Courses\nEmployee Hiring\nEmployee “On Board” Pipeline\nEmployee Benefits Eligibility\nEmployee Benefits Application\nEmployee Benefit Participation\nEmployee Benefit Accruals\nEmployee Headcount Snapshot\nEmployee Compensation\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nEmpl\nMgr\nEmpl\nMgr\nEmpl\nMgr\nEmpl\nMgr\nEmpl\nMgr\nEmpl\nMgr\nEmpl\nMgr\nEmpl\nMgr\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nPeriodic\nDate\nPosition\nEmployee\nOrganization\nBenefit\nAccumulating\nAccumulating\nAccumulating\nTransaction\nTransaction\nTransaction\nTransaction\nAccumulating\nPeriodic\nAccumulating\nPeriodic\nTransaction\nPeriodic\nTransaction\nFact Type\nHiring Processes\nBenefits Processes\nEmployee Management Processes\nFigure 9-4: Bus matrix rows for HR processes.\nSome of these business processes capture performance metrics, but many result \nin factless fact tables, such as beneﬁ t eligibility or participation.\n",
      "page_number": 287
    },
    {
      "number": 32,
      "title": "Segment 32 (pages 297-305)",
      "start_page": 297,
      "end_page": 305,
      "detection_method": "topic_boundary",
      "content": "Chapter 9\n270\n Packaged Analytic Solutions \nand Data Models\nMany  organizations purchase a vendor solution to address their operational HR \napplication needs. Most of these products off er an add-on DW/BI solution. In addi-\ntion, other vendors sell standard data models, potentially with prebuilt data loaders \nfor the popular HR application products.\nVendors and proponents argue these standard, prebuilt solutions and models \nallow for more rapid, less risky implementations by reducing the scope of the data \nmodeling and ETL development eff ort. After all, every HR department hires employ-\nees, signs them up for beneﬁ ts, compensates them, reviews them, and eventually \nprocesses employee separations. Why bother re-creating the wheel by designing \ncustom data models and solutions to support these common business processes \nwhen you can buy a standard data model or complete solution instead?\nAlthough there are undoubtedly common functions, especially within the HR \nspace, businesses typically have unique peculiarities. To handle these nuances, \nmost application software vendors introduce abstractions in their products, which \nenable them to be more easily “customized.”\nThese abstractions, like the party table and associated apparatus to describe each \nrole or generic attribute column names rather than more meaningful labels, provide \nﬂ exibility to adapt to a variety of business situations. Although implementation \nadaptability is a win for vendors who want their products to address a broad range of \npotential customers’ business scenarios, the downside is the associated complexity.\nHR professionals who live with the vendor’s product 24x7 are often willing to \nadjust their vocabulary to accommodate the abstractions. But these abstractions can \nfeel like a foreign language for less-immersed functional managers. Delivering data \nto the business via a packaged DW/BI solution or industry-standard data model may \nbypass the necessary translations into the business’s vernacular.\nBesides the reliance on the vendor’s terminology instead of incorporating the \nbusiness’s vocabulary in the DW/BI solution, another potential sharp corner is \nthe integration of source data from other domains. Can you readily conform the \ndimensions in the vendor solution or industry model with other internally avail-\nable master data? If not, the packaged model is destined to become another isolated \nstovepipe data set. Clearly, this outcome is unappealing; although it may be less of \nan obstacle if all your operational systems are supported by the same ERP vendor, \nor you’re a small organization without an IT shop doing independent development.\nWhat can you realistically expect to gain from a packaged model? Prebuilt generic \nmodels can help identify core business processes and associate common dimensions. \nThat provides some comfort for DW/BI teams feeling initially overwhelmed by the \n\n\nHuman Resources Management 271\ndesign task. After a few days or weeks studying the standard model, most teams \ngain enough conﬁ dence to want to customize the schema for their data.\nHowever, is this knowledge worth the price tag associated with the packaged \nsolution or data model? You could likely gain the same insight by spending a few \nweeks with the business users. You’d not only improve your understanding of the \nbusiness’s needs, but also begin bonding business users to the DW/BI initiative.\nIt’s also worth mentioning that just because a packaged model or solution costs \nthousands of dollars doesn’t mean it exhibits generally accepted dimensional modeling \nbest practices. Unfortunately, some standard models embody common dimensional \nmodeling design ﬂ aws; this isn’t surprising if the model’s designers focused more on \nbest practices for source system data capture rather than those required for BI report-\ning and analytics. It’s diffi  cult to design a predeﬁ ned generic model, even if the vendor \nowns the data capture source code.\nRecursive Employee Hierarchies\nA  common employee characteristic is the name of the employee’s manager. You \ncould simply embed this attribute along with the other attributes in the employee \ndimension. But if the business users want more than the manager’s name, more \ncomplex structures are necessary.\nOne  approach is to include the manager’s employee key as another foreign key in \nthe fact table, as shown in Figure 9-5. This manager employee key joins to a role-\nplaying employee dimension where every attribute name refers to “manager” to \ndiff erentiate the manager’s proﬁ le from the employee’s. This approach associates the \nemployee and their manager whenever a row is inserted into a fact table. BI analyses \ncan easily ﬁ lter and group by either employee or manager attributes with virtually \nidentical query performance because both dimensions provide symmetrical access \nto the fact table. The downside of this approach is these dual foreign keys must be \nembedded in every fact table to support managerial reporting.\nSeparation Date Key (FK)\nOrganization Key (FK)\nEmployee Key (FK)\nManager Key (FK)\nSeparation Profile Key (FK)\nSeparation Count\nEmployee Key (PK)\nEmployee ID (NK)\n...\nEmployee Dimension\nManager Key (PK)\nManager Employee ID (NK)\n...\nManager Dimension\nSeparation Profile Key (PK)\nSeparation Type Description\nSeparation Reason Description\nSeparation Profile Dimension\nEmployee Separation Fact\nDate Dimension\nOrganization Dimension\nFigure 9-5: Dual role-playing employee and manager dimensions.\n\n\nChapter 9\n272\nAnother option is to include the manager’s employee key as an attribute on the \nemployee’s dimension row. The manager key would join to an outrigger consisting of \na role play on the employee dimension where all the attributes reference “manager” \nto diff erentiate them from the employee’s characteristics, as shown in Figure 9-6.\nSeparation Date Key (FK)\nOrganization Key (FK)\nEmployee Key (FK)\nSeparation Profile Key (FK)\nSeparation Count\nEmployee Key (PK)\nEmployee ID (NK)\nEmployee Attributes ...\nManager Key (FK)\nRow Effective Date\nRow Expiration Date\nCurrent Row Indicator\nEmployee Dimension\nManager Key (PK)\nManager Employee ID (NK)\nManager Employee Attributes ...\nRow Effective Date\nRow Expiration Date\nCurrent Row Indicator\nManager Dimension\nEmployee Separation Fact\nFigure 9-6: Manager role-playing dimension as an outrigger.\nIf the manager’s foreign key in the employee dimension is designated as a type 2 \nattribute, then new employee rows would be generated with each manager change. \nHowever, we encourage you to think carefully about the underlying ETL business rules.\nChange Tracking on Embedded Manager Key\nLet’s  walk through an example. Abby is Hayden’s manager. With the outrigger \napproach just described, Hayden’s employee dimension row would include an \nattribute linking to Abby’s row in the manager role-play employee dimension. If \nHayden’s manager changes, and assuming the business wants to track these histori-\ncal changes, then treating the manager foreign key as a type 2 and creating a new \nrow for Hayden to capture his new proﬁ le with a new manager would be appropriate.\nHowever, think about the desired outcome if Abby were still Hayden’s manager, \nbut her employee proﬁ le changes, perhaps caused by something as innocuous as \na home address change. If the home address is designated as a type 2 attribute, \nthis move would spawn a new employee dimension row for Abby. If the manager \nkey is also designated as a type 2 attribute, then Abby’s new employee key would \nalso spawn a new dimension row for Hayden. Now imagine Abby is the CEO of a \nlarge organization. A type 2 change in her proﬁ le would ripple through the entire \ntable; you’d end up replicating a new proﬁ le row for every employee due to a single \ntype 2 attribute change on the CEO’s proﬁ le.\nDoes the business want to capture these manager proﬁ le changes? If not, perhaps \nthe manager key on the employee’s row should be the manager’s durable natural key \n\n\nHuman Resources Management 273\nlinked to a role-playing dimension limited to just the current row for each manager’s \ndurable natural key in the dimension.\nIf you designate the manager’s key in the employee dimension to be a type 1 \nattribute, it would always associate an employee with her current manager. Although \nthis simplistic approach obliterates history, it may completely satisfy the business \nuser’s needs.\n Drilling Up and Down Management Hierarchies\nAdding  an attribute, either a textual label or a foreign key to a role-playing dimen-\nsion, to an employee dimension row is appropriate for handling the ﬁ xed depth, \nmany-to-one employee-to-manager relationship. However, more complex approaches \nmight be required if the business wants to navigate a deeper recursive hierarchy, \nsuch as identifying an employee’s entire management chain or drilling down to \nidentify the activity for all employees who directly or indirectly work for a given \nmanager.\nIf  you use an OLAP tool to query employee data, the embedded manager key \non every employee dimension row may suffi  ce. Popular OLAP products contain a \nparent/child hierarchy structure that works smoothly with variable depth recursive \nhierarchies. In fact, this is one of the strengths of OLAP products.\nHowever, if you want to query the recursive employee/manager relationship in \nthe relational environment, you must use Oracle’s nonstandard CONNECT BY syntax \nor SQL’s recursive common table extension (CTE) syntax. Both approaches are \nvirtually unworkable for business users armed with a BI reporting tool.\nSo you’re left with the options described in Chapter 7 for dealing with vari-\nable depth customer hierarchies. In Figure 9-7, the employee dimension from \nFigure 9-6 relates to the fact table through a bridge table. The bridge table has \none row for each manager and each employee who is directly or indirectly in \ntheir management chain, plus an additional row for the manager to himself. The \nbridge joins shown in Figure 9-7 enable you to drill down within a manager’s \nchain of command.\nSeparation Date Key (FK)\nOrganization Key (FK)\nEmployee Key (FK)\nSeparation Profile Key (FK)\nSeparation Count\nManager Key (FK)\nEmployee Key (FK)\n# Levels from Top\nBottom Flag\nTop Flag\nManagement Hierarchy Bridge\nManager Key (PK)\nManager Employee ID (NK)\nManager Employee Attributes ...\nRow Effective Date\nRow Expiration Date\nCurrent Row Indicator\nManager Dimension\nEmployee Separation Fact\nFigure 9-7: Bridge table to drill down into a manager’s reporting structure.\n\n\nChapter 9\n274\nAs previously described, there are several disadvantages to this approach. The \nbridge table is somewhat challenging to build, plus it contains many rows, so query \nperformance can suff er. The BI user experience is complicated for ad hoc queries, \nalthough we’ve seen analysts eff ectively use it. Finally, if users want to aggregate \ninformation up rather than down a management chain, the join paths must be \nreversed.\nOnce again, the situation is further complicated if you want to track employee pro-\nﬁ le changes in conjunction with the bridge table. If the manager and employee reﬂ ect \nemployee proﬁ les with type 2 changes, the bridge table will experience rapid growth, \nespecially when senior management proﬁ le changes cause new keys to ripple across \nthe organization.\nYou could use durable natural keys in the bridge table, instead of the employee \nkeys which capture type 2 proﬁ le changes. Limiting the relationship to the man-\nagement hierarchy’s current proﬁ les is one thing. However, if the business wants \nto retain a history of employee/manager rollups, you need to embellish the bridge \ntable with eff ective and expiration dates that capture the eff ective timespan for each \nemployee/manager relationship.\nThe propagation of new rows in this bridge table using durable keys is substan-\ntially reduced compared to the Figure 9-7 bridge because new rows are added when \nreporting relationships change, not when any type 2 employee attribute is modiﬁ ed. \nA bridge table built on durable keys is easier to manage, but quite challenging to \nnavigate, especially given the need to associate the relevant organizational structures \nwith the event dates in the fact table. Given the complexities, the bridge table should \nbe buried within a canned BI application for all but a small subset of power BI users.\nThe alternative approaches discussed in Chapter 7 for handling recursive hierar-\nchies, like the pathstring attribute, are also relevant to the management hierarchy \nconundrum. Unfortunately, there’s no silver bullet solution for handling these com-\nplex structures in a simple and fast way.\nMultivalued Skill Keyword Attributes\nLet’s  assume the IT department wants to supplement the employee dimension with \ntechnical skillset proﬁ ciency information. You could consider these technical skills, \nsuch as programming languages, operating systems, or database platforms, to be key-\nwords describing employees. Each employee is tagged with a number of skill keywords. \nYou want to search the IT employee population by their descriptive skills.\nIf the technical skills of interest were a ﬁ nite number, you could include them \nas individual attributes in the employee dimension. The advantage of using posi-\ntional dimension attributes, such as a Linux attribute with domain values such as \n\n\nHuman Resources Management 275\nLinux Skills and No Linux Skills, is they’re easy to query and deliver fast query \nperformance. This approach works well to a point but falls apart when the number \nof potential skills expands.\n Skill Keyword Bridge\nMore realistically, each employee will have a variable, unpredictable number of \nskills. In this case, the skill keyword attribute is a prime candidate to be a multi-\nvalued dimension. Skill keywords, by their nature, are open-ended; new skills are \nadded regularly as domain values. We’ll show two logically equivalent modeling \nschemes for handling open-ended sets of skills.\nFigure 9-8 shows a multivalued dimension design for handling the skills as an \noutrigger bridge table to the employee dimension table. As you’ll see in Chapter 14: \nHealthcare, sometimes the multivalued bridge table is joined directly to a fact table.\nEmployee Key (FK)\nMore FKs ...\nHeadcount Facts ...\nEmployee Headcount\nSnapshot Fact\nEmployee Key (PK)\n...\nEmployee Skill Group Key (FK)\nEmployee Dimension\nEmployee Skill Key (PK)\nEmployee Skill Description\nEmployee Skill Category\nSkills Dimension\nEmployee Skill Group Key (FK)\nEmployee Skill Key (FK)\nEmployee Skill Group Bridge\nFigure 9-8: Skills group keyword bridge table.\nThe skills group bridge identiﬁ es a given set of skill keywords. IT employees who \nare proﬁ cient in Oracle, Unix, and SQL would be assigned the same skills group key. \nIn the skills group bridge table, there would be three rows for this particular group, \none for each of the associated skill keywords (Oracle, Unix, and  SQL).\nAND/OR Query Dilemma\nAssuming  you built the schema shown in Figure 9-8, you are still left with a serious \nquery problem. Query requests against the skill keywords fall into two categories. The \nOR queries (for example, Unix or Linux experience) can be satisﬁ ed by a simple OR \nconstraint on the skills description attribute in the skills dimension table. However, \nAND queries (for example, Unix and Linux experience) are diffi  cult because the AND \nconstraint is a constraint across two rows in the skills dimension. SQL is notoriously \npoor at handling constraints across rows. The answer is to create SQL code using \nunions and intersections, probably in a custom interface that hides the complex logic \nfrom the business user. The SQL code would look like this:\n (SELECT employee_ID, employee_name\nFROM Employee, SkillBridge, Skills\nWHERE Employee.SkillGroupKey = SkillBridge.SkillGroupKey AND\n  SkillGroup.SkillKey = Skill.SkillKey AND\n\n\nChapter 9\n276\n  Skill.Skill = \"UNIX\")\nUNION / INTERSECTION\n (SELECT employee_ID, employee_name\n  FROM Employee, SkillBridge, Skills\n  WHERE Employee.SkillGroupKey = SkillBridge.SkillGroupKey AND \n    SkillGroup.SkillKey = Skill.SkillKey AND \n    Skill.Skill = \"LINUX\")\nUsing the UNION lists employees with Unix or Linux experience, whereas using \nINTERSECTION identiﬁ es employees with Unix and Linux experience.\nSkill Keyword Text String\nYou  can remove the many-to-many bridge and the need for union/intersection SQL \nby simplifying the design. One approach would be to add a skills list outrigger to \nthe employee dimension containing one long text string concatenating all the skill \nkeywords for that list key. You would need a special delimiter such as a backslash or \nvertical bar at the beginning of the skills text string and after each skill in the list. \nThus the skills string containing Unix and C++ would look like |Unix|C++|. This \noutrigger approach presumes a number of employees share a common list of skills. \nIf the lists are not reused frequently, you could collapse the skills list outrigger by \nsimply including the skills list text string as an employee dimension attribute, as \nshown in Figure 9-9.\nEmployee Key (FK)\nMore FKs ...\nHeadcount Facts ...\nEmployee Headcount Snapshot Fact\nEmployee Key (PK)\n...\nEmployee Skill Group List\nEmployee Dimension\nFigure 9-9: Delimited skills list string.\nText string searches can be challenging because of the ambiguity caused by \nsearching on uppercase or lowercase. Is it UNIX or Unix or unix? You can resolve \nthis by coercing the skills list to upper case with the UCase function in most SQL \nenvironments.\nWith the design in Figure 9-9, the AND/OR dilemma can be addressed in a single \nSELECT statement. The OR constraint looks like this:\nUCase(skill_list) like '%|UNIX|% OR UCase(skill_list) like '%|LINUX|%'\nMeanwhile, the AND constraint has exactly the same structure:\nUCase(skill_list) like '%|UNIX|' AND UCase(skill_list) like '%|LINUX|%'\n\n\nHuman Resources Management 277\nThe % symbol is a wild card pattern-matching character deﬁ ned in SQL that \nmatches zero or more characters. The vertical bar delimiter is used explicitly in the \nconstraints to exactly match the desired keywords and not get erroneous matches.\nThe keyword list approach shown in Figure 9-9 can work in any relational database \nbecause it is based on standard SQL. Although the text string approach facilitates \nAND/OR searching, it doesn’t support queries that count by skill keyword.\nSurvey Questionnaire Data\nHR  departments often collect survey data from employees, especially when gather-\ning peer and/or management review data. The department analyzes questionnaire \nresponses to determine the average rating for a reviewed employee and within a \ndepartment.\nTo handle questionnaire data in a dimensional model, a fact table with one row \nfor each question on a respondent’s survey is typically created, as illustrated in \nFigure 9-10. Two role-playing employee dimensions in the schema correspond to the \nresponding employee and reviewed employee. The survey dimension has descriptors \nabout the survey instrument. The question dimension provides the question and \nits categorization; presumably, the same question is asked on multiple surveys. The \nsurvey and question dimensions can be useful when searching for speciﬁ c topics in \na broad database of questionnaires. The response dimension contains the responses \nand perhaps categories of responses, such as favorable or hostile.\nSurvey Key (PK)\nSurvey Title\nSurvey Type\nSurvey Objective\nReview Year\nQuestion Key (PK)\nQuestion Label\nQuestion Category\nQuestion Objective\nSurvey Sent Date Key (FK)\nSurvey Received Date Key (FK)\nSurvey Key (FK)\nResponding Employee Key (FK)\nReviewed Employee Key (FK)\nQuestion Key (FK)\nResponse Category Key (FK)\nSurvey Number (DD)\nResponse\nSurvey Dimension\nResponse Category Key (PK)\nResponse Category Description\nResponse Category Dimension\nEmployee Evaluation Survey Fact\nQuestion Dimension\nDate Dimension (2 views for roles)\nEmployee Dimension (2 views for roles)\nFigure 9-10: Survey schema.\nCreating the simple schema in Figure 9-10 supports robust slicing and dicing \nof survey data. Variations of this schema design would be useful for analyzing all \ntypes of survey data, including customer satisfaction and product usage feedback.\n\n\nChapter 9\n278\n Text Comments\nFacts are typically thought of as continuously valued numeric measures; dimension \nattributes, on the other hand, are drawn from a discrete list of domain values. So \nhow do you handle textual comments, such as a manager’s remarks on a perfor-\nmance review or freeform feedback on a survey question, which seem to defy clean \nclassiﬁ cation into the fact or dimension category? Although IT professionals may \ninstinctively want to simply exclude them from a dimensional design, business \nusers may demand they’re retained to further describe the performance metrics.\nAfter it’s been conﬁ rmed the business is unwilling to relinquish the text com-\nments, you should determine if the comments can be parsed into well-behaved \ndimension attributes. Although there are sometimes opportunities to categorize \nthe text, such as a compliment versus complaint, the full text verbiage is typically \nalso required.\nBecause freeform text takes on so many potential values, designers are some-\ntimes tempted to store the text comment within the fact table. Although cognizant \nthat fact tables are typically limited to foreign keys, degenerate dimensions, and \nnumeric facts, they contend the text comment is just another degenerate dimension. \nUnfortunately, text comments don’t qualify as degenerate dimensions.\nFreeform text ﬁ elds shouldn’t be stored in the fact table because they just add \nbulky clutter to the table. Depending on the database platform, this relatively low \nvalue bulk may get dragged along on every operation involving the fact table’s much \nmore valuable performance metrics.\nRather than treating the comments as textual metrics, we recommend retaining \nthem outside the fact table. The comments should either be captured in a separate \ncomments dimensions (with a corresponding foreign key in the fact table) or as \nan attribute on a transaction-grained dimension table. In some situations, identi-\ncal comments are observed multiple times. At a minimum, this typically occurs \nwith the No Comment comment. If the cardinality of the comments is less than \nthe number of transactions, the text should be captured in a comments dimension. \nOtherwise, if there’s a unique comment for every event, it’s treated as a transaction \ndimension attribute. In either case, regardless of whether the comments are handled \nin a comment or transaction dimension, the query performance when this sizeable \ndimension is joined to the fact table will be slow. However, by the time users are \nviewing comments, they’ve likely signiﬁ cantly ﬁ ltered their query as they can real-\nistically read only a limited number of comments. Meanwhile, the more common \nanalyses focusing on the fact table’s performance metrics won’t be burdened by the \nextra weight of the textual comments on every fact table  query.\n",
      "page_number": 297
    },
    {
      "number": 33,
      "title": "Segment 33 (pages 306-313)",
      "start_page": 306,
      "end_page": 313,
      "detection_method": "topic_boundary",
      "content": "Human Resources Management 279\nSummary\nIn this chapter, we discussed several concepts in the context of HR data. First, \nwe further elaborated on the advantages of embellishing an employee dimension \ntable. In the world of HR, this single table is used to address a number of ques-\ntions regarding the status and proﬁ le of the employee base at any point in time. We \ndrafted a bus matrix representing multiple processes within the HR arena and high-\nlighted a core headcount snapshot fact table, along with the potential advantages \nand disadvantages of vendor-designed solutions and data models. The handling of \nmanagerial rollups and multivalued dimension attributes was discussed. Finally, \nwe provided a brief overview regarding the handling of survey or questionnaire \ndata, along with text comments. \n\n\nFinancial Services\nT\nhe financial services industry encompasses a wide variety of businesses, includ-\ning credit card companies, brokerage firms, and mortgage providers. In this \nchapter, we’ll primarily focus on the retail bank since most readers have some degree \nof personal familiarity with this type of financial institution. A full-service bank offers \na breadth of products, including checking accounts, savings accounts, mortgage \nloans, personal loans, credit cards, and safe deposit boxes. This chapter begins with \na very simplistic schema. We then explore several schema extensions, including the \nhandling of the bank’s broad portfolio of heterogeneous products that vary signifi-\ncantly by line of business.\nWe want to remind you that industry focused chapters like this one are not \nintended to provide full-scale industry solutions. Although various dimensional \nmodeling techniques are discussed in the context of a given industry, the techniques \nare certainly applicable to other businesses. If you don’t work in ﬁ nancial services, \nyou still need to read this chapter. If you do work in ﬁ nancial services, remember \nthat the schemas in this chapter should not be viewed as  complete.\nChapter 10 discusses the following concepts:\n \n■Bus matrix snippet for a bank\n \n■Dimension triage to avoid the “too few dimensions” trap\n \n■Household dimensions \n \n■Bridge tables to associate multiple customers with an account, along with \nweighting factors\n \n■Multiple mini-dimensions in a single fact table\n \n■Dynamic value banding of facts for reporting \n \n■Handling heterogeneous products across lines of business, each with unique \nmetrics and/or dimension attributes, as supertype and subtype schemas\n \n■Hot swappable dimensions\n10\n\n\nChapter 10\n282\n Banking Case Study and Bus Matrix\nThe  bank’s initial goal is to better analyze the bank’s accounts. Business users want the \nability to slice and dice individual accounts, as well as the residential household groupings \nto which they belong. One of the bank’s major objectives is to market more eff ectively by \noff ering additional products to households that already have one or more accounts with \nthe bank. Figure 10-1 illustrates a portion of a bank’s bus matrix.\nNew Business Solicitation\nLead Tracking\nAccount Application Pipeline\nAccount Initiation\nAccount Transactions\nAccount Monthly Snapshot\nAccount Servicing Activities\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nDate\nProspect\nCustomer\nHousehold\nBranch\nAccount\nProduct\nFigure 10-1: Subset of bus matrix rows for a bank.\nAfter conducting interviews with managers and analysts around the bank, the \nfollowing set of requirements were developed:\n \n■Business users want to see ﬁ ve years of historical monthly snapshot data on \nevery account.\n \n■Every account has a primary balance. The business wants to group diff erent \ntypes of accounts in the same analyses and compare primary balances.\n \n■Every type of account (known as products within the bank) has a set of cus-\ntom dimension attributes and numeric facts that tend to be quite diff erent \nfrom product to product.\n \n■Every account is deemed to belong to a single household. There is a surprising \namount of volatility in the account/household relationships due to changes \nin marital status and other life stage factors.\n \n■In addition to the household identiﬁ cation, users are interested in demographic \ninformation both as it pertains to individual customers and households. In \naddition, the bank captures and stores behavior scores relating to the activity \nor characteristics of each account and household.\n\n\nFinancial Services 283\n Dimension Triage to Avoid Too Few \nDimensions\nBased  on the previous business requirements, the grain and dimensionality of the \ninitial model begin to emerge. You can start with a fact table that records the pri-\nmary balances of every account at the end of each month. Clearly, the grain of the \nfact table is one row for each account each month. Based on that grain declaration, \nyou can initially envision a design with only two dimensions: month and account. \nThese two foreign keys form the fact table primary key, as shown in Figure 10-2. \nA data-centric designer might argue that all the other description information, such \nas household, branch, and product characteristics should be embedded as descriptive \nattributes of the account dimension because each account has only one household, \nbranch, and product associated with it.\nMonth Dimension\nAccount Dimension\nMonth End Date Key (PK)\nMonth Attributes ...\nMonth End Date Key (FK)\nAccount Key (FK)\nPrimary Month Ending Balance\nAccount Key (PK)\nAccount Attributes ...\nPrimary Customer Attributes ...\nProduct Attributes ...\nHousehold Attributes ...\nStatus Attributes ...\nBranch Attributes ...\nMonth Account Snapshot Fact\nFigure 10-2: Balance snapshot with too few dimensions.\nAlthough this schema accurately represents the many-to-one and many-to-many \nrelationships in the snapshot data, it does not adequately reﬂ ect the natural business \ndimensions. Rather than collapsing everything into the huge account dimension table, \nadditional analytic dimensions such as product and branch mirror the instinctive \nway users think about their business. These supplemental dimensions provide much \nsmaller points of entry to the fact table. Thus, they address both the performance and \nusability objectives of a dimensional model. Finally, given a big bank may have mil-\nlions of accounts, you should worry about type 2 slowly changing dimension eff ects \npotentially causing this huge dimension to mushroom into something unmanage-\nable. The product and branch attributes are convenient groups of attributes to remove \nfrom the account dimension to cut down on the row growth caused by type 2 change \ntracking. In the section “Mini-Dimensions Revisited,” the changing demographics \nand behavioral attributes will be squeezed out of the account dimension for the same \nreasons.\nThe product and branch dimensions are two separate dimensions as there is \na many-to-many relationship between products and branches. They both change \n\n\nChapter 10\n284\nslowly, but on diff erent rhythms. Most important, business users think of them as \ndistinct dimensions of the banking business.\nIn general,  most dimensional models end up with between ﬁ ve and 20 dimen-\nsions. If you are at or below the low end of this range, you should be suspicious \nthat dimensions may have been inadvertently left out of the design. In this case, \ncarefully consider whether any of the following kinds of dimensions are appropriate \nsupplements to your initial dimensional model:\n \n■Causal  dimensions, such as promotion, contract, deal, store condition, or even \nweather. These dimensions, as discussed in Chapter 3: Retail Sales, provide \nadditional insight into the cause of an event.\n \n■Multiple date  dimensions, especially when the fact table is an accumulating \nsnapshot. Refer to Chapter 4: Inventory for sample fact tables with multiple \ndate stamps.\n \n■Degenerate  dimensions that identify operational transaction control numbers, \nsuch as an order, an invoice, a bill of lading, or a ticket, as initially illustrated \nin Chapter 3.\n \n■Role-playing  dimensions, such as when a single transaction has several busi-\nness entities associated with it, each represented by a separate dimension. In \nChapter 6: Order Management, we described role playing to handle multiple \ndates.\n \n■Status  dimensions that identify the current status of a transaction or monthly \nsnapshot within some larger context, such as an account status.\n \n■An  audit dimension, as discussed in Chapter 6, to track data lineage and \nquality.\n \n■Junk  dimensions of correlated indicators and ﬂ ags, as described in Chapter 6.\nThese dimensions can typically be added gracefully to a design, even after the \nDW/BI system has gone into production because they do not change the grain of \nthe fact table. The addition of these dimensions usually does not alter the existing \ndimension keys or measured facts in the fact table. All existing applications should \ncontinue to run without change.\nNOTE \nAny descriptive attribute that is single-valued in the presence of the \nmeasurements in the fact table is a good candidate to be added to an existing \ndimension or to be its own dimension.\nBased on further study of the bank’s requirements, you can ultimately choose the \nfollowing dimensions for the initial schema: month end date, branch, account, pri-\nmary customer, product, account status, and household. As illustrated in Figure 10-3, \n\n\nFinancial Services 285\nat the intersection of these seven dimensions, you take a monthly snapshot and \nrecord the primary balance and any other metrics that make sense across all prod-\nucts, such as transaction count, interest paid, and fees charged. Remember account \nbalances are just like inventory balances in that they are not additive across any \nmeasure of time. Instead, you must average the account balances by dividing the \nbalance sum by the number of time periods.\nMonth Dimension\nAccount Dimension\nMonth End Date Key (PK)\nMonth Attributes ...\nBranch Dimension\nBranch Key (PK)\nBranch Number (NK)\nBranch Address Attributes ...\nBranch Rollup Attributes ...\nAccount Key (PK)\nAccount Number (NK)\nAccount Address Attributes ...\nAccount Open Date\n...\nHousehold Key (PK)\nHousehold ID (NK)\nHousehold Address Attributes ...\nHousehold Income\nHousehold Homeownership Indicator\nHousehold Presence of Children\n...\nPrimary Customer Key (PK)\nPrimary Customer Name\nPrimary Customer Date of Birth\n...\nProduct Dimension\nProduct Key (PK)\nProduct Code (NK)\nProduct Description\n...\nAccount Status Dimension\n Account Status Key (PK)\n Account Status Description\n Account Status Group\nMonth End Date Key (FK)\nBranch Key (FK)\nAccount Key (FK)\nPrimary Customer Key (FK)\nProduct Key (FK)\nAccount Status Key (FK)\nHousehold Key (FK)\nPrimary Month Ending Balance\nAverage Daily Balance\nNumber of Transactions\nInterest Paid\nFees Charged\nPrimary Customer Dimension\nHousehold Dimension\nMonthly Account Snapshot Fact\nFigure 10-3: Supertype snapshot fact table for all accounts.\nNOTE \nIn this chapter we use the basic object-oriented terms supertype and \nsubtype to refer respectively to the single fact table covering all possible account \ntypes, as well as the multiple fact tables containing speciﬁ c details of each individual \naccount type. In past writings these have been called core and custom fact tables, \nbut it is time to change to the more familiar and accepted terminology.\nThe product dimension consists of a simple hierarchy that describes all the \nbank’s products, including the name of the product, type, and category. The need to \nconstruct a generic product categorization in the bank is the same need that causes \ngrocery stores to construct a generic merchandise hierarchy. The main diff erence \nbetween the bank and grocery store examples is that the bank also develops a large \nnumber of subtype product attributes for each product type. We’ll defer discussion \n\n\nChapter 10\n286\nregarding the handling of these subtype attributes until the “Supertype and Subtype \nSchemas for Heterogeneous Products” section at the end of the chapter.\nThe branch dimension is similar to the facility dimensions we discussed earlier \nin this book, such as the retail store or distribution center warehouse.\nThe account status dimension is a useful dimension to record the condition of \nthe account at the end of each month. The status records whether the account is \nactive or inactive, or whether a status change occurred during the month, such as a \nnew account opening or account closure. Rather than whipsawing the large account \ndimension, or merely embedding a cryptic status code or abbreviation directly in \nthe fact table, we treat status as a full-ﬂ edged dimension with descriptive status \ndecodes, groupings, and status reason descriptions, as appropriate. In many ways, \nyou could consider the account status dimension to be another example of a mini-\ndimension, as we introduced in Chapter 5: Procurement.\n Household Dimension\nRather  than focusing solely on the bank’s accounts, business users also want the \nability to analyze the bank’s relationship with an entire economic unit, referred to as \na household. They are interested in understanding the overall proﬁ le of a household, \nthe magnitude of the existing relationship with the household, and what additional \nproducts should be sold to the household. They also want to capture key demographics \nregarding the household, such as household income, whether they own or rent their \nhome, whether they are retirees, and whether they have children. These demographic \nattributes change over time; as you might suspect, the users want to track the changes. \nIf the bank focuses on accounts for commercial entities, rather than consumers, simi-\nlar requirements to identify and link corporate “households” are common.\nFrom the bank’s perspective, a household may be comprised of several accounts \nand individual account holders. For example, consider John and Mary Smith as a \nsingle household. John has a checking account, whereas Mary has a savings account. \nIn addition, they have a joint checking account, credit card, and mortgage with \nthe bank. All ﬁ ve of these accounts are considered to be a part of the same Smith \nhousehold, despite the fact that minor inconsistencies may exist in the operational \nname and address information.\nThe process of relating individual accounts to households (or the commercial \nbusiness equivalent) is not to be taken lightly. Householding requires the devel-\nopment of business rules and algorithms to assign accounts to households. There \nare specialized products and services to do the matching necessary to determine \nhousehold assignments. It is very common for a large ﬁ nancial services organization \nto invest signiﬁ cant resources in specialized capabilities to support its household-\ning needs.\n\n\nFinancial Services 287\nThe decision to treat account and household as separate dimensions is somewhat \na matter of the designer’s prerogative. Even though they are intuitively correlated, \nyou decide to treat them separately because of the size of the account dimension and \nthe volatility of the account constituents within a household dimension, as men-\ntioned earlier. In a large bank, the account dimension is huge, with easily over 10 \nmillion rows that group into several million households. The household dimension \nprovides a somewhat smaller point of entry into the fact table, without traversing \na 10 million-row account dimension table. Also, given the changing nature of the \nrelationship between accounts and households, you elect to use the fact table to \ncapture the relationship, rather than merely including the household attributes on \neach account dimension row. In this way, you avoid using the type 2 slowly chang-\ning dimension technique with a 10-million row account dimension.\n Multivalued Dimensions and Weighting Factors\nAs  you just saw in the John and Mary Smith example, an account can have one, two, \nor more individual account holders, or customers, associated with it. Obviously, the \ncustomer cannot be included as an account attribute (beyond the designation of a \nprimary customer/account holder); doing so violates the granularity of the dimen-\nsion table because more than one individual can be associated with an account. \nLikewise, you cannot include a customer as an additional dimension in the fact \ntable; doing so violates the granularity of the fact table (one row per account per \nmonth), again because more than one individual can be associated with any given \naccount. This is another classic example of a multivalued dimension. To link an \nindividual customer dimension to an account-grained fact table requires the use of \nan account-to-customer bridge table, as shown in Figure 10-4. At a minimum, the \nprimary key of the bridge table consists of the surrogate account and customer keys. \nThe time stamping of bridge table rows, as discussed in Chapter 7: Accounting, for \ntime-variant relationships is also applicable in this scenario. \nAccount Dimension\nAccount Key (PK)\nAccount Number (NK)\nAccount Address Attributes ...\nAccount Open Date\n...\nAccount-to-Customer Bridge\nAccount Key (FK)\nCustomer Key (FK)\nWeighting Factor\nCustomer Dimension\nCustomer Key (FK)\nCustomer Name\nCustomer Date of Birth\n...\nMonth End Date Key (FK)\nAccount Key (FK)\nMore FKs ...\nPrimary Month Ending Balance\nAverage Daily Balance\nNumber of Transactions\nInterest Paid\nFees Charged\nMonthly Account Snapshot Fact\nFigure 10-4: Account-to-customer bridge table with weighting factor.\n",
      "page_number": 306
    },
    {
      "number": 34,
      "title": "Segment 34 (pages 314-324)",
      "start_page": 314,
      "end_page": 324,
      "detection_method": "topic_boundary",
      "content": "Chapter 10\n288\nIf  an account has two account holders, then the associated bridge table has two \nrows. You assign a numerical weighting factor to each account holder such that the \nsum of all the weighting factors is exactly 1.00. The weighting factors are used to \nallocate any of the numeric additive facts across individual account holders. In this \nway you can add up all numeric facts by individual holder, and the grand total will \nbe the correct grand total amount. This kind of report is a correctly weighted report.\nThe weighting factors are simply a way to allocate the numeric additive facts \nacross the account holders. Some would suggest changing the grain of the fact table \nto be account snapshot by account holder. In this case you would take the weight-\ning factors and physically multiply them against the original numeric facts. This is \nrarely done for three reasons. First, the size of the fact table would be multiplied \nby the average number of account holders. Second, some fact tables have more than \none multivalued dimension. The number of rows would get out of hand in this situ-\nation, and you would start to question the physical signiﬁ cance of an individual row. \nFinally, you may want to see the unallocated numbers, and it is hard to reconstruct \nthese if the allocations have been combined physically with the numeric facts.\nIf  you choose not to apply the weighting factors in a given query, you can still \nsummarize the account snapshots by individual account holder, but in this case you \nget what is called an impact report. A question such as, “What is the total balance \nof all individuals with a speciﬁ c demographic proﬁ le?” would be an example of an \nimpact report. Business users understand impact analyses may result in overcount-\ning because the facts are associated with both account holders.\nIn Figure 10-4, an SQL view could be deﬁ ned combining the fact table and the \naccount-to-customer bridge table so these two tables, when combined, would appear \nto BI tools as a standard fact table with a normal customer foreign key. Two views \ncould be deﬁ ned, one using the weighting factors and one not using the weighting \nfactors.\nNOTE \nAn open-ended, many-valued attribute can be associated with a dimen-\nsion row by using a bridge table to associate the many-valued attributes with the \ndimension.\nIn some ﬁ nancial services companies, the individual customer is identiﬁ ed and \nassociated with each transaction. For example, credit card companies often issue \nunique card numbers to each cardholder. John and Mary Smith may have a joint \ncredit card account, but the numbers on their respective pieces of plastic are unique. \nIn this case, there is no need for an account-to-customer bridge table because the \natomic transaction facts are at the discrete customer grain; account and customer \nwould both be foreign keys in this fact table. However, the bridge table would be \n\n\nFinancial Services 289\nrequired to analyze metrics that are naturally captured at the account level, such \nas the credit card billing data.\n Mini-Dimensions Revisited\nSimilar  to the discussion of the customer dimension in Chapter 8: Customer \nRelationship Management, there are a wide variety of attributes describing the \nbank’s accounts, customers, and households, including monthly credit bureau attri-\nbutes, external demographic data, and calculated scores to identify their behavior, \nretention, proﬁ tability, and delinquency characteristics. Financial services organiza-\ntions are typically interested in understanding and responding to changes in these \nattributes over time.\nAs discussed earlier, it’s unreasonable to rely on slowly changing dimension tech-\nnique type 2 to track changes in the account dimension given the dimension row \ncount and attribute volatility, such as the monthly update of credit bureau attributes. \nInstead, you can break off  the browseable and changeable attributes into multiple \nmini-dimensions, such as credit bureau and demographics mini-dimensions, whose \nkeys are included in the fact table, as illustrated in Figure 10-5. The type 4 mini-\ndimensions enable you to slice and dice the fact table, while readily tracking attribute \nchanges over time, even though they may be updated at diff erent frequencies. Although \nmini-dimensions are extremely powerful, be careful to avoid overusing the technique. \nAccount-oriented ﬁ nancial services are a good environment for using mini-dimensions \nbecause the primary fact table is a very long-running periodic snapshot. Thus every \nmonth a fact table row is guaranteed to exist for every account, providing a home for \nall the associated foreign keys. You can always see the account together with all the \nmini-dimensions for any month.\nCustomer Dimension\nCustomer Key (PK)\nRelatively Constant Attributes ... \nCustomer Demographics Key (PK)\nCustomer Age Band\nCustomer Income Band\nCustomer Marital Status\nCustomer Risk Profile Key (PK)\nCustomer Risk Cluster\nCustomer Delinquency Cluster\nCustomer Demographics Dimension\nCustomer Key (FK)\nCustomer Demographics Key (FK)\nCustomer Risk Profile Key (FK)\nMore FKs ...\nFacts ...\nFact Table\nCustomer Risk Profile Dimension\nFigure 10-5: Multiple mini-dimensions associated with a fact table.\n\n\nChapter 10\n290\nNOTE \nMini-dimensions should consist of correlated clumps of attributes; each \nattribute shouldn’t be its own mini-dimension or you end up with too many dimen-\nsions in the fact table.\nAs described in Chapter 4, one of the compromises associated with mini-dimen-\nsions is the need to band attribute values to maintain reasonable mini-dimension row \ncounts. Rather than storing extremely discrete income amounts, such as $31,257.98, \nyou store income ranges, such as $30,000 to $34,999 in the mini-dimension. \nSimilarly, the proﬁ tability scores may range from 1 through 1200, which you band \ninto ﬁ xed ranges such as less than or equal to 100, 101 to 150, and 151 to 200, in \nthe mini-dimension.\nMost organizations ﬁ nd these banded attribute values support their routine ana-\nlytic requirements, however there are two situations in which banded values may \nbe inadequate. First, data mining analysis often requires discrete values rather than \nﬁ xed bands to be eff ective. Secondly, a limited number of power analysts may want \nto analyze the discrete values to determine if the bands are appropriate. In this case, \nyou still maintain the banded value mini-dimension attributes to support consistent \nday-to-day analytic reporting but also store the key discrete numeric values as facts \nin the fact table. For example, if each account’s proﬁ tability score were recalculated \neach month, you would assign the appropriate proﬁ tability range mini-dimension for \nthat score each month. In addition, you would capture the discrete proﬁ tability score \nas a fact in the monthly account snapshot fact table. Finally, if needed, the current \nproﬁ tability range or score could be included in the account dimension where any \nchanges are handled by deliberately overwriting the type 1 attribute. Each of these \ndata elements should be uniquely labeled so that they are distinguishable. Designers \nmust always carefully balance the incremental value of including such somewhat \nredundant facts and attributes versus the cost in terms of additional complexity for \nboth the ETL processing and BI presentation.\nAdding a Mini-Dimension to a Bridge Table\nIn  the bank account example, the account-to-customer bridge table can get very \nlarge. If you have 20 million accounts and 25 million customers, the bridge table can \ngrow to hundreds of millions of rows after a few years if both the account dimen-\nsion and the customer dimension are slowly changing type 2 dimensions (where \nyou track history by issuing new rows with new keys).\nNow the experienced dimensional modeler asks, “What happens when my cus-\ntomer dimension turns out to be a rapidly changing monster dimension?” This could \nhappen when rapidly changing demographics and status attributes are added to the \n\n\nFinancial Services 291\ncustomer dimension, forcing numerous type 2 additions to the customer dimension. \nNow the 25-million row customer dimension threatens to become several hundred \nmillion rows.\nThe  standard response to a rapidly changing monster dimension is to split off  the \nrapidly changing demographics and status attributes into a type 4 mini-dimension, \noften called a demographics dimension. This works great when this dimension attaches \ndirectly to the fact table along with a customer dimension because it stabilizes the \nlarge customer dimension and keeps it from growing every time a demographics or \nstatus attribute changes. But can you get this same advantage when the customer \ndimension is attached to a bridge table, as in the bank account example?\nThe  solution is to add a foreign key reference in the bridge table to the demo-\ngraphics dimension, as shown in Figure 10-6.\nAccount Dimension\nAccount Key (PK)\nAccount Number (NK)\n...\nAccount-to-Customer Bridge\nAccount Key (FK)\nCustomer Key (FK)\nDemographics Key (FK)\nFact Table\nMonth Key (FK)\nAccount Key (FK)\nMore FKs ...\nFacts ...\nCustomer Dimension\nCustomer Key (PK)\nCustomer Name\n...\nDemographics Key (PK)\nAge Band\nIncome Band\nMarital Status\nDemographics Dimension\nFigure 10-6: Account-to-customer bridge table with an added mini-dimension.\nThe way to visualize the bridge table is that it links every account to its associ-\nated customers and their demographics. The key for the bridge table now consists \nof the account key, customer key, and demographics key.\nDepending on how frequently new demographics are assigned to each customer, \nthe bridge table will perhaps grow signiﬁ cantly. In the above design because the \ngrain of the root bank account fact table is month by account, the bridge table should \nbe limited to changes recorded only at month ends. This takes some of the change \ntracking pressure off  the bridge table.\n Dynamic Value Banding of Facts\nSuppose  business users want the ability to perform value band reporting on a stan-\ndard numeric fact, such as the account balance, but are not willing to live with the \npredeﬁ ned bands in a dimension table. They may want to create a report based on \nthe account balance snapshot, as shown in Figure 10-7. \n\n\nChapter 10\n292\nBalance Range\n0–1000\n1001–2000\n2001–5000\n5001–10000\n10001 and up\nNumber of Accounts\n456,783\n367,881\n117,754\n52,662\n8,437\nTotal of Balances\n$229,305,066\n$552,189,381\n$333,479,328\n$397,229,466\n$104,888,784\nFigure 10-7: Report rows with dynamic value band groups.\nUsing the schema in Figure 10-3, it is diffi  cult to create this report directly from \nthe fact table. SQL has no generalization of the GROUP BY clause that clumps additive \nvalues into ranges. To further complicate matters, the ranges are of unequal size and \nhave textual names, such as “10001 and up.” Also, users typically need the ﬂ exibility \nto redeﬁ ne the bands at query time with diff erent boundaries or levels of precision.\nThe schema design shown in Figure 10-8 enables on-the-ﬂ y value band reporting. \nThe band deﬁ nition table can contain as many sets of diff erent reporting bands as \ndesired. The name of a particular group of bands is stored in the band group column. \nThe band deﬁ nition table is joined to the balance fact using a pair of less-than and \ngreater-than joins. The report uses the band range name as the row header and sorts \nthe report on the sort order attribute.\nBand Definition Table\nMonth End Date Key (FK)\nAccount Key (FK)\nProduct Key (FK)\nMore FKs ...\nPrimary Month Ending Balance\nBand Group Key (PK)\nBand Group Sort Order (PK)\nBand Group Name\nBand Range Name\nBand Lower Value\nBand Upper Value\nMonthly Account Snapshot Fact\nFigure 10-8: Dynamic value band reporting.\nControlling the performance of this query can be a challenge. A value band query \nis by deﬁ nition very lightly constrained. The example report needed to scan the \nbalances of more than 1 million accounts. Perhaps only the month dimension was \nconstrained to the current month. Furthermore the funny joins to the value band-\ning table are not the basis of a nice restricting constraint because they are grouping \nthe 1 million balances. In this situation, you may need to place an index directly \non the balance fact. The performance of a query that constrains or groups on the \nvalue of a fact-like balance will be improved enormously if the DBMS can effi  ciently \nsort and compress the individual fact. This approach was pioneered by the Sybase \nIQ columnar database product in the early 1990s and is now becoming a standard \nindexing option on several of the competing columnar DBMSs.\n\n\nFinancial Services 293\n Supertype and Subtype Schemas \nfor Heterogeneous Products\nIn  many ﬁ nancial service businesses, a dilemma arises because of the heteroge-\nneous nature of the products or services off ered by the institution. As mentioned \nin the introduction, a typical retail bank off ers a myriad of products, from check-\ning accounts to credit cards, to the same customers. Although every account at the \nbank has a primary balance and interest amount associated with it, each product \ntype has many special attributes and measured facts that are not shared by the other \nproducts. For instance, checking accounts have minimum balances, overdraft limits, \nservice charges, and other measures relating to online banking; time deposits such as \ncertiﬁ cates of deposit have few attribute overlaps with checking, but have maturity \ndates, compounding frequencies, and current interest rate.\nBusiness  users typically require two diff erent perspectives that are diffi  cult to \npresent in a single fact table. The ﬁ rst perspective is the global view, including the \nability to slice and dice all accounts simultaneously, regardless of their product \ntype. This global view is needed to plan appropriate customer relationship manage-\nment cross-sell and up-sell strategies against the aggregate customer/household base \nspanning all possible products. In this situation, you need the single supertype fact \ntable (refer to Figure 10-3) that crosses all the lines of business to provide insight \ninto the complete account portfolio. Note, however, that the supertype fact table \ncan present only a limited number of facts that make sense for virtually every line \nof business. You cannot accommodate incompatible facts in the supertype fact table \nbecause there may be several hundred of these facts when all the possible account \ntypes are considered. Similarly, the supertype product dimension must be restricted \nto the subset of common product attributes.\nThe  second perspective is the line-of-business view that focuses on the in-depth \ndetails of one business, such as checking. There is a long list of special facts and attri-\nbutes that make sense only for the checking business. These special facts cannot be \nincluded in the supertype fact table; if you did this for each line of business in a retail \nbank, you would end up with hundreds of special facts, most of which would have \nnull values in any speciﬁ c row. Likewise, if you attempt to include line-of-business \nattributes in the account or product dimension tables, these tables would have hun-\ndreds of special attributes, almost all of which would be empty for any given row. The \nresulting tables would resemble Swiss cheese, littered with data holes. The solution \nto this dilemma for the checking department in this example is to create a subtype \nschema for the checking line of business that is limited to just checking accounts, as \nshown in Figure 10-9.\n\n\nChapter 10\n294\nMonth Key (FK)\nAccount Key (FK)\nPrimary Customer Key (FK)\nBranch Key (FK)\nHousehold Key (FK)\nProduct Key (FK)\nBalance\nChange in Balance\nTotal Deposits\nTotal Withdrawals\nNumber Transactions\nMax Backup Reserve\nNumber Overdraws\nTotal Overdraw Penalties\nCount Local ATM Transactions \nCount Foreign ATM Transactions\nCount Online Transactions\nDays Below Minimum\n+ 10 more facts\nAccount Key (PK)\nAccount Number (NK)\nAccount Address Attributes\nAccount Open Date\n...\nChecking Account Fact\nChecking Account Dimension\nProduct Key (PK)\nProduct Code (NK)\nProduct Description\nPremium Flag\nChecking Type\nInterest Payment Type\nOverdraft Policy\n+ 12 more attributes\nChecking Product Dimension\nFigure 10-9: Line-of-business subtype schema for checking products.\nNow both the subtype checking fact table and corresponding checking account \ndimension are widened to describe all the speciﬁ c facts and attributes that make \nsense only for checking products. These subtype schemas must also contain the \nsupertype facts and attributes to avoid joining tables from the supertype and subtype \nschemas for the complete set of facts and attributes. You can also build separate \nsubtype fact and account tables for the other lines of business to support their in-\ndepth analysis requirements. Although creating account-speciﬁ c schemas sounds \ncomplex, only the DBA sees all the tables at once. From the business users’ perspec-\ntive, either it’s a cross-product analysis that relies on the single supertype fact table \nand its attendant supertype account table, or the analysis focuses on a particular \naccount type and only one of the subtype line of business schemas is utilized. In \ngeneral, it makes less sense to combine data from more than one subtype schema, \nbecause by deﬁ nition, the accounts’ facts and attributes are disjointed (or nearly so).\nThe  keys of the subtype account dimensions are the same keys used in the super-\ntype account dimension, which contains all possible account keys. For example, \nif the bank off ers a “$500 minimum balance with no per check charge” checking \naccount, this account would be identiﬁ ed by the same surrogate key in both the \nsupertype and subtype checking account dimensions. Each subtype account dimen-\nsion is a shrunken conformed dimension with a subset of rows from the supertype \n\n\nFinancial Services 295\naccount dimension table; each subtype account dimension contains attributes spe-\nciﬁ c to a particular account type.\nThis supertype/subtype design technique applies to any business that off ers \nwidely varied products through multiple lines of business. If you work for a technol-\nogy company that sells hardware, software, and services, you can imagine building \nsupertype sales fact and product dimension tables to deliver the global customer \nperspective. The supertype tables would include all facts and dimension attributes \nthat are common across lines of business. The supertype tables would then be \nsupplemented with schemas that do a deep dive into subtype facts and attributes that \nvary by business. Again, a speciﬁ c product would be assigned the same surrogate \nproduct key in both the supertype and subtype product dimensions.\nNOTE \nA family of supertype and subtype fact tables are needed when a business \nhas heterogeneous products that have naturally diff erent facts and descriptors, but \na single customer base that demands an integrated view.\nIf the lines of business in your retail bank are physically separated so each has its \nown location, the subtype fact and dimension tables will likely not reside in the same \nspace as the supertype fact and dimension tables. In this case, the data in the super-\ntype fact table would be duplicated exactly once to implement all the subtype tables. \nRemember that the subtype tables provide a disjointed partitioning of the accounts, \nso there is no overlap between the subtype schemas.\nSupertype and Subtype Products with Common Facts\nThe supertype and subtype product technique just discussed is appropriate for \nfact tables where a single logical row contains many product-speciﬁ c facts. On the \nother hand, the metrics captured by some business processes, such as the bank’s \nnew account solicitations, may not vary by line of business. In this case, you do \nnot need line-of-business fact tables; one supertype fact table suffi  ces. However, \nyou still can have a rich set of heterogeneous products with diverse attributes. In \nthis case, you would generate the complete portfolio of subtype account dimension \ntables, and use them as appropriate, depending on the nature of the application. \nIn a cross product analysis, the supertype account dimension table would be used \nbecause it can span any group of accounts. In a single account type analysis, you \ncould optionally use the subtype account dimension table instead of the supertype \ndimension if you wanted to take advantage of the subtype attributes speciﬁ c to \nthat account  type.\n\n\nChapter 10\n296\n Hot Swappable Dimensions\nA  brokerage house may have many clients who track the stock market. All of them \naccess the same fact table of daily high-low-close stock prices. But each client has a \nconﬁ dential set of attributes describing each stock. The brokerage house can sup-\nport this multi-client situation by having a separate copy of the stock dimension \nfor each client, which is joined to the single fact table at query time. We call these \nhot swappable dimensions. To implement hot swappable dimensions in a relational \nenvironment, referential integrity constraints between the fact table and the various \nstock dimension tables probably must be turned off  to allow the switches to occur \non an individual query basis.\nSummary\nWe began this chapter by discussing the situation in which a fact table has too few \ndimensions and provided suggestions for ferreting out additional dimensions using \na triage process. Approaches for handling the often complex relationship between \naccounts, customers, and households were described. We also discussed the use of \nmultiple mini-dimensions in a single fact table, which is fairly common in ﬁ nancial \nservices schemas.\nWe illustrated a technique for clustering numeric facts into arbitrary value bands \nfor reporting purposes through the use of a separate band table. Finally, we pro-\nvided recommendations for any organization that off ers heterogeneous products \nto the same set of customers. In this case, we create a supertype fact table that \ncontains performance metrics that are common across all lines of business. The \ncompanion dimension table contains rows for the complete account portfolio, but \nthe attributes are limited to those that are applicable across all accounts. Multiple \nsubtype schemas, one of each line of business, complement the supertype schema \nwith account-speciﬁ c f acts and attributes.\n\n\nTelecommunications\nT\nhis chapter unfolds a bit differently than preceding chapters. We begin with \na case study overview but we won’t be designing a dimensional model from \nscratch this time. Instead, we’ll step into a project midstream to conduct a design \nreview, looking for opportunities to improve the initial draft schema. The bulk of \nthis chapter focuses on identifying design flaws in dimensional models.\nWe’ll use a billing vignette drawn from the telecommunications industry as the \nbasis for the case study; it shares similar characteristics with the billing data gener-\nated by a utilities company. At the end of this chapter we’ll describe the handling \nof geographic location information in the data warehouse.\nChapter 11 discusses the following concepts:\n \n■Bus matrix snippet for telecommunications company\n \n■Design review exercise\n \n■Checklist of common design mistakes\n \n■Recommended tactics when conducting design reviews\n \n■Retroﬁ tting existing data structures\n \n■Abstract geographic location dimensions\n Telecommunications Case Study \nand Bus Matrix\nGiven  your extensive experience in dimensional modeling (10 chapters so far), you’ve \nrecently been recruited to a new position as a dimensional modeler on the DW/BI \nteam for a large wireless telecommunications company. On your ﬁ rst day, after a few \nhours of human resources paperwork and orientation, you’re ready to get to work.\nThe DW/BI team is anxious for you to review its initial dimensional design. So \nfar it seems the project is off  to a good start. The business and IT sponsorship com-\nmittee appreciates that the DW/BI program must be business-driven; as such, the \n11\n\n\nChapter 11\n298\ncommittee was fully supportive of the business requirements gathering process. \nBased on the requirements initiative, the team drafted an initial data warehouse bus \nmatrix, illustrated in Figure 11-1. The team identiﬁ ed several core business processes \nand a number of common dimensions. Of course, the complete enterprise matrix \nwould have a much larger number of rows and columns, but you’re comfortable that \nthe key constituencies’ major data requirements have been captured.\nPurchasing\nInternal Inventory\nChannel Inventory\nService Activation\nProduct Sales\nPromotion Participation\nCall Detail Traffic\nDate\nProduct\nCustomer\nService Line #\nSwitch\nCustomer Billing\nCustomer Support Calls\nRepair Work Orders\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nEmployee\nSupport Call\nProfile\nRate Plan\nSales\nOrganization\nFigure 11-1: Sample bus matrix rows for telecommunications company.\nThe sponsorship committee decided to focus on the customer billing process for the \ninitial DW/BI project. Business management determined better access to the metrics \nresulting from the billing process would have a signiﬁ cant impact on the business. \nManagement wants the ability to see monthly usage and billing metrics (otherwise \nknown as revenue) by customer, sales organization, and rate plan to perform sales \nchannel and rate plan analyses. Fortunately, the IT team felt it was feasible to tackle \nthis business process during the ﬁ rst warehouse iteration.\nSome people in the IT organization thought it would be preferable to tackle \nindividual call and message detail traffi  c, such as every call initiated or received by \nevery phone. Although this level of highly granular data would provide interesting \ninsights, it was determined by the joint sponsorship committee that the associated \ndata presents more feasibility challenges while not delivering as much short-term \nbusiness value.\nBased on the sponsors’ direction, the team looked more closely at the customer \nbilling data. Each month, the operational billing system generates a bill for each \nphone number, also known as a service line. Because the wireless company has \nmillions of service lines, this represents a signiﬁ cant amount of data. Each service \n",
      "page_number": 314
    },
    {
      "number": 35,
      "title": "Segment 35 (pages 325-332)",
      "start_page": 325,
      "end_page": 332,
      "detection_method": "topic_boundary",
      "content": "Telecommunications 299\nline is associated with a single customer. However, a customer can have multiple \nwireless service lines, which appear as separate line items on the same bill; each \nservice line has its own set of billing metrics, such as the number of minutes, \nnumber of text messages, amount of data, and monthly service charges. There is a \nsingle rate plan associated with each service line on a given bill, but this plan can \nchange as customers’ usage habits evolve. Finally, a sales organization and channel \nis associated with each service line to evaluate the ongoing billing revenue stream \ngenerated by each channel partner.\nWorking closely with representatives from the business and other DW/BI team \nmembers, the data modeler designed a fact table with the grain being one row per \nbill each month. The team proudly unrolls its draft dimensional modeling master-\npiece, as shown in Figure 11-2, and expectantly looks at you.\nWhat do you think? Before moving on, please spend several minutes studying the \ndesign in Figure 11-2. Try to identify the design ﬂ aws and suggest improvements \nbefore reading ahead.\nBill Dimension\nBill #\nService Line Number\nBill Date\nRate Plan Code (PK and NK)\nRate Plan Abbreviation\nPlan Minutes Allowed\nPlan Messages Allowed\nPlan Data MB Allowed\nNight-Weekend Minute Ind\nService Line Dimension\nService Line Number (PK)\nService Line Area Code\nService Line Activation Date\nCustomer ID (PK and NK)\nCustomer Name\nCustomer Address\nCustomer City\nCustomer State\nCustomer Zip\nOrig Authorization Credit Score\nCustomer Dimension\nBill # (FK)\nCustomer ID (FK)\nSales Org Number (FK)\nSales Channel ID (FK)\nRate Plan Code (FK)\nRate Plan Type Code\nCall Count\nTotal Minute Count\nNight-Weekend Minute Count\nRoam Minute Count\nMessage Count\nData MB Used\nMonth Service Charge\nPrior Month Service Charge\nYear-to-Date Service Charge\nMessage Charge\nData Charge\nRoaming Charge\nTaxes\nRegulatory Charges\nBilling Fact\nSales Org Number (PK and NK)\nSales Channel ID\nSales Org Dimension\nSales Channel ID (PK and NK)\nSales Channel Name\nSales Channel Dimension\nRate Plan Dimension\nFigure 11-2: Draft schema prior to design review.\nGeneral Design Review Considerations\nBefore we discuss the speciﬁ c issues and potential recommendations for the Figure \n11-2 schema, we’ll outline the design issues commonly encountered when conduct-\ning design reviews. Not to insinuate that the DW/BI team in this case study has \nstepped into all these traps, but it may be guilty of violating several. Again, the \ndesign review exercise will be a more eff ective learning tool if you take a moment \nto jot down your personal ideas regarding Figure 11-2 before proceeding.\n\n\nChapter 11\n300\nBalance Business Requirements and Source Realities\nDimensional  models should be designed based on a blended understanding of the \nbusiness’s needs, along with the operational source system’s data realities. While \nrequirements are collected from the business users, the underlying source data \nshould be proﬁ led. Models driven solely by requirements inevitably include data ele-\nments that can’t be sourced. Meanwhile, models driven solely by source system data \nanalysis inevitably omit data elements that are critical to the business’s analytics.\n Focus on Business Processes\nAs  reinforced for 10 chapters, dimensional models should be designed to mirror an \norganization’s primary business process events. Dimensional models should not be \ndesigned solely to deliver speciﬁ c reports or answer speciﬁ c questions. Of course, \nbusiness users’ analytic questions are critical input because they help identify which \nprocesses are priorities for the business. But dimensional models designed to pro-\nduce a speciﬁ c report or answer a speciﬁ c question are unlikely to withstand the \ntest of time, especially when the questions and report formats are slightly modiﬁ ed. \nDeveloping dimensional models that more fully describe the underlying business \nprocess are more resilient to change. Process-centric dimensional models also address \nthe analytic needs from multiple business departments; the same is deﬁ nitely not \ntrue when models are designed to answer a single department’s speciﬁ c need.\nAfter the base processes have been built, it may be useful to design complemen-\ntary schemas, such as summary aggregations, accumulating snapshots that look \nacross a workﬂ ow of processes, consolidated fact tables that combine facts from \nmultiple processes to a common granularity, or subset fact tables that provide access \nto a limited subset of fact data for security or data distribution purposes. Again, these \nare all secondary complements to the core process-centric dimensional models.\n Granularity\nThe  ﬁ rst question to always ask during a design review is, “What’s the grain of the \nfact table?” Surprisingly, you often get inconsistent answers to this inquiry from a \ndesign team. Declaring a clear and concise deﬁ nition of the grain of the fact table \nis critical to a productive modeling eff ort. The project team and business liaisons \nmust share a common understanding of this grain declaration; without this agree-\nment, the design eff ort will spin in circles.\nOf course, if you’ve read this far, you’re aware we strongly believe fact tables \nshould be built at the lowest level of granularity possible for maximum ﬂ exibility \nand extensibility, especially given the unpredictable ﬁ ltering and grouping required \nby business user queries. Users typically don’t need to see a single row at a time, \n\n\nTelecommunications 301\nbut you can’t predict the somewhat arbitrary ways they’ll want to screen and roll \nup the details. The deﬁ nition of the lowest level of granularity possible depends on \nthe business process being modeled. In this case, you want to implement the most \ngranular data available for the selected billing process, not just the most granular \ndata available in the enterprise.\nSingle Granularity for Facts\nAfter  the fact table granularity has been established, facts should be identiﬁ ed \nthat are consistent with the grain declaration. To improve performance or reduce \nquery complexity, aggregated facts such as year-to-date totals sometimes sneak into \nthe fact row. These totals are dangerous because they are not perfectly additive. \nAlthough a year-to-date total reduces the complexity and run time of a few speciﬁ c \nqueries, having it in the fact table invites double counting the year-to-date column \n(or worse) when more than one date is included in the query results. It is important \nthat once the grain of a fact table is chosen, all the additive facts are presented at \na uniform grain.\nYou should prohibit text ﬁ elds, including cryptic indicators and ﬂ ags, from the \nfact table. They almost always take up more space in the fact table than a surrogate \nkey. More important, business users generally want to query, constrain, and report \nagainst these text ﬁ elds. You can provide quicker responses and more ﬂ exible access \nby handling these textual values in a dimension table, along with descriptive rollup \nattributes associated with the ﬂ ags and indicators.\n Dimension Granularity and Hierarchies\nEach  of the dimensions associated with a fact table should take on a single value \nwith each row of fact table measurements. Likewise, each of the dimension attri-\nbutes should take on one value for a given dimension row. If the attributes have a \nmany-to-one relationship, this hierarchical relationship can be represented within \na single dimension. You should generally look for opportunities to collapse or \ndenormalize dimension hierarchies whenever possible.\nExperienced  data modelers often revert to the normalization techniques they’ve \napplied countless times in operational entity-relationship models. These modelers \noften need to be reminded that normalization is absolutely appropriate to support \ntransaction processing and ensure referential integrity. But dimensional models \nsupport analytic processing. Normalization in the dimensional model negatively \nimpacts the model’s twin objectives of understandability and performance. Although \nnormalization is not forbidden in the extract, transform, and load (ETL) system \nwhere data integrity must be ensured, it does place an additional burden on the \ndimension change handling subsystems.\n\n\nChapter 11\n302\nSometimes designers attempt to deal with dimension hierarchies within the fact \ntable. For example, rather than having a single foreign key to the product dimension, \nthey include separate foreign keys for the key elements in the product hierarchy, \nsuch as brand and category. Before you know it, a compact fact table turns into an \nunruly centipede fact table joining to dozens of dimension tables. If the fact table \nhas more than 20 or so foreign keys, you should look for opportunities to combine \nor collapse dimensions.\nElsewhere, normalization appears with the snowﬂ aking of hierarchical relationships \ninto separate dimension tables linked to one another. We generally also discourage this \npractice. Although snowﬂ aking may reduce the disk space consumed by dimension tables, \nthe savings are usually insigniﬁ cant when compared with the entire data warehouse \nenvironment and seldom off set the disadvantages in ease of use or query performance.\nThroughout this book we have occasionally discussed outriggers as permissible \nsnowﬂ akes. Outriggers can play a useful role in dimensional designs, but keep in \nmind that the use of outriggers for a cluster of relatively low-cardinality should be \nthe exception rather than the rule. Be careful to avoid abusing the outrigger tech-\nnique by overusing them in schemas.\nFinally, we sometimes review dimension tables that contain rows for both atomic and \nhierarchical rollups, such as rows for both products and brands in the same dimension \ntable. These dimensions typically have a telltale “level” attribute to distinguish between \nits base and summary rows. This pattern was prevalent and generally accepted decades \nago prior to aggregate navigation capabilities. However, we discourage its continued \nuse given the strong likelihood of user confusion and the risk of overcounting if the \nlevel indicator in every dimension is not constrained in every query.\nDate Dimension\nEvery  fact table should have at least one foreign key to an explicit date dimension. \nDesign teams sometimes join a generic date dimension to a fact table because they \nknow it’s the most common dimension but then can’t articulate what the date refers \nto, presenting challenges for the ETL team and business users alike. We encourage \na meaningful date dimension table with robust date rollup and ﬁ lter attributes.\nFixed Time Series Buckets Instead of Date Dimension\nDesigners  sometimes avoid a date dimension table altogether by representing a time \nseries of monthly buckets of facts on a single fact table row. Legacy operational systems \nmay contain metric sets that are repeated 12 times on a single record to represent \nmonth 1, month 2, and so on. There are several problems with this approach. First, \nthe hard-coded identity of the time slots is inﬂ exible. When you ﬁ ll up all the buck-\nets, you are left with unpleasant choices. You could alter the table to expand the row. \nOtherwise, you could shift everything over by one column, dropping the oldest data, \n\n\nTelecommunications 303\nbut this wreaks havoc with existing query applications. The second problem with \nthis approach is that all the attributes of the date are now the responsibility of the \napplication, not the database. There is no date dimension in which to place calendar \nevent descriptions for constraining. Finally, the ﬁ xed slot approach is ineffi  cient if \nmeasurements are taken only in a particular time period, resulting in null columns \nin many rows. Instead, these recurring time buckets should be presented as separate \nrows in the fact table.\n Degenerate Dimensions\nRather  than treating operational transaction numbers such as the invoice or order \nnumber as degenerate dimensions, teams sometimes want to create a separate \ndimension table for the transaction number. In this case, attributes of the transac-\ntion number dimension include elements from the transaction header record, such \nas the transaction date and customer.\nRemember, transaction numbers are best treated as degenerate dimensions. The \ntransaction date and customer should be captured as foreign keys on the fact table, \nnot as attributes in a transaction dimension. Be on the lookout for a dimension \ntable that has as many (or nearly as many) rows as the fact table; this is a warning \nsign that there may be a degenerate dimension lurking within a dimension table.\nSurrogate Keys\nInstead  of relying on operational keys or identiﬁ ers, we recommend the use of sur-\nrogate keys as the dimension tables’ primary keys. The only permissible deviation \nfrom this guideline applies to the highly predictable and stable date dimension. If \nyou are unclear about the reasons for pursuing this strategy, we suggest backtrack-\ning to Chapter 3: Retail Sales to refresh your memory.\nDimension Decodes and Descriptions\nAll  identiﬁ ers and codes in the dimension tables should be accompanied by descrip-\ntive decodes. This practice often seems counterintuitive to experienced data modelers \nwho have historically tried to reduce data redundancies by relying on look-up codes. \nIn the dimensional model, dimension attributes should be populated with the values \nthat business users want to see on BI reports and application pull-down menus. You \nneed to dismiss the misperception that business users prefer to work with codes. To \nconvince yourself, stroll down to their offi  ces to see the decode listings ﬁ lling their \nbulletin boards or lining their computer monitors. Most users do not memorize the \ncodes outside of a few favorites. New hires are rendered helpless when assaulted with \na lengthy list of meaningless codes.\n\n\nChapter 11\n304\nThe good news is that decodes can usually be sourced from operational systems \nwith relatively minimal additional eff ort or overhead. Occasionally, the descriptions \nare not available from an operational system but need to be provided by business \npartners. In these cases, it is important to determine an ongoing maintenance strat-\negy to maintain data quality.\nFinally, project teams sometimes opt to embed labeling logic in the BI tool’s \nsemantic layer rather than supporting it via dimension table attributes. Although \nsome BI tools provide the ability to decode within the query or reporting applica-\ntion, we recommend that decodes be stored as data elements instead. Applications \nshould be data-driven to minimize the impact of decode additions and changes. \nOf course, decodes that reside in the database also ensure greater report labeling \nconsistency because most organizations ultimately utilize multiple BI products.\n Conformity Commitment\nLast,  but certainly not least, design teams must commit to using shared conformed \ndimensions across process-centric models. Everyone needs to take this pledge \nseriously. Conformed dimensions are absolutely critical to a robust data architec-\nture that ensures consistency and integration. Without conformed dimensions, \nyou inevitably perpetuate incompatible stovepipe views of performance across the \norganization. By the way, dimension tables should conform and be reused whether \nyou drink the Kimball Kool-Aid or embrace a hub-and-spoke architectural alterna-\ntive. Fortunately, operational master data management systems are facilitating the \ndevelopment and deployment of conformed  dimensions.\nDesign Review Guidelines\nBefore diving into a review of the draft model in Figure 11-2, let’s review some prac-\ntical recommendations for conducting dimensional model design reviews. Proper \nadvance preparation increases the likelihood of a successful review process. Here \nare some suggestions when setting up for a design review:\n \n■Invite the right players. The modeling team obviously needs to participate, \nbut you also want representatives from the BI development team to ensure \nthat proposed changes enhance usability. Perhaps most important, it’s critical \nthat folks who are very knowledgeable about the business and their needs \nare sitting at the table. Although diverse perspectives should participate in a \nreview, don’t invite 25 people to the party.\n \n■Designate someone to facilitate the review. Group dynamics, politics, and the \ndesign challenges will drive whether the facilitator should be a neutral resource \nor involved party. Regardless, their role is to keep the team on track toward a \n\n\nTelecommunications 305\ncommon goal. Eff ective facilitators need the right mix of intelligence, enthu-\nsiasm, conﬁ dence, empathy, ﬂ exibility, assertiveness (and a sense of humor).\n \n■Agree on the review’s scope. Ancillary topics will inevitably arise during the \nreview, but agreeing in advance on the scope makes it easier to stay focused \non the task at hand.\n \n■Block time on everyone’s calendar. We typically conduct dimensional model \nreviews as a focused two day eff ort. The entire review team needs to be present \nfor the full two days. Don’t allow players to ﬂ oat in and out to accommodate \nother commitments. Design reviews require undivided attention; it’s disrup-\ntive when participants leave intermittently.\n \n■Reserve the right space. The same conference room should be blocked for \nthe full two days. Optimally, the room should have a large white board; it’s \nespecially helpful if the white board drawings can be saved or printed. If a \nwhite board is unavailable, have ﬂ ip charts on hand. Don’t forget markers \nand tape; drinks and food also help.\n \n■Assign homework. For example, ask everyone involved to make a list of their \ntop ﬁ ve concerns, problem areas, or opportunities for improvement with the \nexisting design. Encourage participants to use complete sentences when mak-\ning their list so that it’s meaningful to others. These lists should be sent to the \nfacilitator in advance of the design review for consolidation. Soliciting advance \ninput gets people engaged and helps avoid “group think” during the review.\nAfter the team gathers to focus on the review, we recommend the following \ntactics:\n \n■Check attitudes at the door. Although it’s easier said than done, don’t be \ndefensive about prior design decisions. Embark on the review thinking change \nis possible; don’t go in resigned to believing nothing can be done to improve \nthe situation.\n \n■Ban technology unless needed for the review process. Laptops and smart-\nphones should also be checked at the door (at least ﬁ guratively). Allowing \nparticipants to check e-mail during the sessions is no diff erent than having \nthem leave to attend an alternative meeting.\n \n■Exhibit strong facilitation skills. Review ground rules and ensure everyone is \nopenly participating and communicating. The facilitator must keep the group \non track and ban side conversations and discussions that are out of scope or \nspiral into the death zone.\n \n■Ensure a common understanding of the current model. Don’t presume every-\none around the table already has a comprehensive perspective. It may be \nworthwhile to dedicate the ﬁ rst hour to walking through the current design \nand reviewing objectives before delving into potential improvements. \n\n\nChapter 11\n306\n \n■Designate someone to act as scribe. He should take copious notes about both \nthe discussions and decisions being made.\n \n■Start with the big picture. Just as when you design from a blank slate, begin \nwith the bus matrix. Focus on a single, high-priority business process, deﬁ ne \nits granularity and then move out to the corresponding dimensions. Follow \nthis same “peeling back the layers of the onion” method with a design review, \nstarting with the fact table and then tackling dimension-related issues. But \ndon’t defer the tough stuff  to the afternoon of the second day.\n \n■Remind everyone that business acceptance is critical. Business acceptance is \nthe ultimate measure of DW/BI success. The review should focus on improv-\ning the business users’ experience.\n \n■Sketch out sample rows with data values. Viewing sample data during the \nreview sessions helps ensure everyone has a common understanding of \nthe recommended improvements.\n \n■Close the meeting with a recap. Don’t let participants leave the room with-\nout clear expectations about their assignments and due dates, along with an \nestablished time for the next follow-up.\nAfter the team completes the design review meeting, here are a few recommenda-\ntions to wrap up the process:\n \n■Assign responsibility for any remaining open issues. Commit to wrestling \nthese issues to the ground following the review, even though this can be chal-\nlenging without an authoritative party involved. \n \n■Don’t let the team’s hard work gather dust. Evaluate the cost/beneﬁ t for the \npotential improvements; some changes will be more painless (or painful) \nthan others. Action plans for implementing the improvements then need to \nbe developed.\n \n■Anticipate future reviews. Plan to reevaluate models every 12 to 24 months. \nTry to view inevitable changes to the design as signs of success, rather than \nfailure.\n Draft Design Exercise Discussion\nNow  that you’ve reviewed the common dimensional modeling mistakes frequently \nencountered during design reviews, refer to the draft design in Figure 11-2. Several \nopportunities for improvement should immediately jump out at you.\n",
      "page_number": 325
    },
    {
      "number": 36,
      "title": "Segment 36 (pages 333-342)",
      "start_page": 333,
      "end_page": 342,
      "detection_method": "topic_boundary",
      "content": "Telecommunications 307\nThe ﬁ rst thing to focus on is the grain of the fact table. The team stated the \ngrain is one row for each bill each month. However, based on your understanding \nfrom the source system documentation and data proﬁ ling eff ort, the lowest level of \nbilling data would be one row per service line on a bill. When you point this out, \nthe team initially directs you to the bill dimension, which includes the service line \nnumber. However, when reminded that each service line has its own set of billing \nmetrics, the team agrees the more appropriate grain declaration would be one row \nper service line per bill. The service line key is moved into the fact table as a foreign \nkey to the service line dimension.\nWhile discussing the granularity, the bill dimension is scrutinized, especially \nbecause the service line key was just moved into the fact table. As the draft model \nwas originally drawn in Figure 11-2, every time a bill row is loaded into the fact \ntable, a row also would be loaded into the bill dimension table. It doesn’t take much \nto convince the team that something is wrong with this picture. Even with the \nmodiﬁ ed granularity to include service line, you would still end up with nearly as \nmany rows in both the fact and bill dimension tables because many customers are \nbilled for one service line. Instead, the bill number should be treated as a degenerate \ndimension. At the same time, you move the bill date into the fact table and join it \nto a robust date dimension playing the role of bill date in this schema.\nYou’ve probably been bothered since ﬁ rst looking at the design by the double \njoins on the sales channel dimension table. The sales channel hierarchy has been \nunnecessarily snowﬂ aked. You opt to collapse the hierarchy by including the sales \nchannel identiﬁ ers (hopefully along with more meaningful descriptors) as additional \nattributes in the sales organization dimension table. In addition, you can eliminate \nthe unneeded sales channel foreign key in the fact table.\nThe design inappropriately treats the rate plan type code as a textual fact. Textual \nfacts are seldom a sound design choice. In this case study, the rate plan type code \nand its decode can be treated as rollup attributes in the rate plan dimension table.\nThe team spent some time discussing the relationship between the service line \nand the customer, sales organization, and rate plan dimensions. Because there is \na single customer, sales organization, and rate plan associated with a service line \nnumber, the dimensions theoretically could be collapsed and modeled as service \nline attributes. However, collapsing the dimensions would result in a schema with \njust two dimensions: bill date and service line. The service line dimension already \nhas millions of rows in it and is rapidly growing. In the end, you opt to treat the \ncustomer, sales organization, and rate plan as separate entities (or mini-dimensions) \nof the service line.\n\n\nChapter 11\n308\nSurrogate keys are used inconsistently throughout the design. Many of the draft \ndimension tables use operational identiﬁ ers as primary keys. You encourage the \nteam to implement surrogate keys for all the dimension primary keys and then \nreference them as fact table foreign keys.\nThe original design was riddled with operational codes and identiﬁ ers. Adding \ndescriptive names makes the data more legible to the business users. If required \nby the business, the operational codes can continue to accompany the descriptors \nas dimension attributes.\nFinally, you notice that there is a year-to-date metric stored in the fact table. \nAlthough the team felt this would enable users to report year-to-date ﬁ gures more \neasily, in reality, year-to-date facts can be confusing and prone to error. You opt \nto remove the year-to-date fact. Instead, users can calculate year-to-date amounts \non-the-ﬂ y by using a constraint on the year in the date dimension or leveraging the \nBI tool’s capabilities.\nAfter two exhausting days, the initial review of the design is complete. Of \ncourse, there’s more ground to cover, including the handling of changes to the \ndimension attributes. In the meantime, everyone on the team agrees the revamped \ndesign, illustrated in Figure 11-3, is a vast improvement. You’ve earned your ﬁ rst \nweek’s pay.\nBill Date Dimension\nRate Plan Key (PK)\nRate Plan Code\nRate Plan Name\nRate Plan Abbreviation\nRate Plan Type Code\nRate Plan Type Description\nPlan Minutes Allowed\nPlan Messages Allowed\nPlan Data MB Allowed\nNight-Weekend Minute Ind\nService Line Dimension\nService Line Key (PK)\nService Line Number (NK)\nService Line Area Code\nService Line Activation Date\nCustomer Key (PK)\nCustomer ID (NK)\nCustomer Name\nCustomer Address\nCustomer City\nCustomer State\nCustomer Zip\nOrig Authorization Credit Score\nCustomer Dimension\nBill Date Key (FK)\nCustomer Key (FK)\nService Line Key (FK)\nSales Organization Key (FK)\nRate Plan Key (FK)\nBill Number (DD)\nCall Count\nTotal Minute Count\nNight-Weekend Minute Count\nRoam Minute Count\nMessage Count\nData MB Used\nMonth Service Charge\nMessage Charge\nData Charge\nRoaming Charge\nTaxes\nRegulatory Charges\nBilling Fact\nSales Organization Key (PK)\nSales Organization Number\nSales Organization Name\nSales Channel ID\nSales Channel Name\nSales Organization Dimension\nRate Plan Dimension\nFigure 11-3: Draft schema following design review.\n\n\nTelecommunications 309\nRemodeling Existing Data Structures\nIt’s  one thing to conduct a review and identify opportunities for improvement. \nHowever, implementing the changes might be easier said than done if the design \nhas already been physically implemented.\nFor example, adding a new attribute to an existing dimension table feels like a \nminor enhancement. It is nearly pain-free if the business data stewards declare it \nto be a slowly changing dimension type 1 attribute. Likewise if the attribute is to \nbe populated starting now with no attempt to backﬁ ll historically accurate values \nbeyond a Not Available attribute value; note that while this tactic is relatively easy \nto implement, it presents analytic challenges and may be deemed unacceptable. But \nif the new attribute is a designated type 2 attribute with the requirement to capture \nhistorical changes, this seemingly simple enhancement just got much more com-\nplicated. In this scenario, rows need to be added to the dimension table to capture \nthe historical changes in the attribute, along with the other dimension attribute \nchanges. Some fact table rows then need to be recast so the appropriate dimension \ntable row is associated with the fact table’s event. This most robust approach con-\nsumes surprisingly more eff ort than you might initially imagine.\nMuch less surprising is the eff ort required to take an existing dimensional model \nand convert it into a structure that leverages newly created conformed dimensions. \nAs discussed in Chapter 4: Inventory, at a minimum, the fact table’s rows must be \ncompletely reprocessed to reference the conformed dimension keys. The task is \nobviously more challenging if there are granularity or other major issues.\nIn addition to thinking about the data-centric challenges of retroﬁ tting existing \ndata structures, there are also unwanted ripples in the BI reporting and analytic \napplications built on the existing data foundation. Using views to buff er the BI appli-\ncations from the physical data structures provides some relief, but it’s typically not \nadequate to avoid unpleasant whipsawing in the BI environment.\nWhen considering enhancements to existing data structures, you must evaluate \nthe costs of tackling the changes alongside the perceived beneﬁ ts. In many cases, \nyou’ll determine improvements need to be made despite the pain. Similarly, you \nmay determine the best approach is to decommission the current structures to put \nthem out of their misery and tackle the subject area with a fresh slate. Finally, in \nsome situations, the best approach is to simply ignore the suboptimal data structures \nbecause the costs compared to the potential beneﬁ ts don’t justify the remodeling \nand schema improvement eff ort. Sometimes, the best time to consider a remodeling \neff ort is when other changes, such as a source system conversion or migration to a \nnew BI tool standard, provide a  catalyst.\n\n\nChapter 11\n310\n Geographic Location Dimension\nLet’s  shift gears and presume you work for a phone company with land lines tied to \na speciﬁ c physical location. The telecommunications and utilities industries have a \nvery well-developed notion of location. Many of their dimensions contain a precise \ngeographic location as part of the attribute set. The location may be resolved to a \nphysical street, city, state, ZIP code, latitude, and longitude. Latitude and longitude \ngeo-coding can be leveraged for geospatial analysis and map-centric visualization. \nSome designers imagine a single master location table where address data is stan-\ndardized and then the location dimension is attached as an outrigger to the service \nline telephone number, equipment inventory, network inventory (including poles \nand switch boxes), real estate inventory, service location, dispatch location, right of \nway, and customer entities. In this scenario, each row in the master location table \nis a speciﬁ c point in space that rolls up to every conceivable geographic grouping.\nStandardizing the attributes associated with points in space is valuable. However, \nthis is a back room ETL task; you don’t need to unveil the single resultant table \ncontaining all the addresses the organization interacts with to the business users. \nGeographic information is naturally handled as attributes within multiple dimen-\nsions, not as a standalone location dimension or outrigger. There is typically little \noverlap between the geographic locations embedded in various dimensions. You \nwould pay a performance price for consolidating all the disparate addresses into a \nsingle dimension.\nOperational systems often embrace data abstraction, but you should typically \navoid generic abstract dimensions, such as a generalized location dimension in the \nDW/BI presentation area because they negatively impact the ease-of-use and query \nperformance objectives. These structures are more acceptable behind the scenes in \nthe ETL back room.\nSummary\nThis chapter provided the opportunity to conduct a design review using an example \ncase study. It provided recommendations for conducting eff ective design reviews, \nalong with a laundry list of common design ﬂ aws to scout for when performing a \nreview. We encourage you to use this laundry list to review your own draft schemas \nwhen searching for potent ial improvements.\n\n\nTransportation\nV\noyages  occur whenever a person or thing travels from one point to another, \nperhaps with stops in the middle. Obviously, voyages are a fundamental \nconcept for organizations in the travel industry. Shippers and internal logistical \nfunctions also relate to the discussion, as well as package delivery services and car \nrental companies. Somewhat unexpected, many of this chapter’s schemas are also \napplicable to telecommunications network route analyses; a phone network can \nbe thought of as a map of possible voyages that a call makes between origin and \ndestination phone numbers.\nIn  this chapter we’ll draw on an airline case study to explore voyages and routes \nbecause many readers are familiar (perhaps too familiar) with the subject matter. \nThe case study lends itself to a discussion of multiple fact tables at diff erent granu-\nlarities. We’ll also elaborate on dimension role playing and additional date and time \ndimension considerations. As usual, the intended audience for this chapter should \nnot be limited to the industries previously listed.\nChapter 12 discusses the following concepts:\n \n■Bus matrix snippet for an airline\n \n■Fact tables at diff erent levels of granularity\n \n■Combining correlated role-playing dimensions \n \n■Country-speciﬁ c date dimensions\n \n■Dates and times in multiple time zones\n \n■Recap of localization issues\n Airline Case Study and Bus Matrix\nWe’ll  begin by exploring a simpliﬁ ed bus matrix, and then dive into the fact tables \nassociated with ﬂ ight activity.\n12\n\n\nChapter 12\n312\nFigure 12-1 shows a snippet of an airline’s bus matrix. This example includes \nan additional column to capture the degenerate dimension associated with most of \nthe bus process events. Like most organizations, airlines are keenly interested in \nrevenue. In this industry, the sale of a ticket represents unearned revenue; revenue \nis earned when a passenger takes a ﬂ ight between origin and destination airports.\nReservations\nIssued Tickets\nUnearned Revenue & Availability\nFlight Activity\nFrequent Flyer Account Credits \nCustomer Care Interactions\nFrequent Flyer Communications\nTime\nAirport\nPassenger\nFare Basis\nAircraft\nMaintenance Work Orders\nCrew Scheduling\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nConf #\nConf #\nTicket #\nConf #\nTicket #\nConf #\nTicket #\nCase #\nTicket #\nWork\nOrder #\nX\nDate\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nCommunication\nProfile\nBooking Channel\nClass of Service\nTransaction ID #\nFigure 12-1: Subset of bus matrix row for an airline.\nThe business and DW/BI team representatives decide the ﬁ rst deliverable should \nfocus on ﬂ ight activity. The marketing department wants to analyze what ﬂ ights the \ncompany’s frequent ﬂ yers take, what fare basis they pay, how often they upgrade, \nhow they earn and redeem their frequent ﬂ yer miles, whether they respond to spe-\ncial fare promotions, how long their overnight stays are, and what proportion of \nthese frequent ﬂ yers have gold, platinum, aluminum, or titanium status. The ﬁ rst \nproject doesn’t focus on reservation or ticketing activity data that didn’t result in a \npassenger boarding a plane. The DW/BI team will contend with those other sources \nof data in subsequent phases.\n Multiple Fact Table Granularities\nWhen  it comes to the grain as you work through the four-step design process, this \ncase presents multiple potential levels of fact table granularity, each having diff er-\nent associated metrics.\n\n\nTransportation 313\nAt the most granular level, the airline captures data at the leg level. The leg \nrepresents an aircraft taking off  at one airport and landing at another without any \nintermediate stops. Capacity planning and ﬂ ight scheduling analysts are interested \nin this discrete level of information because they can look at the number of seats \nto calculate load factors by leg. Operational aircraft ﬂ ight metrics are captured at \nthe leg level, such as ﬂ ight duration and the number of minutes late at departure \nand arrival. Perhaps there’s even a dimension to easily identify on-time arrivals.\nThe  next level of granularity corresponds to a segment. Segments refer to a \nsingle ﬂ ight number (such as Delta ﬂ ight number 40 or DL0040) ﬂ own by a single \naircraft. Segments may have one or more legs associated with them; in most cases \nsegments are composed of just one leg with a single take-off  and landing. If you \ntake a ﬂ ight from San Francisco to Minneapolis with a stop in Denver but no air-\ncraft or ﬂ ight number change, you have ﬂ own one segment (SFO-MSP) but two \nlegs (SFO-DEN and DEN-MSP). Conversely, if the ﬂ ight ﬂ ew nonstop from San \nFrancisco to Minneapolis, you would have ﬂ own one segment as well as one leg. \nThe segment represents the line item on an airline ticket coupon; passenger revenue \nand mileage credit is determined at the segment level. So although some airline \ndepartments focus on leg level operations, the marketing and revenue groups focus \non segment-level metrics.\nNext, you can analyze ﬂ ight activity by trip. The trip provides an accurate picture \nof customer demand. In the prior example, assume the ﬂ ights from San Francisco \nto Minneapolis required the ﬂ yer to change aircraft in Denver. In this case, the trip \nfrom San Francisco to Minneapolis would entail two segments corresponding to the \ntwo involved aircraft. In reality, the passenger just asked to go from San Francisco \nto Minneapolis; the fact that she needs to stop in Denver is merely a necessary evil. \nFor this reason, sales and marketing analysts are also interested in trip level data.\nFinally, the airline collects data for the itinerary, which is equivalent to the entire \nairline ticket or reservation conﬁ rmation number.\nThe DW/BI team and business representatives decide to begin at the segment-level \ngrain. This represents the lowest level of data with meaningful revenue metrics. \nAlternatively, you could lean on the business for rules to allocate the segment-level \nmetrics down to the leg, perhaps based on the mileage of each leg within the seg-\nment. The data warehouse inevitably will tackle the more granular leg level data for \nthe capacity planners and ﬂ ight schedulers at some future point. The conforming \ndimensions built during this ﬁ rst iteration will be leveraged at that time.\n There will be one row in the fact table for each boarding pass collected from \npassengers. The dimensionality associated with this data is quite extensive, as \nillustrated in Figure 12-2. The schema extensively uses the role-playing technique. \nThe multiple date, time, and airport dimensions link to views of a single underly-\ning physical date, time, and airport dimension table, respectively, as we discussed \noriginally in Chapter 6: Order Management.\n\n\nChapter 12\n314\nTime-of-Day Dimension (views for 2 roles)\nAirport Dimension (views for 2 roles)\nDate Dimension (views for 2 roles)\nPassenger Dimension\nPassenger Profile Dimension\nClass of Service Flown Dimension\nBooking Channel Dimension\nAircraft Dimension\nFare Basis Dimension\nSegment-Level Flight Activity Fact\nScheduled Departure Date Key (FK)\nScheduled Departure Time Key (FK)\nActual Departure Date Key (FK)\nActual Departure Time Key (FK)\nPassenger Key (FK)\nPassenger Profile Key (FK)\nSegment Origin Airport Key (FK)\nSegment Destination Airport Key (FK)\nAircraft Key (FK)\nClass of Service Flown Key (FK)\nFare Basis Key (FK)\nBooking Channel Key (FK)\nConfirmation Number (DD)\nTicket Number (DD)\nSegment Sequence Number (DD)\nFlight Number (DD)\nBase Fare Revenue\nPassenger Facility Charges\nAirport Tax\nGovernment Tax\nBaggage Charges\nUpgrade Fees\nTransaction Fees\nSegment Miles Flown\nSegment Miles Earned\nFigure 12-2: Initial segment ﬂ ight activity schema.\nThe  passenger dimension is a garden variety customer dimension with rich attri-\nbutes captured about the most valuable frequent ﬂ yers. Interestingly, frequent ﬂ yers \nare motivated to help maintain this dimension accurately because they want to \nensure they’re receiving appropriate mileage credit. For a large airline, this dimen-\nsion has tens to hundreds of millions of rows.\nMarketing wants to analyze activity by the frequent ﬂ yer tier, which can change \nduring the course of a year. In addition, you learned during the requirements pro-\ncess that the users are interested in slicing and dicing based on the ﬂ yers’ home \nairports, whether they belong to the airline’s airport club at the time of each ﬂ ight, \nand their lifetime mileage tier. Given the change tracking requirements, coupled \nwith the size of the passenger dimension, we opt to create a separate passenger pro-\nﬁ le mini-dimension, as we discussed in Chapter 5: Procurement, with one row for \neach unique combination of frequent ﬂ yer elite tier, home airport, club membership \nstatus, and lifetime mileage tier. Sample rows for this mini-dimension are illustrated \nin Figure 12-3. You considered treating these attributes as slowly changing type \n2 attributes, especially because the attributes don’t rapidly change. But given the \nnumber of passengers, you opt for a type 4 mini-dimension instead. As it turns \nout, marketing analysts often leverage this mini-dimension for their analysis and \nreporting without touching the millions of passenger dimension rows.\n\n\nTransportation 315\nPassenger Profile\nKey\n1\n2\n3\n789\n790\n791\n2468\n2469\n2470\n...\n...\n...\nBasic\nBasic\nBasic\nMidTier\nMidTier\nMidTier\nWarriorTier\nWarriorTier\nWarriorTier\n...\n...\n...\nATL\nATL\nBOS\nATL\nATL\nBOS\nATL\nATL\nBOS\n...\n...\n...\nNon-Member\nClub Member\nNon-Member\nNon-Member\nClub Member\nNon-Member\nClub Member\nClub Member\nClub Member\n...\n...\n...\nUnder 100,000 miles\nUnder 100,000 miles\nUnder 100,000 miles\n100,000-499,999 miles\n100,000-499,999 miles\n100,000-499,999 miles\n1,000,000-1,999,999 miles\n2,000,000-2,999,999 miles\n1,000,000-1,999,999 miles\n...\n...\n...\nFrequent Flyer\nTier\nHome Airport\nClub Membership\nStatus\nLifetime Mileage\nTier\nFigure 12-3: Passenger mini-dimension sample rows.\nThe aircraft dimension contains information about each plane ﬂ own. The origin \nand destination airports associated with each ﬂ ight are called out separately to \nsimplify the user’s view of the data and make access more effi  cient.\nThe class of service ﬂ own describes whether the passenger sat in economy, pre-\nmium economy, business, or ﬁ rst class. The fare basis dimension describes the terms \nsurrounding the fare. It would identify whether it’s an unrestricted fare, a 21-day \nadvance purchase fare with change and cancellation penalties, or a 10 percent off  \nfare due to a special promotion.\nThe  sales channel dimension identiﬁ es how the ticket was purchased, whether \nthrough a travel agency, directly from the airline’s phone number, city ticket offi  ce, or \nwebsite, or via another internet travel services provider. Although the sales channel \nrelates to the entire ticket, each segment should inherit ticket-level dimensional-\nity. In addition, several operational numbers are associated with the ﬂ ight activity \ndata, including the itinerary number, ticket number, ﬂ ight number, and segment \nsequence number.\nThe facts captured at the segment level of granularity include the base fare rev-\nenue, passenger facility charges, airport and government taxes, other ancillary \ncharges and fees, segment miles ﬂ own, and segment miles awarded (in those cases \nin which a minimum number of miles are awarded regardless of the ﬂ ight distance).\n Linking Segments into Trips\nDespite  the powerful dimensional framework you just designed, you cannot easily \nanswer one of the most important questions about your frequent ﬂ yers, namely, \n“Where are they going?” The segment grain masks the true nature of the trip. If \nyou fetch all the segments of a trip and sequence them by segment number, it is still \n\n\nChapter 12\n316\nnearly impossible to discern the trip start and endpoints. Most complete itinerar-\nies start and end at the same airport. If a lengthy stop were used as a criterion for \na meaningful trip destination, it would require extensive and tricky processing at \nthe BI reporting layer whenever you try to summarize trips.\nThe answer is to introduce two more airport role-playing dimensions, trip origin \nand trip destination, while keeping the grain at the ﬂ ight segment level. These are \ndetermined during data extraction by looking on the ticket for any stop of more \nthan four hours, which is the airline’s offi  cial deﬁ nition of a stopover. You need to \nexercise some caution when summarizing data by trip in this schema. Some of the \ndimensions, such as fare basis or class of service ﬂ own, don’t apply at the trip level. \nOn the other hand, it may be useful to see how many trips from San Francisco to \nMinneapolis included an unrestricted fare on a segment.\nIn addition to linking segments into trips on the segment ﬂ ight activity schema, \nif the business users are constantly looking at information at the trip level, rather \nthan by segment, you might create an aggregate fact table at the trip grain. Some of \nthe earlier dimensions discussed, such as class of service and fare basis, obviously \nwould not be applicable. The facts would include aggregated metrics like trip total \nbase fare or trip total taxes, plus additional facts that would appear only in this \ncomplementary trip summary table, such as the number of segments in the trip. \nHowever, you would go to the trouble of creating this aggregate table only if there \nwere obvious performance or usability issues when you use the segment-level table \nas the basis for rolling up the same reports. If a typical trip consists of three seg-\nments, you might barely see a three times performance improvement with such an \naggregate table, meaning it may not be worth the bother.\nRelated Fact Tables\nAs discussed earlier, you would likely create a leg-grained ﬂ ight activity fact table \nto satisfy the more operational needs surrounding the departure and arrival of each \nﬂ ight. Metrics at the leg level might include actual and blocked ﬂ ight durations, \ndeparture and arrival delays, and departure and arrival fuel weights.\nIn addition to the ﬂ ight activity, there will be fact tables to capture reservations \nand issued tickets. Given the focus on maximizing revenue, there might be a rev-\nenue and availability snapshot for each ﬂ ight; it could provide snapshots for the \nﬁ nal 90 days leading up to a ﬂ ight departure with cumulative unearned revenue and \nremaining availability per class of service for each scheduled ﬂ ight. The snapshot \nmight include a dimension supporting the concept of “days prior to departure” to \nfacilitate the comparison of similar ﬂ ights at standard milestones, such as 60 days \nprior to scheduled departure.\n",
      "page_number": 333
    },
    {
      "number": 37,
      "title": "Segment 37 (pages 343-351)",
      "start_page": 343,
      "end_page": 351,
      "detection_method": "topic_boundary",
      "content": "Transportation 317\nExtensions to Other Industries\nUsing the airline case study to illustrate a voyage schema makes intuitive sense \nbecause most people have boarded a plane at one time or another. We’ll brieﬂ y touch \non several other variations on this theme.\nCargo Shipper\nThe schema for a cargo shipper looks quite similar to the airline schemas just \ndeveloped. Suppose a transoceanic shipping company transports bulk goods in \ncontainers from foreign to domestic ports. The items in the containers are shipped \nfrom an original shipper to a ﬁ nal consignor. The trip can have multiple stops at \nintermediate ports. It is possible the containers may be off -loaded from one ship to \nanother at a port. Likewise, it is possible one or more of the legs may be by truck \nrather than ship.\nAs illustrated in Figure 12-4, the grain of the fact table is the container on a spe-\nciﬁ c bill-of-lading number on a particular leg of its trip. The ship mode dimension \nidentiﬁ es the type of shipping company and speciﬁ c vessel. The container dimen-\nsion describes the size of the container and whether it requires electrical power or \nrefrigeration. The commodity dimension describes the item in the container. Almost \nanything that can be shipped can be described by harmonized commodity codes, \nwhich are a kind of master conformed dimension used by agencies, including U.S. \nCustoms. The consignor, foreign transporter, foreign consolidator, shipper, domestic \nconsolidator, domestic transporter, and consignee are all roles played by a master \nbusiness entity dimension that contains all the possible business parties associated \nwith a voyage. The bill-of-lading number is a degenerate dimension. We assume the \nfees and tariff s are applicable to the individual leg of the  voyage.\nTravel Services\nIf  you work for a travel services company, you can complement the ﬂ ight activity \nschema with fact tables to track associated hotel stays and rental car usage. These \nschemas would share several common dimensions, such as the date and customer. \nFor hotel stays, the grain of the fact table is the entire stay, as illustrated in Figure \n12-5. The grain of a similar car rental fact table would be the entire rental episode. \nOf course, if constructing a fact table for a hotel chain rather than a travel services \ncompany, the schema would be much more robust because you’d know far more \nabout the hotel property characteristics, the guest’s use of services, and associated \ndetailed charges.\n\n\nChapter 12\n318\nShipping Transport Fact\nVoyage Departure Date Key (FK)\nLeg Departure Date Key (FK)\nVoyage Origin Port Key (FK)\nVoyage Destination Port Key (FK)\nLeg Origin Port Key (FK)\nLeg Destination Port Key (FK)\nShip Mode Key (FK)\nContainer Key (FK)\nCommodity Key (FK)\nConsignor Key (FK)\nForeign Transporter Key (FK)\nForeign Consolidator Key (FK)\nShipper Key (FK)\nDomestic Consolidator Key (FK)\nDomestic Transporter Key (FK)\nConsignee Key (FK)\nBill-of-Lading Number (DD)\nLeg Fee\nLeg Tariffs\nLeg Miles\nDate Dimension (views for 2 roles)\nPort Dimension (views for 4 roles)\nShip Mode Dimension\nContainer Dimension\nCommodity Dimension\nBusiness Entity Dimension (views for 7 roles)\nFigure 12-4: Shipper schema.\nTravel Services Hotel Stay Fact\nReservation Date Key (FK)\nArrival Date Key (FK)\nDeparture Date Key (FK)\nCustomer Key (FK)\nHotel Property Key (FK)\nSales Channel Key (FK)\nConfirmation Number (DD)\nTicket Number (DD)\nNumber of Nights\nExtended Room Charge\nTax Charge\nDate Dimension (views for 3 roles)\nCustomer Dimension\nSales Channel Dimension\nHotel Property Dimension\nFigure 12-5: Travel services hotel stay schema.\n Combining Correlated Dimensions\nWe stated previously that if a many-to-many relationship exists between two groups \nof dimension attributes, they should be modeled as separate dimensions with sepa-\nrate foreign keys in the fact table. Sometimes, however, you encounter situations \nwhere these dimensions can be combined into a single dimension rather than treat-\ning them as two separate dimensions with two separate foreign keys in the fact table.\n\n\nTransportation 319\nClass of Service\nThe  Figure 12-2 draft schema includes the class of service flown dimension. \nFollowing a design checkpoint with the business community, you learn the \nusers also want to analyze the booking class purchased. In addition, the business users \nwant to easily ﬁ lter and report on activity based on whether an upgrade or down-\ngrade occurred. Your initial reaction might be to include a second role-playing \ndimension and foreign key in the fact table to support both the purchased and \nﬂ own class of service. In addition, you would need a third foreign key for the \nupgrade indicator; otherwise, the BI application would need to include logic to \nidentify numerous scenarios as upgrades, including economy to premium economy, \neconomy to business, economy to ﬁ rst, premium economy to business, and so on. \nIn this situation, however, there are only four rows in the class dimension table \nto indicate ﬁ rst, business, premium economy, and economy classes. Likewise, the \nupgrade indicator dimension also would have just three rows in it, corresponding to \nupgrade, downgrade, or no class change. Because the row counts are so small, you \ncan elect instead to combine the dimensions into a single class of service dimension, \nas illustrated in Figure 12-6.\nClass of Service\nKey\nClass Purchased\nClass Flown\nPurchased-Flown Group\nClass Change\nIndicator\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\nEconomy\nEconomy\nEconomy\nEconomy\nPrem Economy\nPrem Economy\nPrem Economy\nPrem Economy\nBusiness\nBusiness\nBusiness\nBusiness\nFirst\nFirst\nFirst\nFirst\nEconomy\nPrem Economy\nBusiness\nFirst\nEconomy\nPrem Economy\nBusiness\nFirst\nEconomy\nPrem Economy\nBusiness\nFirst\nEconomy\nPrem Economy\nBusiness\nFirst\nEconomy-Economy\nEconomy-Prem Economy\nEconomy-Business\nEconomy-First\nPrem Economy-Economy\nPrem Economy-Prem Economy\nPrem Economy-Business\nPrem Economy-First\nBusiness-Economy\nBusiness-Prem Economy\nBusiness-Business\nBusiness-First\nFirst-Economy\nFirst-Prem Economy\nFirst-Business\nFirst-First\nNo Class Change\nUpgrade\nUpgrade\nUpgrade\nDowngrade\nNo Class Change\nUpgrade\nUpgrade\nDowngrade\nDowngrade\nNo Class Change\nUpgrade\nDowngrade\nDowngrade\nDowngrade\nNo Class Change\nFigure 12-6: Combined class dimension sample rows.\nThe Cartesian product of the separate class dimensions results in a 16-row \ndimension table (4 class purchased rows times 4 class ﬂ own rows). You also have \nthe opportunity in this combined dimension to describe the relationship between \n\n\nChapter 12\n320\nthe purchased and ﬂ own classes, such as a class change indicator. Think of this \ncombined class of service dimension as a type of junk dimension, introduced in \nChapter 6. In this case study, the attributes are tightly correlated. Other airline fact \ntables, such as inventory availability or ticket purchases, would invariably reference \na conformed class dimension table with just four  rows.\nNOTE \nIn most cases, role-playing dimensions should be treated as separate logi-\ncal dimensions created via views on a single physical table. In isolated situations, \nit may make sense to combine the separate dimensions into a single dimension, \nnotably when the data volumes are extremely small or there is a need for additional \nattributes that depend on the combined underlying roles for context and meaning.\nOrigin and Destination\nLikewise,  consider the pros and cons of combining the origin and destination airport \ndimensions. In this situation the data volumes are more signiﬁ cant, so separate role-\nplaying origin and destination dimensions seem more practical. However, the busi-\nness users may need additional attributes that depend on the combination of origin \nand destination. In addition to accessing the characteristics of each airport, business \nusers also want to analyze ﬂ ight activity data by the distance between the city-pair \nairports, as well as the type of city pair (such as domestic or trans-Atlantic). Even \nthe seemingly simple question regarding the total activity between San Francisco \n(SFO) and Denver (DEN), regardless of whether the ﬂ ights originated in SFO or \nDEN, presents some challenges with separate origin and destination dimensions. \nSQL experts could surely answer the question programmatically with separate air-\nport dimensions, but what about the less empowered? Even if experts can derive \nthe correct answer, there’s no standard label for the nondirectional city-pair route. \nSome reporting applications may label it SFO-DEN, whereas others might opt for \nDEN-SFO, San Fran-Denver, Den-SF, and so on. Rather than embedding inconsis-\ntent labels in BI reporting application code, the attribute values should be stored \nin a dimension table, so common standardized labels can be used throughout the \norganization. It would be a shame to go to the bother of creating a data warehouse \nand then allowing application code to implement inconsistent reporting labels. The \nbusiness sponsors of the DW/BI system won’t tolerate that for long.\nTo satisfy the need to access additional city-pair route attributes, you have two \noptions. One is merely to add another dimension to the fact table for the city-pair \nroute descriptors, including the directional route name, nondirectional route name, \ntype, and distance, as shown in Figure 12-7. The other alternative is to combine \n\n\nTransportation 321\nthe origin and destination airport attributes, plus the supplemental city-pair route \nattributes, into a single dimension. Theoretically, the combined dimension could \nhave as many rows as the Cartesian product of all the origin and destination air-\nports. Fortunately, in real life the number of rows is much smaller than this theo-\nretical limit because airlines don’t operate ﬂ ights between every airport where they \nhave a presence. However, with a couple dozen attributes about the origin airport, \nplus a couple dozen identical attributes about the destination airport, along with \nattributes about the route, you would probably be more tempted to treat them as \nseparate dimensions.\nCity-Pair\nRoute Key\nDirectional\nRoute Name\n1\n2\n3\n4\n5\n6\n191\n191\n3,267\n3,267\n6,737\n6,737\nBOS-JFK\nJFK-BOS\nBOS-LGW\nLGW-BOS\nBOS-NRT\nNRT-BOS\nBOS-JFK\nBOS-JFK\nBOS-LGW\nBOS-LGW\nBOS-NRT\nBOS-NRT\nLess than 200 miles\nLess than 200 miles\n3,000 to 3,500 miles\n3,000 to 3,500 miles\nMore than 6,000 miles\nMore than 6,000 miles\nDomestic\nDomestic\nInternational\nInternational\nInternational\nInternational\nNon-Oceanic\nNon-Oceanic\nTransatlantic\nTransatlantic\nTranspacific\nTranspacific\nNon-Directional\nRoute Name\nRoute Distance\nin Miles\nRoute Distance Band\nDom-Intl Ind\nTransocean Ind\nFigure 12-7: City-pair route dimension sample rows.\nSometimes designers suggest using a bridge table containing the origin and \ndestination airport keys to capture the route information. Although the origin \nand destination represent a many-to-many relationship, in this case, you can \ncleanly represent the relationship within the existing fact table rather than \nusing a bridge.\n More Date and Time Considerations\nFrom the earliest chapters in this book we’ve discussed the importance of having a \nverbose date dimension, whether at the individual day, week, or month granular-\nity, that contains descriptive attributes about the date and private labels for ﬁ scal \nperiods and work holidays. In this ﬁ nal section, we’ll introduce several additional \nconsiderations for dealing with date and time dimensions.\n Country-Speciﬁ c Calendars as Outriggers\nIf  the DW/BI system serves multinational needs, you must generalize the standard \ndate dimension to handle multinational calendars in an open-ended number of coun-\ntries. The primary date dimension contains generic calendar attributes about the date, \n\n\nChapter 12\n322\nregardless of the country. If your multinational business spans Gregorian, Hebrew, \nIslamic, and Chinese calendars, you would include four sets of days, months, and \nyears in this primary dimension.\nCountry-speciﬁ c date dimensions supplement the primary date table. The key to \nthe supplemental dimension is the primary date key, along with the country code. \nThe table would include country-speciﬁ c date attributes, such as holiday or season \nnames, as illustrated in Figure 12-8. This approach is similar to the handling of \nmultiple ﬁ scal accounting calendars, as described in Chapter 7: Accounting.\nDate Dimension\nDate Key (FK)\nMore FKs ...\nFacts ...\nDate Key (PK)\nDate\nDay of Week\nDay Number in Epoch\nWeek Number in Epoch\nMonth Number in Epoch\nDay Number in Calendar Month\nDay Number in Calendar Year\nDay Number in Fiscal Month\nLast Day in Fiscal Month Indicator\nCalendar Month\nCalendar Month Number in Year\nCalendar Year-Month (YYYY-MM)\nCalendar Quarter\nCalendar Year-Quarter\nCalendar Year\nFiscal Month\nFiscal Month Number in Year\nFiscal Year-Month\nFiscal Quarter\nFiscal Year-Quarter\nFiscal Year\n...\nDate Key (FK)\nCountry Key (FK)\nCountry Name\nCivil Name\nCivil Holiday Flag\nCivil Holiday Name\nReligious Holiday Flag\nReligious Holiday Name\nWeekday Indicator\nSeason Name\nCountry-Specific Date Outrigger\nFact\nFigure 12-8: Country-speciﬁ c calendar outrigger.\nYou can join this table to the main calendar dimension as an outrigger or directly \nto the fact table. If you provide an interface that requires the user to specify a coun-\ntry name, then the attributes of the country-speciﬁ c supplement can be viewed as \nlogically appended to the primary date table, allowing them to view the calendar \nthrough the eyes of a single country at a time. Country-speciﬁ c calendars can be \n\n\nTransportation 323\nmessy to build in their own right; things get even more complicated if you need to \ndeal with local holidays that occur on diff erent days in diff erent parts of a country.\n Date and Time in Multiple Time Zones\nWhen  operating in multiple countries or even just multiple time zones, you’re faced \nwith a quandary concerning transaction dates and times. Do  you capture the date and \ntime relative to local midnight in each time zone, or do you express the time period \nrelative to a standard, such as the corporate headquarters date/time, Greenwich Mean \nTime (GMT), or Coordinated Universal Time (UTC), also known as Zulu time in the \naviation world? To fully satisfy users’ requirements, the correct answer is probably \nboth. The standard time enables you to see the simultaneous nature of transactions \nacross the business, whereas the local time enables you to understand transaction \ntiming relative to the time of day.\nContrary  to popular belief, there are more than 24 time zones (corresponding \nto the 24 hours of the day) in the world. For example, there is a single time zone in \nChina despite its latitudinal span. Likewise, there is a single time zone in India, off -\nset from UTC by 5.5 hours. In Australia, there are three time zones with its Central \ntime zone off set by one-half hour. Meanwhile, Nepal and some other nations use \none-quarter hour off set. The situation gets even more unpleasant when you account \nfor switches to and from daylight saving time. \nGiven the complexities, it’s unreasonable to think that merely providing a UTC \noff set in a fact table can support equivalized dates and times. Likewise, the off set \ncan’t reside in a time or airport dimension table because the off set depends on both \nlocation and date. The recommended approach for expressing dates and times in \nmultiple time zones is to include separate date and time-of-day dimensions corre-\nsponding to the local and equivalized dates, as shown in Figure 12-9. The time-of-day \ndimensions, as discussed in Chapter 3: Retail Sales, support time period groupings \nsuch as shift numbers or rush period time block  designations.\nFlight Activity Fact\nDeparture Date Key (FK)\nGMT Departure Date Key (FK)\nDeparture Time-of-Day Key (FK)\nGMT Departure Time-of-Day Key (FK)\nMore FKs ...\nDegenerate Dimensions ...\nFacts ...\nDate Dimension\n(2 views for roles)\nTime-of-Day Dimension\n(2 views for roles)\nFigure 12-9: Local and equivalized date/time across time zones.\n\n\nChapter 12\n324\nLocalization Recap\nWe  have discussed the challenges of international DW/BI system in several chapters \nof the book. In addition to the international time zones and calendars discussed in the \nprevious two sections, we have also talked about multi-currency reporting in Chapter \n6 and multi-language support in Chapter 8: Customer Relationship Management.\nAll these database-centric techniques fall under the general theme of localiza-\ntion. Localization in the larger sense also includes the translation of user interface \ntext embedded in BI tools. BI tool vendors implement this form of localization with text \ndatabases containing all the text prompts and labels needed by the tool, which can \nthen be conﬁ gured for each local environment. Of course, this can become quite \ncomplicated because text translated from English to most European languages results \nin text strings that are longer than their English equivalents, which may force a \nredesign of the BI application. Also, Arabic text reads from right to left, and many \nAsian languages are completely diff erent.\nA serious international DW/BI system built to serve business users in many \ncountries needs to be thoughtfully designed to account for a selected set of these \nlocalization issues. But perhaps it is worth thinking about how airport control tow-\ners and airplane pilots around the world deal with language incompatibilities when \ncommunicating critical messages about ﬂ ight directions and altitudes. They all use \none language (English) and unit of measure (feet).\nSummary\nIn this chapter we turned our attention to airline trips or routes; we brieﬂ y touched \non similar scenarios drawn from the shipping and travel services industries. We \nexamined the situation in which we have multiple fact tables at multiple granularities \nwith multiple grain-speciﬁ c facts. We also discussed the possibility of combining \ndimensions into a single dimension table for cases in which the row count volumes \nare extremely small or when there are additional attributes that depend on the com-\nbined dimensions. Again, combining correlated dimensions should be viewed as the \nexception rather than the rule.\nWe wrapped up this chapter by discussing several date and time dimension tech-\nniques, including country-speciﬁ c calendar outriggers and  the handling of absolute \nand relative dates and times.\n\n\nEducation\nW\ne step into the world of an educational institution in this chapter, looking first \nat the applicant pipeline as an accumulating snapshot. When accumulating \nsnapshot fact tables were introduced in Chapter 4: Inventory, a product movement \npipeline illustrated the concept; order fulfillment workflows were captured in an \naccumulating snapshot in Chapter 6: Order Management. In this chapter, rather than \nwatching products or orders move through various states, an accumulating snapshot \nis used to monitor prospective student applicants as they progress through admis-\nsions milestones.\nThe other primary concept discussed in this chapter is the factless fact table. We’ll \nexplore several case study illustrations drawn from higher education to further elabo-\nrate on these special fact tables and discuss the analysis of events that didn’t occur.\nChapter 13 discusses the following concepts:\n \n■Example bus matrix snippet for a university or college\n \n■Applicant tracking and research grant proposals as accumulating snapshot \nfact tables\n \n■Factless fact table for admission events, course registration facilities manage-\nment, and student attendance\n \n■Handling of nonexistent events\n University Case Study and Bus Matrix\nIn  this chapter you’re working for a university, college, or other type of educational \ninstitution. Someone at a higher education client once remarked that running a \nuniversity is akin to operating all the businesses needed to support a small vil-\nlage. Universities are simultaneously a real estate property management company \n(residential student housing), restaurant with multiple outlets (dining halls), retailer \n(bookstore), events management and ticketing agency (athletics and speaker events), \n13\n",
      "page_number": 343
    },
    {
      "number": 38,
      "title": "Segment 38 (pages 352-359)",
      "start_page": 352,
      "end_page": 359,
      "detection_method": "topic_boundary",
      "content": "Chapter 13\n326\npolice department (campus security), professional fundraiser (alumni development), \nconsumer ﬁ nancial services company (ﬁ nancial aid), investment ﬁ rm (endowment \nmanagement), venture capitalist (research and development), job placement ﬁ rm \n(career planning), construction company (buildings and facilities maintenance), and \nmedical services provider (health clinic). In addition to these varied functions, higher \neducation institutions are obviously also focused on attracting high caliber students \nand talented faculty to create a robust educational environment.\nThe bus matrix snippet in Figure 13-1 covers several core processes within an \neducational institution. Traditionally, there has been less focus on revenue and proﬁ t \nin higher education, but with ever-escalating costs and competition, universities \nand colleges cannot ignore these ﬁ nancial metrics. They want to attract and retain \nstudents who align with their academic and other institutional objectives. There’s \na strong interest in analyzing what students are “buying” in terms of courses each \nterm and the associated academic outcomes. Colleges and universities want to \nunderstand many aspects of the student’s experience, along with maintaining an \nongoing relationship well beyond graduation.\n Accumulating Snapshot Fact Tables\n Chapter  4 used an accumulating snapshot fact table to track products identiﬁ ed by \nserial or lot numbers as they move through various inventory stages in a warehouse. \nTake a moment to recall the distinguishing characteristics of an accumulating snap-\nshot fact table:\n \n■A single row represents the complete history of a workflow or pipeline \ninstance.\n \n■Multiple dates represent the standard pipeline milestone events.\n \n■The accumulating snapshot facts often included metrics corresponding to \neach milestone, plus status counts and elapsed durations.\n \n■Each row is revisited and updated whenever the pipeline instance changes; \nboth foreign keys and measured facts may be changed during the fact row \n updates.\nApplicant Pipeline\nNow  envision these same accumulating snapshot characteristics as applied to the \nprospective student admissions pipeline. For those who work in other industries, \nthere are obvious similarities to tracking job applicants through the hiring process \nor sales prospects as they are qualiﬁ ed and become customers.\n\n\nEducation 327\nAdmission Events\nStudent Lifecycle Processes\nFinancial Processes\nApplicant Pipeline\nFinancial Aid Awards\nStudent Enrollment/Profile Snapshot\nStudent Residential Housing\nStudent Course Registration & Outcomes\nStudent Course Instructor Evaluations\nStudent Activities\nCareer Placement Activities\nAdvancement Contacts\nAdvancement Pledges & Gifts\nBudgeting\nEndowment Tracking\nGL Transactions\nPayroll\nProcurement\nEmployee Management Processes\nEmployee Headcount Snapshot \nEmployee Hiring & Separations\nEmployee Benefits & Compensation\nAdministrative Processes\nFacilities Utilization\nEnergy Consumption & Waste Management\nWork Orders\nStaff Performance Management\nFaculty Appointment Management\nResearch Proposal Pipeline\nResearch Expenditures\nFaculty Publications\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nDate/Term\nApplicant-Student-Alum\nEmployee (Faculty, Staff)\nFacility\nAccount\nCourse\nDepartment\nFigure 13-1: Subset of bus matrix rows for educational institution.\n\n\nChapter 13\n328\nIn the case of applicant tracking, prospective students progress through a stan-\ndard set of admissions hurdles or milestones. Perhaps you’re interested in tracking \nactivities around key dates, such as initial inquiry, campus visit, application submit-\nted, application ﬁ le completed, admissions decision notiﬁ cation, and enrolled or \nwithdrawn. At any point in time, admissions and enrollment management analysts \nare interested in how many applicants are at each stage in the pipeline. The process \nis much like a funnel, where many inquiries enter the pipeline, but far less prog-\nress through to the ﬁ nal stage. Admission personnel also would like to analyze the \napplicant pool by a variety of characteristics.\nThe grain of the applicant pipeline accumulating snapshot is one row per prospec-\ntive student; this granularity represents the lowest level of detail captured when the \nprospect enters the pipeline. As more information is collected while the prospective \nstudent progresses toward application, acceptance, and enrollment, you continue \nto revisit and update the fact table row, as illustrated in Figure 13-2.\nApplicant Dimension\nApplicant Key (PK)\nApplicant Name\nApplicant Address Attributes ...\nHigh School\nHigh School GPA\nHigh School Type\nSAT Math Score\nSAT Verbal Score\nSAT Writing Score\nACT Composite Score\nNumber of AP Credits\nGender\nDate of Birth\nEthnicity\nFull time-Part time Indicator\nApplication Source\nIntended Major\n...\nDate Key (PK)\n...\nTerm\nAcademic Year-Term\nAcademic Year\nDate Dimension (views for 6 roles)\nApplication Status Key\nApplication Status Code\nApplication Status Description\nApplication Status Category\nApplication Status Dimension\nApplicant Pipeline Fact\nInitial Inquiry Date Key (FK)\nCampus Visit Date Key (FK)\nApplication Submitted Date Key (FK)\nApplication File Completed Date Key (FK)\nAdmission Decision Notification Date Key (FK)\nApplicant Enroll-Withdraw Date Key (FK)\nApplicant Key (FK)\nApplication Status Key (FK)\nApplication ID (DD)\nInquiry Count\nCampus Visit Count\nApplication Submitted Count\nApplication Completed Count\nAdmit Early Decision Count\nAdmit Regular Decision Count\nWaitlist Count\nDefer to Regular Decision Count\nDeny Count\nEnroll Early Decision Count\nEnroll Regular Decision Count\nAdmit Withdraw Count\nFigure 13-2: Student applicant pipeline as an accumulating snapshot.\nLike earlier accumulating snapshots, there are multiple dates in the fact table \ncorresponding to the standard milestone events. You want to analyze the prospect’s \nprogress by these dates to determine the pace of movement through the pipeline and \nspot bottlenecks. This is especially important if you see a signiﬁ cant lag involving \na candidate whom you’re interested in recruiting. Each of these dates is treated as a \nrole-playing dimension, with a default surrogate key to handle the unknown dates \nfor new and in-process rows.\n\n\nEducation 329\nThe applicant dimension contains many interesting attributes about prospective \nstudents. Analysts are interested in slicing and dicing by applicant characteristics \nsuch as geography, incoming credentials (grade point average, college admissions test \nscores, advanced placement credits, and high school), gender, date of birth, ethnicity, \npreliminary major, application source, and a multitude of others. Analyzing these char-\nacteristics at various stages of the pipeline can help admissions personnel adjust their \nstrategies to encourage more (or fewer) students to proceed to the next mile marker.\nThe facts in the applicant pipeline fact table include a variety of counts that are \nclosely monitored by admissions personnel. If available, this table could include esti-\nmated probabilities that the prospect will apply and subsequently enroll if accepted \nto predict admission yields.\nAlternative Applicant Pipeline Schemas\nAccumulating snapshots are appropriate for short-lived processes that have a deﬁ ned \nbeginning and end, with standard intermediate milestones. This type of fact table \nenables you to see an updated status and ultimately ﬁ nal disposition of each appli-\ncant. However, because accumulating snapshot rows are updated, they do not pre-\nserve applicant counts and statuses at critical points in the admissions calendar, \nsuch as the early decision notiﬁ cation date. Given the close scrutiny of these num-\nbers, analysts might also want to retain snapshots at several important cut-off   dates. \nAlternatively, you could build an admission transaction fact table with one row per \ntransaction per applicant for counting and period-to-period comparisons.\nResearch Grant Proposal Pipeline\nThe  research proposal pipeline is another education-based example of an accumu-\nlating snapshot. Faculty and administration are interested in viewing the lifecycle \nof a grant proposal as it progresses through the pipeline from preliminary proposal \nto grant approval and award receipt. This would support analysis of the number of \noutstanding proposals in each stage of the pipeline by faculty, department, research \ntopic area, or research funding source. Likewise, you could see success rates by \nvarious attributes. Having this information in a common repository would allow \nit to be leveraged by a broader university population.\n Factless Fact Tables\nSo far we’ve largely designed fact tables with very similar structures. Each fact table \ntypically has 5 to approximately 20 foreign key columns, followed by one to poten-\ntially several dozen numeric, continuously valued, preferably additive facts. The \nfacts can be regarded as measurements taken at the intersection of the dimension \n\n\nChapter 13\n330\nkey values. From this perspective, the facts are the justiﬁ cation for the fact table, \nand the key values are simply administrative structure to identify the facts.\nThere are, however, a number of business processes whose fact tables are simi-\nlar to those we’ve been designing with one major distinction. There are no mea-\nsured facts! We introduced factless fact tables while discussing promotion events \nin Chapter 3: Retail Sales, as well as in Chapter 6 to describe sales rep/customer \nassignments. There are numerous examples of factless events in higher education.\nAdmissions Events\n You  can envision a factless fact table to track each prospective student’s attendance \nat an admission event, such as a high school visit, college fair, alumni interview or \ncampus overnight, as illustrated in Figure 13-3.\nAdmissions Event Date Key (FK)\nPlanned Enroll Term Key (FK)\nApplicant Key (FK)\nApplicant Status Key (FK)\nAdmissions Officer Key (FK)\nAdmission Event Key (FK)\nAdmissions Event Attendance Count (=1)\nAdmissions Event Attendance Fact\nPlanned Enroll Term Dimension\nApplication Status Dimension\nAdmissions Event Date Dimension\nApplicant Dimension\nAdmissions Officer Dimension\nAdmission Event Dimension\nFigure 13-3: Admission event attendance as a factless fact table.\nCourse Registrations\nSimilarly,  you can track student course registrations by term using a factless fact \ntable. The grain would be one row for each registered course by student and term, \nas illustrated in Figure 13-4.\nTerm Dimension\nIn this  fact table, the data is at the term level rather than at the more typical cal-\nendar day, week, or month granularity. The term dimension still should conform \nto the calendar date dimension. In other words, each date in the daily calendar \ndimension should identify the term (for example, Fall), term and academic year \n(for example, Fall 2013), and academic year (for example, 2013-2014). The column \nlabels and values must be identical for the attributes common to both the calendar \ndate and term dimensions.\nStudent Dimension and Change Tracking\n The  student dimension is an expanded version of the applicant dimension discussed \nin the ﬁ rst scenario. You still want to retain some information garnered from the \napplication process (for example, geography, credentials, and intended major) but \n\n\nEducation 331\nsupplement it with on-campus information, such as part-time or full-time status, \nresidence, athletic involvement indicator, declared major, and class level status (for \nexample, sophomore).\nInstructor Key (PK)\nInstructor Employee ID (NK)\nInstructor Name\nInstructor Address Attributes...\nInstructor Type\nInstructor Tenure Indicator\nInstructor Original Hire Date\nInstructor Years of Service\nStudent Dimension\nStudent Key (PK)\nStudent ID (NK)\n...\nTerm Key (PK)\nTerm\nAcademic Year-Term\nAcademic Year\nTerm Dimension\nTerm Key (FK)\nStudent Key (FK)\nCourse Key (FK)\nInstructor Key (FK)\nCourse Registration Count (=1)\nCourse Key (PK)\nCourse Name\nCourse Department\nCourse Format\nCourse Credit Hours\nCourse Dimension\nCourse Registration Event Fact\nInstructor Dimension\nFigure 13-4: Course registration events as a factless fact table.\nAs discussed in Chapter 5: Procurement, you could imagine placing some of \nthese attributes in a type 4 mini-dimension because factions throughout the uni-\nversity are interested in tracking changes to them, especially for declared major, \nclass level, and graduation attainment. People in administration and academia are \nkeenly interested in academic progress and retention rates by class, school, depart-\nment, and major. Alternatively, if there’s a strong demand to preserve the students’ \nproﬁ les at the time of course registration, plus ﬁ lter and group by the students’ \ncurrent characteristics, you should consider handling the student information as \na slowly changing dimension type 7 with dual student dimension keys in the fact \ntable, as also described in Chapter 5. The surrogate student key would link to a \ndimension table with type 2 attributes; the student’s durable identiﬁ er would link \nto a view of the complete student dimension containing only the current row for \neach student.\nArtiﬁ cial Count Metric\nA   fact table represents the robust set of many-to-many relationships among dimen-\nsions; it records the collision of dimensions at a point in time and space. This course \nregistration fact table could be queried to answer a number of interesting questions \nregarding registration for the college’s academic off erings, such as which students \nregistered for which courses? How many declared engineering majors are taking an \nout-of-major ﬁ nance course? How many students have registered for a given faculty \nmember’s courses during the last three years? How many students have registered \n\n\nChapter 13\n332\nfor more than one course from a given faculty member? The only peculiarity in \nthese examples is that you don’t have a numeric fact tied to this registration data. \nAs such, analyses of this data will be based largely on counts.\nNOTE \nEvents are modeled as fact tables containing a series of keys, each \nrepresenting a participating dimension in the event. Event tables sometimes have \nno variable measurement facts associated with them and hence are called factless \nfact tables.\nThe SQL for performing counts in this factless fact is asymmetric because of \nthe absence of any facts. When counting the number of registrations for a faculty \nmember, any key can be used as the argument to the COUNT function. For example:\nselect faculty, count(term_key)... group by faculty\nThis gives the simple count of the number of student registrations by faculty, \nsubject to any constraints that may exist in the WHERE clause. An oddity of SQL is \nthat you can count any key and still get the same answer because you are counting \nthe number of keys that ﬂ y by the query, not their distinct values. You would need \nto use a COUNT DISTINCT if you want to count the unique instances of a key rather \nthan the number of keys encountered.\nThe  inevitable confusion surrounding the SQL statement, although not a serious \nsemantic problem, causes some designers to create an artiﬁ cial implied fact, perhaps \ncalled course registration count (as opposed to “dummy”), that is always populated \nby the value 1. Although this fact does not add any information to the fact table, it \nmakes the SQL more readable, such as:\nselect faculty, sum(registration_count)... group by faculty\nAt this point the table is no longer strictly factless, but the “1” is nothing more \nthan an artifact. The SQL will be a bit cleaner and more expressive with the regis-\ntration count. Some BI query tools have an easier time constructing this query with \na few simple user gestures. More important, if you build a summarized aggregate \ntable above this fact table, you need a real column to roll up to meaningful aggre-\ngate registration counts. And ﬁ nally, if deploying to an OLAP cube, you typically \ninclude an explicit count column (always equal to 1) for complex counts because \nthe dimension join keys are not explicitly revealed in a cube.\nIf a measurable fact does surface during the design, it can be added to the schema, \nassuming it is consistent with the grain of student course registrations by term. For \nexample, you could add tuition revenue, earned credit hours, and grade scores to \nthis fact table, but then it’s no longer a factless fact  table.\n\n\nEducation 333\n Multiple Course Instructors\nIf  courses are taught by a single instructor, you can associate an instructor key to \nthe course registration events, as shown in Figure 13-4. However, if some courses \nare co-taught, then it is a dimension attribute that takes on multiple values for the \nfact table’s declared grain. You have several options:\n \n■Alter the grain of the fact table to be one row per instructor per course reg-\nistration per student per term. Although this would address the multiple \ninstructors associated with a course, it’s an unnatural granularity that would \nbe extremely prone to overstated registration count errors.\n \n■Add a bridge table with an instructor group key in either the fact table or as \nan outrigger on the course dimension, as introduced in Chapter 8: Customer \nRelationship Management. There would be one row in this table for each instruc-\ntor who teaches courses on his own. In addition, there would be two rows for \neach instructor team; these rows would associate the same group key with \nindividual instructor keys. The concatenation of the group key and instructor \nkey would uniquely identify each bridge table row. As described in Chapter 10: \nFinancial Services, you could assign a weighting factor to each row in the bridge \nif the teaching workload allocation is clearly deﬁ ned. This approach would \nbe susceptible to the potential overstatement issues surrounding the bridge \ntable usage described in Chapter 10.\n \n■Concatenate  the instructor names into a single, delimited attribute on the \ncourse dimension, as discussed in Chapter 9: Human Resources Management. \nThis option enables users to easily label reports with a single dimension attri-\nbute, but it would not support analysis of registration events by instructor \ncharacteristics.\n \n■If one of the instructors is identiﬁ ed as the primary instructor, then her \ninstructor key could be handled as a single foreign key in the fact table, \njoined to a dimension where the attributes were prefaced with “primary” for \ndiff erentiation.\n Course Registration Periodic Snapshots\nThe  grain of the fact table illustrated in Figure 13-4 is one row for each regis-\ntered course by student and term. Some users at the college or university might be \ninterested in periodic snapshots of the course registration events at key academic \ncalendar dates, such as preregistration, start of the term, course drop/add deadline, \nand end of the term. In this case, the fact table’s grain would be one row for each \nstudent’s registered courses for a term per snapshot date.\n",
      "page_number": 352
    },
    {
      "number": 39,
      "title": "Segment 39 (pages 360-367)",
      "start_page": 360,
      "end_page": 367,
      "detection_method": "topic_boundary",
      "content": "Chapter 13\n334\nFacility Utilization\nThe second type of factless fact table deals with coverage, which can be illustrated \nwith a facilities management scenario. Universities invest a tremendous amount \nof capital in their physical plant and facilities. It would be helpful to understand \nwhich facilities were being used for what purpose during every hour of the day \nduring each term. For example, which facilities were used most heavily? What \nwas the average occupancy rate of the facilities as a function of time of day? \nDoes utilization drop off  signiﬁ cantly on Fridays when no one wants to attend \n(or teach) classes?\nAgain, the factless fact table comes to the rescue. In this case you’d insert one \nrow in the fact table for each facility for standard hourly time blocks during each \nday of the week during a term regardless of whether the facility is being used. \nFigure 13-5 illustrates the schema.\nTime-of-Day Hour Dimension\nTime-of-Day Hour Key (PK)\nTime-of-Day Hour\nDay Part Indicator\nTerm Dimension\nDay of Week Dimension\nTerm Key (FK)\nDay of Week Key (FK)\nTime-of-Day Hour Key (FK)\nFacility Key (FK)\nOwner Department Key (FK)\nAssigned Department Key (FK)\nUtilization Status Key (FK)\nFacility Count (=1)\nFacility Key (PK)\nFacility Building Name - Room\nFacility Building Name\nFacility Building Address attributes...\nFacility Type\nFacility Floor\nFacility Square Footage\nFacility Capacity\nProjector Indicator\nVent Indicator\n...\nFacility Dimension\nFacility Utilization Fact\nDepartment Dimension (2 views for roles)\nUtilization Status Dimension\nFigure 13-5: Facilities utilization as a coverage factless fact table.\nThe facility dimension would include all types of descriptive attributes about the \nfacility, such as the building, facility type (for example, classroom, lab, or offi  ce), \nsquare footage, capacity, and amenities (for example, whiteboard or built-in projec-\ntor). The utilization status dimension would include a text descriptor with values \nof Available or Utilized. Meanwhile, multiple organizations may be involved in \nfacilities utilization. For example, one organization might own the facility during \na time block, but the same or a diff erent organization might be assigned as the \nfacility  user.\n\n\nEducation 335\nStudent Attendance\nYou  can visualize a similar schema to track student attendance in a course. In this \ncase, the grain would be one row for each student who walks through the course’s \nclassroom door each day. This factless fact table would share a number of the same \ndimensions discussed with registration events. The primary diff erence would be \nthe granularity is by calendar date in this schema rather than merely term. This \ndimensional model, illustrated in Figure 13-6, allows business users to answer \nquestions concerning which courses were the most heavily attended. Which courses \nsuff ered the least attendance attrition over the term? Which students attended which \ncourses? Which faculty member taught the most students?\nDate Key (FK)\nStudent Key (FK)\nCourse Key (FK)\nInstructor Key (FK)\nFacility Key (FK)\nAttendance Count\nStudent Attendance Fact\nStudent Dimension\nInstructor Dimension\nDate Dimension\nCourse Dimension\nFacility Dimension\nFigure 13-6: Student attendance fact table.\nExplicit Rows for What Didn’t Happen\nPerhaps people are interested in monitoring students who were registered for a \ncourse but didn’t show up. In this example you can envision adding explicit rows to \nthe fact table for attendance events that didn’t occur. The fact table would no longer \nbe factless as there is an attendance metric equal to either 1 or 0.\nAdding rows is viable in this scenario because the non-attendance events have the \nsame exact dimensionality as the attendance events. Likewise, the fact table won’t \ngrow at an alarming rate, presuming (or perhaps hoping) the no-shows are a small \npercentage of the total students registered for a course. Although this approach is \nreasonable in this scenario, creating rows for events that didn’t happen is ridiculous \nin many other situations, such as adding rows to a customer’s sales transaction for \npromoted products that weren’t purchased by the customer.\n What Didn’t Happen with Multidimensional OLAP\nMultidimensional  OLAP databases do an excellent job of helping users understand \nwhat didn’t happen. When the cube is constructed, multidimensional databases \nhandle the sparsity of the transaction data while minimizing the overhead burden \nof storing explicit zeroes. As such, at least for fact cubes that are not too sparse, the \n\n\nChapter 13\n336\nevent and nonevent data is available for user analysis while reducing some of the \ncomplexities just discussed in the relational star schema world.\nMore Educational Analytic Opportunities\nMany of the business processes described in earlier chapters, such as procurement \nand human resources, are obviously applicable to the university environment given \nthe desire to better monitor and manage costs. Research grants and alumni contri-\nbutions are key sources of revenue, in addition to the tuition revenue.\nResearch grant analysis is often a variation of ﬁ nancial analysis, as discussed in \nChapter 7: Accounting, but at a lower level of detail, much like a subledger. The grain \nwould include additional dimensions to further describe the research grant, such as \nthe corporate or governmental funding source, research topic, grant duration, and \nfaculty investigator. There is a strong need to better understand and manage the \nbudgeted and actual spending associated with each research project. The objective \nis to optimize the spending so a surplus or deﬁ cit situation is avoided, and funds \nare deployed where they will be most productive. Likewise, understanding research \nspending rolled up by various dimensions is necessary to ensure proper institutional \ncontrol of such monies.\nBetter understanding the university’s alumni is much like better understanding \na customer base, as described in Chapter 8. Obviously, there are many interesting \ncharacteristics that would be helpful in maintaining a relationship with your alumni, \nsuch as geographic, demographic, employment, interests, and behavioral information, \nin addition to the data you collected about them as students (for example, affi  liations, \nresidential housing, school, major, length of time to graduate, and honors designa-\ntions). Improved access to a broad range of attributes about the alumni population \nwould allow the institution to better target messages and allocate resources. In addi-\ntion to alumni contributions, alumni relationships can be leveraged for potential \nrecruiting, job placement, and research opportunities. To this end, a robust CRM \noperational system should track all the touch points with alumni to capture mean-\ningful data for the DW/BI analytic  platform.\nSummary\nIn this chapter we focused on two primary concepts. First, we looked at the accu-\nmulating snapshot fact table to track application or research grant pipelines. Even \nthough the accumulating snapshot is used much less frequently than the more com-\nmon transaction and periodic snapshot fact tables, it is very useful for tracking the \ncurrent status of a short-lived process with standard milestones. As we described, \n\n\nEducation 337\naccumulating snapshots are often complemented with transactional or periodic \nsnapshot tables.\nSecond, we explored several examples of factless fact tables. These fact tables \ncapture the relationship between dimensions in the case of an event or coverage, \nbut are unique in that no measurements are collected to serve as actual facts. We \nalso discussed the handling of situation s in which you want to track events that \ndidn’t occur.\n\n\nHealthcare\nT\nhe healthcare industry is undergoing tremendous change as it seeks to both \nimprove patient outcomes, while simultaneously improving operational effi-\nciencies. The challenges are plentiful as organizations attempt to integrate their \nclinical and administrative information. Healthcare data presents several interesting \ndimensional design patterns that we’ll explore in this chapter.\nChapter 14 discusses the following concepts:\n \n■Example bus matrix snippet for a healthcare organization\n \n■Accumulating snapshot fact table to handle the claims billing and payment \npipeline\n \n■Dimension role playing for multiple dates and physicians\n \n■Multivalued dimensions, such as patient diagnoses \n \n■Supertype and subtype handling of healthcare charges\n \n■Treatment of textual comments\n \n■Measurement type dimension for sparse, heterogeneous measurements\n \n■Handling of images with dimensional schemas\n \n■Facility/equipment inventory utilization as transactions and periodic snapshots\n Healthcare Case Study and Bus Matrix\nIn  the face of unprecedented consumer focus and governmental policy regulations, \ncoupled with internal pressures, healthcare organizations need to leverage informa-\ntion more eff ectively to impact both patient outcomes and operational effi  ciencies. \nHealthcare organizations typically wrestle with many disparate systems to collect \ntheir clinical, ﬁ nancial, and operational performance metrics. This information \nneeds to be better integrated to deliver more eff ective patient care, while concur-\nrently managing costs and risks. Healthcare analysts want to better understand \nwhich procedures deliver the best outcomes, while identifying opportunities to \n14\n\n\nChapter 14\n340\nimpact resource utilization, including labor, facilities, and associated equipment \nand supplies. Large healthcare consortiums with networks of physicians, clinics, \nhospitals, pharmacies, and laboratories are focused on these requirements, espe-\ncially as both the federal government and private payers are encouraging providers \nto assume more responsibility for the quality and cost of their healthcare services. \nFigure 14-1 illustrates a sample snippet of a healthcare organization’s bus matrix.\nPatient Encounter Workflow\nClinical Events\nBilling/Revenue Events\nProcedures\nPhysician Orders\nMedications\nLab Test Results\nDisease/Case Management Participation\nPatient Reported Outcomes\nPatient Satisfaction Surveys\nInpatient Facility Charges \nOutpatient Professional Charges\nClaims Billing\nClaims Payments\nCollections and Write-Offs\nOperational Events\nBed Inventory Utilization\nFacilities Utilization\nSupply Procurement\nSupply Utilization\nWorkforce Scheduling\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nDate\nPatient\nPhysician\nDiagnosis\nPayer\nEmployee\nFacility\nProcedure\nFigure 14-1: Subset of bus matrix row for a healthcare consortium.\nTraditionally, healthcare insurance payers have leveraged claims information to \nbetter understand their risk, improve underwriting policies, and detect potential \nfraudulent activity. Payers have historically been more sophisticated than health-\ncare provider organizations in leveraging data analytically, perhaps in part because \ntheir prime data source, claims, was more reliably captured and structured than \n\n\nHealthcare 341\nproviders’ data. However, claims data is both a beneﬁ t and curse for payers’ analytic \neff orts because it historically hasn’t provided the robust, granular clinical picture. \nIncreasingly, healthcare payers are partnering with providers to leverage detailed \npatient information to support more predictive analysis. In many ways, the needs \nand objectives of the providers and payers are converging, especially with the push \nfor shared-risk delivery models.\nEvery patient’s episode of care with a healthcare organization generates mounds \nof information. Patient-centric transactional data falls into two prime categories: \nadministrative and clinical. The claims billing data provides detail on a patient bill \nfrom a physician’s offi  ce, clinic, hospital, or laboratory. The clinical medical record, \non the other hand, is more comprehensive and includes not only the services result-\ning in charges, but also the laboratory test results, prescriptions, physician’s notes \nor orders, and sometimes outcomes.\nThe  issues of conforming common dimensions remain exactly the same for \nhealthcare as in other industries. Obviously, the most important conformed dimen-\nsion is the patient. In Chapter 8: Customer Relationship Management, we described \nthe need for a 360-degree view of customers. It’s easy to argue that a 360-degree \nview of patients is even more critical given the stakes; adoption of patient electronic \nmedical record (EMR) and electronic health record (EHR) systems clearly focus on \nthis objective.\nOther dimensions that must be conformed include:\n \n■Date\n \n■Responsible party \n \n■Employer\n \n■Health plan\n \n■Payer (primary and secondary)\n \n■Physician \n \n■Procedure\n \n■Equipment\n \n■Lab test\n \n■Medication\n \n■Diagnosis\n \n■Facility (offi  ce, clinic, outpatient facility, and hospital)\nIn  the healthcare arena, some of these dimensions are hard to conform, whereas \nothers are easier than they look at ﬁ rst glance. The patient dimension has historically \nbeen challenging, at least in the United States, because of the lack of a reliable national \nidentity number and/or consistent patient identiﬁ er across facilities and physicians. \nTo further complicate matters, the Health Insurance Portability and Accountability Act \n(HIPAA) includes strict privacy and security requirements to protect the conﬁ dential \n\n\nChapter 14\n342\nnature of patient information. Operational process improvements, like electronic \nmedical records, are ensuring more consistent master patient identiﬁ cation.\nThe  diagnosis and treatment dimensions are considerably more structured and \npredictable than you might expect because the insurance industry and government \nhave mandated their content. For example, diagnosis and disease classiﬁ cations fol-\nlow the International Classiﬁ cation of Diseases (ICD) standard for consistent reporting. \nSimilarly, the Healthcare Common Procedure Coding System (HCPCS) is based on the \nAmerican Medical Association’s Current Procedural Terminology (CPT) to describe \nmedical, surgical, and diagnostic services, along with supplies and devices. Dentists \nuse the Current Dental Terminology (CDT) code set, which is updated and distributed \nby the American Dental Association.\nFinally, beyond integrated patient-centric clinical and ﬁ nancial information, \nhealthcare organizations also want to analyze operational information regarding \nthe utilization of their workforce, facilities, and supplies. Much of the discussion \nfrom earlier chapters about human resources, inventory management, and procure-\nment processes is also applicable to healthcare organizations.\n Claims Billing and Payments\nImagine  you work in the healthcare consortium’s billing organization. You receive \nthe primary charges from the physicians and facilities, prepare bills for the respon-\nsible payers, and track the progress of the claims payments received.\nThe dimensional model for the claims billing process must address a number of \nbusiness objectives. You want to analyze the billed dollar amounts by every avail-\nable dimension, including patient, physician, facility, diagnosis, procedure, and \ndate. You want to see how these claims have been paid and what percentage of the \nclaims have not been collected. You want to see how long it takes to get paid, and \nthe current status of all unpaid claims.\nAs we discussed in Chapter 4: Inventory, whenever a source business process is consid-\nered for inclusion in the DW/BI system, there are three essential grain choices. Remember \nthe fact table’s granularity determines what constitutes a fact table row. In other words, \nwhat is the measurement event being recorded?\nThe  transaction grain is the most fundamental. In the healthcare billing example, \nthe transaction grain would include every billing transaction from the physicians \nand facilities, as well as every claim payment transaction received. We’ll talk more \nabout these fact tables in a moment.\nThe  periodic snapshot is the grain of choice for long-running time series, such \nas bank accounts and insurance policies. However, the periodic snapshot doesn’t \n",
      "page_number": 360
    },
    {
      "number": 40,
      "title": "Segment 40 (pages 368-380)",
      "start_page": 368,
      "end_page": 380,
      "detection_method": "topic_boundary",
      "content": "Healthcare 343\ndo a good job of capturing the behavior of relatively short-lived processes, such as \norders or medical claims billing. \nThe  accumulating snapshot grain is chosen to analyze the claims billing and pay-\nment workﬂ ow. A single fact table row represents a single line on a medical claim. \nFurthermore, the row represents the accumulated history of the line item from the \nmoment of creation to the current state. When anything about the line changes, the row \nis revisited and modiﬁ ed appropriately. From the point of view of the billing organiza-\ntion, let’s assume the standard scenario of a claim includes:\n \n■Treatment date\n \n■Primary insurance billing date\n \n■Secondary insurance billing date\n \n■Responsible party billing date\n \n■Last primary insurance payment date\n \n■Last secondary insurance payment date\n \n■Last responsible party payment date\n \n■Zero balance date\nThese dates describe the normal claim workﬂ ow. An accumulating snapshot \ndoes not attempt to fully describe unusual situations. Business users undoubt-\nedly need to see all the details of messy claim payment scenarios because multiple \npayments are sometimes received for a single line, or conversely, a single payment \nsometimes applies to multiple claims. Companion transaction schemas inevitably \nwill be needed. In the meantime, the purpose of the accumulating snapshot grain \nis to place every claim into a standard framework so that the analytic objectives \ndescribed earlier can be satisﬁ ed easily.\nWith a clear understanding that an individual fact table row represents the accu-\nmulated history of a line item on a claim bill, you can identify the dimensions by \ncarefully listing everything known to be true in the context of this row. In this \nhypothetical scenario, you know the patient, responsible party, physician, physi-\ncian organization, procedure, facility, diagnosis, primary insurance organization, \nsecondary insurance organization, and master patient bill ID number, as shown in \nFigure 14-2.\nThe interesting facts accumulated over the claim line’s history include the billed \namount, primary insurance paid amount, secondary insurance paid amount, respon-\nsible party paid amount, total paid amount (calculated), amount sent to collections, \namount written off , amount remaining to be paid (calculated), length of stay, number \nof days from billing to initial primary insurance, secondary insurance, and respon-\nsible party payments, and ﬁ nally, number of days to zero balance.\n\n\nChapter 14\n344\nClaims Billing and Payment Workflow Fact\nTreatment Date Key (FK)\nPrimary Insurance Billing Date Key (FK)\nSecondary Insurance Billing Date Key (FK)\nResponsible Party Billing Date Key (FK)\nLast Primary Insurance Payment Date Key (FK)\nLast Secondary Insurance Payment Date Key (FK)\nLast Responsible Party Payment Date Key (FK)\nZero Balance Date Key (FK)\nPatient Key (FK)\nPhysician Key (FK)\nPhysician Organization Key (FK)\nProcedure Key (FK)\nFacility Key (FK)\nPrimary Diagnosis Key (FK)\nPrimary Insurance Organization Key (FK)\nSecondary Insurance Organization Key (FK)\nResponsible Party Key (FK)\nEmployer Key (FK)\nMaster Bill ID (DD)\nBilled Amount\nPrimary Insurance Paid Amount\nSecondary Insurance Paid Amount\nResponsible Party Paid Amount\nTotal Paid Amount\nSent to Collections Amount\nWritten Off Amount\nUnpaid Balance Amount\nLength of Stay\nBill to Initial Primary Insurance Payment Lag\nBill to Initial Secondary Insurance Payment Lag\nBill to Initial Responsible Party Payment Lag\nBill to Zero Balance Lag\nPatient Dimension\nProcedure Dimension\nPrimary Diagnosis Dimension\nResponsible Party Dimension\nPhysician Dimension\nPhysician Organization Dimension\nFacility Dimension\nInsurance Organization Dimension (views for 2 roles)\nEmployer Dimension\nDate Dimension (views for 8 roles)\nFigure 14-2: Accumulating snapshot fact table for medical claim billing and payment \nworkﬂ ow.\nA row is initially created in this fact table when the charge transactions are \nreceived from the physicians or facilities and the initial bills are generated. On a \ngiven bill, perhaps the primary insurance company is billed, but the secondary \ninsurance and responsible party are not billed, pending a response from the pri-\nmary insurance company. For a period of time after the row is ﬁ rst entered into \nthe fact table, the last seven dates are not applicable. Because the surrogate date \nkeys in the fact table must not be null, they will point to a date dimension row \nreserved for a To Be Determined date.\nIn the weeks after creation of the row, some payments are received. Bills are then \nsent to the secondary insurance company and responsible party. Each time these \nevents take place, the same fact table row is revisited, and the appropriate keys and \nfacts are destructively updated. This destructive updating poses some challenges \nfor the database administrator. If most of the accumulating rows stabilize and stop \nchanging within a given timeframe, a physical reorganization of the database at \nthat time can recover disk storage and improve performance. If the fact table is \n\n\nHealthcare 345\npartitioned on the treatment date key, the physical clustering or partitioning prob-\nably will be well preserved throughout these changes because the treatment date \nis not revisited and changed.\n Date Dimension Role Playing\nAccumulating snapshot fact tables always involve multiple date stamps, like the eight \nforeign keys pointing to the date dimension in Figure 14-2. The eight date foreign \nkeys should not join to a single instance of the date dimension table. Instead, create \neight views on the single underlying date dimension table, and join the fact table \nseparately to these eight views, as if they were eight independent date dimension \ntables. The eight view deﬁ nitions should cosmetically relabel the column names to \nbe distinguishable, so BI tools accessing the views present understandable column \nnames to the business users.\nAlthough the role-playing behavior of the date dimension is a common charac-\nteristic of accumulating snapshot fact tables, other dimensions in Figure 14-2 play \nroles in similar ways, such as the payer dimension. In the section “Supertypes and \nSubtypes for Charges,” the physician dimension will play multiple roles depending \non whether the physician is the referring physician, attending physician, or working \nin a consulting or assisting  capacity.\n Multivalued Diagnoses\nNormally  the dimensions surrounding a fact table take on a single value in the \ncontext of the fact event. However, there are situations where multivaluedness is \nnatural and unavoidable. The diagnosis dimension in healthcare fact tables is a \ngood example. At the moment of a procedure or lab test, the patient has one or more \ndiagnoses. Electronic medical record applications facilitate the physician’s selection \nof multiple diagnoses well beyond the historical practice of providing the minimal \ncoding needed for reimbursement; the result is a richer, more complete picture of \nthe severity of the patient’s medical condition. There is strong analytic incentive to \nretain the multivalued diagnoses, along with the other ﬁ nancial performance data, \nespecially as organizations do more comparative utilization and cost benchmarking.\nIf there were always a maximum of three diagnoses, for instance, you might be \ntempted to create three diagnosis foreign keys in the fact table with correspond-\ning dimensions, almost as if they were roles. However, diagnoses don’t behave like \nindependent roles. And unfortunately, there are often more than three diagnoses, \nespecially for hospitalized elderly patients who may present 20 simultaneous diag-\nnoses! Diagnoses don’t ﬁ t into well-deﬁ ned roles other than potentially the primary \nadmitting and discharging diagnoses. Finally, a design with multiple diagnosis \n\n\nChapter 14\n346\nforeign keys would make for very ineffi  cient BI applications because the query \ndoesn’t know which dimensional slot to constrain for a particular diagnosis.\nThe design shown in Figure 14-3 handles the open-ended nature of multiple diag-\nnoses. The diagnosis foreign key in the fact table is replaced with a diagnosis group \nkey. This diagnosis group key is connected by a many-to-many join to a diagnosis \ngroup bridge table, which contains a separate row for each individual diagnosis in \na particular group.\nDiagnosis Dimension\nDiagnosis Key (PK)\nDiagnosis Code (NK)\nDiagnosis Description\nDiagnosis Section Code\nDiagnosis Section Description\nDiagnosis Category Code\nDiagnosis Category Description\nMore FKs ...\nDiagnosis Group Key (FK)\nMaster Bill ID (DD)\nFacts ...\nDiagnosis Group Key (FK)\nDiagnosis Key (FK)\nClaim Billing Line Item Fact\nDiagnosis Group Bridge\nFigure 14-3: Bridge table to handle multivalued diagnoses.\nIf a patient has three diagnoses, he is assigned a diagnosis group with three cor-\nresponding rows in the bridge table. In Chapter 10: Financial Services, we described \nthe use of a weighting factor on each bridge table row to allocate the fact table’s \nmetrics accordingly. However, in the case of multiple patient diagnoses, it’s virtu-\nally impossible to weight their impact on a patient’s treatment or bill, beyond the \npotential determination of a primary diagnosis. Without a realistic way of assigning \nweighting factors, the analysis of diagnosis codes must largely focus on impact ques-\ntions like “What is the total billed amount for procedures involving the diagnosis of \ncongestive heart failure?” Most healthcare analysts understand impact analysis may \nresult in over counting as the same metrics are associated with multiple diagnoses.\nNOTE \nWeighting factors in multivalued bridge tables provide an elegant way \nto prorate numeric facts to produce correctly weighted reports. However, these \nweighting factors are by no means required in a dimensional design. If there is no \nagreement or enthusiasm within the business community for the weighting factors, \nthey should be left out. Also, in a schema with more than one multivalued dimen-\nsion, it is not worth trying to decide how multiple weighting factors would interact.\nIf the many-to-many join in Figure 14-3 causes problems for a modeling tool that \ninsists on proper foreign-key-to-primary-key relationships, the equivalent design \n\n\nHealthcare 347\nof Figure 14-4 can be used. In this case an extra table whose primary key is a diag-\nnosis group is inserted between the fact and bridge tables. There is likely no new \ninformation in this extra table, unless there were labels for a cluster of diagnoses, \nsuch as the Kimball Syndrome, but now both the fact table and bridge table have \nconventional many-to-one joins in all directions.\nDiagnosis Dimension\nDiagnosis Key (PK)\nDiagnosis Code (NK)\nDiagnosis Description\nDiagnosis Section Code\nDiagnosis Section Description\nDiagnosis Category Code\nDiagnosis Category Description\nForeign Keys ...\nDiagnosis Group Key (FK)\nMaster Bill ID (DD)\nFacts ...\nDiagnosis Group Key (FK)\nDiagnosis Key (FK)\nClaim Billing Line Item Fact\nDiagnosis Group Bridge\nDiagnosis Group Key (PK)\nDiagnosis Group Dimension\nFigure 14-4: Diagnosis group dimension to create a primary key relationship.\nIf a unique diagnosis group is created for every patient encounter, the number \nof rows could become astronomical and many of the groups would be identical. \nProbably a better approach is to have a portfolio of diagnosis groups that are repeat-\nedly used. Each set of diagnoses would be looked up in the master diagnosis group \ntable during the ETL. If the existing group is found, it is used; if not found, a new \ndiagnosis group is created. Chapter 19: ETL Subsystems and Techniques provides \nguidance for creating and administering bridge tables.\nIn an inpatient hospital stay scenario, the diagnosis group may be unique to each \npatient if it evolves over time during the patient’s stay. In this case you would supple-\nment the bridge table with two date stamps to capture begin and end dates. Although \nthe twin date stamps complicate updates to the diagnosis group bridge table, they \nare useful for change tracking, as described more fully in Chapter 7: Accounting.\n Supertypes and Subtypes for Charges\nWe’ve  described a design for billed healthcare treatments to cover both inpatient and \noutpatient claims. In reality, healthcare charges resemble the supertype and subtype \npattern described in Chapter 10. Facility charges for inpatient hospital stays diff er \nfrom professional charges for outpatient treatments in clinics and doctor offi  ces. \nIf you were focused exclusively on hospital stays, it would be reasonable to \ntweak the Figure 14-2 dimensional structure to incorporate more hospital-speciﬁ c \ninformation. Figure 14-5 shows a revised set of dimensions specialized for hospital \nstays, with the new dimensions bolded.\n\n\nChapter 14\n348\nTreatment Date Key (FK)\nPrimary Insurance Billing Date Key (FK)\nSecondary Insurance Billing Date Key (FK)\nResponsible Party Billing Date Key (FK)\nLast Primary Insurance Payment Date Key (FK)\nLast Secondary Insurance Payment Date Key (FK)\nLast Responsible Party Payment Date Key (FK)\nZero Balance Date Key (FK)\nPatient Key (FK)\nAdmitting Physician Key (FK)\nAdmitting Physician Organization Key (FK)\nAttending Physician Key (FK)\nAttending Physician Organization Key (FK)\nProcedure Key (FK)\nFacility Key (FK)\nAdmitting Diagnosis Group Key (FK)\nDischarge Diagnosis Group Key (FK)\nPrimary Insurance Organization Key (FK)\nSecondary Insurance Organization Key (FK)\nResponsible Party Key (FK)\nEmployer Key (FK)\nMaster Bill ID (DD)\nFacts...\nInpatient Hospital Claim Billing and Payment Workflow Fact\nFigure 14-5: Accumulating snapshot for hospital stay charges.\nReferring to Figure 14-5, you can see two roles for the physician: admitting physi-\ncian and attending physician. The ﬁ gure shows physician organizations for both roles \nbecause physicians may represent diff erent organizations in a hospital setting. With \nmore complex surgical events, such as a heart transplant operation, whole teams of \nspecialists and assistants are assembled. In this case, you could include a key in the \nfact table for the primary responsible physician; the other physicians and medical \nstaff  would be linked to the fact row via a group key to a multivalued bridge table.\nYou also have two multivalued diagnosis dimensions on each fact table row. The \nadmitting diagnosis group is determined at the beginning of the hospital stay and \nshould be the same for every treatment row that is part of the same hospital stay. \nThe discharge diagnosis group is not known until the patient is discharged.\nElectronic Medical Records\nMany  healthcare organizations are moving from paper-based processes to elec-\ntronic medical records. In the United States, federally mandated quality goals to \nsupport improved population health management may be achievable only with \n\n\nHealthcare 349\ntheir adoption. Healthcare providers are aggressively implementing electronic \nhealth record systems; the movement is signiﬁ cantly impacting healthcare DW\n/BI initiatives.\nElectronic medical records can present challenges for data warehouse environ-\nments because of their extreme variability and potentially extreme volumes. Patients’ \nmedical record data comes in many diff erent forms, ranging from numeric data to \nfreeform text comments entered by a healthcare professional to images and photo-\ngraphs. We’ll further discuss unstructured data in Chapter 21: Big Data Analytics; \nelectronic medical and/or health records may become a classic use case for big data. \nOne thing is certain. The amount and variability of electronic data in the healthcare \nindustry will continue to grow.\n Measure Type Dimension for Sparse Facts\nAs  designers, it is tempting to strive for a more standardized framework that could \nbe extended to handle data variability. For example, you could potentially handle the \nvariability of lab test results with a measurement type dimension describing what \nthe fact row means, or in other words, what the generic fact represents. The unit \nof measure for a given numeric entry is found in the associated measurement type \ndimension row, along with any additivity restrictions, as shown in Figure 14-6. \nLab Test Measurement Type Dimension\nLab Test Measurement Type Key (PK)\nLab Test Measurement Type Description\nLab Test Measurement Type Unit of Measure\nOrder Date Key (FK)\nTest Date Key (FK)\nPatient Key (FK)\nPhysican Key (FK)\nLab Test Key (FK)\nLab Test Measurement Type Key (FK)\nObserved Test Result Value\nLab Test Result Facts\nFigure 14-6: Lab test observations with measurement type dimension.\nThis approach is superbly ﬂ exible; you can add new measurement types simply by \nadding new rows in the measurement type dimension, not by altering the structure \nof the fact table. This approach also eliminates the nulls in the classic positional fact \ntable design because a row exists only if the measurement exists. However, there \nare trade-off s. Using a measurement type dimension may generate lots of new fact \ntable rows because the grain is “one row per measurement per event” rather than the \nmore typical “one row per event.” If a lab test results in 10 numeric measurements, \nthere are now 10 rows in the fact table rather than a single row in the classic design. \nFor extremely sparse situations, such as clinical laboratory or manufacturing test \nenvironments, this is a reasonable compromise. However, as the density of the facts \n\n\nChapter 14\n350\ngrows, you end up spewing out too many fact rows. At this point you no longer have \nsparse facts and should return to the classic fact table design with ﬁ xed columns.\nMoreover, this measurement type approach may complicate BI data access appli-\ncations. In the relational star schema, combining two numbers that were captured \nas part of a single event is more diffi  cult with this approach because now you must \nfetch two rows from the fact table. SQL likes to perform arithmetic functions within \na row, not across rows. In addition, you must be careful not to mix incompatible \namounts in a calculation because all the numeric measures reside in a single amount \ncolumn. It’s worth noting that multidimensional OLAP cubes are more tolerant of \nperforming calculations across measurement types.\n Freeform Text Comments\nFreeform text comments, such as clinical notes, are sometimes associated with fact \ntable events. Although text comments are not very analytically potent unless they’re \nparsed into well-behaved dimension attributes, business users are often unwilling \nto part with them given the embedded nuggets of information.\nTextual comments should not be stored in a fact table directly because they waste \nspace and rarely participate in queries. Some designers think it’s permissible to store \ntextual ﬁ elds in the fact table, as long as they’re referred to as degenerate dimensions. \nDegenerate dimensions are most typically used for operational transaction control \nnumbers and identiﬁ ers; it’s not an acceptable approach or pattern for contending \nwith bulky text ﬁ elds. Storing freeform comments in the fact table adds clutter that \nmay negatively impact the performance of analysts’ more typical quantitative queries.\nThe unbounded text comments should either be stored in a separate comments \ndimension or treated as attributes in a transaction event dimension. A key consider-\nation when evaluating these two approaches is the text ﬁ eld’s cardinality. If there’s \nnearly a unique comment for every fact table event, storing the textual ﬁ eld in a trans-\naction dimension makes the most sense. However, in many cases, No Comment is \nassociated with numerous fact rows. Because the number of unique text comments in \nthis scenario is much smaller than the number of unique transactions, it would make \nmore sense to store the textual data in a comments dimension with an associated \nforeign key in the fact table. In either case, queries involving both the text comments \nand fact metrics will perform relatively poorly given the need to resolve joins between \ntwo voluminous tables. Often business users want to drill into text comments for \nfurther investigation after highly selective fact table query ﬁ lters have been  applied.\nImages\nSometimes  the data captured in a patient’s electronic medical record is an image, \nin addition to either quantitative numbers or qualitative notes. There are trade-off s \n\n\nHealthcare 351\nbetween capturing a JPEG ﬁ lename in the fact table to refer to an associated image \nversus embedding the image as a blob directly in the database. The advantage of \nusing a JPEG ﬁ lename is that other image creation, viewing, and editing programs \ncan freely access the image. The disadvantage is that a separate database of graphic \nﬁ les must be maintained in synchrony with the fact table.\nFacility/Equipment Inventory Utilization\nIn  addition to ﬁ nancial and clinical data, healthcare organizations are also keenly \ninterested in more operationally oriented metrics, such as utilization and availability \nof their assets, whether referring to patient beds or surgical operating theatres. In \nChapter 4, we discussed product inventory data as transaction events as well as \nperiodic snapshots. Facility or equipment inventories in a healthcare organization \ncan be handled similarly.\nFor example, you can envision a bed utilization periodic snapshot with every bed’s \nstatus at regularly recurring points in time, perhaps at midnight, the start of every \nshift, or even more frequently throughout the day. In addition to a snapshot date and \npotentially time-of-day, this factless fact table would include foreign keys to identify \nthe patient, attending physician, and perhaps an assigned nurse on duty.\nConversely, you can imagine treating the bed inventory data as a transaction \nfact table with one row per movement into and out of a hospital bed. This may be a \nsimplistic transaction fact table with transaction date and time dimension foreign \nkeys, along with dimensions to describe the type of movement, such as ﬁ lled or \nvacated. In the case of operating room utilization and availability, you can envision \na lengthier list of statuses, such as pre-operation, post-operation, or downtime, \nalong with time durations.\nIf the inventory changes are not terribly volatile, such as the beds in a rehabilita-\ntion or eldercare inpatient environment, you should consider a timespan fact table, \nas discussed in Chapter 8, with row eff ective and expiration dates and times to \nrepresent the various states of a bed over a period of  time.\n Dealing with Retroactive Changes\nAs  DW/BI practitioners, we have well-developed techniques for accurately capturing \nthe historical ﬂ ow of data from our enterprise’s source applications. Numeric mea-\nsurements go into fact tables, which are surrounded with contemporary descriptions \nof what you know is true at the time of the measurements, packaged as dimension \ntables. The descriptions of patient, physician, facility, and payer evolve as slowly \nchanging dimensions whenever these entities change their descriptions. \n\n\nChapter 14\n352\nHowever, in the healthcare industry, especially with legacy operational systems, \nyou often need to contend with late arriving data that should have been loaded into \nthe data warehouse weeks or months ago. For example, you might receive data \nregarding patient procedures that occurred several weeks ago, or updates to patient \nproﬁ les that were back-dated as eff ective several months ago. The more delayed the \nincoming records are, the more challenging the DW/BI system’s ETL processing \nbecomes. We’ll discuss these late arriving fact and dimension scenarios in Chapter \n19. Unfortunately, these patterns are common in healthcare DW/BI environments; \nin fact, they may be the dominant modes of processing rather than specialized \ntechniques for outlier cases. Eventually, more eff ective source data capture systems \nshould reduce the frequency of these late arriving data anomalies.\nSummary\nHealthcare provides a wealth of dimensional design examples. In this chapter, the \nenterprise data warehouse bus matrix illustrated the critical linkages between a \nhealthcare organization’s administrative and clinical data. We used an accumulating \nsnapshot grain fact table with role-playing date dimensions for the healthcare claim \nbilling and payment pipeline. We also saw role playing used for the physician and \npayer dimensions in other fact tables of this chapter.\nHealthcare schemas are littered with multivalued dimensions, especially the \ndiagnosis dimension. Complex surgical events might also use multivalued bridge \ntables to represent the teams of involved physicians and other staff  members. The \nbridge tables used with healthcare data seldom contain weighting factors, as dis-\ncussed in earlier chapters, because it is extremely diffi  cult to establish weighting \nbusiness rules, beyond the designation of a “primary” relationship.\nWe discussed medical records and test results, suggesting a measurement type \ndimension to organize sparse, heterogeneous measurements into a single, uniform \nframework. We also discussed the handling of text comments and linked images. \nTransaction and periodic snapshot fact tables were used to represent facility or \nequipment inventory utilization and availability. In closing, we touched upon ret-\nroactive fact and dimension changes that are often all too common with healthcare \nperformance data.\n\n\nElectronic \nCommerce\nA \n web-intensive business’s clickstream data records the gestures of every web \nvisitor. In its most elemental form, the clickstream is every page event recorded \nby each of the company’s web servers. The clickstream contains a number of new \ndimensions, such as page, session, and referrer, which are not found in other data \nsources. The clickstream is a torrent of data; it can be difficult and exasperating for \nDW/BI professionals. Does it connect to the rest of the DW/BI system? Can its dimen-\nsions and facts be conformed in the enterprise data warehouse bus architecture?\nWe start this chapter by describing the raw clickstream data source and designing \nits relevant dimensional models. We discuss the impact of Google Analytics, which \ncan be thought of as an external data warehouse delivering information about your \nwebsite. We then integrate clickstream data into a larger matrix of more conven-\ntional processes for a web retailer, and argue that the proﬁ tability of the web sales \nchannel can be measured if you allocate the right costs back to the individual sales.\nChapter 15 discusses the following concepts:\n \n■Clickstream data and its unique dimensionality\n \n■Role of external services such as Google Analytics\n \n■Integrating clickstream data with the other business processes on the bus \nmatrix\n \n■Assembling a complete view of proﬁ tability for a web enterprise\nClickstream Source Data\nThe clickstream is not just another data source that is extracted, cleaned, and \ndumped into the DW/BI environment. The clickstream is an evolving collection of \ndata sources. There are a number of server log ﬁ le formats for capturing clickstream \ndata. These log ﬁ le formats have optional data components that, if used, can be very \nhelpful in identifying visitors, sessions, and the true meaning of behavior.\n15\n\n\nChapter 15\n354\nBecause of the distributed nature of the web, clickstream data often is collected \nsimultaneously by diff erent physical servers, even when the visitor thinks they are \ninteracting with a single website. Even if the log ﬁ les collected by these separate \nservers are compatible, a very interesting problem arises in synchronizing the log \nﬁ les after the fact. Remember that a busy web server may be processing hundreds \nof page events per second. It is unlikely the clocks on separate servers will be in \nsynchrony to one-hundredth of a second.\nYou also obtain clickstream data from diff erent parties. Besides your own log \nﬁ les, you may get clickstream data from referring partners or from internet service \nproviders (ISPs). Another important form of clickstream data is the search speciﬁ ca-\ntion given to a search engine that then directs the visitor to the website.\nFinally, if you are an ISP providing web access to directly connected customers, \nyou have a unique perspective because you see every click of your captive custom-\ners that may allow more powerful and invasive analyses of the customer’s sessions.\nThe most basic form of clickstream data from a normal website is stateless. That \nis, the log shows an isolated page retrieval event but does not provide a clear tie to \nother page events elsewhere in the log. Without some kind of contextual help, it is \ndiffi  cult or impossible to reliably identify a complete visitor session.\nThe other big frustration with basic clickstream data is the anonymity of the \nsession. Unless visitors agree to reveal their identity in some way, you often cannot \nbe sure who they are, or if you have ever seen them before. In certain situations, \nyou may not distinguish the clicks of two visitors who are simultaneously brows-\ning the website.\nClickstream Data Challenges\nClickstream data contains many ambiguities. Identifying visitor origins, visitor \nsessions, and visitor identities is something of an interpretive art. Browser caches \nand proxy servers make these identiﬁ cations more challenging.\nIdentifying the Visitor Origin\nIf  you are very lucky, your site is the default home page for the visitor’s browser. \nEvery time he opens his browser, your home page is the ﬁ rst thing he sees. This is \npretty unlikely unless you are the webmaster for a portal site or an intranet home \npage, but many sites have buttons which, when clicked, prompt visitors to set their \nURL as the browser’s home page. Unfortunately there is no easy way to determine \nfrom a log whether your site is set as a browser’s home page.\nA visitor may be directed to your site from a search at a portal such as Yahoo! or \nGoogle. Such referrals can come either from the portal’s index, for which you may \nhave paid a placement fee, or from a word or content search.\n\n\nElectronic Commerce 355\nFor some websites, the most common source of visitors is from a browser book-\nmark. For this to happen, the visitor must have previously bookmarked your site, \nand this can occur only after the site’s interest and trust levels cross the visitor’s \nbookmark threshold.\nFinally, your site may be reached as a result of a clickthrough—a deliberate click \non a text or graphical link from another site. This may be a paid-for referral via a \nbanner ad, or a free referral from an individual or cooperating site. In the case of \nclickthroughs, the referring site will almost always be identiﬁ able as a ﬁ eld in the \nweb event record. Capturing this crucial clickstream data is important to verify the \neffi  cacy of marketing programs. It also provides crucial data for auditing invoices \nyou may receive from clickthrough advertising charges.\nIdentifying the Session\nMost  web-centric analyses require every visitor session (visit) to have its own unique \nidentity tag, similar to a supermarket receipt number. This is the session ID. Records \nfor every individual visitor action in a session, whether they are derived from the \nclickstream or an application interaction, must contain this tag. But keep in mind \nthe operational application, such as an order entry system generates this session \nID, not the web server.\nThe  basic protocol for the web, Hyper Text Transfer Protocol (HTTP) is stateless; \nthat is, it lacks the concept of a session. There are no intrinsic login or logout actions \nbuilt into the HTTP protocol, so session identity must be established in some other \nway. There are several ways to do this:\n \n1. In many cases, the individual hits comprising a session can be consolidated by \ncollating time-contiguous log entries from the same host (IP address). If the \nlog contains a number of entries with the same host ID in a short period of \ntime (for example, one hour), you can reasonably assume the entries are for \nthe same session. This method breaks down for websites with large numbers \nof visitors because dynamically assigned IP addresses may be reused immedi-\nately by diff erent visitors over a brief time period. Also, diff erent IP addresses \nmay be used within the same session for the same visitor. This approach also \npresents problems when dealing with browsers that are behind some ﬁ rewalls. \nNotwithstanding these problems, many commercial log analysis products use \nthis method of session tracking, and it requires no cookies or special web \nserver features.\n 2. Another much more satisfactory method is to let the web browser place a \nsession-level cookie into the visitor’s web browser. This cookie will last as \nlong as the browser is open and in general won’t be available in subsequent \n",
      "page_number": 368
    },
    {
      "number": 41,
      "title": "Segment 41 (pages 381-388)",
      "start_page": 381,
      "end_page": 388,
      "detection_method": "topic_boundary",
      "content": "Chapter 15\n356\nbrowser sessions. The cookie value can serve as a temporary session ID not \nonly to the browser, but also to any application that requests the session \ncookie from the browser. But using a transient cookie has the disadvantage \nthat you can’t tell when the visitor returns to the site at a later time in a new \nsession.\n \n3. HTTP’s secure sockets layer (SSL) off ers an opportunity to track a visitor \nsession because it may include a login action by the visitor and the exchange \nof encryption keys. The downside to using this method is that to track the \nsession, the entire information exchange needs to be in high-overhead SSL, \nand the visitor may be put off  by security advisories that can pop up using \ncertain browsers. Also, each host must have its own unique security certiﬁ cate.\n \n4. If page generation is dynamic, you can try to maintain visitor state by plac-\ning a session ID in a hidden ﬁ eld of each page returned to the visitor. This \nsession ID can be returned to the web server as a query string appended to \na subsequent URL. This method of session tracking requires a great deal of \ncontrol over the website’s page generation methods to ensure the thread of \na session ID is not broken. If the visitor clicks links that don’t support this \nsession ID ping-pong, a single session may appear to be multiple sessions. \nThis approach also breaks down if multiple vendors supply content in a single \nsession unless those vendors are closely collaborating.\n \n5. Finally, the website may establish a persistent cookie in the visitor’s machine \nthat is not deleted by the browser when the session ends. Of course, it’s pos-\nsible the visitor will have his browser set to refuse cookies, or may manually \nclean out his cookie ﬁ le, so there is no absolute guarantee that even a per-\nsistent cookie will survive. Although any given cookie can be read only by \nthe website that caused it to be created, certain groups of websites can agree \nto store a common ID tag that would let these sites combine their separate \nnotions of a visitor session into a “super session.”\nIn summary, the most reliable method of session tracking from web server log \nrecords is obtained by setting a persistent cookie in the visitor’s browser. Less reli-\nable, but good results can be obtained by setting a session level and a nonpersistent \ncookie and by associating time-contiguous log entries from the same host. The latter \nmethod requires a robust algorithm in the log postprocessor to ensure satisfactory \nresults and to decide when not to take the results seriously.\nIdentifying the Visitor\nIdentifying  a speciﬁ c visitor who logs into your site presents some of the most \nchallenging problems facing a site designer, webmaster, or manager of the web \nanalytics group.\n\n\nElectronic Commerce 357\n \n■Web visitors want to be anonymous. They may have no reason to trust you, \nthe internet, or their computer with personal identiﬁ cation or credit card \ninformation.\n \n■If you request visitors’ identity, they may not provide accurate information.\n \n■You can’t be sure which family member is visiting your site. If you obtain \nan identity by association, for instance from a persistent cookie left during a \nprevious visit, the identiﬁ cation is only for the computer, not for the speciﬁ c \nvisitor. Any family member or company employee may have been using that \nparticular computer at that moment in time.\n \n■You can’t assume an individual is always at the same computer. Server-\nprovided cookies identify a computer, not an individual. If someone accesses \nthe same website from an offi  ce computer, home computer, and mobile device, \na diff erent website cookie is probably put into each machine.\nClickstream Dimensional Models\nBefore  designing clickstream dimensional models, let’s consider all the dimensions \nthat may have relevance in a clickstream environment. Any single dimensional \nmodel will not use all the dimensions at once, but it is nice to have a portfolio \nof dimensions waiting to be used. The list of dimensions for a web retailer could \ninclude:\n \n■Date\n \n■Time of day\n \n■Part\n \n■Vendor\n \n■Status\n \n■Carrier\n \n■Facilities location\n \n■Product\n \n■Customer\n \n■Media\n \n■Promotion\n \n■Internal organization\n \n■Employee\n \n■Page\n \n■Event\n \n■Session\n \n■Referral\n\n\nChapter 15\n358\nAll the dimensions in the list, except for the last four shown in bold, are familiar \ndimensions, most of which we have already used in earlier chapters of this book. \nBut the last four are the unique dimensions of the clickstream and warrant some \ncareful attention.\nPage Dimension\nThe page dimension  describes the page context for a web page event, as illustrated \nin Figure 15-1. The grain of this dimension is the individual page. The deﬁ nition \nof page must be ﬂ exible enough to handle the evolution of web pages from static \npage delivery to highly dynamic page delivery in which the exact page the customer \nsees is unique at that instant in time. We assume even in the case of the dynamic \npage that there is a well-deﬁ ned function that characterizes the page, and we will \nuse that to describe the page. We will not create a page row for every instance of a \ndynamic page because that would yield a dimension with an astronomical number \nof rows. These rows also would not diff er in interesting ways. You want a row in this \ndimension for each interesting distinguishable type of page. Static pages probably get \ntheir own row, but dynamic pages would be grouped by similar function and type.\nPage Dimension Attribute\nPage Key\nPage Source\nPage Function\nPage Template\nItem Type\nGraphics Type\nAnimation Type\nSound Type\nPage File Name\nSurrogate values (1..N)\nStatic, Dynamic, Unknown, Corrupted, Inapplicable, ...\nPortal, Search, Product description, Corporate information, ...\nSparse, Dense, ...\nProduct SKU, Book ISBN number, Telco rate type, ...\nGIF, JPG, Progressive disclosure, Size pre-declared, ...\nSimilar to graphics type\nSimilar to graphics type\nOptional application dependent name\nSample Data Values/Definitions\nFigure 15-1: Page dimension attributes and sample data values.\nWhen the deﬁ nition of a static page changes because it is altered by the web-\nmaster, the page dimension row can either be type 1 overwritten or treated with \nan alternative slowly changing technique. This decision is a matter of policy for \nthe data warehouse and depends on whether the old and new descriptions of the \npage diff er materially, and whether the old deﬁ nition should be kept for historical \nanalysis purposes.\nWebsite designers, data governance representatives from the business, and the \nDW/BI architects need to collaborate to assign descriptive codes and attributes to \neach page served by the web server, whether the page is dynamic or static. Ideally, \nthe web page developers supply descriptive codes and attributes with each page \n\n\nElectronic Commerce 359\nthey create and embed these codes and attributes into the optional ﬁ elds of the \nweb log ﬁ les. This crucial step is at the foundation of the implementation of this \npage dimension.\nBefore leaving the page dimension, we want to point out that some internet com-\npanies track the more granular individual elements on each page of their web sites, \nincluding graphical elements and links. Each element generates its own row for each \nvisitor for each page request. A single complex web page can generate hundreds of \nrows each time the page is served to a visitor. Obviously, this extreme granularity \ngenerates astronomical amounts of data, often exceeding 10 terabytes per day!\nSimilarly, gaming companies may generate a row for every gesture made by every \nonline game player, which again can result in hundreds of millions of rows per day. \nIn both cases, the most atomic fact table will have extra dimensions describing the \ngraphical element, link, or game situation.\nEvent Dimension\nThe  event dimension describes what happened on a particular page at a particular \npoint in time. The main interesting events are Open Page, Refresh Page, Click Link, \nand Enter Data. You want to capture that information in this small event dimension, \nas illustrated in Figure 15-2.\nEvent Dimension Attribute\nEvent Key\nEvent Type\nEvent Content\nSurrogate values (1..N)\nOpen page, Refresh page, Click link, Unknown, Inapplicable\nApplication-dependent fields eventually driven by XML tags\nSample Data Values/Definitions\nFigure 15-2: Event dimension attributes and sample data values.\nSession Dimension\nThe  session dimension provides one or more levels of diagnosis for the visitor’s \nsession as a whole, as shown in Figure 15-3. For example, the local context of the \nsession might be Requesting Product Information, but the overall session context \nmight be Ordering a Product. The success status would diagnose whether the mis-\nsion was completed. The local context may be decidable from just the identity of \nthe current page, but the overall session context probably can be judged only by \nprocessing the visitor’s complete session at data extract time. The customer status \nattribute is a convenient place to label the customer for periods of time, with labels \nthat are not clear either from the page or immediate session. These statuses may be \nderived from auxiliary business processes in the DW/BI system, but by placing these \nlabels deep within the clickstream, you can directly study the behavior of certain \ntypes of customers. Do not put these labels in the customer dimension because they \n\n\nChapter 15\n360\nmay change over very short periods of time. If there are a large number of these \nstatuses, consider creating a separate customer status mini-dimension rather than \nembedding this information in the session dimension.\nSession Dimension Attribute\nSession Key\nSession Type\nLocal Context\nSession Context\nAction Sequence\nSuccess Status\nCustomer Status\nSurrogate values (1..N)\nClassified, Unclassified, Corrupted, Inapplicable\nPage-derived context like Requesting Product Information\nTrajectory-derived context like Ordering a Product\nSummary label for overall sequence of actions during session\nIdentifies whether overall session mission was accomplished\nNew customer, High value customer, About to cancel, In default\nSample Data Values/Definitions\nFigure 15-3: Session dimension attributes and sample data values.\nThis dimension groups sessions for analysis, such as:\n \n■How many customers consulted your product information before ordering?\n \n■How many customers looked at your product information and never ordered?\n \n■How many customers did not ﬁ nish ordering? Where did they stop?\nReferral Dimension\nThe  referral dimension, illustrated in Figure 15-4, describes how the customer \narrived at the current page. The web server logs usually provide this information. \nThe URL of the previous page is identiﬁ ed, and in some cases additional information \nis present. If the referrer was a search engine, usually the search string is speciﬁ ed. \nIt may not be worthwhile to put the raw search speciﬁ cation into your database \nbecause the search speciﬁ cations are so complicated and idiosyncratic that an ana-\nlyst may not be able to query them usefully. You can assume some kind of simpliﬁ ed \nand cleaned speciﬁ cation is placed in the speciﬁ cation attribute.\nReferral Dimension Attribute\nReferral Key\nReferral Type\nReferring URL\nReferring Site\nReferring Domain\nSearch Type\nSpecification\nTarget\nSurrogate values (1..N)\nIntra site, Remote site, Search engine, Corrupted, Inapplicable\nwww.organization-site.com/linkspage\nwww.organization-site.com\nwww.organization-site.com\nSimple text match, Complex logical match\nActual spec used (useful if simple text, otherwise questionable)\nMeta tags, Body text, Title (where search found its match)\nSample Data Values/Definitions\nFigure 15-4: Referral dimension attributes and sample data values.\n\n\nElectronic Commerce 361\n Clickstream Session Fact Table\nNow  that you have a portfolio of useful clickstream dimensions, you can design \nthe primary clickstream dimensional models based on the web server log data. \nThis business process can then be integrated into the family of other web retailing \nsubject areas.\nWith an eye toward keeping the ﬁ rst fact table from growing astronomically, \nyou should choose the grain to be one row for each completed customer session. \nThis grain is signiﬁ cantly higher than the underlying web server logs which record \neach individual page event, including individual pages as well as each graphical \nelement on each page. While we typically encourage designers to start with the \nmost granular data available in the source system, this is a purposeful deviation \nfrom our standard practices. Perhaps you have a big site recording more than 100 \nmillion page fetches per day, and 1 billion micro page events (graphical elements), \nbut you want to start with a more manageable number of rows to be loaded each \nday. We assume for the sake of argument that the 100 million page fetches boil \ndown to 20 million complete visitor sessions. This could arise if an average visitor \nsession touched 5 pages.\nThe  dimensions that are appropriate for this ﬁ rst fact table are calendar date, time \nof day, customer, page, session, and referrer. Finally, you can add a set of measured \nfacts for this session including session seconds, pages visited, orders placed, units \nordered, and order dollars. The completed design is shown in Figure 15-5.\nDate Dimension (2 views for roles)\nClickstream Session Fact\nUniversal Date Key (FK)\nUniversal Date/Time\nLocal Date Key (FK)\nLocal Date/Time\nCustomer Key (FK)\nEntry Page Key (FK)\nSession Key (FK)\nReferrer Key (FK)\nSession ID (DD)\nSession Seconds\nPages Visited\nOrders Placed\nOrder Quantity\nOrder Dollar Amount\nEntry Page Dimension\nCustomer Dimension\nSession Dimension\nReferrer Dimension\nFigure 15-5: Clickstream fact table design for complete sessions.\n\n\nChapter 15\n362\nThere are a number of interesting aspects to this design. You may wonder why \nthere are two connections from the calendar date dimension to the fact table and \ntwo date/time stamps. This is a case in which both the calendar date and the time \nof day must play two diff erent roles. Because you are interested in measuring the \nprecise times of sessions, you must meet two conﬂ icting requirements. First, you \nwant to make sure you can synchronize all session dates and times internationally \nacross multiple time zones. Perhaps you have other date and time stamps from \nother web servers or nonweb systems elsewhere in the DW/BI environment. To \nachieve true synchronization of events across multiple servers and processes, you \nmust record all session dates and times, uniformly, in a single time zone such as \nGreenwich Mean Time (GMT) or Coordinated Universal Time (UTC). You should \ninterpret the session date and time combinations as the beginning of the session. \nBecause you have the dwell time of the session as a numeric fact, you can tell when \nthe session ended, if that is of interest.\nThe other requirement you meet with this design is to record the date and time of \nthe session relative to the visitor’s wall clock. The best way to represent this informa-\ntion is with a second calendar date foreign key and date/time stamp. Theoretically, \nyou could represent the time zone of the customer in the customer dimension table, \nbut constraints to determine the correct wall clock time would be horrendously \ncomplicated. The time diff erence between two cities (such as London and Sydney) \ncan change by as much as two hours at diff erent times of the year depending on \nwhen these cities go on and off  daylight savings time. This is not the business of \nthe BI reporting application to work out. It is the business of the database to store \nthis information, so it can be constrained in a simple and direct way.\nThe two role-playing calendar date dimension tables are views on a single under-\nlying table. The column names are massaged in the view deﬁ nition, so they are \nslightly diff erent when they show up in the user interface pick lists of BI tools. \nNote that the use of views makes the two instances of each table semantically \nindependent.\nWe modeled the exact instant in time with a full date/time stamp rather than a \ntime-of-day dimension. Unlike the calendar date dimension, a time-of-day dimen-\nsion would contain few if any meaningful attributes. You don’t have labels for each \nhour, minute, or second. Such a time-of-day dimension could be ridiculously large \nif its grain were the individual second or millisecond. Also, the use of an explicit \ndate/time stamp allows direct arithmetic between diff erent date/time stamps to \ncalculate precise time gaps between sessions, even those crossing days. Calculating \ntime gaps using a time-of-day dimension would be awkward.\nThe inclusion of the page dimension in Figure 15-5 may seem surprising given \nthe grain of the design is the customer session. However, in a given session, a very \n\n\nElectronic Commerce 363\ninteresting page is the entry page. The page dimension in this design is the page the \nsession started with. In other words, how did the customer hop onto your bus just \nnow? Coupled with the referrer dimension, you now have an interesting ability to \nanalyze how and why the customer accessed your website. A more elaborate design \nwould also add an exit page dimension.\nYou may be tempted to add the causal dimension to this design, but if the causal \ndimension focuses on individual products, it would be inappropriate to add it to \nthis design. The symptom that the causal dimension does not mesh with this design \nis the multivalued nature of the causal factors for a given complete session. If you \nrun ad campaigns or special deals for several products, how do you represent this \nmultivalued situation if the customer’s session involves several products? The right \nplace for a product-oriented causal dimension will be in the more ﬁ ne-grained table \ndescribed in the next fact table example. Conversely, a more broadly focused mar-\nket conditions dimension that describes conditions aff ecting all products would be \nappropriate for a session-grained fact table.\nThe session seconds fact is the total number of seconds the customer spent on the \nsite during this session. There will be many cases in which you can’t tell when the \ncustomer left. Perhaps the customer typed in a new URL. This won’t be detected by \nconventional web server logs. (If the data is collected by an ISP who can see every \nclick across sessions, this particular issue goes away.) Or perhaps the customer \ngot up out of the chair and didn’t return for 1 hour. Or perhaps the customer just \nclosed the browser without making any more clicks. In all these cases, your extract \nsoftware needs to assign a small and nominal number of seconds to this last session \nstep, so the analysis is not unrealistically distorted.\nWe purposely designed this ﬁ rst clickstream fact table to focus on complete visitor \nsessions while keeping the size under control. The next schema drops down to the \nlowest practical granularity you can support in the data warehouse: the individual \npage event.\n Clickstream Page Event Fact Table\nThe  granularity of the second clickstream fact table is the individual page event in \neach customer session; the underlying micro events recording graphical elements \nsuch as JPGs and GIFs are discarded (unless you are Yahoo! or eBay as described \npreviously). With simple static HTML pages, you can record only one interesting \nevent per page view, namely the page view. As websites employ dynamically created \nXML-based pages, with the ability to establish an on-going dialogue through the \npage, the number and type of events will grow.\nThis fact table could become astronomical in size. You should resist the urge \nto aggregate the table up to a coarser granularity because that inevitably involves \n",
      "page_number": 381
    },
    {
      "number": 42,
      "title": "Segment 42 (pages 389-398)",
      "start_page": 389,
      "end_page": 398,
      "detection_method": "topic_boundary",
      "content": "Chapter 15\n364\ndropping dimensions. Actually, the ﬁ rst clickstream fact table represents just such \nan aggregation; although it is a worthwhile fact table, analysts cannot ask questions \nabout visitor behavior or individual pages.\nHaving chosen the grain, you can choose the appropriate dimensions. The list of \ndimensions includes calendar date, time of day, customer, page, event, session, ses-\nsion ID, step (three roles), product, referrer, and promotion. The completed design \nis shown in Figure 15-6.\nDate Dimension (2 views for roles)\nClickstream Page Event Fact\nUniversal Date Key (FK)\nUniversal Date/Time\nLocal Date Key (FK)\nLocal Date/Time\nCustomer Key (FK)\nPage Key (FK)\nEvent Key (FK)\nSession Key (FK)\nSession ID (DD)\nSession Step Key (FK)\nPurchase Step Key (FK)\nAbandonment Step Key (FK)\nProduct Key (FK)\nReferrer Key (FK)\nPromotion Key (FK)\nPage Seconds\nOrder Quantity\nOrder Dollar Amount\nProduct Key (PK)\nProduct Attributes ...\nPage Dimension\nCustomer Dimension\nEvent Dimension\nSession Dimension\nPromotion Dimension\nReferrer Dimension\nProduct Dimension\nStep Key (PK)\nStep Number\nSteps Until End\nStep Dimension (3 views for roles)\nFigure 15-6: Clickstream fact table design for individual page use.\nFigure 15-6 looks similar to the ﬁ rst design, except for the addition of the page, \nevent, promotion, and step dimensions. This similarity between fact tables is typical \nof dimensional models. One of the charms of dimensional modeling is the “boring” \nsimilarity of the designs. But that is where they get their power. When the designs \nhave a predictable structure, all the software up and down the DW/BI chain, from \nextraction, to database querying, to the BI tools, can exploit this similarity to great \nadvantage.\nThe two roles played by the calendar date and date/time stamps have the same \ninterpretation as in the ﬁ rst design. One role is the universal synchronized time, \nand the other role is the local wall clock time as measured by the customer. In this \nfact table, these dates and times refer to the individual page event.\n\n\nElectronic Commerce 365\nThe page dimension refers to the individual page. This is the main diff erence in \ngrain between the two clickstream fact tables. In this fact table you can see all the \npages accessed by the customers.\nAs described earlier, the session dimension describes the outcome of the session. \nA companion column, the session ID, is a degenerate dimension that does not have \na join to a dimension table. This degenerate dimension is a typical dimensional \nmodeling construct. The session ID is simply a unique identiﬁ er, with no semantic \ncontent, that serves to group together the page events of each customer session \nin an unambiguous way. You did not need a session ID degenerate dimension in \nthe ﬁ rst fact table, but it is included as a “parent key” if you want to easily link to \nthe individual page event fact table. We recommend the session dimension be at a \nhigher level of granularity than the session ID; the session dimension is intended \nto describe classes and categories of sessions, not the characteristics of each indi-\nvidual session.\nA product dimension is shown in this design under the assumption this website \nbelongs to a web retailer. A ﬁ nancial services site probably would have a similar \ndimension. A consulting services site would have a service dimension. An auction \nsite would have a subject or category dimension describing the nature of the items \nbeing auctioned. A news site would have a subject dimension, although with dif-\nferent content than an auction site.\nYou should accompany the product dimension with a promotion dimension so \nyou can attach useful causal interpretations to the changes in demand observed \nfor certain products.\nFor each page event, you should record the number of seconds that elapse before \nthe next page event. Call this page seconds to contrast it with session seconds in \nthe ﬁ rst fact table. This is a simple example of paying attention to conformed facts. \nIf you call both of these measures simply “seconds,” you risk having these seconds \ninappropriately added or combined. Because these seconds are not precisely equiva-\nlent, you should name them diff erently as a warning. In this particular case, you \nwould expect the page seconds for a session in this second fact table to add up to \nthe session seconds in the ﬁ rst fact table.\nThe ﬁ nal facts are units ordered and order dollars. These columns will be zero \nor null for many rows in this fact table if the speciﬁ c page event is not the event \nthat places the order. Nevertheless, it is highly attractive to provide these columns \nbecause they tie the all-important web revenue directly to behavior. If the units \nordered and order dollars were only available through the production order entry \nsystem elsewhere in the DW/BI environment, it would be ineffi  cient to perform the \n\n\nChapter 15\n366\nrevenue-to-behavior analysis across multiple large tables. In many database man-\nagement systems, these null facts are handled effi  ciently and may take up literally \nzero space in the fact table.\n Step Dimension\nBecause  the fact table grain is the individual page event, you can add the powerful \nstep dimension described in Chapter 8: Customer Relationship Management. The \nstep dimension, originally shown in Figure 8-11, provides the position of the speciﬁ c \npage event within the overall session.\nThe step dimension becomes particularly powerful when it is attached to the fact \ntable in various roles. Figure 15-6 shows three roles: overall session, purchase subses-\nsion, and abandonment subsession. A purchase subsession, by deﬁ nition, ends in a \nsuccessful purchase. An abandonment subsession is one that fails to complete a pur-\nchase transaction for some reason. Using these roles of the step dimension allows some \nvery interesting queries. For example, if the purchase step dimension is constrained to \nstep number 1, the query returns nothing but the starting page for successful purchase \nexperiences. Conversely, if the abandonment step dimension is constrained to zero \nsteps remaining, the query returns nothing but the last and presumably most unful-\nﬁ lling pages visited in unsuccessful purchase sessions. Although the whole design \nshown in Figure 15-6 is aimed at product purchases, the step dimension technique \ncan be used in the analysis of any sequential process.\n Aggregate Clickstream Fact Tables\nBoth  clickstream fact tables designed thus far are pretty large. There are many \nbusiness questions that would be forced to summarize millions of rows from these \ntables. For example, if you want to track the total visits and revenue from major \ndemographic groups of customers accessing your website on a month-by-month \nbasis, you can certainly do that with either fact table. In the session-grained fact \ntable, you would constrain the calendar date dimension to the appropriate time span \n(say January, February, and March of the current year). You would then create row \nheaders from the demographics type attribute in the customer dimension and the \nmonth attribute in the calendar dimension (to separately label the three months \nin the output). Finally, you would sum the Order Dollars and count the number of \nsessions. This all works ﬁ ne. But it is likely to be slow without help from an aggre-\ngate table. If this kind of query is frequent, the DBA will be encouraged to build an \naggregate table, as shown in Figure 15-7.\nYou can build this table directly from your ﬁ rst fact table, whose grain is the \nindividual session. To build this aggregate table, you group by month, demographic \ntype, entry page, and session outcome. You count the number of sessions, and sum \n\n\nElectronic Commerce 367\nall the other additive facts. This results in a drastically smaller fact table, almost \ncertainly less than 1% of the original session-grained fact table. This reduction in \nsize translates directly to a corresponding increase in performance for most queries. \nIn other words, you can expect queries directed to this aggregate table to run at \nleast 100 times as fast.\nMonth Dimension\nSession Aggregate Fact\nUniversal Month Key (FK)\nDemographic Key (FK)\nEntry Page Key (FK)\nSession Outcome Key (FK)\nNumber of Sessions\nSession Seconds\nPages Visited\nOrders Placed\nOrder Quantity\nOrder Dollar Amount\nEntry Page Dimension\nDemographic Dimension\nSession Outcome Dimension\nFigure 15-7: Aggregate clickstream fact table.\nAlthough it may not have been obvious, we followed a careful discipline in build-\ning the aggregate table. This aggregate fact table is connected to a set of shrunken \nrollup dimensions directly related to the original dimensions in the more granular \nfact tables. The month dimension is a conformed subset of the calendar day dimen-\nsion’s attributes. The demographic dimension is a conformed subset of customer \ndimension attributes. You should assume the page and session tables are unchanged; \na careful design of the aggregation logic could suggest a conformed shrinking of \nthese tables as well.\nGoogle Analytics\nGoogle  Analytics (GA) is a service provided by Google that is best described as an \nexternal data warehouse that provides many insights about how your website is used. \nTo use GA, you modify each page of your website to include a GA tracking code \n(GATC) embedded in a Java code snippet located in the HTML <head> declaration of \neach page to be tracked. When a visitor accesses the page, information is sent to the \nAnalytics service at Google, as long as the visitor has JavaScript enabled. Virtually \nall of the information described in this chapter can be collected through GA, with \nthe exception of personally identiﬁ able information (PII) which is forbidden by GA’s \nterms of service. GA can be combined with Google’s Adword service to track ad \ncampaigns and conversions (sales). Reportedly, GA is used by more than 50% of the \nmost popular web sites on the internet.\n\n\nChapter 15\n368\nData from GA can be viewed in a BI tool dashboard online directly from the under-\nlying GA databases, or data can be delivered to you in a wide variety of standard and \ncustom reports, making it possible to build your own local business process schema \nsurrounding this data.\nInterestingly, GA’s detailed technical explanation of the data elements that can be \ncollected through the service are described correctly as either dimensions or measures. \nSomeone at Google has been reading our books…\n Integrating Clickstream into Web Retailer’s \nBus Matrix\nThis section considers the business processes needed by a web-based computer retailer. \nThe retailer’s enterprise data warehouse bus matrix is illustrated in Figure 15-8. Note \nthe matrix lists business process subject areas, not individual fact tables. Typically, each \nmatrix row results in a suite of closely associated fact tables and/or OLAP cubes, which \nall represent a particular business process.\nThe Figure 15-8 matrix has a number of striking characteristics. There are a \nlot of check marks. Some of the dimensions, such as date/time, organization, and \nemployee appear in almost every business process. The product and customer dimen-\nsions dominate the middle part of the matrix, where they are attached to business \nprocesses that describe customer-oriented activities. At the top of the matrix, suppli-\ners and parts dominate the processes of acquiring the parts that make up products \nand building them to order for the customer. At the bottom of the matrix, you have \nclassic infrastructure and cost driver business processes that are not directly tied \nto customer behavior.\nThe web visitor clickstream subject area sits squarely among the customer-\noriented processes. It shares the date/time, product, customer, media, causal, and \nservice policy dimensions with several other business processes nearby. In this \nsense it should be obvious that the web visitor clickstream data is well integrated \ninto the fabric of the overall DW/BI system for this retailer. Applications tying the \nweb visitor clickstream will be easy to integrate across all the processes sharing \nthese conformed dimensions because separate queries to each fact table can be \ncombined across individual rows of the report.\nThe  web visitor clickstream business process contains the four special click-\nstream dimensions not found in the others. These dimensions do not pose a problem \nfor applications. Instead, the ability of the web visitor clickstream data to bridge \nbetween the web world and the brick-and-mortar world is exactly the advantage \nyou are looking for. You can constrain and group on attributes from the four web \n\n\nElectronic Commerce 369\ndimensions and explore the eff ect on the other business processes. For example, you \ncan see what kinds of web experiences produce customers who purchase certain \nkinds of service policies and then invoke certain levels of service demands.\nSupplier Purchase Orders\nSupply Chain Management\nCustomer Relationship Management\nSupplier Deliveries\nPart Inventories\nProduct Assembly Bill of Materials\nProduct Assembly to Order\nProduct Promotions\nAdvertising\nCustomer Communications\nCustomer Inquiries\nWeb Visitor Clickstream\nProduct Orders\nService Policy Orders\nProduct Shipments\nCustomer Billing\nCustomer Payments\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nDate and Time\nPart\nVendor\nCarrier\nFacility\nProduct\nCustomer\nMedia\nPromotion\nService Policy\nInternal Organization\nEmployee\nClickstream (4 dims)\nProduct Returns\nProduct Support\nService Policy Responses\nOperations\nEmployee Labor\nHuman Resources\nFacilities Operations\nWeb Site Operations\nFigure 15-8: Bus matrix for web retailer.\nFinally, it should be pointed out that the matrix serves as a kind of communica-\ntions vehicle for all the business teams and senior management to appreciate the \n\n\nChapter 15\n370\nneed to conform dimensions and facts. A given column in the matrix is, in eff ect, \nan invitation list to the meeting for conforming the dimension!\n Proﬁ tability Across Channels Including Web\nAfter  the DW/BI team successfully implements the initial clickstream fact tables \nand ties them to the sales transaction and customer communication business pro-\ncesses, the team may be ready to tackle the most challenging subject area of all: \nweb proﬁ tability.\nYou can tackle web proﬁ tability as an extension of the sales transaction process. \nFundamentally, you are allocating all the activity and infrastructure costs down to \neach sales transaction. You could, as an alternative, try to build web proﬁ tability \non top of the clickstream, but this would involve an even more controversial allo-\ncation process in which you allocate costs down to each session. It would be hard \nto assign activity and infrastructure costs to a session that has no obvious product \ninvolvement and leads to no immediate sale.\nA big beneﬁ t of extending the sales transaction fact table is that you get a view \nof proﬁ tability across all your sales channels, not just the web. In a way, this should \nbe obvious because you know that you must sort out the costs and assign them to \nthe various channels. \nThe grain of the proﬁ t and loss facts is each individual line item sold on a sales \nticket to a customer at a point in time, whether it’s a single sales ticket or single web \npurchasing session. This is the same as the grain of the sales transaction business \nprocess and includes all channels, assumed to be store sales, telesales, and web sales.\nThe dimensions of the proﬁ t and loss facts are also the same as the sales transac-\ntion fact table: date, time, customer, channel, product, promotion, and ticket number \n(degenerate). The big diff erence between the proﬁ tability and sales transaction fact \ntables is the breakdown of the costs, as illustrated in Figure 15-9.\nBefore discussing the allocation of costs, let us examine the format of the proﬁ t \nand loss facts. It is organized as a simple proﬁ t and loss (P&L) statement (refer to \nFigure 6-14). The ﬁ rst fact is familiar units sold. All the other facts are dollar values \nbeginning with the value of the sale as if it were sold at the list or catalog price, \nreferred to as gross revenue. Assuming sales often take place at lower prices, you \nwould account for any diff erence with a manufacturer’s allowance, marketing pro-\nmotion that is a price reduction, or markdown done to move the inventory. When \nthese eff ects are taken into account, you can calculate the net revenue, which is the \ntrue net price the customer pays times the number of units purchased.\nThe rest of the P&L consists of a series of subtractions, where you calculate \nprogressively more far-reaching versions of proﬁ t. You can begin by subtracting \nthe product manufacturing cost if you manufacture it, or equivalently, the product \n\n\nElectronic Commerce 371\nacquisition cost if it is acquired from a supplier. Then subtract the product storage \ncost. At this point, many enterprises call this partial result the gross proﬁ t. You can \ndivide this gross proﬁ t by the gross revenue to get the gross margin ratio.\nDate Dimension (2 views for roles)\nProfitability Fact\nUniversal Date Key (FK)\nUniversal Time of Day Key (FK)\nLocal Date Key (FK)\nLocal Time of Day Key (FK)\nCustomer Key (FK)\nChannel Key (FK)\nProduct Key (FK)\nPromotion Key (FK)\nTicket Number (DD)\nUnits Sold\nGross Revenue\nManufacturing Allowance\nMarketing Promotion\nSales Markdown\nNet Revenue\nManufacturing Cost\nStorage Cost\nGross Profit\nFreight Cost\nSpecial Deal Cost\nOther Overhead Cost\nNet Profit\nCustomer Dimension\nTime of Day Dimension (2 views for roles)\nChannel Dimension\nPromotion Dimension\nProduct Dimension\nFigure 15-9: Proﬁ t and loss facts across sales channels, including web sales.\nObviously, the columns called net revenue and gross proﬁ t are calculated directly \nfrom the columns immediately preceding them in the fact table. But should you \nexplicitly store these columns in the database? The answer depends on whether \nyou provide access to this fact table through a view or whether users or BI applica-\ntions directly access the physical fact table. The structure of the P&L is suffi  ciently \ncomplex that, as the data warehouse provider, you don’t want to risk the impor-\ntant measures like net revenue and gross proﬁ t being computed incorrectly. If you \nprovide all access through views, you can easily provide the computed columns \nwithout physically storing them. But if your users are allowed to access the under-\nlying physical table, you should include net revenue, gross proﬁ t, and net proﬁ t as \nphysical columns.\nBelow the gross proﬁ t you can continue subtracting various costs. Typically, the \nDW/BI team must separately source or estimate each of these costs. Remember the \nactual entries in any given fact table row are the fractions of these total costs allo-\ncated all the way down to the individual fact row grain. Often there is signiﬁ cant \npressure on the DW/BI team to deliver the proﬁ tability business process. Or to put \nit another way, there is tremendous pressure to source all these costs. But how good \n\n\nChapter 15\n372\nare the costs in the various underlying data sets? Sometimes a cost is only available \nas a national average, computed for an entire year. Any allocation scheme is going \nto assign a kind of pro forma value that has no real texture to it. Other costs will be \nbroken down a little more granularly, perhaps to calendar quarter and by geographic \nregion (if relevant). Finally, some costs may be truly activity-based and vary in a \nhighly dynamic, responsive, and realistic way over time.\nWebsite system costs are an important cost driver in electronic commerce busi-\nnesses. Although website costs are classic infrastructure costs, and are therefore \ndiffi  cult to allocate directly to the product and customer activity, this is a key step \nin developing a web-oriented P&L statement. Various allocation schemes are pos-\nsible, including allocating the website costs to various product lines by the number \nof pages devoted to each product, allocating the costs by pages visited, or allocating \nthe costs by actual web-based purchases. \nThe DW/BI team cannot be responsible for implementing activity-based costing \n(ABC) in a large organization. When the team is building a proﬁ tability dimensional \nmodel, the team gets the best cost data available at the moment and publishes the \nP&L. Perhaps some of the numbers are simple rule-of-thumb ratios. Others may be \nhighly detailed activity-based costs. Over time, as the sources of cost improve, the \nDW/BI team incorporates these new sources and notiﬁ es the users that the business \nrules have improved.\nBefore leaving this design, it is worthwhile putting it in perspective. When a \nP&L structure is embedded in a rich dimensional framework, you have immense \npower. You can break down all the components of revenue, cost, and proﬁ t for \nevery conceivable slice and dice provided by the dimensions. You can answer what \nis proﬁ table, but also answer “why” because you can see all the components of the \nP&L, including:\n \n■How proﬁ table is each channel (web sales, telesales, and store sales)? Why?\n \n■How proﬁ table are your customer segments? Why?\n \n■How proﬁ table is each product line? Why?\n \n■How proﬁ table are your promotions? Why?\n \n■When is your business most proﬁ table? Why?\nThe symmetric dimensional approach enables you to combine constraints from \nmany dimensions, allowing compound versions of the proﬁ tability analyses like:\n \n■Who are the proﬁ table customers in each channel? Why?\n \n■Which promotions work well on the web but do not work well in other \nchannels? Why?\n\n\nElectronic Commerce 373\nSummary\nThe web retailer case study used in this chapter is illustrative of any business with \na signiﬁ cant web presence. Besides tackling the clickstream subject area at multiple \nlevels of granularity, the central challenge is eff ectively integrating the clickstream \ndata into the rest of the business. We discussed ways to address the identiﬁ cation \nchallenges associated with the web visitor, their origin, and session boundaries, \nalong with the special dimensions unique to clickstream data, including t he ses-\nsion, page, and step dimensions.\nIn the next chapter, we’ll turn our attention to the primary business processes \nin an insurance company as we recap many of the dimensional modeling patterns \npresented throughout this book.\n",
      "page_number": 389
    },
    {
      "number": 43,
      "title": "Segment 43 (pages 399-408)",
      "start_page": 399,
      "end_page": 408,
      "detection_method": "topic_boundary",
      "content": "Insurance\nW\ne bring together concepts from nearly all the previous chapters to build \na DW/BI system for a property and casualty insurance company in this \nfinal case study. If you are from the insurance industry and jumped directly to \nthis chapter for a quick fix, please accept our apology, but this material depends \nheavily on ideas from the earlier chapters. You’ll need to turn back to the beginning \nof the book to have this chapter make any sense.\nAs  has been our standard procedure, this chapter launches with background \ninformation for a business case. While the requirements unfold, we’ll draft the enter-\nprise data warehouse bus matrix, much like we would in a real-life requirements \nanalysis eff ort. We’ll then design a series of dimensional models by overlaying the \ncore techniques learned thus far.\nChapter 16 reviews the following concepts:\n \n■Requirements-driven approach to dimensional design\n \n■Value chain implications, along with an example bus matrix snippet for an \ninsurance company\n \n■Complementary transaction, periodic snapshot, and accumulating snapshot \nschemas\n \n■Dimension role playing\n \n■Handling of slowly changing dimension attributes\n \n■Mini-dimensions for dealing with large, rapidly changing dimension attributes\n \n■Multivalued dimension attributes\n \n■Degenerate dimensions for operational control numbers\n \n■Audit dimensions to track data lineage\n \n■Heterogeneous supertypes and subtypes to handle products with varied attri-\nbutes and facts \n \n■Junk dimensions for miscellaneous indicators\n16\n\n\nChapter 16\n376\n \n■Conformed dimensions and facts\n \n■Consolidated fact tables combining metrics from separate business processes\n \n■Factless fact tables\n \n■Common mistakes to avoid when designing dimensional models\n Insurance Case Study\nImagine working for a large property and casualty insurer that off ers automobile, \nhomeowner, and personal property insurance. You conduct extensive interviews \nwith business representatives and senior management from the claims, ﬁ eld opera-\ntions, underwriting, ﬁ nance, and marketing departments. Based on these interviews, \nyou learn the industry is in a state of ﬂ ux. Nontraditional players are leveraging \nalternative channels. Meanwhile, the industry is consolidating due to globalization, \nderegulation, and demutualization challenges. Markets are changing, along with \ncustomer needs. Numerous interviewees tell us information is becoming an even \nmore important strategic asset. Regardless of the functional area, there is a strong \ndesire to use information more eff ectively to identify opportunities more quickly \nand respond most appropriately.\nThe good news is that internal systems and processes already capture the bulk \nof the data required. Most insurance companies generate tons of nitty-gritty opera-\ntional data. The bad news is the data is not integrated. Over the years, political and \nIT boundaries have encouraged the construction of tall barriers around isolated \nislands of data. There are multiple disparate sources for information about the \ncompany’s products, customers, and distribution channels. In the legacy opera-\ntional systems, the same policyholder may be identiﬁ ed several times in separate \nautomobile, home, and personal property applications. Traditionally, this segmented \napproach to data was acceptable because the diff erent lines of business functioned \nlargely autonomously; there was little interest in sharing data for cross-selling and \ncollaboration in the past. Now within our case study, business management is \nattempting to better leverage this enormous amount of inconsistent and somewhat \nredundant data.\nBesides the inherent issues surrounding data integration, business users lack the \nability to access data easily when needed. In an attempt to address this shortcom-\ning, several groups within the case study company rallied their own resources and \nhired consultants to solve their individual short-term data needs. In many cases, the \nsame data was extracted from the same source systems to be accessed by separate \norganizations without any strategic overall information delivery strategy.\n\n\nInsurance 377\nIt didn’t take long to recognize the negative ramiﬁ cations associated with separate \nanalytic data repositories because performance results presented at executive meetings \ndiff ered depending on the data source. Management understood this independent \nroute was not viable as a long-term solution because of the lack of integration, large \nvolumes of redundant data, and diffi  culty in interpreting and reconciling the results. \nGiven the importance of information in this brave new insurance world, manage-\nment was motivated to deal with the cost implications surrounding the development, \nsupport, and analytic ineffi  ciencies of these supposed data warehouses that merely \nproliferated operational data islands.\nSenior  management chartered the chief information offi  cer (CIO) with the respon-\nsibility and authority to break down the historical data silos to “achieve information \nnirvana.” They charged the CIO with the ﬁ duciary responsibility to manage and \nleverage the organization’s information assets more eff ectively. The CIO developed \nan overall vision that wed an enterprise strategy for dealing with massive amounts \nof data with a response to the immediate need to become an information-rich orga-\nnization. In the meantime, an enterprise DW/BI team was created to begin designing \nand implementing the vision.\nSenior management has been preaching about a transformation to a more cus-\ntomer-centric focus, instead of the traditional product-centric approach, in an eff ort \nto gain competitive advantage. The CIO jumped on that bandwagon as a catalyst \nfor change. The folks in the trenches have pledged intent to share data rather than \nsquirreling it away for a single purpose. There is a strong desire for everyone to \nhave a common understanding of the state of the business. They’re clamoring to \nget rid of the isolated pockets of data while ensuring they have access to detail and \nsummary data at both the enterprise and line-of-business levels.\nInsurance Value Chain\nThe  primary value chain of an insurance company is seemingly short and simple. The \ncore processes are to issue policies, collect premium payments, and process claims. \nThe organization is interested in better understanding the metrics spawned by each \nof these events. Users want to analyze detailed transactions relating to the formula-\ntion of policies, as well as transactions generated by claims processing. They want \nto measure performance over time by coverage, covered item, policyholder, and \nsales distribution channel characteristics. Although some users are interested in \nthe enterprise perspective, others want to analyze the heterogeneous nature of the \ninsurance company’s individual lines of business.\nObviously, an insurance company is engaged in many other external pro-\ncesses, such as the investment of premium payments or compensation of contract \n\n\nChapter 16\n378\nagents, as well as a host of internally focused activities, such as human resources, \nﬁ nance, and purchasing. For now, we will focus on the core business related to \npolicies and claims.\nThe insurance value chain begins with a variety of policy transactions. Based \non your current understanding of the requirements and underlying data, you opt \nto handle all the transactions impacting a policy as a single business process (and \nfact table). If this perspective is too simplistic to accommodate the metrics, dimen-\nsionality, or analytics required, you should handle the transaction activities as \nseparate fact tables, such as quoting, rating, and underwriting. As discussed in \nChapter 5: Procurement, there are trade-off s between creating separate fact tables \nfor each natural cluster of transaction types versus lumping the transactions into \na single fact table.\nThere is also a need to better understand the premium revenue associated with \neach policy on a monthly basis. This will be key input into the overall proﬁ t picture. \nThe insurance business is very transaction intensive, but the transactions themselves \ndo not represent little pieces of revenue, as is the case with retail or manufactur-\ning sales. You cannot merely add up policy transactions to determine the revenue \namount. The picture is further complicated in insurance because customers pay in \nadvance for services. This same advance-payment model applies to organizations \noff ering magazine subscriptions or extended warranty contracts. Premium payments \nmust be spread across multiple periods because the company earns the revenue over \ntime as it provides insurance coverage. The complex relationship between policy \ntransactions and revenue measurements often makes it impossible to answer rev-\nenue questions by crawling through the individual transactions. Not only is such \ncrawling time-consuming, but also the logic required to interpret the eff ect of dif-\nferent transaction types on revenue can be horrendously complicated. The natural \nconﬂ ict between the detailed transaction view and the snapshot perspective almost \nalways requires building both kinds of fact tables in the warehouse. In this case, \nthe premium snapshot is not merely a summarization of the policy transactions; it \nis quite a separate thing that comes from a separate source.\nDraft Bus Matrix\nBased  on the interview ﬁ ndings, along with an understanding of the key source \nsystems, the team begins to draft an enterprise data warehouse bus matrix with the \ncore policy-centric business processes as rows and core dimensions as columns. \nTwo rows are deﬁ ned in the matrix, one corresponding to the policy transactions \nand another for the monthly premium snapshot.\nAs illustrated in Figure 16-1, the core dimensions include date, policyholder, \nemployee, coverage, covered item, and policy. When drafting the matrix, don’t \n\n\nInsurance 379\nattempt to include all the dimensions. Instead, try to focus on the core common \ndimensions that are reused in more than one schema.\nPolicy Transactions\nPremium Snapshot\nDate\nPolicyholder\nCovered Item\nEmployee\nX\nX\nX\nX\nX\nX\nX\nMonth\nX\nX\nAgent\nX\nX\nX\nPolicy\nCoverage\nFigure 16-1: Initial draft bus matrix.\n Policy Transactions\nLet’s  turn our attention to the ﬁ rst row of the matrix by focusing on the transactions \nfor creating and altering a policy. Assume the policy represents a set of coverages \nsold to the policyholder. Coverages can be considered the insurance company’s \nproducts. Homeowner coverages include ﬁ re, ﬂ ood, theft, and personal liability; \nautomobile coverages include comprehensive, collision damage, uninsured motor-\nist, and personal liability. In a property and casualty insurance company, coverages \napply to a speciﬁ c covered item, such as a particular house or car. Both the coverage \nand covered item are carefully identiﬁ ed in the policy. A particular covered item \nusually has several coverages listed in the policy.\nAgents sell policies to policyholders. Before the policy can be created, a pricing \nactuary determines the premium rate that will be charged given the speciﬁ c cover-\nages, covered items, and qualiﬁ cations of the policyholder. An underwriter, who \ntakes ultimate responsibility for doing business with the policyholder, makes the \nﬁ nal approval.\nThe operational policy transaction system captures the following types of \ntransactions:\n \n■Create policy, alter policy, or cancel policy (with reason)\n \n■Create coverage on covered item, alter coverage, or cancel coverage (with \nreason)\n \n■Rate coverage or decline to rate coverage (with reason)\n \n■Underwrite policy or decline to underwrite policy (with reason)\n\n\nChapter 16\n380\nThe grain of the policy transaction fact table should be one row for each indi-\nvidual policy transaction. Each atomic transaction should be embellished with \nas much context as possible to create a complete dimensional description of the \ntransaction. The dimensions associated with the policy transaction business pro-\ncess include the transaction date, eff ective date, policyholder, employee, coverage, \ncovered item, policy number, and policy transaction type. Now let’s further discuss \nthe dimensions in this schema while taking the opportunity to reinforce concepts \nfrom earlier chapters.\n Dimension Role Playing\nThere  are two dates associated with each policy transaction. The policy transac-\ntion date is the date when the transaction was entered into the operational system, \nwhereas the policy transaction eff ective date is when the transaction legally takes \neff ect. These two foreign keys in the fact table should be uniquely named. The two \nindependent dimensions associated with these keys are implemented using a single \nphysical date table. Multiple logically distinct tables are then presented to the user \nthrough views with unique column names, as described originally in Chapter 6: \nOrder Management.\n Slowly Changing Dimensions\nInsurance  companies typically are very interested in tracking changes to dimensions \nover time. You can apply the three basic techniques for handling slowly changing \ndimension (SCD) attributes to the policyholder dimension, as introduced in Chapter 5.\nWith the type 1 technique, you simply overwrite the dimension attribute’s prior \nvalue. This is the simplest approach to dealing with attribute changes because the \nattributes always represent the most current descriptors. For example, perhaps the \nbusiness agrees to handle changes to the policyholder’s date of birth as a type 1 \nchange based on the assumption that any changes to this attribute are intended as \ncorrections. In this manner, all fact table history for this policyholder appears to \nhave always been associated with the updated date of birth.\nBecause the policyholder’s ZIP code is key input to the insurer’s pricing and risk \nalgorithms, users are very interested in tracking ZIP code changes, so the type 2 \ntechnique is used for this attribute. Type 2 is the most common SCD technique \nwhen there’s a requirement for accurate change tracking over time. In this case, when \nthe ZIP code changes, you create a new policyholder dimension row with a new \nsurrogate key and updated geographic attributes. Do not go back and revisit the fact \ntable. Historical fact table rows, prior to the ZIP code change, still reﬂ ect the old \nsurrogate key. Going forward, you use the policyholder’s new surrogate key, so new \nfact table rows join to the post-change dimension proﬁ le. Although this technique is \nextremely graceful and powerful, it places more burdens on ETL processing. Also, \n\n\nInsurance 381\nthe number of rows in the dimension table grows with each type 2 SCD change. \nGiven there might already be more than 1 million rows in your policyholder dimen-\nsion table, you may opt to use a mini-dimension for tracking ZIP code changes, \nwhich we will review shortly.\nFinally, let’s assume each policyholder is classiﬁ ed as belonging to a particu-\nlar segment. Perhaps nonresidential policyholders were historically categorized as \neither commercial or government entities. Going forward, the business users want \nmore detailed classiﬁ cations to diff erentiate between large multinational, middle \nmarket, and small business commercial customers, in addition to nonproﬁ t organi-\nzations and governmental agencies. For a period of time, users want the ability to \nanalyze results by either the historical or new segment classiﬁ cations. In this case \nyou could use a type 3 approach to track the change for a period of time by adding \na column, labeled Historical for diff erentiation, to retain the old classiﬁ cations. \nThe new classiﬁ cation values would populate the segment attribute that has been \na permanent ﬁ xture on the policyholder dimension. This approach, although not \nextremely common, allows you to see performance by either the current or histori-\ncal segment maps. This is useful when there’s been an en masse change, such as \nthe customer classiﬁ cation realignment. Obviously, the type 3 technique becomes \noverly complex if you need to track more than one version of the historical map or \nbefore-and-after changes for multiple dimension attributes.\n Mini-Dimensions for Large or Rapidly Changing \nDimensions\nAs  mentioned earlier, the policyholder dimension qualiﬁ es as a large dimension with \nmore than 1 million rows. It is often important to accurately track content values \nfor a subset of attributes. For example, you need an accurate description of some \npolicyholder and covered item attributes at the time the policy was created, as well \nas at the time of any adjustment or claim. As discussed in Chapter 5, the practical \nway to track changing attributes in large dimensions is to split the closely monitored, \nmore rapidly changing attributes into one or more type 4 mini-dimensions directly \nlinked to the fact table with a separate surrogate key. The use of mini-dimensions \nhas an impact on the effi  ciency of attribute browsing because users typically want \nto browse and constrain on these changeable attributes. If all possible combina-\ntions of the attribute values in the mini-dimension have been created, handling a \nmini-dimension change simply means placing a diff erent key in the fact table row \nfrom a certain point in time forward. Nothing else needs to be changed or added \nto the database.\nThe covered item is the house, car, or other speciﬁ c insured item. The cov-\nered item dimension contains one row for each actual covered item. The covered \nitem dimension is usually somewhat larger than the policyholder dimension, so \n\n\nChapter 16\n382\nit’s another good place to consider deploying a mini-dimension. You do not want \nto capture the variable descriptions of the physical covered objects as facts because \nmost are textual and are not numeric or continuously valued. You should make every \neff ort to put textual attributes into dimension tables because they are the target of \ntextual constraints and the source of report labels.\n Multivalued Dimension Attributes\nWe discussed multivalued dimension attributes when we associated multiple skills \nwith an employee in Chapter 9: Human Resources Management. In Chapter 10: \nFinancial Services, we associated multiple customers with an account, and then \nin Chapter 14: Healthcare, we modeled a patient’s multiple diagnoses. In this \ncase study, you’ll look at another multivalued modeling situation: the relationship \nbetween commercial customers and their industry classiﬁ cations.\nEach  commercial customer may be associated with one or more Standard Industry \nClassiﬁ cation (SIC) or North American Industry Classiﬁ cation System (NAICS) codes. \nA large, diversiﬁ ed commercial customer could be represented by a dozen or more \nclassiﬁ cation codes. Much like you did with Chapter 14’s diagnosis group, a bridge \ntable ties together all the industry classiﬁ cation codes within a group. This indus-\ntry classiﬁ cation bridge table joins directly to either the fact table or the customer \ndimension as an outrigger. It enables you to report fact table metrics by any industry \nclassiﬁ cation. If the commercial customer’s industry breakdown is proportionally \nidentiﬁ ed, such as 50 percent agricultural services, 30 percent dairy products, and \n20 percent oil and gas drilling, a weighting factor should be included on each \nbridge table row. To handle the case in which no valid industry code is associated \nwith a given customer, you simply create a special bridge table row that represents \n Unknown.\n Numeric Attributes as Facts or Dimensions\nLet’s move on to the coverage dimension. Large insurance companies have dozens \nor even hundreds of separate coverage products available to sell for a given type of \ncovered item. The actual appraised value of a speciﬁ c covered item, like someone’s \nhouse, is a continuously valued numeric quantity that can even vary for a given item \nover time, so treat it as a legitimate fact. In the dimension table, you could store a \nmore descriptive value range, such as $250,000 to $299,999 Appraised Value, for \ngrouping and ﬁ ltering. The basic coverage limit is likely to be more standardized \nand not continuously valued, like Replacement Value or Up to $250,000. In this \ncase, it would also be treated as a dimension  attribute.\n\n\nInsurance 383\n Degenerate Dimension\nThe  policy number will be handled as a degenerate dimension if you have extracted \nall the policy header information into other dimensions. You obviously want to avoid \ncreating a policy transaction fact table with just a small number of keys while embed-\nding all the descriptive details (including the policyholder, dates, and coverages) in \nan overloaded policy dimension. In some cases, there may be one or two attributes \nthat still belong to the policy and not to another dimension. For example, if the \nunderwriter establishes an overall risk grade for the policy based on the totality of \nthe coverages and covered items, then this risk grade probably belongs in a policy \ndimension. Of course, then the policy number is no longer a degenerate dimension.\n Low Cardinality Dimension Tables\nThe  policy transaction type dimension is a small dimension for the transaction types \nlisted earlier with reason descriptions. A transaction type dimension might contain \nless than 50 rows. Even though this table is both narrow in terms of the number \nof columns and shallow in terms of the number of rows, the attributes should still \nbe handled in a dimension table; if the textual characteristics are used for query \nﬁ ltering or report labeling, then they belong in a dimension.\n Audit Dimension\nYou  have the option to associate ETL process metadata with transaction fact rows \nby including a key that links to an audit dimension row created by the extract \nprocess. As discussed in Chapter 6, each audit dimension row describes the data \nlineage of the fact row, including the time of the extract, source table, and extract \nsoftware version.\nPolicy Transaction Fact Table\nThe  policy transaction fact table in Figure 16-2 illustrates several characteristics of \na classic transaction grain fact table. First, the fact table consists almost entirely \nof keys. Transaction schemas enable you to analyze behavior in extreme detail. As \nyou descend to lower granularity with atomic data, the fact table naturally sprouts \nmore dimensionality. In this case, the fact table has a single numeric fact; interpreta-\ntion of the fact depends on the corresponding transaction type dimension. Because \nthere are diff erent kinds of transactions in the same fact table, in this scenario, you \ncannot label the fact more speciﬁ cally. \n\n\nChapter 16\n384\nPolicy Transaction Date Key (FK)\nPolicy Effective Date Key (FK)\nPolicyholder Key (FK)\nEmployee Key (FK)\nCoverage Key (FK)\nCovered Item Key (FK)\nPolicy Transaction Type Key (FK)\nPolicy Transaction Audit Key (FK)\nPolicy Number (DD)\nPolicy Transaction Number (DD)\nPolicy Transaction Dollar Amount\nPolicy Transaction Fact\nEmployee Dimension\nCovered Item Dimension\nDate Dimension (2 views for roles)\nPolicyholder Dimension\nCoverage Dimension\nPolicy Transaction Type Dimension\nPolicy Transaction Audit Dimension\nFigure 16-2: Policy transaction schema.\n Heterogeneous Supertype and Subtype Products\nAlthough there is strong support for an enterprise-wide perspective at our insur-\nance company, the business users don’t want to lose sight of their line-of-business \nspeciﬁ cs. Insurance companies typically are involved in multiple, very diff erent lines \nof business. For example, the detailed parameters of homeowners’ coverages diff er \nsigniﬁ cantly from automobile coverages. And these both diff er substantially from \npersonal property coverage, general liability coverage, and other types of insurance. \nAlthough all coverages can be coded into the generic structures used so far in this \nchapter, insurance companies want to track numerous speciﬁ c attributes that make \nsense only for a particular coverage and covered item. You can generalize the initial \nschema developed in Figure 16-2 by using the supertype and subtype technique \ndiscussed in Chapter 10.\nFigure  16-3 shows a schema to handle the speciﬁ c attributes that describe auto-\nmobiles and their coverages. For each line of business (or coverage type), subtype \ndimension tables for both the covered item and associated coverage are created. \nWhen a BI application needs the speciﬁ c attributes of a single coverage type, it uses \nthe appropriate subtype dimension tables.\nNotice in this schema that you don’t need separate line-of-business fact tables \nbecause the metrics don’t vary by business, but you’d likely put a view on the \nsupertype fact table to present only rows for a given subtype. The subtype dimen-\nsion tables are introduced to handle the special line-of-business attributes. No \nnew keys need to be generated; logically, all we are doing is extending existing \ndimension  rows.\nComplementary Policy Accumulating Snapshot\nFinally,  before leaving policy transactions, you should consider the use of an accu-\nmulating snapshot to capture the cumulative eff ect of the transactions. In this \n",
      "page_number": 399
    },
    {
      "number": 44,
      "title": "Segment 44 (pages 409-418)",
      "start_page": 409,
      "end_page": 418,
      "detection_method": "topic_boundary",
      "content": "Insurance 385\nscenario, the grain of the fact table likely would be one row for each coverage and \ncovered item on a policy. You can envision including policy-centric dates, such as \nquoted, rated, underwritten, eff ective, renewed, and expired. Likewise, multiple \nemployee roles could be included on the fact table for the agent and underwriter. \nMany of the other dimensions discussed would be applicable to this schema, with \nthe exception of the transaction type dimension. The accumulating snapshot likely \nwould have an expanded fact set.\nPolicy Transaction Date Key (FK)\nPolicy Effective Date Key (FK)\nPolicyholder Key (FK)\nEmployee Key (FK)\nCoverage Key (FK)\nCovered Item Key (FK)\nPolicy Transaction Type Key (FK)\nPolicy Transaction Audit Key (FK)\nPolicy Number (DD)\nPolicy Transaction Dollar Amount\nCoverage Key (PK)\nCoverage Description\nLine of Business Description\nLimit\nDeductible\nRental Car Coverage\nWindshield Coverage\nPolicy Transaction Fact\nAutomobile Coverage Dimension\nCovered Item Key (PK)\nCovered Item Description\nVehicle Manufacturer\nVehicle Make\nVehicle Year\nVehicle Classification\nVehicle Engine Size\nVehicle Appraised Value Range\nAutomobile Coverage Item Dimension\nFigure 16-3: Policy transaction schema with subtype automobile dimension tables.\nAs discussed in Chapter 4: Inventory, an accumulating snapshot is eff ective for \nrepresenting information about a pipeline process’s key milestones. It captures the \ncumulative lifespan of a policy, covered items, and coverages; however, it does not \nstore information about each and every transaction that occurred. Unusual trans-\nactional events or unexpected outliers from the standard pipeline would likely be \nmasked with an accumulating perspective. On the other hand, an accumulating \nsnapshot, sourced from the transactions, provides a clear picture of the durations \nor lag times between key process events.\n Premium Periodic Snapshot\nThe  policy transaction schema is useful for answering a wide range of questions. \nHowever, the blizzard of transactions makes it diffi  cult to quickly determine the \nstatus or ﬁ nancial value of an in-force policy at a given point in time. Even if all \nthe necessary detail lies in the transaction data, a snapshot perspective would \nrequire rolling the transactions forward from the beginning of history taking into \naccount complicated business rules for when earned revenue is recognized. Not \nonly is this nearly impractical on a single policy, but it is ridiculous to think about \ngenerating summary top line views of key performance metrics in this manner.\n\n\nChapter 16\n386\nThe answer to this dilemma is to create a separate fact table that operates as a \ncompanion to the policy transaction table. In this case, the business process is the \nmonthly policy premium snapshot. The granularity of the fact table is one row per \ncoverage and covered item on a policy each  month.\n Conformed Dimensions\nOf  course, when designing the premium periodic snapshot table, you should strive to \nreuse as many dimensions from the policy transaction table as possible. Hopefully, \nyou have become a conformed dimension enthusiast by now. As described in Chapter \n4, conformed dimensions used in separate fact tables either must be identical or \nmust represent a shrunken subset of the attributes from the granular dimension.\nThe policyholder, covered item, and coverage dimensions would be identical. \nThe daily date dimension would be replaced with a conformed month dimension \ntable. You don’t need to track all the employees who were involved in policy trans-\nactions on a monthly basis; it may be useful to retain the involved agent, especially \nbecause ﬁ eld operations are so focused on ongoing revenue performance analysis. \nThe transaction type dimension would not be used because it does not apply at the \nperiodic snapshot granularity. Instead, you introduce a status dimension so users \ncan quickly discern the current state of a coverage or policy, such as new policies \nor cancellations this month and over  time.\n Conformed Facts\nWhile  we’re on the topic of conformity, you also need to use conformed facts. If the \nsame facts appear in multiple fact tables, such as facts common to this snapshot fact \ntable as well as the consolidated fact table we’ll discuss later in this chapter, then \nthey must have consistent deﬁ nitions and labels. If the facts are not identical, \nthen they need to be given diff erent names.\nPay-in-Advance Facts\nBusiness  management wants to know how much premium revenue was written (or \nsold) each month, as well as how much revenue was earned. Although a policyholder \nmay contract and pay for coverages on covered items for a period of time, the revenue \nis not earned until the service is provided. In the case of the insurance company, \nthe revenue from a policy is earned month by month as long as the policyholder \ndoesn’t cancel. The correct calculation of a metric like earned premium would \nmean fully replicating all the business rules of the operational revenue recognition \nsystem within the BI application. Typically, the rules for converting a transaction \namount into its monthly revenue impact are complex, especially with mid-month \ncoverage upgrades and downgrades. Fortunately, these metrics can be sourced from \na separate operational system.\n\n\nInsurance 387\nAs illustrated in Figure 16-4, we include two premium revenue metrics in the \nperiodic snapshot fact table to handle the diff erent deﬁ nitions of written versus \nearned premium. Simplistically, if an annual policy for a given coverage and cov-\nered item was written on January 1 for a cost of $600, then the written premium \nfor January would be $600, but the earned premium is $50 ($600 divided by 12 \nmonths). In February, the written premium is zero and the earned premium is still \n$50. If the policy is canceled on March 31, the earned premium for March is $50, \nwhile the written premium is a negative $450. Obviously, at this point the earned \nrevenue stream comes to a crashing halt.\nMonth End Snapshot Date Key (FK)\nPolicyholder Key (FK)\nAgent Key (FK)\nCoverage Key (FK)\nCovered Item Key (FK)\nPolicy Status Key (FK)\nPolicy Number (DD)\nWritten Premium Revenue Amount\nEarned Premium Revenue Amount\nPremium Snapshot Fact\nMonth End Dimension\nAgent Dimension\nPolicyholder Dimension\nCoverage Dimension\nPolicy Status Dimension\nCovered Item Dimension\nFigure 16-4: Periodic premium snapshot schema.\nPay-in-advance business scenarios typically require the combination of transac-\ntion and monthly snapshot fact tables to answer questions of transaction frequency \nand timing, as well as questions of earned income in a given month. You can almost \nnever add enough facts to a snapshot schema to do away with the need for a trans-\naction schema, or vice versa.\nHeterogeneous Supertypes and Subtypes Revisited\nWe  are again confronted with the need to look at the snapshot data with more spe-\nciﬁ c line-of-business attributes, and grapple with snapshot facts that vary by line of \nbusiness. Because the custom facts for each line are incompatible with each other, \nmost of the fact row would be ﬁ lled with nulls if you include all the line-of-business \nfacts on every row. In this scenario, the answer is to separate the monthly snap-\nshot fact table physically by line of business. You end up with the single supertype \nmonthly snapshot schema and a series of subtype snapshots, one for each line of \nbusiness or coverage type. Each of the subtype snapshot fact tables is a copy of a \nsegment of the supertype fact table for just those coverage keys and covered item \nkeys belonging to a particular line of business. We include the supertype facts as \na convenience so analyses within a coverage type can use both the supertype and \ncustom subtype facts without accessing two large fact tables. \n\n\nChapter 16\n388\nMultivalued Dimensions Revisited\nAutomobile insurance provides another opportunity to discuss multivalued dimen-\nsions. Often multiple insured drivers are associated with a policy. You can construct \na bridge table, as illustrated in Figure 16-5, to capture the relationship between the \ninsured drivers and policy. In this case the insurance company can assign realistic \nweighting factors based on each driver’s share of the total premium cost.\nMonth End Snapshot Date Key (FK)\nPolicyholder Key (FK)\nMore FKs...\nPolicy Key (FK)\nWritten Premium Revenue Amount\nEarned Premium Revenue Amount\nPremium Snapshot Fact\nPolicy Key (FK)\nInsured Driver Key (FK)\nWeighting Factor\nPolicy-Insured Driver Bridge\nInsured Driver Key (PK)\nInsured Driver Name\nInsured Driver Address Attributes...\nInsured Driver Date of Birth\nInsured Driver Risk Segment\nInsured Driver Dimension\nFigure 16-5: Bridge table for multiple drivers on a policy.\nBecause these relationships may change over time, you can add eff ective and \nexpiration dates to the bridge table. Before you know it, you end up with a factless \nfact table to capture the evolving relationships between a policy, policy holder, \ncovered item, and insured driver over  time.\nMore Insurance Case Study Background\nUnfortunately, the insurance business has a downside. We learn from the inter-\nviewees that there’s more to life than collecting premium revenue payments. The \nmain costs in this industry result from claim losses. After a policy is in eff ect, then \na claim can be made against a speciﬁ c coverage and covered item. A claimant, who \nmay be the policyholder or a new party not previously known to the insurance \ncompany, makes the claim. When the insurance company opens a new claim, a \nreserve is usually established. The reserve is a preliminary estimate of the insurance \ncompany’s eventual liability for the claim. As further information becomes known, \nthis reserve can be adjusted.\nBefore the insurance company pays any claim, there is usually an investigative \nphase where the insurance company sends out an adjuster to examine the covered \nitem and interview the claimant, policyholder, or other individuals involved. The \ninvestigative phase produces a stream of task transactions. In complex claims, \n\n\nInsurance 389\nvarious outside experts may be required to pass judgment on the claim and the \nextent of the damage.\nIn most cases, after the investigative phase, the insurance company issues a \nnumber of payments. Many of these payments go to third parties such as doctors, \nlawyers, or automotive body shop operators. Some payments may go directly to \nthe claimant. It is important to clearly identify the employee responsible for every \npayment made against an open claim.\nThe insurance company may take possession of the covered item after replacing \nit for the policyholder or claimant. If the item has any remaining value, salvage pay-\nments received by the insurance company are a credit against the claim accounting.\nEventually, the payments are completed and the claim is closed. If nothing \nunusual happens, this is the end of the transaction stream generated by the claim. \nHowever, in some cases, further claim payments or claimant lawsuits may force \na claim to be reopened. An important measure for an insurance company is how \noften and under what circumstances claims are reopened.\nIn addition to analyzing the detailed claims processing transactions, the insur-\nance company also wants to understand what happens over the life of a claim. For \nexample, the time lag between the claim open date and the ﬁ rst payment date is an \nimportant measure of claims processing effi  ciency.\n Updated Insurance Bus Matrix\nWith  a better understanding of the claims side of the business, the draft matrix from \nFigure 16-1 needs to be revisited. Based on the new requirements, you add another \nrow to the matrix to accommodate claim transactions, as shown in Figure 16-6. \nMany of the dimensions identiﬁ ed earlier in the project will be reused; you add new \ncolumns to the matrix for the claim, claimant, and third-party payee.\nPolicy Transactions\nPremium Snapshot\nDate\nPolicyholder\nCovered Item\nEmployee\nX\nX\nX\nX\nX\nX\nX\nMonth\nX\nX\nAgent\nX\nX\nX\nPolicy\nCoverage\nClaimant\nClaim Transactions\nX\nX\nX\nX\nX\nX\nX\nX\nX\n3rd Party Payee\nClaim\nFigure 16-6: Updated insurance bus matrix.\n\n\nChapter 16\n390\n Detailed Implementation Bus Matrix\nDW/BI  teams sometimes struggle with the level of detail captured in an enterprise \ndata warehouse bus matrix. In the planning phase of an architected DW/BI project, it \nmakes sense to stick with rather high-level business processes (or sources). Multiple \nfact tables at diff erent levels of granularity may result from each of these business \nprocess rows. In the subsequent implementation phase, you can take a subset of \nthe matrix to a lower level of detail by reﬂ ecting all the fact tables or OLAP cubes \nresulting from the process as separate matrix rows. At this point the matrix can \nbe enhanced by adding columns to reﬂ ect the granularity and metrics associated \nwith each fact table or cube. Figure 16-7 illustrates a more detailed implementation \nbus matrix.\nClaim Transactions\nThe operational claim processing system generates a slew of transactions, including \nthe following transaction task types:\n \n■Open claim, reopen claim, close claim\n \n■Set reserve, reset reserve, close reserve\n \n■Set salvage estimate, receive salvage payment\n \n■Adjuster inspection, adjuster interview\n \n■Open lawsuit, close lawsuit\n \n■Make payment, receive payment\n \n■Subrogate claim\nWhen updating the Figure 16-6 bus matrix, you determine that this schema uses \na number of dimensions developed for the policy world. You again have two role-\nplaying dates associated with the claim transactions. Unique column labels should \ndistinguish the claim transaction and eff ective dates from those associated with \npolicy transactions. The employee is the employee involved in the transactional \ntask. As mentioned in the business case, this is particularly interesting for payment \nauthorization transactions. The claim transaction type dimension would include \nthe transaction types and groupings just listed.\nAs shown in Figure 16-8, there are several new dimensions in the claim transac-\ntion fact table. The claimant is the party making the claim, typically an individual. \nThe third-party payee may be either an individual or commercial entity. Both the \nclaimant and payee dimensions usually are dirty dimensions because of the dif-\nﬁ culty of reliably identifying them across claims. Unscrupulous potential payees \nmay go out of their way not to identify themselves in a way that would easily tie \nthem to other claims in the insurance company’s  system.\n\n\nPolicy Transactions\nCorporate Policy Transactions\nstc\na\nF\nytir\nalu\nn\na\nr\nG\ne\nb\nu\nC\n \nP\nA\nL\nO\n/elb\na\nT tc\na\nF\nAuto Policy Transactions\nHome Policy Transactions\nClaim Transactions\nClaim Workﬂow\nAccident Involvements\nPolicy Premium Snapshot\nCorporate Policy Premiums\nAuto Policy Premiums\nHome Policy Premiums\n1 row for every policy\ntransaction\n1 row per auto policy\ntransaction\n1 row per home policy\ntransaction\n1 row for every claim task\ntransaction\n1 row per claim\n1 row per loss party and\nafﬁliation on an auto claim\n1 row for every policy, covered\nitem and coverage per month\n1 row per auto policy, covered\nitem and coverage per month\n1 row per home policy, covered\nitem and coverage per month\nClaim Events\nPolicy Transaction Amount\nPolicy Transaction Amount\nPolicy Transaction Amount\nClaim Transaction Amount\nOriginal Reserve, Estimate, Current\nReserve, Claim Paid, Salvage\nCollected, and Subro Collected\nAmounts; Loss to Open, Open to\nEstimate, Open to 1st Payment,\nOpen to Subro, and Open to Closed\nLags; # of Transactions\nAccident Involvement Count\nWritten Premium Revenue and\nEarned Premium Revenue Amounts\nWritten Premium Revenue and\nEarned Premium Revenue Amounts\nWritten Premium Revenue and\nEarned Premium Revenue Amounts\nTrxn\nEff\nTrxn\nEff\nTrxn\nEff\nTrxn\nEff\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nAuto\nHome\nX\nX\nAuto\nX\nAuto\nHome\nX\nAuto\nHome\nX\nX\nAuto\nX\nAuto\nHome\nX\nX\nX\nX\nAgent\nAgent\nAgent\nAgent\nX\nX\nX\nX\nX\nX\nX\nX\nAuto\nX\nX\nX\nX\nX\nX\nX\nDate\nPolicyholder\nCoverage\nEmployee\nPolicy\nCovered Item\nClaimant\n3rd Party Payee\nClaim\nFigure 16-7: Detailed implementation bus matrix.\n\n\nChapter 16\n392\nClaim Transaction Date Key (FK)\nClaim Transaction Effective Date Key (FK)\nPolicyholder Key (FK)\nClaim Transaction Employee Key (FK)\nAgent Key (FK)\nCoverage Key (FK)\nCovered Item Key (FK)\nClaimant Key (FK)\n3rd Party Payee Key (FK)\nClaim Transaction Type Key (FK)\nClaim Profile Key (FK)\nClaim Key (FK)\nPolicy Number (DD)\nClaim Transaction Number (DD)\nClaim Transaction Dollar Amount\nClaim Transaction Fact\nEmployee Dimension (2 views for roles)\nCovered Item Dimension\nDate Dimension (2 views for roles)\nPolicyholder Dimension\nCoverage Dimension\nClaimant Dimension\nClaim Transaction Type Dimension\n3rd Party Payee Dimension\nClaim Profile Dimension\nClaim Dimension\nFigure 16-8: Claim transaction schema.\n Transaction Versus Proﬁ le Junk Dimensions\nBeyond the reused dimensions from the policy-centric schemas and the new claim-\ncentric dimensions just listed, there are a large number of indicators and descriptions \nrelated to a claim. Designers are sometimes tempted to dump all these descriptive \nattributes into a claim dimension. This approach makes sense for high-cardinality \ndescriptors, such as the speciﬁ c address where the loss occurred or a narrative \ndescribing the event. However, in general, you should avoid creating dimensions \nwith the same number of rows as the fact table.\nAs we described in Chapter 6, low-cardinality codiﬁ ed data, like the method \nused to report the loss or an indicator denoting whether the claim resulted from a \ncatastrophic event, are better handled in a junk dimension. In this case, the junk \ndimension would more appropriately be referred to as the claim proﬁ le dimension \nwith one row per unique combination of proﬁ le attributes. Grouping or ﬁ ltering on \nthe proﬁ le attributes would yield faster query responses than if they were alterna-\ntively handled as claim dimension  attributes.\n Claim Accumulating Snapshot\nEven with a robust transaction schema, there is a whole class of urgent business \nquestions that can’t be answered using only transaction detail. It is diffi  cult to \nderive claim-to-date performance measures by traversing through every detailed \nclaim task transaction from the beginning of the claim’s history and appropriately \napplying the transactions.\n\n\nInsurance 393\nOn a periodic basis, perhaps at the close of each day, you can roll forward all the \ntransactions to update an accumulating claim snapshot incrementally. The granu-\nlarity is one row per claim; the row is created once when the claim is opened and \nthen is updated throughout the life of a claim until it is ﬁ nally closed.\nMany of the dimensions are reusable, conformed dimensions, as illustrated in \nFigure 16-9. You should include more dates in this fact table to track the key claim \nmilestones and deliver time lags. These lags may be the raw diff erence between \ntwo dates, or they may be calculated in a more sophisticated way by accounting for \nonly workdays in the calculations. A status dimension is added to quickly identify \nall open, closed, or reopened claims, for example. Transaction-speciﬁ c dimensions \nsuch as employee, payee, and claim transaction type are suppressed, whereas the \nlist of additive, numeric measures has been  expanded.\nClaim Workflow Fact\nCovered Item Dimension\nPolicyholder Dimension\nCoverage Dimension\nClaimant Dimension\nClaim Status Dimension\nClaim Profile Dimension\nClaim Dimension\nClaim Open Date Key (FK)\nClaim Loss Date Key (FK)\nClaim Estimate Date Key (FK)\nClaim 1st Payment Date Key (FK)\nClaim Most Recent Payment Date Key (FK)\nClaim Subrogation Date Key (FK)\nClaim Close Date Key (FK)\nPolicyholder Key (FK)\nClaim Supervisor Key (FK)\nAgent Key (FK)\nCoverage Key (FK)\nCovered Item Key (FK)\nClaimant Key (FK)\nClaim Status Key (FK)\nClaim Profile Key (FK)\nClaim Key (FK)\nPolicy Number (DD)\nOriginal Reserve Dollar Amount\nEstimate Dollar Amount\nCurrent Reserve to Date Dollar Amount\nClaim Paid to Date Dollar Amount\nSalvage Collected to Date Dollar Amount\nSubro Payment Collected to Date Dollar Amount\nClaim Loss to Open Lag\nClaim Open to Estimate Lag\nClaim Open to 1st Payment Lag\nClaim Open to Subrogation Lag\nClaim Open to Closed Lag\nNumber of Claim Transactions\nEmployee Dimension (2 views for roles)\nDate Dimension (7 views for roles)\nFigure 16-9: Claim accumulating snapshot schema.\n Accumulating Snapshot for Complex Workﬂ ows\nAccumulating  snapshot fact tables are typically appropriate for predictable workﬂ ows \nwith well-established milestones. They usually have ﬁ ve to 10 key milestone dates \n\n\nChapter 16\n394\nrepresenting the pipeline’s start, completion, and key events in between. However, \nsometimes workﬂ ows are less predictable. They still have a deﬁ nite start and end \ndate, but the milestones in between are numerous and less stable. Some occurrences \nmay skip over some intermediate milestones, but there’s no reliable pattern.\nIn this situation, the ﬁ rst task is to identify the key dates that link to role-playing \ndate dimensions. These dates represent the most important milestones. The start and \nend dates for the process would certainly qualify; in addition, you should consider \nother commonly occurring critical milestones. These dates (and their associated \ndimensions) will be used extensively for BI application ﬁ ltering.\nHowever, if the number of additional milestones is both voluminous and unpre-\ndictable, they can’t all be handled as additional date foreign keys in the fact table. \nTypically, business users are more interested in the lags between these milestones, \nrather than ﬁ ltering or grouping on the dates themselves. If there were a total of \n20 potential milestone events, there would be 190 potential lag durations: event \nA-to-B, A-to-C, … (19 possible lags from event A), B-to-C, … (18 possible lags from \nevent B), and so on. Instead of physically storing 190 lag metrics, you can get away \nwith just storing 19 of them and then calculate the others. Because every pipeline \noccurrence starts by passing through milestone A, which is the workﬂ ow begin \ndate, you could store all 19 lags from the anchor event A and then calculate the \nother variations. For example, if you want to know the lag from B-to-C, take \nthe A-to-C lag value and subtract the A-to-B lag. If there happens to be a null for one \nof the lags involved in a calculation, then the result also needs to be null because \none of the events never occurred. But such a null result is handled gracefully if you \nare counting or averaging that lag across a number of claim rows.\n Timespan Accumulating Snapshot\nAn  accumulating snapshot does a great job presenting a workﬂ ow’s current state, \nbut it obliterates the intermediate states. For example, a claim can move in and out \nof various states such as opened, denied, closed, disputed, opened again, and closed \nagain. The claim transaction fact table will have separate rows for each of these \nevents, but as discussed earlier, it doesn’t accumulate metrics across transactions; \ntrying to re-create the evolution of a workﬂ ow from these transactional events would \nbe a nightmare. Meanwhile, a classic accumulating snapshot doesn’t allow you to \nre-create the claim workﬂ ow at any arbitrary date in the past.\nAlternatively, you could add eff ective and expiration dates to the accumulating \nsnapshot. In this scenario, instead of destructively updating each row as changes \noccur, you add a new row that preserves the state of a claim for a span of time. \n",
      "page_number": 409
    },
    {
      "number": 45,
      "title": "Segment 45 (pages 419-433)",
      "start_page": 419,
      "end_page": 433,
      "detection_method": "topic_boundary",
      "content": "Insurance 395\nSimilar to a type 2 slowly changing dimension, the fact row includes the following \nadditional columns:\n \n■Snapshot start date\n \n■Snapshot end date (updated when a new row for a given claim is added)\n \n■Snapshot current ﬂ ag (updated when a new row is added)\nMost users are only interested in the current view provided by a classic accumu-\nlating snapshot; you can meet their needs by deﬁ ning a view that ﬁ lters the historical \nsnapshot rows based on the current ﬂ ag. The minority of users and reports who \nneed to look at the pipeline as of any arbitrary date in the past can do so by ﬁ ltering \non the snapshot start and end dates.\nThe timespan accumulating snapshot fact table is more complicated to main-\ntain than a standard accumulating snapshot, but the logic is similar. Where the \nclassic accumulating snapshot updates a row, the timespan snapshot updates \nthe administrative columns on the row formerly known as current, and inserts \na new row.\nPeriodic Instead of Accumulating Snapshot \nIn  cases where a claim is not so short-lived, such as with long-term disability or \nbodily injury claims that have a multiyear life span, you may represent the snapshot \nas a periodic snapshot rather than an accumulating snapshot. The grain of the peri-\nodic snapshot would be one row for every active claim at a regular snapshot interval, \nsuch as monthly. The facts would represent numeric, additive facts that occurred \nduring the period such as amount claimed, amount paid, and change in reserve. \n Policy/Claim Consolidated Periodic Snapshot\nWith  the fact tables designed thus far, you can deliver a robust perspective of the pol-\nicy and claim transactions, in addition to snapshots from both processes. However, \nthe business users are also interested in proﬁ t metrics. Although premium revenue \nand claim loss ﬁ nancial metrics could be derived by separately querying two fact \ntables and then combining the results set, you opt to go the next step in the spirit \nof ease of use and performance for this common drill-across requirement. \nYou can construct another fact table that brings together the premium revenue \nand claim loss metrics, as shown in Figure 16-10. This table has a reduced set \nof dimensions corresponding to the lowest level of granularity common to both \n\n\nChapter 16\n396\nprocesses. As discussed in Chapter 7: Accounting, this is a consolidated fact table \nbecause it combines data from multiple business processes. It is best to develop \nconsolidated fact tables after the base metrics have been delivered in separate atomic \ndimensional models.\nConsolidated Premium/Loss Fact\nMonth End Date Dimension\nPolicyholder Dimension\nCoverage Dimension\nPolicy Status Dimension\nCovered Item Dimension\nClaim Status Dimension\nAgent Dimension\nMonth End Snapshot Date Key (FK)\nPolicyholder Key (FK)\nCoverage Key (FK)\nCovered Item Key (FK)\nAgent Key (FK)\nPolicy Status Key (FK)\nClaim Status Key (FK)\nPolicy Number (DD)\nWritten Premium Revenue Dollar Amount\nEarned Premium Revenue Dollar Amount\nClaim Paid Dollar Amount\nClaim Collected Dollar Amount\nFigure 16-10: Policy/claim consolidated fact table.\n Factless Accident Events\nWe  earlier described factless fact tables as the collision of keys at a point in space \nand time. In the case of an automobile insurer, you can record literal collisions using \na factless fact table. In this situation, the fact table registers the many-to-many cor-\nrelations between the loss parties and loss items, or put in laymen’s terms, all the \ncorrelations between the people and vehicles involved in an accident.\nTwo new dimensions appear in the factless fact table shown in Figure 16-11. The \nloss party captures the individuals involved in the accident, whereas the loss party \nrole identiﬁ es them as passengers, witnesses, legal representation, or some other \ncapacity. As we did in Chapter 3: Retail Sales, we include a fact that is always valued \nat 1 to facilitate counting and aggregation. This factless fact table can represent \ncomplex accidents involving many individuals and vehicles because the number \nof involved parties with various roles is open-ended. When there is more than one \nclaimant or loss party associated with an accident, you can optionally treat these \ndimensions as multivalued dimensions using claimant group and loss party group \nbridge tables. This has the advantage that the grain of the fact table is preserved \nas one record per accident claim. Either schema variation could answer questions \nsuch as, “How many bodily injury claims did you handle where ABC Legal Partners \nrepresented the claimant and EZ-Dent-B-Gone body shop performed the  repair?”\n\n\nInsurance 397\nAccident Involvement Fact\nPolicyholder Dimension\nCoverage Dimension\nLoss Party Dimension\nCovered Item Dimension\nClaim Loss Date Dimension\nLoss Party Role Dimension\nClaim Loss Date Key (FK)\nPolicyholder Key (FK)\nCoverage Key (FK)\nCovered Item Key (FK)\nClaimant Key (FK)\nLoss Party Key (FK)\nLoss Party Role Key (FK)\nClaim Profile Key (FK)\nClaim Number (DD)\nPolicy Number (DD)\nAccident Involvement Count (=1)\nClaimant Dimension\nClaim Profile Dimension\nFigure 16-11: Factless fact table for accident involvements.\nCommon Dimensional Modeling Mistakes \nto Avoid\nAs  we close this ﬁ nal chapter on dimensional modeling techniques, we thought it \nwould be helpful to establish boundaries beyond which designers should not go. \nThus far in this book, we’ve presented concepts by positively stating dimensional \nmodeling best practices. Now rather than reiterating the to-dos, we focus on not-\nto-dos by elaborating on dimensional modeling techniques that should be avoided. \nWe’ve listed the not-to-dos in reverse order of importance; be aware, however, that \neven the less important mistakes can seriously compromise your DW/BI system.\nMistake 10: Place Text Attributes in a Fact Table\nThe process of creating a dimensional model is always a kind of triage. The numeric \nmeasurements delivered from an operational business process source belong in the \nfact table. The descriptive textual attributes comprising the context of the mea-\nsurements go in dimension tables. In nearly every case, if an attribute is used for \nconstraining and grouping, it belongs in a dimension table. Finally, you should make \na ﬁ eld-by-ﬁ eld decision about the leftover codes and pseudo-numeric items, placing \nthem in the fact table if they are more like measurements and used in calculations \nor in a dimension table if they are more like descriptions used for ﬁ ltering and \nlabeling. Don’t lose your nerve and leave true text, especially comment ﬁ elds, in \nthe fact table. You need to get these text attributes off  the main runway of the data \nwarehouse and into dimension tables.\n\n\nChapter 16\n398\nMistake 9: Limit Verbose Descriptors to Save Space\nYou might think you are being a conservative designer by keeping the size of the \ndimensions under control. However, in virtually every data warehouse, the dimen-\nsion tables are geometrically smaller than the fact tables. Having a 100 MB product \ndimension table is insigniﬁ cant if the fact table is one hundred or thousand times \nas large! Our job as designers of easy-to-use dimensional models is to supply as \nmuch verbose descriptive context in each dimension as possible. Make sure every \ncode is augmented with readable descriptive text. Remember the textual attributes \nin the dimension tables provide the browsing, constraining, or ﬁ ltering parameters \nin BI applications, as well as the content for the row and column headers in reports.\nMistake 8: Split Hierarchies into Multiple Dimensions\nA hierarchy is a cascaded series of many-to-one relationships. For example, many \nproducts roll up to a single brand; many brands roll up to a single category. If a \ndimension is expressed at the lowest level of granularity, such as product, then all \nthe higher levels of the hierarchy can be expressed as unique values in the product \nrow. Business users understand hierarchies. Your job is to present the hierarchies \nin the most natural and effi  cient manner in the eyes of the users, not in the eyes of \na data modeler who has focused his entire career on designing third normal form \nentity-relationship models for transaction processing systems.\nA ﬁ xed depth hierarchy belongs together in a single physical ﬂ at dimension \ntable, unless data volumes or velocity of change dictate otherwise. Resist the urge \nto snowﬂ ake a hierarchy by generating a set of progressively smaller subdimen-\nsion tables. Finally, if more than one rollup exists simultaneously for a dimension, \nin most cases it’s perfectly reasonable to include multiple hierarchies in the same \ndimension as long as the dimension has been deﬁ ned at the lowest possible grain \n(and the hierarchies are uniquely labeled).\nMistake 7: Ignore the Need to Track Dimension \nChanges\nContrary to popular belief, business users often want to understand the impact of \nchanges on at least a subset of the dimension tables’ attributes. It is unlikely users \nwill settle for dimension tables with attributes that always reﬂ ect the current state \nof the world. Three basic techniques track slowly moving attribute changes; don’t \nrely on type 1 exclusively. Likewise, if a group of attributes changes rapidly, you \ncan split a dimension to capture the more volatile attributes in a mini-dimension. \n\n\nInsurance 399\nMistake 6: Solve All Performance Problems with \nMore Hardware\nAggregates, or derived summary tables, are a cost-eff ective way to improve query \nperformance. Most BI tool vendors have explicit support for the use of aggregates. \nAdding expensive hardware should be done as part of a balanced program that \nincludes building aggregates, partitioning, creating indices, choosing query-effi  cient \nDBMS software, increasing real memory size, increasing CPU speed, and adding \nparallelism at the hardware level.\nMistake 5: Use Operational Keys to Join Dimensions \nand Facts\nNovice designers are sometimes too literal minded when designing the dimension \ntables’ primary keys that connect to the fact tables’ foreign keys. It is counterpro-\nductive to declare a suite of dimension attributes as the dimension table key and \nthen use them all as the basis of the physical join to the fact table. This includes \nthe unfortunate practice of declaring the dimension key to be the operational key, \nalong with an eff ective date. All types of ugly problems will eventually arise. The \ndimension’s operational or intelligent key should be replaced with a simple integer \nsurrogate key that is sequentially numbered from 1 to N, where N is the total number \nof rows in the dimension table. The date dimension is the sole exception to this rule.\nMistake 4: Neglect to Declare and Comply with the \nFact Grain\nAll dimensional designs should begin by articulating the business process that \ngenerates the numeric performance measurements. Second, the exact granularity \nof that data must be speciﬁ ed. Building fact tables at the most atomic, granular level \nwill gracefully resist the ad hoc attack. Third, surround these measurements with \ndimensions that are true to that grain. Staying true to the grain is a crucial step in \nthe design of a dimensional model. A subtle but serious design error is to add helpful \nfacts to a fact table, such as rows that describe totals for an extended timespan or a \nlarge geographic area. Although these extra facts are well known at the time of the \nindividual measurement and would seem to make some BI applications simpler, they \ncause havoc because all the automatic summations across dimensions overcount \nthese higher-level facts, producing incorrect results. Each diff erent measurement \ngrain demands its own fact table.\n\n\nChapter 16\n400\nMistake 3: Use a Report to Design the Dimensional \nModel\nA dimensional model has nothing to do with an intended report! Rather, it is a model \nof a measurement process. Numeric measurements form the basis of fact tables. \nThe dimensions appropriate for a given fact table are the context that describes the \ncircumstances of the measurements. A dimensional model is based solidly on \nthe physics of a measurement process and is quite independent of how a user chooses \nto deﬁ ne a report. A project team once confessed it had built several hundred fact \ntables to deliver order management data to its business users. It turned out each \nfact table had been constructed to address a speciﬁ c report request; the same data \nwas extracted many, many times to populate all these slightly diff erent fact tables. \nNot surprising, the team was struggling to update the databases within the nightly \nbatch window. Rather than designing a quagmire of report-centric schemas, the \nteam should have focused on the measurement processes. The users’ requirements \ncould have been handled with a well-designed schema for the atomic data along \nwith a handful (not hundreds) of performance-enhancing aggregations.\nMistake 2: Expect Users to Query Normalized \nAtomic Data\nThe lowest level data is always the most dimensional and should be the foundation of \na dimensional design. Data that has been aggregated in any way has been deprived \nof some of its dimensions. You can’t build a dimensional model with aggregated data \nand expect users and their BI tools to seamlessly drill down to third normal form \n(3NF) data for the atomic details. Normalized models may be helpful for preparing \nthe data in the ETL kitchen, but they should never be used for presenting the data \nto business users.\nMistake 1: Fail to Conform Facts and Dimensions \nThis ﬁ nal not-to-do should be presented as two separate mistakes because they \nare both so dangerous to a successful DW/BI design, but we’ve run out of mistake \nnumbers to assign, so they’re lumped into one.\nIt would be a shame to get this far and then build isolated data repository stove-\npipes. We refer to this as snatching defeat from the jaws of victory. If you have a \nnumeric measured fact, such as revenue, in two or more databases sourced from \ndiff erent underlying systems, then you need to take special care to ensure the techni-\ncal deﬁ nitions of these facts exactly match. If the deﬁ nitions do not exactly match, \nthen they shouldn’t both be referred to as revenue. This is conforming the facts.\n\n\nInsurance 401\nFinally, the single most important design technique in the dimensional modeling \narsenal is conforming dimensions. If two or more fact tables are associated with the \nsame dimension, you must be fanatical about making these dimensions identical or \ncarefully chosen subsets of each other. When you conform dimensions across fact \ntables, you can drill across separate data sources because the constraints and row \nheaders mean the same thing and match at the data level. Conformed dimensions \nare the secret sauce needed for building distributed DW/BI environments, adding \nunexpected new data sources to an existing warehouse, and making multiple incom-\npatible technologies function together harmoniously. Conformed dimensions also \nallow teams to be more agile because they’re not re-creating the wheel repeatedly; \nthis translates into a faster delivery of value to the business community.\nSummary\nIn this ﬁ nal case study we designed a series of insurance dimensional models rep-\nresenting the culmination of many important concepts developed throughout this \nbook. Hopefully, you now feel comfortable and conﬁ dent using the vocabulary and \ntools of a dimensional modeler.\nWith dimensional modeling mastered, in the next chapter we discuss all the other \nactivities that occur during the life of a successful DW/BI project. Before you go forth \nto be dimensional, it’s useful to have this holistic perspective and understanding, \neven if your job focus is limited to modeling.\n\n\nKimball DW/BI \nLifecycle Overview\nT\nhe gears shift rather dramatically in this chapter. Rather than focusing on \nKimball dimensional modeling techniques, we turn your attention to every-\nthing else that occurs during the course of a data warehouse/business intelligence \ndesign and implementation project. In this chapter, we’ll cover the life of a DW/BI \nproject from inception through ongoing maintenance, identifying best practices at \neach step, as well as potential vulnerabilities. More comprehensive coverage of the \nKimball Lifecycle is available in The Data Warehouse Lifecycle Toolkit, Second Edition \nby Ralph Kimball, Margy Ross, Warren Thornthwaite, Joy Mundy, and Bob Becker \n(Wiley, 2008). This chapter is a crash course drawn from the complete text, which \nweighs in at a hefty 600+ pages.\nYou may perceive this chapter’s content is only applicable to DW/BI project \nmanagers, but we feel diff erently. Implementing a DW/BI system requires tightly \nintegrated activities. We believe everyone on the project team, including the ana-\nlysts, architects, designers, and developers, needs a high-level understanding of the \ncomplete Lifecycle.\nThis chapter provides an overview of the entire Kimball Lifecycle approach; spe-\nciﬁ c recommendations regarding dimensional modeling and ETL tasks are deferred \nuntil subsequent chapters. We will dive into the collaborative modeling workshop \nprocess in Chapter 18: Dimensional Modeling Process and Tasks, then make a simi-\nlar plunge into ETL activities in Chapter 20: ETL System Design and Development \nProcess and Tasks.\nChapter 17 covers the following concepts:\n \n■Kimball Lifecycle orientation\n \n■DW/BI program/project planning and ongoing management\n \n■Tactics for collecting business requirements, including prioritization\n \n■Process for developing the technical architecture and selecting products\n17\n\n\nChapter 17\n404\n \n■Physical design considerations, including aggregation and indexing\n \n■BI application design and development activities\n \n■Recommendations for deployment, ongoing maintenance, and future growth\nLifecycle Roadmap\nWhen  driving to a place we’ve never been to before, most of us rely on a roadmap, \nalbeit displayed via a GPS. Similarly, a roadmap is extremely useful if we’re about \nto embark on the unfamiliar journey of data warehousing and business intelligence. \nThe authors of The Data Warehouse Lifecycle Toolkit drew on decades of experience \nto develop the Kimball Lifecycle approach. When we ﬁ rst introduced the Lifecycle \nin 1998, we referred to it as the Business Dimensional Lifecycle, a name that rein-\nforced our key tenets for data warehouse success: Focus on the business’s needs, \npresent dimensionally structured data to users, and tackle manageable, iterative \nprojects. In the 1990s, we were one of the few organizations emphasizing these \ncore principles, so the moniker diff erentiated our methods from others. We are still \nvery ﬁ rmly wed to these principles, which have since become generally-accepted \nindustry best practices, but we renamed our approach to be the Kimball Lifecycle \nbecause that’s how most people refer to it.\nThe  overall Kimball Lifecycle approach is encapsulated in Figure 17-1. The dia-\ngram illustrates task sequence, dependency, and concurrency. It serves as a roadmap \nto help teams do the right thing at the right time. The diagram does not reﬂ ect an \nabsolute timeline; although the boxes are equally wide, there’s a vast diff erence in \nthe time and eff ort required for each major activity.\nProgram/\nProject\nPlanning\nProgram/Project Management\nBusiness\nRequirements\nDefinition\nTechnical\nArchitecture\nDesign\nPhysical\nDesign\nBI\nApplication\nDesign\nBI\nApplication\nDevelopment\nETL\nDesign &\nDevelopment\nDeployment\nMaintenance\nDimensional\nModeling\nProduct\nSelection &\nInstallation\nGrowth\nFigure 17-1: Kimball Lifecycle diagram.\n\n\nKimball DW/BI Lifecycle Overview 405\nNOTE \nGiven the recent industry focus on agile methodologies, we want to \nremind readers about the discussion of the topic in Chapter 1: Data Warehousing, \nBusiness Intelligence, and Dimensional Modeling Primer. The Kimball Lifecycle \napproach and agile methodologies share some common doctrines: Focus on busi-\nness value, collaborate with the business, and develop incrementally. However, \nwe also feel strongly that DW/BI system design and development needs to be built \non a solid data architecture and governance foundation, driven by the bus archi-\ntecture. We also believe most situations warrant the bundling of multiple agile \n“deliverables” into a more full-function release before being broadly deployed to \nthe general business community.\nRoadmap Mile Markers\nBefore  diving into speciﬁ cs, take a moment to orient yourself to the roadmap. The \nLifecycle begins with program/project planning, as you would expect. This module \nassesses the organization’s readiness for a DW/BI initiative, establishes the prelimi-\nnary scope and justiﬁ cation, obtains resources, and launches the program/project. \nOngoing project management serves as a foundation to keep the remaining activi-\nties on track.\nThe  second major task in Figure 17-1 focuses on business requirements deﬁ ni-\ntion. There’s a two-way arrow between program/project planning and the business \nrequirements deﬁ nition due to the interplay between these activities. Aligning the \nDW/BI initiative with business requirements is absolutely crucial. Best-of-breed \ntechnologies won’t salvage a DW/BI environment that fails to focus on the business. \nBusiness users and their requirements have an impact on almost every design and \nimplementation decision made during the course of a DW/BI project. In Figure 17-1’s \nroadmap, this is reﬂ ected by the three parallel tracks that follow.\nThe  top track of Figure 17-1 deals with technology. Technical architecture design \nestablishes the overall framework to support the integration of multiple technolo-\ngies. Using the capabilities identiﬁ ed in the architecture design as a shopping list, \nyou then evaluate and select speciﬁ c products. Notice that product selection is not \nthe ﬁ rst box on the roadmap. One of the most frequent mistakes made by novice \nteams is to select products without a clear understanding of what they’re trying to \naccomplish. This is akin to grabbing a hammer whether you need to pound a nail \nor tighten a screw.\nThe  middle track emanating from business requirements deﬁ nition focuses on \ndata. It begins by translating the requirements into a dimensional model, as we’ve \nbeen practicing. The dimensional model is then transformed into a physical struc-\nture. The focus is on performance tuning strategies, such as aggregation, indexing, \nand partitioning, during the physical design. Last but not least, the ETL system is \n\n\nChapter 17\n406\ndesigned and developed. As mentioned earlier, the equally sized boxes don’t rep-\nresent equally sized eff orts; this becomes obvious with the workload diff erential \nbetween the physical design and the demanding ETL-centric activities.\nThe  ﬁ nal set of tasks spawned by the business requirements is the design and \ndevelopment of the BI applications. The DW/BI project isn’t done when you deliver \ndata. BI applications, in the form of parameter-driven templates and analyses, will \nsatisfy a large percentage of the business users’ analytic needs.\nThe technology, data, and BI application tracks, along with a healthy dose of \neducation and support, converge for a well-orchestrated deployment. From there, \non-going maintenance is needed to ensure the DW/BI system remains healthy. \nFinally, you handle future growth by initiating subsequent projects, each return-\ning to the beginning of the Lifecycle all over again.\nNow that you have a high-level understanding of the overall roadmap, we’ll \ndescribe each of the boxes in Figure 17-1 in more detail.\nLifecycle Launch Activities\nThe following sections outline best practices, and pitfalls to avoid, as you launch \na DW/BI project.\nProgram/Project Planning and Management\nNot surprisingly, the DW/BI initiative begins with a series of program and project \nplanning activities. \nAssessing Readiness\nBefore  moving ahead with a DW/BI eff ort, it is prudent to take a moment to assess \nthe organization’s readiness to proceed. Based on our cumulative experience from \nhundreds of client engagements, three factors diff erentiate projects that were pre-\ndominantly smooth sailing versus those that entailed a constant struggle. These \nfactors are leading indicators of DW/BI success; we’ll describe the characteristics \nin rank order of importance.\nThe  most critical readiness factor is to have a strong executive business sponsor. \nBusiness sponsors should have a clear vision for the DW/BI system’s potential impact \non the organization. Optimally, business sponsors have a track record of success \nwith other internal initiatives. They should be politically astute leaders who can \nconvince their peers to support the eff ort. It’s a much riskier scenario if the chief \ninformation offi  cer (CIO) is the designated sponsor; we much prefer visible com-\nmitment from a business partner-in-crime instead.\n\n\nKimball DW/BI Lifecycle Overview 407\nThe  second readiness factor is having a strong, compelling business motivation for \ntackling the DW/BI initiative. This factor often goes hand in hand with sponsorship. \nThe DW/BI project needs to solve critical business problems to garner the resources \nrequired for a successful launch and healthy lifespan. Compelling motivation typi-\ncally creates a sense of urgency, whether the motivation is from external sources, \nsuch as competitive factors, or internal sources, such as the inability to analyze \ncross-organization performance following acquisitions.\nThe  third factor when assessing readiness is feasibility. There are several aspects \nof feasibility, including technical and resource feasibility, but data feasibility is the \nmost crucial. Are you collecting real data in real operational source systems to sup-\nport the business requirements? Data feasibility is a major concern because there \nis no short-term ﬁ x if you’re not already collecting reasonably clean source data at \nthe right granularity.\nScoping and Justiﬁ cation\nWhen  you’re comfortable with the organization’s readiness, it’s time to put boundar-\nies around an initial project. Scoping requires the joint input of the IT organization \nand business management. The scope of a DW/BI project should be both mean-\ningful to the business organization and manageable for the IT organization. You \nshould initially tackle projects that focus on data from a single business process; \nsave the more challenging, cross-process projects for a later phase. Remember to \navoid the Law of Too when scoping—too brief of a timeline for a project with too \nmany source systems and too many users in too many locations with too diverse \nanalytic requirements.\nJustiﬁ cation requires an estimation of the beneﬁ ts and costs associated with the \nDW/BI initiative. Hopefully, the anticipated beneﬁ ts grossly outweigh the costs. IT \nusually is responsible for deriving the expenses. DW/BI systems tend to expand \nrapidly, so be sure the estimates allow room for short-term growth. Unlike opera-\ntional system development where resource requirements tail off  after production, \nongoing DW/BI support needs will not decline appreciably over time.\nThe  business community should have prime responsibility for determining the \nanticipated ﬁ nancial beneﬁ ts. DW/BI environments typically are justiﬁ ed based on \nincreased revenue or proﬁ t opportunities rather than merely focusing on expense \nreduction. Delivering “a single version of the truth” or “ﬂ exible access to information” \nisn’t suffi  cient ﬁ nancial justiﬁ cation. You need to peel back the layers to determine \nthe quantiﬁ able impact of improved decision making made possible by these sound \nbites. If you are struggling with justiﬁ cation, this is likely a symptom that the initia-\ntive is focused on the wrong business sponsor or  problem.\n\n\nChapter 17\n408\nStafﬁ ng\nDW/BI  projects require the integration of a cross-functional team with resources \nfrom both the business and IT communities. It is common for the same person to \nﬁ ll multiple roles on the team; the assignment of named resources to roles depends \non the project’s magnitude and scope, as well as the individual’s availability, capac-\nity, and experience.\nFrom the business side of the house, we’ll need representatives to ﬁ ll the fol-\nlowing roles:\n \n■Business sponsor.  The sponsor is the DW/BI system’s ultimate client, as well \nas its strongest advocate. Sponsorship sometimes takes the form of an execu-\ntive steering committee, especially for cross-enterprise initiatives.\n \n■Business driver.  In a large organization, the sponsor may be too far removed \nor inaccessible to the project team. In this case, the sponsor sometimes del-\negates less strategic DW/BI responsibilities to a middle manager in the orga-\nnization. This driver should possess the same characteristics as the sponsor.\n \n■Business lead.  The business project lead is a well-respected person who is \nhighly involved in the project, communicating with the project manager on \na daily basis. Sometimes the business driver ﬁ lls this role.\n \n■Business users.  Optimally, the business users are the enthusiastic fans of the \nDW/BI environment. You need to involve them early and often, beginning \nwith the project scope and business requirements. From there, you must \nﬁ nd creative ways to maintain their interest and involvement throughout the \nproject. Remember, business user involvement is critical to DW/BI acceptance. \nWithout business users, the DW/BI system is a technical exercise in futility.\nSeveral  positions are staff ed from either the business or IT organizations. These \nstraddlers can be technical resources who understand the business or business \nresources who understand technology:\n \n■Business analyst.  This person is responsible for determining the business \nneeds and translating them into architectural, data, and BI application \nrequirements.\n \n■Data steward.  This subject matter expert is often the current go-to resource \nfor ad hoc analysis. They understand what the data means, how it is used, \nand where data inconsistencies are lurking. Given the need for organizational \nconsensus around core dimensional data, this can be a politically challenging \nrole, as we described in Chapter 4: Inventory.\n \n■BI application designer/developer.  BI application resources are responsible \nfor designing and developing the starter set of analytic templates, as well as \nproviding ongoing BI application support.\n\n\nKimball DW/BI Lifecycle Overview 409\nThe following roles are typically staff ed from the IT organization:\n \n■Project manager.  The project manager is a critical position. This person \nshould be comfortable with and respected by business executives, as well \nas technical resources. The project manager’s communication and project \nmanagement skills must be stellar.\n \n■Technical architect.  The architect is responsible for the overall technical \narchitecture. This person develops the plan that ties together the required \ntechnical functionality and helps evaluate products on the basis of the overall \narchitecture.\n \n■Data architect/modeler.  This resource likely comes from a transactional \ndata background with heavy emphasis on normalization. This person should \nembrace dimensional modeling concepts and be empathetic to the require-\nments of the business rather than focused strictly on saving space or reducing \nthe ETL workload.\n \n■Database administrator.  Like the data modeler, the database administrator \nmust be willing to set aside some traditional database administration truisms, \nsuch as having only one index on a relational table.\n \n■Metadata coordinator.  This person helps establish the metadata repository \nstrategy and ensures that the appropriate metadata is collected, managed, \nand disseminated. \n \n■ETL architect/designer.  This role is responsible for designing the ETL envi-\nronment and processes.\n \n■ETL developer.  Based on direction from the ETL architect/designer, the devel-\noper builds and automates the processes, likely using an ETL tool.\nWe want to point out again that this is a list of roles, not people. Especially in \nsmaller shops, talented individuals will ﬁ ll many of these roles simultaneously.\nDeveloping and Maintaining the Plan\nThe  DW/BI project plan identiﬁ es all the necessary Lifecycle tasks. A detailed task \nlist is available on the Kimball Group website at www.kimballgroup.com; check out \nthe Tools & Utilities tab under The Data Warehouse Lifecycle Toolkit, Second Edition \nbook title.\nAny good project manager knows key team members should develop estimates \nof the eff ort required for their tasks; the project manager can’t dictate the amount of \ntime allowed and expect conformance. The project plan should identify acceptance \ncheckpoints with business representatives after every major roadmap milestone and \ndeliverable to ensure the project remains on track.\nDW/BI projects demand broad communication. Although project manag-\ners typically excel at intra-team communications, they should also establish \n\n\nChapter 17\n410\na communication strategy describing the frequency, forum, and key messaging \nfor other constituencies, including the business sponsors, business community, \nand other IT colleagues.\nFinally, DW/BI projects are vulnerable to scope creep largely due to a strong \nneed to satisfy business users’ requirements. You have several options when con-\nfronted with changes: Increase the scope (by adding time, resources, or budget), \nplay the zero-sum game (by retaining the original scope by giving up something \nin exchange), or say “no” (without actually saying “no” by handling the change as \nan enhancement request). The most important thing about scope decisions is that \nthey shouldn’t be made in an IT vacuum. The right answer depends on the situa-\ntion. Now is the time to leverage the partnership with the business to arrive at an \nanswer that everyone can live with.\nBusiness Requirements Deﬁ nition\nCollaborating  with business users to understand their requirements and ensure \ntheir buy-in is absolutely essential to successful data warehousing and business \nintelligence. This section focuses on back-to-basics techniques for gathering busi-\nness requirements.\nRequirements Preplanning\nBefore  sitting down with business representatives to collect their requirements, we \nsuggest the following to ensure productive sessions:\nChoose the Forum\nBusiness  user requirements sessions are typically interwoven with source system \nexpert data discovery sessions. This dual-pronged approach gives you insight into \nthe needs of the business with the realities of the data. However, you don’t ask \nbusiness representatives about the granularity or dimensionality of their critical \ndata. You need to talk to them about what they do, why they do it, how they make \ndecisions, and how they hope to make decisions in the future. Like organizational \ntherapy, you’re trying to detect the issues and opportunities.\nThere are two primary techniques for gathering requirements: interviews or facili-\ntated sessions. Both have their advantages and disadvantages. Interviews encourage \nindividual participation and are also easier to schedule. Facilitated sessions may \nreduce the elapsed time to gather requirements but require more time commitment \nfrom each participant.\nBased on our experience, surveys are not a reasonable tool for gathering require-\nments because they are ﬂ at and two-dimensional. The self-selected respondents \nanswer only the questions we’ve thought to ask in advance; there’s no option to probe \nmore deeply. In addition, survey instruments do not help forge the bond between \nbusiness users and the DW/BI initiative that we strive for.\n",
      "page_number": 419
    },
    {
      "number": 46,
      "title": "Segment 46 (pages 434-445)",
      "start_page": 434,
      "end_page": 445,
      "detection_method": "topic_boundary",
      "content": "Kimball DW/BI Lifecycle Overview 411\nWe generally use a hybrid approach with interviews to gather the details and then \nfacilitation to bring the group to consensus. Although we’ll describe this hybrid \napproach in more detail, much of the discussion applies to pure facilitation as well. \nThe requirements gathering forum choice depends on the team’s skills, the organi-\nzation’s culture, and what the business users have already been subjected to. One \nsize deﬁ nitely does not ﬁ t all.\nIdentify and Prepare the Requirements Team\nRegardless  of the approach, you need to identify and prepare the involved project \nteam members. If you’re doing interviews, you need to identify a lead interviewer \nwhose primary responsibility is to ask great open-ended questions. Meanwhile, the \ninterview scribe takes copious notes. Although a tape recorder may provide more \ncomplete coverage of each interview, we don’t use one because it changes the meet-\ning dynamics. Our preference is to have a second person in the room with another \nbrain and a set of eyes and ears rather than relying on technology. We often invite \none or two additional project members (depending on the number of interviewees) \nas observers, so they can hear the users’ input directly.\nBefore sitting down with business users, you need to make sure you’re approach-\ning the sessions with the right mindset. Don’t presume you already know it all; \nyou will deﬁ nitely learn more about the business during the sessions. On the other \nhand, you should do some homework by researching available sources, such as the \nannual report, website, and internal organization chart.\nBecause the key to getting the right answers is asking the right questions, we \nrecommend drafting questionnaires. The questionnaire should not be viewed as \na script; it is a tool to organize your thoughts and serve as a fallback device in \ncase your mind goes blank during the session. The questionnaire will be updated \nthroughout the interview process as the team becomes better versed in the busi-\nness’s subject  matter.\nSelect, Schedule, and Prepare Business Representatives\nIf  this is your ﬁ rst foray into DW/BI, or an eff ort to develop a cohesive strategy for \ndealing with existing data stovepipes, you should talk to business people represent-\ning a reasonable horizontal spectrum of the organization. This coverage is critical \nto formulating the enterprise data warehouse bus matrix blueprint. You need to \nunderstand the common data and vocabulary across core business functions to build \nan extensible environment.\nWithin the target user community, you should cover the organization verti-\ncally. DW/BI project teams naturally gravitate toward the business’s power analysts. \nAlthough their insight is valuable, you can’t ignore senior executives and middle \nmanagement. Otherwise, you are vulnerable to being overly focused on the tactical \nhere-and-now and lose sight of the group’s strategic direction.\n\n\nChapter 17\n412\nScheduling the business representatives can be the most onerous requirements \ntask; be especially nice to the department’s administrative assistants. We prefer \nto meet with executives individually. Meeting with a homogeneous group of two to \nthree people is appropriate for those lower on the organization chart. Allow 1 hour \nfor individual meetings and 1½ hours for small groups. The scheduler needs to \nallow ½ hour between meetings for debrieﬁ ng and other necessities. Interviewing \nis extremely taxing due to the focus required. Consequently, don’t schedule more \nthan three to four sessions in a day.\nWhen it comes to preparing the interviewees, the business sponsor should \ncommunicate with them, stressing their commitment to the eff ort and the impor-\ntance of everyone’s participation. The interviewees should be asked to bring copies \nof their key reports and analyses to the session. This communication disseminates \na consistent message about the project, plus conveys the business’s ownership of the \ninitiative. Occasionally interviewees will be reluctant to bring the business’s “crown \njewel” reports to the meeting, especially with an outside consultant. However, almost \nalways we have found these people will enthusiastically race back to their offi  ces at \nthe end of the interview to bring back those same reports.\n Collecting Business Requirements\nIt’s time to sit down face-to-face to gather the business’s requirements. The \nprocess usually ﬂ ows from an introduction through structured questioning to a \nﬁ nal wrap-up.\nLaunch\nResponsibility  for introducing the session should be established prior to gathering in \na conference room. The designated kickoff  person should script the primary talking \npoints for the ﬁ rst few minutes when the tone of the interview meeting is set. The \nintroduction should convey a crisp, business-centric message and not ramble with \nhardware, software, and other technical jargon.\nInterview Flow\nThe  objective of an interview is to get business users to talk about what they do and \nwhy they do it. A simple, nonthreatening place to begin is to ask about job responsi-\nbilities and organizational ﬁ t. This is a lob-ball that interviewees can easily respond \nto. From there, you typically ask about their key performance metrics. Determining \nhow they track progress and success translates directly into the dimensional model; \nthey’re telling you about their key business processes and facts without you asking \nthose questions directly.\nIf you meet with a person who has more hands-on data experience, you should \nprobe to better understand the business’s dimensionality. Questions like “How do \nyou distinguish between products (or agents, providers, or facilities)?” or “How \n\n\nKimball DW/BI Lifecycle Overview 413\ndo you naturally categorize products?” help identify key dimension attributes and \nhierarchies.\nIf the interviewee is more analytic, ask about the types of analysis currently \ngenerated. Understanding the nature of these analyses and whether they are ad \nhoc or standardized provides input into the BI tool requirements, as well as the \nBI application design process. Hopefully, the interviewee brought along copies of \nkey spreadsheets and reports. Rather than stashing them in a folder, it is helpful to \nunderstand how the interviewee uses the analysis today, as well as opportunities for \nimprovement. Contrary to the advice of some industry pundits, you cannot design \nan extensible analytic environment merely by getting users to agree on their top ﬁ ve \nreports. The users’ questions are bound to change; consequently, you must resist \nthe temptation to narrow your design focus to a supposed top ﬁ ve.\nIf you meet with business executives, don’t dive into these tactical details. Instead, \nask them about their vision for better leveraging information throughout the orga-\nnization. Perhaps the project team is envisioning a totally ad hoc environment, \nwhereas business management is more interested in the delivery of standardized \nanalyses. You need to ensure the DW/BI deliverable matches the business demand \nand expectations.\nAsk each interviewee about the impact of improved access to information. You \nlikely already received preliminary project funding, but it never hurts to capture \nmore potential, quantiﬁ able beneﬁ ts.\nWrap-Up\nAs the interview is coming to a conclusion, ask each interviewee about their suc-\ncess criteria for the project. Of course, each criterion should be measurable. “Easy \nto use” and “fast” mean something diff erent to everyone, so the interviewees need to \narticulate speciﬁ cs, such as their expectations regarding the amount of training \nrequired to run predeﬁ ned BI reports.\nAt this point, always make a broad disclaimer. The interviewees must understand \nthat just because you discussed a capability in the meeting doesn’t guarantee it’ll \nbe included in the ﬁ rst phase of the project. Thank interviewees for their brilliant \ninsights, and let them know what’s happening next and what their involvement will be.\nConducting Data-Centric Interviews\nWhile  we’re focused on understanding the business’s requirements, it is helpful to \nintersperse sessions with the source system data gurus or subject matter experts \nto evaluate the feasibility of supporting the business needs. These data-focused \ninterviews are quite diff erent from the ones just described. The goal is to ascer-\ntain whether the necessary core data exists before momentum builds behind the \nrequirements. In these data-centric interviews, you may go so far as to ask for some \ninitial data proﬁ ling results, such as domain values and counts of a few critical \n\n\nChapter 17\n414\ndata ﬁ elds, to be provided subsequently, just to ensure you are not standing on \nquicksand. A more complete data audit will occur during the dimensional modeling \nprocess. Try to learn enough at this point to manage the organization’s expectations \nappropriately.\n Documenting Requirements\nImmediately  following the interview, the interview team should debrief. You must \nensure everyone is on the same page about what was learned. It is also helpful if \neveryone reviews their notes shortly after the session to ﬁ ll in gaps while the inter-\nview is still fresh. Abbreviations and partial sentences in the notes become incom-\nprehensible after a few days! Likewise, examine the reports gathered to gain further \ninsight into the dimensionality that must be supported in the data warehouse.\nAt this point, it is time to document what you’ve heard. Although documentation \nis everyone’s least favorite activity, it is critical for both user validation and project \nteam reference materials. There are two potential levels of documentation resulting \nfrom the requirements process. The ﬁ rst is to write up each individual interview; \nthis activity is time-consuming because the write-up should not be merely a stream-\nof-consciousness transcript but should make sense to someone who wasn’t in the \ninterview. The more critical documentation is a consolidated ﬁ ndings document. \nThis document is organized around key business processes. Because you tackle \nDW/BI projects on a process-by-process basis, it is appropriate to structure the \nbusiness’s requirements into the same buckets that will, in turn, become imple-\nmentation eff orts.\nWhen writing up the ﬁ ndings document, you should begin with an executive \nsummary, followed by a project overview covering the process used and participants \ninvolved. The bulk of the document centers on the business processes; for each \nprocess, describe why business users want to analyze the process’s performance met-\nrics, what capabilities they want, their current limitations, and potential beneﬁ ts or \nimpact. Commentary about the feasibility of tackling each process is also important.\nAs described in Chapter 4 and illustrated in Figure 4-11, the processes are some-\ntimes unveiled in an opportunity/stakeholder matrix to convey the impact across \nthe organization. In this case, the rows of the opportunity matrix identify business \nprocesses, just like a bus matrix. However, in the opportunity matrix, the columns \nidentify the organizational groups or functions. Surprisingly, this matrix is usually \nquite dense because many groups want access to the same core performance  metrics.\nPrioritizing Requirements\nThe  consolidated ﬁ ndings document serves as the basis for presentations back to \nsenior management and other requirements participants. Inevitably you uncov-\nered more than can be tackled in a single iteration, so you need to prioritize. As \ndiscussed with project scope, don’t make this decision in a vacuum; you need to \n\n\nKimball DW/BI Lifecycle Overview 415\nleverage (or foster) your partnership with the business community to establish \nappropriate priorities.\nThe requirements wrap-up presentation is positioned as a ﬁ ndings review and \nprioritization meeting. Participants include senior business representatives (who \noptimally participated in the interviews), as well as the DW/BI manager and other \nsenior IT management. The session begins with an overview of each identiﬁ ed \nbusiness process. You want everyone in the room to have a common understanding \nof the opportunities. Also review the opportunity/stakeholder matrix, as well as a \nsimpliﬁ ed bus matrix.\nAfter the ﬁ ndings have been presented, it is time to prioritize using the prioriti-\nzation grid, illustrated in Figure 17-2. The grid’s vertical axis refers to the potential \nimpact or value to the business. The horizontal axis conveys feasibility. Each of the \nﬁ nding’s business process themes is placed on the grid based on the representatives’ \ncomposite agreement regarding impact and feasibility. It’s visually obvious where \nyou should begin; projects that warrant immediate attention are located in the \nupper-right corner because they’re high-impact projects, as well as highly feasible. \nProjects in the lower-left cell should be avoided like the plague; they’re missions \nimpossible that do little for the business. Likewise, projects in the lower-right cell \ndon’t justify short-term attention, although project teams sometimes gravitate here \nbecause these projects are doable but not very crucial. Finally, projects in the upper-\nleft cell represent meaningful opportunities. These projects have large potential \nbusiness payback but are currently infeasible. While the DW/BI project team focuses \non projects in the shaded upper-right corner, other IT teams should address the \ncurrent feasibility limitations of those in the upper left.\nMed\nFeasibility\nPotential\nBusiness\nImpact\nLow\nHigh\nHigh\nMed\nBP3\nBP1\nBP6\nBP4\nBP2\nBP5\nLow\nFigure 17-2: Prioritization grid based on business impact and feasibility.\n\n\nChapter 17\n416\nLifecycle Technology Track\nOn the Kimball Lifecycle roadmap in Figure 17-1, the business requirements deﬁ -\nnition is followed immediately by three concurrent tracks focused on technology, \ndata, and BI applications, respectively. In the next several sections we’ll zero in on \nthe technology track.\nTechnical Architecture Design\nMuch  like a blueprint for a new home, the technical architecture is the blueprint for \nthe DW/BI environment’s technical services and infrastructure. As the enterprise \ndata warehouse bus architecture introduced in Chapter 4 supports data integra-\ntion, the architecture plan is an organizing framework to support the integration \nof technologies and applications.\nLike housing blueprints, the technical architecture consists of a series of models \nthat unveil greater detail regarding each major component. In both situations, the \narchitecture enables you to catch problems on paper (such as having the dish-\nwasher too far from the sink) and minimize mid-project surprises. It supports the \ncoordination of parallel eff orts while speeding development through the reuse of \nmodular components. The architecture identiﬁ es immediately required components \nversus those that will be incorporated at a later date (such as the deck and screened \nporch). Most important, the architecture serves as a communication tool. Home \nconstruction blueprints enable the architect, general contractor, subcontractors, \nand homeowner to communicate from a common document. Likewise, the DW/BI \ntechnical architecture supports communication regarding a consistent set of techni-\ncal requirements within the team, upward to management, and outward to vendors.\nIn Chapter 1, we discussed several major components of the architecture, includ-\ning ETL and BI services. In this section, we focus on the process of creating the \narchitecture design.\nDW/BI teams typically approach the architecture design process from opposite \nends of the spectrum. Some teams simply don’t understand the beneﬁ ts of an archi-\ntecture and feel that the topic and tasks are too nebulous. They’re so focused on \ndelivery that the architecture feels like a distraction and impediment to progress, \nso they opt to bypass architecture design. Instead, they piece together the technical \ncomponents required for the ﬁ rst iteration with chewing gum and bailing wire, but \nthe integration and interfaces get taxed as more data, more users, or more func-\ntionality are added. Eventually, these teams often end up rebuilding because the \nnon-architected structure couldn’t withstand the stresses. At the other extreme, \nsome teams want to invest two years designing the architecture while forgetting \nthat the primary purpose of a DW/BI environment is to solve business problems, \nnot address any plausible (and not so plausible) technical challenge.\n\n\nKimball DW/BI Lifecycle Overview 417\nNeither end of the spectrum is healthy; the most appropriate response lies \nsomewhere in the middle. We’ve identiﬁ ed the following eight-step process to help \nnavigate these architectural design waters. Every DW/BI system has a technical \narchitecture; the question is whether it is planned and explicit or merely implicit.\nEstablish an Architecture Task Force\nIt  is useful to create a small task force of two to three people focused on architecture \ndesign. Typically, it is the technical architect, along with the ETL architect/designer \nand BI application architect/designer who ensure both back room and front room \nrepresentation.\nCollect Architecture-Related Requirements\nAs  illustrated in Figure 17-1, deﬁ ning the technical architecture is not the ﬁ rst box \nin the Lifecycle diagram. The architecture is created to support business needs; it’s \nnot meant to be an excuse to purchase the latest, greatest products. Consequently, \nkey input into the design process should come from the business requirements \ndeﬁ nition. However, you should listen to the business’s requirements with a slightly \ndiff erent ﬁ lter to drive the architecture design. The primary focus is uncovering the \narchitectural implications associated with the business’s needs. Listen closely for \ntiming, availability, and performance requirements.\nYou should also conduct additional interviews within the IT organization. These \nare technology-focused sessions to understand current standards, planned techni-\ncal directions, and nonnegotiable boundaries. In addition, you should uncover les-\nsons learned from prior information delivery projects, as well as the organization’s \nwillingness to accommodate operational change on behalf of the DW/BI initiative, \nsuch as identifying updated transactions in the source  system.\nDocument Architecture Requirements\nAfter  leveraging the business requirements process and conducting supplemental IT \ninterviews, you need to document your ﬁ ndings. We recommend using a simplistic \ntabular format, just listing each business requirement impacting the architecture, \nalong with a laundry list of architectural implications. For example, if there is a need \nto deliver global sales performance data on a nightly basis, the technical implica-\ntions might include 24/7 worldwide availability, data mirroring for loads, robust \nmetadata to support global access, adequate network bandwidth, and suffi  cient ETL \nhorsepower to handle the complex integration of operational  data.\nCreate the Architecture Model\nAfter  the architecture requirements have been documented, you should begin for-\nmulating models to support the identiﬁ ed needs. At this point, the architecture \nteam often sequesters itself in a conference room for several days of heavy thinking. \n\n\nChapter 17\n418\nThe architecture requirements are grouped into major components, such as ETL, BI, \nmetadata, and infrastructure. From there the team drafts and reﬁ nes the high-level \narchitectural model. This drawing is similar to the front elevation page on housing \nblueprints. It illustrates what the architecture will look like from the street, but it \ncan be dangerously simplistic because signiﬁ cant details are embedded in the pages \nthat follow.\nDetermine Architecture Implementation Phases\nLike  the homeowner’s dream house, you likely can’t implement all aspects of the \ntechnical architecture at once. Some are nonnegotiable mandatory capabilities, \nwhereas others are nice-to-haves. Again, refer back to the business requirements \nto establish architecture priorities because you must minimally provide the archi-\ntectural elements needed to deliver the initial project.\nDesign and Specify the Subsystems\nA  large percentage of the needed functionality will likely be met by the major tool \nvendor’s standard off erings, but there are always a few subsystems that may not \nbe found in off -the-shelf products. You must deﬁ ne these subsystems in enough \ndetail, so either someone can build it for you or you can evaluate products against \nyour needs.\nCreate the Architecture Plan\nThe  technical architecture needs to be documented, including the planned imple-\nmentation phases, for those who were not sequestered in the conference room. The \ntechnical architecture plan document should include adequate detail so skilled \nprofessionals can proceed with construction of the framework, much like carpen-\nters frame a house based on the blueprint. However, it doesn’t typically reference \nspeciﬁ c products, except those already in-house.\nReview and Finalize the Technical Architecture\nEventually we come full circle with the architecture design process. The architecture \ntask force needs to communicate the architecture plan at varying levels of detail \nto the project team, IT colleagues, and business leads. Following the review, docu-\nmentation should be updated and put to use immediately in the product selection \nprocess.\nProduct Selection and Installation\nIn many ways the architecture plan is similar to a shopping list for selecting products \nthat ﬁ t into the plan’s framework. The following six tasks associated with DW/BI \nproduct selection are quite similar to any technology selection.\n\n\nKimball DW/BI Lifecycle Overview 419\nUnderstand the Corporate Purchasing Process\nThe ﬁ rst step before selecting new products is to understand the internal hardware \nand software purchase processes.\nDevelop a Product Evaluation Matrix\nUsing  the architecture plan as a starting point, a spreadsheet-based evaluation \nmatrix should be developed that identiﬁ es the evaluation criteria, along with weight-\ning factors to indicate importance; the more speciﬁ c the criteria, the better. If the \ncriteria are too vague or generic, every vendor will say they can satisfy your needs.\nConduct Market Research\nTo  become informed buyers when selecting products, you should do market research \nto better understand the players and their off erings. A request for proposal (RFP) is \na classic product evaluation tool. Although some organizations have no choice about \ntheir use, you should avoid this technique, if possible. Constructing the RFP and \nevaluating responses are tremendously time-consuming for the team. Meanwhile, \nvendors are motivated to respond to the questions in the most positive light, so the \nresponse evaluation is often more of a beauty contest. In the end, the value of \nthe expenditure may not warrant the eff ort.\nEvaluate a Short List of Options\nDespite the plethora of products available in the market, usually only a small number \nof vendors can meet both functionality and technical requirements. By comparing \npreliminary scores from the evaluation matrix, you can focus on a narrow list of \nvendors and disqualify the rest. After dealing with a limited number of vendors, you \ncan begin the detailed evaluations. Business representatives should be involved in \nthis process if you’re evaluating BI tools. As evaluators, you should drive the process \nrather than allow the vendors to do the driving, sharing relevant information from \nthe architecture plan, so the sessions focus on your needs rather than on product \nbells and whistles. Be sure to talk with vendor references, both those formally pro-\nvided and those elicited from your informal network.\nIf Necessary, Conduct a Prototype\nAfter  performing the detailed evaluations, sometimes a clear winner bubbles to the \ntop, often based on the team’s prior experience or relationships. In other cases, \nthe leader emerges due to existing corporate commitments such as site licenses or \nlegacy hardware purchases. In either situation, when a sole candidate emerges as the \nwinner, you can bypass the prototype step (and the associated investment in both \ntime and money). If no vendor is the apparent winner, you should conduct a prototype \nwith no more than two products. Again, take charge of the process by developing a \nlimited yet realistic business case study.\n\n\nChapter 17\n420\nSelect Product, Install on Trial, and Negotiate\nIt is time to select a product. Rather than immediately signing on the dotted line, \npreserve your negotiating power by making a private, not public, commitment to a \nsingle vendor. Instead of informing the vendor that you’re completely sold, embark \non a trial period where you have the opportunity to put the product to real use in \nyour environment. It takes signiﬁ cant energy to install a product, get trained, and \nbegin using it, so you should walk down this path only with the vendor you fully \nintend to buy from; a trial should not be pursued as another tire-kicking exercise. \nAs the trial draws to a close, you have the opportunity to negotiate a purchase that’s \nbeneﬁ cial to all parties involved.\nLifecycle Data Track\nIn the Figure 17-1 Kimball Lifecycle diagram, the middle track following the busi-\nness requirements deﬁ nition focuses on data. We turn your attention in that direc-\ntion throughout the next several sections.\n Dimensional Modeling\nGiven  this book’s focus for the ﬁ rst 16 chapters, we won’t spend any time discuss-\ning dimensional modeling techniques here. The next chapter provides detailed \nrecommendations about the participants, process, and deliverables surrounding our \niterative workshop approach for designing dimensional models in collaboration with \nbusiness users. It’s required reading for anyone involved in the modeling activity.\nPhysical Design\nThe  dimensional models developed and documented via a preliminary source-to-\ntarget mapping need to be translated into a physical database. With dimensional \nmodeling, the logical and physical designs bear a close resemblance; you don’t \nwant the database administrator to convert your lovely dimensional schema into a \nnormalized structure during the physical design process.\nPhysical database implementation details vary widely by platform and project. \nIn addition, hardware, software, and tools are evolving rapidly, so the following \nphysical design activities and considerations merely scratch the surface.\nDevelop Naming and Database Standards\nTable  and column names are key elements of the users’ experience, both for navi-\ngating the data model and viewing BI applications, so they should be meaningful \nto the business. You must also establish standards surrounding key declarations \nand the permissibility of nulls.\n\n\nKimball DW/BI Lifecycle Overview 421\nDevelop Physical Database Model\nThis  model should be initially built in the development server where it will be used \nby the ETL development team. There are several additional sets of tables that need \nto be designed and deployed as part of the DW/BI system, including staging tables to \nsupport the ETL system, auditing tables for ETL processing and data quality, and \nstructures to support secure access to a subset of the data warehouse.\nDevelop Initial Index Plan\nIn  addition to understanding how the relational database’s query optimizer \nand indexes work, the database administrator also needs to be keenly aware that \nDW/BI requirements diff er signiﬁ cantly from OLTP requirements. Because dimen-\nsion tables have a single column primary key, you’ll have a unique index on that \nkey. If bitmapped indexes are available, you typically add single column bitmapped \nindexes to dimension attributes used commonly for ﬁ ltering and grouping, espe-\ncially those attributes that will be jointly constrained; otherwise, you should evalu-\nate the usefulness of B-tree indexes on these attributes. Similarly, the ﬁ rst fact table \nindex will typically be a B-tree or clustered index on the primary key; placing the \ndate foreign key in the index’s leading position speeds both data loads and queries \nbecause the date is frequently constrained. If the DBMS supports high-cardinality \nbitmapped indexes, these can be a good choice for individual foreign keys in the \nfact tables because they are more agnostic than clustered indexes when the user \nconstrains dimensions in unexpected ways. The determination of other fact table \nindexes depends on the index options and optimization strategies within the plat-\nform. Although OLAP database engines also use indexes and have a query optimizer, \nunlike the relational world, the database administrator has little control in these \nenvironments.\nDesign Aggregations, Including OLAP Database\nContrary  to popular belief, adding more hardware isn’t necessarily the best weapon \nin the performance-tuning arsenal; leveraging aggregate tables is a far more cost-\neff ective alternative. Whether using OLAP technology or relational aggregation \ntables, aggregates need to be designed in the DW/BI environment, as we’ll further \nexplore in Chapter 19: ETL Subsystems and Techniques, and Chapter 20. When \nperformance metrics are aggregated, you either eliminate dimensions or associate \nthe metrics with a shrunken rollup dimension that conforms to the atomic base \ndimension. Because you can’t possibly build, store, and administer every theoreti-\ncal aggregation, two primary factors need to be evaluated. First, think about the \nbusiness users’ access patterns derived from the requirements ﬁ ndings, as well as \nfrom input gained by monitoring actual usage patterns. Second, assess the data’s \nstatistical distribution to identify aggregation points that deliver bang for the buck.\n\n\nChapter 17\n422\nFinalize Physical Storage Details\nThis  includes the nuts-and-bolts storage structures of blocks, ﬁ les, disks, partitions, \nand table spaces or databases. Large fact tables are typically partitioned by activity \ndate, with data segmented by month into separate partitions while appearing to \nusers as a single table. Partitioning by date delivers data loading, maintenance, and \nquery performance advantages.\nThe aggregation, indexing and other performance tuning strategies will evolve as \nactual usage patterns are better understood, so be prepared for the inevitable ongo-\ning modiﬁ cations. However, you must deliver appropriately indexed and aggregated \ndata with the initial rollout to ensure the DW/BI environment delivers reasonable \nquery performance from the start.\nETL Design and Development\nThe  Lifecycle’s data track wraps up with the design and development of the ETL \nsystem. Chapter 19 describes the factors, presented as 34 subsystems, which must \nbe considered during the design. Chapter 20 then provides more granular guidance \nabout the ETL system design and development process and associated tasks. Stay \ntuned for more details regarding ETL.\nLifecycle BI Applications Track\nThe ﬁ nal set of parallel activities following the business requirements deﬁ nition in \nFigure 17-1 is the BI application track where you design and develop the applica-\ntions that address a portion of the users’ analytic requirements. As a BI application \ndeveloper once said, “Remember, this is the fun part!” You’re ﬁ nally using the \ninvestment in technology and data to help business users make better decisions. \nAlthough some may feel that the data warehouse should be a completely ad hoc, \nself-service query environment, delivering parameter-driven BI applications will \nsatisfy a large percentage of the business community’s needs. For many business \nusers, “ad hoc” implies the ability to change the parameters on a report to create \ntheir personalized version. There’s no sense making every user start from scratch. \nConstructing a set of BI applications establishes a consistent analytic framework for \nthe organization, rather than allowing each spreadsheet to tell a slightly diff erent \nstory. BI applications also serve to capture the analytic expertise of the organiza-\ntion, from monitoring performance to identifying exceptions, determining causal \nfactors, and modeling alternative responses; this encapsulation provides a jump \nstart for the less analytically inclined.\n",
      "page_number": 434
    },
    {
      "number": 47,
      "title": "Segment 47 (pages 446-454)",
      "start_page": 446,
      "end_page": 454,
      "detection_method": "topic_boundary",
      "content": "Kimball DW/BI Lifecycle Overview 423\nBI Application Speciﬁ cation\nFollowing  the business requirements deﬁ nition, you need to review the ﬁ ndings \nand collected sample reports to identify a starter set of approximately 10 to 15 BI \nreports and analytic applications. You want to narrow the initial focus to the most \ncritical capabilities to manage expectations and ensure on-time delivery. Business \ncommunity input will be critical to this prioritization process. Although 15 appli-\ncations may not sound like much, numerous analyses can be created from a single \ntemplate merely by changing variables.\nBefore you start designing the initial applications, it’s helpful to establish stan-\ndards, such as common pull-down menus and consistent output look and feel. \nUsing these standards, you specify each application template and capture suffi  cient \ninformation about the layout, input variables, calculations, and breaks, so both the \napplication developer and business representatives share a common understanding.\nDuring the BI application speciﬁ cation activity, you should also consider the appli-\ncations’ organization. You need to identify structured navigational paths to access \nthe applications, reﬂ ecting the way users think about their business. Leveraging \ncustomizable information portals or dashboards are the dominant strategies for \ndisseminating access.\nBI Application Development\nWhen  you move into the development phase for the BI applications, you again \nneed to focus on standards; naming conventions, calculations, libraries, and cod-\ning standards should be established to minimize future rework. The application \ndevelopment activity can begin when the database design is complete, the BI tools \nand metadata are installed, and a subset of historical data has been loaded. The BI \napplication template speciﬁ cations should be revisited to account for the inevitable \nchanges to the model since the speciﬁ cations were completed.\nEach BI tool has product-speciﬁ c tricks that can cause it to jump through hoops \nbackward. Rather than trying to learn the techniques via trial and error, we sug-\ngest investing in appropriate tool-speciﬁ c education or supplemental resources for \nthe development team.\nWhile the BI applications are being developed, several ancillary beneﬁ ts result. BI \napplication developers, armed with a robust access tool, will quickly ﬁ nd needling \nproblems in the data haystack despite the quality assurance performed by the ETL \napplication. This is one reason we prefer to start the BI application development \nactivity prior to the supposed completion of the ETL system. The developers also \nwill be the ﬁ rst to realistically test query response times. Now is the time to review \nthe preliminary performance-tuning strategies.\n\n\nChapter 17\n424\nThe BI application quality assurance activities cannot be completed until the data \nis stabilized. You must ensure there is adequate time in the schedule beyond the ﬁ nal \nETL cutoff  to allow for an orderly wrap-up of the BI application development tasks.\nLifecycle Wrap-up Activities\nThe following sections provide recommendations to ensure your project comes to \nan orderly conclusion, while ensuring you’re poised for future expansion.\nDeployment\nThe  technology, data, and BI application tracks converge at deployment. \nUnfortunately, this convergence does not happen naturally but requires substantial \npreplanning. Perhaps more important, successful deployment demands the courage \nand willpower to honestly assess the project’s preparedness to deploy. Deployment \nis similar to serving a large holiday meal to friends and relatives. It can be diffi  cult \nto predict exactly how long it will take to cook the meal’s main entrée. Of course, if \nthe entrée is not done, the cook is forced to slow down the side dishes to compensate \nfor the lag before calling everyone to the table.\nIn the case of DW/BI deployment, the data is the main entrée. “Cooking” the \ndata in the ETL kitchen is the most unpredictable task. Unfortunately, even if the \ndata isn’t fully cooked, you often still proceed with the DW/BI deployment because \nyou told the warehouse guests they’d be served on a speciﬁ c date and time. Because \nyou’re unwilling to slow down the pace of deployment, you march into their offi  ces \nwith undercooked data. No wonder users sometimes refrain from coming back for \na second helping.\nAlthough testing has undoubtedly occurred during the DW/BI development tasks, \nyou need to perform end-to-end system testing, including data quality assurance, \noperations processing, performance, and usability testing. In addition to critically \nassessing the readiness of the DW/BI deliverables, you also need to package it with \neducation and support for deployment. Because the user community must adopt \nthe DW/BI system for it to be deemed successful, education is critical. The DW/\nBI support strategy depends on a combination of management’s expectations and \nthe realities of the deliverables. Support is often organized into a tiered structure. \nThe ﬁ rst tier is website and self-service support; the second tier is provided by the \npower users residing in the business area; centralized support from the DW/BI team \nprovides the ﬁ nal line of  defense.\n\n\nKimball DW/BI Lifecycle Overview 425\nMaintenance and Growth\nYou  made it through deployment, so now you’re ready to kick back and relax. Not \nso quickly! Your job is far from complete after you deploy. You need to continue \nto manage the existing environment by investing resources in the following areas:\n \n■Support. User support is immediately crucial following the deployment to \nensure the business community gets hooked. You can’t sit back in your cubicle \nand assume that no news from the business community is good news. If \nyou’re not hearing from them, chances are no one is using the DW/BI system. \nRelocate (at least temporarily) to the business community so the users have \neasy access to support resources. If problems with the data or BI applications \nare uncovered, be honest with the business to build credibility while taking \nimmediate action to correct the problems. If the DW/BI deliverable is not of \nhigh quality, the unanticipated support demands for data reconciliation and \napplication rework can be overwhelming.\n \n■Education. You must provide a continuing education program for the DW/\nBI system. The curriculum should include formal refresher and advanced \ncourses, as well as repeat introductory courses. More informal education can \nbe off ered to the developers and power users to encourage the interchange \nof ideas.\n \n■Technical support. The DW/BI system needs to be treated as a production \nenvironment with service level agreements. Of course, technical support \nshould proactively monitor performance and system capacity trends. You \ndon’t want to rely on the business community to tell you that performance \nhas degraded.\n \n■Program support. The DW/BI program lives on beyond the implementation \nof a single phase. You must closely monitor and then market your success. \nCommunication with the varied DW/BI constituencies must continue. You \nmust also ensure that existing implementations continue to address the needs \nof the business. Ongoing checkpoint reviews are a key tool to assess and \nidentify opportunities for improvement.\nIf you’ve done your job correctly, inevitably there will be demand for growth, \neither for new users, new data, new BI applications, or major enhancements to \nexisting deliverables. Unlike traditional systems development initiatives, DW/BI \nchange should be viewed as a sign of success, not failure. As we advised earlier when \ndiscussing project scoping, the DW/BI team should not make decisions about these \ngrowth options in a vacuum; the business needs to be involved in the prioritization \n\n\nChapter 17\n426\nprocess. This is a good time to leverage the prioritization grid illustrated in \nFigure 17-2. If you haven’t done so already, an executive business sponsorship com-\nmittee should be established to set DW/BI priorities that align with the organization’s \noverall objectives. After new priorities have been identiﬁ ed, then you go back to \nthe beginning of the Lifecycle and do it all again, leveraging and building on the \ntechnical, data, and BI application foundations that have already been established, \nwhile turning your attention to the new requirements.\nCommon Pitfalls to Avoid\nAlthough  we can provide positive recommendations about data warehousing and \nbusiness intelligence, some readers better relate to a listing of common pitfalls. \nHere is our favorite top 10 list of common errors to avoid while building a DW/\nBI system. These are all quite lethal errors—one alone may be suffi  cient to bring \ndown the initiative:\n \n■Pitfall 10: Become overly enamored with technology and data rather than \nfocusing on the business’s requirements and goals.\n \n■Pitfall 9: Fail to embrace or recruit an inﬂ uential, accessible, and reasonable \nsenior management visionary as the business sponsor of the DW/BI eff ort.\n \n■Pitfall 8: Tackle a galactic multiyear project rather than pursuing more man-\nageable, although still compelling, iterative development eff orts.\n \n■Pitfall 7: Allocate energy to construct a normalized data structure, yet run \nout of budget before building a viable presentation area based on dimensional \nmodels.\n \n■Pitfall 6: Pay more attention to back room operational performance and ease-\nof-development than to front room query performance and ease of use.\n \n■Pitfall 5: Make the supposedly queryable data in the presentation area overly \ncomplex. Database designers who prefer a more complex presentation should \nspend a year supporting business users; they’d develop a much better appre-\nciation for the need to seek simpler solutions.\n \n■Pitfall 4: Populate dimensional models on a standalone basis without regard \nto a data architecture that ties them together using shared, conformed \ndimensions.\n \n■Pitfall 3: Load only summarized data into the presentation area’s dimensional \nstructures.\n \n■Pitfall 2: Presume the business, its requirements and analytics, and the under-\nlying data and the supporting technology are static.\n \n■Pitfall 1: Neglect to acknowledge that DW/BI success is tied directly to busi-\nness acceptance. If the users haven’t accepted the DW/BI system as a founda-\ntion for improved decision making, your eff orts have been exercises in futility.\n\n\nKimball DW/BI Lifecycle Overview 427\nSummary\nThis chapter provided a high-speed tour of the Kimball Lifecycle approach for DW/\nBI projects. We touched on the key processes and best practices. Although each \nproject is a bit diff erent from the next, they all require attention to the major tasks \ndiscussed to ensure a successful initiative.\nThe next chapter provides much more detailed coverage of the Kimball Lifec ycle’s \ncollaborative workshop approach for iteratively designing dimensional models with \nbusiness representatives. Chapters 19 and 20 delve into ETL system design consid-\nerations and recommended development processes.\n\n\nDimensional \nModeling Process \nand Tasks\nW\ne’ve described countless dimensional modeling patterns in Chapters 1 \nthrough 16 of this book. Now it’s time to turn your attention to the tasks \nand tactics of the dimensional modeling process.\nThis chapter, condensed from content in The Data Warehouse Lifecycle Toolkit, \nSecond Edition (Wiley, 2008), begins with a practical discussion of preliminary \npreparation activities, such as identifying the participants (including business \nrepresentatives) and arranging logistics. The modeling team develops an initial \nhigh-level model diagram, followed by iterative detailed model development, review, \nand validation. Throughout the process, you are reconﬁ rming your understanding \nof the business’s requirements.\nChapter 18 reviews the following concepts:\n \n■Overview of the dimensional modeling process\n \n■Tactical recommendations for the modeling tasks\n \n■Key modeling deliverables\n Modeling Process Overview\nBefore  launching into the dimensional modeling design eff ort, you must involve \nthe right players. Most notably, we strongly encourage the participation of business \nrepresentatives during the modeling sessions. Their involvement and collaboration \nstrongly increases the likelihood that the resultant model addresses the business’s \nneeds. Likewise, the organization’s business data stewards should participate, espe-\ncially when you’re discussing the data they’re responsible for governing.\nCreating a dimensional model is a highly iterative and dynamic process. After \na few preparation steps, the design eff ort begins with an initial graphical model \nderived from the bus matrix, identifying the scope of the design and clarifying the \ngrain of the proposed fact tables and associated dimensions.\n18\n\n\nChapter 18\n430\nAfter completing the high-level model, the design team dives into the dimension \ntables with attribute deﬁ nitions, domain values, sources, relationships, data quality \nconcerns, and transformations. After the dimensions are identiﬁ ed, the fact tables \nare modeled. The last phase of the process involves reviewing and validating the \nmodel with interested parties, especially business representatives. The primary \ngoals are to create a model that meets the business requirements, verify that data \nis available to populate the model, and provide the ETL team with a solid starting \nsource-to-target mapping.\nDimensional models unfold through a series of design sessions with each pass \nresulting in a more detailed and robust design that’s been repeatedly tested against the \nbusiness needs. The process is complete when the model clearly meets the business’s \nrequirements. A typical design requires three to four weeks for a single \nbusiness process dimensional model, but the time required can vary depending \non the team’s experience, the availability of detailed business requirements, the \ninvolvement of business representatives or data stewards authorized to drive to orga-\nnizational consensus, the complexity of the source data, and the ability to leverage \nexisting conformed dimensions.\nFigure 18-1 shows the dimensional modeling process ﬂ ow. The key inputs to the \ndimensional modeling process are the preliminary bus matrix and detailed busi-\nness requirements. The key deliverables of the modeling process are the high-level \ndimensional model, detailed dimension and fact table designs, and issues log.\nPreparation\nHigh Level Dimensional Model\nDetailed Dimensional Model Development\nModel Review and Validation\nIterate and Test\nFinal Design Documentation\nFigure 18-1: Dimensional modeling process ﬂ ow diagram.\nAlthough the graphic portrays a linear progression, the process is quite iterative. \nYou will make multiple passes through the dimensional model starting at a high \nlevel and drilling into each table and column, ﬁ lling in the gaps, adding more detail, \nand changing the design based on new information.\n\n\nDimensional Modeling Process and Tasks 431\nIf an outside expert is engaged to help guide the dimensional modeling eff ort, \ninsist they facilitate the process with the team rather than disappearing for a few \nweeks and returning with a completed design. This ensures the entire team under-\nstands the design and associated trade-off s. It also provides a learning opportunity, \nso the team can carry the model forward and independently tackle the next model.\nGet Organized\nBefore beginning to model, you must appropriately prepare for the dimensional \nmodeling process. In addition to involving the right resources, there are also basic \nlogistical considerations to ensure a productive design eff ort.\n Identify Participants, Especially Business \nRepresentatives\nThe  best dimensional models result from a collaborative team eff ort. No single \nindividual is likely to have the detailed knowledge of the business requirements and \nthe idiosyncrasies of the source systems to eff ectively create the model themselves. \nAlthough the data modeler facilitates the process and has primary responsibility \nfor the deliverables, we believe it’s critically important to get subject matter experts \nfrom the business involved to actively collaborate; their insights are invaluable, \nespecially because they are often the individuals who have historically ﬁ gured \nout how to get data out of the source systems and turned it into valuable analytic \ninformation. Although involving more people in the design activities increases the \nrisk of slowing down the process, the improved richness and completeness of \nthe design justiﬁ es the additional overhead.\nIt’s always helpful to have someone with keen knowledge of the source system \nrealities involved. You might also include some physical DBA and ETL team rep-\nresentatives so they can learn from the insights uncovered during the modeling \neff ort and resist the temptations to apply third normal form (3NF) concepts or defer \ncomplexities to the BI applications in an eff ort to streamline the ETL processing. \nRemember the goal is to trade off  ETL processing complexity for simplicity and \npredictability at the BI presentation layer.\nBefore jumping into the modeling process, you should take time to consider \nthe ongoing stewardship of the DW/BI environment. If the organization has \nan active data governance and stewardship initiative, it is time to tap into that \nfunction. If there is no preexisting stewardship program, it’s time to initiate \nit. An enterprise DW/BI eff ort committed to dimensional modeling must also \nbe committed to a conformed dimension strategy to ensure consistency across \nbusiness processes. An active data stewardship program helps the organization \n\n\nChapter 18\n432\nachieve its conformed dimension strategy. Agreeing on conformed dimensions \nin a large enterprise can be a challenge; the diffi  culty is usually less a techni-\ncal issue and more an organizational communication and consensus building \nchallenge.\nDiff erent groups across the enterprise are often committed to their own pro-\nprietary business rules and deﬁ nitions. Data stewards must work closely with the \ninterested groups to develop common business rules and deﬁ nitions, and then \ncajole the organization into embracing the common rules and deﬁ nitions to develop \nenterprise consensus. Over the years, some have criticized the concept of conformed \ndimensions as being “too hard.” Yes, it’s diffi  cult to get people in diff erent corners \nof the business to agree on common attribute names, deﬁ nitions, and values, but \nthat’s the crux of uniﬁ ed, integrated data. If everyone demands their own labels and \nbusiness rules, then there’s no chance of delivering the single version of the truth \npromised by DW/BI systems. And ﬁ nally, one of the reasons the Kimball approach is \nsometimes criticized as being hard from people who are looking for quick solutions \nis because we have spelled out the detailed steps for actually getting the job done. \nIn Chapter 19: ETL Subsystems and Techniques, these down-in-the-weeds details \nare discussed in the coverage of ETL subsystems 17 and 18.\nReview the Business Requirements\nBefore  the modeling begins, the team must familiarize itself with the business \nrequirements. The ﬁ rst step is to carefully review the requirements documentation, \nas we described in Chapter 17: Kimball DW/BI Lifecycle Overview. It’s the modeling \nteam’s responsibility to translate the business requirements into a ﬂ exible dimen-\nsional model that can support a broad range of analysis, not just speciﬁ c reports. \nSome designers are tempted to skip the requirements review and move directly into \nthe design, but the resulting models are typically driven exclusively by the source \ndata without considering the added value required by the business community. \nHaving appropriate business representation on the modeling team helps further \navoid this data-driven approach.\nLeverage a Modeling Tool\nBefore  jumping into the modeling activities, it’s helpful to have a few tools in place. \nUsing a spreadsheet as the initial documentation tool is eff ective because it enables \nyou to quickly and easily make changes as you iterate through the modeling process.\nAfter the model begins to ﬁ rm up in the later stages of the process, you can con-\nvert to whatever modeling tool is used in your organization. Most modeling tools \nare dimensionally aware with functions to support the creation of a dimensional \nmodel. When the detailed design is complete, the modeling tools can help the DBA \n",
      "page_number": 446
    },
    {
      "number": 48,
      "title": "Segment 48 (pages 455-462)",
      "start_page": 455,
      "end_page": 462,
      "detection_method": "topic_boundary",
      "content": "Dimensional Modeling Process and Tasks 433\nforward engineer the model into the database, including creating the tables, indexes, \npartitions, views, and other physical elements of the database.\nLeverage a Data Proﬁ ling Tool\nThroughout  the modeling process, the teams needs to develop an ever-increasing \nunderstanding of the source data’s structure, content, relationships, and derivation \nrules. You need to verify the data exists in a usable state, or at least its ﬂ aws can be \nmanaged, and understand what it takes to convert it into the dimensional model. \nData proﬁ ling uses query capabilities to explore the actual content and relation-\nships in the source system rather than relying on perhaps incomplete or outdated \ndocumentation. Data proﬁ ling can be as simple as writing some SQL statements \nor as sophisticated as a special purpose tool. The major ETL vendors include data \nproﬁ ling capabilities in their products.\nLeverage or Establish Naming Conventions\nThe  issue of naming conventions inevitably arises during the creation of the dimen-\nsional model. The data model’s labels must be descriptive and consistent from a \nbusiness perspective. Table and column names become key elements of the BI appli-\ncation’s interface. A column name such as “Description” may be perfectly clear in \nthe context of a data model but communicates nothing in the context of a report.\nPart of the process of designing a dimensional model is agreeing on common deﬁ -\nnitions and common labels. Naming is complex because diff erent business groups \nhave diff erent meanings for the same name and diff erent names with the same \nmeaning. People are reluctant to give up the familiar and adopt a new vocabulary. \nSpending time on naming conventions is one of those tiresome tasks that seem to \nhave little payback but is worth it in the long run.\nLarge organizations often have an IT function that owns responsibility for nam-\ning conventions. A common approach is to use a naming standard with three parts: \nprime word, qualiﬁ ers (if appropriate), and class word. Leverage the work of this \nIT function, understanding that sometimes existing naming conventions need to \nbe extended to support more business-friendly table and column names. If the \norganization doesn’t already have a set of naming conventions, you must establish \nthem during the dimensional  modeling.\nCoordinate Calendars and Facilities\nLast,  but not least, you need to schedule the design sessions on participants’ cal-\nendars. Rather than trying to reserve full days, it’s more realistic to schedule \nmorning and afternoon sessions that are two to three hours in duration for three or \nfour days each week. This approach recognizes that the team members have other \n\n\nChapter 18\n434\nresponsibilities and allows them to try to keep up in the hours before, after, and \nbetween design sessions. The design team can leverage the unscheduled time to \nresearch the source data and conﬁ rm requirements, as well as allow time for the \ndata modeler to update the design documentation prior to each session.\nAs we mentioned earlier, the modeling process typically takes three to four weeks \nfor a single business process, such as sales orders, or a couple of tightly related \nbusiness processes such as healthcare facility and professional claim transactions \nin a set of distinct but closely aligned fact tables. There are a multitude of factors \nimpacting the magnitude of the eff ort. Ultimately, the availability of previously \nexisting core dimensions allows the modeling eff ort to focus almost exclusively on \nthe fact table’s performance metrics, which signiﬁ cantly reduces the time required.\nFinally, you must reserve appropriate facilities. It is best to set aside a dedicated \nconference room for the duration of the design eff ort—no easy task in most orga-\nnizations where meeting room facilities are always in short supply. Although we’re \ndreaming, big ﬂ oor-to-ceiling whiteboards on all four walls would be nice, too! In \naddition to a meeting facility, the team needs some basic supplies, such as self-stick \nﬂ ip chart paper. A laptop projector is often useful during the design sessions and \nis absolutely required for the design reviews.\n Design the Dimensional Model\nAs outlined in Chapter 3: Retail Sales, there are four key decisions made during the \ndesign of a dimensional model:\n \n■Identify the business process.\n \n■Declare the grain of the business process.\n \n■Identify the dimensions.\n \n■Identify the facts.\nThe ﬁ rst step of identifying the business process is typically determined at the \nconclusion of the requirements gathering. The prioritization activity described in \nChapter 17 establishes which bus matrix row (and hence business process) will be \nmodeled. With that grounding, the team can proceed with the design tasks. \nThe modeling eff ort typically works through the following sequence of tasks and \ndeliverables, as illustrated in Figure 18-1: \n \n■High-level model deﬁ ning the model’s scope and granularity\n \n■Detailed design with table-by-table attributes and metrics\n \n■Review and validation with IT and business representatives\n \n■Finalization of the design documentation\n\n\nDimensional Modeling Process and Tasks 435\nAs with any data modeling eff ort, dimensional modeling is an iterative process. \nYou will work back and forth between business requirements and source details to \nfurther reﬁ ne the model, changing the model as you learn more.\nThis section describes each of these major tasks. Depending on the design team’s \nexperience and exposure to dimensional modeling concepts, you might begin with basic \ndimensional modeling education before kicking off  the eff ort to ensure everyone is on \nthe same page regarding standard dimensional vocabulary and best practices.\n Reach Consensus on High-Level Bubble Chart\nThe  initial task in the design session is to create a high-level dimensional model \ndiagram for the target business process. Creating the ﬁ rst draft is relatively straight-\nforward because you start with the bus matrix. Although an experienced designer \ncould develop the initial high-level dimensional model and present it to the team \nfor review, we recommend against this approach because it does not allow the entire \nteam to participate in the process.\nThe high-level diagram graphically represents the business process’s dimension \nand fact tables. Shown in Figure 18-2, we often refer to this diagram as the bubble \nchart for obvious reasons. This entity-level graphical model clearly identiﬁ es the \ngrain of the fact table and its associated dimensions to a non-technical audience.\nOrder Date\nSold To\nCustomer\nCurrency\nPromotion\nChannel\nProduct\nOrder Profile\nSales\nPerson\nOrders\nGrain = 1 row\nper order line\nDue Date\nShip To\nCustomer\nBill To\nCustomer\nFigure 18-2: Sample high-level model diagram.\n\n\nChapter 18\n436\nDeclaring the grain requires the modeling team to consider what is needed to \nmeet the business requirements and what is possible based on the data collected \nby the source system. The bubble chart must be rooted in the realities of available \nphysical data sources. A single row of the bus matrix may result in multiple bubble \ncharts, each corresponding to a unique fact table with unique granularity.\nMost of the major dimensions will fall out naturally after you determine the \ngrain. One of the powerful eff ects of a clear fact table grain declaration is you can \nprecisely visualize the associated dimensionality. Choosing the dimensions may \nalso cause you to rethink the grain declaration. If a proposed dimension doesn’t \nmatch the grain of the fact table, either the dimension must be left out, the grain \nof the fact table changed, or a multivalued design solution needs to be considered.\nFigure 18-2’s graphical representation serves several purposes. It facilitates \ndiscussion within the design team before the team dives into the detailed design, \nensuring everyone is on the same page before becoming inundated with minutiae. \nIt’s also a helpful introduction when the team communicates with interested stake-\nholders about the project, its scope, and data contents.\nTo aid in understanding, it is helpful to retain consistency across the high-level \nmodel diagrams for a given business process. Although each fact table is documented \non a separate page, arranging the associated dimensions in a similar sequence across \nthe bubble charts is useful.\nDevelop the Detailed Dimensional Model\nAfter  completing the high-level bubble chart designs, it’s time to focus on the details. \nThe team should meet on a very regular basis to deﬁ ne the detailed dimensional \nmodel, table by table, column by column. The business representatives should \nremain engaged during these interactive sessions; you need their feedback on attri-\nbutes, ﬁ lters, groupings, labels, and metrics. \nIt’s most eff ective to start with the dimension tables and then work on the fact \ntables. We suggest launching the detailed design process with a couple of straight-\nforward dimensions; the date dimension is always a favorite starting point. This \nenables the modeling team to achieve early success, develop an understanding of \nthe modeling process, and learn to work together as a team.\nThe detailed modeling identiﬁ es the interesting and useful attributes within each \ndimension and appropriate metrics for each fact table. You also want to capture the \nsources, deﬁ nitions, and preliminary business rules that specify how these attributes \nand metrics are populated. Ongoing analyses of the source system and systematic \ndata proﬁ ling during the design sessions helps the team better understand the reali-\nties of the underlying source data.\n\n\nDimensional Modeling Process and Tasks 437\n Identify Dimensions and their Attributes\nDuring  the detailed design sessions, key conformed dimensions are deﬁ ned. Because \nthe DW/BI system is an enterprise resource, these deﬁ nitions must be acceptable \nacross the enterprise. The data stewards and business analysts are key resources to \nachieve organizational consensus on table and attribute naming, descriptions, and \ndeﬁ nitions. The design team can take the lead in driving the process and leverag-\ning naming conventions, if available. But it is ultimately a business task to agree on \nstandard business deﬁ nitions and names; the column names must make sense to \nthe business users. This can take some time, but it is an investment that will deliver \nhuge returns for the users’ understanding and willingness to accept the dimensional \nmodel. Don’t be surprised if the governance steering committee must get involved \nto resolve conformed dimension deﬁ nition and naming issues.\nAt this point, the modeling team often also wrestles with the potential inclu-\nsion of junk dimensions or mini-dimensions in a dimensional model. It may not \nbe apparent that these more performance-centric patterns are warranted until the \nteam is deeply immersed in the design.\n Identify the Facts\nDeclaring  the grain crystallizes the discussion about the fact table’s metrics because \nthe facts must all be true to the grain. The data proﬁ ling eff ort identiﬁ es the counts \nand amounts generated by the measurement event’s source system. However, fact \ntables are not limited to just these base facts. There may be additional metrics the \nbusiness wants to analyze that are derived from the base facts.\nIdentify Slowly Changing Dimension Techniques\nAfter  the dimension and fact tables from the high-level model diagram have been \ninitially drafted, you then circle back to the dimension tables. For each dimension \ntable attribute, you deﬁ ne how source system data changes will be reﬂ ected in the \ndimension table. Again, input from the business data stewards is critical to estab-\nlishing appropriate rules. It’s also helpful to ask the source system experts if they \ncan determine whether a data element change is due to a source data correction.\nDocument the Detailed Table Designs\nThe  key deliverables of the detailed modeling phase are the design work-\nsheets, as shown in Figure 18-3; a digital template is available on our website at \nwww.kimballgroup.com under the Tools and Utilities Tab for The Data Warehouse \nLifecycle Toolkit, Second Edition. The worksheets capture details for communication \nto interested stakeholders including other analytical business users, BI applica-\ntion developers, and most important, the ETL developers who will be tasked with \npopulating the design.\n\n\nOrderProﬁleKey\nSurrogate primary key\nSurrogate key\nsmallint\nvarchar\nvarchar\nvarchar\n8\n12\n14\n1\n1\n1\nOEI\nOEI\nOEI\nOrderHeader\nOrderHeader\nOrderHeader\nOrd_Meth\nOrd_Src\nComm_Code\nint\nchar\nint\n1=Phone, 2=Fax,\n3=Internet\nR=Reseller, D=Direct\nSales\n0=Non-Commission,\n1=Commission\n1, 2, 3...\nPhone, Fax,\nInternet\nReseller, Direct\nSales\nCommission, Non-\nCommission\nDerived\nOrderMethod\nMethod used to place order\n(phone, fax, internet)\nOrderSource\nCommissionInd\nColumn Name\nDescription\nSource of the order\n(reseller, direct sales)\nIndicates whether order is\ncommissionable or not\nDatatype\nSize\nExample Values\nSource\nSystem\nSource Table\nSource Field\nName\nSource\nDatatype\nETL Rules\nSCD\nType\nTable Name\nDimOrderProﬁle\nTable Type\ne\ncr\nu\no\nS\nte\ng\nr\na\nT\nDisplay Name\nDescription\nUsed in schemas\nSize\nDimension\nOrderProﬁle\nOrder Proﬁle is the “junk” dimension for miscellaneous information about order transactions\nOrders\n12 rows\nFigure 18-3: Sample detailed dimensional design worksheet.\n\n\nDimensional Modeling Process and Tasks 439\nEach dimension and fact table should be documented in a separate worksheet. At \na minimum, the supporting information required includes the attribute/fact name, \ndescription, sample values, and a slowly changing dimension type indicator for every \ndimension attribute. In addition, the detailed fact table design should identify each \nforeign key relationship, appropriate degenerate dimensions, and rules for each fact \nto indicate whether it’s additive, semi-additive, or non-additive.\nThe dimensional design worksheet is the ﬁ rst step toward creating the source-\nto-target mapping document. The physical design team will further ﬂ esh out the \nmapping with physical table and column names, data types, and key declarations.\nTrack Model Issues\nAny issues, deﬁ nitions, transformation rules, and data quality challenges discovered \nduring the design process should be captured in an issues tracking log. Someone \nshould be assigned the task of capturing and tracking issues during the sessions; the \nproject manager, if they’re participating in the design sessions, often handles this \nresponsibility because they’re typically adept at keeping the list updated and encour-\naging progress on resolving open issues. The facilitator should reserve adequate \ntime at the end of every session to review and validate new issue entries and their \nassignments. Between design sessions, the design team is typically busy proﬁ ling \ndata, seeking clariﬁ cation and agreement on common deﬁ nitions, and meeting with \nsource system experts to resolve outstanding issues.\nMaintain Updated Bus Matrix\nDuring  the detailed modeling process, there are often new discoveries about the \nbusiness process being modeled. Frequently, these ﬁ ndings result in the intro-\nduction of new fact tables to support the business process, new dimensions, or \nthe splitting or combining of dimensions. You must keep the bus matrix updated \nthroughout the design process because it is a key communication and planning \ntool. As discussed in Chapter 16: Insurance, the detailed bus matrix often captures \nadditional information about each fact table’s granularity and metrics.\nReview and Validate the Model\nOnce the design team is conﬁ dent about the model, the process moves into the \nreview and validation phase to get feedback from other interested parties, including:\n \n■IT resources, such as DW/BI team members not involved in the modeling \neff ort, source system experts, and DBAs\n \n■Analytical or power business users not involved in the modeling eff ort\n \n■Broader business user community\n\n\nChapter 18\n440\nIT Reviews\nTypically,  the ﬁ rst review of the detailed dimensional model is with peers in the \nIT organization. This audience is often composed of reviewers who are intimately \nfamiliar with the target business process because they wrote or manage the system \nthat runs it. They are also at least partly familiar with the target data model because \nyou’ve already been pestering them with source data questions. \nIT reviews can be challenging because the participants often lack an understand-\ning of dimensional modeling. In fact, most of them probably fancy themselves as \nproﬁ cient 3NF modelers. Their tendency will be to apply transaction processing-\noriented modeling rules to the dimensional model. Rather than spending the bulk \nof your time debating the merits of diff erent modeling disciplines, it is best to proac-\ntively provide some dimensional modeling education as part of the review process.\nWhen everyone has the basic concepts down, you should begin with a review \nof the bus matrix. This gives everyone a sense of the project scope and overall data \narchitecture, demonstrates the role of conformed dimensions, and shows the relative \nbusiness process priorities. Next, illustrate how the selected row on the matrix trans-\nlates directly into the high-level dimensional model diagram. This gives everyone the \nentity-level map of the model and serves as the guide for the rest of the discussion.\nMost of the review session should be spent going through the dimension and fact \ntable worksheet details. It is also a good idea to review any remaining open issues \nfor each table as you work through the model.\nChanges to the model will likely result from this meeting. Remember to assign \nthe task of capturing the issues and recommendations to someone on the team.\nCore User Review\nIn many projects, this review is not required because the core business users are \nmembers of the modeling team and are already intimately knowledgeable about the \ndimensional model. Otherwise, this review meeting is similar in scope and structure \nto the IT review meeting. The core business users are more technical than typical \nbusiness users and can handle details about the model. In smaller organizations, \nwe often combine the IT review and core user review into one session.\nBroader Business User Review\nThis session is as much education as it is design review. You want to educate people \nwithout overwhelming them, while at the same time illustrating how the dimen-\nsional model supports their business requirements. You should start with the bus \nmatrix as the enterprise DW/BI data roadmap, review the high-level model bubble \ncharts, and ﬁ nally, review the critical dimensions, such as customer and product. \nSometimes the bubble charts are supplemented with diagrams similar to Figure \n18-4 to illustrate the hierarchical drill paths within a dimension.\n",
      "page_number": 455
    },
    {
      "number": 49,
      "title": "Segment 49 (pages 463-473)",
      "start_page": 463,
      "end_page": 473,
      "detection_method": "topic_boundary",
      "content": "Dimensional Modeling Process and Tasks 441\nDepartment #\nDepartment Name\nCategory Code\nCategory Name\nSKU #\nProduct Name\nColor Code\nColor Name\nBrand Code\nBrand Name\nPackage Type ID\nPackage Type Desc\nFigure 18-4: Illustration of hierarchical attribute relationships for business users.\nBe sure to allocate time during this education/review to illustrate how the model \ncan be used to answer a broad range of questions about the business process. We \noften pull some examples from the requirements document and walk through how \nthey would be answered.\nFinalize the Design Documentation\nAfter  the model is in its ﬁ nal form, the design documentation should be compiled \nfrom the design team’s working papers. This document typically includes:\n \n■Brief description of the project\n \n■High-level data model diagram\n \n■Detailed dimensional design worksheet for each fact and dimension table\n \n■Open issues\nSummary\nDimensional modeling is an iterative design process requiring the cooperative \neff ort of people with a diverse set of skills, including business representatives. The \ndesign eff ort begins with an initial graphical model pulled from the bus matrix and \npresented at the entity level. The detailed modeling process drills down into the \ndeﬁ nitions, sources, relationships, data quality problems, and required transforma-\ntions for each table. The primary goals are to create a model that meets the business \nrequirements, verify the data is available to populate the model, and provide the \nETL team with a clear direction.\n\n\nChapter 18\n442\nThe task of determining column and table names is interwoven into the design \nprocess. The organization as a whole must agree on the names, deﬁ nitions, and \nderivations of every column and table in the dimensional model. This is more of a \npolitical process than a technical one, which requires the full attention of the most \ndiplomatic team member. The resulting column names exposed through the BI tool \nmust make sense to the business community.\nThe detailed modeling eff ort is followed by several reviews. The end result is \na dimensional model that has been successfully tested against both the business \nneeds and data realities.\n\n\nETL Subsystems and \nTechniques\nT\nhe  extract, transformation, and load (ETL) system consumes a disproportionate \nshare of the time and effort required to build a DW/BI environment. Developing \nthe ETL system is challenging because so many outside constraints put pressure on its \ndesign: the business requirements, source data realities, budget, processing windows, \nand skill sets of the available staff. Yet it can be hard to appreciate just why the ETL \nsystem is so complex and resource-intensive. Everyone understands the three let-\nters: You get the data out of its original source location (E), you do something to it \n(T), and then you load it (L) into a final set of tables for the business users to query.\nWhen  asked about the best way to design and build the ETL system, many design-\ners say, “Well, that depends.” It depends on the source; it depends on limitations of \nthe data; it depends on the scripting languages and ETL tools available; it depends \non the staff ’s skills; and it depends on the BI tools. But the “it depends” response \nis dangerous because it becomes an excuse to take an unstructured approach to \ndeveloping an ETL system, which in the worse-case scenario results in an undif-\nferentiated spaghetti-mess of tables, modules, processes, scripts, triggers, alerts, and \njob schedules. This “creative” design approach should not be tolerated. With the \nwisdom of hindsight from thousands of successful data warehouses, a set of ETL best \npractices have emerged. There is no reason to tolerate an unstructured approach.\nCareful consideration of these best practices has revealed 34 subsystems are \nrequired in almost every dimensional data warehouse back room. No wonder the \nETL system takes such a large percentage of the DW/BI development resources!\nThis chapter is drawn from The Data Warehouse Lifecycle Toolkit, Second Edition \n(Wiley, 2008). Throughout the chapter we’ve sprinkled pointers to resources on \nthe Kimball Group’s website for more in-depth coverage of several ETL techniques.\nChapter 19 reviews the following concepts:\n \n■Requirements and constraints to be considered before designing the \nETL system\n \n■Three subsystems focused on extracting data from source systems\n19\n\n\nChapter 19\n444\n \n■Five subsystems to deal with value-added cleaning and conforming, including \ndimensional structures to monitor quality errors\n \n■Thirteen subsystems to deliver data into now-familiar dimensional structures, \nsuch as a subsystem to implement slowly changing dimension techniques\n \n■Thirteen subsystems to help manage the production ETL environment\nRound Up the Requirements\nEstablishing the architecture of an ETL system begins with one of the toughest \nchallenges: rounding up the requirements. By this we mean gathering and under-\nstanding all the known requirements, realities, and constraints aff ecting the ETL \nsystem. The list of requirements can be pretty overwhelming, but it’s essential to \nlay them on the table before launching into the development of the ETL system.\nThe ETL system requirements are mostly constraints you must live with and adapt \nyour system to. Within the framework of these requirements, there are opportuni-\nties to make your own decisions, exercise judgment, and leverage creativity, but \nthe requirements dictate the core elements that the ETL system must deliver. The \nfollowing ten sections describe the major requirements areas that impact the design \nand development of the ETL system.\nBefore  launching the ETL design and development eff ort, you should provide a \nshort response for each of the following ten requirements. We have provided \na sample checklist (as a note) for each to get you started. The point of this exercise \nis to ensure you visit each of these topics because any one of them can be a show-\nstopper at some point in the project.\n Business Needs\nFrom  an ETL designer’s view, the business needs are the DW/BI system users’ infor-\nmation requirements. We use the term business needs somewhat narrowly here to \nmean the information content that business users need to make informed business \ndecisions. Because the business needs directly drive the choice of data sources and \ntheir subsequent transformation in the ETL system, the ETL team must understand \nand carefully examine the business needs.\nNOTE \nYou should maintain a list of the key performance indicators (KPIs) \nuncovered during the business requirements deﬁ nition that the project intends to \nsupport, as well as the drill-down and drill-across targets required when a business \nuser needs to investigate “why?” a KPI changed.\n\n\nETL Subsystems and Techniques 445\nCompliance\nChanging  legal and reporting requirements have forced many organizations to \nseriously tighten their reporting and provide proof that the reported numbers are \naccurate, complete, and have not been tampered with. Of course, DW/BI systems in \nregulated businesses, such as telecommunications, have complied with regulatory \nreporting requirements for years. But certainly the whole tenor of ﬁ nancial report-\ning has become much more rigorous for everyone.\nNOTE \nIn consultation with your legal department or chief compliance offi  cer (if \nyou have one!) and the BI delivery team, you should list all data and ﬁ nal reports \nsubject to compliance restrictions. List those data inputs and data transformation \nsteps for which you must maintain the “chain of custody” showing and proving \nthat ﬁ nal reports were derived from the original data delivered from your data \nsources. List the data that you must provide proof of security for the copies under \nyour control, both offl  ine and online. List those data copies you must archive, and \nlist the expected usable lifetime of those archives. Good luck with all this. This is \nwhy you are paid so well….\nData Quality\nThree  powerful forces have converged to put data quality concerns near the top of \nthe list for executives. First, the long-term cultural trend that says, “If only I could \nsee the data, then I could manage my business better” continues to grow; today’s \nknowledge workers believe instinctively that data is a crucial requirement for them \nto function in their jobs. Second, most organizations understand their data sources \nare profoundly distributed, typically around the world, and that eff ectively integrat-\ning a myriad of disparate data sources is required. And third, the sharply increased \ndemands for compliance mean careless handling of data will not be overlooked or \nexcused.\nNOTE \nYou should list those data elements whose quality is known to be unac-\nceptable, and list whether an agreement has been reached with the source systems \nto correct the data before extraction. List those data elements discovered during \ndata proﬁ ling, which will be continuously monitored and ﬂ agged as part of the \nETL process.\n\n\nChapter 19\n446\nSecurity\nSecurity  awareness has increased signiﬁ cantly in the last few years across IT but \noften remains an afterthought and an unwelcome burden to most DW/BI teams. \nThe basic rhythms of the data warehouse are at odds with the security mentality; \nthe data warehouse seeks to publish data widely to decision makers, whereas the \nsecurity interests assume data should be restricted to those with a need to know. \nAdditionally, security must be extended to physical backups. If the media can easily \nbe removed from the backup vault, then security has been compromised as eff ec-\ntively as if the online passwords were compromised.\nDuring the requirements roundup, the DW/BI team should seek clear guidance \nfrom senior management as to what aspects of the DW/BI system carry extra secu-\nrity sensitivity. If these issues have never been examined, it is likely the question \nwill be tossed back to the team. That is the moment when an experienced security \nmanager should be invited to join the design team. Compliance requirements are \nlikely to overlap security requirements; it may be wise to combine these two topics \nduring the requirements  roundup.\nNOTE \nYou should expand the compliance checklist to encompass known secu-\nrity and privacy requirements.\nData Integration\nData  integration is a huge topic for IT because, ultimately, it aims to make all sys-\ntems seamlessly work together. The “360 degree view of the enterprise” is a famil-\niar name for data integration. In many cases, serious data integration must take \nplace among the organization’s primary transaction systems before data arrives at \nthe data warehouse’s back door. But rarely is that data integration complete, unless the \norganization has a comprehensive and centralized master data management (MDM) \nsystem, and even then it’s likely other important operational systems exist outside \nthe primary MDM system.\nData integration usually takes the form of conforming dimensions and con-\nforming facts in the data warehouse. Conforming dimensions means establishing \ncommon dimensional attributes across separated databases, so drill-across reports \ncan be generated using these attributes. Conforming facts means making agree-\nments on common business metrics such as key performance indicators (KPIs) \nacross separated databases so these numbers can be compared mathematically by \ncalculating diff erences and ratios.\n\n\nETL Subsystems and Techniques 447\nNOTE \nYou should use the bus matrix of business processes to generate a priority \nlist for conforming dimensions (columns of the bus matrix). Annotate each row \nof the bus matrix with whether there is a clear executive demand for the busi-\nness process to participate in the integration process, and whether the ETL team \nresponsible for that business process has agreed.\nData Latency\nData  latency describes how quickly source system data must be delivered to the \nbusiness users via the DW/BI system. Obviously, data latency requirements have a \nhuge eff ect on the ETL architecture. Clever processing algorithms, parallelization, \nand potent hardware can speed up traditional batch-oriented data ﬂ ows. But at \nsome point, if the data latency requirement is suffi  ciently urgent, the ETL system’s \narchitecture must convert from batch to microbatch or streaming-oriented. This \nswitch isn’t a gradual or evolutionary change; it’s a major paradigm shift in which \nalmost every step of the data delivery pipeline must be re-implemented.\nNOTE \nYou should list all legitimate and well-vetted business demands for data \nthat must be provided on a daily basis, on a many times per day basis, within a few \nseconds, or instantaneously. Annotate each demand with whether the business \ncommunity understands the data quality trade-off s associated with their particu-\nlar choice. Near the end of Chapter 20: ETL System Design and Development \nProcess and Tasks, we discuss data quality compromises caused by low latency \nrequirements.\nArchiving and Lineage\nArchiving  and lineage requirements were hinted at in the previous compliance and \nsecurity sections. Even without the legal requirements for saving data, every data \nwarehouse needs various copies of old data, either for comparisons with new \ndata to generate change capture records or reprocessing. We recommend staging \nthe data (writing it to disk) after each major activity of the ETL pipeline: after it’s \nbeen extracted, cleaned and conformed, and delivered.\nSo when does staging turn into archiving where the data is kept indeﬁ nitely on \nsome form of permanent media? Our simple answer is a conservative answer. All \nstaged data should be archived unless a conscious decision is made that speciﬁ c \ndata sets will never be recovered in the future. It’s almost always less problematic \nto read the data from permanent media than it is to reprocess the data through \nthe ETL system at a later time. And, of course, it may be impossible to reprocess \n\n\nChapter 19\n448\nthe data according to the old processing algorithms if enough time has passed \nor the original extraction cannot be re-created.\nAnd while we are at it, each staged/archived data set should have accompanying \nmetadata describing the origins and processing steps that produced the data. Again, \nthe tracking of this lineage is explicitly required by certain compliance requirements \nbut should be part of every archiving situation.\nNOTE \nYou should list the data sources and intermediate data steps that will be \narchived, together with retention policies, and compliance, security, and privacy \nconstraints.\nBI Delivery Interfaces\nThe  ﬁ nal step for the ETL system is the handoff  to the BI applications. We take a \nstrong and disciplined position on this handoff . We believe the ETL team, working \nclosely with the modeling team, must take responsibility for the content and struc-\nture of the data that makes the BI applications simple and fast. This attitude is more \nthan a vague motherhood statement. We believe it’s irresponsible to hand off  data to \nthe BI application in such a way as to increase the complexity of the application, slow \ndown the query or report creation, or make the data seem unnecessarily complex \nto the business users. The most elementary and serious error is to hand across a \nfull-blown, normalized physical model and walk away from the job. This is why we \ngo to such lengths to build dimensional structures that comprise the ﬁ nal handoff .\nThe ETL team and data modelers need to closely work with the BI application \ndevelopers to determine the exact requirements for the data handoff . Each BI tool \nhas certain sensitivities that should be avoided and certain features that can be \nexploited if the physical data is in the right format. The same considerations apply \nto data prepared for OLAP cubes.\nNOTE \nYou should list all fact and dimension tables that will be directly exposed \nto your BI tools. This should come directly from the dimensional model speciﬁ -\ncation. List all OLAP cubes and special database structures required by BI tools. \nList all known indexes and aggregations you have agreed to build to support BI \nperformance.\nAvailable Skills\nSome  ETL system design decisions must be made on the basis of available resources \nto build and manage the system. You shouldn’t build a system that depends on \n\n\nETL Subsystems and Techniques 449\ncritical C++ processing modules if those programming skills aren’t in-house or can’t \nbe reasonably acquired. Likewise, you may be much more conﬁ dent in building \nthe ETL system around a major vendor’s ETL tool if you already have those skills \nin-house and know how to manage such a project.\nConsider the big decision of whether to hand code the ETL system or use a ven-\ndor’s ETL package. Technical issues and license costs aside, don’t go off  in a direction \nthat your employees and managers ﬁ nd unfamiliar without seriously considering \nthe decision’s long-term implications.\nNOTE \nYou should inventory your department’s operating system, ETL tool, \nscripting language, programming language, SQL, DBMS, and OLAP skills so you \nunderstand how exposed you are to a shortage or loss of these skills. List those \nskills required to support your current systems and your likely future systems.\nLegacy Licenses\nFinally,  in many cases, major design decisions will be made implicitly by senior \nmanagement’s insistence that you use existing legacy licenses. In many cases, this \nrequirement is one you can live with because the environmental advantages are clear \nto everyone. But in a few cases, the use of a legacy license for ETL development is \na mistake. This is a diffi  cult position to be in, and if you feel strongly enough, you \nmay need to bet your job. If you must approach senior management and challenge \nthe use of an existing legacy license, be well prepared in making the case, and be \nwilling to accept the ﬁ nal decision or possibly seek employment elsewhere.\nNOTE \nYou should list your legacy operating system, ETL tool, scripting lan-\nguage, programming language, SQL, DBMS, and OLAP licenses and whether their \nexclusive use is mandated or merely recomm ended.\nThe 34 Subsystems of ETL\nWith  an understanding of the existing requirements, realities, and constraints, \nyou’re ready to learn about the 34 critical subsystems that form the architec-\nture for every ETL system. This chapter describes all 34 subsystems with equal \nemphasis. The next chapter then describes the practical steps of implementing \nthose subsystems needed for each particular situation. Although we have adopted \nthe industry vernacular, ETL, to describe these steps, the process really has four \nmajor components: \n\n\nChapter 19\n450\n \n■Extracting.  Gathering raw data from the source systems and usually writing \nit to disk in the ETL environment before any signiﬁ cant restructuring of the \ndata takes place. Subsystems 1 through 3 support the extracting process.\n \n■Cleaning and conforming.  Sending source data through a series of processing \nsteps in the ETL system to improve the quality of the data received from the \nsource, and merging data from two or more sources to create and enforce con-\nformed dimensions and conformed metrics. Subsystems 4 through 8 describe \nthe architecture required to support the cleaning and conforming processes.\n \n■Delivering.  Physically structuring and loading the data into the presentation \nserver’s target dimensional models. Subsystems 9 through 21 provide the \ncapabilities for delivering the data to the presentation server.\n \n■Managing.  Managing the related systems and processes of the ETL \nenvironment in a coherent manner. Subsystems 22 through 34 describe the \ncomponents needed to support the ongoing management of the ETL system.\nExtracting: Getting Data into the Data \nWarehouse\nTo no surprise, the initial subsystems of the ETL architecture address the issues of \nunderstanding your source data, extracting the data, and transferring it to the data \nwarehouse environment where the ETL system can operate on it independent of \nthe operational systems. Although the remaining subsystems focus on the trans-\nforming, loading, and system management within the ETL environment, the initial \nsubsystems interface to the source systems for access to the required data.\nSubsystem 1: Data Proﬁ ling\nData  proﬁ ling is the technical analysis of data to describe its content, consistency, \nand structure. In some sense, any time you perform a SELECT DISTINCT investiga-\ntive query on a database ﬁ eld, you are doing data proﬁ ling. There are a variety of \ntools speciﬁ cally designed to do powerful proﬁ ling. It probably pays to invest in \na tool rather than roll your own because the tools enable many data relationships \nto be easily explored with simple user interface gestures. You can be much more \nproductive in the data proﬁ ling stages of a project using a tool rather than hand \ncoding all the data content questions.\nData proﬁ ling plays two distinct roles: strategic and tactical. As soon as a can-\ndidate data source is identiﬁ ed, a light proﬁ ling assessment should be made to \ndetermine its suitability for inclusion in the data warehouse and provide an early \ngo/no go decision. Ideally, this strategic assessment should occur immediately after \n\n\nETL Subsystems and Techniques 451\nidentifying a candidate data source during the business requirements analysis. Early \ndisqualiﬁ cation of a data source is a responsible step that can earn you respect from \nthe rest of the team, even if it is bad news. A late revelation that the data source \ndoesn’t support the mission can knock the DW/BI initiative off  its tracks (and be a \npotentially fatal career outcome for you), especially if this revelation occurs months \ninto a project.\nAfter the basic strategic decision is made to include a data source in the project, a \nlengthy tactical data proﬁ ling eff ort should occur to squeeze out as many problems \nas possible. Usually, this task begins during the data modeling process and extends \ninto the ETL system design process. Sometimes, the ETL team is expected to include \na source with content that hasn’t been thoroughly evaluated. Systems may support \nthe needs of the production processes, yet present ETL challenges, because ﬁ elds \nthat aren’t central to production processing may be unreliable and incomplete for \nanalysis purposes. Issues that show up in this subsystem result in detailed speciﬁ -\ncations that are either 1) sent back to the originator of the data source as requests \nfor improvement or 2) form requirements for the data quality processing described \nin subsystems 4 through 8.\nThe proﬁ ling step provides the ETL team with guidance as to how much data \ncleaning machinery to invoke and protects them from missing major project mile-\nstones due to the unexpected diversion of building systems to deal with dirty data. \nDo the data proﬁ ling upfront! Use the data proﬁ ling results to set the business \nsponsors’ expectations regarding realistic development schedules, limitations in the \nsource data, and the need to invest in better source data capture practices.\nSubsystem 2: Change Data Capture System\nDuring  the data warehouse’s initial historic load, capturing source data content \nchanges is not important because you load all data from a point in time forward. \nHowever, many data warehouse tables are so large that they cannot be refreshed dur-\ning every ETL cycle. You must have a capability to transfer only the relevant changes \nto the source data since the last update. Isolating the latest source data is called \nchange data capture (CDC). The idea behind CDC is simple enough: Just transfer \nthe data that has changed since the last load. But building a good CDC system is \nnot as easy as it sounds. The key goals for the change data capture subsystem are:\n \n■Isolate the changed source data to allow selective processing rather than a \ncomplete refresh.\n \n■Capture all changes (deletions, edits, and insertions) made to the source data, \nincluding changes made through nonstandard interfaces.\n \n■Tag changed data with reason codes to distinguish error corrections from \ntrue updates. \n",
      "page_number": 463
    },
    {
      "number": 50,
      "title": "Segment 50 (pages 474-481)",
      "start_page": 474,
      "end_page": 481,
      "detection_method": "topic_boundary",
      "content": "Chapter 19\n452\n \n■Support compliance tracking with additional metadata.\n \n■Perform the CDC step as early as possible, preferably before a bulk data \ntransfer to the data warehouse.\nCapturing data changes is far from a trivial task. You must carefully evaluate \nyour strategy for each data source. Determining the appropriate strategy to identify \nchanged data may take some detective work. The data proﬁ ling tasks described \nearlier can help the ETL team make this determination. There are several ways to \ncapture source data changes, each eff ective in the appropriate situation, including: \nAudit Columns\nIn  some cases, the source system includes audit columns that store the date and time \na record was added or modiﬁ ed. These columns are usually populated via database \ntriggers that are ﬁ red off  automatically as records are inserted or updated. Sometimes, \nfor performance reasons, the columns are populated by the source application instead \nof database triggers. When these ﬁ elds are loaded by any means other than database \ntriggers, pay special attention to their integrity, analyzing and testing each column \nto ensure that it’s a reliable source to indicate change. If you uncover any NULL \nvalues, you must ﬁ nd an alternative approach for detecting change. The most com-\nmon situation that prevents the ETL system from using audit columns is when the \nﬁ elds are populated by the source application, but the DBA team allows back-end \nscripts to modify data. If this occurs in your environment, you face a high risk of \nmissing changed data during the incremental loads. Finally, you need to understand \nwhat happens when a record is deleted from the source because querying the audit \ncolumn may not capture this event.\nTimed Extracts\nWith  a timed extract, you typically select all rows where the create or modiﬁ ed \ndate ﬁ elds equal SYSDATE-1, meaning all of yesterday’s records. Sounds perfect, \nright? Wrong. Loading records based purely on time is a common mistake made by \ninexperienced ETL developers. This process is horribly unreliable. Time-based data \nselection loads duplicate rows when it is restarted from mid-process failures. This \nmeans manual intervention and data cleanup is required if the process fails for any \nreason. Meanwhile, if the nightly load process fails to run and skips a day, there’s a \nrisk that the missed data will never make it into the data warehouse.\nFull Diff Compare\nA  full diff  compare keeps a full snapshot of yesterday’s data, and compares it, record \nby record, against today’s data to ﬁ nd what changed. The good news is this tech-\nnique is thorough: You are guaranteed to ﬁ nd every change. The obvious bad news is \nthat, in many cases, this technique is very resource-intensive. If a full diff  compare \n\n\nETL Subsystems and Techniques 453\nis required, try to do the comparison on the source machine, so you don’t have \nto transfer the entire table or database into the ETL environment. Of course, the \nsource support folks may have an opinion about this. Also, investigate using cyclic \nredundancy checksum (CRC) algorithms to quickly tell if a complex record has \nchanged without examining each individual ﬁ eld.\nDatabase Log Scraping\nLog  scraping eff ectively takes a snapshot of the database redo log at a scheduled \npoint in time (usually midnight) and scours it for transactions aff ecting the tables \nof interest for the ETL load. Sniffi  ng involves a polling of the redo log, capturing \ntransactions on-the-ﬂ y. Scraping the log for transactions is probably the messiest of \nall techniques. It’s not uncommon for transaction logs to get full and prevent new \ntransactions from processing. When this happens in a production transaction envi-\nronment, the knee-jerk reaction from the responsible DBA may be to empty the log \nso that business operations can resume, but when a log is emptied, all transactions \nwithin them are lost. If you’ve exhausted all other techniques and ﬁ nd log scraping \nis your last resort for ﬁ nding new or changed records, persuade the DBA to create \na special log to meet your speciﬁ c needs.\nMessage Queue Monitoring\nIn  a message-based transaction system, the queue is monitored for all transactions \nagainst the tables of interest. The contents of the stream are similar to what you get \nwith log sniffi  ng. One beneﬁ t of this process is relatively low overhead, assuming \nthe message queue is already in place. However, there may be no replay feature \non the message queue. If the connection to the message queue is lost, you lose data.\nSubsystem 3: Extract System\nObviously,  extracting data from the source systems is a fundamental component \nof the ETL architecture. If you are extremely lucky, all the source data will be in a \nsingle system that can be readily extracted using an ETL tool. In the more common \nsituation, each source might be in a diff erent system, environment, and/or DBMS.\nThe ETL system might be expected to extract data from a wide variety of systems \ninvolving many diff erent types of data and inherent challenges. Organizations need-\ning to extract data from mainframe environments often run into issues involving \nCOBOL copybooks, EBCDIC to ASCII conversions, packed decimals, redeﬁ nes, \nOCCURS ﬁ elds, and multiple and variable record types. Other organizations might \nneed to extract from sources in relational DBMS, ﬂ at ﬁ les, XML sources, web logs, \nor a complex ERP system. Each presents a variety of possible challenges. Some \nsources, especially older legacy systems, may require the use of diff erent procedural \nlanguages than the ETL tool can support or the team is experienced with. In this \n\n\nChapter 19\n454\nsituation, request that the owner of the source system extract the data into a ﬂ at \nﬁ le format.\nNOTE \nAlthough XML-formatted data has many advantages because it is self-\ndescribing, you may not want it for large, frequent data transfers. The payload \nportion of a typical XML formatted ﬁ le can be less than 10 percent of the total \nﬁ le. The exception to this recommendation could be where the XML payload is \na complex deeply hierarchical XML structure, such as an industry standard data \nexchange. In these cases, the DW/BI team must decide whether to “shred” the XML \ninto a large number of destination tables or persist the XML structure within the \ndata warehouse. Recent advances in RDBMS vendors’ support for XML via XPath \nhave made this latter option feasible.\nThere are two primary methods for getting data from a source system: as a ﬁ le \nor a stream. If the source is an aging mainframe system, it is often easier to extract \ninto ﬁ les and then move those ﬁ les to the ETL server.\nNOTE \nIf the source data is unstructured, semistructured, or even hyperstruc-\ntured “big data,” then rather than loading such data as an un-interpretable RDBMS \n“blob,” it is often more eff ective to create a MapReduce/Hadoop extract step that \nbehaves as an ETL fact extractor from the source data, directly delivering loadable \nRDBMS data.\nIf you use an ETL tool and the source data is in a database (not necessarily an \nRDBMS), you may set up the extract as a stream where the data ﬂ ows out of the \nsource system, through the transformation engine, and into the staging database \nas a single process. By contrast, an extract to ﬁ le approach consists of three or four \ndiscrete steps: Extract to the ﬁ le, move the ﬁ le to the ETL server, transform the ﬁ le \ncontents, and load the transformed data into the staging database.\nNOTE \nAlthough the stream extract is more appealing, extracts to ﬁ le have some \nadvantages. They are easy to restart at various points. As long as you save the extract \nﬁ le, you can rerun the load without impacting the source system. You can easily \nencrypt and compress the data before transferring across the network. Finally, it \nis easy to verify that all data has moved correctly by comparing ﬁ le row counts \nbefore and after the transfer. Generally, we recommend a data transfer utility such \nas FTP to move the extracted ﬁ le.\nData  compression is important if large amounts of data need to be transferred over \na signiﬁ cant distance or through a public network. In this case, the communications \n\n\nETL Subsystems and Techniques 455\nlink is often the bottleneck. If too much time is spent transmitting the data, com-\npression can reduce the transmission time by 30 to 50 percent or more, depending \non the nature of the original data ﬁ le.\nData encryption is important if data is transferred through a public network, or \neven internally in some situations. If this is the case, it is best to send everything \nthrough an encrypted link and not worry about what needs to be secure and what \ndoesn’t. Remember to compress before encrypting because encrypted ﬁ les do not \ncompress very well.\nCleaning and Conforming Data\nCleaning and conforming data are critical ETL system tasks. These are the steps \nwhere the ETL system adds value to the data. The other activities, extracting and \ndelivering data, are obviously necessary, but they simply move and load the data. \nThe cleaning and conforming subsystems actually change data and enhance its \nvalue to the organization. In addition, these subsystems can be architected to create \nmetadata used to diagnosis what’s wrong with the source systems. Such diagnoses \ncan eventually lead to business process reengineering initiatives to address the root \ncauses of dirty data and improve data quality over time.\nImproving Data Quality Culture and Processes\nIt  is tempting to blame the original data source for any and all errors that appear \ndownstream. If only the data entry clerks were more careful! We are only slightly \nmore forgiving of keyboard-challenged salespeople who enter customer and product \ninformation into their order forms. Perhaps you can ﬁ x data quality problem by \nimposing constraints on the data entry user interfaces. This approach provides a \nhint about how to think about ﬁ xing data quality because a technical solution often \navoids the real problem. Suppose Social Security number ﬁ elds for customers were \noften blank or ﬁ lled with garbage on an input screen. Someone comes up with brilliant \nidea to require input in the 999-99-9999 format, and to cleverly disallow nonsensi-\ncal entries such as all 9s. What happens? The data entry clerks are forced to supply \nvalid Social Security numbers to progress to the next screen, so when they don’t have \nthe customer’s number, they type in an artiﬁ cial number that passes the roadblock.\nMichael Hammer, in his revolutionary book Reengineering the Corporation \n(Collins, revised 2003), struck the heart of the data quality problem with a bril-\nliant observation. Paraphrasing Hammer: “Seemingly small data quality issues are, \nin reality, important indications of broken business processes.” Not only does this \ninsight correctly focus your attention on the source of data quality problems, but it \nalso shows you the way to the solution.\n\n\nChapter 19\n456\nTechnical attempts to address data quality will not prevail unless they are part \nof an overall quality culture that must come from the top of an organization. The \nfamous Japanese car manufacturing quality attitude permeates every level of those \norganizations, and quality is embraced enthusiastically by all levels, from the CEO \nto the assembly line worker. To cast this in a data context, imagine a company such \nas a large drugstore chain, where a team of buyers contracts with thousands of sup-\npliers to provide the inventory. The buyers have assistants, whose job it is to enter \nthe detailed descriptions of everything purchased by the buyers. These descriptions \ncontain dozens of attributes. But the problem is the assistants have a deadly job and \nare judged on how many items they enter per hour. The assistants have almost no \nawareness of who uses their data. Occasionally, the assistants are scolded for obvious \nerrors. But more insidiously, the data given to the assistants is itself incomplete and \nunreliable. For example, there are no formal standards for toxicity ratings, so there \nis signiﬁ cant variation over time and over product categories for this attribute. How \ndoes the drugstore improve data quality? Here is a nine-step template, not only for \nthe drugstore, but for any organization addressing data quality:\n \n■Declare a high-level commitment to a data quality culture.\n \n■Drive process reengineering at the executive level.\n \n■Spend money to improve the data entry environment.\n \n■Spend money to improve application integration.\n \n■Spend money to change how processes work.\n \n■Promote end-to-end team awareness.\n \n■Promote interdepartmental cooperation.\n \n■Publicly celebrate data quality excellence.\n \n■Continuously measure and improve data quality.\nAt the drugstore, money needs to be spent to improve the data entry system, so \nit provides the content and choices needed by the buyers’ assistants. The company’s \nexecutives need to assure the buyers’ assistants that their work is important and \naff ects many decision makers in a positive way. Diligent eff orts by the assistants \nshould be publicly praised and rewarded. And end-to-end team awareness and \nappreciation of the business value derived from quality data is the ﬁ nal goal.\nSubsystem 4: Data Cleansing System\nThe  ETL data cleansing process is often expected to ﬁ x dirty data, yet at the same \ntime the data warehouse is expected to provide an accurate picture of the data as it \nwas captured by the organization’s production systems. Striking the proper balance \nbetween these conﬂ icting goals is essential.\nOne of our goals in describing the cleansing system is to off er a comprehensive \narchitecture for cleansing data, capturing data quality events, as well as measuring \n\n\nETL Subsystems and Techniques 457\nand ultimately controlling data quality in the data warehouse. Some organizations \nmay ﬁ nd this architecture challenging to implement, but we are convinced it is \nimportant for the ETL team to make a serious eff ort to incorporate as many of these \ncapabilities as possible. If you are new to ETL and ﬁ nd this a daunting challenge, \nyou might well wonder, “What’s the minimum I should focus on?” The answer is \nto start by undertaking the best possible data proﬁ ling analysis. The results of that \neff ort can help you understand the risks of moving forward with potentially dirty \nor unreliable data and help you determine how sophisticated your data cleansing \nsystem needs to be.\nThe purpose of the cleansing subsystems is to marshal technology to support \ndata quality. Goals for the subsystem should include:\n \n■Early diagnosis and triage of data quality issues\n \n■Requirements for source systems and integration eff orts to supply better data\n \n■Provide speciﬁ c descriptions of data errors expected to be encountered in ETL\n \n■Framework for capturing all data quality errors and precisely measuring data \nquality metrics over time\n \n■Attachment of quality conﬁ dence metrics to ﬁ nal data\nQuality Screens\nThe  heart of the ETL architecture is a set of quality screens that act as diagnostic \nﬁ lters in the data ﬂ ow pipelines. Each quality screen is a test. If the test against the \ndata is successful, nothing happens and the screen has no side eff ects. But if the test \nfails, then it must drop an error event row into the error event schema and choose \nto either halt the process, send the off ending data into suspension, or merely tag \nthe data.\nAlthough all quality screens are architecturally similar, it is convenient to divide \nthem into three types, in ascending order of scope. Jack Olson, in his seminal book \nData Quality: The Accuracy Dimension (Morgan Kaufmann, 2002), classiﬁ ed data \nquality screens into three categories: column screens, structure screens, and busi-\nness rule screens.\nColumn  screens test the data within a single column. These are usually simple, \nsomewhat obvious tests, such as testing whether a column contains unexpected \nnull values, if a value falls outside of a prescribed range, or if a value fails to adhere \nto a required format.\nStructure  screens test the relationship of data across columns. Two or more \nattributes may be tested to verify they implement a hierarchy, such as a series of \nmany-to-one relationships. Structure screens also test foreign key/primary key rela-\ntionships between columns in two tables, and also include testing whole blocks of \ncolumns to verify they implement valid postal addresses.\n\n\nChapter 19\n458\nBusiness  rule screens implement more complex tests that do not ﬁ t the simpler \ncolumn or structure screen categories. For example, a customer proﬁ le may be tested \nfor a complex time-dependent business rule, such as requiring a lifetime platinum \nfrequent ﬂ yer to have been a member for at least ﬁ ve years and have ﬂ own more \nthan 2 million miles. Business rule screens also include aggregate threshold data \nquality checks, such as checking to see if a statistically improbable number of MRI \nexaminations have been ordered for minor diagnoses like a sprained elbow. In this \ncase, the screen throws an error only after a threshold of such MRI exams is reached.\nResponding to Quality Events\nWe  have already remarked that each quality screen has to decide what happens \nwhen an error is thrown. The choices are: 1) halting the process; 2) sending the \noff ending record(s) to a suspense ﬁ le for later processing; and 3) merely tagging \nthe data and passing it through to the next step in the pipeline. The third choice is \nby far the best choice, whenever possible. Halting the process is obviously a pain \nbecause it requires manual intervention to diagnose the problem, restart or resume \nthe job, or abort completely. Sending records to a suspense ﬁ le is often a poor solu-\ntion because it is not clear when or if these records will be ﬁ xed and re-introduced \nto the pipeline. Until the records are restored to the data ﬂ ow, the overall integrity \nof the database is questionable because records are missing. We recommend not \nusing the suspense ﬁ le for minor data transgressions. The third option of tagging \nthe data with the error condition often works well. Bad fact table data can be tagged \nwith the audit dimension, as described in subsystem 6. Bad dimension data can \nalso be tagged using an audit dimension, or in the case of missing or garbage data \ncan be tagged with unique error values in the attribute itself.\n Subsystem 5: Error Event Schema\nThe  error event schema is a centralized dimensional schema whose purpose is to \nrecord every error event thrown by a quality screen anywhere in the ETL pipeline. \nAlthough we focus on data warehouse ETL processing, this approach can be used in \ngeneric data integration (DI) applications where data is being transferred between \nlegacy applications. The error event schema is shown in Figure 19-1.\nThe main table is the error event fact table. Its grain is every error thrown (pro-\nduced) by a quality screen anywhere in the ETL system. Remember the grain of \na fact table is the physical description of why a fact table row exists. Thus every \nquality screen error produces exactly one row in this table, and every row in the \ntable corresponds to an observed error.\nThe dimensions of the error event fact table include the calendar date of the \nerror, the batch job in which the error occurred, and the screen that produced \nthe error. The calendar date is not a minute and second time stamp of the error, \n\n\nETL Subsystems and Techniques 459\nbut rather provides a way to constrain and summarize error events by the usual \nattributes of the calendar, such as weekday or last day of a ﬁ scal period. The error \ndate/time fact is a full relational date/time stamp that speciﬁ es precisely when the \nerror occurred. This format is useful for calculating the time interval between error \nevents because you can take the diff erence between two date/time stamps to get the \nnumber of seconds separating events.\nDate Dimension\nError Event Date Key (PK)\n...\nBatch Key (PK)\n...\nBatch Dimension\nError Event Key (PK)\nError Event Date Key (FK)\nScreen Key (FK)\nBatch Key (FK)\nError Date/Time\nSeverity Score\nError Event Fact \nScreen Key (PK)\nScreen Type\nETL Module\nScreen Processing Definition\nException Action\nScreen Dimension\nError Event Key (FK)\nError Event Date Key (FK)\nScreen Key (FK)\nBatch Key (FK)\nError Date/Time\nTable Key (FK)\nField Key (FK)\nRecord Identifier Key (FK)\nError Condition\nError Event Detail Fact \nFigure 19-1: Error event schema.\nThe batch dimension can be generalized to be a processing step in cases in which \ndata is streamed, rather than batched. The screen dimension identiﬁ es precisely \nwhat the screen criterion is and where the code for the screen resides. It also deﬁ nes \nwhat to do when the screen throws an error. (For example, halt the process, send \nthe record to a suspense ﬁ le, or tag the data.)\nThe error event fact table also has a single column primary key, shown as the \nerror event key. This surrogate key, like dimension table primary keys, is a simple \ninteger assigned sequentially as rows are added to the fact table. This key column \nis necessary in those situations in which an enormous burst of error rows is added \nto the error event fact table all at once. Hopefully this won’t happen to you.\nThe error event schema includes a second error event detail fact table at a lower \ngrain. Each row in this table identiﬁ es an individual ﬁ eld in a speciﬁ c record that \nparticipated in an error. Thus a complex structure or business rule error that triggers \na single error event row in the higher level error event fact table may generate many \nrows in this error event detail fact table. The two tables are tied together by the error \nevent key, which is a foreign key in this lower grain table. The error event detail \n",
      "page_number": 474
    },
    {
      "number": 51,
      "title": "Segment 51 (pages 482-489)",
      "start_page": 482,
      "end_page": 489,
      "detection_method": "topic_boundary",
      "content": "Chapter 19\n460\ntable identiﬁ es the table, record, ﬁ eld, and precise error condition. Thus a complete \ndescription of complex multi-ﬁ eld, multi-record errors is preserved by these tables.\nThe error event detail table could also contain a precise date/time stamp to \nprovide a full description of aggregate threshold error events where many records \ngenerate an error condition over a period of time. You should now appreciate that \neach quality screen has the responsibility for populating these tables at the time \nof an error.\n Subsystem 6: Audit Dimension Assembler\nThe  audit dimension is a special dimension that is assembled in the back room \nby the ETL system for each fact table, as we discussed in Chapter 6: Order \nManagement. The audit dimension in Figure 19-2 contains the metadata context at \nthe moment when a speciﬁ c fact table row is created. You might say we have elevated \nmetadata to real data! To visualize how audit dimension rows are created, imagine \nthis shipments fact table is updated once per day from a batch ﬁ le. Suppose today \nyou have a perfect run with no errors ﬂ agged. In this case, you would generate only \none audit dimension row, and it would be attached to every fact row loaded today. \nAll the categories, scores, and version numbers would be the same.\nShip Date Key (FK)\nCustomer Key (FK)\nProduct Key (FK)\nMore FKs ...\nAudit Key (FK)\nOrder Number (DD)\nOrder Line Number (DD)\nFacts ...\nAudit Key (PK)\nOverall Quality Rating\nComplete Flag\nValidation Flag\nOut Of Bounds Flag\nScreen Failed Flag\nRecord Modified Flag\nETL Master Version Number\nAllocation Version Number\nShipments Facts\nAudit Dimension\nFigure 19-2: Sample audit dimension attached to a fact table.\nNow let’s relax the strong assumption of a perfect run. If you had some fact \nrows whose discount dollars triggered an out-of-bounds error, then one more audit \ndimension row would be needed to ﬂ ag this condition. \nSubsystem 7: Deduplication System\nOften  dimensions are derived from several sources. This is a common situation \nfor organizations that have many customer-facing source systems that create and \nmanage separate customer master tables. Customer information may need to be \nmerged from several lines of business and outside sources. Sometimes, the data can \nbe matched through identical values in some key column. However, even when a \n\n\nETL Subsystems and Techniques 461\ndeﬁ nitive match occurs, other columns in the data might contradict one another, \nrequiring a decision on which data should survive.\nUnfortunately, there is seldom a universal column that makes the merge operation \neasy. Sometimes, the only clues available are the similarity of several columns. The \ndiff erent sets of data being integrated and the existing dimension table data may need \nto be evaluated on diff erent ﬁ elds to attempt a match. Sometimes, a match may be \nbased on fuzzy criteria, such as names and addresses that may nearly match except \nfor minor spelling diff erences.\nSurvivorship is the process of combining a set of matched records into a uniﬁ ed \nimage that combines the highest quality columns from the matched records into a \nconformed row. Survivorship involves establishing clear business rules that deﬁ ne \nthe priority sequence for column values from all possible source systems to enable the \ncreation of a single row with the best-survived attributes. If the dimensional design \nis fed from multiple systems, you must maintain separate columns with back refer-\nences, such as natural keys, to all participating source systems used to construct \nthe row.\nThere are a variety of data integration and data standardization tools to consider \nif you have diffi  cult deduplicating, matching, and survivorship data issues. These \ntools are quite mature and in widespread use.\n Subsystem 8: Conforming System\nConforming  consists of all the steps required to align the content of some or all the \ncolumns in a dimension with columns in similar or identical dimensions in other \nparts of the data warehouse. For instance, in a large organization you may have fact \ntables capturing invoices and customer service calls that both utilize the customer \ndimension. It is highly likely the source systems for invoices and customer service \nhave separate customer databases. It is likely there will be little guaranteed consis-\ntency between the two sources of customer information. The data from these two \ncustomer sources needs to be conformed to make some or all the columns describing \ncustomer share the same domains. \nNOTE \nThe process of creating conformed dimensions aligns with an agile \napproach. For two dimensions to be conformed, they must share at least one \ncommon attribute with the same name and same contents. You can start with a \nsingle conformed attribute such as Customer Category and systematically add this \ncolumn in a nondisruptive way to customer dimensions in each of the customer-\nfacing processes. As you augment each customer-facing process, you expand the \nlist of processes that are integrated and can participate in drill-across queries. You \ncan also incrementally grow the list of conformed attributes, such as city, state, and \ncountry. All this can be staged to align with a more agile implementation approach.\n\n\nSource 1\nSource 2\nSource 3\nMerged and\nglobally\nDeduped\nExtracted\nusing adapter\nCleaned and\nlocally\nDeduplicated\nConformed and\nSurvived\nConformed\nDimension ready\nfor Delivery\nExtracted\nusing adapter\nCleaned and\nlocally\nDeduplicated\nConformed and\nSurvived\nExtracted\nusing adapter\nCleaned and\nlocally\nDeduplicated\nConformed and\nSurvived\nReplication\nEngine\nd\nd\nd\nd\nd\nd\nd\nd\nd\nd\nd\nd\nd\nd\nd\nd\nSpecial contents:\n1) dimension version number\n2) back pointers to all source\nnatural keys\nFigure 19-3: Deduplicating and survivorship processing for conformed dimension process.\n\n\nETL Subsystems and Techniques 463\nThe conforming subsystem is responsible for creating and maintaining the con-\nformed dimensions and conformed facts described in Chapter 4: Inventory. To \naccomplish this, incoming data from multiple systems needs to be combined and \nintegrated, so it is structurally identical, deduplicated, ﬁ ltered of invalid data, and \nstandardized in terms of content rows in a conformed image. A large part of the \nconforming process is the deduplicating, matching, and survivorship processes \npreviously described. The conforming process ﬂ ow combining the deduplicating \nand survivorship processing is shown in Figure 19-3.\nThe process of defining and delivering conformed dimensions and facts is \ndescribed later in subsystems 17 (dimension manager) and 18 (fact provider). \n Delivering: Prepare for Presentation\nThe  primary mission of the ETL system is the handoff  of the dimension and fact \ntables in the delivery step. For this reason, the delivery subsystems are the most \npivotal subsystems in the ETL architecture. Although there is considerable variation \nin source data structures and cleaning and conforming logic, the delivery process-\ning techniques for preparing the dimensional table structures are more deﬁ ned and \ndisciplined. Use of these techniques is critical to building a successful dimensional \ndata warehouse that is reliable, scalable, and maintainable.\nMany of these subsystems focus on dimension table processing. Dimension tables \nare the heart of the data warehouse. They provide the context for the fact tables and \nhence for all the measurements. Although dimension tables are usually smaller than \nthe fact tables, they are critical to the success of the DW/BI system as they provide the \nentry points into the fact tables. The delivering process begins with the cleaned and \nconformed data resulting from the subsystems just described. For many dimensions, \nthe basic load plan is relatively simple: You perform basic transformations to the \ndata to build dimension rows for loading into the target presentation table. This \ntypically includes surrogate key assignment, code lookups to provide appropriate \ndescriptions, splitting or combining columns to present the appropriate data val-\nues, or joining underlying third normal form table structures into denormalized \nﬂ at dimensions.\nPreparing fact tables is certainly important because fact tables hold the key mea-\nsurements of the business that the users want to see. Fact tables can be large and \ntime-consuming to load. However, preparing fact tables for presentation is typically \nmore straightforward.\n\n\nChapter 19\n464\nSubsystem 9: Slowly Changing Dimension Manager\nOne  of the more important elements of the ETL architecture is the capability to \nimplement slowly changing dimension (SCD) logic. The ETL system must determine \nhow to handle an attribute value that has changed from the value already stored \nin the data warehouse. If the revised description is determined to be a legitimate \nand reliable update to previous information, the appropriate SCD technique must \nbe applied.\nAs described in Chapter 5: Procurement, when the data warehouse receives \nnotiﬁ cation that an existing row in a dimension has changed, there are three basic \nresponses: type 1 overwrite, type 2 add a new row, and type 3 add a new column. \nThe SCD manager should systematically handle the time variance in the dimen-\nsions using these three techniques, as well as the other SCD techniques. In addition, \nthe SCD manager should maintain appropriate housekeeping columns for type 2 \nchanges. Figure 19-4 shows the overall processing ﬂ ow for handling surrogate key \nmanagement for processing SCDs.\nNew record in source\n(not in Cross Ref)\ninsert\nupdate\nupdate\nfield\ntype 1 or 3\nfield is\ntype 2\ninsert\nupdate\nCRC\noptions\nCRCs\ndifferent\nMost Recent\nSurrogate\nKey Map\nMaster\nDimension\nCross Ref\nAssign surrogate\nkeys & set\ndates/indicator\nUpdate\ndimension\nattribute\nUpdate prior\nmost-recent\nrow\nFind specific\nchanged\nfield(s)\nSource\nExtract\nCRC\nCompare\nMaster\nDimension\nCross Ref\nCRCs match\nIgnore\nAssign surrogate\nkeys & set\ndates/indicator\nFigure 19-4: Processing ﬂ ow for SCD surrogate key management.\n\n\nETL Subsystems and Techniques 465\nThe change data capture process described in subsystem 2 obviously plays an \nimportant role in presenting the changed data to the SCD process. Assuming the \nchange data capture process has eff ectively delivered appropriate changes, the SCD \nprocess can take the appropriate actions.\n Type 1: Overwrite\nThe  type 1 technique is a simple overwrite of one or more attributes in an existing \ndimension row. You take the revised data from the change data capture system \nand overwrite the dimension table contents. Type 1 is appropriate when correcting \ndata or when there is no business need to keep the history of previous values. For \ninstance, you may receive a corrected customer address. In this case, overwriting is \nthe right choice. Note that if the dimension table includes type 2 change tracking, \nyou should overwrite the aff ected column in all existing rows for that particular \ncustomer. Type 1 updates must be propagated forward from the earliest permanently \nstored staging tables to all aff ected staging tables, so if any of them are used to re-\ncreate the ﬁ nal load tables, the eff ect of the overwrite is preserved.\nSome ETL tools contain UPDATE else INSERT functionality. This functionality \nmay be convenient for the developer but can be a performance killer. For maximum \nperformance, existing row UPDATEs should be segregated from new row INSERTs. \nIf type 1 updates cause performance problems, consider disabling database logging \nor use of the DBMS bulk loader.\nType 1 updates invalidate any aggregates built upon the changed column, so the \ndimension manager (subsystem 17) must notify the aff ected fact providers (subsys-\ntem 18) to drop and rebuild the aff ected  aggregates.\n Type 2: Add New Row\nThe  type 2 SCD is the standard technique for accurately tracking changes in dimen-\nsions and associating them correctly with fact rows. Supporting type 2 changes \nrequires a strong change data capture system to detect changes as soon as they occur. \nFor type 2 updates, copy the previous version of the dimension row and create a \nnew dimension row with a new surrogate key. If there is not a previous version of \nthe dimension row, create a new one from scratch. Then update this row with the \ncolumns that have changed and add any other columns that are needed. This is \nthe main workhorse technique for handling dimension attribute changes that need \nto be tracked over time.\nThe type 2 ETL process must also update the most recent surrogate key map table, \nassuming the ETL tool doesn’t automatically handle this. These little two-column \n\n\nChapter 19\n466\ntables are of immense importance when loading fact table data. Subsystem 14, the \nsurrogate key pipeline, supports this process.\nRefer to Figure 19-4 to see the lookup and key assignment logic for handling a \nchanged dimension row during the extract process. In this example, the change \ndata capture process (subsystem 2) uses a CRC compare to determine which \nrows have changed in the source data since the last update. If you are lucky, you \nalready know which dimension records have changed and can omit this CRC \ncompare step. After you identify rows that have changes in type 2 attributes, \nyou can generate a new surrogate key from the key sequence and update the \nsurrogate key map table.\nWhen a new type 2 row is created, you need at least a pair of time stamps, as \nwell as an optional change description attribute. The pair of time stamps deﬁ nes a \nspan of time from the beginning eff ective time to the ending eff ective time when \nthe complete set of dimension attributes is valid. A more sophisticated treatment \nof a type 2 SCD row involves adding ﬁ ve ETL housekeeping columns. Referring to \nFigure 19-4, this also requires the type 2 ETL process to ﬁ nd the prior eff ective row \nand make appropriate updates to these housekeeping columns: \n \n■Change Date (change date as foreign key to date dimension outrigger)\n \n■Row Eff ective Date/Time (exact date/time stamp of change)\n \n■Row End Date/Time (exact date/time stamp of next change, defaults to \n12/31/9999 for most current dimension row)\n \n■Reason for Change column (optional attribute)\n \n■Current Flag  (current/expired)\nNOTE \nIt is possible that back-end scripts are run within the transaction data-\nbase to modify data without updating the respective metadata ﬁ elds such as the \nlast_modiﬁ ed_date. Using these ﬁ elds for the dimension time stamps can cause \ninconsistent results in the data warehouse. Always use the system or as-of date to \nderive the type 2 eff ective time stamps.\nThe type 2 process does not change history as the type 1 process does; thus \ntype 2 changes don’t require rebuilding aff ected aggregate tables as long as the \nchange was made “today” and not backward in time.\nNOTE \nKimball Design Tip #80 (available at www.kimballgroup.com under the \nTools and Utilities tab for this book title) provides in-depth guidance on adding a \nrow change reason code attribute to dimension tables.\n\n\nETL Subsystems and Techniques 467\n Type 3: Add New Attribute\nThe  type 3 technique is designed to support attribute “soft” changes that allow a \nuser to refer either to the old value of the attribute or the new value. For example, if \na sales team is assigned to a newly named sales region, there may be a need to track \nthe old region assignment, as well as the new one. The type 3 technique requires the \nETL system to alter the dimension table to add a new column to the schema, if this \nsituation was not anticipated. Of course, the DBA assigned to work with the ETL \nteam will in all likelihood be responsible for this change. You then need to push \nthe existing column values into the newly created column and populate the original \ncolumn with the new values provided to the ETL system. Figure 19-5 shows how \na type 3 SCD is implemented.\n1127648\nA 107B\nDenim\npants\n38\nMen’s\nwear\nProd ID\n(NK)\nProd Key\n(PK)\nProd\nName\nSize\nCategory\nColor\nBlue\nAdd field; Transfer old value\nOverwrite\nwith new\nvalue\n...\n1127648\nA 107B\nDenim\npants\n38\nLeisure\nwear\nMen’s\nwear\nProd ID\n(NK)\nProd Key\n(PK)\nProd\nName\nSize\nCategory\nColor\nPrior\nCategory\nBlue\nFigure 19-5: Type 3 SCD process.\nSimilar to the type 1 process, type 3 change updates invalidate any aggregates \nbuilt upon the changed column; the dimension manager must notify the aff ected \nfact providers, so they drop and rebuild the aff ected aggregates.\n Type 4: Add Mini-Dimension\nThe  type 4 technique is used when a group of attributes in a dimension change suf-\nﬁ ciently rapidly so that they are split off  to a mini-dimension. This situation is some-\ntimes called a rapidly changing monster dimension. Like type 3, this situation calls \nfor a schema change, hopefully done at design time. The mini-dimension requires \nits own unique primary key, and both the primary key of the main dimension and \nthe primary key of the mini-dimension must appear in the fact table. Figure 19-6 \nshows how a type 4 SCD is implemented.\n",
      "page_number": 482
    },
    {
      "number": 52,
      "title": "Segment 52 (pages 490-497)",
      "start_page": 490,
      "end_page": 497,
      "detection_method": "topic_boundary",
      "content": "Chapter 19\n468\nActivity Date Key (FK)\nCustomer Key (FK)\nDemographics Key (FK)\nProduct Key (FK)\nPromotion Key (FK)\nSales Dollars\nSales Units\nCustomer Key (PK)\nCustomer Attributes...\nSales Fact\nCustomer Dimension\nDemographics Key (PK)\nDemographics Attributes...\nDemographics Dimension\nFigure 19-6: Type 4 SCD process.\n Type 5: Add Mini-Dimension and Type 1 Outrigger \nThe  type 5 technique builds on the type 4 mini-dimension by also embedding a type \n1 reference to the mini-dimension in the primary dimension. This allows accessing \nthe current values in the mini-dimension directly from the base dimension without \nlinking through a fact table. The ETL team must add the type 1 key reference in \nthe base dimension and must overwrite this key reference in all copies of the base \ndimension whenever the current status of the mini-dimension changes over time. \nFigure 19-7 shows how a type 5 SCD is implemented.\nActivity Date Key (FK)\nCustomer Key (FK)\nDemographics Key (FK)\nProduct Key (FK)\nPromotion Key (FK)\nSales Dollars\nSales Units\nCustomer Key (PK)\nDemographics Key (FK - SCD 1)\nCustomer Attributes...\nSales Fact\nCustomer Dimension\nDemographics Key (PK)\nDemographics Attributes...\nDemographics Dimension\nFigure 19-7: Type 5 SCD process.\n Type 6: Add Type 1 Attributes to Type 2 Dimension\nThe  type 6 technique has an embedded attribute that is an alternate value of a \nnormal type 2 attribute in the base dimension. Usually such an attribute is simply \na type 3 alternative reality, but in this case the attribute is systematically overwrit-\nten whenever the attribute is updated. Figure 19-8 shows how a type 6 SCD is \nimplemented.\n Type 7: Dual Type 1 and Type 2 Dimensions\nThe  type 7 technique is a normal type 2 dimension paired with a specially con-\nstructed fact table that has both a normal foreign key to the dimension for type 2 \nhistorical processing, and also a foreign durable key (FDK in Figure 19-9) that is \n\n\nETL Subsystems and Techniques 469\nused alternatively for type 1 current processing, connected to the durable key in \nthe dimension table labeled PDK. The dimension table also contains a current row \nindicator that indicates whether the particular row is the one to be used for current \nSCD 1 perspective. The ETL team must augment a normally constructed fact table \nwith this constant value foreign durable key. Figure 19-9 shows how a type 7 SCD \nis implemented.\nActivity Date Key (FK)\nCustomer Key (FK)\nDemographics Key (FK)\nProduct Key (FK)\nPromotion Key (FK)\nSales Dollars\nSales Units\nCustomer Key (PK)\nCustomer Category (SCD 2)\nCurrent Customer Category (SCD 1)\nCustomer Attributes...\nSales Fact\nCustomer Dimension\nFigure 19-8: Type 6 SCD process.\nActivity Date Key (FK)\nCustomer Key (FK)\nCustomer Durable Key (FDK)\nDemographics Key (FK)\nProduct Key (FK)\nPromotion Key (FK)\nSales Dollars\nSales Units\nCustomer Key (PK)\nCustomer Durable Key (PDK)\nCustomer Attributes...\nCurrent Row Indicator\njoin for SCD 2\nIndicator = Current/True for SCD 1\njoin for SCD 1\nSales Fact\nCustomer Dimension\nFigure 19-9: Type 7 SCD process.\n Subsystem 10: Surrogate Key Generator\nAs  you recall from Chapter 3: Retail Sales, we strongly recommend the use of sur-\nrogate keys for all dimension tables. This implies you need a robust mechanism for \nproducing surrogate keys in the ETL system. The surrogate key generator should \nindependently generate surrogate keys for every dimension; it should be independent \nof database instance and able to serve distributed clients. The goal of the surrogate \nkey generator is to generate a meaningless key, typically an integer, to serve as the \nprimary key for a dimension row.\nAlthough it may be tempting to create surrogate keys via database triggers, \nthis technique may create performance bottlenecks. If the DBMS is used to assign \nsurrogate keys, it is preferable for the ETL process to directly call the database \nsequence generator. For improved effi  ciency, consider having the ETL tool generate \n\n\nChapter 19\n470\nand maintain the surrogate keys. Avoid the temptation of concatenating the opera-\ntional key of the source system and a date/time stamp. Although this approach seems \nsimple, it is fraught with problems and ultimately will not scale. \n Subsystem 11: Hierarchy Manager\nIt  is normal for a dimension to have multiple, simultaneous, embedded hierarchi-\ncal structures. These multiple hierarchies simply coexist in the same dimension as \ndimension attributes. All that is necessary is that every attribute be single valued in \nthe presence of the dimension’s primary key. Hierarchies are either ﬁ xed or ragged. \nA ﬁ xed depth hierarchy has a consistent number of levels and is simply modeled and \npopulated as separate dimension attributes for each of the levels. Slightly ragged hier-\narchies like postal addresses are most often modeled as a ﬁ xed hierarchy. Profoundly \nragged hierarchies are typically found with organization structures that are unbal-\nanced and of indeterminate depth. The data model and ETL solution required to \nsupport these needs require the use of a bridge table containing the organization map. \nSnowﬂ akes  or normalized data structures are not recommended for the presenta-\ntion level. However, the use of a normalized design may be appropriate in the ETL \nstaging area to assist in the maintenance of the ETL data ﬂ ow for populating and \nmaintaining the hierarchy attributes. The ETL system is responsible for enforcing \nthe business rules to assure the hierarchy is populated appropriately in the dimen-\nsion  table.\nSubsystem 12: Special Dimensions Manager\nThe  special dimensions manager is a catch-all subsystem: a placeholder in the ETL \narchitecture for supporting an organization’s speciﬁ c dimensional design character-\nistics. Some organizations’ ETL systems require all the capabilities discussed here, \nwhereas others will be concerned with few of these design techniques: \n Date/Time Dimensions\nThe  date and time dimensions are unique in that they are completely speciﬁ ed \nat the beginning of the data warehouse project, and they don’t have a conventional \nsource. This is okay! Typically, these dimensions are built in an afternoon with a \nspreadsheet. But in a global enterprise environment, even this dimension can be \nchallenging when taking into account multiple ﬁ nancial reporting periods or mul-\ntiple cultural calendars.\n Junk Dimensions\nJunk  dimensions are made up from text and miscellaneous ﬂ ags left over in the \nfact table after you remove all the critical attributes. There are two approaches for \n\n\nETL Subsystems and Techniques 471\ncreating junk dimensions in the ETL system. If the theoretical number of rows in \nthe dimension is ﬁ xed and known, the junk dimension can be created in advance. \nIn other cases, it may be necessary to create newly observed junk dimension rows \non-the-ﬂ y while processing fact row input. As illustrated in Figure 19-10, this pro-\ncess requires assembling the junk dimension attributes and comparing them to the \nexisting junk dimension rows to see if the row already exists. If not, a new dimen-\nsion row must be assembled, a surrogate key created, and the row loaded into the \njunk dimension on-the-ﬂ y during the fact table load process.\ncode 1\ncode M\nCodes and indicators encountered in daily load:\nCombine into\nsingle row\nind 1\nind P\n...\n...\nCompare to existing\ndimension rows; insert\nif new\nSurrogate key for this\njunk dimension row\nload\njunk key (FK)\nfact table\nFigure 19-10: Architecture for building junk dimension rows.\nNOTE \nKimball Design Tip #113 (available at www.kimballgroup.com under the \nTools and Utilities tab for this book title) provides more in-depth guidance on \nbuilding and maintaining junk dimension tables.\nMini-Dimensions\nAs  we just discussed in subsystem 9, mini-dimensions are a technique used to \ntrack dimension attribute changes in a large dimension when the type 2 technique \nis infeasible, such as a customer dimension. From an ETL perspective, creation of \n\n\nChapter 19\n472\nthe mini-dimension is similar to the junk dimension process previously described. \nAgain, there are two alternatives: building all valid combinations in advance or rec-\nognizing and creating new combinations on-the-ﬂ y. Although junk dimensions are \nusually built from the fact table input, mini-dimensions are built from dimension \ntable inputs. The ETL system is responsible for maintaining a multicolumn surrogate \nkey lookup table to identify the base dimension member and appropriate mini-\ndimension row to support the surrogate pipeline process described in Subsystem 14, \nSurrogate Key Pipeline. Keep in mind that very large, complex customer dimensions \noften require several mini-dimensions.\nNOTE \nKimball Design Tip #127 (available at www.kimballgroup.com under the \nTools and Utilities tab for this book title) provides more in-depth guidance on \nbuilding and maintaining mini-dimension tables. \n Shrunken Subset Dimensions\nShrunken  dimensions are conformed dimensions that are a subset of rows and/\nor columns of one of your base dimensions. The ETL data ﬂ ow should build \nconformed shrunken dimensions from the base dimension, rather than indepen-\ndently, to assure conformance. The primary key for the shrunken dimension, \nhowever, must be independently generated; if you attempt to use a key from an \n“example” base dimension row, you will get into trouble if this key is retired or \nsuperseded.\nNOTE \nKimball Design Tip #137 (available at www.kimballgroup.com under the \nTools and Utilities tab for this book title) provides more in-depth guidance on \nbuilding shrunken dimension tables. \nSmall Static Dimensions\nA  few dimensions are created entirely by the ETL system without a real outside \nsource. These are usually small lookup dimensions where an operational code is \ntranslated into words. In these cases, there is no real ETL processing. The lookup \ndimension is simply created directly by the ETL team as a relational table in its \nﬁ nal form.\nUser Maintained Dimensions\nOften  the warehouse requires that totally new “master” dimension tables be created. \nThese dimensions have no formal system of record; rather they are custom descrip-\ntions, groupings, and hierarchies created by the business for reporting and analysis \n\n\nETL Subsystems and Techniques 473\npurposes. The ETL team often ends up with stewardship responsibility for these \ndimensions, but this is typically not successful because the ETL team is not aware of \nchanges that occur to these custom groupings, so the dimensions fall into disrepair \nand become ineff ective. The best-case scenario is to have the appropriate business \nuser department agree to own the maintenance of these attributes. The DW/BI team \nneeds to provide a user interface for this maintenance. Typically, this takes the form \nof a simple application built using the company’s standard visual programming tool. \nThe ETL system should add default attribute values for new rows, which the user \nowner needs to update. If these rows are loaded into the warehouse before they are \nchanged, they still appear in reports with whatever default description is supplied.\nNOTE \nThe ETL process should create a unique default dimension attribute \ndescription that shows someone hasn’t yet done their data stewardship job. We \nfavor a label that concatenates the phrase Not Yet Assigned with the surrogate \nkey value: “Not Yet Assigned 157.” That way, multiple unassigned values do not \ninadvertently get lumped together in reports and aggregate tables. This also helps \nidentify the row for later correction.\nSubsystem 13: Fact Table Builders\nFact  tables hold the measurements of an organization. Dimensional models are \ndeliberately built around these numerical measurements. The fact table builder \nsubsystem focuses on the ETL architectural requirements to eff ectively build the \nthree primary types of fact tables: transaction, periodic snapshot, and accumulating \nsnapshot. An important requirement for loading fact tables is maintaining refer-\nential integrity with the associated dimension tables. The surrogate key pipeline \n(subsystem 14) is designed to help support this need.\n Transaction Fact Table Loader\nThe transaction grain represents a measurement event deﬁ ned at a particular instant. \nA line item on an invoice is an example of a transaction event. A scanner event at \na cash register is another. In these cases, the time stamp in the fact table is very \nsimple. It’s either a single daily grain foreign key or a pair consisting of a daily grain \nforeign key together with a date/time stamp, depending on what the source system \nprovides and the analyses require. The facts in this transaction table must be true \nto the grain and should describe only what took place in that instant.\nTransaction grain fact tables are the largest and most detailed of the three types \nof fact tables. The transaction fact table loader receives data from the changed data \ncapture system and loads it with the proper dimensional foreign keys. The pure \n\n\nChapter 19\n474\naddition of the most current records is the easiest case: simply bulk loading new \nrows into the fact table. In most cases, the target fact table should be partitioned by \ntime to ease the administration and speed the performance of the table. An audit \nkey, sequential ID, or date/time stamp column should be included to allow backup \nor restart of the load job.\nThe addition of late arriving data is more diffi  cult, requiring additional process-\ning capabilities described in subsystem 16. In the event it is necessary to update \nexisting rows, this process should be handled in two phases. The ﬁ rst step is to \ninsert the corrected rows without overwriting or deleting the original rows, and \nthen delete the old rows in a second step. Using a sequentially assigned single sur-\nrogate key for the fact table makes it possible to perform the two steps of insertion \nfollowed by deletion.\n Periodic Snapshot Fact Table Loader\nThe  periodic snapshot grain represents a regular repeating measurement or set of \nmeasurements, like a bank account monthly statement. This fact table also has a \nsingle date column, representing the overall period. The facts in this periodic snap-\nshot table must be true to the grain and should describe only measures appropriate \nto the timespan deﬁ ned by the period. Periodic snapshots are a common fact table \ntype and are frequently used for account balances, monthly ﬁ nancial reporting, \nand inventory balances. The periodicity of a periodic snapshot is typically daily, \nweekly, or monthly. \nPeriodic snapshots have similar loading characteristics to those of transaction \ngrain fact tables. The same processing applies for inserts and updates. Assuming \ndata is promptly delivered to the ETL system, all records for each periodic load can \ncluster in the most recent time partition. Traditionally, periodic snapshots have \nbeen loaded en masse at the end of the appropriate period.\nFor example, a credit card company might load a monthly account snapshot table \nwith the balances in eff ect at the end of the month. More frequently, organizations \nwill populate a hot rolling periodic snapshot. In addition to the rows loaded at the \nend of every month, there are special rows loaded with the most current balances \nin eff ect as of the previous day. As the month progresses, the current month rows \nare continually updated with the most current information and continue in this \nmanner rolling through the month. Note that the hot rolling snapshot can some-\ntimes be diffi  cult to implement if the business rules for calculating the balances \nat the period end are complex. Often these complex calculations are dependent \non other periodic processing outside the data warehouse, and there is not enough \ninformation available to the ETL system to perform these complex calculations on \na more frequent basis.\n\n\nETL Subsystems and Techniques 475\n Accumulating Snapshot Fact Table Loader\nThe  accumulating snapshot grain represents the current evolving status of a process \nthat has a ﬁ nite beginning and end. Usually, these processes are of short duration \nand therefore don’t lend themselves to the periodic snapshot. Order processing is \nthe classic example of an accumulating snapshot. The order is placed, shipped, \nand paid for within one reporting period. The transaction grain provides too much \ndetail separated into individual fact table rows, and the periodic snapshot just is \nthe wrong way to report this data.\nThe design and administration of the accumulating snapshot is quite diff erent \nfrom the ﬁ rst two fact table types. All accumulating snapshot fact tables have a \nset of dates which describe the typical process workﬂ ow. For instance, an order \nmight have an order date, actual ship date, delivery date, ﬁ nal payment date, and \nreturn date. In this example, these ﬁ ve dates appear as ﬁ ve separate date-valued \nforeign surrogate keys. When the order row is ﬁ rst created, the ﬁ rst of these dates \nis well deﬁ ned, but perhaps none of the others have yet happened. This same \nfact row is subsequently revisited as the order winds its way through the order \npipeline. Each time something happens, the accumulating snapshot fact row is \ndestructively modiﬁ ed. The date foreign keys are overwritten, and various facts \nare updated. Often the ﬁ rst date remains inviolate because it describes when \nthe row was created, but all the other dates may well be overwritten, sometimes \nmore than once.\nMany RDBMSs utilize variable row lengths. Repeated updates to accumulating \nsnapshot fact rows may cause the rows to grow due to these variable row lengths, \naff ecting the residency of disk blocks. It may be worthwhile to occasionally drop \nand reload rows after the update activity to improve performance.\nAn accumulating snapshot fact table is an effective way to represent finite \nprocesses with well-deﬁ ned beginnings and endings. However, the accumulating \nsnapshot by deﬁ nition is the most recent view. Often it makes sense to utilize all \nthree fact table types to meet various needs. Periodic history can be captured with \nperiodic extracts, and all the inﬁ nite details involved in the process can be captured \nin an associated transaction grain fact table. The presence of many situations that \nviolate standard scenarios or involve repeated looping though the process would \nprohibit the use of an accumulating snapshot.\nSubsystem 14: Surrogate Key Pipeline\nEvery  ETL system must include a step for replacing the operational natural keys \nin the incoming fact table row with the appropriate dimension surrogate \nkeys. Referential integrity (RI) means that for each foreign key in the fact table, \n",
      "page_number": 490
    },
    {
      "number": 53,
      "title": "Segment 53 (pages 498-505)",
      "start_page": 498,
      "end_page": 505,
      "detection_method": "topic_boundary",
      "content": "Chapter 19\n476\nan entry exists in the corresponding dimension table. If there’s a row in a sales \nfact table for product surrogate key 323442, you need to have a row in the product \ndimension table with the same key, or you won’t know what you’ve sold. You have a \nsale for what appears to be a nonexistent product. Even worse, without the product \nkey in the dimension, a business user can easily construct a query that will omit \nthis sale without even realizing it.\nThe key lookup process should result in a match for every incoming natural key \nor a default value. In the event there is an unresolved referential integrity failure dur-\ning the lookup process, you need to feed these failures back to the responsible ETL \nprocess for resolution, as shown in Figure 19-11. Likewise, the ETL process needs to \nresolve any key collisions that might be encountered during the key lookup process. \nDate\nDimension\nReplace\ndate_ID\nwith surrogate\ndate_key\nReplace\nprod_ID\nwith surrogate\nprod_key\nReplace\nstore_ID\nwith surrogate\nstore_key\nReplace\npromo_ID\nwith surrogate\npromo_key\nLoad fact\ntable rows\ninto DBMS\nFact Table\nwith Natural\nKey IDs\ndate_ID\nproduct_ID\nstore_ID\npromo_ID\ndollar_sales\nunit_sales\ndollar_cost\ndate_key\nproduct_key\nstore_key\npromo_key\ndollar_sales\nunit_sales\ndollar_cost\nFact Table\nwith Surrogate\nKeys\nProduct\nDimension\nStore\nDimension\nReferential Integrity Failures\nKey Collisions\nPromotion\nDimension\nFigure 19-11: Replacing fact record’s operational natural keys with dimension \nsurrogate keys.\nAfter the fact table data has been processed and just before loading into the pre-\nsentation layer, a surrogate key lookup needs to occur to substitute the operational \nnatural keys in the fact table record with the proper current surrogate key. To pre-\nserve referential integrity, always complete the updating of the dimension tables \nﬁ rst. In that way, the dimension tables are always the legitimate source of primary \nkeys you must replace in the fact table (refer to Figure 19-11).\nThe most direct approach is to use the actual dimension table as the source for \nthe most current value of the surrogate key corresponding to each natural key. Each \ntime you need the current surrogate key, look up all the rows in the dimension with \nthe natural key equal to the desired value, and then select the surrogate key that \naligns with the historical context of the fact row using the current row indicator or \nbegin and end eff ect dates. Current hardware environments off er nearly unlimited \naddressable memory, making this approach practical.\n\n\nETL Subsystems and Techniques 477\nDuring processing, each natural key in the incoming fact record is replaced with \nthe correct current surrogate key. Don’t keep the natural key in the fact row—the \nfact table needs to contain only the surrogate key. Do not write the input data to \ndisk until all fact rows have passed all the processing steps. If possible, all required \ndimension tables should be pinned in memory, so they can be randomly accessed \nas each incoming record presents its natural keys.\nAs illustrated at the bottom of Figure 19-11, the surrogate key pipeline needs to \nhandle key collisions in the event you attempt to load a duplicate row. This is an \nexample of a data quality problem appropriate for a traditional structure data qual-\nity screen, as discussed in subsystem 4. In the event a key collision is recognized, \nthe surrogate key pipeline process needs to choose to halt the process, send the \noff ending data into suspension, or apply appropriate business rules to determine \nif it is possible to correct the problem, load the row, and write an explanatory row \ninto the error event schema.\nNote a slightly diff erent process is needed to perform surrogate key lookups if \nyou need to reload history or if you have a lot of late arriving fact rows because you \ndon’t want to map the most current value to a historical event. In this case, you need \nto create logic to ﬁ nd the surrogate key that applied at the time the fact record was \ngenerated. This means ﬁ nding the surrogate key where the fact transaction date is \nbetween the key’s eff ective start date and end date.\nWhen the fact table natural keys have been replaced with surrogate keys, the \nfact row is ready to load. The keys in the fact table row have been chosen to be \nproper foreign keys, and the fact table is guaranteed to have referential integrity \nwith respect to the dimension tables.\n Subsystem 15: Multivalued Dimension \nBridge Table Builder\nSometimes  a fact table must support a dimension that takes on multiple values \nat the lowest granularity of the fact table, as described in Chapter 8: Customer \nRelationship Management. If the grain of the fact table cannot be changed to directly \nsupport this dimension, then the multivalued dimension must be linked to the fact \ntable via a bridge table. Bridge tables are common in the healthcare industry, in \nsales commission environments, and for supporting variable depth hierarchies, as \ndiscussed in subsystem 11.\nThe challenge for the ETL team is building and maintaining the bridge table. As \nmultivalued relationships to the fact row are encountered, the ETL system has the \nchoice of either making each set of observations a unique group or reusing groups \nwhen an identical set of observations occurs. Unfortunately, there is no simple \nanswer for the right choice. In the event the multivalued dimension has type 2 \n\n\nChapter 19\n478\nattributes, the bridge table must also be time varying, such as a patient’s time vari-\nant set of diagnoses.\nOne of the bridge table constructs presented in Chapter 10: Financial Services \nwas the inclusion of a weighting factor to support properly weighted reporting from \nthe bridge table. In many cases, the weighting factor is a familiar allocation factor, \nbut in other cases, the identiﬁ cation of the appropriate weighting factor can be prob-\nlematic because there may be no rational basis for assigning the weighting factor.\nNOTE \nKimball Design Tip #142 (available at www.kimballgroup.com under the \nTools and Utilities tab for this book title) provides more in-depth guidance on \nbuilding and maintaining bridge tables.\n Subsystem 16: Late Arriving Data Handler\nData  warehouses are usually built around the ideal assumption that measured activ-\nity (fact records) arrive in the data warehouse at the same time as the context of \nthe activity (dimension records). When you have both the fact records and the \ncorrect contemporary dimension rows, you have the luxury of ﬁ rst maintaining \nthe dimension keys and then using these up-to-date keys in the accompanying fact \nrows. However, for a variety of reasons, the ETL system may need to process late \narriving fact or dimension data.\nIn some environments, there may need to be special modiﬁ cations to the stan-\ndard processing procedures to deal with late arriving facts, namely fact records \nthat come into the warehouse very much delayed. This is a messy situation because \nyou have to search back in history to decide which dimension keys were in eff ect \nwhen the activity occurred. In addition, you may need to adjust any semi-additive \nbalances in subsequent fact rows. In a heavily compliant environment, it is also \nnecessary to interface with the compliance subsystem because you are about to \nchange history.\nLate arriving dimensions occur when the activity measurement (fact record) \narrives at the data warehouse without its full context. In other words, the statuses of \nthe dimensions attached to the activity measurement are ambiguous or unknown for \nsome period of time. If you are living in the conventional batch update cycle of one or \nmore days’ latency, you can usually just wait for the dimensions to be reported. For \nexample, the identiﬁ cation of the new customer may come in a separate feed delayed \nby several hours; you may just be able to wait until the dependency is resolved.\nBut in many situations, especially real-time environments, this delay is not \nacceptable. You cannot suspend the rows and wait for the dimension updates to \noccur; the business requirements demand that you make the fact row visible before \n\n\nETL Subsystems and Techniques 479\nknowing the dimensional context. The ETL system needs additional capabilities \nto support this requirement. Using customer as the problem dimension, the ETL \nsystem needs to support two situations. The ﬁ rst is to support late arriving type 2 \ndimension updates. In this situation, you need to add the revised customer row to \nthe dimension with a new surrogate key and then go in and destructively modify \nany subsequent fact rows’ foreign key to the customer table. The eff ective dates for \nthe aff ected dimension rows also need to be reset. In addition, you need to scan \nforward in the dimension to see if there have been any subsequent type 2 rows for \nthis customer and change this column in any aff ected rows.\nThe second situation occurs when you receive a fact row with what appears to be a \nvalid customer natural key, but you have not yet loaded this customer in the customer \ndimension. It would be possible to load this row pointing to a default row in the \ndimension table. This approach has the same unpleasant side eff ect discussed earlier \nof requiring destructive updates to the fact rows’ foreign keys when the dimension \nupdates are ﬁ nally processed. Alternatively, if you believe the customer is a valid, but \nnot yet processed customer, you should assign a new customer surrogate key with \na set of dummy attribute values in a new customer dimension row. You then return \nto this dummy dimension row at a later time and make type 1 overwrite changes to \nits attributes when you get complete information on the new customer. At least this \nstep avoids destructively changing any fact table keys.\nThere is no way to avoid a brief provisional period in which the dimensions \nare “not quite right.” But these maintenance steps can minimize the impact of the \nunavoidable updates to the keys and other columns.\nSubsystem 17: Dimension Manager System\nThe  dimension manager is a centralized authority who prepares and publishes con-\nformed dimensions to the data warehouse community. A conformed dimension is \nby necessity a centrally managed resource: Each conformed dimension must have a \nsingle, consistent source. It is the dimension manager’s responsibility to administer \nand publish the conformed dimension(s) for which he has responsibility. There \nmay be multiple dimension managers in an organization, each responsible for a \ndimension. The dimension manager’s responsibilities include the following ETL \nprocessing:\n \n■Implement the common descriptive labels agreed to by the data stewards and \nstakeholders during the dimension design.\n \n■Add new rows to the conformed dimension for new source data, generating \nnew surrogate keys.\n \n■Add new rows for type 2 changes to existing dimension entries, generating \nnew surrogate keys.\n\n\nChapter 19\n480\n \n■Modify rows in place for type 1 changes and type 3 changes, without chang-\ning the surrogate keys.\n \n■Update the version number of the dimension if any type 1 or type 3 changes \nare made.\n \n■Replicate the revised dimension simultaneously to all fact table providers.\nIt is easier to manage conformed dimensions in a single tablespace DBMS on a \nsingle machine because there is only one copy of the dimension table. However, \nmanaging conformed dimensions becomes more diffi  cult in multiple tablespace, \nmultiple DMBS, or multimachine distributed environments. In these situations, the \ndimension manager must carefully manage the simultaneous release of new versions \nof the dimension to every fact provider. Each conformed dimension should have a \nversion number column in each row that is overwritten in every row whenever the \ndimension manager releases the dimension. This version number should be utilized \nto support any drill-across queries to assure that the same release of the dimension \nis being utilized.\nSubsystem 18: Fact Provider System\nThe  fact provider is responsible for receiving conformed dimensions from the dimen-\nsion managers. The fact provider owns the administration of one or more fact tables \nand is responsible for their creation, maintenance, and use. If fact tables are used \nin any drill-across applications, then by deﬁ nition the fact provider must be using \nconformed dimensions provided by the dimension manager. The fact provider’s \nresponsibilities are more complex and include:\n \n■Receive or download replicated dimension from the dimension manager.\n \n■In an environment in which the dimension cannot simply be replicated but \nmust be locally updated, the fact provider must process dimension records \nmarked as new and current to update current key maps in the surrogate key \npipeline and also process any dimension records marked as new but postdated.\n \n■Add all new rows to fact tables after replacing their natural keys with correct \nsurrogate keys.\n \n■Modify rows in all fact tables for error correction, accumulating snapshots, \nand late arriving dimension changes.\n \n■Remove aggregates that have become invalidated.\n \n■Recalculate aff ected aggregates. If the new release of a dimension does not \nchange the version number, aggregates have to be extended to handle only \nnewly loaded fact data. If the version number of the dimension has changed, \nthe entire historical aggregate may have to be recalculated.\n\n\nETL Subsystems and Techniques 481\n \n■Quality ensure all base and aggregate fact tables. Be satisﬁ ed the aggregate \ntables are correctly calculated.\n \n■Bring updated fact and dimension tables online.\n \n■Inform users that the database has been updated. Tell them if major changes \nhave been made, including dimension version changes, postdated records \nbeing added, and changes to historical aggregates.\n Subsystem 19: Aggregate Builder\nAggregates  are the single most dramatic way to aff ect performance in a large data \nwarehouse environment. Aggregations are like indexes; they are speciﬁ c data struc-\ntures created to improve performance. Aggregates can have a signiﬁ cant impact \non performance. The ETL system needs to eff ectively build and use aggregates \nwithout causing signiﬁ cant distraction or consuming extraordinary resources and \nprocessing cycles. \nYou should avoid architectures in which aggregate navigation is built into the \nproprietary query tool. From an ETL viewpoint, the aggregation builder needs to \npopulate and maintain aggregate fact table rows and shrunken dimension tables \nwhere needed by aggregate fact tables. The fastest update strategy is incremental, \nbut a major change to a dimension attribute may require dropping and rebuild-\ning the aggregate. In some environments, it may be faster to dump data out of the \nDBMS and build aggregates with a sort utility rather than building the aggregates \ninside the DBMS. Additive numeric facts can be aggregated easily at extract time \nby calculating break rows in one of the sort packages. Aggregates must always be \nconsistent with the atomic base data. The fact provider (subsystem 18) is respon-\nsible for taking aggregates off -line when they are not consistent with the base data.\nUser feedback on the queries that run slowly is critical input to designing aggrega-\ntions. Although you can depend on informal feedback to some extent, a log of frequently \nattempted slow-running queries should be captured. You should also try to identify the \nnonexistent slow-running queries that never made it into the log because they never \nrun to completion, or aren’t even attempted due to known performance challenges.\n Subsystem 20: OLAP Cube Builder\nOLAP  servers present dimensional data in an intuitive way, enabling a range of \nanalytic users to slice and dice data. OLAP is a sibling of dimensional star schemas \nin the relational database, with intelligence about relationships and calculations \ndeﬁ ned on the server that enable faster query performance and more interesting \nanalytics from a broad range of query tools. Don’t think of an OLAP server as a \n\n\nChapter 19\n482\ncompetitor to a relational data warehouse, but rather an extension. Let the relational \ndatabase do what it does best: Provide storage and management.\nThe relational dimensional schema should be viewed as the foundation for OLAP \ncubes if you elect to include them in your architecture. The process of feeding data \nfrom the dimensional schema is an integral part of the ETL system; the relational \nschemas are the best and preferred source for OLAP cubes. Because many OLAP \nsystems do not directly address referential integrity or data cleaning, the pre-\nferred architecture is to load OLAP cubes after the completion of conventional ETL \nprocesses. Note that some OLAP tools are more sensitive to hierarchies than rela-\ntional schemas. It is important to strongly enforce the integrity of hierarchies within \ndimensions before loading an OLAP cube. Type 2 SCDs ﬁ t an OLAP system well \nbecause a new surrogate key is just treated as a new member. Type 1 SCDs that restate \nhistory do not ﬁ t OLAP well. Overwrites to an attribute value can cause all the cubes \nusing that dimension to be reprocessed in the background, become corrupted, or be \ndropped. Read this last sentence again.\nSubsystem 21: Data Propagation Manager\nThe  data propagation manager is responsible for the ETL processes required to \npresent conformed, integrated enterprise data from the data warehouse presenta-\ntion server to other environments for special purposes. Many organizations need \nto extract data from the presentation layer to share with business partners, cus-\ntomers, and/or vendors for strategic purposes. Similarly, some organizations are \nrequired to submit data to various government organizations for reimbursement \npurposes, such as healthcare organizations that participate in the Medicare pro-\ngram. Many organizations have acquired package analytic applications. Typically, \nthese applications cannot be pointed directly against the existing data warehouse \ntables, so data needs to be extracted from the presentation layer and loaded into \nproprietary data structures required by the analytic applications. Finally, most \ndata mining tools do not run directly against the presentation server. They need \ndata extracted from the data warehouse and fed to the data mining tool in a \nspeciﬁ c format.\nAll the situations previously described require extraction from the DW/BI \npresentation server, possibly some light transformation, and loading into a target \nformat—in other words ETL. Data propagation should be considered a part of the \nETL system; ETL tools should be leveraged to provide this capability. What is dif-\nferent in this situation is that the requirements of the target are not negotiable; you \nmust provide the data as speciﬁ ed by the target.\n\n\nETL Subsystems and Techniques 483\nManaging the ETL Environment\nA  DW/BI environment can have a great dimensional model, well-deployed BI applica-\ntions, and strong management sponsorship. But it cannot be a success until it can be \nrelied upon as a dependable source for business decision making. One of the goals \nfor the DW/BI system is to build a reputation for providing timely, consistent, and \nreliable data to empower the business. To achieve this goal, the ETL system must \nconstantly work toward fulﬁ lling three criteria:\n \n■Reliability. The ETL processes must consistently run. They must run to \ncompletion to provide data on a timely basis that is trustworthy at any level \nof detail.\n \n■Availability. The data warehouse must meet its service level agreements \n(SLAs). The warehouse should be up and available as promised. \n \n■Manageability. A successful data warehouse is never done. It constantly grows \nand changes along with the business. The ETL processes need to gracefully \nevolve as well.\nThe ETL management subsystems are the key components of the architecture \nto help achieve the goals of reliability, availability, and manageability. Operating \nand maintaining a data warehouse in a professional manner is not much diff erent \nthan any other systems operations: Follow standard best practices, plan for disaster, \nand practice. Most of the requisite management subsystems that follow might be \nfamiliar to you. \nSubsystem 22: Job Scheduler\nEvery  enterprise data warehouse should have a robust ETL scheduler. The entire ETL \nprocess should be managed, to the extent possible, through a single metadata-driven \njob control environment. Major ETL tool vendors package scheduling capabilities \ninto their environments. If you elect not to use the scheduler included with the ETL \ntool, or do not use an ETL tool, you need to utilize existing production scheduling \nor perhaps manually code the ETL jobs to execute.\nScheduling is much more than just launching jobs on a schedule. The scheduler \nneeds to be aware of and control the relationships and dependencies between ETL \njobs. It needs to recognize when a ﬁ le or table is ready to be processed. If the orga-\nnization is processing in real time, you need a scheduler that supports your selected \nreal-time architecture. The job control process must also capture metadata regard-\ning the progress and statistics of the ETL process during its execution. Finally, the \n",
      "page_number": 498
    },
    {
      "number": 54,
      "title": "Segment 54 (pages 506-513)",
      "start_page": 506,
      "end_page": 513,
      "detection_method": "topic_boundary",
      "content": "Chapter 19\n484\nscheduler should support a fully automated process, including notifying the problem \nescalation system in the event of any situation that requires resolution.\nThe infrastructure to manage this can be as basic (and labor-intensive) as a set of \nSQL stored procedures, or as sophisticated as an integrated tool designed to manage \nand orchestrate multiplatform data extract and loading processes. If you use an ETL \ntool, it should provide this capability. In any case, you need to set up an environ-\nment for creating, managing, and monitoring the ETL job stream. \nThe job control services needed include:\n \n■Job deﬁ nition. The ﬁ rst step in creating an operations process is to have some \nway to deﬁ ne a series of steps as a job and to specify some relationship among \njobs. This is where the execution ﬂ ow of the ETL process is written. In many \ncases, if the load of a given table fails, it can impact your ability to load tables \nthat depend on it. For example, if the customer table is not properly updated, \nloading sales facts for new customers that did not make it into the customer \ntable is risky. In some databases, it is impossible.\n \n■Job scheduling. At a minimum, the environment needs to provide standard \ncapabilities, such as time- and event-based scheduling. ETL processes are often \nbased on some upstream system event, such as the successful completion of \nthe general ledger close or the successful application of sales adjustments to \nyesterday’s sales ﬁ gures. This includes the ability to monitor database ﬂ ags, \ncheck for the existence of ﬁ les, and compare creation dates.\n \n■Metadata capture. No self-respecting systems person would tolerate a black \nbox scheduling system. The folks responsible for running the loads will \ndemand a workﬂ ow monitoring system (subsystem 27) to understand what \nis going on. The job scheduler needs to capture information about what step \nthe load is on, what time it started, and how long it took. In a handcrafted \nETL system, this can be accomplished by having each step write to a log ﬁ le. \nThe ETL tool should capture this data every time an ETL process executes.\n \n■Logging. This means collecting information about the entire ETL process, \nnot just what is happening at the moment. Log information supports the \nrecovery and restarting of a process in case of errors during the job execution. \nLogging to text ﬁ les is the minimum acceptable level. We prefer a system that \nlogs to a database because the structure makes it easier to create graphs and \nreports. It also makes it possible to create time series studies to help analyze \nand optimize the load process.\n \n■Notiﬁ cation. After the ETL process has been developed and deployed, it \nshould execute in a hands-off  manner. It should run without human inter-\nvention, without fail. If a problem does occur, the control system needs to \ninterface to the problem escalation system (subsystem 30).\n\n\nETL Subsystems and Techniques 485\nNOTE \nSomebody needs to know if anything unforeseen happened during the \nload, especially if a response is critical to continuing the process.\nSubsystem 23: Backup System\nThe  data warehouse is subject to the same risks as any other computer system. Disk \ndrives will fail, power supplies will go out, and sprinkler systems will accidentally \nturn on. In addition to these risks, the warehouse also has a need to keep more \ndata for longer periods of time than operational systems. Although typically not \nmanaged by the ETL team, the backup and recovery process is often designed as \npart of the ETL system. Its goal is to allow the data warehouse to get back to work \nafter a failure. This includes backing up the intermediate staging data necessary to \nrestart failed ETL jobs. The archive and retrieval process is designed to enable user \naccess to older data that has been moved out of the main warehouse onto a less \ncostly, usually lower-performing media.\nBackup\nEven if you have a fully redundant system with a universal power supply, fully RAIDed \ndisks, and parallel processors with failover, some system crisis will eventually visit. \nEven with perfect hardware, someone can always drop the wrong table (or database). \nAt the risk of stating the obvious, it is better to prepare for this than to handle it on-\nthe-ﬂ y. A full scale backup system needs to provide the following capabilities:\n \n■High performance.  The backup needs to ﬁ t into the allotted timeframe. This \nmay include online backups that don’t impact performance signiﬁ cantly, \nincluding real-time partitions. \n \n■Simple administration. The  administration interface should provide tools that \neasily allow you to identify objects to back up (including tables, tablespaces, \nand redo logs), create schedules, and maintain backup veriﬁ cation and logs \nfor subsequent restore.\n \n■Automated, lights-out operations. The  backup facility must provide storage \nmanagement services, automated scheduling, media and device handling, \nreporting, and notiﬁ cation.\nThe backup for the warehouse is usually a physical backup. This is an image \nof the database at a certain point in time, including indexes and physical layout \ninformation.\nArchive and Retrieval\nDeciding  what to move out of the warehouse is a cost-beneﬁ t issue. It costs money \nto keep the data around—it takes up disk space and slows the load and query \n\n\nChapter 19\n486\ntimes. On the other hand, the business users just might need this data to do some \ncritical historical analyses. Likewise an auditor may request archived data as part \nof a compliance procedure. The solution is not to throw the data away but to put \nit some place that costs less but is still accessible. Archiving is the data security \nblanket for the warehouse.\nAs of this writing, the cost of online disk storage is dropping so rapidly that it \nmakes sense to plan many of archiving tasks to simply write to disk. Especially if \ndisk storage is handled by a separate IT resource, the requirement to “migrate and \nrefresh” is replaced by “refresh.” You need to make sure that you can interpret the \ndata at various points in the future.\nHow long it takes the data to get stale depends on the industry, the business, and \nthe particular data in question. In some cases, it is fairly obvious when older data \nhas little value. For example, in an industry with rapid evolution of new products \nand competitors, history doesn’t necessarily help you understand today or predict \ntomorrow. \nAfter a determination has been made to archive certain data, the issue becomes \n“what are the long-term implications of archiving data?” Obviously, you need \nto leverage existing mechanisms to physically move the data from its current media to \nanother media and ensure it can be recovered, along with an audit trail that accounts \nfor the accesses and alterations to the data. But what does it mean to “keep” old data? \nGiven increasing audit and compliance concerns, you may face archival require-\nments to preserve this data for ﬁ ve, 10, or perhaps even 50 years. What media should \nyou utilize? Will you be able to read that media in future years? Ultimately, you \nmay ﬁ nd yourself implementing a library system capable of archiving and regularly \nrefreshing the data, and then migrating it to more current structures and media.\nFinally, if you are archiving data from a system that is no longer going to be used, \nyou may need to “sunset” the data by extracting it from the system and writing it \nin a vanilla format that is independent of the original application. You might need \nto do this if the license to use the application will terminate.\n Subsystem 24: Recovery and Restart System\nAfter  the ETL system is in production, failures can occur for countless reasons \nbeyond the control of the ETL process. Common causes of ETL production failures \ninclude: \n \n■Network failure\n \n■Database failure\n \n■Disk failure\n \n■Memory failure\n \n■Data quality failure\n \n■Unannounced system upgrade \n\n\nETL Subsystems and Techniques 487\nTo protect yourself from these failures, you need a solid backup system \n(subsystem 23) and a companion recovery and restart system. You must plan for \nunrecoverable errors during the load because they will happen. The system should \nanticipate this and provide crash recovery, stop, and restart capability. First, look \nfor appropriate tools and design processes to minimize the impact of a crash. For \nexample, a load process should commit relatively small sets of records at a time and \nkeep track of what has been committed. The size of the set should be adjustable \nbecause the transaction size has performance implications on diff erent DBMSs.\nThe recovery and restart system is used, of course, for either resuming a job that \nhas halted or for backing out the whole job and restarting it. This system is signiﬁ -\ncantly dependent on the capabilities of the backup system. When a failure occurs, \nthe initial knee-jerk reaction is to attempt to salvage whatever has processed and \nrestart the process from that point. This requires an ETL tool with solid and reli-\nable checkpoint functionality, so it can perfectly determine what has processed and \nwhat has not to restart the job at exactly the right point. In many cases, it may be \nbest to back out any rows that have been loaded as part of the process and restart \nfrom the beginning.\nWe often recommend designing fact tables with a single column primary sur-\nrogate key. This surrogate key is a simple integer that is assigned in sequence as \nrows are created to be added to the fact table. With the fact table surrogate key, \nyou can easily resume a load that is halted or back out all the rows in the load by \nconstraining on a range of surrogate keys.\nNOTE \nFact table surrogate keys have a number of uses in the ETL back room. \nFirst, as previously described, they can be used as the basis for backing out or \nresuming an interrupted load. Second, they provide immediate and unambiguous \nidentiﬁ cation of a single fact row without needing to constrain multiple dimen-\nsions to fetch a unique row. Third, updates to fact table rows can be replaced by \ninserts plus deletes because the fact table surrogate key is now the actual key for \nthe fact table. Thus, a row containing updated columns can be inserted into the \nfact table without overwriting the row it is to replace. When all such insertions are \ncomplete, then the underlying old rows can be deleted in a single step. Fourth, the \nfact table surrogate key is an ideal parent key to be used in a parent/child design. \nThe fact table surrogate key appears as a foreign key in the child, along with the \nparent’s dimension foreign keys.\nThe longer an ETL process runs, the more you must be aware of vulnerabilities \ndue to failure. Designing a modular ETL system made up of effi  cient processes that \nare resilient against crashes and unexpected terminations can reduce the risk of \na failure resulting in a massive recovery eff ort. Careful consideration of when to \nphysically stage data by writing it to disk, along with carefully crafted points of \n\n\nChapter 19\n488\nrecovery and load date/time stamps or sequential fact table surrogate keys enable \nyou to specify appropriate restart logic.\nSubsystem 25: Version Control System \nThe  version control system is a “snapshotting” capability for archiving and recover-\ning all the logic and metadata of the ETL pipeline. It controls check-out and check-in \nprocessing for all ETL modules and jobs. It should support source comparisons to \nreveal diff erences between versions. This system provides a librarian function for \nsaving and restoring the complete ETL context of a single version. In certain highly \ncompliant environments, it will be equally important to archive the complete ETL \nsystem context alongside the relevant archived and backup data. Note that master \nversion numbers need to be assigned for the overall ETL system, just like software \nrelease version numbers.\nNOTE \nYou have a master version number for each part of the ETL system as \nwell as one for the system as a whole, don’t you? And you can restore yesterday’s \ncomplete ETL metadata context if it turns out there is a big mistake in the current \nrelease? Thank you for reassuring us.\nSubsystem 26: Version Migration System\nAfter  the ETL team gets past the diffi  cult process of designing and developing the ETL \nprocess and completes the creation of the jobs required to load the data warehouse, the \njobs must be bundled and migrated to the next environment—from development to \ntest and on to production—according to the lifecycle adopted by the organization. The \nversion migration system needs to interface to the version control system to control \nthe process and back out a migration if needed. It should provide a single interface \nfor setting connection information for the entire version.\nMost organizations isolate the development, testing, and production environments. \nYou need to be able to migrate a complete version of the ETL pipeline from devel-\nopment, into test, and ﬁ nally into production. Ideally, the test system is identically \nconﬁ gured to its corresponding production system. Everything done to the production \nsystem should have been designed in development and the deployment script tested \non the test environment. Every back room operation should go through rigorous \nscripting and testing, whether deploying a new schema, adding a column, changing \nindexes, changing the aggregate design, modifying a database parameter, backing \nup, or restoring. Centrally managed front room operations such as deploying new BI \ntools, deploying new corporate reports, and changing security plans should be equally \nrigorously tested and scripted if the BI tools allow it.\n\n\nETL Subsystems and Techniques 489\nSubsystem 27: Workﬂ ow Monitor\nSuccessful  data warehouses are consistently and reliably available, as agreed to with \nthe business community. To achieve this goal, the ETL system must be constantly \nmonitored to ensure the ETL processes are operating effi  ciently and the warehouse \nis being loaded on a consistently timely basis. The job scheduler (subsystem 22) \nshould capture performance data every time an ETL process is initiated. This data \nis part of the process metadata captured in the ETL system. The workﬂ ow monitor \nleverages the metadata captured by the job scheduler to provide a dashboard and \nreporting system taking many aspects of the ETL system into consideration. You’ll \nwant to monitor job status for all job runs initiated by the job scheduler including \npending, running, completed and suspended jobs, and capture the historical data \nto support trending performance over time. Key performance measures include \nthe number of records processed, summaries of errors, and actions taken. Most \nETL tools capture the metrics for measuring ETL performance. Be sure to trigger \nalerts whenever an ETL job takes signiﬁ cantly more or less time to complete than \nindicated by the historical record.\nIn combination with the job scheduler, the workﬂ ow monitor should also track \nperformance and capture measurements of the performance of infrastructure com-\nponents including CPU usage, memory allocation and contention, disk utilization \nand contention, buff er pool usage, database performance, and server utilization and \ncontention. Much of this information is process metadata about the ETL system \nand should be considered as part of the overall metadata strategy (subsystem 34).\nThe workﬂ ow monitor has a more signiﬁ cant strategic role than you might sus-\npect. It is the starting point for the analysis of performance problems across the \nETL pipeline. ETL performance bottlenecks can occur in many places, and a good \nworkﬂ ow monitor shows where the bottlenecks are occurring. Chapter 20, discusses \nmany ways to improve performance in the ETL pipeline, but this list is more or less \nordered starting with the most important bottlenecks:\n \n■Poorly indexed queries against a source system or intermediate table\n \n■SQL syntax causing wrong optimizer choice\n \n■Insuffi  cient random access memory (RAM) causing thrashing\n \n■Sorting in the RDBMS\n \n■Slow transformation steps\n \n■Excessive I/O\n \n■Unnecessary writes followed by reads\n \n■Dropping and rebuilding aggregates from scratch rather than incrementally\n \n■Filtering (change data capture) applied too late in the pipeline\n \n■Untapped opportunities for parallelizing and pipelining\n\n\nChapter 19\n490\n \n■Unnecessary transaction logging especially if doing updates\n \n■Network traffi  c and ﬁ le transfer overhead\nSubsystem 28: Sorting System\nCertain  common ETL processes call for data to be sorted in a particular order, \nsuch as aggregating and joining ﬂ at ﬁ le sources. Because sorting is such a funda-\nmental ETL processing capability, it is called out as a separate subsystem to ensure \nit receives proper attention as a component of the ETL architecture. There are a \nvariety of technologies available to provide sorting capabilities. An ETL tool can \nundoubtedly provide a sort function, the DBMS can provide sorting via the SQL \nSORT clause, and there are a number of sort utilities available.\nSorting simple delimited text ﬁ les with a dedicated sort package is awesomely \nfast. These packages typically allow a single read operation to produce up to eight \ndiff erent sorted outputs. Sorting can produce aggregates where each break row of a \ngiven sort is a row for the aggregate table, and sorting plus counting is often a good \nway to diagnose data quality issues.\nThe key is to choose the most effi  cient sort resource to support the requirements \nwithin your infrastructure. The easy answer for most organizations is to simply \nutilize the ETL tool’s sort function. However, in some situations it may be more \neffi  cient to use a dedicated sort package; although ETL and DBMS vendors claim to \nhave made up much of the performance diff erences.\nSubsystem 29: Lineage and Dependency Analyzer\nTwo  increasingly important elements being requested of the ETL system are the \nability to track both the lineage and dependencies of data in the DW/BI system:\n \n■Lineage. Beginning with a speciﬁ c data element in an intermediate table or BI \nreport, identify the source of that data element, other upstream intermediate \ntables containing that data element and its sources, and all transformations \nthat data element and its sources have undergone.\n \n■Dependency. Beginning with a speciﬁ c data element in a source table or an \nintermediate table, identify all downstream intermediate tables and ﬁ nal BI \nreports containing that data element or its derivations and all transformations \napplied to that data element and its derivations.\nLineage analysis is often an important component in a highly compliant environ-\nment where you must explain the complete processing ﬂ ow that changed any data \nresult. This means the ETL system must display the ultimate physical sources and \nall subsequent transformations of any selected data element, chosen either from the \n\n\nETL Subsystems and Techniques 491\nmiddle of the ETL pipeline or on a ﬁ nal delivered report. Dependency analysis is \nimportant when assessing changes to a source system and the downstream impacts \non the data warehouse and ETL system. This implies the ability to display all aff ected \ndownstream data elements and ﬁ nal report ﬁ elds aff ected by a potential change in \nany selected data element, chosen either in the middle of the ETL pipeline or an \noriginal source (dependency).\nSubsystem 30: Problem Escalation System\nTypically,  the ETL team develops the ETL processes and the quality assurance \nteam tests them thoroughly before they are turned over to the group responsible \nfor day-to-day systems operations. To make this work, the ETL architecture needs \nto include a proactively designed problem escalation system similar to what is in \nplace for other production systems.\nAfter the ETL processes have been developed and tested, the ﬁ rst level of oper-\national support for the ETL system should be a group dedicated to monitoring \nproduction applications. The ETL development team becomes involved only if the \noperational support team cannot resolve a production problem.\nIdeally, you have developed ETL processes, wrapped them into an automated \nscheduler, and have robust workﬂ ow monitoring capabilities peering into the ETL \nprocesses as they execute. The execution of the ETL system should be a hands-off  \noperation. It should run like clockwork without human intervention and without \nfail. If a problem does occur, the ETL process should automatically notify the \nproblem escalation system of any situation that needs attention or resolution. \nThis automatic feed may take the form of simple error logs, operator notiﬁ cation \nmessages, supervisor notiﬁ cation messages, and system developer messages. The \nETL system may notify an individual or a group depending on the severity of the \nsituation or the processes involved. ETL tools can support a variety of messag-\ning capabilities including e-mail alerts, operator messages, and notiﬁ cations to \nmobile devices.\nEach notiﬁ cation event should be written to a database used to understand the \ntypes of problems that arise, their status, and resolution. This data forms part of \nthe process metadata captured by the ETL system (subsystem 34). You need to \nensure that organizational procedures are in place for proper escalation, so every \nproblem is resolved appropriately.\nIn general, the support structure for the ETL system should follow a fairly stan-\ndard support structure. First, level support is typically a help desk that is the ﬁ rst \npoint of contact when a user notices an error. The help desk is responsible for \nresolution whenever feasible. If the help desk cannot resolve the issue, the sec-\nond level support is notiﬁ ed. This is typically a systems administrator or DBA on \n",
      "page_number": 506
    },
    {
      "number": 55,
      "title": "Segment 55 (pages 514-521)",
      "start_page": 514,
      "end_page": 521,
      "detection_method": "topic_boundary",
      "content": "Chapter 19\n492\nthe production control technical staff  capable of supporting general infrastructure \nfailures. The ETL manager is the third level support and should be knowledgeable \nto support most issues that arise in the ETL production process. Finally, when all \nelse fails, the ETL developer should be called in to analyze the situation and assist \nwith resolution. \nSubsystem 31: Parallelizing/Pipelining System\nThe  goal of the ETL system, in addition to providing high quality data, is to load the \ndata warehouse within the allocated processing window. In large organizations with \nhuge data volumes and a large portfolio of dimensions and facts, loading the data \nwithin these constraints can be a challenge. The paralleling/pipelining system pro-\nvides capabilities to enable the ETL system to deliver within these time constraints. \nThe goal of this system is to take advantage of multiple processors or grid computing \nresources commonly available. It is highly desirable, and in many cases necessary, that \nparallelizing and pipelining be automatically invoked for every ETL process unless \nspeciﬁ c conditions preclude it from processing in such a manner, such as waiting on \na condition in the middle of the process.\nParallelizing is a powerful performance technique at every stage of the ETL \npipeline. For example, the extraction process can be parallelized by logically parti-\ntioning on ranges of an attribute. Verify that the source DBMS handles parallelism \ncorrectly and doesn’t spawn conﬂ icting processes. If possible, choose an ETL tool \nthat handles parallelizing of intermediate transformation processes automatically. \nIn some tools it is necessary to hand create parallel processes. This is ﬁ ne until \nyou add additional processors, and the ETL system then can’t take advantage of the \ngreater parallelization opportunities unless you modify the ETL modules by hand \nto increase the number of parallel ﬂ ows.\nSubsystem 32: Security System\nSecurity  is an important consideration for the ETL system. A serious security breach \nis much more likely to come from within the organization than from someone hack-\ning in from the outside. Although we don’t like to think it, the folks on the ETL \nteam present as much a potential threat as any group inside the organization. We \nrecommend administering role-based security on all data and metadata in the ETL \nsystem. To support compliance requirements, you may need to prove that a version \nof an ETL module hasn’t been changed or show who made changes to a module. \nYou should enforce comprehensive authorized access to all ETL data and metadata \nby individual and role. In addition, you’ll want to maintain a historical record of \nall accesses to ETL data and metadata by individual and role. Another issue to be \ncareful of is the bulk data movement process. If you move data across the network, \n\n\nETL Subsystems and Techniques 493\neven if it is within the company ﬁ rewall, it pays to be careful. Make sure to use data \nencryption or a ﬁ le transfer utility that uses a secure transfer protocol.\nAnother back room security issue to consider is administrator access to the \nproduction warehouse server and software. We’ve seen situations where no one on \nthe team had security privileges; in other cases, everyone had access to everything. \nObviously, many members of the team should have privileged access to the devel-\nopment environment, but the production warehouse should be strictly controlled. \nOn the other hand, someone from the DW/BI team needs to be able to reset the \nwarehouse machine if something goes wrong. Finally, the backup media should be \nguarded. The backup media should have as much security surrounding them as \nthe online systems.\nSubsystem 33: Compliance Manager\nIn  highly compliant environments, supporting compliance requirements is a sig-\nniﬁ cant new requirement for the ETL team. Compliance in the data warehouse \ninvolves “maintaining the chain of custody” of the data. In the same way a police \ndepartment must carefully maintain the chain of custody of evidence to argue that \nthe evidence has not been changed or tampered with, the data warehouse must also \ncarefully guard the compliance-sensitive data entrusted to it from the moment it \narrives. Furthermore, the data warehouse must always show the exact condition and \ncontent of such data at any point in time that it may have been under the control \nof the data warehouse. The data warehouse must also track who had authorized \naccess to the data. Finally, when the suspicious auditor looks over your shoulder, \nyou need to link back to an archived and time-stamped version of the data as it \nwas originally received, which you have stored remotely with a trusted third party. \nIf the data warehouse is prepared to meet all these compliance requirements, then \nthe stress of being audited by a hostile government agency or lawyer armed with a \nsubpoena should be greatly reduced.\nThe compliance requirements may mean you cannot actually change any data, for \nany reason. If data must be altered, then a new version of the altered records must \nbe inserted into the database. Each row in each table therefore must have begin \nand end time stamps that accurately represents the span of time when the record \nwas the “current truth.” The big impact of these compliance requirements on the \ndata warehouse can be expressed in simple dimensional modeling terms. Type 1 \nand type 3 changes are dead. In other words, all changes become inserts. No more \ndeletes or overwrites.\nFigure 19-12 shows how a fact table can be augmented so that overwrite changes \nare converted into a fact table equivalent of a type 2 change. The original fact table \nconsisted of the lower seven columns starting with activity date and ending with \n\n\nChapter 19\n494\nnet dollars. The original fact table allowed overwrites. For example, perhaps there \nis a business rule that updates the discount and net dollar amounts after the row is \noriginally created. In the original version of the table, history is lost when the over-\nwrite change takes place, and the chain of custody is broken.\nFact Table Surrogate Key\nBegin Version Date/Time\nEnd Version Date/Time\nChange Reference Key (FK)\nSource Reference Key (FK)\nActivity Date Key (FK)\nActivity Date/Time\nCustomer Key (FK)\nService Key (FK)\nGross Dollars\nDiscount Dollars\nNet Dollars\n(PK)\n(PK)\nOriginal fact table columns\nCompliance-Enabled\nTransaction Grain Fact\nFigure 19-12: Compliance-enabled transaction fact table.\nTo  convert the fact table to be compliance-enabled, ﬁ ve columns are added, as \nshown in bold. A fact table surrogate key is created for each original unmodiﬁ ed fact \ntable row. This surrogate key, like a dimension table surrogate key, is just a unique \ninteger that is assigned as each original fact table row is created. The begin version \ndate/time stamp is the exact time of creation of the fact table row. Initially, the end \nversion date/time is set to a ﬁ ctitious date/time in the future. The change reference \nis set to “original,” and the source reference is set to the operational source.\nWhen an overwrite change is needed, a new row is added to the fact table with the \nsame fact table surrogate key, and the appropriate regular columns changed, such \nas discount dollars and net dollars. The begin version date/time column is set to the \nexact date/time when the change in the database takes place. The end version date/\ntime is set to a ﬁ ctitious date/time in the future. The end version date/time of the \noriginal fact row is now set to the exact date/time when the change in the database \ntakes place. The change reference now provides an explanation for the change, and \nthe source reference provides the source of the revised columns.\nReferring to the design in Figure 19-12, a speciﬁ c moment in time can be selected \nand the fact table constrained to show exactly what the rows contained at that \nmoment. The alterations to a given row can be examined by constraining to a spe-\nciﬁ c fact table surrogate key and sorting by begin version date/time.\nThe compliance machinery is a signiﬁ cant addition to a normal fact table (refer to \nFigure 19-12). If the compliance-enabled table is actually used for only demonstrating \n\n\nETL Subsystems and Techniques 495\ncompliance, then a normal version of the fact table with just the original columns \ncan remain as the main operational table, with the compliance-enabled table existing \nonly in the background. The compliance-enabled table doesn’t need to be indexed \nfor performance because it will not be used in a conventional BI environment.\nFor heaven’s sake, don’t assume that all data is now subject to draconian compli-\nance restrictions. It is essential you receive ﬁ rm guidelines from the chief compliance \noffi  cer before taking any drastic steps.\nThe foundation of a compliance system is the interaction of several of the subsys-\ntems already described married to a few key technologies and capabilities:\n \n■Lineage analysis.  Show where a ﬁ nal piece of data came from to prove the \noriginal source data plus the transformations including stored procedures \nand manual changes. This requires full documentation of all the transforms and \nthe technical ability to rerun the transforms against the original data.\n \n■Dependency analysis.  Show where an original source data element was ever \nused.\n \n■Version control.  It may be necessary to rerun the source data through the \nETL system in eff ect at the time, requiring the exact version of the ETL system \nfor any given data source. \n \n■Backup and restore.  Of course, the requested data may have been archived years \nago and need to be restored for audit purposes. Hopefully, you archived the \nproper version of the ETL system alongside the data, so both the data and \nthe system can be restored. It may be necessary to prove the archived data hasn’t \nbeen altered. During the archival process, the data can be hash-coded and the \nhash and data separated. Have the hash codes archived separately by a trusted \nthird party. Then, when demanded, restore the original data, hash code it again, \nand then compare to the hash codes retrieved from the trusted third party to \nprove the authenticity of the data.\n \n■Security. Show  who has accessed or modiﬁ ed the data and transforms. Be \nprepared to show roles and privileges for users. Guarantee the security log \ncan’t be altered by using a write once media.\n \n■Audit dimension. The  audit dimension ties runtime metadata context directly \nwith the data to capture quality events at the time of the load.\nSubsystem 34: Metadata Repository Manager \nThe  ETL system is responsible for the use and creation of much of the metadata \ninvolved in the DW/BI environment. Part of the overall metadata strategy should \nbe to speciﬁ cally capture ETL metadata, including the process metadata, technical \nmetadata, and business metadata. Develop a balanced strategy between doing noth-\ning and doing too much. Make sure there’s time in the ETL development tasks to \n\n\nChapter 19\n496\ncapture and manage metadata. And ﬁ nally, make sure someone on the DW/BI team \nis assigned the role of metadata manager and owns the responsibility for creating \nand implementing the metadata strategy.\nSummary\nIn this chapter we have introduced the key building blocks of the ETL system. As \nyou may now better appreciate, building an ETL system is unusually challenging; \nthe ETL system must address a number of demanding requirements. This chapter \nidentiﬁ ed and reviewed the 34 subsystems of ETL and gathered these subsystems \ninto four key areas that represent the  ETL process: extracting, cleaning and con-\nforming, delivering, and managing. Careful consideration of all the elements of \nthe ETL architecture is the key to success. You must understand the full breadth \nof requirements and then set an appropriate and eff ective architecture in place. \nETL is more than simply extract, transform, and load; it’s a host of complex and \nimportant tasks. In the next chapter we will describe the processes and tasks for \nbuilding the ETL system.\n\n\nETL System Design \nand Development \nProcess and Tasks\nD\neveloping the extract, transformation, and load (ETL) system is the hidden \npart of the iceberg for most DW/BI projects. So many challenges are buried in \nthe data sources and systems that developing the ETL application invariably takes \nmore time than expected. This chapter is structured as a 10-step plan for creating the \ndata warehouse’s ETL system. The concepts and approach described in this chapter, \nbased on content from The Data Warehouse Lifecycle Toolkit, Second Edition (Wiley, \n2008), apply to systems based on an ETL tool, as well as hand-coded systems. \nChapter 20 discusses the following concepts:\n \n■ETL system planning and design consideration\n \n■Recommendations for one-time historic data loads\n \n■Development tasks for incremental load processing\n \n■Real-time data warehousing considerations \nETL Process Overview\nThis  chapter follows the ﬂ ow of planning and implementing the ETL system. We \nimplicitly discuss the 34 ETL subsystems presented in Chapter 19: ETL Subsystems \nand Techniques, broadly categorized as extracting data, cleaning and conforming, \ndelivering for presentation, and managing the ETL environment.\nBefore beginning the ETL system design for a dimensional model, you should \nhave completed the logical design, drafted your high-level architecture plan, and \ndrafted the source-to-target mapping for all data elements.\nThe ETL system design process is critical. Gather all the relevant information, \nincluding the processing burden the extracts will be allowed to place on the opera-\ntional source systems, and test some key alternatives. Does it make sense to host the \ntransformation process on the source system, target system, or its own platform? \nWhat tools are available on each, and how eff ective are they?\n20\n\n\nChapter 20\n498\nDevelop the ETL Plan\nETL  development starts out with the high-level plan, which is independent of any \nspeciﬁ c technology or approach. However, it’s a good idea to decide on an ETL tool \nbefore doing any detailed planning; this can avoid redesign and rework later in the \nprocess.\nStep 1: Draw the High-Level Plan\nWe  start the design process with a very simple schematic of the known pieces of the \nplan: sources and targets, as shown in Figure 20-1. This schematic is for a ﬁ ctitious \nutility company’s data warehouse, which is primarily sourced from a 30-year-old \nCOBOL system. If most or all the data comes from a modern relational transaction \nprocessing system, the boxes often represent a logical grouping of tables in the \ntransaction system model.\nSources\nCustomer\nMaster\n(RDBMS)\nGeography\nMaster\n(RDBMS)\nDMR system\n(COBOL flat file,\n2000 fields,\none row per customer)\nMeters\n(MS Access)\nDate\n(Spreadsheet)\nSlowly changing\non demographics\nand account\nstatus\n25M customers\n–10k new or\nchanged\ncustomers/day\nCustomer\nTargets\n15,000\ngeogs\ncheck\nRI\nLabels need\ncosmetic work!\nGeography\n“Unbucketize”\nfrom 13 months\nin one row\nProcess 750k\ncustomers/\nday\nMissed meter\nreads, estimated\nbills, restatement\nof bills\nElectricity\nUsage\nOld (pre-1972) meter\ntypes are not in the\nMeters Group’s system\nThere are 73 known\nmeter types\nHow/by whom\nmaintained??\ncheck\nRI\ncheck\nRI\nUsage Month\nElectric Meter\nMeter Read Date\nFigure 20-1: Example high-level data staging plan schematic.\nAs you develop the detailed ETL system speciﬁ cation, the high-level view requires \nadditional details. Figure 20-1 deliberately highlights contemporary questions and \nunresolved issues; this plan should be frequently updated and released. You might \nsometimes keep two versions of the diagram: a simple one for communicating \n\n\nETL System Design and Development Process and Tasks 499\nwith people outside the team and a detailed version for internal DW/BI team \ndocumentation.\nStep 2: Choose an ETL Tool\nThere  are a multitude of ETL tools available in the data warehouse marketplace. \nMost of the major database vendors off er an ETL tool, usually at additional licensing \ncost. There are also excellent ETL tools available from third-party vendors. \nETL tools read data from a range of sources, including ﬂ at ﬁ les, ODBC, OLE DB, \nand native database drivers for most relational databases. The tools contain func-\ntionality for deﬁ ning transformations on that data, including lookups and other \nkinds of joins. They can write data into a variety of target formats. And they all \ncontain some functionality for managing the overall logic ﬂ ow in the ETL system.\nIf  the source systems are relational, the transformation requirements are straight-\nforward, and good developers are on staff , the value of an ETL tool may not be \nimmediately obvious. However, there are several reasons that using an ETL tool is \nan industry standard best practice:\n \n■Self-documentation that comes from using a graphical tool. A hand-coded \nsystem is usually an impenetrable mess of staging tables, SQL scripts, stored \nprocedures, and operating system scripts.\n \n■Metadata foundation for all steps of the ETL process.\n \n■Version control for multi-developer environments and for backing out and \nrestoring consistent versions.\n \n■Advanced transformation logic, such as fuzzy matching algorithms, inte-\ngrated access to name and address deduplication routines, and data mining \nalgorithms.\n \n■Improved system performance at a lower level of expertise. Relatively few SQL \ndevelopers are truly expert on how to use the relational database to manipulate \nextremely large data volumes with excellent performance.\n \n■Sophisticated processing capabilities, including automatically parallel-\nizing tasks, and automatic fail-over when a processing resource becomes \nunavailable.\n \n■One-step conversion of virtualized data transformation modules into their \nphysical equivalents.\nDon’t expect to recoup the investment in an ETL tool on the ﬁ rst phase of the \nDW/BI project. The learning curve is steep enough that developers sometimes feel \nthe project could have been implemented faster by coding. The big advantages \ncome with future phases, and particularly with future modiﬁ cations to existing \nsystems.\n",
      "page_number": 514
    },
    {
      "number": 56,
      "title": "Segment 56 (pages 522-530)",
      "start_page": 522,
      "end_page": 530,
      "detection_method": "topic_boundary",
      "content": "Chapter 20\n500\nStep 3: Develop Default Strategies\nWith an overall idea of what needs to happen and what the ETL tool’s infrastructure \nrequires, you should develop a set of default strategies for the common activities in \nthe ETL system. These activities include:\n \n■Extract from each major source system. At this point in the design process, \nyou can determine the default method for extracting data from each source \nsystem. Will you normally push from the source system to a ﬂ at ﬁ le, extract \nin a stream, use a tool to read the database logs, or another method? This \ndecision can be modiﬁ ed on a table-by-table basis. If using SQL to access \nsource system data, make sure the native data extractors are used rather than \nODBC, if that’s an option.\n \n■Archive extracted and staged data. Extracted or staged data, before it’s been \ntransformed, should be archived for at least a month. Some organizations \npermanently archive extracted and staged data.\n \n■Police data quality for dimensions and particularly facts. Data  quality must \nbe monitored during the ETL process rather than waiting for business users \nto ﬁ nd data problems. Chapter 19 describes a comprehensive architecture \nfor measuring and responding to data quality issues in ETL subsystems 4 \nthrough 8.\n \n■Manage changes to dimension attributes. In Chapter 19, we described the \nlogic required to manage dimension attribute changes in ETL subsystem 9.\n \n■Ensure the data warehouse and ETL system meet the system availability \nrequirements. The ﬁ rst step to meeting availability requirements is to docu-\nment them. You should document when each data source becomes available \nand block out high-level job sequencing.\n \n■Design the data auditing subsystem. Each row in the data warehouse tables \nshould be tagged with auditing information that describes how the data \nentered the system.\n \n■Organize the ETL staging area. Most ETL systems stage the data at least once \nor twice during the ETL process. By staging, we mean the data will be written \nto disk for a later ETL step and for system recovery and archiving.\nStep 4: Drill Down by Target Table\nAfter  overall strategies for common ETL tasks have been developed, you should start \ndrilling into the detailed transformations needed to populate each target table in the \ndata warehouse. As you’re ﬁ nalizing the source-to-target mappings, you also perform \nmore data proﬁ ling to thoroughly understand the necessary data transformations \nfor each table and column.\n\n\nETL System Design and Development Process and Tasks 501\n Ensure Clean Hierarchies\nIt’s  particularly important to investigate whether hierarchical relationships in the \ndimension data are perfectly clean. Consider a product dimension that includes \na hierarchical rollup from product stock keeping unit (SKU) to product category.\nIn our experience, the most reliable hierarchies are well managed in the source \nsystem. The best source systems normalize the hierarchical levels into multiple \ntables, with foreign key constraints between the levels. In this case, you can be \nconﬁ dent the hierarchies are clean. If the source system is not normalized—espe-\ncially if the source for the hierarchies is an Excel spreadsheet on a business user’s \ndesktop—then you must either clean it up or acknowledge that it is not a hierarchy.\nDevelop Detailed Table Schematics\nFigure 20-2 illustrates the level of detail that’s useful for the table-speciﬁ c drilldown; \nit’s for one of the tables in the utility company example previously illustrated.\nDetailed\nMaster\nRecord\n(DMR)\nDMR Extract: 80 cols including 13 monthly\nbuckets for usage. One row per customer meter,\nsorted by customer.\nEBCDIC file, tab-delimited.\nFile name: xdmr_yyyymmdd.dat\nNo transformations on mainframe—need to\nminimize mainframe coding and load.\nCompress\nencrypt ftp\nxdmr_yyyymmdd.dat\nDimension\nprocessing\nFact_stage1 (only fields\nrelevant to fact table),\nincludes customer &\nmonth surrogate keys\nElectric_\nUsage_Fact\nFact_stage4,\nincludes\nread_date\nsurrogate key\nFact_stage3,\nincludes geog\nsurrogate key\nFact_stage2,\nincludes meter\nsurrogate key\nBulk-load into\nelectric_usage_fact\nSort by\ndate\nlookup\ndate_key\nSort by\ngeog\nlookup\ngeog_key\nSort by meter_type,\nlookup meter_key\nUnbucketize: Usage1 keyed to yyyymm, Usage2\nkeyed to yyyymm-1,... When\nyyyymm<subscribe_date, stop processing row\nWill have –13x rows.\nData presorted by cust, so can perform cust_key\nlookup on same pass.\nFigure 20-2: Example draft detailed load schematic for the fact table. \nAll the dimension tables must be processed before the key lookup steps for the fact \ntable. The dimension tables are usually independent from each other, but sometimes \nthey also have processing dependencies. It’s important to clarify these dependencies, \nas they become ﬁ xed points around which the job control  ﬂ ows.\n\n\nChapter 20\n502\nDevelop the ETL Speciﬁ cation Document\nWe’ve  walked through some general strategies for high-level planning and the physi-\ncal design of the ETL system. Now it’s time to pull everything together and develop \na detailed speciﬁ cation for the entire ETL system.\nAll the documents developed so far—the source-to-target mappings, data proﬁ l-\ning reports, physical design decisions—should be rolled into the ﬁ rst sections of \nthe ETL speciﬁ cation. Then document all the decisions discussed in this chapter, \nincluding:\n \n■Default strategy for extracting from each major source system\n \n■Archiving strategy\n \n■Data quality tracking and metadata\n \n■Default strategy for managing changes to dimension attributes\n \n■System availability requirements and strategy\n \n■Design of the data auditing subsystem\n \n■Locations of staging areas\nThe next section of the ETL speciﬁ cation describes the historic and incremen-\ntal load strategies for each table. A good speciﬁ cation includes between two and \n10 pages of detail for each table, and documents the following information and \ndecisions:\n \n■Table design (column names, data types, keys, and constraints)\n \n■Historic data load parameters (number of months) and volumes (row counts)\n \n■Incremental data volumes, measured as new and updated rows per load cycle\n \n■Handling of late arriving data for facts and dimensions\n \n■Load frequency\n \n■Handling of slowly changing dimension (SCD) changes for each dimension \nattribute\n \n■Table partitioning, such as monthly\n \n■Overview of data sources, including a discussion of any unusual source char-\nacteristics, such as an unusually brief access window\n \n■Detailed source-to-target mapping\n \n■Source data proﬁ ling, including at least the minimum and maximum values \nfor each numeric column, count of distinct values in each column, and inci-\ndence of NULLs\n \n■Extract strategy for the source data (for example, source system APIs, direct \nquery from database, or dump to ﬂ at ﬁ les)\n \n■Dependencies, including which other tables must be loaded before this table \nis processed\n \n■Document the transformation logic. It’s easiest to write this section as pseudo \ncode or a diagram, rather than trying to craft complete sentences.\n\n\nETL System Design and Development Process and Tasks 503\n \n■Preconditions to avoid error conditions. For example, the ETL system must \ncheck for ﬁ le or database space before proceeding.\n \n■Cleanup steps, such as deleting working ﬁ les\n \n■An estimate of whether this portion of the ETL system will be easy, medium, \nor diffi  cult to implement\nNOTE \nAlthough most people would agree that all the items described in the \nETL system speciﬁ cation document are necessary, it’s a lot of work to pull this \ndocument together, and even more work to keep it current as changes occur. \nRealistically, if you pull together the “one-pager” high-level ﬂ ow diagram, data \nmodel and source-to-target maps, and a ﬁ ve-page description of what you plan to \ndo, you’ll get a better start than most teams.\nDevelop a Sandbox Source System\nDuring  the ETL development process, the source system data needs to be inves-\ntigated at great depth. If the source system is heavily loaded, and there isn’t some \nkind of reporting instance for operational queries, the DBAs may be willing to set \nup a static snapshot of the database for the ETL development team. Early in the \ndevelopment process, it’s convenient to poke around sandbox versions of the source \nsystems without worrying about launching a kind of killer query.\nIt’s easy to build a sandbox source system that simply copies the original; build a \nsandbox with a subset of data only if the data volumes are extremely large. On the \nplus side, this sandbox could become the basis of training materials and tutorials \nafter the system is deployed into production.\nDevelop One-Time Historic Load Processing\nAfter  the ETL speciﬁ cation has been created, you typically focus on developing \nthe ETL process for the one-time load of historic data. Occasionally, the same ETL \ncode can perform both the initial historic load and ongoing incremental loads, but \nmore often you build separate ETL processes for the historic and ongoing loads. The \nhistoric and incremental load processes have a lot in common, and depending on \nthe ETL tool, signiﬁ cant functionality can be reused from one to the other.\nStep 5: Populate Dimension Tables with Historic Data\nIn  general, you start building the ETL system with the simplest dimension tables. \nAfter these dimension tables have been successfully built, you tackle the historic \nloads for dimensions with one or more columns managed as SCD type 2.\n\n\nChapter 20\n504\nPopulate Type 1 Dimension Tables\nThe easiest type of table to populate is a dimension table for which all attributes \nare managed as type 1 overwrites. With a type 1–only dimension, you extract the \ncurrent value for each dimension attribute from the source system.\nDimension Transformations\nEven the simplest dimension table may require substantial data cleanup and will \ncertainly require surrogate key assignment. \nSimple Data Transformations\nThe  most common, and easiest, form of data transformation is data type conversion. \nAll ETL tools have rich functions for data type conversion. This task can be tedious, \nbut it is seldom onerous. We strongly recommend replacing NULL values with \ndefault values within dimension tables; as we have discussed previously, NULLs \ncan cause problems when they are directly queried.\nCombine from Separate Sources\nOften  dimensions are derived from several sources. Customer information may \nneed to be merged from several lines of business and from outside sources. There \nis seldom a universal key pre-embedded in the various sources that makes this \nmerge operation easy.\nMost consolidation and deduplicating tools and processes work best if names \nand addresses are ﬁ rst parsed into their component pieces. Then you can use a \nset of passes with fuzzy logic that account for misspellings, typos, and alternative \nspellings such as I.B.M., IBM, and International Business Machines. In most orga-\nnizations, there is a large one-time project to consolidate existing customers. This \nis a tremendously valuable role for master data management systems.\nDecode Production Codes\nA  common merging task in data preparation is looking up text equivalents for pro-\nduction codes. In some cases, the text equivalents are sourced informally from a \nnonproduction source such as a spreadsheet. The code lookups are usually stored in \na table in the staging database. Make sure the ETL system includes logic for creat-\ning a default decoded text equivalent for the case in which the production code is \nmissing from the lookup table.\n Validate Many-to-One and One-to-One Relationships\nThe  most important dimensions probably have one or more rollup paths, such as \nproducts rolling up to product model, subcategory, and category, as illustrated in \nFigure 20-3. These hierarchical rollups need to be perfectly clean.\n\n\nETL System Design and Development Process and Tasks 505\nProduct Dimension\nProduct Key (PK)\nProduct SKU\nProduct Name\nProduct Description\nProduct Model\nProduct Model Description\nSubcategory Description\nCategory Description\nCategory Manager\nFigure 20-3: Product dimension table with a hierarchical relationship.\nMany-to-one relationships between attributes, such as a product to product \nmodel, can be veriﬁ ed by sorting on the “many” attribute and verifying that each \nvalue has a unique value on the “one” attribute. For example, this query returns \nthe products that have more than one product model:\nSELECT Product_SKU,\ncount[*] as Row_Count, \ncount(distinct Product_Model) as Model_Count\nFROM StagingDatabase.Product\nGROUP BY Product_SKU\nHAVING count(distinct Product_Model) > 1 ;\nDatabase administrators sometimes want to validate many-to-one relationships \nby loading data into a normalized snowﬂ ake version of the dimension table in the \nstaging database, as illustrated in Figure 20-4. Note that the normalized version \nrequires individual keys at each of the hierarchy levels. This is not a problem if the \nsource system supplies the keys, but if you normalize the dimension in the ETL \nenvironment, you need to create them.\nThe snowﬂ ake structure has some value in the staging area: It prevents you from \nloading data that violates the many-to-one relationship. However, in general, the \nrelationships should be pre-veriﬁ ed as just described, so that you never attempt to \nload bad data into the dimension table. After the data is pre-veriﬁ ed, it’s not tremen-\ndously important whether you make the database engine reconﬁ rm the relationship \nat the moment you load the table.\nIf the source system for a dimensional hierarchy is a normalized database, it’s \nusually unnecessary to repeat the normalized structure in the ETL staging area. \nHowever, if the hierarchical information comes from an informal source such as a \nspreadsheet managed by the marketing department, you may beneﬁ t from normal-\nizing the hierarchy in the ETL system.\n\n\nChapter 20\n506\nProduct Table\nProduct Key (PK)\nProduct SKU\nProduct Name\nProduct Description\nProduct Model Key (FK)\nProduct Model Table\nProduct Model Key (PK)\nProduct Model Number\nProduct Model Description\nProduct Subcategory Key (FK)\nProduct Subcategory Table\nProduct Subcategory Key (PK)\nProduct Subcategory Description\nProduct Category Key (FK)\nProduct Category Key (PK)\nProduct Category Description\nProduct Category Manager\nProduct Category Table\nFigure 20-4: Snowﬂ aked hierarchical relationship in the product dimension.\n Dimension Surrogate Key Assignment\nAfter  you are conﬁ dent you have dimension tables with one row for each true unique \ndimension member, the surrogate keys can be assigned. You maintain a table in the \nETL staging database that matches production keys to surrogate keys; you can use \nthis key map later during fact table processing.\nSurrogate keys are typically assigned as integers, increasing by one for each \nnew key. If the staging area is in an RDBMS, surrogate key assignment is elegantly \naccomplished by creating a sequence. Although syntax varies among the relational \nengines, the process is ﬁ rst to create a sequence and then to populate the key map \ntable.\nHere’s the syntax for the one-time creation of the sequence:\ncreate sequence dim1_seq cache=1000; — choose appropriate cache level\nAnd then here’s the syntax to populate the key map table:\ninsert into dim1_key_map (production_key_id, dim1_key)\nselect production_key_id, dim1_seq.NEXT\nfrom dim1_extract_table;\nDimension Table Loading\nAfter  the dimension data is properly prepared, the load process into the target tables \nis fairly straightforward. Even though the ﬁ rst dimension table is usually small, use \nthe database’s bulk or fast-loading utility or interface. You should use fast-loading \n\n\nETL System Design and Development Process and Tasks 507\ntechniques for most table inserts. Some databases have extended the SQL syntax to \ninclude a BULK INSERT statement. Others have published an API to load data into \nthe table from a stream.\nThe bulk load utilities and APIs come with a range of parameters and transforma-\ntion capabilities including the following:\n \n■Turn off  logging. Transaction logging adds signiﬁ cant overhead and is not \nvaluable when loading data warehouse tables. The ETL system should be \ndesigned with one or more recoverability points where you can restart pro-\ncessing should something go wrong.\n \n■Bulk load in fast mode. However, most of the database engines’ bulk load \nutilities or APIs require several stringent conditions on the target table to bulk \nload in fast mode. If these conditions are not met, the load should not fail; it \nsimply will not use the “fast” path.\n \n■Presort the ﬁ le. Sorting the ﬁ le in the order of the primary index signiﬁ cantly \nspeeds up indexing.\n \n■Transform with caution. In some cases, the loader supports data conversions, \ncalculations, and string and date/time manipulation. Use these features care-\nfully and test performance. In some cases, these transformations cause the \nloader to switch out of high-speed mode into a line-by-line evaluation of the \nload ﬁ le. We recommend using the ETL tool to perform most transformations.\n \n■Truncate table before full refresh. The TRUNCATE TABLE statement is the most \neffi  cient way to delete all the rows in the table. It’s commonly used to clean out \na table from the staging database at the beginning of the day’s ETL processing.\n Load Type 2 Dimension Table History\nRecall  from Chapter 5: Procurement, that dimension attribute changes are typically \nmanaged as type 1 (overwrite) or type 2 (track history by adding new rows to the \ndimension table). Most dimension tables contain a mixture of type 1 and type 2 \nattributes. More advanced SCD techniques are described in Chapter 5.\nDuring the historic load, you need to re-create history for dimension attributes \nthat are managed as type 2. If business users have identiﬁ ed an attribute as impor-\ntant for tracking history, they want that history going back in time, not just from the \ndate the data warehouse is implemented. It’s usually diffi  cult to re-create dimension \nattribute history, and sometimes it’s completely impossible.\nThis process is not well suited for standard SQL processing. It’s better to use \na database cursor construct or, even better, a procedural language such as Visual \nBasic, C, or Java to perform this work. Most ETL tools enable script processing on \nthe data as it ﬂ ows through the ETL system. \n\n\nChapter 20\n508\nWhen you’ve completely reconstructed history, make a ﬁ nal pass through the \ndata to set the row end date column. It’s important to ensure there are no gaps in \nthe series. We prefer to set the row end date for the older version of the dimension \nmember to the day before the row eff ective date for the new row if these row dates \nhave a granularity of a full day. If the eff ective and end dates are actually precise \ndate/time stamps accurate to the minute or second, then the end date/time must be \nset to exactly the begin date/time of the next row so that no gap exists between rows.\nPopulate Date and Other Static Dimensions\nEvery  data warehouse database should have a date dimension, usually at the granu-\nlarity of one row for each day. The date dimension should span the history of the \ndata, starting with the oldest fact transaction in the data warehouse. It’s easy to set \nup the date dimension for the historic data because you know the date range of the \nhistoric fact data being loaded. Most projects build the date dimension by hand, \ntypically in a spreadsheet.\nA handful of other dimensions will be created in a similar way. For example, you \nmay create a budget scenario dimension that holds the values Actual and Budget. \nBusiness data governance representatives should sign off  on all constructed dimen-\nsion tables.\nStep 6: Perform the Fact Table Historic Load\nThe one-time historic fact table load diff ers fairly signiﬁ cantly from the ongoing \nincremental processing. The biggest worry during the historic load is the sheer \nvolume of data, sometimes thousands of times bigger than the daily incremental \nload. On the other hand, you have the luxury of loading into a table that’s not in \nproduction. If it takes several days to load the historic data, that’s usually tolerable.\nHistoric Fact Table Extracts\nAs  you identify records that fall within the basic parameters of the extract, make \nsure these records are useful for the data warehouse. Many transaction systems \nkeep operational information in the source system that may not be interesting from \na business point of view.\nIt’s also a good idea to accumulate audit statistics during this step. As the extract \ncreates the results set, it is often possible to capture various subtotals, totals, and \nrow counts.\nAudit Statistics\nDuring  the planning phase for the ETL system, you identiﬁ ed various measures of \ndata quality. These are usually calculations, such as counts and sums, that you com-\npare between the data warehouse and source systems to cross-check the integrity \n",
      "page_number": 522
    },
    {
      "number": 57,
      "title": "Segment 57 (pages 531-539)",
      "start_page": 531,
      "end_page": 539,
      "detection_method": "topic_boundary",
      "content": "ETL System Design and Development Process and Tasks 509\nof the data. These numbers should tie backward to operational reports and forward \nto the results of the load process in the warehouse. The tie back to the operational \nsystem is important because it is what establishes the credibility of the warehouse.\nNOTE \nThere are scenarios in which it’s diffi  cult or impossible for the warehouse \nto tie back to the source system perfectly. In many cases, the data warehouse extract \nincludes business rules that have not been applied to the source systems. Even \nmore vexing are errors in the source system! Also, diff erences in timing make it \neven more diffi  cult to cross-check the data. If it’s not possible to tie the data back \nexactly, you need to explain the diff erences.\n Fact Table Transformations\nIn most projects, the fact data is relatively clean. The ETL system developer spends \na lot of time improving the dimension table content, but the facts usually require \na fairly modest transformation. This makes sense because in most cases the facts \ncome from transaction systems used to operate the organization.\nThe most common transformations to fact data include transformation of null \nvalues, pivoting or unpivoting the data, and precomputing derived calculations. All \nfact rows then enter the surrogate key pipeline to exchange the natural keys for the \ndimension surrogate keys managed in the ETL system.\nNull Fact Values\nAll  major database engines explicitly support a null value. In many source systems, \nhowever, the null value is represented by a special value of what should be a legitimate \nfact. Perhaps the special value of –1 is understood to represent null. For most fact \ntable metrics, the “–1” in this scenario should be replaced with a true NULL. A null \nvalue for a numeric measure is reasonable and common in the fact table. Nulls do \nthe “right thing” in calculations of sums and averages across fact table rows. It’s only \nin the dimension tables that you should strive to replace null values with specially \ncrafted default values. Finally, you should not allow any null values in the fact table \ncolumns that reference the dimension table keys. These foreign key columns should \nalways be deﬁ ned as NOT NULL.\nImprove Fact Table Content\nAs we have stressed, all the facts in the ﬁ nal fact table row must be expressed in \nthe same grain. This means there must be no facts representing totals for the year \nin a daily fact table or totals for some geography larger than the fact table’s grain. \nIf the extract includes an interleaving of facts at diff erent grains, the transforma-\ntion process must eliminate these aggregations, or move them into the appropriate \naggregate tables.\n\n\nChapter 20\n510\nThe fact row may contain derived facts; although, in many cases it is more effi  cient \nto calculate derived facts in a view or an online analytical processing (OLAP) cube \nrather than in the physical table. For instance, a fact row that contains revenues and \ncosts may want a fact representing net proﬁ t. It is very important that the net proﬁ t \nvalue be correctly calculated every time a user accesses it. If the data warehouse \nforces all users to access the data through a view, it would be ﬁ ne to calculate the \nnet proﬁ t in that view. If users are allowed to see the physical table, or if they often \nﬁ lter on net proﬁ t and thus you’d want to index it, precomputing it and storing it \nphysically is preferable.\nSimilarly, if some facts need to be simultaneously presented with multiple units \nof measure, the same logic applies. If business users access the data through a view \nor OLAP database, then the various versions of the facts can effi  ciently be calculated \nat access time.\n Pipeline the Dimension Surrogate Key Lookup\nIt  is important that referential integrity (RI) is maintained between the fact table \nand dimension tables; you must never have a fact row that references a dimension \nmember that doesn’t exist. Therefore, you should not have a null value for any \nforeign key in the fact table nor should any fact row violate referential integrity to \nany dimension.\nThe surrogate key pipeline is the ﬁ nal operation before you load data into the \ntarget fact table. All other data cleaning, transformation, and processing should \nbe complete. The incoming fact data should look just like the target fact table in \nthe dimensional model, except it still contains the natural keys from the source \nsystem rather than the warehouse’s surrogate keys. The surrogate key pipeline is \nthe process that exchanges the natural keys for the surrogate keys and handles any \nreferential integrity errors.\nDimension table processing must complete before the fact data enters the sur-\nrogate key pipeline. Any new dimension members or type 2 changes to existing \ndimension members must have already been processed, so their keys are available \nto the surrogate key pipeline. \nFirst let’s discuss the referential integrity problem. It’s a simple matter to con-\nﬁ rm that each natural key in the historic fact data is represented in the dimension \ntables. This is a manual step. The historic load is paused at this point, so you can \ninvestigate and ﬁ x any referential integrity problems before proceeding. The dimen-\nsion table is either ﬁ xed, or the fact table extract is redesigned to ﬁ lter out spurious \nrows, as appropriate.\nNow that you’re conﬁ dent there will be no referential integrity violations, you can \ndesign the historic surrogate key pipeline, as shown in Figure 19-11 in the previous \nchapter. In this scenario, you need to include BETWEEN logic on any dimension \n\n\nETL System Design and Development Process and Tasks 511\nwith type 2 changes to locate the dimension row that was in eff ect when the histori-\ncal fact measurement occurred.\nThere are several approaches for designing the historic load’s surrogate key pipe-\nline for best performance; the design depends on the features available in your ETL \ntool, the data volumes you’re processing, and your dimensional design. In theory, \nyou could deﬁ ne a query that joins the fact staging table and each dimension table \non the natural keys, returning the facts and surrogate keys from each dimension \ntable. If the historic data volumes are not huge, this can actually work quite well, \nassuming you staged the fact data in the relational database and indexed the dimen-\nsion tables to support this big query. This approach has several beneﬁ ts:\n \n■It leverages the power of the relational database.\n \n■It performs the surrogate key lookups on all dimensions in parallel.\n \n■It simpliﬁ es the problem of picking up the correct dimension key for type 2 \ndimensions. The join to type 2 dimensions must include a clause specifying \nthat the transaction date falls between the row eff ective date and row end date \nfor that image of the dimension member in the table.\nNo one would be eager to try this approach if the historic fact data volumes \nwere large in the hundreds of gigabytes to terabyte range. The complex join to \nthe type 2 dimension tables create the greatest demands on the system. Many \ndimensional designs include a fairly large number of (usually small) dimension \ntables that are fully type 1, and a smaller number of dimensions containing type \n2 attributes. You could use this relational technique to perform the surrogate key \nlookups for all the type 1 dimensions in one pass and then separately handle the \ntype 2 dimensions. You should ensure the eff ective date and end date columns \nare properly indexed.\nAn alternative to the database join technique described is to use the ETL tool’s \nlookup operator.\nWhen all the fact source keys have been replaced with surrogate keys, the fact \nrow is ready to load. The keys in the fact table row have been chosen to be proper \nforeign keys to the respective dimension tables, and the fact table is guaranteed to \nhave referential integrity with respect to the dimension tables.\n Assign Audit Dimension Key\nFact  tables often include an audit key on each fact row. The audit key points to an \naudit dimension that describes the characteristics of the load, including relatively \nstatic environment variables and measures of data quality. The audit dimension can \nbe quite small. An initial design of the audit dimension might have just two envi-\nronment variables (master ETL version number and proﬁ t allocation logic number), \nand only one quality indicator whose values are Quality Checks Passed and Quality \nProblems Encountered. Over time, these variables and diagnostic indicators can be \n\n\nChapter 20\n512\nmade more detailed and more sophisticated. The audit dimension key is added to the \nfact table either immediately after or immediately before the surrogate key pipeline. \nFact Table Loading\nThe  main concern when loading the fact table is load performance. Some database \ntechnologies support fast loading with a speciﬁ ed batch size. Look at the docu-\nmentation for the fast-loading technology to see how to set this parameter. You \ncan experiment to ﬁ nd the ideal batch size for the size of the rows and the server’s \nmemory conﬁ guration. Most people don’t bother to get so precise and simply choose \na number like 10,000 or 100,000 or 1 million.\nAside from using the bulk loader and a reasonable batch size (if appropriate for \nthe database engine), the best way to improve the performance of the historic load \nis to load into a partitioned table, ideally loading multiple partitions in parallel. The \nsteps to loading into a partitioned table include:\n \n1. Disable foreign key (referential integrity) constraints between the fact table \nand each dimension table before loading data.\n 2. Drop or disable indexes on the fact table.\n \n3. Load the data using fast-loading techniques.\n \n4. Create or enable fact table indexes.\n \n5. If necessary, perform steps to stitch together the table’s partitions.\n \n6. Confirm each dimension table has a unique index on the surrogate \nkey column.\n \n7. Enable foreign key constraints between the fact table and dimension tables.\nDevelop Incremental ETL Processing\nOne of the biggest challenges with the incremental ETL process is identifying new, \nchanged, and deleted rows. After you have a stream of inserts, modiﬁ cations, and \ndeletions, the ETL system can apply transformations following virtually identical \nbusiness rules as for the historic data loads. \nThe historic load for dimensions and facts consisted largely or entirely of inserts. \nIn incremental processing, you primarily perform inserts, but updates for dimen-\nsions and some kinds of fact tables are inevitable. Updates and deletes are expensive \noperations in the data warehouse environment, so we’ll describe techniques to \nimprove the performance of these tasks.\nStep 7: Dimension Table Incremental Processing\nAs  you might expect, the incremental ETL system development begins with the \ndimension tables. Dimension incremental processing is very similar to the historic \nprocessing previously described. \n\n\nETL System Design and Development Process and Tasks 513\nDimension Table Extracts\nIn  many cases, there is a customer master ﬁ le or product master ﬁ le that can serve \nas the single source for a dimension. In other cases, the raw source data is a mixture \nof dimensional and fact data.\nOften it’s easiest to pull the current snapshots of the dimension tables in their \nentirety and let the transformation step determine what has changed and how \nto handle it. If the dimension tables are large, you may need to use the fact table \ntechnique described in the section “Step 8: Fact Table Incremental Processing” for \nidentifying the changed record set. It can take a long time to look up each entry in \na large dimension table, even if it hasn’t changed from the existing entry.\nIf possible, construct the extract to pull only rows that have changed. This is \nparticularly easy and valuable if the source system maintains an indicator of the \ntype of change.\nIdentify New and Changed Dimension Rows\nThe  DW/BI team may not be successful in pushing the responsibility for identify-\ning new, updated, and deleted rows to the source system owners. In this case, the \nETL process needs to perform an expensive comparison operation to identify new \nand changed rows.\nWhen the incoming data is clean, it’s easy to ﬁ nd new dimension rows. The raw \ndata has an operational natural key, which must be matched to the same column in \nthe current dimension row. Remember, the natural key in the dimension table is an \nordinary dimensional attribute and is not the dimension’s surrogate primary key.\nYou can ﬁ nd new dimension members by performing a lookup from the incoming \nstream to the master dimension, comparing on the natural key. Any rows that fail \nthe lookup are new dimension members and should be inserted into the dimen-\nsion table.\nIf the dimension contains any type 2 attributes, set the row eff ective date column \nto the date the dimension member appeared in the system; this is usually yesterday \nif you are processing nightly. Set the row end date column to the default value for \ncurrent rows. This should be the largest date, very far in the future, supported by \nthe system. You should avoid using a null value in this second date column because \nrelational databases may generate an error or return the special value Unknown if \nyou attempt to compare a speciﬁ c value to a NULL.\nThe next step is to determine if the incoming dimension row has changed. The \nsimplest technique is to compare column by column between the incoming data and \nthe current corresponding member stored in the master dimension table.\nIf the dimension is large, with more than a million rows, the simple technique \nof column-wise comparison may be too slow, especially if there are many columns \n\n\nChapter 20\n514\nin the dimension table. A popular alternative method is to use a hash or checksum \nfunction to speed the comparison process. You can add two new housekeeping \ncolumns to the dimension table: hash type1 and hash type2. You should place \na hash of a concatenation of the type 1 attributes in the hash type1 column and \nsimilarly for hash type2. Hashing algorithms convert a very long string into a \nmuch shorter string that is close to unique. The hashes are computed and stored \nin the dimension table. Then compute hashes on the incoming rowset in exactly \nthe same way, and compare them to the stored values. The comparison on a single, \nrelatively short string column is far more effi  cient than the pair-wise comparison \non dozens of separate columns. Alternatively, the relational database engine may \nhave syntax such as EXCEPT that enables a high-performance query to ﬁ nd the \nchanged rows. \nAs a general rule, you do not delete dimension rows that have been deleted in \nthe source system because these dimension members probably still have fact table \ndata associated with them in the data warehouse.\nProcess Changes to Dimension Attributes\nThe ETL application contains business rules to determine how to handle an attri-\nbute value that has changed from the value already stored in the data warehouse. \nIf the revised description is determined to be a legitimate and reliable update to \nprevious information, then the techniques of slowly changing dimensions must \nbe used.\nThe ﬁ rst step in preparing a dimension row is to decide if you already have \nthat row. If all the incoming dimensional information matches the correspond-\ning row in the dimension table, no further action is required. If the dimensional \ninformation has changed, then you can apply changes to the dimension, such as \ntype 1  or type 2. \nNOTE \nYou may recall from Chapter 5 that there are three primary methods for \ntracking changes in attribute values, as well as a set of advanced hybrid techniques. \nType 3 requires a change in the structure of the dimension table, creating a new \nset of columns to hold the “previous” versus “current” versions of the attributes. \nThis type of structural change is seldom automated in the ETL system; it’s more \nlikely to be handled as a one-time change in the data model.\nThe lookup and key assignment logic for handling a changed dimension record \nduring the extract process is shown in Figure 20-5. In this case, the logic ﬂ ow does \nnot assume the incoming data stream is limited only to new or changed rows.\n\n\nETL System Design and Development Process and Tasks 515\nAdd to\ndimension\nEnd processing\nof row\nRow new?\nYES\nYES\nYES\nYES\nNO\nNO\nRow has Type\n2 changes?\nRow has Type\n1 changes?\nRow has any\nchanges?\nUpdate existing “Current”\nrow: Set Row_End_Date and\nIs_Row_Current\nAdd new dimension row for\nthis entity, with a new\nsurrogate key. Set\nRow_Start_Date = yesterday\nand Is_Row_Current = True\nUpdate Type 1\nattributes–usually\nall existing rows\nfor this entity\nFigure 20-5: Logic ﬂ ow for handling dimension updates.\nStep 8: Fact Table Incremental Processing\nMost  data warehouse databases are too large to entirely replace the fact tables in a \nsingle load window. Instead, new and updated fact rows are incrementally processed.\nNOTE \nIt is much more effi  cient to incrementally load only the records that \nhave been added or updated since the previous load. This is especially true in a \njournal-style system where history is never changed and only adjustments in the \ncurrent period are allowed.\nThe ETL process for fact table incremental processing diff ers from the historic \nload. The historic ETL process doesn’t need to be fully automated; you can stop the \n\n\nChapter 20\n516\nprocess to examine the data and prepare for the next step. The incremental process-\ning, by contrast, must be fully automated.\nFact Table Extract and Data Quality Checkpoint\nAs  soon as the new and changed fact rows are extracted from the source system, a \ncopy of the untransformed data should be written to the staging area. At the same \ntime, measures of data quality on the raw extracted data are computed. The staged \ndata serves three purposes:\n \n■Archive for auditability\n \n■Provide a starting point after data quality veriﬁ cation\n \n■Provide a starting point for restarting the process\nFact Table Transformations and Surrogate Key Pipeline\nThe surrogate key pipeline for the incremental fact data is similar to that for the \nhistoric data. The key diff erence is that the error handling for referential integrity \nviolations must be automated. There are several methods for handling referential \nintegrity violations:\n \n■Halt the load. This is seldom a useful solution; although, it’s often the default \nin many ETL tools.\n \n■Throw away error rows. There are situations in which a missing dimen-\nsion value is a signal that the data is irrelevant to the business requirements \nunderlying the data warehouse.\n \n■Write error rows to a ﬁ le or table for later analysis. Design a mechanism \nfor moving corrected rows into a suspense ﬁ le. This approach is not a good \nchoice for a ﬁ nancial system, where it is vital that all rows be loaded.\n \n■Fix error rows by creating a dummy dimension row and returning its sur-\nrogate key to the pipeline.  The most attractive error handling for referential \nintegrity violations in the incremental surrogate key pipeline is to create a \ndummy dimension row on-the-ﬂ y for the unknown natural key. The natural \nkey is the only piece of information that you may have about the dimension \nmember; all the other attributes must be set to default values. This dummy \ndimension row will be corrected with type 1 updates when the detailed infor-\nmation about that dimension member becomes available.\n \n■Fix error rows by mapping to a single unknown member in each dimen-\nsion. This approach is not recommended. The problem is that all error rows \nare mapped to the same dimension member, for any unknown natural key \nvalues in the fact table extract.\n\n\nETL System Design and Development Process and Tasks 517\nFor most systems, you perform the surrogate key lookups against a query, view, \nor physical table that subsets the dimension table. The dimension table rows are \nﬁ ltered, so the lookup works against only the current version of each dimension \nmember.\nLate Arriving Facts and the Surrogate Key Pipeline\nIn  most data warehouses, the incremental load process begins soon after midnight \nand processes all the transactions that occurred the previous day. However, there \nare scenarios in which some facts arrive late. This is most likely to happen when \nthe data sources are distributed across multiple machines or even worldwide, and \nconnectivity or latency problems prevent timely data collection.\nIf all the dimensions are managed completely as type 1 overwrites, late arriving \nfacts present no special challenges. But most systems have a mixture of type 1 and \ntype 2 attributes. The late arriving facts must be associated with the version of the \ndimension member that was in eff ect when the fact occurred. That requires a lookup \nin the dimension table using the row begin and end eff ective dates.\nIncremental Fact Table Load\nIn  the historic fact load, it’s important that data loads use fast-load techniques. In \nmost data warehouses, these fast-load techniques may not be available for the incre-\nmental load. The fast-load technologies often require stringent conditions on the \ntarget table (for example, empty or unindexed). For the incremental load, it’s usually \nfaster to use non-fast-load techniques than to fully populate or index the table. For \nsmall to medium systems, insert performance is usually adequate.\nIf your fact table is very large, you should already have partitioned the fact table \nfor manageability reasons. If incremental data is always loading into an empty \npartition, you should use fast-load techniques. With daily loads, you would create \n365 new fact table partitions each year. This is probably too many partitions for a \nfact table with long history, so consider implementing a process to consolidate daily \npartitions into weekly or monthly partitions.\nLoad Snapshot Fact Tables\nThe largest fact tables are usually transactional. Transaction fact tables are typically \nloaded only through inserts. Periodic snapshot fact tables are usually loaded at \nmonth end. Data for the current month is sometimes updated each day for current-\nmonth-to-date. In this scenario, monthly partitioning of the fact table makes it easy \nto reload the current month with excellent performance.\nAccumulating snapshot fact tables monitor relatively short-lived processes, such \nas ﬁ lling an order. The accumulating snapshot fact table is characterized by many \n",
      "page_number": 531
    },
    {
      "number": 58,
      "title": "Segment 58 (pages 540-547)",
      "start_page": 540,
      "end_page": 547,
      "detection_method": "topic_boundary",
      "content": "Chapter 20\n518\nupdates for each fact row over the life of the process. This table is expensive to \nmaintain; although accumulating snapshots are almost always much smaller than \nthe other two types of fact tables.\nSpeed Up the Load Cycle\nProcessing only data that has been changed is one way to speed up the ETL cycle. \nThis section lists several additional techniques.\nMore Frequent Loading\nAlthough it is a huge leap to move from a monthly or weekly process to a nightly one, \nit is an eff ective way to shorten the load window. Every nightly process involves 1/30 \nthe data volume of a monthly one. Most data warehouses are on a nightly load cycle.\nIf nightly processing is too expensive, consider performing some preprocessing \non the data throughout the day. During the day, data is moved into a staging database \nor operational data store where data cleansing tasks are performed. After midnight, \nyou can consolidate multiple changes to dimension members, perform ﬁ nal data \nquality checks, assign surrogate keys, and move the data into the data warehouse.\nParallel Processing\nAnother  way to shorten the load time is to parallelize the ETL process. This can \nhappen in two ways: multiple steps running in parallel and a single step running \nin parallel.\n \n■Multiple load steps. The ETL job stream is divided into several independent \njobs submitted together. You need to think carefully about what goes into \neach job; the primary goal is to create independent jobs.\n \n■Parallel execution. The database itself can also identify certain tasks it can \nexecute in parallel. For example, creating an index can typically be parallel-\nized across as many processors as are available on the machine.\nNOTE \nThere are good ways and bad ways to break processing into parallel steps. \nOne simple way to parallelize is to extract all source data together, then load and \ntransform the dimensions, and then simultaneously check referential integrity \nbetween the fact table and all dimensions. Unfortunately, this approach is likely \nto be no faster—and possibly much slower—than the even simpler sequential \napproach because each step launches parallel processes that compete for the same \nsystem resources such as network bandwidth, I/O, and memory. To structure \nparallel jobs well, you need to account not just for logically sequential steps but \nalso for system resources.\n\n\nETL System Design and Development Process and Tasks 519\nParallel Structures\nYou  can set up a three-way mirror or clustered conﬁ guration on two servers to \nmaintain a continuous load data warehouse, with one server managing the loads \nand the second handling the queries. The maintenance window is reduced to a \nfew minutes daily to swap the disks attached to each server. This is a great way to \nprovide high system availability.\nDepending on the requirements and available budget, there are several similar \ntechniques you can implement for tables, partitions, and databases. For example, you \ncan load into an offl  ine partition or table, and swap it into active duty with minimum \ndowntime. Other systems have two versions of the data warehouse database, one for \nloading and one for querying. These are less eff ective, but less expensive, versions \nof the functionality provided by clustered servers.\n Step 9: Aggregate Table and OLAP Loads\nAn  aggregate table is logically easy to build. It’s simply the results of a really big \naggregate query stored as a table. The problem with building aggregate tables from \na query on the fact table, of course, occurs when the fact table is just too big to \nprocess within the load window.\nIf the aggregate table includes an aggregation along the date dimension, perhaps \nto monthly grain, the aggregate maintenance process is more complex. The cur-\nrent month of data must be updated, or dropped and re-created, to incorporate the \ncurrent day’s data.\nA similar problem occurs if the aggregate table is deﬁ ned on a dimension attribute \nthat is overwritten as a type 1. Any type 1 change in a dimension attribute aff ects \nall fact table aggregates and OLAP cubes that are deﬁ ned on that attribute. An ETL \nprocess must “back out” the facts from the old aggregate level and move them to \nthe new one.\nIt is extremely important that the aggregate management system keep aggrega-\ntions in sync with the underlying fact data. You do not want to create a system that \nreturns a diff erent result set if the query is directed to the underlying detail facts \nor to a precomputed  aggregation.\nStep 10: ETL System Operation and Automation\nThe ideal ETL operation runs the regular load processes in a lights-out manner, \nwithout human intervention. Although this is a diffi  cult outcome to attain, it is \npossible to get close.\n\n\nChapter 20\n520\nSchedule Jobs\nScheduling  jobs is usually straightforward. The ETL tool should contain function-\nality to schedule a job to kick off  at a certain time. Most ETL tools also contain \nfunctionality to conditionally execute a second task if the ﬁ rst task successfully \ncompleted. It’s common to set up an ETL job stream to launch at a certain time, and \nthen query a database or ﬁ lesystem to see if an event has occurred.\nYou can also write a script to perform this kind of job control. Every ETL tool \nhas a way to invoke a job from the operating system command line. Many orga-\nnizations are very comfortable using scripting languages, such as Perl, to manage \ntheir job schedules.\nAutomatically Handle Predictable Exceptions and Errors \nAlthough  it’s easy enough to launch jobs, it’s a harder task to make sure they run to \ncompletion, gracefully handling data errors and exceptions. Comprehensive error \nhandling is something that needs to be built into the ETL jobs from the outset.\n Gracefully Handle Unpredictable Errors \nSome errors are predictable, such as receiving an early arriving fact or a NULL value \nin a column that’s supposed to be populated. For these errors, you can generally \ndesign your ETL system to ﬁ x the data and continue processing. Other errors are \ncompletely unforeseen and range from receiving data that’s garbled to experiencing \na power outage during processing.\nWe look for ETL tool features and system design practices to help recover from \nthe unexpected. We generally recommend outﬁ tting fact tables with a single column \nsurrogate key that is assigned sequentially to new records that are being loaded. If \na large load job unexpectedly halts, the fact table surrogate key allows the load to \nresume from a reliable point, or back out the load by constraining on a contiguous \nrange of the surrogate  keys.\n Real-Time Implications\nReal-time  processing is an increasingly common requirement in data warehousing. \nThere is a strong possibility that your DW/BI system will have a real-time require-\nment. Some business users expect the data warehouse to be continuously updated \nthroughout the day and grow impatient with stale data. Building a real-time \nDW/BI system requires gathering a very precise understanding of the true business \nrequirements for real-time data and identifying an appropriate ETL architecture, \nincorporating a variety of technologies married with a solid platform.\n\n\nETL System Design and Development Process and Tasks 521\nReal-Time Triage\nAsking business users if they want “real-time” delivery of data is a frustrating \nexercise for the DW/BI team. Faced with no constraints, most users will say, “That \nsounds good; go for it!” This kind of response is almost worthless.\nTo avoid this situation, we recommend dividing the real-time design challenge \ninto three categories, called instantaneous, intra-day, and daily. We use these terms \nwhen we talk to business users about their needs and then design our data delivery \npipelines diff erently for each option. Figure 20-6 summarizes the issues that arise \nas data is delivered faster.\nDaily\nBatch processing ETL\nMicro-batch ETL\nStreaming EII/ETL\nDrive user presentation from\nsource application\nSeparate from fact table\nProbe with queries or subscribe\nto message bus\nDaily hot fact table time partition\nProvisional\nProvisional\nIndividual transactions\nTransaction fragments\nWait for file ready\nConventional file table time partition\nReconciled\nComplete transaction set\nColumn screens\nColumn screens\nColumn screens\nStructure screens\nStructure screens\n--\n--\n--\nBusiness rule screens\nFinal results\nResults updated, corrected nightly\nResults updated, possibly\nrepudiated nightly\nIntra-Day\nInstantaneous\nFigure 20-6: Data quality trade-offs with low latency delivery.\nInstantaneous means the data visible on the screen represents the true state of \nthe source transaction system at every instant. When the source system status \nchanges, the screen instantly and synchronously responds. An instantaneous real-\ntime system is usually implemented as an enterprise information integration (EII) \nsolution, where the source system itself is responsible for supporting the update of \nremote users’ screens and servicing query requests. Obviously, such a system must \nlimit the complexity of the query requests because all the processing is done on the \nsource system. EII solutions typically involve no caching of data in the ETL pipeline \nbecause EII solutions by deﬁ nition have no delays between the source systems and \nthe users’ screens. Some situations are plausible candidates for an instantaneous \nreal-time solution. Inventory status tracking may be a good example, where the deci-\nsion maker has the right to commit available inventory to a customer in real time.\nIntra-day means the data visible on the screen is updated many times per day but \nis not guaranteed to be the absolute current truth. Most of us are familiar with stock \nmarket quote data that is current to within 15 minutes but is not instantaneous. \n\n\nChapter 20\n522\nThe technology for delivering frequent real-time data (as well as the slower daily \ndata) is distinctly diff erent from instantaneous real-time delivery. Frequently deliv-\nered data is usually processed as micro-batches in a conventional ETL architecture. \nThis means the data undergoes the full gamut of change data capture, extract, stag-\ning to ﬁ le storage in the ETL back room of the data warehouse, cleaning and error \nchecking, conforming to enterprise data standards, assigning of surrogate keys, \nand possibly a host of other transformations to make the data ready to load into the \npresentation server. Almost all these steps must be omitted or drastically reduced \nin an EII solution. The big diff erence between intra-day and daily delivered data is \nin the ﬁ rst two steps: change data capture and extract. To capture data many times \nper day from the source system, the data warehouse usually must tap into a high \nbandwidth communications channel, such as message queue traffi  c between legacy \napplications, an accumulating transaction log ﬁ le, or low level database triggers \ncoming from the transaction system every time something happens.\nDaily means the data visible on the screen is valid as of a batch ﬁ le download or \nreconciliation from the source system at the end of the previous working day. There \nis a lot to recommend daily data. Quite often processes are run on the source system \nat the end of the working day that correct the raw data. When this reconciliation \nbecomes available, that signals the ETL system to perform a reliable and stable \ndownload of the data. If you have this situation, you should explain to the busi-\nness users what compromises they will experience if they demand instantaneous \nor intra-day updated data. Daily updated data usually involves reading a batch ﬁ le \nprepared by the source system or performing an extract query when a source system \nreadiness ﬂ ag is set. This, of course, is the simplest extract scenario because you \nwait for the source system to be ready and available.\nReal-Time Architecture Trade-Offs\nResponding  to real-time requirements means you need to change the DW/BI archi-\ntecture to get data to the business users’ screens faster. The architectural choices \ninvolve trade-off s that aff ect data quality and administration.\nYou can assume the overall goals for ETL system owners are not changed or com-\npromised by moving to real-time delivery. You can remain just as committed to data \nquality, integration, security, compliance, backup, recovery, and archiving as you \nwere before starting to design a real-time system. If you agree with this statement, \nthen read the following very carefully! The following sections discuss the typical \ntrade-off s that occur as you implement a more real-time architecture:\nReplace Batch Files\nConsider replacing a batch ﬁ le extract with reading from a message queue or trans-\naction log ﬁ le. A batch ﬁ le delivered from the source system may represent a clean \n\n\nETL System Design and Development Process and Tasks 523\nand consistent view of the source data. The batch ﬁ le may contain only those records \nresulting from completed transactions. Foreign keys in the batch ﬁ les are prob-\nably resolved, such as when the ﬁ le contains an order from a new customer whose \ncomplete identity may be delivered with the batch ﬁ le. Message queue and log ﬁ le \ndata, on the other hand, is raw instantaneous data that may not be subject to any \ncorrective process or business rule enforcement in the source system. In the worst \ncase, this raw data may 1) be incorrect or incomplete because additional transac-\ntions may arrive later; 2) contain unresolved foreign keys that the DW/BI system \nhas not yet processed; and 3) require a parallel batch-oriented ETL data ﬂ ow to cor-\nrect or even replace the hot real-time data each 24 hours. And if the source system \nsubsequently applies complex business rules to the input transactions ﬁ rst seen in \nthe message queues or the log ﬁ les, then you really don’t want to recapitulate these \nbusiness rules in the ETL system!\nLimit Data Quality Screens\nConsider restricting data quality screening only to column screens and simple \ndecode lookups. As the time to process data moving through the ETL pipeline is \nreduced, it may be necessary to eliminate more costly data quality screening, espe-\ncially structure screens and business rule screens. Remember that column screens \ninvolve single ﬁ eld tests and/or simple lookups to replace or expand known values. \nEven in the most aggressive real-time applications, most column screens should \nsurvive. But structure screens and business rule screens by deﬁ nition require mul-\ntiple ﬁ elds, multiple records, and possibly multiple tables. You may not have time to \npass an address block of ﬁ elds to an address analyzer. You may not check referential \nintegrity between tables. You may not be able to perform a remote credit check \nthrough a web service. All this may require informing the users of the provisional \nand potentially unreliable state of the raw real-time data and may require that you \nimplement a parallel, batch-oriented ETL pipeline that overwrites the real-time data \nperiodically with properly checked data.\n Post Facts with Dimensions\nYou should allow early arriving facts to be posted with old copies of dimensions. In \nthe real-time world, it is common to receive transaction events before the context \n(such as the identity of the customer) of those transactions is updated. In other \nwords, the facts arrive before the dimensions. If the real-time system cannot wait \nfor the dimensions to be resolved, then old copies of the dimensions must be used if \nthey are available, or generic empty versions of the dimensions must be used other-\nwise. If and when revised versions of the dimensions are received, the data warehouse \nmay decide to post those into the hot partition or delay updating the dimension until \na batch process takes over, possibly at the end of the day. In any case, the users need \n\n\nChapter 20\n524\nto understand there may be an ephemeral window of time where the dimensions \ndon’t exactly describe the facts.\nEliminate Data Staging\nSome real-time architectures, especially EII systems, stream data directly from the \nproduction source system to the users’ screens without writing the data to perma-\nnent storage in the ETL pipeline. If this kind of system is part of the DW/BI team’s \nresponsibility, then the team should have a serious talk with senior management \nabout whether backup, recovery, archiving, and compliance responsibilities can be \nmet, or whether those responsibilities are now the sole concern of the production \nsource system.\nReal-Time Partitions in the Presentation Server\nTo  support real-time requirements, the data warehouse must seamlessly extend its \nexisting historical time series right up to the current instant. If the customer has \nplaced an order in the last hour, you need to see this order in the context of the \nentire customer relationship. Furthermore, you need to track the hourly status of \nthis most current order as it changes during the day. Even though the gap between \nthe production transaction processing systems and the DW/BI system has shrunk \nin most cases to 24 hours, the insatiable needs of your business users require the \ndata warehouse to ﬁ ll this gap with real-time data.\nOne design solution for responding to this crunch is building a real-time parti-\ntion as an extension of the conventional, static data warehouse. To achieve real-time \nreporting, a special partition is built that is physically and administratively separated \nfrom the conventional data warehouse tables. Ideally, the real-time partition is a true \ndatabase partition where the fact table in question is partitioned by activity date.\nIn either case, the real-time partition ideally should meet the following tough \nset of requirements:\n \n■Contain all the activity that has occurred since the last update of the static \ndata warehouse.\n \n■Link as seamlessly as possible to the grain and content of the static data ware-\nhouse fact tables, ideally as a true physical partition of the fact table.\n \n■Be indexed so lightly that incoming data can continuously be “dribbled in.” \nIdeally, the real-time partition is completely unindexed; however, this may \nnot be possible in certain RDBMSs where indexes have been built that are not \nlogically aligned with the partitioning scheme.\n \n■Support highly responsive queries even in the absence of indexes by pinning \nthe real-time partition in memory.\n\n\nETL System Design and Development Process and Tasks 525\nThe real-time partition can be used eff ectively with both transaction and periodic \nsnapshot fact tables. We have not found this approach needed with accumulating \nsnapshot fact tables.\nTransaction Real-Time Partition\nIf the static data warehouse fact table has a transaction grain, it contains exactly \none row for each individual transaction in the source system from the beginning \nof “recorded history.” The real-time partition has exactly the same dimensional \nstructure as its underlying static fact table. It contains only the transactions that \nhave occurred since midnight when you last loaded the regular fact tables. The real-\ntime partition may be completely unindexed, both because you need to maintain a \ncontinuously open window for loading and because there is no time series because \nonly today’s data is kept in this table.\nIn a relatively large retail environment experiencing 10 million transactions \nper day, the static fact table would be pretty big. Assuming each transaction grain \nrow is 40 bytes wide (seven dimensions plus three facts, all packed into 4-byte col-\numns), you accumulate 400 MB of data each day. Over a year, this would amount to \napproximately 150 GB of raw data. Such a fact table would be heavily indexed and \nsupported by aggregates. But the daily real-time slice of 400 MB should be pinned \nin memory. The real-time partition can remain biased toward very fast-loading \nperformance but at the same time provide speedy query performance.\nPeriodic Snapshot Real-Time Partition\nIf the static data warehouse fact table has a periodic grain (say, monthly), then the \nreal-time partition can be viewed as the current hot rolling month. Suppose you \nare a big retail bank with 15 million accounts. The static fact table has the grain of \naccount by month. A 36-month time series would result in 540 million fact table \nrows. Again, this table would be extensively indexed and supported by aggregates \nto provide query good performance. The real-time partition, on the other hand, is \njust an image of the current developing month, updated continuously as the month \nprogresses. Semi-additive balances and fully additive facts are adjusted as frequently \nas they are reported. In a retail bank, the supertype fact table spanning all account \ntypes is likely to be quite narrow, with perhaps four dimensions and four facts, \nresulting in a real-time partition of 480 MB. The real-time partition again can be \npinned in memory.\nOn the last day of the month, the periodic real-time partition can, with luck, \njust be merged onto the less volatile fact table as the most current month, and the \nprocess can start again with an empty real-time partition.\n",
      "page_number": 540
    },
    {
      "number": 59,
      "title": "Segment 59 (pages 548-555)",
      "start_page": 548,
      "end_page": 555,
      "detection_method": "topic_boundary",
      "content": "Chapter 20\n526\nSummary\nThe previous chapter introduced 34 subsystems that are possible within a compre-\nhensive ETL implementation. In this chapter, we provided detailed practical advice \nfor actually building and deploying the ETL system. Perhaps the most interesting \nperspective is to separate the initial historical loads from the ongoing incremental \nloads. These processes are quite diff erent.\nIn general we recommend using a commercial ETL tool as opposed to maintaining \na library of scripts, even though the ETL tools can be expensive and have a signiﬁ -\ncant learning curve. ETL systems, more than any other part of the DW/BI ediﬁ ce, \nare legacy systems that need to be maintainable and scalable over long periods of \ntime and over changes of personnel.\nWe concluded this chapter with some design perspectives for real-time (low \nlatency) deli very of data. Not only are the real-time architectures diff erent from \nconventional batch processing, but data quality is compromised as the latency is \nprogressively lowered. Business users need to be thoughtful participants in this \ndesign trade-off .\n\n\nBig Data Analytics\nI\nn this chapter, we introduce big data in all its glory and show how it expands \nthe mission of the DW/BI system. We conclude with a comprehensive list of big \ndata best practices.\nChapter 21 discusses the following concepts:\n \n■Comparison of two architectural approaches for tackling big data analytics\n \n■Management, architecture, modeling, and governance best practices for deal-\ning with big data\nBig Data Overview\nWhat  is big data? Its bigness is actually not the most interesting characteristic. Big data \nis structured, semistructured, unstructured, and raw data in many diff erent formats, in \nsome cases looking totally diff erent than the clean scalar numbers and text you have \nstored in your data warehouses for the last 30 years. Much big data cannot be analyzed \nwith anything that looks like SQL. But most important, big data is a paradigm shift \nin how you think about data assets, where you collect them, how you analyze them, \nand how you monetize the insights from the analysis.\nThe big data movement has gathered momentum as a large number of use cases \nhave been recognized that fall into the category of big data analytics. These use \ncases include:\n \n■Search ranking\n \n■Ad tracking\n \n■Location and proximity tracking\n \n■Causal factor discovery\n \n■Social CRM\n \n■Document similarity testing\n21\n\n\nChapter 21\n528\n \n■Genomics analysis\n \n■Cohort group discovery\n \n■In-ﬂ ight aircraft status\n \n■Smart utility meters\n \n■Building sensors\n \n■Satellite image comparison\n \n■CAT scan comparison\n \n■Financial account fraud detection and intervention\n \n■Computer system hacking detection and intervention\n \n■Online game gesture tracking\n \n■Big science data analysis\n \n■Generic name-value pair analysis\n \n■Loan risk analysis and insurance policy underwriting\n \n■Customer churn analysis\nGiven the breadth of potential use cases, this chapter focuses on the architectural \napproaches for tackling big data, along with our recommended best practices, but \nnot speciﬁ c dimensional designs for each use case.\nConventional RDBMSs and SQL simply cannot store or analyze this wide range \nof use cases. To fully address big data, a candidate system would have to be capable \nof the following:\n \n1. Scaling to easily support petabytes (thousands of terabytes) of data.\n 2. Being distributed across thousands of processors, potentially geographically \ndispersed and potentially heterogeneous.\n \n3. Storing the data in the original captured formats while supporting query and \nanalysis applications without converting or moving the data.\n \n4. Subsecond response time for highly constrained standard SQL queries.\n \n5. Embedding arbitrarily complex user-deﬁ ned functions (UDFs) within process-\ning requests.\n \n6. Implementing UDFs in a wide variety of industry-standard procedural \nlanguages.\n \n7. Assembling extensive libraries of reusable UDFs crossing most or all the use \ncases.\n \n8. Executing UDFs as relation scans over petabyte-sized data sets in a few \nminutes.\n \n9. Supporting a wide variety of data types growing to include images, waveforms, \narbitrarily hierarchical data structures, and collections of name-value pairs.\n 10. Loading data to be ready for analysis, at very high rates, at least gigabytes per \nsecond.\n\n\nBig Data Analytics 529\n 11. Integrating data from multiple sources during the load process at very high \nrates (GB/sec).\n 12. Loading data into the database before declaring or discovering its structure.\n 13. Executing certain streaming analytic queries in real time on incoming load \ndata.\n 14. Updating data in place at full load speeds.\n 15. Joining a billion-row dimension table to a trillion-row fact table without \npreclustering the dimension table with the fact table.\n 16. Scheduling and executing complex multi-hundred node workﬂ ows.\n 17. Being conﬁ gured without being subject to a single point of failure.\n 18. Having failover and process continuation when processing nodes fail.\n 19. Supporting extreme, mixed workloads including thousands of geographically \ndispersed online users and programs executing a variety of requests ranging \nfrom ad hoc queries to strategic analysis, while loading data in batch and \nstreaming fashion.\nIn response to these challenges, two architectures have emerged: extended \nRDBMSs and MapReduce/Hadoop.\nExtended RDBMS Architecture\nExisting  RDBMS vendors are extending the classic relational data types to include \nsome of the new data types required by big data, as shown by the arrows in the \nFigure 21-1.\n- Operational & ODS\n- ERP systems\n- User desktops\n- MDM systems\n- External suppliers\n- RDBMS\n- Flat files & XML docs\n- Message queues\n- Proprietary formats\n- Complex structures\n- Unstructured text\n- Images, video\n- Name-value pairs\n- Atomic business process\n  dimensional models with\n  aggregate navigation\n- Conformed dimensions/\n   facts driving EDW\n   integration\n- Specially crafted UDFs\n  embedded in DBMS\n  inner loop\nETL System\nSource Systems\nBI Applications\nMetadata\nInfrastructure and Security\nBack Room\nFront Room\nPresentation Server\n- Extract\n- Clean\n- Conform\n- Deliver\nETL Management Services\nETL Data Stores\n- Queries\n- Standard reports\n- Analytic applications\n- Dashboards\n- Operational BI\n- Data mining & models\n- Genl purpose programs\nBI Data Stores\nBI Management Services\nBI Portal\nFigure 21-1: Relational DBMS architecture showing big data extensions.\n\n\nChapter 21\n530\nExisting  RDBMSs must open their doors to loading and processing a much \nbroader range of data types including complex structures such as vectors, matrices, \nand custom hyperstructured data. At the other end of the spectrum, the RDBMSs \nneed to load and process unstructured and semistructured text, as well as images, \nvideo, and collections of name-value pairs, sometimes called data bags.\nBut  it is not suffi  cient for RDBMSs to merely host the new data types as blobs to be \ndelivered at some later time to a BI application that can interpret the data, although \nthis alternative has always been possible. To really own big data, RDBMSs must \nallow the new data types to be processed within the DBMS inner loop by means of \nspecially crafted user-deﬁ ned functions (UDFs) written by business user analysts.\nFinally,  a valuable use case is to process the data twice through the RDBMS, \nwhere in the ﬁ rst pass the RDBMS is used as a fact extractor on the original data, \nand then in the second pass, these results are automatically fed back to the RDBMS \ninput as conventional relational rows, columns, and data types.\nMapReduce/Hadoop Architecture\nThe  alternative architecture, MapReduce/Hadoop, is an open source top-level Apache \nproject with many components. MapReduce is a processing framework originally \ndeveloped by Google in the early 2000s for performing web page searches across \nthousands of physically separated machines. The MapReduce approach is extremely \ngeneral. Complete MapReduce systems can be implemented in a variety of languages; \nthe most signiﬁ cant implementation is in Java. MapReduce is actually a UDF execu-\ntion framework, where the “F” can be extraordinarily complex. The most signiﬁ cant \nimplementation of MapReduce is Apache Hadoop, known simply as Hadoop. The \nHadoop project has thousands of contributors and a whole industry of diverse appli-\ncations. Hadoop runs natively on its own Hadoop distributed ﬁ le system (HDFS) and \ncan also read and write to Amazon S3 and others. Conventional database vendors \nare also implementing interfaces to allow Hadoop jobs to be run over massively \ndistributed instances of their databases.\nNOTE \nA full discussion of the MapReduce/Hadoop architecture is beyond the \nscope of this book. Interested readers are invited to study the in depth big data \nresources available on our website at www.kimballgroup.com.\nComparison of Big Data Architectures\nThe two big data architecture approaches have separate long-term advantages and \nare likely to coexist far into the future. At the time of this writing, the characteristics \nof the two architectures are summarized in the  Figure 21-2.\n\n\nBig Data Analytics 531\nExtended Relational DBMS\nProprietary, mostly\nOpen source\nExpensive\nData must be structured\nGreat for speedy indexed lookups\nDeep support for relational semantics\nIndirect support for complex data structures\nIndirect support for iteration, complex branching\nDeep support for transaction processing\nLess expensive\nData does not require structuring\nGreat for massive full data scans\nIndirect support for relational semantics, e.g., Hive\nDeep support for complex data structures\nDeep support for iteration, complex branching\nLittle or no support for transaction processing\nMapReduce/Hadoop\nFigure 21-2: Comparison of relational DBMS and MapReduce/Hadoop architectures.\nRecommended Best Practices for Big Data\nAlthough the big data marketplace is anything but mature, the industry now has a \ndecade of accumulated experience. In that time, a number of best practices speciﬁ c \nto big data have emerged. This section attempts to capture these best practices, \nsteering a middle ground between high-level motherhood admonitions versus down-\nin-the-weeds technical minutiae speciﬁ c to a single tool.\nHaving said that, one should recognize that the industry has a well-tested set of \nbest practices developed over the last 30 years for relationally-based data warehouses \nthat surely are relevant to big data. We list them brieﬂ y. They are to:\n \n■Drive the choice of data sources feeding the data warehouse from business \nneeds.\n \n■Focus incessantly on user interface simplicity and performance.\n \n■Think dimensionally: Divide the world into dimensions and facts.\n \n■Integrate separate data sources with conformed dimensions.\n \n■Track time variance with slowly changing dimensions (SCDs).\n \n■Anchor all dimensions with durable surrogate keys.\nIn the remainder of this section, we divide big data best practices into four cat-\negories: management, architecture, data modeling, and governance.\nManagement Best Practices for Big Data\nThe following best practices apply to the overall management of a big data \nenvironment.\nStructure Big Data Environments Around Analytics\nConsider  structuring big data environments around analytics and not ad hoc que-\nrying or standard reporting. Every step in the data pathway from original source \n\n\nChapter 21\n532\nto analyst’s screen must support complex analytic routines implemented as user-\ndeﬁ ned functions (UDFs) or via a metadata-driven development environment that \ncan be programmed for each type of analysis. This includes loaders, cleansers, \nintegrators, user interfaces, and ﬁ nally BI tools, as further discussed in the archi-\ntectural best practices section.\nDelay Building Legacy Environments\nIt’s  not a good idea to attempt building a legacy big data environment at this time. \nThe big data environment is changing too rapidly to consider building a long-lasting \nlegacy foundation. Rather, plan for disruptive changes coming from every direc-\ntion: new data types, competitive challenges, programming approaches, hardware, \nnetworking technology, and services off ered by literally hundreds of new big data \nproviders. For the foreseeable future, maintain a balance among several imple-\nmentation approaches including Hadoop, traditional grid computing, pushdown \noptimization in an RDBMS, on-premise computing, cloud computing, and even \nthe mainframe. None of these approaches will be the single winner in the long \nrun. Platform as a service (PaaS) providers off er an attractive option that can help \nassemble a compatible set of tools. \nThink of Hadoop as a ﬂ exible, general purpose environment for many forms of \nETL processing, where the goal is to add suffi  cient structure and context to big data \nso that it can be loaded into an RDBMS. The same data in Hadoop can be accessed \nand transformed with Hive, Pig, HBase, and MapReduce code written in a variety \nof languages, even simultaneously.\nThis demands ﬂ exibility. Assume you will reprogram and rehost all your big \ndata applications within two years. Choose approaches that can be reprogramed \nand rehosted. Consider using a metadata-driven codeless development environment \nto increase productivity and help insulate from underlying technology  changes.\nBuild From Sandbox Results\nConsider  embracing sandbox silos and building a practice of productionizing sand-\nbox results. Allow data scientists to construct their data experiments and prototypes \nusing their preferred languages and programming environments. Then, after proof \nof concept, systematically reprogram these implementations with an IT turnover \nteam. Here are a couple of examples to illustrate this recommendation:\nThe production environment for custom analytic programming might be MatLab \nwithin PostgreSQL or SAS within a Teradata RDBMS, but the data scientists might be \nbuilding their proofs of concept in a wide variety of their own preferred languages \nand architectures. The key insight here: IT must be uncharacteristically tolerant \nof the range of technologies the data scientists use and be prepared in many cases \nto re-implement the data scientists’ work in a standard set of technologies that can \nbe supported over the long haul. The sandbox development environment might \n\n\nBig Data Analytics 533\nbe custom R code directly accessing Hadoop, but controlled by a metadata-driven \ndriven ETL tool. Then when the data scientist is ready to hand over the proof of \nconcept, much of the logic could immediately be redeployed under the ETL tool to \nrun in a grid computing environment that is scalable, highly available, and secure.\nTry Simple Applications First\nYou  can put your toe in the water with a simple big data application, such as backup \nand archiving. While starting with a big data program, and searching for valuable \nbusiness use cases with limited risk and when assembling the requisite big data \nskills, consider using Hadoop as a low-cost, ﬂ exible backup and archiving technol-\nogy. Hadoop can store and retrieve data in the full range of formats from totally \nunstructured to highly structured specialized formats. This approach may also \nenable you address the sunsetting challenge where original applications may not be \navailable in the distant future (perhaps because of licensing restrictions); you can \ndump data from those applications into your documented format.\nArchitecture Best Practices for Big Data\nThe following best practices aff ect the overall structure and organization of your \nbig data environment.\nPlan a Data Highway\nYou  should plan for a logical data highway with multiple caches of increasing latency. \nPhysically implement only those caches appropriate for your environment. The data \nhighway can have as many as ﬁ ve caches of increasing data latency, each with its \ndistinct analytic advantages and trade-off s, as shown in Figure 21-3.\nRaw Source\n(Immediate)\nReal Time\nCache\n(Seconds)\nBusiness Activity\nCache\n(Minutes)\nTop Line\nCache\n(24 Hours)\nDW and\nLong Time Series\n(Daily, Periodic, Annual)\nFigure 21-3: Big data caches of increasing latency and data quality.\nHere are potential examples of the ﬁ ve data caches:\n \n■Raw source applications: Credit card fraud detection, immediate complex \nevent processing (CEP) including network stability and cyber attack detection.\n \n■Real time applications: Web page ad selection, personalized price promotions, \non-line games monitoring.\n \n■Business activity applications: Low-latency KPI dashboards pushed to users, \ntrouble ticket tracking, process completion tracking, “fused” CEP reporting, \ncustomer service portals and dashboards, and mobile sales apps.\n",
      "page_number": 548
    },
    {
      "number": 60,
      "title": "Segment 60 (pages 556-564)",
      "start_page": 556,
      "end_page": 564,
      "detection_method": "topic_boundary",
      "content": "Chapter 21\n534\n \n■Top line applications: Tactical reporting, promotion tracking, midcourse cor-\nrections based on social media buzz. Top line refers to the common practice \nby senior managers of seeing a quick top line review of what has happened \nin the enterprise over the past 24 hours.\n \n■Data warehouse and long time series applications: All forms of reporting, \nad hoc querying, historical analysis, master data management, large scale \ntemporal dynamics, and Markov chain analysis.\nEach cache that exists in a given environment is physical and distinct from the \nother caches. Data moves from the raw source down this highway through ETL \nprocesses. There may be multiple paths from the raw source to intermediate caches. \nFor instance, data could go to the real-time cache to drive a zero latency-style user \ninterface, but at the same time be extracted directly into a daily top line cache that \nwould look like a classic operational data store (ODS). Then the data from this ODS \ncould feed the data warehouse. Data also ﬂ ows in the reverse direction along the \nhighway. We’ll discuss implementing backﬂ ows later in this section.\nMuch of the data along this highway must remain in nonrelational formats rang-\ning from unstructured text to complex multistructured data, such as images, arrays, \ngraphs, links, matrices, and sets of name-value pairs.\nBuild a Fact Extractor from Big Data\nIt’s a  good idea to use big data analytics as a fact extractor to move data to the next \ncache. For example, the analysis of unstructured text tweets can produce a whole \nset of numerical, trendable sentiment measures including share of voice, audience \nengagement, conversation reach, active advocates, advocate inﬂ uence, advocacy \nimpact, resolution rate, resolution time, satisfaction score, topic trends, sentiment \nratio, and idea impact.\nBuild Comprehensive Ecosystems\nYou  can use big data integration to build comprehensive ecosystems that integrate \nconventional structured RDBMS data, documents, e-mails, and in-house, business-\noriented social networking. One of the potent messages from big data is the ability \nto integrate disparate data sources of diff erent modalities. You get streams of data \nfrom new data producing channels such as social networks, mobile devices, and \nautomated alert processes. Imagine a big ﬁ nancial institution handling millions of \naccounts, tens of millions of associated paper documents, and thousands of profes-\nsionals both within the organization and in the ﬁ eld as partners or customers. Now \nset up a secure social network of all the trusted parties to communicate as business \nis conducted. Much of this communication is signiﬁ cant and should be saved in a \nqueryable way. You could capture all this information in Hadoop, dimensionalize \nit (as you see in the following modeling best practices), use it in the course of busi-\nness, and then back it up and archive it.\n\n\nBig Data Analytics 535\nPlan for Data Quality\nYou  can plan for data quality to be better further along the data highway. This is \nthe classic trade-off  of latency versus quality. Analysts and business users must \naccept the reality that very low latency (that is, immediate) data is unavoidably \ndirty because there are limits to how much cleansing and diagnosing can be done \nin very short time intervals. Tests and corrections on individual ﬁ eld contents can \nbe performed at the fastest data transfer rates. Tests and corrections on structural \nrelationships among ﬁ elds and across data sources are necessarily slower. Tests \nand corrections involving complex business rules range from being instantaneous \n(such as a set of dates being in a certain order) to taking arbitrarily long times (such \nas waiting to see if a threshold of unusual events has been exceeded). And ﬁ nally, \nslower ETL processes, such as those feeding the daily top line cache, often are built \non fundamentally more complete data, for example where incomplete transaction \nsets and repudiated transactions have been eliminated. In this case, the instanta-\nneous data feeds simply do not have the correct  information.\nAdd Value to Data as Soon as Possible\nYou  should apply ﬁ ltering, cleansing, pruning, conforming, matching, joining, and \ndiagnosing at the earliest touch points possible. This is a corollary of the previous \nbest practice. Each step on the data highway provides more time to add value to \nthe data. Filtering, cleansing, and pruning the data reduces the amount transferred \nto the next cache and eliminates irrelevant or corrupted data. To be fair, there is \na school of thought that applies cleansing logic only at analysis run time because \ncleansing might delete “interesting outliers.” Conforming takes the active step of \nplacing highly administered enterprise attributes into major entities such as cus-\ntomer, product, and date. The existence of these conformed attributes allows high \nvalue joins to be made across separate application domains. A shorter name for this \nstep is “integration!” Diagnosing allows many interesting attributes to be added to \ndata, including special conﬁ dence tags and textual identiﬁ ers representing behavior \nclusters identiﬁ ed by a data mining professional.\nImplement Backﬂ ow to Earlier Caches\nYou  should implement backﬂ ows, especially from the data warehouse, to earlier caches \non the data highway. The highly administered dimensions in the data warehouse, such \nas customer, product, and date, should be connected back to data in earlier caches. \nIdeally, all that is needed are unique durable keys for these entities in all the caches. \nThe corollary here is that Job One in each ETL step from one cache to the next is to \nreplace idiosyncratic proprietary keys with the unique durable keys so that analysis \nin each cache can take advantage of the rich upstream content with a simple join on \nthe unique durable key. Can this ETL step be performed even when transferring raw \nsource data into the real time cache in less than a second? Maybe….\n\n\nChapter 21\n536\nDimension data is not the only data to be transferred back down the highway \ntoward the source. Derived data from fact tables, such as historical summaries and \ncomplex data mining ﬁ ndings, can be packaged as simple indicators or grand totals \nand then transferred to earlier caches on the data highway.\nImplement Streaming Data\nYou  should implement streaming data analytics in selected data ﬂ ows. An interest-\ning angle on low latency data is the need to begin serious analysis on the data as \nit streams in, but possibly far before the data transfer process terminates. There is \nsigniﬁ cant interest in streaming analysis systems, which allow SQL-like queries to \nprocess the data as it ﬂ ows into the system. In some use cases, when the results of \na streaming query surpass a threshold, the analysis can be halted without running \nthe job to the bitter end. An academic eff ort, known as continuous query language \n(CQL), has made impressive progress in deﬁ ning the requirements for streaming \ndata processing including clever semantics for dynamically moving time windows \non the streaming data. Look for CQL language extensions and streaming data query \ncapabilities in the load programs for both RDBMSs and HDFS deployed data sets. \nAn ideal implementation would allow streaming data analysis to take place while \nthe data is loaded at gigabytes per second.\nAvoid Boundary Crashes\nYou  should implement far limits on scalability to avoid a boundary crash. In the early \ndays of computer programming, when machines had pathetically small hard drives \nand real memories, boundary crashes were common and were the bane of applica-\ntions development. When the application ran out of disk space or real memory, the \ndeveloper resorted to elaborate measures, usually requiring signiﬁ cant program-\nming that added nothing to the application’s primary function. Boundary crashes for \nnormal database applications have more or less been eliminated, but big data raises \nthis issue again. Hadoop is an architecture that dramatically reduces programming \nscalability concerns because you can, for the most part, indeﬁ nitely add commodity \nhardware. Of course, even commodity hardware must be provisioned, plugged in, \nand have high bandwidth network connections. The lesson is to plan far ahead for \nscaling out to huge volumes and throughputs.\nMove Prototypes to a Private Cloud\nConsider  performing big data prototyping on a public cloud and then moving to a \nprivate cloud. The advantage of a public cloud is it can be provisioned and scaled up \ninstantly. In those cases in which the sensitivity of the data allows quick in-and-out \nprototyping, this can be eff ective. Just remember not to leave a huge data set online with \nthe public cloud provider over the weekend when the programmers have gone home! \n\n\nBig Data Analytics 537\nHowever, keep in mind that in some cases in which you are trying to exploit data locality \nwith rack-aware MapReduce processes, you may not use a public cloud service because \nit may not provide the data storage control needed.\nStrive for Performance Improvements\nSearch for and expect tenfold to hundredfold performance improvements over time, \nrecognizing the paradigm shift for analysis at high speeds. The openness of the big \ndata marketplace has encouraged hundreds of special purpose tightly coded solu-\ntions for speciﬁ c kinds of analysis. This is a giant blessing and a curse. When free \nfrom being controlled by a big vendor’s RDBMS optimizer and inner loop, smart \ndevelopers can implement spot solutions that are truly 100 times as fast as standard \ntechniques. For instance, some impressive progress has been made on the infamous \n“big join” problem in which a billion-row dimension is joined to a trillion-row fact \ntable. The challenge is these individual spot solutions may not be part of a uniﬁ ed \nsingle architecture.\nOne very current big data theme is visualization of data sets. “Flying around” \na petabyte of data requires spectacular performance! Visualization of big data is \nan exciting new area of development that enables both analysis and discovery of \nunexpected features and data proﬁ ling.\nAnother  exciting application that imposes huge performance demands is “seman-\ntic zooming without pre-aggregations,” in which the analyst descends from a highly \naggregated level to progressively more detailed levels in unstructured or semistruc-\ntured data, analogous to zooming in on a map.\nThe important lesson behind this best practice is that revolutionary advances \nin your power to consume and analyze big data can result from 10x to 100x per-\nformance gains, and you have to be prepared to add these developments to your \nsuite of  tools.\nMonitor Compute Resources\nYou  should separate big data analytic workloads from the conventional data ware-\nhouse to preserve service level agreements. If your big data is hosted in Hadoop, \nit probably doesn’t compete for resources with your conventional RDBMS-based \ndata warehouse. However, be cautious if your big data analytics run on the data \nwarehouse machine because big data requirements change rapidly and inevitably \nin the direction of requiring more compute resources.\nExploit In-Database Analytics\nRemember  to exploit the unique capabilities of in-database analytics. The major \nRDBMS players all signiﬁ cantly invest in in-database analytics. After you pay the \nprice of loading data into relational tables, SQL can be combined with analytic \n\n\nChapter 21\n538\nextensions in extremely powerful ways. In particular, PostgreSQL, an open source \ndatabase, has extensible syntax for adding powerful user deﬁ ned functions in the \ninner loop.\nData Modeling Best Practices for Big Data\nThe following best practices aff ect the logical and physical structures of the data.\nThink Dimensionally\nBy thinking dimensionally, we mean dividing the world into dimensions and facts. \nBusiness users ﬁ nd the concept of dimensions to be natural and obvious. No matter \nwhat the format of the data, the basic associated entities such as customer, product, \nservice, location, or time can always be found. In the following best practice you \nsee how, with a little discipline, dimensions can be used to integrate data sources. \nBut before getting to the integration ﬁ nish line, you must identify the dimensions \nin each data source and attach them to every low-level atomic data observation. \nThis process of dimensionalization is a good application for big data analytics. For \nexample, a single Twitter tweet “Wow! That is awesome!” may not seem to con-\ntain anything worth dimensionalizing, but with some analysis you often can get \ncustomer (or citizen or patient), location, product (or service or contract or event), \nmarketplace condition, provider, weather, cohort group (or demographic cluster), \nsession, triggering prior event, ﬁ nal outcome, and the list goes on. Some form of \nautomated dimensionalizing is required to stay ahead of the high-velocity streams \nof data. As we point out in a subsequent best practice, incoming data should be fully \ndimensionalized at the earliest extraction step in as close to real time as  possible.\nIntegrate Separate Data Sources with Conformed Dimensions\nConformed  dimensions are the glue that holds together separate data sources and \nenable them to be combined in a single analysis. Conformed dimensions are perhaps \nthe most powerful best practice from the conventional DW/BI world that should be \ninherited by big data.\nThe basic idea behind conformed dimensions is the presence of one or more \nenterprise attributes (ﬁ elds) in the versions of dimensions associated with separate \ndata sources. For instance, every customer-facing process in an enterprise will have \nsome variation of a customer dimension. These variations of the customer dimension \nmay have diff erent keys, diff erent ﬁ eld deﬁ nitions, and even diff erent granularity. \nBut even in the worst cases of incompatible data, one or more enterprise attributes \ncan be deﬁ ned that can be embedded in all the customer dimension variations. For \ninstance, a customer demographic category is a plausible choice. Such a descriptor \ncould be attached to nearly every customer dimension, even those at higher levels \nof aggregation. After this has been done, analyses on this customer demographic \n\n\nBig Data Analytics 539\ncategory can cross every participating data source with a simple sort-merge process \nafter separate queries are run against the diff erent data sources. Best of all, the step \nof introducing the enterprise attributes into the separate databases can be done in \nan incremental, agile, and nondisruptive way as described in Chapter 8: Customer \nRelationship Management and Chapter 19: ETL Subsystems and Techniques. All \nexisting analysis applications will continue to run as the conformed dimension \ncontent is rolled out.\nAnchor Dimensions with Durable Surrogate Keys\nIf  there is one lesson we have learned in the data warehouse world, it is not to anchor \nmajor entities such as customer, product, and time with the natural keys deﬁ ned by \na speciﬁ c application. These natural keys turn out to be a snare and a delusion in the \nreal world. They are incompatible across applications and are poorly administered, \nand they are administered by someone else who may not have the interests of the \ndata warehouse at heart. The ﬁ rst step in every data source is to augment the natural \nkey coming from a source with an enterprisewide durable surrogate key. Durable \nmeans there is no business rule that can change the key. The durable key belongs \nto the DW/BI system, not to the data source. Surrogate means the keys themselves \nare simple integers either assigned in sequence or generated by a robust hashing \nalgorithm that guarantees uniqueness. An isolated surrogate key has no applications \ncontent. It is just an identiﬁ er.\nThe big data world is ﬁ lled with obvious dimensions that must possess durable \nsurrogate keys. Earlier in this chapter when we proposed pushing data backward \ndown the data highway, we relied on the presence of the durable surrogate keys to \nmake this process work. We also stated that Job One on every data extraction from a \nraw source was to embed the durable surrogate keys in the appropriate dimensions.\nExpect to Integrate Structured and Unstructured Data\nBig  data considerably broadens the integration challenge. Much big data will never \nend up in a relational database; rather it will stay in Hadoop or a grid. But after you \nare armed with conformed dimensions and durable surrogate keys, all forms of data \ncan be combined in single analyses. For example, a medical study can select a group \nof patients with certain demographic and health status attributes and then combine \ntheir conventional DW/BI data with image data (photographs, X-rays, EKGs, and \nso on), free form text data (physician’s notes), social media sentiments (opinions of \ntreatment), and cohort group linkages (patients with similar situations), and doc-\ntors with similar patients.\nUse Slowly Changing Dimensions\nYou  should track time variance with slowly changing dimensions (SCDs). Tracking \ntime variance of dimensions is an old and venerable best practice from the data \n\n\nChapter 21\n540\nwarehouse world. Chapter 5: Procurement makes a powerful case for using SCD \ntechniques for handling time variance. This is just as important in the big data \nworld as it is in the conventional data warehouse world.\nDeclare Data Structure at Analysis Time\nYou  must get used to not declaring data structures until analysis time. One of the \ncharms of big data is putting off  declaring data structures at the time of loading into \nHadoop or a data grid. This brings many advantages. The data structures may not \nbe understood at load time. The data may have such variable content that a single \ndata structure either makes no sense or forces you to modify the data to ﬁ t into \na structure. If you can load data into Hadoop, for instance, without declaring its \nstructure, you can avoid a resource intensive step. And ﬁ nally, diff erent analysts may \nlegitimately see the same data in diff erent ways. Of course, there is a penalty in some \ncases because data without a declared structure may be diffi  cult or impossible to \nindex for rapid access, as in an RDBMS. However, most big data analysis algorithms \nprocess entire data sets without expecting precise ﬁ ltering of subsets of the data.\nThis best practice conﬂ icts with traditional RDBMS methodologies, which puts a \nlot of emphasis on modeling the data carefully before loading. But this does not lead \nto a deadly conﬂ ict. For data destined for an RDBMS, the transfer from a Hadoop or \ndata grid environment and from a name-value pair structure into RDBMS named \ncolumns can be thought of as a valuable ETL step.\nLoad Data as Simple Name-Value Pairs\nConsider  building technology around name-value pair data sources. Big data \nsources are ﬁ lled with surprises. In many cases, you open the ﬁ re hose and discover \nunexpected or undocumented data content, which you must nevertheless load at \ngigabytes per second. The escape from this problem is to load this data as simple \nname-value pairs. For example, if an applicant were to disclose her ﬁ nancial assets, \nas illustrated with Figures 8-7 and 8-8, she might declare something unexpected \nsuch as “rare postage stamp = $10,000.” In a name-value pair data set, this would be \nloaded gracefully, even though you had never seen “rare postage stamp” and didn’t \nknow what to do with it at load time. Of course, this practice meshes nicely with the \nprevious practice of deferring the declaration of data structures until past load time.\nMany MapReduce programming frameworks require data to be presented as \nname-value pairs, which makes sense given the complete possible generality of \nbig data.\nRapidly Prototype Using Data Virtualization\nConsider  using data virtualization to allow rapid prototyping and schema altera-\ntions. Data virtualization is a powerful technique for declaring diff erent logical data \n\n\nBig Data Analytics 541\nstructures on underlying physical data. Standard view deﬁ nitions in SQL are a good \nexample of data virtualization. In theory, data virtualization can present a data \nsource in any format the analyst needs. But data virtualization trades off  the cost of \ncomputing at run time with the cost of ETL to build physical tables before run time. \nData virtualization is a powerful way to prototype data structures and make rapid \nalterations or provide distinct alternatives. The best data virtualization strategy is \nto expect to materialize the virtual schemas when they have been tested and vet-\nted and the analysts want the performance improvements of actual physical tables.\nData Governance Best Practices for Big Data\nThe following best practices apply to managing big data as a valuable enterprise \nasset.\nThere is No Such Thing as Big Data Governance\nNow  that we have your attention, the point is that data governance must be a com-\nprehensive approach for the entire data ecosystem, not a spot solution for big data \nin isolation. Data governance for big data should be an extension of the approach \nused to govern all the enterprise data. At a minimum, data governance embraces \nprivacy, security, compliance, data quality, metadata management, master data \nmanagement, and the business glossary that exposes deﬁ nitions and context to \nthe business community.\nDimensionalize the Data before Applying Governance\nHere  is an interesting challenge big data introduces: You must apply data gover-\nnance principles even when you don’t know what to expect from the content of the \ndata. You may receive data arriving at gigabytes per minute, often as name-value \npairs with unexpected content. The best chance at classifying data in ways that are \nimportant to your data governance responsibilities is to dimensionalize it as fully \nas possible at the earliest stage in the data pipeline. Parse it, match it, and apply \nidentity resolution on-the-ﬂ y. We made this same point when arguing for the ben-\neﬁ ts of data integration, but here we advocate against even using the data before \nthis dimensionalizing step.\nPrivacy is the Most Important Governance Perspective\nIf  you analyze data sets that include identifying information about individuals or \norganizations, privacy is the most important governance perspective. Although \nevery aspect of data governance looms as critically important, in these cases, privacy \ncarries the most responsibility and business risk. Egregious episodes of compro-\nmising the privacy of individuals or groups can damage your reputation, diminish \nmarketplace trust, expose you to civil lawsuits, and get you in trouble with the \n\n\nChapter 21\n542\nlaw. At the least, for most forms of analysis, personal details must be masked, and \ndata aggregated enough to not allow identiﬁ cation of individuals. At the time of \nthis writing, special attention must be paid when storing sensitive data in Hadoop \nbecause after data is written to Hadoop, Hadoop doesn’t manage updates very well. \nData should either be masked or encrypted on write (persistent data masking) or \ndata should be masked on read (dynamic data masking).\nDon’t Choose Big Data over Governance\nDon’t put off  data governance completely in the rush to use big data. Even for \nexploratory big data prototype projects, maintain a checklist of issues to consider \nwhen going forward. You don’t want an ineff ective bureaucracy, but maybe you can \nstrive to deliver an agile bureaucracy! \nSummary\nBig data brings a host of changes and opportunities to IT, and it is easy to think that \na whole new set of rules must be created. But with the beneﬁ t of big data experience, \nmany best practices have emerged. Many of these practices are recognizable exten-\nsions from the DW/BI world, and admittedly quite a few are new and novel ways \nof thinking about data and the mission of IT. But the recognition that the mission \nhas expanded is welcome and is in some ways overdue. The current explosion of \ndata-collecting  channels, new data types, and new analytic opportunities mean the \nlist of best practices will continue to grow in interesting ways.\n",
      "page_number": 556
    },
    {
      "number": 61,
      "title": "Segment 61 (pages 565-572)",
      "start_page": 565,
      "end_page": 572,
      "detection_method": "topic_boundary",
      "content": "Index\nSymbols\n3NF (third normal form) models, 7\nERDs (entity-relationship diagrams), 8\nnormalized 3NF structures, 8\n4-step dimensional design process, 38, 70–72\nA\nabnormal scenario indicators, 255–256\nabstract generic dimensions, 66\ngeographic location dimension, 310\naccessibility goals, 3\naccidents (insurance case study), factless fact \ntables, 396\naccounting case study, 202\nbudgeting, 210–213\nfact tables, consolidated, 224–225\nG/L (general ledger), 203\nchart of accounts, 203–204\ncurrencies, 206\nﬁ nancial statements, 209–210\nﬁ scal calendar, multiple, 208\nhierarchies, 209\njournal entries, 206–207\nperiod close, 204–206\nperiodic snapshot, 203\nyear-to-date facts, 206\nhierarchies\nﬁ xed depth, 214\nmodifying, ragged, 221\nragged, alternative modeling approaches, \n221–223\nragged, bridge table approach, 223\nragged, modifying, 220–221\nragged, shared ownership, 219\nragged, time varying, 220\nragged, variable depth, 215–217\nvariable depth, 214–215\nOLAP and, 226\naccumulating grain fact tables, 12\naccumulating snapshots, 44, 118–119, \n194–196\nclaims (insurance case study), 393\ncomplex workﬂ ows, 393–394\ntimespan accumulating snapshot, \n394–395\nETL systems, 475\nfact tables, 121, 326–329\ncomplementary fact tables, 122\nmilestones, 121\nOLAP cubes, 121–122\nupdates, 121–122\nhealthcare case study, 343\npolicy (insurance case study), 384–385\ntype 2 dimensions and, 196\nactivity-based costing measures, 184\nadditive facts, 11, 42\nadd mini dimension and type 1 outrigger \n(SCD type 5), 55\nadd mini-dimension (SCD type 4), 55\nmultiple, 156–159\nadd new attribute (SCD type 3), 55, 154–155\nmultiple, 156\nadd new row (SCD type 2), 54, 150–152\neff ective date, 152–153\nexpiration date, 152–153\ntype 1 in same dimension, 153\naddresses\nASCII, 236\nCRM and, customer dimension, 233–238\nUnicode, 236–238\nadd type 1 attributes to type 2 dimension \n(SCD type 6), 56\nadmissions events (education case study), \n330\naggregate builder, ETL system, 481\naggregated facts\nas attributes, 64\nCRM and, customer dimension, 239–240\n\n\nIndex  \n544\naggregate fact tables, 45\nclickstream data, 366–367\naggregate OLAP cubes, 8, 45\naggregate tables, ETL system development, \n519\nagile development, 34–35\nconformed dimensions and, 137–138\nairline case study, 311\nbus matrix, 311–315\ncalendars as outriggers, 321–323\nclass of service ﬂ own dimension, 319–320\ndestination airport dimension, 320–321\nfact tables, granularity, 312–316\norigin dimension, 320–321\npassenger dimension, 314\nsales channel dimension, 315\nsegments, linking to trips, 315–316\ntime zones, multiple, 323\naliasing, 171\nallocated facts, 60\nallocating, 184–186\nallocations, proﬁ t and loss fact tables, 60\nALTER TABLE command, 17\nanalytics\nbig data management, 531\nGA (Google Analytics), 367\nin-database, big data and, 537\nanalytic solutions, packaged, 270–271\nAND queries, skill keywords bridge, 275\narchitecture\nbig data best practices\nbackﬂ ow, 535–536\nboundary crashes, 536\ncompute resources, 537\ndata highway planning, 533–534\ndata quality planning, 535\ndata value, 535\necosystems, 534\nfact extractor, 534\nin-database analytics, 537\nperformance improvements, 537\nprototypes, 536\nstreaming data, 536\nDW/BI alternatives, 26–29\nenterprise data warehouse bus architecture, \n22, 123–125\nhub-and-spoke CIF architecture, 28–29\nhybrid hub-and-spoke Kimball architecture, \n29\nindependent data mart architecture, 26–27\nMapReduce/Hadoop, 530\nRDBMS, extension, 529–530\nreal-time processing, 522–524\narchiving, 447–448, 485–486\nartiﬁ cial keys, 98\nASCII (American Standard Code for \nInformation Interchange), 236\natomic grain data, 17, 74\nattributes\naggregated facts as, 64\nbridge tables, CRM and, 247\nchanges, 514\ndetailed dimension model, 437\nexpiration, 266\nﬂ ags, 48\nindicators, 48\nnull, 48, 92\nnumeric values as, 59\npathstring, ragged/variable depth \nhierarchies, 57\nproduct dimensions, 132\nSCD type 3 (add new attribute), 154–155\nmultiple, 156\naudit columns, CDC (change data capture), \n452\naudit dimensions, 66, 192–193, 284, 495\nassembler, 460\ninsurance case study, 383\nkey assignment, 511–512\nautomation, ETL system development\nerrors, 520\nexceptions, 520\njob scheduling, 520\nB\nbackﬂ ow, big data and, 535–536\nbackups, 495\nbackup system, ETL systems, 485\narchiving, 485–486\ncompliance manager, 493–495\ndependency, 490–491\nhigh performance, 485\nlights-out operations, 485\nlineage, 490–491\nmetadata repository, 495\nparallelizing/pipelining system, 492\nproblem escalation system, 491–492\nrecovery and restart system, 486–488\nretrieval, 485–486\nsecurity system, 492–493\nsimple administration, 485\nsorting system, 490\nversion control system, 488\nversion migration system, 488\nworkﬂ ow monitor, 489–490\nbanking case study, 282\nbus matrix, 282–296\ndimensions\nhousehold, 286–287\n\n\nIndex 545\nmini-dimensions, 289–291\nmultivalued, weighting, 287–289\ntoo few, 283–286\nfacts, value banding, 291–292\nheterogeneous products, 293–295\nhot swappable dimensions, 296\nuser perspective, 293\nbehavior\ncustomers, CRM and, 249–251\nsequential, step dimension and, 251–252\nstudy groups, 64, 249\nbehavior tags\nfacts, 241\ntime series, 63, 240–242\nBI application design/development \n(Lifecycle), 408, 423 –424\nBI applications, 22\nBI (business intelligence) delivery interfaces, \n448\nbig data\narchitecture best practices\nbackﬂ ow, 535–536\nboundary crashes, 536\ncompute resources, 537\ndata highway planning, 533–534\ndata quality planning, 535\ndata value, 535\necosystems, 534\nfact extractor, 534\nin-database analytics, 537\nperformance improvements, 537\nprototypes, 536\nstreaming data, 536\ndata governance best practices, 541\ndimensionalizing and, 541\nprivacy, 541–542\ndata modeling best practices\ndata structure declaration, 540\ndata virtualization, 540\ndimension anchoring, 539\nintegrating sources and conﬁ ned \ndimensions, 538\nname-value pairs, 540\nSCDs (slowly changing dimensions), 539\nstructured/unstructured data integration, \n539\nthinking dimensionally, 538\nmanagement best practices\nanalytics and, 531\nlegacy environments and, 532\nsandbox results and, 532–533\nsunsetting and, 533\noverview, 527–529\nblobs, 530\nboundary crashes, big data and, 536\nbridge tables\ncustomer contacts, CRM and, 248\nmini-dimensions, 290–291\nmultivalued\nCRM and, 245–246\ntime varying, 63\nmultivalued dimensions, 63, 477–478\nragged hierarchies and, 223\nragged/variable depth hierarchies, 57\nsparse attributes, CRM and, 247\nbubble chart, dimension modeling and, \n435–436\nbudget fact table, 210\nbudgeting process, 210–213\nbus architecture, 124–125\nenterprise data warehouse bus architecture, \n52\nbusiness analyst, 408\nBusiness Dimensional Lifecycle, 404\nbusiness-driven governance, 136–137\nbusiness driver, 408\nbusiness initiatives, 70\nbusiness lead, 408\nbusiness motivation, Lifecycle planning, 407\nbusiness processes\ncharacteristics, 70–71\ndimensional modeling, 39, 300\nretail sales case study, 74\nvalue chain, 111–112\nbusiness representatives, dimensional \nmodeling, 431–432\nbusiness requirements\ndimensional modeling, 432\nLifecycle, 405, 410\ndocumentation, 414\nforum selection, 410–411\ninterviews, 412–414\nlaunch, 412\nprioritization, 414–415\nrepresentatives, 411–412\nteam, 411\nbusiness rule screens, 458\nbusiness sponsor, 408\nLifecycle planning, 406\nbusiness users, 408\nperspectives, 293\nbus matrix\naccounting, 202\nairline, 311\nbanking, 282\ndetailed implementation bus matrix, 53\ndimensional modeling and, 439\nenterprise data warehouse bus matrix, 52\nhealthcare case study, 339–340\nHR (human resources), 268–269\ninsurance, 378–389\ndetailed implementation, 390\n\n\nIndex  \n546\ninventory, 113–119\nopportunity/stakeholder matrix, 127\norder management, 168\nprocurement, 142–147\ntelecommunications, 297–299\nuniversity, 325–326\nweb retailers, clickstream integration, \n368–370\nC\ncalculation lag, 196–197\ncalendar date dimensions, 48\ncalendars, country-speciﬁ c as outriggers, \n321–323\ncannibalization, 90\ncargo shipper schema, 317\ncase studies\naccounting, 202\nbudgeting, 210–213\nconsolidated fact tables, 224–225\nG/L (general ledger), 203–210\nhierarchies, 214–223\nOLAP and, 226\nairline, 311\ncalendars as outriggers, 321–323\nclass of service ﬂ own dimension, 319–320\ndestination airport dimension, 320–321\nfact table granularity, 312–316\norigin dimension, 320–321\npassenger dimension, 314\nsales channel dimension, 315\ntime zones, multiple, 323\nCRM (customer relationship management)\nanalytic, 231–233\nbridge tables, 245–248\ncomplex customer behavior, 249–251\ncustomer data integration, 256–260\ncustomer dimension and, 233–245\nfact tables, abnormal scenario indicators, \n255–256\nfact tables, satisfaction indicators, \n254–255\nfact tables, timespan, 252–254\nlow latency data, 260–261\noperational, 231–233\nstep dimension, sequential behavior, \n251–252\neducation, 325–326\naccumulating snapshot fact table, \n326–329\nadditional uses, 336\nadmissions events, 330\napplicant pipeline, 326–329\nattendance, 335\nchange tracking, 330\ncourse registrations, 330–333\nfacility use, 334\ninstructors, multiple,  333\nmetrics, artiﬁ cial count, 331–332\nresearch grant proposal, 329\nstudent dimensions, 330\nterm dimensions, 330\nelectronic commerce\nclickstream data, 353–370\nproﬁ tability, sales transactions and, \n370–372\nﬁ nancial services, 282, 287–295\ndimensions, household, 286–287\ndimensions, too few, 283–286\nhealthcare, 339–340\nbilling, 342–344\nclaims, 342–344\ndate dimension, 345\ndiagnosis dimension, 345–347\nEMRs (electronic medical records), \n341–348\nHCPCS (Healthcare Common Procedure \nCoding System), 342\nHIPAA (Health Insurance Portability and \nAccountability Act), 341\nICD (International Classiﬁ cation of \nDiseases), 342\nimages, 350\ninventory, 351\nmeasure type dimension, 349–350\npayments, 342–344\nretroactive changes, 351–352\nsubtypes, 347–348\nsupertypes, 347–348\ntext comments, 350\nHR (Human Resources Management)\nbus matrix, 268\nemployee hierarchies, 271–272\nemployee proﬁ les, 263–267\nhierarchies, 273–274\nmanagers key, 272–273\npackaged data models, 270–271\nperiodic snapshots, 267–268\nskill keywords, 274–277\nsurvey questionnaire, 277–278\ninsurance, 375–377\naccident events factless fact table, 396\naccumulating snapshot, 384–385\nbus matrix, 378, 389–390\nclaim transactions, 390–396\nconformed dimensions, 386\nconformed facts, 386\ndegenerate dimension, 383\ndimensions, 380\n\n\nIndex 547\ndimensions, audit, 383\ndimensions, low cardinality, 383\ndimensions, multivalued, 388\njunk dimensions, 392\nmini-dimensions, 381–382\nmultivalued dimensions, 382\nNAICS (North American Industry \nClassiﬁ cation System), 382\nnumeric attributes, 382\npay-in-advance facts, 386–387\nperiodic snapshot, 385\npolicy transaction fact table, 383\npolicy transactions, 379–380\npremiums, periodic snapshot, 386–388\nSCDs (slowly changing dimensions), \n380–381\nSIC (Standard Industry Classiﬁ cation), \n382\nsupertype/subtype products, 384, 387\ntimespan accumulating snapshot, 394\nvalue chain, 377–378\ninventory\naccumulating snapshot, 118–119\nfact tables, 115–116\nperiodic snapshot, 112–114\nsemi-additive facts, 114–115\ntransactions, 116–118\norder management, 167\naccumulating snapshots, 194–196\naudit dimension, 192–193\ncustomer dimension, 174–175\ndeal dimension, 177–179\nheader/line pattern, 186\nheader/line patterns, 181–182\ninvoice transactions, 187\njunk dimensions, 179–180\nlag calculations, 196\nmultiple currencies, 182–184\nproduct dimension, 172–173\nproﬁ t and loss facts, 189–191\ntransaction granularity, 184–186\ntransactions, 168–171\nunits of measure, multiple, 197–198\nprocurement, 141–142\nbus matrix, 142–143\ncomplementary procurement snapshot \nfact table, 147\ntransactions, 142–145\nretail sales, 72–73\nbusiness process selection, 74\ndimensions, selecting, 76\nfacts, 76–77\nfacts, derived, 77–78\nfacts, non-additive, 78\nfact tables, 79\nfrequent shopper program, 96\ngrain declaration, 74–75\nPOS schema, 94\nretail schema extensibility, 95–97\ntelecommunications, 297–299\ncausal dimension, 89–90, 284\nCDC (change data capture)\nETL system, 451\naudit columns, 452\ndiff  compare, 452\nlog scraping, 453\nmessage queue monitoring, 453\ntimed extracts, 452\ncentipede fact tables, 58, 108–109\nchange reasons, 266–267\nchange tracking, 147–148\neducation case study, 330\nHR (human resources) case study, \nembedded managers key, 272–273\nSCDs, 148\nchart of accounts (G/L), 203–204\nuniform chart of accounts, 204\ncheckpoints, data quality, 516\nCIF (Corporate Information Factory), 28–29\nCIO (chief information offi  cer), 377\nclaim transactions (insurance case study), \n390\nclaim accumulating snapshot, 393–394\njunk dimensions and, 392\nperiodic snapshot, 395–396\ntimespan accumulating snapshot, 394–395\nclass of service ﬂ own dimension (airline case \nstudy), 319–320\ncleaning and conforming, ETL systems, 450\naudit dimension assembler, 460\nconforming system, 461–463\ndata cleansing system, 456\nquality event responses, 458\nquality screens, 457–458\ndata quality improvement, 455–456\ndeduplication system, 460–461\nerror event schema, 458–460\nclickstream data, 353–354\ndimensional models, 357–358\naggregate fact tables, 366–367\ncustomer, 361–362\ndate, 361–362\nevent dimension, 359\nGA (Google Analytics), 367\npage dimension, 358–359\npage event fact table, 363–366\nreferral dimension, 360\nsession dimension, 359–360\nsession fact table, 361–363\nstep dimension, 366\ntime, 361–362\nsession IDs, 355–356\n\n\nIndex  \n548\nvisitor identiﬁ cation, 356–357\nvisitor origins, 354–355\nweb retailer bus matrix integration, \n368–370\ncollaborative design workshops, 38\ncolumn screens, 457\ncomments, survey questionnaire (HR), 278\ncommon dimensions, 130\ncompliance, ETL system, 445\ncompliance manager, ETL system, 493–495\ncomposite keys, 12\ncomputer resources, big data and, 537\nconformed dimensions, 51, 130, 304\nagile movement and, 137–138\ndrill across, 130–131\ngrain, 132\nidentical, 131–132\ninsurance case study, 386\nlimited conformity, 135\nshrunken on bus matrix, 134\nshrunken rollup dimensions, 132\nshrunken with row subset, 132–134\nconformed facts, 42, 139\ninsurance case study, 386\ninventory case study, 138–139\nconforming system, ETL system, 461–463\nconsistency\nadaptability, 4\ngoals, 3\nconsolidated fact tables, 45\naccounting case study, 224–225\ncontacts, bridge tables, 248\ncontribution amount (P&L statement), 191\ncorrectly weighted reports, 288\ncost, activity-based costing measures, 184\nCOUNT DISTINCT, 243\ncountry-speciﬁ c calendars as outriggers, \n321–323\ncourse registrations (education case study), \n330\nCRM (customer relationship management), \n229\nanalytic, 231–233\nbridge tables\ncustomer contacts, 248\nmultivalued, 245–246\nsparse attributes, 247\ncomplex customer behavior, 249–251\ncustomer data integration, 256\nmultiple customer dimension conformity, \n258–259\nsingle customer dimension, 256–258\ncustomer dimension and, 233\naddresses, 233–236\naddresses, international, 236–238\ncounts with Type 2, 243\ndates, 238\nfacts, aggregated, 239–240\nhierarchies, 244–245\nnames, 233–236\nnames, international, 236–238\noutriggers, low cardinality attribute set \nand, 243–244\nscores, 240–243\nsegmentation, 240–243\nfacts\nabnormal scenario indicators, 255–256\nsatisfaction indicators, 254–255\ntimespan, 252–254\nlow latency data, 260–261\noperational, 231–233\noverview, 230–231\nsocial media and, 230\nstep dimension, sequential behavior, \n251–252\ncurrency, multiple\nfact tables, 60\nG/L (general ledger), 206\norder transactions, 182–184\ncurrent date attributes, dimension tables, \n82–83\ncustomer contacts, bridge tables, 248\ncustomer dimension, 158, 174–175\nclickstream data, 361–362\nCRM and, 233\naddresses, 233–236\naddresses, international, 236–238\ncounts with Type 2, 243\ndates, 238\nfacts, aggregated, 239–240\nhierarchies, 244–245\nnames, 233–236\nnames, international, 236–238\noutriggers, low cardinality attribute set \nand, 243–244\nscores, 240–243\nsegmentation, 240–243\nfactless fact tables, 176\nhierarchies, 174–175\nmultiple, partial conformity, 258–259\nsingle, 256–258\nsingle versus multiple dimension tables, \n175–176\ncustomer matching, 257\ncustomer relationship management. case \nstudy. See CRM, 230\nD\ndata architect/modeler, 409\ndata bags, 530\ndatabase administrator, 409\n\n\nIndex 549\ndata cleansing system, ETL system, 456\nquality event responses, 458\nquality screens, 457–458\ndata compression, ETL system, 454\ndata governance, 135–136\nbig data best practices, 541\ndimensionalizing, 541\nprivacy, 541–542\nbusiness-driven governance, 136–137\nobjectives, 137\ndata handlers, late arriving, 478–479\ndata highway planning, 533–534\ndata integration\nconformed dimensions, 130–138\nCRM and, 256\nmultiple customer dimension conformity, \n258–259\nsingle customer dimension, 256–258\nETL system, 444–446\nMDM (master data management), 256\nstructure/unstructured data, 539\nvalue chain integration, 111–112\ndata latency, ETL system, 447\ndata mart, independent data mart \narchitecture, 26–27\ndata mining\nDW/BI system and, 242–243\nnull tracking, 92\ndata modeling, big data best practices\ndata structure declaration, 540\ndata virtualization, 540\ndimension anchoring, 539\nintegrating sources and conformed \ndimensions, 538\nname-value pairs, 540\nSCDs (slowly changing dimensions), 539\nstructured/unstructured data integration, \n539\nthinking dimensionally, 538\ndata models, packaged, 270–271\ndata proﬁ ling\nETL system, 450–451\ntools, 433\ndata propagation, ETL system, 482\ndata quality\ncheckpoints, 516\nETL system, 445\nimprovement, 455–456\nplanning, big data and, 535\ndata steward, 408\ndata structure, analysis time, 540\ndata value, big data and, 535\ndata virtualization, big data and, 540\ndata warehousing versus operational \nprocessing, 2\ndate dimension, 79–81, 284, 302\ncalendar date, 48\nclickstream data, 361–362\ncurrent date attributes, 82–83\nﬁ xed time series buckets and, 302–303\nhealthcare case study, 345\npopulating, 508\nrelative date attributes, 82–83\nrole playing, 171\nsmart keys, 101–102\ntextual attributes, 82\ntime-of-day, 83\ndates\nCRM and, customer dimension, 238\ndimension tables, 89\ntimespan fact tables, 252–254\ntransaction fact table, 170–171\nforeign key, 170\nrole playing, 171\ndate/time\nGMT (Greenwich Mean Time), 323\ntime zones, multiple, 323\nUTC (Coordinated Universal Time, 323\ndate/time dimensions, 470\ndate/time stamp dimensions, 284\ndeal dimensions, 177–178\ndecision-making goals, 4\ndecodes, dimensions, 303–304\ndecoding production codes, 504\ndeduplication system, 460–461\ndegenerate dimension, 47, 284, 303\ninsurance case study, 383\norder number, 178–179\nretail sales case study, 93–94\nsurrogate keys, 101\ntelecommunications case study, 303\ntransaction numbers, 93–94\ndemand planning, 142\ndemographics dimension, 291\nsize, 159\ndenormalized ﬂ attened dimensions, 47\ndependency analysis, 495\ndependency, ETL, 490–491\ndeployment\nLifecycle, 424\nOLAP, 9\nderived facts, 77–78\ndescriptions, dimensions, 303–304\ndescriptive context, dimensions for, 40\ndestination airport dimension (airline case \nstudy), 320–321\ndetailed implementation bus matrix, 53, 390\ndetailed table design documentation, \n437–439\ndiagnosis dimension (healthcare case study), \n345–347\n\n\nIndex  \n550\ndiff  compare, CDC (change data capture), \n452\ndimensional modeling, 7\n3NF (third normal form) models, 7–8\n4-step design process\nbusiness process, 70–71\ndimensions, 72\nfacts, 72\ngrain, 71\natomic grain data, 17\nbeneﬁ ts of thinking dimensionally, 32–33\nbusiness processes, 300\nbusiness representatives, 431–432\ncalendar coordination, 433–434\nclickstream data, 357–367\ndata proﬁ ling tools, 433\ndesign\nbubble chart, 435–436\ndetailed model development, 436–439\ndocumentation ﬁ nalization, 441\nvalidation, 440–441\ndimension tables, 13\nattributes, 13–14\nhierarchical relationships, 15\nsnowﬂ aking, 15\nextensibility, 16\nfacts\nadditive facts, 11\ncomposite keys, 12\nFK (foreign keys), 12\ngrains, 10\nnumeric facts, 11\ntextual facts, 12\nfact tables, 10–12\ngrain categories, 12\nfundamentals\nbusiness processes, 39\nbusiness requirement gathering, 37–38\ncollaborative workshops, 38\ndata realities gathering, 37–38\ndescriptive context, 40\nfacts, 40\nfour-step dimensional design process, 38\ngrain, 39\nmodel extensions, 41\nstar schemas, 40\nLifecycle data track, 420\nmistakes to avoid, 397–401\nmyths, 30\ndepartmental versus enterprise, 31\nintegration, 32\npredictable use, 31–32\nscalability, 31\nsummary data, 30\nnaming conventions, 433\nOLAP (online analytical processing) cube, 8\ndeployment considerations, 9\noverview, 429–131\nparticipant identiﬁ cation, 431–432\nreports, 17\nsimplicity in, 16\nsources, 300\nstar schemas, 8\nterminology, 15\ntools, 432\ndimensional thinking, big data and, 538\ndimension manager system, 479–480\ndimensions\nanchoring, big data and, 539\nattributes, 514\naggregated facts as, 64\nbridge tables, CRM and, 247\nchanges, 514\ndetailed dimension model, 437\nexpiration, 266\nﬂ ags, 48\nindicators, 48\nnull, 48, 92\nnumeric values as, 59\npathstring, ragged/variable depth \nhierarchies, 57\nproduct dimensions, 132\nSCD type 3 (add new attribute), 154–156\nSee also attributes, 48\naudit dimension, 66, 192–193, 284\nassembler, 460\ninsurance case study, 383\naverage number in model, 284\ncalendar date, 48\ncausal, 89–90, 284\nchange reasons, 266–267\nclass of service ﬂ own (airline case study), \n319–320\nconformed, 51, 130, 304\nagile movement and, 137–138\ndrill across, 130–131\ngrain, 132\nidentical, 131–132\ninsurance case study, 386\nlimited conformity, 135\nshrunken, bus matrix and, 134\nshrunken rollup dimensions, 132\nshrunken with row subset, 132–134\ncustomer dimension, 158, 174–175\nconformity, 258–259\nCRM and, 233–245\nfactless fact tables, 176\nhierarchies, 174–175\nsingle, 256–258\nsingle versus multiple dimension tables, \n175–176\ndata governance, big data and, 541\ndate dimension, 48, 284, 302\n",
      "page_number": 565
    },
    {
      "number": 62,
      "title": "Segment 62 (pages 573-581)",
      "start_page": 573,
      "end_page": 581,
      "detection_method": "topic_boundary",
      "content": "Index 551\nﬁ xed time series buckets and, 302–303\nhealthcare case study, 345\npopulating, 508\nrole playing, 171\ndate/time stamp, 284\ndeal dimension, 177–178\ndecodes, 303–304\ndegenerate, 47, 284, 303\norder number, 178–179\ndemographic, 291\nsize, 159\ndenormalized ﬂ attened, 47\ndescriptions, 303–304\ndestination airport (airline case study), \n320–321\ndetailed dimension model, 437\ndiagnosis (healthcare case study), 345–347\ndimensional design models, 72\ndrilling across, 51\nevent dimension, clickstream data, 359\ngeneric, abstract, 66\ngeographic location, 310\ngranularity, hierarchies and, 301–302\nhierarchies\nﬁ xed depth position hierarchies, 56\nragged/variable depth with hierarchy \nbridge tables, 57\nragged/variable depth with pathstring \nattributes, 57\nslightly ragged/variable depth, 57\nhot swappable, 66, 296\nhousehold, 286–287\ninsurance case study, 380\ndegenerate dimension, 383\nmini-dimensions, 381–382\nmultivalued dimensions, 382\nnumeric attributes, 382\nSCDs (slowly changing dimensions), \n380–381\njunk dimensions, 49, 179–180, 284\nkeys, natural, 162\nlate arriving, 67\nlow cardinality, insurance case study, 383\nmeasure type, 65\nhealthcare case study, 349–350\nmini-dimensions, 289–290\nbridge tables, 290–291\ninsurance case study, 381–382\ntype 5 SCD and, 160\nmultivalued\nbridge table builder, 477–478\nbridge tables and, 63\ninsurance case study, 382–388\nweighting, 287–289\norigin (airline case study), 320–321\noutrigger, 50\npage dimension, clickstream data, 358–359\npassenger (airline case study), 314\nproduct dimension\ncharacteristics, 172–173\noperational product master, 173\norder transactions, 172–173\nrapidly changing monster dimension, 55\nreferral dimension, clickstream data, 360\nretail sales case study, 76\nrole-playing, 284\nsales channel, airline case study, 315\nservice level performance, 188–189\nsession dimension, clickstream data, \n359–360\nshrunken, 51\nshrunken rollup, 132\nspecial dimensions manager, ETL systems, \n470\ndate/time dimensions, 470\njunk dimensions, 470\nmini-dimensions, 471\nshrunken subset, 472\nstatic, 472\nuser-maintained, 472–473\nstatic dimension, population, 508\nstatus, 284\nstep dimension, 65\nclickstream data, 366\nsequential behavior, 251–252\nstudent (education case study), 330\nterm (education case study), 330\ntext comments, 65\ntoo few, 283–286\ntransaction proﬁ le dimension, 49, 179\ntransformations\ncombine from separate sources, 504\ndecode production codes, 504\nrelationship validation, 504–505\nsimple data, 504\nsurrogate key assignment, 506\nvalue chain, 52\ndimension surrogate keys, 46\ndimension tables, 13\nattributes, 13–14\ncalendar date dimensions, 48\nchanged rows, 513–514\ndate dimension, 79–81\ncurrent date attributes, 82–83\nsmart keys, 101–102\ntextual attributes, 82\ntime-of-day, 83\ndates, 89\ndegenerate dimensions, 47\nsurrogate keys, 101\ntransaction numbers, 93–94\n\n\nIndex  \n552\ndenormalized ﬂ attened dimensions, 47\ndrilling down, 47\ndurable keys, 46\nextracts, 513\nfact tables, centipede, 108–109\nﬂ ags, 48, 82\nhierarchical relationships, 15\nhierarchies, multiple, 48, 88–89\nhistoric data population, 503–506\nholiday indicator, 82\nindicators, 48, 82\njunk dimensions, 49\nloading, 506–507\nloading history, 507–508\nnatural keys, 46, 98–101\nnew rows, 513–514\nnull attributes, 48\noutrigger dimensions, 50\noutriggers, 106–107\nproduct dimension, 83–84\nattributes with embedded meaning, 85\ndrilling down, 86–87\nmany-to-one hierarchies, 84–85\nnumeric values, 85–86\npromotion dimension, 89–91\nnull items, 92\nrole-playing, 49\nsnowﬂ aking, 15, 50, 104–106\nstore dimension, 87–89\nstructure, 46\nsupernatural keys, 46, 101\nsurrogate keys, 46, 98–100\ntransaction proﬁ le dimensions, 49\nweekday indicator, 82\ndimension terminology, 15\ndimension-to-dimension table joins, 62\ndocumentation\ndetailed table design, 437–439\ndimensional modeling, 441\nETL development, 502–503\nsandbox source system, 503\nLifecycle architecture requirements, 417\nLifecycle business requirements, 414\ndraft design\nexercise discussion, 306–308\nremodeling existing structures, 309\ndrill across, 51, 130–131\ndrill down, 47, 86–87\nETL development, 500\nhierarchies, 501\ntable schematics, 501\nG/L (general ledger) hierarchy, 209\nmanagement hierarchies, 273–274\ndual date/time stamps, 254\ndual type 1 and type 2 dimensions \n(SCD type 7), 56\nduplication, deduplication system, 460–461\ndurable keys, 46\nsupernatural keys, 101\nDW/BI, 1\nalternative architecture, 26–29\ndata mining and, 242–243\ngoals, 3\ninternational goals, 237–238\nKimball architecture, 18\nBI applications, 22\nETL (extract, transformation, and load) \nsystem, 19–21\nhybrid hub-and-spoke Kimball, 29\noperational source systems, 18\npresentation area, 21–22\nrestaurant metaphor, 23–26\npublishing metaphor for DW/BI managers, \n5–7\nsystem users, 2\ndynamic value bands, 64, 291\nE\necosystems, big data and, 534\ncase study, 325–326\neducation\naccumulating snapshot fact table, 326–329\nadditional uses, 336\nadmissions events, 330\napplicant pipeline, 326–329\nattendance, 335\nbus matrix, 325–326\nchange tracking, 330\ncourse registrations, 330–333\nfacility use, 334\ninstructors, multiple, 333\nmetrics, artiﬁ cial count, 331–332\nresearch grant proposal, 329\nstudent dimension, 330–332\nterm dimension, 330\neff ective date, SCD type 2, 152–153\nEHR (electronic health record), 341\nelectronic commerce case study, 353–372\nembedded managers key (HR), 272–273\nembedding attribute meaning, 85\nemployee hierarchies, recursive, 271–272\nemployee proﬁ les, 263–265\ndimension change reasons, 266–267\neff ective time, 265–266\nexpiration, 265–266\nfact events, 267\ntype 2 attributes, 267\nEMRs (electronic medical records), \nhealthcare case study, 341, 348\n\n\nIndex 553\nenterprise data warehouse bus architecture, \n22, 52, 123–125\nenterprise data warehouse bus matrix, 52, \n125–126\ncolumns, 126\nhierarchy levels, 129\ncommon mistakes, 128–129\nopportunity/stakeholder matrix, 127\nprocurement, 142–143\nretroﬁ tting existing models, 129–130\nrows\nnarrowly deﬁ ned, 128\noverly encompassing, 128\noverly generalized, 129\nshrunken conformed dimensions, 134\nuses, 126–127\nERDs (entity-relationship diagrams), 8\nerror event schema, ETL system, 458–460\nerror event schemas, 68\nETL (extract, transformation, and load) \nsystem, 19–21, 443\narchiving, 447–448\nBI, delivery, 448\nbusiness needs, 444\ncleaning and conforming, 450\naudit dimension assembler, 460\nconforming system, 461–463\ndata cleansing system, 456–458\ndata quality, improvement, 455–456\ndeduplication system, 460–461\nerror event schema, 458–460\ncompliance, 445\ndata integration, 446\ndata latency, 447\ndata propagation manager, 482\ndata quality, 445\ndelivering, 450, 463\naggregate builder, 481\ndimension manager system, 479–480\nfact provider system, 480–481\nfact table builders, 473–475\nhierarchy manager, 470\nlate arriving data handler, 478–479\nmultivalued dimension bridge table \nbuilder, 477–478\nSCD manager, 464–468\nspecial dimensions manager, 470–473\nsurrogate key generator, 469–470\nsurrogate key pipeline, 475–477\ndesign, 443\nLifecycle data track, 422\ndeveloper, 409\ndevelopment, 498\nactivities, 500\naggregate tables, 519\ndefault strategies, 500\ndrill down, 500–501\nhigh-level plan, 498\nincremental processing, 512–519\nOLAP loads, 519\none-time historic load data, 503–512\nspeciﬁ cation document, 502–503\nsystem operation and automation, 520\ntools, 499\nETL architect/designer, 409\nextracting, 450\nCDC (change data capture), 451–453\ndata proﬁ ling, 450–451\nextract system, 453–455\nlegacy licenses, 449\nlineage, 447–448\nmanaging, 450, 483\nbackup system, 485–495\njob scheduler, 483–484\nOLAP cube builder, 481–482\nprocess overview, 497\nsecurity, 446\nskills, 448\nsubsystems, 449\nevent dimension, clickstream data, 359\nexpiration date, type 2 SCD, 152–153\nextended allowance amount (P&L \nstatement), 190\nextended discount amount (P&L statement), \n190\nextended distribution cost (P&L statement), \n191\nextended ﬁ xed manufacturing cost (P&L \nstatement), 190\nextended gross amount (P&L statement), \n189\nextended net amount (P&L statement), 190\nextended storage cost (P&L statement), 191\nextended variable manufacturing cost (P&L \nstatement), 190\nextensibility in dimensional modeling, 16\nextracting, ETL systems, 450\nCDC (change data capture), 451\naudit columns, 452\ndiff  compare, 452\nlog scraping, 453\nmessage queue monitoring, 453\ntimed extracts, 452\ndata proﬁ ling, 450–451\nextract system, 453–455\nextraction, 19\nextract system, ETL system, 453–455\nF\nfact extractors, 530\nbig data and, 534\n\n\nIndex  \n554\nfactless fact tables, 44, 97–98, 176\naccidents (insurance case study), 396\nadmissions (education case study), 330\nattendance (education case study), 335\ncourse registration (education case study), \n330–333\nfacility use (education case study), 334\norder management case study, 176\nfact provider system\nETL system, 480–481\nfacts, 10, 12, 72, 79\nabnormal scenario indicators, 255–256\naccumulating snapshots, 44, 121–122, \n326–329\nadditive facts, 11, 42\naggregate, 45\nas attributes, 64\nclickstream data, 366–367\nCRM and customer dimension, 239–240\nallocated facts, 60\nallocating, 184–186\nbehavior tags, 241\nbudget, 210\nbuilders, ETL systems, 473–475\ncentipede, 58, 108–109\ncompliance-enabled, 494\ncomposite keys, 12\nconformed, 42, 138–139\nconsolidated, 45\ncurrency, multiple, 60\nderived, 77–78\ndetailed dimension model, 437\ndimensional modeling process and, 40\ndrill across, 130–131\nemployee proﬁ les, 267\nenhanced, 115–116\nFK (foreign keys), 12\ngrains, 10, 12\ngranularity, airline bus matrix, 312–315\nheader/line fact tables, 59\nhistoric, 508\nincremental processing, 515, 519\ninvoice, 187–188\njoins, avoiding, 259–260\nlag/duration facts, 59\nlate arriving, 62\nloading, 512\nmini-dimension demographics key, 158\nmultiple units of measure, 61\nnon-additive, 42, 78\nnormalization, order transactions, 169–170\nnull, 42, 92\nnumeric facts, 11\nnumeric values, 59, 85–86\npage event, clickstream data, 363–366\npartitioning, smart keys, 102\npay-in-advance, insurance case study, \n386–387\nperiodic snapshots, 43, 120–122\npolicy transactions (insurance case study), \n383\nproﬁ tability, 370–372\nproﬁ t and loss, 189–192\nproﬁ t and loss, allocations and, 60\nreal-time, 68\nreferential integrity, 12\nreports, 17\nretail sales case study, identifying, 76–79\nsatisfaction indicators, 254–255\nsemi-additive, 42, 114–115\nservice level performance, 188–189\nsession, clickstream data, 361–363\nset diff erence, 97\nshrunken rollup dimensions, 132\nsingle granularity and, 301\nsnapshot, complementary procurement, \n147\nstructure, 41–42\nsubtype, 67, 293–295\nsupertype, 67, 293–295\nsurrogate keys, 58, 102–103\ntextual facts, 12\nterminology, 15\ntime-of-day, 83\ntimespan, 252–254\ntimespan tracking, 62\ntransactions, 43, 120\ndates, 170–171\nsingle versus multiple, 143–145\ntransformations, 509–512\nvalue banding, 291–292\nyear-to-date, 206\nYTD (year-to-date), 61\nfact-to-fact joins, avoiding with multipass \nSQL, 61\nfeasibility in Lifecycle planning, 407\nﬁ nancial services case study, 281\nbus matrix, 282\ndimensions\nhot-swappable, 296\nhousehold, 286–287\nmini-dimensions, 289–291\nmultivalued, weighting, 287–289\ntoo few, 283–286\nfacts, value banding, 291–292\nheterogeneous products, 293–295\nOLAP, 226\nuser perspective, 293\nﬁ nancial statements (G/L), 209–210\nﬁ scal calendar, G/L (general ledger), 208\nﬁ xed depth position hierarchies, 56, 214\nﬁ xed time series buckets, date dimensions \nand, 302–303\n\n\nIndex 555\nFK (foreign keys). See foreign keys (FK), 12\nﬂ ags\nas textual attributes, 48\ndimension tables, 82\njunk dimensions and, 179–180\nﬂ attened dimensions, denormalized, 47\nﬂ exible access to information, 407\nforeign keys (FK)\ndemographics dimensions, 291\nfact tables, 12\nmanagers employee key as, 271–272\nmini-dimension keys, 158\nnull, 92\norder transactions, 170\nreferential integrity, 12\nforum, Lifecycle business requirements, \n410–411\nfrequent shopper program, retail sales \nschema, 96\nFROM clause, 18\nG\nGA (Google Analytics), 367\ngeneral ledger. See G/L (general ledger), 203\ngeneric dimensions, abstract, 66\ngeographic location dimension, 310\nG/L (general ledger), 203\nchart of accounts, 203–204\ncurrencies, multiple, 206\nﬁ nancial statements, 209–210\nﬁ scal calendar, multiple, 208\nhierarchies, drill down, 209\njournal entries, 206–207\nperiod close, 204–206\nperiodic snapshot, 203\nyear-to-date facts, 206\nGMT (Greenwich Mean Time), 323\ngoals of DW/BI, 3–4\nGoogle Analytics (GA), 367\ngovernance\nbusiness-driven, 136–137\nobjectives, 137\ngrain, 39\naccumulating snapshots, 44\natomic grain data, 74\nbudget fact table, 210\nconformed dimensions, 132\ndeclaration, 71\nretail sales case study, 74–75\ndimensions, hierarchies and, 301–302\nfact tables, 10\naccumulating snapshot, 12\nperiodic snapshot, 12\ntransaction, 12\nperiodic snapshots, 43\nsingle, facts and, 301\ntransaction fact tables, 43\ngranularity, 300\nGROUP BY clause, 18\ngrowth\nLifecycle, 425–426\nmarket growth, 90\nH\nHadoop, MapReduce/Hadoop, 530\nHCPCS (Healthcare Common Procedure \nCoding System), 342\nHDFS (Hadoop distributed ﬁ le system), 530\nheadcount periodic snapshot, 267–268\nheader/line fact tables, 59\nheader/line patterns, 181–182, 186\nhealthcare case study, 339–340\nbilling, 342–344\nclaims, 342–344\ndate dimension, 345\ndiagnosis dimension, 345–347\nEMRs (electronic medical records), 341, \n348\nHCPCS (Healthcare Common Procedure \nCoding System), 342\nHIPAA (Health Insurance Portability and \nAccountability Act), 341\nICD (International Classiﬁ cation of \nDiseases), 342\nimages, 350\ninventory, 351\nmeasure type dimension, 349–350\npayments, 342–344\nretroactive changes, 351–352\nsubtypes, 347–348\nsupertypes, 347–348\ntext comments, 350\nheterogeneous products, 293–295\nhierarchies\naccounting case study, 214–223\ncustomer dimension, 174–175, 244–245\ndimension granularity, 301–302\ndimension tables, multiple, 88–89\ndrill down, ETL development, 501\nemployees, 271–272\nETL systems, 470\nﬁ xed-depth positional hierarchies, 56\nG/L (general ledger), drill down, 209\nmanagement, drilling up/down, 273–274\nmany-to-one, 84–85\nmatrix columns, 129\nmultiple, 48\nnodes, 215\n\n\nIndex  \n556\nragged/variable depth, 57\nslightly ragged/variable depth, 57\ntrees, 215–216\nhigh performance backup, 485\nHIPAA (Health Insurance Portability and \nAccountability Act), 341\nhistoric fact tables\nextracts, 508\nstatistics audit, 508\nhistoric load data, ETL development, \n503–512\ndimension table population, 503–506\nholiday indicator, 82\nhot response cache, 238\nhot swappable dimensions, 66, 296\nhousehold dimension, 286–287\nHR (human resources) case study, 263\nbus matrix, 268–269\nemployee proﬁ les, 263–265\ndimension change reasons, 266–267\neff ective time, 265–266\nexpiration, 265–266\nfact events, 267\ntype 2 attributes, 267\nhierarchies\nmanagement, 273–274\nrecursive, 271–272\nmanagers key\nas foreign key, 271–272\nembedded, 272–273\npackaged analytic solutions, 270–271\npackaged data models, 270–271\nperiodic snapshots, headcount, 267–268\nskill keywords, 274\nbridge, 275\ntext string, 276–277\nsurvey questionnaire, 277\ntext comments, 278\nHTTP (Hyper Text Transfer Protocol), \n355–356\nhub-and-spoke CIF architecture, 28–29\nhub-and-spoke Kimball hybrid architecture, \n29\nhuman resources management case study. \nSee HR (human resources), 263\nhybrid hub-and-spoke Kimball architecture, \n29\nhybrid techniques, SCDs, 159, 164\nSCD type 5 (add mini-dimension and type \n1 outrigger), 55, 160\nSCD type 6 (add type 1 attributes to type 2 \ndimension), 56, 160–162\nSCD type 7 (dual type 1 and type 2 \ndimension), 56, 162–163\nhyperstructured data, 530\nI\nICD (International Classiﬁ cation of \nDiseases), 342\nidentical conformed dimensions, 131–132\nimages, healthcare case study, 350\nimpact reports, 288\nincremental processing, ETL system \ndevelopment, 512\nchanged dimension rows, 513–514\ndimension attribute changes, 514\ndimension table extracts, 513\nfact tables, 515–519\nnew dimension rows, 513–514\nin-database analytics, big data and, 537\nindependent data mart architecture, 26–27\nindicators\nabnormal, fact tables, 255–256\nas textual attributes, 48\ndimension tables, 82\njunk dimensions and, 179–180\nsatisfaction, fact tables, 254–255\nInmon, Bill, 28–29\ninsurance case study, 375–377\naccidents, factless fact tables, 396\naccumulating snapshot, complementary \npolicy, 384–385\nbus matrix, 378–389\ndetailed implementation, 390\nclaim transactions, 390\nclaim accumulating snapshot, 393–394\njunk dimensions and, 392\nperiodic snapshot, 395–396\ntimespan accumulating snapshot, \n394–395\nconformed dimensions, 386\nconformed facts, 386\ndimensions, 380\naudit, 383\ndegenerate, 383\nlow cardinality, 383\nmini-dimensions, 381–382\nmultivalued, 382, 388\nSCDs (slowly changing dimensions), \n380–381\nNAICS (North American Industry \nClassiﬁ cation System), 382\nnumeric attributes, 382\npay-in-advance facts, 386–387\nperiodic snapshot, 385\npolicy transactions, 379–380, 383\npremiums, periodic snapshot, 386–388\nSIC (Standard Industry Classiﬁ cation), 382\nsupertype/subtype products, 384, 387\nvalue chain, 377–378\ninteger keys, 98\nsequential surrogate keys, 101\n\n\nIndex 557\nintegration\nconformed dimensions, 130–138\ncustomer data, 256\ncustomer dimension conformity, 258–259\nsingle customer dimension, 256, 257, 258\ndimensional modeling myths, 32\nvalue chain, 122–123\ninternational names/addresses, customer \ndimension, 236–238\ninterviews, Lifecycle business requirements, \n412–413\ndata-centric, 413–414\ninventory case study, 112–114\naccumulating snapshot, 118–119\nfact tables, enhanced, 115–116\nperiodic snapshot, 112–114\nsemi-additive facts, 114–115\ntransactions, 116–118\ninventory, healthcare case study, 351\ninvoice transaction fact table, 187–188\nJ\njob scheduler, ETL systems, 483–484\njob scheduling, ETL operation and \nautomation, 520\njoins\ndimension-to-dimension table joins, 62\nfact tables, avoiding, 259–260\nmany-to-one-to-many, 259–260\nmultipass SQL to avoid fact-to-fact joins, 61\njournal entries (G/L), 206–207\njunk dimensions, 49, 179–180, 284\nairline case study, 320\nETL systems, 470\ninsurance case study, 392\norder management case study, 179–180\njustiﬁ cation for program/project planning, \n407\nK\nkeys\ndimension surrogate keys, 46\ndurable, 46\nforeign, 92, 291\nmanagers key (HR), 272–273\nnatural keys, 46, 98–101, 162\nsupernatural keys, 101\nsmart keys, 101–102\nsubtype tables, 294–295\nsupernatural, 46\nsupertype tables, 294–295\nsurrogate, 58, 98–100, 303\nassigning, 506\ndegenerate dimensions, 101\nETL system, 475–477\nfact tables, 102–103\ngenerator, 469–470\nlookup pipelining, 510–511\nkeywords, skill keywords, 274\nbridge, 275\ntext string, 276–277\nKimball Dimensional Modeling Techniques. \nSee dimensional modeling\nKimball DW/BI architecture, 18\nBI applications, 22\nETL (extract, transformation, and load) \nsystem, 19–21\nhub-and-spoke hybrid, 29\npresentation area, 21–22\nrestaurant metaphor, 23–26\nsource systems, operational source systems, \n18\nKimball Lifecycle, 404\nDW/BI initiative and, 404\nKPIs (key performance indicators), 139\nL\nlag calculations, 196–197\nlag/duration facts, 59\nlate arriving data handler, ETL system, \n478–479\nlate arriving dimensions, 67\nlate arriving facts, 62\nlaunch, Lifecycle business requirements, 412\nLaw of Too, 407\nlegacy environments, big data management, \n532\nlegacy licenses, ETL system, 449\nLifecycle\nBI applications, 406\ndevelopment, 423–424\nspeciﬁ cation, 423\nbusiness requirements, 405, 410\ndocumentation, 414\nforum selection, 410–411\ninterviews, 412–413\ninterviews, data-centric, 413–414\nlaunch, 412\nprioritization, 414–415\nrepresentatives, 411–412\nteam, 411\ndata, 405\ndimensional modeling, 420\nETL design/development, 422\nphysical design, 420–422\ndeployment, 424\ngrowth, 425–426\nmaintenance, 425–426\npitfalls, 426\n\n\nIndex  \n558\nproducts\nevaluation matrix, 419\nmarket research, 419\nprotoypes, 419\nprogram/project planning, 405–406\nbusiness motivation, 407\nbusiness sponsor, 406\ndevelopment, 409–410\nfeasibility, 407\njustiﬁ cation, 407\nplanning, 409–410\nreadiness assessment, 406–407\nscoping, 407\nstaffi  ng, 408–409\ntechnical architecture, 405, 416–417\nimplementation phases, 418\nmodel creation, 417\nplan creation, 418\nrequirements, 417\nrequirements collection, 417\nsubsystems, 418\ntask force, 417\nlift, promotion, 89\nlights-out operations, backup, 485\nlimited conformed dimensions, 135\nlineage analysis, 495\nlineage, ETL system, 447–448, 490–491\nloading fact tables, incremental, 517\nlocalization, 237, 324\nlocation, geographic location dimension, 310\nlog scraping, CDC (change data capture), \n453\nlow cardinality dimensions, insurance case \nstudy, 383\nlow latency data, CRM and, 260–261\nM\nmaintenance, Lifecycle, 425–426\nmanagement\nETL systems, 450, 483\nbackup system, 485–495\njob scheduler, 483–484\nmanagement best practices, big data\nanalytics, 531\nlegacy environments, 532\nsandbox results, 532–533\nsunsetting and, 533\nmanagement hierarchies, drilling up/down, \n273–274\nmanagers, publishing metaphor, 5–7\nmany-to-one hierarchies, 84–85\nmany-to-one relationships, 175–176\nmany-to-one-to-many joins, 259–260\nMapReduce/Hadoop, 530\nmarket growth, 90\nmaster dimensions, 130\nMDM (master data management), 137, 256, \n446\nmeaningless keys, 98\nmeasurement, multiple, 61\nmeasure type dimension, 65\nhealthcare case study, 349–350\nmessage queue monitoring, CDC (change \ndata capture), 453\nmetadata coordinator, 409\nmetadata repository, ETL system, 495\nmigration, version migration system, ETL, \n488\nmilestones, accumulating snapshots, 121\nmini-dimension and type 1 outrigger (SCD \ntype 5), 160\nmini-dimensions, 289–290\nbridge tables, 290–291\nETL systems, 471\ninsurance case study, 381–382\ntype 4 SCD, 156–159\nmodeling\nbeneﬁ ts of thinking dimensionally, 32–33\ndimensional, 7–12\natomic grain data, 17\ndimension tables, 13–15\nextensibility, 16\nmyths, 30–32\nreports, 17\nsimplicity in, 16\nterminology, 15\nmultipass SQL, avoiding fact-to-fact table \njoins, 61\nmultiple customer dimension, partial \nconformity, 258–259\nmultiple units of measure, 61, 197–198\nmultivalued bridge tables\nCRM and, 245–246\ntime varying, 63\nmultivalued dimensions\nbridge table builder, 477–478\nbridge tables and, 63\nCRM and, 245–247\neducation case study, 325–333\nﬁ nancial services case study, 287–289\nhealthcare case study, 345–348\nHR (human resources) case study, 274–275\ninsurance case study, 382–388\nweighting factors, 287–289\nmyths about dimensional modeling, 30\ndepartmental versus enterprise, 31\nintegration, 32\npredictable use, 31–32\nscalability, 31\nsummary data, 30\n\n\nIndex 559\nN\nnames\nASCII, 236\nCRM and, customer dimension, 233–238\nUnicode, 236–238\nname-value pairs, 540\nnaming conventions, 433\nnatural keys, 46, 98–101, 162\nsupernatural keys, 101\nNCOA (national change of address), 257\nnodes (hierarchies), 215\nnon-additive facts, 42, 78\nnon-natural keys, 98\nnormalization, 28, 301\nfacts\ncentipede, 108–109\norder transactions, 169–170\noutriggers, 106–107\nsnowﬂ aking, 104–106\nnormalized 3NF structures, 8\nnull attributes, 48\nnull fact values, 509\nnull values\nfact tables, 42\nforeign keys, 92\nnumber attributes, insurance case study, 382\nnumeric facts, 11\nnumeric values\nas attributes, 59, 85–86\nas facts, 59, 85–86\nO\noff -invoice allowance (P&L) statement, 190\nOLAP (online analytical processing) cube, \n8, 40\naccounting case study, 226\naccumulating snapshots, 121–122\naggregate, 45\ncube builder, ETL system, 481–482\ndeployment considerations, 9\nemployee data queries, 273\nﬁ nancial schemas, 226\nLifecycle data physical design, 421\nloads, ETL system, 519\nwhat didn’t happen, 335\none-to-one relationships, 175–176\noperational processing versus data \nwarehousing, 2\noperational product master, product \ndimensions, 173\noperational source systems, 18\noperational system users, 2\nopportunity/stakeholder matrix, 53, 127\norder management case study, 167–168\naccumulating snapshot, 194–196\ntype 2 dimensions and, 196\nallocating, 184–186\naudit dimension, 192–193\nbus matrix, 168\ncurrency, multiple, 182–184\ncustomer dimension, 174–175\nfactless fact tables, 176\nsingle versus multiple dimension tables, \n175–176\ndate, 170–171\nforeign keys, 170\nrole playing, 171\ndeal dimension, 177–178\ndegenerate dimension, order number and, \n178–179\nfact normalization, 169–170\nheader/line patterns, 181–186\njunk dimensions, 179–180\nproduct dimension, 172–173\norder number, degenerate dimensions, \n178–179\norder management case study, role playing, \n171\norigin dimension (airline case study), \n320–321\nOR, skill keywords bridge, 275\noutrigger dimensions, 50, 89, 106–107\ncalendars as, 321–323\nlow cardinality attribute set and, 243–244\ntype 5 and type 1 SCD, 160\noverwrite (type 1 SCD), 54, 149–150\nadd to type 2 attribute, 160–162\ntype 2 in same dimension, 153\nP\npackaged analytic solutions, 270–271\npackaged data models, 270–271\npage dimension, clickstream data, 358–359\npage event fact table, clickstream data, \n363–366\nparallelizing/pipelining system, 492\nparallel processing, fact tables, 518\nparallel structures, fact tables, 519\nparent/child schemas, 59\nparent/child tree structure hierarchy, 216\npartitioning\nfact tables, smart keys, 102\nreal-time processing, 524–525\npassenger dimension, airline case study, 314\npathstring, ragged/variable depth hierarches, \n57\npay-in-advance facts, insurance case study, \n386–387\npayment method, retail sales, 93\n",
      "page_number": 573
    },
    {
      "number": 63,
      "title": "Segment 63 (pages 582-589)",
      "start_page": 582,
      "end_page": 589,
      "detection_method": "topic_boundary",
      "content": "Index  \n560\nperformance measurement, fact tables, 10, 12\nadditive facts, 11\ngrains, 10–12\nnumeric facts, 11\ntextual facts, 12\nperiod close (G/L), 204–206\nperiodic snapshots, 43, 112–114\neducation case study, 329, 333\nETL systems, 474\nfact tables, 120–121\ncomplementary fact tables, 122\nG/L (general ledger), 203\ngrain fact tables, 12\nheadcount, 267–268\nhealthcare case study, 342\ninsurance case study, 385\nclaims, 395–396\npremiums, 386–387\ninventory case study, 112–114\nprocurement case study, 147\nperspectives of business users, 293\nphysical design, Lifecycle data track, 420\naggregations, 421\ndatabase model, 421\ndatabase standards, 420\nindex plan, 421\nnaming standards, 420–421\nOLAP database, 421\nstorage, 422\npipelining system, 492\nplanning, demand planning, 142\nP&L (proﬁ t and loss) statement\ncontribution, 189–191\ngranularity, 191–192\npolicy transactions (insurance case study), \n379–380\nfact table, 383\nPO (purchase orders), 142\nPOS (point-of-sale) system, 73\nPOS schema, retail sales case study, 94\ntransaction numbers, 93–94\npresentation area, 21–22\nprioritization, Lifecycle business \nrequirements, 414–415\nprivacy, data governance and, 541–542\nproblem escalation system, 491–492\nprocurement case study, 141–142\nbus matrix, 142–143\nsnapshot fact table, 147\ntransactions, 142–145\nproduct dimension, 83–84\nattributes with embedded meaning, 85\ncharacteristics, 172–173\ndrilling down, 86–87\nmany-to-one hierarchies, 84–85\nnumeric values, 85–86\noperational product master, 173\norder transactions, 172–173\noperational product master, 173\nproduction codes, decoding, 504\nproducts\nheterogeneous, 293–295\nLifecycle\nevaluation matrix, 419\nmarket research, 419\nprototypes, 419\nproﬁ t and loss facts, 189–191, 370–372\nallocations and, 60\ngranularity, 191–192\nprogram/project planning (Lifecycle), \n405–406\nbusiness motivation, 407\nbusiness sponsor, 406\ndevelopment, 409–410\nfeasibility, 407\njustiﬁ cation, 407\nplanning, 409–410\nreadiness assessment, 406–407\nscoping, 407\nstaffi  ng, 408–409\ntask list, 409\nproject manager, 409\npromotion dimension, 89–91\nnull values, 92\npromotion lift, 89\nprototypes\nbig data and, 536\nLifecycle, 419\npublishing metaphor for DW/BI managers, \n5–7\nQ\nquality events, responses, 458\nquality screens, ETL systems, 457–458\nquestionnaire, HR (human resources), 277\ntext comments, 278\nR\nragged hierarchies\nalternative modeling approaches, 221–223\nbridge table approach, 223\nmodifying, 220–221\npathstring attributes, 57\nshared ownership, 219\ntime varying, 220\nvariable depth, 215–217\nrapidly changing monster dimension, 55\n\n\nIndex 561\nRDBMS (relational database management \nsystem), 40\narchitecture extension, 529–530\nblobs, 530\nfact extractor, 530\nhyperstructured data, 530\nreal-time fact tables, 68\nreal-time processing, 520–522\narchitecture, 522–524\npartitions, 524–525\nrearview mirror metrics, 198\nrecovery and restart system, ETL system, \n486–488\nrecursive hierarchies, employees, 271–272\nreference dimensions, 130\nreferential integrity, 12\nreferral dimension, clickstream data, 360\nrelationships\ndimension tables, 15\nmany-to-one, 175–176\nmany-to-one-to-many joins, 259–260\none-to-one, 175–176\nvalidation, 504–505\nrelative date attributes, 82–83\nremodeling existing data structures, 309\nreports\ncorrectly weighted, 288\ndimensional models, 17\ndynamic value banding, 64\nfact tables, 17\nimpact, 288\nvalue band reporting, 291–292\nrequirements for dimensional modeling, 432\nrestaurant metaphor for Kimball architecture, \n23–26\nretail sales case study, 72–73, 92\nbusiness process selection, 74\ndimensions, selecting, 76\nfacts, 76–77\nderived, 77–78\nnon-additive, 78\nfact tables, 79\nfrequent shopper program, 96\ngrain declaration, 74–75\npayment method, 93\nPOS (point-of-sale) system, 73\nPOS schema, 94\nretail schema extensibility, 95–97\nSKUs, 73\nretain original (SCD type 0), 54, 148–149\nretrieval, 485–486\nretroactive changes, healthcare case study, \n351–352\nreviewing dimensional model, 440, 441\nRFI measures, 240\nRFP (request for proposal), 419\nrole playing, dimensions, 49, 89, 171, 284\nairline case study, 313\nbus matrix and, 171\nhealthcare case study, 345\ninsurance case study, 380\norder management case study, 170\nS\nsales channel dimension, airline case study, \n315\nsales reps, factless fact tables, 176\nsales transactions, web proﬁ tability and, \n370–372\nsandbox results, big data management, \n532–533\nsandbox source system, ETL development, \n503\nsatisfaction indicators in fact tables, 254–255\nscalability, dimensional modeling myths, 31\nSCDs (slowly changing dimensions), 53, 148, \n464–465\nbig data and, 539\ndetailed dimension model, 437\nhybrid techniques, 159–164\ninsurance case study, 380–381\ntype 0 (retain original), 54, 148–149\ntype 1 (overwrite), 54, 149–150\nETL systems, 465\ntype 2 in same dimension, 153\ntype 2 (add new row), 54, 150–152\naccumulating snapshots, 196\ncustomer counts, 243\neff ective date, 152–153\nETL systems, 465–466\nexpiration date, 152–153\ntype 1 in same dimension, 153\ntype 3 (add new attribute), 55, 154–155\nETL systems, 467\nmultiple, 156\ntype 4 (add mini-dimension), 55, 156–159\nETL systems, 467\ntype 5 (add mini-dimension and type 1 \noutrigger), 55, 160\nETL systems, 468\ntype 6 (add type 1 attributes to type 2 \ndimension), 56, 160–162\nETL systems, 468\ntype 7 (dual type 1 and type 2 dimension), \n56, 162–164\nETL systems, 468\nscheduling jobs, ETL operation and \nautomation, 520\nscoping for program/project planning, 407\n\n\nIndex  \n562\nscoring, CRM and customer dimension, \n240–243\nscreening\nETL systems\nbusiness rule screens, 458\ncolumn screens, 457\nstructure screens, 457\nquality screens, 457–458\nsecurity, 495\nETL system, 446, 492–493\ngoals, 4\nsegmentation, CRM and customer dimension, \n240–243\nsegments, airline bus matrix granularity, 313\nlinking to trips, 315–316\nSELECT statement, 18\nsemi-additive facts, 42, 114–115\nsequential behavior, step dimension, 65, \n251–252\nsequential integers, surrogate keys, 101\nservice level performance, 188–189\nsession dimension, clickstream data, 359–360\nsession fact table, clickstream data, 361–363\nsession IDs, clickstream data, 355–356\nset diff erence, 97\nshared dimensions, 130\nshipment invoice fact table, 188\nshrunken dimensions, 51\nconformed\nattribute subset, 132\non bus matrix, 134\nrow subsets and, 132–134\nrollup, 132\nsubsets, ETL systems, 472\nsimple administration backup, 485\nsimple data transformation, dimensions, 504\nsingle customer dimension, data integration \nand, 256–258\nsingle granularity, facts and, 301\nsingle version of the truth, 407\nskill keywords, 274\nbridge, 275\nAND queries, 275\nOR queries, 275\ntext string, 276–277\nskills, ETL system, 448\nSKUs (stock keeping units), 73\nslightly ragged/variable depth hierarchies, 57\nslowly changing dimensions. See SCDs, 148\nsmart keys\ndate dimensions, 101–102\nfact tables, partitioning, 102\nsnapshots\naccumulating, 44, 118–119, 194–196\nclaims (insurance case study), 393–395\neducation case study, 326\nETL systems, 475\nfact tables, 121–122, 326–329\nfact tables, complementary, 122\nhealthcare case study, 343\ninventory case study, 118–119\norder management case study, 194–196\nprocurement case study, 147\ntype 2 dimensions and, 196\nincremental processing, 517\nperiodic, 43\neducation case study, 329\nETL systems, 474\nfact tables, 120–121\nfact tables, complementary, 122\nG/L (general ledger), 203\nheadcounts, 267–268\ninsurance case study, 385, 395–396\ninventory case study, 112–114\npremiums (insurance case study), \n386–388\nsnowﬂ aking, 15, 50, 104–106, 470\noutriggers, 106–107\nsocial media, CRM (customer relationship \nmanagement) and, 230\nsorting\nETL, 490\ninternational information, 237\nsource systems, operational, 18\nspecial dimensions manager, ETL systems, \n470\ndate/time dimensions, 470\njunk dimensions, 470\nmini-dimensions, 471\nshrunken subset, 472\nstatic, 472\nuser-maintained, 472–473\nspeciﬁ cation document, ETL development, \n502–503\nsandbox source system, 503\nSQL multipass to avoid fact-to-fact table \njoins, 61\nstaffi  ng for program/project planning, \n408–409\nstar joins, 16\nstar schemas, 8, 40\nstatic dimensions\nETL systems, 472\npopulation, 508\nstatistics, historic fact table audit, 508\nstatus dimensions, 284\nstep dimension, 65\nclickstream data, 366\nsequential behavior, 251–252\nstewardship, 135–136\n\n\nIndex 563\nstorage, Lifecycle data, 422\nstore dimension, 87–89\nstrategic business initiatives, 70\nstreaming data, big data and, 536\nstrings, skill keywords, 276–277\nstructure screens, 457\nstudent dimension (education case study), \n330\nstudy groups, behavior, 64\nsubsets, shrunken subset dimensions, 472\nsubtypes, 293–294\nfact tables\nkeys, 294–295\nsupertype common facts, 295\nhealthcare case study, 347–348\ninsurance case study, 384, 387\nschemas, 67\nsummary data, dimensional modeling and, 30\nsunsetting, big data management, 533\nsupernatural keys, 46, 101\nsupertypes\nfact tables, 293–294\nkeys, 294–295\nsubtype common facts, 295\nhealthcare case study, 347–348\ninsurance case study, 384–387\nschemas, 67\nsurrogate keys, 58, 98–100, 303\nassignment, 506\ndegenerate dimensions, 101\ndimension tables, 98–100\nETL system, 475–477\ngenerator, 469–470\nfact tables, 102–103\nfact table transformations, 516\nlate arriving facts, 517\nlookup pipelining, 510–511\nsurvey questionnaire (HR), 277\ntext comments, 278\nsynthetic keys, 98\nT\ntags, behavior, in time series, 63\nteam building, Lifecycle business \nrequirements, 411\nrepresentatives, 411–412\ntechnical application design/development \n(Lifecycle), 406\ntechnical architect, 409\ntechnical architecture (Lifecycle), 405, \n416–417\narchitecture implementation phases, 418\nmodel creation, 417\nplan creation, 418\nrequirements\ncollection, 417\ndocumentation, 417\nrequirements collection, 417\nsubsystems, 418\ntask force, 417\ntelecommunications case study, 297–299\nterm dimension (education case study), 330\ntext comments\ndimensions, 65\nhealthcare case study, 350\ntext strings, skill keywords, 276–277\ntext, survey questionnaire (HR) comments, \n278\ntextual attributes, dimension tables, 82\ntextual facts, 12\nThe Data Warehouse Toolkit (Kimball), 2, 80\nthird normal form (3NF) models, 7\nentity-relationship diagrams (ERDs), 8\nnormalized 3NF structures, 8\ntime\nGMT (Greenwich Mean Time), 323\nUTC (Coordinated Universal Time), 323\ntimed extracts, CDC (change data capture), \n452\ntime dimension, 80\nclickstream data, 361–362\ntimeliness goals, 4\ntime-of-day\ndimension, 83\nfact, 83\ntime series\nbehavior tags, 63, 240–242\nﬁ xed time series buckets, date dimensions \nand, 302–303\ntime shifting, 90\ntimespan fact tables, 252–254\ndual date/time stamps, 254\ntimespan tracking in fact tables, 62\ntime varying multivalued bridge tables, 63\ntime zones\nairline case study, 323\nGMT (Greenwich Mean Time), 323\nmultiple, 65\nnumber of, 323\nUTC (Coordinated Universal Time), 323\ntools\ndimensional modeling, 432\ndata proﬁ ling tools, 433\nETL development, 499\ntransactions, 43, 120, 179\nclaim transactions (insurance case study), \n390\nclaim accumulating snapshot, 393–394\njunk dimensions and, 392\nperiodic snapshot, 395–396\ntimespan accumulating snapshot, \n394–395\n\n\nIndex  \n564\nfact tables, 12, 143–145\nhealthcare case study, 342\ninventory transactions, 116–118\ninvoice transactions, 187–188\njournal entries (G/L), 206–207\nnumbers, degenerate dimensions, 93–94\norder management case study\nallocating, 184–186\ndate, 170–171\ndeal dimension, 177–178\ndegenerate dimension, 178–179\nheader/line patterns, 181–182, 186\njunk dimensions, 179–180\nproduct dimension, 172–173\norder transactions, 168\naudit dimension, 192–193\ncustomer dimension, 174–176\nfact normalization, 169–170\nmultiple currency, 182–184\npolicies (insurance case study), 379–380\nprocurement, 142–143\ntransaction proﬁ le dimension, 49, 179\ntransportation, 311\nairline case study, 311–323\ncargo shipper schema, 317\nlocalization and, 324\ntravel services ﬂ ight schema, 317\ntravel services ﬂ ight schema, 317\ntrees (hierarchies), 215\nparent/child structure, 216\ntype 0 (retain original) SCD, 54\nretain original, 148–149\ntype 1 (overwrite) SCD, 54\nadd to type 2 dimension, 160–162\nETL system, 465\noverwrite, 149–150\ntype 2 in same dimension, 153\ntype 2 (add new row) SCD, 54, 150–152\naccumulating snapshots, 196\ncustomer counts, 243\neff ective date, 152–153\nemployee proﬁ le changes, 267\nETL system, 465–466\nexpiration date, 152–153\ntype 1 in same dimension, 153\ntype 3 (add new attribute) SCD, 55, 154–155\nETL system, 467\nmultiple, 156\ntype 4 (add mini-dimension) SCD, 55, \n156–159\nETL system, 467\ntype 5 (add mini-dimension and type 1 \noutrigger) SCD, 55\ntype 5 (add mini-dimension and type \noutrigger) SCD, 160\nETL system, 468\ntype 6 (add type 1 attributes to type 2 \ndimension) SCD, 56, 160–162\nETL system, 468\ntype 7 (dual type 1 and type 2 dimension) \nSCD, 56, 162–163\nas of reporting, 164\nETL system, 468\nU\nUnicode, 236–238\nuniform chart of accounts, 204\nunits of measure, multiple, 197–198\nupdates, accumulating snapshots, 121–122\nuser-maintained dimensions, ETL systems, \n472–473\nUTC (Coordinated Universal Time), 323\nV\nvalidating dimension model, 440–441\nvalidation, relationships, 504–505\nvalue band reporting, 291–292\nvalue chain, 52\ninsurance case study, 377–378\nintegration, 122–123\ninventory case study, 111–112\nvariable depth hierarchies\npathstring attributes, 57\nragged, 215–217\nslightly ragged, 214–215\nvariable depth/ragged hierarchies with \nbridge tables, 57\nvariable depth/slightly ragged hierarchies, \n57\nversion control, 495\nETL system, 488\nversion migration system, ETL system, 488\nvisitor identiﬁ cation, web sites, 356–357\nW\nweekday indicator, 82\nWHERE clause, 18\nworkﬂ ow monitor, ETL system, 489–490\nworkshops, dimensional modeling, 38\nX–Y–Z\nYTD (year-to-date) facts, 61\nG/L (general ledger), 206 \n\n\nwww.kimballgroup.com\nLearn More. Get More. Do More.\nThe Kimball Group is the source for dimensional data \nwarehouse and business intelligence consulting and education. \nAfter all, we wrote the books!\n■  Subscribe to Kimball DESIGN TIPS for practical, \n   reliable guidance\n■  Attend KIMBALL UNIVERSITY for courses consistent with\n   the instructors’ best-selling Toolkit books\n■  Work with Kimball CONSULTANTS to leverage our\n   decades of real-world experience\n  Visit www.kimballgroup.com for more information.\nK I M BA L L  GROUP\nConsulting | Kimball University\n",
      "page_number": 582
    },
    {
      "number": 64,
      "title": "Segment 64 (pages 590-599)",
      "start_page": 590,
      "end_page": 599,
      "detection_method": "topic_boundary",
      "content": "",
      "page_number": 590
    },
    {
      "number": 65,
      "title": "Segment 65 (pages 600-601)",
      "start_page": 600,
      "end_page": 601,
      "detection_method": "topic_boundary",
      "content": "",
      "page_number": 600
    }
  ],
  "pages": [
    {
      "page_number": 3,
      "content": "The Data \nWarehouse \nToolkit\n",
      "content_length": 29,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 5,
      "content": "The Data \nWarehouse \nToolkit\nThird Edition\nRalph Kimball\nMargy Ross\nThe Defi nitive Guide to \nDimensional Modeling\n",
      "content_length": 115,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 6,
      "content": "The Data Warehouse Toolkit: The Deﬁ nitive Guide to Dimensional Modeling, Third Edition\nPublished by\nJohn Wiley & Sons, Inc.\n10475 Crosspoint Boulevard\nIndianapolis, IN 46256\nwww.wiley.com\nCopyright © 2013 by Ralph Kimball and Margy Ross\nPublished by John Wiley & Sons, Inc., Indianapolis, Indiana\nPublished simultaneously in Canada\nISBN: 978-1-118-53080-1\nISBN: 978-1-118-53077-1 (ebk)\nISBN: 978-1-118-73228-1 (ebk)\nISBN: 978-1-118-73219-9 (ebk)\nManufactured in the United States of America\n10 9 8 7 6 5 4 3 2 1\nNo part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or \nby any means, electronic, mechanical, photocopying, recording, scanning or otherwise, except as permit-\nted under Sections 107 or 108 of the 1976 United States Copyright Act, without either the prior written \npermission of the Publisher, or authorization through payment of the appropriate per-copy fee to the \nCopyright Clearance Center, 222 Rosewood Drive, Danvers, MA 01923, (978) 750-8400, fax (978) 646-\n8600. Requests to the Publisher for permission should be addressed to the Permissions Department, John \nWiley & Sons, Inc., 111 River Street, Hoboken, NJ  07030, (201) 748-6011, fax (201) 748-6008, or online \nat http://www.wiley.com/go/permissions.\nLimit of Liability/Disclaimer of Warranty: The publisher and the author make no representations or war-\nranties with respect to the accuracy or completeness of the contents of this work and speciﬁ cally disclaim all \nwarranties, including without limitation warranties of ﬁ tness for a particular purpose. No warranty may be \ncreated or extended by sales or promotional materials. The advice and strategies contained herein may not \nbe suitable for every situation. This work is sold with the understanding that the publisher is not engaged in \nrendering legal, accounting, or other professional services. If professional assistance is required, the services \nof a competent professional person should be sought. Neither the publisher nor the author shall be liable for \ndamages arising herefrom. The fact that an organization or Web site is referred to in this work as a citation \nand/or a potential source of further information does not mean that the author or the publisher endorses \nthe information the organization or website may provide or recommendations it may make. Further, readers \nshould be aware that Internet websites listed in this work may have changed or disappeared between when \nthis work was written and when it is read.\nFor general information on our other products and services please contact our Customer Care \nDepartment within the United States at (877) 762-2974, outside the United States at (317) 572-3993 or fax \n(317) 572-4002.\nWiley publishes in a variety of print and electronic formats and by print-on-demand. Some material \nincluded with standard print versions of this book may not be included in e-books or in print-on-\ndemand. If this book refers to media such as a CD or DVD that is not included in the version you \npurchased, you may download this material at http://booksupport.wiley.com. For more informa-\ntion about Wiley products, visit www.wiley.com.\nLibrary of Congress Control Number: 2013936841\nTrademarks: Wiley and the Wiley logo are trademarks or registered trademarks of John Wiley & Sons, \nInc. and/or its affi  liates, in the United States and other countries, and may not be used without written per-\nmission. All other trademarks are the property of their respective owners. John Wiley & Sons, Inc. is not \nassociated with any product or vendor mentioned in this book.\n",
      "content_length": 3602,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 7,
      "content": "Ralph Kimball founded the Kimball Group. Since the mid-1980s, he has been the \ndata warehouse and business intelligence industry’s thought leader on the dimen-\nsional approach. He has educated tens of thousands of IT professionals. The Toolkit \nbooks written by Ralph and his colleagues have been the industry’s best sellers \nsince 1996. Prior to working at Metaphor and founding Red Brick Systems, Ralph \ncoinvented the Star workstation, the ﬁ rst commercial product with windows, icons, \nand a mouse, at Xerox’s Palo Alto Research Center (PARC). Ralph has a PhD in \nelectrical engineering from Stanford University.\nMargy Ross is president of the Kimball Group. She has focused exclusively on data \nwarehousing and business intelligence since 1982 with an emphasis on business \nrequirements and dimensional modeling. Like Ralph, Margy has taught the dimen-\nsional best practices to thousands of students; she also coauthored ﬁ ve Toolkit books \nwith Ralph. Margy previously worked at Metaphor and cofounded DecisionWorks \nConsulting. She graduated with a BS in industrial engineering from Northwestern \nUniversity.\nAbout the Authors\n",
      "content_length": 1134,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 8,
      "content": "Executive Editor\nRobert Elliott\nProject Editor\nMaureen Spears\nSenior Production Editor\nKathleen Wisor\nCopy Editor\nApostrophe Editing Services\nEditorial Manager\nMary Beth Wakeﬁ eld \nFreelancer Editorial Manager\nRosemarie Graham\nAssociate Director of Marketing\nDavid Mayhew\nMarketing Manager\nAshley Zurcher\nBusiness Manager\nAmy Knies\nProduction Manager\nTim Tate\nVice President and Executive Group \nPublisher\nRichard Swadley\nVice President and Executive Publisher\nNeil Edde\nAssociate Publisher\nJim Minatel\nProject Coordinator, Cover\nKatie Crocker\nProofreader\nWord One, New York\nIndexer\nJohnna VanHoose Dinse\nCover Image\niStockphoto.com / teekid\nCover Designer\nRyan Sneed\nCredits\n",
      "content_length": 676,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 9,
      "content": "F\nirst, thanks to the hundreds of thousands who have read our Toolkit books, \nattended our courses, and engaged us in consulting projects. We have learned as \nmuch from you as we have taught. Collectively, you have had a profoundly positive \nimpact on the data warehousing and business intelligence industry. Congratulations!\nOur Kimball Group colleagues, Bob Becker, Joy Mundy, and Warren Thornthwaite, \nhave worked with us to apply the techniques described in this book literally thou-\nsands of times, over nearly 30 years of working together. Every technique in this \nbook has been thoroughly vetted by practice in the real world. We appreciate their \ninput and feedback on this book—and more important, the years we have shared \nas business partners, along with Julie Kimball. \nBob Elliott, our executive editor at John Wiley & Sons, project editor Maureen \nSpears, and the rest of the Wiley team have supported this project with skill and \nenthusiasm. As always, it has been a pleasure to work with them.\nTo our families, thank you for your unconditional support throughout our \ncareers. Spouses Julie Kimball and Scott Ross and children Sara Hayden Smith, \nBrian Kimball, and Katie Ross all contributed in countless ways to this book.\nAcknowledgments\n",
      "content_length": 1257,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 11,
      "content": "Contents\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .xxvii\n 1  Data Warehousing, Business Intelligence, and Dimensional \nModeling Primer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\nDifferent Worlds of Data Capture and Data Analysis . . . . . . . . . . . . . . . . . . .2\nGoals of Data Warehousing and Business Intelligence . . . . . . . . . . . . . . . . . .3\nPublishing Metaphor for DW/BI Managers . . . . . . . . . . . . . . . . . . . . . . .5\nDimensional Modeling Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .7\nStar Schemas Versus OLAP Cubes  . . . . . . . . . . . . . . . . . . . . . . . . . . . . .8\nFact Tables for Measurements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\nDimension Tables for Descriptive Context . . . . . . . . . . . . . . . . . . . . . . 13\nFacts and Dimensions Joined in a Star Schema . . . . . . . . . . . . . . . . . . . 16\nKimball’s DW/BI Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\nOperational Source Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\nExtract, Transformation, and Load System . . . . . . . . . . . . . . . . . . . . . . 19\nPresentation Area to Support Business Intelligence. . . . . . . . . . . . . . . .21\nBusiness Intelligence Applications  . . . . . . . . . . . . . . . . . . . . . . . . . . . .22\nRestaurant Metaphor for the Kimball Architecture . . . . . . . . . . . . . . . .23\nAlternative DW/BI Architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .26\nIndependent Data Mart Architecture . . . . . . . . . . . . . . . . . . . . . . . . . .26\nHub-and-Spoke Corporate Information Factory Inmon Architecture  . .28\nHybrid Hub-and-Spoke and Kimball Architecture . . . . . . . . . . . . . . . . .29\nDimensional Modeling Myths. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .30\nMyth 1: Dimensional Models are Only for Summary Data . . . . . . . . . .30\nMyth 2: Dimensional Models are Departmental, Not Enterprise  . . . . .31\nMyth 3: Dimensional Models are Not Scalable . . . . . . . . . . . . . . . . . . .31\nMyth 4: Dimensional Models are Only for Predictable Usage . . . . . . . .31\nMyth 5: Dimensional Models Can’t Be Integrated . . . . . . . . . . . . . . . .32\nMore Reasons to Think Dimensionally  . . . . . . . . . . . . . . . . . . . . . . . . . . . .32\nAgile Considerations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .34\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .35\n",
      "content_length": 2667,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 12,
      "content": "Contents\nx\n 2 Kimball Dimensional Modeling Techniques Overview . . . . . . . . . 37\nFundamental Concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\nGather Business Requirements and Data Realities . . . . . . . . . . . . . . . . . 37\nCollaborative Dimensional Modeling Workshops . . . . . . . . . . . . . . . . .38\nFour-Step Dimensional Design Process . . . . . . . . . . . . . . . . . . . . . . . . .38\nBusiness Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .39\nGrain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .39\nDimensions for Descriptive Context . . . . . . . . . . . . . . . . . . . . . . . . . . .40\nFacts for Measurements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .40\nStar Schemas and OLAP Cubes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .40\nGraceful Extensions to Dimensional Models . . . . . . . . . . . . . . . . . . . . . 41\nBasic Fact Table Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\nFact Table Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\nAdditive, Semi-Additive, Non-Additive Facts . . . . . . . . . . . . . . . . . . . .42\nNulls in Fact Tables  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .42\nConformed Facts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .42\nTransaction Fact Tables  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\nPeriodic Snapshot Fact Tables  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\nAccumulating Snapshot Fact Tables . . . . . . . . . . . . . . . . . . . . . . . . . . .44\nFactless Fact Tables  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .44\nAggregate Fact Tables or OLAP Cubes . . . . . . . . . . . . . . . . . . . . . . . . .45\nConsolidated Fact Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .45\nBasic Dimension Table Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .46\nDimension Table Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .46\nDimension Surrogate Keys . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .46\nNatural, Durable, and Supernatural Keys . . . . . . . . . . . . . . . . . . . . . . .46\nDrilling Down  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\nDegenerate Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\nDenormalized Flattened Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . 47\nMultiple Hierarchies in Dimensions  . . . . . . . . . . . . . . . . . . . . . . . . . . .48\nFlags and Indicators as Textual Attributes . . . . . . . . . . . . . . . . . . . . . . .48\nNull Attributes in Dimensions  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .48\nCalendar Date Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .48\nRole-Playing Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .49\nJunk Dimensions  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .49\n",
      "content_length": 3319,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 13,
      "content": "Contents\nxi\nSnowﬂ aked Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .50\nOutrigger Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .50\nIntegration via Conformed Dimensions  . . . . . . . . . . . . . . . . . . . . . . . . . . .50\nConformed Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .51\nShrunken Dimensions  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .51\nDrilling Across . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .51\nValue Chain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .52\nEnterprise Data Warehouse Bus Architecture . . . . . . . . . . . . . . . . . . . .52\nEnterprise Data Warehouse Bus Matrix . . . . . . . . . . . . . . . . . . . . . . . . .52\nDetailed Implementation Bus Matrix . . . . . . . . . . . . . . . . . . . . . . . . . .53\nOpportunity/Stakeholder Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . .53\nDealing with Slowly Changing Dimension Attributes . . . . . . . . . . . . . . . . .53\nType 0: Retain Original  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .54\nType 1: Overwrite  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .54\nType 2: Add New Row . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .54\nType 3: Add New Attribute  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .55\nType 4: Add Mini-Dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .55\nType 5: Add Mini-Dimension and Type 1 Outrigger . . . . . . . . . . . . . . .55\nType 6: Add Type 1 Attributes to Type 2 Dimension. . . . . . . . . . . . . . .56\nType 7: Dual Type 1 and Type 2 Dimensions . . . . . . . . . . . . . . . . . . . .56\nDealing with Dimension Hierarchies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .56\nFixed Depth Positional Hierarchies . . . . . . . . . . . . . . . . . . . . . . . . . . . .56\nSlightly Ragged/Variable Depth Hierarchies . . . . . . . . . . . . . . . . . . . . .57\nRagged/Variable Depth Hierarchies with Hierarchy Bridge Tables  . . . .57\nRagged/Variable Depth Hierarchies with Pathstring Attributes . . . . . . .57\nAdvanced Fact Table Techniques  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .58\nFact Table Surrogate Keys. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .58\nCentipede Fact Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .58\nNumeric Values as Attributes or Facts  . . . . . . . . . . . . . . . . . . . . . . . . .59\nLag/Duration Facts. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .59\nHeader/Line Fact Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .59\nAllocated Facts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .60\nProﬁ t and Loss Fact Tables Using Allocations . . . . . . . . . . . . . . . . . . . .60\nMultiple Currency Facts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .60\nMultiple Units of Measure Facts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .61\n",
      "content_length": 3262,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 14,
      "content": "Contents\nxii\nYear-to-Date Facts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .61\nMultipass SQL to Avoid Fact-to-Fact Table Joins . . . . . . . . . . . . . . . . . .61\nTimespan Tracking in Fact Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . .62\nLate Arriving Facts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .62\nAdvanced Dimension Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .62\nDimension-to-Dimension Table Joins . . . . . . . . . . . . . . . . . . . . . . . . . .62\nMultivalued Dimensions and Bridge Tables . . . . . . . . . . . . . . . . . . . . .63\nTime Varying Multivalued Bridge Tables  . . . . . . . . . . . . . . . . . . . . . . .63\nBehavior Tag Time Series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .63\nBehavior Study Groups  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .64\nAggregated Facts as Dimension Attributes . . . . . . . . . . . . . . . . . . . . . .64\nDynamic Value Bands  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .64\nText Comments Dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .65\nMultiple Time Zones . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .65\nMeasure Type Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .65\nStep Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .65\nHot Swappable Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .66\nAbstract Generic Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .66\nAudit Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .66\nLate Arriving Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .67\nSpecial Purpose Schemas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .67\nSupertype and Subtype Schemas for Heterogeneous Products  . . . . . .67\nReal-Time Fact Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .68\nError Event Schemas  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .68\n 3 Retail Sales . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69\nFour-Step Dimensional Design Process . . . . . . . . . . . . . . . . . . . . . . . . . . . .70\nStep 1: Select the Business Process . . . . . . . . . . . . . . . . . . . . . . . . . . . .70\nStep 2: Declare the Grain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .71\nStep 3: Identify the Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .72\nStep 4: Identify the Facts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .72\nRetail Case Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .72\nStep 1: Select the Business Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74\nStep 2: Declare the Grain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74\nStep 3: Identify the Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76\n",
      "content_length": 3251,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 15,
      "content": "Contents xiii\nStep 4: Identify the Facts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76\nDimension Table Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .79\nDate Dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .79\nProduct Dimension  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .83\nStore Dimension  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .87\nPromotion Dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .89\nOther Retail Sales Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .92\nDegenerate Dimensions for Transaction Numbers . . . . . . . . . . . . . . . .93\nRetail Schema in Action . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .94\nRetail Schema Extensibility  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .95\nFactless Fact Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .97\nDimension and Fact Table Keys . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .98\nDimension Table Surrogate Keys  . . . . . . . . . . . . . . . . . . . . . . . . . . . . .98\nDimension Natural and Durable Supernatural Keys . . . . . . . . . . . . . .100\nDegenerate Dimension Surrogate Keys  . . . . . . . . . . . . . . . . . . . . . . . 101\nDate Dimension Smart Keys . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101\nFact Table Surrogate Keys. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102\nResisting Normalization Urges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .104\nSnowﬂ ake Schemas with Normalized Dimensions . . . . . . . . . . . . . . .104\nOutriggers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .106\nCentipede Fact Tables with Too Many Dimensions . . . . . . . . . . . . . . .108\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .109\n 4 Inventory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  111\nValue Chain Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\nInventory Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112\nInventory Periodic Snapshot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113\nInventory Transactions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116\nInventory Accumulating Snapshot . . . . . . . . . . . . . . . . . . . . . . . . . . . 118\nFact Table Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119\nTransaction Fact Tables  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120\nPeriodic Snapshot Fact Tables  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120\nAccumulating Snapshot Fact Tables . . . . . . . . . . . . . . . . . . . . . . . . . . 121\nComplementary Fact Table Types  . . . . . . . . . . . . . . . . . . . . . . . . . . . 122\n",
      "content_length": 3202,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 16,
      "content": "Contents\nxiv\nValue Chain Integration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122\nEnterprise Data Warehouse Bus Architecture . . . . . . . . . . . . . . . . . . . . . . . 123\nUnderstanding the Bus Architecture  . . . . . . . . . . . . . . . . . . . . . . . . . 124\nEnterprise Data Warehouse Bus Matrix . . . . . . . . . . . . . . . . . . . . . . . . 125\nConformed Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130\nDrilling Across Fact Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130\nIdentical Conformed Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131\nShrunken Rollup Conformed Dimension with Attribute Subset  . . . . . 132\nShrunken Conformed Dimension with Row Subset  . . . . . . . . . . . . . . 132\nShrunken Conformed Dimensions on the Bus Matrix . . . . . . . . . . . . . 134\nLimited Conformity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135\nImportance of Data Governance and Stewardship . . . . . . . . . . . . . . . 135\nConformed Dimensions and the Agile Movement . . . . . . . . . . . . . . . 137\nConformed Facts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139\n 5 Procurement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  141\nProcurement Case Study  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141\nProcurement Transactions and Bus Matrix  . . . . . . . . . . . . . . . . . . . . . . . . 142\nSingle Versus Multiple Transaction Fact Tables . . . . . . . . . . . . . . . . . . 143\nComplementary Procurement Snapshot. . . . . . . . . . . . . . . . . . . . . . . 147\nSlowly Changing Dimension Basics  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147\nType 0: Retain Original  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148\nType 1: Overwrite  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149\nType 2: Add New Row . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150\nType 3: Add New Attribute  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154\nType 4: Add Mini-Dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156\nHybrid Slowly Changing Dimension Techniques . . . . . . . . . . . . . . . . . . . . 159\nType 5: Mini-Dimension and Type 1 Outrigger  . . . . . . . . . . . . . . . . . 160\nType 6: Add Type 1 Attributes to Type 2 Dimension. . . . . . . . . . . . . . 160\nType 7: Dual Type 1 and Type 2 Dimensions . . . . . . . . . . . . . . . . . . . 162\nSlowly Changing Dimension Recap  . . . . . . . . . . . . . . . . . . . . . . . . . . . . .164\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165\n",
      "content_length": 2963,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 17,
      "content": "Contents\nxv\n 6 Order Management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  167\nOrder Management Bus Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168\nOrder Transactions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168\nFact Normalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169\nDimension Role Playing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170\nProduct Dimension Revisited . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172\nCustomer Dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174\nDeal Dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 177\nDegenerate Dimension for Order Number . . . . . . . . . . . . . . . . . . . . . 178\nJunk Dimensions  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179\nHeader/Line Pattern to Avoid . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181\nMultiple Currencies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182\nTransaction Facts at Different Granularity  . . . . . . . . . . . . . . . . . . . . .184\nAnother Header/Line Pattern to Avoid . . . . . . . . . . . . . . . . . . . . . . . . 186\nInvoice Transactions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187\nService Level Performance as Facts, Dimensions, or Both . . . . . . . . . . 188\nProﬁ t and Loss Facts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189\nAudit Dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192\nAccumulating Snapshot for Order Fulﬁ llment Pipeline  . . . . . . . . . . . . . . . 194\nLag Calculations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196\nMultiple Units of Measure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197\nBeyond the Rearview Mirror  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199\n 7 Accounting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201\nAccounting Case Study and Bus Matrix  . . . . . . . . . . . . . . . . . . . . . . . . . .202\nGeneral Ledger Data  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .203\nGeneral Ledger Periodic Snapshot . . . . . . . . . . . . . . . . . . . . . . . . . . .203\nChart of Accounts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .203\nPeriod Close . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .204\nYear-to-Date Facts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .206\nMultiple Currencies Revisited . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .206\nGeneral Ledger Journal Transactions  . . . . . . . . . . . . . . . . . . . . . . . . .206\n",
      "content_length": 3105,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 18,
      "content": "Contents\nxvi\nMultiple Fiscal Accounting Calendars . . . . . . . . . . . . . . . . . . . . . . . . .208\nDrilling Down Through a Multilevel Hierarchy . . . . . . . . . . . . . . . . . .209\nFinancial Statements  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .209\nBudgeting Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 210\nDimension Attribute Hierarchies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 214\nFixed Depth Positional Hierarchies . . . . . . . . . . . . . . . . . . . . . . . . . . . 214\nSlightly Ragged Variable Depth Hierarchies . . . . . . . . . . . . . . . . . . . . 214\nRagged Variable Depth Hierarchies . . . . . . . . . . . . . . . . . . . . . . . . . . 215\nShared Ownership in a Ragged Hierarchy . . . . . . . . . . . . . . . . . . . . . 219\nTime Varying Ragged Hierarchies  . . . . . . . . . . . . . . . . . . . . . . . . . . .220\nModifying Ragged Hierarchies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .220\nAlternative Ragged Hierarchy Modeling Approaches . . . . . . . . . . . . .221\nAdvantages of the Bridge Table Approach for Ragged Hierarchies . . .223\nConsolidated Fact Tables  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .224\nRole of OLAP and Packaged Analytic Solutions . . . . . . . . . . . . . . . . . . . . .226\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .227\n 8 Customer Relationship Management . . . . . . . . . . . . . . . . . . . . 229\nCRM Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .230\nOperational and Analytic CRM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 231\nCustomer Dimension Attributes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233\nName and Address Parsing  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233\nInternational Name and Address Considerations . . . . . . . . . . . . . . . .236\nCustomer-Centric Dates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .238\nAggregated Facts as Dimension Attributes . . . . . . . . . . . . . . . . . . . . .239\nSegmentation Attributes and Scores  . . . . . . . . . . . . . . . . . . . . . . . . .240\nCounts with Type 2 Dimension Changes . . . . . . . . . . . . . . . . . . . . . . 243\nOutrigger for Low Cardinality Attribute Set . . . . . . . . . . . . . . . . . . . . 243\nCustomer Hierarchy Considerations . . . . . . . . . . . . . . . . . . . . . . . . . .244\nBridge Tables for Multivalued Dimensions  . . . . . . . . . . . . . . . . . . . . . . . . 245\nBridge Table for Sparse Attributes  . . . . . . . . . . . . . . . . . . . . . . . . . . . 247\nBridge Table for Multiple Customer Contacts . . . . . . . . . . . . . . . . . . .248\nComplex Customer Behavior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249\nBehavior Study Groups for Cohorts . . . . . . . . . . . . . . . . . . . . . . . . . . 249\n",
      "content_length": 3027,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 19,
      "content": "Contents xvii\nStep Dimension for Sequential Behavior . . . . . . . . . . . . . . . . . . . . . . .251\nTimespan Fact Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .252\nTagging Fact Tables with Satisfaction Indicators . . . . . . . . . . . . . . . . .254\nTagging Fact Tables with Abnormal Scenario Indicators . . . . . . . . . . .255\nCustomer Data Integration Approaches . . . . . . . . . . . . . . . . . . . . . . . . . .256\nMaster Data Management Creating a Single Customer Dimension  . .256\nPartial Conformity of Multiple Customer Dimensions . . . . . . . . . . . . .258\nAvoiding Fact-to-Fact Table Joins . . . . . . . . . . . . . . . . . . . . . . . . . . . .259\nLow Latency Reality Check  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .260\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 261\n 9 Human Resources Management . . . . . . . . . . . . . . . . . . . . . . . . 263\nEmployee Proﬁ le Tracking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263\nPrecise Effective and Expiration Timespans  . . . . . . . . . . . . . . . . . . . .265\nDimension Change Reason Tracking  . . . . . . . . . . . . . . . . . . . . . . . . .266\nProﬁ le Changes as Type 2 Attributes or Fact Events . . . . . . . . . . . . . . 267\nHeadcount Periodic Snapshot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267\nBus Matrix for HR Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .268\nPackaged Analytic Solutions and Data Models . . . . . . . . . . . . . . . . . . . . .270\nRecursive Employee Hierarchies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .271\nChange Tracking on Embedded Manager Key . . . . . . . . . . . . . . . . . .272\nDrilling Up and Down Management Hierarchies . . . . . . . . . . . . . . . .273\nMultivalued Skill Keyword Attributes  . . . . . . . . . . . . . . . . . . . . . . . . . . . . 274\nSkill Keyword Bridge  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275\nSkill Keyword Text String . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 276\nSurvey Questionnaire Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .277\nText Comments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .278\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .279\n 10 Financial Services . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 281\nBanking Case Study and Bus Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . .282\nDimension Triage to Avoid Too Few Dimensions . . . . . . . . . . . . . . . . . . . .283\nHousehold Dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .286\nMultivalued Dimensions and Weighting Factors . . . . . . . . . . . . . . . . . 287\n",
      "content_length": 2961,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 20,
      "content": "Contents\nxviii\nMini-Dimensions Revisited . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .289\nAdding a Mini-Dimension to a Bridge Table . . . . . . . . . . . . . . . . . . . .290\nDynamic Value Banding of Facts  . . . . . . . . . . . . . . . . . . . . . . . . . . . .291\nSupertype and Subtype Schemas for Heterogeneous Products . . . . . . . . .293\nSupertype and Subtype Products with Common Facts  . . . . . . . . . . .295\nHot Swappable Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .296\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .296\n 11 Telecommunications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 297\nTelecommunications Case Study and Bus Matrix  . . . . . . . . . . . . . . . . . . .297\nGeneral Design Review Considerations . . . . . . . . . . . . . . . . . . . . . . . . . . .299\nBalance Business Requirements and Source Realities  . . . . . . . . . . . . .300\nFocus on Business Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .300\nGranularity  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .300\nSingle Granularity for Facts  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .301\nDimension Granularity and Hierarchies . . . . . . . . . . . . . . . . . . . . . . .301\nDate Dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .302\nDegenerate Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .303\nSurrogate Keys . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .303\nDimension Decodes and Descriptions . . . . . . . . . . . . . . . . . . . . . . . .303\nConformity Commitment  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .304\nDesign Review Guidelines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .304\nDraft Design Exercise Discussion  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .306\nRemodeling Existing Data Structures . . . . . . . . . . . . . . . . . . . . . . . . . . . .309\nGeographic Location Dimension  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 310\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 310\n 12 Transportation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  311\nAirline Case Study and Bus Matrix  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 311\nMultiple Fact Table Granularities . . . . . . . . . . . . . . . . . . . . . . . . . . . . 312\nLinking Segments into Trips . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 315\nRelated Fact Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 316\nExtensions to Other Industries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317\nCargo Shipper . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317\nTravel Services . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317\n",
      "content_length": 3156,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 21,
      "content": "Contents xix\nCombining Correlated Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 318\nClass of Service  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 319\nOrigin and Destination  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .320\nMore Date and Time Considerations  . . . . . . . . . . . . . . . . . . . . . . . . . . . . 321\nCountry-Speciﬁ c Calendars as Outriggers  . . . . . . . . . . . . . . . . . . . . . 321\nDate and Time in Multiple Time Zones  . . . . . . . . . . . . . . . . . . . . . . .323\nLocalization Recap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 324\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 324\n 13 Education  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 325\nUniversity Case Study and Bus Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . .325\nAccumulating Snapshot Fact Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . .326\nApplicant Pipeline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .326\nResearch Grant Proposal Pipeline . . . . . . . . . . . . . . . . . . . . . . . . . . . .329\nFactless Fact Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .329\nAdmissions Events . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .330\nCourse Registrations  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .330\nFacility Utilization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .334\nStudent Attendance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335\nMore Educational Analytic Opportunities . . . . . . . . . . . . . . . . . . . . . . . . . 336\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 336\n 14 Healthcare . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 339\nHealthcare Case Study and Bus Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . 339\nClaims Billing and Payments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .342\nDate Dimension Role Playing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .345\nMultivalued Diagnoses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .345\nSupertypes and Subtypes for Charges . . . . . . . . . . . . . . . . . . . . . . . .347\nElectronic Medical Records . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .348\nMeasure Type Dimension for Sparse Facts . . . . . . . . . . . . . . . . . . . . .349\nFreeform Text Comments  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .350\nImages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .350\nFacility/Equipment Inventory Utilization . . . . . . . . . . . . . . . . . . . . . . . . . . 351\nDealing with Retroactive Changes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 351\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 352\n",
      "content_length": 3254,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 22,
      "content": "Contents\nxx\n 15 Electronic Commerce . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 353\nClickstream Source Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 353\nClickstream Data Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .354\nClickstream Dimensional Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 357\nPage Dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .358\nEvent Dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 359\nSession Dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 359\nReferral Dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .360\nClickstream Session Fact Table . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .361\nClickstream Page Event Fact Table . . . . . . . . . . . . . . . . . . . . . . . . . . .363\nStep Dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .366\nAggregate Clickstream Fact Tables . . . . . . . . . . . . . . . . . . . . . . . . . . .366\nGoogle Analytics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .367\nIntegrating Clickstream into Web Retailer’s Bus Matrix . . . . . . . . . . . . . . .368\nProﬁ tability Across Channels Including Web . . . . . . . . . . . . . . . . . . . . . . . 370\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 373\n 16 Insurance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  375\nInsurance Case Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 376\nInsurance Value Chain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 377\nDraft Bus Matrix  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 378\nPolicy Transactions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 379\nDimension Role Playing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .380\nSlowly Changing Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .380\nMini-Dimensions for Large or Rapidly Changing Dimensions . . . . . . .381\nMultivalued Dimension Attributes . . . . . . . . . . . . . . . . . . . . . . . . . . .382\nNumeric Attributes as Facts or Dimensions  . . . . . . . . . . . . . . . . . . . .382\nDegenerate Dimension  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .383\nLow Cardinality Dimension Tables . . . . . . . . . . . . . . . . . . . . . . . . . . .383\nAudit Dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .383\nPolicy Transaction Fact Table . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .383\nHeterogeneous Supertype and Subtype Products  . . . . . . . . . . . . . . .384\nComplementary Policy Accumulating Snapshot . . . . . . . . . . . . . . . . .384\nPremium Periodic Snapshot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .385\nConformed Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .386\nConformed Facts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .386\n",
      "content_length": 3348,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 23,
      "content": "Contents xxi\nPay-in-Advance Facts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .386\nHeterogeneous Supertypes and Subtypes Revisited . . . . . . . . . . . . . .387\nMultivalued Dimensions Revisited . . . . . . . . . . . . . . . . . . . . . . . . . . .388\nMore Insurance Case Study Background . . . . . . . . . . . . . . . . . . . . . . . . . .388\nUpdated Insurance Bus Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .389\nDetailed Implementation Bus Matrix . . . . . . . . . . . . . . . . . . . . . . . . .390\nClaim Transactions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .390\nTransaction Versus Proﬁ le Junk Dimensions  . . . . . . . . . . . . . . . . . . . . 392\nClaim Accumulating Snapshot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 392\nAccumulating Snapshot for Complex Workﬂ ows . . . . . . . . . . . . . . . . 393\nTimespan Accumulating Snapshot . . . . . . . . . . . . . . . . . . . . . . . . . . .394\nPeriodic Instead of Accumulating Snapshot  . . . . . . . . . . . . . . . . . . . .395\nPolicy/Claim Consolidated Periodic Snapshot . . . . . . . . . . . . . . . . . . . . . .395\nFactless Accident Events . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .396\nCommon Dimensional Modeling Mistakes to Avoid . . . . . . . . . . . . . . . . . 397\nMistake 10: Place Text Attributes in a Fact Table . . . . . . . . . . . . . . . . . 397\nMistake 9: Limit Verbose Descriptors to Save Space . . . . . . . . . . . . . .398\nMistake 8: Split Hierarchies into Multiple Dimensions  . . . . . . . . . . . .398\nMistake 7: Ignore the Need to Track Dimension Changes  . . . . . . . . .398\nMistake 6: Solve All Performance Problems with More Hardware . . . .399\nMistake 5: Use Operational Keys to Join Dimensions and Facts . . . . . .399\nMistake 4: Neglect to Declare and Comply with the Fact Grain . . . . .399\nMistake 3: Use a Report to Design the Dimensional Model  . . . . . . . .400\nMistake 2: Expect Users to Query Normalized Atomic Data . . . . . . . .400\nMistake 1: Fail to Conform Facts and Dimensions  . . . . . . . . . . . . . . .400\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .401\n 17 Kimball DW/BI Lifecycle Overview . . . . . . . . . . . . . . . . . . . . . . 403\nLifecycle Roadmap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .404\nRoadmap Mile Markers  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .405\nLifecycle Launch Activities  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .406\nProgram/Project Planning and Management . . . . . . . . . . . . . . . . . . .406\nBusiness Requirements Deﬁ nition  . . . . . . . . . . . . . . . . . . . . . . . . . . . 410\nLifecycle Technology Track . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 416\nTechnical Architecture Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 416\nProduct Selection and Installation . . . . . . . . . . . . . . . . . . . . . . . . . . . 418\n",
      "content_length": 3121,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 24,
      "content": "Contents\nxxii\nLifecycle Data Track . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .420\nDimensional Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .420\nPhysical Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .420\nETL Design and Development . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .422\nLifecycle BI Applications Track  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .422\nBI Application Speciﬁ cation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .423\nBI Application Development  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .423\nLifecycle Wrap-up Activities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 424\nDeployment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 424\nMaintenance and Growth . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .425\nCommon Pitfalls to Avoid . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 426\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .427\n 18 Dimensional Modeling Process and Tasks . . . . . . . . . . . . . . . . . 429\nModeling Process Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .429\nGet Organized . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 431\nIdentify Participants, Especially Business Representatives . . . . . . . . . . 431\nReview the Business Requirements . . . . . . . . . . . . . . . . . . . . . . . . . . . 432\nLeverage a Modeling Tool . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 432\nLeverage a Data Proﬁ ling Tool . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 433\nLeverage or Establish Naming Conventions . . . . . . . . . . . . . . . . . . . . 433\nCoordinate Calendars and Facilities . . . . . . . . . . . . . . . . . . . . . . . . . . 433\nDesign the Dimensional Model  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .434\nReach Consensus on High-Level Bubble Chart . . . . . . . . . . . . . . . . . . 435\nDevelop the Detailed Dimensional Model . . . . . . . . . . . . . . . . . . . . . 436\nReview and Validate the Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 439\nFinalize the Design Documentation . . . . . . . . . . . . . . . . . . . . . . . . . .441\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .441\n 19 ETL Subsystems and Techniques  . . . . . . . . . . . . . . . . . . . . . . . 443\nRound Up the Requirements. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .444\nBusiness Needs  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .444\nCompliance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .445\nData Quality  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .445\nSecurity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .446\nData Integration  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .446\n",
      "content_length": 3288,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 25,
      "content": "Contents xxiii\nData Latency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .447\nArchiving and Lineage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .447\nBI Delivery Interfaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .448\nAvailable Skills . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .448\nLegacy Licenses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .449\nThe 34 Subsystems of ETL  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .449\nExtracting: Getting Data into the Data Warehouse . . . . . . . . . . . . . . . . . .450\nSubsystem 1: Data Proﬁ ling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .450\nSubsystem 2: Change Data Capture System . . . . . . . . . . . . . . . . . . . . 451\nSubsystem 3: Extract System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 453\nCleaning and Conforming Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 455\nImproving Data Quality Culture and Processes . . . . . . . . . . . . . . . . . . 455\nSubsystem 4: Data Cleansing System . . . . . . . . . . . . . . . . . . . . . . . . .456\nSubsystem 5: Error Event Schema  . . . . . . . . . . . . . . . . . . . . . . . . . . .458\nSubsystem 6: Audit Dimension Assembler . . . . . . . . . . . . . . . . . . . . .460\nSubsystem 7: Deduplication System . . . . . . . . . . . . . . . . . . . . . . . . . .460\nSubsystem 8: Conforming System . . . . . . . . . . . . . . . . . . . . . . . . . . .461\nDelivering: Prepare for Presentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .463\nSubsystem 9: Slowly Changing Dimension Manager . . . . . . . . . . . . .464\nSubsystem 10: Surrogate Key Generator  . . . . . . . . . . . . . . . . . . . . . .469\nSubsystem 11: Hierarchy Manager . . . . . . . . . . . . . . . . . . . . . . . . . . . 470\nSubsystem 12: Special Dimensions Manager . . . . . . . . . . . . . . . . . . . 470\nSubsystem 13: Fact Table Builders  . . . . . . . . . . . . . . . . . . . . . . . . . . . 473\nSubsystem 14: Surrogate Key Pipeline . . . . . . . . . . . . . . . . . . . . . . . . 475\nSubsystem 15: Multivalued Dimension Bridge Table Builder . . . . . . . . 477\nSubsystem 16: Late Arriving Data Handler . . . . . . . . . . . . . . . . . . . . . 478\nSubsystem 17: Dimension Manager System . . . . . . . . . . . . . . . . . . . . 479\nSubsystem 18: Fact Provider System . . . . . . . . . . . . . . . . . . . . . . . . . .480\nSubsystem 19: Aggregate Builder . . . . . . . . . . . . . . . . . . . . . . . . . . . .481\nSubsystem 20: OLAP Cube Builder . . . . . . . . . . . . . . . . . . . . . . . . . . .481\nSubsystem 21: Data Propagation Manager . . . . . . . . . . . . . . . . . . . . .482\nManaging the ETL Environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .483\nSubsystem 22: Job Scheduler . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .483\nSubsystem 23: Backup System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .485\nSubsystem 24: Recovery and Restart System  . . . . . . . . . . . . . . . . . . .486\n",
      "content_length": 3182,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 26,
      "content": "Contents\nxxiv\nSubsystem 25: Version Control System  . . . . . . . . . . . . . . . . . . . . . . .488\nSubsystem 26: Version Migration System . . . . . . . . . . . . . . . . . . . . . .488\nSubsystem 27: Workﬂ ow Monitor  . . . . . . . . . . . . . . . . . . . . . . . . . . .489\nSubsystem 28: Sorting System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .490\nSubsystem 29: Lineage and Dependency Analyzer . . . . . . . . . . . . . . .490\nSubsystem 30: Problem Escalation System . . . . . . . . . . . . . . . . . . . . .491\nSubsystem 31: Parallelizing/Pipelining System . . . . . . . . . . . . . . . . . .492\nSubsystem 32: Security System  . . . . . . . . . . . . . . . . . . . . . . . . . . . . .492\nSubsystem 33: Compliance Manager . . . . . . . . . . . . . . . . . . . . . . . . . 493\nSubsystem 34: Metadata Repository Manager  . . . . . . . . . . . . . . . . .495\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .496\n 20 ETL System Design and Development Process and Tasks  . . . . . 497\nETL Process Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 497\nDevelop the ETL Plan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .498\nStep 1: Draw the High-Level Plan . . . . . . . . . . . . . . . . . . . . . . . . . . . .498\nStep 2: Choose an ETL Tool . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .499\nStep 3: Develop Default Strategies . . . . . . . . . . . . . . . . . . . . . . . . . . .500\nStep 4: Drill Down by Target Table . . . . . . . . . . . . . . . . . . . . . . . . . . .500\nDevelop the ETL Speciﬁ cation Document  . . . . . . . . . . . . . . . . . . . . .502\nDevelop One-Time Historic Load Processing . . . . . . . . . . . . . . . . . . . . . . . 503\nStep 5: Populate Dimension Tables with Historic Data . . . . . . . . . . . . 503\nStep 6: Perform the Fact Table Historic Load . . . . . . . . . . . . . . . . . . .508\nDevelop Incremental ETL Processing. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 512\nStep 7: Dimension Table Incremental Processing . . . . . . . . . . . . . . . . 512\nStep 8: Fact Table Incremental Processing . . . . . . . . . . . . . . . . . . . . . 515\nStep 9: Aggregate Table and OLAP Loads  . . . . . . . . . . . . . . . . . . . . . 519\nStep 10: ETL System Operation and Automation . . . . . . . . . . . . . . . . 519\nReal-Time Implications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .520\nReal-Time Triage  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 521\nReal-Time Architecture Trade-Offs . . . . . . . . . . . . . . . . . . . . . . . . . . .522\nReal-Time Partitions in the Presentation Server. . . . . . . . . . . . . . . . . . 524\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 526\n",
      "content_length": 2907,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 27,
      "content": "Contents xxv\n 21 Big Data Analytics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 527\nBig Data Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .527\nExtended RDBMS Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .529\nMapReduce/Hadoop Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . .530\nComparison of Big Data Architectures . . . . . . . . . . . . . . . . . . . . . . . .530\nRecommended Best Practices for Big Data . . . . . . . . . . . . . . . . . . . . . . . . 531\nManagement Best Practices for Big Data . . . . . . . . . . . . . . . . . . . . . . 531\nArchitecture Best Practices for Big Data . . . . . . . . . . . . . . . . . . . . . . . 533\nData Modeling Best Practices for Big Data . . . . . . . . . . . . . . . . . . . . .538\nData Governance Best Practices for Big Data . . . . . . . . . . . . . . . . . . .541\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .542\n \n Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 543\n",
      "content_length": 1131,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 29,
      "content": "T\nhe data warehousing and business intelligence (DW/BI) industry certainly has \nmatured since Ralph Kimball published the ﬁ rst edition of The Data Warehouse \nToolkit (Wiley) in 1996. Although large corporate early adopters paved the way, DW/\nBI has since been embraced by organizations of all sizes. The industry has built \nthousands of DW/BI systems. The volume of data continues to grow as warehouses \nare populated with increasingly atomic data and updated with greater frequency. \nOver the course of our careers, we have seen databases grow from megabytes to \ngigabytes to terabytes to petabytes, yet the basic challenge of DW/BI systems has \nremained remarkably constant. Our job is to marshal an organization’s data and \nbring it to business users for their decision making. Collectively, you’ve delivered \non this objective; business professionals everywhere are making better decisions \nand generating payback on their DW/BI investments.\nSince the ﬁ rst edition of The Data Warehouse Toolkit was published, dimensional \nmodeling has been broadly accepted as the dominant technique for DW/BI presenta-\ntion. Practitioners and pundits alike have recognized that the presentation of data \nmust be grounded in simplicity if it is to stand any chance of success. Simplicity is \nthe fundamental key that allows users to easily understand databases and software \nto effi  ciently navigate databases. In many ways, dimensional modeling amounts \nto holding the fort against assaults on simplicity. By consistently returning to a \nbusiness-driven perspective and by refusing to compromise on the goals of user \nunderstandability and query performance, you establish a coherent design that \nserves the organization’s analytic needs. This dimensionally modeled framework \nbecomes the platform for BI. Based on our experience and the overwhelming feed-\nback from numerous practitioners from companies like your own, we believe that \ndimensional modeling is absolutely critical to a successful DW/BI initiative.\nDimensional modeling also has emerged as the leading architecture for building \nintegrated DW/BI systems. When you use the conformed dimensions and con-\nformed facts of a set of dimensional models, you have a practical and predictable \nframework for incrementally building complex DW/BI systems that are inherently \ndistributed.\nFor all that has changed in our industry, the core dimensional modeling tech-\nniques that Ralph Kimball published 17 years ago have withstood the test of time. \nConcepts such as conformed dimensions, slowly changing dimensions, heteroge-\nneous products, factless fact tables, and the enterprise data warehouse bus matrix \nIntroduction\n",
      "content_length": 2671,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 30,
      "content": "Introduction\nxxviii\ncontinue to be discussed in design workshops around the globe. The original con-\ncepts have been embellished and enhanced by new and complementary techniques. \nWe decided to publish this third edition of Kimball’s seminal work because we felt \nthat it would be useful to summarize our collective dimensional modeling experi-\nence under a single cover. We have each focused exclusively on decision support, \ndata warehousing, and business intelligence for more than three decades. We want \nto share the dimensional modeling patterns that have emerged repeatedly during \nthe course of our careers. This book is loaded with speciﬁ c, practical design recom-\nmendations based on real-world scenarios.\nThe goal of this book is to provide a one-stop shop for dimensional modeling \ntechniques. True to its title, it is a toolkit of dimensional design principles and \ntechniques. We address the needs of those just starting in dimensional DW/BI and \nwe describe advanced concepts for those of you who have been at this a while. We \nbelieve that this book stands alone in its depth of coverage on the topic of dimen-\nsional modeling. It’s the deﬁ nitive guide.\nIntended Audience\nThis book is intended for data warehouse and business intelligence designers, imple-\nmenters, and managers. In addition, business analysts and data stewards who are \nactive participants in a DW/BI initiative will ﬁ nd the content useful.\nEven if you’re not directly responsible for the dimensional model, we believe it \nis important for all members of a project team to be comfortable with dimensional \nmodeling concepts. The dimensional model has an impact on most aspects of a \nDW/BI implementation, beginning with the translation of business requirements, \nthrough the extract, transformation and load (ETL) processes, and ﬁ nally, to the \nunveiling of a data warehouse through business intelligence applications. Due to the \nbroad implications, you need to be conversant in dimensional modeling regardless \nof whether you are responsible primarily for project management, business analysis, \ndata architecture, database design, ETL, BI applications, or education and support. \nWe’ve written this book so it is accessible to a broad audience.\nFor those of you who have read the earlier editions of this book, some of the \nfamiliar case studies will reappear in this edition; however, they have been updated \nsigniﬁ cantly and ﬂ eshed out with richer content, including sample enterprise data \nwarehouse bus matrices for nearly every case study. We have developed vignettes \nfor new subject areas, including big data analytics.\nThe content in this book is somewhat technical. We primarily discuss dimen-\nsional modeling in the context of a relational database with nuances for online \n",
      "content_length": 2777,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 31,
      "content": "Introduction xxix\nanalytical processing (OLAP) cubes noted where appropriate. We presume you \nhave basic knowledge of relational database concepts such as tables, rows, keys, \nand joins. Given we will be discussing dimensional models in a nondenominational \nmanner, we won’t dive into speciﬁ c physical design and tuning guidance for any \ngiven database management systems.\nChapter Preview\nThe book is organized around a series of business vignettes or case studies. We \nbelieve developing the design techniques by example is an extremely eff ective \napproach because it allows us to share very tangible guidance and the beneﬁ ts of \nreal world experience. Although not intended to be full-scale application or indus-\ntry solutions, these examples serve as a framework to discuss the patterns that \nemerge in dimensional modeling. In our experience, it is often easier to grasp the \nmain elements of a design technique by stepping away from the all-too-familiar \ncomplexities of one’s own business. Readers of the earlier editions have responded \nvery favorably to this approach.\nBe forewarned that we deviate from the case study approach in Chapter 2: Kimball \nDimensional Modeling Techniques Overview. Given the broad industry acceptance \nof the dimensional modeling techniques invented by the Kimball Group, we have \nconsolidated the offi  cial listing of our techniques, along with concise descriptions \nand pointers to more detailed coverage and illustrations of these techniques in \nsubsequent chapters. Although not intended to be read from start to ﬁ nish like the \nother chapters, we feel this technique-centric chapter is a useful reference and can \neven serve as a professional checklist for DW/BI designers.\nWith the exception of Chapter 2, the other chapters of this book build on one \nanother. We start with basic concepts and introduce more advanced content as the \nbook unfolds. The chapters should be read in order by every reader. For example, it \nmight be diffi  cult to comprehend Chapter 16: Insurance, unless you have read the \npreceding chapters on retailing, procurement, order management, and customer \nrelationship management.\nThose of you who have read the last edition may be tempted to skip the ﬁ rst \nfew chapters. Although some of the early fact and dimension grounding may be \nfamiliar turf, we don’t want you to sprint too far ahead. You’ll miss out on updates \nto fundamental concepts if you skip ahead too quickly.\nNOTE \nThis book is laced with tips (like this note), key concept listings, and \nchapter pointers to make it more useful and easily referenced in the future.\n",
      "content_length": 2607,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 32,
      "content": "Introduction\nxxx\nChapter 1: Data Warehousing, Business Intelligence, \nand Dimensional Modeling Primer\nThe book begins with a primer on data warehousing, business intelligence, and \ndimensional modeling. We explore the components of the overall DW/BI archi-\ntecture and establish the core vocabulary used during the remainder of the book. \nSome of the myths and misconceptions about dimensional modeling are dispelled.\nChapter 2: Kimball Dimensional Modeling \nTechniques Overview\nThis chapter describes more than 75 dimensional modeling techniques and pat-\nterns. This offi  cial listing of the Kimball techniques includes forward pointers to \nsubsequent chapters where the techniques are brought to life in case study vignettes. \nChapter 3: Retail Sales\nRetailing is the classic example used to illustrate dimensional modeling. We start \nwith the classic because it is one that we all understand. Hopefully, you won’t need \nto think very hard about the industry because we want you to focus on core dimen-\nsional modeling concepts instead. We begin by discussing the four-step process for \ndesigning dimensional models. We explore dimension tables in depth, including \nthe date dimension that will be reused repeatedly throughout the book. We also \ndiscuss degenerate dimensions, snowﬂ aking, and surrogate keys. Even if you’re not \na retailer, this chapter is required reading because it is chock full of fundamentals.\nChapter 4: Inventory\nWe remain within the retail industry for the second case study but turn your atten-\ntion to another business process. This chapter introduces the enterprise data ware-\nhouse bus architecture and the bus matrix with conformed dimensions. These \nconcepts are critical to anyone looking to construct a DW/BI architecture that is \nintegrated and extensible. We also compare the three fundamental types of fact \ntables: transaction, periodic snapshot, and accumulating snapshot.\nChapter 5: Procurement\nThis chapter reinforces the importance of looking at your organization’s value chain \nas you plot your DW/BI environment. We also explore a series of basic and advanced \ntechniques for handling slowly changing dimension attributes; we’ve built on the \nlong-standing foundation of type 1 (overwrite), type 2 (add a row), and type 3 (add \na column) as we introduce readers to type 0 and types 4 through 7. \n",
      "content_length": 2343,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 33,
      "content": "Introduction xxxi\nChapter 6: Order Management\nIn this case study, we look at the business processes that are often the ﬁ rst to be \nimplemented in DW/BI systems as they supply core business performance met-\nrics—what are we selling to which customers at what price? We discuss dimensions \nthat play multiple roles within a schema. We also explore the common challenges \nmodelers face when dealing with order management information, such as header/\nline item considerations, multiple currencies or units of measure, and junk dimen-\nsions with miscellaneous transaction indicators. \nChapter 7: Accounting\nWe discuss the modeling of general ledger information for the data warehouse in \nthis chapter. We describe the appropriate handling of year-to-date facts and multiple \nﬁ scal calendars, as well as consolidated fact tables that combine data from mul-\ntiple business processes. We also provide detailed guidance on dimension attribute \nhierarchies, from simple denormalized ﬁ xed depth hierarchies to bridge tables for \nnavigating more complex ragged, variable depth hierarchies.\nChapter 8: Customer Relationship Management\nNumerous DW/BI systems have been built on the premise that you need to better \nunderstand and service your customers. This chapter discusses the customer dimen-\nsion, including address standardization and bridge tables for multivalued dimension \nattributes. We also describe complex customer behavior modeling patterns, as well \nas the consolidation of customer data from multiple sources.\nChapter 9: Human Resources Management\nThis chapter explores several unique aspects of human resources dimensional \nmodels, including the situation in which a dimension table begins to behave like a \nfact table. We discuss packaged analytic solutions, the handling of recursive man-\nagement hierarchies, and survey questionnaires. Several techniques for handling \nmultivalued skill keyword attributes are compared.\nChapter 10: Financial Services\nThe banking case study explores the concept of supertype and subtype schemas \nfor heterogeneous products in which each line of business has unique descriptive \nattributes and performance metrics. Obviously, the need to handle heterogeneous \nproducts is not unique to ﬁ nancial services. We also discuss the complicated rela-\ntionships among accounts, customers, and households.\n",
      "content_length": 2338,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 34,
      "content": "Introduction\nxxxii\nChapter 11: Telecommunications\nThis chapter is structured somewhat diff erently to encourage you to think critically \nwhen performing a dimensional model design review. We start with a dimensional \ndesign that looks plausible at ﬁ rst glance. Can you ﬁ nd the problems? In addition, \nwe explore the idiosyncrasies of geographic location dimensions.\nChapter 12: Transportation\nIn this case study we look at related fact tables at diff erent levels of granularity \nwhile pointing out the unique characteristics of fact tables describing segments in \na journey or network. We take a closer look at date and time dimensions, covering \ncountry-speciﬁ c calendars and synchronization across multiple time zones.\nChapter 13: Education\nWe look at several factless fact tables in this chapter. In addition, we explore accu-\nmulating snapshot fact tables to handle the student application and research grant \nproposal pipelines. This chapter gives you an appreciation for the diversity of busi-\nness processes in an educational institution.\nChapter 14: Healthcare\nSome of the most complex models that we have ever worked with are from the \nhealthcare industry. This chapter illustrates the handling of such complexities, \nincluding the use of a bridge table to model the multiple diagnoses and providers \nassociated with patient treatment events.\nChapter 15: Electronic Commerce\nThis chapter focuses on the nuances of clickstream web data, including its unique \ndimensionality. We also introduce the step dimension that’s used to better under-\nstand any process that consists of sequential steps.\nChapter 16: Insurance\nThe ﬁ nal case study reinforces many of the patterns we discussed earlier in the book \nin a single set of interrelated schemas. It can be viewed as a pulling-it-all-together \nchapter because the modeling techniques are layered on top of one another.\n",
      "content_length": 1878,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 35,
      "content": "Introduction xxxiii\nChapter 17: Kimball Lifecycle Overview\nNow that you are comfortable designing dimensional models, we provide a high-\nlevel overview of the activities encountered during the life of a typical DW/BI proj-\nect. This chapter is a lightning tour of The Data Warehouse Lifecycle Toolkit, Second \nEdition (Wiley, 2008) that we coauthored with Bob Becker, Joy Mundy, and Warren \nThornthwaite.\nChapter 18: Dimensional Modeling Process and Tasks\nThis chapter outlines speciﬁ c recommendations for tackling the dimensional mod-\neling tasks within the Kimball Lifecycle. The ﬁ rst 16 chapters of this book cover \ndimensional modeling techniques and design patterns; this chapter describes \nresponsibilities, how-tos, and deliverables for the dimensional modeling design \nactivity.\nChapter 19: ETL Subsystems and Techniques\nThe extract, transformation, and load system consumes a disproportionate share \nof the time and eff ort required to build a DW/BI environment. Careful consider-\nation of best practices has revealed 34 subsystems found in almost every dimen-\nsional data warehouse back room. This chapter starts with the requirements and \nconstraints that must be considered before designing the ETL system and then \ndescribes the 34 extraction, cleaning, conforming, delivery, and management \nsubsystems.\nChapter 20: ETL System Design and Development \nProcess and Tasks\nThis chapter delves into speciﬁ c, tactical dos and don’ts surrounding the ETL \ndesign and development activities. It is required reading for anyone tasked with \nETL responsibilities.\nChapter 21: Big Data Analytics\nWe focus on the popular topic of big data in the ﬁ nal chapter. Our perspective \nis that big data is a natural extension of your DW/BI responsibilities. We begin \nwith an overview of several architectural alternatives, including MapReduce and \n",
      "content_length": 1843,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 36,
      "content": "Introduction\nxxxiv\nHadoop, and describe how these alternatives can coexist with your current DW/BI \narchitecture. We then explore the management, architecture, data modeling, and \ndata governance best practices for big data.\nWebsite Resources\nThe Kimball Group’s website is loaded with complementary dimensional modeling \ncontent and resources:\n \n■Register for Kimball Design Tips to receive practical guidance about dimen-\nsional modeling and DW/BI topics.\n \n■Access the archive of more than 300 Design Tips and articles.\n \n■Learn about public and onsite Kimball University classes for quality, vendor-\nindependent education consistent with our experiences and writings.\n \n■Learn about the Kimball Group’s consulting services to leverage our decades \nof DW/BI expertise.\n \n■Pose questions to other dimensionally aware participants on the Kimball \nForum.\nSummary\nThe goal of this book is to communicate the offi  cial dimensional design and devel-\nopment techniques based on the authors’ more than 60 years of experience and \nhard won lessons in real business environments. DW/BI systems must be driven \nfrom the needs of business users, and therefore are designed and presented from a \nsimple dimensional perspective. We are conﬁ dent you will be one giant step closer \nto DW/BI success if you buy into this premise.\nNow that you know where you are headed, it is time to dive into the details. We’ll \nbegin with a primer on DW/BI and dimensional modeling in Chapter 1 to ensure that \neveryone is on the same page regarding key terminology and architectural concepts. \n",
      "content_length": 1569,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 37,
      "content": "Data Warehousing, \nBusiness Intelligence, \nand Dimensional \nModeling Primer\nT\nhis first chapter lays the groundwork for the following chapters. We begin by \nconsidering data warehousing and business intelligence (DW/BI) systems from \na high-level perspective. You may be disappointed to learn that we don’t start with \ntechnology and tools—first and foremost, the DW/BI system must consider the \nneeds of the business. With the business needs firmly in hand, we work backwards \nthrough the logical  and then physical designs, along with decisions about technol-\nogy and tools.\nWe drive stakes in the ground regarding the goals of data warehousing and busi-\nness intelligence in this chapter, while observing the uncanny similarities between \nthe responsibilities of a DW/BI manager and those of a publisher. \nWith this big picture perspective, we explore dimensional modeling core concepts \nand establish fundamental vocabulary. From there, this chapter discusses the major \ncomponents of the Kimball DW/BI architecture, along with a comparison of alterna-\ntive architectural approaches; fortunately, there’s a role for dimensional modeling \nregardless of your architectural persuasion. Finally, we review common dimensional \nmodeling myths. By the end of this chapter, you’ll have an appreciation for the need \nto be one-half DBA (database administrator) and one-half MBA (business analyst) \nas you tackle your DW/BI project.\nChapter 1 discusses the following concepts:\n \n■Business-driven goals of data warehousing and business intelligence\n \n■Publishing metaphor for DW/BI systems\n \n■Dimensional modeling core concepts and vocabulary, including fact and \ndimension tables\n \n■Kimball DW/BI architecture’s components and tenets\n \n■Comparison of alternative DW/BI architectures, and the role of dimensional \nmodeling within each\n \n■Misunderstandings about dimensional modeling\n1\n",
      "content_length": 1878,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 38,
      "content": "Chapter 1\n2\nDifferent Worlds of Data Capture and \nData Analysis\nOne of the most important assets of any organization is its information. This asset \nis almost always used for two purposes: operational record keeping and analytical \ndecision making. Simply speaking, the operational systems are where you put the \ndata in, and the DW/BI system is where you get the data out.\nUsers of an  operational system turn the wheels of the organization. They take \norders, sign up new customers, monitor the status of operational activities, and log \ncomplaints. The operational systems are optimized to process transactions quickly. \nThese systems almost always deal with one transaction record at a time. They predict-\nably perform the same operational tasks over and over, executing the organization’s \nbusiness processes. Given this execution focus, operational systems typically do not \nmaintain history, but rather update data to reﬂ ect the most current state.\nUsers of a DW/BI  system, on the other hand, watch the wheels of the organiza-\ntion turn to evaluate performance. They count the new orders and compare them \nwith last week’s orders, and ask why the new customers signed up, and what the \ncustomers complained about. They worry about whether operational processes are \nworking correctly. Although they need detailed data to support their constantly \nchanging questions, DW/BI users almost never deal with one transaction at a time. \nThese systems are optimized for high-performance queries as users’ questions often \nrequire that hundreds or hundreds of thousands of transactions be searched and \ncompressed into an answer set. To further complicate matters, users of a DW/BI \nsystem typically demand that historical context be preserved to accurately evaluate \nthe organization’s performance over time.\nIn the ﬁ rst edition of The Data Warehouse Toolkit (Wiley, 1996), Ralph Kimball \n devoted an entire chapter to describe the dichotomy between the worlds of opera-\ntional processing  and data warehousing. At this time, it is widely recognized that \nthe DW/BI system has profoundly diff erent needs, clients, structures, and rhythms \nthan the operational systems of record. Unfortunately, we still encounter supposed \nDW/BI systems that are mere copies of the operational systems of record stored on \na separate hardware platform. Although these environments may address the need \nto isolate the operational and analytical environments for performance reasons, \nthey do nothing to address the other inherent diff erences between the two types \nof systems. Business users are underwhelmed by the usability and performance \nprovided by these pseudo data warehouses; these imposters do a disservice to DW/\nBI because they don’t acknowledge their users have drastically diff erent needs than \noperational system users.\n",
      "content_length": 2823,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 39,
      "content": "Data Warehousing, Business Intelligence, and Dimensional Modeling Primer \n3\nGoals of Data Warehousing and \nBusiness Intelligence\nBefore we delve into the details of dimensional modeling, it is helpful to focus on \nthe fundamental goals of data warehousing and business intelligence. The goals  can \nbe readily developed by walking through the halls of any organization and listening \nto business management. These recurring themes have existed for more than three \ndecades:\n \n■“We collect tons of data, but we can’t access it.”\n \n■“We need to slice and dice the data every which way.”\n \n■“Business people need to get at the data easily.”\n \n■“Just show me what is important.”\n \n■“We spend entire meetings arguing about who has the right numbers rather \nthan making decisions.”\n \n■“We want people to use information to support more fact-based decision \nmaking.”\nBased on our experience, these concerns are still so universal that they drive the \nbedrock requirements for the DW/BI system. Now turn these business management \nquotations into requirements.\n \n■The DW/BI system must make information easily accessible. The contents \nof the DW/BI system must be understandable. The  data must be intuitive and \nobvious to the business user, not merely the developer. The data’s structures \nand labels should mimic the business users’ thought processes and vocabu-\nlary. Business users want to separate and combine analytic data in endless \ncombinations. The business intelligence tools and applications that access \nthe data must be simple and easy to use. They also must return query results \nto the user with minimal wait times. We can summarize this requirement by \nsimply saying simple and fast.\n \n■The DW/BI system must present information consistently. The data in the \nDW/BI system must be credible. Data must be carefully assembled from a \nvariety of sources, cleansed, quality  assured, and released only when it is ﬁ t \nfor user consumption. Consistency also implies common labels and deﬁ ni-\ntions for the DW/BI system’s contents are used across data sources. If two \nperformance measures have the same name, they must mean the same thing. \nConversely, if two measures don’t mean the same thing, they should be labeled \ndiff erently.\n",
      "content_length": 2239,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 40,
      "content": "Chapter 1\n4\n \n■The DW/BI system must adapt to change. User needs, business conditions, \ndata,  and technology are all subject to change. The DW/BI system must be \ndesigned to handle this inevitable change gracefully so that it doesn’t invali-\ndate existing data or applications. Existing data and applications should not \nbe changed or disrupted when the business community asks new questions \nor new data is added to the warehouse. Finally, if descriptive data in the DW/\nBI system must be modiﬁ ed, you must appropriately account for the changes \nand make these changes transparent to the users.\n \n■The DW/BI system must present information in a timely way. As the DW/\nBI system is  used more intensively for operational decisions, raw data may \nneed to be converted into actionable information within hours, minutes, \nor even seconds. The DW/BI team and business users need to have realistic \nexpectations for what it means to deliver data when there is little time to \nclean or validate it.\n \n■The DW/BI system must be a secure bastion that protects the information \nassets. An  organization’s informational crown jewels are stored in the data \nwarehouse. At a minimum, the warehouse likely contains information about \nwhat you’re selling to whom at what price—potentially harmful details in the \nhands of the wrong people. The DW/BI system must eff ectively control access \nto the organization’s conﬁ dential information.\n \n■The DW/BI system must serve as the authoritative and trustworthy foun-\ndation for improved decision making. The data warehouse must have the \nright data to support decision making. The most important  outputs from a \nDW/BI system are the decisions that are made based on the analytic evidence \npresented; these decisions deliver the business impact and value attributable \nto the DW/BI system. The original label that predates DW/BI is still the best \ndescription of what you are designing: a decision support system.\n \n■The business community must accept the DW/BI system to deem it successful. \nIt doesn’t matter that you built an elegant solution using best-of-breed products \nand platforms. If the business community does not embrace the DW/BI environ-\nment and actively use it, you have failed the acceptance test. Unlike an opera-\ntional system implementation where business users have no choice but to use \nthe new system, DW/BI usage is sometimes optional. Business users will embrace \nthe DW/BI system if it is the “simple and fast” source for actionable information.\nAlthough each requirement on this list is important, the ﬁ nal two are the most \ncritical, and unfortunately, often the most overlooked. Successful data warehousing \nand business intelligence demands more than being a stellar architect, technician, \nmodeler, or database administrator. With a DW/BI initiative, you have one foot \nin your information technology (IT) comfort zone while your other foot is on the \n",
      "content_length": 2919,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 41,
      "content": "Data Warehousing, Business Intelligence, and Dimensional Modeling Primer \n5\nunfamiliar turf of business users. You must straddle the two, modifying some tried-\nand-true skills to adapt to the unique demands of DW/BI. Clearly, you need to bring \na spectrum of skills to the party to behave like you’re a hybrid DBA/MBA.\n Publishing Metaphor for DW/BI Managers\nWith the  goals of DW/BI as a backdrop, let’s compare the responsibilities of DW/BI \nmanagers with those of a publishing editor-in-chief. As the editor of a high-quality \nmagazine, you would have broad latitude to manage the magazine’s content, style, \nand delivery. Anyone with this job title would likely tackle the following activities:\n \n■Understand the readers:\n \n■Identify their demographic characteristics.\n \n■Find out what readers want in this kind of magazine.\n \n■Identify the “best” readers who will renew their subscriptions and buy \nproducts from the magazine’s advertisers.\n \n■Find potential new readers and make them aware of the magazine.\n \n■Ensure the magazine appeals to the readers:\n \n■Choose interesting and compelling magazine content.\n \n■Make layout and rendering decisions that maximize the readers’ \npleasure.\n \n■Uphold high-quality writing and editing standards while adopting a \nconsistent presentation style.\n \n■Continuously monitor the accuracy of the articles and advertisers’ \nclaims.\n \n■Adapt to changing reader profiles and the availability of new input \nfrom a network of writers and contributors.\n \n■Sustain the publication:\n \n■Attract advertisers and run the magazine profitably.\n \n■Publish the magazine on a regular basis.\n \n■Maintain the readers’ trust.\n \n■Keep the business owners happy.\nYou also can identify items that should be non-goals for the magazine’s editor-\nin-chief, such as building the magazine around a particular printing technology \nor exclusively putting management’s energy into operational effi  ciencies, such as \nimposing a technical writing style that readers don’t easily understand, or creating \nan intricate and crowded layout that is diffi  cult to read.\nBy building the publishing business on a foundation of serving the readers eff ec-\ntively, the magazine is likely to be successful. Conversely, go through the list and \nimagine what happens if you omit any single item; ultimately, the magazine would \nhave serious problems.\n",
      "content_length": 2351,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 42,
      "content": "Chapter 1\n6\nThere are strong parallels that can be drawn between being a conventional pub-\nlisher and being a DW/BI manager. Driven by the needs of the business, DW/BI \nmanagers must publish data that has been collected from a variety of sources and \nedited for quality and consistency. The main responsibility is to serve the readers, \notherwise known as business users. The publishing metaphor underscores the need \nto focus outward on your customers rather than merely focusing inward on prod-\nucts and processes. Although you use technology to deliver the DW/BI system, the \ntechnology is at best a means to an end. As such, the technology and techniques \nused to build the system should not appear directly in your top job responsibilities.\nNow recast the magazine publisher’s responsibilities as DW/BI manager \nresponsibilities:\n \n■Understand the business users:\n \n■Understand their job responsibilities, goals, and objectives.\n \n■Determine the decisions that the business users want to make with the \nhelp of the DW/BI system.\n \n■Identify the “best” users who make effective, high-impact decisions.\n \n■Find potential new users and make them aware of the DW/BI system’s \ncapabilities.\n \n■Deliver high-quality, relevant, and accessible information and analytics to \nthe business users:\n \n■Choose the most robust, actionable data to present in the DW/BI sys-\ntem, carefully selected from the vast universe of possible data sources \nin your organization.\n \n■Make the user interfaces and applications simple and template-driven, \nexplicitly matched to the users’ cognitive processing profiles.\n \n■Make sure the data is accurate and can be trusted, labeling it consis-\ntently across the enterprise.\n \n■Continuously monitor the accuracy of the data and analyses.\n \n■Adapt to changing user profiles, requirements, and business priorities, \nalong with the availability of new data sources. \n \n■Sustain the DW/BI environment:\n \n■Take a portion of the credit for the business decisions made using the \nDW/BI system, and use these successes to justify staffing and ongoing \nexpenditures.\n \n■Update the DW/BI system on a regular basis.\n \n■Maintain the business users’ trust.\n \n■Keep the business users, executive sponsors, and IT management \nhappy.\n",
      "content_length": 2243,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 43,
      "content": "Data Warehousing, Business Intelligence, and Dimensional Modeling Primer \n7\nIf you do a good job with all these responsibilities, you will be a great DW/BI \nmanager! Conversely, go through the list and imagine what happens if you omit \nany single item. Ultimately, the environment would have serious problems. Now \ncontrast this view of a DW/BI manager’s job with your own job description. Chances \nare the preceding list is more oriented toward user and business issues and may not \neven sound like a job in IT. In our opinion, this is what makes data warehousing \nand business intelligence interesting.\nDimensional Modeling Introduction\nNow that  you understand the DW/BI system’s goals, let’s consider the basics of dimen-\nsional modeling. Dimensional modeling is widely accepted as the preferred technique \nfor presenting analytic data because it addresses two simultaneous requirements:\n \n■Deliver data that’s understandable to the business users.\n \n■Deliver fast query performance.\nDimensional modeling is a longstanding technique for making databases simple. \nIn case after case, for more than ﬁ ve decades, IT organizations, consultants, and \nbusiness users have naturally gravitated to a simple dimensional structure to match \nthe fundamental human need for simplicity. Simplicity is critical because it ensures \nthat users can easily understand the data, as well as allows software to navigate and \ndeliver results quickly and effi  ciently.\nImagine an executive who describes her business as, “We sell products in various \nmarkets and measure our performance over time.” Dimensional designers listen \ncarefully to the emphasis on product, market, and time. Most people ﬁ nd it intui-\ntive to think of such a business as a cube of data, with the edges labeled product, \nmarket, and time. Imagine slicing and dicing along each of these dimensions. Points \ninside the cube are where the measurements, such as sales volume or proﬁ t, for \nthat combination of product, market, and time are stored. The ability to visualize \nsomething as abstract as a set of data in a concrete and tangible way is the secret \nof understandability. If this perspective seems too simple, good! A data model that \nstarts simple has a chance of remaining simple at the end of the design. A model \nthat starts complicated surely will be overly complicated at the end, resulting in \nslow query performance and business user rejection. Albert Einstein captured the \nbasic philosophy driving dimensional design when he said, “Make everything as \nsimple as possible, but not simpler.”\nAlthough  dimensional models are often instantiated in relational database man-\nagement systems, they are quite diff erent from third normal form (3NF) models which \n",
      "content_length": 2731,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 44,
      "content": "Chapter 1\n8\nseek to remove data redundancies. Normalized 3NF structures divide data into \nmany discrete entities, each of which becomes a relational table. A database of sales \norders might start with a record for each order line but turn into a complex spider \nweb diagram as a 3NF model, perhaps consisting of hundreds of normalized tables.\nThe industry  sometimes refers to 3NF models as entity-relationship (ER) \nmodels. Entity-relationship diagrams (ER diagrams or ERDs) are drawings that com-\nmunicate the relationships between tables. Both 3NF and dimensional models can \nbe represented in ERDs because both consist of joined relational tables; the key \ndiff erence between 3NF and dimensional models is the degree of normalization. \nBecause both model types can be presented as ERDs, we refrain from referring to \n3NF models as ER models; instead, we call  them normalized models to minimize \nconfusion.\nNormalized 3NF structures  are immensely useful in operational processing \nbecause an update or insert transaction touches the database in only one place. \nNormalized models, however, are too complicated for BI queries. Users can’t under-\nstand, navigate, or remember normalized models that resemble a map of the Los \nAngeles freeway system. Likewise, most relational database management systems \ncan’t effi  ciently query a normalized model; the complexity of users’ unpredictable \nqueries overwhelms the database optimizers, resulting in disastrous query perfor-\nmance. The use of normalized modeling in the DW/BI presentation area defeats the \nintuitive and high-performance retrieval of data. Fortunately, dimensional modeling \naddresses the problem of overly complex schemas in the presentation area. \nNOTE \nA dimensional model contains the same information as a normalized \nmodel, but packages the data in a format that delivers user understandability, query \nperformance, and resilience to change.\n Star Schemas Versus OLAP Cubes\nDimensional  models implemented in relational database management systems are \nreferred to as star schemas because of their resemblance to a star-like structure. \nDimensional models implemented in multidimensional database environments are \nreferred to as online analytical processing (OLAP) cubes, as illustrated in Figure 1-1. \nIf your DW/BI environment includes either star schemas or OLAP cubes, it lever-\nages dimensional concepts. Both stars and cubes have a common logical design with \nrecognizable dimensions; however, the physical implementation diff ers.\nWhen  data is loaded into an OLAP cube, it is stored and indexed using formats \nand techniques that are designed for dimensional data. Performance aggregations \nor precalculated summary tables are often created and managed by the OLAP cube \nengine. Consequently, cubes deliver superior query performance because of the \n",
      "content_length": 2834,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 45,
      "content": "Data Warehousing, Business Intelligence, and Dimensional Modeling Primer \n9\nprecalculations, indexing strategies, and other optimizations. Business users can \ndrill down or up by adding or removing attributes from their analyses with excellent \nperformance without issuing new queries. OLAP cubes also provide more analyti-\ncally robust functions that exceed those available with SQL. The downside is that you \npay a load performance price for these capabilities, especially with large data sets.\nDate\nDimension\nMarket\nDimension\nProduct\nDimension\nMarket\nProduct\nDate\nSales\nFacts\nFigure 1-1: Star schema versus OLAP cube.\nFortunately, most of the recommendations in this book pertain regardless of the \nrelational versus multidimensional database platform. Although the capabilities \nof OLAP technology are continuously improving, we generally recommend that \ndetailed, atomic information be loaded into a star schema; optional OLAP cubes are \nthen populated from the star schema. For this reason, most dimensional modeling \ntechniques in this book are couched in terms of a relational star schema.\nOLAP Deployment Considerations\nHere are some  things to keep in mind if you deploy data into OLAP cubes:\n \n■A star schema hosted in a relational database is a good physical foundation \nfor building an OLAP cube, and is generally regarded as a more stable basis \nfor backup and recovery.\n \n■OLAP cubes have traditionally been noted for extreme performance advan-\ntages over RDBMSs, but that distinction has become less important with \nadvances in computer hardware, such as appliances and in-memory databases, \nand RDBMS software, such as columnar databases.\n \n■OLAP cube data structures are more variable across diff erent vendors than \nrelational DBMSs, thus the ﬁ nal deployment details often depend on which \nOLAP vendor is chosen. It is typically more diffi  cult to port BI applications \nbetween diff erent OLAP tools than to port BI applications across diff erent \nrelational databases.\n",
      "content_length": 1991,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 46,
      "content": "Chapter 1\n10\n \n■OLAP cubes typically off er more sophisticated security options than RDBMSs, \nsuch as limiting access to detailed data but providing more open access to \nsummary data.\n \n■OLAP cubes off er signiﬁ cantly richer analysis capabilities than RDBMSs, \nwhich are saddled by the constraints of SQL. This may be the main justiﬁ ca-\ntion for using an OLAP product.\n \n■OLAP cubes gracefully support slowly changing dimension type 2 changes \n(which are discussed in Chapter 5: Procurement), but cubes often need to be \nreprocessed partially or totally whenever data is overwritten using alternative \nslowly changing dimension techniques.\n \n■OLAP cubes gracefully support transaction and periodic snapshot fact tables, \nbut do not handle accumulating snapshot fact tables because of the limitations \non overwriting data described in the previous point.\n \n■OLAP cubes typically support complex ragged hierarchies of indeterminate \ndepth, such as organization charts or bills of material, using native query \nsyntax that is superior to the approaches required for RDBMSs.\n \n■OLAP cubes may impose detailed constraints on the structure of dimension \nkeys that implement drill-down hierarchies compared to relational databases.\n \n■Some OLAP products do not enable dimensional roles or aliases, thus requir-\ning separate physical dimensions to be deﬁ ned.\nWe’ll return to the world of dimensional modeling in a relational platform as we \nconsider the two key components of a star schema.\n Fact Tables for Measurements\nThe fact table in  a dimensional model stores the performance measurements result-\ning from an organization’s business process events. You should strive to store the \nlow-level measurement data resulting from a business process in a single dimen-\nsional model. Because measurement data is overwhelmingly the largest set of data, \nit should not be replicated in multiple places for multiple organizational functions \naround the enterprise. Allowing business users from multiple organizations to access \na single centralized repository for each set of measurement data ensures the use of \nconsistent data throughout the enterprise.\nThe term fact represents a business measure. Imagine standing in the marketplace \nwatching products being sold and writing down the unit quantity and dollar sales \namount for each product in each sales transaction. These measurements are captured \nas products are scanned at the register, as illustrated in Figure 1-2.\nEach  row in a fact table corresponds to a measurement event. The data on each \nrow is at a speciﬁ c level of detail, referred to as the grain, such as one row per product \n",
      "content_length": 2638,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 47,
      "content": "Data Warehousing, Business Intelligence, and Dimensional Modeling Primer 11\nsold on a sales transaction. One of the core tenets of dimensional modeling is that \nall the measurement rows in a fact table must be at the same grain. Having the dis-\ncipline to create fact tables with a single level of detail ensures that measurements \naren’t inappropriately double-counted.\nTranslates into\nRetail Sales Facts\nDate Key (FK)\nProduct Key (FK)\nStore Key (FK)\nPromotion Key (FK)\nCustomer Key (FK)\nClerk Key (FK)\nTransaction #\nSales Dollars\nSales Units\nFigure 1-2: Business process measurement events translate into fact tables.\nNOTE \nThe idea that a measurement event in the physical world has a one-to-one \nrelationship to a single row in the corresponding fact table is a bedrock principle \nfor dimensional modeling. Everything else builds from this foundation.\nThe most useful facts  are numeric and additive, such as dollar sales amount. \nThroughout this book we will use dollars as the standard currency to make the \ncase study examples more tangible—you can substitute your own local currency \nif it isn’t dollars.\nAdditivity is crucial because BI applications rarely retrieve a single fact table \nrow. Rather, they bring back hundreds, thousands, or even millions of fact rows at \na time, and the most useful thing to do with so many rows is to add them up. No \nmatter how the user slices the data in Figure 1-2, the sales units and dollars sum \nto a valid total. You will see that facts are sometimes semi-additive or even non-\nadditive. Semi-additive facts, such as account balances, cannot be summed across \nthe time dimension. Non-additive facts, such as unit prices, can never be added. You \nare forced to use counts and averages or are reduced to printing out the fact rows \none at a time—an impractical exercise with a billion-row fact table.\nFacts are often described as continuously valued to help sort out what is a fact \nversus a dimension attribute. The dollar sales amount fact is continuously valued in \nthis example because it can take on virtually any value within a broad range. As an \n",
      "content_length": 2102,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 48,
      "content": "Chapter 1\n12\nobserver, you must stand out in the marketplace and wait for the measurement before \nyou have any idea what the value will be.\nIt is  theoretically possible for a measured fact to be textual; however, the condition \nrarely arises. In most cases, a textual measurement is a description of something \nand is drawn from a discrete list of values. The designer should make every eff ort to \nput textual data into dimensions where they can be correlated more eff ectively with \nthe other textual dimension attributes and consume much less space. You should \nnot store redundant textual information in fact tables. Unless the text is unique \nfor every row in the fact table, it belongs in the dimension table. A true text fact is \nrare because the unpredictable content of a text fact, like a freeform text comment, \nmakes it nearly impossible to analyze.\nReferring  to the sample fact table in Figure 1-2, if there is no sales activity for a \ngiven product, you don’t put any rows in the table. It is important that you do not \ntry to ﬁ ll the fact table with zeros representing no activity because these zeros would \noverwhelm most fact tables. By including only true activity, fact tables tend to be \nquite sparse. Despite their sparsity, fact tables usually make up 90 percent or more \nof the total space consumed by a dimensional model. Fact tables tend to be deep in \nterms of the number of rows, but narrow in terms of the number of columns. Given \ntheir size, you should be judicious about fact table space utilization.\nAs  examples are developed throughout this book, you will see that all fact table \ngrains fall into one of three categories: transaction, periodic snapshot, and accu-\nmulating snapshot. Transaction grain fact tables are the most common. We will \nintroduce transaction fact tables in Chapter 3: Retail Sales, and both periodic and \naccumulating snapshots in Chapter 4: Inventory.\n All fact tables have  two or more foreign keys (refer to the FK notation in Figure 1-2) \nthat connect to the dimension tables’ primary keys. For example, the product key in \nthe fact table always matches a speciﬁ c product key in the product dimension table. \nWhen all the keys in the fact table correctly match their respective primary keys in \nthe corresponding dimension tables, the tables satisfy referential integrity. You access \nthe fact table via the dimension tables joined to it.\nThe fact table  generally has its own primary key composed of a subset of the for-\neign keys. This key is often called a composite key. Every table that has a composite \nkey is a fact table. Fact tables express many-to-many relationships. All others are \ndimension tables.\nThere are usually a handful of dimensions that together uniquely identify each \nfact table row. After this subset of the overall dimension list has been identiﬁ ed, the \nrest of the dimensions take on a single value in the context of the fact table row’s \nprimary key. In other words, they go along for the ride.\n",
      "content_length": 2991,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 49,
      "content": "Data Warehousing, Business Intelligence, and Dimensional Modeling Primer 13\n Dimension Tables for Descriptive Context\nDimension tables  are integral companions to a fact table. The dimension tables con-\ntain the textual context associated with a business process measurement event. They \ndescribe the “who, what, where, when, how, and why” associated with the event.\nAs illustrated in Figure 1-3, dimension tables often have many columns or \nattributes. It is not uncommon for a dimension table to have 50 to 100 attributes; \nalthough, some dimension tables naturally have only a handful of attributes. \nDimension tables tend to have fewer rows than fact tables, but can be wide with \nmany large text columns. Each dimension is deﬁ ned by a single primary key (refer \nto the PK notation in Figure 1-3), which serves as the basis for referential integrity \nwith any given fact table to which it is joined.\nProduct Key (PK)\nSKU Number (Natural Key)\nProduct Description\nBrand Name\nCategory Name\nDepartment Name\nPackage Type\nPackage Size\nAbrasive Indicator\nWeight\nWeight Unit of Measure\nStorage Type\nShelf Life Type\nShelf Width\nShelf Height\nShelf Depth\n...\nProduct Dimension\nFigure 1-3: Dimension tables contain descriptive characteristics of business \nprocess nouns.\nDimension  attributes serve as the primary source of query constraints, group-\nings, and report labels. In a query or report request, attributes are identiﬁ ed as the \nby words. For example, when a user wants to see dollar sales by brand, brand must \nbe available as a dimension attribute.\nDimension table attributes play a vital role in the DW/BI system. Because they \nare the source of virtually all constraints and report labels, dimension attributes are \ncritical to making the DW/BI system usable and understandable. Attributes should \nconsist of real words rather than cryptic abbreviations. You should strive to mini-\nmize the use of codes in dimension tables by replacing them with more verbose \n",
      "content_length": 1968,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 50,
      "content": "Chapter 1\n14\ntextual attributes. You may have already trained the business users to memorize \noperational codes, but going forward, minimize their reliance on miniature notes \nattached to their monitor for code translations. You should make standard decodes \nfor the operational codes available as dimension attributes to provide consistent \nlabeling on queries, reports, and BI applications. The decode values should never be \nburied in the reporting applications where inconsistency is inevitable.\nSometimes operational codes or identiﬁ ers have legitimate business signiﬁ cance \nto users or are required to communicate back to the operational world. In these \ncases, the codes should appear as explicit dimension attributes, in addition to the \ncorresponding user-friendly textual descriptors. Operational codes sometimes have \nintelligence embedded in them. For example, the ﬁ rst two digits may identify the \nline of business, whereas the next two digits may identify the global region. Rather \nthan forcing users to interrogate or ﬁ lter on substrings within the operational codes, \npull out the embedded meanings and present them to users as separate dimension \nattributes that can easily be ﬁ ltered, grouped, or reported.\nIn many ways, the data warehouse is only as good as the dimension attributes; the \nanalytic power of the DW/BI environment is directly proportional to the quality and \ndepth of the dimension attributes. The more time spent providing attributes with \nverbose business terminology, the better. The more time spent populating the domain \nvalues in an attribute column, the better. The more time spent ensuring the quality \nof the values in an attribute column, the better. Robust dimension attributes deliver \nrobust analytic slicing-and-dicing capabilities.\nNOTE \nDimensions provide the entry points to the data, and the ﬁ nal labels and \ngroupings on all DW/BI analyses.\nWhen triaging operational source data, it is sometimes unclear whether a \nnumeric data element is a fact or dimension attribute. You often make the decision \nby asking whether the column is a measurement that takes on lots of values and \nparticipates in calculations (making it a fact) or is a discretely valued description \nthat is more or less constant and participates in constraints and row labels (making \nit a dimensional attribute). For example, the standard cost for a product seems like \na constant attribute of the product but may be changed so often that you decide it \nis more like a measured fact. Occasionally, you can’t be certain of the classiﬁ cation; \nit is possible to model the data element either way (or both ways) as a matter of the \ndesigner’s prerogative.\nNOTE \nThe designer’s dilemma of whether a numeric quantity is a fact or a \ndimension attribute is rarely a diffi  cult decision. Continuously valued numeric \n",
      "content_length": 2840,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 51,
      "content": "Data Warehousing, Business Intelligence, and Dimensional Modeling Primer 15\nobservations are almost always facts; discrete numeric observations drawn from a \nsmall list are almost always dimension attributes.\nFigure 1-4  shows that dimension tables often represent hierarchical relation-\nships. For example, products roll up into brands and then into categories. For each \nrow in the product dimension, you should store the associated brand and category \ndescription. The hierarchical descriptive information is stored redundantly in the \nspirit of ease of use and query performance. You should resist the perhaps habitual \nurge to normalize data by storing only the brand code in the product dimension and \ncreating a separate brand lookup table, and likewise for the category description in a \nseparate category lookup table. This normalization is called snowﬂ aking. Instead of \nthird normal form, dimension tables typically are highly denormalized with ﬂ attened \nmany-to-one relationships within a single dimension table. Because dimension tables \ntypically are geometrically smaller than fact tables, improving storage effi  ciency by \nnormalizing or snowﬂ aking has virtually no impact on the overall database size. You \nshould almost always trade off  dimension table space for simplicity and accessibility.\n1\n2\n3 \n4 \n5 \n6 \n7 \n8 \n9 \n10 \n11\nProduct Key\nPowerAll 20 oz\nPowerAll 32 oz\nPowerAll 48 oz\nPowerAll 64 oz\nZipAll 20 oz\nZipAll 32 oz\nZipAll 48 oz\nShiny 20 oz\nShiny 32 oz\nZipGlass 20 oz\nZipGlass 32 oz\nPowerClean\nPowerClean\nPowerClean\nPowerClean\nZippy\nZippy\nZippy\nClean Fast\nClean Fast\nZippy\nZippy\nAll Purpose Cleaner\nAll Purpose Cleaner\nAll Purpose Cleaner\nAll Purpose Cleaner\nAll Purpose Cleaner\nAll Purpose Cleaner\nAll Purpose Cleaner\nGlass Cleaner\nGlass Cleaner\nGlass Cleaner\nGlass Cleaner\nProduct Description\nBrand Name\nCategory Name\nFigure 1-4: Sample rows from a dimension table with denormalized hierarchies.\nContrary to popular folklore, Ralph Kimball didn’t invent the terms  fact and \ndimension. As best as can be determined, the dimension and fact terminology \noriginated from a joint research project conducted by General Mills and Dartmouth \nUniversity in the 1960s. In the 1970s, both AC Nielsen and IRI used the terms con-\nsistently to describe their syndicated data off erings and gravitated to dimensional \nmodels for simplifying the presentation of their analytic information. They under-\nstood that their data wouldn’t be used unless it was packaged simply. It is probably \naccurate to say that no single person invented the dimensional approach. It is an \nirresistible force in designing databases that always results when the designer places \nunderstandability and performance as the highest goals.\n",
      "content_length": 2732,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 52,
      "content": "Chapter 1\n16\nFacts and Dimensions Joined in a Star Schema\nNow  that you understand fact and dimension tables, it’s time to bring the building blocks \ntogether in a dimensional model, as shown in Figure 1-5. Each business process is repre-\nsented by a dimensional model that consists of a fact table containing the event’s numeric \nmeasurements surrounded by a halo of dimension tables that contain the textual context \nthat was true at the moment the event occurred. This characteristic star-like structure \nis often called a star join, a term dating back to the earliest days of relational databases.\nRetail Sales Fact\nDate Key (FK)\nProduct Key (FK)\nStore Key (FK)\nPromotion Key (FK)\nCustomer Key (FK)\nClerk Key (FK)\nTransaction #\nSales Dollars\nSales Units\nDate Dimension\nProduct Dimension\nPromotion Dimension\nClerk Dimension\nStore Dimension\nCustomer Dimension\nFigure 1-5: Fact and dimension tables in a dimensional model.\nThe ﬁ rst thing to notice about the dimensional schema is its simplicity and \nsymmetry. Obviously, business users beneﬁ t from the simplicity because the data \nis easier to understand and navigate. The charm of the design in Figure 1-5 is that \nit is highly recognizable to business users. We have observed literally hundreds of \ninstances in which users immediately agree that the dimensional model is their \nbusiness. Furthermore, the reduced number of tables and use of meaningful busi-\nness descriptors make it easy to navigate and less likely that mistakes will occur.\nThe  simplicity of a dimensional model also has performance beneﬁ ts. Database \noptimizers process these simple schemas with fewer joins more effi  ciently. A data-\nbase engine can make strong assumptions about ﬁ rst constraining the heavily \nindexed dimension tables, and then attacking the fact table all at once with the \nCartesian product of the dimension table keys satisfying the user’s constraints. \nAmazingly, using this approach, the optimizer can evaluate arbitrary n-way joins \nto a fact table in a single pass through the fact table’s index.\nFinally,  dimensional models are gracefully extensible to accommodate change. \nThe predictable framework of a dimensional model withstands unexpected changes \nin user behavior. Every dimension is equivalent; all dimensions are symmetrically-\nequal entry points into the fact table. The dimensional model has no built-in bias \nregarding expected query patterns. There are no preferences for the business ques-\ntions asked this month versus the questions asked next month. You certainly don’t \nwant to adjust schemas if business users suggest new ways to analyze their business. \n",
      "content_length": 2630,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 53,
      "content": "Data Warehousing, Business Intelligence, and Dimensional Modeling Primer 17\nThis  book illustrates repeatedly that the most granular or atomic data has the \nmost dimensionality. Atomic data that has not been aggregated is the most expres-\nsive data; this atomic data should be the foundation for every fact table design to \nwithstand business users’ ad hoc attacks in which they pose unexpected queries. \nWith dimensional models, you can add completely new dimensions to the schema \nas long as a single value of that dimension is deﬁ ned for each existing fact row. \nLikewise, you can add new facts to the fact table, assuming that the level of detail \nis consistent with the existing fact table. You can supplement preexisting dimen-\nsion tables with new, unanticipated attributes. In each case, existing tables can be \nchanged in place either by simply adding new data rows in the table or by executing \nan SQL ALTER TABLE command. Data would not need to be reloaded, and existing BI \napplications would continue to run without yielding diff erent results. We examine \nthis graceful extensibility of dimensional models more fully in Chapter 3.\nAnother  way to think about the complementary nature of fact and dimension \ntables is to see them translated into a report. As illustrated in Figure 1-6, dimension \nattributes supply the report ﬁ lters and labeling, whereas the fact tables supply the \nreport’s numeric values. \nProduct Dimension\nDate Dimension\nStore Dimension\nSales Fact\nProduct Key (PK)\nSKU Number (Natural Key)\nProduct Description\nPackage Type\nPackage Size\nBrand Name\nCategory Name\n... and more\nDate Key (FK)\nProduct Key (FK)\nStore Key (FK)\n...\nTransaction #\nSales Dollars\nSales Units\nStore Key (PK)\nStore Number\nStore Name\nStore State\nStore ZIP\nDistrict\nRegion\n... and more\nDate Key (PK)\nDate\nDay of Week\nMonth\nYear\n...and more\nFilter\nSum\nGroup by\nGroup by\nSales Activity for June 2013\nDistrict\nAtherton\nAtherton\nBelmont\nBelmont\nBrand Name\nPowerClean\nZippy\nClean Fast\nZippy\nSales Dollars\n2,035\n707\n2,330\n527\nFigure 1-6: Dimensional attributes and facts form a simple report.\n",
      "content_length": 2091,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 54,
      "content": "Chapter 1\n18\nYou can easily envision the SQL that’s written (or more likely generated by a BI \ntool) to create this report:\nSELECT\n     store.district_name,\n     product.brand,\n     sum(sales_facts.sales_dollars) AS \"Sales Dollars\"\nFROM\n     store,\n     product,\n     date,\n     sales_facts\nWHERE\n     date.month_name=\"January\" AND\n     date.year=2013 AND\n     store.store_key = sales_facts.store_key AND \n     product.product_key = sales_facts.product_key AND\n     date.date_key = sales_facts.date_key\nGROUP BY\n     store.district_name,\n     product.brand\nIf  you study this code snippet line-by-line, the ﬁ rst two lines under the SELECT \nstatement identify the dimension attributes in the report, followed by the aggre-\ngated metric from the fact table. The FROM clause identiﬁ es all the tables involved \nin the query. The ﬁ rst two lines in the WHERE clause declare the report’s ﬁ lter, and \nthe remainder declare the joins between the dimension and fact tables. Finally, the \nGROUP BY clause establishes the aggregation within the report.\nKimball’s DW/BI Architecture\nLet’s build on your understanding of DW/BI systems and dimensional modeling \nfundamentals by investigating the components of a DW/BI environment based on \nthe Kimball architecture. You need to learn the strategic signiﬁ cance of each com-\nponent to avoid confusing their role and function.\nAs  illustrated in Figure 1-7, there are four separate and distinct components to \nconsider in the DW/BI environment: operational source systems, ETL system, data \npresentation area, and business intelligence applications. \nOperational Source Systems\nThese  are the operational systems of record that capture the business’s transactions. \nThink of the source systems as outside the data warehouse because presumably you \nhave little or no control over the content and format of the data in these operational \nsystems. The main priorities of the source systems are processing performance and avail-\nability. Operational queries against source systems are narrow, one-record-at-a-time \n",
      "content_length": 2048,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 55,
      "content": "Data Warehousing, Business Intelligence, and Dimensional Modeling Primer 19\nqueries that are part of the normal transaction ﬂ ow and severely restricted in their \ndemands on the operational system. It is safe to assume that source systems are not \nqueried in the broad and unexpected ways that DW/BI systems typically are queried. \nSource systems maintain little historical data; a good data warehouse can relieve \nthe source systems of much of the responsibility for representing the past. In many \ncases, the source systems are special purpose applications without any commitment \nto sharing common data such as product, customer, geography, or calendar with other \noperational systems in the organization. Of course, a broadly adopted cross-application \nenterprise resource planning (ERP) system or operational master data management \nsystem could help address these shortcomings.\nSource\nTransactions\nBack Room\nETL System:\n•  Transform from\n \nsource-to-target\n• Conform\n \ndimensions\n• Normalization\n \noptional\n•  No user query\n \nsupport\nDesign Goals:\n•  Throughput\n• Integrity and\n \nconsistency\nPresentation Area:\n•  Dimensional (star\n \nschema or OLAP\n \ncube)\n• Atomic and\n \nsummary data\n• Organized by\n \nbusiness process\n•  Uses conformed\n \ndimensions\nDesign Goals:\n•  Ease-of-use\n• Query performance\nBI Applications:\n•  Ad hoc queries\n• Standard reports\n• Analytic apps\n•  Data mining and\n \nmodels\nEnterprise DW Bus\nArchitecture\nFront Room\nFigure 1-7: Core elements of the Kimball DW/BI architecture.\nExtract, Transformation, and Load System\nThe extract, transformation, and load (ETL)  system of the DW/BI environment consists \nof a work area, instantiated data structures, and a set of processes. The ETL system \nis everything between the operational source systems and the DW/BI presentation \narea. We elaborate on the architecture of ETL systems and associated techniques \nin Chapter 19: ETL Subsystems and Techniques, but we want to introduce this \nfundamental piece of the overall DW/BI system puzzle.\nExtraction  is the ﬁ rst step in the process of getting data into the data warehouse \nenvironment. Extracting means reading and understanding the source data and \ncopying the data needed into the ETL system for further manipulation. At this \npoint, the data belongs to the data warehouse.\nAfter the data is extracted to the ETL system, there are numerous potential trans-\nformations, such as cleansing the data (correcting misspellings, resolving domain \n",
      "content_length": 2468,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 56,
      "content": "Chapter 1\n20\nconﬂ icts, dealing with missing elements, or parsing into standard formats), com-\nbining data from multiple sources, and de-duplicating data. The ETL system adds \nvalue to the data with these cleansing and conforming tasks by changing the data \nand enhancing it. In addition, these activities can be architected to create diagnos-\ntic metadata, eventually leading to business process reengineering to improve data \nquality in the source systems over time.\nThe ﬁ nal step of the ETL process is the physical structuring and loading of data \ninto the presentation area’s target dimensional models. Because the primary mis-\nsion of the ETL system is to hand off  the dimension and fact tables in the delivery \nstep, these subsystems are critical. Many of these deﬁ ned subsystems focus on \ndimension table processing, such as surrogate key assignments, code lookups to \nprovide appropriate descriptions, splitting, or combining columns to present the \nappropriate data values, or joining underlying third normal form table structures \ninto ﬂ attened denormalized dimensions. In contrast, fact tables are typically large \nand time consuming to load, but preparing them for the presentation area is typically \nstraightforward. When the dimension and fact tables in a dimensional model have \nbeen updated, indexed, supplied with appropriate aggregates, and further quality \nassured, the business community is notiﬁ ed that the new data has been published.\nThere remains industry consternation about whether the data in the ETL system \nshould be repurposed into physical normalized structures prior to loading into the \npresentation area’s dimensional structures for querying and reporting. The ETL \nsystem is typically dominated by the simple activities of sorting and sequential \nprocessing. In many cases, the ETL system is not based on relational technology but \ninstead may rely on a system of ﬂ at ﬁ les. After validating the data for conformance \nwith the deﬁ ned one-to-one and many-to-one business rules, it may be pointless to \ntake the ﬁ nal step of building a 3NF physical database, just before transforming the \ndata once again into denormalized structures for the BI presentation area.\nHowever, there are cases in which the data arrives at the doorstep of the ETL \nsystem in a 3NF relational format. In these situations, the ETL system develop-\ners may be more comfortable performing the cleansing and transformation tasks \nusing normalized structures. Although a normalized database for ETL processing \nis acceptable, we have some reservations about this approach. The creation of both \nnormalized structures for the ETL and dimensional structures for presentation \nmeans that the data is potentially extracted, transformed, and loaded twice—once \ninto the normalized database and then again when you load the dimensional model. \nObviously, this two-step process requires more time and investment for the develop-\nment, more time for the periodic loading or updating of data, and more capacity to \nstore the multiple copies of the data. At the bottom line, this typically translates into \nthe need for larger development, ongoing support, and hardware platform budgets. \n",
      "content_length": 3191,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 57,
      "content": "Data Warehousing, Business Intelligence, and Dimensional Modeling Primer 21\nUnfortunately, some DW/BI initiatives have failed miserably because they focused \nall their energy and resources on constructing the normalized structures rather \nthan allocating time to developing a dimensional presentation area that supports \nimproved business decision making. Although enterprise-wide data consistency is a \nfundamental goal of the DW/BI environment, there may be eff ective and less costly \napproaches than physically creating normalized tables in the ETL system, if these \nstructures don’t already exist.\nNOTE \nIt is acceptable to create a normalized database to support the ETL \nprocesses; however, this is not the end goal. The normalized structures must be \noff -limits to user queries because they defeat the twin goals of understandability \nand performance.\n Presentation Area to Support Business Intelligence\nThe DW/BI  presentation area is where data is organized, stored, and made available \nfor direct querying by users, report writers, and other analytical BI applications. \nBecause the back room ETL system is off -limits, the presentation area is the DW/BI \nenvironment as far as the business community is concerned; it is all the business \nsees and touches via their access tools and BI applications. The original pre-release \nworking title for the ﬁ rst edition of The Data Warehouse Toolkit was Getting the Data \nOut. This is what the presentation area with its dimensional models is all about.\nWe have several strong opinions about the presentation area. First of all, we insist \nthat the data be presented, stored, and accessed in dimensional schemas, either \nrelational star schemas or OLAP cubes. Fortunately, the industry has matured to the \npoint where we’re no longer debating this approach; it has concluded that dimen-\nsional modeling is the most viable technique for delivering data to DW/BI users.\nOur second stake in the ground about the presentation area is that it must \ncontain detailed, atomic data. Atomic data is required to withstand assaults from \nunpredictable ad hoc user queries. Although the presentation area also may contain \nperformance-enhancing aggregated data, it is not suffi  cient to deliver these sum-\nmaries without the underlying granular data in a dimensional form. In other words, \nit is completely unacceptable to store only summary data in dimensional models \nwhile the atomic data is locked up in normalized models. It is impractical to expect \na user to drill down through dimensional data almost to the most granular level and \nthen lose the beneﬁ ts of a dimensional presentation at the ﬁ nal step. Although DW/\nBI users and applications may look infrequently at a single line item on an order, \nthey may be very interested in last week’s orders for products of a given size (or \nﬂ avor, package type, or manufacturer) for customers who ﬁ rst purchased within \n",
      "content_length": 2918,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 58,
      "content": "Chapter 1\n22\nthe last 6 months (or reside in a given state or have certain credit terms). The most \nﬁ nely grained data must be available in the presentation area so that users can ask \nthe most precise questions possible. Because users’ requirements are unpredictable \nand constantly changing, you must provide access to the exquisite details so they \ncan roll up to address the questions of the moment.\nThe presentation data area should be structured around business process mea-\nsurement events. This approach naturally aligns with the operational source data \ncapture systems. Dimensional models should correspond to physical data capture \nevents; they should not be designed to deliver the report-of-the-day. An enterprise’s \nbusiness processes cross the boundaries of organizational departments and func-\ntions. In other words, you should construct a single fact table for atomic sales metrics \nrather than populating separate similar, but slightly diff erent, databases containing \nsales metrics for the sales, marketing, logistics, and ﬁ nance teams.\nAll  the dimensional structures must be built using common, conformed dimen-\nsions. This is the basis of the enterprise data warehouse bus architecture described \nin Chapter 4. Adherence to the bus architecture is the ﬁ nal stake in the ground \nfor the presentation area. Without shared, conformed dimensions, a dimensional \nmodel becomes a standalone application. Isolated stovepipe data sets that cannot be \ntied together are the bane of the DW/BI movement as they perpetuate incompatible \nviews of the enterprise. If you have any hope of building a robust and integrated \nDW/BI environment, you must commit to the enterprise bus architecture. When \ndimensional models have been designed with conformed dimensions, they can be \nreadily combined and used together. The presentation area in a large enterprise \nDW/BI solution ultimately consists of dozens of dimensional models with many of \nthe associated dimension tables shared across fact tables.\nUsing the bus architecture is the secret to building distributed DW/BI systems. \nWhen the bus architecture is used as a framework, you can develop the enterprise \ndata warehouse in an agile, decentralized, realistically scoped, iterative manner.\nNOTE \nData in the queryable presentation area of the DW/BI system must be \ndimensional, atomic (complemented by performance-enhancing aggregates), busi-\nness process-centric, and adhere to the enterprise data warehouse bus architecture. \nThe data must not be structured according to individual departments’ interpreta-\ntion of the data.\nBusiness Intelligence Applications\nThe  ﬁ nal major component of the Kimball DW/BI architecture is the business intelligence \n(BI) application. The term BI application loosely refers to the range of capabilities pro-\nvided to business users to leverage the presentation area for analytic decision making. \n",
      "content_length": 2899,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 59,
      "content": "Data Warehousing, Business Intelligence, and Dimensional Modeling Primer 23\nBy definition, all BI applications query the data in the DW/BI presentation area. \nQuerying, obviously, is the whole point of using data for improved decision making.\nA BI application can be as simple as an ad hoc query tool or as complex as a sophis-\nticated data mining or modeling application. Ad hoc query tools, as powerful as they \nare, can be understood and used eff ectively by only a small percentage of the potential \nDW/BI business user population. Most business users will likely access the data via \nprebuilt parameter-driven applications and templates that do not require users to con-\nstruct queries directly. Some of the more sophisticated applications, such as modeling \nor forecasting tools, may upload results back into the operational source systems, ETL \nsystem, or presentation area.\nRestaurant Metaphor for the Kimball Architecture\nOne of  our favorite metaphors reinforces the importance of separating the overall \nDW/BI environment into distinct components. In this case, we’ll consider the simi-\nlarities between a restaurant and the DW/BI environment.\nETL in the Back Room Kitchen\nThe ETL system is analogous to the kitchen of a restaurant. The restaurant’s kitchen \nis a world unto itself. Talented chefs take raw materials and transform them into \nappetizing, delicious meals for the restaurant’s diners. But long before a commercial \nkitchen swings into operation, a signiﬁ cant amount of planning goes into designing \nthe workspace layout and components.\nThe kitchen is organized with several design goals in mind. First, the layout must \nbe highly effi  cient. Restaurant managers want high kitchen throughput. When the \nrestaurant is packed and everyone is hungry, there is no time for wasted movement. \nDelivering consistent quality from the restaurant’s kitchen is the second important \ngoal. The establishment is doomed if the plates coming out of the kitchen repeat-\nedly fail to meet expectations. To achieve consistency, chefs create their special \nsauces once in the kitchen, rather than sending ingredients out to the table where \nvariations will inevitably occur. Finally, the kitchen’s output, the meals delivered \nto restaurant customers, must also be of high integrity. You wouldn’t want someone \nto get food poisoning from dining at your restaurant. Consequently, kitchens are \ndesigned with integrity in mind; salad preparation doesn’t happen on the same \nsurfaces where raw chicken is handled.\nJust as quality, consistency, and integrity are major considerations when designing \nthe restaurant’s kitchen, they are also ongoing concerns for everyday management \nof the restaurant. Chefs strive to obtain the best raw materials possible. Procured \nproducts must meet quality standards and are rejected if they don’t meet minimum \nstandards. Most ﬁ ne restaurants modify their menus based on the availability of \nquality ingredients.\n",
      "content_length": 2955,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 60,
      "content": "Chapter 1\n24\nThe restaurant staff s its kitchen with skilled professionals wielding the tools of \ntheir trade. Cooks manipulate razor-sharp knives with incredible conﬁ dence and \nease. They operate powerful equipment and work around extremely hot surfaces \nwithout incident.\nGiven the dangerous surroundings, the back room kitchen is off  limits to res-\ntaurant patrons. Things happen in the kitchen that customers just shouldn’t see. It \nsimply isn’t safe. Professional cooks handling sharp knives shouldn’t be distracted \nby diners’ inquiries. You also wouldn’t want patrons entering the kitchen to dip their \nﬁ ngers into a sauce to see whether they want to order an entree. To prevent these \nintrusions, most restaurants have a closed door that separates the kitchen from the \narea where diners are served. Even restaurants that boast an open kitchen format \ntypically have a barrier, such as a partial wall of glass, separating the two environ-\nments. Diners are invited to watch but can’t wander into the kitchen. Although part \nof the kitchen may be visible, there are always out-of-view back rooms where the \nless visually desirable preparation occurs. \nThe data warehouse’s ETL system resembles the restaurant’s kitchen. Source data \nis magically transformed into meaningful, presentable information. The back room \nETL system must be laid out and architected long before any data is extracted from \nthe source. Like the kitchen, the ETL system is designed to ensure throughput. \nIt must transform raw source data into the target model effi  ciently, minimizing \nunnecessary movement.\nObviously, the ETL system is also highly concerned about data quality, integrity, and \nconsistency. Incoming data is checked for reasonable quality as it enters. Conditions \nare continually monitored to ensure ETL outputs are of high integrity. Business rules \nto consistently derive value-add metrics and attributes are applied once by skilled \nprofessionals in the ETL system rather than relying on each patron to develop them \nindependently. Yes, that puts extra burden on the ETL team, but it’s done to deliver a \nbetter, more consistent product to the DW/BI patrons.\nNOTE \nA properly designed DW/BI environment trades off  work in the front \nroom BI applications in favor of work in the back room ETL system. Front room \nwork must be done over and over by business users, whereas back room work is \ndone once by the ETL staff .\nFinally, ETL system should be off  limits to the business users and BI application \ndevelopers. Just as you don’t want restaurant patrons wandering into the kitchen \nand potentially consuming semi-cooked food, you don’t want busy ETL profession-\nals distracted by unpredictable inquiries from BI users. The consequences might \nbe highly unpleasant if users dip their ﬁ ngers into interim staging pots while data \npreparation is still in process. As with the restaurant kitchen, activities occur in \n",
      "content_length": 2926,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 61,
      "content": "Data Warehousing, Business Intelligence, and Dimensional Modeling Primer 25\nthe ETL system that the DW/BI patrons shouldn’t see. When the data is ready and \nquality checked for user consumption, it’s brought through the doorway into the \nDW/BI presentation area.\nData Presentation and BI in the Front Dining Room\nNow turn your attention to the restaurant’s dining room. What are the key fac-\ntors that diff erentiate restaurants? According to the popular restaurant ratings and \nreviews, restaurants are typically scored on four distinct qualities:\n \n■Food (quality, taste, and presentation)\n \n■Decor (appealing, comfortable surroundings for the patrons) \n \n■Service (prompt food delivery, attentive support staff , and food received \nas ordered) \n \n■Cost\nMost patrons focus initially on the food score when they’re evaluating dining \noptions. First and foremost, does the restaurant serve good food? That’s the res-\ntaurant’s primary deliverable. However, the decor, service, and cost factors also \naff ect the patrons’ overall dining experience and are considerations when evaluating \nwhether to eat at a restaurant. \nOf course, the primary deliverable from the DW/BI kitchen is the data in \nthe presentation area. What data is available? Like the restaurant, the DW/BI \nsystem provides “menus” to describe what’s available via metadata, published \nreports, and parameterized analytic applications. The DW/BI patrons expect con-\nsistency and high quality. The presentation area’s data must be properly prepared \nand safe to consume.\nThe presentation area’s decor should be organized for the patrons’ comfort. It \nmust be designed based on the preferences of the BI diners, not the development \nstaff . Service is also critical in the DW/BI system. Data must be delivered, as ordered, \npromptly in a form that is appealing to the business user or BI application developer.\nFinally, cost is a factor for the DW/BI system. The kitchen staff  may be dream-\ning up elaborate, expensive meals, but if there’s no market at that price point, the \nrestaurant won’t survive.\nIf restaurant patrons like their dining experience, then everything is rosy for \nthe restaurant manager. The dining room is always busy; sometimes there’s even \na waiting list. The restaurant manager’s performance metrics are all promising: \nhigh numbers of diners, table turnovers, and nightly revenue and proﬁ t, while staff  \nturnover is low. Things look so good that the restaurant’s owner is considering an \nexpansion site to handle the traffi  c. On the other hand, if the restaurant’s diners \naren’t happy, things go downhill in a hurry. With a limited number of patrons, \nthe restaurant isn’t making enough money to cover its expenses, and the staff  isn’t \nmaking any tips. In a relatively short time, the restaurant closes.\n",
      "content_length": 2801,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 62,
      "content": "Chapter 1\n26\nRestaurant managers often proactively check on their diners’ satisfaction with \nthe food and dining experience. If a patron is unhappy, they take immediate action \nto rectify the situation. Similarly, DW/BI managers should proactively monitor sat-\nisfaction. You can’t aff ord to wait to hear complaints. Often, people will abandon \na restaurant without even voicing their concerns. Over time, managers notice that \ndiner counts have dropped but may not even know why.\nInevitably, the prior DW/BI patrons will locate another “restaurant” that bet-\nter suits their needs and preferences, wasting the millions of dollars invested to \ndesign, build, and staff  the DW/BI system. Of course, you can prevent this unhappy \nending by managing the restaurant proactively; make sure the kitchen is properly \norganized and utilized to deliver as needed to the presentation area’s food, decor, \nservice, and cost.\nAlternative DW/BI Architectures\nHaving  just described the Kimball architecture, let’s discuss several other DW/BI \narchitectural approaches. We’ll quickly review the two dominant alternatives to the \nKimball architecture, highlighting the similarities and diff erences. We’ll then close \nthis section by focusing on a hybrid approach that combines alternatives.\nFortunately, over the past few decades, the diff erences between the Kimball \narchitecture and the alternatives have softened. Even more fortunate, there’s a role \nfor dimensional modeling regardless of your architectural predisposition.\nWe acknowledge that organizations have successfully constructed DW/BI systems \nbased on the approaches advocated by others. We strongly believe that rather than \nencouraging more consternation over our philosophical diff erences, the industry \nwould be far better off  devoting energy to ensure that our DW/BI deliverables are \nbroadly accepted by the business to make better, more informed decisions. The \narchitecture should merely be a means to this objective.\nIndependent Data Mart Architecture\nWith  this approach, analytic data is deployed on a departmental basis without con-\ncern to sharing and integrating information across the enterprise, as illustrated in \nFigure 1-8. Typically, a single department identiﬁ es requirements for data from an \noperational source system. The department works with IT staff  or outside consul-\ntants to construct a database that satisﬁ es their departmental needs, reﬂ ecting their \nbusiness rules and preferred labeling. Working in isolation, this departmental data \nmart addresses the department’s analytic requirements.\nMeanwhile, another department is interested in the same source data. It’s extremely \ncommon for multiple departments to be interested in the same performance met-\nrics resulting from an organization’s core business process events. But because this \n",
      "content_length": 2831,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 63,
      "content": "Data Warehousing, Business Intelligence, and Dimensional Modeling Primer 27\ndepartment doesn’t have access to the data mart initially constructed by the other \ndepartment, it proceeds down a similar path on its own, obtaining resources and \nbuilding a departmental solution that contains similar, but slightly diff erent data. \nWhen business users from these two departments discuss organizational perfor-\nmance based on reports from their respective repositories, not surprisingly, none of \nthe numbers match because of the diff erences in business rules and labeling.\nSource\nTransactions\nBack Room\nBI Applications for\nDepartment #1\nData Mart for\nDepartment #1\nData Mart for\nDepartment #2\nData Mart for\nDepartment #3\nBI Applications for\nDepartment #2\nBI Applications for\nDepartment #3\nFront Room\nETL\nETL\nETL\nETL\nETL\nFigure 1-8: Simpliﬁ ed illustration of the independent data mart “architecture.”\nThese standalone analytic silos represent a DW/BI “architecture” that’s essen-\ntially un-architected. Although no industry leaders advocate these independent \ndata marts, this approach is prevalent, especially in large organizations. It mirrors \nthe way many organizations fund IT projects, plus it requires zero cross-organi-\nzational data governance and coordination. It’s the path of least resistance for fast \ndevelopment at relatively low cost, at least in the short run. Of course, multiple \nuncoordinated extracts from the same operational sources and redundant storage \nof analytic data are ineffi  cient and wasteful in the long run. Without any enterprise \nperspective, this independent approach results in myriad standalone point solutions \nthat perpetuate incompatible views of the organization’s performance, resulting in \nunnecessary organizational debate and reconciliation.\nWe strongly discourage the independent data mart approach. However, often \nthese independent data marts have embraced dimensional modeling because they’re \ninterested in delivering data that’s easy for the business to understand and highly \nresponsive to queries. So our concepts of dimensional modeling are often applied \nin this architecture, despite the complete disregard for some of our core tenets, such \nas focusing on atomic details, building by business process instead of department, \nand leveraging conformed dimensions for enterprise consistency and integration.\n",
      "content_length": 2363,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 64,
      "content": "Chapter 1\n28\nHub-and-Spoke Corporate Information Factory \nInmon Architecture\nThe  hub-and-spoke Corporate Information Factory (CIF) approach is advocated \nby Bill Inmon and others in the industry. Figure 1-9 illustrates a simpliﬁ ed version \nof the CIF, focusing on the core elements and concepts that warrant discussion.\nSource\nTransactions\nBack Room\nEnterprise Data\nWarehouse (EDW)\n•  Normalized\n \ntables (3NF)\n• Atomic data\n• User queryable\nFront Room\nD\na\nt\na\n \nA\nc\nq\nu\ni\ns\ni\nt\ni\no\nn\nD\na\nt\na\n \nD\ne\nl\ni\nv\ne\nr\ny\nB\nI\n \nA\np\np\nl\ni\nc\na\nt\ni\no\nn\ns\nData Marts:\n•  Dimensional\n• Often\n \nsummarized\n• Often\n \ndepartmental\nFigure 1-9: Simpliﬁ ed illustration of the hub-and-spoke Corporate Information Factory \narchitecture.\nWith the CIF, data is extracted from the operational source systems and processed \nthrough an ETL system sometimes referred to as data acquisition. The atomic data \nthat results from this processing lands in a 3NF database; this normalized, atomic \nrepository is referred to as the Enterprise Data Warehouse (EDW) within the CIF \narchitecture. Although the Kimball architecture enables optional normalization to \nsupport ETL processing, the normalized EDW is a mandatory construct in the CIF. \nLike the Kimball approach, the CIF advocates enterprise data coordination and inte-\ngration. The CIF says the normalized EDW ﬁ lls this role, whereas the Kimball archi-\ntecture stresses the importance of an enterprise bus with conformed dimensions.\nNOTE \nThe process of normalization does not technically speak to integration. \nNormalization  simply creates physical tables that implement many-to-one rela-\ntionships. Integration, on the other hand, requires that inconsistencies arising \nfrom separate sources be resolved. Separate incompatible database sources can be \nnormalized to the hilt without addressing integration. The Kimball architecture \n",
      "content_length": 1862,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 65,
      "content": "Data Warehousing, Business Intelligence, and Dimensional Modeling Primer 29\nbased on conformed dimensions reverses this logic and focuses on resolving data \ninconsistencies without explicitly requiring normalization.\nOrganizations who have adopted the CIF approach often have business users \naccessing the EDW repository due to its level of detail or data availability timeli-\nness. However, subsequent ETL data delivery processes also populate downstream \nreporting and analytic environments to support business users. Although often \ndimensionally structured, the resultant analytic databases typically diff er from \nstructures in the Kimball architecture’s presentation area in that they’re frequently \ndepartmentally-centric (rather than organized around business processes) and popu-\nlated with aggregated data (rather than atomic details). If the data delivery ETL \nprocesses apply business rules beyond basic summarization, such as departmental \nrenaming of columns or alternative calculations, it may be diffi  cult to tie these \nanalytic databases to the EDW’s atomic repository.\nNOTE \nThe most extreme form of a pure CIF architecture is unworkable as a data \nwarehouse, in our opinion. Such an architecture locks the atomic data in diffi  cult-\nto-query normalized structures, while delivering departmentally incompatible data \nmarts to diff erent groups of business users. But before being too depressed by this \nview, stay tuned for the next section.\nHybrid Hub-and-Spoke and Kimball Architecture\nThe  ﬁ nal architecture warranting discussion is the marriage of the Kimball and \nInmon CIF architectures. As illustrated in Figure 1-10, this architecture populates \na CIF-centric EDW that is completely off -limits to business users for analysis and \nreporting. It’s merely the source to populate a Kimball-esque presentation area \nin which the data is dimensional, atomic (complemented by aggregates), process-\ncentric, and conforms to the enterprise data warehouse bus architecture.\nSome proponents of this blended approach claim it’s the best of both worlds. Yes, it \nblends the two enterprise-oriented approaches. It may leverage a preexisting invest-\nment in an integrated repository, while addressing the performance and usability \nissues associated with the 3NF EDW by offl  oading queries to the dimensional presen-\ntation area. And because the end deliverable to the business users and BI applications \nis constructed based on Kimball tenets, who can argue with the approach?\nIf you’ve already invested in the creation of a 3NF EDW, but it’s not delivering \non the users’ expectations of fast and ﬂ exible reporting and analysis, this hybrid \napproach might be appropriate for your organization. If you’re starting with a blank \nsheet of paper, the hybrid approach will likely cost more time and money, both dur-\ning development and ongoing operation, given the multiple movements of data and \n",
      "content_length": 2913,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 66,
      "content": "Chapter 1\n30\nredundant storage of atomic details. If you have the appetite, the perceived need, and \nperhaps most important, the budget and organizational patience to fully normalize \nand instantiate your data before loading it into dimensional structures that are well \ndesigned according to the Kimball methods, go for it.\nSource\nTransactions\nBack Room\nETL\nETL\nPresentation Area:\n•  Dimensional (star\n \nschema or OLAP\n \ncube)\n• Atomic and\n \nsummary data\n• Organized by\n \nbusiness process\n•  Uses conformed\n \ndimensions\nEnterprise DW Bus\nArchitecture\nFront Room\nEnterprise Data\nWarehouse (EDW)\n•  Normalized\n \ntables (3NF)\n• Atomic data\nB\nI\n \nA\np\np\nl\ni\nc\na\nt\ni\no\nn\ns\nFigure 1-10: Hybrid architecture with 3NF structures and dimensional Kimball \npresentation area.\nDimensional Modeling Myths\nDespite  the widespread acceptance of dimensional modeling, some misperceptions \npersist in the industry. These false assertions are a distraction, especially when you \nwant to align your team around common best practices. If folks in your organiza-\ntion continually lob criticisms about dimensional modeling, this section should \nbe on their recommended reading list; their perceptions may be clouded by these \ncommon misunderstandings.\nMyth 1: Dimensional Models are Only \nfor Summary Data\nThis  ﬁ rst myth is frequently the root cause of ill-designed dimensional models. \nBecause you can’t possibly predict all the questions asked by business users, you \nneed to provide them with queryable access to the most detailed data so they can \nroll it up based on the business question. Data at the lowest level of detail is practi-\ncally impervious to surprises or changes. Summary data should complement the \n",
      "content_length": 1699,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 67,
      "content": "Data Warehousing, Business Intelligence, and Dimensional Modeling Primer 31\ngranular detail solely to provide improved performance for common queries, but \nnot replace the details.\nA related corollary to this ﬁ rst myth is that only a limited amount of historical \ndata should be stored in dimensional structures. Nothing about a dimensional model \nprohibits storing substantial history. The amount of history available in dimensional \nmodels must only be driven by the business’s requirements.\nMyth 2: Dimensional Models are Departmental, \nNot Enterprise\nRather  than drawing boundaries based on organizational departments, dimensional \nmodels should be organized around business processes, such as orders, invoices, and \nservice calls. Multiple business functions often want to analyze the same metrics \nresulting from a single business process. Multiple extracts of the same source data \nthat create multiple, inconsistent analytic databases should be avoided.\nMyth 3: Dimensional Models are Not Scalable\nDimensional  models are extremely scalable. Fact tables frequently have billions of \nrows; fact tables containing 2 trillion rows have been reported. The database ven-\ndors have wholeheartedly embraced DW/BI and continue to incorporate capabilities \ninto their products to optimize dimensional models’ scalability and performance.\nBoth normalized and dimensional models contain the same information and data \nrelationships; the logical content is identical. Every data relationship expressed in \none model can be accurately expressed in the other. Both normalized and dimen-\nsional models can answer exactly the same questions, albeit with varying diffi  culty.\nMyth 4: Dimensional Models are Only \nfor Predictable Usage\nDimensional  models should not be designed by focusing on predeﬁ ned reports \nor analyses; the design should center on measurement processes. Obviously, it’s \nimportant to consider the BI application’s ﬁ ltering and labeling requirements. But \nyou shouldn’t design for a top ten list of reports in a vacuum because this list is \nbound to change, making the dimensional model a moving target. The key is to \nfocus on the organization’s measurement events that are typically stable, unlike \nanalyses that are constantly evolving.\nA related corollary is that dimensional models aren’t responsive to changing busi-\nness needs. On the contrary, because of their symmetry, dimensional structures are \nextremely ﬂ exible and adaptive to change. The secret to query ﬂ exibility is building \n",
      "content_length": 2512,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 68,
      "content": "Chapter 1\n32\nfact tables at the most granular level. Dimensional models that deliver only summary \ndata are bound to be problematic; users run into analytic brick walls when they try \nto drill down into details not available in the summary tables. Developers also run \ninto brick walls because they can’t easily accommodate new dimensions, attributes, \nor facts with these prematurely summarized tables. The correct starting point for \nyour dimensional models is to express data at the lowest detail possible for maxi-\nmum ﬂ exibility and extensibility. Remember, when you pre-suppose the business \nquestion, you’ll likely pre-summarize the data, which can be fatal in the long run.\nAs the architect Mies van der Rohe is credited with saying, “God is in the details.” \nDelivering dimensional models populated with the most detailed data possible ensures \nmaximum ﬂ exibility and extensibility. Delivering anything less in your dimensional \nmodels undermines the foundation necessary for robust business intelligence.\nMyth 5: Dimensional Models Can’t Be Integrated\nDimensional  models most certainly can be integrated if they conform to the enterprise \ndata warehouse bus architecture. Conformed dimensions are built and maintained \nas centralized, persistent master data in the ETL system and then reused across \ndimensional models to enable data integration and ensure semantic consistency. Data \nintegration depends on standardized labels, values, and deﬁ nitions. It is hard work \nto reach organizational consensus and then implement the corresponding ETL rules, \nbut you can’t dodge the eff ort, regardless of whether you’re populating normalized \nor dimensional models.\nPresentation area databases that don’t adhere to the bus architecture \nwith shared conformed dimensions lead to standalone solutions. You can’t hold \ndimensional modeling responsible for organizations’ failure to embrace one of its \nfundamental tenets.\nMore Reasons to Think Dimensionally\nThe majority  of this book focuses on dimensional modeling for designing databases \nin the DW/BI presentation area. But dimensional modeling concepts go beyond the \ndesign of simple and fast data structures. You should think dimensionally at other \ncritical junctures of a DW/BI project.\nWhen gathering requirements for a DW/BI initiative, you need to listen for and \nthen synthesize the ﬁ ndings around business processes. Sometimes teams get lulled \ninto focusing on a set of required reports or dashboard gauges. Instead you should \nconstantly ask yourself about the business process measurement events producing \nthe report or dashboard metrics. When specifying the project’s scope, you must stand \n",
      "content_length": 2667,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 69,
      "content": "Data Warehousing, Business Intelligence, and Dimensional Modeling Primer 33\nﬁ rm to focus on a single business process per project and not sign up to deploy a \ndashboard that covers a handful of them in a single iteration.\nAlthough it’s critical that the DW/BI team concentrates on business processes, it’s \nequally important to get IT and business management on the same wavelength. Due \nto historical IT funding policies, the business may be more familiar with depart-\nmental data deployments. You need to shift their mindset about the DW/BI rollout \nto a process perspective. When prioritizing opportunities and developing the DW/\nBI roadmap, business processes are the unit of work. Fortunately, business man-\nagement typically embraces this approach because it mirrors their thinking about \nkey performance indicators. Plus, they’ve lived with the inconsistencies, incessant \ndebates, and never ending reconciliations caused by the departmental approach, so \nthey’re ready for a fresh tactic. Working with business leadership partners, rank each \nbusiness process on business value and feasibility, then tackle processes with the \nhighest impact and feasibility scores ﬁ rst. Although prioritization is a joint activity \nwith the business, your underlying understanding of the organization’s business \nprocesses is essential to its eff ectiveness and subsequent actionability.\nIf tasked with drafting the DW/BI system’s data architecture, you need to wrap \nyour head around the organization’s processes, along with the associated master \ndescriptive dimension data. The prime deliverable for this activity, the enterprise \ndata warehouse bus matrix, will be fully vetted in Chapter 4. The matrix also serves \nas a useful tool for touting the potential beneﬁ ts of a more rigorous master data \nmanagement platform.\nData stewardship or governance programs should focus ﬁ rst on the major dimen-\nsions. Depending on the industry, the list might include date, customer, product, \nemployee, facility, provider, student, faculty, account, and so on. Thinking about \nthe central nouns used to describe the business translates into a list of data gov-\nernance eff orts to be led by subject matter experts from the business community. \nEstablishing data governance responsibilities for these nouns is the key to eventually \ndeploying dimensions that deliver consistency and address the business’s needs for \nanalytic ﬁ ltering, grouping, and labeling. Robust dimensions translate into robust \nDW/BI systems.\nAs you can see, the fundamental motivation for dimensional modeling is front and \ncenter long before you design star schemas or OLAP cubes. Likewise, the dimen-\nsional model will remain in the forefront during the subsequent ETL system and BI \napplication designs. Dimensional modeling concepts link the business and technical \ncommunities together as they jointly design the DW/BI deliverables. We’ll elaborate \non these ideas in Chapter 17: Kimball DW/BI Lifecycle Overview and Chapter 18: \nDimensional Modeling Process and Tasks, but wanted to plant the seeds early so \nthey have time to germinate.\n",
      "content_length": 3106,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 70,
      "content": "Chapter 1\n34\nAgile Considerations\nCurrently,  there’s signiﬁ cant interest within the DW/BI industry on agile development \npractices. At the risk of oversimpliﬁ cation, agile methodologies focus on manage-\nably sized increments of work that can be completed within reasonable timeframes \nmeasured in weeks, rather than tackling a much larger scoped (and hence riskier) \nproject with deliverables promised in months or years. Sounds good, doesn’t it?\nMany of the core tenets of agile methodologies align with Kimball best practices, \nincluding\n \n■Focus on delivering business value. This has been the Kimball mantra for \ndecades.\n \n■Value collaboration between the development team and business stakehold-\ners. Like the agile camp, we strongly encourage a close partnership with the \nbusiness.\n \n■Stress ongoing face-to-face communication, feedback, and prioritization with \nthe business stakeholders.\n \n■Adapt quickly to inevitably evolving requirements.\n \n■Tackle development in an iterative, incremental manner. \nAlthough this list is compelling, a common criticism of the agile approaches is the \nlack of planning and architecture, coupled with ongoing governance challenges. The \nenterprise data warehouse bus matrix is a powerful tool to address these shortcom-\nings. The bus matrix provides a framework and master plan for agile development, \nplus identiﬁ es the reusable common descriptive dimensions that provide both data \nconsistency and reduced time-to-market delivery. With the right collaborative mix \nof business and IT stakeholders in a room, the enterprise data warehouse bus matrix \ncan be produced in relatively short order. Incremental development work can produce \ncomponents of the framework until suffi  cient functionality is available and then \nreleased to the business community.\nSome clients and students lament that although they want to deliver consistently \ndeﬁ ned conformed dimensions in their DW/BI environments, it’s “just not feasible.” \nThey explain that they would if they could, but with the focus on agile development \ntechniques, it’s “impossible” to take the time to get organizational agreement on \nconformed dimensions. We argue that conformed dimensions enable agile DW/BI \ndevelopment, along with agile decision making. As you ﬂ esh out the portfolio of mas-\nter conformed dimensions, the development crank starts turning faster and faster. \nThe time-to-market for a new business process data source shrinks as developers \nreuse existing conformed dimensions. Ultimately, new ETL development focuses \nalmost exclusively on delivering more fact tables because the associated dimension \ntables are already sitting on the shelf ready to go.\n",
      "content_length": 2682,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 71,
      "content": "Data Warehousing, Business Intelligence, and Dimensional Modeling Primer 35\nWithout a framework like the enterprise data warehouse bus matrix, some DW/\nBI teams have fallen into the trap of using agile techniques to create analytic or \nreporting solutions in a vacuum. In most situations, the team worked with a small \nset of users to extract a limited set of source data and make it available to solve \ntheir unique problems. The outcome is often a standalone data stovepipe that others \ncan’t leverage, or worse yet, delivers data that doesn’t tie to the organization’s other \nanalytic information. We encourage agility, when appropriate, however building \nisolated data sets should be avoided. As with most things in life, moderation and \nbalance between extremes is almost always prudent.\nSummary\nIn this chapter we discussed the overriding goals for DW/BI systems and the fun-\ndamental concepts of dimensional modeling. The Kimball DW/BI architecture and \nseveral alternatives were compared. We closed out the chapter by identifying com-\nmon misunderstandings that some still hold about dimensional modeling, despite \nits widespread acceptance across the industry, and challenged you to think dimen-\nsionally beyond data modeling. In the next chapter, you get a turbocharged tour \nof dimensional modeling patterns and techniques, and then begin putting these \nconcepts into action in your ﬁ rst case study in Chapter 3.\n",
      "content_length": 1425,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 73,
      "content": "Kimball Dimensional \nModeling \nTechniques \nOverview\nS\ntarting with the first edition of The Data Warehouse Toolkit (Wiley, 1996), the \nKimball Group has defined the complete set of techniques for modeling data \nin a dimensional way. In the first two editions of this book, we felt the techniques \nneeded to be introduced through familiar use cases drawn from various industries. \nAlthough we still feel business use cases are an essential pedagogical approach, the \ntechniques have become so standardized that some dimensional modelers reverse \nthe logic by starting with the technique and then proceeding to the use case for \ncontext. All of this is good news!\nThe Kimball techniques have been accepted as industry best practices. \nAs evidence, some former Kimball University students have published their own \ndimensional modeling books. These books usually explain the Kimball techniques \naccurately, but it is a sign of our techniques’ resilience that alternative books have \nnot extended the library of techniques in signiﬁ cant ways or off ered conﬂ icting \nguidance.\nThis chapter is the “offi  cial” list of Kimball Dimensional Modeling Techniques \nfrom the inventors of these design patterns. We don’t expect you to read this chapter \nfrom beginning to end at ﬁ rst. But we intend the chapter to be a reference for our \ntechniques. With each technique, we’ve included pointers to subsequent chapters \nfor further explanation and illustrations based on the motivating use cases. \nFundamental Concepts\nThe techniques in this section must be considered during every dimensional \ndesign. Nearly every chapter in the book references or illustrates the concepts in \nthis section.\nGather Business Requirements and Data Realities\nBefore  launching a dimensional modeling eff ort, the team needs to understand the \nneeds of the business, as well as the realities of the underlying source data. You \n2\n",
      "content_length": 1900,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 74,
      "content": "Chapter 2\n38\nuncover the requirements via sessions with business representatives to understand \ntheir objectives based on key performance indicators, compelling business issues, \ndecision-making processes, and supporting analytic needs. At the same time, data \nrealities are uncovered by meeting with source system experts and doing high-level \ndata proﬁ ling to assess data feasibilities.\nChapter 1 \nDW/BI and Dimensional Modeling Primer , p 5\nChapter 3 \nRetail Sales , p 70\nChapter 11 Telecommunications , p 297\nChapter 17 Lifecycle Overview , p 412 \nChapter 18 Dimensional Modeling Process and Tasks , p 431\nChapter 19 ETL Subsystems and Techniques ,p 444\nCollaborative Dimensional Modeling Workshops\nDimensional  models should be designed in collaboration with subject matter experts \nand data governance representatives from the business. The data modeler is in \ncharge, but the model should unfold via a series of highly interactive workshops \nwith business representatives. These workshops provide another opportunity to \nﬂ esh out the requirements with the business. Dimensional models should not be \ndesigned in isolation by folks who don’t fully understand the business and their \nneeds; collaboration is critical!\nChapter 3 \nRetail Sales , p 70\nChapter 4 \nInventory , p 135\nChapter 18 Dimensional Modeling Process and Tasks , p 429\nFour-Step Dimensional Design Process\nThe  four key decisions made during the design of a dimensional model include:\n \n1. Select the business process.\n 2. Declare the grain.\n \n3. Identify the dimensions.\n \n4. Identify the facts.\nThe answers to these questions are determined by considering the needs of the \nbusiness along with the realities of the underlying source data during the collab-\norative modeling sessions. Following the business process, grain, dimension, and \nfact declarations, the design team determines the table and column names, sample \ndomain values, and business rules. Business data governance representatives must \nparticipate in this detailed design activity to ensure business buy-in.\n",
      "content_length": 2051,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 75,
      "content": "Kimball Dimensional Modeling Techniques Overview 39\nChapter 3 \nRetail Sales , p 70\nChapter 11 Telecommunications , p 300\nChapter 18 Dimensional Modeling Process and Tasks , p 434\nBusiness Processes\nBusiness processes  are the operational activities performed by your organization, \nsuch as taking an order, processing an insurance claim, registering students for a \nclass, or snapshotting every account each month. Business process events generate \nor capture performance metrics that translate into facts in a fact table. Most fact \ntables focus on the results of a single business process. Choosing the process is \nimportant because it deﬁ nes a speciﬁ c design target and allows the grain, dimen-\nsions, and facts to be declared. Each business process corresponds to a row in the \nenterprise data warehouse bus matrix.\nChapter 1 \nDW/BI and Dimensional Modeling Primer , p 10\nChapter 3 \nRetail Sales , p 70\nChapter 17 Lifecycle Overview , p 414\nChapter 18 Dimensional Modeling Process and Tasks , p 435\nGrain\nDeclaring  the grain is the pivotal step in a dimensional design. The grain establishes \nexactly what a single fact table row represents. The grain declaration becomes a bind-\ning contract on the design. The grain must be declared before choosing dimensions \nor facts because every candidate dimension or fact must be consistent with the grain. \nThis consistency enforces a uniformity on all dimensional designs that is critical to \nBI application performance and ease of use. Atomic grain refers to the lowest level at \nwhich data is captured by a given business process. We strongly encourage you to start \nby focusing on atomic-grained data because it withstands the assault of unpredictable \nuser queries; rolled-up summary grains are important for performance tuning, but they \npre-suppose the business’s common questions. Each proposed fact table grain results \nin a separate physical table; diff erent grains must not be mixed in the same fact table.\nChapter 1 \nDW/BI and Dimensional Modeling Primer, p 30\nChapter 3 \nRetail Sales , p 71\nChapter 4 \nInventory , p 112\nChapter 6 \nOrder Management, p 184\nChapter 11 Telecommunications , p 300\nChapter 12 Transportation , p 312\nChapter 18 Dimensional Modeling Process and Tasks , p 435\n",
      "content_length": 2249,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 76,
      "content": "Chapter 2\n40\nDimensions for Descriptive Context\nDimensions  provide the “who, what, where, when, why, and how” context surround-\ning a business process event. Dimension tables contain the descriptive attributes \nused by BI applications for ﬁ ltering and grouping the facts. With the grain of a fact \ntable ﬁ rmly in mind, all the possible dimensions can be identiﬁ ed. Whenever pos-\nsible, a dimension should be single valued when associated with a given fact row. \nDimension tables are sometimes called the “soul” of the data warehouse because \nthey contain the entry points and descriptive labels that enable the DW/BI system \nto be leveraged for business analysis. A disproportionate amount of eff ort is put \ninto the data governance and development of dimension tables because they are \nthe drivers of the user’s BI experience. \nChapter 1 \nDW/BI and Dimensional Modeling Primer , p 13\nChapter 3 \nRetail Sales , p 72\nChapter 11 Telecommunications , p 301\nChapter 18 Dimensional Modeling Process and Tasks , p 437\nChapter 19 ETL Subsystems and Techniques , p 463\nFacts for Measurements\nFacts  are the measurements that result from a business process event and are almost \nalways numeric. A single fact table row has a one-to-one relationship to a measurement \nevent as described by the fact table’s grain. Thus a fact table corresponds to a physi-\ncal observable event, and not to the demands of a particular report. Within a fact \ntable, only facts consistent with the declared grain are allowed. For example, in a \nretail sales transaction, the quantity of a product sold and its extended price are \ngood facts, whereas the store manager’s salary is disallowed. \nChapter 1 \nDW/BI and Dimensional Modeling Primer , p 10\nChapter 3 \nRetail Sales , p 72\nChapter 4 \nInventory , p 112\nChapter 18 Dimensional Modeling Process and Tasks , p 437\nStar Schemas and OLAP Cubes\nStar schemas  are dimensional structures deployed in a relational database management \nsystem (RDBMS). They characteristically consist of fact tables linked to associated \ndimension tables via primary/foreign key relationships. An online analytical processing \n(OLAP) cube is a dimensional structure implemented in a multidimensional database; \nit can be equivalent in content to, or more often derived from, a relational star schema. \nAn OLAP cube contains dimensional attributes and facts, but it is accessed through \nlanguages with more analytic capabilities than SQL, such as XMLA and MDX. OLAP \n",
      "content_length": 2470,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 77,
      "content": "Kimball Dimensional Modeling Techniques Overview 41\ncubes are included in this list of basic techniques because an OLAP cube is often \nthe ﬁ nal step in the deployment of a dimensional DW/BI system, or may exist as an \naggregate structure based on a more atomic relational star schema. \nChapter 1 \nDW/BI and Dimensional Modeling Primer , p 8\nChapter 3 \nRetail Sales , p 94\nChapter 5 \nProcurement , p 149\nChapter 6 \nOrder Management , p 170\nChapter 7 \nAccounting , p 226\nChapter 9 \nHuman Resources Management , p 273\nChapter 13 Education , p 335\nChapter 19 ETL Subsystems and Techniques , p 481\nChapter 20 ETL System Process and Tasks , p 519\nGraceful Extensions to Dimensional Models\nDimensional  models are resilient when data relationships change. All the following \nchanges can be implemented without altering any existing BI query or application, \nand without any change in query results.\n \n■Facts consistent with the grain of an existing fact table can be added by creat-\ning new columns.\n \n■Dimensions can be added to an existing fact table by creating new foreign key \ncolumns, presuming they don’t alter the fact table’s grain.\n \n■Attributes can be added to an existing dimension table by creating new \ncolumns.\n \n■The grain of a fact table can be made more atomic by adding attributes to an exist-\ning dimension table, and then restating the fact table at the lower grain, being \ncareful to preserve the existing column names in the fact and dimension tables.\nChapter 3 \nRetail Sales , p 95\nBasic Fact Table Techniques\nThe techniques in this section apply to all fact tables. There are illustrations of fact \ntables in nearly every chapter.\nFact Table Structure\nA fact table  contains the numeric measures produced by an operational measure-\nment event in the real world. At the lowest grain, a fact table row corresponds to a \nmeasurement event and vice versa. Thus the fundamental design of a fact table is \nentirely based on a physical activity and is not inﬂ uenced by the eventual reports \n",
      "content_length": 2004,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 78,
      "content": "Chapter 2\n42\nthat may be produced. In addition to numeric measures, a fact table always contains \nforeign keys for each of its associated dimensions, as well as optional degenerate \ndimension keys and date/time stamps. Fact tables are the primary target of compu-\ntations and dynamic aggregations arising from queries.\nChapter 1 \nDW/BI and Dimensional Modeling Primer , p 10\nChapter 3 \nRetail Sales , p 76\nChapter 5 \nProcurement, p 143\nChapter 6 \nOrder Management, p 169\nAdditive, Semi-Additive, Non-Additive Facts\nThe  numeric measures in a fact table fall into three categories. The most ﬂ exible and \nuseful facts are fully additive; additive measures can be summed across any of the \ndimensions associated with the fact table. Semi-additive measures can be summed \nacross some dimensions, but not all; balance amounts are common semi-additive facts \nbecause they are additive across all dimensions except time. Finally, some measures \nare completely non-additive, such as ratios. A good approach for non-additive facts is, \nwhere possible, to store the fully additive components of the non-additive measure \nand sum these components into the ﬁ nal answer set before calculating the ﬁ nal \nnon-additive fact. This ﬁ nal calculation is often done in the BI layer or OLAP cube. \nChapter 1 \nDW/BI and Dimensional Modeling Primer , p 10\nChapter 3 \nRetail Sales , p 76\nChapter 4 \nInventory , p 114\nChapter 7 \nAccounting , p 204\nNulls in Fact Tables\nNull-valued measurements behave gracefully in fact tables. The aggregate functions \n(SUM, COUNT, MIN, MAX, and AVG) all do the “right thing” with null facts. However, \nnulls must be avoided in the fact table’s foreign keys because these nulls would \nautomatically cause a referential integrity  violation. Rather than a null foreign key, \nthe associated dimension table must have a default row (and surrogate key) repre-\nsenting the unknown or not applicable condition.\nChapter 3 \nRetail Sales , p 92\nChapter 20 ETL System Process and Tasks , p 509\nConformed Facts\nIf  the same measurement appears in separate fact tables, care must be taken to make \nsure the technical deﬁ nitions of the facts are identical if they are to be compared \n",
      "content_length": 2184,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 79,
      "content": "Kimball Dimensional Modeling Techniques Overview 43\nor computed together. If the separate fact deﬁ nitions are consistent, the conformed \nfacts should be identically named; but if they are incompatible, they should be dif-\nferently named to alert the business users and BI applications. \nChapter 4 \nInventory , p 138\nChapter 16 Insurance , p 386\nTransaction Fact Tables\nA  row in a transaction fact table corresponds to a measurement event at a point in \nspace and time. Atomic transaction grain fact tables are the most dimensional and \nexpressive fact tables; this robust dimensionality enables the maximum slicing \nand dicing of transaction data. Transaction fact tables may be dense or sparse \nbecause rows exist only if measurements take place. These fact tables always con-\ntain a foreign key for each associated dimension, and optionally contain precise \ntime stamps and degenerate dimension keys. The measured numeric facts must be \nconsistent with the transaction grain.\nChapter 3 \nRetail Sales , p 79\nChapter 4 \nInventory , p 116\nChapter 5 \nProcurement , p 142\nChapter 6 \nOrder Management , p 168\nChapter 7 \nAccounting , p 206\nChapter 11 Telecommunications , p 306\nChapter 12 Transportation , p 312\nChapter 14 Healthcare , p 351\nChapter 15 Electronic Commerce , p 363\nChapter 16 Insurance , p 379\nChapter 19 ETL Subsystems and Techniques , p 473\nPeriodic Snapshot Fact Tables\nA  row in a periodic snapshot fact table summarizes many measurement events occur-\nring over a standard period, such as a day, a week, or a month. The grain is the \nperiod, not the individual transaction. Periodic snapshot fact tables often contain \nmany facts because any measurement event consistent with the fact table grain is \npermissible. These fact tables are uniformly dense in their foreign keys because \neven if no activity takes place during the period, a row is typically inserted in the \nfact table containing a zero or null for each fact. \n",
      "content_length": 1940,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 80,
      "content": "Chapter 2\n44\nChapter 4 \nInventory , p 113\nChapter 7 \nAccounting , p 204\nChapter 9 \nHuman Resources Management , p 267\nChapter 10 Financial Services , p 283\nChapter 13 Education , p 333\nChapter 14 Healthcare, p 351\nChapter 16 Insurance , p 385\nChapter 19 ETL Subsystems and Techniques , p 474\nAccumulating Snapshot Fact Tables\nA  row in an accumulating snapshot fact table summarizes the measurement events \noccurring at predictable steps between the beginning and the end of a process. \nPipeline or workﬂ ow processes, such as order fulﬁ llment or claim processing, that \nhave a deﬁ ned start point, standard intermediate steps, and deﬁ ned end point can be \nmodeled with this type of fact table. There is a date foreign key in the fact table for \neach critical milestone in the process. An individual row in an accumulating snap-\nshot fact table, corresponding for instance to a line on an order, is initially inserted \nwhen the order line is created. As pipeline progress occurs, the accumulating fact \ntable row is revisited and updated. This consistent updating of accumulating snap-\nshot fact rows is unique among the three types of fact tables. In addition to the date \nforeign keys associated with each critical process step, accumulating snapshot fact \ntables contain foreign keys for other dimensions and optionally contain degener-\nate dimensions. They often include numeric lag measurements consistent with the \ngrain, along with milestone completion counters. \nChapter 4 \nInventory , p 118\nChapter 5 \nProcurement , p 147\nChapter 6 \nOrder Management , p 194\nChapter 13 Education , p 326\nChapter 14 Healthcare , p 342\nChapter 16 Insurance , p 392\nChapter 19 ETL Subsystems and Techniques , p 475\nFactless Fact Tables\nAlthough  most measurement events capture numerical results, it is possible that \nthe event merely records a set of dimensional entities coming together at a moment \nin time. For example, an event of a student attending a class on a given day may \nnot have a recorded numeric fact, but a fact row with foreign keys for calendar day, \nstudent, teacher, location, and class is well-deﬁ ned. Likewise, customer communi-\ncations are events, but there may be no associated metrics. Factless fact tables can \n",
      "content_length": 2230,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 81,
      "content": "Kimball Dimensional Modeling Techniques Overview 45\nalso be used to analyze what didn’t happen. These queries always have two parts: a \nfactless coverage table that contains all the possibilities of events that might happen \nand an activity table that contains the events that did happen. When the activity \nis subtracted from the coverage, the result is the set of events that did not happen. \nChapter 3 \nRetail Sales , p 97\nChapter 6 \nOrder Management , p 176\nChapter 13 Education , p 329\nChapter 16 Insurance , p 396\nAggregate Fact Tables or OLAP Cubes\nAggregate fact tables  are simple numeric rollups of atomic fact table data built solely \nto accelerate query performance. These aggregate fact tables should be available to \nthe BI layer at the same time as the atomic fact tables so that BI tools smoothly \nchoose the appropriate aggregate level at query time. This process, known as \naggregate navigation, must be open so that every report writer, query tool, and BI \napplication harvests the same performance beneﬁ ts. A properly designed set of \naggregates should behave like database indexes, which accelerate query perfor-\nmance but are not encountered directly by the BI applications or business users. \nAggregate fact tables contain foreign keys to shrunken conformed dimensions, as \nwell as aggregated facts created by summing measures from more atomic fact tables. \nFinally, aggregate OLAP cubes with summarized measures are frequently built in \nthe same way as relational aggregates, but the OLAP cubes are meant to be accessed \ndirectly by the business users. \nChapter 15 Electronic Commerce , p 366\nChapter 19 ETL Subsystems and Techniques , p 481\nChapter 20 ETL System Process and Tasks , p 519\nConsolidated Fact Tables\nIt  is often convenient to combine facts from multiple processes together into a single \nconsolidated fact table if they can be expressed at the same grain. For example, sales \nactuals can be consolidated with sales forecasts in a single fact table to make the task \nof analyzing actuals versus forecasts simple and fast, as compared to assembling a \ndrill-across application using separate fact tables. Consolidated fact tables add bur-\nden to the ETL processing, but ease the analytic burden on the BI applications. They \nshould be considered for cross-process metrics that are frequently analyzed together.\nChapter 7 \nAccounting , p 224\nChapter 16 Insurance , p 395\n",
      "content_length": 2409,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 82,
      "content": "Chapter 2\n46\nBasic Dimension Table Techniques\nThe techniques in this section apply to all dimension tables. Dimension tables are \ndiscussed and illustrated in every chapter.\nDimension Table Structure\nEvery  dimension table has a single primary key column. This primary key is embedded \nas a foreign key in any associated fact table where the dimension row’s descriptive \ncontext is exactly correct for that fact table row. Dimension tables are usually wide, ﬂ at \ndenormalized tables with many low-cardinality text attributes. While operational codes \nand indicators can be treated as attributes, the most powerful dimension attributes \nare populated with verbose descriptions. Dimension table attributes are the primary \ntarget of constraints and grouping speciﬁ cations from queries and BI applications. The \ndescriptive labels on reports are typically dimension attribute domain values.\nChapter 1 \nDW/BI and Dimensional Modeling Primer , p 13\nChapter 3 \nRetail Sales , p 79\nChapter 11 Telecommunications , p 301\nDimension Surrogate Keys\nA  dimension table is designed with one column serving as a unique primary key. \nThis primary key cannot be the operational system’s natural key because there will \nbe multiple dimension rows for that natural key when changes are tracked over time. \nIn addition, natural keys for a dimension may be created by more than one source \nsystem, and these natural keys may be incompatible or poorly administered. The \nDW/BI system needs to claim control of the primary keys of all dimensions; rather \nthan using explicit natural keys or natural keys with appended dates, you should \ncreate anonymous integer primary keys for every dimension. These dimension sur-\nrogate keys are simple integers, assigned in sequence, starting with the value 1, \nevery time a new key is needed. The date dimension is exempt from the surrogate \nkey rule; this highly predictable and stable dimension can use a more meaningful \nprimary key. See the section “Calendar Date Dimensions.”\nChapter 3 \nRetail Sales , p 98\nChapter 19 ETL Subsystems and Techniques , p 469\nChapter 20 ETL System Process and Tasks , p 506\nNatural, Durable, and Supernatural Keys\nNatural keys  created by  operational source systems are subject to business rules outside \nthe control of the DW/BI system. For instance, an employee number (natural key) may \n",
      "content_length": 2345,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 83,
      "content": "Kimball Dimensional Modeling Techniques Overview 47\nbe changed if the employee resigns and then is rehired. When the data warehouse \nwants to have a single key for that employee, a new durable key must be created that is \npersistent and does not change in this situation. This key is sometimes referred to as \na durable supernatural key. The best durable keys have a format that is independent of \nthe original business process and thus should be simple integers assigned in sequence \nbeginning with 1. While multiple surrogate keys may be associated with an employee \nover time as their proﬁ le changes, the durable key never changes. \nChapter 3 \nRetail Sales , p 100\nChapter 20 ETL System Process and Tasks , p 510\nChapter 21 Big Data Analytics, p 539\nDrilling Down\nDrilling down  is the most fundamental way data is analyzed by business users. Drilling \ndown simply means adding a row header to an existing query; the new row header \nis a dimension attribute appended to the GROUP BY expression in an SQL query. The \nattribute can come from any dimension attached to the fact table in the query. Drilling \ndown does not require the deﬁ nition of predetermined hierarchies or drill-down paths. \nSee the section “Drilling Across.”\nChapter 3 \nRetail Sales , p 86\nDegenerate Dimensions\nSometimes  a dimension is deﬁ ned that has no content except for its primary key. \nFor example, when an invoice has multiple line items, the line item fact rows inherit \nall the descriptive dimension foreign keys of the invoice, and the invoice is left with \nno unique content. But the invoice number remains a valid dimension key for fact \ntables at the line item level. This degenerate dimension is placed in the fact table with \nthe explicit acknowledgment that there is no associated dimension table. Degenerate \ndimensions are most common with transaction and accumulating snapshot fact tables.\nChapter 3 \nRetail Sales , p 93\nChapter 6 \nOrder Management , p 178\nChapter 11 Telecommunications , p 303\nChapter 16 Insurance , p 383\nDenormalized Flattened Dimensions\nIn  general, dimensional designers must resist the normalization urges caused by years \nof operational database designs and instead denormalize the many-to-one ﬁ xed depth \n",
      "content_length": 2226,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 84,
      "content": "Chapter 2\n48\nhierarchies into separate attributes on a ﬂ attened dimension row. Dimension denor-\nmalization supports dimensional modeling’s twin objectives of simplicity and speed.\nChapter 1 \nDW/BI and Dimensional Modeling Primer , p 13\nChapter 3 \nRetail Sales , p 84\nMultiple Hierarchies in Dimensions\nMany  dimensions contain more than one natural hierarchy. For example, calendar \ndate dimensions may have a day to week to ﬁ scal period hierarchy, as well as a \nday to month to year hierarchy. Location intensive dimensions may have multiple \ngeographic hierarchies. In all of these cases, the separate hierarchies can gracefully \ncoexist in the same dimension table. \nChapter 3 \nRetail Sales , p 88\nChapter 19 ETL Subsystems and Techniques , p 470\nFlags and Indicators as Textual Attributes\nCryptic  abbreviations, true/false ﬂ ags, and operational indicators should be sup-\nplemented in dimension tables with full text words that have meaning when \nindependently viewed. Operational codes with embedded meaning within the \ncode value should be broken down with each part of the code expanded into its \nown separate descriptive dimension attribute.\nChapter 3 \nRetail Sales , p 82\nChapter 11 Telecommunications, p 301 \nChapter 16 Insurance , p 383\nNull Attributes in Dimensions\nNull-valued  dimension attributes result when a given dimension row has not been \nfully populated, or when there are attributes that are not applicable to all the dimen-\nsion’s rows. In both cases, we recommend substituting a descriptive string, such as \nUnknown or Not Applicable in place of the null value. Nulls in dimension attributes \nshould be avoided because diff erent databases handle grouping and constraining \non nulls inconsistently.\nChapter 3 \nRetail Sales , p 92\nCalendar Date Dimensions\nCalendar date dimensions  are attached to virtually every fact table to allow navigation \nof the fact table through familiar dates, months, ﬁ scal periods, and special days on \n",
      "content_length": 1960,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 85,
      "content": "Kimball Dimensional Modeling Techniques Overview 49\nthe calendar. You would never want to compute Easter in SQL, but rather want to \nlook it up in the calendar date dimension. The calendar date dimension typically \nhas many attributes describing characteristics such as week number, month name, \nﬁ scal period, and national holiday indicator. To facilitate partitioning, the primary \nkey of a date dimension can be more meaningful, such as an integer representing \nYYYYMMDD, instead of a sequentially-assigned surrogate key. However, the date \ndimension table needs a special row to represent unknown or to-be-determined \ndates. When further precision is needed, a separate date/time stamp can be added \nto the fact table. The date/time stamp is not a foreign key to a dimension table, but \nrather is a standalone column. If business users constrain or group on time-of-day \nattributes, such as day part grouping or shift number, then you would add a separate \ntime-of-day dimension foreign key to the fact table.\nChapter 3 \nRetail Sales , p 79\nChapter 7 \nAccounting , p 208\nChapter 8 \nCustomer Relationship Management , p 238\nChapter 12 Transportation , p 321\nChapter 19 ETL Subsystems and Techniques , p 470\nRole-Playing Dimensions\nA  single physical dimension can be referenced multiple times in a fact table, with \neach reference linking to a logically distinct role for the dimension. For instance, a \nfact table can have several dates, each of which is represented by a foreign key to the \ndate dimension. It is essential that each foreign key refers to a separate view of \nthe date dimension so that the references are independent. These separate dimen-\nsion views (with unique attribute column names) are called roles. \nChapter 6 \nOrder Management , p 170\nChapter 12 Transportation , p 312\nChapter 14 Healthcare , p 345\nChapter 16 Insurance , p 380\nJunk Dimensions\nTransactional  business processes typically produce a number of miscellaneous, low-\ncardinality ﬂ ags and indicators. Rather than making separate dimensions for each \nﬂ ag and attribute, you can create a single junk dimension combining them together. \nThis dimension, frequently labeled as a transaction proﬁ le dimension in a schema, \ndoes not need to be the Cartesian product of all the attributes’ possible values, but \nshould only contain the combination of values that actually occur in the source data. \n",
      "content_length": 2383,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 86,
      "content": "Chapter 2\n50\nChapter 6 \nOrder Management , p 179\nChapter 12 Transportation , p 318\nChapter 16 Insurance , p 392\nChapter 19 ETL Subsystems and Techniques , p 470\nSnowﬂ aked Dimensions\nWhen  a hierarchical relationship in a dimension table is normalized, low-cardinal-\nity attributes appear as secondary tables connected to the base dimension table by \nan attribute key. When this process is repeated with all the dimension table’s hier-\narchies, a characteristic multilevel structure is created that is called a snowﬂ ake. \nAlthough the snowﬂ ake represents hierarchical data accurately, you should avoid \nsnowﬂ akes because it is diffi  cult for business users to understand and navigate \nsnowﬂ akes. They can also negatively impact query performance. A ﬂ attened denor-\nmalized dimension table contains exactly the same information as a snowﬂ aked \ndimension. \nChapter 3 \nRetail Sales , p 104\nChapter 11 Telecommunications , p 301\nChapter 20 ETL System Process and Tasks , p 504\nOutrigger Dimensions\nA  dimension can contain a reference to another dimension table. For instance, a \nbank account dimension can reference a separate dimension representing the date \nthe account was opened. These secondary dimension references are called outrigger \ndimensions. Outrigger dimensions are permissible, but should be used sparingly. In \nmost cases, the correlations between dimensions should be demoted to a fact table, \nwhere both dimensions are represented as separate foreign keys. \nChapter 3 \nRetail Sales , p 106\nChapter 5 \nProcurement , p 160\nChapter 8 \nCustomer Relationship Management , p 243\nChapter 12 Transportation , p 321\nIntegration via Conformed Dimensions\nOne of the marquee successes of the dimensional modeling approach has been to \ndeﬁ ne a simple but powerful recipe for integrating data from diff erent business \nprocesses.\n",
      "content_length": 1839,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 87,
      "content": "Kimball Dimensional Modeling Techniques Overview 51\nConformed Dimensions\nDimension  tables conform when attributes in separate dimension tables have the \nsame column names and domain contents. Information from separate fact tables \ncan be combined in a single report by using conformed dimension attributes that \nare associated with each fact table. When a conformed attribute is used as the \nrow header (that is, the grouping column in the SQL query), the results from the \nseparate fact tables can be aligned on the same rows in a drill-across report. This \nis the essence of integration in an enterprise DW/BI system. Conformed dimen-\nsions, deﬁ ned once in collaboration with the business’s data governance represen-\ntatives, are reused across fact tables; they deliver both analytic consistency and \nreduced future development costs because the wheel is not repeatedly re-created.\nChapter 4 \nInventory , p 130\nChapter 8 \nCustomer Relationship Management , p 256\nChapter 11 Telecommunications , p 304\nChapter 16 Insurance , p 386\nChapter 18 Dimensional Modeling Process and Tasks , p 431\nChapter 19 ETL Subsystems and Techniques , p 461\nShrunken Dimensions\nShrunken dimensions  are conformed dimensions that are a subset of rows and/or \ncolumns of a base dimension. Shrunken rollup dimensions are required when con-\nstructing aggregate fact tables. They are also necessary for business processes that \nnaturally capture data at a higher level of granularity, such as a forecast by month \nand brand (instead of the more atomic date and product associated with sales data). \nAnother case of conformed dimension subsetting occurs when two dimensions are \nat the same level of detail, but one represents only a subset of rows.\nChapter 4 \nInventory , p 132\nChapter 19 ETL Subsystems and Techniques , p 472\nChapter 20 ETL System Process and Tasks , p 504\nDrilling Across\nDrilling across  simply means making separate queries against two or more fact tables \nwhere the row headers of each query consist of identical conformed attributes. The \nanswer sets from the two queries are aligned by performing a sort-merge opera-\ntion on the common dimension attribute row headers. BI tool vendors refer to this \nfunctionality by various names, including stitch and multipass query.\nChapter 4 \nInventory , p 130\n",
      "content_length": 2301,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 88,
      "content": "Chapter 2\n52\nValue Chain\nA value chain  identiﬁ es the natural ﬂ ow of an organization’s primary business \nprocesses. For example, a retailer’s value chain may consist of purchasing to ware-\nhousing to retail sales. A general ledger value chain may consist of budgeting to \ncommitments to payments. Operational source systems typically produce transac-\ntions or snapshots at each step of the value chain. Because each process produces \nunique metrics at unique time intervals with unique granularity and dimensionality, \neach process typically spawns at least one atomic fact table. \nChapter 4 \nInventory , p 111\nChapter 7 \nAccounting , p 210\nChapter 16 Insurance , p 377\nEnterprise Data Warehouse Bus Architecture\nThe enterprise data warehouse bus architecture provides an incremental approach \nto building the enterprise DW/BI system. This architecture decomposes the DW/\nBI planning process into manageable pieces by focusing on business processes, \nwhile delivering integration via standardized conformed dimensions that are reused \nacross processes. It provides an architectural framework, while also decomposing \nthe program to encourage manageable agile implementations corresponding to the \nrows on the enterprise data warehouse bus matrix. The bus architecture is tech-\nnology and database platform independent; both relational and OLAP dimensional \nstructures can participate. \nChapter 1 \nDW/BI and Dimensional Modeling Primer , p 21\nChapter 4 \nInventory , p 123\nEnterprise Data Warehouse Bus Matrix\nThe enterprise data warehouse bus matrix is the essential tool for designing and com-\nmunicating the enterprise data warehouse bus architecture. The rows of the matrix \nare business processes and the columns are dimensions. The shaded cells of the \nmatrix indicate whether a dimension is associated with a given business process. The \ndesign team scans each row to test whether a candidate dimension is well-deﬁ ned for \nthe business process and also scans each column to see where a dimension should be \nconformed across multiple business processes. Besides the technical design consid-\nerations, the bus matrix is used as input to prioritize DW/BI projects with business \nmanagement as teams should implement one row of the matrix at a time. \n",
      "content_length": 2255,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 89,
      "content": "Kimball Dimensional Modeling Techniques Overview 53\nChapter 4 \nInventory , p 125\nChapter 5 \nProcurement , p 143\nChapter 6 \nOrder Management , p 168\nChapter 7 \nAccounting, p 202\nChapter 9 \nHuman Resources Management , p 268\nChapter 10 Financial Services , p 282\nChapter 11 Telecommunications , p 297\nChapter 12 Transportation , p 311\nChapter 13 Education , p 325\nChapter 14 Healthcare , p 339\nChapter 15 Electronic Commerce , p 368\nChapter 16 Insurance , p 389\nDetailed Implementation Bus Matrix\nThe detailed implementation bus matrix is a more granular bus matrix where each \nbusiness process row has been expanded to show speciﬁ c fact tables or OLAP cubes. \nAt this level of detail, the precise grain statement and list of facts can be documented.\nChapter 5 \nProcurement , p 143\nChapter 16 Insurance , p 390\nOpportunity/Stakeholder Matrix\nAfter  the enterprise data warehouse bus matrix rows have been identiﬁ ed, you can \ndraft a diff erent matrix by replacing the dimension columns with business func-\ntions, such as marketing, sales, and ﬁ nance, and then shading the matrix cells to \nindicate which business functions are interested in which business process rows. \nThe opportunity/stakeholder matrix helps identify which business groups should be \ninvited to the collaborative design sessions for each process-centric row. \nChapter 4 \nInventory , p 127\nDealing with Slowly Changing Dimension Attributes\nThe  following section describes the fundamental approaches for dealing with slowly \nchanging dimension (SCD) attributes. It is quite common to have attributes in the \nsame dimension table that are handled with diff erent change tracking techniques.\n",
      "content_length": 1660,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 90,
      "content": "Chapter 2\n54\nType 0: Retain Original\nWith type 0,  the dimension attribute value never changes, so facts are always grouped \nby this original value. Type 0 is appropriate for any attribute labeled “original,” such \nas a customer’s original credit score or a durable identiﬁ er. It also applies to most \nattributes in a date dimension. \nChapter 5 \nProcurement , p 148\nType 1: Overwrite\nWith type 1,  the old attribute value in the dimension row is overwritten with the new \nvalue; type 1 attributes always reﬂ ects the most recent assignment, and therefore \nthis technique destroys history. Although this approach is easy to implement and \ndoes not create additional dimension rows, you must be careful that aggregate fact \ntables and OLAP cubes aff ected by this change are recomputed. \nChapter 5 \nProcurement , p 149\nChapter 16 Insurance , p 380\nChapter 19 ETL Subsystems and Techniques , p 465\nType 2: Add New Row\nType 2  changes add a new row in the dimension with the updated attribute values. \nThis requires generalizing the primary key of the dimension beyond the natural or \ndurable key because there will potentially be multiple rows describing each member. \nWhen a new row is created for a dimension member, a new primary surrogate key is \nassigned and used as a foreign key in all fact tables from the moment of the update \nuntil a subsequent change creates a new dimension key and updated dimension row.\nA minimum of three additional columns should be added to the dimension row \nwith type 2 changes: 1) row eff ective date or date/time stamp; 2) row expiration \ndate or date/time stamp; and 3) current row indicator. \nChapter 5 \nProcurement , p 150\nChapter 8 \nCustomer Relationship Management , p 243\nChapter 9 \nHuman Resources Management , p 263\nChapter 16 Insurance , p 380\nChapter 19 ETL Subsystems and Techniques , p 465\nChapter 20 ETL System Process and Tasks , p 507\n",
      "content_length": 1885,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 91,
      "content": "Kimball Dimensional Modeling Techniques Overview 55\nType 3: Add New Attribute\nType 3  changes add a new attribute in the dimension to preserve the old attribute \nvalue; the new value overwrites the main attribute as in a type 1 change. This kind of \ntype 3 change is sometimes called an alternate reality. A business user can group and \nﬁ lter fact data by either the current value or alternate reality. This slowly changing \ndimension technique is used relatively infrequently.\nChapter 5 \nProcurement , p 154\nChapter 16 Insurance , p 380\nChapter 19 ETL Subsystems and Techniques , p 467\nType 4: Add Mini-Dimension\nThe type 4  technique is used when a group of attributes in a dimension rapidly \nchanges and is split off  to a mini-dimension. This situation is sometimes called a \nrapidly changing monster dimension. Frequently used attributes in multimillion-row \ndimension tables are mini-dimension design candidates, even if they don’t fre-\nquently change. The type 4 mini-dimension requires its own unique primary key; \nthe primary keys of both the base dimension and mini-dimension are captured in \nthe associated fact tables. \nChapter 5 \nProcurement , p 156\nChapter 10 Financial Services , p 289\nChapter 16 Insurance , p 381\nChapter 19 ETL Subsystems and Techniques , p 467\nType 5: Add Mini-Dimension and Type 1 Outrigger\nThe type 5  technique is used to accurately preserve historical attribute values, \nplus report historical facts according to current attribute values. Type 5 builds on \nthe type 4 mini-dimension by also embedding a current type 1 reference to the \nmini-dimension in the base dimension. This enables the currently-assigned mini-\ndimension attributes to be accessed along with the others in the base dimension \nwithout linking through a fact table. Logically, you’d represent the base dimension \nand mini-dimension outrigger as a single table in the presentation area. The ETL \nteam must overwrite this type 1 mini-dimension reference whenever the current \nmini-dimension assignment changes. \nChapter 5 \nProcurement , p 160\nChapter 19 ETL Subsystems and Techniques , p 468\n",
      "content_length": 2099,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 92,
      "content": "Chapter 2\n56\nType 6: Add Type 1 Attributes to Type 2 Dimension\nLike  type 5, type 6 also delivers both historical and current dimension attribute \nvalues. Type 6 builds on the type 2 technique by also embedding current type \n1 versions of the same attributes in the dimension row so that fact rows can be \nﬁ ltered or grouped by either the type 2 attribute value in eff ect when the measure-\nment occurred or the attribute’s current value. In this case, the type 1 attribute is \nsystematically overwritten on all rows associated with a particular durable key \nwhenever the attribute is updated. \nChapter 5 \nProcurement , p 160\nChapter 19 ETL Subsystems and Techniques , p 468\nType 7: Dual Type 1 and Type 2 Dimensions\nType 7  is the ﬁ nal hybrid technique used to support both as-was and as-is report-\ning. A fact table can be accessed through a dimension modeled both as a type 1 \ndimension showing only the most current attribute values, or as a type 2 dimen-\nsion showing correct contemporary historical proﬁ les. The same dimension table \nenables both perspectives. Both the durable key and primary surrogate key of the \ndimension are placed in the fact table. For the type 1 perspective, the current ﬂ ag \nin the dimension is constrained to be current, and the fact table is joined via the \ndurable key. For the type 2 perspective, the current ﬂ ag is not constrained, and the \nfact table is joined via the surrogate primary key. These two perspectives would be \ndeployed as separate views to the BI applications. \nChapter 5 \nProcurement , p 162\nChapter 19 ETL Subsystems and Techniques , p 468\nDealing with Dimension Hierarchies\nDimensional hierarchies are commonplace. This section describes approaches for \ndealing with hierarchies, starting with the most basic.\nFixed Depth Positional Hierarchies\nA ﬁ xed depth hierarchy is a series of many-to-one relationships, such as product \nto brand to category to department. When a ﬁ xed depth hierarchy is deﬁ ned and \nthe hierarchy levels have agreed upon names, the hierarchy levels should appear \nas separate positional attributes in a dimension table. A ﬁ xed depth hierarchy is \nby far the easiest to understand and navigate as long as the above criteria are met. \nIt also delivers predictable and fast query performance. When the hierarchy is not \na series of many-to-one relationships or the number of levels varies such that the \n",
      "content_length": 2389,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 93,
      "content": "Kimball Dimensional Modeling Techniques Overview 57\nlevels do not have agreed upon names, a ragged hierarchy technique, described \nbelow, must be used. \nChapter 3 \nRetail Sales , p 84\nChapter 7 \nAccounting , p 214\nChapter 19 ETL Subsystems and Techniques , p 470\nChapter 20 ETL System Process and Tasks , p 501\nSlightly Ragged/Variable Depth Hierarchies\nSlightly ragged  hierarchies don’t have a ﬁ xed number of levels, but the range in depth \nis small. Geographic hierarchies often range in depth from perhaps three levels to \nsix levels. Rather than using the complex machinery for unpredictably variable \nhierarchies, you can force-ﬁ t slightly ragged hierarchies into a ﬁ xed depth positional \ndesign with separate dimension attributes for the maximum number of levels, and \nthen populate the attribute value based on rules from the business. \nChapter 7 \nAccounting , p 214\nRagged/Variable Depth Hierarchies with Hierarchy Bridge Tables\nRagged hierarchies  of indeterminate depth are diffi  cult to model and query in a \nrelational database. Although SQL extensions and OLAP access languages provide \nsome support for recursive parent/child relationships, these approaches have limita-\ntions. With SQL extensions, alternative ragged hierarchies cannot be substituted at \nquery time, shared ownership structures are not supported, and time varying ragged \nhierarchies are not supported. All these objections can be overcome in relational \ndatabases by modeling a ragged hierarchy with a specially constructed bridge table. \nThis bridge table contains a row for every possible path in the ragged hierarchy \nand enables all forms of hierarchy traversal to be accomplished with standard SQL \nrather than using special language extensions. \nChapter 7 \nAccounting , p 215\nChapter 9 \nHuman Resources Management , p 273\nRagged/Variable Depth Hierarchies with Pathstring Attributes\nThe  use of a bridge table for ragged variable depth hierarchies can be avoided by \nimplementing a pathstring attribute in the dimension. For each row in the dimen-\nsion, the pathstring attribute contains a specially encoded text string containing \nthe complete path description from the supreme node of a hierarchy down to the \nnode described by the particular dimension row. Many of the standard hierarchy \n",
      "content_length": 2286,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 94,
      "content": "Chapter 2\n58\nanalysis requests can then be handled by standard SQL, without resorting to SQL \nlanguage extensions. However, the pathstring approach does not enable rapid sub-\nstitution of alternative hierarchies or shared ownership hierarchies. The pathstring \napproach may also be vulnerable to structure changes in the ragged hierarchy that \ncould force the entire hierarchy to be relabeled. \nChapter 7 \nAccounting , p 221\nAdvanced Fact Table Techniques\nThe techniques in this section refer to less common fact table patterns. \nFact Table Surrogate Keys\nSurrogate  keys are used to implement the primary keys of almost all dimension \ntables. In addition, single column surrogate fact keys can be useful, albeit not \nrequired. Fact table surrogate keys, which are not associated with any dimension, \nare assigned sequentially during the ETL load process and are used 1) as the single \ncolumn primary key of the fact table; 2) to serve as an immediate identiﬁ er of a fact \ntable row without navigating multiple dimensions for ETL purposes; 3) to allow an \ninterrupted load process to either back out or resume; 4) to allow fact table update \noperations to be decomposed into less risky inserts plus deletes. \nChapter 3 \nRetail Sales , p 102\nChapter 19 ETL Subsystems and Techniques , p 486\nChapter 20 ETL System Process and Tasks , p 520\nCentipede Fact Tables\nSome  designers create separate normalized dimensions for each level of a many-to-\none hierarchy, such as a date dimension, month dimension, quarter dimension, and \nyear dimension, and then include all these foreign keys in a fact table. This results \nin a centipede fact table with dozens of hierarchically related dimensions. Centipede \nfact tables should be avoided. All these ﬁ xed depth, many-to-one hierarchically \nrelated dimensions should be collapsed back to their unique lowest grains, such as \nthe date for the example mentioned. Centipede fact tables also result when design-\ners embed numerous foreign keys to individual low-cardinality dimension tables \nrather than creating a junk dimension. \nChapter 3 \nRetail Sales , p 108\n",
      "content_length": 2101,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 95,
      "content": "Kimball Dimensional Modeling Techniques Overview 59\nNumeric Values as Attributes or Facts\nDesigners  sometimes encounter numeric values that don’t clearly fall into either \nthe fact or dimension attribute categories. A classic example is a product’s standard \nlist price. If the numeric value is used primarily for calculation purposes, it likely \nbelongs in the fact table. If a stable numeric value is used predominantly for ﬁ ltering \nand grouping, it should be treated as a dimension attribute; the discrete numeric \nvalues can be supplemented with value band attributes (such as $0-50). In some \ncases, it is useful to model the numeric value as both a fact and dimension attribute, \nsuch as a quantitative on-time delivery metric and qualitative textual descriptor. \nChapter 3 \nRetail Sales , p 85\nChapter 6 \nOrder Management , p 188\nChapter 8 \nCustomer Relationship Management , p 254\nChapter 16 Insurance , p 382\nLag/Duration Facts\nAccumulating  snapshot fact tables capture multiple process milestones, each with a \ndate foreign key and possibly a date/time stamp. Business users often want to analyze \nthe lags or durations between these milestones; sometimes these lags are just the \ndiff erences between dates, but other times the lags are based on more complicated \nbusiness rules. If there are dozens of steps in a pipeline, there could be hundreds \nof possible lags. Rather than forcing the user’s query to calculate each possible lag \nfrom the date/time stamps or date dimension foreign keys, just one time lag can be \nstored for each step measured against the process’s start point. Then every possible \nlag between two steps can be calculated as a simple subtraction between the two \nlags stored in the fact table. \nChapter 6 \nOrder Management , p 196\nChapter 16 Insurance , p 393\nHeader/Line Fact Tables\nOperational  transaction systems often consist of a transaction header row that’s \nassociated with multiple transaction lines. With header/line schemas (also known \nas parent/child schemas), all the header-level dimension foreign keys and degenerate \ndimensions should be included on the line-level fact table.\nChapter 6 \nOrder Management , p 181\nChapter 12 Transportation , p 315\nChapter 15 Electronic Commerce , p 363\n",
      "content_length": 2243,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 96,
      "content": "Chapter 2\n60\nAllocated Facts\nIt  is quite common in header/line transaction data to encounter facts of diff er-\ning granularity, such as a header freight charge. You should strive to allocate \nthe header facts down to the line level based on rules provided by the business, so the \nallocated facts can be sliced and rolled up by all the dimensions. In many cases, you \ncan avoid creating a header-level fact table, unless this aggregation delivers query \nperformance advantages.\nChapter 6 \nOrder Management , p 184\nProﬁ t and Loss Fact Tables Using Allocations\nFact  tables that expose the full equation of proﬁ t are among the most powerful deliv-\nerables of an enterprise DW/BI system. The equation of proﬁ t is (revenue) – (costs) = \n(proﬁ t). Fact tables ideally implement the proﬁ t equation at the grain of the atomic \nrevenue transaction and contain many components of cost. Because these tables are \nat the atomic grain, numerous rollups are possible, including customer proﬁ tabil-\nity, product proﬁ tability, promotion proﬁ tability, channel proﬁ tability, and others. \nHowever, these fact tables are diffi  cult to build because the cost components must \nbe allocated from their original sources to the fact table’s grain. This allocation step \nis often a major ETL subsystem and is a politically charged step that requires high-\nlevel executive support. For these reasons, proﬁ t and loss fact tables are typically \nnot tackled during the early implementation phases of a DW/BI program.\nChapter 6 \nOrder Management , p 189\nChapter 15 Electronic Commerce , p 370\nMultiple Currency Facts\nFact  tables that record ﬁ nancial transactions in multiple currencies should contain \na pair of columns for every ﬁ nancial fact in the row. One column contains the fact \nexpressed in the true currency of the transaction, and the other contains the same \nfact expressed in a single standard currency that is used throughout the fact table. \nThe standard currency value is created in an ETL process according to an approved \nbusiness rule for currency conversion. This fact table also must have a currency \ndimension to identify the transaction’s true currency. \nChapter 6 \nOrder Management , p 182\nChapter 7 \nAccounting , p 206\n",
      "content_length": 2227,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 97,
      "content": "Kimball Dimensional Modeling Techniques Overview 61\nMultiple Units of Measure Facts\nSome  business processes require facts to be stated simultaneously in several units \nof measure. For example, depending on the perspective of the business user, a \nsupply chain may need to report the same facts as pallets, ship cases, retail cases, \nor individual scan units. If the fact table contains a large number of facts, each of \nwhich must be expressed in all units of measure, a convenient technique is to store \nthe facts once in the table at an agreed standard unit of measure, but also simulta-\nneously store conversion factors between the standard measure and all the others. \nThis fact table could be deployed through views to each user constituency, using \nan appropriate selected conversion factor. The conversion factors must reside in the \nunderlying fact table row to ensure the view calculation is simple and correct, while \nminimizing query complexity.\nChapter 6 \nOrder Management , p 197\nYear-to-Date Facts\nBusiness  users often request year-to-date (YTD) values in a fact table. It is hard to \nargue against a single request, but YTD requests can easily morph into “YTD at the \nclose of the ﬁ scal period” or “ﬁ scal period to date.” A more reliable, extensible way \nto handle these assorted requests is to calculate the YTD metrics in the BI applica-\ntions or OLAP cube rather than storing YTD facts in the fact table.\nChapter 7 \nAccounting , p 206\nMultipass SQL to Avoid Fact-to-Fact Table Joins\nA  BI application must never issue SQL that joins two fact tables together across the \nfact table’s foreign keys. It is impossible to control the cardinality of the answer set \nof such a join in a relational database, and incorrect results will be returned to the \nBI tool. For instance, if two fact tables contain customer’s product shipments and \nreturns, these two fact tables must not be joined directly across the customer \nand product foreign keys. Instead, the technique of drilling across two fact tables \nshould be used, where the answer sets from shipments and returns are separately \ncreated, and the results sort-merged on the common row header attribute values to \nproduce the correct result. \nChapter 4 \nInventory , p 130\nChapter 8 \nCustomer Relationship Management , p 259\n",
      "content_length": 2293,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 98,
      "content": "Chapter 2\n62\nTimespan Tracking in Fact Tables\nThere  are three basic fact table grains: transaction, periodic snapshot, and accu-\nmulating snapshot. In isolated cases, it is useful to add a row eff ective date, row \nexpiration date, and current row indicator to the fact table, much like you do with \ntype 2 slowly changing dimensions, to capture a timespan when the fact row was \neff ective. Although an unusual pattern, this pattern addresses scenarios such as \nslowly changing inventory balances where a frequent periodic snapshot would load \nidentical rows with each snapshot. \nChapter 8 \nCustomer Relationship Management , p 252\nChapter 16 Insurance , p 394\nLate Arriving Facts\nA  fact row is late arriving if the most current dimensional context for new fact rows \ndoes not match the incoming row. This happens when the fact row is delayed. In \nthis case, the relevant dimensions must be searched to ﬁ nd the dimension keys that \nwere eff ective when the late arriving measurement event occurred.\nChapter 14 Healthcare , p 351\nChapter 19 ETL Subsystems and Techniques , p 478\nAdvanced Dimension Techniques\nThe techniques in this section refer to more advanced dimension table patterns.\nDimension-to-Dimension Table Joins\nDimensions  can contain references to other dimensions. Although these relation-\nships can be modeled with outrigger dimensions, in some cases, the existence of a \nforeign key to the outrigger dimension in the base dimension can result in explosive \ngrowth of the base dimension because type 2 changes in the outrigger force cor-\nresponding type 2 processing in the base dimension. This explosive growth can \noften be avoided if you demote the correlation between dimensions by placing the \nforeign key of the outrigger in the fact table rather than in the base dimension. This \nmeans the correlation between the dimensions can be discovered only by traversing \nthe fact table, but this may be acceptable, especially if the fact table is a periodic \nsnapshot where all the keys for all the dimensions are guaranteed to be present for \neach reporting period.\nChapter 6 \nOrder Management , p 175\n",
      "content_length": 2121,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 99,
      "content": "Kimball Dimensional Modeling Techniques Overview 63\nMultivalued Dimensions and Bridge Tables\nIn  a classic dimensional schema, each dimension attached to a fact table has a single \nvalue consistent with the fact table’s grain. But there are a number of situations in \nwhich a dimension is legitimately multivalued. For example, a patient receiving a \nhealthcare treatment may have multiple simultaneous diagnoses. In these cases, the \nmultivalued dimension must be attached to the fact table through a group dimen-\nsion key to a bridge table with one row for each simultaneous diagnosis in a group.\nChapter 8 \nCustomer Relationship Management , p 245\nChapter 9 \nHuman Resources Management , p 275\nChapter 10 Financial Services , p 287\nChapter 13 Education , p 333\nChapter 14 Healthcare , p 345\nChapter 16 Insurance , p 382\nChapter 19 ETL Subsystems and Techniques , p 477\nTime Varying Multivalued Bridge Tables\nA multivalued bridge table may need to be based on a type 2 slowly changing dimen-\nsion. For example, the bridge table that implements the many-to-many relationship \nbetween bank accounts and individual customers usually must be based on type \n2 account and customer dimensions. In this case, to prevent incorrect linkages \nbetween accounts and customers, the bridge table must include eff ective and expi-\nration date/time stamps, and the requesting application must constrain the bridge \ntable to a speciﬁ c moment in time to produce a consistent snapshot. \nChapter 7 \nAccounting , p 220\nChapter 10 Financial Services , p 286\nBehavior Tag Time Series\nAlmost  all text in a data warehouse is descriptive text in dimension tables. Data \nmining customer cluster analyses typically results in textual behavior tags, often \nidentiﬁ ed on a periodic basis. In this case, the customers’ behavior measurements \nover time become a sequence of these behavior tags; this time series should be \nstored as positional attributes in the customer dimension, along with an optional \ntext string for the complete sequence of tags. The behavior tags are modeled in a \npositional design because the behavior tags are the target of complex simultaneous \nqueries rather than numeric computations. \nChapter 8 \nCustomer Relationship Management , p 240\n",
      "content_length": 2241,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 100,
      "content": "Chapter 2\n64\nBehavior Study Groups\nComplex  customer behavior can sometimes be discovered only by running lengthy \niterative analyses. In these cases, it is impractical to embed the behavior analyses \ninside every BI application that wants to constrain all the members of the customer \ndimension who exhibit the complex behavior. The results of the complex behavior \nanalyses, however, can be captured in a simple table, called a study group, consisting \nonly of the customers’ durable keys. This static table can then be used as a kind of \nﬁ lter on any dimensional schema with a customer dimension by constraining the \nstudy group column to the customer dimension’s durable key in the target schema \nat query time. Multiple study groups can be deﬁ ned and derivative study groups \ncan be created with intersections, unions, and set diff erences. \nChapter 8 \nCustomer Relationship Management , p 249\nAggregated Facts as Dimension Attributes\nBusiness  users are often interested in constraining the customer dimension based \non aggregated performance metrics, such as ﬁ ltering on all customers who spent \nover a certain dollar amount during last year or perhaps over the customer’s lifetime. \nSelected aggregated facts can be placed in a dimension as targets for constraining and \nas row labels for reporting. The metrics are often presented as banded ranges in the \ndimension table. Dimension attributes representing aggregated performance metrics \nadd burden to the ETL processing, but ease the analytic burden in the BI layer.\nChapter 8 \nCustomer Relationship Management , p 239\nDynamic Value Bands\nA dynamic value banding report is organized as a series of report row headers that \ndeﬁ ne a progressive set of varying-sized ranges of a target numeric fact. For instance, \na common value banding report in a bank has many rows with labels such as \n“Balance from 0 to $10,” “Balance from $10.01 to $25,” and so on. This kind of \nreport is dynamic because the speciﬁ c row headers are deﬁ ned at query time, not \nduring the ETL processing. The row deﬁ nitions can be implemented in a small value \nbanding dimension table that is joined via greater-than/less-than joins to the fact \ntable, or the deﬁ nitions can exist only in an SQL CASE statement. The value band-\ning dimension approach is probably higher performing, especially in a columnar \ndatabase, because the CASE statement approach involves an almost unconstrained \nrelation scan of the fact table. \nChapter 10 Financial Services , p 291\n",
      "content_length": 2499,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 101,
      "content": "Kimball Dimensional Modeling Techniques Overview 65\nText Comments Dimension\nRather  than treating freeform comments as textual metrics in a fact table, they \nshould be stored outside the fact table in a separate comments dimension (or as \nattributes in a dimension with one row per transaction if the comments’ cardinal-\nity matches the number of unique transactions) with a corresponding foreign key \nin the fact table. \nChapter 9 \nHuman Resources Management , p 278\nChapter 14 Healthcare , p 350\nMultiple Time Zones\nTo  capture both universal standard time, as well as local times in multi-time zone \napplications, dual foreign keys should be placed in the aff ected fact tables that join \nto two role-playing date (and potentially time-of-day) dimension tables. \nChapter 12 Transportation , p 323\nChapter 15 Electronic Commerce , p 361\nMeasure Type Dimensions\nSometimes  when a fact table has a long list of facts that is sparsely populated in any \nindividual row, it is tempting to create a measure type dimension that collapses the \nfact table row down to a single generic fact identiﬁ ed by the measure type dimen-\nsion. We generally do not recommend this approach. Although it removes all the \nempty fact columns, it multiplies the size of the fact table by the average number \nof occupied columns in each row, and it makes intra-column computations much \nmore diffi  cult. This technique is acceptable when the number of potential facts is \nextreme (in the hundreds), but less than a handful would be applicable to any given \nfact table row. \nChapter 6 \nOrder Management , p 169\nChapter 14 Healthcare , p 349\nStep Dimensions\nSequential  processes, such as web page events, normally have a separate row in a \ntransaction fact table for each step in a process. To tell where the individual step \nﬁ ts into the overall session, a step dimension is used that shows what step number \nis represented by the current step and how many more steps were required to com-\nplete the session. \n",
      "content_length": 1988,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 102,
      "content": "Chapter 2\n66\nChapter 8 \nCustomer Relationship Management , p 251\nChapter 15 Electronic Commerce , p 366\nHot Swappable Dimensions\nHot swappable dimensions  are used when the same fact table is alternatively paired \nwith diff erent copies of the same dimension. For example, a single fact table con-\ntaining stock ticker quotes could be simultaneously exposed to multiple separate \ninvestors, each of whom has unique and proprietary attributes assigned to diff erent \nstocks.\nChapter 10 Financial Services , p 296\nAbstract Generic Dimensions\nSome  modelers are attracted to abstract generic dimensions. For example, their \nschemas include a single generic location dimension rather than embedded geo-\ngraphic attributes in the store, warehouse, and customer dimensions. Similarly, \ntheir person dimension includes rows for employees, customers, and vendor \ncontacts because they are all human beings, regardless that signiﬁ cantly diff erent \nattributes are collected for each type. Abstract generic dimensions should be avoided \nin dimensional models. The attribute sets associated with each type often diff er. If \nthe attributes are common, such as a geographic state, then they should be uniquely \nlabeled to distinguish a store’s state from a customer’s. Finally, dumping all variet-\nies of locations, people, or products into a single dimension invariably results in \na larger dimension table. Data abstraction may be appropriate in the operational \nsource system or ETL processing, but it negatively impacts query performance and \nlegibility in the dimensional model.\nChapter 9 \nHuman Resources Management , p 270\nChapter 11 Telecommunications , p 310\nAudit Dimensions\nWhen  a fact table row is created in the ETL back room, it is helpful to create \nan audit dimension containing the ETL processing metadata known at the time. \nA simple audit dimension row could contain one or more basic indicators of data \nquality, perhaps derived from examining an error event schema that records \ndata quality violations encountered while processing the data. Other useful audit \ndimension attributes could include environment variables describing the versions \nof ETL code used to create the fact rows or the ETL process execution time stamps. \n",
      "content_length": 2239,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 103,
      "content": "Kimball Dimensional Modeling Techniques Overview 67\nThese environment variables are especially useful for compliance and auditing \npurposes because they enable BI tools to drill down to determine which rows were \ncreated with what versions of the ETL software. \nChapter 6 \nOrder Management , p 192\nChapter 16 Insurance , p 383\nChapter 19 ETL Subsystems and Techniques , p 460\nChapter 20 ETL System Process and Tasks , p 511\nLate Arriving Dimensions\nSometimes  the facts from an operational business process arrive minutes, hours, \ndays, or weeks before the associated dimension context. For example, in a real-time \ndata delivery situation, an inventory depletion row may arrive showing the natural \nkey of a customer committing to purchase a particular product. In a real-time ETL \nsystem, this row must be posted to the BI layer, even if the identity of the customer \nor product cannot be immediately determined. In these cases, special dimension \nrows are created with the unresolved natural keys as attributes. Of course, these \ndimension rows must contain generic unknown values for most of the descriptive \ncolumns; presumably the proper dimensional context will follow from the source at \na later time. When this dimensional context is eventually supplied, the placeholder \ndimension rows are updated with type 1 overwrites. Late arriving dimension data \nalso occurs when retroactive changes are made to type 2 dimension attributes. \nIn this case, a new row needs to be inserted in the dimension table, and then the \nassociated fact rows must be restated.\nChapter 14 Healthcare , p 351\nChapter 19 ETL Subsystems and Techniques , p 478\nChapter 20 ETL System Process and Tasks , p 523\nSpecial Purpose Schemas\nThe following design patterns are needed for speciﬁ c use cases.\nSupertype and Subtype Schemas for Heterogeneous Products\nFinancial  services and other businesses frequently off er a wide variety of products \nin disparate lines of business. For example, a retail bank may off er dozens of types \nof accounts ranging from checking accounts to mortgages to business loans, but all \nare examples of an account. Attempts to build a single, consolidated fact table with \nthe union of all possible facts, linked to dimension tables with all possible attributes \n",
      "content_length": 2270,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 104,
      "content": "Chapter 2\n68\nof these divergent products, will fail because there can be hundreds of incompatible \nfacts and attributes. The solution is to build a single supertype fact table that has the \nintersection of the facts from all the account types (along with a supertype dimen-\nsion table containing the common attributes), and then systematically build separate \nfact tables (and associated dimension tables) for each of the subtypes. Supertype and \nsubtype fact tables are also called core and custom fact tables. \nChapter 10 Financial Services , p 293\nChapter 14 Healthcare , p 347\nChapter 16 Insurance , p 384\nReal-Time Fact Tables\nReal-time fact tables  need to be updated more frequently than the more traditional \nnightly batch process. There are many techniques for supporting this requirement, \ndepending on the capabilities of the DBMS or OLAP cube used for ﬁ nal deployment \nto the BI reporting layer. For example, a “hot partition” can be deﬁ ned on a fact table \nthat is pinned in physical memory. Aggregations and indexes are deliberately not \nbuilt on this partition. Other DBMSs or OLAP cubes may support deferred updat-\ning that allows existing queries to run to completion but then perform the updates. \nChapter 8 \nCustomer Relationship Management, p 260 \nChapter 20 ETL System Process and Tasks , p 520\nError Event Schemas\nManaging  data quality in a data warehouse requires a comprehensive system of \ndata quality screens or ﬁ lters that test the data as it ﬂ ows from the source sys-\ntems to the BI platform. When a data quality screen detects an error, this event \nis recorded in a special dimensional schema that is available only in the ETL \nback room. This schema consists of an error event fact table whose grain is the \nindividual error event and an associated error event detail fact table whose grain \nis each column in each table that participates in an error event.\nChapter 19 ETL Subsystems and Techniques , p  458\n",
      "content_length": 1943,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 105,
      "content": "Retail Sales\nT\nhe best way to understand the principles of dimensional modeling is to work \nthrough a series of tangible examples. By visualizing real cases, you hold the \nparticular design challenges and solutions in your mind more effectively than if they \nare presented abstractly. This book uses case studies from a range of businesses to \nhelp move past the idiosyncrasies of your own environment and reinforce dimen-\nsional modeling best practices.\nTo learn dimensional modeling, please read all the chapters in this book, even \nif you don’t manage a retail store or work for a telecommunications company. The \nchapters are not intended to be full-scale solutions for a given industry or business \nfunction. Each chapter covers a set of dimensional modeling patterns that comes \nup in nearly every kind of business. Universities, insurance companies, banks, and \nairlines alike surely need the techniques developed in this retail chapter. Besides, \nthinking about someone else’s business is refreshing. It is too easy to let historical \ncomplexities derail you when dealing with data from your company. By stepping out-\nside your organization and then returning with a well-understood design principle \n(or two), it is easier to remember the spirit of the design principles as you descend \ninto the intricate details of your business.\nChapter 3 discusses the following concepts:\n \n■Four-step process for designing dimensional models\n \n■Fact table granularity\n \n■Transaction fact tables\n \n■Additive, non-additive, and derived facts\n \n■Dimension attributes, including indicators, numeric descriptors, and multiple \nhierarchies\n \n■Calendar date dimensions, plus time-of-day\n \n■Causal dimensions, such as promotion\n \n■Degenerate dimensions, such as the transaction receipt number\n3\n",
      "content_length": 1784,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 106,
      "content": "Chapter 3\n70\n \n■Nulls in a dimensional model\n \n■Extensibility of dimension models\n \n■Factless fact tables\n \n■Surrogate, natural, and durable keys\n \n■Snowﬂ aked dimension attributes\n \n■Centipede fact tables with “too many dimensions”\n Four-Step Dimensional Design Process\nThroughout this book, we will approach the design of a dimensional model by \nconsistently considering four steps, as the following sections discuss in more detail.\n Step 1: Select the Business Process\nA business process is a low-level activity performed by an organization, such as taking \norders, invoicing, receiving payments, handling service calls, registering students, \nperforming a medical procedure, or processing claims. To identify your organiza-\ntion’s business processes, it’s helpful to understand several common characteristics: \n \n■Business processes are frequently expressed as action verbs because they repre-\nsent activities that the business performs. The companion dimensions describe \ndescriptive context associated with each business process event.\n \n■Business processes are typically supported by an operational system, such as \nthe billing or purchasing system.\n \n■Business processes generate or capture key performance metrics. Sometimes \nthe metrics are a direct result of the business process; the measurements are \nderivations at other times. Analysts invariably want to scrutinize and evaluate \nthese metrics by a seemingly limitless combination of ﬁ lters and constraints.\n \n■Business processes are usually triggered by an input and result in output \nmetrics. In many organizations, there’s a series of processes in which the \noutputs from one process become the inputs to the next. In the parlance of a \ndimensional modeler, this series of processes results in a series of fact tables.\nYou need to listen carefully to the business to identify the organization’s business \nprocesses because business users can’t readily answer the question, “What busi-\nness process are you interested in?” The performance measurements users want to \nanalyze in the DW/BI system result from business process events. \nSometimes  business users talk about strategic business initiatives instead of busi-\nness processes. These initiatives are typically broad enterprise plans championed \nby executive leadership to deliver competitive advantage. In order to tie a business \ninitiative to a business process representing a project-sized unit of work for the \n",
      "content_length": 2439,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 107,
      "content": "Retail Sales 71\nDW/BI team, you need to decompose the business initiative into the underlying \nprocesses. This means digging a bit deeper to understand the data and operational \nsystems that support the initiative’s analytic requirements. \nIt’s also worth noting what a business process is not. Organizational business \ndepartments or functions do not equate to business processes. By focusing on pro-\ncesses, rather than on functional departments, consistent information is delivered \nmore economically throughout the organization. If you design departmentally bound \ndimensional models, you inevitably duplicate data with diff erent labels and data \nvalues. The best way to ensure consistency is to publish the data once.\n Step 2: Declare the Grain\nDeclaring the grain means specifying exactly what an individual fact table row \nrepresents. The grain conveys the level of detail associated with the fact table \nmeasurements. It provides the answer to the question, “How do you describe a \nsingle row in the fact table?” The grain is determined by the physical realities of \nthe operational system that captures the business process’s events.\nExample grain declarations include:\n \n■One row per scan of an individual product on a customer’s sales transaction \n \n■One row per line item on a bill from a doctor\n \n■One row per individual boarding pass scanned at an airport gate\n \n■One row per daily snapshot of the inventory levels for each item in a warehouse\n \n■One row per bank account each month\nThese grain declarations are expressed in business terms. Perhaps you were \nexpecting the grain to be a traditional declaration of the fact table’s primary key. \nAlthough the grain ultimately is equivalent to the primary key, it’s a mistake to list \na set of dimensions and then assume this list is the grain declaration. Whenever \npossible, you should express the grain in business terms. \nDimensional modelers sometimes try to bypass this seemingly unnecessary step \nof the four-step design process. Please don’t! Declaring the grain is a critical step that \ncan’t be taken lightly. In debugging thousands of dimensional designs over the years, \nthe most frequent error is not declaring the grain of the fact table at the beginning \nof the design process. If the grain isn’t clearly deﬁ ned, the whole design rests on \nquicksand; discussions about candidate dimensions go around in circles, and rogue \nfacts sneak into the design. An inappropriate grain haunts a DW/BI implementation! \nIt is extremely important that everyone on the design team reaches agreement on \nthe fact table’s granularity. Having said this, you may discover in steps 3 or 4 of the \ndesign process that the grain statement is wrong. This is okay, but then you must \nreturn to step 2, restate the grain correctly, and revisit steps 3 and 4  again.\n",
      "content_length": 2820,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 108,
      "content": "Chapter 3\n72\n Step 3: Identify the Dimensions\nDimensions  fall out of the question, “How do business people describe the data \nresulting from the business process measurement events?” You need to decorate \nfact tables with a robust set of dimensions representing all possible descriptions \nthat take on single values in the context of each measurement. If you are clear about \nthe grain, the dimensions typically can easily be identiﬁ ed as they represent the \n“who, what, where, when, why, and how” associated with the event. Examples of \ncommon dimensions include date, product, customer, employee, and facility. With \nthe choice of each dimension, you then list all the discrete, text-like attributes that \nﬂ esh out each dimension table. \n Step 4: Identify the Facts\nFacts are determined by answering the question, “What is the process measuring?” \nBusiness users are keenly interested in analyzing these performance metrics. All \ncandidate facts in a design must be true to the grain deﬁ ned in step 2. Facts that \nclearly belong to a diff erent grain must be in a separate fact table. Typical facts are \nnumeric additive ﬁ gures, such as quantity ordered or dollar cost amount.\nYou need to consider both your business users’ requirements and the realities \nof your source data in tandem to make decisions regarding the four steps, as illus-\ntrated in Figure 3-1. We strongly encourage you to resist the temptation to model \nthe data by looking at source data alone. It may be less intimidating to dive into the \ndata rather than interview a business person; however, the data is no substitute for \nbusiness user input. Unfortunately, many organizations have attempted this path-\nof-least-resistance data-driven approach but without much  success.\nDimensional Model\nBusiness Process\nGrain\nDimensions\nFacts\nData\nRealities\nBusiness\nRequirements\nFigure 3-1: Key input to the four-step dimensional design process. \nRetail Case Study\nLet’s  start with a brief description of the retail business used in this case study. We \nbegin with this industry simply because it is one we are all familiar with. But the \npatterns discussed in the context of this case study are relevant to virtually every \ndimensional model regardless of the industry.\n",
      "content_length": 2241,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 109,
      "content": "Retail Sales 73\nImagine you work in the headquarters of a large grocery chain. The business has \n100 grocery stores spread across ﬁ ve states. Each store has a full complement of \ndepartments, including grocery, frozen foods, dairy, meat, produce, bakery, ﬂ oral, \nand health/beauty aids. Each store has approximately 60,000 individual products, \ncalled stock keeping units (SKUs), on its  shelves. \nData  is collected at several interesting places in a grocery store. Some of the most \nuseful data is collected at the cash registers as customers purchase products. The point-\nof-sale (POS) system scans product barcodes at the cash register, measuring consumer \ntakeaway at the front door of the grocery store, as illustrated in Figure 3-2’s cash register \nreceipt. Other data is captured at the store’s back door where vendors make  deliveries. \nAllstar Grocery\n123 Loon Street\nGreen Prairie, MN 55555\n(952) 555-1212\nStore: 0022\nCashier: 00245409/Alan\n2.50\n4.99\n1.99\n3.19\n12.67\n12.67\n4/15/2013 10:56 AM\n4\n0030503347 Baked Well Multigrain Muffins\n2840201912 SoySoy Milk Quart\nTOTAL\nAMOUNT TENDERED\nCASH\nTransaction: 649\n0064900220415201300245409\nThank you for shopping at Allstar\nITEM COUNT:\n2120201195 Diet Cola 12-pack\n \nSaved $.50 off $5.49\n0070806048 Sparkly Toothpaste\n \nCoupon $.30 off $2.29\nFigure 3-2: Sample cash register receipt.\nAt the grocery store, management is concerned with the logistics of ordering, \nstocking, and selling products while maximizing proﬁ t. The proﬁ t ultimately comes \n",
      "content_length": 1505,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 110,
      "content": "Chapter 3\n74\nfrom charging as much as possible for each product, lowering costs for product \nacquisition and overhead, and at the same time attracting as many customers as \npossible in a highly competitive environment. Some of the most signiﬁ cant manage-\nment decisions have to do with pricing and promotions. Both store management \nand headquarters marketing spend a great deal of time tinkering with pricing and \npromotions. Promotions in a grocery store include temporary price reductions, ads \nin newspapers and newspaper inserts, displays in the grocery store, and coupons. \nThe most direct and eff ective way to create a surge in the volume of product sold \nis to lower the price dramatically. A 50-cent reduction in the price of paper towels, \nespecially when coupled with an ad and display, can cause the sale of the paper \ntowels to jump by a factor of 10. Unfortunately, such a big price reduction usually \nis not sustainable because the towels probably are being sold at a loss. As a result of \nthese issues, the visibility of all forms of promotion is an important part of analyz-\ning the operations of a grocery store.\nNow that we have described our business case study, we’ll begin to design the \ndimensional model.\nStep 1: Select the Business Process\nThe  ﬁ rst step in the design is to decide what business process to model by combin-\ning an understanding of the business requirements with an understanding of the \navailable source data.\nNOTE \nThe ﬁ rst DW/BI project should focus on the business process that is \nboth the most critical to the business users, as well as the most feasible. Feasibility \ncovers a range of considerations, including data availability and quality, as well as \norganizational readiness.\nIn our retail case study, management wants to better understand customer pur-\nchases as captured by the POS system. Thus the business process you’re modeling \nis POS retail sales transactions. This data enables the business users to analyze \nwhich products are selling in which stores on which days under what promotional \nconditions in which transactions. \nStep 2: Declare the Grain\nAfter  the business process has been identiﬁ ed, the design team faces a serious deci-\nsion about the granularity. What level of data detail should be made available in \nthe dimensional model? \nTackling  data at its lowest atomic grain makes sense for many reasons. Atomic \ndata is highly dimensional. The more detailed and atomic the fact measurement, \n",
      "content_length": 2471,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 111,
      "content": "Retail Sales 75\nthe more things you know for sure. All those things you know for sure translate \ninto dimensions. In this regard, atomic data is a perfect match for the dimensional \napproach.\nAtomic data provides maximum analytic ﬂ exibility because it can be con-\nstrained and rolled up in every way possible. Detailed data in a dimensional model \nis poised and ready for the ad hoc attack by business users.\nNOTE \nYou should develop dimensional models representing the most detailed, \natomic information captured by a business process. \nOf course, you could declare a more summarized granularity representing an \naggregation of the atomic data. However, as soon as you select a higher level grain, \nyou limit yourself to fewer and/or potentially less detailed dimensions. The less \ngranular model is immediately vulnerable to unexpected user requests to drill down \ninto the details. Users inevitably run into an analytic wall when not given access to \nthe atomic data. Although aggregated data plays an important role for performance \ntuning, it is not a substitute for giving users access to the lowest level details; users \ncan easily summarize atomic data, but it’s impossible to create details from sum-\nmary data. Unfortunately, some industry pundits remain confused about this point. \nThey claim dimensional models are only appropriate for summarized data and then \ncriticize the dimensional modeling approach for its supposed need to anticipate the \nbusiness question. This misunderstanding goes away when detailed, atomic data is \nmade available in a dimensional model.\nIn our case study, the most granular data is an individual product on a POS transac-\ntion, assuming the POS system rolls up all sales for a given product within a shopping \ncart into a single line item. Although users probably are not interested in analyzing \nsingle items associated with a speciﬁ c POS transaction, you can’t predict all the ways \nthey’ll want to cull through that data. For example, they may want to understand the \ndiff erence in sales on Monday versus Sunday. Or they may want to assess whether it’s \nworthwhile to stock so many individual sizes of certain brands. Or they may want \nto understand how many shoppers took advantage of the 50-cents-off  promotion on \nshampoo. Or they may want to determine the impact of decreased sales when a com-\npetitive diet soda product was promoted heavily. Although none of these queries calls \nfor data from one speciﬁ c transaction, they are broad questions that require detailed \ndata sliced in precise ways. None of them could have been answered if you elected to \nprovide access only to summarized data.\nNOTE \nA DW/BI system almost always demands data expressed at the lowest \npossible grain, not because queries want to see individual rows but because queries \nneed to cut through the details in very precise ways.\n",
      "content_length": 2861,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 112,
      "content": "Chapter 3\n76\nStep 3: Identify the Dimensions\nAfter  the grain of the fact table has been chosen, the choice of dimensions is straight-\nforward. The product and transaction fall out immediately. Within the framework \nof the primary dimensions, you can ask whether other dimensions can be attributed \nto the POS measurements, such as the date of the sale, the store where the sale \noccurred, the promotion under which the product is sold, the cashier who handled \nthe sale, and potentially the method of payment. We express this as another design \nprinciple.\nNOTE \nA careful grain statement determines the primary dimensionality of the \nfact table. You then add more dimensions to the fact table if these additional dimen-\nsions naturally take on only one value under each combination of the primary \ndimensions. If the additional dimension violates the grain by causing additional \nfact rows to be generated, the dimension needs to be disqualiﬁ ed or the grain state-\nment needs to be revisited.\nThe following descriptive dimensions apply to the case: date, product, store, \npromotion, cashier, and method of payment. In addition, the POS transaction ticket \nnumber is included as a special dimension, as described in the section “Degenerate \nDimensions for Transaction Numbers” later in this chapter.\nBefore ﬂ eshing out the dimension tables with descriptive attributes, let’s complete \nthe ﬁ nal step of the four-step process. You don’t want to lose sight of the forest for \nthe trees at this stage of the  design.\n Step 4: Identify the Facts\nThe  fourth and ﬁ nal step in the design is to make a careful determination of which \nfacts will appear in the fact table. Again, the grain declaration helps anchor your \nthinking. Simply put, the facts must be true to the grain: the individual product \nline item on the POS transaction in this case. When considering potential facts, \nyou may again discover adjustments need to be made to either your earlier grain \nassumptions or choice of dimensions.\nThe facts collected by the POS system include the sales quantity (for example, \nthe number of cans of chicken noodle soup), per unit regular, discount, and net \npaid prices, and extended discount and sales dollar amounts. The extended sales \ndollar amount equals the sales quantity multiplied by the net unit price. Likewise, \nthe extended discount dollar amount is the sales quantity multiplied by the unit \ndiscount amount. Some sophisticated POS systems also provide a standard dollar \ncost for the product as delivered to the store by the vendor. Presuming this cost \nfact is readily available and doesn’t require a heroic activity-based costing initiative, \n",
      "content_length": 2661,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 113,
      "content": "Retail Sales 77\nyou can include the extended cost amount in the fact table. The fact table begins \nto take shape in Figure 3-3.\nRetail Sales Fact\nDate Key (FK)\nProduct Key (FK)\nStore Key (FK)\nPromotion Key (FK)\nCashier Key (FK)\nPayment Method Key (FK)\nPOS Transaction # (DD)\nSales Quantity\nRegular Unit Price\nDiscount Unit Price\nNet Unit Price\nExtended Discount Dollar Amount\nExtended Sales Dollar Amount\nExtended Cost Dollar Amount\nExtended Gross Profit Dollar Amount\nDate Dimension\nProduct Dimension\nPromotion Dimension\nPayment Method Dimension\nStore Dimension\nCashier Dimension\nFigure 3-3: Measured facts in retail sales schema.\nFour of the facts, sales quantity and the extended discount, sales, and cost dollar \namounts, are beautifully additive across all the dimensions. You can slice and dice \nthe fact table by the dimension attributes with impunity, and every sum of these \nfour facts is valid and correct.\nDerived Facts\nYou  can compute the gross proﬁ t by subtracting the extended cost dollar amount \nfrom the extended sales dollar amount, or revenue. Although computed, gross proﬁ t \nis also perfectly additive across all the dimensions; you can calculate the gross \nproﬁ t of any combination of products sold in any set of stores on any set of days. \nDimensional modelers sometimes question whether a calculated derived fact should \nbe stored in the database. We generally recommend it be stored physically. In this \ncase study, the gross proﬁ t calculation is straightforward, but storing it means it’s \ncomputed consistently in the ETL process, eliminating the possibility of user cal-\nculation errors. The cost of a user incorrectly representing gross proﬁ t overwhelms \nthe minor incremental storage cost. Storing it also ensures all users and BI reporting \napplications refer to gross proﬁ t consistently. Because gross proﬁ t can be calculated \nfrom adjacent data within a single fact table row, some would argue that you should \nperform the calculation in a view that is indistinguishable from the table. This is \na reasonable approach if all users access the data via the view and no users with \nad hoc query tools can sneak around the view to get at the physical table. Views \nare a reasonable way to minimize user error while saving on storage, but the DBA \n",
      "content_length": 2282,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 114,
      "content": "Chapter 3\n78\nmust allow no exceptions to accessing the data through the view. Likewise, some \norganizations want to perform the calculation in the BI tool. Again, this works if all \nusers access the data using a common tool, which is seldom the case in our expe-\nrience. However, sometimes non-additive metrics on a report such as percentages \nor ratios must be computed in the BI application because the calculation cannot \nbe precalculated and stored in a fact table. OLAP cubes excel in these situations.\nNon-Additive Facts\nGross  margin can be calculated by dividing the gross proﬁ t by the extended sales \ndollar revenue. Gross margin is a non-additive fact because it can’t be summarized \nalong any dimension. You can calculate the gross margin of any set of products, \nstores, or days by remembering to sum the revenues and costs respectively before \ndividing. \nNOTE \nPercentages and ratios, such as gross margin, are non-additive. The \nnumerator and denominator should be stored in the fact table. The ratio can then \nbe calculated in a BI tool for any slice of the fact table by remembering to calculate \nthe ratio of the sums, not the sum of the ratios.\nUnit  price is another non-additive fact. Unlike the extended amounts in the fact \ntable, summing unit price across any of the dimensions results in a meaningless, \nnonsensical number. Consider this simple example: You sold one widget at a unit \nprice of $1.00 and four widgets at a unit price of 50 cents each. You could sum \nthe sales quantity to determine that ﬁ ve widgets were sold. Likewise, you could \nsum the sales dollar amounts ($1.00 and $2.00) to arrive at a total sales amount \nof $3.00. However, you can’t sum the unit prices ($1.00 and 50 cents) and declare \nthat the total unit price is $1.50. Similarly, you shouldn’t announce that the average \nunit price is 75 cents. The properly weighted average unit price should be calcu-\nlated by taking the total sales amount ($3.00) and dividing by the total quantity \n(ﬁ ve widgets) to arrive at a 60 cent average unit price. You’d never arrive at this \nconclusion by looking at the unit price for each transaction line in isolation. To \nanalyze the average price, you must add up the sales dollars and sales quantities \nbefore dividing the total dollars by the total quantity sold. Fortunately, many BI \ntools perform this function correctly. Some question whether non-additive facts \nshould be physically stored in a fact table. This is a legitimate question given \ntheir limited analytic value, aside from printing individual values on a report or \napplying a ﬁ lter directly on the fact, which are both atypical. In some situations, \na fundamentally non-additive fact such as a temperature is supplied by the source \nsystem. These non-additive facts may be averaged carefully over many records, if \nthe business analysts agree that this makes  sense.\n",
      "content_length": 2878,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 115,
      "content": "Retail Sales 79\nTransaction Fact Tables\nTransactional business processes are the most common. The fact tables representing \nthese processes share several characteristics:\n \n■The grain of atomic transaction fact tables can be succinctly expressed in the \ncontext of the transaction, such as one row per transaction or one row per \ntransaction line.\n \n■Because these fact tables record a transactional event, they are often sparsely \npopulated. In our case study, we certainly wouldn’t sell every product in \nevery shopping cart.\n \n■Even though transaction fact tables are unpredictably and sparsely populated, \nthey can be truly enormous. Most billion and trillion row tables in a data \nwarehouse are transaction fact tables.\n \n■Transaction fact tables tend to be highly dimensional.\n \n■The metrics resulting from transactional events are typically additive as long \nas they have been extended by the quantity amount, rather than capturing \nper unit metrics.\nAt this early stage of the design, it is often helpful to estimate the number of rows \nin your largest table, the fact table. In this case study, it simply may be a matter of \ntalking with a source system expert to understand how many POS transaction line \nitems are generated on a periodic basis. Retail traffi  c ﬂ uctuates signiﬁ cantly from \nday to day, so you need to understand the transaction activity over a reasonable \nperiod of time. Alternatively, you could estimate the number of rows added to the \nfact table annually by dividing the chain’s annual gross revenue by the average item \nselling price. Assuming that gross revenues are $4 billion per year and that the aver-\nage price of an item on a customer ticket is $2.00, you can calculate that there are \napproximately 2 billion transaction line items per year. This is a typical engineer’s \nestimate that gets you surprisingly close to sizing a design directly from your arm-\nchair. As designers, you always should be triangulating to determine whether your \ncalculations are  reasonable.\n Dimension Table Details\nNow that we’ve walked through the four-step process, let’s return to the dimension \ntables and focus on populating them with robust attributes.\n Date Dimension\nThe  date dimension is a special dimension because it is the one dimension nearly \nguaranteed to be in every dimensional model since virtually every business process \n",
      "content_length": 2365,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 116,
      "content": "Chapter 3\n80\ncaptures a time series of performance metrics. In fact, date is usually the ﬁ rst dimen-\nsion in the underlying partitioning scheme of the database so that the successive \ntime interval data loads are placed into virgin territory on the disk.\nFor  readers of the ﬁ rst edition of The Data Warehouse Toolkit (Wiley, 1996), this \ndimension was referred to as the time dimension. However, for more than a decade, \nwe’ve used the “date dimension” to mean a daily grained dimension table. This helps \ndistinguish between date and time-of-day dimensions.\nUnlike most of the other dimensions, you can build the date dimension table in \nadvance. You may put 10 or 20 years of rows representing individual days in the table, \nso you can cover the history you have stored, as well as several years in the future. \nEven 20 years’ worth of days is only approximately 7,300 rows, which is a relatively \nsmall dimension table. For a daily date dimension table in a retail environment, we \nrecommend the partial list of columns shown in Figure 3-4.\nDate Dimension\nDate Key (PK)\nDate\nFull Date Description\nDay of Week\nDay Number in Calendar Month\nDay Number in Calendar Year\nDay Number in Fiscal Month\nDay Number in Fiscal Year\nLast Day in Month Indicator\nCalendar Week Ending Date\nCalendar Week Number in Year\nCalendar Month Name\nCalendar Month Number in Year\nCalendar Year-Month (YYYY-MM)\nCalendar Quarter\nCalendar Year-Quarter\nCalendar Year\nFiscal Week\nFiscal Week Number in Year\nFiscal Month\nFiscal Month Number in Year\nFiscal Year-Month\nFiscal Quarter\nFiscal Year-Quarter\nFiscal Half Year\nFiscal Year\nHoliday Indicator\nWeekday Indicator\nSQL Date Stamp\n...\nFigure 3-4: Date dimension table.\n",
      "content_length": 1692,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 117,
      "content": "Retail Sales 81\nEach column in the date dimension table is deﬁ ned by the particular day that the \nrow represents. The day-of-week column contains the day’s name, such as Monday. \nThis column would be used to create reports comparing Monday business with \nSunday business. The day number in calendar month column starts with 1 at the \nbeginning of each month and runs to 28, 29, 30, or 31 depending on the month. \nThis column is useful for comparing the same day each month. Similarly, you could \nhave a month number in year (1, . . ., 12). All these integers support simple date \narithmetic across year and month boundaries. \nFor reporting, you should include both long and abbreviated labels. For exam-\nple, you would want a month name attribute with values such as January. In \naddition, a year-month (YYYY-MM) column is useful as a report column header. \nYou likely also want a quarter number (Q1, . . ., Q4), as well as a year-quarter, \nsuch as 2013-Q1. You would include similar columns for the ﬁ scal periods if \nthey diff er from calendar periods. Sample rows containing several date dimen-\nsion columns are illustrated in Figure 3-5.\nJanuary 1, 2013\nJanuary 2, 2013\nJanuary 3, 2013\nJanuary 4, 2013\nJanuary 5, 2013\nJanuary 6, 2013\nJanuary 7, 2013\nJanuary 8, 2013\nJanuary\nJanuary\nJanuary\nJanuary\nJanuary\nJanuary\nJanuary\nJanuary\nQ1\nQ1\nQ1\nQ1\nQ1\nQ1\nQ1\nQ1\n2013\n2013\n2013\n2013\n2013\n2013\n2013\n2013\nF2013-01\nF2013-01\nF2013-01\nF2013-01\nF2013-01\nF2013-01\nF2013-01\nF2013-01\nHoliday\nNon-Holiday\nNon-Holiday\nNon-Holiday\nNon-Holiday\nNon-Holiday\nNon-Holiday\nNon-Holiday\nWeekday\nWeekday\nWeekday\nWeekday\nWeekday\nWeekday\nWeekday\nWeekday\nTuesday\nWednesday\nThursday\nFriday\nSaturday\nSunday\nMonday\nTuesday\n20130101\n20130102\n20130103\n20130104\n20130105\n20130106\n20130107\n20130108\n01/01/2013\n01/02/2013\n01/03/2013\n01/04/2013\n01/05/2013\n01/06/2013\n01/07/2013\n01/08/2013\nDate Key\nDate\nFull Date\nDescription\nDay of\nWeek\nCalendar\nMonth\nCalendar\nQuarter\nCalendar\nYear\nFiscal Year-\nMonth\nHoliday\nIndicator\nWeekday\nIndicator\nFigure 3-5: Date dimension sample rows.\nNOTE \nA sample date dimension is available at www.kimballgroup.com under \nthe Tools and Utilities tab for this book title.\nSome designers pause at this point to ask why an explicit date dimension table is \nneeded. They reason that if the date key in the fact table is a date type column, then \nany SQL query can directly constrain on the fact table date key and use natural SQL \ndate semantics to ﬁ lter on month or year while avoiding a supposedly expensive \njoin. This reasoning falls apart for several reasons. First, if your relational database \ncan’t handle an effi  cient join to the date dimension table, you’re in deep trouble. \nMost database optimizers are quite effi  cient at resolving dimensional queries; it is \nnot necessary to avoid joins like the plague. \nSince the average business user is not versed in SQL date semantics, he would \nbe unable to request typical calendar groupings. SQL date functions do not support \n",
      "content_length": 2976,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 118,
      "content": "Chapter 3\n82\nﬁ ltering by attributes such as weekdays versus weekends, holidays, ﬁ scal periods, \nor seasons. Presuming the business needs to slice data by these nonstandard date \nattributes, then an explicit date dimension table is essential. Calendar logic belongs \nin a dimension table, not in the application code. \nNOTE \nDimensional models always need an explicit date dimension table. There \nare many date attributes not supported by the SQL date function, including week \nnumbers, ﬁ scal periods, seasons, holidays, and weekends. Rather than attempting \nto determine these nonstandard calendar calculations in a query, you should look \nthem up in a date dimension table.\n Flags and Indicators as Textual Attributes\nLike  many operational ﬂ ags and indicators, the date dimension’s holiday indicator \nis a simple indicator with two potential values. Because dimension table attributes \nserve as report labels and values in pull-down query ﬁ lter lists, this indicator should \nbe populated with meaningful values such as Holiday or Non-holiday instead of \nthe cryptic Y/N, 1/0, or True/False. As illustrated in Figure 3-6, imagine a report \ncomparing holiday versus non-holiday sales for a product. More meaningful domain \nvalues for this indicator translate into a more meaningful, self-explanatory report. \nRather than decoding ﬂ ags into understandable labels in the BI application, we prefer \nthat decoded values be stored in the database so they’re consistently available to all \nusers regardless of their BI reporting environment or tools.\nMonthly Sales\nExtended Sales\nDollar Amount\nHoliday\nIndicator\nHoliday\nIndicator\nOR\nPeriod:\nProduct\nJune 2013\nBaked Well Sourdough\nN\nY\n1,009\n6,298\nMonthly Sales\nExtended Sales\nDollar Amount\nPeriod:\nProduct\nJune 2013\nBaked Well Sourdough\nHoliday\nNon-holiday\n6,298\n1,009\nFigure 3-6: Sample reports with cryptic versus textual indicators.\nA  similar argument holds true for the weekday indicator that would have a value \nof Weekday or Weekend. Saturdays and Sundays obviously would be assigned the \nweekend value. Of course, multiple date table attributes can be jointly constrained, \nso you can easily compare weekday holidays with weekend  holidays.\nCurrent and Relative Date Attributes\nMost  date dimension attributes are not subject to updates. June 1, 2013 will always \nroll up to June, Calendar Q2, and 2013. However, there are attributes you can add \n",
      "content_length": 2404,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 119,
      "content": "Retail Sales 83\nto the basic date dimension that will change over time, including IsCurrentDay, \nIsCurrentMonth, IsPrior60Days, and so on. IsCurrentDay obviously must be updated \neach day; the attribute is useful for generating reports that always run for today. A \nnuance to consider is the day that IsCurrentDay refers to. Most data warehouses \nload data daily, so IsCurrentDay would refer to yesterday (or more accurately, the \nmost recent day loaded). You might also add attributes to the date dimension that \nare unique to your corporate calendar, such as IsFiscalMonthEnd.\nSome date dimensions include updated lag attributes. The lag day column would \ntake the value 0 for today, –1 for yesterday, +1 for tomorrow, and so on. This attribute \ncould easily be a computed column rather than physically stored. It might be useful \nto set up similar structures for month, quarter, and year. Many BI tools include func-\ntionality to do prior period calculations, so these lag columns may be unnecessary.\nTime-of-Day as a Dimension or Fact\nAlthough date and time are comingled in an operational date/time stamp, time-of-\nday is typically separated from the date dimension to avoid a row count explosion \nin the date dimension. As noted earlier, a date dimension with 20 years of history \ncontains approximately 7,300 rows. If you changed the grain of this dimension to \none row per minute in a day, you’d end up with over 10 million rows to accommodate \nthe 1,440 minutes per day. If you tracked time to the second, you’d have more than \n31 million rows per year! Because the date dimension is likely the most frequently \nconstrained dimension in a schema, it should be kept as small and manageable as \npossible.\nIf  you want to ﬁ lter or roll up time periods based on summarized day part group-\nings, such as activity during 15-minute intervals, hours, shifts, lunch hour, or prime \ntime, time-of-day would be treated as a full-ﬂ edged dimension table with one row per \ndiscrete time period, such as one row per minute within a 24-hour period resulting \nin a dimension with 1,440 rows. \nIf there’s no need to roll up or ﬁ lter on time-of-day groupings, time-of-day should \nbe handled as a simple date/time fact in the fact table. By the way, business users \nare often more interested in time lags, such as the transaction’s duration, rather \nthan discreet start and stop times. Time lags can easily be computed by taking the \ndiff erence between date/time stamps. These date/time stamps also allow an applica-\ntion to determine the time gap between two transactions of interest, even if these \ntransactions exist in diff erent days, months, or  years.\nProduct Dimension \nThe  product dimension describes every SKU in the grocery store. Although a typi-\ncal store may stock 60,000 SKUs, when you account for diff erent merchandising \nschemes and historical products that are no longer available, the product dimension \n",
      "content_length": 2918,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 120,
      "content": "Chapter 3\n84\nmay have 300,000 or more rows. The product dimension is almost always sourced \nfrom the operational product master ﬁ le. Most retailers administer their product \nmaster ﬁ le at headquarters and download a subset to each store’s POS system at \nfrequent intervals. It is headquarters’ responsibility to deﬁ ne the appropriate product \nmaster record (and unique SKU number) for each new product. \n Flatten Many-to-One Hierarchies\nThe  product dimension represents the many descriptive attributes of each SKU. The \nmerchandise hierarchy is an important group of attributes. Typically, individual \nSKUs roll up to brands, brands roll up to categories, and categories roll up to depart-\nments. Each of these is a many-to-one relationship. This merchandise hierarchy and \nadditional attributes are shown for a subset of products in Figure 3-7.\nBaked Well\nFluffy\nFluffy\nLight\nColdpack\nFreshlike\nFrigid\nIcy\nIcy\nBread\nBread\nBread\nSweeten Bread\nFrozen Desserts\nFrozen Desserts\nFrozen Desserts\nFrozen Desserts\nFrozen Desserts\nBakery\nBakery\nBakery\nBakery\nFrozen Foods\nFrozen Foods\nFrozen Foods\nFrozen Foods\nFrozen Foods\nReduced Fat\nRegular Fat\nReduced Fat\nNon-Fat\nNon-Fat\nReduced Fat\nRegular Fat\nRegular Fat\nRegular Fat\nFresh\nPre-Packaged\nPre-Packaged\nPre-Packaged\nIce Cream\nIce Cream\nIce Cream\nIce Cream\nNovelties\n1 \n2 \n3 \n4 \n5 \n6 \n7 \n8 \n9\nBaked Well Light Sourdough Fresh Bread\nFluffy Sliced Whole Wheat\nFluffy Light Sliced Whole Wheat\nLight Mini Cinnamon Rolls\nDiet Lovers Vanilla 2 Gallon\nLight and Creamy Butter Pecan 1 Pint\nChocolate Lovers 1/2 Gallon\nStrawberry Ice Creamy 1 Pint\nIcy Ice Cream Sandwiches\nProduct\nKey\nProduct Description\nBrand\nDescription\nSubcategory\nDescription\nCategory\nDescription\nDepartment\nDescription\nFat Content\nFigure 3-7: Product dimension sample rows.\nFor each SKU, all levels of the merchandise hierarchy are well deﬁ ned. Some \nattributes, such as the SKU description, are unique. In this case, there are 300,000 \ndiff erent values in the SKU description column. At the other extreme, there are only \nperhaps 50 distinct values of the department attribute. Thus, on average, there are \n6,000 repetitions of each unique value in the department attribute. This is perfectly \nacceptable! You do not need to separate these repeated values into a second nor-\nmalized table to save space. Remember dimension table space requirements pale in \ncomparison with fact table space considerations.\nNOTE \nKeeping the repeated low cardinality values in the primary dimension \ntable is a fundamental dimensional modeling technique. Normalizing these values \ninto separate tables defeats the primary goals of simplicity and performance, as \ndiscussed in “Resisting Normalization Urges” later in this chapter.\nMany of the attributes in the product dimension table are not part of the mer-\nchandise hierarchy. The package type attribute might have values such as Bottle, \nBag, Box, or Can. Any SKU in any department could have one of these values. \n",
      "content_length": 2965,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 121,
      "content": "Retail Sales 85\nIt often makes sense to combine a constraint on this attribute with a constraint \non a merchandise hierarchy attribute. For example, you could look at all the SKUs \nin the Cereal category packaged in Bags. Put another way, you can browse among \ndimension attributes regardless of whether they belong to the merchandise hier-\narchy. Product dimension tables typically have more than one explicit hierarchy. \nA recommended partial product dimension for a retail grocery dimensional model \nis shown in Figure 3-8.\nProduct Key (PK)\nSKU Number (NK)\nProduct Description\nBrand Description\nSubcategory Description\nCategory Description\nDepartment Number\nDepartment Description\nPackage Type Description\nPackage Size\nFat Content\nDiet Type\nWeight\nWeight Unit of Measure\nStorage Type\nShelf Life Type\nShelf Width\nShelf Height\nShelf Depth\n...\nProduct Dimension\nFigure 3-8: Product dimension table.\nAttributes with Embedded Meaning\nOften  operational product codes, identiﬁ ed in the dimension table by the NK notation \nfor natural key, have embedded meaning with diff erent parts of the code representing \nsigniﬁ cant characteristics of the product. In this case, the multipart attribute should \nbe both preserved in its entirety within the dimension table, as well as broken down \ninto its component parts, which are handled as separate attributes. For example, if \nthe ﬁ fth through ninth characters in the operational code identify the manufacturer, \nthe manufacturer’s name should also be included as a dimension table attribute.\n Numeric Values as Attributes or Facts\nYou  will sometimes encounter numeric values that don’t clearly fall into either the \nfact or dimension attribute categories. A classic example is the standard list price \n",
      "content_length": 1746,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 122,
      "content": "Chapter 3\n86\nfor a product. It’s deﬁ nitely a numeric value, so the initial instinct is to place it in \nthe fact table. But typically the standard price changes infrequently, unlike most \nfacts that are often diff erently valued on every measurement event.\nIf the numeric value is used primarily for calculation purposes, it likely belongs \nin the fact table. Because standard price is non-additive, you might multiply it by \nthe quantity for an extended amount which would be additive. Alternatively, if the \nstandard price is used primarily for price variance analysis, perhaps the variance \nmetric should be stored in the fact table instead. If the stable numeric value is used \npredominantly for ﬁ ltering and grouping, it should be treated as a product dimen-\nsion attribute.\nSometimes numeric values serve both calculation and ﬁ ltering/grouping func-\ntions. In these cases, you should store the value in both the fact and dimension \ntables. Perhaps the standard price in the fact table represents the valuation at the \ntime of the sales transaction, whereas the dimension attribute is labeled to indicate \nit’s the current standard price.\nNOTE \nData elements that are used both for fact calculations and dimension \nconstraining, grouping, and labeling should be stored in both locations, even \nthough a clever programmer could write applications that access these data \nelements from a single location. It is important that dimensional models be as \nconsistent as possible and application development be predictably simple. Data \ninvolved in calculations should be in fact tables and data involved in constraints, \ngroups and labels should be in dimension tables.\n Drilling Down on Dimension Attributes\nA  reasonable product dimension table can have 50 or more descriptive attributes. \nEach attribute is a rich source for constraining and constructing row header labels. \nDrilling down is nothing more than asking for a row header from a dimension that \nprovides more information. \nLet’s say you have a simple report summarizing the sales dollar amount by depart-\nment. As illustrated in Figure 3-9, if you want to drill down, you can drag any \nother attribute, such as brand, from the product dimension into the report next to \ndepartment, and you can automatically drill down to this next level of detail. You \ncould drill down by the fat content attribute, even though it isn’t in the merchandise \nhierarchy rollup.\nNOTE \nDrilling down in a dimensional model is nothing more than adding row \nheader attributes from the dimension tables. Drilling up is removing row headers. \nYou can drill down or up on attributes from more than one explicit hierarchy and \nwith attributes that are part of no hierarchy.\n",
      "content_length": 2714,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 123,
      "content": "Retail Sales 87\nDepartment\nName\nSales Dollar\nAmount\nBakery\nFrozen Foods\n12,331\n31,776\nDrill down by brand name:\nBakery\nBakery\nBakery\nFrozen Foods\nFrozen Foods\nFrozen Foods\nFrozen Foods\nFrozen Foods\nBaked Well\nFluffy\nLight\nColdpack\nFreshlike\nFrigid\nIcy\nQuickFreeze\n3,009 \n3,024 \n6,298 \n5,321 \n10,476 \n7,328 \n2,184 \n6,467\nDepartment\nName\nBrand\nName\nSales Dollar\nAmount\nOr drill down by fat content:\nBakery\nBakery\nBakery\nFrozen Foods\nFrozen Foods\nFrozen Foods\nNonfat\nReduced fat\nRegular fat\nNonfat\nReduced fat\nRegular fat\n6,298 \n5,027 \n1,006 \n5,321 \n10,476 \n15,979\nDepartment\nName\nFat\nContent\nSales Dollar\nAmount\nFigure 3-9: Drilling down on dimension attributes.\nThe product dimension is a common dimension in many dimensional models. \nGreat care should be taken to ﬁ ll this dimension with as many descriptive attributes \nas possible. A robust and complete set of dimension attributes translates into robust \nand complete analysis capabilities for the business users. We’ll further explore the \nproduct dimension in Chapter 5: Procurement where we’ll also discuss the handling \nof product attribute changes.\nStore Dimension \nThe  store dimension describes every store in the grocery chain. Unlike the product \nmaster ﬁ le that is almost guaranteed to be available in every large grocery business, \nthere may not be a comprehensive store master ﬁ le. POS systems may simply sup-\nply a store number on the transaction records. In these cases, project teams must \nassemble the necessary components of the store dimension from multiple opera-\ntional sources. Often there will be a store real estate department at headquarters \nwho will help deﬁ ne a detailed store master ﬁ le.\n",
      "content_length": 1673,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 124,
      "content": "Chapter 3\n88\n Multiple Hierarchies in Dimension Tables\nThe  store dimension is the case study’s primary geographic dimension. Each store \ncan be thought of as a location. You can roll stores up to any geographic attribute, \nsuch as ZIP code, county, and state in the United States. Contrary to popular \nbelief, cities and states within the United States are not a hierarchy. Since many \nstates have identically named cities, you’ll want to include a City-State attribute \nin the store dimension.\nStores likely also roll up an internal organization hierarchy consisting of store \ndistricts and regions. These two diff erent store hierarchies are both easily repre-\nsented in the dimension because both the geographic and organizational hierarchies \nare well deﬁ ned for a single store row. \nNOTE \nIt is not uncommon to represent multiple hierarchies in a dimension \ntable. The attribute names and values should be unique across the multiple \nhierarchies.\nA recommended retail store dimension table is shown in Figure 3-10.\nStore Key (PK)\nStore Number (NK)\nStore Name\nStore Street Address\nStore City\nStore County\nStore City-State\nStore State\nStore Zip Code\nStore Manager\nStore District\nStore Region\nFloor Plan Type\nPhoto Processing Type\nFinancial Service Type\nSelling Square Footage\nTotal Square Footage\nFirst Open Date\nLast Remodel Date\n...\nStore Dimension\nFigure 3-10: Store dimension table.\nThe ﬂ oor plan type, photo processing type, and ﬁ nance services type are all short \ntext descriptors that describe the particular store. These should not be one-character \ncodes but rather should be 10- to 20-character descriptors that make sense when \nviewed in a pull-down ﬁ lter list or used as a report label.\n",
      "content_length": 1707,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 125,
      "content": "Retail Sales 89\nThe column describing selling square footage is numeric and theoretically addi-\ntive across stores. You might be tempted to place it in the fact table. However, it is \nclearly a constant attribute of a store and is used as a constraint or label more often \nthan it is used as an additive element in a summation. For these reasons, selling \nsquare footage belongs in the store dimension table.\nDates Within Dimension Tables\nThe ﬁ rst open date and last remodel date in the store dimension could be date type \ncolumns. However, if users want to group and constrain on nonstandard calendar \nattributes (like the open date’s ﬁ scal period), then they are typically join keys to \ncopies of the date dimension table. These date dimension copies are declared in SQL \nby the view construct and are semantically distinct from the primary date dimen-\nsion. The view declaration would look like the following:\ncreate view first_open_date (first_open_day_number, first_open_month, \n...)\n    as select day_number, month, ... \n    from date\nNow  the system acts as if there is another physical copy of the date dimension \ntable called FIRST_OPEN_DATE. Constraints on this new date table have nothing to \ndo with constraints on the primary date dimension joined to the fact table. The ﬁ rst \nopen date view is a permissible outrigger to the store dimension; outriggers will be \ndescribed in more detail later in this chapter. Notice we have carefully relabeled all \nthe columns in the view so they cannot be confused with columns from the primary \ndate dimension. These distinct logical views on a single physical date dimension are \nan example of dimension role playing, which we’ll discuss more fully in Chapter 6: \nOrder  Management.\nPromotion Dimension\nThe  promotion dimension is potentially the most interesting dimension in the \nretail sales schema. The promotion dimension describes the promotion condi-\ntions under which a product is sold. Promotion conditions include temporary \nprice reductions, end aisle displays, newspaper ads, and coupons. This dimension \nis often called a causal dimension because it describes factors thought to cause a \nchange in product sales.\nBusiness analysts at both headquarters and the stores are interested in determin-\ning whether a promotion is eff ective. Promotions are judged on one or more of the \nfollowing factors:\n \n■Whether  the products under promotion experienced a gain in sales, called \nlift, during the promotional period. The lift can be measured only if the store \ncan agree on what the baseline sales of the promoted products would have \n",
      "content_length": 2599,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 126,
      "content": "Chapter 3\n90\nbeen without the promotion. Baseline values can be estimated from prior sales \nhistory and, in some cases, with the help of sophisticated models.\n \n■Whether  the products under promotion showed a drop in sales just prior to \nor after the promotion, canceling the gain in sales during the promotion (time \nshifting). In other words, did you transfer sales from regularly priced products \nto temporarily reduced priced products?\n \n■Whether  the products under promotion showed a gain in sales but other \nproducts nearby on the shelf showed a corresponding sales decrease \n(cannibalization).\n \n■Whether  all the products in the promoted category of products experienced a \nnet overall gain in sales taking into account the time periods before, during, \nand after the promotion (market growth).\n \n■Whether the promotion was proﬁ table. Usually the proﬁ t of a promotion is \ntaken to be the incremental gain in proﬁ t of the promoted category over the \nbaseline sales taking into account time shifting and cannibalization, as well \nas the costs of the promotion.\nThe causal conditions potentially aff ecting a sale are not necessarily tracked \ndirectly by the POS system. The transaction system keeps track of price reduc-\ntions and markdowns. The presence of coupons also typically is captured with \nthe transaction because the customer either presents coupons at the time of sale \nor does not. Ads and in-store display conditions may need to be linked from other \nsources. \nThe various possible causal conditions are highly correlated. A temporary price \nreduction usually is associated with an ad and perhaps an end aisle display. For \nthis reason, it makes sense to create one row in the promotion dimension for each \ncombination of promotion conditions that occurs. Over the course of a year, there \nmay be 1,000 ads, 5,000 temporary price reductions, and 1,000 end aisle displays, \nbut there may be only 10,000 combinations of these three conditions aff ecting any \nparticular product. For example, in a given promotion, most of the stores would run \nall three promotion mechanisms simultaneously, but a few of the stores may not \ndeploy the end aisle displays. In this case, two separate promotion condition rows \nwould be needed, one for the normal price reduction plus ad plus display and one \nfor the price reduction plus ad only. A recommended promotion dimension table \nis shown in Figure 3-11.\nFrom a purely logical point of view, you could record similar information about \nthe promotions by separating the four causal mechanisms (price reductions, ads, \ndisplays, and coupons) into separate dimensions rather than combining them into \none dimension. Ultimately, this choice is the designer’s prerogative. The trade-off s \nin favor of keeping the four dimensions together include the following:\n",
      "content_length": 2817,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 127,
      "content": "Retail Sales 91\n \n■If the four causal mechanisms are highly correlated, the combined single \ndimension is not much larger than any one of the separated dimensions \nwould be.\n \n■The combined single dimension can be browsed effi  ciently to see how the vari-\nous price reductions, ads, displays, and coupons are used together. However, \nthis browsing only shows the possible promotion combinations. Browsing in \nthe dimension table does not reveal which stores or products were aff ected \nby the promotion; this information is found in the fact table.\nPromotion Key (PK)\nPromotion Code\nPromotion Name\nPrice Reduction Type\nPromotion Media Type\nAd Type\nDisplay Type\nCoupon Type\nAd Media Name\nDisplay Provider\nPromotion Cost\nPromotion Begin Date\nPromotion End Date\n...\nPromotion Dimension\nFigure 3-11: Promotion dimension table.\nThe trade-off s in favor of separating the causal mechanisms into four distinct \ndimension tables include the following:\n \n■The separated dimensions may be more understandable to the business com-\nmunity if users think of these mechanisms separately. This would be revealed \nduring the business requirement interviews.\n \n■Administration of the separate dimensions may be more straightforward than \nadministering a combined dimension.\nKeep in mind there is no diff erence in the content between these two choices.\nNOTE \nThe inclusion of promotion cost attribute in the promotion dimension \nshould be done with careful thought. This attribute can be used for constraining \nand grouping. However, this cost should not appear in the POS transaction fact \ntable representing individual product sales because it is at the wrong grain; this \ncost would have to reside in a fact table whose grain is the overall promotion.\n",
      "content_length": 1739,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 128,
      "content": "Chapter 3\n92\n Null Foreign Keys, Attributes, and Facts\nTypically,  many sales transactions include products that are not being promoted. \nHopefully, consumers aren’t just ﬁ lling their shopping cart with promoted products; \nyou want them paying full price for some products in their cart! The promotion \ndimension must include a row, with a unique key such as 0 or –1, to identify this \nno promotion condition and avoid a null promotion key in the fact table. Referential \nintegrity is violated if you put a null in a fact table column declared as a foreign key \nto a dimension table. In addition to the referential integrity alarms, null keys are \nthe source of great confusion to users because they can’t join on null keys.\nWARNING \nYou must avoid null keys in the fact table. A proper design includes \na row in the corresponding dimension table to identify that the dimension is not \napplicable to the measurement.\nWe  sometimes encounter nulls as dimension attribute values. These usually result \nwhen a given dimension row has not been fully populated, or when there are attri-\nbutes that are not applicable to all the dimension’s rows. In either case, we recom-\nmend substituting a descriptive string, such as Unknown or Not Applicable, in place \nof the null value. Null values essentially disappear in pull-down menus of possible \nattribute values or in report groupings; special syntax is required to identify them. \nIf users sum up facts by grouping on a fully populated dimension attribute, and then \nalternatively, sum by grouping on a dimension attribute with null values, they’ll get \ndiff erent query results. And you’ll get a phone call because the data doesn’t appear to \nbe consistent. Rather than leaving the attribute null, or substituting a blank space or \na period, it’s best to label the condition; users can then purposely decide to exclude \nthe Unknown or Not Applicable from their query. It’s worth noting that some OLAP \nproducts prohibit null attribute values, so this is one more reason to avoid them.\nFinally, we can also encounter nulls as metrics in the fact table. We generally \nleave these null so that they’re properly handled in aggregate functions such as SUM, \nMIN, MAX, COUNT, and AVG which do the “right thing” with nulls. Substituting a zero \ninstead would improperly skew these aggregated calculations.\nData  mining tools may use diff erent techniques for tracking nulls. You may need \nto do some additional transformation work beyond the above recommendations if \ncreating an observation set for data  mining.\nOther Retail Sales Dimensions\nAny  descriptive attribute that takes on a single value in the presence of a fact table \nmeasurement event is a good candidate to be added to an existing dimension or \n",
      "content_length": 2750,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 129,
      "content": "Retail Sales 93\nbe its own dimension. The decision whether a dimension should be associated \nwith a fact table should be a binary yes/no based on the fact table’s declared \ngrain. For example, there’s probably a cashier identiﬁ ed for each transaction. The \ncorresponding cashier dimension would likely contain a small subset of non-\nprivate employee attributes. Like the promotion dimension, the cashier dimension \nwill likely have a No Cashier row for transactions that are processed through \nself-service registers.\nA  trickier situation unfolds for the payment method. Perhaps the store has rigid \nrules and only accepts one payment method per transaction. This would make \nyour life as a dimensional modeler easier because you’d attach a simple payment \nmethod dimension to the sales schema that would likely include a payment method \ndescription, along with perhaps a grouping of payment methods into either cash \nequivalent or credit payment types.\nIn real life, payment methods often present a more complicated scenario. If \nmultiple payment methods are accepted on a single POS transaction, the payment \nmethod does not take on a single value at the declared grain. Rather than altering \nthe declared grain to be something unnatural such as one row per payment method \nper product on a POS transaction, you would likely capture the payment method in \na separate fact table with a granularity of either one row per transaction (then the \nvarious payment method options would appear as separate facts) or one row per \npayment method per transaction (which would require a separate payment method \ndimension to associate with each  row).\n Degenerate Dimensions for Transaction Numbers\nThe  retail sales fact table includes the POS transaction number on every line item \nrow. In an operational parent/child database, the POS transaction number would \nbe the key to the transaction header record, containing all the information valid \nfor the transaction as a whole, such as the transaction date and store identiﬁ er. \nHowever, in the dimensional model, you have already extracted this interesting \nheader information into other dimensions. The POS transaction number is still \nuseful because it serves as the grouping key for pulling together all the products \npurchased in a single market basket transaction. It also potentially enables you to \nlink back to the operational system.\nAlthough the POS transaction number looks like a dimension key in the fact \ntable, the descriptive items that might otherwise fall in a POS transaction dimension \nhave been stripped off . Because the resulting dimension is empty, we refer to the \nPOS transaction number as a degenerate dimension (identiﬁ ed by the DD notation \n",
      "content_length": 2716,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 130,
      "content": "Chapter 3\n94\nin this book’s ﬁ gures). The natural operational ticket number, such as the POS \ntransaction number, sits by itself in the fact table without joining to a dimension \ntable. Degenerate dimensions are very common when the grain of a fact table rep-\nresents a single transaction or transaction line because the degenerate dimension \nrepresents the unique identiﬁ er of the parent. Order numbers, invoice numbers, \nand bill-of-lading numbers almost always appear as degenerate dimensions in a \ndimensional model.\nDegenerate dimensions often play an integral role in the fact table’s primary \nkey. In our case study, the primary key of the retail sales fact table consists of the \ndegenerate POS transaction number and product key, assuming scans of identical \nproducts in the market basket are grouped together as a single line item. \nNOTE \nOperational transaction control numbers such as order numbers, invoice \nnumbers, and bill-of-lading numbers usually give rise to empty dimensions and are \nrepresented as degenerate dimensions in transaction fact tables. The degenerate \ndimension is a dimension key without a corresponding dimension table.\nIf, for some reason, one or more attributes are legitimately left over after all the \nother dimensions have been created and seem to belong to this header entity, you \nwould simply create a normal dimension row with a normal join. However, you would \nno longer have a degenerate dimension.\n Retail Schema in Action\nWith  our retail POS schema designed, let’s illustrate how it would be put to use in \na query environment. A business user might be interested in better understanding \nweekly sales dollar volume by promotion for the snacks category during January \n2013 for stores in the Boston district. As illustrated in Figure 3-12, you would place \nquery constraints on month and year in the date dimension, district in the store \ndimension, and category in the product dimension.\nIf the query tool summed the sales dollar amount grouped by week ending \ndate and promotion, the SQL query results would look similar to those below in \nFigure 3-13. You can plainly see the relationship between the dimensional model \nand the associated query. High-quality dimension attributes are crucial because they \nare the source of query constraints and report labels. If you use a BI tool with more \nfunctionality, the results would likely appear as a cross-tabular “pivoted” report, \nwhich may be more appealing to business users than the columnar data resulting \nfrom an SQL  statement.\n",
      "content_length": 2535,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 131,
      "content": "Retail Sales 95\nDate Key (PK)\nDate\nDay of Week\nCalendar Month\nCalendar Quarter\nCalendar Year\n...\nDate Key (FK)\nProduct Key (FK)\nStore Key (FK)\nPromotion Key (FK)\nCashier Key (FK)\nPayment Method Key (FK)\nPOS Transaction # (DD)\nSales Quantity\nRegular Unit Price\nDiscount Unit Price\nNet Unit Price\nExtended Discount Dollar Amount\nExtended Sales Dollar Amount\nExtended Cost Dollar Amount\nExtended Gross Profit Dollar Amount\nProduct Key (PK)\nSKU Number (NK)\nProduct Description\nBrand Description\nCategory Description\n...\nPromotion Key (PK)\nPromotion Code (NK)\nPromotion Name\nPromotion Media Type\nPromotion Begin Date\n...\nSnacks\nJanuary\n2013\nBoston\nPayment Method Key (PK)\nPayment Method Description\nPayment Method Group\nStore Key (PK)\nStore Number (NK)\nStore Name\nStore District\nStore Region\n...\nCashier Key (PK)\nCashier Employee ID (NK)\nCashier Name\n...\nDate Dimension\nRetail Sales Facts\nProduct Dimension\nPromotion Dimension\nPayment Method Dimension\nStore Dimension\nCashier Dimension\nFigure 3-12: Querying the retail sales schema.\nCalendar Week\nEnding Date\nJanuary 6, 2013 \nJanuary 13, 2013 \nJanuary 20, 2013 \nJanuary 27, 2013\nNo Promotion\nNo Promotion\nSuper Bowl Promotion\nSuper Bowl Promotion\n2,647 \n4,851 \n7,248 \n13,798\nExtended Sales\nDollar Amount\nPromotion Name\nDepartment\nName\nNo Promotion\nExtended Sales\nDollar Amount\nSuper Bowl Promotion\nExtended Sales\nDollar Amount\nJanuary 6, 2013 \nJanuary 13, 2013 \nJanuary 20, 2013 \nJanuary 27, 2013\n2,647 \n4,851 \n0 \n0\n0 \n0 \n7,248 \n13,798\nFigure 3-13: Query results and cross-tabular report.\n Retail Schema Extensibility \nLet’s  turn our attention to extending the initial dimensional design. Several years \nafter the rollout of the retail sales schema, the retailer implements a frequent shop-\nper program. Rather than knowing an unidentiﬁ ed shopper purchased 26 items on \n",
      "content_length": 1817,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 132,
      "content": "Chapter 3\n96\na cash register receipt, you can now identify the speciﬁ c shopper. Just imagine the \nbusiness users’ interest in analyzing shopping patterns by a multitude of geographic, \ndemographic, behavioral, and other diff erentiating shopper characteristics.\nThe  handling of this new frequent shopper information is relatively straightfor-\nward. You’d create a frequent shopper dimension table and add another foreign key \nin the fact table. Because you can’t ask shoppers to bring in all their old cash register \nreceipts to tag their historical sales transactions with their new frequent shopper \nnumber, you’d substitute a default shopper dimension surrogate key, corresponding \nto a Prior to Frequent Shopper Program dimension row, on the historical fact table \nrows. Likewise, not everyone who shops at the grocery store will have a frequent \nshopper card, so you’d also want to include a Frequent Shopper Not Identiﬁ ed row \nin the shopper dimension. As we discussed earlier with the promotion dimension, \nyou can’t have a null frequent shopper key in the fact table.\nOur original schema gracefully extends to accommodate this new dimension \nlargely because the POS transaction data was initially modeled at its most granular \nlevel. The addition of dimensions applicable at that granularity did not alter the \nexisting dimension keys or facts; all existing BI applications continue to run without \nany changes. If the grain was originally declared as daily retail sales (transactions \nsummarized by day, store, product, and promotion) rather than the transaction line \ndetail, you would not have been able to incorporate the frequent shopper dimen-\nsion. Premature summarization or aggregation inherently limits your ability to add \nsupplemental dimensions because the additional dimensions often don’t apply at \nthe higher grain.\nThe predictable symmetry of dimensional models enable them to absorb some \nrather signiﬁ cant changes in source data and/or modeling assumptions without \ninvalidating existing BI applications, including:\n \n■New dimension attributes. If you discover new textual descriptors of a dimen-\nsion, you can add these attributes as new columns. All existing applications \nwill be oblivious to the new attributes and continue to function. If the new \nattributes are available only after a speciﬁ c point in time, then Not Available \nor its equivalent should be populated in the old dimension rows. Be fore-\nwarned that this scenario is more complicated if the business users want to \ntrack historical changes to this newly identiﬁ ed attribute. If this is the case, \npay close attention to the slowly changing dimension coverage in Chapter 5.\n \n■New dimensions. As we just discussed, you can add a dimension to an exist-\ning fact table by adding a new foreign key column and populating it correctly \nwith values of the primary key from the new dimension.\n",
      "content_length": 2888,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 133,
      "content": "Retail Sales 97\n \n■New measured facts. If new measured facts become available, you can add \nthem gracefully to the fact table. The simplest case is when the new facts are \navailable in the same measurement event and at the same grain as the existing \nfacts. In this case, the fact table is altered to add the new columns, and the \nvalues are populated into the table. If the new facts are only available from \na point in time forward, null values need to be placed in the older fact rows. \nA more complex situation arises when new measured facts occur naturally \nat a diff erent grain. If the new facts cannot be allocated or assigned to the \noriginal grain of the fact table, the new facts belong in their own fact table \nbecause it’s a mistake to mix grains in the same fact table.\n Factless Fact Tables\nThere  is one important question that cannot be answered by the previous retail sales \nschema: What products were on promotion but did not sell? The sales fact table \nrecords only the SKUs actually sold. There are no fact table rows with zero facts \nfor SKUs that didn’t sell because doing so would enlarge the fact table enormously. \nIn the relational world, a promotion coverage or event fact table is needed to \nanswer the question concerning what didn’t happen. The promotion coverage fact \ntable keys would be date, product, store, and promotion in this case study. This \nobviously looks similar to the sales fact table you just designed; however, the grain \nwould be signiﬁ cantly diff erent. In the case of the promotion coverage fact table, \nyou’d load one row for each product on promotion in a store each day (or week, if \nretail promotions are a week in duration) regardless of whether the product sold. \nThis fact table enables you to see the relationship between the keys as deﬁ ned by a \npromotion, independent of other events, such as actual product sales. We refer to it \nas a factless fact table because it has no measurement metrics; it merely captures the \nrelationship between the involved keys, as illustrated in Figure 3-14. To facilitate \ncounting, you can include a dummy fact, such as promotion count in this example, \nwhich always contains the constant value of 1; this is a cosmetic enhancement that \nenables the BI application to avoid counting one of the foreign keys.\nTo  determine what products were on promotion but didn’t sell requires a two-\nstep process. First, you’d query the promotion factless fact table to determine the \nuniverse of products that were on promotion on a given day. You’d then determine \nwhat products sold from the POS sales fact table. The answer to our original ques-\ntion is the set diff erence between these two lists of products. If you work with data \n",
      "content_length": 2720,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 134,
      "content": "Chapter 3\n98\nin an OLAP cube, it is often easier to answer the “what didn’t happen” question \nbecause the cube typically contains explicit cells for nonbehavior.\nDate Key (PK)\nDate\nDay of Week\nCalendar Month\nCalendar Quarter\nCalendar Year\n...\nStore Key (PK)\nStore Number (NK)\nStore Name\nStore District\nStore Region\n...\nDate Key (FK)\nProduct Key (FK)\nStore Key (FK)\nPromotion Key (FK)\nPromotion Count (=1)\nProduct Key (PK)\nSKU Number (NK)\nProduct Description\nBrand Description\nCategory Description\n...\nPromotion Key (PK)\nPromotion Code (NK)\nPromotion Name\nPromotion Media Type\nPromotion Begin Date\n...\nDate Dimension\nStore Dimension\nPromotion Coverage Facts\nProduct Dimension\nPromotion Dimension\nFigure 3-14: Promotion coverage factless fact table.\nDimension and Fact Table Keys\nNow that the schemas have been designed, we’ll focus on the dimension and fact \ntables’ primary keys, along with other row identiﬁ ers.\n Dimension Table Surrogate Keys\nThe  unique primary key of a dimension table should be a surrogate key rather than \nrelying on the operational system identiﬁ er, known as the natural key. Surrogate keys \ngo by many other aliases: meaningless keys, integer keys, non-natural keys, artiﬁ -\ncial keys, and synthetic keys. Surrogate keys are simply integers that are assigned \nsequentially as needed to populate a dimension. The ﬁ rst product row is assigned a \nproduct surrogate key with the value of 1; the next product row is assigned product \nkey 2; and so forth. The actual surrogate key value has no business signiﬁ cance. The \nsurrogate keys merely serve to join the dimension tables to the fact table. Throughout \nthis book, column names with a Key suffi  x, identiﬁ ed as a primary key (PK) or \nforeign key (FK), imply a surrogate.\nModelers sometimes are reluctant to relinquish the natural keys because they \nwant to navigate the fact table based on the operational code while avoiding a join \nto the dimension table. They also don’t want to lose the embedded intelligence \nthat’s often part of a natural multipart key. However, you should avoid relying on \n",
      "content_length": 2078,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 135,
      "content": "Retail Sales 99\nintelligent dimension keys because any assumptions you make eventually may be \ninvalidated. Likewise, queries and data access applications should not have any \nbuilt-in dependency on the keys because the logic also would be vulnerable to \ninvalidation. Even if the natural keys appear to be stable and devoid of meaning, \ndon’t be tempted to use them as the dimension table’s primary key.\nNOTE \nEvery join between dimension and fact tables in the data warehouse \nshould be based on meaningless integer surrogate keys. You should avoid using a \nnatural key as the dimension table’s primary key.\nInitially,  it may be faster to implement a dimensional model using operational \nnatural keys, but surrogate keys pay off  in the long run. We sometimes think of \nthem as being similar to a ﬂ u shot for the data warehouse—like an immunization, \nthere’s a small amount of pain to initiate and administer surrogate keys, but the long \nrun beneﬁ ts are substantial, especially considering the reduced risk of substantial \nrework. Here are several advantages:\n \n■Buff er the data warehouse from operational changes. Surrogate keys enable \nthe warehouse team to maintain control of the DW/BI environment rather \nthan being whipsawed by operational rules for generating, updating, deleting, \nrecycling, and reusing production codes. In many organizations, historical \noperational codes, such as inactive account numbers or obsolete product \ncodes, get reassigned after a period of dormancy. If account numbers get \nrecycled following 12 months of inactivity, the operational systems don’t miss \na beat because their business rules prohibit data from hanging around for that \nlong. But the DW/BI system may retain data for years. Surrogate keys provide \nthe warehouse with a mechanism to diff erentiate these two separate instances \nof the same operational account number. If you rely solely on operational \ncodes, you might also be vulnerable to key overlaps in the case of an acquisi-\ntion or consolidation of data.\n \n■Integrate multiple source systems. Surrogate keys enable the data warehouse \nteam to integrate data from multiple operational source systems, even if they \nlack consistent source keys by using a back room cross-reference mapping \ntable to link the multiple natural keys to a common surrogate.\n \n■Improve performance. The surrogate key is as small an integer as possible \nwhile ensuring it will comfortably accommodate the future anticipated car-\ndinality (number of rows in the dimension). Often the operational code is a \nbulky alphanumeric character string or even a group of ﬁ elds. The smaller \nsurrogate key translates into smaller fact tables, smaller fact table indexes, \nand more fact table rows per block input-output operation. Typically, a 4-byte \n",
      "content_length": 2783,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 136,
      "content": "Chapter 3\n100\ninteger is suffi  cient to handle most dimensions. A 4-byte integer is a single \ninteger, not four decimal digits. It has 32 bits and therefore can handle approx-\nimately 2 billion positive values (232) or 4 billion total positive and negative \nvalues (–232 to +232). This is more than enough for just about any dimension. \nRemember, if you have a large fact table with 1 billion rows of data, every byte \nin each fact table row translates into another gigabyte of storage.\n \n■Handle null or unknown conditions. As mentioned earlier, special surrogate \nkey values are used to record dimension conditions that may not have an \noperational code, such as the No Promotion condition or the anonymous \ncustomer. You can assign a surrogate key to identify these despite the lack of \noperational coding. Similarly, fact tables sometimes have dates that are yet \nto be determined. There is no SQL date type value for Date to Be Determined \nor Date Not Applicable. \n \n■Support dimension attribute change tracking. One of the primary techniques \nfor handling changes to dimension attributes relies on surrogate keys to handle \nthe multiple proﬁ les for a single natural key. This is actually one of the most \nimportant reasons to use surrogate keys, which we’ll describe in Chapter 5. \nA pseudo surrogate key created by simply gluing together the natural key \nwith a time stamp is perilous. You need to avoid multiple joins between the \ndimension and fact tables, sometimes referred to as double-barreled joins, due \nto their adverse impact on performance and ease of use.\nOf course, some eff ort is required to assign and administer surrogate keys, but \nit’s not nearly as intimidating as many people imagine. You need to establish and \nmaintain a cross-reference table in the ETL system that will be used to substitute the \nappropriate surrogate key on each fact and dimension table row. We lay out a process \nfor administering surrogate keys in Chapter 19: ETL Subsystems and Techniques.\n Dimension Natural and Durable Supernatural Keys\nLike surrogate keys, the natural keys assigned and used by operational source sys-\ntems go by other names, such as business keys, production keys, and operational \nkeys. They are identiﬁ ed with the NK notation in the book’s ﬁ gures. The natural \nkey is often modeled as an attribute in the dimension table. If the natural key comes \nfrom multiple sources, you might use a character data type that prepends a source \ncode, such as SAP|43251 or CRM|6539152. If the same entity is represented in both \noperational source systems, then you’d likely have two natural key attributes in \nthe dimension corresponding to both sources. Operational natural keys are often \ncomposed of meaningful constituent parts, such as the product’s line of business \nor country of origin; these components should be split apart and made available as \nseparate attributes.\n",
      "content_length": 2894,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 137,
      "content": "Retail Sales 101\nIn a  dimension table with attribute change tracking, it’s important to have an iden-\ntiﬁ er that uniquely and reliably identiﬁ es the dimension entity across its attribute \nchanges. Although the operational natural key may seem to ﬁ t this bill, sometimes \nthe natural key changes due to unexpected business rules (like an organizational \nmerger) or to handle either duplicate entries or data integration from multiple \nsources. If the dimension’s natural keys are not absolutely protected and preserved \nover time, the ETL system needs to assign permanent durable identiﬁ ers, also known \nas supernatural keys. A persistent durable supernatural key is controlled by the DW/\nBI system and remains immutable for the life of the system. Like the dimension \nsurrogate key, it’s a simple integer sequentially assigned. And like the natural keys \ndiscussed earlier, the durable supernatural key is handled as a dimension attribute; \nit’s not a replacement for the dimension table’s surrogate primary key. Chapter 19 \nalso discusses the ETL system’s responsibility for these durable identiﬁ ers.\nDegenerate Dimension Surrogate Keys\nAlthough  surrogate keys aren’t typically assigned to degenerate dimensions, each \nsituation needs to be evaluated to determine if one is required. A surrogate key is \nnecessary if the transaction control numbers are not unique across locations or get \nreused. For example, the retailer’s POS system may not assign unique transaction \nnumbers across stores. The system may wrap back to zero and reuse previous con-\ntrol numbers when its maximum has been reached. Also, the transaction control \nnumber may be a bulky 24-byte alphanumeric column. Finally, depending on the \ncapabilities of the BI tool, you may need to assign a surrogate key (and create an \nassociated dimension table) to drill across on the transaction number. Obviously, \ncontrol number dimensions modeled in this way with corresponding dimension \ntables are no longer degenerate.\nDate Dimension Smart Keys\nAs  we’ve noted, the date dimension has unique characteristics and requirements. \nCalendar dates are ﬁ xed and predetermined; you never need to worry about deleting \ndates or handling new, unexpected dates on the calendar. Because of its predict-\nability, you can use a more intelligent key for the date dimension.\nIf  a sequential integer serves as the primary key of the date dimension, it should \nbe chronologically assigned. In other words, January 1 of the ﬁ rst year would \nbe assigned surrogate key value 1, January 2 would be assigned surrogate key 2, \nFebruary 1 would be assigned surrogate key 32, a nd so on.\nMore commonly, the primary key of the date dimension is a meaningful integer \nformatted as yyyymmdd. The yyyymmdd key is not intended to provide business \nusers and their BI applications with an intelligent key so they can bypass the date \ndimension and directly query the fact table. Filtering on the fact table’s yyyymmdd \n",
      "content_length": 2963,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 138,
      "content": "Chapter 3\n102\nkey would have a detrimental impact on usability and performance. Filtering and \ngrouping on calendar attributes should occur in a dimension table, not in the BI \napplication’s code.\nHowever,  the yyyymmdd key is useful for partitioning fact tables. Partitioning \nenables a table to be segmented into smaller tables under the covers. Partitioning \na large fact table on the basis of date is eff ective because it allows old data to be \nremoved gracefully and new data to be loaded and indexed in the current parti-\ntion without disturbing the rest of the fact table. It reduces the time required for \nloads, backups, archiving, and query response. Programmatically updating and \nmaintaining partitions is straightforward if the date key is an ordered integer: year \nincrements by 1 up to the number of years wanted, month increments by 1 up to \n12, and so on. Using a smart yyyymmdd key provides the beneﬁ ts of a surrogate, \nplus the advantages of easier partition management.\nAlthough the yyyymmdd integer is the most common approach for date dimen-\nsion keys, some relational database optimizers prefer a true date type column for \npartitioning. In these cases, the optimizer knows there are 31 values between \nMarch 1 and April 1, as opposed to the apparent 100 values between 20130301 and \n20130401. Likewise, it understands there are 31 values between December 1 and \nJanuary 1, as opposed to the 8,900 integer values between 20121201 and 20130101. \nThis intelligence can impact the query strategy chosen by the optimizer and further \nreduce query times. If the optimizer incorporates date type intelligence, it should \nbe considered for the date key. If the only rationale for a date type key is simpliﬁ ed \nadministration for the DBA, then you can feel less compelled.\nWith more intelligent date keys, whether chronologically assigned or a more \nmeaningful yyyymmdd integer or date type column, you need to reserve a special \ndate key value for the situation in which the date is unknown when the fact row is \ninitially loaded.\n Fact Table Surrogate Keys\nAlthough  we’re adamant about using surrogate keys for dimension tables, we’re less \ndemanding about a surrogate key for fact tables. Fact table surrogate keys typically \nonly make sense for back room ETL processing. As we mentioned, the primary \nkey of a fact table typically consists of a subset of the table’s foreign keys and/or \ndegenerate dimension. However, single column surrogate keys for fact tables have \nsome interesting back room beneﬁ ts.\nLike its dimensional counterpart, a fact table surrogate key is a simple integer, \ndevoid of any business content, that is assigned in sequence as fact table rows are \ngenerated. Although the fact table surrogate key is unlikely to deliver query perfor-\nmance advantages, it does have the following beneﬁ ts:\n",
      "content_length": 2839,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 139,
      "content": "Retail Sales 103\n \n■Immediate unique identiﬁ cation. A single fact table row is immediately iden-\ntiﬁ ed by the key. During ETL processing, a speciﬁ c row can be identiﬁ ed \nwithout navigating multiple dimensions.\n \n■Backing out or resuming a bulk load. If a large number of rows are being \nloaded with sequentially assigned surrogate keys, and the process halts before \ncompletion, the DBA can determine exactly where the process stopped by \nﬁ nding the maximum key in the table. The DBA could back out the complete \nload by specifying the range of keys just loaded or perhaps could resume the \nload from exactly the correct point.\n \n■Replacing updates with inserts plus deletes. The fact table surrogate key \nbecomes the true physical key of the fact table. No longer is the key of the \nfact table determined by a set of dimensional foreign keys, at least as far as \nthe RDBMS is concerned. Thus it becomes possible to replace a fact table \nupdate operation with an insert followed by a delete. The ﬁ rst step is to \nplace the new row into the database with all the same business foreign keys \nas the row it is to replace. This is now possible because the key enforce-\nment depends only on the surrogate key, and the replacement row has a \nnew surrogate key. Then the second step deletes the original row, thereby \naccomplishing the update. For a large set of updates, this sequence is more \neffi  cient than a set of true update operations. The insertions can be pro-\ncessed with the ability to back out or resume the insertions as described in \nthe previous bullet. These insertions do not need to be protected with full \ntransaction machinery. Then the ﬁ nal deletion step can be performed safely \nbecause the insertions have run to completion.\n \n■Using the fact table surrogate key as a parent in a parent/child schema. In \nthose cases in which one fact table contains rows that are parents of those in \na lower grain fact table, the fact table surrogate key in the parent table is also \nexposed in the child table. The argument of using the fact table surrogate \nkey in this case rather than a natural parent key is similar to the argument \nfor using surrogate keys in dimension tables. Natural keys are messy and \nunpredictable, whereas surrogate keys are clean integers and are assigned by \nthe ETL system, not the source system. Of course, in addition to including \nthe parent fact table’s surrogate key, the lower grained fact table should also \ninclude the parent’s dimension foreign keys so the child facts can be sliced \nand diced without traversing the parent fact table’s surrogate key. And as we’ll \ndiscuss in Chapter 4: Inventory, you should never join fact tables directly to \nother fact tables.\n",
      "content_length": 2716,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 140,
      "content": "Chapter 3\n104\nResisting Normalization Urges\nIn this section, let’s directly confront several of the natural urges that tempt model-\ners coming from a more normalized background. We’ve been consciously breaking \nsome traditional modeling rules because we’re focused on delivering value through \nease of use and performance, not on transaction processing effi  ciencies.\n Snowﬂ ake Schemas with Normalized Dimensions\nThe  ﬂ attened, denormalized dimension tables with repeating textual values make \ndata modelers from the operational world uncomfortable. Let’s revisit the case study \nproduct dimension table. The 300,000 products roll up into 50 distinct depart-\nments. Rather than redundantly storing the 20-byte department description in the \nproduct dimension table, modelers with a normalized upbringing want to store a \n2-byte department code and then create a new department dimension for the depart-\nment decodes. In fact, they would feel more comfortable if all the descriptors in the \noriginal design were normalized into separate dimension tables. They argue this \ndesign saves space because the 300,000-row dimension table only contains codes, \nnot lengthy descriptors.\nIn addition, some modelers contend that more normalized dimension tables are \neasier to maintain. If a department description changes, they’d need to update only \nthe one occurrence in the department dimension rather than the 6,000 repetitions \nin the original product dimension. Maintenance often is addressed by normaliza-\ntion disciplines, but all this happens back in the ETL system long before the data \nis loaded into a presentation area’s dimensional schema.\nDimension table normalization is referred to as snowﬂ aking. Redundant attributes \nare removed from the ﬂ at, denormalized dimension table and placed in separate \nnormalized dimension tables. Figure 3-15 illustrates the partial snowﬂ aking of the \nproduct dimension into third normal form. The contrast between Figure 3-15 and \nFigure 3-8 is startling. The plethora of snowﬂ aked tables (even in our simplistic \nexample) is overwhelming. Imagine the impact on Figure 3-12 if all the schema’s \nhierarchies were normalized.\nSnowﬂ aking is a legal extension of the dimensional model, however, we encour-\nage you to resist the urge to snowﬂ ake given the two primary design drivers: ease \nof use and performance.\n",
      "content_length": 2355,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 141,
      "content": "Retail Sales 105\nProduct Key (PK)\nSKU Number (Natural Key)\nProduct Description\nBrand Key (FK)\nPackage Type Key (FK)\nFat Content\nWeight\nWeight Unit of Measure\nStorage Type Key (FK)\nShelf Width\nShelf Height\nShelf Depth\n...\nBrand Key (PK)\nBrand Description\nCategory Key (FK)\nCategory Key (PK)\nCategory Description\nDepartment Key (FK)\nDepartment Key (PK)\nDepartment Number\nDepartment Description\nShelf Life Type Key (PK)\nShelf Life Type Description\nPackage Type Key (PK)\nPackage Type Description\nStorage Type Key (PK)\nStorage Type Description\nShelf Life Type Key (FK)\nProduct Dimension\nBrand Dimension\nCategory Dimension\nShelf Life Type Dimension\nDepartment Dimension\nPackage Type Dimension\nStorage Type Dimension\nFigure 3-15: Snowﬂ aked product dimension.\n \n■The multitude of snowﬂ aked tables makes for a much more complex presen-\ntation. Business users inevitably will struggle with the complexity; simplicity \nis one of the primary objectives of a dimensional model.\n \n■Most database optimizers also struggle with the snowﬂ aked schema’s complex-\nity. Numerous tables and joins usually translate into slower query performance. \nThe complexities of the resulting join speciﬁ cations increase the chances that \nthe optimizer will get sidetracked and choose a poor strategy.\n \n■The minor disk space savings associated with snowﬂ aked dimension tables \nare insigniﬁ cant. If you replace the 20-byte department description in the \n300,000 row product dimension table with a 2-byte code, you’d save a whop-\nping 5.4 MB (300,000 x 18 bytes); meanwhile, you may have a 10 GB fact \ntable! Dimension tables are almost always geometrically smaller than fact \ntables. Eff orts to normalize dimension tables to save disk space are usually \na waste of time.\n \n■Snowﬂ aking negatively impacts the users’ ability to browse within a dimen-\nsion. Browsing often involves constraining one or more dimension attributes \nand looking at the distinct values of another attribute in the presence of these \nconstraints. Browsing allows users to understand the relationship between \ndimension attribute values.\n",
      "content_length": 2085,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 142,
      "content": "Chapter 3\n106\n \n■Obviously, a snowﬂ aked product dimension table responds well if you just \nwant a list of the category descriptions. However, if you want to see all the \nbrands within a category, you need to traverse the brand and category dimen-\nsions. If you want to also list the package types for each brand in a category, \nyou’d be traversing even more tables. The SQL needed to perform these seem-\ningly simple queries is complex, and you haven’t touched the other dimensions \nor fact table.\n \n■Finally, snowﬂ aking defeats the use of bitmap indexes. Bitmap indexes are \nuseful when indexing low-cardinality columns, such as the category and \ndepartment attributes in the product dimension table. They greatly speed \nthe performance of a query or constraint on the single column in question. \nSnowﬂ aking inevitably would interfere with your ability to leverage this per-\nformance tuning technique.\nNOTE \nFixed depth hierarchies should be flattened in dimension tables. \nNormalized, snowﬂ aked dimension tables penalize cross-attribute browsing and \nprohibit the use of bitmapped indexes. Disk space savings gained by normalizing \nthe dimension tables typically are less than 1 percent of the total disk space needed \nfor the overall schema. You should knowingly sacriﬁ ce this dimension table space \nin the spirit of performance and ease of use advantages.\nSome database vendors argue their platform has the horsepower to query a fully \nnormalized dimensional model without performance penalties. If you can achieve \nsatisfactory performance without physically denormalizing the dimension tables, \nthat’s ﬁ ne. However, you’ll still want to implement a logical dimensional model with \ndenormalized dimensions to present an easily understood schema to the business \nusers and their BI applications.\nIn the past, some BI tools indicated a preference for snowﬂ ake schemas; snowﬂ ak-\ning to address the idiosyncratic requirements of a BI tool is acceptable. Likewise, if \nall the data is delivered to business users via an OLAP cube (where the snowﬂ aked \ndimensions are used to populate the cube but are never visible to the users), then \nsnowﬂ aking is acceptable. However, in these situations, you need to consider the \nimpact on users of alternative BI tools and the ﬂ exibility to migrate to alternatives \nin the future.\n Outriggers\nAlthough  we generally do not recommend snowﬂ aking, there are situations in which \nit is permissible to build an outrigger dimension that attaches to a dimension within \n",
      "content_length": 2514,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 143,
      "content": "Retail Sales 107\nthe fact table’s immediate halo, as illustrated in Figure 3-16. In this example, the \n“once removed” outrigger is a date dimension snowﬂ aked off  a primary dimension. \nThe outrigger date attributes are descriptively and uniquely labeled to distinguish \nthem from the other dates associated with the business process. It only makes \nsense to outrigger a primary dimension table’s date attribute if the business wants \nto ﬁ lter and group this date by nonstandard calendar attributes, such as the ﬁ scal \nperiod, business day indicator, or holiday period. Otherwise, you could just treat \nthe date attribute as a standard date type column in the product dimension. If a date \noutrigger is used, be careful that the outrigger dates fall within the range stored in \nthe standard date dimension table.\nProduct Key (PK)\nSKU Number (NK)\nProduct Description\nBrand Description\nSubcategory Description\nCategory Description\nDepartment Number\nDepartment Description\nPackage Type Description\nPackage Size\nProduct Introduction Date Key (FK)\n...\nProduct Dimension\nProduct Introduction Date Key (PK)\nProduct Introduction Date\nProduct Introduction Calendar Month\nProduct Introduction Calendar Year\nProduct Introduction Fiscal Month\nProduct Introduction Fiscal Quarter\nProduct Introduction Fiscal Year\nProduct Introduction Holiday Period Indicator\n...\nProduct Introduction Date Dimension\nFigure 3-16: Example of a permissible outrigger.\nYou’ll encounter more outrigger examples later in the book, such as the han-\ndling of customers’ county-level demographic attributes in Chapter 8: Customer \nRelationship Management.\nAlthough outriggers may save space and ensure the same attributes are referenced \nconsistently, there are downsides. Outriggers introduce more joins, which can nega-\ntively impact performance. More important, outriggers can negatively impact the \nlegibility for business users and hamper their ability to browse among attributes \nwithin a single dimension.\nWARNING \nThough outriggers are permissible, a dimensional model should \nnot be littered with outriggers given the potentially negative impact. Outriggers \nshould be the exception rather than the rule.\n",
      "content_length": 2177,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 144,
      "content": "Chapter 3\n108\n Centipede Fact Tables with Too Many Dimensions\nThe  fact table in a dimensional schema is naturally highly normalized and compact. \nThere is no way to further normalize the extremely complex many-to-many relation-\nships among the keys in the fact table because the dimensions are not correlated \nwith each other. Every store is open every day. Sooner or later, almost every product \nis sold on promotion in most or all of our stores.\nInterestingly, while uncomfortable with denormalized dimension tables, some \nmodelers are tempted to denormalize the fact table. They have an uncontrollable \nurge to normalize dimension hierarchies but know snowﬂ aking is highly discour-\naged, so the normalized tables end up joined to the fact table instead. Rather than \nhaving a single product foreign key on the fact table, they include foreign keys for \nthe frequently analyzed elements on the product hierarchy, such as brand, category, \nand department. Likewise, the date key suddenly turns into a series of keys joining \nto separate week, month, quarter, and year dimension tables. Before you know it, \nyour compact fact table has turned into an unruly monster that joins to literally \ndozens of dimension tables. We aff ectionately refer to these designs as centipede \nfact tables because they appear to have nearly 100 legs, as shown in Figure 3-17. \nPOS Retail Sales Transaction Fact\nDate Key (FK)\nWeek Key (FK)\nMonth Key (FK)\nQuarter Key (FK)\nYear Key (FK)\nFiscal Year Key (FK)\nFiscal Month Key (FK)\nProduct Key (FK)\nBrand Key (FK)\nCategory Key (FK)\nDepartment Key (FK)\nPackage Type Key (FK)\nStore Key (FK)\nStore County Key (FK)\nStore State Key (FK)\nStore District Key (FK)\nStore Region Key (FK)\nStore Floor Plan Key (FK)\nPromotion Key (FK)\nPromotion Reduction Type Key (FK)\nPromotion Media Type Key (FK)\nPOS Transaction Number (DD)\nSales Quantity\nExtended Discount Dollar Amount\nExtended Sales Dollar Amount\nExtended Cost Dollar Amount\nDate Dimension\nBrand Dimension\nCategory Dimension\nDepartment Dimension\nPackage Type Dimension\nPromotion Dimension\nPromotion Reduction Type Dimension\nPromotion Media Type Dimension\nWeek Dimension\nMonth Dimension\nQuarter Dimension\nYear Dimension\nFiscal Year Dimension\nFiscal Month Dimension\nStore Dimension\nStore County Dimension\nStore State Dimension\nStore District Dimension\nStore Region Dimension\nStore Floor Plan Dimension\nProduct Dimension\nFigure 3-17: Centipede fact table with too many normalized dimensions.\n",
      "content_length": 2462,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 145,
      "content": "Retail Sales 109\nEven with its tight format, the fact table is the behemoth in a dimensional model. \nDesigning a fact table with too many dimensions leads to signiﬁ cantly increased fact \ntable disk space requirements. Although denormalized dimension tables consume \nextra space, fact table space consumption is a concern because it is your largest \ntable by orders of magnitude. There is no way to index the enormous multipart \nkey eff ectively in the centipede example. The numerous joins are an issue for both \nusability and query performance.\nMost business processes can be represented with less than 20 dimensions in the \nfact table. If a design has 25 or more dimensions, you should look for ways to com-\nbine correlated dimensions into a single dimension. Perfectly correlated attributes, \nsuch as the levels of a hierarchy, as well as attributes with a reasonable statistical \ncorrelation, should be part of the same dimension. It’s a good decision to combine \ndimensions when the resulting new single dimension is noticeably smaller than the \nCartesian product of the separate dimensions.\nNOTE \nA very large number of dimensions typically are a sign that several \ndimensions are not completely independent and should be combined into a \nsingle dimension. It is a dimensional modeling mistake to represent elements of \na single hierarchy as separate dimensions in the fact table.\nDevelopments with columnar databases may reduce the query and storage penal-\nties associated with wide centipede fact table designs. Rather than storing each table \nrow, a columnar database stores each table column as a contiguous object that is \nheavily indexed for access. Even though the underlying physical storage is colum-\nnar, at the query level, the table appears to be made up of familiar rows. But when \nqueried, only the named columns are actually retrieved from the disk, rather than \nthe entire row in a more conventional row-oriented relational database. Columnar \ndatabases are much more tolerant of the centipede fact tables just described; how-\never, the ability to browse across hierarchically related dimension attributes may \nbe compromised.\nSummary\nThis chapter was your ﬁ rst exposure to designing a dimensional model. Regardless \nof the industry, we strongly encourage the four-step process for tackling dimensional \nmodel designs. Remember it is especially important to clearly state the grain associ-\nated with a dimensional schema. Loading the fact table with atomic data provides \nthe greatest ﬂ exibility because the data can be summarized “every which way.” As \n",
      "content_length": 2579,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 146,
      "content": "Chapter 3\n110\nsoon as the fact table is restricted to more aggregated information, you run into \nwalls when the summarization assumptions prove to be invalid. Also it is vitally \nimportant to populate your dimension tables with verbose, robust descriptive attri-\nbutes for analytic ﬁ ltering and labeling.\nIn the next chapter we’ll remain within the retail industry to discuss techniques \nfor tackling a second business process within the organization, ensuring your earlier \neff orts are leveraged while avoiding stovepipes.\n",
      "content_length": 526,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 147,
      "content": "Inventory\nI\nn Chapter 3: Retail Sales, we developed a dimensional model for the sales transac-\ntions in a large grocery chain. We remain within the same industry in this chapter \nbut move up the value chain to tackle the inventory process. The designs developed \nin this chapter apply to a broad set of inventory pipelines both inside and outside \nthe retail industry.\nMore important, this chapter provides a thorough discussion of the enterprise \ndata warehouse bus architecture. The bus architecture is essential to creating an \nintegrated DW/BI system. It provides a framework for planning the overall environ-\nment, even though it will be built incrementally. We will underscore the importance \nof using common conformed dimensions and facts across dimensional models, and \nwill close by encouraging the adoption of an enterprise data governance program.\nChapter 4 discusses the following concepts:\n \n■Representing organizational value chains via a series of dimensional models\n \n■Semi-additive facts\n \n■Three fact table types: periodic snapshots, transaction, and accumulating \nsnapshots \n \n■Enterprise data warehouse bus architecture and bus matrix\n \n■Opportunity/stakeholder matrix\n \n■Conformed dimensions and facts, and their impact on agile methods\n \n■Importance of data governance\n Value Chain Introduction\nMost  organizations have an underlying value chain of key business processes. The \nvalue chain identiﬁ es the natural, logical ﬂ ow of an organization’s primary activi-\nties. For example, a retailer issues purchase orders to product manufacturers. The \nproducts are delivered to the retailer’s warehouse, where they are held in inven-\ntory. A delivery is then made to an individual store, where again the products sit in \n4\n",
      "content_length": 1741,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 148,
      "content": "Chapter 4\n112\ninventory until a consumer makes a purchase. Figure 4-1 illustrates this subset of a \nretailer’s value chain.  Obviously, products sourced from manufacturers that deliver \ndirectly to the retail store would bypass the warehousing processes.\nIssue Purchase\nOrder to\nManufacturer\nReceive\nWarehouse\nDeliveries\nWarehouse\nProduct\nInventory\nReceive\nStore\nDeliveries\nStore\nProduct\nInventory\nRetail\nSales\nFigure 4-1: Subset of a retailer’s value chain.\nOperational source systems typically produce transactions or snapshots at each \nstep of the value chain. The primary objective of most analytic DW/BI systems is \nto monitor the performance results of these key processes. Because each process \nproduces unique metrics at unique time intervals with unique granularity and \ndimensionality, each process typically spawns one or more fact tables. To this end, \nthe value chain provides high-level insight into the overall data architecture for an \nenterprise DW/BI environment. We’ll devote more time to this topic in the “Value \nChain Integration” section later in this chapter.\n Inventory Models\nIn  the meantime, we’ll discuss several complementary inventory models. The ﬁ rst \nis the inventory periodic snapshot where product inventory levels are measured at \nregular intervals and placed as separate rows in a fact table. These periodic snapshot \nrows appear over time as a series of data layers in the dimensional model, much like \ngeologic layers represent the accumulation of sediment over long periods of time. \nWe’ll then discuss a second inventory model where every transaction that impacts \n",
      "content_length": 1607,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 149,
      "content": "Inventory 113\ninventory levels as products move through the warehouse is recorded. Finally, in the \nthird model, we’ll describe the inventory accumulating snapshot where a fact table \nrow is inserted for each product delivery and then the row is updated as the product \nmoves through the warehouse. Each model tells a diff erent story. For some analytic \nrequirements, two or even all three models may be appropriate simultaneously.\n Inventory Periodic Snapshot\nLet’s  return to our retail case study. Optimized inventory levels in the stores can have \na major impact on chain proﬁ tability. Making sure the right product is in the right \nstore at the right time minimizes out-of-stocks (where the product isn’t available \non the shelf to be sold) and reduces overall inventory carrying costs. The retailer \nwants to analyze daily quantity-on-hand inventory levels by product and store.\nIt is time to put the four-step dimensional design process to work again. The \nbusiness process we’re interested in analyzing is the periodic snapshotting of retail \nstore inventory. The most atomic level of detail provided by the operational inven-\ntory system is a daily inventory for each product in each store. The dimensions \nimmediately fall out of this grain declaration: date, product, and store. This often \nhappens with periodic snapshot fact tables where you cannot express the granular-\nity in the context of a transaction, so a list of dimensions is needed instead. In this \ncase study, there are no additional descriptive dimensions at this granularity. For \nexample, promotion dimensions are typically associated with product movement, \nsuch as when the product is ordered, received, or sold, but not with inventory.\nThe simplest view of inventory involves only a single fact: quantity on hand. \nThis leads to an exceptionally clean dimensional design, as shown in Figure 4-2.\nDate Dimension\nStore Inventory Snapshot Fact\nDate Key (PK)\n...\nDate Key (FK)\nProduct Key (FK)\nStore Key (FK)\nQuantity on Hand\nProduct Key (PK)\nStorage Requirement Type\n...\nStore Key (PK)\n...\nProduct Dimension\nStore Dimension\nFigure 4-2: Store inventory periodic snapshot schema.\nThe date dimension table in this case study is identical to the table developed \nin Chapter 3 for retail store sales. The product and store dimensions may be deco-\nrated with additional attributes that would be useful for inventory analysis. For \nexample, the product dimension could be enhanced with columns such as the \nminimum reorder quantity or the storage requirement, assuming they are constant \nand discrete descriptors of each product. If the minimum reorder quantity varies for \n",
      "content_length": 2647,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 150,
      "content": "Chapter 4\n114\na product by store, it couldn’t be included as a product dimension attribute. In the \nstore dimension, you might include attributes to identify the frozen and refrigerated \nstorage square footages. \nEven a schema as simple as Figure 4-2 can be very useful. Numerous insights can \nbe derived if inventory levels are measured frequently for many products in many \nlocations. However, this periodic snapshot fact table faces a serious challenge that \nChapter 3’s sales transaction fact table did not. The sales fact table was reasonably \nsparse because you don’t sell every product in every shopping cart. Inventory, on \nthe other hand, generates dense snapshot tables. Because the retailer strives to \navoid out-of-stock situations in which the product is not available, there may be \na row in the fact table for every product in every store every day. In that case you \nwould include the zero out-of-stock measurements as explicit rows. For the grocery \nretailer with 60,000 products stocked in 100 stores, approximately 6 million rows \n(60,000 products x 100 stores) would be inserted with each nightly fact table load. \nHowever, because the row width is just 14 bytes, the fact table would grow by only \n84 MB with each load. \nAlthough the data volumes in this case are manageable, the denseness of some \nperiodic snapshots may mandate compromises. Perhaps the most obvious is to \nreduce the snapshot frequencies over time. It may be acceptable to keep the last 60 \ndays of inventory at the daily level and then revert to less granular weekly snap-\nshots for historical data. In this way, instead of retaining 1,095 snapshots during \na 3-year period, the number could be reduced to 208 total snapshots; the 60 daily \nand 148 weekly snapshots should be stored in two separate fact tables given their \nunique periodicity.\n Semi-Additive Facts\nWe  stressed the importance of fact additivity in Chapter 3. In the inventory snap-\nshot schema, the quantity on hand can be summarized across products or stores \nand result in a valid total. Inventory levels, however, are not additive across dates \nbecause they represent snapshots of a level or balance at one point in time. Because \ninventory levels (and all forms of ﬁ nancial account balances) are additive across \nsome dimensions but not all, we refer to them as semi-additive facts.\nThe semi-additive nature of inventory balance facts is even more understand-\nable if you think about your checking account balances. On Monday, presume \nthat you have $50 in your account. On Tuesday, the balance remains unchanged. \nOn Wednesday, you deposit another $50 so the balance is now $100. The account \nhas no further activity through the end of the week. On Friday, you can’t merely \nadd up the daily balances during the week and declare that the ending balance is \n$400 (based on $50 + $50 + $100 + $100 + $100). The most useful way to combine \n",
      "content_length": 2901,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 151,
      "content": "Inventory 115\naccount balances and inventory levels across dates is to average them (resulting in \nan $80 average balance in the checking example). You are probably familiar with \nyour bank referring to the average daily balance on a monthly account summary.\nNOTE \nAll measures that record a static level (inventory levels, ﬁ nancial account \nbalances, and measures of intensity such as room temperatures) are inherently \nnon-additive across the date dimension and possibly other dimensions. In these \ncases, the measure may be aggregated across dates by averaging over the number \nof time periods.\nUnfortunately, you cannot use the SQL AVG function to calculate the average \nover time. This function averages over all the rows received by the query, not just \nthe number of dates. For example, if a query requested the average inventory for \na cluster of three products in four stores across seven dates (e.g., the average daily \ninventory of a brand in a geographic region during a week), the SQL AVG function \nwould divide the summed inventory value by 84 (3 products × 4 stores × 7 dates). \nObviously, the correct answer is to divide the summed inventory value by 7, which \nis the number of daily time periods.\nOLAP products provide the capability to deﬁ ne aggregation rules within the \ncube, so semi-additive measures like balances are less problematic if the data is \ndeployed via OLAP cubes.\nEnhanced Inventory Facts\nThe  simplistic view in the periodic inventory snapshot fact table enables you to see \na time series of inventory levels. For most inventory analysis, quantity on hand isn’t \nenough. Quantity on hand needs to be used in conjunction with additional facts to \nmeasure the velocity of inventory movement and develop other interesting metrics \nsuch as the number of turns and number of days’ supply.\nIf quantity sold (or equivalently, quantity shipped for a warehouse location) was \nadded to each fact row, you could calculate the number of turns and days’ supply. \nFor daily inventory snapshots, the number of turns measured each day is calculated \nas the quantity sold divided by the quantity on hand. For an extended time span, \nsuch as a year, the number of turns is the total quantity sold divided by the daily \naverage quantity on hand. The number of days’ supply is a similar calculation. Over \na time span, the number of days’ supply is the ﬁ nal quantity on hand divided by \nthe average quantity sold.\nIn addition to the quantity sold, inventory analysts are also interested in the \nextended value of the inventory at cost, as well as the value at the latest selling price. \nThe initial periodic snapshot is embellished in Figure 4-3.\n",
      "content_length": 2665,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 152,
      "content": "Chapter 4\n116\nDate Dimension\nStore Inventory Snapshot Fact\nDate Key (PK)\n...\nDate Key (FK)\nProduct Key (FK)\nStore Key (FK)\nQuantity on Hand\nQuantity Sold\nInventory Dollar Value at Cost\nInventory Dollar Value at Latest Selling Price\nProduct Key (PK)\n...\nStore Key (PK)\n...\nProduct Dimension\nStore Dimension\nFigure 4-3: Enhanced inventory periodic snapshot.\nNotice that quantity on hand is semi-additive, but the other measures in the \nenhanced periodic snapshot are all fully additive. The quantity sold amount has been \nrolled up to the snapshot’s daily granularity. The valuation columns are extended, \nadditive amounts. In some periodic snapshot inventory schemas, it is useful to \nstore the beginning balance, the inventory change or delta, along with the ending \nbalance. In this scenario, the balances are again semi-additive, whereas the deltas \nare fully additive across all the dimensions.\nThe periodic snapshot is the most common inventory schema. We’ll brieﬂ y dis-\ncuss two alternative perspectives that complement the inventory snapshot just \ndesigned. For a change of pace, rather than describing these models in the context \nof the retail store inventory, we’ll move up the value chain to discuss the inventory \nlocated in the warehouses.\n Inventory Transactions\nA  second way to model an inventory business process is to record every transac-\ntion that aff ects inventory. Inventory transactions at the warehouse might include \nthe following:\n \n■Receive product.\n \n■Place product into inspection hold.\n \n■Release product from inspection hold.\n \n■Return product to vendor due to inspection failure.\n \n■Place product in bin.\n \n■Pick product from bin.\n \n■Package product for shipment.\n \n■Ship product to customer.\n \n■Receive product from customer.\n \n■Return product to inventory from customer return.\n \n■Remove product from inventory.\n",
      "content_length": 1847,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 153,
      "content": "Inventory 117\nEach inventory transaction identiﬁ es the date, product, warehouse, vendor, trans-\naction type, and in most cases, a single amount representing the inventory quantity \nimpact caused by the transaction. Assuming the granularity of the fact table is one \nrow per inventory transaction, the resulting schema is illustrated in Figure 4-4.\nDate Dimension\nWarehouse Inventory Transaction Fact\nDate Key (FK)\nProduct Key (FK)\nWarehouse Key (FK)\nInventory Transaction Type Key (FK)\nInventory Transaction Number (DD)\nInventory Transaction Dollar Amount\nInventory Transaction Type Key (PK)\nInventory Transaction Type Description\nInventory Transaction Type Group\nWarehouse Key (PK)\nWarehouse Number (NK)\nWarehouse Name\nWarehouse Address\nWarehouse City\nWarehouse City-State\nWarehouse State\nWarehouse ZIP\nWarehouse Zone\nWarehouse Total Square Footage\n...\nProduct Dimension\nInventory Transaction Type Dimension\nWarehouse Dimension\nFigure 4-4: Warehouse inventory transaction model.\nEven though the transaction fact table is simple, it contains detailed information \nthat mirrors individual inventory manipulations. The transaction fact table is use-\nful for measuring the frequency and timing of speciﬁ c transaction types to answer \nquestions that couldn’t be answered by the less granular periodic snapshot.\nEven so, it is impractical to use the transaction fact table as the sole basis for ana-\nlyzing inventory performance. Although it is theoretically possible to reconstruct the \nexact inventory position at any moment in time by rolling all possible transactions \nforward from a known inventory position, it is too cumbersome and impractical \nfor broad analytic questions that span dates, products, warehouses, or vendors.\nNOTE \nRemember there’s more to life than transactions alone. Some form of a \nsnapshot table to give a more cumulative view of a process often complements \na transaction fact table. \nBefore leaving the transaction fact table, our example presumes each type of \ntransaction impacting inventory levels positively or negatively has consistent dimen-\nsionality: date, product, warehouse, vendor, and transaction type. We recognize \nsome transaction types may have varied dimensionality in the real world. For \nexample, a shipper may be associated with the warehouse receipts and shipments; \ncustomer information is likely associated with shipments and customer returns. If the \n",
      "content_length": 2402,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 154,
      "content": "Chapter 4\n118\ntransactions’ dimensionality varies by event, then a series of related fact tables should \nbe designed rather than capturing all inventory transactions in a single fact table.\nNOTE \nIf performance measurements have different natural granularity or \ndimensionality, they likely result from separate processes that should be modeled \nas separate fact tables.\n Inventory Accumulating Snapshot\nThe  ﬁ nal inventory model is the accumulating snapshot. Accumulating snapshot \nfact tables are used for processes that have a deﬁ nite beginning, deﬁ nite end, and \nidentiﬁ able milestones in between. In this inventory model, one row is placed in the \nfact table when a particular product is received at the warehouse. The disposition \nof the product is tracked on this single fact row until it leaves the warehouse. In \nthis example, the accumulating snapshot model is only possible if you can reliably \ndistinguish products received in one shipment from those received at a later time; \nit is also appropriate if you track product movement by product serial number or \nlot number.\nNow assume that inventory levels for a product lot captured a series of well-\ndeﬁ ned events or milestones as it moves through the warehouse, such as receiving, \ninspection, bin placement, and shipping. As illustrated in Figure 4-5, the inventory \naccumulating snapshot fact table with its multitude of dates and facts looks quite \ndiff erent from the transaction or periodic snapshot schemas.\nInventory Receipt Accumulating Fact\nProduct Lot Receipt Number (DD)\nDate Received Key (FK)\nDate Inspected Key (FK)\nDate Bin Placement Key (FK)\nDate Initial Shipment Key (FK)\nDate Last Shipment Key (FK)\nProduct Key (FK)\nWarehouse Key (FK)\nVendor Key (FK)\nQuantity Received\nQuantity Inspected\nQuantity Returned to Vendor\nQuantity Placed in Bin\nQuantity Shipped to Customer\nQuantity Returned by Customer\nQuantity Returned to Inventory\nQuantity Damaged\nReceipt to Inspected Lag\nReceipt to Bin Placement Lag\nReceipt to Initial Shipment Lag\nInitial to Last Shipment Lag\nDate Received Dimension\nProduct Dimension\nWarehouse Dimension\nVendor Dimension\nDate Inspected Dimension\nDate Bin Placement Dimension\nDate Initial Shipment Dimension\nDate Last Shipment Dimension\nFigure 4-5: Warehouse inventory accumulating snapshot.\n",
      "content_length": 2295,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 155,
      "content": "Inventory 119\nThe accumulating snapshot fact table provides an updated status of the lot as it \nmoves through standard milestones represented by multiple date-valued foreign \nkeys. Each accumulating snapshot fact table row is updated repeatedly until the \nproducts received in a lot are completely depleted from the warehouse, as shown \nin Figure 4-6.\nDate Inspected\nKey\nReceipt to\nInspected Lag\nReceipt to Bin\nPlacement Lag\nDate Bin\nPlacement Key\nProduct\nKey\nQuantity\nReceived\nLot Receipt\nNumber\nDate Received\nKey\n101\n20130101\n0\n0\n1\n100\nFact row inserted when lot received:\nDate Inspected\nKey\nReceipt to\nInspected Lag\nReceipt to Bin\nPlacement Lag\nDate Bin\nPlacement Key\nProduct\nKey\nQuantity\nReceived\nLot Receipt\nNumber\nDate Received\nKey\n101\n20130101\n20130103\n0\n1\n100\n2\nFact row updated when lot inspected:\nDate Inspected\nKey\nReceipt to\nInspected Lag\nReceipt to Bin\nPlacement Lag\nDate Bin\nPlacement Key\nProduct\nKey\nQuantity\nReceived\nLot Receipt\nNumber\nDate Received\nKey\n101\n20130101\n20130103\n20130104\n1\n100\n2\n3\nFact row updated when lot placed in bin:\nFigure 4-6: Evolution of an accumulating snapshot fact row.\nFact Table Types\nThere are just three fundamental types of fact tables: transaction, periodic snapshot, \nand accumulating snapshot. Amazingly, this simple pattern holds true regardless \nof the industry. All three types serve a useful purpose; you often need two comple-\nmentary fact tables to get a complete picture of the business, yet the administration \nand rhythm of the three fact tables are quite diff erent. Figure 4-7 compares and \ncontrasts the variations.\nTransaction\nDiscrete transaction point\nin time\nRecurring snapshots at\nregular, predictable intervals\n1 row per transaction or\ntransaction line \n1 row per snapshot period\nplus other dimensions\nSnapshot date\nCumulative performance\nfor time interval\nPredictably dense\nTransaction date\nTransaction performance\nSparse or dense, depending\non activity\nNo updates, unless error\ncorrection\nNo updates, unless error\ncorrection\nIndeterminate time span for\nevolving pipeline/workflow\n1 row per pipeline\noccurrence\nMultiple dates for pipeline’s\nkey milestones\nPerformance for pipeline\noccurrence\nSparse or dense, depending\non pipeline occurrence\nUpdated whenever pipeline\nactivity occurs\nPeriodicity\nGrain\nDate dimension(s)\nFacts\nFact table sparsity\nFact table updates\nPeriodic Snapshot\nAccumulating Snapshot\nFigure 4-7: Fact table type comparisons.\n",
      "content_length": 2415,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 156,
      "content": "Chapter 4\n120\nTransaction Fact Tables\nThe  most fundamental view of the business’s operations is at the individual transac-\ntion or transaction line level. These fact tables represent an event that occurred at \nan instantaneous point in time. A row exists in the fact table for a given customer \nor product only if a transaction event occurred. Conversely, a given customer or \nproduct likely is linked to multiple rows in the fact table because hopefully the \ncustomer or product is involved in more than one transaction.\nTransaction data ﬁ ts easily into a dimensional framework. Atomic transaction \ndata is the most naturally dimensional data, enabling you to analyze behavior in \nextreme detail. After a transaction has been posted in the fact table, you typically \ndon’t revisit it.\nHaving made a solid case for the charm of transaction detail, you may be think-\ning that all you need is a big, fast server to handle the gory transaction minutiae, \nand your job is over. Unfortunately, even with transaction level data, there are busi-\nness questions that are impractical to answer using only these details. As indicated \nearlier, you cannot survive on transactions  alone.\nPeriodic Snapshot Fact Tables\nPeriodic  snapshots are needed to see the cumulative performance of the business \nat regular, predictable time intervals. Unlike the transaction fact table where a row \nis loaded for each event occurrence, with the periodic snapshot, you take a picture \n(hence the snapshot terminology) of the activity at the end of a day, week, or month, \nthen another picture at the end of the next period, and so on. The periodic snap-\nshots are stacked consecutively into the fact table. The periodic snapshot fact table \noften is the only place to easily retrieve a regular, predictable view of longitudinal \nperformance trends.\nWhen transactions equate to little pieces of revenue, you can move easily from \nindividual transactions to a daily snapshot merely by adding up the transactions. \nIn this situation, the periodic snapshot represents an aggregation of the transac-\ntional activity that occurred during a time period; you would build the snapshot \nonly if needed for performance reasons. The design of the snapshot table is closely \nrelated to the design of its companion transaction table in this case. The fact tables \nshare many dimension tables; the snapshot usually has fewer dimensions overall. \nConversely, there are usually more facts in a summarized periodic snapshot table \nthan in a transactional table because any activity that happens during the period \nis fair game for a metric in a periodic snapshot.\nIn many businesses, however, transaction details are not easily summarized to \npresent management performance metrics. As you saw in this inventory case study, \n",
      "content_length": 2785,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 157,
      "content": "Inventory 121\ncrawling through the transactions would be extremely time-consuming, plus the \nlogic required to interpret the eff ect of diff erent kinds of transactions on inventory \nlevels could be horrendously complicated, presuming you even have access to the \nrequired historical data. The periodic snapshot again comes to the rescue to provide \nmanagement with a quick, ﬂ exible view of inventory levels. Hopefully, the data for \nthis snapshot schema is sourced directly from an operational system that handles \nthese complex calculations. If not, the ETL system must also implement this com-\nplex logic to correctly interpret the impact of each transaction type.\nAccumulating Snapshot Fact Tables\nLast,  but not least, the third type of fact table is the accumulating snapshot. Although \nperhaps not as common as the other two fact table types, accumulating snapshots \ncan be very insightful. Accumulating snapshots represent processes that have a \ndeﬁ nite beginning and deﬁ nite end together with a standard set of intermediate \nprocess steps. Accumulating snapshots are most appropriate when business users \nwant to perform workﬂ ow or pipeline analysis.\nAccumulating snapshots always have multiple date foreign keys, representing the \npredictable major events or process milestones; sometimes there’s an additional date \ncolumn that indicates when the snapshot row was last updated. As we’ll discuss in \nChapter 6: Order Management, these dates are each handled by a role-playing date \ndimension. Because most of these dates are not known when the fact row is ﬁ rst \nloaded, a default surrogate date key is used for the undeﬁ ned  dates.\nLags Between Milestones and Milestone Counts\nBecause  accumulating snapshots often represent the effi  ciency and elapsed time of \na workﬂ ow or pipeline, the fact table typically contains metrics representing the \ndurations or lags between key milestones. It would be diffi  cult to answer duration \nquestions using a transaction fact table because you would need to correlate rows \nto calculate time lapses. Sometimes the lag metrics are simply the raw diff erence \nbetween the milestone dates or date/time stamps. In other situations, the lag calcula-\ntion is made more complicated by taking workdays and holidays into consideration.\nAccumulating snapshot fact tables sometimes include milestone completion coun-\nters, valued as either 0 or 1. Finally, accumulating snapshots often have a foreign \nkey to a status dimension, which is updated to reﬂ ect the pipeline’s latest status.\nAccumulating Snapshot Updates and OLAP Cubes\nIn  sharp contrast to the other fact table types, you purposely revisit accumulating \nsnapshot fact table rows to update them. Unlike the periodic snapshot where the \nprior snapshots are preserved, the accumulating snapshot merely reﬂ ects the most \n",
      "content_length": 2829,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 158,
      "content": "Chapter 4\n122\ncurrent status and metrics. Accumulating snapshots do not attempt to accommodate \ncomplex scenarios that occur infrequently. The analysis of these outliers can always \nbe done with the transaction fact table.\nIt is worth noting that accumulating snapshots are typically problematic for \nOLAP cubes. Because updates to an accumulating snapshot force both facts and \ndimension foreign keys to change, much of the cube would need to be reprocessed \nwith updates to these snapshots, unless the fact row is only loaded once the pipeline \noccurrence is complete.\nComplementary Fact Table Types\nSometimes accumulating and periodic snapshots work in conjunction with one \nanother, such as when you incrementally build the monthly snapshot by adding the \neff ect of each day’s transactions to a rolling accumulating snapshot while also storing \n36 months of historical data in a periodic snapshot. Ideally, when the last day of the \nmonth has been reached, the accumulating snapshot simply becomes the new regular \nmonth in the time series, and a new accumulating snapshot is started the next day.\nTransactions and snapshots are the yin and yang of dimensional designs. Used \ntogether, companion transaction and snapshot fact tables provide a complete view \nof the business. Both are needed because there is often no simple way to combine \nthese two contrasting perspectives in a single fact table. Although there is some \ntheoretical data redundancy between transaction and snapshot tables, you don’t \nobject to such redundancy because as DW/BI publishers, your mission is to publish \ndata so that the organization can eff ectively analyze it. These separate types of fact \ntables each provide diff erent vantage points on the same story. Amazingly, these \nthree types of fact tables turn out to be all the fact table types needed for the use \ncases described in this  book.\nValue Chain Integration\nNow  that we’ve completed the design of three inventory models, let’s revisit our ear-\nlier discussion about the retailer’s value chain. Both business and IT organizations \nare typically interested in value chain integration. Business management needs to \nlook across the business’s processes to better evaluate performance. For example, \nnumerous DW/BI projects have focused on better understanding customer behavior \nfrom an end-to-end perspective. Obviously, this requires the ability to consistently \nlook at customer information across processes, such as quotes, orders, invoicing, \npayments, and customer service. Similarly, organizations want to analyze their \nproducts across processes, or their employees, students, vendors, and so on.\n",
      "content_length": 2650,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 159,
      "content": "Inventory 123\nIT managers recognize integration is needed to deliver on the promises of data \nwarehousing and business intelligence. Many consider it their ﬁ duciary respon-\nsibility to manage the organization’s information assets. They know they’re not \nfulﬁ lling their responsibilities if they allow standalone, nonintegrated databases \nto proliferate. In addition to addressing the business’s needs, IT also beneﬁ ts from \nintegration because it allows the organization to better leverage scarce resources \nand gain effi  ciencies through the use of reusable components.\nFortunately, the senior managers who typically are most interested in integration \nalso have the necessary organizational inﬂ uence and economic willpower to make \nit happen. If they don’t place a high value on integration, you face a much more \nserious organizational challenge, or put more bluntly, your integration project will \nprobably fail. It shouldn’t be the sole responsibility of the DW/BI manager to garner \norganizational consensus for integration across the value chain. The political sup-\nport of senior management is important; it takes the DW/BI manager off  the hook \nand places the burden on senior leadership’s shoulders where it belongs. \nIn Chapters 3 and 4, we modeled data from several processes of the retailer’s value \nchain. Although separate fact tables in separate dimensional schemas represent the \ndata from each process, the models share several common business dimensions: \ndate, product, and store. We’ve logically represented this dimension sharing in \nFigure 4-8. Using shared, common dimensions is absolutely critical to designing \ndimensional models that can be integrated.\nStore Dimension\nDate Dimension\nRetail Sales\nTransaction Facts\nRetail Inventory\nSnapshot Facts\nWarehouse Inventory\nTransaction Facts\nPromotion Dimension\nProduct Dimension\nWarehouse Dimension\nFigure 4-8: Sharing dimensions among business processes.\nEnterprise Data Warehouse Bus Architecture\nObviously,  building the enterprise’s DW/BI system in one galactic eff ort is too daunt-\ning, yet building it as isolated pieces defeats the overriding goal of consistency. For \nlong-term DW/BI success, you need to use an architected, incremental approach to \nbuild the enterprise’s warehouse. The approach we advocate is the enterprise data \nwarehouse bus architecture.\n",
      "content_length": 2347,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 160,
      "content": "Chapter 4\n124\nUnderstanding the Bus Architecture\nContrary  to popular belief, the word bus is not shorthand for business; it’s an old \nterm from the electrical power industry that is now used in the computer industry. \nA bus is a common structure to which everything connects and from which every-\nthing derives power. The bus in a computer is a standard interface speciﬁ cation \nthat enables you to plug in a disk drive, DVD, or any number of other specialized \ncards or devices. Because of the computer’s bus standard, these peripheral devices \nwork together and usefully coexist, even though they were manufactured at diff er-\nent times by diff erent vendors.\nNOTE \nBy deﬁ ning a standard bus interface for the DW/BI environment, separate \ndimensional models can be implemented by diff erent groups at diff erent times. \nThe separate business process subject areas plug together and usefully coexist if \nthey adhere to the standard.\nIf you refer back to the value chain diagram in Figure 4-1, you can envision many \nbusiness processes plugging into the enterprise data warehouse bus, as illustrated \nin Figure 4-9. Ultimately, all the processes of an organization’s value chain create \na family of dimensional models that share a comprehensive set of common, con-\nformed dimensions.\nStore Sales\nStore Inventory\nPurchase Orders\nDate\nProduct\nStore\nPromotion\nWarehouse\nVendor\nShipper\nFigure 4-9: Enterprise data warehouse bus with shared dimensions.\nThe enterprise data warehouse bus architecture provides a rational approach to \ndecomposing the enterprise DW/BI planning task. The master suite of standard-\nized dimensions and facts has a uniform interpretation across the enterprise. This \nestablishes the data architecture framework. You can then tackle the implementation \nof separate process-centric dimensional models, with each implementation closely \n",
      "content_length": 1859,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 161,
      "content": "Inventory 125\nadhering to the architecture. As the separate dimensional models become available, \nthey ﬁ t together like the pieces of a puzzle. At some point, enough dimensional models \nexist to make good on the promise of an integrated enterprise DW/BI environment.\nThe bus architecture enables DW/BI managers to get the best of both worlds. \nThey have an architectural framework guiding the overall design, but the problem \nhas been divided into bite-sized business process chunks that can be implemented \nin realistic time frames. Separate development teams follow the architecture while \nworking fairly independently and asynchronously.\nThe bus architecture is independent of technology and database platforms. All \nﬂ avors of relational and OLAP-based dimensional models can be full participants \nin the enterprise data warehouse bus if they are designed around conformed dimen-\nsions and facts. DW/BI systems inevitably consist of separate machines with diff erent \noperating systems and database management systems. Designed coherently, they \nshare a common architecture of conformed dimensions and facts, allowing them \nto be fused into an integrated whole.\n Enterprise Data Warehouse Bus Matrix\nWe  recommend using an enterprise data warehouse bus matrix to document and com-\nmunicate the bus architecture, as illustrated in Figure 4-10. Others have renamed the \nbus matrix, such as the conformance or event matrix, but these are merely synonyms \nfor this fundamental Kimball concept ﬁ rst introduced in the 1990s.\nIssue Purchase Orders\nReceive Warehouse Deliveries\nWarehouse Inventory\nReceive Store Deliveries\nStore Inventory\nRetail Sales\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nRetail Sales Forecast\nX\nX\nRetail Promotion Tracking\nX\nX\nX\nX\nX\nCustomer Returns\nX\nReturns to Vendor\nX\nX\nX\nX\nX\nX\nFrequent Shopper Sign-Ups\nBUSINESS PROCESSES\nCOMMON DIMENSIONS\nPromotion\nStore\nX\nX\nX\nX\nX\nX\nX\nX\nX\nEmployee\nCustomer\nWarehouse\nProduct\nDate\nFigure 4-10: Sample enterprise data warehouse bus matrix for a retailer.\n",
      "content_length": 2016,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 162,
      "content": "Chapter 4\n126\nWorking in a tabular fashion, the organization’s business processes are repre-\nsented as matrix rows. It is important to remember you are identifying business \nprocesses, not the organization’s business departments. The matrix rows translate \ninto dimensional models representing the organization’s primary activities and \nevents, which are often recognizable by their operational source. When it’s time to \ntackle a DW/BI development project, start with a single business process matrix row \nbecause that minimizes the risk of signing up for an overly ambitious implementa-\ntion. Most implementation risk comes from biting off  too much ETL system design \nand development. Focusing on the results of a single process, often captured by a \nsingle underlying source system, reduces the ETL development risk.\nAfter individual business processes are enumerated, you sometimes identify more \ncomplex consolidated processes. Although dimensional models that cross processes \ncan be immensely beneﬁ cial in terms of both query performance and ease of use, \nthey are typically more diffi  cult to implement because the ETL eff ort grows with \neach additional major source integrated into a single dimensional model. It is pru-\ndent to focus on the individual processes as building blocks before tackling the task \nof consolidating. Proﬁ tability is a classic example of a consolidated process in which \nseparate revenue and cost factors are combined from diff erent processes to provide a \ncomplete view of proﬁ tability. Although a granular proﬁ tability dimensional model \nis exciting, it is deﬁ nitely not the ﬁ rst dimensional model you should attempt to \nimplement; you could easily drown while trying to wrangle all the revenue and \ncost components.\nThe columns of the bus matrix represent the common dimensions used across \nthe enterprise. It is often helpful to create a list of core dimensions before ﬁ lling \nin the matrix to assess whether a given dimension should be associated with a busi-\nness process. The number of bus matrix rows and columns varies by organization. \nFor many, the matrix is surprisingly square with approximately 25 to 50 rows and \na comparable number of columns. In other industries, like insurance, there tend to \nbe more columns than rows.\nAfter the core processes and dimensions are identiﬁ ed, you shade or “X” the \nmatrix cells to indicate which columns are related to each row. Presto! You can \nimmediately see the logical relationships and interplay between the organization’s \nconformed dimensions and key business  processes.\nMultiple Matrix Uses\nCreating  the enterprise data warehouse bus matrix is one of the most important \nDW/BI implementation deliverables. It is a hybrid resource that serves multiple \npurposes, including architecture planning, database design, data governance \ncoordination, project estimating, and organizational communication.\n",
      "content_length": 2905,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 163,
      "content": "Inventory 127\nAlthough it is relatively straightforward to lay out the rows and columns, the \nenterprise bus matrix deﬁ nes the overall data architecture for the DW/BI system. \nThe matrix delivers the big picture perspective, regardless of database or technol-\nogy preferences.\nThe matrix’s columns address the demands of master data management and \ndata integration head-on. As core dimensions participating in multiple dimensional \nmodels are deﬁ ned by folks with data governance responsibilities and built by the \nDW/BI team, you can envision their use across processes rather than designing in \na vacuum based on the needs of a single process, or even worse, a single depart-\nment. Shared dimensions supply potent integration glue, allowing the business to \ndrill across processes.\nEach business process-centric implementation project incrementally builds out \nthe overall architecture. Multiple development teams can work on components \nof the matrix independently and asynchronously, with conﬁ dence they’ll ﬁ t together. \nProject managers can look across the process rows to see the dimensionality of \neach dimensional model at a glance. This vantage point is useful as they’re gauging \nthe magnitude of the project’s eff ort. A project focused on a business process with \nfewer dimensions usually requires less eff ort, especially if the politically charged \ndimensions are already sitting on the shelf.\nThe matrix enables you to communicate effectively within and across data \ngovernance and DW/BI teams. Even more important, you can use the matrix to \ncommunicate upward and outward throughout the organization. The matrix is a \nsuccinct deliverable that visually conveys the master plan. IT management needs \nto understand this perspective to coordinate across project teams and resist the \norganizational urge to deploy more departmental solutions quickly. IT management \nmust also ensure that distributed DW/BI development teams are committed to the \nbus architecture. Business management needs to also appreciate the holistic plan; \nyou want them to understand the staging of the DW/BI rollout by business process. \nIn addition, the matrix illustrates the importance of identifying experts from the \nbusiness to serve as data governance leaders for the common dimensions. It is a \ntribute to its simplicity that the matrix can be used eff ectively to communicate \nwith developers, architects, modelers, and project managers, as well as senior IT \nand business management.\n Opportunity/Stakeholder Matrix\nYou can  draft a diff erent matrix that leverages the same business process rows, \nbut replaces the dimension columns with business functions, such as merchandis-\ning, marketing, store operations, and ﬁ nance. Based on each function’s requirements, \nthe matrix cells are shaded to indicate which business functions are interested in \n",
      "content_length": 2853,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 164,
      "content": "Chapter 4\n128\nwhich business processes (and projects), as illustrated in Figure 4-11’s opportunity/\nstakeholder matrix variation. It also identiﬁ es which groups need to be invited to \nthe detailed requirements, dimensional modeling, and BI application speciﬁ cation \nparties after a process-centric row is queued up as a project.\nIssue Purchase Orders\nReceive Warehouse Deliveries\nWarehouse Inventory\nReceive Store Deliveries\nStore Inventory\nRetail Sales\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nRetail Sales Forecast\nX\nX\nX\nX\nRetail Promotion Tracking\nX\nX\nX\nX\nX\nX\nCustomer Returns\nX\nX\nReturns to Vendor\nX\nX\nX\nX\nFrequent Shopper Sign-Ups\nX\nX\nX\nBUSINESS PROCESSES\nSTAKEHOLDERS\nFinance\nLogistics\nStore Operations\nMarketing\nMerchandising\nFigure 4-11: Opportunity/stakeholder matrix.\nCommon Bus Matrix Mistakes\nWhen  drafting a bus matrix, people sometimes struggle with the level of detail \nexpressed by each row, resulting in the following missteps:\n \n■Departmental  or overly encompassing rows. The matrix rows shouldn’t cor-\nrespond to the boxes on a corporate organization chart representing functional \ngroups. Some departments may be responsible or acutely interested in a single \nbusiness process, but the matrix rows shouldn’t look like a list of the CEO’s \ndirect reports.\n \n■Report-centric  or too narrowly deﬁ ned rows. At the opposite extreme, the \nbus matrix shouldn’t resemble a laundry list of requested reports. A single \nbusiness process supports numerous analyses; the matrix row should refer-\nence the business process, not the derivative reports or analytics.\nWhen deﬁ ning the matrix columns, architects naturally fall into the similar traps \nof deﬁ ning columns that are either too broad or too narrow:\n",
      "content_length": 1737,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 165,
      "content": "Inventory 129\n \n■Overly  generalized columns. A “person” column on the bus matrix may refer \nto a wide variety of people, from internal employees to external suppliers \nand customer contacts. Because there’s virtually zero overlap between these \npopulations, it adds confusion to lump them into a single, generic dimension. \nSimilarly, it’s not beneﬁ cial to put internal and external addresses referring \nto corporate facilities, employee addresses, and customer sites into a generic \nlocation column in the matrix.\n \n■Separate  columns for each level of a hierarchy. The columns of the bus \nmatrix should refer to dimensions at their most granular level. Some \nbusiness process rows may require an aggregated version of the detailed \ndimension, such as inventory snapshot metrics at the weekly level. Rather \nthan creating separate matrix columns for each level of the calendar hierarchy, \nuse a single column for dates. To express levels of detail above a daily grain, \nyou can denote the granularity within the matrix cell; alternatively, you can \nsubdivide the date column to indicate the hierarchical level associated with \neach business process row. It’s important to retain the overarching identiﬁ ca-\ntion of common dimensions deployed at diff erent levels of granularity. Some \nindustry pundits advocate matrices that treat every dimension table attribute \nas a separate, independent column; this defeats the concept of dimensions \nand results in a completely unruly matrix.\nRetroﬁ tting Existing Models to a Bus Matrix\nIt  is unacceptable to build separate dimensional models that ignore a framework \ntying them together. Isolated, independent dimensional models are worse than \nsimply a lost opportunity for analysis. They deliver access to irreconcilable views \nof the organization and further enshrine the reports that cannot be compared with \none another. Independent dimensional models become legacy implementations \nin their own right; by their existence, they block the development of a coherent \nDW/BI environment.\nSo what happens if you’re not starting with a blank slate? Perhaps several dimen-\nsional models have been constructed without regard to an architecture using \nconformed dimensions. Can you rescue your stovepipes and convert them to the \nbus architecture? To answer this question, you should start ﬁ rst with an honest \nappraisal of your existing non-integrated dimensional structures. This typically \nentails meetings with the separate teams (including the clandestine pseudo IT \nteams within business organizations) to determine the gap between the current \nenvironment and the organization’s architected goal. When the gap is understood, \nyou need to develop an incremental plan to convert the standalone dimensional \nmodels to the enterprise architecture. The plan needs to be internally sold. Senior \nIT and business management must understand the current state of data chaos, the \n",
      "content_length": 2920,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 166,
      "content": "Chapter 4\n130\nrisks of doing nothing, and the beneﬁ ts of moving forward according to your game \nplan. Management also needs to appreciate that the conversion will require a sig-\nniﬁ cant commitment of support, resources, and funding.\nIf an existing dimensional model is based on a sound dimensional design, per-\nhaps you can map an existing dimension to a standardized version. The original \ndimension table would be rebuilt using a cross-reference map. Likewise, the fact \ntable would need to be reprocessed to replace the original dimension keys with the \nconformed dimension keys. Of course, if the original and conformed dimension \ntables contain diff erent attributes, rework of the preexisting BI applications and \nqueries is inevitable.\nMore typically, existing dimensional models are riddled with dimensional model-\ning errors beyond the lack of adherence to standardized dimensions. In some cases, \nthe stovepipe dimensional model has outlived its useful life. Isolated dimensional \nmodels often are built for a speciﬁ c functional area. When others try to leverage \nthe data, they typically discover that the dimensional model was implemented at \nan inappropriate level of granularity and is missing key dimensionality. The eff ort \nrequired to retroﬁ t these dimensional models into the enterprise DW/BI architec-\nture may exceed the eff ort to start over from scratch. As diffi  cult as it is to admit, \nstovepipe dimensional models often have to be shut down and rebuilt in the proper \nbus architecture framework.\n Conformed Dimensions\nNow  that you understand the importance of the enterprise bus architecture, let’s fur-\nther explore the standardized conformed dimensions that serve as the cornerstone \nof the bus because they’re shared across business process fact tables. Conformed \ndimensions go by many other aliases: common dimensions, master dimensions, ref-\nerence dimensions, and shared dimensions. Conformed dimensions should be built \nonce in the ETL system and then replicated either logically or physically throughout \nthe enterprise DW/BI environment. When built, it’s extremely important that the \nDW/BI development teams take the pledge to use these dimensions. It’s a policy \ndecision that is critical to making the enterprise DW/BI system function; their usage \nshould be mandated by the organization’s CIO.\n Drilling Across Fact Tables\nIn  addition to consistency and reusability, conformed dimensions enable you to com-\nbine performance measurements from diff erent business processes in a single report, \nas illustrated in Figure 4-12. You can use multipass SQL to query each dimensional \n",
      "content_length": 2625,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 167,
      "content": "Inventory 131\nmodel separately and then outer-join the query results based on a common dimen-\nsion attribute, such as Figure 4-12’s product name. The full outer-join ensures all \nrows are included in the combined report, even if they only appear in one set of \nquery results. This linkage, often referred to as drill across, is straightforward if the \ndimension table attribute values are identical.\nProduct Description\nBaked Well Sourdough\nFluffy Light Sliced White\nFluffy Sliced Whole Wheat\n1,201\n1,472\n846\n935\n801\n513\n1,042\n922\n368\nOpen Orders Qty\nInventory Qty\nSales Qty\nFigure 4-12: Drilling across fact tables with conformed dimension attributes.\nDrilling across is supported by many BI products and platforms. Their implemen-\ntations diff er on whether the results are joined in temporary tables, the application \nserver, or the report. The vendors also use diff erent terms to describe this technique, \nincluding multipass, multi-select, multi-fact, or stitch queries. Because metrics from \ndiff erent fact tables are brought together with a drill-across query, often any cross-\nfact calculations must be done in the BI application after the separate conformed \nresults have been returned.\nConformed dimensions come in several diff erent ﬂ avors, as described in the \nfollowing sections.\nIdentical Conformed Dimensions\nAt  the most basic level, conformed dimensions mean the same thing with every pos-\nsible fact table to which they are joined. The date dimension table connected to the \nsales facts is identical to the date dimension table connected to the inventory facts. \nIdentical conformed dimensions have consistent dimension keys, attribute column \nnames, attribute deﬁ nitions, and attribute values (which translate into consistent \nreport labels and groupings). Dimension attributes don’t conform if they’re called \nMonth in one dimension and Month Name in another; likewise, they don’t conform \nif the attribute value is “July” in one dimension and “JULY” in another. Identical \nconformed dimensions in two dimensional models may be the same physical table \nwithin the database. However, given the typical complexity of the DW/BI system’s \ntechnical environment with multiple database platforms, it is more likely that the \ndimension is built once in the ETL system and then duplicated synchronously out-\nward to each dimensional model. In either case, the conformed date dimensions in \nboth dimensional models have the same number of rows, same key values, same \nattribute labels, same attribute data deﬁ nitions, and same attribute values. Attribute \ncolumn names should be uniquely labeled across dimensions.\n",
      "content_length": 2631,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 168,
      "content": "Chapter 4\n132\nMost  conformed dimensions are deﬁ ned naturally at the most granular level \npossible. The product dimension’s grain will be the individual product; the date \ndimension’s grain will be the individual day. However, sometimes dimensions at the \nsame level of granularity do not fully conform. For example, there might be product \nand store attributes needed for inventory analysis, but they aren’t appropriate for \nanalyzing retail sales data. The dimension tables still conform if the keys and com-\nmon columns are identical, but the supplemental attributes used by the inventory \nschema are not conformed. It is physically impossible to drill across processes using \nthese add-on attributes.\n Shrunken Rollup Conformed Dimension \nwith Attribute Subset\nDimensions  also conform when they contain a subset of attributes from a more \ngranular dimension. Shrunken rollup dimensions are required when a fact table \ncaptures performance metrics at a higher level of granularity than the atomic \nbase dimension. This would be the case if you had a weekly inventory snapshot in \naddition to the daily snapshot. In other situations, facts are generated by another \nbusiness process at a higher level of granularity. For example, the retail sales pro-\ncess captures data at the atomic product level, whereas forecasting generates data \nat the brand level. You couldn’t share a single product dimension table across the \ntwo business process schemas because the granularity is diff erent. The product \nand brand dimensions still conform if the brand table attributes are a strict subset \nof the atomic product table’s attributes. Attributes that are common to both the \ndetailed and rolled-up dimension tables, such as the brand and category descrip-\ntions, should be labeled, deﬁ ned, and identically valued in both tables, as illustrated \nin Figure 4-13. However, the primary keys of the detailed and rollup dimension \ntables are separate.\nNOTE \nShrunken rollup dimensions conform to the base atomic dimension if \nthe attributes are a strict subset of the atomic dimension’s attributes.\nShrunken Conformed Dimension with Row Subset\nAnother  case of conformed dimension subsetting occurs when two dimensions are \nat the same level of detail, but one represents only a subset of rows. For example, a \ncorporate product dimension contains rows for the full portfolio of products across \nmultiple disparate lines of business, as illustrated in Figure 4-14. Analysts in the \n",
      "content_length": 2475,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 169,
      "content": "Inventory 133\nseparate businesses may want to view only their subset of the corporate dimension, \nrestricted to the product rows for their business. By using a subset of rows, they \naren’t encumbered with the corporation’s entire product set. Of course, the fact table \njoined to this subsetted dimension must be limited to the same subset of products. \nIf a user attempts to use a shrunken subset dimension while accessing a fact table \nconsisting of the complete product set, they may encounter unexpected query results \nbecause referential integrity would be violated. You need to be cognizant of the \npotential opportunity for user confusion or error with dimension row subsetting. \nWe will further elaborate on dimension subsets when we discuss supertype and \nsubtype dimensions in Chapter 10: Financial Services.\nProduct Key (PK)\nProduct Description\nSKU Number (Natural Key)\nBrand Description\nSubcategory Description\nCategory Description\nDepartment Description\nPackage Type Description\nPackage Size\nFat Content Description\nDiet Type Description\nWeight\nWeight Units of Measure\n...\nBrand Key (PK)\nBrand Description\nSubcategory Description\nCategory Description\nDepartment Description\nMonth Key (PK)\nCalendar Month Name\nCalendar Month Number\nCalendar YYYY-MM\nCalendar Year\nDate Key (PK)\nDate\nFull Date Description\nDay of Week\nDay Number in Month\nCalendar Month Name\nCalendar Month Number\nCalendar YYYY-MM\nCalendar Year\nFiscal Week\nFiscal Month\n...\nProduct Dimension\nDate Dimension\nBrand Dimension\nMonth Dimension\nConforms\nConforms\nFigure 4-13: Conforming shrunken rollup dimensions.\n",
      "content_length": 1585,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 170,
      "content": "Chapter 4\n134\nAppliance\nProducts\nApparel\nProducts\nCorporate\nProduct Dimension\nDrilling across requires common conformed attributes.\nFigure 4-14: Conforming dimension subsets at the same granularity.\nConformed date and month dimensions are a unique example of both row \nand column dimension subsetting. Obviously, you can’t simply use the same date \ndimension table for daily and monthly fact tables because of the diff erence in rollup \ngranularity. However, the month dimension may consist of the month-end daily \ndate table rows with the exclusion of all columns that don’t apply at the monthly \ngranularity, such as the weekday/weekend indicator, week ending date, holiday \nindicator, day number within year, and others. Sometimes a month-end indicator on \nthe daily date dimension is used to facilitate creation of this month dimension  table. \nShrunken Conformed Dimensions on the Bus Matrix\nThe  bus matrix identiﬁ es the reuse of common dimensions across business processes. \nTypically, the shaded cells of the matrix indicate that the atomic dimension is \nassociated with a given process. When shrunken rollup or subset dimensions are \ninvolved, you want to reinforce their conformance with the atomic dimensions. \nTherefore, you don’t want to create a new, unrelated column on the bus matrix. \nInstead, there are two viable approaches to represent the shrunken dimensions within \nthe matrix, as illustrated in Figure 4-15:\n \n■Mark the cell for the atomic dimension, but then textually document the \nrollup or row subset granularity within the cell.\n \n■Subdivide the dimension column to indicate the common rollup or subset \ngranularities, such as day and month if processes collect data at both of these \ngrains.\n",
      "content_length": 1722,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 171,
      "content": "Inventory 135\nDate\nOR\nDate\nDay\nMonth\nIssue Purchase Orders\nReceive Deliveries\nInventory\nRetail Sales\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nMonth\nRetail Sales Forecast\nFigure 4-15: Alternatives for identifying shrunken dimensions on the bus matrix. \nLimited Conformity\nNow that we’ve preached about the importance of conformed dimensions, we’ll \ndiscuss the situation in which it may not be realistic or necessary to establish con-\nformed dimensions for the organization. If a conglomerate has subsidiaries spanning \nwidely varied industries, there may be little point in trying to integrate. If each line \nof business has unique customers and unique products and there’s no interest in \ncross-selling across lines, it may not make sense to attempt an enterprise archi-\ntecture because there likely isn’t much perceived business value. The willingness \nto seek a common deﬁ nition for product, customer, or other core dimensions is a \nmajor litmus test for an organization theoretically intent on building an enterprise \nDW/BI system. If the organization is unwilling to agree on common deﬁ nitions, the \norganization shouldn’t attempt to build an enterprise DW/BI environment. It would \nbe better to build separate, self-contained data warehouses for each subsidiary. But \nthen don’t complain when someone asks for “enterprise performance” without going \nthrough this logic.\nAlthough organizations may ﬁ nd it diffi  cult to combine data across disparate lines \nof business, some degree of integration is typically an ultimate goal. Rather than \nthrowing your hands in the air and declaring it can’t possibly be done, you should \nstart down the path toward conformity. Perhaps there are a handful of attributes that \ncan be conformed across lines of business. Even if it is merely a product description, \ncategory, and line of business attribute that is common to all businesses, this least-\ncommon-denominator approach is still a step in the right direction. You don’t need \nto get everyone to agree on everything related to a dimension before  proceeding.\n Importance of Data Governance and Stewardship\nWe’ve  touted the importance of conformed dimensions, but we also need to acknowl-\nedge a key challenge: reaching enterprise consensus on dimension attribute names \n",
      "content_length": 2252,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 172,
      "content": "Chapter 4\n136\nand contents (and the handling of content changes which we’ll discuss in Chapter 5: \nProcurement). In many organizations, business rules and data deﬁ nitions have \ntraditionally been established departmentally. The consequences of this commonly \nencountered lack of data governance and control are the ubiquitous departmental \ndata silos that perpetuate similar but slightly diff erent versions of the truth. Business \nand IT management need to recognize the importance of addressing this shortfall \nif you stand any chance of bringing order to the chaos; if management is reluctant \nto drive change, the project will never achieve its goals.\nOnce the data governance issues and opportunities are acknowledged by senior \nleadership, resources need to be identiﬁ ed to spearhead the eff ort. IT is often tempted \nto try leading the charge. They are frustrated by the isolated projects re-creating \ndata around the organization, consuming countless IT and outside resources while \ndelivering inconsistent solutions that ultimately just increase the complexity of \nthe organization’s data architecture at signiﬁ cant cost. Although IT can facilitate \nthe deﬁ nition of conformed dimensions, it is seldom successful as the sole driver, \neven if it’s a temporary assignment. IT simply lacks the organizational authority to \nmake things happen. \nBusiness-Driven Governance\nTo  boost the likelihood of business acceptance, subject matter experts from the \nbusiness need to lead the initiative. Leading a cross-organizational governance \nprogram is not for the faint of heart. The governance resources identiﬁ ed by busi-\nness leadership should have the following characteristics:\n \n■Respect from the organization\n \n■Broad knowledge of the enterprise’s operations\n \n■Ability to balance organizational needs against departmental requirements\n \n■Gravitas and authority to challenge the status quo and enforce policies\n \n■Strong communication skills\n \n■Politically savvy negotiation and consensus building skills\nClearly, not everyone is cut out for the job! Typically those tapped to spearhead \nthe governance program are highly valued and in demand. It takes the right skills, \nexperience, and conﬁ dence to rationalize diverse business perspectives and drive \nthe design of common reference data, together with the necessary organizational \ncompromises. Over the years, some have criticized conformed dimensions as being \ntoo hard. Yes, it’s diffi  cult to get people in diff erent corners of the business to agree \non common attribute names, deﬁ nitions, and values, but that’s the crux of uniﬁ ed, \nintegrated data. If everyone demands their own labels and business rules, there’s \nno chance of delivering on the promises made to establish a single version of the \n",
      "content_length": 2773,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 173,
      "content": "Inventory 137\ntruth. The data governance program is critical in facilitating a culture shift away \nfrom the typical siloed environment in which each department retains control of \ntheir data and analytics to one where information is shared and leveraged across \nthe organization.\nGovernance Objectives\nOne of the key objectives of the data governance function is to reach agreement on \ndata deﬁ nitions, labels, and domain values so that everyone is speaking the same \nlanguage. Otherwise, the same words may describe diff erent things; diff erent words \nmay describe the same thing; and the same value may have diff erent meaning. \nEstablishing common master data is often a politically charged issue; the chal-\nlenges are cultural and geopolitical rather than technical. Deﬁ ning a foundation of \nmaster descriptive conformed dimensions requires eff ort. But after it’s agreed upon, \nsubsequent DW/BI eff orts can leverage the work, both ensuring consistency and \nreducing the implementation’s delivery cycle time.\nIn addition to tackling data deﬁ nitions and contents, the data governance func-\ntion also establishes policies and responsibilities for data quality and accuracy, as \nwell as data security and access controls.\nHistorically,  DW/BI teams created the “recipes” for conformed dimensions and \nmanaged the data cleansing and integration mapping in the ETL system; the opera-\ntional systems focused on accurately capturing performance metrics, but there was \noften little eff ort to ensure consistent common reference data. Enterprise resource \nplanning (ERP) systems promised to ﬁ ll the void, but many organizations still \nrely on separate best-of-breed point solutions for niche requirements. Recently, \noperational master data management (MDM) solutions have addressed the need \nfor centralized master data at the source where the transactions are captured. \nAlthough technology can encourage data integration, it doesn’t ﬁ x the problem. \nA strong data governance function is a necessary prerequisite for conforming infor-\nmation regardless of technical  approach.\nConformed Dimensions and the Agile Movement\nSome  lament that although they want to deliver and share consistently deﬁ ned \nmaster conformed dimensions in their DW/BI environments, it’s “just not feasible.” \nThey explain they would if they could, but with senior management focused on \nusing agile development techniques, it’s “impossible” to take the time to get organi-\nzational agreement on conformed dimensions. You can turn this argument upside \ndown by challenging that conformed dimensions enable agile DW/BI development, \nalong with agile decision making.\n",
      "content_length": 2646,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 174,
      "content": "Chapter 4\n138\nConformed dimensions allow a dimension table to be built and maintained once \nrather than re-creating slightly diff erent versions during each development cycle. \nReusing conformed dimensions across projects is where you get the leverage for \nmore agile DW/BI development. As you ﬂ esh out the portfolio of master conformed \ndimensions, the development crank starts turning faster and faster. The time-to-\nmarket for a new business process data source shrinks as developers reuse existing \nconformed dimensions. Ultimately, new ETL development focuses almost exclusively \non delivering more fact tables because the associated dimension tables are already \nsitting on the shelf ready to go.\nDeﬁ ning a conformed dimension requires organizational consensus and com-\nmitment to data stewardship. But you don’t need to get everyone to agree on every \nattribute in every dimension table. At a minimum, you should identify a subset \nof attributes that have signiﬁ cance across the enterprise. These commonly referenced \ndescriptive characteristics become the starter set of conformed attributes, enabling \ndrill-across integration. Even just a single attribute, such as enterprise product \ncategory, is a viable starting point for the integration eff ort. Over time, you can \niteratively expand from this minimalist starting point by adding attributes. These \ndimensions could be tackled during architectural agile sprints. When a series of \nsprint deliverables combine to deliver suffi  cient value, they constitute a release to \nthe business users.\nIf you fail to focus on conformed dimensions because you’re under pressure to \ndeliver something yesterday, the departmental analytic data silos will likely have \ninconsistent categorizations and labels. Even more troubling, data sets may look \nlike they can be compared and integrated due to similar labels, but the underlying \nbusiness rules may be slightly diff erent. Business users waste inordinate amounts \nof time trying to reconcile and resolve these data inconsistencies, which negatively \nimpact their ability to be agile decision makers.\nThe senior IT managers who are demanding agile systems development practices \nshould be exerting even greater organizational pressure, in conjunction with their \npeers in the business, on the development of consistent conformed dimensions if \nthey’re interested in both long-term development effi  ciencies and long-term decision-\nmaking eff ectiveness across the enterprise.\n Conformed Facts\nThus  far we have considered the central task of setting up conformed dimensions to \ntie dimensional models together. This is 95 percent or more of the data architecture \neff ort. The remaining 5 percent of the eff ort goes into establishing conformed fact \ndeﬁ nitions.\n",
      "content_length": 2772,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 175,
      "content": "Inventory 139\nRevenue,  proﬁ t, standard prices and costs, measures of quality and customer \nsatisfaction, and other key performance indicators (KPIs) are facts that must also \nconform. If facts live in more than one dimensional model, the underlying deﬁ ni-\ntions and equations for these facts must be the same if they are to be called the \nsame thing. If they are labeled identically, they need to be deﬁ ned in the same \ndimensional context and with the same units of measure from dimensional model \nto dimensional model. For example, if several business processes report revenue, \nthen these separate revenue metrics can be added and compared only if they have \nthe same ﬁ nancial deﬁ nitions. If there are deﬁ nitional diff erences, then it is essential \nthat the revenue facts be labeled uniquely.\nNOTE \nYou must be disciplined in your data naming practices. If it is impos-\nsible to conform a fact exactly, you should give diff erent names to the diff erent \ninterpretations so that business users do not combine these incompatible facts in \ncalculations. \nSometimes a fact has a natural unit of measure in one fact table and another natu-\nral unit of measure in another fact table. For example, the ﬂ ow of product down the \nretail value chain may best be measured in shipping cases at the warehouse but in \nscanned units at the store. Even if all the dimensional considerations have been cor-\nrectly taken into account, it would be diffi  cult to use these two incompatible units of \nmeasure in one drill-across report. The usual solution to this kind of problem is to \nrefer the user to a conversion factor buried in the product dimension table and hope \nthat the user can ﬁ nd the conversion factor and correctly use it. This is unacceptable \nfor both overhead and vulnerability to error. The correct solution is to carry the fact \nin both units of measure, so a report can easily glide down the value chain, picking \noff  comparable facts. Chapter 6: Order Management talks more about multiple units \nof  measure.\nSummary\nIn this chapter we developed dimensional models for the three complementary \nviews of inventory. The periodic snapshot is a good choice for long-running, con-\ntinuously replenished inventory scenarios. The accumulating snapshot is a good \nchoice for ﬁ nite inventory pipeline situations with a deﬁ nite beginning and end. \nFinally, most inventory analysis will require a transactional schema to augment \nthese snapshot models.\nWe introduced key concepts surrounding the enterprise data warehouse bus \narchitecture and matrix. Each business process of the value chain, supported by a \n",
      "content_length": 2618,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 176,
      "content": "Chapter 4\n140\nprimary source system, translates into a row in the bus matrix, and eventually, a \ndimensional model. The matrix rows share a surprising number of standardized, \nconformed dimensions. Developing and adhering to the enterprise bus architecture \nis an absolute must if you intend to build a DW/BI system compose d of an integrated \nset of dimensional models.\n",
      "content_length": 371,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 177,
      "content": "Procurement\nW\ne explore procurement processes in this chapter. This subject area has \nobvious cross-industry appeal because it is applicable to any organization \nthat acquires products or services for either use or resale.\nIn addition to developing several purchasing models, this chapter provides \nin-depth coverage of the techniques for handling dimension table attribute value \nchanges. Although descriptive attributes in dimension tables are relatively static, \nthey are subject to change over time. Product lines are restructured, causing product \nhierarchies to change. Customers move, causing their geographic information to \nchange. We’ll describe several approaches to deal with these inevitable dimension \ntable changes. Followers of the Kimball methods will recognize the type 1, 2, and \n3 techniques. Continuing in this tradition, we’ve expanded the slowly changing \ndimension technique line-up with types 0, 4, 5, 6, and 7.\nChapter 5 discusses the following concepts:\n \n■Bus matrix snippet for procurement processes\n \n■Blended versus separate transaction schemas \n \n■Slowly changing dimension technique types 0 through 7, covering both basic \nand advanced hybrid scenarios \nProcurement Case Study\nThus far  we have studied downstream sales and inventory processes in the retailer’s \nvalue chain. We explained the importance of mapping out the enterprise data ware-\nhouse bus architecture where conformed dimensions are used across process-centric \nfact tables. In this chapter we’ll extend these concepts as we work our way further \nup the value chain to the procurement processes.\n5\n",
      "content_length": 1597,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 178,
      "content": "Chapter 5\n142\nFor many companies, procurement is a critical business activity. Eff ective pro-\ncurement of products at the right price for resale is obviously important to retail-\ners and distributors. Procurement also has strong bottom line implications for any \norganization that buys products as raw materials for manufacturing. Signiﬁ cant \ncost savings opportunities are associated with reducing the number of suppliers \nand negotiating agreements with preferred suppliers.\nDemand  planning drives effi  cient materials management. After demand is fore-\ncasted, procurement’s goal is to source the appropriate materials or products in \nthe most economical manner. Procurement involves a wide range of activities from \nnegotiating contracts to issuing purchase requisitions and purchase orders (POs) \nto tracking receipts and authorizing payments. The following list gives you a better \nsense of a procurement organization’s common analytic requirements:\n \n■Which materials or products are most frequently purchased? How many ven-\ndors supply these products? At what prices? Looking at demand across the \nenterprise (rather than at a single physical location), are there opportunities \nto negotiate favorable pricing by consolidating suppliers, single sourcing, or \nmaking guaranteed buys?\n \n■Are your employees purchasing from the preferred vendors or skirting the \nnegotiated vendor agreements with maverick spending?\n \n■Are you receiving the negotiated pricing from your vendors or is there vendor \ncontract purchase price variance?\n \n■How are your vendors performing? What is the vendor’s ﬁ ll rate? On-time \ndelivery performance? Late deliveries outstanding? Percent back ordered? \nRejection rate based on receipt inspection?\n Procurement Transactions and Bus Matrix\nAs  you begin working through the four-step dimensional design process, you deter-\nmine that procurement is the business process to be modeled. In studying the \nprocess, you observe a ﬂ urry of procurement transactions, such as purchase requisi-\ntions, purchase orders, shipping notiﬁ cations, receipts, and payments. Similar to the \napproach taken in Chapter 4: Inventory, you could initially design a fact table with \nthe grain of one row per procurement transaction with transaction date, product, \nvendor, contract terms, and procurement transaction type as key dimensions. The \nprocurement transaction quantity and dollar amount are the facts. The resulting \ndesign is shown in Figure 5-1.\n",
      "content_length": 2471,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 179,
      "content": "Procurement 143\nDate Dimension\nProcurement Transaction Fact\nProduct Dimension\nContract Terms Dimension\nProcurement Transaction Type Dimension\nVendor Dimension\nVendor Key (PK)\nVendor Name\nVendor Street Address\nVendor City\nVendor City-State\nVendor ZIP-Postal Code\nVendor State-Province\nVendor Country\nVendor Status\nVendor Minority Ownership Flag\nVendor Corporate Parent\n...\nProcurement Transaction Date Key (FK)\nProduct Key (FK)\nVendor Key (FK)\nContract Terms Key (FK)\nProcurement Transaction Type Key (FK)\nContract Number (DD)\nProcurement Transaction Quantity\nProcurement Transaction Dollar Amount\nContract Terms Key (PK)\nContract Terms Description\nContract Terms Type\nProcurement Transaction Type Key (PK)\nProcurement Transaction Type Description\nProcurement Transaction Type Category\nFigure 5-1: Procurement fact table with multiple transaction types.\nIf you work for the same grocery retailer from the earlier case studies, the trans-\naction date and product dimensions are the same conformed dimensions developed \noriginally in Chapter 3: Retail Sales. If you work with manufacturing procurement, \nthe raw materials products likely are located in a separate raw materials dimen-\nsion table rather than included in the product dimension for salable products. The \nvendor, contract terms, and procurement transaction type dimensions are new \nto this schema. The vendor dimension contains one row for each vendor, along \nwith interesting descriptive attributes to support a variety of vendor analyses. The \ncontract terms dimension contains one row for each generalized set of negotiated \nterms, similar to the promotion dimension in Chapter 3. The procurement trans-\naction type dimension enables grouping or ﬁ ltering on transaction types, such as \npurchase orders. The contract number is a degenerate dimension; it could be used \nto determine the volume of business conducted under each negotiated contract.\n Single Versus Multiple Transaction Fact Tables\nAs  you review the initial procurement schema design with business users, you learn \nseveral new details. First, the business users describe the various procurement trans-\nactions diff erently. To the business, purchase orders, shipping notices, warehouse \nreceipts, and vendor payments are all viewed as separate and unique processes.\nSeveral of the procurement transactions come from diff erent source systems. \nThere is a purchasing system that provides purchase requisitions and purchase \norders, a warehousing system that provides shipping notices and warehouse receipts, \nand an accounts payable system that deals with vendor payments.\n",
      "content_length": 2601,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 180,
      "content": "Chapter 5\n144\nYou further discover that several transaction types have diff erent dimensionality. \nFor example, discounts taken are applicable to vendor payments but not to the other \ntransaction types. Similarly, the name of the employee who received the goods at \nthe warehouse applies to receipts but doesn’t make sense elsewhere.\nThere are also a variety of interesting control numbers, such as purchase order \nand payment check numbers, created at various steps in the procurement pipeline. \nThese control numbers are perfect candidates for degenerate dimensions. For certain \ntransaction types, more than one control number may apply.\nAs you sort through these new details, you are faced with a design decision. \nShould you build a blended transaction fact table with a transaction type dimension \nto view all procurement transactions together, or do you build separate fact tables \nfor each transaction type? This is a common design quandary that surfaces in many \ntransactional situations, not just procurement.\nAs dimensional modelers, you need to make design decisions based on a thor-\nough understanding of the business requirements weighed against the realities of \nthe underlying source data. There is no simple formula to make the deﬁ nite deter-\nmination of whether to use a single fact table or multiple fact tables. A single fact \ntable may be the most appropriate solution in some situations, whereas multiple \nfact tables are most appropriate in others. When faced with this design decision, \nthe following considerations help sort out the options:\n \n■What are the users’ analytic requirements? The goal is to reduce complexity \nby presenting the data in the most eff ective form for business users. How will \nthe business users most commonly analyze this data? Which approach most \nnaturally aligns with their business-centric perspective? \n \n■Are there really multiple unique business processes? In the procurement \nexample, it seems buying products (purchase orders) is distinctly diff erent \nfrom receiving products (receipts). The existence of separate control num-\nbers for each step in the process is a clue that you are dealing with separate \nprocesses. Given this situation, you would lean toward separate fact tables. By \ncontrast, in Chapter 4’s inventory example, the varied inventory transactions \nwere part of a single inventory process resulting in a single fact table design.\n \n■Are multiple source systems capturing metrics with unique granularities? \nThere are three separate source systems in this case study: purchasing, ware-\nhousing, and accounts payable. This would suggest separate fact tables. \n \n■What is the dimensionality of the facts? In this procurement example, several \ndimensions are applicable to some transaction types but not to others. This \nwould again lead you to separate fact tables.\n",
      "content_length": 2844,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 181,
      "content": "Procurement 145\nA simple way to consider these trade-off s is to draft a bus matrix. As illustrated in \nFigure 5-2, you can include two additional columns identifying the atomic granular-\nity and metrics for each row. These matrix embellishments cause it to more closely \nresemble the detailed implementation bus matrix, which we’ll more thoroughly \ndiscuss in Chapter 16: Insurance.\nBusiness Processes\nAtomic Granularity\nMetrics\nPurchase Requisitions\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nDate\nProduct\nVendor\nContract Terms\nEmployee\nWarehouse\nCarrier\nPurchase Orders\nShipping Notifications\nWarehouse Receipts\nVendor Invoices\nVendor Payments\n1 row per requisition line\n1 row per PO line\n1 row per shipping notice line\n1 row per receipt line\n1 row per invoice line\n1 row per payment\nRequisition Quantity & Dollars\nPO Quantity & Dollars\nShipped Quantity\nReceived Quantity\nInvoice Quantity & Dollars\nInvoice, Discount & Net\nPayment Dollars\nFigure 5-2: Sample bus matrix rows for procurement processes.\nBased on the bus matrix for this hypothetical case study, multiple transaction \nfact tables would be implemented, as illustrated in Figure 5-3. In this example, there \nare separate fact tables for purchase requisitions, purchase orders, shipping notices, \nwarehouse receipts, and vendor payments. This decision was reached because users \nview these activities as separate and distinct business processes, the data comes \nfrom diff erent source systems, and there is unique dimensionality for the various \ntransaction types. Multiple fact tables enable richer, more descriptive dimensions \nand attributes. The single fact table approach would have required generalized \nlabeling for some dimensions. For example, purchase order date and receipt date \nwould likely have been generalized to simply transaction date. Likewise, purchasing \nagent and receiving clerk would become employee. This generalization reduces the \nlegibility of the resulting dimensional model. Also, with separate fact tables as you \nprogress from purchase requisitions to payments, the fact tables inherit dimensions \nfrom the previous steps.\nMultiple fact tables may require more time to manage and administer because \nthere are more tables to load, index, and aggregate. Some would argue this approach \nincreases the complexity of the ETL processes. Actually, it may simplify the ETL \nactivities. Loading the operational data from separate source systems into separate \nfact tables likely requires less complex ETL processing than attempting to integrate \ndata from the multiple sources into a single fact table.\n",
      "content_length": 2617,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 182,
      "content": "Chapter 5\n146\nDate Dimension\nVendor Dimension\nEmployee Dimension\nCarrier Dimension\nProduct Dimension\nContract Terms Dimension\nWarehouse Dimension\nPurchase Requisition Date Key (FK)\nProduct Key (FK)\nVendor Key (FK)\nContract Terms Key (FK)\nEmployee Requested By Key (FK)\nContract Number (DD)\nPurchase Requisition Number (DD)\nPurchase Requisition Quantity\nPurchase Requisition Dollar Amount\nPurchase Order Date Key (FK)\nRequested By Date Key (FK)\nProduct Key (FK)\nVendor Key (FK)\nContract Terms Key (FK)\nWarehouse Key (FK)\nCarrier Key (FK)\nEmployee Ordered By Key (FK)\nEmployee Purchase Agent Key (FK)\nContract Number (DD)\nPurchase Requisition Number (DD)\nPurchase Order Number (DD)\nPurchase Order Quantity\nPurchase Order Dollar Amount\nPurchase Order Fact\nPurchase Requisition Fact\nShipping Notification Date Key (FK)\nEstimated Arrival Date Key (FK)\nRequested By Date Key (FK)\nProduct Key (FK)\nVendor Key (FK)\nWarehouse Key (FK)\nCarrier Key (FK)\nEmployee Ordered By Key (FK)\nPurchase Order Number (DD)\nShipping Notification Number (DD)\nShipped Quantity\nShipping Notices Fact\nWarehouse Receipt Date Key (FK)\nRequested By Date Key (FK)\nProduct Key (FK)\nVendor Key (FK)\nWarehouse Key (FK)\nCarrier Key (FK)\nEmployee Ordered By Key (FK)\nEmployee Received By Key (FK)\nPurchase Order Number (DD)\nShipping Notification Number (DD)\nWarehouse Receipt Number (DD)\nReceived Quantity\nWarehouse Receipts Fact\nVendor Payment Date Key (FK)\nProduct Key (FK)\nVendor Key (FK)\nWarehouse Key (FK)\nContract Terms Key (FK)\nContract Number (DD)\nPayment Check Number (DD)\nVendor Invoice Dollar Amount\nVendor Discount Dollar Amount\nVendor Net Payment Dollar Amount\nVendor Payment Fact\nFigure 5-3: Multiple fact tables for procurement processes.\n",
      "content_length": 1716,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 183,
      "content": "Procurement 147\n Complementary Procurement Snapshot\nApart  from the decision regarding multiple procurement transaction fact tables, \nyou may also need to develop a snapshot fact table to fully address the business’s \nneeds. As suggested in Chapter 4, an accumulating snapshot such as Figure 5-4 that \ncrosses processes would be extremely useful if the business is interested in monitor-\ning product movement as it proceeds through the procurement pipeline (including \nthe duration of each stage). Remember that an accumulating snapshot is meant to \nmodel processes with well-deﬁ ned milestones. If the process is a continuous ﬂ ow \nthat never really ends, it is not a good candidate for an accumulating snapshot.\nPurchase Order Date Key (FK)\nRequested By Date Key (FK)\nWarehouse Receipt Date Key (FK)\nVendor Invoice Date Key (FK)\nVendor Payment Date Key (FK)\nProduct Key (FK)\nVendor Key (FK)\nContract Terms Key (FK)\nEmployee Ordered By Key (FK)\nWarehouse Key (FK)\nCarrier Key (FK)\nContract Number (DD)\nPurchase Order Number (DD)\nWarehouse Receipt Number (DD)\nVendor Invoice Number (DD)\nPayment Check Number (DD)\nPurchase Order Quantity\nPurchase Order Dollar Amount\nShipped Quantity\nReceived Quantity\nVendor Invoice Dollar Amount\nVendor Discount Dollar Amount\nVendor Net Payment Dollar Amount\nPO to Requested By Date Lag\nPO to Receipt Date Lag\nRequested By to Receipt Date Lag\nReceipt to Payment Date Lag\nInvoice to Payment Date Lag\nProcurement Pipeline Fact\nRequested By Date Dimension\nVendor Invoice Date Dimension\nProduct Dimension\nContract Terms Dimension\nWarehouse Dimension\nPurchase Order Date Dimension\nWarehouse Receipt Date Dimension\nVendor Payment Date Dimension\nVendor Dimension\nEmployee Dimension\nCarrier Dimension\nFigure 5-4: Procurement pipeline accumulating snapshot schema.\nSlowly Changing Dimension Basics\nTo  this point, we have pretended dimensions are independent of time. Unfortunately, \nthis is not the case in the real world. Although dimension table attributes are relatively \nstatic, they aren’t ﬁ xed forever; attribute values change, albeit rather slowly, over time. \n",
      "content_length": 2095,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 184,
      "content": "Chapter 5\n148\nDimensional designers must proactively work with the business’s data governance \nrepresentatives to determine the appropriate change-handling strategy. You shouldn’t \nsimply jump to the conclusion that the business doesn’t care about dimension changes \njust because they weren’t mentioned during the requirements gathering. Although \nIT may assume accurate change tracking is unnecessary, business users may assume \nthe DW/BI system will allow them to see the impact of every attribute value change. \nIt is obviously better to get on the same page sooner rather than later.\nNOTE \nThe business’s data governance and stewardship representatives must be \nactively involved in decisions regarding the handling of slowly changing dimension \nattributes; IT shouldn’t make determinations on its own.\nWhen  change tracking is needed, it might be tempting to put every changing \nattribute into the fact table on the assumption that dimension tables are static. This \nis unacceptable and unrealistic. Instead you need strategies to deal with slowly \nchanging attributes within dimension tables. Since Ralph Kimball ﬁ rst introduced \nthe notion of slowly changing dimensions in 1995, some IT professionals in a never-\nending quest to speak in acronym-ese termed them SCDs. The acronym stuck.\nFor each dimension table attribute, you must specify a strategy to handle change. \nIn other words, when an attribute value changes in the operational world, how will \nyou respond to the change in the dimensional model? In the following sections, we \ndescribe several basic techniques for dealing with attribute changes, followed by \nmore advanced options. You may need to employ a combination of these techniques \nwithin a single dimension table.\nKimball  method followers are likely already familiar with SCD types 1, 2, and 3. \nBecause legibility is part of our mantra, we sometimes wish we had given these tech-\nniques more descriptive names in the ﬁ rst place, such as “overwrite.” But after nearly \ntwo decades, the “type numbers” are squarely part of the DW/BI vernacular. As you’ll \nsee in the following sections, we’ve decided to expand the theme by assigning new \nSCD type numbers to techniques that have been described, but less precisely labeled, \nin the past; our hope is that assigning speciﬁ c numbers facilitates clearer communica-\ntion among team members.\nType 0: Retain Original\nThis  technique hasn’t been given a type number in the past, but it’s been around \nsince the beginning of SCDs. With type 0, the dimension attribute value never \nchanges, so facts are always grouped by this original value. Type 0 is appropriate \nfor any attribute labeled “original,” such as customer original credit score. It also \napplies to most attributes in a date dimension.\n",
      "content_length": 2772,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 185,
      "content": "Procurement 149\nAs we staunchly advocated in Chapter 3, the dimension table’s primary key is \na surrogate key rather than relying on the natural operational key. Although we \ndemoted the natural key to being an ordinary dimension attribute, it still has special \nsigniﬁ cance. Presuming it’s durable, it would remain inviolate. Persistent durable \nkeys are always type 0 attributes. Unless otherwise noted, throughout this chapter’s \nSCD discussion, the durable supernatural key is assumed to remain constant, as \ndescribed in Chapter 3.\n Type 1: Overwrite\nWith  the slowly changing dimension type 1 response, you overwrite the old attri-\nbute value in the dimension row, replacing it with the current value; the attribute \nalways reﬂ ects the most recent assignment.\nAssume you work for an electronics retailer where products roll up into the retail \nstore’s departments. One of the products is IntelliKidz software. The existing row in \nthe product dimension table for IntelliKidz looks like the top half of Figure 5-5. Of \ncourse, there would be additional descriptive attributes in the product dimension, \nbut we’ve abbreviated the attribute listing for clarity. \nIntelliKidz\nEducation\n12345 ABC922-Z\nProduct\nKey\nSKU (NK)\nProduct\nDescription\nDepartment\nName\nIntelliKidz\nStrategy\n12345 ABC922-Z\nUpdated row in Product dimension:\nOriginal row in Product dimension:\nProduct\nKey\nSKU (NK)\nProduct\nDescription\nDepartment\nName\nFigure 5-5: SCD type 1 sample rows.\nSuppose a new merchandising person decides IntelliKidz software should be \nmoved from the Education department to the Strategy department on February 1, \n2013 to boost sales. With a type 1 response, you’d simply update the existing row \nin the dimension table with the new department description, as illustrated in the \nupdated row of Figure 5-5.\nIn this case, no dimension or fact table keys were modiﬁ ed when IntelliKidz’s \ndepartment changed. The fact table rows still reference product key 12345, regardless \nof IntelliKidz’s departmental location. When sales take off  following the move to the \nStrategy department, you have no information to explain the performance improve-\nment because the historical and more recent facts both appear as if IntelliKidz \nalways rolled up into Strategy.\n",
      "content_length": 2256,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 186,
      "content": "Chapter 5\n150\nThe type 1 response is the simplest approach for dimension attribute changes. In \nthe dimension table, you merely overwrite the preexisting value with the current \nassignment. The fact table is untouched. The problem with a type 1 response is that \nyou lose all history of attribute changes. Because overwriting obliterates historical \nattribute values, you’re left solely with the attribute values as they exist today. A type \n1 response is appropriate if the attribute change is an insigniﬁ cant correction. It also \nmay be appropriate if there is no value in keeping the old description. However, too \noften DW/BI teams use a type 1 response as the default for dealing with slowly chang-\ning dimensions and end up totally missing the mark if the business needs to track \nhistorical changes accurately. After you implement a type 1, it’s diffi  cult to change \ncourse in the future.\nNOTE \nThe type 1 response is easy to implement, but it does not maintain any \nhistory of prior attribute values.\nBefore we leave the topic of type 1 changes, be forewarned that the same BI \napplications can produce diff erent results before versus after the type 1 attribute \nchange. When the dimension attribute’s type 1 overwrite occurs, the fact rows are \nassociated with the new descriptive context. Business users who rolled up sales by \ndepartment on January 31 will get diff erent department totals when they run the \nsame report on February 1 following the type 1 overwrite.\nThere’s another easily overlooked catch to be aware of. With a type 1 response \nto deal with the relocation of IntelliKidz, any preexisting aggregations based on the \ndepartment value need to be rebuilt. The aggregated summary data must continue \nto tie to the detailed atomic data, where it now appears that IntelliKidz has always \nrolled up into the Strategy department.\nFinally, if a dimensional model is deployed via an OLAP cube and the type 1 \nattribute is a hierarchical rollup attribute, like the product’s department in our \nexample, the cube likely needs to be reprocessed when the type 1 attribute changes. \nAt a minimum, similar to the relational environment, the cube’s performance aggre-\ngations need to be recalculated. \nWARNING \nEven though type 1 changes appear the easiest to implement, \nremember they invalidate relational tables and OLAP cubes that have aggregated \ndata over the aff ected attribute.\n Type 2: Add New Row\nIn  Chapter 1: Data Warehousing, Business Intelligence, and Dimensional Modeling \nPrimer, we stated one of the DW/BI system’s goals was to correctly represent history. \n",
      "content_length": 2593,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 187,
      "content": "Procurement 151\nA type 2 response is the predominant technique for supporting this requirement \nwhen it comes to slowly changing dimension attributes.\nUsing the type 2 approach, when IntelliKidz’s department changed on February \n1, 2013, a new product dimension row for IntelliKidz is inserted to reﬂ ect the new \ndepartment attribute value. There are two product dimension rows for IntelliKidz, \nas illustrated in Figure 5-6. Each row contains a version of IntelliKidz’s attribute \nproﬁ le that was true for a span of time.\nOriginal row in Product dimension:\nProduct\nKey\nSKU (NK)\nProduct\nDescription\nDepartment\nName\n…\nRow\nEffective \nDate\nRow\nExpiration \nDate\nCurrent Row\nIndicator\n12345 ABC922-Z\nIntelliKidz\nEducation\n…\n2012-01-01\n9999-12-31\nCurrent\nRows in Product dimension following department reassignment:\nProduct\nKey\nSKU (NK)\nProduct\nDescription\nDepartment\nName\n...\nRow\nEffective \nDate\nRow\nExpiration\nDate\nCurrent Row\nIndicator\n12345\n25984\nABC922-Z\nABC922-Z\nIntelliKidz\nIntelliKidz\nEducation\nStrategy\n... \n...\n2012-01-01\n2013-02-01\n2013-01-31\n9999-12-31\nExpired \nCurrent\nFigure 5-6: SCD type 2 sample rows.\nWith type 2 changes, the fact table is again untouched; you don’t go back to \nthe historical fact table rows to modify the product key. In the fact table, rows for \nIntelliKidz prior to February 1, 2013, would reference product key 12345 when the \nproduct rolled up to the Education department. After February 1, new IntelliKidz \nfact rows would have product key 25984 to reﬂ ect the move to the Strategy depart-\nment. This is why we say type 2 responses perfectly partition or segment history to \naccount for the change. Reports summarizing pre-February 1 facts look identical \nwhether the report is generated before or after the type 2 change. \nWe want to reinforce that reported results may diff er depending on whether \nattribute changes are handled as a type 1 or type 2. Let’s presume the electronic \nretailer sells $500 of IntelliKidz software during January 2013, followed by a $100 \nsale in February 2013. If the department attribute is a type 1, the results from a \nquery reporting January and February sales would indicate $600 under Strategy. \nConversely, if the department name attribute is a type 2, the sales would be reported \nas $500 for the Education department and $100 for the Strategy department.\nUnlike the type 1 approach, there is no need to revisit preexisting aggregation \ntables when using the type 2 technique. Likewise, OLAP cubes do not need to be \nreprocessed if hierarchical attributes are handled as type 2.\nIf you constrain on the department attribute, the two product proﬁ les are diff er-\nentiated. If you constrain on the product description, the query automatically fetches \nboth IntelliKidz product dimension rows and automatically joins to the fact table for \n",
      "content_length": 2814,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 188,
      "content": "Chapter 5\n152\nthe complete product history. If you need to count the number of products correctly, \nthen you would just use the SKU natural key attribute as the basis of the distinct \ncount rather than the surrogate key; the natural key column becomes the glue that \nholds the separate type 2 rows for a single product together. \nNOTE \nThe type 2 response is the primary workhorse technique for accurately \ntracking slowly changing dimension attributes. It is extremely powerful because \nthe new dimension row automatically partitions history in the fact table.\nType 2 is the safest response if the business is not absolutely certain about the \nSCD business rules for an attribute. As we’ll discuss in the “Type 6: Add Type 1 \nAttributes to Type 2 Dimension” and “Type 7: Dual Type 1 and Type 2 Dimensions” \nsections later in the chapter, you can provide the illusion of a type 1 overwrite when \nan attribute has been handled with the type 2 response. The converse is not true. If \nyou treat an attribute as type 1, reverting to type 2 retroactively requires signiﬁ cant \neff ort to create new dimension rows and then appropriately rekey the fact table.\nType 2 Effective and Expiration Dates\nWhen  a dimension table includes type 2 attributes, you should include several \nadministrative columns on each row, as shown in Figure 5-6. The eff ective and \nexpiration dates refer to the moment when the row’s attribute values become valid \nor invalid. Eff ective and expiration dates or date/time stamps are necessary in the \nETL system because it needs to know which surrogate key is valid when loading \nhistorical fact rows. The eff ective and expiration dates support precise time slic-\ning of the dimension; however, there is no need to constrain on these dates in the \ndimension table to get the right answer from the fact table. The row eff ective date \nis the ﬁ rst date the descriptive proﬁ le is valid. When a new product is ﬁ rst loaded \nin the dimension table, the expiration date is set to December 31, 9999. By avoiding \na null in the expiration date, you can reliably use a BETWEEN command to ﬁ nd the \ndimension rows that were in eff ect on a certain date. \nWhen a new proﬁ le row is added to the dimension to capture a type 2 attribute \nchange, the previous row is expired. We typically suggest the end date on the \nold row should be just prior to the eff ective date of the new row leaving no gaps \nbetween these eff ective and expiration dates. The deﬁ nition of “just prior” depends \non the grain of the changes being tracked. Typically, the eff ective and expiration \ndates represent changes that occur during a day; if you’re tracking more granular \nchanges, you’d use a date/time stamp instead. In this case, you may elect to apply \ndiff erent business rules, such as setting the row expiration date exactly equal to the \n",
      "content_length": 2839,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 189,
      "content": "Procurement 153\neff ective date of the next row. This would require logic such as “>= eff ective date \nand < expiration date” constraints, invalidating the use of BETWEEN.\nSome argue that a single eff ective date is adequate, but this makes for more \ncomplicated searches to locate the dimension row with the latest eff ective date \nthat is less than or equal to a date ﬁ lter. Storing an explicit second date simpli-\nﬁ es the query processing. Likewise, a current row indicator is another useful \nadministrative dimension attribute to quickly constrain queries to only the cur-\nrent proﬁ les.\nThe type 2 response to slowly changing dimensions requires the use of surrogate \nkeys, but you’re already using them anyhow, right? You certainly can’t use the opera-\ntional natural key because there are multiple proﬁ le versions for the same natural key. \nIt is not suffi  cient to use the natural key with two or three version digits because you’d \nbe vulnerable to the entire list of potential operational issues discussed in Chapter 3. \nLikewise, it is inadvisable to append an eff ective date to the otherwise primary key \nof the dimension table to uniquely identify each version. With the type 2 response, \nyou create a new dimension row with a new single-column primary key to uniquely \nidentify the new product proﬁ le. This single-column primary key establishes the link-\nage between the fact and dimension tables for a given set of product characteristics. \nThere’s no need to create a confusing secondary join based on the dimension row’s \neff ective or expiration dates.\nWe recognize some of you may be concerned about the administration of surro-\ngate keys to support type 2 changes. In Chapter 19: ETL Subsystems and Techniques \nand Chapter 20: ETL System Design and Development Process and Tasks, we’ll dis-\ncuss a workﬂ ow for managing surrogate keys and accommodating type 2 changes \nin more detail. \nType 1 Attributes in Type 2 Dimensions\nIt  is not uncommon to mix multiple slowly changing dimension techniques within \nthe same dimension. When type 1 and type 2 are both used in a dimension, some-\ntimes a type 1 attribute change necessitates updating multiple dimension rows. Let’s \npresume the dimension table includes a product introduction date. If this attribute \nis corrected using type 1 logic after a type 2 change to another attribute occurs, \nthe introduction date should probably be updated on both versions of IntelliKidz’s \nproﬁ le, as illustrated in Figure 5-7.\nThe data stewards need to be involved in deﬁ ning the ETL business rules in \nscenarios like this. Although the DW/BI team can facilitate discussion regarding \nproper update handling, the business’s data stewards should make the ﬁ nal deter-\nmination, not the DW/BI  team.\n",
      "content_length": 2761,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 190,
      "content": "Chapter 5\n154\nOriginal row in Product dimension:\nProduct\nKey\nSKU (NK)\nProduct\nDescription\nDepartment\nName\nIntroduction\nDate\nIntroduction\nDate\n…\nRow\nEffective \nDate\nRow\nExpiration \nDate\nCurrent Row\nIndicator\n12345 ABC922-Z\nIntelliKidz\nEducation\n2012-12-15\n…\n2012-01-01\n9999-12-31\nCurrent\nRows in Product dimension following type 2 change to Department Name and type 1 change to Introduction Date:\nProduct\nKey\nSKU (NK)\nProduct\nDescription\nDepartment\nName\n...\nRow\nEffective \nDate\nRow\nExpiration \nDate\nCurrent Row\nIndicator\n12345\n25984\nABC922-Z\nABC922-Z\nIntelliKidz\nIntelliKidz\nEducation\nStrategy\n2012-01-01 \n2012-01-01\n... \n...\n2012-01-01\n2013-02-01\n2013-01-31\n9999-12-31\nExpired \nCurrent\nFigure 5-7: Type 1 updates in a dimension with type 2 attributes sample rows.\n Type 3: Add New Attribute\nAlthough  the type 2 response partitions history, it does not enable you to associ-\nate the new attribute value with old fact history or vice versa. With the type 2 \nresponse, when you constrain the department attribute to Strategy, you see only \nIntelliKidz facts from after February 1, 2013. In most cases, this is exactly what \nyou want.\nHowever, sometimes you want to see fact data as if the change never occurred. \nThis happens most frequently with sales force reorganizations. District boundaries \nmay be redrawn, but some users still want the ability to roll up recent sales for the \nprior districts just to see how they would have done under the old organizational \nstructure. For a few transitional months, there may be a need to track history for \nthe new districts and conversely to track new fact data in terms of old district \nboundaries. A type 2 response won’t support this requirement, but type 3 comes \nto the rescue.\nIn our software example, let’s assume there is a legitimate business need to \ntrack both the new and prior values of the department attribute for a period of \ntime around the February 1 change. With a type 3 response, you do not issue a \nnew dimension row, but rather add a new column to capture the attribute change, \nas illustrated in Figure 5-8. You would alter the product dimension table to add \na prior department attribute, and populate this new column with the existing \ndepartment value (Education). The original department attribute is treated as a \ntype 1 where you overwrite to reﬂ ect the current value (Strategy). All existing \nreports and queries immediately switch over to the new department description, \nbut you can still report on the old department value by querying on the prior \ndepartment attribute.\n",
      "content_length": 2549,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 191,
      "content": "Procurement 155\nIntelliKidz\nEducation\n12345 ABC922-Z\nProduct\nKey\nSKU (NK)\nProduct\nDescription\nDepartment\nName\nIntelliKidz\nStrategy\nEducation\n12345 ABC922-Z\nUpdated row in Product dimension:\nOriginal row in Product dimension:\nProduct\nKey\nSKU (NK)\nProduct\nDescription\nDepartment\nName\nPrior\nDepartment\nName\nFigure 5-8: SCD type 3 sample rows.\nDon’t be fooled into thinking the higher type number associated with type 3 \nindicates it is the preferred approach; the techniques have not been presented in \ngood, better, and best practice sequence. Frankly, type 3 is infrequently used. It is \nappropriate when there’s a strong need to support two views of the world simulta-\nneously. Type 3 is distinguished from type 2 because the pair of current and prior \nattribute values are regarded as true at the same time. \nNOTE \nThe type 3 slowly changing dimension technique enables you to see \nnew and historical fact data by either the new or prior attribute values, sometimes \ncalled alternate realities.\nType 3 is not useful for attributes that change unpredictably, such as a customer’s \nhome state. There would be no beneﬁ t in reporting facts based on a prior home state \nattribute that reﬂ ects a change from 10 days ago for some customers or 10 years \nago for others. These unpredictable changes are typically handled best with type \n2 instead.\nType 3 is most appropriate when there’s a signiﬁ cant change impacting many \nrows in the dimension table, such as a product line or sales force reorganization. \nThese en masse changes are prime candidates because business users often want \nthe ability to analyze performance metrics using either the pre- or post-hierarchy \nreorganization for a period of time. With type 3 changes, the prior column is labeled \nto distinctly represent the prechanged grouping, such as 2012 department or pre-\nmerger department. These column names provide clarity, but there may be unwanted \nripples in the BI layer.\nFinally, if the type 3 attribute represents a hierarchical rollup level within the \ndimension, then as discussed with type 1, the type 3 update and additional column \nwould likely cause OLAP cubes to be reprocessed.\n",
      "content_length": 2157,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 192,
      "content": "Chapter 5\n156\nMultiple Type 3 Attributes\nIf a dimension attribute changes with a predictable rhythm, sometimes the business \nwants to summarize performance metrics based on any of the historic attribute \nvalues. Imagine the product line is recategorized at the start of every year and the \nbusiness wants to look at multiple years of historic facts based on the department \nassignment for the current year or any prior year.\nIn this case, we take advantage of the regular, predictable nature of these changes \nby generalizing the type 3 approach to a series of type 3 dimension attributes, as \nillustrated in Figure 5-9. On every dimension row, there is a current department \nattribute that is overwritten, plus attributes for each annual designation, such as \n2012 department. Business users can roll up the facts with any of the department \nassignments. If a product were introduced in 2013, the department attributes for \n2012 and 2011 would contain Not Applicable values.\nIntelliKidz\nStrategy\nEducation\nNot Applicable\n12345 ABC922-Z\nUpdated row in Product dimension:\nProduct\nKey\nSKU (NK)\nProduct\nDescription\nCurrent\nDepartment\nName\n2012\nDepartment\nName\n2011\nDepartment\nName\nFigure 5-9: Dimension table with multiple SCD type 3 attributes.\nThe most recent assignment column should be identiﬁ ed as the current depart-\nment. This attribute will be used most frequently; you don’t want to modify existing \nqueries and reports to accommodate next year’s change. When the departments are \nreassigned in January 2014, you’d alter the table to add a 2013 department attribute, \npopulate this column with the current department values, and then overwrite the \ncurrent attribute with the 2014 department  assignment. \n Type 4: Add Mini-Dimension\nThus  far we’ve focused on slow evolutionary changes to dimension tables. What \nhappens when the rate of change speeds up, especially within a large multimillion-\nrow dimension table? Large dimensions present two challenges that warrant special \ntreatment. The size of these dimensions can negatively impact browsing and query \nﬁ ltering performance. Plus our tried-and-true type 2 technique for change tracking \nis unappealing because we don’t want to add more rows to a dimension that already \nhas millions of rows, particularly if changes happen frequently.\nFortunately, a single technique comes to the rescue to address both the browsing \nperformance and change tracking challenges. The solution is to break off  frequently \nanalyzed or frequently changing attributes into a separate dimension, referred to \nas a mini-dimension. For example, you could create a mini-dimension for a group \n",
      "content_length": 2634,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 193,
      "content": "Procurement 157\nof more volatile customer demographic attributes, such as age, purchase frequency \nscore, and income level, presuming these columns are used extensively and changes \nto these attributes are important to the business. There would be one row in the \nmini-dimension for each unique combination of age, purchase frequency score, \nand income level encountered in the data, not one row per customer. With this \napproach, the mini-dimension becomes a set of demographic proﬁ les. Although the \nnumber of rows in the customer dimension may be in the millions, the number of \nmini-dimension rows should be a signiﬁ cantly smaller. You leave behind the more \nconstant attributes in the original multimillion-row customer table.\nSample rows for a demographic mini-dimension are illustrated in Figure 5-10. \nWhen creating the mini-dimension, continuously variable attributes, such as income, \nare converted to banded ranges. In other words, the attributes in the mini-dimension \nare typically forced to take on a relatively small number of discrete values. Although \nthis restricts use to a set of predeﬁ ned bands, it drastically reduces the number of \ncombinations in the mini-dimension. If you stored income at a speciﬁ c dollar and \ncents value in the mini-dimension, when combined with the other demographic \nattributes, you could end up with as many rows in the mini-dimension as in the \ncustomer dimension itself. The use of band ranges is probably the most signiﬁ cant \ncompromise associated with the mini-dimension technique. Although grouping \nfacts from multiple band values is viable, changing to more discreet bands (such \nas $30,000-34,999) at a later time is diffi  cult. If users insist on access to a speciﬁ c \nraw data value, such as a credit bureau score that is updated monthly, it should be \nincluded in the fact table, in addition to being value banded in the demographic \nmini-dimension. In Chapter 10: Financial Services, we’ll discuss dynamic value \nbanding of facts; however, such queries are much less effi  cient than constraining \nthe value band in a mini-dimension table.\nLow\nMedium\nHigh\nLow\nMedium\nHigh\n... \nLow\nMedium\nHigh \n...\n<$30,000\n<$30,000\n<$30,000\n$30,000-39,999\n$30,000-39,999\n$30,000-39,999 \n... \n<$30,000\n<$30,000\n<$30,000\n...\n1 \n2 \n3 \n4 \n5 \n6 \n... \n142 \n143 \n144 \n...\n21-25 \n21-25 \n21-25 \n21-25 \n21-25 \n21-25 \n... \n26-30 \n26-30 \n26-30 \n...\nDemographics\nKey\nAge Band\nIncome Level\nPurchase\nFrequency\nScore\nFigure 5-10: SCD type 4 mini-dimension sample rows.\n",
      "content_length": 2503,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 194,
      "content": "Chapter 5\n158\nEvery  time a fact table row is built, two foreign keys related to the customer would \nbe included: the customer dimension key and the mini-dimension demographics \nkey in eff ect at the time of the event, as shown in Figure 5-11. The mini-dimension \ndelivers performance beneﬁ ts by providing a smaller point of entry to the facts. \nQueries can avoid the huge customer dimension table unless attributes from that \ntable are constrained or used as report labels.\nCustomer Dimension\nCustomer Key (PK)\nCustomer ID (NK)\nCustomer Name\nCustomer Address\nCustomer City-State\nCustomer State\nCustomer ZIP-Postal Code\nCustomer Date of Birth\nDemographics Dimension\nDemographics Key (PK)\nAge Band\nPurchase Frequency Score\nIncome Level\nFact Table\nDate Key (FK)\nCustomer Key (FK)\nDemographics Key (FK)\nMore FKs...\nFacts...\nFigure 5-11: Type 4 mini-dimension with customer dimension.\nWhen  the mini-dimension key participates as a foreign key in the fact table, another \nbeneﬁ t is that the fact table captures the demographic proﬁ le changes. Let’s presume \nwe are loading data into a periodic snapshot fact table on a monthly basis. Referring \nback to our sample demographic mini-dimension sample rows in Figure 5-10, if one \nof our customers, John Smith, were 25 years old with a low purchase frequency score \nand an income of $25,000, you’d begin by assigning demographics key 1 when loading \nthe fact table. If John has a birthday several weeks later and turns 26 years old, you’d \nassign demographics key 142 when the fact table was next loaded; the demographics \nkey on John’s earlier fact table rows would not be changed. In this manner, the fact \ntable tracks the age change. You’d continue to assign demographics key 142 when \nthe fact table is loaded until there’s another change in John’s demographic proﬁ le. If \nJohn receives a raise to $32,000 several months later, a new demographics key would \nbe reﬂ ected in the next fact table load. Again, the earlier rows would be unchanged. \nOLAP cubes also readily accommodate type 4 mini-dimensions.\nCustomer  dimensions are somewhat unique in that customer attributes frequently \nare queried independently from the fact table. For example, users may want to know \nhow many customers live in Dade County by age bracket for segmentation and proﬁ l-\ning. Rather than forcing any analysis that combines customer and demographic data \nto link through the fact table, the most recent value of the demographics key also \ncan exist as a foreign key on the customer dimension table. We’ll further describe \nthis customer demographic outrigger as an SCD type 5 in the next section.\n",
      "content_length": 2629,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 195,
      "content": "Procurement 159\nThe  demographic dimension cannot be allowed to grow too large. If you have \nﬁ ve demographic attributes, each with 10 possible values, then the demographics \ndimension could have 100,000 (105) rows. This is a reasonable upper limit for the \nnumber of rows in a mini-dimension if you build out all the possible combina-\ntions in advance. An alternate ETL approach is to build only the mini-dimension \nrows that actually occur in the data. However, there are certainly cases where even \nthis approach doesn’t help and you need to support more than ﬁ ve demographic \nattributes with 10 values each. We’ll discuss the use of multiple mini-dimensions \nassociated with a single fact table in Chapter 10.\nDemographic proﬁ le changes sometimes occur outside a business event, such \nas when a customer’s proﬁ le is updated in the absence of a sales transaction. If the \nbusiness requires accurate point-in-time proﬁ ling, a supplemental factless fact table \nwith eff ective and expiration dates can capture every relationship change between \nthe customer and demographics dimensions.\nHybrid Slowly Changing Dimension \nTechniques\nIn  this ﬁ nal section, we’ll discuss hybrid approaches that combine the basic SCD \ntechniques. Designers sometimes become enamored with these hybrids because they \nseem to provide the best of all worlds. However, the price paid for greater analytic \nﬂ exibility is often greater complexity. Although IT professionals may be impressed \nby elegant ﬂ exibility, business users may be just as easily turned off  by complexity. \nYou should not pursue these options unless the business agrees they are needed to \naddress their requirements.\nThese ﬁ nal approaches are most relevant if you’ve been asked to preserve the \nhistorically accurate dimension attribute associated with a fact event, while sup-\nporting the option to report historical facts according to the current attribute values. \nThe basic slowly changing dimension techniques do not enable this requirement \neasily on their own.\nWe’ll start by considering a technique that combines type 4 with a type 1 outrig-\nger; because 4 + 1 = 5, we’re calling this type 5. Next, we’ll describe type 6, which \ncombines types 1 through 3 for a single dimension attribute; it’s aptly named type \n6 because 2 + 3 + 1 or 2 × 3 × 1 both equal 6. Finally, we’ll ﬁ nish up with type 7, \nwhich just happens to be the next available sequence number; there is no underly-\ning mathematical signiﬁ cance to this  label.\n",
      "content_length": 2492,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 196,
      "content": "Chapter 5\n160\n Type 5: Mini-Dimension and Type 1 Outrigger\nLet’s return to the type 4 mini-dimension. An embellishment to this technique is to \nadd a current mini-dimension key as an attribute in the primary dimension. This \nmini-dimension key reference is a type 1 attribute, overwritten with every proﬁ le \nchange. You wouldn’t want to track this attribute as a type 2 because then you’d be \ncapturing volatile changes within the large multimillion-row dimension and avoid-\ning this explosive growth was one of the original motivations for type 4.\nThe type 5 technique is useful if you want a current proﬁ le count in the absence \nof fact table metrics or want to roll up historical facts based on the customer’s cur-\nrent proﬁ le. You’d logically represent the primary dimension and mini-dimension \noutrigger as a single table in the presentation area, as shown in Figure 5-12. To \nminimize user confusion and potential error, the current attributes in this role-\nplaying dimension should have distinct column names distinguishing them, such \nas current age band. Even with unique labeling, be aware that presenting users with \ntwo avenues for accessing demographic data, through either the mini-dimension \nor outrigger, can deliver more functionality and complexity than some can  handle.\nCurrent Demographics Dimension\nCurrent Demographics Key (PK)\nCurrent Age Band\nCurrent Purchase Frequency Score\nCurrent Income Level\nLogical representation to the BI tools:\nView of Demographics Dimension\nFact Table\nDate Key (FK)\nCustomer Key (FK)\nDemographics Key (FK)\nMore FKs...\nFacts\nCustomer Dimension\nCustomer Key (PK)\nCustomer ID (NK)\nCustomer Name\n...\nCurrent Demographics Key (FK)\nDemographics Dimension\nDemographics Key (PK)\nAge Band\nPurchase Frequency Score\nIncome Level\nFact Table\nDate Key (FK)\nCustomer Key (FK)\nDemographics Key (FK)\nMore FKs...\nFacts\nCustomer Dimension\nCustomer Key (PK)\nCustomer ID (NK)\nCustomer Name\n...\nCurrent Age Band\nCurrent Purchase Frequency Score\nCurrent Income Level\nDemographics Dimension\nDemographics Key (PK)\nAge Band\nPurchase Frequency Score\nIncome Level\nFigure 5-12: Type 4 mini-dimension with type 1 outrigger in customer dimension.\nNOTE \nThe type 4 mini-dimension terminology refers to when the demograph-\nics key is part of the fact table composite key. If the demographics key is a foreign \nkey in the customer dimension, it is referred to as an outrigger.\n Type 6: Add Type 1 Attributes to Type 2 Dimension\nLet’s  return to the electronics retailer’s product dimension. With type 6, you would \nhave two department attributes on each row. The current department column \n",
      "content_length": 2611,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 197,
      "content": "Procurement 161\nrepresents the current assignment; the historic department column is a type 2 \nattribute representing the historically accurate department value. \nWhen IntelliKidz software is introduced, the product dimension row would look \nlike the ﬁ rst scenario in Figure 5-13.\nOriginal row in Product dimension:\nProduct\nKey\nSKU (NK)\nProduct\nDescription\nHistoric\nDepartment\nName\nCurrent\nDepartment\nName\n…\nRow\nEffective \nDate\nRow\nExpiration \nDate\nCurrent Row\nIndicator\n12345 ABC922-Z\nIntelliKidz\nEducation\nEducation\n…\n2012-01-01\n9999-12-31\nCurrent\nRows in Product dimension following first department reassignment:\nProduct\nKey\nSKU (NK)\nProduct\nDescription\nHistoric\nDepartment\nName\nCurrent\nDepartment\nName\n...\nRow\nEffective \nDate\nRow\nExpiration \nDate\nCurrent Row\nIndicator\n12345\n25984\nABC922-Z\nABC922-Z\nIntelliKidz\nIntelliKidz\nEducation\nStrategy\nStrategy \nStrategy\n... \n...\n2012-01-01\n2013-02-01\n2013-01-31\n9999-12-31\nExpired \nCurrent\nRows in Product dimension following second department reassignment:\nProduct\nKey\nSKU (NK)\nProduct\nDescription\nHistoric\nDepartment\nName\nCurrent\nDepartment\nName\n...\nRow\nEffective \nDate\nRow\nExpiration \nDate\nCurrent Row\nIndicator\n12345\n25984\n31726\nABC922-Z\nABC922-Z\nABC922-Z\nIntelliKidz\nIntelliKidz\nIntelliKidz\nEducation\nStrategy\nCritical Thinking\nCritical Thinking\nCritical Thinking\nCritical Thinking\n... \n...\n...\n2012-01-01\n2013-02-01\n2013-07-01\n2013-01-31\n2013-06-30\n9999-12-31\nExpired\nExpired \nCurrent\nFigure 5-13: SCD type 6 sample rows.\nWhen the departments are restructured and IntelliKidz is moved to the Strategy \ndepartment, you’d use a type 2 response to capture the attribute change by issu-\ning a new row. In this new IntelliKidz dimension row, the current department will \nbe identical to the historical department. For all previous instances of IntelliKidz \ndimension rows, the current department attribute will be overwritten to reﬂ ect the \ncurrent structure. Both IntelliKidz rows would identify the Strategy department as \nthe current department (refer to the second scenario in Figure 5-13).\nIn this manner you can use the historic attribute to group facts based on the attribute \nvalue that was in eff ect when the facts occurred. Meanwhile, the current attri-\nbute rolls up all the historical fact data for both product keys 12345 and 25984 into \nthe current department assignment. If IntelliKidz were then moved into the Critical \nThinking software department, the product table would look like Figure 5-13’s ﬁ nal \nset of rows. The current column groups all facts by the current assignment, while \nthe historic column preserves the historic assignments accurately and segments the \nfacts accordingly.\nWith this hybrid approach, you issue a new row to capture the change (type 2) \nand add a new column to track the current assignment (type 3), where subsequent \nchanges are handled as a type 1 response. An engineer at a technology company \n",
      "content_length": 2896,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 198,
      "content": "Chapter 5\n162\nsuggested we refer to this combo approach as type 6 because both the sum and \nproduct of 1, 2, and 3 equals 6. \nAgain, although this technique may be naturally appealing to some, it is impor-\ntant to always consider the business users’ perspective as you strive to arrive at a \nreasonable balance between ﬂ exibility and complexity. You may want to limit which \ncolumns are exposed to some users so they’re not overwhelmed by choices.\n Type 7: Dual Type 1 and Type 2 Dimensions\nWhen  we ﬁ rst described type 6, someone asked if the technique would be appropri-\nate for supporting both current and historic perspectives for 150 attributes in a large \ndimension table. That question sent us back to the drawing board.\n In  this ﬁ nal hybrid technique, the dimension natural key (assuming it’s durable) \nis included as a fact table foreign key, in addition to the surrogate key for type 2 \ntracking, as illustrated in Figure 5-14. If the natural key is unwieldy or ever reas-\nsigned, you should use a separate durable supernatural key instead. The type 2 \ndimension contains historically accurate attributes for ﬁ ltering and grouping based \non the eff ective values when the fact event occurred. The durable key joins to a \ndimension with just the current type 1 values. Again, the column labels in this table \nshould be prefaced with “current” to reduce the risk of user confusion. You can use \nthese dimension attributes to summarize or ﬁ lter facts based on the current proﬁ le, \nregardless of the attribute values in eff ect when the fact event occurred. \nFact Table\nDate Key (FK)\nProduct Key (FK)\nDurable Product Key (FK)\nMore FKs...\nFacts\nProduct Dimension\nProduct Key (PK)\nDurable Product Key (DK)\nProduct Description\nDepartment Name\n... \nRow Effective Date\nRow Expiration Date\nCurrent Row Indicator\nCurrent Product Dimension\nDurable Product Key (PK)\nCurrent Product Description\nCurrent Department Name\n...\nView of Product Dimension\n(where Current Row Indicator=Current)\nFigure 5-14: Type 7 with dual foreign keys for dual type 1 and type 2 dimension tables.\nThis approach delivers the same functionality as type 6. Although the type 6 \nresponse spawns more attribute columns in a single dimension table, this approach \nrelies on two foreign keys in the fact table. Type 7 invariably requires less ETL eff ort \nbecause the current type 1 attribute table could easily be delivered via a view of \nthe type 2 dimension table, limited to the most current rows. The incremental cost \nof this ﬁ nal technique is the additional column carried in the fact table; however, \n",
      "content_length": 2584,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 199,
      "content": "Procurement 163\nqueries based on current attribute values would be ﬁ ltering on a smaller dimension \ntable than previously described with type 6.\nOf course, you could avoid storing the durable key in the fact table by joining the \ntype 1 view containing current attributes to the durable key in the type 2 dimension \ntable itself. In this case, however, queries that are only interested in current rollups \nwould need to traverse from the type 1 outrigger through the more voluminous \ntype 2 dimension before ﬁ nally reaching the facts, which would likely negatively \nimpact query performance for current reporting.\nA variation of this dual type 1 and type 2 dimension table approach again relies \non a view to deliver current type 1 attributes. However, in this case, the view associ-\nates the current attribute values with all the durable key’s type 2 rows, as illustrated \nin Figure 5-15.\nFact Table\nDate Key (FK)\nProduct Key (FK)\nMore FKs...\nFacts\nProduct Dimension\nProduct Key (PK)\nDurable Product Key\nProduct Description\nDepartment Name\n... \nRow Effective Date\nRow Expiration Date\nCurrent Row Indicator\nCurrent Product Dimension\nProduct Key (PK)\nDurable Product Key\nCurrent Product Description\nCurrent Department Name\n...\nView of Product Dimension\nFigure 5-15: Type 7 variation with single surrogate key for dual type 1 and type 2 \ndimension tables.\nBoth dimension tables in Figure 5-15 have the same number of rows, but the \ncontents of the tables are diff erent, as shown in Figure 5-16.\nRows in Product dimension:\nProduct\nKey\nSKU (NK)\nDurable\nProduct\nKey\nProduct\nDescription\nDepartment\nName\n...\nRow Effective \nDate\nRow\nExpiration \nDate\nCurrent Row\nIndicator\n12345\n25984\n31726\nABC922-Z\nABC922-Z\nABC922-Z\n12345\n12345\n12345\nIntelliKidz\nIntelliKidz\nIntelliKidz\nEducation\nStrategy\nCritical Thinking\n... \n...\n...\n2012-01-01\n2013-02-01\n2013-07-01\n2013-01-31\n2013-06-30\n9999-12-31\nExpired\nExpired \nCurrent\nRows in Product dimension’s current view:\nProduct\nKey\nSKU (NK)\nDurable\nProduct\nKey\nCurrent\nProduct\nDescription\nCurrent\nDepartment\nName\n...\n12345\n25984\n31726\nABC922-Z\nABC922-Z\nABC922-Z\n12345 \n12345 \n12345\nIntelliKidz\nIntelliKidz\nIntelliKidz\nCritical Thinking\nCritical Thinking\nCritical Thinking\n... \n...\n...\nFigure 5-16: SCD type 7 variation sample rows.\n",
      "content_length": 2261,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 200,
      "content": "Chapter 5\n164\nType 7 for Random “As Of” Reporting\nFinally,  although it’s uncommon, you might be asked to roll up historical facts \nbased on any speciﬁ c point-in-time proﬁ le, in addition to reporting by the attribute \nvalues in eff ect when the fact event occurred or by the attribute’s current values. \nFor example, perhaps the business wants to report three years of historical metrics \nbased on the hierarchy in eff ect on December 1 of last year. In this case, you can \nuse the dual dimension keys in the fact table to your advantage. First ﬁ lter on the \ntype 2 dimension row eff ective and expiration dates to locate the rows in eff ect on \nDecember 1 of last year. With this constraint, a single row for each durable key \nin the type 2 dimension is identiﬁ ed. Then join this ﬁ ltered set to the durable key in \nthe fact table to roll up any facts based on the point-in-time attribute values. It’s as \nif you’re deﬁ ning the meaning of “current” on-the-ﬂ y. Obviously, you must ﬁ lter \non the row eff ective and expiration dates, or you’ll have multiple type 2 rows for \neach durable key. Finally, only unveil this capability to a limited, highly analytic \naudience; this embellishment is not for the timid.\nSlowly Changing Dimension Recap\nWe’ve summarized the techniques for tracking dimension attribute changes in \nFigure 5-17. This chart highlights the implications of each slowly changing dimen-\nsion technique on the analysis of performance metrics in the fact table.\nSCD Type\nDimension Table Action\nImpact on Fact Analysis\nType 0\nNo change to attribute value.\nFacts associated with attribute’s original value.\nType 1\nOverwrite attribute value.\nFacts associated with attribute’s current value.\nType 2\nType 3\nType 4\nType 5\nType 6\nAdd new dimension row for profile\nwith new attribute value.\nAdd new column to preserve attribute’s\ncurrent and prior values.\nAdd mini-dimension table containing\nrapidly changing attributes.\nAdd type 4 mini-dimension, along with\noverwritten type 1 mini-dimension key\nin base dimension.\nAdd type 1 overwritten attributes to\ntype 2 dimension row, and overwrite\nall prior dimension rows.\nFacts associated with attribute value in effect when\nfact occured.\nFacts associated with both current and prior attribute\nalternative values.\nFacts associated with rapidly changing attributes in\neffect when fact occured.\nFacts associated with rapidly changing attributes in\neffect when fact occurred, plus current rapidly changing\nattribute values.\nFacts associated with attribute value in effect when fact\noccurred, plus current values.\nFacts associated with attribute value in effect when fact\noccurred, plus current values.\nType 7\nAdd type 2 dimension row with new\nattribute value, plus view limited to\ncurrent rows and/or attribute values.\nFigure 5-17: Slowly changing dimension techniques summary.\n",
      "content_length": 2830,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 201,
      "content": "Procurement 165\nSummary\nIn this chapter we discussed several approaches to handling procurement data. \nEff ectively managing procurement performance can have a major impact on an \norganization’s bottom line.\nWe also introduced techniques to deal with changes to dimension attribute \nvalues. The slowly changing responses range from doing nothing (type 0) to \noverwriting the value (type 1) to complicated hybrid approaches (such as types 5 \nthrough 7) which combine techniques to support requirements for both historic \nattribute preservation and current attribute reporting. You’ll undoubtedly need to \nre-read this section as you consider slowly changing dimension attribute strategies \nfor your DW/BI system.\n",
      "content_length": 712,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 203,
      "content": "Order Management\nO\nrder  management consists of several critical business processes, including \norder, shipment, and invoice processing. These processes spawn metrics, \nsuch as sales volume and invoice revenue, that are key performance indicators for \nany organization that sells products or services to others. In fact, these foundation \nmetrics are so crucial that DW/BI teams frequently tackle one of the order manage-\nment processes for their initial implementation. Clearly, the topics in this case study \ntranscend industry boundaries.\nIn this chapter we’ll explore several diff erent order management transactions, \nincluding the common characteristics and complications encountered when \ndimensionally modeling these transactions. We’ll further develop the concept of \nan accumulating snapshot to analyze the order fulﬁ llment pipeline from initial \norder to invoicing.\nChapter 6 discusses the following concepts:\n \n■Bus matrix snippet for order management processes\n \n■Orders transaction schema\n \n■Fact table normalization considerations\n \n■Role-playing dimensions\n \n■Ship-to/bill-to customer dimension considerations\n \n■Factors to determine if single or multiple dimensions\n \n■Junk dimensions for miscellaneous ﬂ ags and indicators versus alternative \ndesigns\n \n■More on degenerate dimensions\n \n■Multiple currencies and units of measure\n \n■Handling of facts with diff erent granularity\n \n■Patterns to avoid with header and line item transactions\n \n■Invoicing transaction schema with proﬁ t and loss facts\n \n■Audit dimension\n6\n",
      "content_length": 1536,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 204,
      "content": "Chapter 6\n168\n \n■Quantitative measures and qualitative descriptors of service level performance\n \n■Order fulﬁ llment pipeline as accumulating snapshot schema\n \n■Lag calculations\n Order Management Bus Matrix\nThe  order management function is composed of a series of business processes. In \nits most simplistic form, you can envision a subset of the enterprise data warehouse \nbus matrix that resembles Figure 6-1.\nQuoting\nOrdering\nShipping to Customer\nShipment Invoicing\nReceiving Payments\nCustomer Returns\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nDate\nCustomer\nProduct\nSales Rep\nDeal\nWarehouse\nShipper\nFigure 6-1: Bus matrix rows for order management processes.\nAs described in earlier chapters, the bus matrix closely corresponds to the orga-\nnization’s value chain. In this chapter we’ll focus on the order and invoice rows \nof the matrix. We’ll also describe an accumulating snapshot fact table to evaluate \nperformance across multiple stages of the overall order fulﬁ llment process.\n Order Transactions\nThe  natural granularity for an order transaction fact table is one row for each line \nitem on an order. The dimensions associated with the orders business process are \norder date, requested ship date, product, customer, sales rep, and deal. The facts \ninclude the order quantity and extended order line gross, discount, and net (equal \nto the gross amount less discount) dollar amounts. The resulting schema would \nlook similar to Figure 6-2.\n",
      "content_length": 1479,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 205,
      "content": "Order Management 169\nOrder Date Key (FK)\nRequested Ship Date Key (FK)\nCustomer Key (FK)\nProduct Key (FK)\nSales Rep Key (FK)\nDeal Key (FK)\nOrder Number (DD)\nOrder Line Number (DD)\nOrder Line Quantity\nExtended Order Line Gross Dollar Amount\nExtended Order Line Discount Dollar Amount\nExtended Order Line Net Dollar Amount\nCustomer Dimension\nProduct Dimension\nDeal Dimension\nOrder Date Dimension\nOrder Line Transaction Fact\nRequested Ship Date Dimension\nSales Rep Dimension\nFigure 6-2: Order transaction fact table.\n Fact Normalization\nRather  than storing the list of facts in Figure 6-2, some designers want to further nor-\nmalize the fact table so there’s a single, generic fact amount along with a dimension \nthat identiﬁ es the type of measurement. In this scenario, the fact table granularity is \none row per measurement per order line, instead of the more natural one row per order \nline event. The measurement type dimension would indicate whether the fact is the \ngross order amount, order discount amount, or some other measure. This technique \nmay make sense when the set of facts is extremely lengthy, but sparsely populated \nfor a given fact row, and no computations are made between facts. You could use this \ntechnique to deal with manufacturing quality test data where the facts vary widely \ndepending on the test conducted.\nHowever, you should generally resist the urge to normalize the fact table in this \nway. Facts usually are not sparsely populated within a row. In the order transaction \nschema, if you were to normalize the facts, you’d be multiplying the number of rows \nin the fact table by the number of fact types. For example, assume you started with \n10 million order line fact table rows, each with six keys and four facts. If the fact \nrows were normalized, you’d end up with 40 million fact rows, each with seven \nkeys and one fact. In addition, if any arithmetic function is performed between \nthe facts (such as discount amount as a percentage of gross order amount), it is \nfar easier if the facts are in the same row in a relational star schema because SQL \nmakes it diffi  cult to perform a ratio or diff erence between facts in diff erent rows. \nIn Chapter 14: Healthcare, we’ll explore a situation where a measurement type \ndimension makes more sense. This pattern is also more appropriate if the primary \nplatform supporting BI applications is an OLAP cube; the cube enables computations \n",
      "content_length": 2426,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 206,
      "content": "Chapter 6\n170\nthat cut the cube along any dimension, regardless if it’s a date, product, customer, \nor measurement type.\n Dimension Role Playing\nBy  now you know to expect a date dimension in every fact table because you’re \nalways looking at performance over time. In a transaction fact table, the primary date \ncolumn is the transaction date, such as the order date. Sometimes you discover other \ndates associated with each transaction, such as the requested ship date for the order.\nEach  of the dates should be a foreign key in the fact table, as shown in Figure 6-3. \nHowever, you cannot simply join these two foreign keys to the same date dimension \ntable. SQL would interpret this two-way simultaneous join as requiring both the \ndates to be identical, which isn’t very likely.\nLogical views or aliases of the\nsingle physical date dimension\nOrder Date Key (FK)\nRequested Ship Date Key (FK)\nCustomer Key (FK)\nProduct Key (FK)\nSales Rep Key (FK)\nDeal Key (FK)\nOrder Number (DD)\nOrder Line Number (DD)\nOrder Quantity\nExtended Order Line Gross Dollar Amount\nExtended Order Line Discount Dollar Amount\nExtended Order Line Net Dollar Amount\nOrder Line Transaction Fact\nDate Key\nDate\nDay of Week\nMonth\nQuarter\n...\nDate Dimension\nOrder Date Key\nOrder Date\nOrder Day of Week\nOrder Month\nOrder Quarter\n...\nRequested Ship Date Key\nRequested Ship Date\nRequested Ship Day of Week\nRequested Ship Month\nRequested Ship Quarter\n...\nOrder Date Dimension\nRequested Ship Date Dimension\nFigure 6-3: Role-playing date dimensions.\nEven though you cannot literally join to a single date dimension table, you can \nbuild and administer a single physical date dimension table. You then create the \nillusion of two independent date dimensions by using views or aliases. Be careful to \nuniquely label the columns in each of the views or aliases. For example, the order \nmonth attribute should be uniquely labeled to distinguish it from the requested \nship month. If you don’t establish unique column names, you wouldn’t be able to \ntell the columns apart when both are dragged into a report.\nAs we brieﬂ y described in Chapter 3: Retail Sales, we would deﬁ ne the order date \nand requested order date views as follows:\ncreate view order_date\n  (order_date_key, order_day_of_week, order_month, ...) \n  as select date_key, day_of_week, month, ... from date\n",
      "content_length": 2333,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 207,
      "content": "Order Management 171\nand\ncreate view req_ship_date\n  (req_ship_date_key, req_ship_day_of_week, req_ship_month, ...) \n  as select date_key, day_of_week, month, ... from date\nAlternatively,  SQL supports the concept of aliasing. Many BI tools also enable \naliasing within their semantic layer. However, we caution against this approach \nif multiple BI tools, along with direct SQL-based access, are used within the \norganization.\nRegardless  of the implementation approach, you now have two unique logical date \ndimensions that can be used as if they were independent with completely unrelated \nconstraints. This is referred to as role playing because the date dimension simultane-\nously serves diff erent roles in a single fact table. You’ll see additional examples of \ndimension role playing sprinkled throughout this book.\nNOTE \nRole playing in a dimensional model occurs when a single dimension \nsimultaneously appears several times in the same fact table. The underlying dimen-\nsion may exist as a single physical table, but each of the roles should be presented \nto the BI tools as a separately labeled view.\nIt’s worth noting that some OLAP products do not support multiple roles of the \nsame dimension; in this scenario, you’d need to create two separate dimensions for \nthe two roles. In addition, some OLAP products that enable multiple roles do not \nenable attribute renaming for each role. In the end, OLAP environments may be \nlittered with a plethora of separate dimensions, which are treated simply as roles \nin the relational star schema.\nTo  handle the multiple dates, some designers are tempted to create a single date \ntable with a key for each unique order date and requested ship date combination. \nThis approach falls apart on several fronts. First, the clean and simple daily date \ntable with approximately 365 rows per year would balloon in size if it needed to \nhandle all the date combinations. Second, a combination date table would no longer \nconform to the other frequently used daily, weekly, and monthly date dimensions.\nRole Playing and the Bus Matrix\nThe  most common technique to document role playing on the bus matrix is to \nindicate the multiple roles within a single cell, as illustrated in Figure 6-4. We used \na similar approach in Chapter 4: Inventory for documenting shrunken conformed \ndimensions. This method is especially appropriate for the date dimension on the \nbus matrix given its numerous logical roles. Alternatively, if the number of roles is \nlimited and frequently reused across processes, you can create subcolumns within \na single conformed dimension column on the matrix.\n",
      "content_length": 2628,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 208,
      "content": "Chapter 6\n172\nQuoting\nOrdering\nShipping to Customer\nShipment Invoicing\nReceiving Payments\nCustomer Returns\nQuote Date\nOrder Date\nRequested Ship Date\nShipment Date\nInvoice Date\nPayment Receipt Date\nReturn Date\nDate\nFigure 6-4: Communicating role-playing dimensions on the bus matrix.\nProduct Dimension Revisited\nEach  of the case study vignettes presented so far has included a product dimen-\nsion. The product dimension is one of the most common and most important \ndimension tables. It describes the complete portfolio of products sold by a com-\npany. In many cases, the number of products in the portfolio turns out to be sur-\nprisingly large, at least from an outsider’s perspective. For example, a prominent \nU.S. manufacturer of dog and cat food tracks more than 25,000 manufacturing \nvariations of its products, including retail products everyone (or every dog and \ncat) is familiar with, as well as numerous specialized products sold through com-\nmercial and veterinary channels. Some durable goods manufacturers, such as \nwindow companies, sell millions of unique product conﬁ gurations.\nMost  product dimension tables share the following characteristics:\n \n■Numerous verbose, descriptive columns. For manufacturers, it’s not unusual \nto maintain 100 or more descriptors about the products they sell. Dimension \ntable attributes naturally describe the dimension row, do not vary because \nof the inﬂ uence of another dimension, and are virtually constant over time, \nalthough some attributes do change slowly over time.\n \n■One or more attribute hierarchies, plus non-hierarchical attributes. Products \ntypically roll up according to multiple deﬁ ned hierarchies. The many-to-one \nﬁ xed depth hierarchical data should be presented in a single ﬂ attened, denor-\nmalized product dimension table. You should resist creating normalized snow-\nﬂ aked sub-tables; the costs of a more complicated presentation and slower \nintra-dimension browsing performance outweigh the minimal storage savings \nbeneﬁ ts. Product dimension tables can have thousands of entries. With so many \n",
      "content_length": 2075,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 209,
      "content": "Order Management 173\nrows, it is not too useful to request a pull-down list of the product descriptions. \nIt is essential to have the ability to constrain on one attribute, such as ﬂ avor, \nand then another attribute, such as package type, before attempting to display \nthe product descriptions. Any attributes, regardless of whether they belong to \na single hierarchy, should be used freely for browsing and drilling up or down. \nMany product dimension attributes are standalone low-cardinality attributes, \nnot part of explicit hierarchies.\nThe  existence of an operational product master helps create and maintain the \nproduct dimension, but a number of transformations and administrative steps must \noccur to convert the operational master ﬁ le into the dimension table, including the \nfollowing:\n \n■Remap the operational product code to a surrogate key. As we discussed in \nChapter 3, this meaningless surrogate primary key is needed to avoid havoc \ncaused by duplicate use of an operational product code over time. It also \nmight be necessary to integrate product information sourced from diff erent \noperational systems. Finally, as you just learned in Chapter 5: Procurement, \nthe surrogate key is needed to track type 2 product attribute changes. \n \n■Add descriptive attribute values to augment or replace operational codes. \nYou shouldn’t accept the excuse that the business users are familiar with the \noperational codes. The only reason business users are familiar with codes is that \nthey have been forced to use them! The columns in a product dimension are \nthe sole source of query constraints and report labels, so the contents must be \nlegible. Cryptic abbreviations are as bad as outright numeric codes; they also \nshould be augmented or replaced with readable text. Multiple abbreviated codes \nin a single column should be expanded and separated into distinct attributes.\n \n■Quality check the attribute values to ensure no misspellings, impossible \nvalues, or multiple variations. BI applications and reports rely on the precise \ncontents of the dimension attributes. SQL will produce another line in a report \nif the attribute value varies in any way based on trivial punctuation or spell-\ning diff erences. You should ensure that the attribute values are completely \npopulated because missing values easily cause misinterpretations. Incomplete \nor poorly administered textual dimension attributes lead to incomplete or \npoorly produced reports.\n \n■Document the attribute deﬁ nitions, interpretations, and origins in the \nmetadata. Remember that the metadata is analogous to the DW/BI encyclo-\npedia. You must be vigilant about populating and maintaining the metadata \n repository.\n",
      "content_length": 2702,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 210,
      "content": "Chapter 6\n174\nCustomer Dimension\nThe  customer dimension contains one row for each discrete location to which you \nship a product. Customer dimension tables can range from moderately sized (thou-\nsands of rows) to extremely large (millions of rows) depending on the nature of the \nbusiness. A typical customer dimension is shown in Figure 6-5.\nCustomer Key (PK)\nCustomer ID (Natural Key)\nCustomer Name\nCustomer Ship To Address\nCustomer Ship To City\nCustomer Ship To County\nCustomer Ship To City-State\nCustomer Ship To State\nCustomer Ship To ZIP\nCustomer Ship To ZIP Region\nCustomer Ship To ZIP Sectional Center\nCustomer Bill To Name\nCustomer Bill To Address\nCustomer Organization Name\nCustomer Corporate Parent Name\nCustomer Credit Rating\nCustomer Dimension\nFigure 6-5: Sample customer dimension.\nSeveral  independent hierarchies typically coexist in a customer dimension. The natu-\nral geographic hierarchy is clearly deﬁ ned by the ship-to location. Because the ship-to \nlocation is a point in space, any number of geographic hierarchies may be deﬁ ned by \nnesting more expansive geographic entities around the point. In the United States, \nthe usual geographic hierarchy is city, county, and state. It is often useful to include a \ncity-state attribute because the same city name exists in multiple states. The ZIP code \nidentiﬁ es a secondary geographic breakdown. The ﬁ rst digit of the ZIP code identiﬁ es \na geographic region of the United States (for example, 0 for the Northeast and 9 for \ncertain western states), whereas the ﬁ rst three digits of the ZIP code identify a mailing \nsectional center.\nAlthough these geographic characteristics may be captured and managed in a \nsingle master data management system, you should embed the attributes within \nthe respective dimensions rather than relying on an abstract, generic geography/\nlocation dimension that includes one row for every point in space independent of \nthe dimensions. We’ll talk more about this in Chapter 11: Telecommunications.\nAnother common hierarchy is the customer’s organizational hierarchy, assuming \nthe customer is a corporate entity. For each customer ship-to address, you might \nhave a customer bill-to and customer parent corporation. For every row in the \n",
      "content_length": 2244,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 211,
      "content": "Order Management 175\ncustomer dimension, both the physical geographies and organizational affi  liation \nare well deﬁ ned, even though the hierarchies roll up diff erently.\nNOTE \nIt is natural and common, especially for customer-oriented dimensions, \nfor a dimension to simultaneously support multiple independent hierarchies. The \nhierarchies may have diff erent numbers of levels. Drilling up and drilling down \nwithin each of these hierarchies must be supported in a dimensional model.\nThe alert reader may have a concern with the implied assumption that multiple \nship-tos roll up to a single bill-to in a many-to-one relationship. The real world may \nnot be quite this clean and simple. There are always a few exceptions involving \nship-to addresses that are associated with more than one bill-to. Obviously, this \nbreaks the simple hierarchical relationship assumed in Figure 6-5. If this is a rare \noccurrence, it would be reasonable to generalize the customer dimension so that the \ngrain of the dimension is each unique ship-to/bill-to combination. In this scenario, \nif there are two sets of bill-to information associated with a given ship-to location, \nthen there would be two rows in the dimension, one for each combination. On the \nother hand, if many of the ship-tos are associated with many bill-tos in a robust \nmany-to-many relationship, then the ship-to and bill-to customers probably need to \nbe handled as separate dimensions that are linked together by the fact table. With \neither approach, exactly the same information is preserved. We’ll spend more time \non organizational hierarchies, including the handling of variable depth recursive \nrelationships, in Chapter 7: Accounting.\n Single Versus Multiple Dimension Tables\nAnother  potential hierarchy in the customer dimension might be the manufacturer’s \nsales organization. Designers sometimes question whether sales organization attri-\nbutes should be modeled as a separate dimension or added to the customer dimension. \nIf sales reps are highly correlated with customers in a one-to-one or many-to-one rela-\ntionship, combining the sales organization attributes with the customer attributes in \na single dimension is a viable approach. The resulting dimension is only as big as the \nlarger of the two dimensions. The relationships between sales teams and customers \ncan be browsed effi  ciently in the single dimension without traversing the fact table.\nHowever, sometimes the relationship between sales organization and customer \nis more complicated. The following factors must be taken into consideration:\n \n■Is the one-to-one or many-to-one relationship actually a many-to-many? \nAs we discussed earlier, if the many-to-many relationship is an exceptional \ncondition, then you may still be tempted to combine the sales rep attributes \ninto the customer dimension, knowing multiple surrogate keys are needed to \nhandle these rare many-to-many occurrences. However, if the many-to-many \n",
      "content_length": 2965,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 212,
      "content": "Chapter 6\n176\nrelationship is the norm, you should handle the sales rep and customer as \nseparate dimensions.\n \n■Does the sales rep and customer relationship vary over time or under the \ninﬂ uence of another dimension? If so, you’d likely create separate dimensions \nfor the rep and customer.\n \n■Is the customer dimension extremely large? If there are millions of customer \nrows, you’d be more likely to treat the sales rep as a separate dimension rather \nthan forcing all sales rep analysis through a voluminous customer dimension.\n \n■Do the sales rep and customer dimensions participate independently in \nother fact tables? Again, you’d likely keep the dimensions separate. Creating a \nsingle customer dimension with sales rep attributes exclusively around order \ndata may cause users to be confused when they’re analyzing other processes \ninvolving sales reps.\n \n■Does the business think about the sales rep and customer as separate \nthings? This factor may be tough to discern and impossible to quantify. But \nthere’s no sense forcing two critical dimensions into a single blended dimen-\nsion if this runs counter to the business’s perspectives.\nWhen entities have a ﬁ xed, time-invariant, strongly correlated relationship, they \nshould be modeled as a single dimension. In most other cases, the design likely will \nbe simpler and more manageable when the entities are separated into two dimen-\nsions (while remembering the general guidelines concerning too many dimensions). \nIf you’ve already identiﬁ ed 25 dimensions in your schema, you should consider \ncombining dimensions, if possible.\nWhen the dimensions are separate, some designers want to create a little table \nwith just the two dimension keys to show the correlation without using the order \nfact table. In many scenarios, this two-dimension table is unnecessary. There is no \nreason to avoid the fact table to respond to this relationship inquiry. Fact tables are \nincredibly effi  cient because they contain only dimension keys and measurements, \nalong with the occasional degenerate dimension. The fact table is created speciﬁ cally \nto represent the correlations and many-to-many relationships between dimensions.\nAs we discussed in Chapter 5, you could capture the customer’s currently assigned \nsales rep by including the relevant descriptors as type 1 attributes. Alternatively, you \ncould use the slowly changing dimension (SCD) type 5 technique by embedding a \ntype 1 foreign key to a sales rep dimension outrigger within the customer dimen-\nsion; the current values could be presented as if they’re included on the customer \ndimension via a view declaration.\n Factless Fact Table for Customer/Rep Assignments\nBefore  we leave the topic of sales rep assignments to customers, users sometimes \nwant the ability to analyze the complex assignment of sales reps to customers over \n",
      "content_length": 2852,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 213,
      "content": "Order Management 177\ntime, even if no order activity has occurred. In this case, you could construct a \nfactless fact table, as illustrated in Figure 6-6, to capture the sales rep coverage. \nThe coverage table would provide a complete map of the historical assignments of \nsales reps to customers, even if some of the assignments never resulted in a sale. \nThis factless fact table contains dual date keys for the eff ective and expiration dates \nof each assignment. The expiration date on the current rep assignment row would \nreference a special date dimension row that identiﬁ es a future, undetermined date.\nAssignment Effective Date Key (FK)\nAssignment Expiration Date Key (FK)\nSales Rep Key (FK)\nCustomer Key (FK)\nCustomer Assignment Counter (=1)\nSales Rep-Customer Assignment Fact\nDate Dimension (views for 2 roles)\nSales Rep Dimension\nCustomer Dimension\nFigure 6-6: Factless fact table for sales rep assignments to customers.\nYou may want to compare the assignments fact table with the order transactions \nfact table to identify rep assignments that have not yet resulted in order activity. You \nwould do so by leveraging SQL’s capabilities to perform set operations (for example, \nselecting all the reps in the coverage table and subtracting all the reps in the orders \ntable) or by writing a correlated subquery.\nDeal Dimension\nThe  deal dimension is similar to the promotion dimension from Chapter 3. The deal \ndimension describes the incentives off ered to customers that theoretically aff ect the \ncustomers’ desire to purchase products. This dimension is also sometimes referred \nto as the contract. As shown in Figure 6-7, the deal dimension describes the full \ncombination of terms, allowances, and incentives that pertain to the particular \norder line item.\nDeal Key (PK)\nDeal ID (NK)\nDeal Description\nDeal Terms Description\nDeal Terms Type Description\nAllowance Description\nAllowance Type Description\nSpecial Incentive Description\nSpecial Incentive Type Description\nLocal Budget Indicator\nDeal Dimension\nFigure 6-7: Sample deal dimension.\n",
      "content_length": 2057,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 214,
      "content": "Chapter 6\n178\nThe same issues you faced in the retail promotion dimension also arise with this \ndeal dimension. If the terms, allowances, and incentives are usefully correlated, it \nmakes sense to package them into a single deal dimension. If the terms, allowances, \nand incentives are quite uncorrelated and you end up generating the Cartesian \nproduct of these factors in the dimension, it probably makes sense to split the deal \ndimension into its separate components. Again, this is not an issue of gaining or \nlosing information because the schema contains the same information in both cases. \nThe issues of user convenience and administrative complexity determine whether \nto represent these deal factors as multiple dimensions. In a very large fact table, \nwith hundreds of millions or billions of rows, the desire to reduce the number of \nkeys in the fact table composite key favors treating the deal attributes as a single \ndimension, assuming this meshes with the business users’ perspectives. Certainly \nany deal dimension smaller than 100,000 rows would be tractable in this design.\n Degenerate Dimension for Order Number\nEach  line item row in the order fact table includes the order number as a degenerate \ndimension. Unlike an operational header/line or parent/child database, the order \nnumber in a dimensional model is typically not tied to an order header table. You \ncan triage all the interesting details from the order header into separate dimensions \nsuch as the order date and customer ship-to. The order number is still useful for \nseveral reasons. It enables you to group the separate line items on the order and \nanswer questions such as “What is the average number of line items on an order?” \nThe order number is occasionally used to link the data warehouse back to the \noperational world. It may also play a role in the fact table’s primary key. Because \nthe order number sits in the fact table without joining to a dimension table, it is a \ndegenerate dimension.\nNOTE \nDegenerate dimensions typically are reserved for operational transaction \nidentiﬁ ers. They should not be used as an excuse to stick cryptic codes in the fact \ntable without joining to dimension tables for descriptive decodes.\nAlthough there is likely no analytic purpose for the order transaction line num-\nber, it may be included in the fact table as a second degenerate dimension given its \npotential role in the primary key, along with the linkage to the operational system \nof record. In this case, the primary key for the line item grain fact table would be \nthe order number and line number.\nSometimes data elements belong to the order itself and do not naturally fall into \nother dimension tables. In this situation, the order number is no longer a degenerate \ndimension but is a standard dimension with its own surrogate key and attributes. \n",
      "content_length": 2850,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 215,
      "content": "Order Management 179\nHowever, designers with a strong operational background should resist the urge to \nsimply dump the traditional order header information into an order dimension. In \nalmost all cases, the header information belongs in other analytic dimensions that \ncan be associated with the line item grain fact table rather than merely being cast \noff  into a dimension that closely resembles the operational order header record.\n Junk Dimensions\nWhen  modeling complex transactional source data, you often encounter a number \nof miscellaneous indicators and ﬂ ags that are populated with a small range of dis-\ncrete values. You have several rather unappealing options for handling these low \ncardinality ﬂ ags and indicators, including:\n \n■Ignore the ﬂ ags and indicators. You can ask the obligatory question about \neliminating these miscellaneous ﬂ ags because they seem rather insigniﬁ cant, \nbut this notion is often vetoed quickly because someone occasionally needs \nthem. If the indicators are incomprehensible or inconsistently populated, \nperhaps they should be left out.\n \n■Leave the ﬂ ags and indicators unchanged on the fact row. You don’t want \nto store illegible cryptic indicators in the fact table. Likewise, you don’t \nwant to store bulky descriptors on the fact row, which would cause the \ntable to swell alarmingly. It would be a shame to leave a handful of textual \nindicators on the row.\n \n■Make each ﬂ ag and indicator into its own dimension. Adding separate foreign \nkeys to the fact table is acceptable if the resulting number of foreign keys is \nstill reasonable (no more than 20 or so). However, if the list of foreign keys \nis already lengthy, you should avoid adding more clutter to the fact table.\n \n■Store the ﬂ ags and indicators in an order header dimension. Rather than \ntreating the order number as a degenerate dimension, you could make it a \nregular dimension with the low cardinality ﬂ ags and indicators as attributes. \nAlthough this approach accurately represents the data relationships, it is ill-\nadvised, as described below.\nAn  appropriate alternative approach for tackling these ﬂ ags and indicators is to \nstudy them carefully and then pack them into one or more junk dimensions. A junk \ndimension is akin to the junk drawer in your kitchen. The kitchen junk drawer is a \ndumping ground for miscellaneous household items, such as rubber bands, paper \nclips, batteries, and tape. Although it may be easier to locate the rubber bands if \na separate kitchen drawer is dedicated to them, you don’t have adequate storage \ncapacity to do so. Besides, you don’t have enough stray rubber bands, nor do you \nneed them frequently, to warrant the allocation of a single-purpose storage space. \n",
      "content_length": 2734,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 216,
      "content": "Chapter 6\n180\nThe junk drawer provides you with satisfactory access while still retaining storage \nspace for the more critical and frequently accessed dishes and silverware. In the \ndimensional modeling world, the junk dimension nomenclature is reserved for DW/\nBI professionals. We typically refer to the junk dimension as a transaction indicator \nor transaction proﬁ le dimension when talking with the business users.\nNOTE \nA junk dimension is a grouping of low-cardinality ﬂ ags and indicators. \nBy creating a junk dimension, you remove the ﬂ ags from the fact table and place \nthem into a useful dimensional framework.\nIf a single junk dimension has 10 two-value indicators, such as cash versus credit \npayment type, there would be a maximum of 1,024 (210) rows. It probably isn’t \ninteresting to browse among these ﬂ ags within the dimension because every ﬂ ag \nmay occur with every other ﬂ ag. However, the junk dimension is a useful holding \nplace for constraining or reporting on these ﬂ ags. The fact table would have a single, \nsmall surrogate key for the junk dimension.\nOn the other hand, if you have highly uncorrelated attributes that take on more \nnumerous values, it may not make sense to lump them together into a single junk \ndimension. Unfortunately, the decision is not entirely formulaic. If you have ﬁ ve \nindicators that each take on only three values, a single junk dimension is the best \nroute for these attributes because the dimension has only 243 (35) possible rows. \nHowever, if the ﬁ ve uncorrelated indicators each have 100 possible values, we’d \nsuggest creating separate dimensions because there are now 100 million (1005) \npossible combinations.\nFigure 6-8 illustrates sample rows from an order indicator dimension. A subtle \nissue regarding junk dimensions is whether you should create rows for the full \nCartesian product of all the combinations beforehand or create junk dimension \nrows for the combinations as you encounter them in the data. The answer depends \non how many possible combinations you expect and what the maximum number \ncould be. Generally, when the number of theoretical combinations is high and you \ndon’t expect to encounter them all, you build a junk dimension row at extract time \nwhenever you encounter a new combination of ﬂ ags or indicators.\nNow that junk dimensions have been explained, contrast them to the handling \nof the ﬂ ags and indicators as attributes in an order header dimension. If you want \nto analyze order facts where the order type is Inbound (refer to Figure 6-8’s junk \ndimension rows), the fact table would be constrained to order indicator key equals \n1, 2, 5, 6, 9, 10, and probably a few others. On the other hand, if these attributes \nwere stored in an order header dimension, the constraint on the fact table would be \nan enormous list of all order numbers with an inbound order type.\n",
      "content_length": 2872,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 217,
      "content": "Order Management 181\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n12\n11\nCash\nCash\nCash\nCash\nVisa\nVisa\nVisa\nVisa\nMasterCard\nMasterCard\nMasterCard\nMasterCard\nCash\nCash\nCash\nCash\nCredit\nCredit\nCredit\nCredit\nCredit\nCredit\nCredit\nCredit\nInbound\nInbound\nOutbound\nOutbound\nInbound\nInbound\nOutbound\nOutbound\nInbound\nInbound\nOutbound\nOutbound\nCommissionable\nNon-Commissionable\nCommissionable\nNon-Commissionable\nCommissionable\nNon-Commissionable\nCommissionable\nNon-Commissionable\nCommissionable\nNon-Commissionable\nCommissionable\nNon-Commissionable\nOrder\nIndicator Key\nPayment Type\nDescription\nPayment Type\nGroup\nOrder Type\nCommission Credit\nIndicator\nFigure 6-8: Sample rows of order indicator junk dimension.\n Header/Line Pattern to Avoid\nThere  are two common design mistakes to avoid when you model header/line data \ndimensionally. Unfortunately, both of these patterns still accurately represent the \ndata relationships, so they don’t stick out like a sore thumb. Perhaps equally unfor-\ntunate is that both patterns often feel more comfortable to data modelers and ETL \nteam members with signiﬁ cant transaction processing experience than the patterns \nwe advocate. We’ll discuss the ﬁ rst common mistake here; the other is covered in \nthe section “Another Header/Line Pattern to Avoid.”\nFigure 6-9 illustrates a header/line modeling pattern we frequently observe when \nconducting design reviews. In this example, the operational order header is virtually \nreplicated in the dimensional model as a dimension. The header dimension contains \nall the data from its operational equivalent. The natural key for this dimension is \nthe order number. The grain of the fact table is one row per order line item, but \nthere’s not much dimensionality associated with it because most descriptive context \nis embedded in the order header dimension.\nAlthough  this design accurately represents the header/line relationship, there are \nobvious ﬂ aws. The order header dimension is likely very large, especially relative to \nthe fact table itself. If there are typically ﬁ ve line items per order, the dimension is \n20 percent as large as the fact table; there should be orders of magnitude diff erences \nbetween the size of a fact table and its associated dimensions. Also, dimension tables \ndon’t normally grow at nearly the same rate as the fact table. With this design, you \nwould add one row to the dimension table and an average of ﬁ ve rows to the fact \ntable for every new order. Any analysis of the order’s interesting characteristics, \n",
      "content_length": 2500,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 218,
      "content": "Chapter 6\n182\nsuch as the customer, sales rep, or deal involved, would need to traverse this large \ndimension  table.\nOrder Number (PK)\nOrder Date\nOrder Month\n...\nRequested Ship Date\nRequested Ship Month\n...\nCustomer ID\nCustomer Name\n...\nSales Rep Number\nSales Rep Name\n...\nDeal ID\nDeal Description\n...\nOrder Number (FK)\nProduct Key (FK)\nOrder Line Number (DD)\nOrder Line Quantity\nExtended Order Line Gross Dollar Amount\nExtended Order Line Discount Dollar Amount\nExtended Order Line Net Dollar Amount\n1 row per Order Header\nOrder Line Transaction Fact\nProduct Dimension\nOrder Header Dimension\n1 row per Order Line\nFigure 6-9: Pattern to avoid: treating transaction header as a dimension.\n Multiple Currencies\nSuppose  you track the orders of a large multinational U.S.-based company with sales \noffi  ces around the world. You may be capturing order transactions in more than \n15 diff erent currencies. You certainly wouldn’t want to include columns in the fact \ntable for each currency.\nThe most common analytic requirement is that order transactions be expressed \nin both the local transaction currency and the standardized corporate currency, \nsuch as U.S. dollars in this example. To satisfy this need, each order fact would be \nreplaced with a pair of facts, one for the applicable local currency and another for \nthe equivalent standard corporate currency, as illustrated in Figure 6-10. The con-\nversion rate used to construct each fact row with the dual metrics would depend \non the business’s requirements. It might be the rate at the moment the order was \ncaptured, an end of day rate, or some other rate based on deﬁ ned business rules. This \ntechnique would preserve the transactional metrics, plus allow all transactions to \neasily roll up to the corporate currency without complicated reporting application \ncoding. The metrics in standard currency would be fully additive. The local currency \nmetrics would be additive only for a single speciﬁ ed currency; otherwise, you’d be \ntrying to sum Japanese yen, Thai bhat, and British pounds. You’d also supplement \n",
      "content_length": 2076,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 219,
      "content": "Order Management 183\nthe fact table with a currency dimension to identify the currency type associated \nwith the local currency facts; a currency dimension is needed even if the location \nof the transaction is otherwise known because the location does not necessarily \nguarantee which currency was used.\nOrder Date Key (FK)\nRequested Ship Date Key (FK)\nCustomer Key (FK)\nProduct Key (FK)\nSales Rep Key (FK)\nDeal Key (FK)\nLocal Currency Dimension Key (FK) \nOrder Number (DD)\nOrder Line Number (DD)\nOrder Line Quantity\nExtended Order Line Gross USD Amount\nExtended Order Line Discount USD Amount\nExtended Order Line Net USD Amount\nExtended Order Line Gross Local Currency Amount\nExtended Order Line Discount Local Currency Amount\nExtended Order Line Net Local Currency Amount\nLocal Currency Key (PK)\nLocal Currency Name\nLocal Currency Abbreviation\nLocal Currency Dimension\nOrder Line Transaction Fact\nFigure 6-10: Metrics in multiple currencies within the fact table.\nThis technique can be expanded to support other relatively common examples. \nIf the business’s sales offi  ces roll up into a handful of regional centers, you could \nsupplement the fact table with a third set of metrics representing the transactional \namounts converted into the appropriate regional currency. Likewise, the fact table \ncolumns could represent currencies for the customer ship-to and customer bill-to, \nor the currencies as quoted and shipped.\nIn each of the scenarios, the fact table could physically contain a full set of metrics \nin one currency, along with the appropriate currency conversion rate(s) for that row. \nRather than burdening the business users with appropriately multiplying or divid-\ning by the stored rate, the intra-row extrapolation should be done in a view behind \nthe scenes; all reporting applications would access the facts via this logical layer.\nSometimes the multi-currency support requirements are more complicated than \njust described. You may need to allow a manager in any country to see order volume \nin any currency. In this case, you can embellish the initial design with an additional \ncurrency conversion fact table, as shown in Figure 6-11. The dimensions in this \nfact table represent currencies, not countries, because the relationship between \ncurrencies and countries is not one-to-one. The more common needs of the local \nsales rep and sales management in headquarters would be met simply by querying \nthe orders fact table, but those with less predictable requirements would use the \n",
      "content_length": 2510,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 220,
      "content": "Chapter 6\n184\ncurrency conversion table in a specially crafted query. Navigating the currency \nconversion table is obviously more complicated than using the converted metrics \non the orders fact table.\nCurrency Conversion Fact\nConversion Date Key (FK)\nSource Currency Key (FK)\nDestination Currency Key (FK)\nSource-Destination Exchange Rate\nDestination-Source Exchange Rate\nFigure 6-11: Tracking multiple currencies with daily currency exchange fact table.\nWithin each currency conversion fact table row, the amount expressed in local \ncurrency is absolutely accurate because the sale occurred in that currency on that \nday. The equivalent U.S. dollar value would be based on a conversion rate to U.S. \ndollars for that day. The conversion rate table contains the combinations of rel-\nevant currency exchange rates going in both directions because the symmetric \nrates between two currencies are not equal. It is unlikely this conversion fact table \nneeds to include the full Cartesian product of all possible currency combinations. \nAlthough there are approximately 100 unique currencies globally, there wouldn’t \nneed to be 10,000 daily rows in this currency fact table as there’s not a meaningful \nmarket for every possible pair; likewise, all theoretical combinations are probably \noverkill for the business users.\nThe use of a currency conversion table may also be required to support the busi-\nness’s need for multiple rates, such as an end of month or end of quarter close rate, \nwhich may not be deﬁ ned until long after the transactions have been loaded into \nthe orders fact table.\n Transaction Facts at Different Granularity\nIt  is quite common in header/line operational data to encounter facts of diff ering \ngranularity. On an order, there may be a shipping charge that applies to the entire \norder. The designer’s ﬁ rst response should be to try to force all the facts down to \nthe lowest level, as illustrated in Figure 6-12. This procedure is broadly referred \nto as allocating. Allocating the parent order facts to the child line item level is \ncritical if you want the ability to slice and dice and roll up all order facts by all \ndimensions, including product.\nUnfortunately,  allocating header-level facts down to the line item level may entail \na political wrestling match. It is wonderful if the entire allocation issue is handled by \nthe ﬁ nance department, not by the DW/BI team. Getting organizational agreement \non allocation rules is often a controversial and complicated process. The DW/BI team \n",
      "content_length": 2524,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 221,
      "content": "Order Management 185\nshouldn’t be distracted and delayed by the inevitable organizational negotiation. \nFortunately, in many companies, the need to rationally allocate costs has already \nbeen recognized. A task force, independent of the DW/BI project, already may have \nestablished activity-based costing measures. This is just another name for allocating.\nAllocated\nOrder Header Transaction Fact\nOrder Date Key (FK)\nRequested Ship Date Key (FK)\nCustomer Key (FK)\nSales Rep Key (FK)\nDeal Key (FK)\nOrder Number (PK)\nOrder Shipping Charges Dollar Amount\nOrder Line Transaction Fact\nOrder Date Key (FK)\nRequested Ship Date Key (FK)\nCustomer Key (FK)\nProduct Key (FK)\nSales Rep Key (FK)\nDeal Key (FK)\nOrder Number (DD)\nOrder Line Number (DD)\nOrder Line Quantity\nExtended Order Line Gross Dollar Amount\nExtended Order Line Discount Dollar Amount\nExtended Order Line Net Dollar Amount\nAllocated Order Line Shipping Charges Dollar Amount\nFigure 6-12: Allocating header facts to line items.\nIf the shipping charges and other header-level facts cannot be successfully allo-\ncated, they must be presented in an aggregate table for the overall order. We clearly \nprefer the allocation approach, if possible, because the separate higher-level fact \ntable has some inherent usability issues. Without allocations, you cannot explore \nheader facts by product because the product isn’t identiﬁ ed in a header-grain fact \ntable. If you are successful in allocating facts down to the lowest level, the problem \ngoes away.\nWARNING \nYou shouldn’t mix fact granularities such as order header and order \nline facts within a single fact table. Instead, either allocate the higher-level facts \nto a more detailed level or create two separate fact tables to handle the diff erently \ngrained facts. Allocation is the preferred approach. \nOptimally, the business data stewards obtain enterprise consensus on the allocation \nrules. But sometimes organizations refuse to agree. For example, the ﬁ nance depart-\nment may want to allocate the header freight charged based on the extended gross \norder amount on each line; meanwhile, the logistics group wants the freight charge \nto be allocated based on the weight of the line’s products. In this case, you would \nhave two allocated freight charges on every order line fact table row; the uniquely \ncalculated metrics would also be uniquely labeled. Obviously, agreeing on a single, \nstandard allocation scheme is preferable.\n",
      "content_length": 2445,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 222,
      "content": "Chapter 6\n186\nDesign teams sometimes attempt to devise alternative techniques for handling \nheader/line facts at diff erent granularity, including the following:\n \n■Repeat the unallocated header fact on every line. This approach is fraught \nwith peril given the risk of overstating the header amount when it’s summed \non every line.\n \n■Store the unallocated amount on the transaction’s ﬁ rst or last line. This tac-\ntic eliminates the risk of overcounting, but if the ﬁ rst or last lines are excluded \nfrom the query results due to a ﬁ lter constraint on the product dimension, it \nappears there were no header facts associated with this transaction.\n \n■Set up a special product key for the header fact. Teams who adopt this \napproach sometimes recycle an existing line fact column. For example, if \nproduct key = 99999, then the gross order metric is a header fact, like the \nfreight charge. Dimensional models should be straightforward and legible. You \ndon’t want to embed complexities requiring a business user to wear a special \ndecoder ring to navigate the dimensional model successfully.\nAnother Header/Line Pattern to Avoid\n The  second header/line pattern to avoid is illustrated in Figure 6-13. In this example, \nthe order header is no longer treated as a monolithic dimension but as a fact table \ninstead. The header’s associated descriptive information is grouped into dimen-\nsions surrounding the order fact. The line item fact table (identical in structure and \ngranularity as the ﬁ rst diagram) joins to the header fact based on the order number.\nOrder Date Dimension\nRequested Ship Date Dimension\nDeal Dimension\nOrder Header Transaction Fact\nCustomer Dimension\nSales Rep Dimension\nProduct Dimension\nOrder Date Key (FK)\nRequested Ship Date Key (FK)\nCustomer Key (FK)\nSales Rep Key (FK)\nDeal Key (FK)\nOrder Number (PK)\nExtended Order Total Gross Dollar Amount\nExtended Order Total Discount Dollar Amount\nExtended Order Total Net Dollar Amount\nOrder Total Shipping Charges Dollar Amount\nOrder Line Transaction Fact\nOrder Number (FK)\nOrder Line Number (DD)\nProduct Key (FK)\nOrder Line Quantity\nExtended Order Line Gross Dollar Amount\nExtended Order Line Discount Dollar Amount\nExtended Order Line Net Dollar Amount\nFigure 6-13: Pattern to avoid: not inheriting header dimensionality in line facts.\n",
      "content_length": 2310,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 223,
      "content": "Order Management 187\nAgain, this design accurately represents the parent/child relationship of the order \nheader and line items, but there are still ﬂ aws. Every time the user wants to slice \nand dice the line facts by any of the header attributes, a large header fact table needs \nto be associated with an even larger line fact table. \nInvoice Transactions\nIn  a manufacturing company, invoicing typically occurs when products are shipped \nfrom your facility to the customer. Visualize shipments at the loading dock as boxes \nof product are placed into a truck destined for a particular customer address. The \ninvoice associated with the shipment is created at this time. The invoice has mul-\ntiple line items, each corresponding to a particular product being shipped. Various \nprices, discounts, and allowances are associated with each line item. The extended \nnet amount for each line item is also available.\nAlthough you don’t show it on the invoice to the customer, a number of other \ninteresting facts are potentially known about each product at the time of shipment. \nYou certainly know list prices; manufacturing and distribution costs may be avail-\nable as well. Thus you know a lot about the state of your business at the moment \nof customer invoicing.\nIn  the invoice fact table, you can see all the company’s products, customers, \ncontracts and deals, off -invoice discounts and allowances, revenue generated by \ncustomers, variable and ﬁ xed costs associated with manufacturing and delivering \nproducts (if available), money left over after delivery of product (proﬁ t contribution), \nand customer satisfaction metrics such as on-time shipment.\nNOTE \nFor any company that ships products to customers or bills customers \nfor services rendered, the optimal place to start a DW/BI project typically is with \ninvoices. We often refer to invoicing as the most powerful data because it combines \nthe company’s customers, products, and components of proﬁ tability.\nYou should choose the grain of the invoice fact table to be the individual invoice \nline item. A sample invoice fact table associated with manufacturer shipments is \nillustrated in Figure 6-14.\nAs expected, the invoice fact table contains a number of dimensions from earlier \nin this chapter. The conformed date dimension table again would play multiple \nroles in the fact table. The customer, product, and deal dimensions also would \nconform, so you can drill across fact tables using common attributes. If a single \norder number is associated with each invoice line item, it would be included as a \nsecond degenerate dimension.\n",
      "content_length": 2601,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 224,
      "content": "Chapter 6\n188\nDate Dimension (views for 3 roles)\nProduct Dimension\nDeal Dimension\nShipper Dimension\nShipment Invoice Line Transaction Fact\nCustomer Dimension\nSales Rep Dimension\nWarehouse Dimension\nService Level Dimension\nInvoice Date Key (FK)\nRequested Ship Date Key (FK)\nActual Ship Date Key (FK)\nCustomer Key (FK)\nProduct Key (FK)\nSales Rep Key (FK)\nDeal Key (FK)\nWarehouse Key (FK)\nShipper Key (FK)\nService Level Key (FK)\nInvoice Number (DD)\nInvoice Line Number (DD)\nInvoice Line Quantity\nExtended Invoice Line Gross Dollar Amount\nExtended Invoice Line Allowance Dollar Amount\nExtended Invoice Line Discount Dollar Amount\nExtended Invoice Line Net Dollar Amount\nExtended Invoice Line Fixed Mfg Cost Dollar Amount\nExtended Invoice Line Variable Mfg Cost Dollar Amount\nExtended Invoice Line Storage Cost Dollar Amount\nExtended Invoice Line Distribution Cost Dollar Amount\nExtended Invoice Line Contribution Dollar Amount\nShipment On-Time Counter\nRequested to Actual Ship Lag\nFigure 6-14: Shipment invoice fact table.\nThe  shipment invoice fact table also contains some interesting new dimensions. \nThe warehouse dimension contains one row for each manufacturer warehouse loca-\ntion. This is a relatively simple dimension with name, address, contact person, and \nstorage facility type. The attributes are somewhat reminiscent of the store dimension \nfrom Chapter 3. The shipper dimension describes the method and carrier by which \nthe product was shipped from the manufacturer to the customer. \n Service Level Performance as Facts, \nDimensions, or Both\nThe  fact table in Figure 6-14 includes several critical dates intended to capture \nshipment service levels. All these dates are known when the operational invoicing \nprocess occurs. Delivering the multiple event dates in the invoicing fact table with \ncorresponding role-playing date dimensions allows business users to ﬁ lter, group, \nand trend on any of these dates. But sometimes the business requirements are more \ndemanding.\nYou could include an additional on-time counter in the fact table that’s set to an \nadditive zero or one depending on whether the line shipped on time. Likewise, you \ncould include lag metrics representing the number of days, positive or negative, \nbetween the requested and actual ship dates. As described later in this chapter, the \nlag calculation may be more sophisticated than the simple diff erence between dates.\n",
      "content_length": 2405,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 225,
      "content": "Order Management 189\nIn addition to the quantitative service metrics, you could also include a qualita-\ntive assessment of performance by adding either a new dimension or adding more \ncolumns to the junk dimension. Either way, the attribute values might look similar \nto those shown in Figure 6-15.\nOn-time\nEarly\nEarly\nEarly\nToo early\nLate\nLate\nLate\nToo late\n1 \n2 \n3 \n4 \n5 \n6\n7 \n8 \n9\nOn-time\n1 day early\n2 days early\n3 days early\n> 3 days early\n1 day late\n2 days late\n3 days late\n> 3 days late\nService Level\nKey\nService Level\nDescription\nService Level\nGroup\nFigure 6-15: Sample qualitative service level descriptors.\nIf service level performance at the invoice line is closely watched by business \nusers, you may embrace all the patterns just described, since quantitative metrics \nwith qualitative text provide diff erent perspectives on the same performance.\n Proﬁ t and Loss Facts\nIf  your organization has tackled activity-based costing or implemented a robust \nenterprise resource planning (ERP) system, you might be in a position to identify \nmany of the incremental revenues and costs associated with shipping ﬁ nished prod-\nucts to the customer. It is traditional to arrange these revenues and costs in sequence \nfrom the top line, which represents the undiscounted value of the products shipped \nto the customer, down to the bottom line, which represents the money left over after \ndiscounts, allowances, and costs. This list of revenues and costs is referred to as a \nproﬁ t and loss (P&L) statement. You typically don’t attempt to carry it all the way \nto a complete view of company proﬁ t including general and administrative costs. \nFor this reason, the bottom line in the P&L statement is referred to as contribution.\nKeeping in mind that each row in the invoice fact table represents a single line \nitem on the invoice, the elements of the P&L statement shown in Figure 6-14 have \nthe following interpretations:\n \n■Quantity shipped: Number of cases of the particular line item’s product. \nThe use of multiple equivalent quantities with diff erent units of measure is \ndiscussed in the section “Multiple Units of Measure.”\n \n■Extended gross amount:  Also known as extended list price because it is the \nquantity shipped multiplied by the list unit price. This and all subsequent \n",
      "content_length": 2294,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 226,
      "content": "Chapter 6\n190\ndollar values are extended amounts or, in other words, unit rates multiplied by \nthe quantity shipped. This insistence on additive values simpliﬁ es most access \nand reporting applications. It is relatively rare for a business user to ask for \nthe unit price from a single fact table row. When the user wants an average \nprice drawn from many rows, the extended prices are ﬁ rst added, and then \nthe result is divided by the sum of the quantities.\n \n■Extended allowance amount:  Amount subtracted from the invoice line gross \namount for deal-related allowances. The allowances are described in the \nadjoined deal dimension. The allowance amount is often called an off -invoice \nallowance. The actual invoice may have several allowances for a given line \nitem; the allowances are combined together in this simpliﬁ ed example. If \nthe allowances need to be tracked separately and there are potentially many \nsimultaneous allowances on a given line item, an allowance detail fact table \ncould augment the invoice line fact table, serving as a drill-down for details \non the allowance total in the invoice line fact table.\n \n■Extended discount amount:  Amount subtracted for volume or payment term \ndiscounts. The discount descriptions are found in the deal dimension. As \ndiscussed earlier regarding the deal dimension, the decision to describe the \nallowances and discount types together is the designer’s prerogative. It makes \nsense to do this if allowances and discounts are correlated and business users \nwant to browse within the deal dimension to study the relationships between \nallowances and discounts. \nAll allowances and discounts in this fact table are represented at the line \nitem level. As discussed earlier, some allowances and discounts may be cal-\nculated operationally at the invoice level, not at the line item level. An effort \nshould be made to allocate them down to the line item. An invoice P&L state-\nment that does not include the product dimension poses a serious limitation \non your ability to present meaningful contribution slices of the business.\n \n■Extended net amount:  Amount the customer is expected to pay for this line \nitem before tax. It is equal to the gross invoice amount less the allowances \nand discounts.\nThe facts described so far likely would be displayed to the customer on the invoice \ndocument. The following cost amounts, leading to a bottom line contribution, are \nfor internal consumption only.\n \n■Extended ﬁ xed manufacturing cost:  Amount identiﬁ ed by manufacturing as \nthe pro rata ﬁ xed manufacturing cost of the invoice line’s product.\n \n■Extended variable manufacturing cost:  Amount identiﬁ ed by manufacturing \nas the variable manufacturing cost of the product on the invoice line. This \namount may be more or less activity-based, reﬂ ecting the actual location and \n",
      "content_length": 2841,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 227,
      "content": "Order Management 191\ntime of the manufacturing run that produced the product being shipped to \nthe customer. Conversely, this number may be a standard value set by a com-\nmittee. If the manufacturing costs or any of the other storage and distribution \ncosts are averages of averages, the detailed P&Ls may become meaningless. \nThe DW/BI system may illuminate this problem and accelerate the adoption \nof activity-based costing methods.\n \n■Extended storage cost:  Cost charged to the invoice line for storage prior to \nbeing shipped to the customer.\n \n■Extended distribution cost:  Cost charged to the invoice line for transportation \nfrom the point of manufacture to the point of shipment. This cost is notori-\nous for not being activity-based. The distribution cost possibly can include \nfreight to the customer if the company pays the freight, or the freight cost \ncan be presented as a separate line item in the P&L.\n \n■Contribution amount:  Extended net invoice less all the costs just discussed. This \nis not the true bottom line of the overall company because general and admin-\nistrative expenses and other ﬁ nancial adjustments have not been made, but it is \nimportant nonetheless. This column sometimes has alternative labels, such as \nmargin, depending on the company culture.\nYou should step back and admire the robust dimensional model you just built. \nYou constructed a detailed P&L view of your business, showing all the activity-based \nelements of revenue and costs. You have a full equation of proﬁ tability. However, \nwhat makes this design so compelling is that the P&L view sits inside a rich dimen-\nsional framework of dates, customers, products, and causal factors. Do you want \nto see customer proﬁ tability? Just constrain and group on the customer dimension \nand bring the components of the P&L into the report. Do you want to see product \nproﬁ tability? Do you want to see deal proﬁ tability? All these analyses are equally \neasy and take the same analytic form in the BI applications. Somewhat tongue in \ncheek, we recommend you not deliver this dimensional model too early in your \ncareer because you will get promoted and won’t be able to work directly on any \nmore DW/BI systems!\nProﬁ tability Words of Warning\nWe  must balance the last paragraph with a more sober note and pass along some \ncautionary words of warning. It goes without saying that most of the business users \nprobably are very interested in granular P&L data that can be rolled up to analyze \ncustomer and product proﬁ tability. The reality is that delivering these detailed \nP&L statements often is easier said than done. The problems arise with the cost \nfacts. Even with advanced ERP implementations, it is fairly common to be unable \nto capture the cost facts at this atomic level of granularity. You will face a complex \nprocess of mapping or allocating the original cost data down to the invoice line \n",
      "content_length": 2904,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 228,
      "content": "Chapter 6\n192\nlevel. Furthermore, each type of cost may require a separate extraction from a \nsource system. Ten cost facts may mean 10 diff erent extract and transformation \nprograms. Before signing up for mission impossible, be certain to perform a detailed \nassessment of what is available and feasible from the source systems. You certainly \ndon’t want the DW/BI team saddled with driving the organization to consensus on \nactivity-based costing as a side project, on top of managing a number of parallel \nextract implementations. If time and organization patience permits, proﬁ tability is \noften tackled as a consolidated dimensional model after the components of revenue \nand cost have been sourced and delivered separately to business users in the DW/\nBI environment.\n Audit Dimension\nAs  mentioned, Figure 6-14’s invoice line item design is one of the most powerful \nbecause it provides a detailed look at customers, products, revenues, costs, and \nbottom line proﬁ t in one schema. During the building of rows for this fact table, \na wealth of interesting back room metadata is generated, including data quality \nindicators, unusual processing requirements, and environment version numbers \nthat identify how the data was processed during the ETL. Although this metadata is \nfrequently of interest to ETL developers and IT management, there are times when \nit can be interesting to the business users, too. For instance, business users might \nwant to ask the following:\n \n■What is my conﬁ dence in these reported numbers?\n \n■Were there any anomalous values encountered while processing this source \ndata?\n \n■What version of the cost allocation logic was used when calculating the costs?\n \n■What version of the foreign currency conversion rules was used when calcu-\nlating the revenues?\nThese kinds of questions are often hard to answer because the metadata required \nis not readily available. However, if you anticipate these kinds of questions, you can \ninclude an audit dimension with any fact table to expose the metadata context that \nwas true when the fact table rows were built. Figure 6-16 illustrates an example \naudit dimension.\nThe audit dimension is added to the fact table by including an audit dimension \nforeign key. The audit dimension itself contains the metadata conditions encountered \nwhen processing fact table rows. It is best to start with a modest audit dimension \ndesign, such as shown in Figure 6-16, both to keep the ETL processing from getting \ntoo complicated and to limit the number of possible audit dimension rows. The ﬁ rst \nthree attributes (quality indicator, out of bounds indicator, and amount adjusted ﬂ ag) \nare all sourced from a special ETL processing table called the error event table, which \n",
      "content_length": 2745,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 229,
      "content": "Order Management 193\nis discussed in Chapter 19: ETL Subsystems and Techniques. The cost allocation and \nforeign currency versions are environmental variables that should be available in an \nETL back room status table.\nDate Dimension (views for 3 roles)\nProduct Dimension\nDeal Dimension\nShipper Dimension\nShipment Invoice Line Transaction Fact\nCustomer Dimension\nSales Rep Dimension\nWarehouse Dimension\nService Level Dimension\nInvoice Date Key (FK)\nRequested Ship Date Key (FK)\nActual Ship Date Key (FK)\nCustomer Key (FK)\nProduct Key (FK)\nSales Rep Key (FK)\nDeal Key (FK)\nWarehouse Key (FK)\nShipper Key (FK)\nService Level Key (FK)\nAudit Key (FK)\nInvoice Number (DD)\nInvoice Line Number (DD)\nInvoice Line Quantity\nExtended Invoice Line Gross Dollar Amount\n...\nAudit Dimension\nAudit Key (PK)\nQuality Indicator\nOut of Bounds Indicator\nAmount Adjusted Flag\nCost Allocation Version\nForeign Currency Version\nFigure 6-16: Sample audit dimension included on invoice fact table.\nArmed with the audit dimension, some powerful queries can be performed. You \nmight want to take this morning’s invoice report and ask if any of the reported \nnumbers were based on out-of-bounds measures. Because the audit dimension is \nnow just an ordinary dimension, you can just add the out-of-bounds indicator to \nyour standard report. In the resulting “instrumented” report shown in Figure 6-17, \nyou see multiple rows showing normal and abnormal out-of-bounds results.\nStandard Report:\nAxon\nAxon\nEast\nWest\n1,438 \n2,249\n235,000 \n480,000\nProduct\nWarehouse\nInvoice Line\nQuantity\nExtended Invoice Line\nGross Amount\nInstrumented Reported (with Out of Bounds Indicator added):\nAxon\nAxon\nAxon\nAxon\nEast\nEast\nWest\nWest\nAbnormal\nNormal\nAbnormal\nNormal\n14 \n1,424 \n675 \n1,574\nProduct\nWarehouse\nOut of Bounds\nIndicator\nInvoice Line\nQuantity\n2,350 \n232,650 \n144,000 \n336,000\nExtended Invoice Line\nGross Amount\nFigure 6-17: Audit dimension attribute included on standard report.\n",
      "content_length": 1940,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 230,
      "content": "Chapter 6\n194\n Accumulating Snapshot for Order \nFulﬁ llment Pipeline\nThe  order management process can be thought of as a pipeline, especially in a \nbuild-to-order manufacturing business, as illustrated in Figure 6-18. Customers \nplace an order that goes into the backlog until it is released to manufacturing to be \nbuilt. The manufactured products are placed in ﬁ nished goods inventory and then \nshipped to the customers and invoiced. Unique transactions are generated at each \nspigot of the pipeline. Thus far we’ve considered each of these pipeline activities as \na separate transaction fact table. Doing so allows you to decorate the detailed facts \ngenerated by each process with the greatest number of detailed dimensions. It also \nallows you to isolate analysis to the performance of a single business process, which \nis often precisely what the business users want.\nOrders\nBacklog\nShipment\nInvoicing\nRelease to\nManufacturing\nFinished\nGoods\nInventory\nFigure 6-18: Order fulﬁ llment pipeline diagram.\nHowever, there are times when business users want to analyze the entire order \nfulﬁ llment pipeline. They want to better understand product velocity, or how quickly \nproducts move through the pipeline. The accumulating snapshot fact table provides \nthis perspective of the business, as illustrated in Figure 6-19. It enables you to see \nan updated status and ultimately the ﬁ nal disposition of each order.\nThe  accumulating snapshot complements alternative schemas’ perspectives of \nthe pipeline. If you’re interested in understanding the amount of product ﬂ owing \nthrough the pipeline, such as the quantity ordered, produced, or shipped, transac-\ntion schemas monitor each of the pipeline’s major events. Periodic snapshots would \nprovide insight into the amount of product sitting in the pipeline, such as the \nbackorder or ﬁ nished goods inventories, or the amount of product ﬂ owing through \na pipeline spigot during a predeﬁ ned interval. The accumulating snapshot helps \nyou better understand the current state of an order, as well as product movement \nvelocities to identify pipeline bottlenecks and ineffi  ciencies. If you only captured \nperformance in transaction event fact tables, it would be wildly diffi  cult to calculate \nthe average number of days to move between milestones.\nThe  accumulating snapshot looks different from the transaction fact tables \ndesigned thus far in this chapter. The reuse of conformed dimensions is to be \nexpected, but the number of date and fact columns is larger. Each date represents \na major milestone of the fulﬁ llment pipeline. The dates are handled as dimension \n",
      "content_length": 2626,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 231,
      "content": "Order Management 195\nroles by creating either physically distinct tables or logically distinct views. The date \ndimension needs to have a row for Unknown or To Be Determined because many of \nthese fact table dates are unknown when a pipeline row is initially loaded. Obviously, \nyou don’t need to declare all the date columns in the fact table’s primary key.\nOrder Date Key (FK)\nBacklog Date Key (FK)\nRelease to Manufacturing Date Key (FK)\nFinished Inventory Placement Date Key (FK)\nRequested Ship Date Key (FK)\nScheduled Ship Date Key (FK)\nActual Ship Date Key (FK)\nArrival Date Key (FK)\nInvoice Date Key (FK)\nProduct Key (FK)\nCustomer Key (FK)\nSales Rep Key (FK)\nDeal Key (FK)\nManufacturing Facility Key (FK)\nWarehouse Key (FK)\nShipper Key (FK)\nOrder Number (DD)\nOrder Line Number (DD)\nInvoice Number (DD)\nOrder Quantity\nExtended Order Line Dollar Amount\nRelease to Manufacturing Quantity\nManufacturing Pass Inspection Quantity\nManufacturing Fail Inspection Quantity\nFinished Goods Inventory Quantity\nAuthorized to Sell Quantity\nShipment Quantity\nShipment Damage Quantity\nCustomer Return Quantity\nInvoice Quantity\nExtended Invoice Dollar Amount\nOrder to Manufacturing Release Lag\nManufacturing Release to Inventory Lag\nInventory to Shipment Lag\nOrder to Shipment Lag\nCustomer Dimension\nShipper  Dimension\nManufacturing Facility Dimension\nProduct Dimension\nDeal Dimension\nWarehouse Dimension\nOrder Fulfillment Accumulating Fact\nSales Rep Dimension\nDate Dimension (views for 9 roles)\nFigure 6-19: Order fulﬁ llment accumulating snapshot fact table.\nThe fundamental diff erence between accumulating snapshots and other fact tables \nis that you can revisit and update existing fact table rows as more information becomes \navailable. The grain of an accumulating snapshot fact table in Figure 6-19 is one row \nper order line item. However, unlike the order transaction fact table illustrated in \nFigure 6-2 with the same granularity, accumulating snapshot fact rows are modiﬁ ed \nwhile the order moves through the pipeline as more information is collected from \nevery stage of the cycle.\n",
      "content_length": 2085,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 232,
      "content": "Chapter 6\n196\nNOTE \nAccumulating snapshot fact tables typically have multiple dates repre-\nsenting the major milestones of the process. However, just because a fact table \nhas several dates doesn’t dictate that it is an accumulating snapshot. The primary \ndiff erentiator of an accumulating snapshot is that you revisit the fact rows as \nactivity occurs.\nThe accumulating snapshot technique is especially useful when the product mov-\ning through the pipeline is uniquely identiﬁ ed, such as an automobile with a vehicle \nidentiﬁ cation number, electronics equipment with a serial number, lab specimens \nwith an identiﬁ cation number, or process manufacturing batches with a lot num-\nber. The accumulating snapshot helps you understand throughput and yield. If the \ngranularity of an accumulating snapshot is at the serial or lot number, you can see \nthe disposition of a discrete product as it moves through the manufacturing and test \npipeline. The accumulating snapshot ﬁ ts most naturally with short-lived processes \nwith a deﬁ nite beginning and end. Long-lived processes, such as bank accounts, \nare typically better modeled with periodic snapshot fact tables.\nAccumulating Snapshots and Type 2 Dimensions\nAccumulating  snapshots present the latest state of a workﬂ ow or pipeline. If the \ndimensions associated with an accumulating snapshot contain type 2 attributes, \nthe fact table should be updated to reference the most current surrogate dimension \nkey for active pipelines. When a single fact table pipeline row is complete, the row \nis typically not revisited to reﬂ ect future type 2 changes.\n Lag Calculations\nThe  lengthy list of date columns captures the spans of time over which the order is \nprocessed through the fulﬁ llment pipeline. The numerical diff erence between any \ntwo of these dates is a number that can be usefully averaged over all the dimensions. \nThese date lag calculations represent basic measures of fulﬁ llment effi  ciency. You \ncould build a view on this fact table that calculated a large number of these date \ndiff erences and presented them as if they were stored in the underlying table. These \nview columns could include metrics such as orders to manufacturing release lag, \nmanufacturing release to ﬁ nished goods lag, and order to shipment lag, depending \non the date spans monitored by the organization.\nRather than calculating a simple diff erence between two dates via a view, the \nETL system may calculate elapsed times that incorporate more intelligence, such \nas workday lags that account for weekends and holidays rather than just the raw \nnumber of days between milestone dates. The lag metrics may also be calculated \nby the ETL system at a lower level of granularity (such as the number of hours or \n",
      "content_length": 2756,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 233,
      "content": "Order Management 197\nminutes between milestone events based on operational timestamps) for short-lived \nand closely monitored processes.\n Multiple Units of Measure\nSometimes,  diff erent functional organizations within the business want to see the \nsame performance metrics expressed in diff erent units of measure. For instance, \nmanufacturing managers may want to see the product ﬂ ow in terms of pallets or \nshipping cases. Sales and marketing managers, on the other hand, may want to see \nthe quantities in retail cases, scan units (sales packs), or equivalized consumer units \n(such as individual cans of soda).\nDesigners are tempted to bury the unit-of-measure conversion factors, such as \nship case factor, in the product dimension. Business users are then required to \nappropriately multiply (or was it divide?) the order quantity by the conversion factor. \nObviously, this approach places a burden on users, in addition to being susceptible \nto calculation errors. The situation is further complicated because the conversion \nfactors may change over time, so users would also need to determine which factor \nis applicable at a speciﬁ c point in time.\nRather than risk miscalculating the equivalent quantities by placing conversion \nfactors in a dimension table, they should be stored in the fact table instead. In the \norders pipeline fact table, assume you have 10 basic fundamental quantity facts, in \naddition to ﬁ ve units of measure. If you physically store all the facts expressed in \nthe diff erent units of measure, you end up with 50 (10 × 5) facts in each fact row. \nInstead, you can compromise by building an underlying physical row with 10 quan-\ntity facts and 4 unit-of-measure conversion factors. You need only four conversion \nfactors rather than ﬁ ve because the base facts are already expressed in one of the \nunits of measure. The physical design now has 14 quantity-related facts (10 + 4), as \nshown in Figure 6-20. With this design, you can see performance across the value \nchain based on diff erent units of measure.\nOf course, you would deliver this fact table to the business users through one \nor more views. The extra computation involved in multiplying quantities by con-\nversion factors is negligible; intra-row computations are very effi  cient. The most \ncomprehensive view could show all 50 facts expressed in every unit of measure, \nbut the view could be simpliﬁ ed to deliver only a subset of the quantities in units \nof measure relevant to a user. Obviously, each unit of measures’ metrics should be \nuniquely labeled.\nNOTE \nPackaging all the facts and conversion factors together in the same fact \ntable row provides the safest guarantee that these factors will be used correctly. \nThe converted facts are presented in a view(s) to the users.\n",
      "content_length": 2787,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 234,
      "content": "Chapter 6\n198\nDate Keys (FKs)\nProduct Key (FK)\nMore FKs...\nOrder Quantity Shipping Cases\nRelease to Manufacturing Quantity Shipping Cases\nManufacturing Pass Inspection Quantity Shipping Cases\nManufacturing Fail Inspection Quantity Shipping Cases\nFinished Goods Inventory Quantity Shipping Cases\nAuthorized to Sell Quantity Shipping Cases\nShipment Quantity Shipping Cases\nShipment Damage Quantity Shipping Cases\nCustomer Return Quantity Shipping Cases\nInvoice Quantity Shipping Cases\nPallet Conversion Factor\nRetail Cases Conversion Factor\nScan Units Conversion Factor\nEquivalized Consumer Units Conversion Factor\nOrder Fulfillment Accumulating Fact\nFigure 6-20: Physical fact table supporting multiple units of measure with conversion \nfactors.\nFinally, another side beneﬁ t of storing these factors in the fact table is it reduces \nthe pressure on the product dimension table to issue new product rows to reﬂ ect \nminor conversion factor modiﬁ cations. These factors, especially if they evolve rou-\ntinely over time, behave more like facts than dimension attributes.\nBeyond the Rearview Mirror\nMuch  of what we’ve discussed in this chapter focuses on eff ective ways to analyze \nhistorical product movement performance. People sometimes refer to these as rear-\nview mirror metrics because they enable you to look backward and see where you’ve \nbeen. As the brokerage industry reminds people, past performance is no guarantee of \nfuture results. Many organizations want to supplement these historical performance \nmetrics with facts from other processes to help project what lies ahead. For example, \nrather than focusing on the pipeline at the time an order is received, organizations \nare analyzing the key drivers impacting the creation of an order. In a sales organiza-\ntion, drivers such as prospecting or quoting activity can be extrapolated to provide \nvisibility to the expected order activity volume. Many organizations do a better job \ncollecting the rearview mirror information than they do the early indicators. As these \nfront window leading indicators are captured, they can be added gracefully to the \nDW/BI environment. They’re just more rows on the enterprise data warehouse bus \nmatrix sharing common dimensions.\n",
      "content_length": 2231,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 235,
      "content": "Order Management 199\nSummary\nThis chapter covered a lengthy laundry list of topics in the context of the order \nmanagement process. Multiples were discussed on several fronts: multiple references \nto the same dimension in a fact table (role-playing dimensions), multiple equivalent \nunits of measure, and multiple currencies. We explored several of the common chal-\nlenges encountered when modeling header/line transaction data, including facts at \ndiff erent levels of granularity and junk dimensions, plus design patterns to avoid. \nWe also explored the rich set of facts associated with invoice transactions. Finally, \nthe order fulﬁ llment pipeline illustrated the power of accumulating snapshot fact \ntables where you can see the updated status of a speciﬁ c product or order as it moves \nthrough a ﬁ nite pi peline.\n",
      "content_length": 822,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 237,
      "content": "Accounting\nF\ninancial analysis spans a variety of accounting applications, including the gen-\neral ledger, as well as detailed subledgers for purchasing and accounts payable, \ninvoicing and accounts receivable, and fixed assets. Because we’ve already touched \nupon purchase orders and invoices earlier in this book, we’ll focus on the general \nledger in this chapter. Given the need for accurate handling of a company’s financial \nrecords, general ledgers were one of the first applications to be computerized decades \nago. Perhaps some of you are still running your business on a 20-year-old ledger \nsystem. In this chapter, we’ll discuss the data collected by the general ledger, both \nin terms of journal entry transactions and snapshots at the close of an accounting \nperiod. We’ll also talk about the budgeting process.\nChapter 7 discusses the following concepts:\n \n■Bus matrix snippet for accounting processes\n \n■General ledger periodic snapshots and journal transactions\n \n■Chart of accounts\n \n■Period close\n \n■Year-to-date facts\n \n■Multiple ﬁ scal accounting calendars\n \n■Drilling down through a multi-ledger hierarchy\n \n■Budgeting chain and associated processes \n \n■Fixed depth position hierarchies\n \n■Slightly ragged, variable depth hierarchies\n \n■Totally ragged hierarchies of indeterminate depth using a bridge table and \nalternative modeling techniques\n \n■Shared ownership in a ragged hierarchy\n \n■Time varying ragged hierarchies\n \n■Consolidated fact tables that combine metrics from multiple business processes\n \n■Role of OLAP and packaged analytic ﬁ nancial solutions\n7\n",
      "content_length": 1585,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 238,
      "content": "Chapter 7\n202\nAccounting Case Study and Bus Matrix\nBecause  ﬁ nance was an early adopter of technology, it comes as no surprise that \nearly decision support solutions focused on the analysis of ﬁ nancial data. Financial \nanalysts are some of the most data-literate and spreadsheet-savvy individuals. Often \ntheir analysis is disseminated or leveraged by many others in the organization. \nManagers at all levels need timely access to key ﬁ nancial metrics. In addition to \nreceiving standard reports, they need the ability to analyze performance trends, vari-\nances, and anomalies with relative speed and minimal eff ort. Like many operational \nsource systems, the data in the general ledger is likely scattered among hundreds of \ntables. Gaining access to ﬁ nancial data and/or creating ad hoc reports may require \na decoder ring to navigate through the maze of screens. This runs counter to many \norganizations’ objective to push ﬁ scal responsibility and accountability to the line \nmanagers.\nThe DW/BI system can provide a single source of usable, understandable ﬁ nan-\ncial information, ensuring everyone is working off  the same data with common \ndeﬁ nitions and common tools. The audience for ﬁ nancial data is quite diverse in \nmany organizations, ranging from analysts to operational managers to executives. \nFor each group, you need to determine which subset of corporate ﬁ nancial data is \nneeded, in which format, and with what frequency. Analysts and managers want to \nview information at a high level and then drill to the journal entries for more detail. \nFor executives, ﬁ nancial data from the DW/BI system often feeds their dashboard or \nscorecard of key performance indicators. Armed with direct access to information, \nmanagers can obtain answers to questions more readily than when forced to work \nthrough a middleman. Meanwhile, ﬁ nance can turn their attention to information \ndissemination and value-added analysis, rather than focusing on report creation.\nImproved  access to accounting data allows you to focus on opportunities to better \nmanage risk, streamline operations, and identify potential cost savings. Although it \nhas cross-organization impact, many businesses focus their initial DW/BI implemen-\ntation on strategic, revenue-generating opportunities. Consequently, accounting data \nis often not the ﬁ rst subject area tackled by the DW/BI team. Given its proﬁ ciency \nwith technology, the ﬁ nance department has often already performed magic with \nspreadsheets and desktop databases to create workaround analytic solutions, per-\nhaps to its short-term detriment, as these imperfect interim ﬁ xes are likely stressed \nto their limits.\nFigure 7-1 illustrates an accounting-focused excerpt from an organization’s bus \nmatrix. The dimensions associated with accounting processes, such as the general \nledger account or organizational cost center, are frequently used solely by these \nprocesses, unlike the core customer, product, and employee dimensions which are \nused repeatedly across many diverse business processes.\n",
      "content_length": 3054,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 239,
      "content": "Accounting 203\nGeneral Ledger Transactions\nGeneral Ledger Snapshot\nBudget\nCommitment\nPayments\nActual-Budget Variance\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nDate\nLedger\nAccount\nOrganization\nBudget Line\nCommitment\nProfile\nPayment\nProfile\nFigure 7-1: Bus matrix rows for accounting processes.\nGeneral Ledger Data\nThe general ledger (G/L) is a core foundation ﬁ nancial system that ties together the \ndetailed information collected by subledgers or separate systems for purchasing, \npayables (what you owe to others), and receivables (what others owe you). As we \nwork through a basic design for G/L data, you’ll discover the need for two comple-\nmentary schemas with periodic snapshot and transaction fact tables.\nGeneral Ledger Periodic Snapshot\nWe’ll  begin by delving into a snapshot of the general ledger accounts at the end of \neach ﬁ scal period (or month if the ﬁ scal accounting periods align with calendar \nmonths). Referring back to our four-step process for designing dimensional models \n(see Chapter 3: Retail Sales), the business process is the general ledger. The grain \nof this periodic snapshot is one row per accounting period for the most granular \nlevel in the general ledger’s chart of accounts.\nChart of Accounts\nThe  cornerstone of the general ledger is the chart of accounts. The ledger’s chart of \naccounts is the epitome of an intelligent key because it usually consists of a series of \nidentiﬁ ers. For example, the ﬁ rst set of digits may identify the account, account type \n(for example, asset, liability, equity, income, or expense), and other account rollups. \nSometimes intelligence is embedded in the account numbering scheme. For example, \naccount numbers from 1,000 through 1,999 might be asset accounts, whereas account \nnumbers ranging from 2,000 to 2,999 may identify liabilities. Obviously, in the data \n",
      "content_length": 1860,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 240,
      "content": "Chapter 7\n204\nwarehouse, you’d include the account type as a dimension attribute rather than forc-\ning users to ﬁ lter on the ﬁ rst digit of the account number.\nThe chart of accounts likely associates the organization cost center with the \naccount. Typically, the organization attributes provide a complete rollup from cost \ncenter to department to division, for example. If the corporate general ledger com-\nbines data across multiple business units, the chart of accounts would also indicate \nthe business unit or subsidiary company.\nObviously, charts of accounts vary from organization to organization. They’re \noften extremely complicated, with hundreds or even thousands of cost centers in \nlarge organizations. In this case study vignette, the chart of accounts naturally \ndecomposes into two dimensions. One dimension represents accounts in the general \nledger, whereas the other represents the organization rollup.\nThe organization rollup may be a ﬁ xed depth hierarchy, which would be handled \nas separate hierarchical attributes in the cost center dimension. If the organization \nhierarchy is ragged with an unbalanced rollup structure, you need the more power-\nful variable depth hierarchy techniques described in the section “Ragged Variable \nDepth Hierarchies.”\nIf  you are tasked with building a comprehensive general ledger spanning multiple \norganizations in the DW/BI system, you should try to conform the chart of accounts \nso the account types mean the same thing across organizations. At the data level, \nthis means the master conformed account dimension contains carefully deﬁ ned \naccount names. Capital Expenditures and Offi  ce Supplies need to have the same \nﬁ nancial meaning across organizations. Of course, this kind of conformed dimen-\nsion has an old and familiar name in ﬁ nancial circles: the uniform chart of accounts.\nThe G/L sometimes tracks ﬁ nancial results for multiple sets of books or sub-\nledgers to support diff erent requirements, such as taxation or regulatory agency \nreporting. You can treat this as a separate dimension because it’s such a fundamen-\ntal ﬁ lter, but we alert you to carefully read the cautionary note in the next section.\n Period Close\nAt  the end of each accounting period, the ﬁ nance organization is responsible for \nﬁ nalizing the ﬁ nancial results so that they can be offi  cially reported internally \nand externally. It typically takes several days at the end of each period to recon-\ncile and balance the books before they can be closed with ﬁ nance’s offi  cial stamp \nof approval. From there, ﬁ nance’s focus turns to reporting and interpreting the \nresults. It often produces countless reports and responds to countless variations \non the same questions each month.\nFinancial analysts are constantly looking to streamline the processes for period-\nend closing, reconciliation, and reporting of general ledger results. Although \n",
      "content_length": 2901,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 241,
      "content": "Accounting 205\noperational general ledger systems often support these requisite capabilities, they \nmay be cumbersome, especially if you’re not dealing with a modern G/L. This chap-\nter focuses on easily analyzing the closed ﬁ nancial results, rather than facilitating \nthe close. However, in many organizations, general ledger trial balances are loaded \ninto the DW/BI system leveraging the capabilities of the DW/BI presentation area \nto ﬁ nd the needles in the general ledger haystack, and then making the appropriate \noperational adjustments before the period ends.\nThe sample schema in Figure 7-2 shows general ledger account balances at the \nend of each accounting period which would be very useful for many kinds of ﬁ nan-\ncial analyses, such as account rankings, trending patterns, and period-to-period \ncomparisons.\nAccounting Period Key (FK)\nLedger Key (FK)\nAccount Key (FK)\nOrganization Key (FK)\nPeriod End Balance Amount\nPeriod Debit Amount\nPeriod Credit Amount\nPeriod Net Change Amount\nAccount Key (PK)\nAccount Name\nAccount Category\nAccount Type\nAccounting Period Key (PK)\nAccounting Period Number\nAccounting Period Description\nAccounting Period Fiscal Year\nLedger Key (PK)\nLedger Book Name\nOrganization Key (PK)\nCost Center Name\nCost Center Number\nDepartment Name\nDepartment Number\nDivision Name\nBusiness Unit Name\nCompany Name\nAccounting Period Dimension\nGeneral Ledger Snapshot Fact\nLedger Dimension\nOrganization Dimension\nAccount Dimension\nFigure 7-2: General ledger periodic snapshot.\nFor the moment, we’re just representing actual ledger facts in the Figure 7-2 \nschema; we’ll expand our view to cover budget data in the section “Budgeting \nProcess.” In this table, the balance amount is a semi-additive fact. Although the \nbalance doesn’t represent G/L activity, we include the fact in the design because \nit is so useful. Otherwise, you would need to go back to the beginning of time to \ncalculate an accurate end-of-period balance.\nWARNING \nThe ledger dimension is a convenient and intuitive dimension \nthat enables multiple ledgers to be stored in the same fact table. However, every \nquery that accesses this fact table must constrain the ledger dimension to a single \nvalue (for example, Final Approved Domestic Ledger) or the queries will double \ncount values from the various ledgers in this table. The best way to deploy this \nschema is to release separate views to the business users with the ledger dimension \npre-constrained to a single value.\n",
      "content_length": 2475,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 242,
      "content": "Chapter 7\n206\nThe two most important dimensions in the proposed general ledger design are \naccount and organization. The account dimension is carefully derived from the \nuniform chart of accounts in the enterprise. The organization dimension describes \nthe ﬁ nancial reporting entities in the enterprise. Unfortunately, these two crucial \ndimensions almost never conform to operational dimensions such as customer, \nproduct, service, or facility. This leads to a characteristic but unavoidable business \nuser frustration that the “GL doesn’t tie to my operational reports.” It is best to gently \nexplain this to the business users in the interview process, rather than promising \nto ﬁ x it because this is a deep seated issue in the underlying data.\n Year-to-Date Facts\nDesigners  are often tempted to store “to-date” columns in fact tables. They think \nit would be helpful to store quarter-to-date or year-to-date additive totals on each \nfact row so they don’t need to calculate them. Remember that numeric facts must \nbe consistent with the grain. To-date facts are not true to the grain and are fraught \nwith peril. When fact rows are queried and summarized in arbitrary ways, these \nuntrue-to-the-grain facts produce nonsensical, overstated results. They should be \nleft out of the relational schema design and calculated in the BI reporting application \ninstead. It’s worth noting that OLAP cubes handle to-date metrics more gracefully.\nNOTE \nIn general, “to-date” totals should be calculated, not stored in the \nfact table.\n Multiple Currencies Revisited\nIf  the general ledger consolidates data that has been captured in multiple curren-\ncies, you would handle it much as we discussed in Chapter 6: Order Management. \nWith ﬁ nancial data, you typically want to represent the facts both in terms of the \nlocal currency, as well as a standardized corporate currency. In this case, each \nfact table row would represent one set of fact amounts expressed in local currency \nand a separate set of fact amounts on the same row expressed in the equivalent \ncorporate currency. Doing so allows you to easily summarize the facts in a com-\nmon corporate currency without jumping through hoops in the BI applications. \nOf course, you’d also add a currency dimension as a foreign key in the fact table \nto identify the local currency  type.\n General Ledger Journal Transactions\nWhile  the end-of-period snapshot addresses a multitude of ﬁ nancial analyses, many \nusers need to dive into the underlying details. If an anomaly is identiﬁ ed at the \n",
      "content_length": 2542,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 243,
      "content": "Accounting 207\nsummary level, analysts want to look at the detailed transactions to sort through \nthe issue. Others need access to the details because the summarized monthly bal-\nances may obscure large disparities at the granular transaction level. Again, you \ncan complement the periodic snapshot with a detailed journal entry transaction \nschema. Of course, the accounts payable and receivable subledgers may contain \ntransactions at progressively lower levels of detail, which would be captured in \nseparate fact tables with additional dimensionality.\nThe grain of the fact table is now one row for every general ledger journal entry \ntransaction. The journal entry transaction identiﬁ es the G/L account and the appli-\ncable debit or credit amount. As illustrated in Figure 7-3, several dimensions from \nthe last schema are reused, including the account and organization. If the ledger \ntracks multiple sets of books, you’d also include the ledger/book dimension. You \nwould normally capture journal entry transactions by transaction posting date, \nso use a daily-grained date table in this schema. Depending on the business rules \nassociated with the source data, you may need a second role-playing date dimension \nto distinguish the posting date from the eff ective accounting date.\nPost Date Key (FK)\nJournal Entry Effective Date/Time\nLedger Key (FK)\nAccount Key (FK)\nOrganization Key (FK)\nDebit-Credit Indicator Key (FK)\nJournal Entry Number (DD)\nJournal Entry Amount\nDebit-Credit Indicator Key (PK)\nDebit-Credit Indicator Description\nGeneral Ledger Journal Entry Fact\nLedger Dimension\nOrganization Dimension\nPost Date Dimension\nAccount Dimension\nDebit-Credit Indicator Dimension\nFigure 7-3: General ledger journal entry transactions.\nThe journal entry number is likely a degenerate dimension with no linkage to \nan associated dimension table. If the journal entry numbers from the source are \nordered, then this degenerate dimension can be used to order the journal entries \nbecause the calendar date dimension on this fact table is too coarse to provide this \nsorting. If the journal entry numbers do not easily support the sort, then an eff ective \ndate/time stamp must be added to the fact table. Depending on the source data, you \nmay have a journal entry transaction type and even a description. In this situation, \nyou would create a separate journal entry transaction proﬁ le dimension (not shown). \nAssuming the descriptions are not just freeform text, this dimension would have \nsigniﬁ cantly fewer rows than the fact table, which would have one row per journal \nentry line. The speciﬁ c journal entry number would still be treated as degenerate.\nEach row in the journal entry fact table is identiﬁ ed as either a credit or a debit. \nThe debit/credit indicator takes on two, and only two, values.\n",
      "content_length": 2816,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 244,
      "content": "Chapter 7\n208\n Multiple Fiscal Accounting Calendars\nIn Figure 7-3, the data is captured by posting date, but users may also want to \nsummarize the data by ﬁ scal account period. Unfortunately, ﬁ scal accounting peri-\nods often do not align with standard Gregorian calendar months. For example, a \ncompany may have 13 4-week accounting periods in a ﬁ scal year that begins on \nSeptember 1 rather than 12 monthly periods beginning on January 1. If you deal \nwith a single ﬁ scal calendar, then each day in a year corresponds to a single calendar \nmonth, as well as a single accounting period. Given these relationships, the calendar \nand accounting periods are merely hierarchical attributes on the daily date dimen-\nsion. The daily date dimension table would simultaneously conform to a calendar \nmonth dimension table, as well as to a ﬁ scal accounting period dimension table.\nIn other situations, you may deal with multiple ﬁ scal accounting calendars that \nvary by subsidiary or line of business. If the number of unique ﬁ scal calendars is a \nﬁ xed, low number, then you can include each set of uniquely labeled ﬁ scal calendar \nattributes on a single date dimension. A given row in the daily date dimension would \nbe identiﬁ ed as belonging to accounting period 1 for subsidiary A but accounting \nperiod 7 for subsidiary B.\nIn a more complex situation with a large number of diff erent ﬁ scal calendars, \nyou could identify the offi  cial corporate ﬁ scal calendar in the date dimension. You \nthen have several options to address the subsidiary-speciﬁ c ﬁ scal calendars. The \nmost common approach is to create a date dimension outrigger with a multipart key \nconsisting of the date and subsidiary keys. There would be one row in this table for \neach day for each subsidiary. The attributes in this outrigger would consist of ﬁ s-\ncal groupings (such as ﬁ scal week end date and ﬁ scal period end date). You would \nneed a mechanism for ﬁ ltering on a speciﬁ c subsidiary in the outrigger. Doing so \nthrough a view would then allow the outrigger to be presented as if it were logically \npart of the date dimension table. \nA  second approach for tackling the subsidiary-speciﬁ c calendars would be to \ncreate separate physical date dimensions for each subsidiary calendar, using a \ncommon set of surrogate date keys. This option would likely be used if the fact \ndata were decentralized by subsidiary. Depending on the BI tool’s capabilities, it \nmay be easier to either ﬁ lter on the subsidiary outrigger as described in option \n1 or ensure usage of the appropriate subsidiary-speciﬁ c physical date dimension \ntable (option 2). Finally, you could allocate another foreign key in the fact table to \na subsidiary ﬁ scal period dimension table. The number of rows in this table would \nbe the number of ﬁ scal periods (approximately 36 for 3 years) times the number of \nunique calendars. This approach simpliﬁ es user access but puts additional strain \non the ETL system because it must insert the appropriate ﬁ scal period key during \nthe transformation  process.\n",
      "content_length": 3065,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 245,
      "content": "Accounting 209\nDrilling Down Through a Multilevel Hierarchy\nVery  large enterprises or government agencies may have multiple ledgers arranged \nin an ascending hierarchy, perhaps by enterprise, division, and department. At the \nlowest level, department ledger entries may be consolidated to roll up to a single \ndivision ledger entry. Then the division ledger entries may be consolidated to the \nenterprise level. This would be particularly common for the periodic snapshot grain \nof these ledgers. One way to model this hierarchy is by introducing the parent snap-\nshot’s fact table surrogate key in the fact table, as shown in Figure 7-4. In this case, \nbecause you deﬁ ne a parent/child relationship between rows, you add an explicit \nfact table surrogate key, a single column numeric identiﬁ er incremented as you add \nrows to the fact table.\nFact Table Surrogate Key (PK)\nAccounting Period Key (FK)\nLedger Key (FK)\nAccount Key (FK)\nOrganization Key (FK)\nParent Snapshot Key (FK)\nPeriod End Balance Amount\nPeriod Debit Amount\nPeriod Credit Amount\nPeriod Net Change Amount\nGeneral Ledger Snapshot Fact\nLedger Dimension\nOrganization Dimension\nAccounting Period Dimension\nAccount Dimension\nFigure 7-4: Design for drilling down through multiple ledgers.\nYou can use the parent snapshot surrogate key to drill down in your multilayer \ngeneral ledger. Suppose that you detect a large travel amount at the top level of the \nledger. You grab the surrogate key for that high-level entry and then fetch all the entries \nwhose parent snapshot key equals that key. This exposes the entries at the next lower \nlevel that contribute to the original high-level record of interest. The SQL would look \nsomething like  this:\nSelect * from GL_Fact where Parent_Snapshot_key =\n  (select fact_table_surrogate_key from GL_Fact f, Account a\n   where <joins> and a.Account = 'Travel' and f.Amount > 1000)\nFinancial Statements\nOne  of the primary functions of a general ledger system is to produce the organiza-\ntion’s offi  cial ﬁ nancial reports, such as the balance sheet and income statement. The \noperational system typically handles the production of these reports. You wouldn’t \nwant the DW/BI system to attempt to replace the reports published by the opera-\ntional ﬁ nancial systems.\n",
      "content_length": 2271,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 246,
      "content": "Chapter 7\n210\nHowever, DW/BI teams sometimes create complementary aggregated data that \nprovides simpliﬁ ed access to report information that can be more widely dissemi-\nnated throughout the organization. Dimensions in the ﬁ nancial statement schema \nwould include the accounting period and cost center. Rather than looking at general \nledger account level data, the fact data would be aggregated and tagged with the \nappropriate ﬁ nancial statement line number and label. In this manner, managers \ncould easily look at performance trends for a given line in the ﬁ nancial statement \nover time for their organization. Similarly, key performance indicators and ﬁ nancial \nratios may be made available at the same level of detail.\n Budgeting Process\nMost  modern general ledger systems include the capability to integrate budget data \ninto the general ledger. However, if the G/L either lacks this capability or it has not \nbeen implemented, you need to provide an alternative mechanism for supporting \nthe budgeting process and variance comparisons.\nWithin most organizations, the budgeting process can be viewed as a series of \nevents. Prior to the start of a ﬁ scal year, each cost center manager typically creates a \nbudget, broken down by budget line items, which is then approved. In reality, bud-\ngeting is seldom simply a once-per-year event. Budgets are becoming more dynamic \nbecause there are budget adjustments as the year progresses, reﬂ ecting changes in \nbusiness conditions or the realities of actual spending versus the original budget. \nManagers want to see the current budget’s status, as well as how the budget has \nbeen altered since the ﬁ rst approved version. As the year unfolds, commitments to \nspend the budgeted monies are made. Finally, payments are processed.\nAs a dimensional modeler, you can view the budgeting chain as a series of fact \ntables, as shown in Figure 7-5. This chain consists of a budget fact table, commit-\nments fact table, and payments fact table, where there is a logical ﬂ ow that starts \nwith a budget being established for each organization and each account. Then dur-\ning the operational period, commitments are made against the budgets, and ﬁ nally \npayments are made against those commitments.\nWe’ll  begin with the budget fact table. For an expense budget line item, each row \nidentiﬁ es what an organization in the company is allowed to spend for what purpose \nduring a given time frame. Similarly, if the line item reﬂ ects an income forecast, \nwhich is just another variation of a budget, it would identify what an organization \nintends to earn from what source during a time frame.\nYou  could further identify the grain to be a snapshot of the current status of \neach line item in each budget each month. Although this grain has a familiar ring \nto it (because it feels like a management report), it is a poor choice as the fact table \n",
      "content_length": 2894,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 247,
      "content": "Accounting 211\ngrain. The facts in such a “status report” are all semi-additive balances, rather than \nfully additive facts. Also, this grain makes it diffi  cult to determine how much has \nchanged since the previous month or quarter because you must obtain the rows \nfrom several time periods and then subtract them from each other. Finally, this grain \nchoice would require the fact table to contain many duplicated rows when nothing \nchanges in successive months for a given line item.\nMonth Key (FK)\nOrganization Key (FK)\nAccount Key (FK)\nBudget Key (FK)\nBudget Amount\nMonth Key (FK)\nOrganization Key (FK)\nAccount Key (FK)\nBudget Key (FK)\nCommitment Key (FK)\nCommitment Amount\nMonth Key (FK)\nOrganization Key (FK)\nAccount Key (FK)\nBudget Key (FK)\nCommitment Key (FK)\nPayment Key (FK)\nPayment Amount\nBudget Key (PK)\nBudget Name\nBudget Version\nBudget Approval Date\nCommitment Key (PK)\nCommitment Description\nCommitment Party\nPayment Key (PK)\nPayment Description\nPayment Party\nOrganization Dimension\nBudget Dimension\nOrganization Dimension\nBudget Dimension\nOrganization Dimension\nBudget Dimension\nPayment Dimension\nPayment Fact\nAccount Dimension\nMonth Dimension\nAccount Dimension\nCommitment Dimension\nMonth Dimension\nAccount Dimension\nCommitment Dimension\nMonth Dimension\nBudget Fact\nCommitment Fact\nFigure 7-5: Chain of budget processes.\nInstead, the grain you’re interested in is the net change of the budget line item \nin an organizational cost center that occurred during the month. Although this \nsuffi  ces for budget reporting purposes, the accountants eventually need to tie the \nbudget line item back to a speciﬁ c general ledger account that’s aff ected, so you’ll \nalso go down to the G/L account level.\n",
      "content_length": 1716,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 248,
      "content": "Chapter 7\n212\nGiven the grain, the associated budget dimensions would include eff ective \nmonth, organization cost center, budget line item, and G/L account, as illustrated \nin Figure 7-6. The organization is identical to the dimension used earlier with the \ngeneral ledger data. The account dimension is also a reused dimension. The only \ncomplication regarding the account dimension is that sometimes a single budget \nline item impacts more than one G/L account. In that case, you would need to \nallocate the budget line to the individual G/L accounts. Because the grain of the \nbudget fact table is by G/L account, a single budget line for a cost center may be \nrepresented as several rows in the fact table.\nEffective Date Dimension\nAccount Dimension\nOrganization Dimension\nBudget Line Item Dimension\nBudget Effective Date Key (PK)\nBudget Effective Date Month\nBudget Effective Date Year\n...\nBudget Effective Date Key (FK)\nBudget Line Item Key (FK)\nAccount Key (FK)\nOrganization Key (FK)\nBudget Amount\nBudget Line Item Key (PK)\nBudget Name\nBudget Version\nBudget Line Description\nBudget Year\nBudget Line Subcategory Description\nBudget Line Category Description\nBudget Fact\nFigure 7-6: Budget schema.\nThe budget line item identiﬁ es the purpose of the proposed spending, such as \nemployee wages or offi  ce supplies. There are typically several levels of summariza-\ntion categories associated with a budget line item. All the budget line items may not \nhave the same number of levels in their summarization hierarchy, such as when some \nonly have a category rollup, but not a subcategory. In this case, you may populate the \ndimension attributes by replicating the category name in the subcategory column to \navoid having line items roll up to a Not Applicable subcategory bucket. The budget \nline item dimension would also identify the budget year and/or budget version.\nThe eff ective month is the month during which the budget changes are posted. \nThe ﬁ rst entries for a given budget year would show the eff ective month when the \nbudget is ﬁ rst approved. If the budget is updated or modiﬁ ed as the budget year \ngets underway, the eff ective months would occur during the budget year. If you \ndon’t adjust a budget throughout the year, then the only entries would be the ﬁ rst \nones when the budget is initially approved. This is what is meant when the grain \nis speciﬁ ed to be the net change. It’s critical that you understand this point, or you \nwon’t understand what is in this budget fact table or how it’s used.\nSometimes budgets are created as annual spending plans; other times, they’re \nbroken down by month or quarter. Figure 7-6 assumes the budget is an annual \namount, with the budget year identiﬁ ed in the budget line item dimension. If you \nneed to express the budget data by spending month, you would need to include a \nsecond month dimension table that plays the role of spending month.\n",
      "content_length": 2911,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 249,
      "content": "Accounting 213\nThe budget fact table has a single budget amount fact that is fully additive. If you \nbudget for a multinational organization, the budget amount may be tagged with the \nexpected currency conversion factor for planning purposes. If the budget amount \nfor a given budget line and account is modiﬁ ed during the year, an additional row \nis added to the budget fact table representing the net change. For example, if the \noriginal budget were $200,000, you might have another row in June for a $40,000 \nincrease and then another in October for a negative $25,000 as you tighten your \nbelt going into year-end.\nWhen the budget year begins, managers make commitments to spend the budget \nthrough purchase orders, work orders, or other forms of contracts. Managers are \nkeenly interested in monitoring their commitments and comparing them to the \nannual budget to manage their spending. We can envision a second fact table for \nthe commitments (refer to Figure 7-5) that shares the same dimensions, in addi-\ntion to dimensions identifying the speciﬁ c commitment document (purchase order, \nwork order, or contract) and commitment party. In this case, the fact would be the \ncommitted amount.\nFinally, payments are made as monies are transferred to the party named in the \ncommitment. From a practical point of view, the money is no longer available in \nthe budget when the commitment is made. But the ﬁ nance department is interested \nin the relationship between commitments and payments because it manages the \ncompany’s cash. The dimensions associated with the payments fact table would \ninclude the commitment fact table dimensions, plus a payment dimension to identify \nthe type of payment, as well as the payee to whom the payment was actually made. \nReferring the budgeting chain shown in Figure 7-5, the list of dimensions expands \nas you move from the budget to commitments to payments.\nWith this design, you can create a number of interesting analyses. To look at \nthe current budgeted amount by department and line item, you can constrain \non all dates up to the present, adding the amounts by department and line item. \nBecause the grain is the net change of the line items, adding up all the entries \nover time does exactly the right thing. You end up with the current approved \nbudget amount, and you get exactly those line items in the given departments \nthat have a budget.\nTo ask for all the changes to the budget for various line items, simply constrain \non a single month. You’ll report only those line items that experienced a change \nduring the month.\nTo compare current commitments to the current budget, separately sum the \ncommitment amounts and budget amounts from the beginning of time to the cur-\nrent date (or any date of interest). Then combine the two answer sets on the row \nheaders. This is a standard drill-across application using multipass SQL. Similarly, \nyou could drill across commitments and payments.\n",
      "content_length": 2948,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 250,
      "content": "Chapter 7\n214\nDimension Attribute Hierarchies\nAlthough the budget chain use case described in this chapter is reasonably simple, \nit contains a number of hierarchies, along with a number of choices for the designer. \nRemember a hierarchy is deﬁ ned by a series of many-to-one relationships. You likely \nhave at least four hierarchies: calendar levels, account levels, geographic levels, and \norganization levels.\n Fixed Depth Positional Hierarchies\nIn  the budget chain, the calendar levels are familiar ﬁ xed depth position hierarchies. \nAs the name suggests, a ﬁ xed position hierarchy has a ﬁ xed set of levels, all with \nmeaningful labels. Think of these levels as rollups. One calendar hierarchy may \nbe day ➪ ﬁ scal period ➪ year. Another could be day ➪ month ➪ year. These two \nhierarchies may be diff erent if there is no simple relationship between ﬁ scal periods \nand months. For example, some organizations have 5-4-4 ﬁ scal periods, consisting \nof a 5-week span followed by two 4-week spans. A single calendar date dimension \ncan comfortably represent these two hierarches at the same time in sets of parallel \nattributes since the grain of the date dimension is the individual day.\nThe account dimension may also have a ﬁ xed many-to-one hierarchy such as \nexecutive level, director level, and manager level accounts. The grain of the dimen-\nsion is the manager level account, but the detailed accounts at the lowest grain roll \nup to the director and executive levels.\nIn a ﬁ xed position hierarchy, it is important that each level have a speciﬁ c name. \nThat way the business user knows how to constrain and interpret each  level.\nWARNING \nAvoid ﬁ xed position hierarchies with abstract names such as Level-1, \nLevel-2, and so on. This is a cheap way to avoid correctly modeling a ragged hierar-\nchy. When the levels have abstract names, the business user has no way of knowing \nwhere to place a constraint, or what the attribute values in a level mean in a report. \nIf a ragged hierarchy attempts to hide within a ﬁ xed position hierarchy with abstract \nnames, the individual levels are essentially meaningless.\n Slightly Ragged Variable Depth Hierarchies\nGeographic  hierarchies present an interesting challenge. Figure 7-7 shows three \npossibilities. The simple location has four levels: address, city, state, and country. \nThe medium complex location adds a zone level, and the complex location adds \nboth district and zone levels. If you need to represent all three types of locations \n",
      "content_length": 2506,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 251,
      "content": "Accounting 215\nin a single geographic hierarchy, you have a slightly variable hierarchy. You can \ncombine all three types if you are willing to make a compromise. For the medium \nlocation that has no concept of district, you can propagate the city name down into \nthe district attribute. For the simple location that has no concept of either district or \nzone, you can propagate the city name down into both these attributes. The business \ndata governance representatives may instead decide to propagate labels upward or \neven populate the empty levels with Not Applicable. The business representatives \nneed to visualize the appropriate row label values on a report if the attribute is \ngrouped on. Regardless of the business rules applied, you have the advantage of a \nclean positional design with attribute names that make reasonable sense across all \nthree geographies. The key to this compromise is the narrow range of geographic \nhierarchies, ranging from four levels to only six levels. If the data ranged from \nfour levels to eight or ten or even more, this design compromise would not work. \nRemember the attribute names need to make sense.\nSimple Loc\nLoc Key (PK)\nAddress+\nCity\nCity\nCity\nState\nCountry\n...\nMedium Loc\nLoc Key (PK)\nAddress+\nCity\nCity\nZone\nState\nCountry\n...\nComplex Loc\nLoc Key (PK)\nAddress+\nCity\nDistrict\nZone\nState\nCountry\n...\nFigure 7-7: Sample data values exist simultaneously in a single location dimension \ncontaining simple, intermediate, and complex hierarchies.\n Ragged Variable Depth Hierarchies\nIn  the budget use case, the organization structure is an excellent example of a ragged \nhierarchy of indeterminate depth. In this chapter, we often refer to the hierarchical struc-\nture as a “tree” and the individual organizations in that tree as “nodes.” Imagine your \nenterprise consists of 13 organizations with the rollup structure shown in Figure 7-8. \nEach of these organizations has its own budget, commitments, and payments.\nFor a single organization, you can request a speciﬁ c budget for an account with a \nsimple join from the organization dimension to the fact table, as shown in Figure 7-9. \nBut you also want to roll up the budget across portions of the tree or even all the tree. \nFigure 7-9 contains no information about the organizational rollup.\n",
      "content_length": 2295,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 252,
      "content": "Chapter 7\n216\n1\n2\n3\n4\n5\n6\n10\n13\n11\n12\n8\n9\n7\nFigure 7-8: Organization rollup structure.\nOrganization Dimension\nPosting Date Key (FK)\nOrganization Key (FK)\nAccount Key (FK)\nTransaction Key (FK)\nLedger Key (FK)\nTransaction ID (DD)\nAmount\nBalance\nOrganization Key (PK)\nOrganization Name\n...\nGeneral Ledger Fact\nFigure 7-9: Organization dimension joined to fact table.\nThe  classic way to represent a parent/child tree structure is by placing recur-\nsive pointers in the organization dimension from each row to its parent, as shown \nin Figure 7-10. The original deﬁ nition of SQL did not provide a way to evaluate \nthese recursive pointers. Oracle implemented a CONNECT BY function that traversed \nthese pointers in a downward fashion starting at a high-level parent in the tree \nand progressively enumerated all the child nodes in lower levels until the tree \nwas exhausted. But the problem with Oracle CONNECT BY and other more general \napproaches, such as SQL Server’s recursive common table expressions, is that the \nrepresentation of the tree is entangled with the organization dimension because \nthese approaches depend on the recursive pointer embedded in the data. It is imprac-\ntical to switch from one rollup structure to another because many of the recursive \npointers would have to be destructively modiﬁ ed. It is also impractical to maintain \norganizations as type 2 slowly changing dimension attributes because changing the \nkey for a high-level node would ripple key changes down to the bottom of the tree.\nThe solution to the problem of representing arbitrary rollup structures is to build \na special kind of bridge table that is independent from the primary dimension table \nand contains all the information about the rollup. The grain of this bridge table is \n",
      "content_length": 1774,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 253,
      "content": "Accounting 217\neach path in the tree from a parent to all the children below that parent, as shown \nin Figure 7-11. The ﬁ rst column in the map table is the primary key of the parent, \nand the second column is the primary key of the child. A row must be constructed \nfrom each possible parent to each possible child, including a row that connects the \nparent to itself. \nOrganization Dimension\nOrganization Key (PK)\nOrganization Name\n...\nOrganization Parent Key (FK)\nRecursive\nPointer\nFigure 7-10: Classic parent/child recursive design.\nThe example tree depicted in Figure 7-8 results in 43 rows in Figure 7-11. There \nare 13 paths from node number 1, 5 paths from node number 2, one path from node \nnumber 3 to itself, as so on.\nThe highest parent ﬂ ag in the map table means the particular path comes from \nthe highest parent in the tree. The lowest child ﬂ ag means the particular path ends \nin a “leaf node” of the tree.\nIf you constrain the organization dimension table to a single row, you can join \nthe dimension table to the map table to the fact table, as shown in Figure 7-12. For \nexample, if you constrain the organization table to node number 1 and simply fetch \nan additive fact from the fact table, you get 13 hits on the fact table, which traverses \nthe entire tree in a single query. If you perform the same query except constrain the \nmap table lowest child ﬂ ag to true, then you fetch only the additive fact from the six \nleaf nodes, numbers 3, 5, 6, 8, 10, and 11. Again, this answer was computed without \ntraversing the tree at query time!\nNOTE \nThe article “Building Hierarchy Bridge Tables” (available at www\n.kimballgroup.com under the Tools and Utilities tab for this book title) provides \na code example for building the hierarchy bridge table described in this section.\nYou must be careful when using the map bridge table to constrain the organization \ndimension to a single row, or else you risk overcounting the children and grandchil-\ndren in the tree. For example, if instead of a constraint such as “Node Organization \nNumber = 1” you constrain on “Node Organization Location = California”, you \nwould have this problem. In this case you need to craft a custom query, rather than \na simple join, with the following constraint:\nGLfact.orgkey in (select distinct bridge.childkey\n              from innerorgdim, bridge\n              where innerorgdim.state = 'California' and\n              innerorgdim.orgkey = bridge.parentkey)\n",
      "content_length": 2459,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 254,
      "content": "Chapter 7\n218\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n2\n2\n2\n2\n2\n3\n4\n4\n4\n5\n6\n7\n7\n8\n7\n7\n7\n7\n7\n9\n9\n9\n9\n9\n10\n10\n10\n11\n12\n13\nParent\nOrganization\nKey\nDepth from\nParent\nHighest\nParent\nFlag\nLowest\nChild\nFlag\nChild\nOrganization\nKey\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n2\n3\n4\n5\n6\n3\n4\n5\n6\n5\n6\n7\n8\n8\n9\n10\n11\n12\n13\n9\n10\n11\n12\n13\n10\n11\n12\n11\n12\n13\n0\n1\n2\n2\n3\n3\n1\n2\n2\n3\n4\n4\n3\n0\n1\n1\n2\n2\n0\n0\n1\n1\n0\n0\n0\n1\n0\n1\n2\n3\n3\n2\n0\n1\n2\n2\n1\n0\n1\n1\n0\n0\n0\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nTRUE\nFALSE\nTRUE\nTRUE\nFALSE\nTRUE\nFALSE\nFALSE\nTRUE\nTRUE\nTRUE\nFALSE\nTRUE\nFALSE\nTRUE\nTRUE\nTRUE\nFALSE\nTRUE\nTRUE\nTRUE\nTRUE\nFALSE\nTRUE\nTRUE\nFALSE\nFALSE\nTRUE\nTRUE\nTRUE\nFALSE\nFALSE\nTRUE\nTRUE\nTRUE\nFALSE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nOrganization Map Bridge\nParent Organization Key (FK)\nChild Organization Key (FK)\nDepth from Parent\nHighest Parent Flag\nLowest Child Flag\nSample Organization Map bridge table rows for Figure 7-8:\nFigure 7-11: Organization map bridge table sample rows.\n",
      "content_length": 1125,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 255,
      "content": "Accounting 219\nOrganization Map Bridge\nParent Organization Key (FK)\nChild Organization Key (FK)\nDepth from Parent\nHighest Parent Flag\nLowest Child Flag\nOrganization Dimension\nOrganization Key (PK)\nOrganization Name\n...\nPosting Date Key (FK)\nOrganization Key (FK)\nAccount Key (FK)\nTransaction Profile Key (FK)\nLedger Version Key (FK)\nTransaction ID (DD)\nAmount\nBalance\nGeneral Ledger Fact\nFIGURE 7-12: Joining organization map bridge table to fact table.\nShared Ownership in a Ragged Hierarchy\nThe  map table can represent partial or shared ownership, as shown in Figure 7-13. \nFor instance, suppose node 10 is 50 percent owned by node 6 and 50 percent \nowned by node 11. In this case, any budget or commitment or payment attributed \nto node 10 ﬂ ows upward through node 6 with a 50 percent weighting and also \nupward through node 11 with a 50 percent weighting. You now need to add extra \npath rows to the original 43 rows to accommodate the connection of node 10 up \nto node 6 and its parents. All the relevant path rows ending in node 10 now need \na 50 percent weighting in the ownership percentage column in the map table. \nOther path rows not ending in node 10 do not have their ownership percentage \ncolumn  changed.\nOrganization Map Bridge\nParent Organization Key (FK)\nChild Organization Key (FK)\nDepth from Parent\nHighest Parent Flag\nLowest Child Flag\nPercent Ownership\nOrganization Dimension\nOrganization Key (PK)\nOrganization Name\n...\nPosting Date Key (FK)\nOrganization Key (FK)\nAccount Key (FK)\nTransaction Profile Key (FK)\nLedger Version Key (FK)\nTransaction ID (DD)\nAmount\nBalance\nGeneral Ledger Fact\nFIGURE 7-13: Bridge table for ragged hierarchy with shared ownership.\n",
      "content_length": 1683,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 256,
      "content": "Chapter 7\n220\n Time Varying Ragged Hierarchies\nThe  ragged hierarchy bridge table can accommodate slowly changing hierarchies \nwith the addition of two date/time stamps, as shown in Figure 7-14. When a given \nnode no longer is a child of another node, the end eff ective date/time of the old \nrelationship must be set to the date/time of the change, and new path rows inserted \ninto the bridge table with the correct begin eff ective date/time.\nOrganization Map Bridge\nParent Organization Key (FK)\nChild Organization Key (FK)\nDepth from Parent\nHighest Parent Flag\nLowest Child Flag\nBegin Effective Date/Time\nEnd Effective Date/Time\nOrganization Dimension\nOrganization Key (PK)\nOrganization Name\n...\nPosting Date Key (FK)\nOrganization Key (FK)\nAccount Key (FK)\nTransaction Profile Key (FK)\nLedger Version Key (FK)\nTransaction ID (DD)\nAmount\nBalance\nGeneral Ledger Fact\nFigure 7-14: Bridge table for time varying ragged hierarchies.\nWARNING \nWhen using the bridge table in Figure 7-14, the query must always \nconstrain to a single date/time to “freeze” the bridge table to a single consistent \nview of the hierarchy. Failing to constrain in this way otherwise would result in \nmultiple paths being fetched that could not exist at the same time.\nModifying Ragged Hierarchies\nThe  organization map bridge table can easily be modiﬁ ed. Suppose you want to \nmove nodes 4, 5, and 6 from their original location reporting up to node 2 to a new \nlocation reporting up to node 9, as shown in Figure 7-15.\nIn the static case in which the bridge table only reﬂ ects the current rollup struc-\nture, you merely delete the higher level paths in the tree pointing into the group \nof nodes 4, 5, and 6. Then you attach nodes 4, 5, and 6 into the parents 1, 7, and 9. \nHere is the static SQL:\nDelete from Org_Map where child_org in (4, 5,6) and\n  parent_org not in (4,5,6)\nInsert into Org_Map (parent_org, child_org)\n  select parent_org, 4 from Org_Map where parent_org in (1, 7, 9)\nInsert into Org_Map (parent_org, child_org)\n  select parent_org, 5 from Org_Map where parent_org in (1, 7, 9)\nInsert into Org_Map (parent_org, child_org)\n  select parent_org, 6 from Org_Map where parent_org in (1, 7, 9)\n",
      "content_length": 2185,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 257,
      "content": "Accounting 221\n11\n12\n5\n6\n10\n13\n4\n3\n8\n9\n2\n7\n1\nFigure 7-15: Changes to Figure 7-8’s organization structure.\nIn the time varying case in which the bridge table has the pair of date/time \nstamps, the logic is similar. You can ﬁ nd the higher level paths in the tree point-\ning into the group of nodes 4, 5, and 6 and set their end eff ective date/times to the \nmoment of the change. Then you attach nodes 4, 5, and 6 into the parents 1, 7, and \n9 with the appropriate date/times. Here is the time varying SQL:\nUpdate Org_Map set end_eff_date = #December 31, 2012#\n  where child_org in (4, 5,6) and parent_org not in (4,5,6)\n  and #Jan 1, 2013# between begin_eff_date and end_eff_date\nInsert into Org_Map\n  (parent_org, child_org, begin_eff_date, end_eff_date)\n  values (1, 4, #Jan 1, 2013#, #Dec 31, 9999#)\nInsert into Org_Map\n  (parent_org, child_org, begin_eff_date, end_eff_date)\n  values (7, 4, #Jan 1, 2013#, #Dec 31, 9999#) \nInsert into Org_Map\n  (parent_org, child_org, begin_eff_date, end_eff_date)\n  values (9, 4, #Jan 1, 2013#, #Dec 31, 9999#) \nIdentical insert statements for nodes 5 and 6 …\nThis simple recipe for changing the bridge table avoids nightmarish scenarios \nwhen changing other types of hierarchical models. In the bridge table, only the \npaths directly involved in the change are aff ected. All other paths are untouched. \nIn most other schemes with clever node labels, a change in the tree structure can \naff ect many or even all the nodes in the tree, as shown in the next  section.\n Alternative Ragged Hierarchy Modeling Approaches\nIn  addition to using recursive pointers in the organization dimension, there are at \nleast two other ways to model a ragged hierarchy, both involving clever columns \nplaced in the organization dimension. There are two disadvantages to these schemes \n",
      "content_length": 1807,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 258,
      "content": "Chapter 7\n222\ncompared to the bridge table approach. First, the deﬁ nition of the hierarchy is locked \ninto the dimension and cannot easily be replaced. Second, both of these schemes are \nvulnerable to a relabeling disaster in which a large part of the tree must be relabeled \ndue to a single small change. Textbooks (like this one!) usually show a tiny example, \nbut you need to tread cautiously if there are thousands of nodes in your tree.\nOne scheme adds a pathstring attribute to the organization dimension table, \nas shown in Figure 7-16. The values of the pathstring attribute are shown within \neach node. In this scenario, there is no bridge table. At each level, the pathstring \nstarts with the full pathstring of the parent and then adds the letters A, B, C, and \nso on, from left to right under that parent. The ﬁ nal character is a “+” if the node \nhas children and is a period if the node has no children. The tree can be navigated \nby using wild cards in constraints against the pathstring, for example,\n \n■A* retrieves the whole tree where the asterisk is a variable length wild card.\n \n■*. retrieves only the leaf nodes.\n \n■?+ retrieves the topmost node where the question mark is a single character \nwild card.\nAA+\nAAA.\nAAB+\nAABA.\nAABB.\nABBA+\nABBB.\nABBAA.\nABBAB.\nABA.\nABB+\nAB+\nA+\nFigure 7-16: Alternate ragged hierarchy design using pathstring attribute.\nThe pathstring approach is fairly sensitive to relabeling ripples caused by orga-\nnization changes; if a new node is inserted somewhere in the tree, all the nodes to \nthe right of that node under the same parent must be relabeled.\nAnother similar scheme, known to computer scientists as the modiﬁ ed preordered \ntree traversal approach, numbers the tree as shown in Figure 7-17. Every node has a \npair of numbers that identiﬁ es all the nodes below that point. The whole tree can be \nenumerated by using the node numbers in the topmost node. If the values in each node \nhave the names Left and Right, then all the nodes in the example tree can be found with \n",
      "content_length": 2031,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 259,
      "content": "Accounting 223\nthe constraint “Left between 1 and 26.” Leaf nodes can be found where Left and Right \ndiff er by 1, meaning there aren’t any children. This approach is even more vulnerable \nto the relabeling disaster than the pathstring approach because the entire tree must \nbe carefully numbered in sequence, top to bottom and left to right. Any change to the \ntree causes the entire rest of the tree to the right to be relabeled.\n2,11\n3,4\n5,10\n6,7\n8,9\n16,21\n22,23\n17,18\n19,20\n13,14\n15,24\n12,25\n1,26\nFigure 7-17: Alternative ragged hierarchy design using the modiﬁ ed preordered tree \ntraversal approach.\nAdvantages of the Bridge Table Approach for Ragged \nHierarchies\nAlthough  the bridge table requires more ETL work to set up and more work when \nquerying, it off ers exceptional ﬂ exibility for analyzing ragged hierarches of inde-\nterminate depth. In particular, the bridge table allows\n \n■Alternative rollup structures to be selected at query time\n \n■Shared ownership rollups\n \n■Time varying ragged hierarchies\n \n■Limited impact when nodes undergo slowly changing dimension (SCD) \ntype 2 changes\n \n■Limited impact when the tree structure is changed\nYou can use the organization hierarchy bridge table to fetch a fact across all three \nfact tables in the budget chain. Figure 7-18 shows how an organization map table \ncan connect to the three budget chain fact tables. This would allow a drill-across \nreport such as ﬁ nding all the travel budgets, commitments, and payments made by \nall the lowest leaf nodes in a complex organizational  structure.\n",
      "content_length": 1555,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 260,
      "content": "Chapter 7\n224\nOrganization Map\nParent Organization Key (FK)\nChild Organization Key (FK)\nDepth from Parent\nHighest Parent Flag\nLowest Child Flag\nOrganization Dimension\nOrganization Key\nCost Center Number\nCost Center Name\n...\nMonth Key (FK)\nOrganization Key (FK)\nAccount Key (FK)\nBudget Key (FK)\nBudget Amount\nMonth Key (FK)\nOrganization Key (FK)\nAccount Key (FK)\nBudget Key (FK)\nCommitment Key (FK)\nCommitment Amount\nMonth Key (FK)\nOrganization Key (FK)\nAccount Key (FK)\nBudget Key (FK)\nCommitment Key (FK)\nPayment Key (FK)\nPayment Amount\nPayment Fact\nBudget Fact\nCommitment Fact\nOrganization Dimension\nOrganization Dimension\nOrganization Dimension\nFigure 7-18: Drilling across and rolling up the budget chain.\n Consolidated Fact Tables\nIn  the last section, we discussed comparing metrics generated by separate business \nprocesses by drilling across fact tables, such as budget and commitments. If this \ntype of drill-across analysis is extremely common in the user community, it likely \nmakes sense to create a single fact table that combines the metrics once rather than \nrelying on business users or their BI reporting applications to stitch together result \nsets, especially given the inherent issues of complexity, accuracy, tool capabilities, \nand performance.\nMost typically, business managers are interested in comparing actual to budget \nvariances. At this point, you can presume the annual budgets and/or forecasts have \nbeen broken down by accounting period. Figure 7-19 shows the actual and budget \namounts, as well as the variance (which is a calculated diff erence) by the common \ndimensions.\n",
      "content_length": 1607,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 261,
      "content": "Accounting 225\nAccounting Period Key (FK)\nAccount Key (FK)\nOrganization Key (FK)\nAccounting Period Actual Amount\nAccounting Period Budget Amount\nAccounting Period Budget Variance\nBudget Variance Fact\nOrganization Dimension\nAccounting Period Dimension\nAccount Dimension\nFigure 7-19: Actual versus budget consolidated fact table.\nAgain, in a multinational organization, you would likely see the actual amounts \nin both local and the equivalent standard currency, based on the eff ective conversion \nrate. In addition, you may convert the actual results based on the planned currency \nconversion factor. Given the unpredictable nature of currency ﬂ uctuations, it is \nuseful to monitor performance based on both the eff ective and planned conversion \nrates. In this manner, remote managers aren’t penalized for currency rate changes \noutside their control. Likewise, ﬁ nance can better understand the big picture impact \nof unexpected currency conversion ﬂ uctuations on the organization’s annual plan.\nFact tables that combine metrics from multiple business processes at a com-\nmon granularity are referred to as consolidated fact tables. Although consolidated \nfact tables can be useful, both in terms of performance and usability, they often \nrepresent a dimensionality compromise as they consolidate facts at the “least \ncommon denominator” of dimensionality. One potential risk associated with \nconsolidated fact tables is that project teams sometimes base designs solely on \nthe granularity of the consolidated table, while failing to meet user requirements \nthat demand the ability to dive into more granular data. These schemas run into \nserious problems if project teams attempt to force a one-to-one correspondence \nto combine data with diff erent granularity or dimensionality.\nNOTE \nWhen facts from multiple business processes are combined in a consoli-\ndated fact table, they must live at the same level of granularity and dimensionality. \nBecause the separate facts seldom naturally live at a common grain, you are forced \nto eliminate or aggregate some dimensions to support the one-to-one correspon-\ndence, while retaining the atomic data in separate fact tables. Project teams should \nnot create artiﬁ cial facts or dimensions in an attempt to force-ﬁ t the consolidation \nof diff erently grained fact data.\n",
      "content_length": 2322,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 262,
      "content": "Chapter 7\n226\n Role of OLAP and Packaged Analytic \nSolutions\nWhile discussing ﬁ nancial dimensional models in the context of relational data-\nbases, it is worth noting that multidimensional OLAP vendors have long played a \nrole in this arena. OLAP products have been used extensively for ﬁ nancial reporting, \nbudgeting, and consolidation applications. Relational dimensional models often feed \nﬁ nancial OLAP cubes. OLAP cubes can deliver fast query performance that is critical \nfor executive usage. The data volumes, especially for general ledger balances or ﬁ nan-\ncial statement aggregates, do not typically overwhelm the practical size constraints \nof a multidimensional product. OLAP is well suited to handle complicated organiza-\ntional rollups, as well as complex calculations, including inter-row manipulations. \nMost OLAP vendors provide ﬁ nance-speciﬁ c capabilities, such as ﬁ nancial functions \n(for example, net present value or compound growth), the appropriate handling of \nﬁ nancial statement data (in the expected sequential order such as income before \nexpenses), and the proper treatment of debits and credits depending on the account \ntype, as well as more advanced functions such as ﬁ nancial consolidation. OLAP \ncubes often also readily support complex security models, such as limiting access \nto detailed data while providing more open access to summary metrics.\nGiven the standard nature of general ledger processing, purchasing a general \nledger package rather than attempting to build one from scratch has been a popu-\nlar route for years. Nearly all the operational packages also off er a complementary \nanalytic solution, sometimes in partnership with an OLAP vendor. In many cases, \nprecanned solutions based on the vendor’s cumulative experience are a sound way \nto jump start a ﬁ nancial DW/BI implementation with potentially reduced cost and \nrisk. The analytic solutions often have tools to assist with the extraction and staging \nof the operational data, as well as tools to assist with analysis and interpretation. \nHowever, when leveraging packaged solutions, you need to be cautious in order to \navoid stovepipe applications. You could easily end up with separate ﬁ nancial, CRM, \nhuman resources, and ERP packaged analytic solutions from as many diff erent \nvendors, none of which integrate with other internal data. You need to conform \ndimensions across the entire DW/BI environment, regardless of whether you build \na solution or implement packages. Packaged analytic solutions can turbocharge a \nDW/BI implementation; however, they do not alleviate the need for conformance. \nMost organizations inevitably rely on a combination of building, buying, and inte-\ngrating for a complete  solution.\n",
      "content_length": 2739,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 263,
      "content": "Accounting 227\nSummary\nIn this chapter, we focused primarily on ﬁ nancial data in the general ledger, both in \nterms of periodic snapshots as well as journal entry transactions. We discussed the \nhandling of common G/L data challenges, including multiple currencies, multiple \nﬁ scal years, unbalanced organizational trees, and the urge to create to-date totals.\nWe used the familiar organization rollup structure to show how to model complex \nragged hierarchies of indeterminate depth. We introduced a special bridge table for \nthese hierarchies, and compared this approach to others.\nWe explored the series of events in a budgeting process chain. We described the \nuse of “net change” granularity in this situation rather than creating snapshots of \nthe budget data totals. We also discussed the concept of consolidated fact tables \nthat combine the results of separate business processes when they are frequently \nanalyzed together.\nFinally, we discussed the natural ﬁ t of OLAP products for ﬁ nancial analysis. We \nalso stressed the importance of integrating analytic packages into the overall DW/\nBI environment through the use of conformed dimensions .\n",
      "content_length": 1159,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 265,
      "content": "Customer \nRelationship \nManagement\nL\nong  before the customer relationship management (CRM) buzzword existed, \norganizations were designing and developing customer-centric dimensional \nmodels to better understand their customers’ behavior. For decades, these models \nwere used to respond to management’s inquiries about which customers were solic-\nited, who responded, and what was the magnitude of their response. The business \nvalue of understanding the full spectrum of customers’ interactions and transactions \nhas propelled CRM to the top of the charts. CRM not only includes familiar resi-\ndential and commercial customers, but also citizens, patients, students, and many \nother categories of people and organizations whose behavior and preferences are \nimportant. CRM is a mission-critical business strategy that many view as essential \nto an organization’s survival. \nIn this chapter we start with a CRM overview, including its operational and ana-\nlytic roles. We then introduce the basic design of the customer dimension, including \ncommon attributes such as dates, segmentation attributes, repeated contact roles, \nand aggregated facts. We discuss customer name and address parsing, along with \ninternational considerations. We remind you of the challenges of modeling complex \nhierarchies when we describe various kinds of customer hierarchies.\nChapter 8 discusses the following concepts:\n \n■CRM overview\n \n■Customer name and address parsing, including international considerations\n \n■Handling of dates, aggregated facts, and segmentation behavior attributes and \nscores in a customer dimension\n \n■Outriggers for low cardinality attributes\n \n■Bridge tables for sparse attributes, along with trade-off s of bridge tables versus \na positional design\n \n■Bridge tables for multiple customer contacts\n \n■Behavior study groups to capture customer cohort groups\n8\n",
      "content_length": 1869,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 266,
      "content": "Chapter 8\n230\n \n■Step dimensions to analyze sequential customer behavior\n \n■Timespan fact tables with eff ective and expiration dates\n \n■Embellishing fact tables with dimensions for satisfaction or abnormal scenarios\n \n■Integrating customer data via master data management or partial conformity \nduring the downstream ETL processing\n \n■Warnings about fact-to-fact table joins\n \n■Reality check on real time, low latency requirements\nBecause this chapter’s customer-centric modeling issues and patterns are relevant \nacross industries and functional areas, we have not included a bus matrix.\nCRM Overview\nRegardless  of the industry, organizations have ﬂ ocked to the concept of CRM. \nThey’ve jumped on the bandwagon in an attempt to migrate from a product-centric \norientation to one that is driven by customer needs. Although all-encompassing \nterms such as customer relationship management sometimes seem ambiguous and/\nor overly ambitious, the premise behind CRM is far from rocket science. It’s based \non the simple notion that the better you know your customers, the better you can \nmaintain long-lasting, valuable relationships with them. The goal of CRM is to \nmaximize relationships with your customers over their lifetime. It entails focus-\ning all aspects of the business, from marketing, sales, operations, and service, on \nestablishing and sustaining mutually beneﬁ cial customer relations. To do so, the \norganization must develop a single, integrated view of each customer.\nCRM promises signiﬁ cant returns for organizations that embrace it, both for \nincreased revenue and operational effi  ciencies. Switching to a customer-driven \nperspective can lead to increased sales eff ectiveness and closure rates, revenue \ngrowth, enhanced sales productivity at reduced cost, improved customer proﬁ t-\nability margins, higher customer satisfaction, and increased customer retention. \nUltimately, every organization wants more loyal, more proﬁ table customers. As it \noften requires a sizeable investment to attract new customers, you can’t aff ord to \nhave the proﬁ table ones leave.\nIn many organizations, the view of the customer varies depending on the product \nline business unit, business function, and/or geographic location. Each group may \nuse diff erent customer data in diff erent ways with diff erent results. The evolution \nfrom the existing silos to a more integrated perspective obviously requires organi-\nzational commitment. CRM is like a stick of dynamite that knocks down the silo \nwalls. It requires the right integration of business processes, people resources, and \napplication technology to be eff ective.\nOver  the past decade, the explosive growth of social media, location tracking tech-\nnology, network usage monitoring, multimedia applications, and sensor networks \n",
      "content_length": 2800,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 267,
      "content": "Customer Relationship Management 231\nhas provided an ocean of customer behavioral data that even Main Street enterprises \nrecognize as providing actionable insights. Although much of this data lies outside \nthe comfort zone of relational databases, the new “big data” techniques can bring \nthis data back into the DW/BI fold. Chapter 21: Big Data Analytics discusses the \nbest practices for bringing this new kind of big data into the DW/BI environment. \nBut setting aside the purely technological challenges, the real message is the need \nfor profound integration. You must step up to the challenge of integrating as many \nas 100 customer-facing data sources, most of which are external. These data sources \nare at diff erent grains, have incompatible customer attributes, and are not under \nyour control. Any questions?\nBecause it is human nature to resist change, it comes as no surprise that people-\nrelated issues often challenge CRM implementations. CRM involves brand new \nways of interacting with customers and often entails radical changes to the sales \nchannels. CRM requires new information ﬂ ows based on the complete acquisition \nand dissemination of customer “touch point” data. Often organization structures and \nincentive systems are dramatically altered. \nIn Chapter 17: Kimball DW/BI Lifecycle Overview, we’ll stress the importance of \nhaving support from both senior business and IT management for a DW/BI initiative. \nThis advice also applies to a CRM implementation because of its cross-functional \nfocus. CRM requires a clear business vision. Without business strategy, buy-in, and \nauthorization to change, CRM becomes an exercise in futility. Neither IT nor the \nbusiness community can successfully implement CRM on its own; CRM demands \na joint commitment of support.\nOperational and Analytic CRM\nIt  could be said that CRM suff ers from a split personality syndrome because it needs \nto address both operational and analytic requirements. Eff ective CRM relies on the \ncollection of data at every interaction you have with a customer and then leveraging \nthat breadth of data through analysis. \nOn the operational front, CRM calls for the synchronization of customer-facing \nprocesses. Often operational systems must either be updated or supplemented to coor-\ndinate across sales, marketing, operations, and service. Think about all the customer \ninteractions that occur during the purchase and usage of a product or service, from \nthe initial prospect contact, quote generation, purchase transaction, fulﬁ llment, pay-\nment transaction, and on-going customer service. Rather than thinking about these \nprocesses as independent silos (or multiple silos varying by product line), the CRM \nmindset is to integrate these customer activities. Key customer metrics and charac-\nteristics are collected at each touch point and made available to the others.\nAs data is created on the operational side of the CRM equation, you obviously \nneed to store and analyze the historical metrics resulting from the customer \n",
      "content_length": 3033,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 268,
      "content": "Chapter 8\n232\ninteraction and transaction systems. Sounds familiar, doesn’t it? The DW/BI system \nsits at the core of CRM. It serves as the repository to collect and integrate the \nbreadth of customer information found in the operational systems, as well as from \nexternal sources. The data warehouse is the foundation that supports the panoramic \n360-degree view of your customers. \nAnalytic CRM is enabled via accurate, integrated, and accessible customer data \nin the DW/BI system. You can measure the eff ectiveness of decisions made in the \npast to optimize future interactions. Customer data can be leveraged to better iden-\ntify up-sell and cross-sell opportunities, pinpoint ineffi  ciencies, generate demand, \nand improve retention. In addition, the historical, integrated data can be leveraged \nto generate models or scores that close the loop back to the operational world. \nRecalling the major components of a DW/BI environment from Chapter 1: Data \nWarehousing, Business Intelligence, and Dimensional Modeling Primer, you can \nenvision the model results pushed back to where the relationship is operationally \nmanaged (such as the rep, call center, or website), as illustrated in Figure 8-1. The \nmodel output can translate into speciﬁ c proactive or reactive tactics recommended \nfor the next point of customer contact, such as the appropriate next product off er or \nanti-attrition response. The model results are also retained in the DW/BI environ-\nment for subsequent analysis.\nIntegrate\n(ETL)\nStore\n(Data Presentation)\nAnalyze and\nReport\n(BI Applications)\nModel\n(BI Applications)\nCollect\n(Operational\nSource System)\nFigure 8-1: Closed loop analytic CRM.\n",
      "content_length": 1672,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 269,
      "content": "Customer Relationship Management 233\nIn  other situations, information must feed back to the operational website or call \ncenter systems on a more real-time basis. In this case, the closed loop is much tighter \nthan Figure 8-1 because it’s a matter of collection and storage, and then feedback \nto the collection system. Today’s operational processes must combine the current \nview with a historical view, so a decision maker can decide, for example, whether \nto grant credit to a customer in real time, while considering the customer’s lifetime \nhistory. But generally, the integration requirements for operational CRM are not as \nfar reaching as for analytic CRM. \nObviously, as the organization becomes more centered on the customer, so must \nthe DW/BI system. CRM will inevitably drive change in the data warehouse. DW/BI \nenvironments will grow even more rapidly as you collect more and more informa-\ntion about your customers. ETL processes will grow more complicated as you match \nand integrate data from multiple sources. Most important, the need for a conformed \ncustomer dimension becomes even more paramount. \nCustomer Dimension Attributes\nThe  conformed customer dimension is a critical element for eff ective CRM. A well-\nmaintained, well-deployed conformed customer dimension is the cornerstone of \nsound CRM analysis.\nThe customer dimension is typically the most challenging dimension for any \nDW/BI system. In a large organization, the customer dimension can be extremely \ndeep (with many millions of rows), extremely wide (with dozens or even hundreds \nof attributes), and sometimes subject to rapid change. The biggest retailers, credit \ncard companies, and government agencies have monster customer dimensions whose \nsize exceeds 100 million rows. To further complicate matters, the customer dimen-\nsion often represents an amalgamation of data from multiple internal and external \nsource systems. \nIn this next section, we focus on numerous customer dimension design con-\nsiderations. We’ll begin with name/address parsing and other common customer \nattributes, including coverage of dimension outriggers, and then move on to other \ninteresting customer attributes. Of course, the list of customer attributes is typically \nquite lengthy. The more descriptive information you capture about your customers, \nthe more robust the customer dimension, and the more interesting the analyses.\nName and Address Parsing\nRegardless  of whether you deal with individual human beings or commercial enti-\nties, customers’ name and address attributes are typically captured. The operational \nhandling of name and address information is usually too simplistic to be very useful \n",
      "content_length": 2683,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 270,
      "content": "Chapter 8\n234\nin the DW/BI system. Many designers feel a liberal design of general purpose col-\numns for names and addresses, such as Name-1 through Name-3 and Address-1 \nthrough Address-6, can handle any situation. Unfortunately, these catchall columns \nare virtually worthless when it comes to better understanding and segmenting \nthe customer base. Designing the name and location columns in a generic way \ncan actually contribute to data quality problems. Consider the sample design in \nFigure 8-2 with general purpose columns.\nMs. R. Jane Smith, Atty\n123 Main Rd, North West, Ste 100A\nPO Box 2348\nKensington\nArk.\n88887-2348 \n888-555-3333 x776 main, 555-4444 fax\nName\nAddress 1 \nAddress 2 \nCity \nState\nZIP Code \nPhone Number\nColumn\nSample Data Value\nFigure 8-2: Sample customer name/address data in overly general columns.\nIn this design, the name column is far too limited. There is no consistent mecha-\nnism for handling salutations, titles, or suffi  xes. You can’t identify what the person’s \nﬁ rst name is, or how she should be addressed in a personalized greeting. If you \nlook at additional sample data from this operational system, you would potentially \nﬁ nd multiple customers listed in a single name attribute. You might also ﬁ nd addi-\ntional descriptive information in the name column, such as Conﬁ dential, Trustee, \nor UGMA (Uniform Gift to Minors Act).\nIn the sample address attributes, inconsistent abbreviations are used in various \nplaces. The address columns may contain enough room for any address, but there \nis no discipline imposed by the columns that can guarantee conformance with \npostal authority regulations or support address matching and latitude/longitude \nidentiﬁ cation.\nInstead of using a few, general purpose columns, the name and location attributes \nshould be broken down into as many elemental parts as possible. The extract process \nneeds to perform signiﬁ cant parsing on the original dirty names and addresses. After \nthe attributes have been parsed, they can be standardized. For example, Rd would \nbecome Road and Ste would become Suite. The attributes can also be veriﬁ ed, such \nas verifying the ZIP code and associated state combination is correct. Fortunately, \nthere are name and address data cleansing and scrubbing tools available in the \nmarket to assist with parsing, standardization, and veriﬁ cation.\n",
      "content_length": 2360,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 271,
      "content": "Customer Relationship Management 235\nA sample set of name and location attributes for individuals in the United States is \nshown in Figure 8-3. Every attribute is ﬁ lled in with sample data to make the design \nclearer, but no single real instance would look like this. Of course, the business data \ngovernance representatives should be involved in determining the analytic value of \nthese parsed data elements in the customer dimension.\nMs.\nJane\nMs. Smith\nR. Jane\nSmith\nJr.\nEnglish\nAttorney \n123 \nMain\nRoad\nNorth West\nKensington\nCornwall\nBerkeleyshire\nArkansas\nSouth\nUnited States\nNorth America\n88887 \n2348 \nUnited States\n1 \n888 \n5553333 \n776 \n1 \n509 \n5554444 \nRJSmith@ABCGenIntl.com\nwww.ABCGenIntl.com\nX.509 \nVerisign\n7346531\nSalutation\nInformal Greeting Name\nFormal Greeting Name\nFirst and Middle Names\nSurname\nSuffix\nEthnicity\nTitle\nStreet Number\nStreet Name\nStreet Type\nStreet Direction\nCity\nDistrict\nSecond District\nState\nRegion\nCountry\nContinent\nPrimary Postal Code\nSecondary Postal Code\nPostal Code Type\nOffice Telephone Country Code\nOffice Telephone Area Code\nOffice Telephone Number\nOffice Extension\nMobile Telephone Country Code\nMobile Telephone Area Code\nMobile Telephone Number\nE-mail\nWeb Site\nPublic Key Authentication\nCertificate Authority\nUnique Individual Identifier\nColumn\nSample Data Value\nFigure 8-3: Sample customer name/address data with parsed name and address \nelements.\n",
      "content_length": 1394,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 272,
      "content": "Chapter 8\n236\nCommercial customers typically have multiple addresses, such as physical and \nshipping addresses; each of these addresses would follow much the same logic as \nthe address structure shown in Figure 8-3.\nInternational Name and Address Considerations\nInternational  display and printing typically requires representing foreign characters, \nincluding not just the accented characters from western European alphabets, but \nalso Cyrillic, Arabic, Japanese, Chinese, and dozens of other less familiar writing \nsystems. It is important to understand this is not a font problem. This is a character \nset problem. A font is simply an artist’s rendering of a set of characters. There are \nhundreds of fonts available for standard English, but standard English has a rela-\ntively small character set that is enough for anyone’s use unless you are a professional \ntypographer. This small character set is usually encoded in American Standard Code \nfor Information Interchange (ASCII), which is an 8-bit encoding that has a maximum \nof 255 possible characters. Only approximately 100 of these 255 characters have a \nstandard interpretation that can be invoked from a normal English keyboard, but \nthis is usually enough for English speaking computer users. It should be clear that \nASCII is woefully inadequate for representing the thousands of characters needed \nfor non-English writing systems.\nAn  international body of system architects, the Unicode Consortium, deﬁ ned a \nstandard known as Unicode for representing characters and alphabets in almost all \nthe world’s languages and cultures. Their work can be accessed on the web at www.\nunicode.org. The Unicode Standard, version 6.2.0 has deﬁ ned speciﬁ c interpreta-\ntions for 110,182 possible characters and now covers the principal written languages \nof the Americas, Europe, the Middle East, Africa, India, Asia, and Paciﬁ ca. Unicode \nis the foundation you must use for addressing international character sets. \nBut it is important to understand that implementing Unicode solutions is done in \nthe foundation layers of your systems. First, the operating system must be Unicode-\ncompliant. Fortunately, the most current releases of all the major operating systems \nare Unicode-compliant.\nAbove the operating system, all the devices that capture, store, transmit, and \nprint characters must be Unicode-compliant. Data warehouse back room tools must \nbe Unicode-compliant, including sort packages, programming languages, and auto-\nmated ETL packages. Finally, the DW/BI applications, including database engines, \nBI application servers and their report writers and query tools, web servers, and \nbrowsers must all be Unicode-compliant. The DW/BI architect should not only talk \nto the vendors of each package in the data pipeline, but also should conduct various \nend-to-end tests. Capture some names and addresses with Unicode characters at \nthe data capture screens of one of the legacy applications, and send them through the \nsystem. Get them to print out of a ﬁ nal report or a ﬁ nal browser window from \n",
      "content_length": 3068,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 273,
      "content": "Customer Relationship Management 237\nthe DW/BI system and see if the special characters are still there. That simple \ntest will cut through a lot of the confusion. Note that even when you do this, the \nsame character, such as an a-umlaut, sorts diff erently in diff erent countries such as \nNorway and Germany. Even though you can’t solve all the variations in international \ncollating sequences, at least both the Norwegians and the Germans will agree that \nthe character is an a-umlaut.\nCustomer geographic attributes become more complicated if you deal with cus-\ntomers from multiple countries. Even if you don’t have international customers, you \nmay need to contend with international names and addresses somewhere in the \nDW/BI system for international suppliers and human resources personnel records.\nNOTE \nCustomer dimensions sometimes include a full address block attribute. \nThis is a specially crafted column that assembles a postally-valid address for the \ncustomer including mail stop, ZIP code, and other attributes needed to satisfy postal \nauthorities. This attribute is useful for international locations where addresses \nhave local idiosyncrasies.\nInternational DW/BI Goals\nAfter  committing to a Unicode foundation, you need to keep the following goals in \nmind, in addition to the name and address parsing requirements discussed earlier:\n \n■Universal and consistent. As they say, in for a penny, in for a pound. If you \nare going to design a system for international use, you want it to work around \nthe world. You need to think carefully if BI tools are to produce translated ver-\nsions of reports in many languages. It may be tempting to provide translated \nversions of dimensions for each language, but translated dimensions give rise \nto some subtle problems. \n \n■Sorting  sequences will be different, so either the reports will be sorted \ndifferently or all reports except those in the “root” language will appear \nto be unsorted. \n \n■If the attribute cardinalities are not faithfully preserved across lan-\nguages, then either group totals will not be the same across reports, or \nsome groups in various languages will contain duplicated row headers \nthat look like mistakes. To avoid the worst of these problems, you \nshould translate dimensions after the report is run; the report first \nneeds to be produced in a single root language, and then the report \nface needs to be translated into the intended target languages. \n \n■All  the BI tool messages and prompts need to be translated for the \nbenefit of the business user. This process is known as localization and \nis further discussed in Chapter 12: Transportation. \n",
      "content_length": 2647,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 274,
      "content": "Chapter 8\n238\n \n■End-to-end data quality and downstream compatibility. The data warehouse \ncannot be the only step in the data pipeline that worries about the integrity \nof international names and addresses. A proper design requires support from \nthe ﬁ rst step of capturing the name and the address, through the data cleaning \nand storage steps, to the ﬁ nal steps of performing geographic and demographic \nanalysis and printing reports.\n \n■Cultural correctness. In many cases, foreign customers and partners will see \nthe results from your DW/BI system in some form. If we don’t understand \nwhich name is a ﬁ rst name and which is a last name, and if you don’t under-\nstand how to refer to a person, you run the risk of insulting these individuals, \nor at the very least, looking stupid. When outputs are punctuated improperly, or \nmisspelled, your foreign customers and partners will wish they were doing \nbusiness with a local company, rather than you.\n \n■Real-time  customer response. DW/BI systems can play an operational role \nby supporting real-time customer response systems. A customer service rep-\nresentative may answer the telephone and may have 5 seconds or less to wait \nfor a greeting to appear on the screen that the data warehouse recommends \nusing with the customer. The greeting may include a proper salutation and \na proper use of the customer’s title and name. This greeting represents an \nexcellent use of a hot response cache that contains precalculated responses \nfor each customer.\n \n■Other kinds of addresses. We are in the midst of a revolution in communication \nand networking. If you are designing a system for identifying international \nnames and addresses, you must anticipate the need to store electronic names, \nsecurity tokens, and internet addresses.\nSimilar to international addresses, telephone numbers must be presented \ndiff erently depending on where the phone call originates. You need to provide \nattributes to represent the complete foreign dialing sequence, complete domestic \ndialing sequence, and local dialing sequence. Unfortunately, complete foreign dial-\ning sequences vary by origin country. \n Customer-Centric Dates\nCustomer  dimensions often contains dates, such as the date of the ﬁ rst purchase, \ndate of last purchase, and date of birth. Although these dates initially may be SQL \ndate type columns, if you want to summarize these dates by your unique calen-\ndar attributes, such as seasons, quarters, and ﬁ scal periods, the dates should be \nchanged to foreign key references to the date dimension. You need to be careful \nthat all such dates fall within the span of the corporate date dimension. These date \ndimension roles are declared as semantically distinct views, such as a First Purchase \n",
      "content_length": 2754,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 275,
      "content": "Customer Relationship Management 239\nDate dimension table with unique column labels. The system behaves as if there \nis another physical date table. Constraints on any of these tables have nothing to \ndo with constraints on the primary date dimension table. This design, as shown \nin Figure 8-4, is an example of a dimension outrigger, which is discussed in the \nsection “Outrigger for Low Cardinality Attribute Set.”\nDate of 1st Purchase Key (PK)\nDate of 1st Purchase\nDate of 1st Purchase Month\nDate of 1st Purchase Year\nDate of 1st Purchase Fiscal Month\nDate of 1st Purchase Fiscal Quarter\nDate of 1st Purchase Fiscal Year\nDate of 1st Purchase Season\n…\nDate of 1st Purchase Dimension\nCustomer Key (PK)\nCustomer ID (Natural Key)\nCustomer Salutation\nCustomer First Name\nCustomer Surname\nCustomer City\nCustomer State\n…\nDate of 1st Purchase Key (FK)\nCustomer Dimension\nTransaction Date Key (FK)\nCustomer Key (FK)\nMore FKs …\nFacts …\nFact Table\nFigure 8-4: Date dimension outrigger.\n Aggregated Facts as Dimension Attributes\nBusiness  users are often interested in constraining the customer dimension based on \naggregated performance metrics, such as ﬁ ltering on all customers who spent more \nthan a certain dollar amount during last year. Or to make matters worse, perhaps \nthey want to constrain based on how much the customer has purchased in a lifetime. \nProviding aggregated facts as dimension attributes is sure to be a crowd-pleaser with \nthe business users. They could issue a query to identify all customers who satisﬁ ed the \nspending criteria and then issue another fact query to analyze the behavior for \nthat customer dimension subset. But rather than all that, you can instead store \nan aggregated fact as a dimension attribute. This allows business users to simply \nconstrain on the spending attribute just like they might on a geographic attribute. \nThese attributes are meant to be used for constraining and labeling; they’re not to be \nused in numeric calculations. Although there are query usability and performance \nadvantages of storing these attributes, the main burden falls on the back room ETL \nprocesses to ensure the attributes are accurate, up-to-date, and consistent with the \nactual fact rows. These attributes can require signiﬁ cant care and feeding. If you \nopt to include some aggregated facts as dimension attributes, be certain to focus on \nthose that will be frequently used. Also strive to minimize the frequency with which \nthese attributes need to be updated. For example, an attribute for last year’s spending \nwould require much less maintenance than one providing year-to-date behavior. \nRather than storing attributes down to the speciﬁ c dollar value, they are sometimes \n",
      "content_length": 2714,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 276,
      "content": "Chapter 8\n240\nreplaced (or supplemented) with more meaningful descriptive values, such as High \nSpender as discussed in the next section. These descriptive values minimize your \nvulnerability that the numeric attributes might not tie back to the appropriate fact \ntables. In addition, they ensure that all users have a consistent deﬁ nition for high \nspenders, for example, rather than resorting to their own individual business rules.\nSegmentation Attributes and Scores\nSome  of the most powerful attributes in a customer dimension are segmentation \nclassiﬁ cations. These attributes obviously vary greatly by business context. For an \nindividual customer, they may include:\n \n■Gender\n \n■Ethnicity\n \n■Age or other life stage classiﬁ cations\n \n■Income or other lifestyle classiﬁ cations\n \n■Status (such as new, active, inactive, and closed)\n \n■Referring source\n \n■Business-speciﬁ c market segment (such as a preferred customer identiﬁ er)\nSimilarly,  many organizations score their customers to characterize them. \nStatistical segmentation models typically generate these scores which cluster cus-\ntomers in a variety of ways, such as based on their purchase behavior, payment \nbehavior, propensity to churn, or probability to default. Each customer is tagged \nwith a resultant score.\n Behavior Tag Time Series \nOne  popular approach for scoring and proﬁ ling customers looks at the recency (R), \nfrequency (F), and intensity (I) of the customer’s behavior. These are known as the \nRFI measures; sometimes intensity is replaced with monetary (M), so it’s also known \nas RFM. Recency is how many days has it been since the customer last ordered \nor visited your site. Frequency is how many times the customer has ordered or \nvisited, typically in the past year. And intensity is how much money the customer \nhas spent over the same time period. When dealing with a large customer base, \nevery customer’s behavior can be modeled as a point in an RFI cube, as depicted \nin Figure 8-5. In this ﬁ gure, the scales along each axis are quintiles, from 1 to 5, \nwhich spread the actual values into even groups.\nIf you have millions of points in the cube, it becomes diffi  cult to see meaning-\nful clusters of these points. This is a good time to ask a data mining professional \nwhere the meaningful clusters are. The data mining professional may come back \nwith a list of behavior tags like the following, which is drawn from a slightly more \ncomplicated scenario that includes credit behavior and returns:\n",
      "content_length": 2500,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 277,
      "content": "Customer Relationship Management 241\nA: High volume repeat customer, good credit, few product returns\nB: High volume repeat customer, good credit, many product returns\nC: Recent new customer, no established credit pattern\nD: Occasional customer, good credit\nE: Occasional customer, poor credit\nF: Former good customer, not seen recently\nG: Frequent window shopper, mostly unproductive\nH: Other\n5\nHighest\nLowest\nHighest\nLowest\nHighest\nLowest\nRecency\nFrequency\nIntensity\n4\n3\n2\n1\n1\n1\n2\n3\n4\n5\n2\n3\n4\n5\nFigure 8-5: Recency, frequency, intensity (RFI) cube.\nNow you can look at the customers’ time series data and associate each customer \nin each reporting period with the nearest cluster. The data miner can help do this. \nThus, the last 10 observations of a customer named John Doe could look like:\nJohn Doe: C C C D D A A A B B\nThis time series of behavior tags is unusual because although it comes from \na regular periodic measurement process, the observed “values” are textual. The \nbehavior tags are not numeric and cannot be computed or averaged, but they can \nbe queried. For example, you may want to ﬁ nd all the customers who were an A \nsometime in the ﬁ fth, fourth, or third prior period and were a B in the second or ﬁ rst \nprior period. Perhaps you are concerned by progressions like this and fear losing a \nvaluable customer because of the increasing number of returns.\nBehavior  tags should not be stored as regular facts. The main use of behavior tags \nis formulating complex query patterns like the example in the previous paragraph. \nIf the behavior tags were stored in separate fact rows, such querying would be \nextremely diffi  cult, requiring a cascade of correlated subqueries. The recommended \nway to handle behavior tags is to build an explicit time series of attributes in the \ncustomer dimension. This is another example of a positional design. BI interfaces \n",
      "content_length": 1881,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 278,
      "content": "Chapter 8\n242\nare simple because the columns are in the same table, and performance is good \nbecause you can build bitmapped indexes on them.\nIn addition to the separate columns for each behavior tag time period, it would \nbe a good idea to create a single attribute with all the behavior tags concatenated \ntogether, such as CCCDDAAABB. This column would support wild card searches \nfor exotic patterns, such as “D followed by a B.” \nNOTE \nIn addition to the customer dimension’s time series of behavior tags, it \nwould be reasonable to include the contemporary behavior tag value in a mini-\ndimension to analyze facts by the behavior tag in eff ect when the fact row was \nloaded.\nRelationship Between Data Mining and DW/BI System\nThe  data mining team can be a great client of the data warehouse, and especially \ngreat users of customer behavior data. However, there can be a mismatch between \nthe velocity that the data warehouse can deliver data and the velocity that the data \nminers can consume data. For example, a decision tree tool can process hundreds \nof records per second, but a big drill-across report that produces “customer observa-\ntions” can never deliver data at such speeds. Consider the following seven-way drill \nacross a report that might produce millions of customer observations from census, \ndemographic, external credit, internal credit, purchases, returns, and website data:\nSELECT Customer Identifier, Census Tract, City, County, State,\n  Postal Code, Demographic Cluster, Age, Sex, Marital Status,\n  Years of Residency, Number of Dependents, Employment Profile,\n  Education Profile, Sports Magazine Reader Flag,\n  Personal Computer Owner Flag, Cellular Telephone Owner Flag,\n  Current Credit Rating, Worst Historical Credit Rating,\n  Best Historical Credit Rating, Date First Purchase,\n  Date Last Purchase, Number Purchases Last Year,\n  Change in Number Purchases vs. Previous Year,\n  Total Number Purchases Lifetime, Total Value Purchases Lifetime,\n  Number Returned Purchases Lifetime, Maximum Debt,\n  Average Age Customer's Debt Lifetime, Number Late Payments,\n  Number Fully Paid, Times Visited Web Site,\n  Change in Frequency of Web Site Access,\n  Number of Pages Visited Per Session,\n  Average Dwell Time Per Session, Number Web Product Orders,\n  Value Web Product Orders, Number Web Site Visits to Partner Web \n  Sites, Change in Partner Web Site Visits\nFROM *** WHERE *** ORDER BY *** GROUP BY ***\n",
      "content_length": 2437,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 279,
      "content": "Customer Relationship Management 243\nData mining teams would love this data! For example a big ﬁ le of millions of these \nobservations could be analyzed by a decision tree tool where the tool is “aimed” \nat the Total Value Purchases Lifetime column, which is highlighted above. In this \nanalysis, the decision tree tool would determine which of the other columns “predict \nthe variance” of the target ﬁ eld. Maybe the answer is Best Historical Credit Rating \nand Number of Dependents. Armed with this answer, the enterprise now has a \nsimple way to predict who is going to be a good lifetime customer, without needing \nto know all the other data content.\nBut the data mining team wants to use these observations over and over for \ndiff erent kinds of analyses perhaps with neural networks or case-based reasoning \ntools. Rather than producing this answer set on demand as a big, expensive query, \nthis answer set should be written to a ﬁ le and given to the data mining team to \nanalyze on its servers.\n Counts with Type 2 Dimension Changes\nBusinesses  frequently want to count customers based on their attributes without \njoining to a fact table. If you used type 2 to track customer dimension changes, \nyou need to be careful to avoid overcounting because you may have multiple rows \nin the customer dimension for the same individual. Doing a COUNT DISTINCT on a \nunique customer identiﬁ er is a possibility, assuming the attribute is indeed unique \nand durable. A current row indicator in the customer dimension is also helpful to \ndo counts based on the most up-to-date descriptive values for a customer. \nThings get more complicated if you need to do a customer count at a given histori-\ncal point in time using eff ective and expiration dates in the customer dimension. For \nexample, if you need to know the number of customers you had at the beginning of \n2013, you could constrain the row eff ective date <= ‘1/1/2013’ and row expiration \ndate >= ‘1/1/2013’ to restrict the result set to only those rows that were valid on \n1/1/2013. Note the comparison operators are dependent on the business rules used \nto set the row eff ective/expiration dates. In this example, the row expiration date on \nthe no longer valid customer row is 1 day less than the eff ective date on the new  row.\n Outrigger for Low Cardinality Attribute Set\nIn  Chapter 3: Retail Sales, we encouraged designers to avoid snowﬂ aking where low \ncardinality columns in the dimension are removed to separate normalized tables, \nwhich then link back into the original dimension table. Generally, snowﬂ aking is \nnot recommended in a DW/BI environment because it almost always makes the \nuser presentation more complex, in addition to negatively impacting browsing per-\nformance. In spite of this prohibition against snowﬂ aking, there are some special \n",
      "content_length": 2827,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 280,
      "content": "Chapter 8\n244\nsituations in which it is permissible to build a dimension outrigger that begins to \nlook like a snowﬂ aked table.\nIn Figure 8-6, the dimension outrigger is a set of data from an external data pro-\nvider consisting of 150 demographic and socio-economic attributes regarding the \ncustomers’ county of residence. The data for all customers residing in a given county \nis identical. Rather than repeating this large block of data for every customer within \na county, opt to model it as an outrigger. There are several reasons for bending the \n“no snowﬂ ake” rule. First, the demographic data is available at a signiﬁ cantly dif-\nferent grain than the primary dimension data and it’s not as analytically valuable. \nIt is loaded at diff erent times than the rest of the data in the customer dimension. \nAlso, you do save signiﬁ cant space in this case if the underlying customer dimen-\nsion is large. If you have a query tool that insists on a classic star schema with no \nsnowﬂ akes, the outrigger can be hidden under a view declaration.\nCountry Demographics Outrigger Dimension\nCounty Demographics Key (PK)\nTotal Population\nPopulation under 5 Years\n% Population under 5 Years\nPopulation under 18 Years\n% Population under 18 Years\nPopulation 65 Years and Older\n% Population 65 Years and Older\nFemale Population\n% Female Population\nMale Population\n% Male Population\nNumber of High School Graduates\nNumber of College Graduates\nNumber of Housing Units\nHome Ownership Rate\n...\nCustomer Dimension\nCustomer Key (PK)\nCustomer ID (Natural Key)\nCustomer Salutation\nCustomer First Name\nCustomer Surname\nCustomer City\nCustomer County\nCounty Demographics Key (FK)\nCustomer State\n...\nFact Table\nCustomer Key (FK)\nMore FKs ...\nFacts ...\nFigure 8-6: Dimension outrigger for cluster of low cardinality attributes.\nWARNING \nDimension outriggers are permissible, but they should be the \nexception rather than the rule. A red warning ﬂ ag should go up if your design \nis riddled with outriggers; you may have succumbed to the temptation to overly \nnormalize the design.\nCustomer Hierarchy Considerations\nOne  of the most challenging aspects of dealing with commercial customers is mod-\neling their internal organizational hierarchy. Commercial customers often have a \n",
      "content_length": 2259,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 281,
      "content": "Customer Relationship Management 245\nnested hierarchy of entities ranging from individual locations or organizations up \nthrough regional offi  ces, business unit headquarters, and ultimate parent companies. \nThese hierarchical relationships may change frequently as customers reorganize \nthemselves internally or are involved in acquisitions and divestitures. \nNOTE \nIn Chapter 7: Accounting, we described how to handle ﬁ xed hierar-\nchies, slightly variable hierarchies, and ragged hierarchies of indeterminate depth. \nChapter 7 focuses on ﬁ nancial cost center rollups, but the techniques are exactly \ntransferrable to customer hierarchies. If you skipped Chapter 7, you need to back-\ntrack to read that chapter to make sense of the following recommendations.\nAlthough relatively uncommon, the lucky ones amongst us sometimes are \nconfronted with a customer hierarchy that has a highly predictable ﬁ xed number \nof levels. Suppose you track a maximum of three rollup levels, such as the ultimate \ncorporate parent, business unit headquarters, and regional headquarters. In this \ncase, you have three distinct attributes in the customer dimension corresponding \nto these three levels. For commercial customers with complicated organizational \nhierarchies, you’d populate all three levels to appropriately represent the three diff er-\nent entities involved at each rollup level. This is the ﬁ xed depth hierarchy approach \nfrom Chapter 7.\nBy contrast, if another customer had a mixture of one, two, and three level \norganizations, you’d duplicate the lower-level value to populate the higher-level attri-\nbutes. In this way, all regional headquarters would sum to the sum of all business \nunit headquarters, which would sum to the sum of all ultimate corporate parents. \nYou can report by any level of the hierarchy and see the complete customer base \nrepresented. This is the slightly variable hierarchy approach.\nBut in many cases, complex commercial customer hierarchies are ragged hier-\narchies with an indeterminate depth, so you must use a ragged hierarchy modeling \ntechnique, as described in Chapter 7. For example, if a utility company is devising a \ncustom rate plan for all the utility consumers that are part of a huge customer with \nmany levels of offi  ces, branch locations, manufacturing locations, and sales loca-\ntions, you cannot use a ﬁ xed hierarchy. As pointed out in Chapter 7, the worst design \nis a set of generic levels named such as Level-1, Level-2, and so on. This makes for \nan unusable customer dimension because you don’t know how to constrain against \nthese levels when you have a ragged hierarchy of indeterminate depth.\n Bridge Tables for Multivalued Dimensions\nA  fundamental tenet of dimensional modeling is to decide on the grain of the fact \ntable, and then carefully add dimensions and facts to the design that are true to \nthe grain. For example, if you record customer purchase transactions, the grain of \n",
      "content_length": 2949,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 282,
      "content": "Chapter 8\n246\nthe individual purchase is natural and physically compelling. You do not want to \nchange that grain. Thus you normally require any dimension attached to this fact \ntable to take on a single value because then there’s a clean single foreign key in the \nfact table that identiﬁ es a single member of the dimension. Dimensions such as \nthe customer, location, product or service, and time are always single valued. But \nyou may have some “problem” dimensions that take on multiple values at the grain \nof the individual transaction. Common examples of these multivalued dimensions \ninclude:\n \n■Demographic descriptors drawn from a multiplicity of sources\n \n■Contact addresses for a commercial customer \n \n■Professional skills of a job applicant\n \n■Hobbies of an individual\n \n■Diagnoses or symptoms of a patient\n \n■Optional features for an automobile or truck\n \n■Joint account holders in a bank account\n \n■Tenants in a rental property\nWhen faced with a multivalued dimension, there are two basic choices: a posi-\ntional design or bridge table design. Positional designs are very attractive because \nthe multivalued dimension is spread out into named columns that are easy to query. \nFor example, if modeling the hobbies of an individual as previously mentioned, \nyou could have a hobby dimension with named columns for all the hobbies gath-\nered from your customers, including stamp collecting, coin collecting, astronomy, \nphotography, and many others! Immediately you can see the problem. The posi-\ntional design approach isn’t very scalable. You can easily run out of columns in \nyour database, and it is awkward to add new columns. Also if you have a column \nfor every possible hobby, then any single individual’s hobby dimension row will \ncontain mostly null values.\nThe bridge table approach to multivalued dimensions is powerful but comes \nwith a big compromise. The bridge table removes the scalability and null value \nobjections because rows in the bridge table exist only if they are actually needed, \nand you can add hundreds or even thousands of hobbies in the previous example. \nBut the resulting table design requires a complex query that must be hidden from \ndirect view by the business users.\nWARNING \nBe aware that complex queries using bridge tables may require SQL \nthat is beyond the normal reach of BI tools. \n",
      "content_length": 2341,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 283,
      "content": "Customer Relationship Management 247\nIn the next two sections, we illustrate multivalued bridge table designs that \nﬁ t with the customer-centric topics of this chapter. We will revisit multivalued \nbridges in Chapter 9: Human Resources Management, Chapter 10: Financial \nServices, Chapter 13: Education, Chapter 14:  Healthcare, and Chapter 16: Insurance. \nWe’ll then describe how to build these bridges in Chapter 19: ETL Subsystems and \nTechniques.\nBridge Table for Sparse Attributes\nOrganizations  are increasingly collecting demographics and status information \nabout their customers, but the traditional ﬁ xed column modeling approach for \nhandling these attributes becomes diffi  cult to scale with hundreds of attributes.\nPositional designs have a named column for each attribute. BI tool interfaces are \neasy to construct for positional attributes because the named columns are easily \npresented in the tool. Because many columns contain low cardinality contents, the \nquery performance using these attributes can be very good if bitmapped indexes \nare placed on each column. Positional designs can be scaled up to perhaps 100 or \nso columns before the databases and user interfaces become awkward or hard to \nmaintain. Columnar databases are well suited to these kinds of designs because \nnew columns can be easily added with minimal disruption to the internal storage \nof the data, and the low-cardinality columns containing only a few discrete values \nare dramatically compressed.\nWhen the number of diff erent attributes grows beyond your comfort zone, and \nif new attributes are added frequently, a bridge table is recommended. Ultimately, \nwhen you have a very large and expanding set of demographics indicators, using \noutriggers or mini-dimensions simply does not gracefully scale. For example, you \nmay collect loan application information as a set of open ended name-value pairs, \nas shown in Figure 8-7. Name-value pair data is interesting because the values can \nbe numeric, textual, a ﬁ le pointer, a URL, or even a recursive reference to enclosed \nname-value pair data.\nOver a period of time, you could collect hundreds or even thousands of diff erent \nloan application variables. For a true name-value pair data source, the value ﬁ eld \nitself can be stored as a text string to handle the open-ended modality of the val-\nues, which is interpreted by the analysis application. In these situations whenever \nthe number of variables is open-ended and unpredictable, a bridge table design is \nappropriate, as shown in Figure  8-8.\n",
      "content_length": 2551,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 284,
      "content": "Chapter 8\n248\nLoan Application Name-Value Pair Data\nPhotograph: <image>\nPrimary Income: $72345 \nOther Taxable Income: $2345 \nTax-Free Income: $3456\nLong Term Gains: $2367 \nGarnished Wages: $789 \nPending Judgment Potential: $555 \nAlimony: $666 \nJointly Owned Real Estate Appraised Value: $123456 \nJointly Owned Real Estate Image: <image>\nJointly Owned Real Estate MLS Listing: <URL>\nPercentage Ownership Real Estate: 50 \nNumber Dependents: 4 \nPre-existing Medical Disability: Back Injury\nNumber of Weeks Lost to Disability: 6\nEmployer Disability Support Statement: <document archive>\nPrevious Bankruptcy Declaration Type: 11 \nYears Since Bankruptcy: 8 \nSpouse Financial Disclosure: <name-value pair>\n... 100 more name-value pairs...\nFigure 8-7: Loan application name-value pair data.\nLoan Application Fact\nApplication Date Key (FK)\nApplicant Key (FK)\nLoan Type Key (FK)\nApplication ID (DD)\nLoan Officer Key (FK)\nUnderwriter Key (FK)\nBranch Key (FK)\nStatus Key (FK)\nApplication Disclosure Key (FK)\nApplication Disclosure Dimension\nApplication Disclosure Key (PK)\nApplication Disclosure Description\nApplication Disclosure Bridge\nApplication Disclosure Key (FK)\nDisclosure Item Key (FK)\nDisclosure Item Dimension\nDisclosure Item Key (PK)\nItem Name\nItem Value Type\nItem Value Text String\nFigure 8-8: Bridge table for wide and sparse name-value pair data set.\nBridge Table for Multiple Customer Contacts\nLarge  commercial customers have many points of contact, including decision mak-\ners, purchasing agents, department heads, and user liaisons; each point of contact is \nassociated with a speciﬁ c role. Because the number of contacts is unpredictable but \npossibly large, a bridge table design is a convenient way to handle this situation, as \nshown in Figure 8-9. Some care should be taken not to overdo the contact dimen-\nsion and make it a dumping ground for every employee or citizen or salesperson or \nhuman being the organization interacts with. Restrict the dimension for this use \ncase of contacts as part of the customer relationship.\n",
      "content_length": 2040,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 285,
      "content": "Customer Relationship Management 249\nCustomer Dimension\nCustomer Key (PK)\nCustomer Name \nCustomer Type\nCustomer Contact Group (FK)\nDate of First Contact\n...\nContact Group Dimension\nContact Group Key (PK)\nContact Group Name\nContact Group Bridge\nContact Group Key (FK)\nContact Key (FK)\nContact Role\nContact Dimension\nContact Key (PK)\nContact Name\nContact Street Address\n...\nFigure 8-9: Bridge table design for multiple contacts.\nComplex Customer Behavior\nCustomer behavior can be very complex. In this section, we’ll discuss the han-\ndling of customer cohort groups and capturing sequential behavior. We’ll also cover \nprecise timespan fact tables and tagging fact events with indicators of customer \nsatisfaction or abnormal scenarios.\n Behavior Study Groups for Cohorts\nWith  customer analysis, simple queries such as how much was sold to custom-\ners in this geographic area in the past year rapidly evolve to much more complex \ninquiries, such as how many customers bought more this past month than their \naverage monthly purchase amount from last year. The latter question is too complex \nfor business users to express in a single SQL request. Some BI tool vendors allow \nembedded subqueries, whereas others have implemented drill-across capabilities \nin which complex requests are broken into multiple select statements and then \ncombined in a subsequent pass.\nIn  other situations, you may want to capture the set of customers from a query or \nexception report, such as the top 100 customers from last year, customers who spent \nmore than $1,000 last month, or customers who received a speciﬁ c test solicitation, and \nthen use that group of customers, called a behavior study group, for subsequent analyses \nwithout reprocessing to identify the initial condition. To create a behavior study group, \nrun a query (or series of queries) to identify the set of customers you want to further \nanalyze, and then capture the customer durable keys of the identiﬁ ed set as an actual \nphysical table consisting of a single customer key column. By leveraging the custom-\ners’ durable keys, the study group dimension is impervious to type 2 changes to the \ncustomer dimension which may occur after the study group members are identiﬁ ed.\nNOTE \nThe secret to building complex behavioral study group queries is to \ncapture the keys of the customers or products whose behavior you are tracking. \nYou then use the captured keys to subsequently constrain other fact tables without \nhaving to rerun the original behavior analysis.\n",
      "content_length": 2519,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 286,
      "content": "Chapter 8\n250\nYou can now use this special behavior study group dimension table of customer \nkeys whenever you want to constrain any analysis on any table to that set of spe-\ncially deﬁ ned customers. The only requirement is that the fact table contains a \ncustomer key reference. The use of the behavior study group dimension is shown \nin Figure 8-10.\nCustomer Behavior Study\nGroup Dimension\nCustomer ID (Durable Key)\nCustomer Dimension\nCustomer Key (PK)\nCustomer ID (Durable Key)\n...\nPOS Retail Sales Transaction Fact\nDate Key (FK)\nCustomer Key (FK)\nMore FKs ...\nSales Quantity\nSales Dollar Amount\nFigure 8-10: Behavior study group dimension joined to customer dimension’s \ndurable key.\nThe behavior study group dimension is attached with an equijoin to the customer \ndimension’s durable key (refer to Customer ID in Figure 8-10). This can even be \ndone in a view that hides the explicit join to the behavior dimension. In this way, \nthe resulting dimensional model looks and behaves like an uncomplicated star. If the \nspecial dimension table is hidden under a view, it should be labeled to uniquely \nidentify it as being associated with the top 100 customers, for example. Virtually \nany BI tool can now analyze this specially restricted schema without paying syn-\ntax or user-interface penalties for the complex processing that deﬁ ned the original \nsubset of customers.\nNOTE \nThe exceptional simplicity of study group tables allows them to be com-\nbined with union, intersection, and set diff erence operations. For example, a set of \nproblem customers this month can be intersected with the set of problem custom-\ners from last month to identify customers who were problems for two consecutive \nmonths.\nStudy groups can be made even more powerful by including an occurrence date \nas a second column correlated with each durable key. For example, a panel study \nof consumer purchases can be conducted where consumers enter the study when \nthey exhibit some behavior such as switching brands of peanut butter. Then fur-\nther purchases can be tracked after the event to see if they switched brands again. \nTo get this right, these purchase events must be tracked with the right time stamps to \nget the behavior in the right sequence.\nLike many design decisions, this one represents certain compromises. First, \nthis approach requires a user interface for capturing, creating, and administering \n",
      "content_length": 2399,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 287,
      "content": "Customer Relationship Management 251\nreal physical behavior study group tables in the data warehouse. After a complex \nexception report has been deﬁ ned, you need the ability to capture the resulting keys \ninto an applet to create the special behavior study group dimension. These study \ngroup tables must live in the same space as the primary fact table because they are \ngoing to be joined directly to the customer dimension table. This obviously aff ects \nthe DBA’s responsibilities. \n Step Dimension for Sequential Behavior\nMost  DW/BI systems do not have good examples of sequential processes. Usually \nmeasurements are taken at a particular place watching the stream of customers or \nproducts going by. Sequential measurements, by contrast, need to follow a customer \nor a product through a series of steps, often measured by diff erent data capture \nsystems. Perhaps the most familiar example of a sequential process comes from \nweb events where a session is constructed by collecting individual page events on \nmultiple web servers tied together via a customer’s cookie. Understanding where \nan individual step ﬁ ts in the overall sequence is a major challenge when analyzing \nsequential processes.\nBy introducing a step dimension, you can place an individual step into the context \nof an overall session, as shown in Figure 8-11. \nTransaction Fact\nTransaction Date Key (FK)\nCustomer Key (FK)\nSession Key (FK)\nTransaction Number (DD)\nSession Step Key (FK)\nPurchase Step Key (FK)\nAbandon Step Key (FK)\nMore FKs ...\nFacts...\nStep Dimension (3 Roles)\nStep Key (PK)\nTotal Number Steps\nThis Step Number\nSteps Until End\nSample Step Dimension Rows:\nStep\nKey\nTotal\nNumber\nSteps\nThis\nStep\nNumber\nSteps\nUntil\nEnd\n1 \n2 \n3 \n4 \n5 \n6 \n7 \n8 \n9 \n10\n1 \n2 \n2 \n3 \n3 \n3 \n4 \n4 \n4 \n4\n1 \n1 \n2 \n1 \n2 \n3 \n1 \n2 \n3 \n4\n0 \n1 \n0 \n2 \n1 \n0 \n3 \n2 \n1 \n0\nFigure 8-11: Step dimension to capture sequential activities.\nThe step dimension is an abstract dimension deﬁ ned in advance. The ﬁ rst row in \nthe dimension is used only for one-step sessions, where the current step is the ﬁ rst \nstep and there are no more steps remaining. The next two rows in the step dimension \nare used for two-step sessions. The ﬁ rst row (Step Key = 2) is for step number 1 where \nthere is one more step to go, and the next row (Step Key = 3) is for step number 2 \n",
      "content_length": 2317,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 288,
      "content": "Chapter 8\n252\nwhere there are no more steps. The step dimension can be prebuilt to accommodate \nsessions of at least 100 steps. In Figure 8-11 you see the step dimension can be asso-\nciated with a transaction fact table whose grain is the individual page event. In this \nexample, the step dimension has three roles. The ﬁ rst role is the overall session. The \nsecond role is a successful purchase subsession, where a sequence of page events \nleads to a conﬁ rmed purchase. The third role is the abandoned shopping cart, where \nthe sequence of page events is terminated without a purchase.\nUsing the step dimension, a speciﬁ c page can immediately be placed into one or \nmore understandable contexts (overall session, successful purchase, and abandoned \nshopping cart). But even more interestingly, a query can constrain exclusively only \nto the ﬁ rst page of successful purchases. This is a classic web event query, where \nthe “attractant” page of successful sessions is identiﬁ ed. Conversely, a query could \nconstrain exclusively to the last page of abandoned shopping carts, where the cus-\ntomer is about to decide to go elsewhere.\nAnother approach for modeling sequential behavior takes advantage of speciﬁ c \nﬁ xed codes for each possible step. If you track customer product purchases in a \nretail environment, and if each product can be encoded, for instance, as a 5 digit \nnumber, then you can create a single wide text column for each customer with the \nsequence of product codes. You separate the codes with a unique non-numeric \ncharacter. Such a sequence might look like\n11254|45882|53340|74934|21399|93636|36217|87952|…etc.\nNow using wild cards you can search for speciﬁ c products bought sequentially, \nor bought with other products intervening, or situations in which one product was \nbought but another was never bought. Modern relational DBMSs can store and \nprocess wide text ﬁ elds of 64,000 characters or more with wild card searches.\n Timespan Fact Tables\nIn more  operational applications, you may want to retrieve the exact status of a cus-\ntomer at some arbitrary instant in the past. Was the customer on fraud alert when \ndenied an extension of credit? How long had he been on fraud alert? How many \ntimes in the past two years has he been on fraud alert? How many customers were on \nfraud alert at some point in the past two years? All these questions can be addressed \nif you carefully manage the transaction fact table containing all customer events. The \nkey modeling step is to include a pair of date/time stamps, as shown in Figure 8-12. \nThe ﬁ rst date/time stamp is the precise moment of the transaction, and the second \ndate/time stamp is the exact moment of the next transaction. If this is done correctly, \nthen the time history of customer transactions maintains an unbroken sequence \nof date/time stamps with no gaps. Each actual transaction enables you to associate \n",
      "content_length": 2905,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 289,
      "content": "Customer Relationship Management 253\nboth demographics and status with the customer. Dense transaction fact tables are \ninteresting because you potentially can change the demographics and especially \nthe status each time a transaction occurs.\nDate Dimension\nDemographics Dimension\nCustomer Dimension\nStatus Dimension\nCustomer Transaction Fact\nTransaction Date Key (FK)\nCustomer Key (FK)\nDemographics Key (FK)\nStatus Key (FK)\nTransaction Number (DD)\nMore FKs ...\nBegin Effective Date/Time\nEnd Effective Date/Time\nAmount\nFigure 8-12: Twin date/time stamps in a timespan fact table.\nThe critical insight is that the pair of date/time stamps on a given transaction \ndeﬁ nes a span of time in which the demographics and the status are constant. \nQueries can take advantage of this “quiet” span of time. Thus if you want to know \nwhat the status of the customer “Jane Smith” was on July 18, 2013 at 6:33 am, you \ncan issue the following query:\nSelect Customer.Customer_Name, Status\nFrom Transaction_Fact, Customer_dim, Status_dim\nWhere Transaction_Fact_Customer_Key = Customer_dim.Customer_key\n    And Transaction_Fact.Status_key = Status_dim.Status_key\n    And Customer_dim.Customer_Name = 'Jane Smith' \n    And #July 18, 2013 6:33:00# >= Transaction_Fact.Begin_Eff_\nDateTime\n    And #July 18, 2013 6:33:00# < Transaction_Fact.End_Eff_DateTime\nThese date/time stamps can be used to perform tricky queries on your customer \nbase. If you want to ﬁ nd all the customers who were on fraud alert sometime in the \nyear 2013, issue the following query:\nSelect Customer.Customer_Name\nFrom Transaction_Fact, Customer_dim, Status_dim\nWhere <joins>\n   And Status_dim Status_Description = 'Fraud Alert'\n   And Transaction_Fact.Begin_Eff_DateTime <= 12/31/2013:23:59:59\n   And Transaction_Fact.End_Eff_DateTime >= 1/1/2013:0:0:0\nAmazingly, this one query handles all the possible cases of begin and end eff ec-\ntive date/times straddling the beginning or end of 2013, being entirely contained \nwith 2013, or completely straddling 2013.\n",
      "content_length": 2018,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 290,
      "content": "Chapter 8\n254\nYou can even count the number of days each customer was on fraud alert in 2013:\nSelect Customer.Customer_Name,\n    sum( least(12/31/2013:23:59:59, Transaction_Fact.End_Eff_\nDateTime) \n       - greatest(1/1/2013:0:0:0, Transaction_Fact.Begin_Eff_\nDateTime))\nFrom Transaction_Fact, Customer_dim, Status_dim\nWhere <joins>\n   And Status_dim Status_Description = 'Fraud Alert'\n   And Transaction_Fact.Begin_Eff_DateTime <= 12/31/2013:23:59:59\n   And Transaction_Fact.End_Eff_DateTime >= 1/1/2013:0:0:0\nGroup By Customer.Customer_Name\nBack Room Administration of Dual Date/Time Stamps\nFor a given customer, the date/time stamps on the sequence of transactions must \nform a perfect unbroken sequence with no gaps. It is tempting to make the end \neff ective date/time stamp be one “tick” less than the beginning eff ective date/time \nstamp of the next transaction, so the query SQL can use the BETWEEN syntax \nrather than the uglier constraints shown above. However, in many situations the \nlittle gap deﬁ ned by that tick could be signiﬁ cant if a transaction could fall within \nthe gap. By making the end eff ective date/time exactly equal to the begin date time \nof the next transaction, you eliminate this risk.\nUsing the pair of date/time stamps requires a two-step process whenever a new \ntransaction row is entered. In the ﬁ rst step, the end eff ective date/time stamp of \nthe most current transaction must be set to a ﬁ ctitious date/time far in the future. \nAlthough it would be semantically correct to insert NULL for this date/time, nulls \nbecome a headache when you encounter them in constraints because they can cause \na database error when you ask if the ﬁ eld is equal to a speciﬁ c value. By using a \nﬁ ctitious date/time far in the future, this problem is avoided.\nIn the second step, after the new transaction is entered into the database, the ETL \nprocess must retrieve the previous transaction and set its end eff ective date/time to \nthe date/time of the newly entered transaction. Although this two-step process is a \nnoticeable cost of this twin date/time approach, it is a classic and desirable trade-\noff  between extra ETL overhead in the back room and reduced query complexity \nin the front  room.\n Tagging Fact Tables with Satisfaction Indicators\nAlthough  proﬁ tability might be the most important key performance indicator in \nmany organizations, customer satisfaction is a close second. And in organizations \nwithout proﬁ t metrics, such as government agencies, satisfaction is (or should be) \nnumber one.\n",
      "content_length": 2544,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 291,
      "content": "Customer Relationship Management 255\nSatisfaction, like profitability, requires integration across many sources. \nVirtually every customer facing process is a potential source of satisfaction infor-\nmation, whether the source is sales, returns, customer support, billing, website \nactivity, social media, or even geopositioning data.\nSatisfaction data can be either numeric or textual. In the Chapter 6: Order \nManagement, you saw how classic measures of customer satisfaction could be \nmodeled both ways simultaneously. The on-time measures could be both additive \nnumeric facts as well as textual attributes in a service level dimension. Other purely \nnumeric measures of satisfaction include numbers of product returns, numbers \nof lost customers, numbers of support calls, and product attitude metrics from \nsocial media.\nFigure 8-13 illustrates a frequent ﬂ yer satisfaction dimension that could be added \nto the ﬂ ight activity fact tables described in Chapter 12. Textual satisfaction data is \ngenerally modeled in two ways, depending on the number of satisfaction attributes \nand the sparsity of the incoming data. When the list of satisfaction attributes is \nbounded and reasonably stable, a positional design is very eff ective, as shown in \nFigure 8-13. \nSatisfaction Dimension\nSatisfaction Key (PK)\nDelayed Arrival Indicator\nDiversion to Other Airport Indicator\nLost Luggage Indicator\nFailure to Get Upgrade Indicator\nMiddle Seat Indicator\nPersonnel Problem Indicator\nFigure 8-13: Positional satisfaction dimension for airline frequent ﬂ yers.\nTagging Fact Tables with Abnormal \nScenario Indicators\nAccumulating  snapshot fact tables depend on a series of dates that implement the \n“standard scenario” for the pipeline process. For order fulﬁ llment, you may have \nthe steps of order created, order shipped, order delivered, order paid, and order \nreturned as standard steps in the order scenario. This kind of design is successful \nwhen 90 percent or more of the orders progress through these steps (hopefully \nwithout the return) without any unusual exceptions.\nBut if an occasional situation deviates from the standard scenario, you don’t \nhave a good way to reveal what happened. For example, maybe when the order \n",
      "content_length": 2231,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 292,
      "content": "Chapter 8\n256\nwas shipped, the delivery truck had a ﬂ at tire. A decision was made to unload the \ndelivery to another truck, but unfortunately it began to rain and the shipment was \nwater damaged. Then it was refused by the customer, and ultimately there was a \nlawsuit. None of these unusual steps are modeled in the standard scenario in the \naccumulating snapshot. Nor should they be!\nThe way to describe unusual departures from the standard scenario is to add \na delivery status dimension to the accumulating snapshot fact table. For the case \nof the weird delivery scenario, you tag this order fulﬁ llment row with the status \nWeird. Then if the analyst wants to see the complete story, the analyst can join to a \ncompanion transaction fact table through the order number and line number that \nhas every step of the story. The transaction fact table joins to a transaction dimen-\nsion, which indeed has Flat Tire, Damaged Shipment, and Lawsuit as transactions. \nEven though this transaction dimension will grow over time with unusual entries, \nit is well bounded and stable.\n Customer Data Integration Approaches\nIn  typical environments with many customer facing processes, you need to choose \nbetween two approaches: a single customer dimension derived from all the versions \nof customer source system records or multiple customer dimensions tied together \nby conformed attributes.\nMaster Data Management Creating a Single \nCustomer Dimension\nIn  some cases, you can build a single customer dimension that is the “best of breed” \nchoice among a number of available customer data sources. It is likely that such \na conformed customer dimension is a distillation of data from several operational \nsystems within your organization. But it would be typical for a unique customer to \nhave multiple identiﬁ ers in multiple touch point systems. To make matters worse, \ndata entry systems often don’t incorporate adequate validation rules. Obviously, an \noperational CRM objective is to create a unique customer identiﬁ er and restrict the \ncreation of unnecessary identiﬁ ers. In the meantime, the DW/BI team will likely \nbe responsible for sorting out and integrating the disparate sources of customer \ninformation.\nSome  organizations are lucky enough to have a centralized master data manage-\nment (MDM) system that takes responsibility for creating and controlling the single \nenterprise-wide customer entity. But such centralization is rare in the real world. \nMore frequently, the data warehouse extracts multiple incompatible customer data \n",
      "content_length": 2547,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 293,
      "content": "Customer Relationship Management 257\nﬁ les and builds a “downstream” MDM system. These two styles of MDM are illus-\ntrated in Figure 8-14.\nEnterprise\nMDM\nDownstream\nMDM\nOperational\nApp #1\nOperational\nApp #2\nOperational\nApp #3\nOperational\nApp #1\nOperational\nApp #2\nOperational\nApp #3\nEDW\nEDW\nFigure 8-14: Two styles of master data management.\nUnfortunately,  there’s no secret weapon for tackling this data consolidation. The \nattributes in the customer dimension should represent the “best” source available \nin the enterprise. A national change of address (NCOA) process should be integrated \nto ensure address changes are captured. Much of the heavy lifting associated with \ncustomer data consolidation demands customer matching or deduplicating logic. \nRemoving duplicates or invalid addresses from large customer lists is critical to \neliminate the costs associated with redundant, misdirected, or undeliverable com-\nmunication, avoid misleading customer counts, and improve customer satisfaction \nthrough higher quality communication.\nThe  science of customer matching is more sophisticated than it might ﬁ rst appear. \nIt involves fuzzy logic, address parsing algorithms, and enormous look-up direc-\ntories to validate address elements and postal codes, which vary signiﬁ cantly by \ncountry. There are specialized, commercially available software and service off erings \nthat perform individual customer or commercial entity matching with remarkable \naccuracy. Often these products match the address components to standardized \ncensus codes, such as state codes, country codes, census tracts, block groups, metro-\npolitan statistical areas (MSAs), and latitude/longitude, which facilitate the merging \n",
      "content_length": 1708,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 294,
      "content": "Chapter 8\n258\nof external data. As discussed in Chapter 10, there are also householding capabilities \nto group or link customers sharing similar name and/or address information. Rather \nthan merely performing intraﬁ le matching, some services maintain an enormous \nexternal reference ﬁ le of everyone in the United States to match against. Although \nthese products and services are expensive and/or complex, it’s worthwhile to make \nthe investment if customer matching is strategic to the organization. In the end, \neff ective consolidation of customer data depends on a balance of capturing the data \nas accurately as possible in the source systems, coupled with powerful data cleans-\ning/merging tools in the ETL process.\nPartial Conformity of Multiple Customer Dimensions\nEnterprises  today build customer knowledge stores that collect all the internal and \nexternal customer-facing data sources they can ﬁ nd. A large organization could \nhave as many as 20 internal data sources and 50 or more external data sources, \nall of which relate in some way to the customer. These sources can vary wildly in \ngranularity and consistency. Of course, there is no guaranteed high-quality customer \nkey deﬁ ned across all these data sources and no consistent attributes. You don’t have \nany control over these sources. It seems like a hopeless mess.\nIn Chapter 4: Inventory, we laid the groundwork for conformed dimensions, \nwhich are the required glue for achieving integration across separate data sources. \nIn the ideal case, you examine all the data sources and deﬁ ne a single compre-\nhensive dimension which you attach to all the data sources, either simultaneously \nwithin a single tablespace or by replicating across multiple tablespaces. Such a \nsingle comprehensive conformed dimension becomes a wonderful driver for creating \nintegrated queries, analyses, and reports by making consistent row labels available \nfor drill-across queries.\nBut in the extreme integration world with dozens of customer-related dimensions \nof diff erent granularity and diff erent quality, such a single comprehensive customer \ndimension is impossible to build. Fortunately, you can implement a lighter weight \nkind of conformed customer dimension. Remember the essential requirement for \ntwo dimensions to be conformed is they share one or more specially administered \nattributes that have the same column names and data values. Instead of requiring \ndozens of customer-related dimensions to be identical, you only require they share \nthe specially administered conformed attributes.\nNot  only have you taken the pressure off  the data warehouse by relaxing the \nrequirement that all the customer dimensions in your environment be equal from \ntop to bottom, but in addition you can proceed in an incremental and agile way \nto plant the specially administered conformed attributes in each of the customer-\nrelated dimensions. For example, suppose you start by deﬁ ning a fairly high-level \n",
      "content_length": 2971,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 295,
      "content": "Customer Relationship Management 259\ncategorization of customers called customer category. You can proceed methodically \nacross all the customer-related dimensions, planting this attribute in each dimension \nwithout changing the grain of any target dimension and without invalidating any \nexisting applications that depend on those dimensions. Over a period of time, you \ngradually increase the scope of integration as you add the special attributes to the \nseparate customer dimensions attached to diff erent sources. At any point in time, \nyou can stop and perform drill-across reports using the dimensions where you have \ninserted the customer category attribute.\nWhen the customer category attribute has been inserted into as many of the \ncustomer-related dimensions as possible, you can then deﬁ ne more conformed attri-\nbutes. Geographic attributes such as city, county, state, and country should be even \neasier than the customer category. Over a period of time, the scope and power of the \nconformed customer dimensions let you do increasingly sophisticated analyses. This \nincremental development with its closely spaced deliverables ﬁ ts an agile approach.\n Avoiding Fact-to-Fact Table Joins\nDW/BI  systems should be built process-by-process, not department-by-department, \non a foundation of conformed dimensions to support integration. You can imagine \nquerying the sales or support fact tables to better understand a customers’ purchase \nor service history. \nBecause  the sales and support tables both contain a customer foreign key, you \ncan further imagine joining both fact tables to a common customer dimension to \nsimultaneously summarize sales facts along with support facts for a given customer. \nUnfortunately, the many-to-one-to-many join will return the wrong answer in a \nrelational environment due to the diff erences in fact table cardinality, even when \nthe relational database is working perfectly. There is no combination of inner, outer, \nleft, or right joins that produces the desired answer when the two fact tables have \nincompatible cardinalities.\nConsider the case in which you have a fact table of customer solicitations, \nand another fact table with the customer responses to solicitations, as shown in \nFigure 8-15. There is a one-to-many relationship between customer and solicita-\ntion, and another one-to-many relationship between customer and response. The \nsolicitation and response fact tables have diff erent cardinalities; in other words, not \nevery solicitation results in a response (unfortunately for the marketing department) \nand some responses are received for which there is no solicitation. Simultaneously \njoining the solicitations fact table to the customer dimension, which is, in turn, \njoined to the responses fact table, does not return the correct answer in a relational \nDBMS due to the cardinality diff erences. Fortunately, this problem is easily avoided. \nYou simply issue the drill-across technique explained in Chapter 4 to query the \n",
      "content_length": 3002,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 296,
      "content": "Chapter 8\n260\nsolicitations table and responses table in separate queries and then outer join the two \nanswer sets. The drill-across approach has additional beneﬁ ts for better controlling \nperformance parameters, in addition to supporting queries that combine data from \nfact tables in diff erent physical locations.\nCustomer Solicitation Fact\nSolicitation Date Key (FK)\nCustomer Key (FK)\nMore FKs ...\nSolicitation Facts ...\nCustomer Response Fact\nResponse Date Key (FK)\nCustomer Key (FK)\nMore FKs ...\nResponse Facts ...\nCustomer Dimension\nCustomer Key (PK)\nCustomer ID (Natural Key)\n...\nFigure 8-15: Many-to-one-to-many joined tables should not be queried with a single \nSELECT statement.\nWARNING \nBe very careful when simultaneously joining a single dimension \ntable to two fact tables of diff erent cardinality. In many cases, relational engines \nreturn the “wrong” answer.\nIf business users are frequently combining data from multiple business pro-\ncesses, a ﬁ nal approach is to deﬁ ne an additional fact table that combines the data \nonce into a consolidated fact table rather than relying on users to consistently \nand accurately combine the data on their own, as described in Chapter 7. Merely \nusing SQL to drill across fact tables to combine the results makes more sense when \nthe underlying processes are less closely correlated. Of course, when constructing the \nconsolidated fact table, you still need to establish business rules to deal with \nthe diff ering cardinality. For example, does the consolidated fact table include all the \nsolicitations and responses or only those where both a solicitation and response \noccurred?\nLow Latency Reality Check \nThe  behavior of a customer in the last few hours or minutes can be extremely inter-\nesting. You may even want to make decisions while dealing with the customer in \nreal time. But you need to be thoughtful in recognizing the costs and limitations \nof low latency data. Generally, data quality suff ers as the data is delivered closer \nto real time.\nBusiness users may automatically think that the faster the information arrives in \nthe DW/BI system, the better. But decreasing the latency increases the data quality \n",
      "content_length": 2185,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 297,
      "content": "Customer Relationship Management 261\nproblems. Figure 20-6 summarizes the issues that arise as data is delivered faster. \nIn the conventional batch world, perhaps downloading a batch ﬁ le once each 24 \nhours, you typically get complete transaction sets. For example, if a commercial \ncustomer places an order, they may have to pass a credit check and verify the ﬁ nal \ncommitment. The batch download includes orders only where all these steps have \ntaken place. In addition, because the batch download is processed just once each 24 \nhours, the ETL team has the time to run the full spectrum of data quality checks, \nas we’ll describe in Chapter 19: ETL Subsystems and Techniques.\nIf the data is extracted many times per day, then the guarantee of complete \ntransaction sets may have to be relinquished. The customer may have placed the \norder but has not passed the credit check. Thus there is the possibility that results \nmay have to be adjusted after the fact. You also may not run the full spectrum of \ndata quality checks because you don’t have time for extensive multitable lookups. \nFinally, you may have to post data into the data warehouse when all the keys have \nnot been resolved. In this case, temporary dimensional entries may need to be used \nwhile waiting for additional data feeds.\nFinally, if you deliver data instantaneously, you may be getting only transaction \nfragments, and you may not have time to perform any data quality checks or other \nprocessing of the data.\nLow latency data delivery can be very valuable, but the business users need to \nbe informed about these trade-off s. An interesting hybrid approach is to provide \nlow latency intraday delivery but then revert to a batch extract at night, thereby \ncorrecting various data problems that could not be addressed during the day. We \ndiscuss the impact of low latency requirements on the ETL system in Chapter 20: \nETL System Design and Development Process and Tasks.\nSummary\nIn this chapter, we focused exclusively on the customer, beginning with an over-\nview of customer relationship management (CRM) basics. We then delved into \ndesign issues surrounding the customer dimension table. We discussed name and \naddress parsing where operational ﬁ elds are decomposed to their basic elements \nso that they can be standardized and validated. We explored several other types \nof common customer dimension attributes, such as dates, segmentation attributes, \nand aggregated facts. Dimension outriggers that contain a large block of relatively \nlow-cardinality attributes were described.\nThis chapter introduced the use of bridge tables to handle unpredictable, sparsely \npopulated dimension attributes, as well as multivalued dimension attributes. \n",
      "content_length": 2728,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 298,
      "content": "Chapter 8\n262\nWe also explored several complex customer behavior scenarios, including sequential \nactivities, timespan fact tables, and tagging fact events with indicators to identify \nabnormal situations.\nWe closed the chapter by discussing alternative approaches for consistently iden-\ntifying customers and consolidating a rich set of characteristics from the source \ndata, either via operational master data management or downstream processing in \nthe ETL back room with potentially partial conformity. Fi nally, we touched on the \nchallenges of low latency data requirements.\n",
      "content_length": 581,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 299,
      "content": "Human Resources \nManagement\nT\nhis  chapter, which focuses on human resources (HR) data, is the last in the series \ndealing with cross-industry business applications. Similar to the accounting \nand finance data described in Chapter 7: Accounting, HR information is dissemi-\nnated broadly throughout the organization. Organizations want to better understand \ntheir employees’ demographics, skills, earnings, and performance to maximize their \nimpact. In this chapter we’ll explore several dimensional modeling techniques in the \ncontext of HR data.\nChapter 9 discusses the following concepts:\n \n■Dimension tables to track employee proﬁ le changes\n \n■Periodic headcount snapshots\n \n■Bus matrix for a snippet of HR-centric processes\n \n■Pros and cons of packaged DW/BI solutions or data models\n \n■Recursive employee hierarchies\n \n■Multivalued skill keyword attributes handled via dimension attributes, out-\nriggers, or bridges\n \n■Survey questionnaire data\n \n■Text comments\n Employee Proﬁ le Tracking\nThus  far the dimensional models we have designed closely resemble each other; \nthe fact tables contain key performance metrics that typically can be added across \nall the dimensions. It is easy for dimensional modelers to get lulled into a kind of \nadditive complacency. In most cases, this is exactly how it is supposed to work. \nHowever, with HR employee data, a robust employee dimension supports numerous \nmetrics required by the business on its own.\n9\n",
      "content_length": 1453,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 300,
      "content": "Chapter 9\n264\nTo frame the problem with a business vignette, let’s assume you work in the HR \ndepartment of a large enterprise. Each employee has a detailed HR proﬁ le with \nat least 100 attributes, including hire date, job grade, salary, review dates, review \noutcomes, vacation entitlement, organization, education, address, insurance plan, \nand many others. Employees are constantly hired, transferred, and promoted, as \nwell as adjusting their proﬁ les in a variety of ways.\nA high-priority business requirement is to accurately track and analyze \nemployee proﬁ le changes. You might immediately visualize a schema in which \neach employee proﬁ le change event is captured in a transaction-grained fact table, \nas depicted in Figure 9-1. The granularity of this somewhat generalized fact \ntable would be one row per employee proﬁ le transaction. Because no numeric \nmetrics are associated with changes made to employee proﬁ les, such as a new \naddress or job grade promotion, the fact table is factless.\nEmployee Key (PK)\nEmployee ID (NK)\n...\nTransaction Date Key (FK)\nTransaction Date/Time\nEmployee Key (FK)\nEmployee Transaction Type Key (FK)\nEmployee Dimension\nEmployee Transaction Type Key (PK)\nEmployee Transaction Type Description\n1 row per employee profile transaction\nEmployee Transaction Type Dimension\nEmployee Transaction Fact\nTransaction Date Dimension\nFigure 9-1: Initial draft schema for tracking employees’ proﬁ le changes.\nIn this draft schema, the dimensions include the transaction date, transaction \ntype, and employee. The transaction type dimension refers to the reason code that \ncaused the creation of this particular row, such as a promotion or address change. \nThe employee dimension is extremely wide with many attribute columns.\nWe envision using the type 2 slowly changing dimension technique for tracking \nchanged proﬁ le attributes within the employee dimension. Consequently, with every \nemployee proﬁ le transaction in the Figure 9-1 fact table, you would also create a \nnew type 2 row in the employee dimension that represents the employee’s proﬁ le as \na result of the proﬁ le change event. This new row continues to accurately describe \nthe employee until the next employee transaction occurs at some indeterminate \ntime in the future. The alert reader is quick to point out that the employee proﬁ le \ntransaction fact table and type 2 employee dimension table have the same number of \nrows; plus they are almost always joined to one another. At this point dimensional \nmodeling alarms should be going off . You certainly don’t want to have as many rows \nin a fact table as you do in a related dimension table.\nInstead of using the initial schema, you can simplify the design by embellish-\ning the employee dimension table to make it more powerful and thereby doing \n",
      "content_length": 2804,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 301,
      "content": "Human Resources Management 265\naway with the proﬁ le transaction event fact table. As depicted in Figure 9-2, the \nemployee dimension contains a snapshot of the employee proﬁ le characteristics \nfollowing the employee’s proﬁ le change. The transaction type description becomes \na change reason attribute in the employee dimension to track the cause for the pro-\nﬁ le change. In some cases, the aff ected characteristics are numeric. If the numeric \nattributes are summarized rather than simply constrained upon, they belong in a \nfact table instead.\nEmployee Key (PK)\nEmployee ID (NK)\nEmployee Name ...\nEmployee Address ...\nJob Grade ...\nSalary ...\nEducation ...\nOriginal Hire Date (FK)\nLast Review Date (FK)\nAppraisal Rating ...\nHealth Insurance Plan ...\nVacation Plan ...\nChange Reason Code\nChange Reason Description\nRow Effective Date/Time\nRow Expiration Date/Time\nCurrent Row Indicator\nEmployee Dimension\nFigure 9-2: Employee dimension with proﬁ le characteristics.\nAs you’d expect, the surrogate employee key is the primary key of the dimen-\nsion table; the durable natural employee ID used in the HR operational system to \npersistently identify an employee is included as a dimension attribute.\nPrecise Effective and Expiration Timespans\nAs  discussed in Chapter 5: Procurement with the coverage of slowly changing \ndimension techniques, you should include two columns on the employee dimension \nto capture when a speciﬁ c row is eff ective and then expired. These columns deﬁ ne \na precise timespan during which the employee’s proﬁ le is accurate. Historically, \nwhen daily data latency was the norm, the eff ective and expiration columns were \ndates. However, if you load data from any business process on a more frequent basis, \nthe columns should be date/time stamps so that you can associate the appropriate \nemployee proﬁ le row, which may diff er between 9 a.m. and 9 p.m. on the same day, \nto operational events.\n",
      "content_length": 1927,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 302,
      "content": "Chapter 9\n266\nThe  expiration attribute for the current row is set to a future date. When the row \nneeds to be expired because the ETL system has detected a new proﬁ le of attributes, \nthe expiration attribute is typically set to “just before” the new row’s eff ective stamp, \nmeaning either the prior day, minute, or second.\nIf the employee’s proﬁ le is accurately changed for a period of time, then the \nemployee reverts back to an earlier set of characteristics, a new employee dimen-\nsion row is inserted. You should resist the urge to simply revisit the earlier proﬁ le \nrow and modify the expiration date because multiple dimension rows would be \neff ective at the same time.\nThe current row indicator enables the most recent status of any employee to be \nretrieved quickly. If a new proﬁ le row occurs for this employee, the indicator in the \nformer proﬁ le row needs to be updated to indicate it is no longer the current proﬁ le.\nOn its own, a date/time stamped type 2 employee dimension answers a number \nof interesting HR inquiries. You can choose an exact historical point in time and ask \nhow many employees you have and what their detailed proﬁ les were at that speciﬁ c \nmoment by constraining the date/time to be equal to or greater than the eff ective \ndate/time and strictly less than the expiration date/time. The query can perform \ncounts and constraints against all the rows returned from these constraints.\nDimension Change Reason Tracking\nWhen  a dimension row contains type 2 attributes, you can embellish it with a change \nreason. In this way, some ETL-centric metadata is embedded with the actual data. \nThe change reason attribute could contain a two-character abbreviation for each \nchanged attribute on a dimension row. For example, the change reason attribute \nvalue for a last name change could be LN or a more legible value, such as Last \nName, depending on the intended usage and audience. If someone asks how many \npeople changed ZIP codes last year, the SELECT statement would include a LIKE \noperator and wild cards, such as \"WHERE ChangeReason LIKE '%ZIP%’\".\nBecause multiple dimension attributes may change concurrently and be repre-\nsented by a single new row in the dimension, the change reason would be multi-\nvalued. As we’ll explore later in the chapter when discussing employee skills, the \nmultiple reason codes could be handled as a single text string attribute, such as \n“|Last Name|ZIP|” or via a multivalued bridge table.\nNOTE \nThe eff ective and expiration date/time stamps, along with a reason code \ndescription, on each row of a type 2 slowly changing dimension allows very precise \ntime slicing of the dimension by itself.\nFinally, employee proﬁ le changes may be captured in the underlying source \nsystem by a set of micro-transactions corresponding to each individual employee \n",
      "content_length": 2832,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 303,
      "content": "Human Resources Management 267\nattribute change. In the DW/BI system, you may want to encapsulate the series of \nmicro-transactions from the source system and treat them as a super transaction, \nsuch as an employee promotion because it would be silly to treat these artiﬁ cial \nmicro-transactions as separate type 2 changes. The new type 2 employee dimen-\nsion row would reﬂ ect all the relevant changed attributes in one step. Identifying \nthese super transactions may be tricky. Obviously the best way to identify them is \nto ensure the HR operational application captures the higher level action.\nProﬁ le Changes as Type 2 Attributes or Fact Events\nWe just described the handling of employee attribute changes as slowly changing \ndimension type 2 attributes with proﬁ le eff ective and expiration dates within the \nemployee dimension. Designers sometimes wholeheartedly embrace this pattern \nand try to leverage it to capture every employee-centric change. This results in a \ndimension table with potentially hundreds of attributes and millions of rows for \na 100,000-employee organization given the attributes’ volatility.\nTracking changes within the employee dimension table enables you to easily \nassociate the employee’s accurate proﬁ le with multiple business processes. You \nsimply load these fact tables with the employee key in eff ect when the fact event \noccurred, and ﬁ lter and group based on the full spectrum of employee attributes.\nBut the pendulum can swing too far. You probably shouldn’t use the employee \ndimension to track every employee review event, every beneﬁ t participation event, \nor every professional development event. As illustrated in Figure 9-4’s bus matrix \nin the next section, many of these events involve other dimensions, like an event \ndate, organization, beneﬁ t description, reviewer, approver, exit interviewer, separa-\ntion reasons, and the list goes on. Consequently, most of them should be handled \nas separate process-centric fact tables. Although many human resources events are \nfactless, capturing them within a fact table enables business users to easily count \nor trend by time periods and all the other associated dimensions.\nIt’s certainly common to include the outcome of these HR events, like the job \ngrade resulting from a promotion, as an attribute on the employee dimension. But \ndesigners sometimes err by including lots of foreign keys to outriggers for the \nreviewer, beneﬁ t, separation reason and other dimensions within the employee \ndimension, resulting in an overloaded dimension that’s diffi  cult to  navigate.\n Headcount Periodic Snapshot\nIn  addition to proﬁ ling employees in HR, you also want to report statuses of the \nemployees on a regular basis. Business managers are interested in counts, statistics, \nand totals, including number of employees, salary paid, vacation days taken, vaca-\ntion days accrued, number of new hires, and number of promotions. They want \n",
      "content_length": 2943,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 304,
      "content": "Chapter 9\n268\nto analyze the data by all possible slices, including time and organization, plus \nemployee characteristics.\nAs shown in Figure 9-3, the employee headcount periodic snapshot consists of an \nordinary looking fact table with three dimensions: month, employee, and organiza-\ntion. The month dimension table contains the usual descriptors for the corporate cal-\nendar at the month grain. The employee key corresponds to the employee dimension \nrow in eff ect at the end of the last day of the given reporting month to guarantee the \nmonth-end report is a correct depiction of the employees’ proﬁ les. The organization \ndimension contains a description of the organization to which the employee belongs \nat the close of the relevant month.\n1 row per employee per month\nMonth Key (FK)\nOrganization Key (FK)\nEmployee Key (FK)\nEmployee Count\nNew Hire Count\nTransfer Count\nPromotion Count\nSalary Paid\nOvertime Paid\nRetirement Fund Paid\nRetirement Fund Employee Contribution\nVacation Days Accrued\nVacation Days Taken\nVacation Days Balance\nEmployee Headcount Snapshot Fact\nMonth Dimension\nEmployee Dimension\nOrganization Dimension\nFigure 9-3: Employee headcount periodic snapshot.\nThe facts in this headcount snapshot consist of monthly numeric metrics and \ncounts that may be diffi  cult to calculate from the employee dimension table alone. \nThese monthly counts and metrics are additive across all the dimensions or dimen-\nsion attributes, except for any facts labeled as balances. These balances, like all \nbalances, are semi-additive and must be averaged across the month dimension after \nadding across the other dimensions.\n Bus Matrix for HR Processes\nAlthough  an employee dimension with precise type 2 slowly changing dimension \ntracking coupled with a monthly periodic snapshot of core HR performance metrics \nis a good start, they just scratch the surface when it comes to tracking HR data. \n",
      "content_length": 1906,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 305,
      "content": "Human Resources Management 269\nFigure 9-4 illustrates other processes that HR professionals and functional manag-\ners are likely keen to analyze. We’ve embellished this preliminary bus matrix with \nthe type of fact table that might be used for each process; however, your source \ndata realities and business requirements may warrant a diff erent or complementary \ntreatment.\nEmployee Position Snapshot\nEmployee Requisition Pipeline\nEmployee Performance Review Pipeline\nEmployee Disciplinary Action Pipeline\nEmployee Separations\nEmployee Performance Review\nEmployee Prof Dev Completed Courses\nEmployee Hiring\nEmployee “On Board” Pipeline\nEmployee Benefits Eligibility\nEmployee Benefits Application\nEmployee Benefit Participation\nEmployee Benefit Accruals\nEmployee Headcount Snapshot\nEmployee Compensation\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nEmpl\nMgr\nEmpl\nMgr\nEmpl\nMgr\nEmpl\nMgr\nEmpl\nMgr\nEmpl\nMgr\nEmpl\nMgr\nEmpl\nMgr\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nPeriodic\nDate\nPosition\nEmployee\nOrganization\nBenefit\nAccumulating\nAccumulating\nAccumulating\nTransaction\nTransaction\nTransaction\nTransaction\nAccumulating\nPeriodic\nAccumulating\nPeriodic\nTransaction\nPeriodic\nTransaction\nFact Type\nHiring Processes\nBenefits Processes\nEmployee Management Processes\nFigure 9-4: Bus matrix rows for HR processes.\nSome of these business processes capture performance metrics, but many result \nin factless fact tables, such as beneﬁ t eligibility or participation.\n",
      "content_length": 1463,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 306,
      "content": "Chapter 9\n270\n Packaged Analytic Solutions \nand Data Models\nMany  organizations purchase a vendor solution to address their operational HR \napplication needs. Most of these products off er an add-on DW/BI solution. In addi-\ntion, other vendors sell standard data models, potentially with prebuilt data loaders \nfor the popular HR application products.\nVendors and proponents argue these standard, prebuilt solutions and models \nallow for more rapid, less risky implementations by reducing the scope of the data \nmodeling and ETL development eff ort. After all, every HR department hires employ-\nees, signs them up for beneﬁ ts, compensates them, reviews them, and eventually \nprocesses employee separations. Why bother re-creating the wheel by designing \ncustom data models and solutions to support these common business processes \nwhen you can buy a standard data model or complete solution instead?\nAlthough there are undoubtedly common functions, especially within the HR \nspace, businesses typically have unique peculiarities. To handle these nuances, \nmost application software vendors introduce abstractions in their products, which \nenable them to be more easily “customized.”\nThese abstractions, like the party table and associated apparatus to describe each \nrole or generic attribute column names rather than more meaningful labels, provide \nﬂ exibility to adapt to a variety of business situations. Although implementation \nadaptability is a win for vendors who want their products to address a broad range of \npotential customers’ business scenarios, the downside is the associated complexity.\nHR professionals who live with the vendor’s product 24x7 are often willing to \nadjust their vocabulary to accommodate the abstractions. But these abstractions can \nfeel like a foreign language for less-immersed functional managers. Delivering data \nto the business via a packaged DW/BI solution or industry-standard data model may \nbypass the necessary translations into the business’s vernacular.\nBesides the reliance on the vendor’s terminology instead of incorporating the \nbusiness’s vocabulary in the DW/BI solution, another potential sharp corner is \nthe integration of source data from other domains. Can you readily conform the \ndimensions in the vendor solution or industry model with other internally avail-\nable master data? If not, the packaged model is destined to become another isolated \nstovepipe data set. Clearly, this outcome is unappealing; although it may be less of \nan obstacle if all your operational systems are supported by the same ERP vendor, \nor you’re a small organization without an IT shop doing independent development.\nWhat can you realistically expect to gain from a packaged model? Prebuilt generic \nmodels can help identify core business processes and associate common dimensions. \nThat provides some comfort for DW/BI teams feeling initially overwhelmed by the \n",
      "content_length": 2906,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 307,
      "content": "Human Resources Management 271\ndesign task. After a few days or weeks studying the standard model, most teams \ngain enough conﬁ dence to want to customize the schema for their data.\nHowever, is this knowledge worth the price tag associated with the packaged \nsolution or data model? You could likely gain the same insight by spending a few \nweeks with the business users. You’d not only improve your understanding of the \nbusiness’s needs, but also begin bonding business users to the DW/BI initiative.\nIt’s also worth mentioning that just because a packaged model or solution costs \nthousands of dollars doesn’t mean it exhibits generally accepted dimensional modeling \nbest practices. Unfortunately, some standard models embody common dimensional \nmodeling design ﬂ aws; this isn’t surprising if the model’s designers focused more on \nbest practices for source system data capture rather than those required for BI report-\ning and analytics. It’s diffi  cult to design a predeﬁ ned generic model, even if the vendor \nowns the data capture source code.\nRecursive Employee Hierarchies\nA  common employee characteristic is the name of the employee’s manager. You \ncould simply embed this attribute along with the other attributes in the employee \ndimension. But if the business users want more than the manager’s name, more \ncomplex structures are necessary.\nOne  approach is to include the manager’s employee key as another foreign key in \nthe fact table, as shown in Figure 9-5. This manager employee key joins to a role-\nplaying employee dimension where every attribute name refers to “manager” to \ndiff erentiate the manager’s proﬁ le from the employee’s. This approach associates the \nemployee and their manager whenever a row is inserted into a fact table. BI analyses \ncan easily ﬁ lter and group by either employee or manager attributes with virtually \nidentical query performance because both dimensions provide symmetrical access \nto the fact table. The downside of this approach is these dual foreign keys must be \nembedded in every fact table to support managerial reporting.\nSeparation Date Key (FK)\nOrganization Key (FK)\nEmployee Key (FK)\nManager Key (FK)\nSeparation Profile Key (FK)\nSeparation Count\nEmployee Key (PK)\nEmployee ID (NK)\n...\nEmployee Dimension\nManager Key (PK)\nManager Employee ID (NK)\n...\nManager Dimension\nSeparation Profile Key (PK)\nSeparation Type Description\nSeparation Reason Description\nSeparation Profile Dimension\nEmployee Separation Fact\nDate Dimension\nOrganization Dimension\nFigure 9-5: Dual role-playing employee and manager dimensions.\n",
      "content_length": 2577,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 308,
      "content": "Chapter 9\n272\nAnother option is to include the manager’s employee key as an attribute on the \nemployee’s dimension row. The manager key would join to an outrigger consisting of \na role play on the employee dimension where all the attributes reference “manager” \nto diff erentiate them from the employee’s characteristics, as shown in Figure 9-6.\nSeparation Date Key (FK)\nOrganization Key (FK)\nEmployee Key (FK)\nSeparation Profile Key (FK)\nSeparation Count\nEmployee Key (PK)\nEmployee ID (NK)\nEmployee Attributes ...\nManager Key (FK)\nRow Effective Date\nRow Expiration Date\nCurrent Row Indicator\nEmployee Dimension\nManager Key (PK)\nManager Employee ID (NK)\nManager Employee Attributes ...\nRow Effective Date\nRow Expiration Date\nCurrent Row Indicator\nManager Dimension\nEmployee Separation Fact\nFigure 9-6: Manager role-playing dimension as an outrigger.\nIf the manager’s foreign key in the employee dimension is designated as a type 2 \nattribute, then new employee rows would be generated with each manager change. \nHowever, we encourage you to think carefully about the underlying ETL business rules.\nChange Tracking on Embedded Manager Key\nLet’s  walk through an example. Abby is Hayden’s manager. With the outrigger \napproach just described, Hayden’s employee dimension row would include an \nattribute linking to Abby’s row in the manager role-play employee dimension. If \nHayden’s manager changes, and assuming the business wants to track these histori-\ncal changes, then treating the manager foreign key as a type 2 and creating a new \nrow for Hayden to capture his new proﬁ le with a new manager would be appropriate.\nHowever, think about the desired outcome if Abby were still Hayden’s manager, \nbut her employee proﬁ le changes, perhaps caused by something as innocuous as \na home address change. If the home address is designated as a type 2 attribute, \nthis move would spawn a new employee dimension row for Abby. If the manager \nkey is also designated as a type 2 attribute, then Abby’s new employee key would \nalso spawn a new dimension row for Hayden. Now imagine Abby is the CEO of a \nlarge organization. A type 2 change in her proﬁ le would ripple through the entire \ntable; you’d end up replicating a new proﬁ le row for every employee due to a single \ntype 2 attribute change on the CEO’s proﬁ le.\nDoes the business want to capture these manager proﬁ le changes? If not, perhaps \nthe manager key on the employee’s row should be the manager’s durable natural key \n",
      "content_length": 2476,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 309,
      "content": "Human Resources Management 273\nlinked to a role-playing dimension limited to just the current row for each manager’s \ndurable natural key in the dimension.\nIf you designate the manager’s key in the employee dimension to be a type 1 \nattribute, it would always associate an employee with her current manager. Although \nthis simplistic approach obliterates history, it may completely satisfy the business \nuser’s needs.\n Drilling Up and Down Management Hierarchies\nAdding  an attribute, either a textual label or a foreign key to a role-playing dimen-\nsion, to an employee dimension row is appropriate for handling the ﬁ xed depth, \nmany-to-one employee-to-manager relationship. However, more complex approaches \nmight be required if the business wants to navigate a deeper recursive hierarchy, \nsuch as identifying an employee’s entire management chain or drilling down to \nidentify the activity for all employees who directly or indirectly work for a given \nmanager.\nIf  you use an OLAP tool to query employee data, the embedded manager key \non every employee dimension row may suffi  ce. Popular OLAP products contain a \nparent/child hierarchy structure that works smoothly with variable depth recursive \nhierarchies. In fact, this is one of the strengths of OLAP products.\nHowever, if you want to query the recursive employee/manager relationship in \nthe relational environment, you must use Oracle’s nonstandard CONNECT BY syntax \nor SQL’s recursive common table extension (CTE) syntax. Both approaches are \nvirtually unworkable for business users armed with a BI reporting tool.\nSo you’re left with the options described in Chapter 7 for dealing with vari-\nable depth customer hierarchies. In Figure 9-7, the employee dimension from \nFigure 9-6 relates to the fact table through a bridge table. The bridge table has \none row for each manager and each employee who is directly or indirectly in \ntheir management chain, plus an additional row for the manager to himself. The \nbridge joins shown in Figure 9-7 enable you to drill down within a manager’s \nchain of command.\nSeparation Date Key (FK)\nOrganization Key (FK)\nEmployee Key (FK)\nSeparation Profile Key (FK)\nSeparation Count\nManager Key (FK)\nEmployee Key (FK)\n# Levels from Top\nBottom Flag\nTop Flag\nManagement Hierarchy Bridge\nManager Key (PK)\nManager Employee ID (NK)\nManager Employee Attributes ...\nRow Effective Date\nRow Expiration Date\nCurrent Row Indicator\nManager Dimension\nEmployee Separation Fact\nFigure 9-7: Bridge table to drill down into a manager’s reporting structure.\n",
      "content_length": 2541,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 310,
      "content": "Chapter 9\n274\nAs previously described, there are several disadvantages to this approach. The \nbridge table is somewhat challenging to build, plus it contains many rows, so query \nperformance can suff er. The BI user experience is complicated for ad hoc queries, \nalthough we’ve seen analysts eff ectively use it. Finally, if users want to aggregate \ninformation up rather than down a management chain, the join paths must be \nreversed.\nOnce again, the situation is further complicated if you want to track employee pro-\nﬁ le changes in conjunction with the bridge table. If the manager and employee reﬂ ect \nemployee proﬁ les with type 2 changes, the bridge table will experience rapid growth, \nespecially when senior management proﬁ le changes cause new keys to ripple across \nthe organization.\nYou could use durable natural keys in the bridge table, instead of the employee \nkeys which capture type 2 proﬁ le changes. Limiting the relationship to the man-\nagement hierarchy’s current proﬁ les is one thing. However, if the business wants \nto retain a history of employee/manager rollups, you need to embellish the bridge \ntable with eff ective and expiration dates that capture the eff ective timespan for each \nemployee/manager relationship.\nThe propagation of new rows in this bridge table using durable keys is substan-\ntially reduced compared to the Figure 9-7 bridge because new rows are added when \nreporting relationships change, not when any type 2 employee attribute is modiﬁ ed. \nA bridge table built on durable keys is easier to manage, but quite challenging to \nnavigate, especially given the need to associate the relevant organizational structures \nwith the event dates in the fact table. Given the complexities, the bridge table should \nbe buried within a canned BI application for all but a small subset of power BI users.\nThe alternative approaches discussed in Chapter 7 for handling recursive hierar-\nchies, like the pathstring attribute, are also relevant to the management hierarchy \nconundrum. Unfortunately, there’s no silver bullet solution for handling these com-\nplex structures in a simple and fast way.\nMultivalued Skill Keyword Attributes\nLet’s  assume the IT department wants to supplement the employee dimension with \ntechnical skillset proﬁ ciency information. You could consider these technical skills, \nsuch as programming languages, operating systems, or database platforms, to be key-\nwords describing employees. Each employee is tagged with a number of skill keywords. \nYou want to search the IT employee population by their descriptive skills.\nIf the technical skills of interest were a ﬁ nite number, you could include them \nas individual attributes in the employee dimension. The advantage of using posi-\ntional dimension attributes, such as a Linux attribute with domain values such as \n",
      "content_length": 2830,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 311,
      "content": "Human Resources Management 275\nLinux Skills and No Linux Skills, is they’re easy to query and deliver fast query \nperformance. This approach works well to a point but falls apart when the number \nof potential skills expands.\n Skill Keyword Bridge\nMore realistically, each employee will have a variable, unpredictable number of \nskills. In this case, the skill keyword attribute is a prime candidate to be a multi-\nvalued dimension. Skill keywords, by their nature, are open-ended; new skills are \nadded regularly as domain values. We’ll show two logically equivalent modeling \nschemes for handling open-ended sets of skills.\nFigure 9-8 shows a multivalued dimension design for handling the skills as an \noutrigger bridge table to the employee dimension table. As you’ll see in Chapter 14: \nHealthcare, sometimes the multivalued bridge table is joined directly to a fact table.\nEmployee Key (FK)\nMore FKs ...\nHeadcount Facts ...\nEmployee Headcount\nSnapshot Fact\nEmployee Key (PK)\n...\nEmployee Skill Group Key (FK)\nEmployee Dimension\nEmployee Skill Key (PK)\nEmployee Skill Description\nEmployee Skill Category\nSkills Dimension\nEmployee Skill Group Key (FK)\nEmployee Skill Key (FK)\nEmployee Skill Group Bridge\nFigure 9-8: Skills group keyword bridge table.\nThe skills group bridge identiﬁ es a given set of skill keywords. IT employees who \nare proﬁ cient in Oracle, Unix, and SQL would be assigned the same skills group key. \nIn the skills group bridge table, there would be three rows for this particular group, \none for each of the associated skill keywords (Oracle, Unix, and  SQL).\nAND/OR Query Dilemma\nAssuming  you built the schema shown in Figure 9-8, you are still left with a serious \nquery problem. Query requests against the skill keywords fall into two categories. The \nOR queries (for example, Unix or Linux experience) can be satisﬁ ed by a simple OR \nconstraint on the skills description attribute in the skills dimension table. However, \nAND queries (for example, Unix and Linux experience) are diffi  cult because the AND \nconstraint is a constraint across two rows in the skills dimension. SQL is notoriously \npoor at handling constraints across rows. The answer is to create SQL code using \nunions and intersections, probably in a custom interface that hides the complex logic \nfrom the business user. The SQL code would look like this:\n (SELECT employee_ID, employee_name\nFROM Employee, SkillBridge, Skills\nWHERE Employee.SkillGroupKey = SkillBridge.SkillGroupKey AND\n  SkillGroup.SkillKey = Skill.SkillKey AND\n",
      "content_length": 2528,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 312,
      "content": "Chapter 9\n276\n  Skill.Skill = \"UNIX\")\nUNION / INTERSECTION\n (SELECT employee_ID, employee_name\n  FROM Employee, SkillBridge, Skills\n  WHERE Employee.SkillGroupKey = SkillBridge.SkillGroupKey AND \n    SkillGroup.SkillKey = Skill.SkillKey AND \n    Skill.Skill = \"LINUX\")\nUsing the UNION lists employees with Unix or Linux experience, whereas using \nINTERSECTION identiﬁ es employees with Unix and Linux experience.\nSkill Keyword Text String\nYou  can remove the many-to-many bridge and the need for union/intersection SQL \nby simplifying the design. One approach would be to add a skills list outrigger to \nthe employee dimension containing one long text string concatenating all the skill \nkeywords for that list key. You would need a special delimiter such as a backslash or \nvertical bar at the beginning of the skills text string and after each skill in the list. \nThus the skills string containing Unix and C++ would look like |Unix|C++|. This \noutrigger approach presumes a number of employees share a common list of skills. \nIf the lists are not reused frequently, you could collapse the skills list outrigger by \nsimply including the skills list text string as an employee dimension attribute, as \nshown in Figure 9-9.\nEmployee Key (FK)\nMore FKs ...\nHeadcount Facts ...\nEmployee Headcount Snapshot Fact\nEmployee Key (PK)\n...\nEmployee Skill Group List\nEmployee Dimension\nFigure 9-9: Delimited skills list string.\nText string searches can be challenging because of the ambiguity caused by \nsearching on uppercase or lowercase. Is it UNIX or Unix or unix? You can resolve \nthis by coercing the skills list to upper case with the UCase function in most SQL \nenvironments.\nWith the design in Figure 9-9, the AND/OR dilemma can be addressed in a single \nSELECT statement. The OR constraint looks like this:\nUCase(skill_list) like '%|UNIX|% OR UCase(skill_list) like '%|LINUX|%'\nMeanwhile, the AND constraint has exactly the same structure:\nUCase(skill_list) like '%|UNIX|' AND UCase(skill_list) like '%|LINUX|%'\n",
      "content_length": 2011,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 313,
      "content": "Human Resources Management 277\nThe % symbol is a wild card pattern-matching character deﬁ ned in SQL that \nmatches zero or more characters. The vertical bar delimiter is used explicitly in the \nconstraints to exactly match the desired keywords and not get erroneous matches.\nThe keyword list approach shown in Figure 9-9 can work in any relational database \nbecause it is based on standard SQL. Although the text string approach facilitates \nAND/OR searching, it doesn’t support queries that count by skill keyword.\nSurvey Questionnaire Data\nHR  departments often collect survey data from employees, especially when gather-\ning peer and/or management review data. The department analyzes questionnaire \nresponses to determine the average rating for a reviewed employee and within a \ndepartment.\nTo handle questionnaire data in a dimensional model, a fact table with one row \nfor each question on a respondent’s survey is typically created, as illustrated in \nFigure 9-10. Two role-playing employee dimensions in the schema correspond to the \nresponding employee and reviewed employee. The survey dimension has descriptors \nabout the survey instrument. The question dimension provides the question and \nits categorization; presumably, the same question is asked on multiple surveys. The \nsurvey and question dimensions can be useful when searching for speciﬁ c topics in \na broad database of questionnaires. The response dimension contains the responses \nand perhaps categories of responses, such as favorable or hostile.\nSurvey Key (PK)\nSurvey Title\nSurvey Type\nSurvey Objective\nReview Year\nQuestion Key (PK)\nQuestion Label\nQuestion Category\nQuestion Objective\nSurvey Sent Date Key (FK)\nSurvey Received Date Key (FK)\nSurvey Key (FK)\nResponding Employee Key (FK)\nReviewed Employee Key (FK)\nQuestion Key (FK)\nResponse Category Key (FK)\nSurvey Number (DD)\nResponse\nSurvey Dimension\nResponse Category Key (PK)\nResponse Category Description\nResponse Category Dimension\nEmployee Evaluation Survey Fact\nQuestion Dimension\nDate Dimension (2 views for roles)\nEmployee Dimension (2 views for roles)\nFigure 9-10: Survey schema.\nCreating the simple schema in Figure 9-10 supports robust slicing and dicing \nof survey data. Variations of this schema design would be useful for analyzing all \ntypes of survey data, including customer satisfaction and product usage feedback.\n",
      "content_length": 2361,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 314,
      "content": "Chapter 9\n278\n Text Comments\nFacts are typically thought of as continuously valued numeric measures; dimension \nattributes, on the other hand, are drawn from a discrete list of domain values. So \nhow do you handle textual comments, such as a manager’s remarks on a perfor-\nmance review or freeform feedback on a survey question, which seem to defy clean \nclassiﬁ cation into the fact or dimension category? Although IT professionals may \ninstinctively want to simply exclude them from a dimensional design, business \nusers may demand they’re retained to further describe the performance metrics.\nAfter it’s been conﬁ rmed the business is unwilling to relinquish the text com-\nments, you should determine if the comments can be parsed into well-behaved \ndimension attributes. Although there are sometimes opportunities to categorize \nthe text, such as a compliment versus complaint, the full text verbiage is typically \nalso required.\nBecause freeform text takes on so many potential values, designers are some-\ntimes tempted to store the text comment within the fact table. Although cognizant \nthat fact tables are typically limited to foreign keys, degenerate dimensions, and \nnumeric facts, they contend the text comment is just another degenerate dimension. \nUnfortunately, text comments don’t qualify as degenerate dimensions.\nFreeform text ﬁ elds shouldn’t be stored in the fact table because they just add \nbulky clutter to the table. Depending on the database platform, this relatively low \nvalue bulk may get dragged along on every operation involving the fact table’s much \nmore valuable performance metrics.\nRather than treating the comments as textual metrics, we recommend retaining \nthem outside the fact table. The comments should either be captured in a separate \ncomments dimensions (with a corresponding foreign key in the fact table) or as \nan attribute on a transaction-grained dimension table. In some situations, identi-\ncal comments are observed multiple times. At a minimum, this typically occurs \nwith the No Comment comment. If the cardinality of the comments is less than \nthe number of transactions, the text should be captured in a comments dimension. \nOtherwise, if there’s a unique comment for every event, it’s treated as a transaction \ndimension attribute. In either case, regardless of whether the comments are handled \nin a comment or transaction dimension, the query performance when this sizeable \ndimension is joined to the fact table will be slow. However, by the time users are \nviewing comments, they’ve likely signiﬁ cantly ﬁ ltered their query as they can real-\nistically read only a limited number of comments. Meanwhile, the more common \nanalyses focusing on the fact table’s performance metrics won’t be burdened by the \nextra weight of the textual comments on every fact table  query.\n",
      "content_length": 2831,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 315,
      "content": "Human Resources Management 279\nSummary\nIn this chapter, we discussed several concepts in the context of HR data. First, \nwe further elaborated on the advantages of embellishing an employee dimension \ntable. In the world of HR, this single table is used to address a number of ques-\ntions regarding the status and proﬁ le of the employee base at any point in time. We \ndrafted a bus matrix representing multiple processes within the HR arena and high-\nlighted a core headcount snapshot fact table, along with the potential advantages \nand disadvantages of vendor-designed solutions and data models. The handling of \nmanagerial rollups and multivalued dimension attributes was discussed. Finally, \nwe provided a brief overview regarding the handling of survey or questionnaire \ndata, along with text comments. \n",
      "content_length": 809,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 317,
      "content": "Financial Services\nT\nhe financial services industry encompasses a wide variety of businesses, includ-\ning credit card companies, brokerage firms, and mortgage providers. In this \nchapter, we’ll primarily focus on the retail bank since most readers have some degree \nof personal familiarity with this type of financial institution. A full-service bank offers \na breadth of products, including checking accounts, savings accounts, mortgage \nloans, personal loans, credit cards, and safe deposit boxes. This chapter begins with \na very simplistic schema. We then explore several schema extensions, including the \nhandling of the bank’s broad portfolio of heterogeneous products that vary signifi-\ncantly by line of business.\nWe want to remind you that industry focused chapters like this one are not \nintended to provide full-scale industry solutions. Although various dimensional \nmodeling techniques are discussed in the context of a given industry, the techniques \nare certainly applicable to other businesses. If you don’t work in ﬁ nancial services, \nyou still need to read this chapter. If you do work in ﬁ nancial services, remember \nthat the schemas in this chapter should not be viewed as  complete.\nChapter 10 discusses the following concepts:\n \n■Bus matrix snippet for a bank\n \n■Dimension triage to avoid the “too few dimensions” trap\n \n■Household dimensions \n \n■Bridge tables to associate multiple customers with an account, along with \nweighting factors\n \n■Multiple mini-dimensions in a single fact table\n \n■Dynamic value banding of facts for reporting \n \n■Handling heterogeneous products across lines of business, each with unique \nmetrics and/or dimension attributes, as supertype and subtype schemas\n \n■Hot swappable dimensions\n10\n",
      "content_length": 1744,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 318,
      "content": "Chapter 10\n282\n Banking Case Study and Bus Matrix\nThe  bank’s initial goal is to better analyze the bank’s accounts. Business users want the \nability to slice and dice individual accounts, as well as the residential household groupings \nto which they belong. One of the bank’s major objectives is to market more eff ectively by \noff ering additional products to households that already have one or more accounts with \nthe bank. Figure 10-1 illustrates a portion of a bank’s bus matrix.\nNew Business Solicitation\nLead Tracking\nAccount Application Pipeline\nAccount Initiation\nAccount Transactions\nAccount Monthly Snapshot\nAccount Servicing Activities\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nDate\nProspect\nCustomer\nHousehold\nBranch\nAccount\nProduct\nFigure 10-1: Subset of bus matrix rows for a bank.\nAfter conducting interviews with managers and analysts around the bank, the \nfollowing set of requirements were developed:\n \n■Business users want to see ﬁ ve years of historical monthly snapshot data on \nevery account.\n \n■Every account has a primary balance. The business wants to group diff erent \ntypes of accounts in the same analyses and compare primary balances.\n \n■Every type of account (known as products within the bank) has a set of cus-\ntom dimension attributes and numeric facts that tend to be quite diff erent \nfrom product to product.\n \n■Every account is deemed to belong to a single household. There is a surprising \namount of volatility in the account/household relationships due to changes \nin marital status and other life stage factors.\n \n■In addition to the household identiﬁ cation, users are interested in demographic \ninformation both as it pertains to individual customers and households. In \naddition, the bank captures and stores behavior scores relating to the activity \nor characteristics of each account and household.\n",
      "content_length": 1880,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 319,
      "content": "Financial Services 283\n Dimension Triage to Avoid Too Few \nDimensions\nBased  on the previous business requirements, the grain and dimensionality of the \ninitial model begin to emerge. You can start with a fact table that records the pri-\nmary balances of every account at the end of each month. Clearly, the grain of the \nfact table is one row for each account each month. Based on that grain declaration, \nyou can initially envision a design with only two dimensions: month and account. \nThese two foreign keys form the fact table primary key, as shown in Figure 10-2. \nA data-centric designer might argue that all the other description information, such \nas household, branch, and product characteristics should be embedded as descriptive \nattributes of the account dimension because each account has only one household, \nbranch, and product associated with it.\nMonth Dimension\nAccount Dimension\nMonth End Date Key (PK)\nMonth Attributes ...\nMonth End Date Key (FK)\nAccount Key (FK)\nPrimary Month Ending Balance\nAccount Key (PK)\nAccount Attributes ...\nPrimary Customer Attributes ...\nProduct Attributes ...\nHousehold Attributes ...\nStatus Attributes ...\nBranch Attributes ...\nMonth Account Snapshot Fact\nFigure 10-2: Balance snapshot with too few dimensions.\nAlthough this schema accurately represents the many-to-one and many-to-many \nrelationships in the snapshot data, it does not adequately reﬂ ect the natural business \ndimensions. Rather than collapsing everything into the huge account dimension table, \nadditional analytic dimensions such as product and branch mirror the instinctive \nway users think about their business. These supplemental dimensions provide much \nsmaller points of entry to the fact table. Thus, they address both the performance and \nusability objectives of a dimensional model. Finally, given a big bank may have mil-\nlions of accounts, you should worry about type 2 slowly changing dimension eff ects \npotentially causing this huge dimension to mushroom into something unmanage-\nable. The product and branch attributes are convenient groups of attributes to remove \nfrom the account dimension to cut down on the row growth caused by type 2 change \ntracking. In the section “Mini-Dimensions Revisited,” the changing demographics \nand behavioral attributes will be squeezed out of the account dimension for the same \nreasons.\nThe product and branch dimensions are two separate dimensions as there is \na many-to-many relationship between products and branches. They both change \n",
      "content_length": 2508,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 320,
      "content": "Chapter 10\n284\nslowly, but on diff erent rhythms. Most important, business users think of them as \ndistinct dimensions of the banking business.\nIn general,  most dimensional models end up with between ﬁ ve and 20 dimen-\nsions. If you are at or below the low end of this range, you should be suspicious \nthat dimensions may have been inadvertently left out of the design. In this case, \ncarefully consider whether any of the following kinds of dimensions are appropriate \nsupplements to your initial dimensional model:\n \n■Causal  dimensions, such as promotion, contract, deal, store condition, or even \nweather. These dimensions, as discussed in Chapter 3: Retail Sales, provide \nadditional insight into the cause of an event.\n \n■Multiple date  dimensions, especially when the fact table is an accumulating \nsnapshot. Refer to Chapter 4: Inventory for sample fact tables with multiple \ndate stamps.\n \n■Degenerate  dimensions that identify operational transaction control numbers, \nsuch as an order, an invoice, a bill of lading, or a ticket, as initially illustrated \nin Chapter 3.\n \n■Role-playing  dimensions, such as when a single transaction has several busi-\nness entities associated with it, each represented by a separate dimension. In \nChapter 6: Order Management, we described role playing to handle multiple \ndates.\n \n■Status  dimensions that identify the current status of a transaction or monthly \nsnapshot within some larger context, such as an account status.\n \n■An  audit dimension, as discussed in Chapter 6, to track data lineage and \nquality.\n \n■Junk  dimensions of correlated indicators and ﬂ ags, as described in Chapter 6.\nThese dimensions can typically be added gracefully to a design, even after the \nDW/BI system has gone into production because they do not change the grain of \nthe fact table. The addition of these dimensions usually does not alter the existing \ndimension keys or measured facts in the fact table. All existing applications should \ncontinue to run without change.\nNOTE \nAny descriptive attribute that is single-valued in the presence of the \nmeasurements in the fact table is a good candidate to be added to an existing \ndimension or to be its own dimension.\nBased on further study of the bank’s requirements, you can ultimately choose the \nfollowing dimensions for the initial schema: month end date, branch, account, pri-\nmary customer, product, account status, and household. As illustrated in Figure 10-3, \n",
      "content_length": 2452,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 321,
      "content": "Financial Services 285\nat the intersection of these seven dimensions, you take a monthly snapshot and \nrecord the primary balance and any other metrics that make sense across all prod-\nucts, such as transaction count, interest paid, and fees charged. Remember account \nbalances are just like inventory balances in that they are not additive across any \nmeasure of time. Instead, you must average the account balances by dividing the \nbalance sum by the number of time periods.\nMonth Dimension\nAccount Dimension\nMonth End Date Key (PK)\nMonth Attributes ...\nBranch Dimension\nBranch Key (PK)\nBranch Number (NK)\nBranch Address Attributes ...\nBranch Rollup Attributes ...\nAccount Key (PK)\nAccount Number (NK)\nAccount Address Attributes ...\nAccount Open Date\n...\nHousehold Key (PK)\nHousehold ID (NK)\nHousehold Address Attributes ...\nHousehold Income\nHousehold Homeownership Indicator\nHousehold Presence of Children\n...\nPrimary Customer Key (PK)\nPrimary Customer Name\nPrimary Customer Date of Birth\n...\nProduct Dimension\nProduct Key (PK)\nProduct Code (NK)\nProduct Description\n...\nAccount Status Dimension\n Account Status Key (PK)\n Account Status Description\n Account Status Group\nMonth End Date Key (FK)\nBranch Key (FK)\nAccount Key (FK)\nPrimary Customer Key (FK)\nProduct Key (FK)\nAccount Status Key (FK)\nHousehold Key (FK)\nPrimary Month Ending Balance\nAverage Daily Balance\nNumber of Transactions\nInterest Paid\nFees Charged\nPrimary Customer Dimension\nHousehold Dimension\nMonthly Account Snapshot Fact\nFigure 10-3: Supertype snapshot fact table for all accounts.\nNOTE \nIn this chapter we use the basic object-oriented terms supertype and \nsubtype to refer respectively to the single fact table covering all possible account \ntypes, as well as the multiple fact tables containing speciﬁ c details of each individual \naccount type. In past writings these have been called core and custom fact tables, \nbut it is time to change to the more familiar and accepted terminology.\nThe product dimension consists of a simple hierarchy that describes all the \nbank’s products, including the name of the product, type, and category. The need to \nconstruct a generic product categorization in the bank is the same need that causes \ngrocery stores to construct a generic merchandise hierarchy. The main diff erence \nbetween the bank and grocery store examples is that the bank also develops a large \nnumber of subtype product attributes for each product type. We’ll defer discussion \n",
      "content_length": 2462,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 322,
      "content": "Chapter 10\n286\nregarding the handling of these subtype attributes until the “Supertype and Subtype \nSchemas for Heterogeneous Products” section at the end of the chapter.\nThe branch dimension is similar to the facility dimensions we discussed earlier \nin this book, such as the retail store or distribution center warehouse.\nThe account status dimension is a useful dimension to record the condition of \nthe account at the end of each month. The status records whether the account is \nactive or inactive, or whether a status change occurred during the month, such as a \nnew account opening or account closure. Rather than whipsawing the large account \ndimension, or merely embedding a cryptic status code or abbreviation directly in \nthe fact table, we treat status as a full-ﬂ edged dimension with descriptive status \ndecodes, groupings, and status reason descriptions, as appropriate. In many ways, \nyou could consider the account status dimension to be another example of a mini-\ndimension, as we introduced in Chapter 5: Procurement.\n Household Dimension\nRather  than focusing solely on the bank’s accounts, business users also want the \nability to analyze the bank’s relationship with an entire economic unit, referred to as \na household. They are interested in understanding the overall proﬁ le of a household, \nthe magnitude of the existing relationship with the household, and what additional \nproducts should be sold to the household. They also want to capture key demographics \nregarding the household, such as household income, whether they own or rent their \nhome, whether they are retirees, and whether they have children. These demographic \nattributes change over time; as you might suspect, the users want to track the changes. \nIf the bank focuses on accounts for commercial entities, rather than consumers, simi-\nlar requirements to identify and link corporate “households” are common.\nFrom the bank’s perspective, a household may be comprised of several accounts \nand individual account holders. For example, consider John and Mary Smith as a \nsingle household. John has a checking account, whereas Mary has a savings account. \nIn addition, they have a joint checking account, credit card, and mortgage with \nthe bank. All ﬁ ve of these accounts are considered to be a part of the same Smith \nhousehold, despite the fact that minor inconsistencies may exist in the operational \nname and address information.\nThe process of relating individual accounts to households (or the commercial \nbusiness equivalent) is not to be taken lightly. Householding requires the devel-\nopment of business rules and algorithms to assign accounts to households. There \nare specialized products and services to do the matching necessary to determine \nhousehold assignments. It is very common for a large ﬁ nancial services organization \nto invest signiﬁ cant resources in specialized capabilities to support its household-\ning needs.\n",
      "content_length": 2931,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 323,
      "content": "Financial Services 287\nThe decision to treat account and household as separate dimensions is somewhat \na matter of the designer’s prerogative. Even though they are intuitively correlated, \nyou decide to treat them separately because of the size of the account dimension and \nthe volatility of the account constituents within a household dimension, as men-\ntioned earlier. In a large bank, the account dimension is huge, with easily over 10 \nmillion rows that group into several million households. The household dimension \nprovides a somewhat smaller point of entry into the fact table, without traversing \na 10 million-row account dimension table. Also, given the changing nature of the \nrelationship between accounts and households, you elect to use the fact table to \ncapture the relationship, rather than merely including the household attributes on \neach account dimension row. In this way, you avoid using the type 2 slowly chang-\ning dimension technique with a 10-million row account dimension.\n Multivalued Dimensions and Weighting Factors\nAs  you just saw in the John and Mary Smith example, an account can have one, two, \nor more individual account holders, or customers, associated with it. Obviously, the \ncustomer cannot be included as an account attribute (beyond the designation of a \nprimary customer/account holder); doing so violates the granularity of the dimen-\nsion table because more than one individual can be associated with an account. \nLikewise, you cannot include a customer as an additional dimension in the fact \ntable; doing so violates the granularity of the fact table (one row per account per \nmonth), again because more than one individual can be associated with any given \naccount. This is another classic example of a multivalued dimension. To link an \nindividual customer dimension to an account-grained fact table requires the use of \nan account-to-customer bridge table, as shown in Figure 10-4. At a minimum, the \nprimary key of the bridge table consists of the surrogate account and customer keys. \nThe time stamping of bridge table rows, as discussed in Chapter 7: Accounting, for \ntime-variant relationships is also applicable in this scenario. \nAccount Dimension\nAccount Key (PK)\nAccount Number (NK)\nAccount Address Attributes ...\nAccount Open Date\n...\nAccount-to-Customer Bridge\nAccount Key (FK)\nCustomer Key (FK)\nWeighting Factor\nCustomer Dimension\nCustomer Key (FK)\nCustomer Name\nCustomer Date of Birth\n...\nMonth End Date Key (FK)\nAccount Key (FK)\nMore FKs ...\nPrimary Month Ending Balance\nAverage Daily Balance\nNumber of Transactions\nInterest Paid\nFees Charged\nMonthly Account Snapshot Fact\nFigure 10-4: Account-to-customer bridge table with weighting factor.\n",
      "content_length": 2708,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 324,
      "content": "Chapter 10\n288\nIf  an account has two account holders, then the associated bridge table has two \nrows. You assign a numerical weighting factor to each account holder such that the \nsum of all the weighting factors is exactly 1.00. The weighting factors are used to \nallocate any of the numeric additive facts across individual account holders. In this \nway you can add up all numeric facts by individual holder, and the grand total will \nbe the correct grand total amount. This kind of report is a correctly weighted report.\nThe weighting factors are simply a way to allocate the numeric additive facts \nacross the account holders. Some would suggest changing the grain of the fact table \nto be account snapshot by account holder. In this case you would take the weight-\ning factors and physically multiply them against the original numeric facts. This is \nrarely done for three reasons. First, the size of the fact table would be multiplied \nby the average number of account holders. Second, some fact tables have more than \none multivalued dimension. The number of rows would get out of hand in this situ-\nation, and you would start to question the physical signiﬁ cance of an individual row. \nFinally, you may want to see the unallocated numbers, and it is hard to reconstruct \nthese if the allocations have been combined physically with the numeric facts.\nIf  you choose not to apply the weighting factors in a given query, you can still \nsummarize the account snapshots by individual account holder, but in this case you \nget what is called an impact report. A question such as, “What is the total balance \nof all individuals with a speciﬁ c demographic proﬁ le?” would be an example of an \nimpact report. Business users understand impact analyses may result in overcount-\ning because the facts are associated with both account holders.\nIn Figure 10-4, an SQL view could be deﬁ ned combining the fact table and the \naccount-to-customer bridge table so these two tables, when combined, would appear \nto BI tools as a standard fact table with a normal customer foreign key. Two views \ncould be deﬁ ned, one using the weighting factors and one not using the weighting \nfactors.\nNOTE \nAn open-ended, many-valued attribute can be associated with a dimen-\nsion row by using a bridge table to associate the many-valued attributes with the \ndimension.\nIn some ﬁ nancial services companies, the individual customer is identiﬁ ed and \nassociated with each transaction. For example, credit card companies often issue \nunique card numbers to each cardholder. John and Mary Smith may have a joint \ncredit card account, but the numbers on their respective pieces of plastic are unique. \nIn this case, there is no need for an account-to-customer bridge table because the \natomic transaction facts are at the discrete customer grain; account and customer \nwould both be foreign keys in this fact table. However, the bridge table would be \n",
      "content_length": 2928,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 325,
      "content": "Financial Services 289\nrequired to analyze metrics that are naturally captured at the account level, such \nas the credit card billing data.\n Mini-Dimensions Revisited\nSimilar  to the discussion of the customer dimension in Chapter 8: Customer \nRelationship Management, there are a wide variety of attributes describing the \nbank’s accounts, customers, and households, including monthly credit bureau attri-\nbutes, external demographic data, and calculated scores to identify their behavior, \nretention, proﬁ tability, and delinquency characteristics. Financial services organiza-\ntions are typically interested in understanding and responding to changes in these \nattributes over time.\nAs discussed earlier, it’s unreasonable to rely on slowly changing dimension tech-\nnique type 2 to track changes in the account dimension given the dimension row \ncount and attribute volatility, such as the monthly update of credit bureau attributes. \nInstead, you can break off  the browseable and changeable attributes into multiple \nmini-dimensions, such as credit bureau and demographics mini-dimensions, whose \nkeys are included in the fact table, as illustrated in Figure 10-5. The type 4 mini-\ndimensions enable you to slice and dice the fact table, while readily tracking attribute \nchanges over time, even though they may be updated at diff erent frequencies. Although \nmini-dimensions are extremely powerful, be careful to avoid overusing the technique. \nAccount-oriented ﬁ nancial services are a good environment for using mini-dimensions \nbecause the primary fact table is a very long-running periodic snapshot. Thus every \nmonth a fact table row is guaranteed to exist for every account, providing a home for \nall the associated foreign keys. You can always see the account together with all the \nmini-dimensions for any month.\nCustomer Dimension\nCustomer Key (PK)\nRelatively Constant Attributes ... \nCustomer Demographics Key (PK)\nCustomer Age Band\nCustomer Income Band\nCustomer Marital Status\nCustomer Risk Profile Key (PK)\nCustomer Risk Cluster\nCustomer Delinquency Cluster\nCustomer Demographics Dimension\nCustomer Key (FK)\nCustomer Demographics Key (FK)\nCustomer Risk Profile Key (FK)\nMore FKs ...\nFacts ...\nFact Table\nCustomer Risk Profile Dimension\nFigure 10-5: Multiple mini-dimensions associated with a fact table.\n",
      "content_length": 2322,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 326,
      "content": "Chapter 10\n290\nNOTE \nMini-dimensions should consist of correlated clumps of attributes; each \nattribute shouldn’t be its own mini-dimension or you end up with too many dimen-\nsions in the fact table.\nAs described in Chapter 4, one of the compromises associated with mini-dimen-\nsions is the need to band attribute values to maintain reasonable mini-dimension row \ncounts. Rather than storing extremely discrete income amounts, such as $31,257.98, \nyou store income ranges, such as $30,000 to $34,999 in the mini-dimension. \nSimilarly, the proﬁ tability scores may range from 1 through 1200, which you band \ninto ﬁ xed ranges such as less than or equal to 100, 101 to 150, and 151 to 200, in \nthe mini-dimension.\nMost organizations ﬁ nd these banded attribute values support their routine ana-\nlytic requirements, however there are two situations in which banded values may \nbe inadequate. First, data mining analysis often requires discrete values rather than \nﬁ xed bands to be eff ective. Secondly, a limited number of power analysts may want \nto analyze the discrete values to determine if the bands are appropriate. In this case, \nyou still maintain the banded value mini-dimension attributes to support consistent \nday-to-day analytic reporting but also store the key discrete numeric values as facts \nin the fact table. For example, if each account’s proﬁ tability score were recalculated \neach month, you would assign the appropriate proﬁ tability range mini-dimension for \nthat score each month. In addition, you would capture the discrete proﬁ tability score \nas a fact in the monthly account snapshot fact table. Finally, if needed, the current \nproﬁ tability range or score could be included in the account dimension where any \nchanges are handled by deliberately overwriting the type 1 attribute. Each of these \ndata elements should be uniquely labeled so that they are distinguishable. Designers \nmust always carefully balance the incremental value of including such somewhat \nredundant facts and attributes versus the cost in terms of additional complexity for \nboth the ETL processing and BI presentation.\nAdding a Mini-Dimension to a Bridge Table\nIn  the bank account example, the account-to-customer bridge table can get very \nlarge. If you have 20 million accounts and 25 million customers, the bridge table can \ngrow to hundreds of millions of rows after a few years if both the account dimen-\nsion and the customer dimension are slowly changing type 2 dimensions (where \nyou track history by issuing new rows with new keys).\nNow the experienced dimensional modeler asks, “What happens when my cus-\ntomer dimension turns out to be a rapidly changing monster dimension?” This could \nhappen when rapidly changing demographics and status attributes are added to the \n",
      "content_length": 2783,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 327,
      "content": "Financial Services 291\ncustomer dimension, forcing numerous type 2 additions to the customer dimension. \nNow the 25-million row customer dimension threatens to become several hundred \nmillion rows.\nThe  standard response to a rapidly changing monster dimension is to split off  the \nrapidly changing demographics and status attributes into a type 4 mini-dimension, \noften called a demographics dimension. This works great when this dimension attaches \ndirectly to the fact table along with a customer dimension because it stabilizes the \nlarge customer dimension and keeps it from growing every time a demographics or \nstatus attribute changes. But can you get this same advantage when the customer \ndimension is attached to a bridge table, as in the bank account example?\nThe  solution is to add a foreign key reference in the bridge table to the demo-\ngraphics dimension, as shown in Figure 10-6.\nAccount Dimension\nAccount Key (PK)\nAccount Number (NK)\n...\nAccount-to-Customer Bridge\nAccount Key (FK)\nCustomer Key (FK)\nDemographics Key (FK)\nFact Table\nMonth Key (FK)\nAccount Key (FK)\nMore FKs ...\nFacts ...\nCustomer Dimension\nCustomer Key (PK)\nCustomer Name\n...\nDemographics Key (PK)\nAge Band\nIncome Band\nMarital Status\nDemographics Dimension\nFigure 10-6: Account-to-customer bridge table with an added mini-dimension.\nThe way to visualize the bridge table is that it links every account to its associ-\nated customers and their demographics. The key for the bridge table now consists \nof the account key, customer key, and demographics key.\nDepending on how frequently new demographics are assigned to each customer, \nthe bridge table will perhaps grow signiﬁ cantly. In the above design because the \ngrain of the root bank account fact table is month by account, the bridge table should \nbe limited to changes recorded only at month ends. This takes some of the change \ntracking pressure off  the bridge table.\n Dynamic Value Banding of Facts\nSuppose  business users want the ability to perform value band reporting on a stan-\ndard numeric fact, such as the account balance, but are not willing to live with the \npredeﬁ ned bands in a dimension table. They may want to create a report based on \nthe account balance snapshot, as shown in Figure 10-7. \n",
      "content_length": 2253,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 328,
      "content": "Chapter 10\n292\nBalance Range\n0–1000\n1001–2000\n2001–5000\n5001–10000\n10001 and up\nNumber of Accounts\n456,783\n367,881\n117,754\n52,662\n8,437\nTotal of Balances\n$229,305,066\n$552,189,381\n$333,479,328\n$397,229,466\n$104,888,784\nFigure 10-7: Report rows with dynamic value band groups.\nUsing the schema in Figure 10-3, it is diffi  cult to create this report directly from \nthe fact table. SQL has no generalization of the GROUP BY clause that clumps additive \nvalues into ranges. To further complicate matters, the ranges are of unequal size and \nhave textual names, such as “10001 and up.” Also, users typically need the ﬂ exibility \nto redeﬁ ne the bands at query time with diff erent boundaries or levels of precision.\nThe schema design shown in Figure 10-8 enables on-the-ﬂ y value band reporting. \nThe band deﬁ nition table can contain as many sets of diff erent reporting bands as \ndesired. The name of a particular group of bands is stored in the band group column. \nThe band deﬁ nition table is joined to the balance fact using a pair of less-than and \ngreater-than joins. The report uses the band range name as the row header and sorts \nthe report on the sort order attribute.\nBand Definition Table\nMonth End Date Key (FK)\nAccount Key (FK)\nProduct Key (FK)\nMore FKs ...\nPrimary Month Ending Balance\nBand Group Key (PK)\nBand Group Sort Order (PK)\nBand Group Name\nBand Range Name\nBand Lower Value\nBand Upper Value\nMonthly Account Snapshot Fact\nFigure 10-8: Dynamic value band reporting.\nControlling the performance of this query can be a challenge. A value band query \nis by deﬁ nition very lightly constrained. The example report needed to scan the \nbalances of more than 1 million accounts. Perhaps only the month dimension was \nconstrained to the current month. Furthermore the funny joins to the value band-\ning table are not the basis of a nice restricting constraint because they are grouping \nthe 1 million balances. In this situation, you may need to place an index directly \non the balance fact. The performance of a query that constrains or groups on the \nvalue of a fact-like balance will be improved enormously if the DBMS can effi  ciently \nsort and compress the individual fact. This approach was pioneered by the Sybase \nIQ columnar database product in the early 1990s and is now becoming a standard \nindexing option on several of the competing columnar DBMSs.\n",
      "content_length": 2374,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 329,
      "content": "Financial Services 293\n Supertype and Subtype Schemas \nfor Heterogeneous Products\nIn  many ﬁ nancial service businesses, a dilemma arises because of the heteroge-\nneous nature of the products or services off ered by the institution. As mentioned \nin the introduction, a typical retail bank off ers a myriad of products, from check-\ning accounts to credit cards, to the same customers. Although every account at the \nbank has a primary balance and interest amount associated with it, each product \ntype has many special attributes and measured facts that are not shared by the other \nproducts. For instance, checking accounts have minimum balances, overdraft limits, \nservice charges, and other measures relating to online banking; time deposits such as \ncertiﬁ cates of deposit have few attribute overlaps with checking, but have maturity \ndates, compounding frequencies, and current interest rate.\nBusiness  users typically require two diff erent perspectives that are diffi  cult to \npresent in a single fact table. The ﬁ rst perspective is the global view, including the \nability to slice and dice all accounts simultaneously, regardless of their product \ntype. This global view is needed to plan appropriate customer relationship manage-\nment cross-sell and up-sell strategies against the aggregate customer/household base \nspanning all possible products. In this situation, you need the single supertype fact \ntable (refer to Figure 10-3) that crosses all the lines of business to provide insight \ninto the complete account portfolio. Note, however, that the supertype fact table \ncan present only a limited number of facts that make sense for virtually every line \nof business. You cannot accommodate incompatible facts in the supertype fact table \nbecause there may be several hundred of these facts when all the possible account \ntypes are considered. Similarly, the supertype product dimension must be restricted \nto the subset of common product attributes.\nThe  second perspective is the line-of-business view that focuses on the in-depth \ndetails of one business, such as checking. There is a long list of special facts and attri-\nbutes that make sense only for the checking business. These special facts cannot be \nincluded in the supertype fact table; if you did this for each line of business in a retail \nbank, you would end up with hundreds of special facts, most of which would have \nnull values in any speciﬁ c row. Likewise, if you attempt to include line-of-business \nattributes in the account or product dimension tables, these tables would have hun-\ndreds of special attributes, almost all of which would be empty for any given row. The \nresulting tables would resemble Swiss cheese, littered with data holes. The solution \nto this dilemma for the checking department in this example is to create a subtype \nschema for the checking line of business that is limited to just checking accounts, as \nshown in Figure 10-9.\n",
      "content_length": 2940,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 330,
      "content": "Chapter 10\n294\nMonth Key (FK)\nAccount Key (FK)\nPrimary Customer Key (FK)\nBranch Key (FK)\nHousehold Key (FK)\nProduct Key (FK)\nBalance\nChange in Balance\nTotal Deposits\nTotal Withdrawals\nNumber Transactions\nMax Backup Reserve\nNumber Overdraws\nTotal Overdraw Penalties\nCount Local ATM Transactions \nCount Foreign ATM Transactions\nCount Online Transactions\nDays Below Minimum\n+ 10 more facts\nAccount Key (PK)\nAccount Number (NK)\nAccount Address Attributes\nAccount Open Date\n...\nChecking Account Fact\nChecking Account Dimension\nProduct Key (PK)\nProduct Code (NK)\nProduct Description\nPremium Flag\nChecking Type\nInterest Payment Type\nOverdraft Policy\n+ 12 more attributes\nChecking Product Dimension\nFigure 10-9: Line-of-business subtype schema for checking products.\nNow both the subtype checking fact table and corresponding checking account \ndimension are widened to describe all the speciﬁ c facts and attributes that make \nsense only for checking products. These subtype schemas must also contain the \nsupertype facts and attributes to avoid joining tables from the supertype and subtype \nschemas for the complete set of facts and attributes. You can also build separate \nsubtype fact and account tables for the other lines of business to support their in-\ndepth analysis requirements. Although creating account-speciﬁ c schemas sounds \ncomplex, only the DBA sees all the tables at once. From the business users’ perspec-\ntive, either it’s a cross-product analysis that relies on the single supertype fact table \nand its attendant supertype account table, or the analysis focuses on a particular \naccount type and only one of the subtype line of business schemas is utilized. In \ngeneral, it makes less sense to combine data from more than one subtype schema, \nbecause by deﬁ nition, the accounts’ facts and attributes are disjointed (or nearly so).\nThe  keys of the subtype account dimensions are the same keys used in the super-\ntype account dimension, which contains all possible account keys. For example, \nif the bank off ers a “$500 minimum balance with no per check charge” checking \naccount, this account would be identiﬁ ed by the same surrogate key in both the \nsupertype and subtype checking account dimensions. Each subtype account dimen-\nsion is a shrunken conformed dimension with a subset of rows from the supertype \n",
      "content_length": 2328,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 331,
      "content": "Financial Services 295\naccount dimension table; each subtype account dimension contains attributes spe-\nciﬁ c to a particular account type.\nThis supertype/subtype design technique applies to any business that off ers \nwidely varied products through multiple lines of business. If you work for a technol-\nogy company that sells hardware, software, and services, you can imagine building \nsupertype sales fact and product dimension tables to deliver the global customer \nperspective. The supertype tables would include all facts and dimension attributes \nthat are common across lines of business. The supertype tables would then be \nsupplemented with schemas that do a deep dive into subtype facts and attributes that \nvary by business. Again, a speciﬁ c product would be assigned the same surrogate \nproduct key in both the supertype and subtype product dimensions.\nNOTE \nA family of supertype and subtype fact tables are needed when a business \nhas heterogeneous products that have naturally diff erent facts and descriptors, but \na single customer base that demands an integrated view.\nIf the lines of business in your retail bank are physically separated so each has its \nown location, the subtype fact and dimension tables will likely not reside in the same \nspace as the supertype fact and dimension tables. In this case, the data in the super-\ntype fact table would be duplicated exactly once to implement all the subtype tables. \nRemember that the subtype tables provide a disjointed partitioning of the accounts, \nso there is no overlap between the subtype schemas.\nSupertype and Subtype Products with Common Facts\nThe supertype and subtype product technique just discussed is appropriate for \nfact tables where a single logical row contains many product-speciﬁ c facts. On the \nother hand, the metrics captured by some business processes, such as the bank’s \nnew account solicitations, may not vary by line of business. In this case, you do \nnot need line-of-business fact tables; one supertype fact table suffi  ces. However, \nyou still can have a rich set of heterogeneous products with diverse attributes. In \nthis case, you would generate the complete portfolio of subtype account dimension \ntables, and use them as appropriate, depending on the nature of the application. \nIn a cross product analysis, the supertype account dimension table would be used \nbecause it can span any group of accounts. In a single account type analysis, you \ncould optionally use the subtype account dimension table instead of the supertype \ndimension if you wanted to take advantage of the subtype attributes speciﬁ c to \nthat account  type.\n",
      "content_length": 2635,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 332,
      "content": "Chapter 10\n296\n Hot Swappable Dimensions\nA  brokerage house may have many clients who track the stock market. All of them \naccess the same fact table of daily high-low-close stock prices. But each client has a \nconﬁ dential set of attributes describing each stock. The brokerage house can sup-\nport this multi-client situation by having a separate copy of the stock dimension \nfor each client, which is joined to the single fact table at query time. We call these \nhot swappable dimensions. To implement hot swappable dimensions in a relational \nenvironment, referential integrity constraints between the fact table and the various \nstock dimension tables probably must be turned off  to allow the switches to occur \non an individual query basis.\nSummary\nWe began this chapter by discussing the situation in which a fact table has too few \ndimensions and provided suggestions for ferreting out additional dimensions using \na triage process. Approaches for handling the often complex relationship between \naccounts, customers, and households were described. We also discussed the use of \nmultiple mini-dimensions in a single fact table, which is fairly common in ﬁ nancial \nservices schemas.\nWe illustrated a technique for clustering numeric facts into arbitrary value bands \nfor reporting purposes through the use of a separate band table. Finally, we pro-\nvided recommendations for any organization that off ers heterogeneous products \nto the same set of customers. In this case, we create a supertype fact table that \ncontains performance metrics that are common across all lines of business. The \ncompanion dimension table contains rows for the complete account portfolio, but \nthe attributes are limited to those that are applicable across all accounts. Multiple \nsubtype schemas, one of each line of business, complement the supertype schema \nwith account-speciﬁ c f acts and attributes.\n",
      "content_length": 1893,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 333,
      "content": "Telecommunications\nT\nhis chapter unfolds a bit differently than preceding chapters. We begin with \na case study overview but we won’t be designing a dimensional model from \nscratch this time. Instead, we’ll step into a project midstream to conduct a design \nreview, looking for opportunities to improve the initial draft schema. The bulk of \nthis chapter focuses on identifying design flaws in dimensional models.\nWe’ll use a billing vignette drawn from the telecommunications industry as the \nbasis for the case study; it shares similar characteristics with the billing data gener-\nated by a utilities company. At the end of this chapter we’ll describe the handling \nof geographic location information in the data warehouse.\nChapter 11 discusses the following concepts:\n \n■Bus matrix snippet for telecommunications company\n \n■Design review exercise\n \n■Checklist of common design mistakes\n \n■Recommended tactics when conducting design reviews\n \n■Retroﬁ tting existing data structures\n \n■Abstract geographic location dimensions\n Telecommunications Case Study \nand Bus Matrix\nGiven  your extensive experience in dimensional modeling (10 chapters so far), you’ve \nrecently been recruited to a new position as a dimensional modeler on the DW/BI \nteam for a large wireless telecommunications company. On your ﬁ rst day, after a few \nhours of human resources paperwork and orientation, you’re ready to get to work.\nThe DW/BI team is anxious for you to review its initial dimensional design. So \nfar it seems the project is off  to a good start. The business and IT sponsorship com-\nmittee appreciates that the DW/BI program must be business-driven; as such, the \n11\n",
      "content_length": 1660,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 334,
      "content": "Chapter 11\n298\ncommittee was fully supportive of the business requirements gathering process. \nBased on the requirements initiative, the team drafted an initial data warehouse bus \nmatrix, illustrated in Figure 11-1. The team identiﬁ ed several core business processes \nand a number of common dimensions. Of course, the complete enterprise matrix \nwould have a much larger number of rows and columns, but you’re comfortable that \nthe key constituencies’ major data requirements have been captured.\nPurchasing\nInternal Inventory\nChannel Inventory\nService Activation\nProduct Sales\nPromotion Participation\nCall Detail Traffic\nDate\nProduct\nCustomer\nService Line #\nSwitch\nCustomer Billing\nCustomer Support Calls\nRepair Work Orders\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nEmployee\nSupport Call\nProfile\nRate Plan\nSales\nOrganization\nFigure 11-1: Sample bus matrix rows for telecommunications company.\nThe sponsorship committee decided to focus on the customer billing process for the \ninitial DW/BI project. Business management determined better access to the metrics \nresulting from the billing process would have a signiﬁ cant impact on the business. \nManagement wants the ability to see monthly usage and billing metrics (otherwise \nknown as revenue) by customer, sales organization, and rate plan to perform sales \nchannel and rate plan analyses. Fortunately, the IT team felt it was feasible to tackle \nthis business process during the ﬁ rst warehouse iteration.\nSome people in the IT organization thought it would be preferable to tackle \nindividual call and message detail traffi  c, such as every call initiated or received by \nevery phone. Although this level of highly granular data would provide interesting \ninsights, it was determined by the joint sponsorship committee that the associated \ndata presents more feasibility challenges while not delivering as much short-term \nbusiness value.\nBased on the sponsors’ direction, the team looked more closely at the customer \nbilling data. Each month, the operational billing system generates a bill for each \nphone number, also known as a service line. Because the wireless company has \nmillions of service lines, this represents a signiﬁ cant amount of data. Each service \n",
      "content_length": 2292,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 335,
      "content": "Telecommunications 299\nline is associated with a single customer. However, a customer can have multiple \nwireless service lines, which appear as separate line items on the same bill; each \nservice line has its own set of billing metrics, such as the number of minutes, \nnumber of text messages, amount of data, and monthly service charges. There is a \nsingle rate plan associated with each service line on a given bill, but this plan can \nchange as customers’ usage habits evolve. Finally, a sales organization and channel \nis associated with each service line to evaluate the ongoing billing revenue stream \ngenerated by each channel partner.\nWorking closely with representatives from the business and other DW/BI team \nmembers, the data modeler designed a fact table with the grain being one row per \nbill each month. The team proudly unrolls its draft dimensional modeling master-\npiece, as shown in Figure 11-2, and expectantly looks at you.\nWhat do you think? Before moving on, please spend several minutes studying the \ndesign in Figure 11-2. Try to identify the design ﬂ aws and suggest improvements \nbefore reading ahead.\nBill Dimension\nBill #\nService Line Number\nBill Date\nRate Plan Code (PK and NK)\nRate Plan Abbreviation\nPlan Minutes Allowed\nPlan Messages Allowed\nPlan Data MB Allowed\nNight-Weekend Minute Ind\nService Line Dimension\nService Line Number (PK)\nService Line Area Code\nService Line Activation Date\nCustomer ID (PK and NK)\nCustomer Name\nCustomer Address\nCustomer City\nCustomer State\nCustomer Zip\nOrig Authorization Credit Score\nCustomer Dimension\nBill # (FK)\nCustomer ID (FK)\nSales Org Number (FK)\nSales Channel ID (FK)\nRate Plan Code (FK)\nRate Plan Type Code\nCall Count\nTotal Minute Count\nNight-Weekend Minute Count\nRoam Minute Count\nMessage Count\nData MB Used\nMonth Service Charge\nPrior Month Service Charge\nYear-to-Date Service Charge\nMessage Charge\nData Charge\nRoaming Charge\nTaxes\nRegulatory Charges\nBilling Fact\nSales Org Number (PK and NK)\nSales Channel ID\nSales Org Dimension\nSales Channel ID (PK and NK)\nSales Channel Name\nSales Channel Dimension\nRate Plan Dimension\nFigure 11-2: Draft schema prior to design review.\nGeneral Design Review Considerations\nBefore we discuss the speciﬁ c issues and potential recommendations for the Figure \n11-2 schema, we’ll outline the design issues commonly encountered when conduct-\ning design reviews. Not to insinuate that the DW/BI team in this case study has \nstepped into all these traps, but it may be guilty of violating several. Again, the \ndesign review exercise will be a more eff ective learning tool if you take a moment \nto jot down your personal ideas regarding Figure 11-2 before proceeding.\n",
      "content_length": 2673,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 336,
      "content": "Chapter 11\n300\nBalance Business Requirements and Source Realities\nDimensional  models should be designed based on a blended understanding of the \nbusiness’s needs, along with the operational source system’s data realities. While \nrequirements are collected from the business users, the underlying source data \nshould be proﬁ led. Models driven solely by requirements inevitably include data ele-\nments that can’t be sourced. Meanwhile, models driven solely by source system data \nanalysis inevitably omit data elements that are critical to the business’s analytics.\n Focus on Business Processes\nAs  reinforced for 10 chapters, dimensional models should be designed to mirror an \norganization’s primary business process events. Dimensional models should not be \ndesigned solely to deliver speciﬁ c reports or answer speciﬁ c questions. Of course, \nbusiness users’ analytic questions are critical input because they help identify which \nprocesses are priorities for the business. But dimensional models designed to pro-\nduce a speciﬁ c report or answer a speciﬁ c question are unlikely to withstand the \ntest of time, especially when the questions and report formats are slightly modiﬁ ed. \nDeveloping dimensional models that more fully describe the underlying business \nprocess are more resilient to change. Process-centric dimensional models also address \nthe analytic needs from multiple business departments; the same is deﬁ nitely not \ntrue when models are designed to answer a single department’s speciﬁ c need.\nAfter the base processes have been built, it may be useful to design complemen-\ntary schemas, such as summary aggregations, accumulating snapshots that look \nacross a workﬂ ow of processes, consolidated fact tables that combine facts from \nmultiple processes to a common granularity, or subset fact tables that provide access \nto a limited subset of fact data for security or data distribution purposes. Again, these \nare all secondary complements to the core process-centric dimensional models.\n Granularity\nThe  ﬁ rst question to always ask during a design review is, “What’s the grain of the \nfact table?” Surprisingly, you often get inconsistent answers to this inquiry from a \ndesign team. Declaring a clear and concise deﬁ nition of the grain of the fact table \nis critical to a productive modeling eff ort. The project team and business liaisons \nmust share a common understanding of this grain declaration; without this agree-\nment, the design eff ort will spin in circles.\nOf course, if you’ve read this far, you’re aware we strongly believe fact tables \nshould be built at the lowest level of granularity possible for maximum ﬂ exibility \nand extensibility, especially given the unpredictable ﬁ ltering and grouping required \nby business user queries. Users typically don’t need to see a single row at a time, \n",
      "content_length": 2837,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 337,
      "content": "Telecommunications 301\nbut you can’t predict the somewhat arbitrary ways they’ll want to screen and roll \nup the details. The deﬁ nition of the lowest level of granularity possible depends on \nthe business process being modeled. In this case, you want to implement the most \ngranular data available for the selected billing process, not just the most granular \ndata available in the enterprise.\nSingle Granularity for Facts\nAfter  the fact table granularity has been established, facts should be identiﬁ ed \nthat are consistent with the grain declaration. To improve performance or reduce \nquery complexity, aggregated facts such as year-to-date totals sometimes sneak into \nthe fact row. These totals are dangerous because they are not perfectly additive. \nAlthough a year-to-date total reduces the complexity and run time of a few speciﬁ c \nqueries, having it in the fact table invites double counting the year-to-date column \n(or worse) when more than one date is included in the query results. It is important \nthat once the grain of a fact table is chosen, all the additive facts are presented at \na uniform grain.\nYou should prohibit text ﬁ elds, including cryptic indicators and ﬂ ags, from the \nfact table. They almost always take up more space in the fact table than a surrogate \nkey. More important, business users generally want to query, constrain, and report \nagainst these text ﬁ elds. You can provide quicker responses and more ﬂ exible access \nby handling these textual values in a dimension table, along with descriptive rollup \nattributes associated with the ﬂ ags and indicators.\n Dimension Granularity and Hierarchies\nEach  of the dimensions associated with a fact table should take on a single value \nwith each row of fact table measurements. Likewise, each of the dimension attri-\nbutes should take on one value for a given dimension row. If the attributes have a \nmany-to-one relationship, this hierarchical relationship can be represented within \na single dimension. You should generally look for opportunities to collapse or \ndenormalize dimension hierarchies whenever possible.\nExperienced  data modelers often revert to the normalization techniques they’ve \napplied countless times in operational entity-relationship models. These modelers \noften need to be reminded that normalization is absolutely appropriate to support \ntransaction processing and ensure referential integrity. But dimensional models \nsupport analytic processing. Normalization in the dimensional model negatively \nimpacts the model’s twin objectives of understandability and performance. Although \nnormalization is not forbidden in the extract, transform, and load (ETL) system \nwhere data integrity must be ensured, it does place an additional burden on the \ndimension change handling subsystems.\n",
      "content_length": 2796,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 338,
      "content": "Chapter 11\n302\nSometimes designers attempt to deal with dimension hierarchies within the fact \ntable. For example, rather than having a single foreign key to the product dimension, \nthey include separate foreign keys for the key elements in the product hierarchy, \nsuch as brand and category. Before you know it, a compact fact table turns into an \nunruly centipede fact table joining to dozens of dimension tables. If the fact table \nhas more than 20 or so foreign keys, you should look for opportunities to combine \nor collapse dimensions.\nElsewhere, normalization appears with the snowﬂ aking of hierarchical relationships \ninto separate dimension tables linked to one another. We generally also discourage this \npractice. Although snowﬂ aking may reduce the disk space consumed by dimension tables, \nthe savings are usually insigniﬁ cant when compared with the entire data warehouse \nenvironment and seldom off set the disadvantages in ease of use or query performance.\nThroughout this book we have occasionally discussed outriggers as permissible \nsnowﬂ akes. Outriggers can play a useful role in dimensional designs, but keep in \nmind that the use of outriggers for a cluster of relatively low-cardinality should be \nthe exception rather than the rule. Be careful to avoid abusing the outrigger tech-\nnique by overusing them in schemas.\nFinally, we sometimes review dimension tables that contain rows for both atomic and \nhierarchical rollups, such as rows for both products and brands in the same dimension \ntable. These dimensions typically have a telltale “level” attribute to distinguish between \nits base and summary rows. This pattern was prevalent and generally accepted decades \nago prior to aggregate navigation capabilities. However, we discourage its continued \nuse given the strong likelihood of user confusion and the risk of overcounting if the \nlevel indicator in every dimension is not constrained in every query.\nDate Dimension\nEvery  fact table should have at least one foreign key to an explicit date dimension. \nDesign teams sometimes join a generic date dimension to a fact table because they \nknow it’s the most common dimension but then can’t articulate what the date refers \nto, presenting challenges for the ETL team and business users alike. We encourage \na meaningful date dimension table with robust date rollup and ﬁ lter attributes.\nFixed Time Series Buckets Instead of Date Dimension\nDesigners  sometimes avoid a date dimension table altogether by representing a time \nseries of monthly buckets of facts on a single fact table row. Legacy operational systems \nmay contain metric sets that are repeated 12 times on a single record to represent \nmonth 1, month 2, and so on. There are several problems with this approach. First, \nthe hard-coded identity of the time slots is inﬂ exible. When you ﬁ ll up all the buck-\nets, you are left with unpleasant choices. You could alter the table to expand the row. \nOtherwise, you could shift everything over by one column, dropping the oldest data, \n",
      "content_length": 3027,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 339,
      "content": "Telecommunications 303\nbut this wreaks havoc with existing query applications. The second problem with \nthis approach is that all the attributes of the date are now the responsibility of the \napplication, not the database. There is no date dimension in which to place calendar \nevent descriptions for constraining. Finally, the ﬁ xed slot approach is ineffi  cient if \nmeasurements are taken only in a particular time period, resulting in null columns \nin many rows. Instead, these recurring time buckets should be presented as separate \nrows in the fact table.\n Degenerate Dimensions\nRather  than treating operational transaction numbers such as the invoice or order \nnumber as degenerate dimensions, teams sometimes want to create a separate \ndimension table for the transaction number. In this case, attributes of the transac-\ntion number dimension include elements from the transaction header record, such \nas the transaction date and customer.\nRemember, transaction numbers are best treated as degenerate dimensions. The \ntransaction date and customer should be captured as foreign keys on the fact table, \nnot as attributes in a transaction dimension. Be on the lookout for a dimension \ntable that has as many (or nearly as many) rows as the fact table; this is a warning \nsign that there may be a degenerate dimension lurking within a dimension table.\nSurrogate Keys\nInstead  of relying on operational keys or identiﬁ ers, we recommend the use of sur-\nrogate keys as the dimension tables’ primary keys. The only permissible deviation \nfrom this guideline applies to the highly predictable and stable date dimension. If \nyou are unclear about the reasons for pursuing this strategy, we suggest backtrack-\ning to Chapter 3: Retail Sales to refresh your memory.\nDimension Decodes and Descriptions\nAll  identiﬁ ers and codes in the dimension tables should be accompanied by descrip-\ntive decodes. This practice often seems counterintuitive to experienced data modelers \nwho have historically tried to reduce data redundancies by relying on look-up codes. \nIn the dimensional model, dimension attributes should be populated with the values \nthat business users want to see on BI reports and application pull-down menus. You \nneed to dismiss the misperception that business users prefer to work with codes. To \nconvince yourself, stroll down to their offi  ces to see the decode listings ﬁ lling their \nbulletin boards or lining their computer monitors. Most users do not memorize the \ncodes outside of a few favorites. New hires are rendered helpless when assaulted with \na lengthy list of meaningless codes.\n",
      "content_length": 2611,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 340,
      "content": "Chapter 11\n304\nThe good news is that decodes can usually be sourced from operational systems \nwith relatively minimal additional eff ort or overhead. Occasionally, the descriptions \nare not available from an operational system but need to be provided by business \npartners. In these cases, it is important to determine an ongoing maintenance strat-\negy to maintain data quality.\nFinally, project teams sometimes opt to embed labeling logic in the BI tool’s \nsemantic layer rather than supporting it via dimension table attributes. Although \nsome BI tools provide the ability to decode within the query or reporting applica-\ntion, we recommend that decodes be stored as data elements instead. Applications \nshould be data-driven to minimize the impact of decode additions and changes. \nOf course, decodes that reside in the database also ensure greater report labeling \nconsistency because most organizations ultimately utilize multiple BI products.\n Conformity Commitment\nLast,  but certainly not least, design teams must commit to using shared conformed \ndimensions across process-centric models. Everyone needs to take this pledge \nseriously. Conformed dimensions are absolutely critical to a robust data architec-\nture that ensures consistency and integration. Without conformed dimensions, \nyou inevitably perpetuate incompatible stovepipe views of performance across the \norganization. By the way, dimension tables should conform and be reused whether \nyou drink the Kimball Kool-Aid or embrace a hub-and-spoke architectural alterna-\ntive. Fortunately, operational master data management systems are facilitating the \ndevelopment and deployment of conformed  dimensions.\nDesign Review Guidelines\nBefore diving into a review of the draft model in Figure 11-2, let’s review some prac-\ntical recommendations for conducting dimensional model design reviews. Proper \nadvance preparation increases the likelihood of a successful review process. Here \nare some suggestions when setting up for a design review:\n \n■Invite the right players. The modeling team obviously needs to participate, \nbut you also want representatives from the BI development team to ensure \nthat proposed changes enhance usability. Perhaps most important, it’s critical \nthat folks who are very knowledgeable about the business and their needs \nare sitting at the table. Although diverse perspectives should participate in a \nreview, don’t invite 25 people to the party.\n \n■Designate someone to facilitate the review. Group dynamics, politics, and the \ndesign challenges will drive whether the facilitator should be a neutral resource \nor involved party. Regardless, their role is to keep the team on track toward a \n",
      "content_length": 2688,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 341,
      "content": "Telecommunications 305\ncommon goal. Eff ective facilitators need the right mix of intelligence, enthu-\nsiasm, conﬁ dence, empathy, ﬂ exibility, assertiveness (and a sense of humor).\n \n■Agree on the review’s scope. Ancillary topics will inevitably arise during the \nreview, but agreeing in advance on the scope makes it easier to stay focused \non the task at hand.\n \n■Block time on everyone’s calendar. We typically conduct dimensional model \nreviews as a focused two day eff ort. The entire review team needs to be present \nfor the full two days. Don’t allow players to ﬂ oat in and out to accommodate \nother commitments. Design reviews require undivided attention; it’s disrup-\ntive when participants leave intermittently.\n \n■Reserve the right space. The same conference room should be blocked for \nthe full two days. Optimally, the room should have a large white board; it’s \nespecially helpful if the white board drawings can be saved or printed. If a \nwhite board is unavailable, have ﬂ ip charts on hand. Don’t forget markers \nand tape; drinks and food also help.\n \n■Assign homework. For example, ask everyone involved to make a list of their \ntop ﬁ ve concerns, problem areas, or opportunities for improvement with the \nexisting design. Encourage participants to use complete sentences when mak-\ning their list so that it’s meaningful to others. These lists should be sent to the \nfacilitator in advance of the design review for consolidation. Soliciting advance \ninput gets people engaged and helps avoid “group think” during the review.\nAfter the team gathers to focus on the review, we recommend the following \ntactics:\n \n■Check attitudes at the door. Although it’s easier said than done, don’t be \ndefensive about prior design decisions. Embark on the review thinking change \nis possible; don’t go in resigned to believing nothing can be done to improve \nthe situation.\n \n■Ban technology unless needed for the review process. Laptops and smart-\nphones should also be checked at the door (at least ﬁ guratively). Allowing \nparticipants to check e-mail during the sessions is no diff erent than having \nthem leave to attend an alternative meeting.\n \n■Exhibit strong facilitation skills. Review ground rules and ensure everyone is \nopenly participating and communicating. The facilitator must keep the group \non track and ban side conversations and discussions that are out of scope or \nspiral into the death zone.\n \n■Ensure a common understanding of the current model. Don’t presume every-\none around the table already has a comprehensive perspective. It may be \nworthwhile to dedicate the ﬁ rst hour to walking through the current design \nand reviewing objectives before delving into potential improvements. \n",
      "content_length": 2718,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 342,
      "content": "Chapter 11\n306\n \n■Designate someone to act as scribe. He should take copious notes about both \nthe discussions and decisions being made.\n \n■Start with the big picture. Just as when you design from a blank slate, begin \nwith the bus matrix. Focus on a single, high-priority business process, deﬁ ne \nits granularity and then move out to the corresponding dimensions. Follow \nthis same “peeling back the layers of the onion” method with a design review, \nstarting with the fact table and then tackling dimension-related issues. But \ndon’t defer the tough stuff  to the afternoon of the second day.\n \n■Remind everyone that business acceptance is critical. Business acceptance is \nthe ultimate measure of DW/BI success. The review should focus on improv-\ning the business users’ experience.\n \n■Sketch out sample rows with data values. Viewing sample data during the \nreview sessions helps ensure everyone has a common understanding of \nthe recommended improvements.\n \n■Close the meeting with a recap. Don’t let participants leave the room with-\nout clear expectations about their assignments and due dates, along with an \nestablished time for the next follow-up.\nAfter the team completes the design review meeting, here are a few recommenda-\ntions to wrap up the process:\n \n■Assign responsibility for any remaining open issues. Commit to wrestling \nthese issues to the ground following the review, even though this can be chal-\nlenging without an authoritative party involved. \n \n■Don’t let the team’s hard work gather dust. Evaluate the cost/beneﬁ t for the \npotential improvements; some changes will be more painless (or painful) \nthan others. Action plans for implementing the improvements then need to \nbe developed.\n \n■Anticipate future reviews. Plan to reevaluate models every 12 to 24 months. \nTry to view inevitable changes to the design as signs of success, rather than \nfailure.\n Draft Design Exercise Discussion\nNow  that you’ve reviewed the common dimensional modeling mistakes frequently \nencountered during design reviews, refer to the draft design in Figure 11-2. Several \nopportunities for improvement should immediately jump out at you.\n",
      "content_length": 2150,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 343,
      "content": "Telecommunications 307\nThe ﬁ rst thing to focus on is the grain of the fact table. The team stated the \ngrain is one row for each bill each month. However, based on your understanding \nfrom the source system documentation and data proﬁ ling eff ort, the lowest level of \nbilling data would be one row per service line on a bill. When you point this out, \nthe team initially directs you to the bill dimension, which includes the service line \nnumber. However, when reminded that each service line has its own set of billing \nmetrics, the team agrees the more appropriate grain declaration would be one row \nper service line per bill. The service line key is moved into the fact table as a foreign \nkey to the service line dimension.\nWhile discussing the granularity, the bill dimension is scrutinized, especially \nbecause the service line key was just moved into the fact table. As the draft model \nwas originally drawn in Figure 11-2, every time a bill row is loaded into the fact \ntable, a row also would be loaded into the bill dimension table. It doesn’t take much \nto convince the team that something is wrong with this picture. Even with the \nmodiﬁ ed granularity to include service line, you would still end up with nearly as \nmany rows in both the fact and bill dimension tables because many customers are \nbilled for one service line. Instead, the bill number should be treated as a degenerate \ndimension. At the same time, you move the bill date into the fact table and join it \nto a robust date dimension playing the role of bill date in this schema.\nYou’ve probably been bothered since ﬁ rst looking at the design by the double \njoins on the sales channel dimension table. The sales channel hierarchy has been \nunnecessarily snowﬂ aked. You opt to collapse the hierarchy by including the sales \nchannel identiﬁ ers (hopefully along with more meaningful descriptors) as additional \nattributes in the sales organization dimension table. In addition, you can eliminate \nthe unneeded sales channel foreign key in the fact table.\nThe design inappropriately treats the rate plan type code as a textual fact. Textual \nfacts are seldom a sound design choice. In this case study, the rate plan type code \nand its decode can be treated as rollup attributes in the rate plan dimension table.\nThe team spent some time discussing the relationship between the service line \nand the customer, sales organization, and rate plan dimensions. Because there is \na single customer, sales organization, and rate plan associated with a service line \nnumber, the dimensions theoretically could be collapsed and modeled as service \nline attributes. However, collapsing the dimensions would result in a schema with \njust two dimensions: bill date and service line. The service line dimension already \nhas millions of rows in it and is rapidly growing. In the end, you opt to treat the \ncustomer, sales organization, and rate plan as separate entities (or mini-dimensions) \nof the service line.\n",
      "content_length": 2979,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 344,
      "content": "Chapter 11\n308\nSurrogate keys are used inconsistently throughout the design. Many of the draft \ndimension tables use operational identiﬁ ers as primary keys. You encourage the \nteam to implement surrogate keys for all the dimension primary keys and then \nreference them as fact table foreign keys.\nThe original design was riddled with operational codes and identiﬁ ers. Adding \ndescriptive names makes the data more legible to the business users. If required \nby the business, the operational codes can continue to accompany the descriptors \nas dimension attributes.\nFinally, you notice that there is a year-to-date metric stored in the fact table. \nAlthough the team felt this would enable users to report year-to-date ﬁ gures more \neasily, in reality, year-to-date facts can be confusing and prone to error. You opt \nto remove the year-to-date fact. Instead, users can calculate year-to-date amounts \non-the-ﬂ y by using a constraint on the year in the date dimension or leveraging the \nBI tool’s capabilities.\nAfter two exhausting days, the initial review of the design is complete. Of \ncourse, there’s more ground to cover, including the handling of changes to the \ndimension attributes. In the meantime, everyone on the team agrees the revamped \ndesign, illustrated in Figure 11-3, is a vast improvement. You’ve earned your ﬁ rst \nweek’s pay.\nBill Date Dimension\nRate Plan Key (PK)\nRate Plan Code\nRate Plan Name\nRate Plan Abbreviation\nRate Plan Type Code\nRate Plan Type Description\nPlan Minutes Allowed\nPlan Messages Allowed\nPlan Data MB Allowed\nNight-Weekend Minute Ind\nService Line Dimension\nService Line Key (PK)\nService Line Number (NK)\nService Line Area Code\nService Line Activation Date\nCustomer Key (PK)\nCustomer ID (NK)\nCustomer Name\nCustomer Address\nCustomer City\nCustomer State\nCustomer Zip\nOrig Authorization Credit Score\nCustomer Dimension\nBill Date Key (FK)\nCustomer Key (FK)\nService Line Key (FK)\nSales Organization Key (FK)\nRate Plan Key (FK)\nBill Number (DD)\nCall Count\nTotal Minute Count\nNight-Weekend Minute Count\nRoam Minute Count\nMessage Count\nData MB Used\nMonth Service Charge\nMessage Charge\nData Charge\nRoaming Charge\nTaxes\nRegulatory Charges\nBilling Fact\nSales Organization Key (PK)\nSales Organization Number\nSales Organization Name\nSales Channel ID\nSales Channel Name\nSales Organization Dimension\nRate Plan Dimension\nFigure 11-3: Draft schema following design review.\n",
      "content_length": 2397,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 345,
      "content": "Telecommunications 309\nRemodeling Existing Data Structures\nIt’s  one thing to conduct a review and identify opportunities for improvement. \nHowever, implementing the changes might be easier said than done if the design \nhas already been physically implemented.\nFor example, adding a new attribute to an existing dimension table feels like a \nminor enhancement. It is nearly pain-free if the business data stewards declare it \nto be a slowly changing dimension type 1 attribute. Likewise if the attribute is to \nbe populated starting now with no attempt to backﬁ ll historically accurate values \nbeyond a Not Available attribute value; note that while this tactic is relatively easy \nto implement, it presents analytic challenges and may be deemed unacceptable. But \nif the new attribute is a designated type 2 attribute with the requirement to capture \nhistorical changes, this seemingly simple enhancement just got much more com-\nplicated. In this scenario, rows need to be added to the dimension table to capture \nthe historical changes in the attribute, along with the other dimension attribute \nchanges. Some fact table rows then need to be recast so the appropriate dimension \ntable row is associated with the fact table’s event. This most robust approach con-\nsumes surprisingly more eff ort than you might initially imagine.\nMuch less surprising is the eff ort required to take an existing dimensional model \nand convert it into a structure that leverages newly created conformed dimensions. \nAs discussed in Chapter 4: Inventory, at a minimum, the fact table’s rows must be \ncompletely reprocessed to reference the conformed dimension keys. The task is \nobviously more challenging if there are granularity or other major issues.\nIn addition to thinking about the data-centric challenges of retroﬁ tting existing \ndata structures, there are also unwanted ripples in the BI reporting and analytic \napplications built on the existing data foundation. Using views to buff er the BI appli-\ncations from the physical data structures provides some relief, but it’s typically not \nadequate to avoid unpleasant whipsawing in the BI environment.\nWhen considering enhancements to existing data structures, you must evaluate \nthe costs of tackling the changes alongside the perceived beneﬁ ts. In many cases, \nyou’ll determine improvements need to be made despite the pain. Similarly, you \nmay determine the best approach is to decommission the current structures to put \nthem out of their misery and tackle the subject area with a fresh slate. Finally, in \nsome situations, the best approach is to simply ignore the suboptimal data structures \nbecause the costs compared to the potential beneﬁ ts don’t justify the remodeling \nand schema improvement eff ort. Sometimes, the best time to consider a remodeling \neff ort is when other changes, such as a source system conversion or migration to a \nnew BI tool standard, provide a  catalyst.\n",
      "content_length": 2935,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 346,
      "content": "Chapter 11\n310\n Geographic Location Dimension\nLet’s  shift gears and presume you work for a phone company with land lines tied to \na speciﬁ c physical location. The telecommunications and utilities industries have a \nvery well-developed notion of location. Many of their dimensions contain a precise \ngeographic location as part of the attribute set. The location may be resolved to a \nphysical street, city, state, ZIP code, latitude, and longitude. Latitude and longitude \ngeo-coding can be leveraged for geospatial analysis and map-centric visualization. \nSome designers imagine a single master location table where address data is stan-\ndardized and then the location dimension is attached as an outrigger to the service \nline telephone number, equipment inventory, network inventory (including poles \nand switch boxes), real estate inventory, service location, dispatch location, right of \nway, and customer entities. In this scenario, each row in the master location table \nis a speciﬁ c point in space that rolls up to every conceivable geographic grouping.\nStandardizing the attributes associated with points in space is valuable. However, \nthis is a back room ETL task; you don’t need to unveil the single resultant table \ncontaining all the addresses the organization interacts with to the business users. \nGeographic information is naturally handled as attributes within multiple dimen-\nsions, not as a standalone location dimension or outrigger. There is typically little \noverlap between the geographic locations embedded in various dimensions. You \nwould pay a performance price for consolidating all the disparate addresses into a \nsingle dimension.\nOperational systems often embrace data abstraction, but you should typically \navoid generic abstract dimensions, such as a generalized location dimension in the \nDW/BI presentation area because they negatively impact the ease-of-use and query \nperformance objectives. These structures are more acceptable behind the scenes in \nthe ETL back room.\nSummary\nThis chapter provided the opportunity to conduct a design review using an example \ncase study. It provided recommendations for conducting eff ective design reviews, \nalong with a laundry list of common design ﬂ aws to scout for when performing a \nreview. We encourage you to use this laundry list to review your own draft schemas \nwhen searching for potent ial improvements.\n",
      "content_length": 2394,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 347,
      "content": "Transportation\nV\noyages  occur whenever a person or thing travels from one point to another, \nperhaps with stops in the middle. Obviously, voyages are a fundamental \nconcept for organizations in the travel industry. Shippers and internal logistical \nfunctions also relate to the discussion, as well as package delivery services and car \nrental companies. Somewhat unexpected, many of this chapter’s schemas are also \napplicable to telecommunications network route analyses; a phone network can \nbe thought of as a map of possible voyages that a call makes between origin and \ndestination phone numbers.\nIn  this chapter we’ll draw on an airline case study to explore voyages and routes \nbecause many readers are familiar (perhaps too familiar) with the subject matter. \nThe case study lends itself to a discussion of multiple fact tables at diff erent granu-\nlarities. We’ll also elaborate on dimension role playing and additional date and time \ndimension considerations. As usual, the intended audience for this chapter should \nnot be limited to the industries previously listed.\nChapter 12 discusses the following concepts:\n \n■Bus matrix snippet for an airline\n \n■Fact tables at diff erent levels of granularity\n \n■Combining correlated role-playing dimensions \n \n■Country-speciﬁ c date dimensions\n \n■Dates and times in multiple time zones\n \n■Recap of localization issues\n Airline Case Study and Bus Matrix\nWe’ll  begin by exploring a simpliﬁ ed bus matrix, and then dive into the fact tables \nassociated with ﬂ ight activity.\n12\n",
      "content_length": 1531,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 348,
      "content": "Chapter 12\n312\nFigure 12-1 shows a snippet of an airline’s bus matrix. This example includes \nan additional column to capture the degenerate dimension associated with most of \nthe bus process events. Like most organizations, airlines are keenly interested in \nrevenue. In this industry, the sale of a ticket represents unearned revenue; revenue \nis earned when a passenger takes a ﬂ ight between origin and destination airports.\nReservations\nIssued Tickets\nUnearned Revenue & Availability\nFlight Activity\nFrequent Flyer Account Credits \nCustomer Care Interactions\nFrequent Flyer Communications\nTime\nAirport\nPassenger\nFare Basis\nAircraft\nMaintenance Work Orders\nCrew Scheduling\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nConf #\nConf #\nTicket #\nConf #\nTicket #\nConf #\nTicket #\nCase #\nTicket #\nWork\nOrder #\nX\nDate\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nCommunication\nProfile\nBooking Channel\nClass of Service\nTransaction ID #\nFigure 12-1: Subset of bus matrix row for an airline.\nThe business and DW/BI team representatives decide the ﬁ rst deliverable should \nfocus on ﬂ ight activity. The marketing department wants to analyze what ﬂ ights the \ncompany’s frequent ﬂ yers take, what fare basis they pay, how often they upgrade, \nhow they earn and redeem their frequent ﬂ yer miles, whether they respond to spe-\ncial fare promotions, how long their overnight stays are, and what proportion of \nthese frequent ﬂ yers have gold, platinum, aluminum, or titanium status. The ﬁ rst \nproject doesn’t focus on reservation or ticketing activity data that didn’t result in a \npassenger boarding a plane. The DW/BI team will contend with those other sources \nof data in subsequent phases.\n Multiple Fact Table Granularities\nWhen  it comes to the grain as you work through the four-step design process, this \ncase presents multiple potential levels of fact table granularity, each having diff er-\nent associated metrics.\n",
      "content_length": 1930,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 349,
      "content": "Transportation 313\nAt the most granular level, the airline captures data at the leg level. The leg \nrepresents an aircraft taking off  at one airport and landing at another without any \nintermediate stops. Capacity planning and ﬂ ight scheduling analysts are interested \nin this discrete level of information because they can look at the number of seats \nto calculate load factors by leg. Operational aircraft ﬂ ight metrics are captured at \nthe leg level, such as ﬂ ight duration and the number of minutes late at departure \nand arrival. Perhaps there’s even a dimension to easily identify on-time arrivals.\nThe  next level of granularity corresponds to a segment. Segments refer to a \nsingle ﬂ ight number (such as Delta ﬂ ight number 40 or DL0040) ﬂ own by a single \naircraft. Segments may have one or more legs associated with them; in most cases \nsegments are composed of just one leg with a single take-off  and landing. If you \ntake a ﬂ ight from San Francisco to Minneapolis with a stop in Denver but no air-\ncraft or ﬂ ight number change, you have ﬂ own one segment (SFO-MSP) but two \nlegs (SFO-DEN and DEN-MSP). Conversely, if the ﬂ ight ﬂ ew nonstop from San \nFrancisco to Minneapolis, you would have ﬂ own one segment as well as one leg. \nThe segment represents the line item on an airline ticket coupon; passenger revenue \nand mileage credit is determined at the segment level. So although some airline \ndepartments focus on leg level operations, the marketing and revenue groups focus \non segment-level metrics.\nNext, you can analyze ﬂ ight activity by trip. The trip provides an accurate picture \nof customer demand. In the prior example, assume the ﬂ ights from San Francisco \nto Minneapolis required the ﬂ yer to change aircraft in Denver. In this case, the trip \nfrom San Francisco to Minneapolis would entail two segments corresponding to the \ntwo involved aircraft. In reality, the passenger just asked to go from San Francisco \nto Minneapolis; the fact that she needs to stop in Denver is merely a necessary evil. \nFor this reason, sales and marketing analysts are also interested in trip level data.\nFinally, the airline collects data for the itinerary, which is equivalent to the entire \nairline ticket or reservation conﬁ rmation number.\nThe DW/BI team and business representatives decide to begin at the segment-level \ngrain. This represents the lowest level of data with meaningful revenue metrics. \nAlternatively, you could lean on the business for rules to allocate the segment-level \nmetrics down to the leg, perhaps based on the mileage of each leg within the seg-\nment. The data warehouse inevitably will tackle the more granular leg level data for \nthe capacity planners and ﬂ ight schedulers at some future point. The conforming \ndimensions built during this ﬁ rst iteration will be leveraged at that time.\n There will be one row in the fact table for each boarding pass collected from \npassengers. The dimensionality associated with this data is quite extensive, as \nillustrated in Figure 12-2. The schema extensively uses the role-playing technique. \nThe multiple date, time, and airport dimensions link to views of a single underly-\ning physical date, time, and airport dimension table, respectively, as we discussed \noriginally in Chapter 6: Order Management.\n",
      "content_length": 3297,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 350,
      "content": "Chapter 12\n314\nTime-of-Day Dimension (views for 2 roles)\nAirport Dimension (views for 2 roles)\nDate Dimension (views for 2 roles)\nPassenger Dimension\nPassenger Profile Dimension\nClass of Service Flown Dimension\nBooking Channel Dimension\nAircraft Dimension\nFare Basis Dimension\nSegment-Level Flight Activity Fact\nScheduled Departure Date Key (FK)\nScheduled Departure Time Key (FK)\nActual Departure Date Key (FK)\nActual Departure Time Key (FK)\nPassenger Key (FK)\nPassenger Profile Key (FK)\nSegment Origin Airport Key (FK)\nSegment Destination Airport Key (FK)\nAircraft Key (FK)\nClass of Service Flown Key (FK)\nFare Basis Key (FK)\nBooking Channel Key (FK)\nConfirmation Number (DD)\nTicket Number (DD)\nSegment Sequence Number (DD)\nFlight Number (DD)\nBase Fare Revenue\nPassenger Facility Charges\nAirport Tax\nGovernment Tax\nBaggage Charges\nUpgrade Fees\nTransaction Fees\nSegment Miles Flown\nSegment Miles Earned\nFigure 12-2: Initial segment ﬂ ight activity schema.\nThe  passenger dimension is a garden variety customer dimension with rich attri-\nbutes captured about the most valuable frequent ﬂ yers. Interestingly, frequent ﬂ yers \nare motivated to help maintain this dimension accurately because they want to \nensure they’re receiving appropriate mileage credit. For a large airline, this dimen-\nsion has tens to hundreds of millions of rows.\nMarketing wants to analyze activity by the frequent ﬂ yer tier, which can change \nduring the course of a year. In addition, you learned during the requirements pro-\ncess that the users are interested in slicing and dicing based on the ﬂ yers’ home \nairports, whether they belong to the airline’s airport club at the time of each ﬂ ight, \nand their lifetime mileage tier. Given the change tracking requirements, coupled \nwith the size of the passenger dimension, we opt to create a separate passenger pro-\nﬁ le mini-dimension, as we discussed in Chapter 5: Procurement, with one row for \neach unique combination of frequent ﬂ yer elite tier, home airport, club membership \nstatus, and lifetime mileage tier. Sample rows for this mini-dimension are illustrated \nin Figure 12-3. You considered treating these attributes as slowly changing type \n2 attributes, especially because the attributes don’t rapidly change. But given the \nnumber of passengers, you opt for a type 4 mini-dimension instead. As it turns \nout, marketing analysts often leverage this mini-dimension for their analysis and \nreporting without touching the millions of passenger dimension rows.\n",
      "content_length": 2496,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 351,
      "content": "Transportation 315\nPassenger Profile\nKey\n1\n2\n3\n789\n790\n791\n2468\n2469\n2470\n...\n...\n...\nBasic\nBasic\nBasic\nMidTier\nMidTier\nMidTier\nWarriorTier\nWarriorTier\nWarriorTier\n...\n...\n...\nATL\nATL\nBOS\nATL\nATL\nBOS\nATL\nATL\nBOS\n...\n...\n...\nNon-Member\nClub Member\nNon-Member\nNon-Member\nClub Member\nNon-Member\nClub Member\nClub Member\nClub Member\n...\n...\n...\nUnder 100,000 miles\nUnder 100,000 miles\nUnder 100,000 miles\n100,000-499,999 miles\n100,000-499,999 miles\n100,000-499,999 miles\n1,000,000-1,999,999 miles\n2,000,000-2,999,999 miles\n1,000,000-1,999,999 miles\n...\n...\n...\nFrequent Flyer\nTier\nHome Airport\nClub Membership\nStatus\nLifetime Mileage\nTier\nFigure 12-3: Passenger mini-dimension sample rows.\nThe aircraft dimension contains information about each plane ﬂ own. The origin \nand destination airports associated with each ﬂ ight are called out separately to \nsimplify the user’s view of the data and make access more effi  cient.\nThe class of service ﬂ own describes whether the passenger sat in economy, pre-\nmium economy, business, or ﬁ rst class. The fare basis dimension describes the terms \nsurrounding the fare. It would identify whether it’s an unrestricted fare, a 21-day \nadvance purchase fare with change and cancellation penalties, or a 10 percent off  \nfare due to a special promotion.\nThe  sales channel dimension identiﬁ es how the ticket was purchased, whether \nthrough a travel agency, directly from the airline’s phone number, city ticket offi  ce, or \nwebsite, or via another internet travel services provider. Although the sales channel \nrelates to the entire ticket, each segment should inherit ticket-level dimensional-\nity. In addition, several operational numbers are associated with the ﬂ ight activity \ndata, including the itinerary number, ticket number, ﬂ ight number, and segment \nsequence number.\nThe facts captured at the segment level of granularity include the base fare rev-\nenue, passenger facility charges, airport and government taxes, other ancillary \ncharges and fees, segment miles ﬂ own, and segment miles awarded (in those cases \nin which a minimum number of miles are awarded regardless of the ﬂ ight distance).\n Linking Segments into Trips\nDespite  the powerful dimensional framework you just designed, you cannot easily \nanswer one of the most important questions about your frequent ﬂ yers, namely, \n“Where are they going?” The segment grain masks the true nature of the trip. If \nyou fetch all the segments of a trip and sequence them by segment number, it is still \n",
      "content_length": 2502,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 352,
      "content": "Chapter 12\n316\nnearly impossible to discern the trip start and endpoints. Most complete itinerar-\nies start and end at the same airport. If a lengthy stop were used as a criterion for \na meaningful trip destination, it would require extensive and tricky processing at \nthe BI reporting layer whenever you try to summarize trips.\nThe answer is to introduce two more airport role-playing dimensions, trip origin \nand trip destination, while keeping the grain at the ﬂ ight segment level. These are \ndetermined during data extraction by looking on the ticket for any stop of more \nthan four hours, which is the airline’s offi  cial deﬁ nition of a stopover. You need to \nexercise some caution when summarizing data by trip in this schema. Some of the \ndimensions, such as fare basis or class of service ﬂ own, don’t apply at the trip level. \nOn the other hand, it may be useful to see how many trips from San Francisco to \nMinneapolis included an unrestricted fare on a segment.\nIn addition to linking segments into trips on the segment ﬂ ight activity schema, \nif the business users are constantly looking at information at the trip level, rather \nthan by segment, you might create an aggregate fact table at the trip grain. Some of \nthe earlier dimensions discussed, such as class of service and fare basis, obviously \nwould not be applicable. The facts would include aggregated metrics like trip total \nbase fare or trip total taxes, plus additional facts that would appear only in this \ncomplementary trip summary table, such as the number of segments in the trip. \nHowever, you would go to the trouble of creating this aggregate table only if there \nwere obvious performance or usability issues when you use the segment-level table \nas the basis for rolling up the same reports. If a typical trip consists of three seg-\nments, you might barely see a three times performance improvement with such an \naggregate table, meaning it may not be worth the bother.\nRelated Fact Tables\nAs discussed earlier, you would likely create a leg-grained ﬂ ight activity fact table \nto satisfy the more operational needs surrounding the departure and arrival of each \nﬂ ight. Metrics at the leg level might include actual and blocked ﬂ ight durations, \ndeparture and arrival delays, and departure and arrival fuel weights.\nIn addition to the ﬂ ight activity, there will be fact tables to capture reservations \nand issued tickets. Given the focus on maximizing revenue, there might be a rev-\nenue and availability snapshot for each ﬂ ight; it could provide snapshots for the \nﬁ nal 90 days leading up to a ﬂ ight departure with cumulative unearned revenue and \nremaining availability per class of service for each scheduled ﬂ ight. The snapshot \nmight include a dimension supporting the concept of “days prior to departure” to \nfacilitate the comparison of similar ﬂ ights at standard milestones, such as 60 days \nprior to scheduled departure.\n",
      "content_length": 2927,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 353,
      "content": "Transportation 317\nExtensions to Other Industries\nUsing the airline case study to illustrate a voyage schema makes intuitive sense \nbecause most people have boarded a plane at one time or another. We’ll brieﬂ y touch \non several other variations on this theme.\nCargo Shipper\nThe schema for a cargo shipper looks quite similar to the airline schemas just \ndeveloped. Suppose a transoceanic shipping company transports bulk goods in \ncontainers from foreign to domestic ports. The items in the containers are shipped \nfrom an original shipper to a ﬁ nal consignor. The trip can have multiple stops at \nintermediate ports. It is possible the containers may be off -loaded from one ship to \nanother at a port. Likewise, it is possible one or more of the legs may be by truck \nrather than ship.\nAs illustrated in Figure 12-4, the grain of the fact table is the container on a spe-\nciﬁ c bill-of-lading number on a particular leg of its trip. The ship mode dimension \nidentiﬁ es the type of shipping company and speciﬁ c vessel. The container dimen-\nsion describes the size of the container and whether it requires electrical power or \nrefrigeration. The commodity dimension describes the item in the container. Almost \nanything that can be shipped can be described by harmonized commodity codes, \nwhich are a kind of master conformed dimension used by agencies, including U.S. \nCustoms. The consignor, foreign transporter, foreign consolidator, shipper, domestic \nconsolidator, domestic transporter, and consignee are all roles played by a master \nbusiness entity dimension that contains all the possible business parties associated \nwith a voyage. The bill-of-lading number is a degenerate dimension. We assume the \nfees and tariff s are applicable to the individual leg of the  voyage.\nTravel Services\nIf  you work for a travel services company, you can complement the ﬂ ight activity \nschema with fact tables to track associated hotel stays and rental car usage. These \nschemas would share several common dimensions, such as the date and customer. \nFor hotel stays, the grain of the fact table is the entire stay, as illustrated in Figure \n12-5. The grain of a similar car rental fact table would be the entire rental episode. \nOf course, if constructing a fact table for a hotel chain rather than a travel services \ncompany, the schema would be much more robust because you’d know far more \nabout the hotel property characteristics, the guest’s use of services, and associated \ndetailed charges.\n",
      "content_length": 2495,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 354,
      "content": "Chapter 12\n318\nShipping Transport Fact\nVoyage Departure Date Key (FK)\nLeg Departure Date Key (FK)\nVoyage Origin Port Key (FK)\nVoyage Destination Port Key (FK)\nLeg Origin Port Key (FK)\nLeg Destination Port Key (FK)\nShip Mode Key (FK)\nContainer Key (FK)\nCommodity Key (FK)\nConsignor Key (FK)\nForeign Transporter Key (FK)\nForeign Consolidator Key (FK)\nShipper Key (FK)\nDomestic Consolidator Key (FK)\nDomestic Transporter Key (FK)\nConsignee Key (FK)\nBill-of-Lading Number (DD)\nLeg Fee\nLeg Tariffs\nLeg Miles\nDate Dimension (views for 2 roles)\nPort Dimension (views for 4 roles)\nShip Mode Dimension\nContainer Dimension\nCommodity Dimension\nBusiness Entity Dimension (views for 7 roles)\nFigure 12-4: Shipper schema.\nTravel Services Hotel Stay Fact\nReservation Date Key (FK)\nArrival Date Key (FK)\nDeparture Date Key (FK)\nCustomer Key (FK)\nHotel Property Key (FK)\nSales Channel Key (FK)\nConfirmation Number (DD)\nTicket Number (DD)\nNumber of Nights\nExtended Room Charge\nTax Charge\nDate Dimension (views for 3 roles)\nCustomer Dimension\nSales Channel Dimension\nHotel Property Dimension\nFigure 12-5: Travel services hotel stay schema.\n Combining Correlated Dimensions\nWe stated previously that if a many-to-many relationship exists between two groups \nof dimension attributes, they should be modeled as separate dimensions with sepa-\nrate foreign keys in the fact table. Sometimes, however, you encounter situations \nwhere these dimensions can be combined into a single dimension rather than treat-\ning them as two separate dimensions with two separate foreign keys in the fact table.\n",
      "content_length": 1571,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 355,
      "content": "Transportation 319\nClass of Service\nThe  Figure 12-2 draft schema includes the class of service flown dimension. \nFollowing a design checkpoint with the business community, you learn the \nusers also want to analyze the booking class purchased. In addition, the business users \nwant to easily ﬁ lter and report on activity based on whether an upgrade or down-\ngrade occurred. Your initial reaction might be to include a second role-playing \ndimension and foreign key in the fact table to support both the purchased and \nﬂ own class of service. In addition, you would need a third foreign key for the \nupgrade indicator; otherwise, the BI application would need to include logic to \nidentify numerous scenarios as upgrades, including economy to premium economy, \neconomy to business, economy to ﬁ rst, premium economy to business, and so on. \nIn this situation, however, there are only four rows in the class dimension table \nto indicate ﬁ rst, business, premium economy, and economy classes. Likewise, the \nupgrade indicator dimension also would have just three rows in it, corresponding to \nupgrade, downgrade, or no class change. Because the row counts are so small, you \ncan elect instead to combine the dimensions into a single class of service dimension, \nas illustrated in Figure 12-6.\nClass of Service\nKey\nClass Purchased\nClass Flown\nPurchased-Flown Group\nClass Change\nIndicator\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\nEconomy\nEconomy\nEconomy\nEconomy\nPrem Economy\nPrem Economy\nPrem Economy\nPrem Economy\nBusiness\nBusiness\nBusiness\nBusiness\nFirst\nFirst\nFirst\nFirst\nEconomy\nPrem Economy\nBusiness\nFirst\nEconomy\nPrem Economy\nBusiness\nFirst\nEconomy\nPrem Economy\nBusiness\nFirst\nEconomy\nPrem Economy\nBusiness\nFirst\nEconomy-Economy\nEconomy-Prem Economy\nEconomy-Business\nEconomy-First\nPrem Economy-Economy\nPrem Economy-Prem Economy\nPrem Economy-Business\nPrem Economy-First\nBusiness-Economy\nBusiness-Prem Economy\nBusiness-Business\nBusiness-First\nFirst-Economy\nFirst-Prem Economy\nFirst-Business\nFirst-First\nNo Class Change\nUpgrade\nUpgrade\nUpgrade\nDowngrade\nNo Class Change\nUpgrade\nUpgrade\nDowngrade\nDowngrade\nNo Class Change\nUpgrade\nDowngrade\nDowngrade\nDowngrade\nNo Class Change\nFigure 12-6: Combined class dimension sample rows.\nThe Cartesian product of the separate class dimensions results in a 16-row \ndimension table (4 class purchased rows times 4 class ﬂ own rows). You also have \nthe opportunity in this combined dimension to describe the relationship between \n",
      "content_length": 2462,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 356,
      "content": "Chapter 12\n320\nthe purchased and ﬂ own classes, such as a class change indicator. Think of this \ncombined class of service dimension as a type of junk dimension, introduced in \nChapter 6. In this case study, the attributes are tightly correlated. Other airline fact \ntables, such as inventory availability or ticket purchases, would invariably reference \na conformed class dimension table with just four  rows.\nNOTE \nIn most cases, role-playing dimensions should be treated as separate logi-\ncal dimensions created via views on a single physical table. In isolated situations, \nit may make sense to combine the separate dimensions into a single dimension, \nnotably when the data volumes are extremely small or there is a need for additional \nattributes that depend on the combined underlying roles for context and meaning.\nOrigin and Destination\nLikewise,  consider the pros and cons of combining the origin and destination airport \ndimensions. In this situation the data volumes are more signiﬁ cant, so separate role-\nplaying origin and destination dimensions seem more practical. However, the busi-\nness users may need additional attributes that depend on the combination of origin \nand destination. In addition to accessing the characteristics of each airport, business \nusers also want to analyze ﬂ ight activity data by the distance between the city-pair \nairports, as well as the type of city pair (such as domestic or trans-Atlantic). Even \nthe seemingly simple question regarding the total activity between San Francisco \n(SFO) and Denver (DEN), regardless of whether the ﬂ ights originated in SFO or \nDEN, presents some challenges with separate origin and destination dimensions. \nSQL experts could surely answer the question programmatically with separate air-\nport dimensions, but what about the less empowered? Even if experts can derive \nthe correct answer, there’s no standard label for the nondirectional city-pair route. \nSome reporting applications may label it SFO-DEN, whereas others might opt for \nDEN-SFO, San Fran-Denver, Den-SF, and so on. Rather than embedding inconsis-\ntent labels in BI reporting application code, the attribute values should be stored \nin a dimension table, so common standardized labels can be used throughout the \norganization. It would be a shame to go to the bother of creating a data warehouse \nand then allowing application code to implement inconsistent reporting labels. The \nbusiness sponsors of the DW/BI system won’t tolerate that for long.\nTo satisfy the need to access additional city-pair route attributes, you have two \noptions. One is merely to add another dimension to the fact table for the city-pair \nroute descriptors, including the directional route name, nondirectional route name, \ntype, and distance, as shown in Figure 12-7. The other alternative is to combine \n",
      "content_length": 2832,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 357,
      "content": "Transportation 321\nthe origin and destination airport attributes, plus the supplemental city-pair route \nattributes, into a single dimension. Theoretically, the combined dimension could \nhave as many rows as the Cartesian product of all the origin and destination air-\nports. Fortunately, in real life the number of rows is much smaller than this theo-\nretical limit because airlines don’t operate ﬂ ights between every airport where they \nhave a presence. However, with a couple dozen attributes about the origin airport, \nplus a couple dozen identical attributes about the destination airport, along with \nattributes about the route, you would probably be more tempted to treat them as \nseparate dimensions.\nCity-Pair\nRoute Key\nDirectional\nRoute Name\n1\n2\n3\n4\n5\n6\n191\n191\n3,267\n3,267\n6,737\n6,737\nBOS-JFK\nJFK-BOS\nBOS-LGW\nLGW-BOS\nBOS-NRT\nNRT-BOS\nBOS-JFK\nBOS-JFK\nBOS-LGW\nBOS-LGW\nBOS-NRT\nBOS-NRT\nLess than 200 miles\nLess than 200 miles\n3,000 to 3,500 miles\n3,000 to 3,500 miles\nMore than 6,000 miles\nMore than 6,000 miles\nDomestic\nDomestic\nInternational\nInternational\nInternational\nInternational\nNon-Oceanic\nNon-Oceanic\nTransatlantic\nTransatlantic\nTranspacific\nTranspacific\nNon-Directional\nRoute Name\nRoute Distance\nin Miles\nRoute Distance Band\nDom-Intl Ind\nTransocean Ind\nFigure 12-7: City-pair route dimension sample rows.\nSometimes designers suggest using a bridge table containing the origin and \ndestination airport keys to capture the route information. Although the origin \nand destination represent a many-to-many relationship, in this case, you can \ncleanly represent the relationship within the existing fact table rather than \nusing a bridge.\n More Date and Time Considerations\nFrom the earliest chapters in this book we’ve discussed the importance of having a \nverbose date dimension, whether at the individual day, week, or month granular-\nity, that contains descriptive attributes about the date and private labels for ﬁ scal \nperiods and work holidays. In this ﬁ nal section, we’ll introduce several additional \nconsiderations for dealing with date and time dimensions.\n Country-Speciﬁ c Calendars as Outriggers\nIf  the DW/BI system serves multinational needs, you must generalize the standard \ndate dimension to handle multinational calendars in an open-ended number of coun-\ntries. The primary date dimension contains generic calendar attributes about the date, \n",
      "content_length": 2377,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 358,
      "content": "Chapter 12\n322\nregardless of the country. If your multinational business spans Gregorian, Hebrew, \nIslamic, and Chinese calendars, you would include four sets of days, months, and \nyears in this primary dimension.\nCountry-speciﬁ c date dimensions supplement the primary date table. The key to \nthe supplemental dimension is the primary date key, along with the country code. \nThe table would include country-speciﬁ c date attributes, such as holiday or season \nnames, as illustrated in Figure 12-8. This approach is similar to the handling of \nmultiple ﬁ scal accounting calendars, as described in Chapter 7: Accounting.\nDate Dimension\nDate Key (FK)\nMore FKs ...\nFacts ...\nDate Key (PK)\nDate\nDay of Week\nDay Number in Epoch\nWeek Number in Epoch\nMonth Number in Epoch\nDay Number in Calendar Month\nDay Number in Calendar Year\nDay Number in Fiscal Month\nLast Day in Fiscal Month Indicator\nCalendar Month\nCalendar Month Number in Year\nCalendar Year-Month (YYYY-MM)\nCalendar Quarter\nCalendar Year-Quarter\nCalendar Year\nFiscal Month\nFiscal Month Number in Year\nFiscal Year-Month\nFiscal Quarter\nFiscal Year-Quarter\nFiscal Year\n...\nDate Key (FK)\nCountry Key (FK)\nCountry Name\nCivil Name\nCivil Holiday Flag\nCivil Holiday Name\nReligious Holiday Flag\nReligious Holiday Name\nWeekday Indicator\nSeason Name\nCountry-Specific Date Outrigger\nFact\nFigure 12-8: Country-speciﬁ c calendar outrigger.\nYou can join this table to the main calendar dimension as an outrigger or directly \nto the fact table. If you provide an interface that requires the user to specify a coun-\ntry name, then the attributes of the country-speciﬁ c supplement can be viewed as \nlogically appended to the primary date table, allowing them to view the calendar \nthrough the eyes of a single country at a time. Country-speciﬁ c calendars can be \n",
      "content_length": 1801,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 359,
      "content": "Transportation 323\nmessy to build in their own right; things get even more complicated if you need to \ndeal with local holidays that occur on diff erent days in diff erent parts of a country.\n Date and Time in Multiple Time Zones\nWhen  operating in multiple countries or even just multiple time zones, you’re faced \nwith a quandary concerning transaction dates and times. Do  you capture the date and \ntime relative to local midnight in each time zone, or do you express the time period \nrelative to a standard, such as the corporate headquarters date/time, Greenwich Mean \nTime (GMT), or Coordinated Universal Time (UTC), also known as Zulu time in the \naviation world? To fully satisfy users’ requirements, the correct answer is probably \nboth. The standard time enables you to see the simultaneous nature of transactions \nacross the business, whereas the local time enables you to understand transaction \ntiming relative to the time of day.\nContrary  to popular belief, there are more than 24 time zones (corresponding \nto the 24 hours of the day) in the world. For example, there is a single time zone in \nChina despite its latitudinal span. Likewise, there is a single time zone in India, off -\nset from UTC by 5.5 hours. In Australia, there are three time zones with its Central \ntime zone off set by one-half hour. Meanwhile, Nepal and some other nations use \none-quarter hour off set. The situation gets even more unpleasant when you account \nfor switches to and from daylight saving time. \nGiven the complexities, it’s unreasonable to think that merely providing a UTC \noff set in a fact table can support equivalized dates and times. Likewise, the off set \ncan’t reside in a time or airport dimension table because the off set depends on both \nlocation and date. The recommended approach for expressing dates and times in \nmultiple time zones is to include separate date and time-of-day dimensions corre-\nsponding to the local and equivalized dates, as shown in Figure 12-9. The time-of-day \ndimensions, as discussed in Chapter 3: Retail Sales, support time period groupings \nsuch as shift numbers or rush period time block  designations.\nFlight Activity Fact\nDeparture Date Key (FK)\nGMT Departure Date Key (FK)\nDeparture Time-of-Day Key (FK)\nGMT Departure Time-of-Day Key (FK)\nMore FKs ...\nDegenerate Dimensions ...\nFacts ...\nDate Dimension\n(2 views for roles)\nTime-of-Day Dimension\n(2 views for roles)\nFigure 12-9: Local and equivalized date/time across time zones.\n",
      "content_length": 2478,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 360,
      "content": "Chapter 12\n324\nLocalization Recap\nWe  have discussed the challenges of international DW/BI system in several chapters \nof the book. In addition to the international time zones and calendars discussed in the \nprevious two sections, we have also talked about multi-currency reporting in Chapter \n6 and multi-language support in Chapter 8: Customer Relationship Management.\nAll these database-centric techniques fall under the general theme of localiza-\ntion. Localization in the larger sense also includes the translation of user interface \ntext embedded in BI tools. BI tool vendors implement this form of localization with text \ndatabases containing all the text prompts and labels needed by the tool, which can \nthen be conﬁ gured for each local environment. Of course, this can become quite \ncomplicated because text translated from English to most European languages results \nin text strings that are longer than their English equivalents, which may force a \nredesign of the BI application. Also, Arabic text reads from right to left, and many \nAsian languages are completely diff erent.\nA serious international DW/BI system built to serve business users in many \ncountries needs to be thoughtfully designed to account for a selected set of these \nlocalization issues. But perhaps it is worth thinking about how airport control tow-\ners and airplane pilots around the world deal with language incompatibilities when \ncommunicating critical messages about ﬂ ight directions and altitudes. They all use \none language (English) and unit of measure (feet).\nSummary\nIn this chapter we turned our attention to airline trips or routes; we brieﬂ y touched \non similar scenarios drawn from the shipping and travel services industries. We \nexamined the situation in which we have multiple fact tables at multiple granularities \nwith multiple grain-speciﬁ c facts. We also discussed the possibility of combining \ndimensions into a single dimension table for cases in which the row count volumes \nare extremely small or when there are additional attributes that depend on the com-\nbined dimensions. Again, combining correlated dimensions should be viewed as the \nexception rather than the rule.\nWe wrapped up this chapter by discussing several date and time dimension tech-\nniques, including country-speciﬁ c calendar outriggers and  the handling of absolute \nand relative dates and times.\n",
      "content_length": 2381,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 361,
      "content": "Education\nW\ne step into the world of an educational institution in this chapter, looking first \nat the applicant pipeline as an accumulating snapshot. When accumulating \nsnapshot fact tables were introduced in Chapter 4: Inventory, a product movement \npipeline illustrated the concept; order fulfillment workflows were captured in an \naccumulating snapshot in Chapter 6: Order Management. In this chapter, rather than \nwatching products or orders move through various states, an accumulating snapshot \nis used to monitor prospective student applicants as they progress through admis-\nsions milestones.\nThe other primary concept discussed in this chapter is the factless fact table. We’ll \nexplore several case study illustrations drawn from higher education to further elabo-\nrate on these special fact tables and discuss the analysis of events that didn’t occur.\nChapter 13 discusses the following concepts:\n \n■Example bus matrix snippet for a university or college\n \n■Applicant tracking and research grant proposals as accumulating snapshot \nfact tables\n \n■Factless fact table for admission events, course registration facilities manage-\nment, and student attendance\n \n■Handling of nonexistent events\n University Case Study and Bus Matrix\nIn  this chapter you’re working for a university, college, or other type of educational \ninstitution. Someone at a higher education client once remarked that running a \nuniversity is akin to operating all the businesses needed to support a small vil-\nlage. Universities are simultaneously a real estate property management company \n(residential student housing), restaurant with multiple outlets (dining halls), retailer \n(bookstore), events management and ticketing agency (athletics and speaker events), \n13\n",
      "content_length": 1751,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 362,
      "content": "Chapter 13\n326\npolice department (campus security), professional fundraiser (alumni development), \nconsumer ﬁ nancial services company (ﬁ nancial aid), investment ﬁ rm (endowment \nmanagement), venture capitalist (research and development), job placement ﬁ rm \n(career planning), construction company (buildings and facilities maintenance), and \nmedical services provider (health clinic). In addition to these varied functions, higher \neducation institutions are obviously also focused on attracting high caliber students \nand talented faculty to create a robust educational environment.\nThe bus matrix snippet in Figure 13-1 covers several core processes within an \neducational institution. Traditionally, there has been less focus on revenue and proﬁ t \nin higher education, but with ever-escalating costs and competition, universities \nand colleges cannot ignore these ﬁ nancial metrics. They want to attract and retain \nstudents who align with their academic and other institutional objectives. There’s \na strong interest in analyzing what students are “buying” in terms of courses each \nterm and the associated academic outcomes. Colleges and universities want to \nunderstand many aspects of the student’s experience, along with maintaining an \nongoing relationship well beyond graduation.\n Accumulating Snapshot Fact Tables\n Chapter  4 used an accumulating snapshot fact table to track products identiﬁ ed by \nserial or lot numbers as they move through various inventory stages in a warehouse. \nTake a moment to recall the distinguishing characteristics of an accumulating snap-\nshot fact table:\n \n■A single row represents the complete history of a workflow or pipeline \ninstance.\n \n■Multiple dates represent the standard pipeline milestone events.\n \n■The accumulating snapshot facts often included metrics corresponding to \neach milestone, plus status counts and elapsed durations.\n \n■Each row is revisited and updated whenever the pipeline instance changes; \nboth foreign keys and measured facts may be changed during the fact row \n updates.\nApplicant Pipeline\nNow  envision these same accumulating snapshot characteristics as applied to the \nprospective student admissions pipeline. For those who work in other industries, \nthere are obvious similarities to tracking job applicants through the hiring process \nor sales prospects as they are qualiﬁ ed and become customers.\n",
      "content_length": 2381,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 363,
      "content": "Education 327\nAdmission Events\nStudent Lifecycle Processes\nFinancial Processes\nApplicant Pipeline\nFinancial Aid Awards\nStudent Enrollment/Profile Snapshot\nStudent Residential Housing\nStudent Course Registration & Outcomes\nStudent Course Instructor Evaluations\nStudent Activities\nCareer Placement Activities\nAdvancement Contacts\nAdvancement Pledges & Gifts\nBudgeting\nEndowment Tracking\nGL Transactions\nPayroll\nProcurement\nEmployee Management Processes\nEmployee Headcount Snapshot \nEmployee Hiring & Separations\nEmployee Benefits & Compensation\nAdministrative Processes\nFacilities Utilization\nEnergy Consumption & Waste Management\nWork Orders\nStaff Performance Management\nFaculty Appointment Management\nResearch Proposal Pipeline\nResearch Expenditures\nFaculty Publications\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nDate/Term\nApplicant-Student-Alum\nEmployee (Faculty, Staff)\nFacility\nAccount\nCourse\nDepartment\nFigure 13-1: Subset of bus matrix rows for educational institution.\n",
      "content_length": 1137,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 364,
      "content": "Chapter 13\n328\nIn the case of applicant tracking, prospective students progress through a stan-\ndard set of admissions hurdles or milestones. Perhaps you’re interested in tracking \nactivities around key dates, such as initial inquiry, campus visit, application submit-\nted, application ﬁ le completed, admissions decision notiﬁ cation, and enrolled or \nwithdrawn. At any point in time, admissions and enrollment management analysts \nare interested in how many applicants are at each stage in the pipeline. The process \nis much like a funnel, where many inquiries enter the pipeline, but far less prog-\nress through to the ﬁ nal stage. Admission personnel also would like to analyze the \napplicant pool by a variety of characteristics.\nThe grain of the applicant pipeline accumulating snapshot is one row per prospec-\ntive student; this granularity represents the lowest level of detail captured when the \nprospect enters the pipeline. As more information is collected while the prospective \nstudent progresses toward application, acceptance, and enrollment, you continue \nto revisit and update the fact table row, as illustrated in Figure 13-2.\nApplicant Dimension\nApplicant Key (PK)\nApplicant Name\nApplicant Address Attributes ...\nHigh School\nHigh School GPA\nHigh School Type\nSAT Math Score\nSAT Verbal Score\nSAT Writing Score\nACT Composite Score\nNumber of AP Credits\nGender\nDate of Birth\nEthnicity\nFull time-Part time Indicator\nApplication Source\nIntended Major\n...\nDate Key (PK)\n...\nTerm\nAcademic Year-Term\nAcademic Year\nDate Dimension (views for 6 roles)\nApplication Status Key\nApplication Status Code\nApplication Status Description\nApplication Status Category\nApplication Status Dimension\nApplicant Pipeline Fact\nInitial Inquiry Date Key (FK)\nCampus Visit Date Key (FK)\nApplication Submitted Date Key (FK)\nApplication File Completed Date Key (FK)\nAdmission Decision Notification Date Key (FK)\nApplicant Enroll-Withdraw Date Key (FK)\nApplicant Key (FK)\nApplication Status Key (FK)\nApplication ID (DD)\nInquiry Count\nCampus Visit Count\nApplication Submitted Count\nApplication Completed Count\nAdmit Early Decision Count\nAdmit Regular Decision Count\nWaitlist Count\nDefer to Regular Decision Count\nDeny Count\nEnroll Early Decision Count\nEnroll Regular Decision Count\nAdmit Withdraw Count\nFigure 13-2: Student applicant pipeline as an accumulating snapshot.\nLike earlier accumulating snapshots, there are multiple dates in the fact table \ncorresponding to the standard milestone events. You want to analyze the prospect’s \nprogress by these dates to determine the pace of movement through the pipeline and \nspot bottlenecks. This is especially important if you see a signiﬁ cant lag involving \na candidate whom you’re interested in recruiting. Each of these dates is treated as a \nrole-playing dimension, with a default surrogate key to handle the unknown dates \nfor new and in-process rows.\n",
      "content_length": 2889,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 365,
      "content": "Education 329\nThe applicant dimension contains many interesting attributes about prospective \nstudents. Analysts are interested in slicing and dicing by applicant characteristics \nsuch as geography, incoming credentials (grade point average, college admissions test \nscores, advanced placement credits, and high school), gender, date of birth, ethnicity, \npreliminary major, application source, and a multitude of others. Analyzing these char-\nacteristics at various stages of the pipeline can help admissions personnel adjust their \nstrategies to encourage more (or fewer) students to proceed to the next mile marker.\nThe facts in the applicant pipeline fact table include a variety of counts that are \nclosely monitored by admissions personnel. If available, this table could include esti-\nmated probabilities that the prospect will apply and subsequently enroll if accepted \nto predict admission yields.\nAlternative Applicant Pipeline Schemas\nAccumulating snapshots are appropriate for short-lived processes that have a deﬁ ned \nbeginning and end, with standard intermediate milestones. This type of fact table \nenables you to see an updated status and ultimately ﬁ nal disposition of each appli-\ncant. However, because accumulating snapshot rows are updated, they do not pre-\nserve applicant counts and statuses at critical points in the admissions calendar, \nsuch as the early decision notiﬁ cation date. Given the close scrutiny of these num-\nbers, analysts might also want to retain snapshots at several important cut-off   dates. \nAlternatively, you could build an admission transaction fact table with one row per \ntransaction per applicant for counting and period-to-period comparisons.\nResearch Grant Proposal Pipeline\nThe  research proposal pipeline is another education-based example of an accumu-\nlating snapshot. Faculty and administration are interested in viewing the lifecycle \nof a grant proposal as it progresses through the pipeline from preliminary proposal \nto grant approval and award receipt. This would support analysis of the number of \noutstanding proposals in each stage of the pipeline by faculty, department, research \ntopic area, or research funding source. Likewise, you could see success rates by \nvarious attributes. Having this information in a common repository would allow \nit to be leveraged by a broader university population.\n Factless Fact Tables\nSo far we’ve largely designed fact tables with very similar structures. Each fact table \ntypically has 5 to approximately 20 foreign key columns, followed by one to poten-\ntially several dozen numeric, continuously valued, preferably additive facts. The \nfacts can be regarded as measurements taken at the intersection of the dimension \n",
      "content_length": 2726,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 366,
      "content": "Chapter 13\n330\nkey values. From this perspective, the facts are the justiﬁ cation for the fact table, \nand the key values are simply administrative structure to identify the facts.\nThere are, however, a number of business processes whose fact tables are simi-\nlar to those we’ve been designing with one major distinction. There are no mea-\nsured facts! We introduced factless fact tables while discussing promotion events \nin Chapter 3: Retail Sales, as well as in Chapter 6 to describe sales rep/customer \nassignments. There are numerous examples of factless events in higher education.\nAdmissions Events\n You  can envision a factless fact table to track each prospective student’s attendance \nat an admission event, such as a high school visit, college fair, alumni interview or \ncampus overnight, as illustrated in Figure 13-3.\nAdmissions Event Date Key (FK)\nPlanned Enroll Term Key (FK)\nApplicant Key (FK)\nApplicant Status Key (FK)\nAdmissions Officer Key (FK)\nAdmission Event Key (FK)\nAdmissions Event Attendance Count (=1)\nAdmissions Event Attendance Fact\nPlanned Enroll Term Dimension\nApplication Status Dimension\nAdmissions Event Date Dimension\nApplicant Dimension\nAdmissions Officer Dimension\nAdmission Event Dimension\nFigure 13-3: Admission event attendance as a factless fact table.\nCourse Registrations\nSimilarly,  you can track student course registrations by term using a factless fact \ntable. The grain would be one row for each registered course by student and term, \nas illustrated in Figure 13-4.\nTerm Dimension\nIn this  fact table, the data is at the term level rather than at the more typical cal-\nendar day, week, or month granularity. The term dimension still should conform \nto the calendar date dimension. In other words, each date in the daily calendar \ndimension should identify the term (for example, Fall), term and academic year \n(for example, Fall 2013), and academic year (for example, 2013-2014). The column \nlabels and values must be identical for the attributes common to both the calendar \ndate and term dimensions.\nStudent Dimension and Change Tracking\n The  student dimension is an expanded version of the applicant dimension discussed \nin the ﬁ rst scenario. You still want to retain some information garnered from the \napplication process (for example, geography, credentials, and intended major) but \n",
      "content_length": 2340,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 367,
      "content": "Education 331\nsupplement it with on-campus information, such as part-time or full-time status, \nresidence, athletic involvement indicator, declared major, and class level status (for \nexample, sophomore).\nInstructor Key (PK)\nInstructor Employee ID (NK)\nInstructor Name\nInstructor Address Attributes...\nInstructor Type\nInstructor Tenure Indicator\nInstructor Original Hire Date\nInstructor Years of Service\nStudent Dimension\nStudent Key (PK)\nStudent ID (NK)\n...\nTerm Key (PK)\nTerm\nAcademic Year-Term\nAcademic Year\nTerm Dimension\nTerm Key (FK)\nStudent Key (FK)\nCourse Key (FK)\nInstructor Key (FK)\nCourse Registration Count (=1)\nCourse Key (PK)\nCourse Name\nCourse Department\nCourse Format\nCourse Credit Hours\nCourse Dimension\nCourse Registration Event Fact\nInstructor Dimension\nFigure 13-4: Course registration events as a factless fact table.\nAs discussed in Chapter 5: Procurement, you could imagine placing some of \nthese attributes in a type 4 mini-dimension because factions throughout the uni-\nversity are interested in tracking changes to them, especially for declared major, \nclass level, and graduation attainment. People in administration and academia are \nkeenly interested in academic progress and retention rates by class, school, depart-\nment, and major. Alternatively, if there’s a strong demand to preserve the students’ \nproﬁ les at the time of course registration, plus ﬁ lter and group by the students’ \ncurrent characteristics, you should consider handling the student information as \na slowly changing dimension type 7 with dual student dimension keys in the fact \ntable, as also described in Chapter 5. The surrogate student key would link to a \ndimension table with type 2 attributes; the student’s durable identiﬁ er would link \nto a view of the complete student dimension containing only the current row for \neach student.\nArtiﬁ cial Count Metric\nA   fact table represents the robust set of many-to-many relationships among dimen-\nsions; it records the collision of dimensions at a point in time and space. This course \nregistration fact table could be queried to answer a number of interesting questions \nregarding registration for the college’s academic off erings, such as which students \nregistered for which courses? How many declared engineering majors are taking an \nout-of-major ﬁ nance course? How many students have registered for a given faculty \nmember’s courses during the last three years? How many students have registered \n",
      "content_length": 2459,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 368,
      "content": "Chapter 13\n332\nfor more than one course from a given faculty member? The only peculiarity in \nthese examples is that you don’t have a numeric fact tied to this registration data. \nAs such, analyses of this data will be based largely on counts.\nNOTE \nEvents are modeled as fact tables containing a series of keys, each \nrepresenting a participating dimension in the event. Event tables sometimes have \nno variable measurement facts associated with them and hence are called factless \nfact tables.\nThe SQL for performing counts in this factless fact is asymmetric because of \nthe absence of any facts. When counting the number of registrations for a faculty \nmember, any key can be used as the argument to the COUNT function. For example:\nselect faculty, count(term_key)... group by faculty\nThis gives the simple count of the number of student registrations by faculty, \nsubject to any constraints that may exist in the WHERE clause. An oddity of SQL is \nthat you can count any key and still get the same answer because you are counting \nthe number of keys that ﬂ y by the query, not their distinct values. You would need \nto use a COUNT DISTINCT if you want to count the unique instances of a key rather \nthan the number of keys encountered.\nThe  inevitable confusion surrounding the SQL statement, although not a serious \nsemantic problem, causes some designers to create an artiﬁ cial implied fact, perhaps \ncalled course registration count (as opposed to “dummy”), that is always populated \nby the value 1. Although this fact does not add any information to the fact table, it \nmakes the SQL more readable, such as:\nselect faculty, sum(registration_count)... group by faculty\nAt this point the table is no longer strictly factless, but the “1” is nothing more \nthan an artifact. The SQL will be a bit cleaner and more expressive with the regis-\ntration count. Some BI query tools have an easier time constructing this query with \na few simple user gestures. More important, if you build a summarized aggregate \ntable above this fact table, you need a real column to roll up to meaningful aggre-\ngate registration counts. And ﬁ nally, if deploying to an OLAP cube, you typically \ninclude an explicit count column (always equal to 1) for complex counts because \nthe dimension join keys are not explicitly revealed in a cube.\nIf a measurable fact does surface during the design, it can be added to the schema, \nassuming it is consistent with the grain of student course registrations by term. For \nexample, you could add tuition revenue, earned credit hours, and grade scores to \nthis fact table, but then it’s no longer a factless fact  table.\n",
      "content_length": 2644,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 369,
      "content": "Education 333\n Multiple Course Instructors\nIf  courses are taught by a single instructor, you can associate an instructor key to \nthe course registration events, as shown in Figure 13-4. However, if some courses \nare co-taught, then it is a dimension attribute that takes on multiple values for the \nfact table’s declared grain. You have several options:\n \n■Alter the grain of the fact table to be one row per instructor per course reg-\nistration per student per term. Although this would address the multiple \ninstructors associated with a course, it’s an unnatural granularity that would \nbe extremely prone to overstated registration count errors.\n \n■Add a bridge table with an instructor group key in either the fact table or as \nan outrigger on the course dimension, as introduced in Chapter 8: Customer \nRelationship Management. There would be one row in this table for each instruc-\ntor who teaches courses on his own. In addition, there would be two rows for \neach instructor team; these rows would associate the same group key with \nindividual instructor keys. The concatenation of the group key and instructor \nkey would uniquely identify each bridge table row. As described in Chapter 10: \nFinancial Services, you could assign a weighting factor to each row in the bridge \nif the teaching workload allocation is clearly deﬁ ned. This approach would \nbe susceptible to the potential overstatement issues surrounding the bridge \ntable usage described in Chapter 10.\n \n■Concatenate  the instructor names into a single, delimited attribute on the \ncourse dimension, as discussed in Chapter 9: Human Resources Management. \nThis option enables users to easily label reports with a single dimension attri-\nbute, but it would not support analysis of registration events by instructor \ncharacteristics.\n \n■If one of the instructors is identiﬁ ed as the primary instructor, then her \ninstructor key could be handled as a single foreign key in the fact table, \njoined to a dimension where the attributes were prefaced with “primary” for \ndiff erentiation.\n Course Registration Periodic Snapshots\nThe  grain of the fact table illustrated in Figure 13-4 is one row for each regis-\ntered course by student and term. Some users at the college or university might be \ninterested in periodic snapshots of the course registration events at key academic \ncalendar dates, such as preregistration, start of the term, course drop/add deadline, \nand end of the term. In this case, the fact table’s grain would be one row for each \nstudent’s registered courses for a term per snapshot date.\n",
      "content_length": 2578,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 370,
      "content": "Chapter 13\n334\nFacility Utilization\nThe second type of factless fact table deals with coverage, which can be illustrated \nwith a facilities management scenario. Universities invest a tremendous amount \nof capital in their physical plant and facilities. It would be helpful to understand \nwhich facilities were being used for what purpose during every hour of the day \nduring each term. For example, which facilities were used most heavily? What \nwas the average occupancy rate of the facilities as a function of time of day? \nDoes utilization drop off  signiﬁ cantly on Fridays when no one wants to attend \n(or teach) classes?\nAgain, the factless fact table comes to the rescue. In this case you’d insert one \nrow in the fact table for each facility for standard hourly time blocks during each \nday of the week during a term regardless of whether the facility is being used. \nFigure 13-5 illustrates the schema.\nTime-of-Day Hour Dimension\nTime-of-Day Hour Key (PK)\nTime-of-Day Hour\nDay Part Indicator\nTerm Dimension\nDay of Week Dimension\nTerm Key (FK)\nDay of Week Key (FK)\nTime-of-Day Hour Key (FK)\nFacility Key (FK)\nOwner Department Key (FK)\nAssigned Department Key (FK)\nUtilization Status Key (FK)\nFacility Count (=1)\nFacility Key (PK)\nFacility Building Name - Room\nFacility Building Name\nFacility Building Address attributes...\nFacility Type\nFacility Floor\nFacility Square Footage\nFacility Capacity\nProjector Indicator\nVent Indicator\n...\nFacility Dimension\nFacility Utilization Fact\nDepartment Dimension (2 views for roles)\nUtilization Status Dimension\nFigure 13-5: Facilities utilization as a coverage factless fact table.\nThe facility dimension would include all types of descriptive attributes about the \nfacility, such as the building, facility type (for example, classroom, lab, or offi  ce), \nsquare footage, capacity, and amenities (for example, whiteboard or built-in projec-\ntor). The utilization status dimension would include a text descriptor with values \nof Available or Utilized. Meanwhile, multiple organizations may be involved in \nfacilities utilization. For example, one organization might own the facility during \na time block, but the same or a diff erent organization might be assigned as the \nfacility  user.\n",
      "content_length": 2234,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 371,
      "content": "Education 335\nStudent Attendance\nYou  can visualize a similar schema to track student attendance in a course. In this \ncase, the grain would be one row for each student who walks through the course’s \nclassroom door each day. This factless fact table would share a number of the same \ndimensions discussed with registration events. The primary diff erence would be \nthe granularity is by calendar date in this schema rather than merely term. This \ndimensional model, illustrated in Figure 13-6, allows business users to answer \nquestions concerning which courses were the most heavily attended. Which courses \nsuff ered the least attendance attrition over the term? Which students attended which \ncourses? Which faculty member taught the most students?\nDate Key (FK)\nStudent Key (FK)\nCourse Key (FK)\nInstructor Key (FK)\nFacility Key (FK)\nAttendance Count\nStudent Attendance Fact\nStudent Dimension\nInstructor Dimension\nDate Dimension\nCourse Dimension\nFacility Dimension\nFigure 13-6: Student attendance fact table.\nExplicit Rows for What Didn’t Happen\nPerhaps people are interested in monitoring students who were registered for a \ncourse but didn’t show up. In this example you can envision adding explicit rows to \nthe fact table for attendance events that didn’t occur. The fact table would no longer \nbe factless as there is an attendance metric equal to either 1 or 0.\nAdding rows is viable in this scenario because the non-attendance events have the \nsame exact dimensionality as the attendance events. Likewise, the fact table won’t \ngrow at an alarming rate, presuming (or perhaps hoping) the no-shows are a small \npercentage of the total students registered for a course. Although this approach is \nreasonable in this scenario, creating rows for events that didn’t happen is ridiculous \nin many other situations, such as adding rows to a customer’s sales transaction for \npromoted products that weren’t purchased by the customer.\n What Didn’t Happen with Multidimensional OLAP\nMultidimensional  OLAP databases do an excellent job of helping users understand \nwhat didn’t happen. When the cube is constructed, multidimensional databases \nhandle the sparsity of the transaction data while minimizing the overhead burden \nof storing explicit zeroes. As such, at least for fact cubes that are not too sparse, the \n",
      "content_length": 2317,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 372,
      "content": "Chapter 13\n336\nevent and nonevent data is available for user analysis while reducing some of the \ncomplexities just discussed in the relational star schema world.\nMore Educational Analytic Opportunities\nMany of the business processes described in earlier chapters, such as procurement \nand human resources, are obviously applicable to the university environment given \nthe desire to better monitor and manage costs. Research grants and alumni contri-\nbutions are key sources of revenue, in addition to the tuition revenue.\nResearch grant analysis is often a variation of ﬁ nancial analysis, as discussed in \nChapter 7: Accounting, but at a lower level of detail, much like a subledger. The grain \nwould include additional dimensions to further describe the research grant, such as \nthe corporate or governmental funding source, research topic, grant duration, and \nfaculty investigator. There is a strong need to better understand and manage the \nbudgeted and actual spending associated with each research project. The objective \nis to optimize the spending so a surplus or deﬁ cit situation is avoided, and funds \nare deployed where they will be most productive. Likewise, understanding research \nspending rolled up by various dimensions is necessary to ensure proper institutional \ncontrol of such monies.\nBetter understanding the university’s alumni is much like better understanding \na customer base, as described in Chapter 8. Obviously, there are many interesting \ncharacteristics that would be helpful in maintaining a relationship with your alumni, \nsuch as geographic, demographic, employment, interests, and behavioral information, \nin addition to the data you collected about them as students (for example, affi  liations, \nresidential housing, school, major, length of time to graduate, and honors designa-\ntions). Improved access to a broad range of attributes about the alumni population \nwould allow the institution to better target messages and allocate resources. In addi-\ntion to alumni contributions, alumni relationships can be leveraged for potential \nrecruiting, job placement, and research opportunities. To this end, a robust CRM \noperational system should track all the touch points with alumni to capture mean-\ningful data for the DW/BI analytic  platform.\nSummary\nIn this chapter we focused on two primary concepts. First, we looked at the accu-\nmulating snapshot fact table to track application or research grant pipelines. Even \nthough the accumulating snapshot is used much less frequently than the more com-\nmon transaction and periodic snapshot fact tables, it is very useful for tracking the \ncurrent status of a short-lived process with standard milestones. As we described, \n",
      "content_length": 2710,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 373,
      "content": "Education 337\naccumulating snapshots are often complemented with transactional or periodic \nsnapshot tables.\nSecond, we explored several examples of factless fact tables. These fact tables \ncapture the relationship between dimensions in the case of an event or coverage, \nbut are unique in that no measurements are collected to serve as actual facts. We \nalso discussed the handling of situation s in which you want to track events that \ndidn’t occur.\n",
      "content_length": 452,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 375,
      "content": "Healthcare\nT\nhe healthcare industry is undergoing tremendous change as it seeks to both \nimprove patient outcomes, while simultaneously improving operational effi-\nciencies. The challenges are plentiful as organizations attempt to integrate their \nclinical and administrative information. Healthcare data presents several interesting \ndimensional design patterns that we’ll explore in this chapter.\nChapter 14 discusses the following concepts:\n \n■Example bus matrix snippet for a healthcare organization\n \n■Accumulating snapshot fact table to handle the claims billing and payment \npipeline\n \n■Dimension role playing for multiple dates and physicians\n \n■Multivalued dimensions, such as patient diagnoses \n \n■Supertype and subtype handling of healthcare charges\n \n■Treatment of textual comments\n \n■Measurement type dimension for sparse, heterogeneous measurements\n \n■Handling of images with dimensional schemas\n \n■Facility/equipment inventory utilization as transactions and periodic snapshots\n Healthcare Case Study and Bus Matrix\nIn  the face of unprecedented consumer focus and governmental policy regulations, \ncoupled with internal pressures, healthcare organizations need to leverage informa-\ntion more eff ectively to impact both patient outcomes and operational effi  ciencies. \nHealthcare organizations typically wrestle with many disparate systems to collect \ntheir clinical, ﬁ nancial, and operational performance metrics. This information \nneeds to be better integrated to deliver more eff ective patient care, while concur-\nrently managing costs and risks. Healthcare analysts want to better understand \nwhich procedures deliver the best outcomes, while identifying opportunities to \n14\n",
      "content_length": 1699,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 376,
      "content": "Chapter 14\n340\nimpact resource utilization, including labor, facilities, and associated equipment \nand supplies. Large healthcare consortiums with networks of physicians, clinics, \nhospitals, pharmacies, and laboratories are focused on these requirements, espe-\ncially as both the federal government and private payers are encouraging providers \nto assume more responsibility for the quality and cost of their healthcare services. \nFigure 14-1 illustrates a sample snippet of a healthcare organization’s bus matrix.\nPatient Encounter Workflow\nClinical Events\nBilling/Revenue Events\nProcedures\nPhysician Orders\nMedications\nLab Test Results\nDisease/Case Management Participation\nPatient Reported Outcomes\nPatient Satisfaction Surveys\nInpatient Facility Charges \nOutpatient Professional Charges\nClaims Billing\nClaims Payments\nCollections and Write-Offs\nOperational Events\nBed Inventory Utilization\nFacilities Utilization\nSupply Procurement\nSupply Utilization\nWorkforce Scheduling\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nDate\nPatient\nPhysician\nDiagnosis\nPayer\nEmployee\nFacility\nProcedure\nFigure 14-1: Subset of bus matrix row for a healthcare consortium.\nTraditionally, healthcare insurance payers have leveraged claims information to \nbetter understand their risk, improve underwriting policies, and detect potential \nfraudulent activity. Payers have historically been more sophisticated than health-\ncare provider organizations in leveraging data analytically, perhaps in part because \ntheir prime data source, claims, was more reliably captured and structured than \n",
      "content_length": 1733,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 377,
      "content": "Healthcare 341\nproviders’ data. However, claims data is both a beneﬁ t and curse for payers’ analytic \neff orts because it historically hasn’t provided the robust, granular clinical picture. \nIncreasingly, healthcare payers are partnering with providers to leverage detailed \npatient information to support more predictive analysis. In many ways, the needs \nand objectives of the providers and payers are converging, especially with the push \nfor shared-risk delivery models.\nEvery patient’s episode of care with a healthcare organization generates mounds \nof information. Patient-centric transactional data falls into two prime categories: \nadministrative and clinical. The claims billing data provides detail on a patient bill \nfrom a physician’s offi  ce, clinic, hospital, or laboratory. The clinical medical record, \non the other hand, is more comprehensive and includes not only the services result-\ning in charges, but also the laboratory test results, prescriptions, physician’s notes \nor orders, and sometimes outcomes.\nThe  issues of conforming common dimensions remain exactly the same for \nhealthcare as in other industries. Obviously, the most important conformed dimen-\nsion is the patient. In Chapter 8: Customer Relationship Management, we described \nthe need for a 360-degree view of customers. It’s easy to argue that a 360-degree \nview of patients is even more critical given the stakes; adoption of patient electronic \nmedical record (EMR) and electronic health record (EHR) systems clearly focus on \nthis objective.\nOther dimensions that must be conformed include:\n \n■Date\n \n■Responsible party \n \n■Employer\n \n■Health plan\n \n■Payer (primary and secondary)\n \n■Physician \n \n■Procedure\n \n■Equipment\n \n■Lab test\n \n■Medication\n \n■Diagnosis\n \n■Facility (offi  ce, clinic, outpatient facility, and hospital)\nIn  the healthcare arena, some of these dimensions are hard to conform, whereas \nothers are easier than they look at ﬁ rst glance. The patient dimension has historically \nbeen challenging, at least in the United States, because of the lack of a reliable national \nidentity number and/or consistent patient identiﬁ er across facilities and physicians. \nTo further complicate matters, the Health Insurance Portability and Accountability Act \n(HIPAA) includes strict privacy and security requirements to protect the conﬁ dential \n",
      "content_length": 2348,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 378,
      "content": "Chapter 14\n342\nnature of patient information. Operational process improvements, like electronic \nmedical records, are ensuring more consistent master patient identiﬁ cation.\nThe  diagnosis and treatment dimensions are considerably more structured and \npredictable than you might expect because the insurance industry and government \nhave mandated their content. For example, diagnosis and disease classiﬁ cations fol-\nlow the International Classiﬁ cation of Diseases (ICD) standard for consistent reporting. \nSimilarly, the Healthcare Common Procedure Coding System (HCPCS) is based on the \nAmerican Medical Association’s Current Procedural Terminology (CPT) to describe \nmedical, surgical, and diagnostic services, along with supplies and devices. Dentists \nuse the Current Dental Terminology (CDT) code set, which is updated and distributed \nby the American Dental Association.\nFinally, beyond integrated patient-centric clinical and ﬁ nancial information, \nhealthcare organizations also want to analyze operational information regarding \nthe utilization of their workforce, facilities, and supplies. Much of the discussion \nfrom earlier chapters about human resources, inventory management, and procure-\nment processes is also applicable to healthcare organizations.\n Claims Billing and Payments\nImagine  you work in the healthcare consortium’s billing organization. You receive \nthe primary charges from the physicians and facilities, prepare bills for the respon-\nsible payers, and track the progress of the claims payments received.\nThe dimensional model for the claims billing process must address a number of \nbusiness objectives. You want to analyze the billed dollar amounts by every avail-\nable dimension, including patient, physician, facility, diagnosis, procedure, and \ndate. You want to see how these claims have been paid and what percentage of the \nclaims have not been collected. You want to see how long it takes to get paid, and \nthe current status of all unpaid claims.\nAs we discussed in Chapter 4: Inventory, whenever a source business process is consid-\nered for inclusion in the DW/BI system, there are three essential grain choices. Remember \nthe fact table’s granularity determines what constitutes a fact table row. In other words, \nwhat is the measurement event being recorded?\nThe  transaction grain is the most fundamental. In the healthcare billing example, \nthe transaction grain would include every billing transaction from the physicians \nand facilities, as well as every claim payment transaction received. We’ll talk more \nabout these fact tables in a moment.\nThe  periodic snapshot is the grain of choice for long-running time series, such \nas bank accounts and insurance policies. However, the periodic snapshot doesn’t \n",
      "content_length": 2760,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 379,
      "content": "Healthcare 343\ndo a good job of capturing the behavior of relatively short-lived processes, such as \norders or medical claims billing. \nThe  accumulating snapshot grain is chosen to analyze the claims billing and pay-\nment workﬂ ow. A single fact table row represents a single line on a medical claim. \nFurthermore, the row represents the accumulated history of the line item from the \nmoment of creation to the current state. When anything about the line changes, the row \nis revisited and modiﬁ ed appropriately. From the point of view of the billing organiza-\ntion, let’s assume the standard scenario of a claim includes:\n \n■Treatment date\n \n■Primary insurance billing date\n \n■Secondary insurance billing date\n \n■Responsible party billing date\n \n■Last primary insurance payment date\n \n■Last secondary insurance payment date\n \n■Last responsible party payment date\n \n■Zero balance date\nThese dates describe the normal claim workﬂ ow. An accumulating snapshot \ndoes not attempt to fully describe unusual situations. Business users undoubt-\nedly need to see all the details of messy claim payment scenarios because multiple \npayments are sometimes received for a single line, or conversely, a single payment \nsometimes applies to multiple claims. Companion transaction schemas inevitably \nwill be needed. In the meantime, the purpose of the accumulating snapshot grain \nis to place every claim into a standard framework so that the analytic objectives \ndescribed earlier can be satisﬁ ed easily.\nWith a clear understanding that an individual fact table row represents the accu-\nmulated history of a line item on a claim bill, you can identify the dimensions by \ncarefully listing everything known to be true in the context of this row. In this \nhypothetical scenario, you know the patient, responsible party, physician, physi-\ncian organization, procedure, facility, diagnosis, primary insurance organization, \nsecondary insurance organization, and master patient bill ID number, as shown in \nFigure 14-2.\nThe interesting facts accumulated over the claim line’s history include the billed \namount, primary insurance paid amount, secondary insurance paid amount, respon-\nsible party paid amount, total paid amount (calculated), amount sent to collections, \namount written off , amount remaining to be paid (calculated), length of stay, number \nof days from billing to initial primary insurance, secondary insurance, and respon-\nsible party payments, and ﬁ nally, number of days to zero balance.\n",
      "content_length": 2493,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 380,
      "content": "Chapter 14\n344\nClaims Billing and Payment Workflow Fact\nTreatment Date Key (FK)\nPrimary Insurance Billing Date Key (FK)\nSecondary Insurance Billing Date Key (FK)\nResponsible Party Billing Date Key (FK)\nLast Primary Insurance Payment Date Key (FK)\nLast Secondary Insurance Payment Date Key (FK)\nLast Responsible Party Payment Date Key (FK)\nZero Balance Date Key (FK)\nPatient Key (FK)\nPhysician Key (FK)\nPhysician Organization Key (FK)\nProcedure Key (FK)\nFacility Key (FK)\nPrimary Diagnosis Key (FK)\nPrimary Insurance Organization Key (FK)\nSecondary Insurance Organization Key (FK)\nResponsible Party Key (FK)\nEmployer Key (FK)\nMaster Bill ID (DD)\nBilled Amount\nPrimary Insurance Paid Amount\nSecondary Insurance Paid Amount\nResponsible Party Paid Amount\nTotal Paid Amount\nSent to Collections Amount\nWritten Off Amount\nUnpaid Balance Amount\nLength of Stay\nBill to Initial Primary Insurance Payment Lag\nBill to Initial Secondary Insurance Payment Lag\nBill to Initial Responsible Party Payment Lag\nBill to Zero Balance Lag\nPatient Dimension\nProcedure Dimension\nPrimary Diagnosis Dimension\nResponsible Party Dimension\nPhysician Dimension\nPhysician Organization Dimension\nFacility Dimension\nInsurance Organization Dimension (views for 2 roles)\nEmployer Dimension\nDate Dimension (views for 8 roles)\nFigure 14-2: Accumulating snapshot fact table for medical claim billing and payment \nworkﬂ ow.\nA row is initially created in this fact table when the charge transactions are \nreceived from the physicians or facilities and the initial bills are generated. On a \ngiven bill, perhaps the primary insurance company is billed, but the secondary \ninsurance and responsible party are not billed, pending a response from the pri-\nmary insurance company. For a period of time after the row is ﬁ rst entered into \nthe fact table, the last seven dates are not applicable. Because the surrogate date \nkeys in the fact table must not be null, they will point to a date dimension row \nreserved for a To Be Determined date.\nIn the weeks after creation of the row, some payments are received. Bills are then \nsent to the secondary insurance company and responsible party. Each time these \nevents take place, the same fact table row is revisited, and the appropriate keys and \nfacts are destructively updated. This destructive updating poses some challenges \nfor the database administrator. If most of the accumulating rows stabilize and stop \nchanging within a given timeframe, a physical reorganization of the database at \nthat time can recover disk storage and improve performance. If the fact table is \n",
      "content_length": 2580,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 381,
      "content": "Healthcare 345\npartitioned on the treatment date key, the physical clustering or partitioning prob-\nably will be well preserved throughout these changes because the treatment date \nis not revisited and changed.\n Date Dimension Role Playing\nAccumulating snapshot fact tables always involve multiple date stamps, like the eight \nforeign keys pointing to the date dimension in Figure 14-2. The eight date foreign \nkeys should not join to a single instance of the date dimension table. Instead, create \neight views on the single underlying date dimension table, and join the fact table \nseparately to these eight views, as if they were eight independent date dimension \ntables. The eight view deﬁ nitions should cosmetically relabel the column names to \nbe distinguishable, so BI tools accessing the views present understandable column \nnames to the business users.\nAlthough the role-playing behavior of the date dimension is a common charac-\nteristic of accumulating snapshot fact tables, other dimensions in Figure 14-2 play \nroles in similar ways, such as the payer dimension. In the section “Supertypes and \nSubtypes for Charges,” the physician dimension will play multiple roles depending \non whether the physician is the referring physician, attending physician, or working \nin a consulting or assisting  capacity.\n Multivalued Diagnoses\nNormally  the dimensions surrounding a fact table take on a single value in the \ncontext of the fact event. However, there are situations where multivaluedness is \nnatural and unavoidable. The diagnosis dimension in healthcare fact tables is a \ngood example. At the moment of a procedure or lab test, the patient has one or more \ndiagnoses. Electronic medical record applications facilitate the physician’s selection \nof multiple diagnoses well beyond the historical practice of providing the minimal \ncoding needed for reimbursement; the result is a richer, more complete picture of \nthe severity of the patient’s medical condition. There is strong analytic incentive to \nretain the multivalued diagnoses, along with the other ﬁ nancial performance data, \nespecially as organizations do more comparative utilization and cost benchmarking.\nIf there were always a maximum of three diagnoses, for instance, you might be \ntempted to create three diagnosis foreign keys in the fact table with correspond-\ning dimensions, almost as if they were roles. However, diagnoses don’t behave like \nindependent roles. And unfortunately, there are often more than three diagnoses, \nespecially for hospitalized elderly patients who may present 20 simultaneous diag-\nnoses! Diagnoses don’t ﬁ t into well-deﬁ ned roles other than potentially the primary \nadmitting and discharging diagnoses. Finally, a design with multiple diagnosis \n",
      "content_length": 2757,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 382,
      "content": "Chapter 14\n346\nforeign keys would make for very ineffi  cient BI applications because the query \ndoesn’t know which dimensional slot to constrain for a particular diagnosis.\nThe design shown in Figure 14-3 handles the open-ended nature of multiple diag-\nnoses. The diagnosis foreign key in the fact table is replaced with a diagnosis group \nkey. This diagnosis group key is connected by a many-to-many join to a diagnosis \ngroup bridge table, which contains a separate row for each individual diagnosis in \na particular group.\nDiagnosis Dimension\nDiagnosis Key (PK)\nDiagnosis Code (NK)\nDiagnosis Description\nDiagnosis Section Code\nDiagnosis Section Description\nDiagnosis Category Code\nDiagnosis Category Description\nMore FKs ...\nDiagnosis Group Key (FK)\nMaster Bill ID (DD)\nFacts ...\nDiagnosis Group Key (FK)\nDiagnosis Key (FK)\nClaim Billing Line Item Fact\nDiagnosis Group Bridge\nFigure 14-3: Bridge table to handle multivalued diagnoses.\nIf a patient has three diagnoses, he is assigned a diagnosis group with three cor-\nresponding rows in the bridge table. In Chapter 10: Financial Services, we described \nthe use of a weighting factor on each bridge table row to allocate the fact table’s \nmetrics accordingly. However, in the case of multiple patient diagnoses, it’s virtu-\nally impossible to weight their impact on a patient’s treatment or bill, beyond the \npotential determination of a primary diagnosis. Without a realistic way of assigning \nweighting factors, the analysis of diagnosis codes must largely focus on impact ques-\ntions like “What is the total billed amount for procedures involving the diagnosis of \ncongestive heart failure?” Most healthcare analysts understand impact analysis may \nresult in over counting as the same metrics are associated with multiple diagnoses.\nNOTE \nWeighting factors in multivalued bridge tables provide an elegant way \nto prorate numeric facts to produce correctly weighted reports. However, these \nweighting factors are by no means required in a dimensional design. If there is no \nagreement or enthusiasm within the business community for the weighting factors, \nthey should be left out. Also, in a schema with more than one multivalued dimen-\nsion, it is not worth trying to decide how multiple weighting factors would interact.\nIf the many-to-many join in Figure 14-3 causes problems for a modeling tool that \ninsists on proper foreign-key-to-primary-key relationships, the equivalent design \n",
      "content_length": 2445,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 383,
      "content": "Healthcare 347\nof Figure 14-4 can be used. In this case an extra table whose primary key is a diag-\nnosis group is inserted between the fact and bridge tables. There is likely no new \ninformation in this extra table, unless there were labels for a cluster of diagnoses, \nsuch as the Kimball Syndrome, but now both the fact table and bridge table have \nconventional many-to-one joins in all directions.\nDiagnosis Dimension\nDiagnosis Key (PK)\nDiagnosis Code (NK)\nDiagnosis Description\nDiagnosis Section Code\nDiagnosis Section Description\nDiagnosis Category Code\nDiagnosis Category Description\nForeign Keys ...\nDiagnosis Group Key (FK)\nMaster Bill ID (DD)\nFacts ...\nDiagnosis Group Key (FK)\nDiagnosis Key (FK)\nClaim Billing Line Item Fact\nDiagnosis Group Bridge\nDiagnosis Group Key (PK)\nDiagnosis Group Dimension\nFigure 14-4: Diagnosis group dimension to create a primary key relationship.\nIf a unique diagnosis group is created for every patient encounter, the number \nof rows could become astronomical and many of the groups would be identical. \nProbably a better approach is to have a portfolio of diagnosis groups that are repeat-\nedly used. Each set of diagnoses would be looked up in the master diagnosis group \ntable during the ETL. If the existing group is found, it is used; if not found, a new \ndiagnosis group is created. Chapter 19: ETL Subsystems and Techniques provides \nguidance for creating and administering bridge tables.\nIn an inpatient hospital stay scenario, the diagnosis group may be unique to each \npatient if it evolves over time during the patient’s stay. In this case you would supple-\nment the bridge table with two date stamps to capture begin and end dates. Although \nthe twin date stamps complicate updates to the diagnosis group bridge table, they \nare useful for change tracking, as described more fully in Chapter 7: Accounting.\n Supertypes and Subtypes for Charges\nWe’ve  described a design for billed healthcare treatments to cover both inpatient and \noutpatient claims. In reality, healthcare charges resemble the supertype and subtype \npattern described in Chapter 10. Facility charges for inpatient hospital stays diff er \nfrom professional charges for outpatient treatments in clinics and doctor offi  ces. \nIf you were focused exclusively on hospital stays, it would be reasonable to \ntweak the Figure 14-2 dimensional structure to incorporate more hospital-speciﬁ c \ninformation. Figure 14-5 shows a revised set of dimensions specialized for hospital \nstays, with the new dimensions bolded.\n",
      "content_length": 2530,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 384,
      "content": "Chapter 14\n348\nTreatment Date Key (FK)\nPrimary Insurance Billing Date Key (FK)\nSecondary Insurance Billing Date Key (FK)\nResponsible Party Billing Date Key (FK)\nLast Primary Insurance Payment Date Key (FK)\nLast Secondary Insurance Payment Date Key (FK)\nLast Responsible Party Payment Date Key (FK)\nZero Balance Date Key (FK)\nPatient Key (FK)\nAdmitting Physician Key (FK)\nAdmitting Physician Organization Key (FK)\nAttending Physician Key (FK)\nAttending Physician Organization Key (FK)\nProcedure Key (FK)\nFacility Key (FK)\nAdmitting Diagnosis Group Key (FK)\nDischarge Diagnosis Group Key (FK)\nPrimary Insurance Organization Key (FK)\nSecondary Insurance Organization Key (FK)\nResponsible Party Key (FK)\nEmployer Key (FK)\nMaster Bill ID (DD)\nFacts...\nInpatient Hospital Claim Billing and Payment Workflow Fact\nFigure 14-5: Accumulating snapshot for hospital stay charges.\nReferring to Figure 14-5, you can see two roles for the physician: admitting physi-\ncian and attending physician. The ﬁ gure shows physician organizations for both roles \nbecause physicians may represent diff erent organizations in a hospital setting. With \nmore complex surgical events, such as a heart transplant operation, whole teams of \nspecialists and assistants are assembled. In this case, you could include a key in the \nfact table for the primary responsible physician; the other physicians and medical \nstaff  would be linked to the fact row via a group key to a multivalued bridge table.\nYou also have two multivalued diagnosis dimensions on each fact table row. The \nadmitting diagnosis group is determined at the beginning of the hospital stay and \nshould be the same for every treatment row that is part of the same hospital stay. \nThe discharge diagnosis group is not known until the patient is discharged.\nElectronic Medical Records\nMany  healthcare organizations are moving from paper-based processes to elec-\ntronic medical records. In the United States, federally mandated quality goals to \nsupport improved population health management may be achievable only with \n",
      "content_length": 2054,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 385,
      "content": "Healthcare 349\ntheir adoption. Healthcare providers are aggressively implementing electronic \nhealth record systems; the movement is signiﬁ cantly impacting healthcare DW\n/BI initiatives.\nElectronic medical records can present challenges for data warehouse environ-\nments because of their extreme variability and potentially extreme volumes. Patients’ \nmedical record data comes in many diff erent forms, ranging from numeric data to \nfreeform text comments entered by a healthcare professional to images and photo-\ngraphs. We’ll further discuss unstructured data in Chapter 21: Big Data Analytics; \nelectronic medical and/or health records may become a classic use case for big data. \nOne thing is certain. The amount and variability of electronic data in the healthcare \nindustry will continue to grow.\n Measure Type Dimension for Sparse Facts\nAs  designers, it is tempting to strive for a more standardized framework that could \nbe extended to handle data variability. For example, you could potentially handle the \nvariability of lab test results with a measurement type dimension describing what \nthe fact row means, or in other words, what the generic fact represents. The unit \nof measure for a given numeric entry is found in the associated measurement type \ndimension row, along with any additivity restrictions, as shown in Figure 14-6. \nLab Test Measurement Type Dimension\nLab Test Measurement Type Key (PK)\nLab Test Measurement Type Description\nLab Test Measurement Type Unit of Measure\nOrder Date Key (FK)\nTest Date Key (FK)\nPatient Key (FK)\nPhysican Key (FK)\nLab Test Key (FK)\nLab Test Measurement Type Key (FK)\nObserved Test Result Value\nLab Test Result Facts\nFigure 14-6: Lab test observations with measurement type dimension.\nThis approach is superbly ﬂ exible; you can add new measurement types simply by \nadding new rows in the measurement type dimension, not by altering the structure \nof the fact table. This approach also eliminates the nulls in the classic positional fact \ntable design because a row exists only if the measurement exists. However, there \nare trade-off s. Using a measurement type dimension may generate lots of new fact \ntable rows because the grain is “one row per measurement per event” rather than the \nmore typical “one row per event.” If a lab test results in 10 numeric measurements, \nthere are now 10 rows in the fact table rather than a single row in the classic design. \nFor extremely sparse situations, such as clinical laboratory or manufacturing test \nenvironments, this is a reasonable compromise. However, as the density of the facts \n",
      "content_length": 2590,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 386,
      "content": "Chapter 14\n350\ngrows, you end up spewing out too many fact rows. At this point you no longer have \nsparse facts and should return to the classic fact table design with ﬁ xed columns.\nMoreover, this measurement type approach may complicate BI data access appli-\ncations. In the relational star schema, combining two numbers that were captured \nas part of a single event is more diffi  cult with this approach because now you must \nfetch two rows from the fact table. SQL likes to perform arithmetic functions within \na row, not across rows. In addition, you must be careful not to mix incompatible \namounts in a calculation because all the numeric measures reside in a single amount \ncolumn. It’s worth noting that multidimensional OLAP cubes are more tolerant of \nperforming calculations across measurement types.\n Freeform Text Comments\nFreeform text comments, such as clinical notes, are sometimes associated with fact \ntable events. Although text comments are not very analytically potent unless they’re \nparsed into well-behaved dimension attributes, business users are often unwilling \nto part with them given the embedded nuggets of information.\nTextual comments should not be stored in a fact table directly because they waste \nspace and rarely participate in queries. Some designers think it’s permissible to store \ntextual ﬁ elds in the fact table, as long as they’re referred to as degenerate dimensions. \nDegenerate dimensions are most typically used for operational transaction control \nnumbers and identiﬁ ers; it’s not an acceptable approach or pattern for contending \nwith bulky text ﬁ elds. Storing freeform comments in the fact table adds clutter that \nmay negatively impact the performance of analysts’ more typical quantitative queries.\nThe unbounded text comments should either be stored in a separate comments \ndimension or treated as attributes in a transaction event dimension. A key consider-\nation when evaluating these two approaches is the text ﬁ eld’s cardinality. If there’s \nnearly a unique comment for every fact table event, storing the textual ﬁ eld in a trans-\naction dimension makes the most sense. However, in many cases, No Comment is \nassociated with numerous fact rows. Because the number of unique text comments in \nthis scenario is much smaller than the number of unique transactions, it would make \nmore sense to store the textual data in a comments dimension with an associated \nforeign key in the fact table. In either case, queries involving both the text comments \nand fact metrics will perform relatively poorly given the need to resolve joins between \ntwo voluminous tables. Often business users want to drill into text comments for \nfurther investigation after highly selective fact table query ﬁ lters have been  applied.\nImages\nSometimes  the data captured in a patient’s electronic medical record is an image, \nin addition to either quantitative numbers or qualitative notes. There are trade-off s \n",
      "content_length": 2951,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 387,
      "content": "Healthcare 351\nbetween capturing a JPEG ﬁ lename in the fact table to refer to an associated image \nversus embedding the image as a blob directly in the database. The advantage of \nusing a JPEG ﬁ lename is that other image creation, viewing, and editing programs \ncan freely access the image. The disadvantage is that a separate database of graphic \nﬁ les must be maintained in synchrony with the fact table.\nFacility/Equipment Inventory Utilization\nIn  addition to ﬁ nancial and clinical data, healthcare organizations are also keenly \ninterested in more operationally oriented metrics, such as utilization and availability \nof their assets, whether referring to patient beds or surgical operating theatres. In \nChapter 4, we discussed product inventory data as transaction events as well as \nperiodic snapshots. Facility or equipment inventories in a healthcare organization \ncan be handled similarly.\nFor example, you can envision a bed utilization periodic snapshot with every bed’s \nstatus at regularly recurring points in time, perhaps at midnight, the start of every \nshift, or even more frequently throughout the day. In addition to a snapshot date and \npotentially time-of-day, this factless fact table would include foreign keys to identify \nthe patient, attending physician, and perhaps an assigned nurse on duty.\nConversely, you can imagine treating the bed inventory data as a transaction \nfact table with one row per movement into and out of a hospital bed. This may be a \nsimplistic transaction fact table with transaction date and time dimension foreign \nkeys, along with dimensions to describe the type of movement, such as ﬁ lled or \nvacated. In the case of operating room utilization and availability, you can envision \na lengthier list of statuses, such as pre-operation, post-operation, or downtime, \nalong with time durations.\nIf the inventory changes are not terribly volatile, such as the beds in a rehabilita-\ntion or eldercare inpatient environment, you should consider a timespan fact table, \nas discussed in Chapter 8, with row eff ective and expiration dates and times to \nrepresent the various states of a bed over a period of  time.\n Dealing with Retroactive Changes\nAs  DW/BI practitioners, we have well-developed techniques for accurately capturing \nthe historical ﬂ ow of data from our enterprise’s source applications. Numeric mea-\nsurements go into fact tables, which are surrounded with contemporary descriptions \nof what you know is true at the time of the measurements, packaged as dimension \ntables. The descriptions of patient, physician, facility, and payer evolve as slowly \nchanging dimensions whenever these entities change their descriptions. \n",
      "content_length": 2690,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 388,
      "content": "Chapter 14\n352\nHowever, in the healthcare industry, especially with legacy operational systems, \nyou often need to contend with late arriving data that should have been loaded into \nthe data warehouse weeks or months ago. For example, you might receive data \nregarding patient procedures that occurred several weeks ago, or updates to patient \nproﬁ les that were back-dated as eff ective several months ago. The more delayed the \nincoming records are, the more challenging the DW/BI system’s ETL processing \nbecomes. We’ll discuss these late arriving fact and dimension scenarios in Chapter \n19. Unfortunately, these patterns are common in healthcare DW/BI environments; \nin fact, they may be the dominant modes of processing rather than specialized \ntechniques for outlier cases. Eventually, more eff ective source data capture systems \nshould reduce the frequency of these late arriving data anomalies.\nSummary\nHealthcare provides a wealth of dimensional design examples. In this chapter, the \nenterprise data warehouse bus matrix illustrated the critical linkages between a \nhealthcare organization’s administrative and clinical data. We used an accumulating \nsnapshot grain fact table with role-playing date dimensions for the healthcare claim \nbilling and payment pipeline. We also saw role playing used for the physician and \npayer dimensions in other fact tables of this chapter.\nHealthcare schemas are littered with multivalued dimensions, especially the \ndiagnosis dimension. Complex surgical events might also use multivalued bridge \ntables to represent the teams of involved physicians and other staff  members. The \nbridge tables used with healthcare data seldom contain weighting factors, as dis-\ncussed in earlier chapters, because it is extremely diffi  cult to establish weighting \nbusiness rules, beyond the designation of a “primary” relationship.\nWe discussed medical records and test results, suggesting a measurement type \ndimension to organize sparse, heterogeneous measurements into a single, uniform \nframework. We also discussed the handling of text comments and linked images. \nTransaction and periodic snapshot fact tables were used to represent facility or \nequipment inventory utilization and availability. In closing, we touched upon ret-\nroactive fact and dimension changes that are often all too common with healthcare \nperformance data.\n",
      "content_length": 2370,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 389,
      "content": "Electronic \nCommerce\nA \n web-intensive business’s clickstream data records the gestures of every web \nvisitor. In its most elemental form, the clickstream is every page event recorded \nby each of the company’s web servers. The clickstream contains a number of new \ndimensions, such as page, session, and referrer, which are not found in other data \nsources. The clickstream is a torrent of data; it can be difficult and exasperating for \nDW/BI professionals. Does it connect to the rest of the DW/BI system? Can its dimen-\nsions and facts be conformed in the enterprise data warehouse bus architecture?\nWe start this chapter by describing the raw clickstream data source and designing \nits relevant dimensional models. We discuss the impact of Google Analytics, which \ncan be thought of as an external data warehouse delivering information about your \nwebsite. We then integrate clickstream data into a larger matrix of more conven-\ntional processes for a web retailer, and argue that the proﬁ tability of the web sales \nchannel can be measured if you allocate the right costs back to the individual sales.\nChapter 15 discusses the following concepts:\n \n■Clickstream data and its unique dimensionality\n \n■Role of external services such as Google Analytics\n \n■Integrating clickstream data with the other business processes on the bus \nmatrix\n \n■Assembling a complete view of proﬁ tability for a web enterprise\nClickstream Source Data\nThe clickstream is not just another data source that is extracted, cleaned, and \ndumped into the DW/BI environment. The clickstream is an evolving collection of \ndata sources. There are a number of server log ﬁ le formats for capturing clickstream \ndata. These log ﬁ le formats have optional data components that, if used, can be very \nhelpful in identifying visitors, sessions, and the true meaning of behavior.\n15\n",
      "content_length": 1849,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 390,
      "content": "Chapter 15\n354\nBecause of the distributed nature of the web, clickstream data often is collected \nsimultaneously by diff erent physical servers, even when the visitor thinks they are \ninteracting with a single website. Even if the log ﬁ les collected by these separate \nservers are compatible, a very interesting problem arises in synchronizing the log \nﬁ les after the fact. Remember that a busy web server may be processing hundreds \nof page events per second. It is unlikely the clocks on separate servers will be in \nsynchrony to one-hundredth of a second.\nYou also obtain clickstream data from diff erent parties. Besides your own log \nﬁ les, you may get clickstream data from referring partners or from internet service \nproviders (ISPs). Another important form of clickstream data is the search speciﬁ ca-\ntion given to a search engine that then directs the visitor to the website.\nFinally, if you are an ISP providing web access to directly connected customers, \nyou have a unique perspective because you see every click of your captive custom-\ners that may allow more powerful and invasive analyses of the customer’s sessions.\nThe most basic form of clickstream data from a normal website is stateless. That \nis, the log shows an isolated page retrieval event but does not provide a clear tie to \nother page events elsewhere in the log. Without some kind of contextual help, it is \ndiffi  cult or impossible to reliably identify a complete visitor session.\nThe other big frustration with basic clickstream data is the anonymity of the \nsession. Unless visitors agree to reveal their identity in some way, you often cannot \nbe sure who they are, or if you have ever seen them before. In certain situations, \nyou may not distinguish the clicks of two visitors who are simultaneously brows-\ning the website.\nClickstream Data Challenges\nClickstream data contains many ambiguities. Identifying visitor origins, visitor \nsessions, and visitor identities is something of an interpretive art. Browser caches \nand proxy servers make these identiﬁ cations more challenging.\nIdentifying the Visitor Origin\nIf  you are very lucky, your site is the default home page for the visitor’s browser. \nEvery time he opens his browser, your home page is the ﬁ rst thing he sees. This is \npretty unlikely unless you are the webmaster for a portal site or an intranet home \npage, but many sites have buttons which, when clicked, prompt visitors to set their \nURL as the browser’s home page. Unfortunately there is no easy way to determine \nfrom a log whether your site is set as a browser’s home page.\nA visitor may be directed to your site from a search at a portal such as Yahoo! or \nGoogle. Such referrals can come either from the portal’s index, for which you may \nhave paid a placement fee, or from a word or content search.\n",
      "content_length": 2816,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 391,
      "content": "Electronic Commerce 355\nFor some websites, the most common source of visitors is from a browser book-\nmark. For this to happen, the visitor must have previously bookmarked your site, \nand this can occur only after the site’s interest and trust levels cross the visitor’s \nbookmark threshold.\nFinally, your site may be reached as a result of a clickthrough—a deliberate click \non a text or graphical link from another site. This may be a paid-for referral via a \nbanner ad, or a free referral from an individual or cooperating site. In the case of \nclickthroughs, the referring site will almost always be identiﬁ able as a ﬁ eld in the \nweb event record. Capturing this crucial clickstream data is important to verify the \neffi  cacy of marketing programs. It also provides crucial data for auditing invoices \nyou may receive from clickthrough advertising charges.\nIdentifying the Session\nMost  web-centric analyses require every visitor session (visit) to have its own unique \nidentity tag, similar to a supermarket receipt number. This is the session ID. Records \nfor every individual visitor action in a session, whether they are derived from the \nclickstream or an application interaction, must contain this tag. But keep in mind \nthe operational application, such as an order entry system generates this session \nID, not the web server.\nThe  basic protocol for the web, Hyper Text Transfer Protocol (HTTP) is stateless; \nthat is, it lacks the concept of a session. There are no intrinsic login or logout actions \nbuilt into the HTTP protocol, so session identity must be established in some other \nway. There are several ways to do this:\n \n1. In many cases, the individual hits comprising a session can be consolidated by \ncollating time-contiguous log entries from the same host (IP address). If the \nlog contains a number of entries with the same host ID in a short period of \ntime (for example, one hour), you can reasonably assume the entries are for \nthe same session. This method breaks down for websites with large numbers \nof visitors because dynamically assigned IP addresses may be reused immedi-\nately by diff erent visitors over a brief time period. Also, diff erent IP addresses \nmay be used within the same session for the same visitor. This approach also \npresents problems when dealing with browsers that are behind some ﬁ rewalls. \nNotwithstanding these problems, many commercial log analysis products use \nthis method of session tracking, and it requires no cookies or special web \nserver features.\n 2. Another much more satisfactory method is to let the web browser place a \nsession-level cookie into the visitor’s web browser. This cookie will last as \nlong as the browser is open and in general won’t be available in subsequent \n",
      "content_length": 2754,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 392,
      "content": "Chapter 15\n356\nbrowser sessions. The cookie value can serve as a temporary session ID not \nonly to the browser, but also to any application that requests the session \ncookie from the browser. But using a transient cookie has the disadvantage \nthat you can’t tell when the visitor returns to the site at a later time in a new \nsession.\n \n3. HTTP’s secure sockets layer (SSL) off ers an opportunity to track a visitor \nsession because it may include a login action by the visitor and the exchange \nof encryption keys. The downside to using this method is that to track the \nsession, the entire information exchange needs to be in high-overhead SSL, \nand the visitor may be put off  by security advisories that can pop up using \ncertain browsers. Also, each host must have its own unique security certiﬁ cate.\n \n4. If page generation is dynamic, you can try to maintain visitor state by plac-\ning a session ID in a hidden ﬁ eld of each page returned to the visitor. This \nsession ID can be returned to the web server as a query string appended to \na subsequent URL. This method of session tracking requires a great deal of \ncontrol over the website’s page generation methods to ensure the thread of \na session ID is not broken. If the visitor clicks links that don’t support this \nsession ID ping-pong, a single session may appear to be multiple sessions. \nThis approach also breaks down if multiple vendors supply content in a single \nsession unless those vendors are closely collaborating.\n \n5. Finally, the website may establish a persistent cookie in the visitor’s machine \nthat is not deleted by the browser when the session ends. Of course, it’s pos-\nsible the visitor will have his browser set to refuse cookies, or may manually \nclean out his cookie ﬁ le, so there is no absolute guarantee that even a per-\nsistent cookie will survive. Although any given cookie can be read only by \nthe website that caused it to be created, certain groups of websites can agree \nto store a common ID tag that would let these sites combine their separate \nnotions of a visitor session into a “super session.”\nIn summary, the most reliable method of session tracking from web server log \nrecords is obtained by setting a persistent cookie in the visitor’s browser. Less reli-\nable, but good results can be obtained by setting a session level and a nonpersistent \ncookie and by associating time-contiguous log entries from the same host. The latter \nmethod requires a robust algorithm in the log postprocessor to ensure satisfactory \nresults and to decide when not to take the results seriously.\nIdentifying the Visitor\nIdentifying  a speciﬁ c visitor who logs into your site presents some of the most \nchallenging problems facing a site designer, webmaster, or manager of the web \nanalytics group.\n",
      "content_length": 2785,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 393,
      "content": "Electronic Commerce 357\n \n■Web visitors want to be anonymous. They may have no reason to trust you, \nthe internet, or their computer with personal identiﬁ cation or credit card \ninformation.\n \n■If you request visitors’ identity, they may not provide accurate information.\n \n■You can’t be sure which family member is visiting your site. If you obtain \nan identity by association, for instance from a persistent cookie left during a \nprevious visit, the identiﬁ cation is only for the computer, not for the speciﬁ c \nvisitor. Any family member or company employee may have been using that \nparticular computer at that moment in time.\n \n■You can’t assume an individual is always at the same computer. Server-\nprovided cookies identify a computer, not an individual. If someone accesses \nthe same website from an offi  ce computer, home computer, and mobile device, \na diff erent website cookie is probably put into each machine.\nClickstream Dimensional Models\nBefore  designing clickstream dimensional models, let’s consider all the dimensions \nthat may have relevance in a clickstream environment. Any single dimensional \nmodel will not use all the dimensions at once, but it is nice to have a portfolio \nof dimensions waiting to be used. The list of dimensions for a web retailer could \ninclude:\n \n■Date\n \n■Time of day\n \n■Part\n \n■Vendor\n \n■Status\n \n■Carrier\n \n■Facilities location\n \n■Product\n \n■Customer\n \n■Media\n \n■Promotion\n \n■Internal organization\n \n■Employee\n \n■Page\n \n■Event\n \n■Session\n \n■Referral\n",
      "content_length": 1502,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 394,
      "content": "Chapter 15\n358\nAll the dimensions in the list, except for the last four shown in bold, are familiar \ndimensions, most of which we have already used in earlier chapters of this book. \nBut the last four are the unique dimensions of the clickstream and warrant some \ncareful attention.\nPage Dimension\nThe page dimension  describes the page context for a web page event, as illustrated \nin Figure 15-1. The grain of this dimension is the individual page. The deﬁ nition \nof page must be ﬂ exible enough to handle the evolution of web pages from static \npage delivery to highly dynamic page delivery in which the exact page the customer \nsees is unique at that instant in time. We assume even in the case of the dynamic \npage that there is a well-deﬁ ned function that characterizes the page, and we will \nuse that to describe the page. We will not create a page row for every instance of a \ndynamic page because that would yield a dimension with an astronomical number \nof rows. These rows also would not diff er in interesting ways. You want a row in this \ndimension for each interesting distinguishable type of page. Static pages probably get \ntheir own row, but dynamic pages would be grouped by similar function and type.\nPage Dimension Attribute\nPage Key\nPage Source\nPage Function\nPage Template\nItem Type\nGraphics Type\nAnimation Type\nSound Type\nPage File Name\nSurrogate values (1..N)\nStatic, Dynamic, Unknown, Corrupted, Inapplicable, ...\nPortal, Search, Product description, Corporate information, ...\nSparse, Dense, ...\nProduct SKU, Book ISBN number, Telco rate type, ...\nGIF, JPG, Progressive disclosure, Size pre-declared, ...\nSimilar to graphics type\nSimilar to graphics type\nOptional application dependent name\nSample Data Values/Definitions\nFigure 15-1: Page dimension attributes and sample data values.\nWhen the deﬁ nition of a static page changes because it is altered by the web-\nmaster, the page dimension row can either be type 1 overwritten or treated with \nan alternative slowly changing technique. This decision is a matter of policy for \nthe data warehouse and depends on whether the old and new descriptions of the \npage diff er materially, and whether the old deﬁ nition should be kept for historical \nanalysis purposes.\nWebsite designers, data governance representatives from the business, and the \nDW/BI architects need to collaborate to assign descriptive codes and attributes to \neach page served by the web server, whether the page is dynamic or static. Ideally, \nthe web page developers supply descriptive codes and attributes with each page \n",
      "content_length": 2568,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 395,
      "content": "Electronic Commerce 359\nthey create and embed these codes and attributes into the optional ﬁ elds of the \nweb log ﬁ les. This crucial step is at the foundation of the implementation of this \npage dimension.\nBefore leaving the page dimension, we want to point out that some internet com-\npanies track the more granular individual elements on each page of their web sites, \nincluding graphical elements and links. Each element generates its own row for each \nvisitor for each page request. A single complex web page can generate hundreds of \nrows each time the page is served to a visitor. Obviously, this extreme granularity \ngenerates astronomical amounts of data, often exceeding 10 terabytes per day!\nSimilarly, gaming companies may generate a row for every gesture made by every \nonline game player, which again can result in hundreds of millions of rows per day. \nIn both cases, the most atomic fact table will have extra dimensions describing the \ngraphical element, link, or game situation.\nEvent Dimension\nThe  event dimension describes what happened on a particular page at a particular \npoint in time. The main interesting events are Open Page, Refresh Page, Click Link, \nand Enter Data. You want to capture that information in this small event dimension, \nas illustrated in Figure 15-2.\nEvent Dimension Attribute\nEvent Key\nEvent Type\nEvent Content\nSurrogate values (1..N)\nOpen page, Refresh page, Click link, Unknown, Inapplicable\nApplication-dependent fields eventually driven by XML tags\nSample Data Values/Definitions\nFigure 15-2: Event dimension attributes and sample data values.\nSession Dimension\nThe  session dimension provides one or more levels of diagnosis for the visitor’s \nsession as a whole, as shown in Figure 15-3. For example, the local context of the \nsession might be Requesting Product Information, but the overall session context \nmight be Ordering a Product. The success status would diagnose whether the mis-\nsion was completed. The local context may be decidable from just the identity of \nthe current page, but the overall session context probably can be judged only by \nprocessing the visitor’s complete session at data extract time. The customer status \nattribute is a convenient place to label the customer for periods of time, with labels \nthat are not clear either from the page or immediate session. These statuses may be \nderived from auxiliary business processes in the DW/BI system, but by placing these \nlabels deep within the clickstream, you can directly study the behavior of certain \ntypes of customers. Do not put these labels in the customer dimension because they \n",
      "content_length": 2617,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 396,
      "content": "Chapter 15\n360\nmay change over very short periods of time. If there are a large number of these \nstatuses, consider creating a separate customer status mini-dimension rather than \nembedding this information in the session dimension.\nSession Dimension Attribute\nSession Key\nSession Type\nLocal Context\nSession Context\nAction Sequence\nSuccess Status\nCustomer Status\nSurrogate values (1..N)\nClassified, Unclassified, Corrupted, Inapplicable\nPage-derived context like Requesting Product Information\nTrajectory-derived context like Ordering a Product\nSummary label for overall sequence of actions during session\nIdentifies whether overall session mission was accomplished\nNew customer, High value customer, About to cancel, In default\nSample Data Values/Definitions\nFigure 15-3: Session dimension attributes and sample data values.\nThis dimension groups sessions for analysis, such as:\n \n■How many customers consulted your product information before ordering?\n \n■How many customers looked at your product information and never ordered?\n \n■How many customers did not ﬁ nish ordering? Where did they stop?\nReferral Dimension\nThe  referral dimension, illustrated in Figure 15-4, describes how the customer \narrived at the current page. The web server logs usually provide this information. \nThe URL of the previous page is identiﬁ ed, and in some cases additional information \nis present. If the referrer was a search engine, usually the search string is speciﬁ ed. \nIt may not be worthwhile to put the raw search speciﬁ cation into your database \nbecause the search speciﬁ cations are so complicated and idiosyncratic that an ana-\nlyst may not be able to query them usefully. You can assume some kind of simpliﬁ ed \nand cleaned speciﬁ cation is placed in the speciﬁ cation attribute.\nReferral Dimension Attribute\nReferral Key\nReferral Type\nReferring URL\nReferring Site\nReferring Domain\nSearch Type\nSpecification\nTarget\nSurrogate values (1..N)\nIntra site, Remote site, Search engine, Corrupted, Inapplicable\nwww.organization-site.com/linkspage\nwww.organization-site.com\nwww.organization-site.com\nSimple text match, Complex logical match\nActual spec used (useful if simple text, otherwise questionable)\nMeta tags, Body text, Title (where search found its match)\nSample Data Values/Definitions\nFigure 15-4: Referral dimension attributes and sample data values.\n",
      "content_length": 2350,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 397,
      "content": "Electronic Commerce 361\n Clickstream Session Fact Table\nNow  that you have a portfolio of useful clickstream dimensions, you can design \nthe primary clickstream dimensional models based on the web server log data. \nThis business process can then be integrated into the family of other web retailing \nsubject areas.\nWith an eye toward keeping the ﬁ rst fact table from growing astronomically, \nyou should choose the grain to be one row for each completed customer session. \nThis grain is signiﬁ cantly higher than the underlying web server logs which record \neach individual page event, including individual pages as well as each graphical \nelement on each page. While we typically encourage designers to start with the \nmost granular data available in the source system, this is a purposeful deviation \nfrom our standard practices. Perhaps you have a big site recording more than 100 \nmillion page fetches per day, and 1 billion micro page events (graphical elements), \nbut you want to start with a more manageable number of rows to be loaded each \nday. We assume for the sake of argument that the 100 million page fetches boil \ndown to 20 million complete visitor sessions. This could arise if an average visitor \nsession touched 5 pages.\nThe  dimensions that are appropriate for this ﬁ rst fact table are calendar date, time \nof day, customer, page, session, and referrer. Finally, you can add a set of measured \nfacts for this session including session seconds, pages visited, orders placed, units \nordered, and order dollars. The completed design is shown in Figure 15-5.\nDate Dimension (2 views for roles)\nClickstream Session Fact\nUniversal Date Key (FK)\nUniversal Date/Time\nLocal Date Key (FK)\nLocal Date/Time\nCustomer Key (FK)\nEntry Page Key (FK)\nSession Key (FK)\nReferrer Key (FK)\nSession ID (DD)\nSession Seconds\nPages Visited\nOrders Placed\nOrder Quantity\nOrder Dollar Amount\nEntry Page Dimension\nCustomer Dimension\nSession Dimension\nReferrer Dimension\nFigure 15-5: Clickstream fact table design for complete sessions.\n",
      "content_length": 2027,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 398,
      "content": "Chapter 15\n362\nThere are a number of interesting aspects to this design. You may wonder why \nthere are two connections from the calendar date dimension to the fact table and \ntwo date/time stamps. This is a case in which both the calendar date and the time \nof day must play two diff erent roles. Because you are interested in measuring the \nprecise times of sessions, you must meet two conﬂ icting requirements. First, you \nwant to make sure you can synchronize all session dates and times internationally \nacross multiple time zones. Perhaps you have other date and time stamps from \nother web servers or nonweb systems elsewhere in the DW/BI environment. To \nachieve true synchronization of events across multiple servers and processes, you \nmust record all session dates and times, uniformly, in a single time zone such as \nGreenwich Mean Time (GMT) or Coordinated Universal Time (UTC). You should \ninterpret the session date and time combinations as the beginning of the session. \nBecause you have the dwell time of the session as a numeric fact, you can tell when \nthe session ended, if that is of interest.\nThe other requirement you meet with this design is to record the date and time of \nthe session relative to the visitor’s wall clock. The best way to represent this informa-\ntion is with a second calendar date foreign key and date/time stamp. Theoretically, \nyou could represent the time zone of the customer in the customer dimension table, \nbut constraints to determine the correct wall clock time would be horrendously \ncomplicated. The time diff erence between two cities (such as London and Sydney) \ncan change by as much as two hours at diff erent times of the year depending on \nwhen these cities go on and off  daylight savings time. This is not the business of \nthe BI reporting application to work out. It is the business of the database to store \nthis information, so it can be constrained in a simple and direct way.\nThe two role-playing calendar date dimension tables are views on a single under-\nlying table. The column names are massaged in the view deﬁ nition, so they are \nslightly diff erent when they show up in the user interface pick lists of BI tools. \nNote that the use of views makes the two instances of each table semantically \nindependent.\nWe modeled the exact instant in time with a full date/time stamp rather than a \ntime-of-day dimension. Unlike the calendar date dimension, a time-of-day dimen-\nsion would contain few if any meaningful attributes. You don’t have labels for each \nhour, minute, or second. Such a time-of-day dimension could be ridiculously large \nif its grain were the individual second or millisecond. Also, the use of an explicit \ndate/time stamp allows direct arithmetic between diff erent date/time stamps to \ncalculate precise time gaps between sessions, even those crossing days. Calculating \ntime gaps using a time-of-day dimension would be awkward.\nThe inclusion of the page dimension in Figure 15-5 may seem surprising given \nthe grain of the design is the customer session. However, in a given session, a very \n",
      "content_length": 3082,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 399,
      "content": "Electronic Commerce 363\ninteresting page is the entry page. The page dimension in this design is the page the \nsession started with. In other words, how did the customer hop onto your bus just \nnow? Coupled with the referrer dimension, you now have an interesting ability to \nanalyze how and why the customer accessed your website. A more elaborate design \nwould also add an exit page dimension.\nYou may be tempted to add the causal dimension to this design, but if the causal \ndimension focuses on individual products, it would be inappropriate to add it to \nthis design. The symptom that the causal dimension does not mesh with this design \nis the multivalued nature of the causal factors for a given complete session. If you \nrun ad campaigns or special deals for several products, how do you represent this \nmultivalued situation if the customer’s session involves several products? The right \nplace for a product-oriented causal dimension will be in the more ﬁ ne-grained table \ndescribed in the next fact table example. Conversely, a more broadly focused mar-\nket conditions dimension that describes conditions aff ecting all products would be \nappropriate for a session-grained fact table.\nThe session seconds fact is the total number of seconds the customer spent on the \nsite during this session. There will be many cases in which you can’t tell when the \ncustomer left. Perhaps the customer typed in a new URL. This won’t be detected by \nconventional web server logs. (If the data is collected by an ISP who can see every \nclick across sessions, this particular issue goes away.) Or perhaps the customer \ngot up out of the chair and didn’t return for 1 hour. Or perhaps the customer just \nclosed the browser without making any more clicks. In all these cases, your extract \nsoftware needs to assign a small and nominal number of seconds to this last session \nstep, so the analysis is not unrealistically distorted.\nWe purposely designed this ﬁ rst clickstream fact table to focus on complete visitor \nsessions while keeping the size under control. The next schema drops down to the \nlowest practical granularity you can support in the data warehouse: the individual \npage event.\n Clickstream Page Event Fact Table\nThe  granularity of the second clickstream fact table is the individual page event in \neach customer session; the underlying micro events recording graphical elements \nsuch as JPGs and GIFs are discarded (unless you are Yahoo! or eBay as described \npreviously). With simple static HTML pages, you can record only one interesting \nevent per page view, namely the page view. As websites employ dynamically created \nXML-based pages, with the ability to establish an on-going dialogue through the \npage, the number and type of events will grow.\nThis fact table could become astronomical in size. You should resist the urge \nto aggregate the table up to a coarser granularity because that inevitably involves \n",
      "content_length": 2929,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 400,
      "content": "Chapter 15\n364\ndropping dimensions. Actually, the ﬁ rst clickstream fact table represents just such \nan aggregation; although it is a worthwhile fact table, analysts cannot ask questions \nabout visitor behavior or individual pages.\nHaving chosen the grain, you can choose the appropriate dimensions. The list of \ndimensions includes calendar date, time of day, customer, page, event, session, ses-\nsion ID, step (three roles), product, referrer, and promotion. The completed design \nis shown in Figure 15-6.\nDate Dimension (2 views for roles)\nClickstream Page Event Fact\nUniversal Date Key (FK)\nUniversal Date/Time\nLocal Date Key (FK)\nLocal Date/Time\nCustomer Key (FK)\nPage Key (FK)\nEvent Key (FK)\nSession Key (FK)\nSession ID (DD)\nSession Step Key (FK)\nPurchase Step Key (FK)\nAbandonment Step Key (FK)\nProduct Key (FK)\nReferrer Key (FK)\nPromotion Key (FK)\nPage Seconds\nOrder Quantity\nOrder Dollar Amount\nProduct Key (PK)\nProduct Attributes ...\nPage Dimension\nCustomer Dimension\nEvent Dimension\nSession Dimension\nPromotion Dimension\nReferrer Dimension\nProduct Dimension\nStep Key (PK)\nStep Number\nSteps Until End\nStep Dimension (3 views for roles)\nFigure 15-6: Clickstream fact table design for individual page use.\nFigure 15-6 looks similar to the ﬁ rst design, except for the addition of the page, \nevent, promotion, and step dimensions. This similarity between fact tables is typical \nof dimensional models. One of the charms of dimensional modeling is the “boring” \nsimilarity of the designs. But that is where they get their power. When the designs \nhave a predictable structure, all the software up and down the DW/BI chain, from \nextraction, to database querying, to the BI tools, can exploit this similarity to great \nadvantage.\nThe two roles played by the calendar date and date/time stamps have the same \ninterpretation as in the ﬁ rst design. One role is the universal synchronized time, \nand the other role is the local wall clock time as measured by the customer. In this \nfact table, these dates and times refer to the individual page event.\n",
      "content_length": 2054,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 401,
      "content": "Electronic Commerce 365\nThe page dimension refers to the individual page. This is the main diff erence in \ngrain between the two clickstream fact tables. In this fact table you can see all the \npages accessed by the customers.\nAs described earlier, the session dimension describes the outcome of the session. \nA companion column, the session ID, is a degenerate dimension that does not have \na join to a dimension table. This degenerate dimension is a typical dimensional \nmodeling construct. The session ID is simply a unique identiﬁ er, with no semantic \ncontent, that serves to group together the page events of each customer session \nin an unambiguous way. You did not need a session ID degenerate dimension in \nthe ﬁ rst fact table, but it is included as a “parent key” if you want to easily link to \nthe individual page event fact table. We recommend the session dimension be at a \nhigher level of granularity than the session ID; the session dimension is intended \nto describe classes and categories of sessions, not the characteristics of each indi-\nvidual session.\nA product dimension is shown in this design under the assumption this website \nbelongs to a web retailer. A ﬁ nancial services site probably would have a similar \ndimension. A consulting services site would have a service dimension. An auction \nsite would have a subject or category dimension describing the nature of the items \nbeing auctioned. A news site would have a subject dimension, although with dif-\nferent content than an auction site.\nYou should accompany the product dimension with a promotion dimension so \nyou can attach useful causal interpretations to the changes in demand observed \nfor certain products.\nFor each page event, you should record the number of seconds that elapse before \nthe next page event. Call this page seconds to contrast it with session seconds in \nthe ﬁ rst fact table. This is a simple example of paying attention to conformed facts. \nIf you call both of these measures simply “seconds,” you risk having these seconds \ninappropriately added or combined. Because these seconds are not precisely equiva-\nlent, you should name them diff erently as a warning. In this particular case, you \nwould expect the page seconds for a session in this second fact table to add up to \nthe session seconds in the ﬁ rst fact table.\nThe ﬁ nal facts are units ordered and order dollars. These columns will be zero \nor null for many rows in this fact table if the speciﬁ c page event is not the event \nthat places the order. Nevertheless, it is highly attractive to provide these columns \nbecause they tie the all-important web revenue directly to behavior. If the units \nordered and order dollars were only available through the production order entry \nsystem elsewhere in the DW/BI environment, it would be ineffi  cient to perform the \n",
      "content_length": 2833,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 402,
      "content": "Chapter 15\n366\nrevenue-to-behavior analysis across multiple large tables. In many database man-\nagement systems, these null facts are handled effi  ciently and may take up literally \nzero space in the fact table.\n Step Dimension\nBecause  the fact table grain is the individual page event, you can add the powerful \nstep dimension described in Chapter 8: Customer Relationship Management. The \nstep dimension, originally shown in Figure 8-11, provides the position of the speciﬁ c \npage event within the overall session.\nThe step dimension becomes particularly powerful when it is attached to the fact \ntable in various roles. Figure 15-6 shows three roles: overall session, purchase subses-\nsion, and abandonment subsession. A purchase subsession, by deﬁ nition, ends in a \nsuccessful purchase. An abandonment subsession is one that fails to complete a pur-\nchase transaction for some reason. Using these roles of the step dimension allows some \nvery interesting queries. For example, if the purchase step dimension is constrained to \nstep number 1, the query returns nothing but the starting page for successful purchase \nexperiences. Conversely, if the abandonment step dimension is constrained to zero \nsteps remaining, the query returns nothing but the last and presumably most unful-\nﬁ lling pages visited in unsuccessful purchase sessions. Although the whole design \nshown in Figure 15-6 is aimed at product purchases, the step dimension technique \ncan be used in the analysis of any sequential process.\n Aggregate Clickstream Fact Tables\nBoth  clickstream fact tables designed thus far are pretty large. There are many \nbusiness questions that would be forced to summarize millions of rows from these \ntables. For example, if you want to track the total visits and revenue from major \ndemographic groups of customers accessing your website on a month-by-month \nbasis, you can certainly do that with either fact table. In the session-grained fact \ntable, you would constrain the calendar date dimension to the appropriate time span \n(say January, February, and March of the current year). You would then create row \nheaders from the demographics type attribute in the customer dimension and the \nmonth attribute in the calendar dimension (to separately label the three months \nin the output). Finally, you would sum the Order Dollars and count the number of \nsessions. This all works ﬁ ne. But it is likely to be slow without help from an aggre-\ngate table. If this kind of query is frequent, the DBA will be encouraged to build an \naggregate table, as shown in Figure 15-7.\nYou can build this table directly from your ﬁ rst fact table, whose grain is the \nindividual session. To build this aggregate table, you group by month, demographic \ntype, entry page, and session outcome. You count the number of sessions, and sum \n",
      "content_length": 2829,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 403,
      "content": "Electronic Commerce 367\nall the other additive facts. This results in a drastically smaller fact table, almost \ncertainly less than 1% of the original session-grained fact table. This reduction in \nsize translates directly to a corresponding increase in performance for most queries. \nIn other words, you can expect queries directed to this aggregate table to run at \nleast 100 times as fast.\nMonth Dimension\nSession Aggregate Fact\nUniversal Month Key (FK)\nDemographic Key (FK)\nEntry Page Key (FK)\nSession Outcome Key (FK)\nNumber of Sessions\nSession Seconds\nPages Visited\nOrders Placed\nOrder Quantity\nOrder Dollar Amount\nEntry Page Dimension\nDemographic Dimension\nSession Outcome Dimension\nFigure 15-7: Aggregate clickstream fact table.\nAlthough it may not have been obvious, we followed a careful discipline in build-\ning the aggregate table. This aggregate fact table is connected to a set of shrunken \nrollup dimensions directly related to the original dimensions in the more granular \nfact tables. The month dimension is a conformed subset of the calendar day dimen-\nsion’s attributes. The demographic dimension is a conformed subset of customer \ndimension attributes. You should assume the page and session tables are unchanged; \na careful design of the aggregation logic could suggest a conformed shrinking of \nthese tables as well.\nGoogle Analytics\nGoogle  Analytics (GA) is a service provided by Google that is best described as an \nexternal data warehouse that provides many insights about how your website is used. \nTo use GA, you modify each page of your website to include a GA tracking code \n(GATC) embedded in a Java code snippet located in the HTML <head> declaration of \neach page to be tracked. When a visitor accesses the page, information is sent to the \nAnalytics service at Google, as long as the visitor has JavaScript enabled. Virtually \nall of the information described in this chapter can be collected through GA, with \nthe exception of personally identiﬁ able information (PII) which is forbidden by GA’s \nterms of service. GA can be combined with Google’s Adword service to track ad \ncampaigns and conversions (sales). Reportedly, GA is used by more than 50% of the \nmost popular web sites on the internet.\n",
      "content_length": 2234,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 404,
      "content": "Chapter 15\n368\nData from GA can be viewed in a BI tool dashboard online directly from the under-\nlying GA databases, or data can be delivered to you in a wide variety of standard and \ncustom reports, making it possible to build your own local business process schema \nsurrounding this data.\nInterestingly, GA’s detailed technical explanation of the data elements that can be \ncollected through the service are described correctly as either dimensions or measures. \nSomeone at Google has been reading our books…\n Integrating Clickstream into Web Retailer’s \nBus Matrix\nThis section considers the business processes needed by a web-based computer retailer. \nThe retailer’s enterprise data warehouse bus matrix is illustrated in Figure 15-8. Note \nthe matrix lists business process subject areas, not individual fact tables. Typically, each \nmatrix row results in a suite of closely associated fact tables and/or OLAP cubes, which \nall represent a particular business process.\nThe Figure 15-8 matrix has a number of striking characteristics. There are a \nlot of check marks. Some of the dimensions, such as date/time, organization, and \nemployee appear in almost every business process. The product and customer dimen-\nsions dominate the middle part of the matrix, where they are attached to business \nprocesses that describe customer-oriented activities. At the top of the matrix, suppli-\ners and parts dominate the processes of acquiring the parts that make up products \nand building them to order for the customer. At the bottom of the matrix, you have \nclassic infrastructure and cost driver business processes that are not directly tied \nto customer behavior.\nThe web visitor clickstream subject area sits squarely among the customer-\noriented processes. It shares the date/time, product, customer, media, causal, and \nservice policy dimensions with several other business processes nearby. In this \nsense it should be obvious that the web visitor clickstream data is well integrated \ninto the fabric of the overall DW/BI system for this retailer. Applications tying the \nweb visitor clickstream will be easy to integrate across all the processes sharing \nthese conformed dimensions because separate queries to each fact table can be \ncombined across individual rows of the report.\nThe  web visitor clickstream business process contains the four special click-\nstream dimensions not found in the others. These dimensions do not pose a problem \nfor applications. Instead, the ability of the web visitor clickstream data to bridge \nbetween the web world and the brick-and-mortar world is exactly the advantage \nyou are looking for. You can constrain and group on attributes from the four web \n",
      "content_length": 2693,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 405,
      "content": "Electronic Commerce 369\ndimensions and explore the eff ect on the other business processes. For example, you \ncan see what kinds of web experiences produce customers who purchase certain \nkinds of service policies and then invoke certain levels of service demands.\nSupplier Purchase Orders\nSupply Chain Management\nCustomer Relationship Management\nSupplier Deliveries\nPart Inventories\nProduct Assembly Bill of Materials\nProduct Assembly to Order\nProduct Promotions\nAdvertising\nCustomer Communications\nCustomer Inquiries\nWeb Visitor Clickstream\nProduct Orders\nService Policy Orders\nProduct Shipments\nCustomer Billing\nCustomer Payments\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nDate and Time\nPart\nVendor\nCarrier\nFacility\nProduct\nCustomer\nMedia\nPromotion\nService Policy\nInternal Organization\nEmployee\nClickstream (4 dims)\nProduct Returns\nProduct Support\nService Policy Responses\nOperations\nEmployee Labor\nHuman Resources\nFacilities Operations\nWeb Site Operations\nFigure 15-8: Bus matrix for web retailer.\nFinally, it should be pointed out that the matrix serves as a kind of communica-\ntions vehicle for all the business teams and senior management to appreciate the \n",
      "content_length": 1382,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 406,
      "content": "Chapter 15\n370\nneed to conform dimensions and facts. A given column in the matrix is, in eff ect, \nan invitation list to the meeting for conforming the dimension!\n Proﬁ tability Across Channels Including Web\nAfter  the DW/BI team successfully implements the initial clickstream fact tables \nand ties them to the sales transaction and customer communication business pro-\ncesses, the team may be ready to tackle the most challenging subject area of all: \nweb proﬁ tability.\nYou can tackle web proﬁ tability as an extension of the sales transaction process. \nFundamentally, you are allocating all the activity and infrastructure costs down to \neach sales transaction. You could, as an alternative, try to build web proﬁ tability \non top of the clickstream, but this would involve an even more controversial allo-\ncation process in which you allocate costs down to each session. It would be hard \nto assign activity and infrastructure costs to a session that has no obvious product \ninvolvement and leads to no immediate sale.\nA big beneﬁ t of extending the sales transaction fact table is that you get a view \nof proﬁ tability across all your sales channels, not just the web. In a way, this should \nbe obvious because you know that you must sort out the costs and assign them to \nthe various channels. \nThe grain of the proﬁ t and loss facts is each individual line item sold on a sales \nticket to a customer at a point in time, whether it’s a single sales ticket or single web \npurchasing session. This is the same as the grain of the sales transaction business \nprocess and includes all channels, assumed to be store sales, telesales, and web sales.\nThe dimensions of the proﬁ t and loss facts are also the same as the sales transac-\ntion fact table: date, time, customer, channel, product, promotion, and ticket number \n(degenerate). The big diff erence between the proﬁ tability and sales transaction fact \ntables is the breakdown of the costs, as illustrated in Figure 15-9.\nBefore discussing the allocation of costs, let us examine the format of the proﬁ t \nand loss facts. It is organized as a simple proﬁ t and loss (P&L) statement (refer to \nFigure 6-14). The ﬁ rst fact is familiar units sold. All the other facts are dollar values \nbeginning with the value of the sale as if it were sold at the list or catalog price, \nreferred to as gross revenue. Assuming sales often take place at lower prices, you \nwould account for any diff erence with a manufacturer’s allowance, marketing pro-\nmotion that is a price reduction, or markdown done to move the inventory. When \nthese eff ects are taken into account, you can calculate the net revenue, which is the \ntrue net price the customer pays times the number of units purchased.\nThe rest of the P&L consists of a series of subtractions, where you calculate \nprogressively more far-reaching versions of proﬁ t. You can begin by subtracting \nthe product manufacturing cost if you manufacture it, or equivalently, the product \n",
      "content_length": 2978,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 407,
      "content": "Electronic Commerce 371\nacquisition cost if it is acquired from a supplier. Then subtract the product storage \ncost. At this point, many enterprises call this partial result the gross proﬁ t. You can \ndivide this gross proﬁ t by the gross revenue to get the gross margin ratio.\nDate Dimension (2 views for roles)\nProfitability Fact\nUniversal Date Key (FK)\nUniversal Time of Day Key (FK)\nLocal Date Key (FK)\nLocal Time of Day Key (FK)\nCustomer Key (FK)\nChannel Key (FK)\nProduct Key (FK)\nPromotion Key (FK)\nTicket Number (DD)\nUnits Sold\nGross Revenue\nManufacturing Allowance\nMarketing Promotion\nSales Markdown\nNet Revenue\nManufacturing Cost\nStorage Cost\nGross Profit\nFreight Cost\nSpecial Deal Cost\nOther Overhead Cost\nNet Profit\nCustomer Dimension\nTime of Day Dimension (2 views for roles)\nChannel Dimension\nPromotion Dimension\nProduct Dimension\nFigure 15-9: Proﬁ t and loss facts across sales channels, including web sales.\nObviously, the columns called net revenue and gross proﬁ t are calculated directly \nfrom the columns immediately preceding them in the fact table. But should you \nexplicitly store these columns in the database? The answer depends on whether \nyou provide access to this fact table through a view or whether users or BI applica-\ntions directly access the physical fact table. The structure of the P&L is suffi  ciently \ncomplex that, as the data warehouse provider, you don’t want to risk the impor-\ntant measures like net revenue and gross proﬁ t being computed incorrectly. If you \nprovide all access through views, you can easily provide the computed columns \nwithout physically storing them. But if your users are allowed to access the under-\nlying physical table, you should include net revenue, gross proﬁ t, and net proﬁ t as \nphysical columns.\nBelow the gross proﬁ t you can continue subtracting various costs. Typically, the \nDW/BI team must separately source or estimate each of these costs. Remember the \nactual entries in any given fact table row are the fractions of these total costs allo-\ncated all the way down to the individual fact row grain. Often there is signiﬁ cant \npressure on the DW/BI team to deliver the proﬁ tability business process. Or to put \nit another way, there is tremendous pressure to source all these costs. But how good \n",
      "content_length": 2281,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 408,
      "content": "Chapter 15\n372\nare the costs in the various underlying data sets? Sometimes a cost is only available \nas a national average, computed for an entire year. Any allocation scheme is going \nto assign a kind of pro forma value that has no real texture to it. Other costs will be \nbroken down a little more granularly, perhaps to calendar quarter and by geographic \nregion (if relevant). Finally, some costs may be truly activity-based and vary in a \nhighly dynamic, responsive, and realistic way over time.\nWebsite system costs are an important cost driver in electronic commerce busi-\nnesses. Although website costs are classic infrastructure costs, and are therefore \ndiffi  cult to allocate directly to the product and customer activity, this is a key step \nin developing a web-oriented P&L statement. Various allocation schemes are pos-\nsible, including allocating the website costs to various product lines by the number \nof pages devoted to each product, allocating the costs by pages visited, or allocating \nthe costs by actual web-based purchases. \nThe DW/BI team cannot be responsible for implementing activity-based costing \n(ABC) in a large organization. When the team is building a proﬁ tability dimensional \nmodel, the team gets the best cost data available at the moment and publishes the \nP&L. Perhaps some of the numbers are simple rule-of-thumb ratios. Others may be \nhighly detailed activity-based costs. Over time, as the sources of cost improve, the \nDW/BI team incorporates these new sources and notiﬁ es the users that the business \nrules have improved.\nBefore leaving this design, it is worthwhile putting it in perspective. When a \nP&L structure is embedded in a rich dimensional framework, you have immense \npower. You can break down all the components of revenue, cost, and proﬁ t for \nevery conceivable slice and dice provided by the dimensions. You can answer what \nis proﬁ table, but also answer “why” because you can see all the components of the \nP&L, including:\n \n■How proﬁ table is each channel (web sales, telesales, and store sales)? Why?\n \n■How proﬁ table are your customer segments? Why?\n \n■How proﬁ table is each product line? Why?\n \n■How proﬁ table are your promotions? Why?\n \n■When is your business most proﬁ table? Why?\nThe symmetric dimensional approach enables you to combine constraints from \nmany dimensions, allowing compound versions of the proﬁ tability analyses like:\n \n■Who are the proﬁ table customers in each channel? Why?\n \n■Which promotions work well on the web but do not work well in other \nchannels? Why?\n",
      "content_length": 2557,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 409,
      "content": "Electronic Commerce 373\nSummary\nThe web retailer case study used in this chapter is illustrative of any business with \na signiﬁ cant web presence. Besides tackling the clickstream subject area at multiple \nlevels of granularity, the central challenge is eff ectively integrating the clickstream \ndata into the rest of the business. We discussed ways to address the identiﬁ cation \nchallenges associated with the web visitor, their origin, and session boundaries, \nalong with the special dimensions unique to clickstream data, including t he ses-\nsion, page, and step dimensions.\nIn the next chapter, we’ll turn our attention to the primary business processes \nin an insurance company as we recap many of the dimensional modeling patterns \npresented throughout this book.\n",
      "content_length": 771,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 411,
      "content": "Insurance\nW\ne bring together concepts from nearly all the previous chapters to build \na DW/BI system for a property and casualty insurance company in this \nfinal case study. If you are from the insurance industry and jumped directly to \nthis chapter for a quick fix, please accept our apology, but this material depends \nheavily on ideas from the earlier chapters. You’ll need to turn back to the beginning \nof the book to have this chapter make any sense.\nAs  has been our standard procedure, this chapter launches with background \ninformation for a business case. While the requirements unfold, we’ll draft the enter-\nprise data warehouse bus matrix, much like we would in a real-life requirements \nanalysis eff ort. We’ll then design a series of dimensional models by overlaying the \ncore techniques learned thus far.\nChapter 16 reviews the following concepts:\n \n■Requirements-driven approach to dimensional design\n \n■Value chain implications, along with an example bus matrix snippet for an \ninsurance company\n \n■Complementary transaction, periodic snapshot, and accumulating snapshot \nschemas\n \n■Dimension role playing\n \n■Handling of slowly changing dimension attributes\n \n■Mini-dimensions for dealing with large, rapidly changing dimension attributes\n \n■Multivalued dimension attributes\n \n■Degenerate dimensions for operational control numbers\n \n■Audit dimensions to track data lineage\n \n■Heterogeneous supertypes and subtypes to handle products with varied attri-\nbutes and facts \n \n■Junk dimensions for miscellaneous indicators\n16\n",
      "content_length": 1539,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 412,
      "content": "Chapter 16\n376\n \n■Conformed dimensions and facts\n \n■Consolidated fact tables combining metrics from separate business processes\n \n■Factless fact tables\n \n■Common mistakes to avoid when designing dimensional models\n Insurance Case Study\nImagine working for a large property and casualty insurer that off ers automobile, \nhomeowner, and personal property insurance. You conduct extensive interviews \nwith business representatives and senior management from the claims, ﬁ eld opera-\ntions, underwriting, ﬁ nance, and marketing departments. Based on these interviews, \nyou learn the industry is in a state of ﬂ ux. Nontraditional players are leveraging \nalternative channels. Meanwhile, the industry is consolidating due to globalization, \nderegulation, and demutualization challenges. Markets are changing, along with \ncustomer needs. Numerous interviewees tell us information is becoming an even \nmore important strategic asset. Regardless of the functional area, there is a strong \ndesire to use information more eff ectively to identify opportunities more quickly \nand respond most appropriately.\nThe good news is that internal systems and processes already capture the bulk \nof the data required. Most insurance companies generate tons of nitty-gritty opera-\ntional data. The bad news is the data is not integrated. Over the years, political and \nIT boundaries have encouraged the construction of tall barriers around isolated \nislands of data. There are multiple disparate sources for information about the \ncompany’s products, customers, and distribution channels. In the legacy opera-\ntional systems, the same policyholder may be identiﬁ ed several times in separate \nautomobile, home, and personal property applications. Traditionally, this segmented \napproach to data was acceptable because the diff erent lines of business functioned \nlargely autonomously; there was little interest in sharing data for cross-selling and \ncollaboration in the past. Now within our case study, business management is \nattempting to better leverage this enormous amount of inconsistent and somewhat \nredundant data.\nBesides the inherent issues surrounding data integration, business users lack the \nability to access data easily when needed. In an attempt to address this shortcom-\ning, several groups within the case study company rallied their own resources and \nhired consultants to solve their individual short-term data needs. In many cases, the \nsame data was extracted from the same source systems to be accessed by separate \norganizations without any strategic overall information delivery strategy.\n",
      "content_length": 2596,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 413,
      "content": "Insurance 377\nIt didn’t take long to recognize the negative ramiﬁ cations associated with separate \nanalytic data repositories because performance results presented at executive meetings \ndiff ered depending on the data source. Management understood this independent \nroute was not viable as a long-term solution because of the lack of integration, large \nvolumes of redundant data, and diffi  culty in interpreting and reconciling the results. \nGiven the importance of information in this brave new insurance world, manage-\nment was motivated to deal with the cost implications surrounding the development, \nsupport, and analytic ineffi  ciencies of these supposed data warehouses that merely \nproliferated operational data islands.\nSenior  management chartered the chief information offi  cer (CIO) with the respon-\nsibility and authority to break down the historical data silos to “achieve information \nnirvana.” They charged the CIO with the ﬁ duciary responsibility to manage and \nleverage the organization’s information assets more eff ectively. The CIO developed \nan overall vision that wed an enterprise strategy for dealing with massive amounts \nof data with a response to the immediate need to become an information-rich orga-\nnization. In the meantime, an enterprise DW/BI team was created to begin designing \nand implementing the vision.\nSenior management has been preaching about a transformation to a more cus-\ntomer-centric focus, instead of the traditional product-centric approach, in an eff ort \nto gain competitive advantage. The CIO jumped on that bandwagon as a catalyst \nfor change. The folks in the trenches have pledged intent to share data rather than \nsquirreling it away for a single purpose. There is a strong desire for everyone to \nhave a common understanding of the state of the business. They’re clamoring to \nget rid of the isolated pockets of data while ensuring they have access to detail and \nsummary data at both the enterprise and line-of-business levels.\nInsurance Value Chain\nThe  primary value chain of an insurance company is seemingly short and simple. The \ncore processes are to issue policies, collect premium payments, and process claims. \nThe organization is interested in better understanding the metrics spawned by each \nof these events. Users want to analyze detailed transactions relating to the formula-\ntion of policies, as well as transactions generated by claims processing. They want \nto measure performance over time by coverage, covered item, policyholder, and \nsales distribution channel characteristics. Although some users are interested in \nthe enterprise perspective, others want to analyze the heterogeneous nature of the \ninsurance company’s individual lines of business.\nObviously, an insurance company is engaged in many other external pro-\ncesses, such as the investment of premium payments or compensation of contract \n",
      "content_length": 2888,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 414,
      "content": "Chapter 16\n378\nagents, as well as a host of internally focused activities, such as human resources, \nﬁ nance, and purchasing. For now, we will focus on the core business related to \npolicies and claims.\nThe insurance value chain begins with a variety of policy transactions. Based \non your current understanding of the requirements and underlying data, you opt \nto handle all the transactions impacting a policy as a single business process (and \nfact table). If this perspective is too simplistic to accommodate the metrics, dimen-\nsionality, or analytics required, you should handle the transaction activities as \nseparate fact tables, such as quoting, rating, and underwriting. As discussed in \nChapter 5: Procurement, there are trade-off s between creating separate fact tables \nfor each natural cluster of transaction types versus lumping the transactions into \na single fact table.\nThere is also a need to better understand the premium revenue associated with \neach policy on a monthly basis. This will be key input into the overall proﬁ t picture. \nThe insurance business is very transaction intensive, but the transactions themselves \ndo not represent little pieces of revenue, as is the case with retail or manufactur-\ning sales. You cannot merely add up policy transactions to determine the revenue \namount. The picture is further complicated in insurance because customers pay in \nadvance for services. This same advance-payment model applies to organizations \noff ering magazine subscriptions or extended warranty contracts. Premium payments \nmust be spread across multiple periods because the company earns the revenue over \ntime as it provides insurance coverage. The complex relationship between policy \ntransactions and revenue measurements often makes it impossible to answer rev-\nenue questions by crawling through the individual transactions. Not only is such \ncrawling time-consuming, but also the logic required to interpret the eff ect of dif-\nferent transaction types on revenue can be horrendously complicated. The natural \nconﬂ ict between the detailed transaction view and the snapshot perspective almost \nalways requires building both kinds of fact tables in the warehouse. In this case, \nthe premium snapshot is not merely a summarization of the policy transactions; it \nis quite a separate thing that comes from a separate source.\nDraft Bus Matrix\nBased  on the interview ﬁ ndings, along with an understanding of the key source \nsystems, the team begins to draft an enterprise data warehouse bus matrix with the \ncore policy-centric business processes as rows and core dimensions as columns. \nTwo rows are deﬁ ned in the matrix, one corresponding to the policy transactions \nand another for the monthly premium snapshot.\nAs illustrated in Figure 16-1, the core dimensions include date, policyholder, \nemployee, coverage, covered item, and policy. When drafting the matrix, don’t \n",
      "content_length": 2909,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 415,
      "content": "Insurance 379\nattempt to include all the dimensions. Instead, try to focus on the core common \ndimensions that are reused in more than one schema.\nPolicy Transactions\nPremium Snapshot\nDate\nPolicyholder\nCovered Item\nEmployee\nX\nX\nX\nX\nX\nX\nX\nMonth\nX\nX\nAgent\nX\nX\nX\nPolicy\nCoverage\nFigure 16-1: Initial draft bus matrix.\n Policy Transactions\nLet’s  turn our attention to the ﬁ rst row of the matrix by focusing on the transactions \nfor creating and altering a policy. Assume the policy represents a set of coverages \nsold to the policyholder. Coverages can be considered the insurance company’s \nproducts. Homeowner coverages include ﬁ re, ﬂ ood, theft, and personal liability; \nautomobile coverages include comprehensive, collision damage, uninsured motor-\nist, and personal liability. In a property and casualty insurance company, coverages \napply to a speciﬁ c covered item, such as a particular house or car. Both the coverage \nand covered item are carefully identiﬁ ed in the policy. A particular covered item \nusually has several coverages listed in the policy.\nAgents sell policies to policyholders. Before the policy can be created, a pricing \nactuary determines the premium rate that will be charged given the speciﬁ c cover-\nages, covered items, and qualiﬁ cations of the policyholder. An underwriter, who \ntakes ultimate responsibility for doing business with the policyholder, makes the \nﬁ nal approval.\nThe operational policy transaction system captures the following types of \ntransactions:\n \n■Create policy, alter policy, or cancel policy (with reason)\n \n■Create coverage on covered item, alter coverage, or cancel coverage (with \nreason)\n \n■Rate coverage or decline to rate coverage (with reason)\n \n■Underwrite policy or decline to underwrite policy (with reason)\n",
      "content_length": 1774,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 416,
      "content": "Chapter 16\n380\nThe grain of the policy transaction fact table should be one row for each indi-\nvidual policy transaction. Each atomic transaction should be embellished with \nas much context as possible to create a complete dimensional description of the \ntransaction. The dimensions associated with the policy transaction business pro-\ncess include the transaction date, eff ective date, policyholder, employee, coverage, \ncovered item, policy number, and policy transaction type. Now let’s further discuss \nthe dimensions in this schema while taking the opportunity to reinforce concepts \nfrom earlier chapters.\n Dimension Role Playing\nThere  are two dates associated with each policy transaction. The policy transac-\ntion date is the date when the transaction was entered into the operational system, \nwhereas the policy transaction eff ective date is when the transaction legally takes \neff ect. These two foreign keys in the fact table should be uniquely named. The two \nindependent dimensions associated with these keys are implemented using a single \nphysical date table. Multiple logically distinct tables are then presented to the user \nthrough views with unique column names, as described originally in Chapter 6: \nOrder Management.\n Slowly Changing Dimensions\nInsurance  companies typically are very interested in tracking changes to dimensions \nover time. You can apply the three basic techniques for handling slowly changing \ndimension (SCD) attributes to the policyholder dimension, as introduced in Chapter 5.\nWith the type 1 technique, you simply overwrite the dimension attribute’s prior \nvalue. This is the simplest approach to dealing with attribute changes because the \nattributes always represent the most current descriptors. For example, perhaps the \nbusiness agrees to handle changes to the policyholder’s date of birth as a type 1 \nchange based on the assumption that any changes to this attribute are intended as \ncorrections. In this manner, all fact table history for this policyholder appears to \nhave always been associated with the updated date of birth.\nBecause the policyholder’s ZIP code is key input to the insurer’s pricing and risk \nalgorithms, users are very interested in tracking ZIP code changes, so the type 2 \ntechnique is used for this attribute. Type 2 is the most common SCD technique \nwhen there’s a requirement for accurate change tracking over time. In this case, when \nthe ZIP code changes, you create a new policyholder dimension row with a new \nsurrogate key and updated geographic attributes. Do not go back and revisit the fact \ntable. Historical fact table rows, prior to the ZIP code change, still reﬂ ect the old \nsurrogate key. Going forward, you use the policyholder’s new surrogate key, so new \nfact table rows join to the post-change dimension proﬁ le. Although this technique is \nextremely graceful and powerful, it places more burdens on ETL processing. Also, \n",
      "content_length": 2923,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 417,
      "content": "Insurance 381\nthe number of rows in the dimension table grows with each type 2 SCD change. \nGiven there might already be more than 1 million rows in your policyholder dimen-\nsion table, you may opt to use a mini-dimension for tracking ZIP code changes, \nwhich we will review shortly.\nFinally, let’s assume each policyholder is classiﬁ ed as belonging to a particu-\nlar segment. Perhaps nonresidential policyholders were historically categorized as \neither commercial or government entities. Going forward, the business users want \nmore detailed classiﬁ cations to diff erentiate between large multinational, middle \nmarket, and small business commercial customers, in addition to nonproﬁ t organi-\nzations and governmental agencies. For a period of time, users want the ability to \nanalyze results by either the historical or new segment classiﬁ cations. In this case \nyou could use a type 3 approach to track the change for a period of time by adding \na column, labeled Historical for diff erentiation, to retain the old classiﬁ cations. \nThe new classiﬁ cation values would populate the segment attribute that has been \na permanent ﬁ xture on the policyholder dimension. This approach, although not \nextremely common, allows you to see performance by either the current or histori-\ncal segment maps. This is useful when there’s been an en masse change, such as \nthe customer classiﬁ cation realignment. Obviously, the type 3 technique becomes \noverly complex if you need to track more than one version of the historical map or \nbefore-and-after changes for multiple dimension attributes.\n Mini-Dimensions for Large or Rapidly Changing \nDimensions\nAs  mentioned earlier, the policyholder dimension qualiﬁ es as a large dimension with \nmore than 1 million rows. It is often important to accurately track content values \nfor a subset of attributes. For example, you need an accurate description of some \npolicyholder and covered item attributes at the time the policy was created, as well \nas at the time of any adjustment or claim. As discussed in Chapter 5, the practical \nway to track changing attributes in large dimensions is to split the closely monitored, \nmore rapidly changing attributes into one or more type 4 mini-dimensions directly \nlinked to the fact table with a separate surrogate key. The use of mini-dimensions \nhas an impact on the effi  ciency of attribute browsing because users typically want \nto browse and constrain on these changeable attributes. If all possible combina-\ntions of the attribute values in the mini-dimension have been created, handling a \nmini-dimension change simply means placing a diff erent key in the fact table row \nfrom a certain point in time forward. Nothing else needs to be changed or added \nto the database.\nThe covered item is the house, car, or other speciﬁ c insured item. The cov-\nered item dimension contains one row for each actual covered item. The covered \nitem dimension is usually somewhat larger than the policyholder dimension, so \n",
      "content_length": 2997,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 418,
      "content": "Chapter 16\n382\nit’s another good place to consider deploying a mini-dimension. You do not want \nto capture the variable descriptions of the physical covered objects as facts because \nmost are textual and are not numeric or continuously valued. You should make every \neff ort to put textual attributes into dimension tables because they are the target of \ntextual constraints and the source of report labels.\n Multivalued Dimension Attributes\nWe discussed multivalued dimension attributes when we associated multiple skills \nwith an employee in Chapter 9: Human Resources Management. In Chapter 10: \nFinancial Services, we associated multiple customers with an account, and then \nin Chapter 14: Healthcare, we modeled a patient’s multiple diagnoses. In this \ncase study, you’ll look at another multivalued modeling situation: the relationship \nbetween commercial customers and their industry classiﬁ cations.\nEach  commercial customer may be associated with one or more Standard Industry \nClassiﬁ cation (SIC) or North American Industry Classiﬁ cation System (NAICS) codes. \nA large, diversiﬁ ed commercial customer could be represented by a dozen or more \nclassiﬁ cation codes. Much like you did with Chapter 14’s diagnosis group, a bridge \ntable ties together all the industry classiﬁ cation codes within a group. This indus-\ntry classiﬁ cation bridge table joins directly to either the fact table or the customer \ndimension as an outrigger. It enables you to report fact table metrics by any industry \nclassiﬁ cation. If the commercial customer’s industry breakdown is proportionally \nidentiﬁ ed, such as 50 percent agricultural services, 30 percent dairy products, and \n20 percent oil and gas drilling, a weighting factor should be included on each \nbridge table row. To handle the case in which no valid industry code is associated \nwith a given customer, you simply create a special bridge table row that represents \n Unknown.\n Numeric Attributes as Facts or Dimensions\nLet’s move on to the coverage dimension. Large insurance companies have dozens \nor even hundreds of separate coverage products available to sell for a given type of \ncovered item. The actual appraised value of a speciﬁ c covered item, like someone’s \nhouse, is a continuously valued numeric quantity that can even vary for a given item \nover time, so treat it as a legitimate fact. In the dimension table, you could store a \nmore descriptive value range, such as $250,000 to $299,999 Appraised Value, for \ngrouping and ﬁ ltering. The basic coverage limit is likely to be more standardized \nand not continuously valued, like Replacement Value or Up to $250,000. In this \ncase, it would also be treated as a dimension  attribute.\n",
      "content_length": 2703,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 419,
      "content": "Insurance 383\n Degenerate Dimension\nThe  policy number will be handled as a degenerate dimension if you have extracted \nall the policy header information into other dimensions. You obviously want to avoid \ncreating a policy transaction fact table with just a small number of keys while embed-\nding all the descriptive details (including the policyholder, dates, and coverages) in \nan overloaded policy dimension. In some cases, there may be one or two attributes \nthat still belong to the policy and not to another dimension. For example, if the \nunderwriter establishes an overall risk grade for the policy based on the totality of \nthe coverages and covered items, then this risk grade probably belongs in a policy \ndimension. Of course, then the policy number is no longer a degenerate dimension.\n Low Cardinality Dimension Tables\nThe  policy transaction type dimension is a small dimension for the transaction types \nlisted earlier with reason descriptions. A transaction type dimension might contain \nless than 50 rows. Even though this table is both narrow in terms of the number \nof columns and shallow in terms of the number of rows, the attributes should still \nbe handled in a dimension table; if the textual characteristics are used for query \nﬁ ltering or report labeling, then they belong in a dimension.\n Audit Dimension\nYou  have the option to associate ETL process metadata with transaction fact rows \nby including a key that links to an audit dimension row created by the extract \nprocess. As discussed in Chapter 6, each audit dimension row describes the data \nlineage of the fact row, including the time of the extract, source table, and extract \nsoftware version.\nPolicy Transaction Fact Table\nThe  policy transaction fact table in Figure 16-2 illustrates several characteristics of \na classic transaction grain fact table. First, the fact table consists almost entirely \nof keys. Transaction schemas enable you to analyze behavior in extreme detail. As \nyou descend to lower granularity with atomic data, the fact table naturally sprouts \nmore dimensionality. In this case, the fact table has a single numeric fact; interpreta-\ntion of the fact depends on the corresponding transaction type dimension. Because \nthere are diff erent kinds of transactions in the same fact table, in this scenario, you \ncannot label the fact more speciﬁ cally. \n",
      "content_length": 2364,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 420,
      "content": "Chapter 16\n384\nPolicy Transaction Date Key (FK)\nPolicy Effective Date Key (FK)\nPolicyholder Key (FK)\nEmployee Key (FK)\nCoverage Key (FK)\nCovered Item Key (FK)\nPolicy Transaction Type Key (FK)\nPolicy Transaction Audit Key (FK)\nPolicy Number (DD)\nPolicy Transaction Number (DD)\nPolicy Transaction Dollar Amount\nPolicy Transaction Fact\nEmployee Dimension\nCovered Item Dimension\nDate Dimension (2 views for roles)\nPolicyholder Dimension\nCoverage Dimension\nPolicy Transaction Type Dimension\nPolicy Transaction Audit Dimension\nFigure 16-2: Policy transaction schema.\n Heterogeneous Supertype and Subtype Products\nAlthough there is strong support for an enterprise-wide perspective at our insur-\nance company, the business users don’t want to lose sight of their line-of-business \nspeciﬁ cs. Insurance companies typically are involved in multiple, very diff erent lines \nof business. For example, the detailed parameters of homeowners’ coverages diff er \nsigniﬁ cantly from automobile coverages. And these both diff er substantially from \npersonal property coverage, general liability coverage, and other types of insurance. \nAlthough all coverages can be coded into the generic structures used so far in this \nchapter, insurance companies want to track numerous speciﬁ c attributes that make \nsense only for a particular coverage and covered item. You can generalize the initial \nschema developed in Figure 16-2 by using the supertype and subtype technique \ndiscussed in Chapter 10.\nFigure  16-3 shows a schema to handle the speciﬁ c attributes that describe auto-\nmobiles and their coverages. For each line of business (or coverage type), subtype \ndimension tables for both the covered item and associated coverage are created. \nWhen a BI application needs the speciﬁ c attributes of a single coverage type, it uses \nthe appropriate subtype dimension tables.\nNotice in this schema that you don’t need separate line-of-business fact tables \nbecause the metrics don’t vary by business, but you’d likely put a view on the \nsupertype fact table to present only rows for a given subtype. The subtype dimen-\nsion tables are introduced to handle the special line-of-business attributes. No \nnew keys need to be generated; logically, all we are doing is extending existing \ndimension  rows.\nComplementary Policy Accumulating Snapshot\nFinally,  before leaving policy transactions, you should consider the use of an accu-\nmulating snapshot to capture the cumulative eff ect of the transactions. In this \n",
      "content_length": 2489,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 421,
      "content": "Insurance 385\nscenario, the grain of the fact table likely would be one row for each coverage and \ncovered item on a policy. You can envision including policy-centric dates, such as \nquoted, rated, underwritten, eff ective, renewed, and expired. Likewise, multiple \nemployee roles could be included on the fact table for the agent and underwriter. \nMany of the other dimensions discussed would be applicable to this schema, with \nthe exception of the transaction type dimension. The accumulating snapshot likely \nwould have an expanded fact set.\nPolicy Transaction Date Key (FK)\nPolicy Effective Date Key (FK)\nPolicyholder Key (FK)\nEmployee Key (FK)\nCoverage Key (FK)\nCovered Item Key (FK)\nPolicy Transaction Type Key (FK)\nPolicy Transaction Audit Key (FK)\nPolicy Number (DD)\nPolicy Transaction Dollar Amount\nCoverage Key (PK)\nCoverage Description\nLine of Business Description\nLimit\nDeductible\nRental Car Coverage\nWindshield Coverage\nPolicy Transaction Fact\nAutomobile Coverage Dimension\nCovered Item Key (PK)\nCovered Item Description\nVehicle Manufacturer\nVehicle Make\nVehicle Year\nVehicle Classification\nVehicle Engine Size\nVehicle Appraised Value Range\nAutomobile Coverage Item Dimension\nFigure 16-3: Policy transaction schema with subtype automobile dimension tables.\nAs discussed in Chapter 4: Inventory, an accumulating snapshot is eff ective for \nrepresenting information about a pipeline process’s key milestones. It captures the \ncumulative lifespan of a policy, covered items, and coverages; however, it does not \nstore information about each and every transaction that occurred. Unusual trans-\nactional events or unexpected outliers from the standard pipeline would likely be \nmasked with an accumulating perspective. On the other hand, an accumulating \nsnapshot, sourced from the transactions, provides a clear picture of the durations \nor lag times between key process events.\n Premium Periodic Snapshot\nThe  policy transaction schema is useful for answering a wide range of questions. \nHowever, the blizzard of transactions makes it diffi  cult to quickly determine the \nstatus or ﬁ nancial value of an in-force policy at a given point in time. Even if all \nthe necessary detail lies in the transaction data, a snapshot perspective would \nrequire rolling the transactions forward from the beginning of history taking into \naccount complicated business rules for when earned revenue is recognized. Not \nonly is this nearly impractical on a single policy, but it is ridiculous to think about \ngenerating summary top line views of key performance metrics in this manner.\n",
      "content_length": 2581,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 422,
      "content": "Chapter 16\n386\nThe answer to this dilemma is to create a separate fact table that operates as a \ncompanion to the policy transaction table. In this case, the business process is the \nmonthly policy premium snapshot. The granularity of the fact table is one row per \ncoverage and covered item on a policy each  month.\n Conformed Dimensions\nOf  course, when designing the premium periodic snapshot table, you should strive to \nreuse as many dimensions from the policy transaction table as possible. Hopefully, \nyou have become a conformed dimension enthusiast by now. As described in Chapter \n4, conformed dimensions used in separate fact tables either must be identical or \nmust represent a shrunken subset of the attributes from the granular dimension.\nThe policyholder, covered item, and coverage dimensions would be identical. \nThe daily date dimension would be replaced with a conformed month dimension \ntable. You don’t need to track all the employees who were involved in policy trans-\nactions on a monthly basis; it may be useful to retain the involved agent, especially \nbecause ﬁ eld operations are so focused on ongoing revenue performance analysis. \nThe transaction type dimension would not be used because it does not apply at the \nperiodic snapshot granularity. Instead, you introduce a status dimension so users \ncan quickly discern the current state of a coverage or policy, such as new policies \nor cancellations this month and over  time.\n Conformed Facts\nWhile  we’re on the topic of conformity, you also need to use conformed facts. If the \nsame facts appear in multiple fact tables, such as facts common to this snapshot fact \ntable as well as the consolidated fact table we’ll discuss later in this chapter, then \nthey must have consistent deﬁ nitions and labels. If the facts are not identical, \nthen they need to be given diff erent names.\nPay-in-Advance Facts\nBusiness  management wants to know how much premium revenue was written (or \nsold) each month, as well as how much revenue was earned. Although a policyholder \nmay contract and pay for coverages on covered items for a period of time, the revenue \nis not earned until the service is provided. In the case of the insurance company, \nthe revenue from a policy is earned month by month as long as the policyholder \ndoesn’t cancel. The correct calculation of a metric like earned premium would \nmean fully replicating all the business rules of the operational revenue recognition \nsystem within the BI application. Typically, the rules for converting a transaction \namount into its monthly revenue impact are complex, especially with mid-month \ncoverage upgrades and downgrades. Fortunately, these metrics can be sourced from \na separate operational system.\n",
      "content_length": 2736,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 423,
      "content": "Insurance 387\nAs illustrated in Figure 16-4, we include two premium revenue metrics in the \nperiodic snapshot fact table to handle the diff erent deﬁ nitions of written versus \nearned premium. Simplistically, if an annual policy for a given coverage and cov-\nered item was written on January 1 for a cost of $600, then the written premium \nfor January would be $600, but the earned premium is $50 ($600 divided by 12 \nmonths). In February, the written premium is zero and the earned premium is still \n$50. If the policy is canceled on March 31, the earned premium for March is $50, \nwhile the written premium is a negative $450. Obviously, at this point the earned \nrevenue stream comes to a crashing halt.\nMonth End Snapshot Date Key (FK)\nPolicyholder Key (FK)\nAgent Key (FK)\nCoverage Key (FK)\nCovered Item Key (FK)\nPolicy Status Key (FK)\nPolicy Number (DD)\nWritten Premium Revenue Amount\nEarned Premium Revenue Amount\nPremium Snapshot Fact\nMonth End Dimension\nAgent Dimension\nPolicyholder Dimension\nCoverage Dimension\nPolicy Status Dimension\nCovered Item Dimension\nFigure 16-4: Periodic premium snapshot schema.\nPay-in-advance business scenarios typically require the combination of transac-\ntion and monthly snapshot fact tables to answer questions of transaction frequency \nand timing, as well as questions of earned income in a given month. You can almost \nnever add enough facts to a snapshot schema to do away with the need for a trans-\naction schema, or vice versa.\nHeterogeneous Supertypes and Subtypes Revisited\nWe  are again confronted with the need to look at the snapshot data with more spe-\nciﬁ c line-of-business attributes, and grapple with snapshot facts that vary by line of \nbusiness. Because the custom facts for each line are incompatible with each other, \nmost of the fact row would be ﬁ lled with nulls if you include all the line-of-business \nfacts on every row. In this scenario, the answer is to separate the monthly snap-\nshot fact table physically by line of business. You end up with the single supertype \nmonthly snapshot schema and a series of subtype snapshots, one for each line of \nbusiness or coverage type. Each of the subtype snapshot fact tables is a copy of a \nsegment of the supertype fact table for just those coverage keys and covered item \nkeys belonging to a particular line of business. We include the supertype facts as \na convenience so analyses within a coverage type can use both the supertype and \ncustom subtype facts without accessing two large fact tables. \n",
      "content_length": 2511,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 424,
      "content": "Chapter 16\n388\nMultivalued Dimensions Revisited\nAutomobile insurance provides another opportunity to discuss multivalued dimen-\nsions. Often multiple insured drivers are associated with a policy. You can construct \na bridge table, as illustrated in Figure 16-5, to capture the relationship between the \ninsured drivers and policy. In this case the insurance company can assign realistic \nweighting factors based on each driver’s share of the total premium cost.\nMonth End Snapshot Date Key (FK)\nPolicyholder Key (FK)\nMore FKs...\nPolicy Key (FK)\nWritten Premium Revenue Amount\nEarned Premium Revenue Amount\nPremium Snapshot Fact\nPolicy Key (FK)\nInsured Driver Key (FK)\nWeighting Factor\nPolicy-Insured Driver Bridge\nInsured Driver Key (PK)\nInsured Driver Name\nInsured Driver Address Attributes...\nInsured Driver Date of Birth\nInsured Driver Risk Segment\nInsured Driver Dimension\nFigure 16-5: Bridge table for multiple drivers on a policy.\nBecause these relationships may change over time, you can add eff ective and \nexpiration dates to the bridge table. Before you know it, you end up with a factless \nfact table to capture the evolving relationships between a policy, policy holder, \ncovered item, and insured driver over  time.\nMore Insurance Case Study Background\nUnfortunately, the insurance business has a downside. We learn from the inter-\nviewees that there’s more to life than collecting premium revenue payments. The \nmain costs in this industry result from claim losses. After a policy is in eff ect, then \na claim can be made against a speciﬁ c coverage and covered item. A claimant, who \nmay be the policyholder or a new party not previously known to the insurance \ncompany, makes the claim. When the insurance company opens a new claim, a \nreserve is usually established. The reserve is a preliminary estimate of the insurance \ncompany’s eventual liability for the claim. As further information becomes known, \nthis reserve can be adjusted.\nBefore the insurance company pays any claim, there is usually an investigative \nphase where the insurance company sends out an adjuster to examine the covered \nitem and interview the claimant, policyholder, or other individuals involved. The \ninvestigative phase produces a stream of task transactions. In complex claims, \n",
      "content_length": 2276,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 425,
      "content": "Insurance 389\nvarious outside experts may be required to pass judgment on the claim and the \nextent of the damage.\nIn most cases, after the investigative phase, the insurance company issues a \nnumber of payments. Many of these payments go to third parties such as doctors, \nlawyers, or automotive body shop operators. Some payments may go directly to \nthe claimant. It is important to clearly identify the employee responsible for every \npayment made against an open claim.\nThe insurance company may take possession of the covered item after replacing \nit for the policyholder or claimant. If the item has any remaining value, salvage pay-\nments received by the insurance company are a credit against the claim accounting.\nEventually, the payments are completed and the claim is closed. If nothing \nunusual happens, this is the end of the transaction stream generated by the claim. \nHowever, in some cases, further claim payments or claimant lawsuits may force \na claim to be reopened. An important measure for an insurance company is how \noften and under what circumstances claims are reopened.\nIn addition to analyzing the detailed claims processing transactions, the insur-\nance company also wants to understand what happens over the life of a claim. For \nexample, the time lag between the claim open date and the ﬁ rst payment date is an \nimportant measure of claims processing effi  ciency.\n Updated Insurance Bus Matrix\nWith  a better understanding of the claims side of the business, the draft matrix from \nFigure 16-1 needs to be revisited. Based on the new requirements, you add another \nrow to the matrix to accommodate claim transactions, as shown in Figure 16-6. \nMany of the dimensions identiﬁ ed earlier in the project will be reused; you add new \ncolumns to the matrix for the claim, claimant, and third-party payee.\nPolicy Transactions\nPremium Snapshot\nDate\nPolicyholder\nCovered Item\nEmployee\nX\nX\nX\nX\nX\nX\nX\nMonth\nX\nX\nAgent\nX\nX\nX\nPolicy\nCoverage\nClaimant\nClaim Transactions\nX\nX\nX\nX\nX\nX\nX\nX\nX\n3rd Party Payee\nClaim\nFigure 16-6: Updated insurance bus matrix.\n",
      "content_length": 2072,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 426,
      "content": "Chapter 16\n390\n Detailed Implementation Bus Matrix\nDW/BI  teams sometimes struggle with the level of detail captured in an enterprise \ndata warehouse bus matrix. In the planning phase of an architected DW/BI project, it \nmakes sense to stick with rather high-level business processes (or sources). Multiple \nfact tables at diff erent levels of granularity may result from each of these business \nprocess rows. In the subsequent implementation phase, you can take a subset of \nthe matrix to a lower level of detail by reﬂ ecting all the fact tables or OLAP cubes \nresulting from the process as separate matrix rows. At this point the matrix can \nbe enhanced by adding columns to reﬂ ect the granularity and metrics associated \nwith each fact table or cube. Figure 16-7 illustrates a more detailed implementation \nbus matrix.\nClaim Transactions\nThe operational claim processing system generates a slew of transactions, including \nthe following transaction task types:\n \n■Open claim, reopen claim, close claim\n \n■Set reserve, reset reserve, close reserve\n \n■Set salvage estimate, receive salvage payment\n \n■Adjuster inspection, adjuster interview\n \n■Open lawsuit, close lawsuit\n \n■Make payment, receive payment\n \n■Subrogate claim\nWhen updating the Figure 16-6 bus matrix, you determine that this schema uses \na number of dimensions developed for the policy world. You again have two role-\nplaying dates associated with the claim transactions. Unique column labels should \ndistinguish the claim transaction and eff ective dates from those associated with \npolicy transactions. The employee is the employee involved in the transactional \ntask. As mentioned in the business case, this is particularly interesting for payment \nauthorization transactions. The claim transaction type dimension would include \nthe transaction types and groupings just listed.\nAs shown in Figure 16-8, there are several new dimensions in the claim transac-\ntion fact table. The claimant is the party making the claim, typically an individual. \nThe third-party payee may be either an individual or commercial entity. Both the \nclaimant and payee dimensions usually are dirty dimensions because of the dif-\nﬁ culty of reliably identifying them across claims. Unscrupulous potential payees \nmay go out of their way not to identify themselves in a way that would easily tie \nthem to other claims in the insurance company’s  system.\n",
      "content_length": 2400,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 427,
      "content": "Policy Transactions\nCorporate Policy Transactions\nstc\na\nF\nytir\nalu\nn\na\nr\nG\ne\nb\nu\nC\n \nP\nA\nL\nO\n/elb\na\nT tc\na\nF\nAuto Policy Transactions\nHome Policy Transactions\nClaim Transactions\nClaim Workﬂow\nAccident Involvements\nPolicy Premium Snapshot\nCorporate Policy Premiums\nAuto Policy Premiums\nHome Policy Premiums\n1 row for every policy\ntransaction\n1 row per auto policy\ntransaction\n1 row per home policy\ntransaction\n1 row for every claim task\ntransaction\n1 row per claim\n1 row per loss party and\nafﬁliation on an auto claim\n1 row for every policy, covered\nitem and coverage per month\n1 row per auto policy, covered\nitem and coverage per month\n1 row per home policy, covered\nitem and coverage per month\nClaim Events\nPolicy Transaction Amount\nPolicy Transaction Amount\nPolicy Transaction Amount\nClaim Transaction Amount\nOriginal Reserve, Estimate, Current\nReserve, Claim Paid, Salvage\nCollected, and Subro Collected\nAmounts; Loss to Open, Open to\nEstimate, Open to 1st Payment,\nOpen to Subro, and Open to Closed\nLags; # of Transactions\nAccident Involvement Count\nWritten Premium Revenue and\nEarned Premium Revenue Amounts\nWritten Premium Revenue and\nEarned Premium Revenue Amounts\nWritten Premium Revenue and\nEarned Premium Revenue Amounts\nTrxn\nEff\nTrxn\nEff\nTrxn\nEff\nTrxn\nEff\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nAuto\nHome\nX\nX\nAuto\nX\nAuto\nHome\nX\nAuto\nHome\nX\nX\nAuto\nX\nAuto\nHome\nX\nX\nX\nX\nAgent\nAgent\nAgent\nAgent\nX\nX\nX\nX\nX\nX\nX\nX\nAuto\nX\nX\nX\nX\nX\nX\nX\nDate\nPolicyholder\nCoverage\nEmployee\nPolicy\nCovered Item\nClaimant\n3rd Party Payee\nClaim\nFigure 16-7: Detailed implementation bus matrix.\n",
      "content_length": 1564,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 428,
      "content": "Chapter 16\n392\nClaim Transaction Date Key (FK)\nClaim Transaction Effective Date Key (FK)\nPolicyholder Key (FK)\nClaim Transaction Employee Key (FK)\nAgent Key (FK)\nCoverage Key (FK)\nCovered Item Key (FK)\nClaimant Key (FK)\n3rd Party Payee Key (FK)\nClaim Transaction Type Key (FK)\nClaim Profile Key (FK)\nClaim Key (FK)\nPolicy Number (DD)\nClaim Transaction Number (DD)\nClaim Transaction Dollar Amount\nClaim Transaction Fact\nEmployee Dimension (2 views for roles)\nCovered Item Dimension\nDate Dimension (2 views for roles)\nPolicyholder Dimension\nCoverage Dimension\nClaimant Dimension\nClaim Transaction Type Dimension\n3rd Party Payee Dimension\nClaim Profile Dimension\nClaim Dimension\nFigure 16-8: Claim transaction schema.\n Transaction Versus Proﬁ le Junk Dimensions\nBeyond the reused dimensions from the policy-centric schemas and the new claim-\ncentric dimensions just listed, there are a large number of indicators and descriptions \nrelated to a claim. Designers are sometimes tempted to dump all these descriptive \nattributes into a claim dimension. This approach makes sense for high-cardinality \ndescriptors, such as the speciﬁ c address where the loss occurred or a narrative \ndescribing the event. However, in general, you should avoid creating dimensions \nwith the same number of rows as the fact table.\nAs we described in Chapter 6, low-cardinality codiﬁ ed data, like the method \nused to report the loss or an indicator denoting whether the claim resulted from a \ncatastrophic event, are better handled in a junk dimension. In this case, the junk \ndimension would more appropriately be referred to as the claim proﬁ le dimension \nwith one row per unique combination of proﬁ le attributes. Grouping or ﬁ ltering on \nthe proﬁ le attributes would yield faster query responses than if they were alterna-\ntively handled as claim dimension  attributes.\n Claim Accumulating Snapshot\nEven with a robust transaction schema, there is a whole class of urgent business \nquestions that can’t be answered using only transaction detail. It is diffi  cult to \nderive claim-to-date performance measures by traversing through every detailed \nclaim task transaction from the beginning of the claim’s history and appropriately \napplying the transactions.\n",
      "content_length": 2238,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 429,
      "content": "Insurance 393\nOn a periodic basis, perhaps at the close of each day, you can roll forward all the \ntransactions to update an accumulating claim snapshot incrementally. The granu-\nlarity is one row per claim; the row is created once when the claim is opened and \nthen is updated throughout the life of a claim until it is ﬁ nally closed.\nMany of the dimensions are reusable, conformed dimensions, as illustrated in \nFigure 16-9. You should include more dates in this fact table to track the key claim \nmilestones and deliver time lags. These lags may be the raw diff erence between \ntwo dates, or they may be calculated in a more sophisticated way by accounting for \nonly workdays in the calculations. A status dimension is added to quickly identify \nall open, closed, or reopened claims, for example. Transaction-speciﬁ c dimensions \nsuch as employee, payee, and claim transaction type are suppressed, whereas the \nlist of additive, numeric measures has been  expanded.\nClaim Workflow Fact\nCovered Item Dimension\nPolicyholder Dimension\nCoverage Dimension\nClaimant Dimension\nClaim Status Dimension\nClaim Profile Dimension\nClaim Dimension\nClaim Open Date Key (FK)\nClaim Loss Date Key (FK)\nClaim Estimate Date Key (FK)\nClaim 1st Payment Date Key (FK)\nClaim Most Recent Payment Date Key (FK)\nClaim Subrogation Date Key (FK)\nClaim Close Date Key (FK)\nPolicyholder Key (FK)\nClaim Supervisor Key (FK)\nAgent Key (FK)\nCoverage Key (FK)\nCovered Item Key (FK)\nClaimant Key (FK)\nClaim Status Key (FK)\nClaim Profile Key (FK)\nClaim Key (FK)\nPolicy Number (DD)\nOriginal Reserve Dollar Amount\nEstimate Dollar Amount\nCurrent Reserve to Date Dollar Amount\nClaim Paid to Date Dollar Amount\nSalvage Collected to Date Dollar Amount\nSubro Payment Collected to Date Dollar Amount\nClaim Loss to Open Lag\nClaim Open to Estimate Lag\nClaim Open to 1st Payment Lag\nClaim Open to Subrogation Lag\nClaim Open to Closed Lag\nNumber of Claim Transactions\nEmployee Dimension (2 views for roles)\nDate Dimension (7 views for roles)\nFigure 16-9: Claim accumulating snapshot schema.\n Accumulating Snapshot for Complex Workﬂ ows\nAccumulating  snapshot fact tables are typically appropriate for predictable workﬂ ows \nwith well-established milestones. They usually have ﬁ ve to 10 key milestone dates \n",
      "content_length": 2261,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 430,
      "content": "Chapter 16\n394\nrepresenting the pipeline’s start, completion, and key events in between. However, \nsometimes workﬂ ows are less predictable. They still have a deﬁ nite start and end \ndate, but the milestones in between are numerous and less stable. Some occurrences \nmay skip over some intermediate milestones, but there’s no reliable pattern.\nIn this situation, the ﬁ rst task is to identify the key dates that link to role-playing \ndate dimensions. These dates represent the most important milestones. The start and \nend dates for the process would certainly qualify; in addition, you should consider \nother commonly occurring critical milestones. These dates (and their associated \ndimensions) will be used extensively for BI application ﬁ ltering.\nHowever, if the number of additional milestones is both voluminous and unpre-\ndictable, they can’t all be handled as additional date foreign keys in the fact table. \nTypically, business users are more interested in the lags between these milestones, \nrather than ﬁ ltering or grouping on the dates themselves. If there were a total of \n20 potential milestone events, there would be 190 potential lag durations: event \nA-to-B, A-to-C, … (19 possible lags from event A), B-to-C, … (18 possible lags from \nevent B), and so on. Instead of physically storing 190 lag metrics, you can get away \nwith just storing 19 of them and then calculate the others. Because every pipeline \noccurrence starts by passing through milestone A, which is the workﬂ ow begin \ndate, you could store all 19 lags from the anchor event A and then calculate the \nother variations. For example, if you want to know the lag from B-to-C, take \nthe A-to-C lag value and subtract the A-to-B lag. If there happens to be a null for one \nof the lags involved in a calculation, then the result also needs to be null because \none of the events never occurred. But such a null result is handled gracefully if you \nare counting or averaging that lag across a number of claim rows.\n Timespan Accumulating Snapshot\nAn  accumulating snapshot does a great job presenting a workﬂ ow’s current state, \nbut it obliterates the intermediate states. For example, a claim can move in and out \nof various states such as opened, denied, closed, disputed, opened again, and closed \nagain. The claim transaction fact table will have separate rows for each of these \nevents, but as discussed earlier, it doesn’t accumulate metrics across transactions; \ntrying to re-create the evolution of a workﬂ ow from these transactional events would \nbe a nightmare. Meanwhile, a classic accumulating snapshot doesn’t allow you to \nre-create the claim workﬂ ow at any arbitrary date in the past.\nAlternatively, you could add eff ective and expiration dates to the accumulating \nsnapshot. In this scenario, instead of destructively updating each row as changes \noccur, you add a new row that preserves the state of a claim for a span of time. \n",
      "content_length": 2927,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 431,
      "content": "Insurance 395\nSimilar to a type 2 slowly changing dimension, the fact row includes the following \nadditional columns:\n \n■Snapshot start date\n \n■Snapshot end date (updated when a new row for a given claim is added)\n \n■Snapshot current ﬂ ag (updated when a new row is added)\nMost users are only interested in the current view provided by a classic accumu-\nlating snapshot; you can meet their needs by deﬁ ning a view that ﬁ lters the historical \nsnapshot rows based on the current ﬂ ag. The minority of users and reports who \nneed to look at the pipeline as of any arbitrary date in the past can do so by ﬁ ltering \non the snapshot start and end dates.\nThe timespan accumulating snapshot fact table is more complicated to main-\ntain than a standard accumulating snapshot, but the logic is similar. Where the \nclassic accumulating snapshot updates a row, the timespan snapshot updates \nthe administrative columns on the row formerly known as current, and inserts \na new row.\nPeriodic Instead of Accumulating Snapshot \nIn  cases where a claim is not so short-lived, such as with long-term disability or \nbodily injury claims that have a multiyear life span, you may represent the snapshot \nas a periodic snapshot rather than an accumulating snapshot. The grain of the peri-\nodic snapshot would be one row for every active claim at a regular snapshot interval, \nsuch as monthly. The facts would represent numeric, additive facts that occurred \nduring the period such as amount claimed, amount paid, and change in reserve. \n Policy/Claim Consolidated Periodic Snapshot\nWith  the fact tables designed thus far, you can deliver a robust perspective of the pol-\nicy and claim transactions, in addition to snapshots from both processes. However, \nthe business users are also interested in proﬁ t metrics. Although premium revenue \nand claim loss ﬁ nancial metrics could be derived by separately querying two fact \ntables and then combining the results set, you opt to go the next step in the spirit \nof ease of use and performance for this common drill-across requirement. \nYou can construct another fact table that brings together the premium revenue \nand claim loss metrics, as shown in Figure 16-10. This table has a reduced set \nof dimensions corresponding to the lowest level of granularity common to both \n",
      "content_length": 2302,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 432,
      "content": "Chapter 16\n396\nprocesses. As discussed in Chapter 7: Accounting, this is a consolidated fact table \nbecause it combines data from multiple business processes. It is best to develop \nconsolidated fact tables after the base metrics have been delivered in separate atomic \ndimensional models.\nConsolidated Premium/Loss Fact\nMonth End Date Dimension\nPolicyholder Dimension\nCoverage Dimension\nPolicy Status Dimension\nCovered Item Dimension\nClaim Status Dimension\nAgent Dimension\nMonth End Snapshot Date Key (FK)\nPolicyholder Key (FK)\nCoverage Key (FK)\nCovered Item Key (FK)\nAgent Key (FK)\nPolicy Status Key (FK)\nClaim Status Key (FK)\nPolicy Number (DD)\nWritten Premium Revenue Dollar Amount\nEarned Premium Revenue Dollar Amount\nClaim Paid Dollar Amount\nClaim Collected Dollar Amount\nFigure 16-10: Policy/claim consolidated fact table.\n Factless Accident Events\nWe  earlier described factless fact tables as the collision of keys at a point in space \nand time. In the case of an automobile insurer, you can record literal collisions using \na factless fact table. In this situation, the fact table registers the many-to-many cor-\nrelations between the loss parties and loss items, or put in laymen’s terms, all the \ncorrelations between the people and vehicles involved in an accident.\nTwo new dimensions appear in the factless fact table shown in Figure 16-11. The \nloss party captures the individuals involved in the accident, whereas the loss party \nrole identiﬁ es them as passengers, witnesses, legal representation, or some other \ncapacity. As we did in Chapter 3: Retail Sales, we include a fact that is always valued \nat 1 to facilitate counting and aggregation. This factless fact table can represent \ncomplex accidents involving many individuals and vehicles because the number \nof involved parties with various roles is open-ended. When there is more than one \nclaimant or loss party associated with an accident, you can optionally treat these \ndimensions as multivalued dimensions using claimant group and loss party group \nbridge tables. This has the advantage that the grain of the fact table is preserved \nas one record per accident claim. Either schema variation could answer questions \nsuch as, “How many bodily injury claims did you handle where ABC Legal Partners \nrepresented the claimant and EZ-Dent-B-Gone body shop performed the  repair?”\n",
      "content_length": 2355,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 433,
      "content": "Insurance 397\nAccident Involvement Fact\nPolicyholder Dimension\nCoverage Dimension\nLoss Party Dimension\nCovered Item Dimension\nClaim Loss Date Dimension\nLoss Party Role Dimension\nClaim Loss Date Key (FK)\nPolicyholder Key (FK)\nCoverage Key (FK)\nCovered Item Key (FK)\nClaimant Key (FK)\nLoss Party Key (FK)\nLoss Party Role Key (FK)\nClaim Profile Key (FK)\nClaim Number (DD)\nPolicy Number (DD)\nAccident Involvement Count (=1)\nClaimant Dimension\nClaim Profile Dimension\nFigure 16-11: Factless fact table for accident involvements.\nCommon Dimensional Modeling Mistakes \nto Avoid\nAs  we close this ﬁ nal chapter on dimensional modeling techniques, we thought it \nwould be helpful to establish boundaries beyond which designers should not go. \nThus far in this book, we’ve presented concepts by positively stating dimensional \nmodeling best practices. Now rather than reiterating the to-dos, we focus on not-\nto-dos by elaborating on dimensional modeling techniques that should be avoided. \nWe’ve listed the not-to-dos in reverse order of importance; be aware, however, that \neven the less important mistakes can seriously compromise your DW/BI system.\nMistake 10: Place Text Attributes in a Fact Table\nThe process of creating a dimensional model is always a kind of triage. The numeric \nmeasurements delivered from an operational business process source belong in the \nfact table. The descriptive textual attributes comprising the context of the mea-\nsurements go in dimension tables. In nearly every case, if an attribute is used for \nconstraining and grouping, it belongs in a dimension table. Finally, you should make \na ﬁ eld-by-ﬁ eld decision about the leftover codes and pseudo-numeric items, placing \nthem in the fact table if they are more like measurements and used in calculations \nor in a dimension table if they are more like descriptions used for ﬁ ltering and \nlabeling. Don’t lose your nerve and leave true text, especially comment ﬁ elds, in \nthe fact table. You need to get these text attributes off  the main runway of the data \nwarehouse and into dimension tables.\n",
      "content_length": 2075,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 434,
      "content": "Chapter 16\n398\nMistake 9: Limit Verbose Descriptors to Save Space\nYou might think you are being a conservative designer by keeping the size of the \ndimensions under control. However, in virtually every data warehouse, the dimen-\nsion tables are geometrically smaller than the fact tables. Having a 100 MB product \ndimension table is insigniﬁ cant if the fact table is one hundred or thousand times \nas large! Our job as designers of easy-to-use dimensional models is to supply as \nmuch verbose descriptive context in each dimension as possible. Make sure every \ncode is augmented with readable descriptive text. Remember the textual attributes \nin the dimension tables provide the browsing, constraining, or ﬁ ltering parameters \nin BI applications, as well as the content for the row and column headers in reports.\nMistake 8: Split Hierarchies into Multiple Dimensions\nA hierarchy is a cascaded series of many-to-one relationships. For example, many \nproducts roll up to a single brand; many brands roll up to a single category. If a \ndimension is expressed at the lowest level of granularity, such as product, then all \nthe higher levels of the hierarchy can be expressed as unique values in the product \nrow. Business users understand hierarchies. Your job is to present the hierarchies \nin the most natural and effi  cient manner in the eyes of the users, not in the eyes of \na data modeler who has focused his entire career on designing third normal form \nentity-relationship models for transaction processing systems.\nA ﬁ xed depth hierarchy belongs together in a single physical ﬂ at dimension \ntable, unless data volumes or velocity of change dictate otherwise. Resist the urge \nto snowﬂ ake a hierarchy by generating a set of progressively smaller subdimen-\nsion tables. Finally, if more than one rollup exists simultaneously for a dimension, \nin most cases it’s perfectly reasonable to include multiple hierarchies in the same \ndimension as long as the dimension has been deﬁ ned at the lowest possible grain \n(and the hierarchies are uniquely labeled).\nMistake 7: Ignore the Need to Track Dimension \nChanges\nContrary to popular belief, business users often want to understand the impact of \nchanges on at least a subset of the dimension tables’ attributes. It is unlikely users \nwill settle for dimension tables with attributes that always reﬂ ect the current state \nof the world. Three basic techniques track slowly moving attribute changes; don’t \nrely on type 1 exclusively. Likewise, if a group of attributes changes rapidly, you \ncan split a dimension to capture the more volatile attributes in a mini-dimension. \n",
      "content_length": 2630,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 435,
      "content": "Insurance 399\nMistake 6: Solve All Performance Problems with \nMore Hardware\nAggregates, or derived summary tables, are a cost-eff ective way to improve query \nperformance. Most BI tool vendors have explicit support for the use of aggregates. \nAdding expensive hardware should be done as part of a balanced program that \nincludes building aggregates, partitioning, creating indices, choosing query-effi  cient \nDBMS software, increasing real memory size, increasing CPU speed, and adding \nparallelism at the hardware level.\nMistake 5: Use Operational Keys to Join Dimensions \nand Facts\nNovice designers are sometimes too literal minded when designing the dimension \ntables’ primary keys that connect to the fact tables’ foreign keys. It is counterpro-\nductive to declare a suite of dimension attributes as the dimension table key and \nthen use them all as the basis of the physical join to the fact table. This includes \nthe unfortunate practice of declaring the dimension key to be the operational key, \nalong with an eff ective date. All types of ugly problems will eventually arise. The \ndimension’s operational or intelligent key should be replaced with a simple integer \nsurrogate key that is sequentially numbered from 1 to N, where N is the total number \nof rows in the dimension table. The date dimension is the sole exception to this rule.\nMistake 4: Neglect to Declare and Comply with the \nFact Grain\nAll dimensional designs should begin by articulating the business process that \ngenerates the numeric performance measurements. Second, the exact granularity \nof that data must be speciﬁ ed. Building fact tables at the most atomic, granular level \nwill gracefully resist the ad hoc attack. Third, surround these measurements with \ndimensions that are true to that grain. Staying true to the grain is a crucial step in \nthe design of a dimensional model. A subtle but serious design error is to add helpful \nfacts to a fact table, such as rows that describe totals for an extended timespan or a \nlarge geographic area. Although these extra facts are well known at the time of the \nindividual measurement and would seem to make some BI applications simpler, they \ncause havoc because all the automatic summations across dimensions overcount \nthese higher-level facts, producing incorrect results. Each diff erent measurement \ngrain demands its own fact table.\n",
      "content_length": 2368,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 436,
      "content": "Chapter 16\n400\nMistake 3: Use a Report to Design the Dimensional \nModel\nA dimensional model has nothing to do with an intended report! Rather, it is a model \nof a measurement process. Numeric measurements form the basis of fact tables. \nThe dimensions appropriate for a given fact table are the context that describes the \ncircumstances of the measurements. A dimensional model is based solidly on \nthe physics of a measurement process and is quite independent of how a user chooses \nto deﬁ ne a report. A project team once confessed it had built several hundred fact \ntables to deliver order management data to its business users. It turned out each \nfact table had been constructed to address a speciﬁ c report request; the same data \nwas extracted many, many times to populate all these slightly diff erent fact tables. \nNot surprising, the team was struggling to update the databases within the nightly \nbatch window. Rather than designing a quagmire of report-centric schemas, the \nteam should have focused on the measurement processes. The users’ requirements \ncould have been handled with a well-designed schema for the atomic data along \nwith a handful (not hundreds) of performance-enhancing aggregations.\nMistake 2: Expect Users to Query Normalized \nAtomic Data\nThe lowest level data is always the most dimensional and should be the foundation of \na dimensional design. Data that has been aggregated in any way has been deprived \nof some of its dimensions. You can’t build a dimensional model with aggregated data \nand expect users and their BI tools to seamlessly drill down to third normal form \n(3NF) data for the atomic details. Normalized models may be helpful for preparing \nthe data in the ETL kitchen, but they should never be used for presenting the data \nto business users.\nMistake 1: Fail to Conform Facts and Dimensions \nThis ﬁ nal not-to-do should be presented as two separate mistakes because they \nare both so dangerous to a successful DW/BI design, but we’ve run out of mistake \nnumbers to assign, so they’re lumped into one.\nIt would be a shame to get this far and then build isolated data repository stove-\npipes. We refer to this as snatching defeat from the jaws of victory. If you have a \nnumeric measured fact, such as revenue, in two or more databases sourced from \ndiff erent underlying systems, then you need to take special care to ensure the techni-\ncal deﬁ nitions of these facts exactly match. If the deﬁ nitions do not exactly match, \nthen they shouldn’t both be referred to as revenue. This is conforming the facts.\n",
      "content_length": 2557,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 437,
      "content": "Insurance 401\nFinally, the single most important design technique in the dimensional modeling \narsenal is conforming dimensions. If two or more fact tables are associated with the \nsame dimension, you must be fanatical about making these dimensions identical or \ncarefully chosen subsets of each other. When you conform dimensions across fact \ntables, you can drill across separate data sources because the constraints and row \nheaders mean the same thing and match at the data level. Conformed dimensions \nare the secret sauce needed for building distributed DW/BI environments, adding \nunexpected new data sources to an existing warehouse, and making multiple incom-\npatible technologies function together harmoniously. Conformed dimensions also \nallow teams to be more agile because they’re not re-creating the wheel repeatedly; \nthis translates into a faster delivery of value to the business community.\nSummary\nIn this ﬁ nal case study we designed a series of insurance dimensional models rep-\nresenting the culmination of many important concepts developed throughout this \nbook. Hopefully, you now feel comfortable and conﬁ dent using the vocabulary and \ntools of a dimensional modeler.\nWith dimensional modeling mastered, in the next chapter we discuss all the other \nactivities that occur during the life of a successful DW/BI project. Before you go forth \nto be dimensional, it’s useful to have this holistic perspective and understanding, \neven if your job focus is limited to modeling.\n",
      "content_length": 1497,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 439,
      "content": "Kimball DW/BI \nLifecycle Overview\nT\nhe gears shift rather dramatically in this chapter. Rather than focusing on \nKimball dimensional modeling techniques, we turn your attention to every-\nthing else that occurs during the course of a data warehouse/business intelligence \ndesign and implementation project. In this chapter, we’ll cover the life of a DW/BI \nproject from inception through ongoing maintenance, identifying best practices at \neach step, as well as potential vulnerabilities. More comprehensive coverage of the \nKimball Lifecycle is available in The Data Warehouse Lifecycle Toolkit, Second Edition \nby Ralph Kimball, Margy Ross, Warren Thornthwaite, Joy Mundy, and Bob Becker \n(Wiley, 2008). This chapter is a crash course drawn from the complete text, which \nweighs in at a hefty 600+ pages.\nYou may perceive this chapter’s content is only applicable to DW/BI project \nmanagers, but we feel diff erently. Implementing a DW/BI system requires tightly \nintegrated activities. We believe everyone on the project team, including the ana-\nlysts, architects, designers, and developers, needs a high-level understanding of the \ncomplete Lifecycle.\nThis chapter provides an overview of the entire Kimball Lifecycle approach; spe-\nciﬁ c recommendations regarding dimensional modeling and ETL tasks are deferred \nuntil subsequent chapters. We will dive into the collaborative modeling workshop \nprocess in Chapter 18: Dimensional Modeling Process and Tasks, then make a simi-\nlar plunge into ETL activities in Chapter 20: ETL System Design and Development \nProcess and Tasks.\nChapter 17 covers the following concepts:\n \n■Kimball Lifecycle orientation\n \n■DW/BI program/project planning and ongoing management\n \n■Tactics for collecting business requirements, including prioritization\n \n■Process for developing the technical architecture and selecting products\n17\n",
      "content_length": 1865,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 440,
      "content": "Chapter 17\n404\n \n■Physical design considerations, including aggregation and indexing\n \n■BI application design and development activities\n \n■Recommendations for deployment, ongoing maintenance, and future growth\nLifecycle Roadmap\nWhen  driving to a place we’ve never been to before, most of us rely on a roadmap, \nalbeit displayed via a GPS. Similarly, a roadmap is extremely useful if we’re about \nto embark on the unfamiliar journey of data warehousing and business intelligence. \nThe authors of The Data Warehouse Lifecycle Toolkit drew on decades of experience \nto develop the Kimball Lifecycle approach. When we ﬁ rst introduced the Lifecycle \nin 1998, we referred to it as the Business Dimensional Lifecycle, a name that rein-\nforced our key tenets for data warehouse success: Focus on the business’s needs, \npresent dimensionally structured data to users, and tackle manageable, iterative \nprojects. In the 1990s, we were one of the few organizations emphasizing these \ncore principles, so the moniker diff erentiated our methods from others. We are still \nvery ﬁ rmly wed to these principles, which have since become generally-accepted \nindustry best practices, but we renamed our approach to be the Kimball Lifecycle \nbecause that’s how most people refer to it.\nThe  overall Kimball Lifecycle approach is encapsulated in Figure 17-1. The dia-\ngram illustrates task sequence, dependency, and concurrency. It serves as a roadmap \nto help teams do the right thing at the right time. The diagram does not reﬂ ect an \nabsolute timeline; although the boxes are equally wide, there’s a vast diff erence in \nthe time and eff ort required for each major activity.\nProgram/\nProject\nPlanning\nProgram/Project Management\nBusiness\nRequirements\nDefinition\nTechnical\nArchitecture\nDesign\nPhysical\nDesign\nBI\nApplication\nDesign\nBI\nApplication\nDevelopment\nETL\nDesign &\nDevelopment\nDeployment\nMaintenance\nDimensional\nModeling\nProduct\nSelection &\nInstallation\nGrowth\nFigure 17-1: Kimball Lifecycle diagram.\n",
      "content_length": 1993,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 441,
      "content": "Kimball DW/BI Lifecycle Overview 405\nNOTE \nGiven the recent industry focus on agile methodologies, we want to \nremind readers about the discussion of the topic in Chapter 1: Data Warehousing, \nBusiness Intelligence, and Dimensional Modeling Primer. The Kimball Lifecycle \napproach and agile methodologies share some common doctrines: Focus on busi-\nness value, collaborate with the business, and develop incrementally. However, \nwe also feel strongly that DW/BI system design and development needs to be built \non a solid data architecture and governance foundation, driven by the bus archi-\ntecture. We also believe most situations warrant the bundling of multiple agile \n“deliverables” into a more full-function release before being broadly deployed to \nthe general business community.\nRoadmap Mile Markers\nBefore  diving into speciﬁ cs, take a moment to orient yourself to the roadmap. The \nLifecycle begins with program/project planning, as you would expect. This module \nassesses the organization’s readiness for a DW/BI initiative, establishes the prelimi-\nnary scope and justiﬁ cation, obtains resources, and launches the program/project. \nOngoing project management serves as a foundation to keep the remaining activi-\nties on track.\nThe  second major task in Figure 17-1 focuses on business requirements deﬁ ni-\ntion. There’s a two-way arrow between program/project planning and the business \nrequirements deﬁ nition due to the interplay between these activities. Aligning the \nDW/BI initiative with business requirements is absolutely crucial. Best-of-breed \ntechnologies won’t salvage a DW/BI environment that fails to focus on the business. \nBusiness users and their requirements have an impact on almost every design and \nimplementation decision made during the course of a DW/BI project. In Figure 17-1’s \nroadmap, this is reﬂ ected by the three parallel tracks that follow.\nThe  top track of Figure 17-1 deals with technology. Technical architecture design \nestablishes the overall framework to support the integration of multiple technolo-\ngies. Using the capabilities identiﬁ ed in the architecture design as a shopping list, \nyou then evaluate and select speciﬁ c products. Notice that product selection is not \nthe ﬁ rst box on the roadmap. One of the most frequent mistakes made by novice \nteams is to select products without a clear understanding of what they’re trying to \naccomplish. This is akin to grabbing a hammer whether you need to pound a nail \nor tighten a screw.\nThe  middle track emanating from business requirements deﬁ nition focuses on \ndata. It begins by translating the requirements into a dimensional model, as we’ve \nbeen practicing. The dimensional model is then transformed into a physical struc-\nture. The focus is on performance tuning strategies, such as aggregation, indexing, \nand partitioning, during the physical design. Last but not least, the ETL system is \n",
      "content_length": 2909,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 442,
      "content": "Chapter 17\n406\ndesigned and developed. As mentioned earlier, the equally sized boxes don’t rep-\nresent equally sized eff orts; this becomes obvious with the workload diff erential \nbetween the physical design and the demanding ETL-centric activities.\nThe  ﬁ nal set of tasks spawned by the business requirements is the design and \ndevelopment of the BI applications. The DW/BI project isn’t done when you deliver \ndata. BI applications, in the form of parameter-driven templates and analyses, will \nsatisfy a large percentage of the business users’ analytic needs.\nThe technology, data, and BI application tracks, along with a healthy dose of \neducation and support, converge for a well-orchestrated deployment. From there, \non-going maintenance is needed to ensure the DW/BI system remains healthy. \nFinally, you handle future growth by initiating subsequent projects, each return-\ning to the beginning of the Lifecycle all over again.\nNow that you have a high-level understanding of the overall roadmap, we’ll \ndescribe each of the boxes in Figure 17-1 in more detail.\nLifecycle Launch Activities\nThe following sections outline best practices, and pitfalls to avoid, as you launch \na DW/BI project.\nProgram/Project Planning and Management\nNot surprisingly, the DW/BI initiative begins with a series of program and project \nplanning activities. \nAssessing Readiness\nBefore  moving ahead with a DW/BI eff ort, it is prudent to take a moment to assess \nthe organization’s readiness to proceed. Based on our cumulative experience from \nhundreds of client engagements, three factors diff erentiate projects that were pre-\ndominantly smooth sailing versus those that entailed a constant struggle. These \nfactors are leading indicators of DW/BI success; we’ll describe the characteristics \nin rank order of importance.\nThe  most critical readiness factor is to have a strong executive business sponsor. \nBusiness sponsors should have a clear vision for the DW/BI system’s potential impact \non the organization. Optimally, business sponsors have a track record of success \nwith other internal initiatives. They should be politically astute leaders who can \nconvince their peers to support the eff ort. It’s a much riskier scenario if the chief \ninformation offi  cer (CIO) is the designated sponsor; we much prefer visible com-\nmitment from a business partner-in-crime instead.\n",
      "content_length": 2372,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 443,
      "content": "Kimball DW/BI Lifecycle Overview 407\nThe  second readiness factor is having a strong, compelling business motivation for \ntackling the DW/BI initiative. This factor often goes hand in hand with sponsorship. \nThe DW/BI project needs to solve critical business problems to garner the resources \nrequired for a successful launch and healthy lifespan. Compelling motivation typi-\ncally creates a sense of urgency, whether the motivation is from external sources, \nsuch as competitive factors, or internal sources, such as the inability to analyze \ncross-organization performance following acquisitions.\nThe  third factor when assessing readiness is feasibility. There are several aspects \nof feasibility, including technical and resource feasibility, but data feasibility is the \nmost crucial. Are you collecting real data in real operational source systems to sup-\nport the business requirements? Data feasibility is a major concern because there \nis no short-term ﬁ x if you’re not already collecting reasonably clean source data at \nthe right granularity.\nScoping and Justiﬁ cation\nWhen  you’re comfortable with the organization’s readiness, it’s time to put boundar-\nies around an initial project. Scoping requires the joint input of the IT organization \nand business management. The scope of a DW/BI project should be both mean-\ningful to the business organization and manageable for the IT organization. You \nshould initially tackle projects that focus on data from a single business process; \nsave the more challenging, cross-process projects for a later phase. Remember to \navoid the Law of Too when scoping—too brief of a timeline for a project with too \nmany source systems and too many users in too many locations with too diverse \nanalytic requirements.\nJustiﬁ cation requires an estimation of the beneﬁ ts and costs associated with the \nDW/BI initiative. Hopefully, the anticipated beneﬁ ts grossly outweigh the costs. IT \nusually is responsible for deriving the expenses. DW/BI systems tend to expand \nrapidly, so be sure the estimates allow room for short-term growth. Unlike opera-\ntional system development where resource requirements tail off  after production, \nongoing DW/BI support needs will not decline appreciably over time.\nThe  business community should have prime responsibility for determining the \nanticipated ﬁ nancial beneﬁ ts. DW/BI environments typically are justiﬁ ed based on \nincreased revenue or proﬁ t opportunities rather than merely focusing on expense \nreduction. Delivering “a single version of the truth” or “ﬂ exible access to information” \nisn’t suffi  cient ﬁ nancial justiﬁ cation. You need to peel back the layers to determine \nthe quantiﬁ able impact of improved decision making made possible by these sound \nbites. If you are struggling with justiﬁ cation, this is likely a symptom that the initia-\ntive is focused on the wrong business sponsor or  problem.\n",
      "content_length": 2904,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 444,
      "content": "Chapter 17\n408\nStafﬁ ng\nDW/BI  projects require the integration of a cross-functional team with resources \nfrom both the business and IT communities. It is common for the same person to \nﬁ ll multiple roles on the team; the assignment of named resources to roles depends \non the project’s magnitude and scope, as well as the individual’s availability, capac-\nity, and experience.\nFrom the business side of the house, we’ll need representatives to ﬁ ll the fol-\nlowing roles:\n \n■Business sponsor.  The sponsor is the DW/BI system’s ultimate client, as well \nas its strongest advocate. Sponsorship sometimes takes the form of an execu-\ntive steering committee, especially for cross-enterprise initiatives.\n \n■Business driver.  In a large organization, the sponsor may be too far removed \nor inaccessible to the project team. In this case, the sponsor sometimes del-\negates less strategic DW/BI responsibilities to a middle manager in the orga-\nnization. This driver should possess the same characteristics as the sponsor.\n \n■Business lead.  The business project lead is a well-respected person who is \nhighly involved in the project, communicating with the project manager on \na daily basis. Sometimes the business driver ﬁ lls this role.\n \n■Business users.  Optimally, the business users are the enthusiastic fans of the \nDW/BI environment. You need to involve them early and often, beginning \nwith the project scope and business requirements. From there, you must \nﬁ nd creative ways to maintain their interest and involvement throughout the \nproject. Remember, business user involvement is critical to DW/BI acceptance. \nWithout business users, the DW/BI system is a technical exercise in futility.\nSeveral  positions are staff ed from either the business or IT organizations. These \nstraddlers can be technical resources who understand the business or business \nresources who understand technology:\n \n■Business analyst.  This person is responsible for determining the business \nneeds and translating them into architectural, data, and BI application \nrequirements.\n \n■Data steward.  This subject matter expert is often the current go-to resource \nfor ad hoc analysis. They understand what the data means, how it is used, \nand where data inconsistencies are lurking. Given the need for organizational \nconsensus around core dimensional data, this can be a politically challenging \nrole, as we described in Chapter 4: Inventory.\n \n■BI application designer/developer.  BI application resources are responsible \nfor designing and developing the starter set of analytic templates, as well as \nproviding ongoing BI application support.\n",
      "content_length": 2632,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 445,
      "content": "Kimball DW/BI Lifecycle Overview 409\nThe following roles are typically staff ed from the IT organization:\n \n■Project manager.  The project manager is a critical position. This person \nshould be comfortable with and respected by business executives, as well \nas technical resources. The project manager’s communication and project \nmanagement skills must be stellar.\n \n■Technical architect.  The architect is responsible for the overall technical \narchitecture. This person develops the plan that ties together the required \ntechnical functionality and helps evaluate products on the basis of the overall \narchitecture.\n \n■Data architect/modeler.  This resource likely comes from a transactional \ndata background with heavy emphasis on normalization. This person should \nembrace dimensional modeling concepts and be empathetic to the require-\nments of the business rather than focused strictly on saving space or reducing \nthe ETL workload.\n \n■Database administrator.  Like the data modeler, the database administrator \nmust be willing to set aside some traditional database administration truisms, \nsuch as having only one index on a relational table.\n \n■Metadata coordinator.  This person helps establish the metadata repository \nstrategy and ensures that the appropriate metadata is collected, managed, \nand disseminated. \n \n■ETL architect/designer.  This role is responsible for designing the ETL envi-\nronment and processes.\n \n■ETL developer.  Based on direction from the ETL architect/designer, the devel-\noper builds and automates the processes, likely using an ETL tool.\nWe want to point out again that this is a list of roles, not people. Especially in \nsmaller shops, talented individuals will ﬁ ll many of these roles simultaneously.\nDeveloping and Maintaining the Plan\nThe  DW/BI project plan identiﬁ es all the necessary Lifecycle tasks. A detailed task \nlist is available on the Kimball Group website at www.kimballgroup.com; check out \nthe Tools & Utilities tab under The Data Warehouse Lifecycle Toolkit, Second Edition \nbook title.\nAny good project manager knows key team members should develop estimates \nof the eff ort required for their tasks; the project manager can’t dictate the amount of \ntime allowed and expect conformance. The project plan should identify acceptance \ncheckpoints with business representatives after every major roadmap milestone and \ndeliverable to ensure the project remains on track.\nDW/BI projects demand broad communication. Although project manag-\ners typically excel at intra-team communications, they should also establish \n",
      "content_length": 2574,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 446,
      "content": "Chapter 17\n410\na communication strategy describing the frequency, forum, and key messaging \nfor other constituencies, including the business sponsors, business community, \nand other IT colleagues.\nFinally, DW/BI projects are vulnerable to scope creep largely due to a strong \nneed to satisfy business users’ requirements. You have several options when con-\nfronted with changes: Increase the scope (by adding time, resources, or budget), \nplay the zero-sum game (by retaining the original scope by giving up something \nin exchange), or say “no” (without actually saying “no” by handling the change as \nan enhancement request). The most important thing about scope decisions is that \nthey shouldn’t be made in an IT vacuum. The right answer depends on the situa-\ntion. Now is the time to leverage the partnership with the business to arrive at an \nanswer that everyone can live with.\nBusiness Requirements Deﬁ nition\nCollaborating  with business users to understand their requirements and ensure \ntheir buy-in is absolutely essential to successful data warehousing and business \nintelligence. This section focuses on back-to-basics techniques for gathering busi-\nness requirements.\nRequirements Preplanning\nBefore  sitting down with business representatives to collect their requirements, we \nsuggest the following to ensure productive sessions:\nChoose the Forum\nBusiness  user requirements sessions are typically interwoven with source system \nexpert data discovery sessions. This dual-pronged approach gives you insight into \nthe needs of the business with the realities of the data. However, you don’t ask \nbusiness representatives about the granularity or dimensionality of their critical \ndata. You need to talk to them about what they do, why they do it, how they make \ndecisions, and how they hope to make decisions in the future. Like organizational \ntherapy, you’re trying to detect the issues and opportunities.\nThere are two primary techniques for gathering requirements: interviews or facili-\ntated sessions. Both have their advantages and disadvantages. Interviews encourage \nindividual participation and are also easier to schedule. Facilitated sessions may \nreduce the elapsed time to gather requirements but require more time commitment \nfrom each participant.\nBased on our experience, surveys are not a reasonable tool for gathering require-\nments because they are ﬂ at and two-dimensional. The self-selected respondents \nanswer only the questions we’ve thought to ask in advance; there’s no option to probe \nmore deeply. In addition, survey instruments do not help forge the bond between \nbusiness users and the DW/BI initiative that we strive for.\n",
      "content_length": 2666,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 447,
      "content": "Kimball DW/BI Lifecycle Overview 411\nWe generally use a hybrid approach with interviews to gather the details and then \nfacilitation to bring the group to consensus. Although we’ll describe this hybrid \napproach in more detail, much of the discussion applies to pure facilitation as well. \nThe requirements gathering forum choice depends on the team’s skills, the organi-\nzation’s culture, and what the business users have already been subjected to. One \nsize deﬁ nitely does not ﬁ t all.\nIdentify and Prepare the Requirements Team\nRegardless  of the approach, you need to identify and prepare the involved project \nteam members. If you’re doing interviews, you need to identify a lead interviewer \nwhose primary responsibility is to ask great open-ended questions. Meanwhile, the \ninterview scribe takes copious notes. Although a tape recorder may provide more \ncomplete coverage of each interview, we don’t use one because it changes the meet-\ning dynamics. Our preference is to have a second person in the room with another \nbrain and a set of eyes and ears rather than relying on technology. We often invite \none or two additional project members (depending on the number of interviewees) \nas observers, so they can hear the users’ input directly.\nBefore sitting down with business users, you need to make sure you’re approach-\ning the sessions with the right mindset. Don’t presume you already know it all; \nyou will deﬁ nitely learn more about the business during the sessions. On the other \nhand, you should do some homework by researching available sources, such as the \nannual report, website, and internal organization chart.\nBecause the key to getting the right answers is asking the right questions, we \nrecommend drafting questionnaires. The questionnaire should not be viewed as \na script; it is a tool to organize your thoughts and serve as a fallback device in \ncase your mind goes blank during the session. The questionnaire will be updated \nthroughout the interview process as the team becomes better versed in the busi-\nness’s subject  matter.\nSelect, Schedule, and Prepare Business Representatives\nIf  this is your ﬁ rst foray into DW/BI, or an eff ort to develop a cohesive strategy for \ndealing with existing data stovepipes, you should talk to business people represent-\ning a reasonable horizontal spectrum of the organization. This coverage is critical \nto formulating the enterprise data warehouse bus matrix blueprint. You need to \nunderstand the common data and vocabulary across core business functions to build \nan extensible environment.\nWithin the target user community, you should cover the organization verti-\ncally. DW/BI project teams naturally gravitate toward the business’s power analysts. \nAlthough their insight is valuable, you can’t ignore senior executives and middle \nmanagement. Otherwise, you are vulnerable to being overly focused on the tactical \nhere-and-now and lose sight of the group’s strategic direction.\n",
      "content_length": 2960,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 448,
      "content": "Chapter 17\n412\nScheduling the business representatives can be the most onerous requirements \ntask; be especially nice to the department’s administrative assistants. We prefer \nto meet with executives individually. Meeting with a homogeneous group of two to \nthree people is appropriate for those lower on the organization chart. Allow 1 hour \nfor individual meetings and 1½ hours for small groups. The scheduler needs to \nallow ½ hour between meetings for debrieﬁ ng and other necessities. Interviewing \nis extremely taxing due to the focus required. Consequently, don’t schedule more \nthan three to four sessions in a day.\nWhen it comes to preparing the interviewees, the business sponsor should \ncommunicate with them, stressing their commitment to the eff ort and the impor-\ntance of everyone’s participation. The interviewees should be asked to bring copies \nof their key reports and analyses to the session. This communication disseminates \na consistent message about the project, plus conveys the business’s ownership of the \ninitiative. Occasionally interviewees will be reluctant to bring the business’s “crown \njewel” reports to the meeting, especially with an outside consultant. However, almost \nalways we have found these people will enthusiastically race back to their offi  ces at \nthe end of the interview to bring back those same reports.\n Collecting Business Requirements\nIt’s time to sit down face-to-face to gather the business’s requirements. The \nprocess usually ﬂ ows from an introduction through structured questioning to a \nﬁ nal wrap-up.\nLaunch\nResponsibility  for introducing the session should be established prior to gathering in \na conference room. The designated kickoff  person should script the primary talking \npoints for the ﬁ rst few minutes when the tone of the interview meeting is set. The \nintroduction should convey a crisp, business-centric message and not ramble with \nhardware, software, and other technical jargon.\nInterview Flow\nThe  objective of an interview is to get business users to talk about what they do and \nwhy they do it. A simple, nonthreatening place to begin is to ask about job responsi-\nbilities and organizational ﬁ t. This is a lob-ball that interviewees can easily respond \nto. From there, you typically ask about their key performance metrics. Determining \nhow they track progress and success translates directly into the dimensional model; \nthey’re telling you about their key business processes and facts without you asking \nthose questions directly.\nIf you meet with a person who has more hands-on data experience, you should \nprobe to better understand the business’s dimensionality. Questions like “How do \nyou distinguish between products (or agents, providers, or facilities)?” or “How \n",
      "content_length": 2759,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 449,
      "content": "Kimball DW/BI Lifecycle Overview 413\ndo you naturally categorize products?” help identify key dimension attributes and \nhierarchies.\nIf the interviewee is more analytic, ask about the types of analysis currently \ngenerated. Understanding the nature of these analyses and whether they are ad \nhoc or standardized provides input into the BI tool requirements, as well as the \nBI application design process. Hopefully, the interviewee brought along copies of \nkey spreadsheets and reports. Rather than stashing them in a folder, it is helpful to \nunderstand how the interviewee uses the analysis today, as well as opportunities for \nimprovement. Contrary to the advice of some industry pundits, you cannot design \nan extensible analytic environment merely by getting users to agree on their top ﬁ ve \nreports. The users’ questions are bound to change; consequently, you must resist \nthe temptation to narrow your design focus to a supposed top ﬁ ve.\nIf you meet with business executives, don’t dive into these tactical details. Instead, \nask them about their vision for better leveraging information throughout the orga-\nnization. Perhaps the project team is envisioning a totally ad hoc environment, \nwhereas business management is more interested in the delivery of standardized \nanalyses. You need to ensure the DW/BI deliverable matches the business demand \nand expectations.\nAsk each interviewee about the impact of improved access to information. You \nlikely already received preliminary project funding, but it never hurts to capture \nmore potential, quantiﬁ able beneﬁ ts.\nWrap-Up\nAs the interview is coming to a conclusion, ask each interviewee about their suc-\ncess criteria for the project. Of course, each criterion should be measurable. “Easy \nto use” and “fast” mean something diff erent to everyone, so the interviewees need to \narticulate speciﬁ cs, such as their expectations regarding the amount of training \nrequired to run predeﬁ ned BI reports.\nAt this point, always make a broad disclaimer. The interviewees must understand \nthat just because you discussed a capability in the meeting doesn’t guarantee it’ll \nbe included in the ﬁ rst phase of the project. Thank interviewees for their brilliant \ninsights, and let them know what’s happening next and what their involvement will be.\nConducting Data-Centric Interviews\nWhile  we’re focused on understanding the business’s requirements, it is helpful to \nintersperse sessions with the source system data gurus or subject matter experts \nto evaluate the feasibility of supporting the business needs. These data-focused \ninterviews are quite diff erent from the ones just described. The goal is to ascer-\ntain whether the necessary core data exists before momentum builds behind the \nrequirements. In these data-centric interviews, you may go so far as to ask for some \ninitial data proﬁ ling results, such as domain values and counts of a few critical \n",
      "content_length": 2919,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 450,
      "content": "Chapter 17\n414\ndata ﬁ elds, to be provided subsequently, just to ensure you are not standing on \nquicksand. A more complete data audit will occur during the dimensional modeling \nprocess. Try to learn enough at this point to manage the organization’s expectations \nappropriately.\n Documenting Requirements\nImmediately  following the interview, the interview team should debrief. You must \nensure everyone is on the same page about what was learned. It is also helpful if \neveryone reviews their notes shortly after the session to ﬁ ll in gaps while the inter-\nview is still fresh. Abbreviations and partial sentences in the notes become incom-\nprehensible after a few days! Likewise, examine the reports gathered to gain further \ninsight into the dimensionality that must be supported in the data warehouse.\nAt this point, it is time to document what you’ve heard. Although documentation \nis everyone’s least favorite activity, it is critical for both user validation and project \nteam reference materials. There are two potential levels of documentation resulting \nfrom the requirements process. The ﬁ rst is to write up each individual interview; \nthis activity is time-consuming because the write-up should not be merely a stream-\nof-consciousness transcript but should make sense to someone who wasn’t in the \ninterview. The more critical documentation is a consolidated ﬁ ndings document. \nThis document is organized around key business processes. Because you tackle \nDW/BI projects on a process-by-process basis, it is appropriate to structure the \nbusiness’s requirements into the same buckets that will, in turn, become imple-\nmentation eff orts.\nWhen writing up the ﬁ ndings document, you should begin with an executive \nsummary, followed by a project overview covering the process used and participants \ninvolved. The bulk of the document centers on the business processes; for each \nprocess, describe why business users want to analyze the process’s performance met-\nrics, what capabilities they want, their current limitations, and potential beneﬁ ts or \nimpact. Commentary about the feasibility of tackling each process is also important.\nAs described in Chapter 4 and illustrated in Figure 4-11, the processes are some-\ntimes unveiled in an opportunity/stakeholder matrix to convey the impact across \nthe organization. In this case, the rows of the opportunity matrix identify business \nprocesses, just like a bus matrix. However, in the opportunity matrix, the columns \nidentify the organizational groups or functions. Surprisingly, this matrix is usually \nquite dense because many groups want access to the same core performance  metrics.\nPrioritizing Requirements\nThe  consolidated ﬁ ndings document serves as the basis for presentations back to \nsenior management and other requirements participants. Inevitably you uncov-\nered more than can be tackled in a single iteration, so you need to prioritize. As \ndiscussed with project scope, don’t make this decision in a vacuum; you need to \n",
      "content_length": 3006,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 451,
      "content": "Kimball DW/BI Lifecycle Overview 415\nleverage (or foster) your partnership with the business community to establish \nappropriate priorities.\nThe requirements wrap-up presentation is positioned as a ﬁ ndings review and \nprioritization meeting. Participants include senior business representatives (who \noptimally participated in the interviews), as well as the DW/BI manager and other \nsenior IT management. The session begins with an overview of each identiﬁ ed \nbusiness process. You want everyone in the room to have a common understanding \nof the opportunities. Also review the opportunity/stakeholder matrix, as well as a \nsimpliﬁ ed bus matrix.\nAfter the ﬁ ndings have been presented, it is time to prioritize using the prioriti-\nzation grid, illustrated in Figure 17-2. The grid’s vertical axis refers to the potential \nimpact or value to the business. The horizontal axis conveys feasibility. Each of the \nﬁ nding’s business process themes is placed on the grid based on the representatives’ \ncomposite agreement regarding impact and feasibility. It’s visually obvious where \nyou should begin; projects that warrant immediate attention are located in the \nupper-right corner because they’re high-impact projects, as well as highly feasible. \nProjects in the lower-left cell should be avoided like the plague; they’re missions \nimpossible that do little for the business. Likewise, projects in the lower-right cell \ndon’t justify short-term attention, although project teams sometimes gravitate here \nbecause these projects are doable but not very crucial. Finally, projects in the upper-\nleft cell represent meaningful opportunities. These projects have large potential \nbusiness payback but are currently infeasible. While the DW/BI project team focuses \non projects in the shaded upper-right corner, other IT teams should address the \ncurrent feasibility limitations of those in the upper left.\nMed\nFeasibility\nPotential\nBusiness\nImpact\nLow\nHigh\nHigh\nMed\nBP3\nBP1\nBP6\nBP4\nBP2\nBP5\nLow\nFigure 17-2: Prioritization grid based on business impact and feasibility.\n",
      "content_length": 2067,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 452,
      "content": "Chapter 17\n416\nLifecycle Technology Track\nOn the Kimball Lifecycle roadmap in Figure 17-1, the business requirements deﬁ -\nnition is followed immediately by three concurrent tracks focused on technology, \ndata, and BI applications, respectively. In the next several sections we’ll zero in on \nthe technology track.\nTechnical Architecture Design\nMuch  like a blueprint for a new home, the technical architecture is the blueprint for \nthe DW/BI environment’s technical services and infrastructure. As the enterprise \ndata warehouse bus architecture introduced in Chapter 4 supports data integra-\ntion, the architecture plan is an organizing framework to support the integration \nof technologies and applications.\nLike housing blueprints, the technical architecture consists of a series of models \nthat unveil greater detail regarding each major component. In both situations, the \narchitecture enables you to catch problems on paper (such as having the dish-\nwasher too far from the sink) and minimize mid-project surprises. It supports the \ncoordination of parallel eff orts while speeding development through the reuse of \nmodular components. The architecture identiﬁ es immediately required components \nversus those that will be incorporated at a later date (such as the deck and screened \nporch). Most important, the architecture serves as a communication tool. Home \nconstruction blueprints enable the architect, general contractor, subcontractors, \nand homeowner to communicate from a common document. Likewise, the DW/BI \ntechnical architecture supports communication regarding a consistent set of techni-\ncal requirements within the team, upward to management, and outward to vendors.\nIn Chapter 1, we discussed several major components of the architecture, includ-\ning ETL and BI services. In this section, we focus on the process of creating the \narchitecture design.\nDW/BI teams typically approach the architecture design process from opposite \nends of the spectrum. Some teams simply don’t understand the beneﬁ ts of an archi-\ntecture and feel that the topic and tasks are too nebulous. They’re so focused on \ndelivery that the architecture feels like a distraction and impediment to progress, \nso they opt to bypass architecture design. Instead, they piece together the technical \ncomponents required for the ﬁ rst iteration with chewing gum and bailing wire, but \nthe integration and interfaces get taxed as more data, more users, or more func-\ntionality are added. Eventually, these teams often end up rebuilding because the \nnon-architected structure couldn’t withstand the stresses. At the other extreme, \nsome teams want to invest two years designing the architecture while forgetting \nthat the primary purpose of a DW/BI environment is to solve business problems, \nnot address any plausible (and not so plausible) technical challenge.\n",
      "content_length": 2852,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 453,
      "content": "Kimball DW/BI Lifecycle Overview 417\nNeither end of the spectrum is healthy; the most appropriate response lies \nsomewhere in the middle. We’ve identiﬁ ed the following eight-step process to help \nnavigate these architectural design waters. Every DW/BI system has a technical \narchitecture; the question is whether it is planned and explicit or merely implicit.\nEstablish an Architecture Task Force\nIt  is useful to create a small task force of two to three people focused on architecture \ndesign. Typically, it is the technical architect, along with the ETL architect/designer \nand BI application architect/designer who ensure both back room and front room \nrepresentation.\nCollect Architecture-Related Requirements\nAs  illustrated in Figure 17-1, deﬁ ning the technical architecture is not the ﬁ rst box \nin the Lifecycle diagram. The architecture is created to support business needs; it’s \nnot meant to be an excuse to purchase the latest, greatest products. Consequently, \nkey input into the design process should come from the business requirements \ndeﬁ nition. However, you should listen to the business’s requirements with a slightly \ndiff erent ﬁ lter to drive the architecture design. The primary focus is uncovering the \narchitectural implications associated with the business’s needs. Listen closely for \ntiming, availability, and performance requirements.\nYou should also conduct additional interviews within the IT organization. These \nare technology-focused sessions to understand current standards, planned techni-\ncal directions, and nonnegotiable boundaries. In addition, you should uncover les-\nsons learned from prior information delivery projects, as well as the organization’s \nwillingness to accommodate operational change on behalf of the DW/BI initiative, \nsuch as identifying updated transactions in the source  system.\nDocument Architecture Requirements\nAfter  leveraging the business requirements process and conducting supplemental IT \ninterviews, you need to document your ﬁ ndings. We recommend using a simplistic \ntabular format, just listing each business requirement impacting the architecture, \nalong with a laundry list of architectural implications. For example, if there is a need \nto deliver global sales performance data on a nightly basis, the technical implica-\ntions might include 24/7 worldwide availability, data mirroring for loads, robust \nmetadata to support global access, adequate network bandwidth, and suffi  cient ETL \nhorsepower to handle the complex integration of operational  data.\nCreate the Architecture Model\nAfter  the architecture requirements have been documented, you should begin for-\nmulating models to support the identiﬁ ed needs. At this point, the architecture \nteam often sequesters itself in a conference room for several days of heavy thinking. \n",
      "content_length": 2819,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 454,
      "content": "Chapter 17\n418\nThe architecture requirements are grouped into major components, such as ETL, BI, \nmetadata, and infrastructure. From there the team drafts and reﬁ nes the high-level \narchitectural model. This drawing is similar to the front elevation page on housing \nblueprints. It illustrates what the architecture will look like from the street, but it \ncan be dangerously simplistic because signiﬁ cant details are embedded in the pages \nthat follow.\nDetermine Architecture Implementation Phases\nLike  the homeowner’s dream house, you likely can’t implement all aspects of the \ntechnical architecture at once. Some are nonnegotiable mandatory capabilities, \nwhereas others are nice-to-haves. Again, refer back to the business requirements \nto establish architecture priorities because you must minimally provide the archi-\ntectural elements needed to deliver the initial project.\nDesign and Specify the Subsystems\nA  large percentage of the needed functionality will likely be met by the major tool \nvendor’s standard off erings, but there are always a few subsystems that may not \nbe found in off -the-shelf products. You must deﬁ ne these subsystems in enough \ndetail, so either someone can build it for you or you can evaluate products against \nyour needs.\nCreate the Architecture Plan\nThe  technical architecture needs to be documented, including the planned imple-\nmentation phases, for those who were not sequestered in the conference room. The \ntechnical architecture plan document should include adequate detail so skilled \nprofessionals can proceed with construction of the framework, much like carpen-\nters frame a house based on the blueprint. However, it doesn’t typically reference \nspeciﬁ c products, except those already in-house.\nReview and Finalize the Technical Architecture\nEventually we come full circle with the architecture design process. The architecture \ntask force needs to communicate the architecture plan at varying levels of detail \nto the project team, IT colleagues, and business leads. Following the review, docu-\nmentation should be updated and put to use immediately in the product selection \nprocess.\nProduct Selection and Installation\nIn many ways the architecture plan is similar to a shopping list for selecting products \nthat ﬁ t into the plan’s framework. The following six tasks associated with DW/BI \nproduct selection are quite similar to any technology selection.\n",
      "content_length": 2413,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 455,
      "content": "Kimball DW/BI Lifecycle Overview 419\nUnderstand the Corporate Purchasing Process\nThe ﬁ rst step before selecting new products is to understand the internal hardware \nand software purchase processes.\nDevelop a Product Evaluation Matrix\nUsing  the architecture plan as a starting point, a spreadsheet-based evaluation \nmatrix should be developed that identiﬁ es the evaluation criteria, along with weight-\ning factors to indicate importance; the more speciﬁ c the criteria, the better. If the \ncriteria are too vague or generic, every vendor will say they can satisfy your needs.\nConduct Market Research\nTo  become informed buyers when selecting products, you should do market research \nto better understand the players and their off erings. A request for proposal (RFP) is \na classic product evaluation tool. Although some organizations have no choice about \ntheir use, you should avoid this technique, if possible. Constructing the RFP and \nevaluating responses are tremendously time-consuming for the team. Meanwhile, \nvendors are motivated to respond to the questions in the most positive light, so the \nresponse evaluation is often more of a beauty contest. In the end, the value of \nthe expenditure may not warrant the eff ort.\nEvaluate a Short List of Options\nDespite the plethora of products available in the market, usually only a small number \nof vendors can meet both functionality and technical requirements. By comparing \npreliminary scores from the evaluation matrix, you can focus on a narrow list of \nvendors and disqualify the rest. After dealing with a limited number of vendors, you \ncan begin the detailed evaluations. Business representatives should be involved in \nthis process if you’re evaluating BI tools. As evaluators, you should drive the process \nrather than allow the vendors to do the driving, sharing relevant information from \nthe architecture plan, so the sessions focus on your needs rather than on product \nbells and whistles. Be sure to talk with vendor references, both those formally pro-\nvided and those elicited from your informal network.\nIf Necessary, Conduct a Prototype\nAfter  performing the detailed evaluations, sometimes a clear winner bubbles to the \ntop, often based on the team’s prior experience or relationships. In other cases, \nthe leader emerges due to existing corporate commitments such as site licenses or \nlegacy hardware purchases. In either situation, when a sole candidate emerges as the \nwinner, you can bypass the prototype step (and the associated investment in both \ntime and money). If no vendor is the apparent winner, you should conduct a prototype \nwith no more than two products. Again, take charge of the process by developing a \nlimited yet realistic business case study.\n",
      "content_length": 2744,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 456,
      "content": "Chapter 17\n420\nSelect Product, Install on Trial, and Negotiate\nIt is time to select a product. Rather than immediately signing on the dotted line, \npreserve your negotiating power by making a private, not public, commitment to a \nsingle vendor. Instead of informing the vendor that you’re completely sold, embark \non a trial period where you have the opportunity to put the product to real use in \nyour environment. It takes signiﬁ cant energy to install a product, get trained, and \nbegin using it, so you should walk down this path only with the vendor you fully \nintend to buy from; a trial should not be pursued as another tire-kicking exercise. \nAs the trial draws to a close, you have the opportunity to negotiate a purchase that’s \nbeneﬁ cial to all parties involved.\nLifecycle Data Track\nIn the Figure 17-1 Kimball Lifecycle diagram, the middle track following the busi-\nness requirements deﬁ nition focuses on data. We turn your attention in that direc-\ntion throughout the next several sections.\n Dimensional Modeling\nGiven  this book’s focus for the ﬁ rst 16 chapters, we won’t spend any time discuss-\ning dimensional modeling techniques here. The next chapter provides detailed \nrecommendations about the participants, process, and deliverables surrounding our \niterative workshop approach for designing dimensional models in collaboration with \nbusiness users. It’s required reading for anyone involved in the modeling activity.\nPhysical Design\nThe  dimensional models developed and documented via a preliminary source-to-\ntarget mapping need to be translated into a physical database. With dimensional \nmodeling, the logical and physical designs bear a close resemblance; you don’t \nwant the database administrator to convert your lovely dimensional schema into a \nnormalized structure during the physical design process.\nPhysical database implementation details vary widely by platform and project. \nIn addition, hardware, software, and tools are evolving rapidly, so the following \nphysical design activities and considerations merely scratch the surface.\nDevelop Naming and Database Standards\nTable  and column names are key elements of the users’ experience, both for navi-\ngating the data model and viewing BI applications, so they should be meaningful \nto the business. You must also establish standards surrounding key declarations \nand the permissibility of nulls.\n",
      "content_length": 2387,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 457,
      "content": "Kimball DW/BI Lifecycle Overview 421\nDevelop Physical Database Model\nThis  model should be initially built in the development server where it will be used \nby the ETL development team. There are several additional sets of tables that need \nto be designed and deployed as part of the DW/BI system, including staging tables to \nsupport the ETL system, auditing tables for ETL processing and data quality, and \nstructures to support secure access to a subset of the data warehouse.\nDevelop Initial Index Plan\nIn  addition to understanding how the relational database’s query optimizer \nand indexes work, the database administrator also needs to be keenly aware that \nDW/BI requirements diff er signiﬁ cantly from OLTP requirements. Because dimen-\nsion tables have a single column primary key, you’ll have a unique index on that \nkey. If bitmapped indexes are available, you typically add single column bitmapped \nindexes to dimension attributes used commonly for ﬁ ltering and grouping, espe-\ncially those attributes that will be jointly constrained; otherwise, you should evalu-\nate the usefulness of B-tree indexes on these attributes. Similarly, the ﬁ rst fact table \nindex will typically be a B-tree or clustered index on the primary key; placing the \ndate foreign key in the index’s leading position speeds both data loads and queries \nbecause the date is frequently constrained. If the DBMS supports high-cardinality \nbitmapped indexes, these can be a good choice for individual foreign keys in the \nfact tables because they are more agnostic than clustered indexes when the user \nconstrains dimensions in unexpected ways. The determination of other fact table \nindexes depends on the index options and optimization strategies within the plat-\nform. Although OLAP database engines also use indexes and have a query optimizer, \nunlike the relational world, the database administrator has little control in these \nenvironments.\nDesign Aggregations, Including OLAP Database\nContrary  to popular belief, adding more hardware isn’t necessarily the best weapon \nin the performance-tuning arsenal; leveraging aggregate tables is a far more cost-\neff ective alternative. Whether using OLAP technology or relational aggregation \ntables, aggregates need to be designed in the DW/BI environment, as we’ll further \nexplore in Chapter 19: ETL Subsystems and Techniques, and Chapter 20. When \nperformance metrics are aggregated, you either eliminate dimensions or associate \nthe metrics with a shrunken rollup dimension that conforms to the atomic base \ndimension. Because you can’t possibly build, store, and administer every theoreti-\ncal aggregation, two primary factors need to be evaluated. First, think about the \nbusiness users’ access patterns derived from the requirements ﬁ ndings, as well as \nfrom input gained by monitoring actual usage patterns. Second, assess the data’s \nstatistical distribution to identify aggregation points that deliver bang for the buck.\n",
      "content_length": 2963,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 458,
      "content": "Chapter 17\n422\nFinalize Physical Storage Details\nThis  includes the nuts-and-bolts storage structures of blocks, ﬁ les, disks, partitions, \nand table spaces or databases. Large fact tables are typically partitioned by activity \ndate, with data segmented by month into separate partitions while appearing to \nusers as a single table. Partitioning by date delivers data loading, maintenance, and \nquery performance advantages.\nThe aggregation, indexing and other performance tuning strategies will evolve as \nactual usage patterns are better understood, so be prepared for the inevitable ongo-\ning modiﬁ cations. However, you must deliver appropriately indexed and aggregated \ndata with the initial rollout to ensure the DW/BI environment delivers reasonable \nquery performance from the start.\nETL Design and Development\nThe  Lifecycle’s data track wraps up with the design and development of the ETL \nsystem. Chapter 19 describes the factors, presented as 34 subsystems, which must \nbe considered during the design. Chapter 20 then provides more granular guidance \nabout the ETL system design and development process and associated tasks. Stay \ntuned for more details regarding ETL.\nLifecycle BI Applications Track\nThe ﬁ nal set of parallel activities following the business requirements deﬁ nition in \nFigure 17-1 is the BI application track where you design and develop the applica-\ntions that address a portion of the users’ analytic requirements. As a BI application \ndeveloper once said, “Remember, this is the fun part!” You’re ﬁ nally using the \ninvestment in technology and data to help business users make better decisions. \nAlthough some may feel that the data warehouse should be a completely ad hoc, \nself-service query environment, delivering parameter-driven BI applications will \nsatisfy a large percentage of the business community’s needs. For many business \nusers, “ad hoc” implies the ability to change the parameters on a report to create \ntheir personalized version. There’s no sense making every user start from scratch. \nConstructing a set of BI applications establishes a consistent analytic framework for \nthe organization, rather than allowing each spreadsheet to tell a slightly diff erent \nstory. BI applications also serve to capture the analytic expertise of the organiza-\ntion, from monitoring performance to identifying exceptions, determining causal \nfactors, and modeling alternative responses; this encapsulation provides a jump \nstart for the less analytically inclined.\n",
      "content_length": 2506,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 459,
      "content": "Kimball DW/BI Lifecycle Overview 423\nBI Application Speciﬁ cation\nFollowing  the business requirements deﬁ nition, you need to review the ﬁ ndings \nand collected sample reports to identify a starter set of approximately 10 to 15 BI \nreports and analytic applications. You want to narrow the initial focus to the most \ncritical capabilities to manage expectations and ensure on-time delivery. Business \ncommunity input will be critical to this prioritization process. Although 15 appli-\ncations may not sound like much, numerous analyses can be created from a single \ntemplate merely by changing variables.\nBefore you start designing the initial applications, it’s helpful to establish stan-\ndards, such as common pull-down menus and consistent output look and feel. \nUsing these standards, you specify each application template and capture suffi  cient \ninformation about the layout, input variables, calculations, and breaks, so both the \napplication developer and business representatives share a common understanding.\nDuring the BI application speciﬁ cation activity, you should also consider the appli-\ncations’ organization. You need to identify structured navigational paths to access \nthe applications, reﬂ ecting the way users think about their business. Leveraging \ncustomizable information portals or dashboards are the dominant strategies for \ndisseminating access.\nBI Application Development\nWhen  you move into the development phase for the BI applications, you again \nneed to focus on standards; naming conventions, calculations, libraries, and cod-\ning standards should be established to minimize future rework. The application \ndevelopment activity can begin when the database design is complete, the BI tools \nand metadata are installed, and a subset of historical data has been loaded. The BI \napplication template speciﬁ cations should be revisited to account for the inevitable \nchanges to the model since the speciﬁ cations were completed.\nEach BI tool has product-speciﬁ c tricks that can cause it to jump through hoops \nbackward. Rather than trying to learn the techniques via trial and error, we sug-\ngest investing in appropriate tool-speciﬁ c education or supplemental resources for \nthe development team.\nWhile the BI applications are being developed, several ancillary beneﬁ ts result. BI \napplication developers, armed with a robust access tool, will quickly ﬁ nd needling \nproblems in the data haystack despite the quality assurance performed by the ETL \napplication. This is one reason we prefer to start the BI application development \nactivity prior to the supposed completion of the ETL system. The developers also \nwill be the ﬁ rst to realistically test query response times. Now is the time to review \nthe preliminary performance-tuning strategies.\n",
      "content_length": 2786,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 460,
      "content": "Chapter 17\n424\nThe BI application quality assurance activities cannot be completed until the data \nis stabilized. You must ensure there is adequate time in the schedule beyond the ﬁ nal \nETL cutoff  to allow for an orderly wrap-up of the BI application development tasks.\nLifecycle Wrap-up Activities\nThe following sections provide recommendations to ensure your project comes to \nan orderly conclusion, while ensuring you’re poised for future expansion.\nDeployment\nThe  technology, data, and BI application tracks converge at deployment. \nUnfortunately, this convergence does not happen naturally but requires substantial \npreplanning. Perhaps more important, successful deployment demands the courage \nand willpower to honestly assess the project’s preparedness to deploy. Deployment \nis similar to serving a large holiday meal to friends and relatives. It can be diffi  cult \nto predict exactly how long it will take to cook the meal’s main entrée. Of course, if \nthe entrée is not done, the cook is forced to slow down the side dishes to compensate \nfor the lag before calling everyone to the table.\nIn the case of DW/BI deployment, the data is the main entrée. “Cooking” the \ndata in the ETL kitchen is the most unpredictable task. Unfortunately, even if the \ndata isn’t fully cooked, you often still proceed with the DW/BI deployment because \nyou told the warehouse guests they’d be served on a speciﬁ c date and time. Because \nyou’re unwilling to slow down the pace of deployment, you march into their offi  ces \nwith undercooked data. No wonder users sometimes refrain from coming back for \na second helping.\nAlthough testing has undoubtedly occurred during the DW/BI development tasks, \nyou need to perform end-to-end system testing, including data quality assurance, \noperations processing, performance, and usability testing. In addition to critically \nassessing the readiness of the DW/BI deliverables, you also need to package it with \neducation and support for deployment. Because the user community must adopt \nthe DW/BI system for it to be deemed successful, education is critical. The DW/\nBI support strategy depends on a combination of management’s expectations and \nthe realities of the deliverables. Support is often organized into a tiered structure. \nThe ﬁ rst tier is website and self-service support; the second tier is provided by the \npower users residing in the business area; centralized support from the DW/BI team \nprovides the ﬁ nal line of  defense.\n",
      "content_length": 2482,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 461,
      "content": "Kimball DW/BI Lifecycle Overview 425\nMaintenance and Growth\nYou  made it through deployment, so now you’re ready to kick back and relax. Not \nso quickly! Your job is far from complete after you deploy. You need to continue \nto manage the existing environment by investing resources in the following areas:\n \n■Support. User support is immediately crucial following the deployment to \nensure the business community gets hooked. You can’t sit back in your cubicle \nand assume that no news from the business community is good news. If \nyou’re not hearing from them, chances are no one is using the DW/BI system. \nRelocate (at least temporarily) to the business community so the users have \neasy access to support resources. If problems with the data or BI applications \nare uncovered, be honest with the business to build credibility while taking \nimmediate action to correct the problems. If the DW/BI deliverable is not of \nhigh quality, the unanticipated support demands for data reconciliation and \napplication rework can be overwhelming.\n \n■Education. You must provide a continuing education program for the DW/\nBI system. The curriculum should include formal refresher and advanced \ncourses, as well as repeat introductory courses. More informal education can \nbe off ered to the developers and power users to encourage the interchange \nof ideas.\n \n■Technical support. The DW/BI system needs to be treated as a production \nenvironment with service level agreements. Of course, technical support \nshould proactively monitor performance and system capacity trends. You \ndon’t want to rely on the business community to tell you that performance \nhas degraded.\n \n■Program support. The DW/BI program lives on beyond the implementation \nof a single phase. You must closely monitor and then market your success. \nCommunication with the varied DW/BI constituencies must continue. You \nmust also ensure that existing implementations continue to address the needs \nof the business. Ongoing checkpoint reviews are a key tool to assess and \nidentify opportunities for improvement.\nIf you’ve done your job correctly, inevitably there will be demand for growth, \neither for new users, new data, new BI applications, or major enhancements to \nexisting deliverables. Unlike traditional systems development initiatives, DW/BI \nchange should be viewed as a sign of success, not failure. As we advised earlier when \ndiscussing project scoping, the DW/BI team should not make decisions about these \ngrowth options in a vacuum; the business needs to be involved in the prioritization \n",
      "content_length": 2566,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 462,
      "content": "Chapter 17\n426\nprocess. This is a good time to leverage the prioritization grid illustrated in \nFigure 17-2. If you haven’t done so already, an executive business sponsorship com-\nmittee should be established to set DW/BI priorities that align with the organization’s \noverall objectives. After new priorities have been identiﬁ ed, then you go back to \nthe beginning of the Lifecycle and do it all again, leveraging and building on the \ntechnical, data, and BI application foundations that have already been established, \nwhile turning your attention to the new requirements.\nCommon Pitfalls to Avoid\nAlthough  we can provide positive recommendations about data warehousing and \nbusiness intelligence, some readers better relate to a listing of common pitfalls. \nHere is our favorite top 10 list of common errors to avoid while building a DW/\nBI system. These are all quite lethal errors—one alone may be suffi  cient to bring \ndown the initiative:\n \n■Pitfall 10: Become overly enamored with technology and data rather than \nfocusing on the business’s requirements and goals.\n \n■Pitfall 9: Fail to embrace or recruit an inﬂ uential, accessible, and reasonable \nsenior management visionary as the business sponsor of the DW/BI eff ort.\n \n■Pitfall 8: Tackle a galactic multiyear project rather than pursuing more man-\nageable, although still compelling, iterative development eff orts.\n \n■Pitfall 7: Allocate energy to construct a normalized data structure, yet run \nout of budget before building a viable presentation area based on dimensional \nmodels.\n \n■Pitfall 6: Pay more attention to back room operational performance and ease-\nof-development than to front room query performance and ease of use.\n \n■Pitfall 5: Make the supposedly queryable data in the presentation area overly \ncomplex. Database designers who prefer a more complex presentation should \nspend a year supporting business users; they’d develop a much better appre-\nciation for the need to seek simpler solutions.\n \n■Pitfall 4: Populate dimensional models on a standalone basis without regard \nto a data architecture that ties them together using shared, conformed \ndimensions.\n \n■Pitfall 3: Load only summarized data into the presentation area’s dimensional \nstructures.\n \n■Pitfall 2: Presume the business, its requirements and analytics, and the under-\nlying data and the supporting technology are static.\n \n■Pitfall 1: Neglect to acknowledge that DW/BI success is tied directly to busi-\nness acceptance. If the users haven’t accepted the DW/BI system as a founda-\ntion for improved decision making, your eff orts have been exercises in futility.\n",
      "content_length": 2617,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 463,
      "content": "Kimball DW/BI Lifecycle Overview 427\nSummary\nThis chapter provided a high-speed tour of the Kimball Lifecycle approach for DW/\nBI projects. We touched on the key processes and best practices. Although each \nproject is a bit diff erent from the next, they all require attention to the major tasks \ndiscussed to ensure a successful initiative.\nThe next chapter provides much more detailed coverage of the Kimball Lifec ycle’s \ncollaborative workshop approach for iteratively designing dimensional models with \nbusiness representatives. Chapters 19 and 20 delve into ETL system design consid-\nerations and recommended development processes.\n",
      "content_length": 638,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 465,
      "content": "Dimensional \nModeling Process \nand Tasks\nW\ne’ve described countless dimensional modeling patterns in Chapters 1 \nthrough 16 of this book. Now it’s time to turn your attention to the tasks \nand tactics of the dimensional modeling process.\nThis chapter, condensed from content in The Data Warehouse Lifecycle Toolkit, \nSecond Edition (Wiley, 2008), begins with a practical discussion of preliminary \npreparation activities, such as identifying the participants (including business \nrepresentatives) and arranging logistics. The modeling team develops an initial \nhigh-level model diagram, followed by iterative detailed model development, review, \nand validation. Throughout the process, you are reconﬁ rming your understanding \nof the business’s requirements.\nChapter 18 reviews the following concepts:\n \n■Overview of the dimensional modeling process\n \n■Tactical recommendations for the modeling tasks\n \n■Key modeling deliverables\n Modeling Process Overview\nBefore  launching into the dimensional modeling design eff ort, you must involve \nthe right players. Most notably, we strongly encourage the participation of business \nrepresentatives during the modeling sessions. Their involvement and collaboration \nstrongly increases the likelihood that the resultant model addresses the business’s \nneeds. Likewise, the organization’s business data stewards should participate, espe-\ncially when you’re discussing the data they’re responsible for governing.\nCreating a dimensional model is a highly iterative and dynamic process. After \na few preparation steps, the design eff ort begins with an initial graphical model \nderived from the bus matrix, identifying the scope of the design and clarifying the \ngrain of the proposed fact tables and associated dimensions.\n18\n",
      "content_length": 1764,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 466,
      "content": "Chapter 18\n430\nAfter completing the high-level model, the design team dives into the dimension \ntables with attribute deﬁ nitions, domain values, sources, relationships, data quality \nconcerns, and transformations. After the dimensions are identiﬁ ed, the fact tables \nare modeled. The last phase of the process involves reviewing and validating the \nmodel with interested parties, especially business representatives. The primary \ngoals are to create a model that meets the business requirements, verify that data \nis available to populate the model, and provide the ETL team with a solid starting \nsource-to-target mapping.\nDimensional models unfold through a series of design sessions with each pass \nresulting in a more detailed and robust design that’s been repeatedly tested against the \nbusiness needs. The process is complete when the model clearly meets the business’s \nrequirements. A typical design requires three to four weeks for a single \nbusiness process dimensional model, but the time required can vary depending \non the team’s experience, the availability of detailed business requirements, the \ninvolvement of business representatives or data stewards authorized to drive to orga-\nnizational consensus, the complexity of the source data, and the ability to leverage \nexisting conformed dimensions.\nFigure 18-1 shows the dimensional modeling process ﬂ ow. The key inputs to the \ndimensional modeling process are the preliminary bus matrix and detailed busi-\nness requirements. The key deliverables of the modeling process are the high-level \ndimensional model, detailed dimension and fact table designs, and issues log.\nPreparation\nHigh Level Dimensional Model\nDetailed Dimensional Model Development\nModel Review and Validation\nIterate and Test\nFinal Design Documentation\nFigure 18-1: Dimensional modeling process ﬂ ow diagram.\nAlthough the graphic portrays a linear progression, the process is quite iterative. \nYou will make multiple passes through the dimensional model starting at a high \nlevel and drilling into each table and column, ﬁ lling in the gaps, adding more detail, \nand changing the design based on new information.\n",
      "content_length": 2150,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 467,
      "content": "Dimensional Modeling Process and Tasks 431\nIf an outside expert is engaged to help guide the dimensional modeling eff ort, \ninsist they facilitate the process with the team rather than disappearing for a few \nweeks and returning with a completed design. This ensures the entire team under-\nstands the design and associated trade-off s. It also provides a learning opportunity, \nso the team can carry the model forward and independently tackle the next model.\nGet Organized\nBefore beginning to model, you must appropriately prepare for the dimensional \nmodeling process. In addition to involving the right resources, there are also basic \nlogistical considerations to ensure a productive design eff ort.\n Identify Participants, Especially Business \nRepresentatives\nThe  best dimensional models result from a collaborative team eff ort. No single \nindividual is likely to have the detailed knowledge of the business requirements and \nthe idiosyncrasies of the source systems to eff ectively create the model themselves. \nAlthough the data modeler facilitates the process and has primary responsibility \nfor the deliverables, we believe it’s critically important to get subject matter experts \nfrom the business involved to actively collaborate; their insights are invaluable, \nespecially because they are often the individuals who have historically ﬁ gured \nout how to get data out of the source systems and turned it into valuable analytic \ninformation. Although involving more people in the design activities increases the \nrisk of slowing down the process, the improved richness and completeness of \nthe design justiﬁ es the additional overhead.\nIt’s always helpful to have someone with keen knowledge of the source system \nrealities involved. You might also include some physical DBA and ETL team rep-\nresentatives so they can learn from the insights uncovered during the modeling \neff ort and resist the temptations to apply third normal form (3NF) concepts or defer \ncomplexities to the BI applications in an eff ort to streamline the ETL processing. \nRemember the goal is to trade off  ETL processing complexity for simplicity and \npredictability at the BI presentation layer.\nBefore jumping into the modeling process, you should take time to consider \nthe ongoing stewardship of the DW/BI environment. If the organization has \nan active data governance and stewardship initiative, it is time to tap into that \nfunction. If there is no preexisting stewardship program, it’s time to initiate \nit. An enterprise DW/BI eff ort committed to dimensional modeling must also \nbe committed to a conformed dimension strategy to ensure consistency across \nbusiness processes. An active data stewardship program helps the organization \n",
      "content_length": 2730,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 468,
      "content": "Chapter 18\n432\nachieve its conformed dimension strategy. Agreeing on conformed dimensions \nin a large enterprise can be a challenge; the diffi  culty is usually less a techni-\ncal issue and more an organizational communication and consensus building \nchallenge.\nDiff erent groups across the enterprise are often committed to their own pro-\nprietary business rules and deﬁ nitions. Data stewards must work closely with the \ninterested groups to develop common business rules and deﬁ nitions, and then \ncajole the organization into embracing the common rules and deﬁ nitions to develop \nenterprise consensus. Over the years, some have criticized the concept of conformed \ndimensions as being “too hard.” Yes, it’s diffi  cult to get people in diff erent corners \nof the business to agree on common attribute names, deﬁ nitions, and values, but \nthat’s the crux of uniﬁ ed, integrated data. If everyone demands their own labels and \nbusiness rules, then there’s no chance of delivering the single version of the truth \npromised by DW/BI systems. And ﬁ nally, one of the reasons the Kimball approach is \nsometimes criticized as being hard from people who are looking for quick solutions \nis because we have spelled out the detailed steps for actually getting the job done. \nIn Chapter 19: ETL Subsystems and Techniques, these down-in-the-weeds details \nare discussed in the coverage of ETL subsystems 17 and 18.\nReview the Business Requirements\nBefore  the modeling begins, the team must familiarize itself with the business \nrequirements. The ﬁ rst step is to carefully review the requirements documentation, \nas we described in Chapter 17: Kimball DW/BI Lifecycle Overview. It’s the modeling \nteam’s responsibility to translate the business requirements into a ﬂ exible dimen-\nsional model that can support a broad range of analysis, not just speciﬁ c reports. \nSome designers are tempted to skip the requirements review and move directly into \nthe design, but the resulting models are typically driven exclusively by the source \ndata without considering the added value required by the business community. \nHaving appropriate business representation on the modeling team helps further \navoid this data-driven approach.\nLeverage a Modeling Tool\nBefore  jumping into the modeling activities, it’s helpful to have a few tools in place. \nUsing a spreadsheet as the initial documentation tool is eff ective because it enables \nyou to quickly and easily make changes as you iterate through the modeling process.\nAfter the model begins to ﬁ rm up in the later stages of the process, you can con-\nvert to whatever modeling tool is used in your organization. Most modeling tools \nare dimensionally aware with functions to support the creation of a dimensional \nmodel. When the detailed design is complete, the modeling tools can help the DBA \n",
      "content_length": 2833,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 469,
      "content": "Dimensional Modeling Process and Tasks 433\nforward engineer the model into the database, including creating the tables, indexes, \npartitions, views, and other physical elements of the database.\nLeverage a Data Proﬁ ling Tool\nThroughout  the modeling process, the teams needs to develop an ever-increasing \nunderstanding of the source data’s structure, content, relationships, and derivation \nrules. You need to verify the data exists in a usable state, or at least its ﬂ aws can be \nmanaged, and understand what it takes to convert it into the dimensional model. \nData proﬁ ling uses query capabilities to explore the actual content and relation-\nships in the source system rather than relying on perhaps incomplete or outdated \ndocumentation. Data proﬁ ling can be as simple as writing some SQL statements \nor as sophisticated as a special purpose tool. The major ETL vendors include data \nproﬁ ling capabilities in their products.\nLeverage or Establish Naming Conventions\nThe  issue of naming conventions inevitably arises during the creation of the dimen-\nsional model. The data model’s labels must be descriptive and consistent from a \nbusiness perspective. Table and column names become key elements of the BI appli-\ncation’s interface. A column name such as “Description” may be perfectly clear in \nthe context of a data model but communicates nothing in the context of a report.\nPart of the process of designing a dimensional model is agreeing on common deﬁ -\nnitions and common labels. Naming is complex because diff erent business groups \nhave diff erent meanings for the same name and diff erent names with the same \nmeaning. People are reluctant to give up the familiar and adopt a new vocabulary. \nSpending time on naming conventions is one of those tiresome tasks that seem to \nhave little payback but is worth it in the long run.\nLarge organizations often have an IT function that owns responsibility for nam-\ning conventions. A common approach is to use a naming standard with three parts: \nprime word, qualiﬁ ers (if appropriate), and class word. Leverage the work of this \nIT function, understanding that sometimes existing naming conventions need to \nbe extended to support more business-friendly table and column names. If the \norganization doesn’t already have a set of naming conventions, you must establish \nthem during the dimensional  modeling.\nCoordinate Calendars and Facilities\nLast,  but not least, you need to schedule the design sessions on participants’ cal-\nendars. Rather than trying to reserve full days, it’s more realistic to schedule \nmorning and afternoon sessions that are two to three hours in duration for three or \nfour days each week. This approach recognizes that the team members have other \n",
      "content_length": 2737,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 470,
      "content": "Chapter 18\n434\nresponsibilities and allows them to try to keep up in the hours before, after, and \nbetween design sessions. The design team can leverage the unscheduled time to \nresearch the source data and conﬁ rm requirements, as well as allow time for the \ndata modeler to update the design documentation prior to each session.\nAs we mentioned earlier, the modeling process typically takes three to four weeks \nfor a single business process, such as sales orders, or a couple of tightly related \nbusiness processes such as healthcare facility and professional claim transactions \nin a set of distinct but closely aligned fact tables. There are a multitude of factors \nimpacting the magnitude of the eff ort. Ultimately, the availability of previously \nexisting core dimensions allows the modeling eff ort to focus almost exclusively on \nthe fact table’s performance metrics, which signiﬁ cantly reduces the time required.\nFinally, you must reserve appropriate facilities. It is best to set aside a dedicated \nconference room for the duration of the design eff ort—no easy task in most orga-\nnizations where meeting room facilities are always in short supply. Although we’re \ndreaming, big ﬂ oor-to-ceiling whiteboards on all four walls would be nice, too! In \naddition to a meeting facility, the team needs some basic supplies, such as self-stick \nﬂ ip chart paper. A laptop projector is often useful during the design sessions and \nis absolutely required for the design reviews.\n Design the Dimensional Model\nAs outlined in Chapter 3: Retail Sales, there are four key decisions made during the \ndesign of a dimensional model:\n \n■Identify the business process.\n \n■Declare the grain of the business process.\n \n■Identify the dimensions.\n \n■Identify the facts.\nThe ﬁ rst step of identifying the business process is typically determined at the \nconclusion of the requirements gathering. The prioritization activity described in \nChapter 17 establishes which bus matrix row (and hence business process) will be \nmodeled. With that grounding, the team can proceed with the design tasks. \nThe modeling eff ort typically works through the following sequence of tasks and \ndeliverables, as illustrated in Figure 18-1: \n \n■High-level model deﬁ ning the model’s scope and granularity\n \n■Detailed design with table-by-table attributes and metrics\n \n■Review and validation with IT and business representatives\n \n■Finalization of the design documentation\n",
      "content_length": 2444,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 471,
      "content": "Dimensional Modeling Process and Tasks 435\nAs with any data modeling eff ort, dimensional modeling is an iterative process. \nYou will work back and forth between business requirements and source details to \nfurther reﬁ ne the model, changing the model as you learn more.\nThis section describes each of these major tasks. Depending on the design team’s \nexperience and exposure to dimensional modeling concepts, you might begin with basic \ndimensional modeling education before kicking off  the eff ort to ensure everyone is on \nthe same page regarding standard dimensional vocabulary and best practices.\n Reach Consensus on High-Level Bubble Chart\nThe  initial task in the design session is to create a high-level dimensional model \ndiagram for the target business process. Creating the ﬁ rst draft is relatively straight-\nforward because you start with the bus matrix. Although an experienced designer \ncould develop the initial high-level dimensional model and present it to the team \nfor review, we recommend against this approach because it does not allow the entire \nteam to participate in the process.\nThe high-level diagram graphically represents the business process’s dimension \nand fact tables. Shown in Figure 18-2, we often refer to this diagram as the bubble \nchart for obvious reasons. This entity-level graphical model clearly identiﬁ es the \ngrain of the fact table and its associated dimensions to a non-technical audience.\nOrder Date\nSold To\nCustomer\nCurrency\nPromotion\nChannel\nProduct\nOrder Profile\nSales\nPerson\nOrders\nGrain = 1 row\nper order line\nDue Date\nShip To\nCustomer\nBill To\nCustomer\nFigure 18-2: Sample high-level model diagram.\n",
      "content_length": 1656,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 472,
      "content": "Chapter 18\n436\nDeclaring the grain requires the modeling team to consider what is needed to \nmeet the business requirements and what is possible based on the data collected \nby the source system. The bubble chart must be rooted in the realities of available \nphysical data sources. A single row of the bus matrix may result in multiple bubble \ncharts, each corresponding to a unique fact table with unique granularity.\nMost of the major dimensions will fall out naturally after you determine the \ngrain. One of the powerful eff ects of a clear fact table grain declaration is you can \nprecisely visualize the associated dimensionality. Choosing the dimensions may \nalso cause you to rethink the grain declaration. If a proposed dimension doesn’t \nmatch the grain of the fact table, either the dimension must be left out, the grain \nof the fact table changed, or a multivalued design solution needs to be considered.\nFigure 18-2’s graphical representation serves several purposes. It facilitates \ndiscussion within the design team before the team dives into the detailed design, \nensuring everyone is on the same page before becoming inundated with minutiae. \nIt’s also a helpful introduction when the team communicates with interested stake-\nholders about the project, its scope, and data contents.\nTo aid in understanding, it is helpful to retain consistency across the high-level \nmodel diagrams for a given business process. Although each fact table is documented \non a separate page, arranging the associated dimensions in a similar sequence across \nthe bubble charts is useful.\nDevelop the Detailed Dimensional Model\nAfter  completing the high-level bubble chart designs, it’s time to focus on the details. \nThe team should meet on a very regular basis to deﬁ ne the detailed dimensional \nmodel, table by table, column by column. The business representatives should \nremain engaged during these interactive sessions; you need their feedback on attri-\nbutes, ﬁ lters, groupings, labels, and metrics. \nIt’s most eff ective to start with the dimension tables and then work on the fact \ntables. We suggest launching the detailed design process with a couple of straight-\nforward dimensions; the date dimension is always a favorite starting point. This \nenables the modeling team to achieve early success, develop an understanding of \nthe modeling process, and learn to work together as a team.\nThe detailed modeling identiﬁ es the interesting and useful attributes within each \ndimension and appropriate metrics for each fact table. You also want to capture the \nsources, deﬁ nitions, and preliminary business rules that specify how these attributes \nand metrics are populated. Ongoing analyses of the source system and systematic \ndata proﬁ ling during the design sessions helps the team better understand the reali-\nties of the underlying source data.\n",
      "content_length": 2855,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 473,
      "content": "Dimensional Modeling Process and Tasks 437\n Identify Dimensions and their Attributes\nDuring  the detailed design sessions, key conformed dimensions are deﬁ ned. Because \nthe DW/BI system is an enterprise resource, these deﬁ nitions must be acceptable \nacross the enterprise. The data stewards and business analysts are key resources to \nachieve organizational consensus on table and attribute naming, descriptions, and \ndeﬁ nitions. The design team can take the lead in driving the process and leverag-\ning naming conventions, if available. But it is ultimately a business task to agree on \nstandard business deﬁ nitions and names; the column names must make sense to \nthe business users. This can take some time, but it is an investment that will deliver \nhuge returns for the users’ understanding and willingness to accept the dimensional \nmodel. Don’t be surprised if the governance steering committee must get involved \nto resolve conformed dimension deﬁ nition and naming issues.\nAt this point, the modeling team often also wrestles with the potential inclu-\nsion of junk dimensions or mini-dimensions in a dimensional model. It may not \nbe apparent that these more performance-centric patterns are warranted until the \nteam is deeply immersed in the design.\n Identify the Facts\nDeclaring  the grain crystallizes the discussion about the fact table’s metrics because \nthe facts must all be true to the grain. The data proﬁ ling eff ort identiﬁ es the counts \nand amounts generated by the measurement event’s source system. However, fact \ntables are not limited to just these base facts. There may be additional metrics the \nbusiness wants to analyze that are derived from the base facts.\nIdentify Slowly Changing Dimension Techniques\nAfter  the dimension and fact tables from the high-level model diagram have been \ninitially drafted, you then circle back to the dimension tables. For each dimension \ntable attribute, you deﬁ ne how source system data changes will be reﬂ ected in the \ndimension table. Again, input from the business data stewards is critical to estab-\nlishing appropriate rules. It’s also helpful to ask the source system experts if they \ncan determine whether a data element change is due to a source data correction.\nDocument the Detailed Table Designs\nThe  key deliverables of the detailed modeling phase are the design work-\nsheets, as shown in Figure 18-3; a digital template is available on our website at \nwww.kimballgroup.com under the Tools and Utilities Tab for The Data Warehouse \nLifecycle Toolkit, Second Edition. The worksheets capture details for communication \nto interested stakeholders including other analytical business users, BI applica-\ntion developers, and most important, the ETL developers who will be tasked with \npopulating the design.\n",
      "content_length": 2786,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 474,
      "content": "OrderProﬁleKey\nSurrogate primary key\nSurrogate key\nsmallint\nvarchar\nvarchar\nvarchar\n8\n12\n14\n1\n1\n1\nOEI\nOEI\nOEI\nOrderHeader\nOrderHeader\nOrderHeader\nOrd_Meth\nOrd_Src\nComm_Code\nint\nchar\nint\n1=Phone, 2=Fax,\n3=Internet\nR=Reseller, D=Direct\nSales\n0=Non-Commission,\n1=Commission\n1, 2, 3...\nPhone, Fax,\nInternet\nReseller, Direct\nSales\nCommission, Non-\nCommission\nDerived\nOrderMethod\nMethod used to place order\n(phone, fax, internet)\nOrderSource\nCommissionInd\nColumn Name\nDescription\nSource of the order\n(reseller, direct sales)\nIndicates whether order is\ncommissionable or not\nDatatype\nSize\nExample Values\nSource\nSystem\nSource Table\nSource Field\nName\nSource\nDatatype\nETL Rules\nSCD\nType\nTable Name\nDimOrderProﬁle\nTable Type\ne\ncr\nu\no\nS\nte\ng\nr\na\nT\nDisplay Name\nDescription\nUsed in schemas\nSize\nDimension\nOrderProﬁle\nOrder Proﬁle is the “junk” dimension for miscellaneous information about order transactions\nOrders\n12 rows\nFigure 18-3: Sample detailed dimensional design worksheet.\n",
      "content_length": 970,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 475,
      "content": "Dimensional Modeling Process and Tasks 439\nEach dimension and fact table should be documented in a separate worksheet. At \na minimum, the supporting information required includes the attribute/fact name, \ndescription, sample values, and a slowly changing dimension type indicator for every \ndimension attribute. In addition, the detailed fact table design should identify each \nforeign key relationship, appropriate degenerate dimensions, and rules for each fact \nto indicate whether it’s additive, semi-additive, or non-additive.\nThe dimensional design worksheet is the ﬁ rst step toward creating the source-\nto-target mapping document. The physical design team will further ﬂ esh out the \nmapping with physical table and column names, data types, and key declarations.\nTrack Model Issues\nAny issues, deﬁ nitions, transformation rules, and data quality challenges discovered \nduring the design process should be captured in an issues tracking log. Someone \nshould be assigned the task of capturing and tracking issues during the sessions; the \nproject manager, if they’re participating in the design sessions, often handles this \nresponsibility because they’re typically adept at keeping the list updated and encour-\naging progress on resolving open issues. The facilitator should reserve adequate \ntime at the end of every session to review and validate new issue entries and their \nassignments. Between design sessions, the design team is typically busy proﬁ ling \ndata, seeking clariﬁ cation and agreement on common deﬁ nitions, and meeting with \nsource system experts to resolve outstanding issues.\nMaintain Updated Bus Matrix\nDuring  the detailed modeling process, there are often new discoveries about the \nbusiness process being modeled. Frequently, these ﬁ ndings result in the intro-\nduction of new fact tables to support the business process, new dimensions, or \nthe splitting or combining of dimensions. You must keep the bus matrix updated \nthroughout the design process because it is a key communication and planning \ntool. As discussed in Chapter 16: Insurance, the detailed bus matrix often captures \nadditional information about each fact table’s granularity and metrics.\nReview and Validate the Model\nOnce the design team is conﬁ dent about the model, the process moves into the \nreview and validation phase to get feedback from other interested parties, including:\n \n■IT resources, such as DW/BI team members not involved in the modeling \neff ort, source system experts, and DBAs\n \n■Analytical or power business users not involved in the modeling eff ort\n \n■Broader business user community\n",
      "content_length": 2609,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 476,
      "content": "Chapter 18\n440\nIT Reviews\nTypically,  the ﬁ rst review of the detailed dimensional model is with peers in the \nIT organization. This audience is often composed of reviewers who are intimately \nfamiliar with the target business process because they wrote or manage the system \nthat runs it. They are also at least partly familiar with the target data model because \nyou’ve already been pestering them with source data questions. \nIT reviews can be challenging because the participants often lack an understand-\ning of dimensional modeling. In fact, most of them probably fancy themselves as \nproﬁ cient 3NF modelers. Their tendency will be to apply transaction processing-\noriented modeling rules to the dimensional model. Rather than spending the bulk \nof your time debating the merits of diff erent modeling disciplines, it is best to proac-\ntively provide some dimensional modeling education as part of the review process.\nWhen everyone has the basic concepts down, you should begin with a review \nof the bus matrix. This gives everyone a sense of the project scope and overall data \narchitecture, demonstrates the role of conformed dimensions, and shows the relative \nbusiness process priorities. Next, illustrate how the selected row on the matrix trans-\nlates directly into the high-level dimensional model diagram. This gives everyone the \nentity-level map of the model and serves as the guide for the rest of the discussion.\nMost of the review session should be spent going through the dimension and fact \ntable worksheet details. It is also a good idea to review any remaining open issues \nfor each table as you work through the model.\nChanges to the model will likely result from this meeting. Remember to assign \nthe task of capturing the issues and recommendations to someone on the team.\nCore User Review\nIn many projects, this review is not required because the core business users are \nmembers of the modeling team and are already intimately knowledgeable about the \ndimensional model. Otherwise, this review meeting is similar in scope and structure \nto the IT review meeting. The core business users are more technical than typical \nbusiness users and can handle details about the model. In smaller organizations, \nwe often combine the IT review and core user review into one session.\nBroader Business User Review\nThis session is as much education as it is design review. You want to educate people \nwithout overwhelming them, while at the same time illustrating how the dimen-\nsional model supports their business requirements. You should start with the bus \nmatrix as the enterprise DW/BI data roadmap, review the high-level model bubble \ncharts, and ﬁ nally, review the critical dimensions, such as customer and product. \nSometimes the bubble charts are supplemented with diagrams similar to Figure \n18-4 to illustrate the hierarchical drill paths within a dimension.\n",
      "content_length": 2887,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 477,
      "content": "Dimensional Modeling Process and Tasks 441\nDepartment #\nDepartment Name\nCategory Code\nCategory Name\nSKU #\nProduct Name\nColor Code\nColor Name\nBrand Code\nBrand Name\nPackage Type ID\nPackage Type Desc\nFigure 18-4: Illustration of hierarchical attribute relationships for business users.\nBe sure to allocate time during this education/review to illustrate how the model \ncan be used to answer a broad range of questions about the business process. We \noften pull some examples from the requirements document and walk through how \nthey would be answered.\nFinalize the Design Documentation\nAfter  the model is in its ﬁ nal form, the design documentation should be compiled \nfrom the design team’s working papers. This document typically includes:\n \n■Brief description of the project\n \n■High-level data model diagram\n \n■Detailed dimensional design worksheet for each fact and dimension table\n \n■Open issues\nSummary\nDimensional modeling is an iterative design process requiring the cooperative \neff ort of people with a diverse set of skills, including business representatives. The \ndesign eff ort begins with an initial graphical model pulled from the bus matrix and \npresented at the entity level. The detailed modeling process drills down into the \ndeﬁ nitions, sources, relationships, data quality problems, and required transforma-\ntions for each table. The primary goals are to create a model that meets the business \nrequirements, verify the data is available to populate the model, and provide the \nETL team with a clear direction.\n",
      "content_length": 1532,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 478,
      "content": "Chapter 18\n442\nThe task of determining column and table names is interwoven into the design \nprocess. The organization as a whole must agree on the names, deﬁ nitions, and \nderivations of every column and table in the dimensional model. This is more of a \npolitical process than a technical one, which requires the full attention of the most \ndiplomatic team member. The resulting column names exposed through the BI tool \nmust make sense to the business community.\nThe detailed modeling eff ort is followed by several reviews. The end result is \na dimensional model that has been successfully tested against both the business \nneeds and data realities.\n",
      "content_length": 654,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 479,
      "content": "ETL Subsystems and \nTechniques\nT\nhe  extract, transformation, and load (ETL) system consumes a disproportionate \nshare of the time and effort required to build a DW/BI environment. Developing \nthe ETL system is challenging because so many outside constraints put pressure on its \ndesign: the business requirements, source data realities, budget, processing windows, \nand skill sets of the available staff. Yet it can be hard to appreciate just why the ETL \nsystem is so complex and resource-intensive. Everyone understands the three let-\nters: You get the data out of its original source location (E), you do something to it \n(T), and then you load it (L) into a final set of tables for the business users to query.\nWhen  asked about the best way to design and build the ETL system, many design-\ners say, “Well, that depends.” It depends on the source; it depends on limitations of \nthe data; it depends on the scripting languages and ETL tools available; it depends \non the staff ’s skills; and it depends on the BI tools. But the “it depends” response \nis dangerous because it becomes an excuse to take an unstructured approach to \ndeveloping an ETL system, which in the worse-case scenario results in an undif-\nferentiated spaghetti-mess of tables, modules, processes, scripts, triggers, alerts, and \njob schedules. This “creative” design approach should not be tolerated. With the \nwisdom of hindsight from thousands of successful data warehouses, a set of ETL best \npractices have emerged. There is no reason to tolerate an unstructured approach.\nCareful consideration of these best practices has revealed 34 subsystems are \nrequired in almost every dimensional data warehouse back room. No wonder the \nETL system takes such a large percentage of the DW/BI development resources!\nThis chapter is drawn from The Data Warehouse Lifecycle Toolkit, Second Edition \n(Wiley, 2008). Throughout the chapter we’ve sprinkled pointers to resources on \nthe Kimball Group’s website for more in-depth coverage of several ETL techniques.\nChapter 19 reviews the following concepts:\n \n■Requirements and constraints to be considered before designing the \nETL system\n \n■Three subsystems focused on extracting data from source systems\n19\n",
      "content_length": 2223,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 480,
      "content": "Chapter 19\n444\n \n■Five subsystems to deal with value-added cleaning and conforming, including \ndimensional structures to monitor quality errors\n \n■Thirteen subsystems to deliver data into now-familiar dimensional structures, \nsuch as a subsystem to implement slowly changing dimension techniques\n \n■Thirteen subsystems to help manage the production ETL environment\nRound Up the Requirements\nEstablishing the architecture of an ETL system begins with one of the toughest \nchallenges: rounding up the requirements. By this we mean gathering and under-\nstanding all the known requirements, realities, and constraints aff ecting the ETL \nsystem. The list of requirements can be pretty overwhelming, but it’s essential to \nlay them on the table before launching into the development of the ETL system.\nThe ETL system requirements are mostly constraints you must live with and adapt \nyour system to. Within the framework of these requirements, there are opportuni-\nties to make your own decisions, exercise judgment, and leverage creativity, but \nthe requirements dictate the core elements that the ETL system must deliver. The \nfollowing ten sections describe the major requirements areas that impact the design \nand development of the ETL system.\nBefore  launching the ETL design and development eff ort, you should provide a \nshort response for each of the following ten requirements. We have provided \na sample checklist (as a note) for each to get you started. The point of this exercise \nis to ensure you visit each of these topics because any one of them can be a show-\nstopper at some point in the project.\n Business Needs\nFrom  an ETL designer’s view, the business needs are the DW/BI system users’ infor-\nmation requirements. We use the term business needs somewhat narrowly here to \nmean the information content that business users need to make informed business \ndecisions. Because the business needs directly drive the choice of data sources and \ntheir subsequent transformation in the ETL system, the ETL team must understand \nand carefully examine the business needs.\nNOTE \nYou should maintain a list of the key performance indicators (KPIs) \nuncovered during the business requirements deﬁ nition that the project intends to \nsupport, as well as the drill-down and drill-across targets required when a business \nuser needs to investigate “why?” a KPI changed.\n",
      "content_length": 2369,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 481,
      "content": "ETL Subsystems and Techniques 445\nCompliance\nChanging  legal and reporting requirements have forced many organizations to \nseriously tighten their reporting and provide proof that the reported numbers are \naccurate, complete, and have not been tampered with. Of course, DW/BI systems in \nregulated businesses, such as telecommunications, have complied with regulatory \nreporting requirements for years. But certainly the whole tenor of ﬁ nancial report-\ning has become much more rigorous for everyone.\nNOTE \nIn consultation with your legal department or chief compliance offi  cer (if \nyou have one!) and the BI delivery team, you should list all data and ﬁ nal reports \nsubject to compliance restrictions. List those data inputs and data transformation \nsteps for which you must maintain the “chain of custody” showing and proving \nthat ﬁ nal reports were derived from the original data delivered from your data \nsources. List the data that you must provide proof of security for the copies under \nyour control, both offl  ine and online. List those data copies you must archive, and \nlist the expected usable lifetime of those archives. Good luck with all this. This is \nwhy you are paid so well….\nData Quality\nThree  powerful forces have converged to put data quality concerns near the top of \nthe list for executives. First, the long-term cultural trend that says, “If only I could \nsee the data, then I could manage my business better” continues to grow; today’s \nknowledge workers believe instinctively that data is a crucial requirement for them \nto function in their jobs. Second, most organizations understand their data sources \nare profoundly distributed, typically around the world, and that eff ectively integrat-\ning a myriad of disparate data sources is required. And third, the sharply increased \ndemands for compliance mean careless handling of data will not be overlooked or \nexcused.\nNOTE \nYou should list those data elements whose quality is known to be unac-\nceptable, and list whether an agreement has been reached with the source systems \nto correct the data before extraction. List those data elements discovered during \ndata proﬁ ling, which will be continuously monitored and ﬂ agged as part of the \nETL process.\n",
      "content_length": 2239,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 482,
      "content": "Chapter 19\n446\nSecurity\nSecurity  awareness has increased signiﬁ cantly in the last few years across IT but \noften remains an afterthought and an unwelcome burden to most DW/BI teams. \nThe basic rhythms of the data warehouse are at odds with the security mentality; \nthe data warehouse seeks to publish data widely to decision makers, whereas the \nsecurity interests assume data should be restricted to those with a need to know. \nAdditionally, security must be extended to physical backups. If the media can easily \nbe removed from the backup vault, then security has been compromised as eff ec-\ntively as if the online passwords were compromised.\nDuring the requirements roundup, the DW/BI team should seek clear guidance \nfrom senior management as to what aspects of the DW/BI system carry extra secu-\nrity sensitivity. If these issues have never been examined, it is likely the question \nwill be tossed back to the team. That is the moment when an experienced security \nmanager should be invited to join the design team. Compliance requirements are \nlikely to overlap security requirements; it may be wise to combine these two topics \nduring the requirements  roundup.\nNOTE \nYou should expand the compliance checklist to encompass known secu-\nrity and privacy requirements.\nData Integration\nData  integration is a huge topic for IT because, ultimately, it aims to make all sys-\ntems seamlessly work together. The “360 degree view of the enterprise” is a famil-\niar name for data integration. In many cases, serious data integration must take \nplace among the organization’s primary transaction systems before data arrives at \nthe data warehouse’s back door. But rarely is that data integration complete, unless the \norganization has a comprehensive and centralized master data management (MDM) \nsystem, and even then it’s likely other important operational systems exist outside \nthe primary MDM system.\nData integration usually takes the form of conforming dimensions and con-\nforming facts in the data warehouse. Conforming dimensions means establishing \ncommon dimensional attributes across separated databases, so drill-across reports \ncan be generated using these attributes. Conforming facts means making agree-\nments on common business metrics such as key performance indicators (KPIs) \nacross separated databases so these numbers can be compared mathematically by \ncalculating diff erences and ratios.\n",
      "content_length": 2414,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 483,
      "content": "ETL Subsystems and Techniques 447\nNOTE \nYou should use the bus matrix of business processes to generate a priority \nlist for conforming dimensions (columns of the bus matrix). Annotate each row \nof the bus matrix with whether there is a clear executive demand for the busi-\nness process to participate in the integration process, and whether the ETL team \nresponsible for that business process has agreed.\nData Latency\nData  latency describes how quickly source system data must be delivered to the \nbusiness users via the DW/BI system. Obviously, data latency requirements have a \nhuge eff ect on the ETL architecture. Clever processing algorithms, parallelization, \nand potent hardware can speed up traditional batch-oriented data ﬂ ows. But at \nsome point, if the data latency requirement is suffi  ciently urgent, the ETL system’s \narchitecture must convert from batch to microbatch or streaming-oriented. This \nswitch isn’t a gradual or evolutionary change; it’s a major paradigm shift in which \nalmost every step of the data delivery pipeline must be re-implemented.\nNOTE \nYou should list all legitimate and well-vetted business demands for data \nthat must be provided on a daily basis, on a many times per day basis, within a few \nseconds, or instantaneously. Annotate each demand with whether the business \ncommunity understands the data quality trade-off s associated with their particu-\nlar choice. Near the end of Chapter 20: ETL System Design and Development \nProcess and Tasks, we discuss data quality compromises caused by low latency \nrequirements.\nArchiving and Lineage\nArchiving  and lineage requirements were hinted at in the previous compliance and \nsecurity sections. Even without the legal requirements for saving data, every data \nwarehouse needs various copies of old data, either for comparisons with new \ndata to generate change capture records or reprocessing. We recommend staging \nthe data (writing it to disk) after each major activity of the ETL pipeline: after it’s \nbeen extracted, cleaned and conformed, and delivered.\nSo when does staging turn into archiving where the data is kept indeﬁ nitely on \nsome form of permanent media? Our simple answer is a conservative answer. All \nstaged data should be archived unless a conscious decision is made that speciﬁ c \ndata sets will never be recovered in the future. It’s almost always less problematic \nto read the data from permanent media than it is to reprocess the data through \nthe ETL system at a later time. And, of course, it may be impossible to reprocess \n",
      "content_length": 2543,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 484,
      "content": "Chapter 19\n448\nthe data according to the old processing algorithms if enough time has passed \nor the original extraction cannot be re-created.\nAnd while we are at it, each staged/archived data set should have accompanying \nmetadata describing the origins and processing steps that produced the data. Again, \nthe tracking of this lineage is explicitly required by certain compliance requirements \nbut should be part of every archiving situation.\nNOTE \nYou should list the data sources and intermediate data steps that will be \narchived, together with retention policies, and compliance, security, and privacy \nconstraints.\nBI Delivery Interfaces\nThe  ﬁ nal step for the ETL system is the handoff  to the BI applications. We take a \nstrong and disciplined position on this handoff . We believe the ETL team, working \nclosely with the modeling team, must take responsibility for the content and struc-\nture of the data that makes the BI applications simple and fast. This attitude is more \nthan a vague motherhood statement. We believe it’s irresponsible to hand off  data to \nthe BI application in such a way as to increase the complexity of the application, slow \ndown the query or report creation, or make the data seem unnecessarily complex \nto the business users. The most elementary and serious error is to hand across a \nfull-blown, normalized physical model and walk away from the job. This is why we \ngo to such lengths to build dimensional structures that comprise the ﬁ nal handoff .\nThe ETL team and data modelers need to closely work with the BI application \ndevelopers to determine the exact requirements for the data handoff . Each BI tool \nhas certain sensitivities that should be avoided and certain features that can be \nexploited if the physical data is in the right format. The same considerations apply \nto data prepared for OLAP cubes.\nNOTE \nYou should list all fact and dimension tables that will be directly exposed \nto your BI tools. This should come directly from the dimensional model speciﬁ -\ncation. List all OLAP cubes and special database structures required by BI tools. \nList all known indexes and aggregations you have agreed to build to support BI \nperformance.\nAvailable Skills\nSome  ETL system design decisions must be made on the basis of available resources \nto build and manage the system. You shouldn’t build a system that depends on \n",
      "content_length": 2373,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 485,
      "content": "ETL Subsystems and Techniques 449\ncritical C++ processing modules if those programming skills aren’t in-house or can’t \nbe reasonably acquired. Likewise, you may be much more conﬁ dent in building \nthe ETL system around a major vendor’s ETL tool if you already have those skills \nin-house and know how to manage such a project.\nConsider the big decision of whether to hand code the ETL system or use a ven-\ndor’s ETL package. Technical issues and license costs aside, don’t go off  in a direction \nthat your employees and managers ﬁ nd unfamiliar without seriously considering \nthe decision’s long-term implications.\nNOTE \nYou should inventory your department’s operating system, ETL tool, \nscripting language, programming language, SQL, DBMS, and OLAP skills so you \nunderstand how exposed you are to a shortage or loss of these skills. List those \nskills required to support your current systems and your likely future systems.\nLegacy Licenses\nFinally,  in many cases, major design decisions will be made implicitly by senior \nmanagement’s insistence that you use existing legacy licenses. In many cases, this \nrequirement is one you can live with because the environmental advantages are clear \nto everyone. But in a few cases, the use of a legacy license for ETL development is \na mistake. This is a diffi  cult position to be in, and if you feel strongly enough, you \nmay need to bet your job. If you must approach senior management and challenge \nthe use of an existing legacy license, be well prepared in making the case, and be \nwilling to accept the ﬁ nal decision or possibly seek employment elsewhere.\nNOTE \nYou should list your legacy operating system, ETL tool, scripting lan-\nguage, programming language, SQL, DBMS, and OLAP licenses and whether their \nexclusive use is mandated or merely recomm ended.\nThe 34 Subsystems of ETL\nWith  an understanding of the existing requirements, realities, and constraints, \nyou’re ready to learn about the 34 critical subsystems that form the architec-\nture for every ETL system. This chapter describes all 34 subsystems with equal \nemphasis. The next chapter then describes the practical steps of implementing \nthose subsystems needed for each particular situation. Although we have adopted \nthe industry vernacular, ETL, to describe these steps, the process really has four \nmajor components: \n",
      "content_length": 2346,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 486,
      "content": "Chapter 19\n450\n \n■Extracting.  Gathering raw data from the source systems and usually writing \nit to disk in the ETL environment before any signiﬁ cant restructuring of the \ndata takes place. Subsystems 1 through 3 support the extracting process.\n \n■Cleaning and conforming.  Sending source data through a series of processing \nsteps in the ETL system to improve the quality of the data received from the \nsource, and merging data from two or more sources to create and enforce con-\nformed dimensions and conformed metrics. Subsystems 4 through 8 describe \nthe architecture required to support the cleaning and conforming processes.\n \n■Delivering.  Physically structuring and loading the data into the presentation \nserver’s target dimensional models. Subsystems 9 through 21 provide the \ncapabilities for delivering the data to the presentation server.\n \n■Managing.  Managing the related systems and processes of the ETL \nenvironment in a coherent manner. Subsystems 22 through 34 describe the \ncomponents needed to support the ongoing management of the ETL system.\nExtracting: Getting Data into the Data \nWarehouse\nTo no surprise, the initial subsystems of the ETL architecture address the issues of \nunderstanding your source data, extracting the data, and transferring it to the data \nwarehouse environment where the ETL system can operate on it independent of \nthe operational systems. Although the remaining subsystems focus on the trans-\nforming, loading, and system management within the ETL environment, the initial \nsubsystems interface to the source systems for access to the required data.\nSubsystem 1: Data Proﬁ ling\nData  proﬁ ling is the technical analysis of data to describe its content, consistency, \nand structure. In some sense, any time you perform a SELECT DISTINCT investiga-\ntive query on a database ﬁ eld, you are doing data proﬁ ling. There are a variety of \ntools speciﬁ cally designed to do powerful proﬁ ling. It probably pays to invest in \na tool rather than roll your own because the tools enable many data relationships \nto be easily explored with simple user interface gestures. You can be much more \nproductive in the data proﬁ ling stages of a project using a tool rather than hand \ncoding all the data content questions.\nData proﬁ ling plays two distinct roles: strategic and tactical. As soon as a can-\ndidate data source is identiﬁ ed, a light proﬁ ling assessment should be made to \ndetermine its suitability for inclusion in the data warehouse and provide an early \ngo/no go decision. Ideally, this strategic assessment should occur immediately after \n",
      "content_length": 2592,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 487,
      "content": "ETL Subsystems and Techniques 451\nidentifying a candidate data source during the business requirements analysis. Early \ndisqualiﬁ cation of a data source is a responsible step that can earn you respect from \nthe rest of the team, even if it is bad news. A late revelation that the data source \ndoesn’t support the mission can knock the DW/BI initiative off  its tracks (and be a \npotentially fatal career outcome for you), especially if this revelation occurs months \ninto a project.\nAfter the basic strategic decision is made to include a data source in the project, a \nlengthy tactical data proﬁ ling eff ort should occur to squeeze out as many problems \nas possible. Usually, this task begins during the data modeling process and extends \ninto the ETL system design process. Sometimes, the ETL team is expected to include \na source with content that hasn’t been thoroughly evaluated. Systems may support \nthe needs of the production processes, yet present ETL challenges, because ﬁ elds \nthat aren’t central to production processing may be unreliable and incomplete for \nanalysis purposes. Issues that show up in this subsystem result in detailed speciﬁ -\ncations that are either 1) sent back to the originator of the data source as requests \nfor improvement or 2) form requirements for the data quality processing described \nin subsystems 4 through 8.\nThe proﬁ ling step provides the ETL team with guidance as to how much data \ncleaning machinery to invoke and protects them from missing major project mile-\nstones due to the unexpected diversion of building systems to deal with dirty data. \nDo the data proﬁ ling upfront! Use the data proﬁ ling results to set the business \nsponsors’ expectations regarding realistic development schedules, limitations in the \nsource data, and the need to invest in better source data capture practices.\nSubsystem 2: Change Data Capture System\nDuring  the data warehouse’s initial historic load, capturing source data content \nchanges is not important because you load all data from a point in time forward. \nHowever, many data warehouse tables are so large that they cannot be refreshed dur-\ning every ETL cycle. You must have a capability to transfer only the relevant changes \nto the source data since the last update. Isolating the latest source data is called \nchange data capture (CDC). The idea behind CDC is simple enough: Just transfer \nthe data that has changed since the last load. But building a good CDC system is \nnot as easy as it sounds. The key goals for the change data capture subsystem are:\n \n■Isolate the changed source data to allow selective processing rather than a \ncomplete refresh.\n \n■Capture all changes (deletions, edits, and insertions) made to the source data, \nincluding changes made through nonstandard interfaces.\n \n■Tag changed data with reason codes to distinguish error corrections from \ntrue updates. \n",
      "content_length": 2879,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 488,
      "content": "Chapter 19\n452\n \n■Support compliance tracking with additional metadata.\n \n■Perform the CDC step as early as possible, preferably before a bulk data \ntransfer to the data warehouse.\nCapturing data changes is far from a trivial task. You must carefully evaluate \nyour strategy for each data source. Determining the appropriate strategy to identify \nchanged data may take some detective work. The data proﬁ ling tasks described \nearlier can help the ETL team make this determination. There are several ways to \ncapture source data changes, each eff ective in the appropriate situation, including: \nAudit Columns\nIn  some cases, the source system includes audit columns that store the date and time \na record was added or modiﬁ ed. These columns are usually populated via database \ntriggers that are ﬁ red off  automatically as records are inserted or updated. Sometimes, \nfor performance reasons, the columns are populated by the source application instead \nof database triggers. When these ﬁ elds are loaded by any means other than database \ntriggers, pay special attention to their integrity, analyzing and testing each column \nto ensure that it’s a reliable source to indicate change. If you uncover any NULL \nvalues, you must ﬁ nd an alternative approach for detecting change. The most com-\nmon situation that prevents the ETL system from using audit columns is when the \nﬁ elds are populated by the source application, but the DBA team allows back-end \nscripts to modify data. If this occurs in your environment, you face a high risk of \nmissing changed data during the incremental loads. Finally, you need to understand \nwhat happens when a record is deleted from the source because querying the audit \ncolumn may not capture this event.\nTimed Extracts\nWith  a timed extract, you typically select all rows where the create or modiﬁ ed \ndate ﬁ elds equal SYSDATE-1, meaning all of yesterday’s records. Sounds perfect, \nright? Wrong. Loading records based purely on time is a common mistake made by \ninexperienced ETL developers. This process is horribly unreliable. Time-based data \nselection loads duplicate rows when it is restarted from mid-process failures. This \nmeans manual intervention and data cleanup is required if the process fails for any \nreason. Meanwhile, if the nightly load process fails to run and skips a day, there’s a \nrisk that the missed data will never make it into the data warehouse.\nFull Diff Compare\nA  full diff  compare keeps a full snapshot of yesterday’s data, and compares it, record \nby record, against today’s data to ﬁ nd what changed. The good news is this tech-\nnique is thorough: You are guaranteed to ﬁ nd every change. The obvious bad news is \nthat, in many cases, this technique is very resource-intensive. If a full diff  compare \n",
      "content_length": 2777,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 489,
      "content": "ETL Subsystems and Techniques 453\nis required, try to do the comparison on the source machine, so you don’t have \nto transfer the entire table or database into the ETL environment. Of course, the \nsource support folks may have an opinion about this. Also, investigate using cyclic \nredundancy checksum (CRC) algorithms to quickly tell if a complex record has \nchanged without examining each individual ﬁ eld.\nDatabase Log Scraping\nLog  scraping eff ectively takes a snapshot of the database redo log at a scheduled \npoint in time (usually midnight) and scours it for transactions aff ecting the tables \nof interest for the ETL load. Sniffi  ng involves a polling of the redo log, capturing \ntransactions on-the-ﬂ y. Scraping the log for transactions is probably the messiest of \nall techniques. It’s not uncommon for transaction logs to get full and prevent new \ntransactions from processing. When this happens in a production transaction envi-\nronment, the knee-jerk reaction from the responsible DBA may be to empty the log \nso that business operations can resume, but when a log is emptied, all transactions \nwithin them are lost. If you’ve exhausted all other techniques and ﬁ nd log scraping \nis your last resort for ﬁ nding new or changed records, persuade the DBA to create \na special log to meet your speciﬁ c needs.\nMessage Queue Monitoring\nIn  a message-based transaction system, the queue is monitored for all transactions \nagainst the tables of interest. The contents of the stream are similar to what you get \nwith log sniffi  ng. One beneﬁ t of this process is relatively low overhead, assuming \nthe message queue is already in place. However, there may be no replay feature \non the message queue. If the connection to the message queue is lost, you lose data.\nSubsystem 3: Extract System\nObviously,  extracting data from the source systems is a fundamental component \nof the ETL architecture. If you are extremely lucky, all the source data will be in a \nsingle system that can be readily extracted using an ETL tool. In the more common \nsituation, each source might be in a diff erent system, environment, and/or DBMS.\nThe ETL system might be expected to extract data from a wide variety of systems \ninvolving many diff erent types of data and inherent challenges. Organizations need-\ning to extract data from mainframe environments often run into issues involving \nCOBOL copybooks, EBCDIC to ASCII conversions, packed decimals, redeﬁ nes, \nOCCURS ﬁ elds, and multiple and variable record types. Other organizations might \nneed to extract from sources in relational DBMS, ﬂ at ﬁ les, XML sources, web logs, \nor a complex ERP system. Each presents a variety of possible challenges. Some \nsources, especially older legacy systems, may require the use of diff erent procedural \nlanguages than the ETL tool can support or the team is experienced with. In this \n",
      "content_length": 2873,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 490,
      "content": "Chapter 19\n454\nsituation, request that the owner of the source system extract the data into a ﬂ at \nﬁ le format.\nNOTE \nAlthough XML-formatted data has many advantages because it is self-\ndescribing, you may not want it for large, frequent data transfers. The payload \nportion of a typical XML formatted ﬁ le can be less than 10 percent of the total \nﬁ le. The exception to this recommendation could be where the XML payload is \na complex deeply hierarchical XML structure, such as an industry standard data \nexchange. In these cases, the DW/BI team must decide whether to “shred” the XML \ninto a large number of destination tables or persist the XML structure within the \ndata warehouse. Recent advances in RDBMS vendors’ support for XML via XPath \nhave made this latter option feasible.\nThere are two primary methods for getting data from a source system: as a ﬁ le \nor a stream. If the source is an aging mainframe system, it is often easier to extract \ninto ﬁ les and then move those ﬁ les to the ETL server.\nNOTE \nIf the source data is unstructured, semistructured, or even hyperstruc-\ntured “big data,” then rather than loading such data as an un-interpretable RDBMS \n“blob,” it is often more eff ective to create a MapReduce/Hadoop extract step that \nbehaves as an ETL fact extractor from the source data, directly delivering loadable \nRDBMS data.\nIf you use an ETL tool and the source data is in a database (not necessarily an \nRDBMS), you may set up the extract as a stream where the data ﬂ ows out of the \nsource system, through the transformation engine, and into the staging database \nas a single process. By contrast, an extract to ﬁ le approach consists of three or four \ndiscrete steps: Extract to the ﬁ le, move the ﬁ le to the ETL server, transform the ﬁ le \ncontents, and load the transformed data into the staging database.\nNOTE \nAlthough the stream extract is more appealing, extracts to ﬁ le have some \nadvantages. They are easy to restart at various points. As long as you save the extract \nﬁ le, you can rerun the load without impacting the source system. You can easily \nencrypt and compress the data before transferring across the network. Finally, it \nis easy to verify that all data has moved correctly by comparing ﬁ le row counts \nbefore and after the transfer. Generally, we recommend a data transfer utility such \nas FTP to move the extracted ﬁ le.\nData  compression is important if large amounts of data need to be transferred over \na signiﬁ cant distance or through a public network. In this case, the communications \n",
      "content_length": 2550,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 491,
      "content": "ETL Subsystems and Techniques 455\nlink is often the bottleneck. If too much time is spent transmitting the data, com-\npression can reduce the transmission time by 30 to 50 percent or more, depending \non the nature of the original data ﬁ le.\nData encryption is important if data is transferred through a public network, or \neven internally in some situations. If this is the case, it is best to send everything \nthrough an encrypted link and not worry about what needs to be secure and what \ndoesn’t. Remember to compress before encrypting because encrypted ﬁ les do not \ncompress very well.\nCleaning and Conforming Data\nCleaning and conforming data are critical ETL system tasks. These are the steps \nwhere the ETL system adds value to the data. The other activities, extracting and \ndelivering data, are obviously necessary, but they simply move and load the data. \nThe cleaning and conforming subsystems actually change data and enhance its \nvalue to the organization. In addition, these subsystems can be architected to create \nmetadata used to diagnosis what’s wrong with the source systems. Such diagnoses \ncan eventually lead to business process reengineering initiatives to address the root \ncauses of dirty data and improve data quality over time.\nImproving Data Quality Culture and Processes\nIt  is tempting to blame the original data source for any and all errors that appear \ndownstream. If only the data entry clerks were more careful! We are only slightly \nmore forgiving of keyboard-challenged salespeople who enter customer and product \ninformation into their order forms. Perhaps you can ﬁ x data quality problem by \nimposing constraints on the data entry user interfaces. This approach provides a \nhint about how to think about ﬁ xing data quality because a technical solution often \navoids the real problem. Suppose Social Security number ﬁ elds for customers were \noften blank or ﬁ lled with garbage on an input screen. Someone comes up with brilliant \nidea to require input in the 999-99-9999 format, and to cleverly disallow nonsensi-\ncal entries such as all 9s. What happens? The data entry clerks are forced to supply \nvalid Social Security numbers to progress to the next screen, so when they don’t have \nthe customer’s number, they type in an artiﬁ cial number that passes the roadblock.\nMichael Hammer, in his revolutionary book Reengineering the Corporation \n(Collins, revised 2003), struck the heart of the data quality problem with a bril-\nliant observation. Paraphrasing Hammer: “Seemingly small data quality issues are, \nin reality, important indications of broken business processes.” Not only does this \ninsight correctly focus your attention on the source of data quality problems, but it \nalso shows you the way to the solution.\n",
      "content_length": 2764,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 492,
      "content": "Chapter 19\n456\nTechnical attempts to address data quality will not prevail unless they are part \nof an overall quality culture that must come from the top of an organization. The \nfamous Japanese car manufacturing quality attitude permeates every level of those \norganizations, and quality is embraced enthusiastically by all levels, from the CEO \nto the assembly line worker. To cast this in a data context, imagine a company such \nas a large drugstore chain, where a team of buyers contracts with thousands of sup-\npliers to provide the inventory. The buyers have assistants, whose job it is to enter \nthe detailed descriptions of everything purchased by the buyers. These descriptions \ncontain dozens of attributes. But the problem is the assistants have a deadly job and \nare judged on how many items they enter per hour. The assistants have almost no \nawareness of who uses their data. Occasionally, the assistants are scolded for obvious \nerrors. But more insidiously, the data given to the assistants is itself incomplete and \nunreliable. For example, there are no formal standards for toxicity ratings, so there \nis signiﬁ cant variation over time and over product categories for this attribute. How \ndoes the drugstore improve data quality? Here is a nine-step template, not only for \nthe drugstore, but for any organization addressing data quality:\n \n■Declare a high-level commitment to a data quality culture.\n \n■Drive process reengineering at the executive level.\n \n■Spend money to improve the data entry environment.\n \n■Spend money to improve application integration.\n \n■Spend money to change how processes work.\n \n■Promote end-to-end team awareness.\n \n■Promote interdepartmental cooperation.\n \n■Publicly celebrate data quality excellence.\n \n■Continuously measure and improve data quality.\nAt the drugstore, money needs to be spent to improve the data entry system, so \nit provides the content and choices needed by the buyers’ assistants. The company’s \nexecutives need to assure the buyers’ assistants that their work is important and \naff ects many decision makers in a positive way. Diligent eff orts by the assistants \nshould be publicly praised and rewarded. And end-to-end team awareness and \nappreciation of the business value derived from quality data is the ﬁ nal goal.\nSubsystem 4: Data Cleansing System\nThe  ETL data cleansing process is often expected to ﬁ x dirty data, yet at the same \ntime the data warehouse is expected to provide an accurate picture of the data as it \nwas captured by the organization’s production systems. Striking the proper balance \nbetween these conﬂ icting goals is essential.\nOne of our goals in describing the cleansing system is to off er a comprehensive \narchitecture for cleansing data, capturing data quality events, as well as measuring \n",
      "content_length": 2798,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 493,
      "content": "ETL Subsystems and Techniques 457\nand ultimately controlling data quality in the data warehouse. Some organizations \nmay ﬁ nd this architecture challenging to implement, but we are convinced it is \nimportant for the ETL team to make a serious eff ort to incorporate as many of these \ncapabilities as possible. If you are new to ETL and ﬁ nd this a daunting challenge, \nyou might well wonder, “What’s the minimum I should focus on?” The answer is \nto start by undertaking the best possible data proﬁ ling analysis. The results of that \neff ort can help you understand the risks of moving forward with potentially dirty \nor unreliable data and help you determine how sophisticated your data cleansing \nsystem needs to be.\nThe purpose of the cleansing subsystems is to marshal technology to support \ndata quality. Goals for the subsystem should include:\n \n■Early diagnosis and triage of data quality issues\n \n■Requirements for source systems and integration eff orts to supply better data\n \n■Provide speciﬁ c descriptions of data errors expected to be encountered in ETL\n \n■Framework for capturing all data quality errors and precisely measuring data \nquality metrics over time\n \n■Attachment of quality conﬁ dence metrics to ﬁ nal data\nQuality Screens\nThe  heart of the ETL architecture is a set of quality screens that act as diagnostic \nﬁ lters in the data ﬂ ow pipelines. Each quality screen is a test. If the test against the \ndata is successful, nothing happens and the screen has no side eff ects. But if the test \nfails, then it must drop an error event row into the error event schema and choose \nto either halt the process, send the off ending data into suspension, or merely tag \nthe data.\nAlthough all quality screens are architecturally similar, it is convenient to divide \nthem into three types, in ascending order of scope. Jack Olson, in his seminal book \nData Quality: The Accuracy Dimension (Morgan Kaufmann, 2002), classiﬁ ed data \nquality screens into three categories: column screens, structure screens, and busi-\nness rule screens.\nColumn  screens test the data within a single column. These are usually simple, \nsomewhat obvious tests, such as testing whether a column contains unexpected \nnull values, if a value falls outside of a prescribed range, or if a value fails to adhere \nto a required format.\nStructure  screens test the relationship of data across columns. Two or more \nattributes may be tested to verify they implement a hierarchy, such as a series of \nmany-to-one relationships. Structure screens also test foreign key/primary key rela-\ntionships between columns in two tables, and also include testing whole blocks of \ncolumns to verify they implement valid postal addresses.\n",
      "content_length": 2710,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 494,
      "content": "Chapter 19\n458\nBusiness  rule screens implement more complex tests that do not ﬁ t the simpler \ncolumn or structure screen categories. For example, a customer proﬁ le may be tested \nfor a complex time-dependent business rule, such as requiring a lifetime platinum \nfrequent ﬂ yer to have been a member for at least ﬁ ve years and have ﬂ own more \nthan 2 million miles. Business rule screens also include aggregate threshold data \nquality checks, such as checking to see if a statistically improbable number of MRI \nexaminations have been ordered for minor diagnoses like a sprained elbow. In this \ncase, the screen throws an error only after a threshold of such MRI exams is reached.\nResponding to Quality Events\nWe  have already remarked that each quality screen has to decide what happens \nwhen an error is thrown. The choices are: 1) halting the process; 2) sending the \noff ending record(s) to a suspense ﬁ le for later processing; and 3) merely tagging \nthe data and passing it through to the next step in the pipeline. The third choice is \nby far the best choice, whenever possible. Halting the process is obviously a pain \nbecause it requires manual intervention to diagnose the problem, restart or resume \nthe job, or abort completely. Sending records to a suspense ﬁ le is often a poor solu-\ntion because it is not clear when or if these records will be ﬁ xed and re-introduced \nto the pipeline. Until the records are restored to the data ﬂ ow, the overall integrity \nof the database is questionable because records are missing. We recommend not \nusing the suspense ﬁ le for minor data transgressions. The third option of tagging \nthe data with the error condition often works well. Bad fact table data can be tagged \nwith the audit dimension, as described in subsystem 6. Bad dimension data can \nalso be tagged using an audit dimension, or in the case of missing or garbage data \ncan be tagged with unique error values in the attribute itself.\n Subsystem 5: Error Event Schema\nThe  error event schema is a centralized dimensional schema whose purpose is to \nrecord every error event thrown by a quality screen anywhere in the ETL pipeline. \nAlthough we focus on data warehouse ETL processing, this approach can be used in \ngeneric data integration (DI) applications where data is being transferred between \nlegacy applications. The error event schema is shown in Figure 19-1.\nThe main table is the error event fact table. Its grain is every error thrown (pro-\nduced) by a quality screen anywhere in the ETL system. Remember the grain of \na fact table is the physical description of why a fact table row exists. Thus every \nquality screen error produces exactly one row in this table, and every row in the \ntable corresponds to an observed error.\nThe dimensions of the error event fact table include the calendar date of the \nerror, the batch job in which the error occurred, and the screen that produced \nthe error. The calendar date is not a minute and second time stamp of the error, \n",
      "content_length": 2997,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 495,
      "content": "ETL Subsystems and Techniques 459\nbut rather provides a way to constrain and summarize error events by the usual \nattributes of the calendar, such as weekday or last day of a ﬁ scal period. The error \ndate/time fact is a full relational date/time stamp that speciﬁ es precisely when the \nerror occurred. This format is useful for calculating the time interval between error \nevents because you can take the diff erence between two date/time stamps to get the \nnumber of seconds separating events.\nDate Dimension\nError Event Date Key (PK)\n...\nBatch Key (PK)\n...\nBatch Dimension\nError Event Key (PK)\nError Event Date Key (FK)\nScreen Key (FK)\nBatch Key (FK)\nError Date/Time\nSeverity Score\nError Event Fact \nScreen Key (PK)\nScreen Type\nETL Module\nScreen Processing Definition\nException Action\nScreen Dimension\nError Event Key (FK)\nError Event Date Key (FK)\nScreen Key (FK)\nBatch Key (FK)\nError Date/Time\nTable Key (FK)\nField Key (FK)\nRecord Identifier Key (FK)\nError Condition\nError Event Detail Fact \nFigure 19-1: Error event schema.\nThe batch dimension can be generalized to be a processing step in cases in which \ndata is streamed, rather than batched. The screen dimension identiﬁ es precisely \nwhat the screen criterion is and where the code for the screen resides. It also deﬁ nes \nwhat to do when the screen throws an error. (For example, halt the process, send \nthe record to a suspense ﬁ le, or tag the data.)\nThe error event fact table also has a single column primary key, shown as the \nerror event key. This surrogate key, like dimension table primary keys, is a simple \ninteger assigned sequentially as rows are added to the fact table. This key column \nis necessary in those situations in which an enormous burst of error rows is added \nto the error event fact table all at once. Hopefully this won’t happen to you.\nThe error event schema includes a second error event detail fact table at a lower \ngrain. Each row in this table identiﬁ es an individual ﬁ eld in a speciﬁ c record that \nparticipated in an error. Thus a complex structure or business rule error that triggers \na single error event row in the higher level error event fact table may generate many \nrows in this error event detail fact table. The two tables are tied together by the error \nevent key, which is a foreign key in this lower grain table. The error event detail \n",
      "content_length": 2349,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 496,
      "content": "Chapter 19\n460\ntable identiﬁ es the table, record, ﬁ eld, and precise error condition. Thus a complete \ndescription of complex multi-ﬁ eld, multi-record errors is preserved by these tables.\nThe error event detail table could also contain a precise date/time stamp to \nprovide a full description of aggregate threshold error events where many records \ngenerate an error condition over a period of time. You should now appreciate that \neach quality screen has the responsibility for populating these tables at the time \nof an error.\n Subsystem 6: Audit Dimension Assembler\nThe  audit dimension is a special dimension that is assembled in the back room \nby the ETL system for each fact table, as we discussed in Chapter 6: Order \nManagement. The audit dimension in Figure 19-2 contains the metadata context at \nthe moment when a speciﬁ c fact table row is created. You might say we have elevated \nmetadata to real data! To visualize how audit dimension rows are created, imagine \nthis shipments fact table is updated once per day from a batch ﬁ le. Suppose today \nyou have a perfect run with no errors ﬂ agged. In this case, you would generate only \none audit dimension row, and it would be attached to every fact row loaded today. \nAll the categories, scores, and version numbers would be the same.\nShip Date Key (FK)\nCustomer Key (FK)\nProduct Key (FK)\nMore FKs ...\nAudit Key (FK)\nOrder Number (DD)\nOrder Line Number (DD)\nFacts ...\nAudit Key (PK)\nOverall Quality Rating\nComplete Flag\nValidation Flag\nOut Of Bounds Flag\nScreen Failed Flag\nRecord Modified Flag\nETL Master Version Number\nAllocation Version Number\nShipments Facts\nAudit Dimension\nFigure 19-2: Sample audit dimension attached to a fact table.\nNow let’s relax the strong assumption of a perfect run. If you had some fact \nrows whose discount dollars triggered an out-of-bounds error, then one more audit \ndimension row would be needed to ﬂ ag this condition. \nSubsystem 7: Deduplication System\nOften  dimensions are derived from several sources. This is a common situation \nfor organizations that have many customer-facing source systems that create and \nmanage separate customer master tables. Customer information may need to be \nmerged from several lines of business and outside sources. Sometimes, the data can \nbe matched through identical values in some key column. However, even when a \n",
      "content_length": 2353,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 497,
      "content": "ETL Subsystems and Techniques 461\ndeﬁ nitive match occurs, other columns in the data might contradict one another, \nrequiring a decision on which data should survive.\nUnfortunately, there is seldom a universal column that makes the merge operation \neasy. Sometimes, the only clues available are the similarity of several columns. The \ndiff erent sets of data being integrated and the existing dimension table data may need \nto be evaluated on diff erent ﬁ elds to attempt a match. Sometimes, a match may be \nbased on fuzzy criteria, such as names and addresses that may nearly match except \nfor minor spelling diff erences.\nSurvivorship is the process of combining a set of matched records into a uniﬁ ed \nimage that combines the highest quality columns from the matched records into a \nconformed row. Survivorship involves establishing clear business rules that deﬁ ne \nthe priority sequence for column values from all possible source systems to enable the \ncreation of a single row with the best-survived attributes. If the dimensional design \nis fed from multiple systems, you must maintain separate columns with back refer-\nences, such as natural keys, to all participating source systems used to construct \nthe row.\nThere are a variety of data integration and data standardization tools to consider \nif you have diffi  cult deduplicating, matching, and survivorship data issues. These \ntools are quite mature and in widespread use.\n Subsystem 8: Conforming System\nConforming  consists of all the steps required to align the content of some or all the \ncolumns in a dimension with columns in similar or identical dimensions in other \nparts of the data warehouse. For instance, in a large organization you may have fact \ntables capturing invoices and customer service calls that both utilize the customer \ndimension. It is highly likely the source systems for invoices and customer service \nhave separate customer databases. It is likely there will be little guaranteed consis-\ntency between the two sources of customer information. The data from these two \ncustomer sources needs to be conformed to make some or all the columns describing \ncustomer share the same domains. \nNOTE \nThe process of creating conformed dimensions aligns with an agile \napproach. For two dimensions to be conformed, they must share at least one \ncommon attribute with the same name and same contents. You can start with a \nsingle conformed attribute such as Customer Category and systematically add this \ncolumn in a nondisruptive way to customer dimensions in each of the customer-\nfacing processes. As you augment each customer-facing process, you expand the \nlist of processes that are integrated and can participate in drill-across queries. You \ncan also incrementally grow the list of conformed attributes, such as city, state, and \ncountry. All this can be staged to align with a more agile implementation approach.\n",
      "content_length": 2903,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 498,
      "content": "Source 1\nSource 2\nSource 3\nMerged and\nglobally\nDeduped\nExtracted\nusing adapter\nCleaned and\nlocally\nDeduplicated\nConformed and\nSurvived\nConformed\nDimension ready\nfor Delivery\nExtracted\nusing adapter\nCleaned and\nlocally\nDeduplicated\nConformed and\nSurvived\nExtracted\nusing adapter\nCleaned and\nlocally\nDeduplicated\nConformed and\nSurvived\nReplication\nEngine\nd\nd\nd\nd\nd\nd\nd\nd\nd\nd\nd\nd\nd\nd\nd\nd\nSpecial contents:\n1) dimension version number\n2) back pointers to all source\nnatural keys\nFigure 19-3: Deduplicating and survivorship processing for conformed dimension process.\n",
      "content_length": 563,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 499,
      "content": "ETL Subsystems and Techniques 463\nThe conforming subsystem is responsible for creating and maintaining the con-\nformed dimensions and conformed facts described in Chapter 4: Inventory. To \naccomplish this, incoming data from multiple systems needs to be combined and \nintegrated, so it is structurally identical, deduplicated, ﬁ ltered of invalid data, and \nstandardized in terms of content rows in a conformed image. A large part of the \nconforming process is the deduplicating, matching, and survivorship processes \npreviously described. The conforming process ﬂ ow combining the deduplicating \nand survivorship processing is shown in Figure 19-3.\nThe process of defining and delivering conformed dimensions and facts is \ndescribed later in subsystems 17 (dimension manager) and 18 (fact provider). \n Delivering: Prepare for Presentation\nThe  primary mission of the ETL system is the handoff  of the dimension and fact \ntables in the delivery step. For this reason, the delivery subsystems are the most \npivotal subsystems in the ETL architecture. Although there is considerable variation \nin source data structures and cleaning and conforming logic, the delivery process-\ning techniques for preparing the dimensional table structures are more deﬁ ned and \ndisciplined. Use of these techniques is critical to building a successful dimensional \ndata warehouse that is reliable, scalable, and maintainable.\nMany of these subsystems focus on dimension table processing. Dimension tables \nare the heart of the data warehouse. They provide the context for the fact tables and \nhence for all the measurements. Although dimension tables are usually smaller than \nthe fact tables, they are critical to the success of the DW/BI system as they provide the \nentry points into the fact tables. The delivering process begins with the cleaned and \nconformed data resulting from the subsystems just described. For many dimensions, \nthe basic load plan is relatively simple: You perform basic transformations to the \ndata to build dimension rows for loading into the target presentation table. This \ntypically includes surrogate key assignment, code lookups to provide appropriate \ndescriptions, splitting or combining columns to present the appropriate data val-\nues, or joining underlying third normal form table structures into denormalized \nﬂ at dimensions.\nPreparing fact tables is certainly important because fact tables hold the key mea-\nsurements of the business that the users want to see. Fact tables can be large and \ntime-consuming to load. However, preparing fact tables for presentation is typically \nmore straightforward.\n",
      "content_length": 2623,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 500,
      "content": "Chapter 19\n464\nSubsystem 9: Slowly Changing Dimension Manager\nOne  of the more important elements of the ETL architecture is the capability to \nimplement slowly changing dimension (SCD) logic. The ETL system must determine \nhow to handle an attribute value that has changed from the value already stored \nin the data warehouse. If the revised description is determined to be a legitimate \nand reliable update to previous information, the appropriate SCD technique must \nbe applied.\nAs described in Chapter 5: Procurement, when the data warehouse receives \nnotiﬁ cation that an existing row in a dimension has changed, there are three basic \nresponses: type 1 overwrite, type 2 add a new row, and type 3 add a new column. \nThe SCD manager should systematically handle the time variance in the dimen-\nsions using these three techniques, as well as the other SCD techniques. In addition, \nthe SCD manager should maintain appropriate housekeeping columns for type 2 \nchanges. Figure 19-4 shows the overall processing ﬂ ow for handling surrogate key \nmanagement for processing SCDs.\nNew record in source\n(not in Cross Ref)\ninsert\nupdate\nupdate\nfield\ntype 1 or 3\nfield is\ntype 2\ninsert\nupdate\nCRC\noptions\nCRCs\ndifferent\nMost Recent\nSurrogate\nKey Map\nMaster\nDimension\nCross Ref\nAssign surrogate\nkeys & set\ndates/indicator\nUpdate\ndimension\nattribute\nUpdate prior\nmost-recent\nrow\nFind specific\nchanged\nfield(s)\nSource\nExtract\nCRC\nCompare\nMaster\nDimension\nCross Ref\nCRCs match\nIgnore\nAssign surrogate\nkeys & set\ndates/indicator\nFigure 19-4: Processing ﬂ ow for SCD surrogate key management.\n",
      "content_length": 1581,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 501,
      "content": "ETL Subsystems and Techniques 465\nThe change data capture process described in subsystem 2 obviously plays an \nimportant role in presenting the changed data to the SCD process. Assuming the \nchange data capture process has eff ectively delivered appropriate changes, the SCD \nprocess can take the appropriate actions.\n Type 1: Overwrite\nThe  type 1 technique is a simple overwrite of one or more attributes in an existing \ndimension row. You take the revised data from the change data capture system \nand overwrite the dimension table contents. Type 1 is appropriate when correcting \ndata or when there is no business need to keep the history of previous values. For \ninstance, you may receive a corrected customer address. In this case, overwriting is \nthe right choice. Note that if the dimension table includes type 2 change tracking, \nyou should overwrite the aff ected column in all existing rows for that particular \ncustomer. Type 1 updates must be propagated forward from the earliest permanently \nstored staging tables to all aff ected staging tables, so if any of them are used to re-\ncreate the ﬁ nal load tables, the eff ect of the overwrite is preserved.\nSome ETL tools contain UPDATE else INSERT functionality. This functionality \nmay be convenient for the developer but can be a performance killer. For maximum \nperformance, existing row UPDATEs should be segregated from new row INSERTs. \nIf type 1 updates cause performance problems, consider disabling database logging \nor use of the DBMS bulk loader.\nType 1 updates invalidate any aggregates built upon the changed column, so the \ndimension manager (subsystem 17) must notify the aff ected fact providers (subsys-\ntem 18) to drop and rebuild the aff ected  aggregates.\n Type 2: Add New Row\nThe  type 2 SCD is the standard technique for accurately tracking changes in dimen-\nsions and associating them correctly with fact rows. Supporting type 2 changes \nrequires a strong change data capture system to detect changes as soon as they occur. \nFor type 2 updates, copy the previous version of the dimension row and create a \nnew dimension row with a new surrogate key. If there is not a previous version of \nthe dimension row, create a new one from scratch. Then update this row with the \ncolumns that have changed and add any other columns that are needed. This is \nthe main workhorse technique for handling dimension attribute changes that need \nto be tracked over time.\nThe type 2 ETL process must also update the most recent surrogate key map table, \nassuming the ETL tool doesn’t automatically handle this. These little two-column \n",
      "content_length": 2603,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 502,
      "content": "Chapter 19\n466\ntables are of immense importance when loading fact table data. Subsystem 14, the \nsurrogate key pipeline, supports this process.\nRefer to Figure 19-4 to see the lookup and key assignment logic for handling a \nchanged dimension row during the extract process. In this example, the change \ndata capture process (subsystem 2) uses a CRC compare to determine which \nrows have changed in the source data since the last update. If you are lucky, you \nalready know which dimension records have changed and can omit this CRC \ncompare step. After you identify rows that have changes in type 2 attributes, \nyou can generate a new surrogate key from the key sequence and update the \nsurrogate key map table.\nWhen a new type 2 row is created, you need at least a pair of time stamps, as \nwell as an optional change description attribute. The pair of time stamps deﬁ nes a \nspan of time from the beginning eff ective time to the ending eff ective time when \nthe complete set of dimension attributes is valid. A more sophisticated treatment \nof a type 2 SCD row involves adding ﬁ ve ETL housekeeping columns. Referring to \nFigure 19-4, this also requires the type 2 ETL process to ﬁ nd the prior eff ective row \nand make appropriate updates to these housekeeping columns: \n \n■Change Date (change date as foreign key to date dimension outrigger)\n \n■Row Eff ective Date/Time (exact date/time stamp of change)\n \n■Row End Date/Time (exact date/time stamp of next change, defaults to \n12/31/9999 for most current dimension row)\n \n■Reason for Change column (optional attribute)\n \n■Current Flag  (current/expired)\nNOTE \nIt is possible that back-end scripts are run within the transaction data-\nbase to modify data without updating the respective metadata ﬁ elds such as the \nlast_modiﬁ ed_date. Using these ﬁ elds for the dimension time stamps can cause \ninconsistent results in the data warehouse. Always use the system or as-of date to \nderive the type 2 eff ective time stamps.\nThe type 2 process does not change history as the type 1 process does; thus \ntype 2 changes don’t require rebuilding aff ected aggregate tables as long as the \nchange was made “today” and not backward in time.\nNOTE \nKimball Design Tip #80 (available at www.kimballgroup.com under the \nTools and Utilities tab for this book title) provides in-depth guidance on adding a \nrow change reason code attribute to dimension tables.\n",
      "content_length": 2399,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 503,
      "content": "ETL Subsystems and Techniques 467\n Type 3: Add New Attribute\nThe  type 3 technique is designed to support attribute “soft” changes that allow a \nuser to refer either to the old value of the attribute or the new value. For example, if \na sales team is assigned to a newly named sales region, there may be a need to track \nthe old region assignment, as well as the new one. The type 3 technique requires the \nETL system to alter the dimension table to add a new column to the schema, if this \nsituation was not anticipated. Of course, the DBA assigned to work with the ETL \nteam will in all likelihood be responsible for this change. You then need to push \nthe existing column values into the newly created column and populate the original \ncolumn with the new values provided to the ETL system. Figure 19-5 shows how \na type 3 SCD is implemented.\n1127648\nA 107B\nDenim\npants\n38\nMen’s\nwear\nProd ID\n(NK)\nProd Key\n(PK)\nProd\nName\nSize\nCategory\nColor\nBlue\nAdd field; Transfer old value\nOverwrite\nwith new\nvalue\n...\n1127648\nA 107B\nDenim\npants\n38\nLeisure\nwear\nMen’s\nwear\nProd ID\n(NK)\nProd Key\n(PK)\nProd\nName\nSize\nCategory\nColor\nPrior\nCategory\nBlue\nFigure 19-5: Type 3 SCD process.\nSimilar to the type 1 process, type 3 change updates invalidate any aggregates \nbuilt upon the changed column; the dimension manager must notify the aff ected \nfact providers, so they drop and rebuild the aff ected aggregates.\n Type 4: Add Mini-Dimension\nThe  type 4 technique is used when a group of attributes in a dimension change suf-\nﬁ ciently rapidly so that they are split off  to a mini-dimension. This situation is some-\ntimes called a rapidly changing monster dimension. Like type 3, this situation calls \nfor a schema change, hopefully done at design time. The mini-dimension requires \nits own unique primary key, and both the primary key of the main dimension and \nthe primary key of the mini-dimension must appear in the fact table. Figure 19-6 \nshows how a type 4 SCD is implemented.\n",
      "content_length": 1970,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 504,
      "content": "Chapter 19\n468\nActivity Date Key (FK)\nCustomer Key (FK)\nDemographics Key (FK)\nProduct Key (FK)\nPromotion Key (FK)\nSales Dollars\nSales Units\nCustomer Key (PK)\nCustomer Attributes...\nSales Fact\nCustomer Dimension\nDemographics Key (PK)\nDemographics Attributes...\nDemographics Dimension\nFigure 19-6: Type 4 SCD process.\n Type 5: Add Mini-Dimension and Type 1 Outrigger \nThe  type 5 technique builds on the type 4 mini-dimension by also embedding a type \n1 reference to the mini-dimension in the primary dimension. This allows accessing \nthe current values in the mini-dimension directly from the base dimension without \nlinking through a fact table. The ETL team must add the type 1 key reference in \nthe base dimension and must overwrite this key reference in all copies of the base \ndimension whenever the current status of the mini-dimension changes over time. \nFigure 19-7 shows how a type 5 SCD is implemented.\nActivity Date Key (FK)\nCustomer Key (FK)\nDemographics Key (FK)\nProduct Key (FK)\nPromotion Key (FK)\nSales Dollars\nSales Units\nCustomer Key (PK)\nDemographics Key (FK - SCD 1)\nCustomer Attributes...\nSales Fact\nCustomer Dimension\nDemographics Key (PK)\nDemographics Attributes...\nDemographics Dimension\nFigure 19-7: Type 5 SCD process.\n Type 6: Add Type 1 Attributes to Type 2 Dimension\nThe  type 6 technique has an embedded attribute that is an alternate value of a \nnormal type 2 attribute in the base dimension. Usually such an attribute is simply \na type 3 alternative reality, but in this case the attribute is systematically overwrit-\nten whenever the attribute is updated. Figure 19-8 shows how a type 6 SCD is \nimplemented.\n Type 7: Dual Type 1 and Type 2 Dimensions\nThe  type 7 technique is a normal type 2 dimension paired with a specially con-\nstructed fact table that has both a normal foreign key to the dimension for type 2 \nhistorical processing, and also a foreign durable key (FDK in Figure 19-9) that is \n",
      "content_length": 1930,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 505,
      "content": "ETL Subsystems and Techniques 469\nused alternatively for type 1 current processing, connected to the durable key in \nthe dimension table labeled PDK. The dimension table also contains a current row \nindicator that indicates whether the particular row is the one to be used for current \nSCD 1 perspective. The ETL team must augment a normally constructed fact table \nwith this constant value foreign durable key. Figure 19-9 shows how a type 7 SCD \nis implemented.\nActivity Date Key (FK)\nCustomer Key (FK)\nDemographics Key (FK)\nProduct Key (FK)\nPromotion Key (FK)\nSales Dollars\nSales Units\nCustomer Key (PK)\nCustomer Category (SCD 2)\nCurrent Customer Category (SCD 1)\nCustomer Attributes...\nSales Fact\nCustomer Dimension\nFigure 19-8: Type 6 SCD process.\nActivity Date Key (FK)\nCustomer Key (FK)\nCustomer Durable Key (FDK)\nDemographics Key (FK)\nProduct Key (FK)\nPromotion Key (FK)\nSales Dollars\nSales Units\nCustomer Key (PK)\nCustomer Durable Key (PDK)\nCustomer Attributes...\nCurrent Row Indicator\njoin for SCD 2\nIndicator = Current/True for SCD 1\njoin for SCD 1\nSales Fact\nCustomer Dimension\nFigure 19-9: Type 7 SCD process.\n Subsystem 10: Surrogate Key Generator\nAs  you recall from Chapter 3: Retail Sales, we strongly recommend the use of sur-\nrogate keys for all dimension tables. This implies you need a robust mechanism for \nproducing surrogate keys in the ETL system. The surrogate key generator should \nindependently generate surrogate keys for every dimension; it should be independent \nof database instance and able to serve distributed clients. The goal of the surrogate \nkey generator is to generate a meaningless key, typically an integer, to serve as the \nprimary key for a dimension row.\nAlthough it may be tempting to create surrogate keys via database triggers, \nthis technique may create performance bottlenecks. If the DBMS is used to assign \nsurrogate keys, it is preferable for the ETL process to directly call the database \nsequence generator. For improved effi  ciency, consider having the ETL tool generate \n",
      "content_length": 2030,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 506,
      "content": "Chapter 19\n470\nand maintain the surrogate keys. Avoid the temptation of concatenating the opera-\ntional key of the source system and a date/time stamp. Although this approach seems \nsimple, it is fraught with problems and ultimately will not scale. \n Subsystem 11: Hierarchy Manager\nIt  is normal for a dimension to have multiple, simultaneous, embedded hierarchi-\ncal structures. These multiple hierarchies simply coexist in the same dimension as \ndimension attributes. All that is necessary is that every attribute be single valued in \nthe presence of the dimension’s primary key. Hierarchies are either ﬁ xed or ragged. \nA ﬁ xed depth hierarchy has a consistent number of levels and is simply modeled and \npopulated as separate dimension attributes for each of the levels. Slightly ragged hier-\narchies like postal addresses are most often modeled as a ﬁ xed hierarchy. Profoundly \nragged hierarchies are typically found with organization structures that are unbal-\nanced and of indeterminate depth. The data model and ETL solution required to \nsupport these needs require the use of a bridge table containing the organization map. \nSnowﬂ akes  or normalized data structures are not recommended for the presenta-\ntion level. However, the use of a normalized design may be appropriate in the ETL \nstaging area to assist in the maintenance of the ETL data ﬂ ow for populating and \nmaintaining the hierarchy attributes. The ETL system is responsible for enforcing \nthe business rules to assure the hierarchy is populated appropriately in the dimen-\nsion  table.\nSubsystem 12: Special Dimensions Manager\nThe  special dimensions manager is a catch-all subsystem: a placeholder in the ETL \narchitecture for supporting an organization’s speciﬁ c dimensional design character-\nistics. Some organizations’ ETL systems require all the capabilities discussed here, \nwhereas others will be concerned with few of these design techniques: \n Date/Time Dimensions\nThe  date and time dimensions are unique in that they are completely speciﬁ ed \nat the beginning of the data warehouse project, and they don’t have a conventional \nsource. This is okay! Typically, these dimensions are built in an afternoon with a \nspreadsheet. But in a global enterprise environment, even this dimension can be \nchallenging when taking into account multiple ﬁ nancial reporting periods or mul-\ntiple cultural calendars.\n Junk Dimensions\nJunk  dimensions are made up from text and miscellaneous ﬂ ags left over in the \nfact table after you remove all the critical attributes. There are two approaches for \n",
      "content_length": 2573,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 507,
      "content": "ETL Subsystems and Techniques 471\ncreating junk dimensions in the ETL system. If the theoretical number of rows in \nthe dimension is ﬁ xed and known, the junk dimension can be created in advance. \nIn other cases, it may be necessary to create newly observed junk dimension rows \non-the-ﬂ y while processing fact row input. As illustrated in Figure 19-10, this pro-\ncess requires assembling the junk dimension attributes and comparing them to the \nexisting junk dimension rows to see if the row already exists. If not, a new dimen-\nsion row must be assembled, a surrogate key created, and the row loaded into the \njunk dimension on-the-ﬂ y during the fact table load process.\ncode 1\ncode M\nCodes and indicators encountered in daily load:\nCombine into\nsingle row\nind 1\nind P\n...\n...\nCompare to existing\ndimension rows; insert\nif new\nSurrogate key for this\njunk dimension row\nload\njunk key (FK)\nfact table\nFigure 19-10: Architecture for building junk dimension rows.\nNOTE \nKimball Design Tip #113 (available at www.kimballgroup.com under the \nTools and Utilities tab for this book title) provides more in-depth guidance on \nbuilding and maintaining junk dimension tables.\nMini-Dimensions\nAs  we just discussed in subsystem 9, mini-dimensions are a technique used to \ntrack dimension attribute changes in a large dimension when the type 2 technique \nis infeasible, such as a customer dimension. From an ETL perspective, creation of \n",
      "content_length": 1429,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 508,
      "content": "Chapter 19\n472\nthe mini-dimension is similar to the junk dimension process previously described. \nAgain, there are two alternatives: building all valid combinations in advance or rec-\nognizing and creating new combinations on-the-ﬂ y. Although junk dimensions are \nusually built from the fact table input, mini-dimensions are built from dimension \ntable inputs. The ETL system is responsible for maintaining a multicolumn surrogate \nkey lookup table to identify the base dimension member and appropriate mini-\ndimension row to support the surrogate pipeline process described in Subsystem 14, \nSurrogate Key Pipeline. Keep in mind that very large, complex customer dimensions \noften require several mini-dimensions.\nNOTE \nKimball Design Tip #127 (available at www.kimballgroup.com under the \nTools and Utilities tab for this book title) provides more in-depth guidance on \nbuilding and maintaining mini-dimension tables. \n Shrunken Subset Dimensions\nShrunken  dimensions are conformed dimensions that are a subset of rows and/\nor columns of one of your base dimensions. The ETL data ﬂ ow should build \nconformed shrunken dimensions from the base dimension, rather than indepen-\ndently, to assure conformance. The primary key for the shrunken dimension, \nhowever, must be independently generated; if you attempt to use a key from an \n“example” base dimension row, you will get into trouble if this key is retired or \nsuperseded.\nNOTE \nKimball Design Tip #137 (available at www.kimballgroup.com under the \nTools and Utilities tab for this book title) provides more in-depth guidance on \nbuilding shrunken dimension tables. \nSmall Static Dimensions\nA  few dimensions are created entirely by the ETL system without a real outside \nsource. These are usually small lookup dimensions where an operational code is \ntranslated into words. In these cases, there is no real ETL processing. The lookup \ndimension is simply created directly by the ETL team as a relational table in its \nﬁ nal form.\nUser Maintained Dimensions\nOften  the warehouse requires that totally new “master” dimension tables be created. \nThese dimensions have no formal system of record; rather they are custom descrip-\ntions, groupings, and hierarchies created by the business for reporting and analysis \n",
      "content_length": 2267,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 509,
      "content": "ETL Subsystems and Techniques 473\npurposes. The ETL team often ends up with stewardship responsibility for these \ndimensions, but this is typically not successful because the ETL team is not aware of \nchanges that occur to these custom groupings, so the dimensions fall into disrepair \nand become ineff ective. The best-case scenario is to have the appropriate business \nuser department agree to own the maintenance of these attributes. The DW/BI team \nneeds to provide a user interface for this maintenance. Typically, this takes the form \nof a simple application built using the company’s standard visual programming tool. \nThe ETL system should add default attribute values for new rows, which the user \nowner needs to update. If these rows are loaded into the warehouse before they are \nchanged, they still appear in reports with whatever default description is supplied.\nNOTE \nThe ETL process should create a unique default dimension attribute \ndescription that shows someone hasn’t yet done their data stewardship job. We \nfavor a label that concatenates the phrase Not Yet Assigned with the surrogate \nkey value: “Not Yet Assigned 157.” That way, multiple unassigned values do not \ninadvertently get lumped together in reports and aggregate tables. This also helps \nidentify the row for later correction.\nSubsystem 13: Fact Table Builders\nFact  tables hold the measurements of an organization. Dimensional models are \ndeliberately built around these numerical measurements. The fact table builder \nsubsystem focuses on the ETL architectural requirements to eff ectively build the \nthree primary types of fact tables: transaction, periodic snapshot, and accumulating \nsnapshot. An important requirement for loading fact tables is maintaining refer-\nential integrity with the associated dimension tables. The surrogate key pipeline \n(subsystem 14) is designed to help support this need.\n Transaction Fact Table Loader\nThe transaction grain represents a measurement event deﬁ ned at a particular instant. \nA line item on an invoice is an example of a transaction event. A scanner event at \na cash register is another. In these cases, the time stamp in the fact table is very \nsimple. It’s either a single daily grain foreign key or a pair consisting of a daily grain \nforeign key together with a date/time stamp, depending on what the source system \nprovides and the analyses require. The facts in this transaction table must be true \nto the grain and should describe only what took place in that instant.\nTransaction grain fact tables are the largest and most detailed of the three types \nof fact tables. The transaction fact table loader receives data from the changed data \ncapture system and loads it with the proper dimensional foreign keys. The pure \n",
      "content_length": 2761,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 510,
      "content": "Chapter 19\n474\naddition of the most current records is the easiest case: simply bulk loading new \nrows into the fact table. In most cases, the target fact table should be partitioned by \ntime to ease the administration and speed the performance of the table. An audit \nkey, sequential ID, or date/time stamp column should be included to allow backup \nor restart of the load job.\nThe addition of late arriving data is more diffi  cult, requiring additional process-\ning capabilities described in subsystem 16. In the event it is necessary to update \nexisting rows, this process should be handled in two phases. The ﬁ rst step is to \ninsert the corrected rows without overwriting or deleting the original rows, and \nthen delete the old rows in a second step. Using a sequentially assigned single sur-\nrogate key for the fact table makes it possible to perform the two steps of insertion \nfollowed by deletion.\n Periodic Snapshot Fact Table Loader\nThe  periodic snapshot grain represents a regular repeating measurement or set of \nmeasurements, like a bank account monthly statement. This fact table also has a \nsingle date column, representing the overall period. The facts in this periodic snap-\nshot table must be true to the grain and should describe only measures appropriate \nto the timespan deﬁ ned by the period. Periodic snapshots are a common fact table \ntype and are frequently used for account balances, monthly ﬁ nancial reporting, \nand inventory balances. The periodicity of a periodic snapshot is typically daily, \nweekly, or monthly. \nPeriodic snapshots have similar loading characteristics to those of transaction \ngrain fact tables. The same processing applies for inserts and updates. Assuming \ndata is promptly delivered to the ETL system, all records for each periodic load can \ncluster in the most recent time partition. Traditionally, periodic snapshots have \nbeen loaded en masse at the end of the appropriate period.\nFor example, a credit card company might load a monthly account snapshot table \nwith the balances in eff ect at the end of the month. More frequently, organizations \nwill populate a hot rolling periodic snapshot. In addition to the rows loaded at the \nend of every month, there are special rows loaded with the most current balances \nin eff ect as of the previous day. As the month progresses, the current month rows \nare continually updated with the most current information and continue in this \nmanner rolling through the month. Note that the hot rolling snapshot can some-\ntimes be diffi  cult to implement if the business rules for calculating the balances \nat the period end are complex. Often these complex calculations are dependent \non other periodic processing outside the data warehouse, and there is not enough \ninformation available to the ETL system to perform these complex calculations on \na more frequent basis.\n",
      "content_length": 2868,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 511,
      "content": "ETL Subsystems and Techniques 475\n Accumulating Snapshot Fact Table Loader\nThe  accumulating snapshot grain represents the current evolving status of a process \nthat has a ﬁ nite beginning and end. Usually, these processes are of short duration \nand therefore don’t lend themselves to the periodic snapshot. Order processing is \nthe classic example of an accumulating snapshot. The order is placed, shipped, \nand paid for within one reporting period. The transaction grain provides too much \ndetail separated into individual fact table rows, and the periodic snapshot just is \nthe wrong way to report this data.\nThe design and administration of the accumulating snapshot is quite diff erent \nfrom the ﬁ rst two fact table types. All accumulating snapshot fact tables have a \nset of dates which describe the typical process workﬂ ow. For instance, an order \nmight have an order date, actual ship date, delivery date, ﬁ nal payment date, and \nreturn date. In this example, these ﬁ ve dates appear as ﬁ ve separate date-valued \nforeign surrogate keys. When the order row is ﬁ rst created, the ﬁ rst of these dates \nis well deﬁ ned, but perhaps none of the others have yet happened. This same \nfact row is subsequently revisited as the order winds its way through the order \npipeline. Each time something happens, the accumulating snapshot fact row is \ndestructively modiﬁ ed. The date foreign keys are overwritten, and various facts \nare updated. Often the ﬁ rst date remains inviolate because it describes when \nthe row was created, but all the other dates may well be overwritten, sometimes \nmore than once.\nMany RDBMSs utilize variable row lengths. Repeated updates to accumulating \nsnapshot fact rows may cause the rows to grow due to these variable row lengths, \naff ecting the residency of disk blocks. It may be worthwhile to occasionally drop \nand reload rows after the update activity to improve performance.\nAn accumulating snapshot fact table is an effective way to represent finite \nprocesses with well-deﬁ ned beginnings and endings. However, the accumulating \nsnapshot by deﬁ nition is the most recent view. Often it makes sense to utilize all \nthree fact table types to meet various needs. Periodic history can be captured with \nperiodic extracts, and all the inﬁ nite details involved in the process can be captured \nin an associated transaction grain fact table. The presence of many situations that \nviolate standard scenarios or involve repeated looping though the process would \nprohibit the use of an accumulating snapshot.\nSubsystem 14: Surrogate Key Pipeline\nEvery  ETL system must include a step for replacing the operational natural keys \nin the incoming fact table row with the appropriate dimension surrogate \nkeys. Referential integrity (RI) means that for each foreign key in the fact table, \n",
      "content_length": 2819,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 512,
      "content": "Chapter 19\n476\nan entry exists in the corresponding dimension table. If there’s a row in a sales \nfact table for product surrogate key 323442, you need to have a row in the product \ndimension table with the same key, or you won’t know what you’ve sold. You have a \nsale for what appears to be a nonexistent product. Even worse, without the product \nkey in the dimension, a business user can easily construct a query that will omit \nthis sale without even realizing it.\nThe key lookup process should result in a match for every incoming natural key \nor a default value. In the event there is an unresolved referential integrity failure dur-\ning the lookup process, you need to feed these failures back to the responsible ETL \nprocess for resolution, as shown in Figure 19-11. Likewise, the ETL process needs to \nresolve any key collisions that might be encountered during the key lookup process. \nDate\nDimension\nReplace\ndate_ID\nwith surrogate\ndate_key\nReplace\nprod_ID\nwith surrogate\nprod_key\nReplace\nstore_ID\nwith surrogate\nstore_key\nReplace\npromo_ID\nwith surrogate\npromo_key\nLoad fact\ntable rows\ninto DBMS\nFact Table\nwith Natural\nKey IDs\ndate_ID\nproduct_ID\nstore_ID\npromo_ID\ndollar_sales\nunit_sales\ndollar_cost\ndate_key\nproduct_key\nstore_key\npromo_key\ndollar_sales\nunit_sales\ndollar_cost\nFact Table\nwith Surrogate\nKeys\nProduct\nDimension\nStore\nDimension\nReferential Integrity Failures\nKey Collisions\nPromotion\nDimension\nFigure 19-11: Replacing fact record’s operational natural keys with dimension \nsurrogate keys.\nAfter the fact table data has been processed and just before loading into the pre-\nsentation layer, a surrogate key lookup needs to occur to substitute the operational \nnatural keys in the fact table record with the proper current surrogate key. To pre-\nserve referential integrity, always complete the updating of the dimension tables \nﬁ rst. In that way, the dimension tables are always the legitimate source of primary \nkeys you must replace in the fact table (refer to Figure 19-11).\nThe most direct approach is to use the actual dimension table as the source for \nthe most current value of the surrogate key corresponding to each natural key. Each \ntime you need the current surrogate key, look up all the rows in the dimension with \nthe natural key equal to the desired value, and then select the surrogate key that \naligns with the historical context of the fact row using the current row indicator or \nbegin and end eff ect dates. Current hardware environments off er nearly unlimited \naddressable memory, making this approach practical.\n",
      "content_length": 2560,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 513,
      "content": "ETL Subsystems and Techniques 477\nDuring processing, each natural key in the incoming fact record is replaced with \nthe correct current surrogate key. Don’t keep the natural key in the fact row—the \nfact table needs to contain only the surrogate key. Do not write the input data to \ndisk until all fact rows have passed all the processing steps. If possible, all required \ndimension tables should be pinned in memory, so they can be randomly accessed \nas each incoming record presents its natural keys.\nAs illustrated at the bottom of Figure 19-11, the surrogate key pipeline needs to \nhandle key collisions in the event you attempt to load a duplicate row. This is an \nexample of a data quality problem appropriate for a traditional structure data qual-\nity screen, as discussed in subsystem 4. In the event a key collision is recognized, \nthe surrogate key pipeline process needs to choose to halt the process, send the \noff ending data into suspension, or apply appropriate business rules to determine \nif it is possible to correct the problem, load the row, and write an explanatory row \ninto the error event schema.\nNote a slightly diff erent process is needed to perform surrogate key lookups if \nyou need to reload history or if you have a lot of late arriving fact rows because you \ndon’t want to map the most current value to a historical event. In this case, you need \nto create logic to ﬁ nd the surrogate key that applied at the time the fact record was \ngenerated. This means ﬁ nding the surrogate key where the fact transaction date is \nbetween the key’s eff ective start date and end date.\nWhen the fact table natural keys have been replaced with surrogate keys, the \nfact row is ready to load. The keys in the fact table row have been chosen to be \nproper foreign keys, and the fact table is guaranteed to have referential integrity \nwith respect to the dimension tables.\n Subsystem 15: Multivalued Dimension \nBridge Table Builder\nSometimes  a fact table must support a dimension that takes on multiple values \nat the lowest granularity of the fact table, as described in Chapter 8: Customer \nRelationship Management. If the grain of the fact table cannot be changed to directly \nsupport this dimension, then the multivalued dimension must be linked to the fact \ntable via a bridge table. Bridge tables are common in the healthcare industry, in \nsales commission environments, and for supporting variable depth hierarchies, as \ndiscussed in subsystem 11.\nThe challenge for the ETL team is building and maintaining the bridge table. As \nmultivalued relationships to the fact row are encountered, the ETL system has the \nchoice of either making each set of observations a unique group or reusing groups \nwhen an identical set of observations occurs. Unfortunately, there is no simple \nanswer for the right choice. In the event the multivalued dimension has type 2 \n",
      "content_length": 2879,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 514,
      "content": "Chapter 19\n478\nattributes, the bridge table must also be time varying, such as a patient’s time vari-\nant set of diagnoses.\nOne of the bridge table constructs presented in Chapter 10: Financial Services \nwas the inclusion of a weighting factor to support properly weighted reporting from \nthe bridge table. In many cases, the weighting factor is a familiar allocation factor, \nbut in other cases, the identiﬁ cation of the appropriate weighting factor can be prob-\nlematic because there may be no rational basis for assigning the weighting factor.\nNOTE \nKimball Design Tip #142 (available at www.kimballgroup.com under the \nTools and Utilities tab for this book title) provides more in-depth guidance on \nbuilding and maintaining bridge tables.\n Subsystem 16: Late Arriving Data Handler\nData  warehouses are usually built around the ideal assumption that measured activ-\nity (fact records) arrive in the data warehouse at the same time as the context of \nthe activity (dimension records). When you have both the fact records and the \ncorrect contemporary dimension rows, you have the luxury of ﬁ rst maintaining \nthe dimension keys and then using these up-to-date keys in the accompanying fact \nrows. However, for a variety of reasons, the ETL system may need to process late \narriving fact or dimension data.\nIn some environments, there may need to be special modiﬁ cations to the stan-\ndard processing procedures to deal with late arriving facts, namely fact records \nthat come into the warehouse very much delayed. This is a messy situation because \nyou have to search back in history to decide which dimension keys were in eff ect \nwhen the activity occurred. In addition, you may need to adjust any semi-additive \nbalances in subsequent fact rows. In a heavily compliant environment, it is also \nnecessary to interface with the compliance subsystem because you are about to \nchange history.\nLate arriving dimensions occur when the activity measurement (fact record) \narrives at the data warehouse without its full context. In other words, the statuses of \nthe dimensions attached to the activity measurement are ambiguous or unknown for \nsome period of time. If you are living in the conventional batch update cycle of one or \nmore days’ latency, you can usually just wait for the dimensions to be reported. For \nexample, the identiﬁ cation of the new customer may come in a separate feed delayed \nby several hours; you may just be able to wait until the dependency is resolved.\nBut in many situations, especially real-time environments, this delay is not \nacceptable. You cannot suspend the rows and wait for the dimension updates to \noccur; the business requirements demand that you make the fact row visible before \n",
      "content_length": 2724,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 515,
      "content": "ETL Subsystems and Techniques 479\nknowing the dimensional context. The ETL system needs additional capabilities \nto support this requirement. Using customer as the problem dimension, the ETL \nsystem needs to support two situations. The ﬁ rst is to support late arriving type 2 \ndimension updates. In this situation, you need to add the revised customer row to \nthe dimension with a new surrogate key and then go in and destructively modify \nany subsequent fact rows’ foreign key to the customer table. The eff ective dates for \nthe aff ected dimension rows also need to be reset. In addition, you need to scan \nforward in the dimension to see if there have been any subsequent type 2 rows for \nthis customer and change this column in any aff ected rows.\nThe second situation occurs when you receive a fact row with what appears to be a \nvalid customer natural key, but you have not yet loaded this customer in the customer \ndimension. It would be possible to load this row pointing to a default row in the \ndimension table. This approach has the same unpleasant side eff ect discussed earlier \nof requiring destructive updates to the fact rows’ foreign keys when the dimension \nupdates are ﬁ nally processed. Alternatively, if you believe the customer is a valid, but \nnot yet processed customer, you should assign a new customer surrogate key with \na set of dummy attribute values in a new customer dimension row. You then return \nto this dummy dimension row at a later time and make type 1 overwrite changes to \nits attributes when you get complete information on the new customer. At least this \nstep avoids destructively changing any fact table keys.\nThere is no way to avoid a brief provisional period in which the dimensions \nare “not quite right.” But these maintenance steps can minimize the impact of the \nunavoidable updates to the keys and other columns.\nSubsystem 17: Dimension Manager System\nThe  dimension manager is a centralized authority who prepares and publishes con-\nformed dimensions to the data warehouse community. A conformed dimension is \nby necessity a centrally managed resource: Each conformed dimension must have a \nsingle, consistent source. It is the dimension manager’s responsibility to administer \nand publish the conformed dimension(s) for which he has responsibility. There \nmay be multiple dimension managers in an organization, each responsible for a \ndimension. The dimension manager’s responsibilities include the following ETL \nprocessing:\n \n■Implement the common descriptive labels agreed to by the data stewards and \nstakeholders during the dimension design.\n \n■Add new rows to the conformed dimension for new source data, generating \nnew surrogate keys.\n \n■Add new rows for type 2 changes to existing dimension entries, generating \nnew surrogate keys.\n",
      "content_length": 2796,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 516,
      "content": "Chapter 19\n480\n \n■Modify rows in place for type 1 changes and type 3 changes, without chang-\ning the surrogate keys.\n \n■Update the version number of the dimension if any type 1 or type 3 changes \nare made.\n \n■Replicate the revised dimension simultaneously to all fact table providers.\nIt is easier to manage conformed dimensions in a single tablespace DBMS on a \nsingle machine because there is only one copy of the dimension table. However, \nmanaging conformed dimensions becomes more diffi  cult in multiple tablespace, \nmultiple DMBS, or multimachine distributed environments. In these situations, the \ndimension manager must carefully manage the simultaneous release of new versions \nof the dimension to every fact provider. Each conformed dimension should have a \nversion number column in each row that is overwritten in every row whenever the \ndimension manager releases the dimension. This version number should be utilized \nto support any drill-across queries to assure that the same release of the dimension \nis being utilized.\nSubsystem 18: Fact Provider System\nThe  fact provider is responsible for receiving conformed dimensions from the dimen-\nsion managers. The fact provider owns the administration of one or more fact tables \nand is responsible for their creation, maintenance, and use. If fact tables are used \nin any drill-across applications, then by deﬁ nition the fact provider must be using \nconformed dimensions provided by the dimension manager. The fact provider’s \nresponsibilities are more complex and include:\n \n■Receive or download replicated dimension from the dimension manager.\n \n■In an environment in which the dimension cannot simply be replicated but \nmust be locally updated, the fact provider must process dimension records \nmarked as new and current to update current key maps in the surrogate key \npipeline and also process any dimension records marked as new but postdated.\n \n■Add all new rows to fact tables after replacing their natural keys with correct \nsurrogate keys.\n \n■Modify rows in all fact tables for error correction, accumulating snapshots, \nand late arriving dimension changes.\n \n■Remove aggregates that have become invalidated.\n \n■Recalculate aff ected aggregates. If the new release of a dimension does not \nchange the version number, aggregates have to be extended to handle only \nnewly loaded fact data. If the version number of the dimension has changed, \nthe entire historical aggregate may have to be recalculated.\n",
      "content_length": 2476,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 517,
      "content": "ETL Subsystems and Techniques 481\n \n■Quality ensure all base and aggregate fact tables. Be satisﬁ ed the aggregate \ntables are correctly calculated.\n \n■Bring updated fact and dimension tables online.\n \n■Inform users that the database has been updated. Tell them if major changes \nhave been made, including dimension version changes, postdated records \nbeing added, and changes to historical aggregates.\n Subsystem 19: Aggregate Builder\nAggregates  are the single most dramatic way to aff ect performance in a large data \nwarehouse environment. Aggregations are like indexes; they are speciﬁ c data struc-\ntures created to improve performance. Aggregates can have a signiﬁ cant impact \non performance. The ETL system needs to eff ectively build and use aggregates \nwithout causing signiﬁ cant distraction or consuming extraordinary resources and \nprocessing cycles. \nYou should avoid architectures in which aggregate navigation is built into the \nproprietary query tool. From an ETL viewpoint, the aggregation builder needs to \npopulate and maintain aggregate fact table rows and shrunken dimension tables \nwhere needed by aggregate fact tables. The fastest update strategy is incremental, \nbut a major change to a dimension attribute may require dropping and rebuild-\ning the aggregate. In some environments, it may be faster to dump data out of the \nDBMS and build aggregates with a sort utility rather than building the aggregates \ninside the DBMS. Additive numeric facts can be aggregated easily at extract time \nby calculating break rows in one of the sort packages. Aggregates must always be \nconsistent with the atomic base data. The fact provider (subsystem 18) is respon-\nsible for taking aggregates off -line when they are not consistent with the base data.\nUser feedback on the queries that run slowly is critical input to designing aggrega-\ntions. Although you can depend on informal feedback to some extent, a log of frequently \nattempted slow-running queries should be captured. You should also try to identify the \nnonexistent slow-running queries that never made it into the log because they never \nrun to completion, or aren’t even attempted due to known performance challenges.\n Subsystem 20: OLAP Cube Builder\nOLAP  servers present dimensional data in an intuitive way, enabling a range of \nanalytic users to slice and dice data. OLAP is a sibling of dimensional star schemas \nin the relational database, with intelligence about relationships and calculations \ndeﬁ ned on the server that enable faster query performance and more interesting \nanalytics from a broad range of query tools. Don’t think of an OLAP server as a \n",
      "content_length": 2641,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 518,
      "content": "Chapter 19\n482\ncompetitor to a relational data warehouse, but rather an extension. Let the relational \ndatabase do what it does best: Provide storage and management.\nThe relational dimensional schema should be viewed as the foundation for OLAP \ncubes if you elect to include them in your architecture. The process of feeding data \nfrom the dimensional schema is an integral part of the ETL system; the relational \nschemas are the best and preferred source for OLAP cubes. Because many OLAP \nsystems do not directly address referential integrity or data cleaning, the pre-\nferred architecture is to load OLAP cubes after the completion of conventional ETL \nprocesses. Note that some OLAP tools are more sensitive to hierarchies than rela-\ntional schemas. It is important to strongly enforce the integrity of hierarchies within \ndimensions before loading an OLAP cube. Type 2 SCDs ﬁ t an OLAP system well \nbecause a new surrogate key is just treated as a new member. Type 1 SCDs that restate \nhistory do not ﬁ t OLAP well. Overwrites to an attribute value can cause all the cubes \nusing that dimension to be reprocessed in the background, become corrupted, or be \ndropped. Read this last sentence again.\nSubsystem 21: Data Propagation Manager\nThe  data propagation manager is responsible for the ETL processes required to \npresent conformed, integrated enterprise data from the data warehouse presenta-\ntion server to other environments for special purposes. Many organizations need \nto extract data from the presentation layer to share with business partners, cus-\ntomers, and/or vendors for strategic purposes. Similarly, some organizations are \nrequired to submit data to various government organizations for reimbursement \npurposes, such as healthcare organizations that participate in the Medicare pro-\ngram. Many organizations have acquired package analytic applications. Typically, \nthese applications cannot be pointed directly against the existing data warehouse \ntables, so data needs to be extracted from the presentation layer and loaded into \nproprietary data structures required by the analytic applications. Finally, most \ndata mining tools do not run directly against the presentation server. They need \ndata extracted from the data warehouse and fed to the data mining tool in a \nspeciﬁ c format.\nAll the situations previously described require extraction from the DW/BI \npresentation server, possibly some light transformation, and loading into a target \nformat—in other words ETL. Data propagation should be considered a part of the \nETL system; ETL tools should be leveraged to provide this capability. What is dif-\nferent in this situation is that the requirements of the target are not negotiable; you \nmust provide the data as speciﬁ ed by the target.\n",
      "content_length": 2773,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 519,
      "content": "ETL Subsystems and Techniques 483\nManaging the ETL Environment\nA  DW/BI environment can have a great dimensional model, well-deployed BI applica-\ntions, and strong management sponsorship. But it cannot be a success until it can be \nrelied upon as a dependable source for business decision making. One of the goals \nfor the DW/BI system is to build a reputation for providing timely, consistent, and \nreliable data to empower the business. To achieve this goal, the ETL system must \nconstantly work toward fulﬁ lling three criteria:\n \n■Reliability. The ETL processes must consistently run. They must run to \ncompletion to provide data on a timely basis that is trustworthy at any level \nof detail.\n \n■Availability. The data warehouse must meet its service level agreements \n(SLAs). The warehouse should be up and available as promised. \n \n■Manageability. A successful data warehouse is never done. It constantly grows \nand changes along with the business. The ETL processes need to gracefully \nevolve as well.\nThe ETL management subsystems are the key components of the architecture \nto help achieve the goals of reliability, availability, and manageability. Operating \nand maintaining a data warehouse in a professional manner is not much diff erent \nthan any other systems operations: Follow standard best practices, plan for disaster, \nand practice. Most of the requisite management subsystems that follow might be \nfamiliar to you. \nSubsystem 22: Job Scheduler\nEvery  enterprise data warehouse should have a robust ETL scheduler. The entire ETL \nprocess should be managed, to the extent possible, through a single metadata-driven \njob control environment. Major ETL tool vendors package scheduling capabilities \ninto their environments. If you elect not to use the scheduler included with the ETL \ntool, or do not use an ETL tool, you need to utilize existing production scheduling \nor perhaps manually code the ETL jobs to execute.\nScheduling is much more than just launching jobs on a schedule. The scheduler \nneeds to be aware of and control the relationships and dependencies between ETL \njobs. It needs to recognize when a ﬁ le or table is ready to be processed. If the orga-\nnization is processing in real time, you need a scheduler that supports your selected \nreal-time architecture. The job control process must also capture metadata regard-\ning the progress and statistics of the ETL process during its execution. Finally, the \n",
      "content_length": 2441,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 520,
      "content": "Chapter 19\n484\nscheduler should support a fully automated process, including notifying the problem \nescalation system in the event of any situation that requires resolution.\nThe infrastructure to manage this can be as basic (and labor-intensive) as a set of \nSQL stored procedures, or as sophisticated as an integrated tool designed to manage \nand orchestrate multiplatform data extract and loading processes. If you use an ETL \ntool, it should provide this capability. In any case, you need to set up an environ-\nment for creating, managing, and monitoring the ETL job stream. \nThe job control services needed include:\n \n■Job deﬁ nition. The ﬁ rst step in creating an operations process is to have some \nway to deﬁ ne a series of steps as a job and to specify some relationship among \njobs. This is where the execution ﬂ ow of the ETL process is written. In many \ncases, if the load of a given table fails, it can impact your ability to load tables \nthat depend on it. For example, if the customer table is not properly updated, \nloading sales facts for new customers that did not make it into the customer \ntable is risky. In some databases, it is impossible.\n \n■Job scheduling. At a minimum, the environment needs to provide standard \ncapabilities, such as time- and event-based scheduling. ETL processes are often \nbased on some upstream system event, such as the successful completion of \nthe general ledger close or the successful application of sales adjustments to \nyesterday’s sales ﬁ gures. This includes the ability to monitor database ﬂ ags, \ncheck for the existence of ﬁ les, and compare creation dates.\n \n■Metadata capture. No self-respecting systems person would tolerate a black \nbox scheduling system. The folks responsible for running the loads will \ndemand a workﬂ ow monitoring system (subsystem 27) to understand what \nis going on. The job scheduler needs to capture information about what step \nthe load is on, what time it started, and how long it took. In a handcrafted \nETL system, this can be accomplished by having each step write to a log ﬁ le. \nThe ETL tool should capture this data every time an ETL process executes.\n \n■Logging. This means collecting information about the entire ETL process, \nnot just what is happening at the moment. Log information supports the \nrecovery and restarting of a process in case of errors during the job execution. \nLogging to text ﬁ les is the minimum acceptable level. We prefer a system that \nlogs to a database because the structure makes it easier to create graphs and \nreports. It also makes it possible to create time series studies to help analyze \nand optimize the load process.\n \n■Notiﬁ cation. After the ETL process has been developed and deployed, it \nshould execute in a hands-off  manner. It should run without human inter-\nvention, without fail. If a problem does occur, the control system needs to \ninterface to the problem escalation system (subsystem 30).\n",
      "content_length": 2937,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 521,
      "content": "ETL Subsystems and Techniques 485\nNOTE \nSomebody needs to know if anything unforeseen happened during the \nload, especially if a response is critical to continuing the process.\nSubsystem 23: Backup System\nThe  data warehouse is subject to the same risks as any other computer system. Disk \ndrives will fail, power supplies will go out, and sprinkler systems will accidentally \nturn on. In addition to these risks, the warehouse also has a need to keep more \ndata for longer periods of time than operational systems. Although typically not \nmanaged by the ETL team, the backup and recovery process is often designed as \npart of the ETL system. Its goal is to allow the data warehouse to get back to work \nafter a failure. This includes backing up the intermediate staging data necessary to \nrestart failed ETL jobs. The archive and retrieval process is designed to enable user \naccess to older data that has been moved out of the main warehouse onto a less \ncostly, usually lower-performing media.\nBackup\nEven if you have a fully redundant system with a universal power supply, fully RAIDed \ndisks, and parallel processors with failover, some system crisis will eventually visit. \nEven with perfect hardware, someone can always drop the wrong table (or database). \nAt the risk of stating the obvious, it is better to prepare for this than to handle it on-\nthe-ﬂ y. A full scale backup system needs to provide the following capabilities:\n \n■High performance.  The backup needs to ﬁ t into the allotted timeframe. This \nmay include online backups that don’t impact performance signiﬁ cantly, \nincluding real-time partitions. \n \n■Simple administration. The  administration interface should provide tools that \neasily allow you to identify objects to back up (including tables, tablespaces, \nand redo logs), create schedules, and maintain backup veriﬁ cation and logs \nfor subsequent restore.\n \n■Automated, lights-out operations. The  backup facility must provide storage \nmanagement services, automated scheduling, media and device handling, \nreporting, and notiﬁ cation.\nThe backup for the warehouse is usually a physical backup. This is an image \nof the database at a certain point in time, including indexes and physical layout \ninformation.\nArchive and Retrieval\nDeciding  what to move out of the warehouse is a cost-beneﬁ t issue. It costs money \nto keep the data around—it takes up disk space and slows the load and query \n",
      "content_length": 2425,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 522,
      "content": "Chapter 19\n486\ntimes. On the other hand, the business users just might need this data to do some \ncritical historical analyses. Likewise an auditor may request archived data as part \nof a compliance procedure. The solution is not to throw the data away but to put \nit some place that costs less but is still accessible. Archiving is the data security \nblanket for the warehouse.\nAs of this writing, the cost of online disk storage is dropping so rapidly that it \nmakes sense to plan many of archiving tasks to simply write to disk. Especially if \ndisk storage is handled by a separate IT resource, the requirement to “migrate and \nrefresh” is replaced by “refresh.” You need to make sure that you can interpret the \ndata at various points in the future.\nHow long it takes the data to get stale depends on the industry, the business, and \nthe particular data in question. In some cases, it is fairly obvious when older data \nhas little value. For example, in an industry with rapid evolution of new products \nand competitors, history doesn’t necessarily help you understand today or predict \ntomorrow. \nAfter a determination has been made to archive certain data, the issue becomes \n“what are the long-term implications of archiving data?” Obviously, you need \nto leverage existing mechanisms to physically move the data from its current media to \nanother media and ensure it can be recovered, along with an audit trail that accounts \nfor the accesses and alterations to the data. But what does it mean to “keep” old data? \nGiven increasing audit and compliance concerns, you may face archival require-\nments to preserve this data for ﬁ ve, 10, or perhaps even 50 years. What media should \nyou utilize? Will you be able to read that media in future years? Ultimately, you \nmay ﬁ nd yourself implementing a library system capable of archiving and regularly \nrefreshing the data, and then migrating it to more current structures and media.\nFinally, if you are archiving data from a system that is no longer going to be used, \nyou may need to “sunset” the data by extracting it from the system and writing it \nin a vanilla format that is independent of the original application. You might need \nto do this if the license to use the application will terminate.\n Subsystem 24: Recovery and Restart System\nAfter  the ETL system is in production, failures can occur for countless reasons \nbeyond the control of the ETL process. Common causes of ETL production failures \ninclude: \n \n■Network failure\n \n■Database failure\n \n■Disk failure\n \n■Memory failure\n \n■Data quality failure\n \n■Unannounced system upgrade \n",
      "content_length": 2600,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 523,
      "content": "ETL Subsystems and Techniques 487\nTo protect yourself from these failures, you need a solid backup system \n(subsystem 23) and a companion recovery and restart system. You must plan for \nunrecoverable errors during the load because they will happen. The system should \nanticipate this and provide crash recovery, stop, and restart capability. First, look \nfor appropriate tools and design processes to minimize the impact of a crash. For \nexample, a load process should commit relatively small sets of records at a time and \nkeep track of what has been committed. The size of the set should be adjustable \nbecause the transaction size has performance implications on diff erent DBMSs.\nThe recovery and restart system is used, of course, for either resuming a job that \nhas halted or for backing out the whole job and restarting it. This system is signiﬁ -\ncantly dependent on the capabilities of the backup system. When a failure occurs, \nthe initial knee-jerk reaction is to attempt to salvage whatever has processed and \nrestart the process from that point. This requires an ETL tool with solid and reli-\nable checkpoint functionality, so it can perfectly determine what has processed and \nwhat has not to restart the job at exactly the right point. In many cases, it may be \nbest to back out any rows that have been loaded as part of the process and restart \nfrom the beginning.\nWe often recommend designing fact tables with a single column primary sur-\nrogate key. This surrogate key is a simple integer that is assigned in sequence as \nrows are created to be added to the fact table. With the fact table surrogate key, \nyou can easily resume a load that is halted or back out all the rows in the load by \nconstraining on a range of surrogate keys.\nNOTE \nFact table surrogate keys have a number of uses in the ETL back room. \nFirst, as previously described, they can be used as the basis for backing out or \nresuming an interrupted load. Second, they provide immediate and unambiguous \nidentiﬁ cation of a single fact row without needing to constrain multiple dimen-\nsions to fetch a unique row. Third, updates to fact table rows can be replaced by \ninserts plus deletes because the fact table surrogate key is now the actual key for \nthe fact table. Thus, a row containing updated columns can be inserted into the \nfact table without overwriting the row it is to replace. When all such insertions are \ncomplete, then the underlying old rows can be deleted in a single step. Fourth, the \nfact table surrogate key is an ideal parent key to be used in a parent/child design. \nThe fact table surrogate key appears as a foreign key in the child, along with the \nparent’s dimension foreign keys.\nThe longer an ETL process runs, the more you must be aware of vulnerabilities \ndue to failure. Designing a modular ETL system made up of effi  cient processes that \nare resilient against crashes and unexpected terminations can reduce the risk of \na failure resulting in a massive recovery eff ort. Careful consideration of when to \nphysically stage data by writing it to disk, along with carefully crafted points of \n",
      "content_length": 3111,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 524,
      "content": "Chapter 19\n488\nrecovery and load date/time stamps or sequential fact table surrogate keys enable \nyou to specify appropriate restart logic.\nSubsystem 25: Version Control System \nThe  version control system is a “snapshotting” capability for archiving and recover-\ning all the logic and metadata of the ETL pipeline. It controls check-out and check-in \nprocessing for all ETL modules and jobs. It should support source comparisons to \nreveal diff erences between versions. This system provides a librarian function for \nsaving and restoring the complete ETL context of a single version. In certain highly \ncompliant environments, it will be equally important to archive the complete ETL \nsystem context alongside the relevant archived and backup data. Note that master \nversion numbers need to be assigned for the overall ETL system, just like software \nrelease version numbers.\nNOTE \nYou have a master version number for each part of the ETL system as \nwell as one for the system as a whole, don’t you? And you can restore yesterday’s \ncomplete ETL metadata context if it turns out there is a big mistake in the current \nrelease? Thank you for reassuring us.\nSubsystem 26: Version Migration System\nAfter  the ETL team gets past the diffi  cult process of designing and developing the ETL \nprocess and completes the creation of the jobs required to load the data warehouse, the \njobs must be bundled and migrated to the next environment—from development to \ntest and on to production—according to the lifecycle adopted by the organization. The \nversion migration system needs to interface to the version control system to control \nthe process and back out a migration if needed. It should provide a single interface \nfor setting connection information for the entire version.\nMost organizations isolate the development, testing, and production environments. \nYou need to be able to migrate a complete version of the ETL pipeline from devel-\nopment, into test, and ﬁ nally into production. Ideally, the test system is identically \nconﬁ gured to its corresponding production system. Everything done to the production \nsystem should have been designed in development and the deployment script tested \non the test environment. Every back room operation should go through rigorous \nscripting and testing, whether deploying a new schema, adding a column, changing \nindexes, changing the aggregate design, modifying a database parameter, backing \nup, or restoring. Centrally managed front room operations such as deploying new BI \ntools, deploying new corporate reports, and changing security plans should be equally \nrigorously tested and scripted if the BI tools allow it.\n",
      "content_length": 2667,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 525,
      "content": "ETL Subsystems and Techniques 489\nSubsystem 27: Workﬂ ow Monitor\nSuccessful  data warehouses are consistently and reliably available, as agreed to with \nthe business community. To achieve this goal, the ETL system must be constantly \nmonitored to ensure the ETL processes are operating effi  ciently and the warehouse \nis being loaded on a consistently timely basis. The job scheduler (subsystem 22) \nshould capture performance data every time an ETL process is initiated. This data \nis part of the process metadata captured in the ETL system. The workﬂ ow monitor \nleverages the metadata captured by the job scheduler to provide a dashboard and \nreporting system taking many aspects of the ETL system into consideration. You’ll \nwant to monitor job status for all job runs initiated by the job scheduler including \npending, running, completed and suspended jobs, and capture the historical data \nto support trending performance over time. Key performance measures include \nthe number of records processed, summaries of errors, and actions taken. Most \nETL tools capture the metrics for measuring ETL performance. Be sure to trigger \nalerts whenever an ETL job takes signiﬁ cantly more or less time to complete than \nindicated by the historical record.\nIn combination with the job scheduler, the workﬂ ow monitor should also track \nperformance and capture measurements of the performance of infrastructure com-\nponents including CPU usage, memory allocation and contention, disk utilization \nand contention, buff er pool usage, database performance, and server utilization and \ncontention. Much of this information is process metadata about the ETL system \nand should be considered as part of the overall metadata strategy (subsystem 34).\nThe workﬂ ow monitor has a more signiﬁ cant strategic role than you might sus-\npect. It is the starting point for the analysis of performance problems across the \nETL pipeline. ETL performance bottlenecks can occur in many places, and a good \nworkﬂ ow monitor shows where the bottlenecks are occurring. Chapter 20, discusses \nmany ways to improve performance in the ETL pipeline, but this list is more or less \nordered starting with the most important bottlenecks:\n \n■Poorly indexed queries against a source system or intermediate table\n \n■SQL syntax causing wrong optimizer choice\n \n■Insuffi  cient random access memory (RAM) causing thrashing\n \n■Sorting in the RDBMS\n \n■Slow transformation steps\n \n■Excessive I/O\n \n■Unnecessary writes followed by reads\n \n■Dropping and rebuilding aggregates from scratch rather than incrementally\n \n■Filtering (change data capture) applied too late in the pipeline\n \n■Untapped opportunities for parallelizing and pipelining\n",
      "content_length": 2698,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 526,
      "content": "Chapter 19\n490\n \n■Unnecessary transaction logging especially if doing updates\n \n■Network traffi  c and ﬁ le transfer overhead\nSubsystem 28: Sorting System\nCertain  common ETL processes call for data to be sorted in a particular order, \nsuch as aggregating and joining ﬂ at ﬁ le sources. Because sorting is such a funda-\nmental ETL processing capability, it is called out as a separate subsystem to ensure \nit receives proper attention as a component of the ETL architecture. There are a \nvariety of technologies available to provide sorting capabilities. An ETL tool can \nundoubtedly provide a sort function, the DBMS can provide sorting via the SQL \nSORT clause, and there are a number of sort utilities available.\nSorting simple delimited text ﬁ les with a dedicated sort package is awesomely \nfast. These packages typically allow a single read operation to produce up to eight \ndiff erent sorted outputs. Sorting can produce aggregates where each break row of a \ngiven sort is a row for the aggregate table, and sorting plus counting is often a good \nway to diagnose data quality issues.\nThe key is to choose the most effi  cient sort resource to support the requirements \nwithin your infrastructure. The easy answer for most organizations is to simply \nutilize the ETL tool’s sort function. However, in some situations it may be more \neffi  cient to use a dedicated sort package; although ETL and DBMS vendors claim to \nhave made up much of the performance diff erences.\nSubsystem 29: Lineage and Dependency Analyzer\nTwo  increasingly important elements being requested of the ETL system are the \nability to track both the lineage and dependencies of data in the DW/BI system:\n \n■Lineage. Beginning with a speciﬁ c data element in an intermediate table or BI \nreport, identify the source of that data element, other upstream intermediate \ntables containing that data element and its sources, and all transformations \nthat data element and its sources have undergone.\n \n■Dependency. Beginning with a speciﬁ c data element in a source table or an \nintermediate table, identify all downstream intermediate tables and ﬁ nal BI \nreports containing that data element or its derivations and all transformations \napplied to that data element and its derivations.\nLineage analysis is often an important component in a highly compliant environ-\nment where you must explain the complete processing ﬂ ow that changed any data \nresult. This means the ETL system must display the ultimate physical sources and \nall subsequent transformations of any selected data element, chosen either from the \n",
      "content_length": 2586,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 527,
      "content": "ETL Subsystems and Techniques 491\nmiddle of the ETL pipeline or on a ﬁ nal delivered report. Dependency analysis is \nimportant when assessing changes to a source system and the downstream impacts \non the data warehouse and ETL system. This implies the ability to display all aff ected \ndownstream data elements and ﬁ nal report ﬁ elds aff ected by a potential change in \nany selected data element, chosen either in the middle of the ETL pipeline or an \noriginal source (dependency).\nSubsystem 30: Problem Escalation System\nTypically,  the ETL team develops the ETL processes and the quality assurance \nteam tests them thoroughly before they are turned over to the group responsible \nfor day-to-day systems operations. To make this work, the ETL architecture needs \nto include a proactively designed problem escalation system similar to what is in \nplace for other production systems.\nAfter the ETL processes have been developed and tested, the ﬁ rst level of oper-\national support for the ETL system should be a group dedicated to monitoring \nproduction applications. The ETL development team becomes involved only if the \noperational support team cannot resolve a production problem.\nIdeally, you have developed ETL processes, wrapped them into an automated \nscheduler, and have robust workﬂ ow monitoring capabilities peering into the ETL \nprocesses as they execute. The execution of the ETL system should be a hands-off  \noperation. It should run like clockwork without human intervention and without \nfail. If a problem does occur, the ETL process should automatically notify the \nproblem escalation system of any situation that needs attention or resolution. \nThis automatic feed may take the form of simple error logs, operator notiﬁ cation \nmessages, supervisor notiﬁ cation messages, and system developer messages. The \nETL system may notify an individual or a group depending on the severity of the \nsituation or the processes involved. ETL tools can support a variety of messag-\ning capabilities including e-mail alerts, operator messages, and notiﬁ cations to \nmobile devices.\nEach notiﬁ cation event should be written to a database used to understand the \ntypes of problems that arise, their status, and resolution. This data forms part of \nthe process metadata captured by the ETL system (subsystem 34). You need to \nensure that organizational procedures are in place for proper escalation, so every \nproblem is resolved appropriately.\nIn general, the support structure for the ETL system should follow a fairly stan-\ndard support structure. First, level support is typically a help desk that is the ﬁ rst \npoint of contact when a user notices an error. The help desk is responsible for \nresolution whenever feasible. If the help desk cannot resolve the issue, the sec-\nond level support is notiﬁ ed. This is typically a systems administrator or DBA on \n",
      "content_length": 2867,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 528,
      "content": "Chapter 19\n492\nthe production control technical staff  capable of supporting general infrastructure \nfailures. The ETL manager is the third level support and should be knowledgeable \nto support most issues that arise in the ETL production process. Finally, when all \nelse fails, the ETL developer should be called in to analyze the situation and assist \nwith resolution. \nSubsystem 31: Parallelizing/Pipelining System\nThe  goal of the ETL system, in addition to providing high quality data, is to load the \ndata warehouse within the allocated processing window. In large organizations with \nhuge data volumes and a large portfolio of dimensions and facts, loading the data \nwithin these constraints can be a challenge. The paralleling/pipelining system pro-\nvides capabilities to enable the ETL system to deliver within these time constraints. \nThe goal of this system is to take advantage of multiple processors or grid computing \nresources commonly available. It is highly desirable, and in many cases necessary, that \nparallelizing and pipelining be automatically invoked for every ETL process unless \nspeciﬁ c conditions preclude it from processing in such a manner, such as waiting on \na condition in the middle of the process.\nParallelizing is a powerful performance technique at every stage of the ETL \npipeline. For example, the extraction process can be parallelized by logically parti-\ntioning on ranges of an attribute. Verify that the source DBMS handles parallelism \ncorrectly and doesn’t spawn conﬂ icting processes. If possible, choose an ETL tool \nthat handles parallelizing of intermediate transformation processes automatically. \nIn some tools it is necessary to hand create parallel processes. This is ﬁ ne until \nyou add additional processors, and the ETL system then can’t take advantage of the \ngreater parallelization opportunities unless you modify the ETL modules by hand \nto increase the number of parallel ﬂ ows.\nSubsystem 32: Security System\nSecurity  is an important consideration for the ETL system. A serious security breach \nis much more likely to come from within the organization than from someone hack-\ning in from the outside. Although we don’t like to think it, the folks on the ETL \nteam present as much a potential threat as any group inside the organization. We \nrecommend administering role-based security on all data and metadata in the ETL \nsystem. To support compliance requirements, you may need to prove that a version \nof an ETL module hasn’t been changed or show who made changes to a module. \nYou should enforce comprehensive authorized access to all ETL data and metadata \nby individual and role. In addition, you’ll want to maintain a historical record of \nall accesses to ETL data and metadata by individual and role. Another issue to be \ncareful of is the bulk data movement process. If you move data across the network, \n",
      "content_length": 2875,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 529,
      "content": "ETL Subsystems and Techniques 493\neven if it is within the company ﬁ rewall, it pays to be careful. Make sure to use data \nencryption or a ﬁ le transfer utility that uses a secure transfer protocol.\nAnother back room security issue to consider is administrator access to the \nproduction warehouse server and software. We’ve seen situations where no one on \nthe team had security privileges; in other cases, everyone had access to everything. \nObviously, many members of the team should have privileged access to the devel-\nopment environment, but the production warehouse should be strictly controlled. \nOn the other hand, someone from the DW/BI team needs to be able to reset the \nwarehouse machine if something goes wrong. Finally, the backup media should be \nguarded. The backup media should have as much security surrounding them as \nthe online systems.\nSubsystem 33: Compliance Manager\nIn  highly compliant environments, supporting compliance requirements is a sig-\nniﬁ cant new requirement for the ETL team. Compliance in the data warehouse \ninvolves “maintaining the chain of custody” of the data. In the same way a police \ndepartment must carefully maintain the chain of custody of evidence to argue that \nthe evidence has not been changed or tampered with, the data warehouse must also \ncarefully guard the compliance-sensitive data entrusted to it from the moment it \narrives. Furthermore, the data warehouse must always show the exact condition and \ncontent of such data at any point in time that it may have been under the control \nof the data warehouse. The data warehouse must also track who had authorized \naccess to the data. Finally, when the suspicious auditor looks over your shoulder, \nyou need to link back to an archived and time-stamped version of the data as it \nwas originally received, which you have stored remotely with a trusted third party. \nIf the data warehouse is prepared to meet all these compliance requirements, then \nthe stress of being audited by a hostile government agency or lawyer armed with a \nsubpoena should be greatly reduced.\nThe compliance requirements may mean you cannot actually change any data, for \nany reason. If data must be altered, then a new version of the altered records must \nbe inserted into the database. Each row in each table therefore must have begin \nand end time stamps that accurately represents the span of time when the record \nwas the “current truth.” The big impact of these compliance requirements on the \ndata warehouse can be expressed in simple dimensional modeling terms. Type 1 \nand type 3 changes are dead. In other words, all changes become inserts. No more \ndeletes or overwrites.\nFigure 19-12 shows how a fact table can be augmented so that overwrite changes \nare converted into a fact table equivalent of a type 2 change. The original fact table \nconsisted of the lower seven columns starting with activity date and ending with \n",
      "content_length": 2914,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 530,
      "content": "Chapter 19\n494\nnet dollars. The original fact table allowed overwrites. For example, perhaps there \nis a business rule that updates the discount and net dollar amounts after the row is \noriginally created. In the original version of the table, history is lost when the over-\nwrite change takes place, and the chain of custody is broken.\nFact Table Surrogate Key\nBegin Version Date/Time\nEnd Version Date/Time\nChange Reference Key (FK)\nSource Reference Key (FK)\nActivity Date Key (FK)\nActivity Date/Time\nCustomer Key (FK)\nService Key (FK)\nGross Dollars\nDiscount Dollars\nNet Dollars\n(PK)\n(PK)\nOriginal fact table columns\nCompliance-Enabled\nTransaction Grain Fact\nFigure 19-12: Compliance-enabled transaction fact table.\nTo  convert the fact table to be compliance-enabled, ﬁ ve columns are added, as \nshown in bold. A fact table surrogate key is created for each original unmodiﬁ ed fact \ntable row. This surrogate key, like a dimension table surrogate key, is just a unique \ninteger that is assigned as each original fact table row is created. The begin version \ndate/time stamp is the exact time of creation of the fact table row. Initially, the end \nversion date/time is set to a ﬁ ctitious date/time in the future. The change reference \nis set to “original,” and the source reference is set to the operational source.\nWhen an overwrite change is needed, a new row is added to the fact table with the \nsame fact table surrogate key, and the appropriate regular columns changed, such \nas discount dollars and net dollars. The begin version date/time column is set to the \nexact date/time when the change in the database takes place. The end version date/\ntime is set to a ﬁ ctitious date/time in the future. The end version date/time of the \noriginal fact row is now set to the exact date/time when the change in the database \ntakes place. The change reference now provides an explanation for the change, and \nthe source reference provides the source of the revised columns.\nReferring to the design in Figure 19-12, a speciﬁ c moment in time can be selected \nand the fact table constrained to show exactly what the rows contained at that \nmoment. The alterations to a given row can be examined by constraining to a spe-\nciﬁ c fact table surrogate key and sorting by begin version date/time.\nThe compliance machinery is a signiﬁ cant addition to a normal fact table (refer to \nFigure 19-12). If the compliance-enabled table is actually used for only demonstrating \n",
      "content_length": 2463,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 531,
      "content": "ETL Subsystems and Techniques 495\ncompliance, then a normal version of the fact table with just the original columns \ncan remain as the main operational table, with the compliance-enabled table existing \nonly in the background. The compliance-enabled table doesn’t need to be indexed \nfor performance because it will not be used in a conventional BI environment.\nFor heaven’s sake, don’t assume that all data is now subject to draconian compli-\nance restrictions. It is essential you receive ﬁ rm guidelines from the chief compliance \noffi  cer before taking any drastic steps.\nThe foundation of a compliance system is the interaction of several of the subsys-\ntems already described married to a few key technologies and capabilities:\n \n■Lineage analysis.  Show where a ﬁ nal piece of data came from to prove the \noriginal source data plus the transformations including stored procedures \nand manual changes. This requires full documentation of all the transforms and \nthe technical ability to rerun the transforms against the original data.\n \n■Dependency analysis.  Show where an original source data element was ever \nused.\n \n■Version control.  It may be necessary to rerun the source data through the \nETL system in eff ect at the time, requiring the exact version of the ETL system \nfor any given data source. \n \n■Backup and restore.  Of course, the requested data may have been archived years \nago and need to be restored for audit purposes. Hopefully, you archived the \nproper version of the ETL system alongside the data, so both the data and \nthe system can be restored. It may be necessary to prove the archived data hasn’t \nbeen altered. During the archival process, the data can be hash-coded and the \nhash and data separated. Have the hash codes archived separately by a trusted \nthird party. Then, when demanded, restore the original data, hash code it again, \nand then compare to the hash codes retrieved from the trusted third party to \nprove the authenticity of the data.\n \n■Security. Show  who has accessed or modiﬁ ed the data and transforms. Be \nprepared to show roles and privileges for users. Guarantee the security log \ncan’t be altered by using a write once media.\n \n■Audit dimension. The  audit dimension ties runtime metadata context directly \nwith the data to capture quality events at the time of the load.\nSubsystem 34: Metadata Repository Manager \nThe  ETL system is responsible for the use and creation of much of the metadata \ninvolved in the DW/BI environment. Part of the overall metadata strategy should \nbe to speciﬁ cally capture ETL metadata, including the process metadata, technical \nmetadata, and business metadata. Develop a balanced strategy between doing noth-\ning and doing too much. Make sure there’s time in the ETL development tasks to \n",
      "content_length": 2785,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 532,
      "content": "Chapter 19\n496\ncapture and manage metadata. And ﬁ nally, make sure someone on the DW/BI team \nis assigned the role of metadata manager and owns the responsibility for creating \nand implementing the metadata strategy.\nSummary\nIn this chapter we have introduced the key building blocks of the ETL system. As \nyou may now better appreciate, building an ETL system is unusually challenging; \nthe ETL system must address a number of demanding requirements. This chapter \nidentiﬁ ed and reviewed the 34 subsystems of ETL and gathered these subsystems \ninto four key areas that represent the  ETL process: extracting, cleaning and con-\nforming, delivering, and managing. Careful consideration of all the elements of \nthe ETL architecture is the key to success. You must understand the full breadth \nof requirements and then set an appropriate and eff ective architecture in place. \nETL is more than simply extract, transform, and load; it’s a host of complex and \nimportant tasks. In the next chapter we will describe the processes and tasks for \nbuilding the ETL system.\n",
      "content_length": 1065,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 533,
      "content": "ETL System Design \nand Development \nProcess and Tasks\nD\neveloping the extract, transformation, and load (ETL) system is the hidden \npart of the iceberg for most DW/BI projects. So many challenges are buried in \nthe data sources and systems that developing the ETL application invariably takes \nmore time than expected. This chapter is structured as a 10-step plan for creating the \ndata warehouse’s ETL system. The concepts and approach described in this chapter, \nbased on content from The Data Warehouse Lifecycle Toolkit, Second Edition (Wiley, \n2008), apply to systems based on an ETL tool, as well as hand-coded systems. \nChapter 20 discusses the following concepts:\n \n■ETL system planning and design consideration\n \n■Recommendations for one-time historic data loads\n \n■Development tasks for incremental load processing\n \n■Real-time data warehousing considerations \nETL Process Overview\nThis  chapter follows the ﬂ ow of planning and implementing the ETL system. We \nimplicitly discuss the 34 ETL subsystems presented in Chapter 19: ETL Subsystems \nand Techniques, broadly categorized as extracting data, cleaning and conforming, \ndelivering for presentation, and managing the ETL environment.\nBefore beginning the ETL system design for a dimensional model, you should \nhave completed the logical design, drafted your high-level architecture plan, and \ndrafted the source-to-target mapping for all data elements.\nThe ETL system design process is critical. Gather all the relevant information, \nincluding the processing burden the extracts will be allowed to place on the opera-\ntional source systems, and test some key alternatives. Does it make sense to host the \ntransformation process on the source system, target system, or its own platform? \nWhat tools are available on each, and how eff ective are they?\n20\n",
      "content_length": 1818,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 534,
      "content": "Chapter 20\n498\nDevelop the ETL Plan\nETL  development starts out with the high-level plan, which is independent of any \nspeciﬁ c technology or approach. However, it’s a good idea to decide on an ETL tool \nbefore doing any detailed planning; this can avoid redesign and rework later in the \nprocess.\nStep 1: Draw the High-Level Plan\nWe  start the design process with a very simple schematic of the known pieces of the \nplan: sources and targets, as shown in Figure 20-1. This schematic is for a ﬁ ctitious \nutility company’s data warehouse, which is primarily sourced from a 30-year-old \nCOBOL system. If most or all the data comes from a modern relational transaction \nprocessing system, the boxes often represent a logical grouping of tables in the \ntransaction system model.\nSources\nCustomer\nMaster\n(RDBMS)\nGeography\nMaster\n(RDBMS)\nDMR system\n(COBOL flat file,\n2000 fields,\none row per customer)\nMeters\n(MS Access)\nDate\n(Spreadsheet)\nSlowly changing\non demographics\nand account\nstatus\n25M customers\n–10k new or\nchanged\ncustomers/day\nCustomer\nTargets\n15,000\ngeogs\ncheck\nRI\nLabels need\ncosmetic work!\nGeography\n“Unbucketize”\nfrom 13 months\nin one row\nProcess 750k\ncustomers/\nday\nMissed meter\nreads, estimated\nbills, restatement\nof bills\nElectricity\nUsage\nOld (pre-1972) meter\ntypes are not in the\nMeters Group’s system\nThere are 73 known\nmeter types\nHow/by whom\nmaintained??\ncheck\nRI\ncheck\nRI\nUsage Month\nElectric Meter\nMeter Read Date\nFigure 20-1: Example high-level data staging plan schematic.\nAs you develop the detailed ETL system speciﬁ cation, the high-level view requires \nadditional details. Figure 20-1 deliberately highlights contemporary questions and \nunresolved issues; this plan should be frequently updated and released. You might \nsometimes keep two versions of the diagram: a simple one for communicating \n",
      "content_length": 1823,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 535,
      "content": "ETL System Design and Development Process and Tasks 499\nwith people outside the team and a detailed version for internal DW/BI team \ndocumentation.\nStep 2: Choose an ETL Tool\nThere  are a multitude of ETL tools available in the data warehouse marketplace. \nMost of the major database vendors off er an ETL tool, usually at additional licensing \ncost. There are also excellent ETL tools available from third-party vendors. \nETL tools read data from a range of sources, including ﬂ at ﬁ les, ODBC, OLE DB, \nand native database drivers for most relational databases. The tools contain func-\ntionality for deﬁ ning transformations on that data, including lookups and other \nkinds of joins. They can write data into a variety of target formats. And they all \ncontain some functionality for managing the overall logic ﬂ ow in the ETL system.\nIf  the source systems are relational, the transformation requirements are straight-\nforward, and good developers are on staff , the value of an ETL tool may not be \nimmediately obvious. However, there are several reasons that using an ETL tool is \nan industry standard best practice:\n \n■Self-documentation that comes from using a graphical tool. A hand-coded \nsystem is usually an impenetrable mess of staging tables, SQL scripts, stored \nprocedures, and operating system scripts.\n \n■Metadata foundation for all steps of the ETL process.\n \n■Version control for multi-developer environments and for backing out and \nrestoring consistent versions.\n \n■Advanced transformation logic, such as fuzzy matching algorithms, inte-\ngrated access to name and address deduplication routines, and data mining \nalgorithms.\n \n■Improved system performance at a lower level of expertise. Relatively few SQL \ndevelopers are truly expert on how to use the relational database to manipulate \nextremely large data volumes with excellent performance.\n \n■Sophisticated processing capabilities, including automatically parallel-\nizing tasks, and automatic fail-over when a processing resource becomes \nunavailable.\n \n■One-step conversion of virtualized data transformation modules into their \nphysical equivalents.\nDon’t expect to recoup the investment in an ETL tool on the ﬁ rst phase of the \nDW/BI project. The learning curve is steep enough that developers sometimes feel \nthe project could have been implemented faster by coding. The big advantages \ncome with future phases, and particularly with future modiﬁ cations to existing \nsystems.\n",
      "content_length": 2457,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 536,
      "content": "Chapter 20\n500\nStep 3: Develop Default Strategies\nWith an overall idea of what needs to happen and what the ETL tool’s infrastructure \nrequires, you should develop a set of default strategies for the common activities in \nthe ETL system. These activities include:\n \n■Extract from each major source system. At this point in the design process, \nyou can determine the default method for extracting data from each source \nsystem. Will you normally push from the source system to a ﬂ at ﬁ le, extract \nin a stream, use a tool to read the database logs, or another method? This \ndecision can be modiﬁ ed on a table-by-table basis. If using SQL to access \nsource system data, make sure the native data extractors are used rather than \nODBC, if that’s an option.\n \n■Archive extracted and staged data. Extracted or staged data, before it’s been \ntransformed, should be archived for at least a month. Some organizations \npermanently archive extracted and staged data.\n \n■Police data quality for dimensions and particularly facts. Data  quality must \nbe monitored during the ETL process rather than waiting for business users \nto ﬁ nd data problems. Chapter 19 describes a comprehensive architecture \nfor measuring and responding to data quality issues in ETL subsystems 4 \nthrough 8.\n \n■Manage changes to dimension attributes. In Chapter 19, we described the \nlogic required to manage dimension attribute changes in ETL subsystem 9.\n \n■Ensure the data warehouse and ETL system meet the system availability \nrequirements. The ﬁ rst step to meeting availability requirements is to docu-\nment them. You should document when each data source becomes available \nand block out high-level job sequencing.\n \n■Design the data auditing subsystem. Each row in the data warehouse tables \nshould be tagged with auditing information that describes how the data \nentered the system.\n \n■Organize the ETL staging area. Most ETL systems stage the data at least once \nor twice during the ETL process. By staging, we mean the data will be written \nto disk for a later ETL step and for system recovery and archiving.\nStep 4: Drill Down by Target Table\nAfter  overall strategies for common ETL tasks have been developed, you should start \ndrilling into the detailed transformations needed to populate each target table in the \ndata warehouse. As you’re ﬁ nalizing the source-to-target mappings, you also perform \nmore data proﬁ ling to thoroughly understand the necessary data transformations \nfor each table and column.\n",
      "content_length": 2490,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 537,
      "content": "ETL System Design and Development Process and Tasks 501\n Ensure Clean Hierarchies\nIt’s  particularly important to investigate whether hierarchical relationships in the \ndimension data are perfectly clean. Consider a product dimension that includes \na hierarchical rollup from product stock keeping unit (SKU) to product category.\nIn our experience, the most reliable hierarchies are well managed in the source \nsystem. The best source systems normalize the hierarchical levels into multiple \ntables, with foreign key constraints between the levels. In this case, you can be \nconﬁ dent the hierarchies are clean. If the source system is not normalized—espe-\ncially if the source for the hierarchies is an Excel spreadsheet on a business user’s \ndesktop—then you must either clean it up or acknowledge that it is not a hierarchy.\nDevelop Detailed Table Schematics\nFigure 20-2 illustrates the level of detail that’s useful for the table-speciﬁ c drilldown; \nit’s for one of the tables in the utility company example previously illustrated.\nDetailed\nMaster\nRecord\n(DMR)\nDMR Extract: 80 cols including 13 monthly\nbuckets for usage. One row per customer meter,\nsorted by customer.\nEBCDIC file, tab-delimited.\nFile name: xdmr_yyyymmdd.dat\nNo transformations on mainframe—need to\nminimize mainframe coding and load.\nCompress\nencrypt ftp\nxdmr_yyyymmdd.dat\nDimension\nprocessing\nFact_stage1 (only fields\nrelevant to fact table),\nincludes customer &\nmonth surrogate keys\nElectric_\nUsage_Fact\nFact_stage4,\nincludes\nread_date\nsurrogate key\nFact_stage3,\nincludes geog\nsurrogate key\nFact_stage2,\nincludes meter\nsurrogate key\nBulk-load into\nelectric_usage_fact\nSort by\ndate\nlookup\ndate_key\nSort by\ngeog\nlookup\ngeog_key\nSort by meter_type,\nlookup meter_key\nUnbucketize: Usage1 keyed to yyyymm, Usage2\nkeyed to yyyymm-1,... When\nyyyymm<subscribe_date, stop processing row\nWill have –13x rows.\nData presorted by cust, so can perform cust_key\nlookup on same pass.\nFigure 20-2: Example draft detailed load schematic for the fact table. \nAll the dimension tables must be processed before the key lookup steps for the fact \ntable. The dimension tables are usually independent from each other, but sometimes \nthey also have processing dependencies. It’s important to clarify these dependencies, \nas they become ﬁ xed points around which the job control  ﬂ ows.\n",
      "content_length": 2336,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 538,
      "content": "Chapter 20\n502\nDevelop the ETL Speciﬁ cation Document\nWe’ve  walked through some general strategies for high-level planning and the physi-\ncal design of the ETL system. Now it’s time to pull everything together and develop \na detailed speciﬁ cation for the entire ETL system.\nAll the documents developed so far—the source-to-target mappings, data proﬁ l-\ning reports, physical design decisions—should be rolled into the ﬁ rst sections of \nthe ETL speciﬁ cation. Then document all the decisions discussed in this chapter, \nincluding:\n \n■Default strategy for extracting from each major source system\n \n■Archiving strategy\n \n■Data quality tracking and metadata\n \n■Default strategy for managing changes to dimension attributes\n \n■System availability requirements and strategy\n \n■Design of the data auditing subsystem\n \n■Locations of staging areas\nThe next section of the ETL speciﬁ cation describes the historic and incremen-\ntal load strategies for each table. A good speciﬁ cation includes between two and \n10 pages of detail for each table, and documents the following information and \ndecisions:\n \n■Table design (column names, data types, keys, and constraints)\n \n■Historic data load parameters (number of months) and volumes (row counts)\n \n■Incremental data volumes, measured as new and updated rows per load cycle\n \n■Handling of late arriving data for facts and dimensions\n \n■Load frequency\n \n■Handling of slowly changing dimension (SCD) changes for each dimension \nattribute\n \n■Table partitioning, such as monthly\n \n■Overview of data sources, including a discussion of any unusual source char-\nacteristics, such as an unusually brief access window\n \n■Detailed source-to-target mapping\n \n■Source data proﬁ ling, including at least the minimum and maximum values \nfor each numeric column, count of distinct values in each column, and inci-\ndence of NULLs\n \n■Extract strategy for the source data (for example, source system APIs, direct \nquery from database, or dump to ﬂ at ﬁ les)\n \n■Dependencies, including which other tables must be loaded before this table \nis processed\n \n■Document the transformation logic. It’s easiest to write this section as pseudo \ncode or a diagram, rather than trying to craft complete sentences.\n",
      "content_length": 2226,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 539,
      "content": "ETL System Design and Development Process and Tasks 503\n \n■Preconditions to avoid error conditions. For example, the ETL system must \ncheck for ﬁ le or database space before proceeding.\n \n■Cleanup steps, such as deleting working ﬁ les\n \n■An estimate of whether this portion of the ETL system will be easy, medium, \nor diffi  cult to implement\nNOTE \nAlthough most people would agree that all the items described in the \nETL system speciﬁ cation document are necessary, it’s a lot of work to pull this \ndocument together, and even more work to keep it current as changes occur. \nRealistically, if you pull together the “one-pager” high-level ﬂ ow diagram, data \nmodel and source-to-target maps, and a ﬁ ve-page description of what you plan to \ndo, you’ll get a better start than most teams.\nDevelop a Sandbox Source System\nDuring  the ETL development process, the source system data needs to be inves-\ntigated at great depth. If the source system is heavily loaded, and there isn’t some \nkind of reporting instance for operational queries, the DBAs may be willing to set \nup a static snapshot of the database for the ETL development team. Early in the \ndevelopment process, it’s convenient to poke around sandbox versions of the source \nsystems without worrying about launching a kind of killer query.\nIt’s easy to build a sandbox source system that simply copies the original; build a \nsandbox with a subset of data only if the data volumes are extremely large. On the \nplus side, this sandbox could become the basis of training materials and tutorials \nafter the system is deployed into production.\nDevelop One-Time Historic Load Processing\nAfter  the ETL speciﬁ cation has been created, you typically focus on developing \nthe ETL process for the one-time load of historic data. Occasionally, the same ETL \ncode can perform both the initial historic load and ongoing incremental loads, but \nmore often you build separate ETL processes for the historic and ongoing loads. The \nhistoric and incremental load processes have a lot in common, and depending on \nthe ETL tool, signiﬁ cant functionality can be reused from one to the other.\nStep 5: Populate Dimension Tables with Historic Data\nIn  general, you start building the ETL system with the simplest dimension tables. \nAfter these dimension tables have been successfully built, you tackle the historic \nloads for dimensions with one or more columns managed as SCD type 2.\n",
      "content_length": 2423,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 540,
      "content": "Chapter 20\n504\nPopulate Type 1 Dimension Tables\nThe easiest type of table to populate is a dimension table for which all attributes \nare managed as type 1 overwrites. With a type 1–only dimension, you extract the \ncurrent value for each dimension attribute from the source system.\nDimension Transformations\nEven the simplest dimension table may require substantial data cleanup and will \ncertainly require surrogate key assignment. \nSimple Data Transformations\nThe  most common, and easiest, form of data transformation is data type conversion. \nAll ETL tools have rich functions for data type conversion. This task can be tedious, \nbut it is seldom onerous. We strongly recommend replacing NULL values with \ndefault values within dimension tables; as we have discussed previously, NULLs \ncan cause problems when they are directly queried.\nCombine from Separate Sources\nOften  dimensions are derived from several sources. Customer information may \nneed to be merged from several lines of business and from outside sources. There \nis seldom a universal key pre-embedded in the various sources that makes this \nmerge operation easy.\nMost consolidation and deduplicating tools and processes work best if names \nand addresses are ﬁ rst parsed into their component pieces. Then you can use a \nset of passes with fuzzy logic that account for misspellings, typos, and alternative \nspellings such as I.B.M., IBM, and International Business Machines. In most orga-\nnizations, there is a large one-time project to consolidate existing customers. This \nis a tremendously valuable role for master data management systems.\nDecode Production Codes\nA  common merging task in data preparation is looking up text equivalents for pro-\nduction codes. In some cases, the text equivalents are sourced informally from a \nnonproduction source such as a spreadsheet. The code lookups are usually stored in \na table in the staging database. Make sure the ETL system includes logic for creat-\ning a default decoded text equivalent for the case in which the production code is \nmissing from the lookup table.\n Validate Many-to-One and One-to-One Relationships\nThe  most important dimensions probably have one or more rollup paths, such as \nproducts rolling up to product model, subcategory, and category, as illustrated in \nFigure 20-3. These hierarchical rollups need to be perfectly clean.\n",
      "content_length": 2365,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 541,
      "content": "ETL System Design and Development Process and Tasks 505\nProduct Dimension\nProduct Key (PK)\nProduct SKU\nProduct Name\nProduct Description\nProduct Model\nProduct Model Description\nSubcategory Description\nCategory Description\nCategory Manager\nFigure 20-3: Product dimension table with a hierarchical relationship.\nMany-to-one relationships between attributes, such as a product to product \nmodel, can be veriﬁ ed by sorting on the “many” attribute and verifying that each \nvalue has a unique value on the “one” attribute. For example, this query returns \nthe products that have more than one product model:\nSELECT Product_SKU,\ncount[*] as Row_Count, \ncount(distinct Product_Model) as Model_Count\nFROM StagingDatabase.Product\nGROUP BY Product_SKU\nHAVING count(distinct Product_Model) > 1 ;\nDatabase administrators sometimes want to validate many-to-one relationships \nby loading data into a normalized snowﬂ ake version of the dimension table in the \nstaging database, as illustrated in Figure 20-4. Note that the normalized version \nrequires individual keys at each of the hierarchy levels. This is not a problem if the \nsource system supplies the keys, but if you normalize the dimension in the ETL \nenvironment, you need to create them.\nThe snowﬂ ake structure has some value in the staging area: It prevents you from \nloading data that violates the many-to-one relationship. However, in general, the \nrelationships should be pre-veriﬁ ed as just described, so that you never attempt to \nload bad data into the dimension table. After the data is pre-veriﬁ ed, it’s not tremen-\ndously important whether you make the database engine reconﬁ rm the relationship \nat the moment you load the table.\nIf the source system for a dimensional hierarchy is a normalized database, it’s \nusually unnecessary to repeat the normalized structure in the ETL staging area. \nHowever, if the hierarchical information comes from an informal source such as a \nspreadsheet managed by the marketing department, you may beneﬁ t from normal-\nizing the hierarchy in the ETL system.\n",
      "content_length": 2051,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 542,
      "content": "Chapter 20\n506\nProduct Table\nProduct Key (PK)\nProduct SKU\nProduct Name\nProduct Description\nProduct Model Key (FK)\nProduct Model Table\nProduct Model Key (PK)\nProduct Model Number\nProduct Model Description\nProduct Subcategory Key (FK)\nProduct Subcategory Table\nProduct Subcategory Key (PK)\nProduct Subcategory Description\nProduct Category Key (FK)\nProduct Category Key (PK)\nProduct Category Description\nProduct Category Manager\nProduct Category Table\nFigure 20-4: Snowﬂ aked hierarchical relationship in the product dimension.\n Dimension Surrogate Key Assignment\nAfter  you are conﬁ dent you have dimension tables with one row for each true unique \ndimension member, the surrogate keys can be assigned. You maintain a table in the \nETL staging database that matches production keys to surrogate keys; you can use \nthis key map later during fact table processing.\nSurrogate keys are typically assigned as integers, increasing by one for each \nnew key. If the staging area is in an RDBMS, surrogate key assignment is elegantly \naccomplished by creating a sequence. Although syntax varies among the relational \nengines, the process is ﬁ rst to create a sequence and then to populate the key map \ntable.\nHere’s the syntax for the one-time creation of the sequence:\ncreate sequence dim1_seq cache=1000; — choose appropriate cache level\nAnd then here’s the syntax to populate the key map table:\ninsert into dim1_key_map (production_key_id, dim1_key)\nselect production_key_id, dim1_seq.NEXT\nfrom dim1_extract_table;\nDimension Table Loading\nAfter  the dimension data is properly prepared, the load process into the target tables \nis fairly straightforward. Even though the ﬁ rst dimension table is usually small, use \nthe database’s bulk or fast-loading utility or interface. You should use fast-loading \n",
      "content_length": 1795,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 543,
      "content": "ETL System Design and Development Process and Tasks 507\ntechniques for most table inserts. Some databases have extended the SQL syntax to \ninclude a BULK INSERT statement. Others have published an API to load data into \nthe table from a stream.\nThe bulk load utilities and APIs come with a range of parameters and transforma-\ntion capabilities including the following:\n \n■Turn off  logging. Transaction logging adds signiﬁ cant overhead and is not \nvaluable when loading data warehouse tables. The ETL system should be \ndesigned with one or more recoverability points where you can restart pro-\ncessing should something go wrong.\n \n■Bulk load in fast mode. However, most of the database engines’ bulk load \nutilities or APIs require several stringent conditions on the target table to bulk \nload in fast mode. If these conditions are not met, the load should not fail; it \nsimply will not use the “fast” path.\n \n■Presort the ﬁ le. Sorting the ﬁ le in the order of the primary index signiﬁ cantly \nspeeds up indexing.\n \n■Transform with caution. In some cases, the loader supports data conversions, \ncalculations, and string and date/time manipulation. Use these features care-\nfully and test performance. In some cases, these transformations cause the \nloader to switch out of high-speed mode into a line-by-line evaluation of the \nload ﬁ le. We recommend using the ETL tool to perform most transformations.\n \n■Truncate table before full refresh. The TRUNCATE TABLE statement is the most \neffi  cient way to delete all the rows in the table. It’s commonly used to clean out \na table from the staging database at the beginning of the day’s ETL processing.\n Load Type 2 Dimension Table History\nRecall  from Chapter 5: Procurement, that dimension attribute changes are typically \nmanaged as type 1 (overwrite) or type 2 (track history by adding new rows to the \ndimension table). Most dimension tables contain a mixture of type 1 and type 2 \nattributes. More advanced SCD techniques are described in Chapter 5.\nDuring the historic load, you need to re-create history for dimension attributes \nthat are managed as type 2. If business users have identiﬁ ed an attribute as impor-\ntant for tracking history, they want that history going back in time, not just from the \ndate the data warehouse is implemented. It’s usually diffi  cult to re-create dimension \nattribute history, and sometimes it’s completely impossible.\nThis process is not well suited for standard SQL processing. It’s better to use \na database cursor construct or, even better, a procedural language such as Visual \nBasic, C, or Java to perform this work. Most ETL tools enable script processing on \nthe data as it ﬂ ows through the ETL system. \n",
      "content_length": 2707,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 544,
      "content": "Chapter 20\n508\nWhen you’ve completely reconstructed history, make a ﬁ nal pass through the \ndata to set the row end date column. It’s important to ensure there are no gaps in \nthe series. We prefer to set the row end date for the older version of the dimension \nmember to the day before the row eff ective date for the new row if these row dates \nhave a granularity of a full day. If the eff ective and end dates are actually precise \ndate/time stamps accurate to the minute or second, then the end date/time must be \nset to exactly the begin date/time of the next row so that no gap exists between rows.\nPopulate Date and Other Static Dimensions\nEvery  data warehouse database should have a date dimension, usually at the granu-\nlarity of one row for each day. The date dimension should span the history of the \ndata, starting with the oldest fact transaction in the data warehouse. It’s easy to set \nup the date dimension for the historic data because you know the date range of the \nhistoric fact data being loaded. Most projects build the date dimension by hand, \ntypically in a spreadsheet.\nA handful of other dimensions will be created in a similar way. For example, you \nmay create a budget scenario dimension that holds the values Actual and Budget. \nBusiness data governance representatives should sign off  on all constructed dimen-\nsion tables.\nStep 6: Perform the Fact Table Historic Load\nThe one-time historic fact table load diff ers fairly signiﬁ cantly from the ongoing \nincremental processing. The biggest worry during the historic load is the sheer \nvolume of data, sometimes thousands of times bigger than the daily incremental \nload. On the other hand, you have the luxury of loading into a table that’s not in \nproduction. If it takes several days to load the historic data, that’s usually tolerable.\nHistoric Fact Table Extracts\nAs  you identify records that fall within the basic parameters of the extract, make \nsure these records are useful for the data warehouse. Many transaction systems \nkeep operational information in the source system that may not be interesting from \na business point of view.\nIt’s also a good idea to accumulate audit statistics during this step. As the extract \ncreates the results set, it is often possible to capture various subtotals, totals, and \nrow counts.\nAudit Statistics\nDuring  the planning phase for the ETL system, you identiﬁ ed various measures of \ndata quality. These are usually calculations, such as counts and sums, that you com-\npare between the data warehouse and source systems to cross-check the integrity \n",
      "content_length": 2580,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 545,
      "content": "ETL System Design and Development Process and Tasks 509\nof the data. These numbers should tie backward to operational reports and forward \nto the results of the load process in the warehouse. The tie back to the operational \nsystem is important because it is what establishes the credibility of the warehouse.\nNOTE \nThere are scenarios in which it’s diffi  cult or impossible for the warehouse \nto tie back to the source system perfectly. In many cases, the data warehouse extract \nincludes business rules that have not been applied to the source systems. Even \nmore vexing are errors in the source system! Also, diff erences in timing make it \neven more diffi  cult to cross-check the data. If it’s not possible to tie the data back \nexactly, you need to explain the diff erences.\n Fact Table Transformations\nIn most projects, the fact data is relatively clean. The ETL system developer spends \na lot of time improving the dimension table content, but the facts usually require \na fairly modest transformation. This makes sense because in most cases the facts \ncome from transaction systems used to operate the organization.\nThe most common transformations to fact data include transformation of null \nvalues, pivoting or unpivoting the data, and precomputing derived calculations. All \nfact rows then enter the surrogate key pipeline to exchange the natural keys for the \ndimension surrogate keys managed in the ETL system.\nNull Fact Values\nAll  major database engines explicitly support a null value. In many source systems, \nhowever, the null value is represented by a special value of what should be a legitimate \nfact. Perhaps the special value of –1 is understood to represent null. For most fact \ntable metrics, the “–1” in this scenario should be replaced with a true NULL. A null \nvalue for a numeric measure is reasonable and common in the fact table. Nulls do \nthe “right thing” in calculations of sums and averages across fact table rows. It’s only \nin the dimension tables that you should strive to replace null values with specially \ncrafted default values. Finally, you should not allow any null values in the fact table \ncolumns that reference the dimension table keys. These foreign key columns should \nalways be deﬁ ned as NOT NULL.\nImprove Fact Table Content\nAs we have stressed, all the facts in the ﬁ nal fact table row must be expressed in \nthe same grain. This means there must be no facts representing totals for the year \nin a daily fact table or totals for some geography larger than the fact table’s grain. \nIf the extract includes an interleaving of facts at diff erent grains, the transforma-\ntion process must eliminate these aggregations, or move them into the appropriate \naggregate tables.\n",
      "content_length": 2724,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 546,
      "content": "Chapter 20\n510\nThe fact row may contain derived facts; although, in many cases it is more effi  cient \nto calculate derived facts in a view or an online analytical processing (OLAP) cube \nrather than in the physical table. For instance, a fact row that contains revenues and \ncosts may want a fact representing net proﬁ t. It is very important that the net proﬁ t \nvalue be correctly calculated every time a user accesses it. If the data warehouse \nforces all users to access the data through a view, it would be ﬁ ne to calculate the \nnet proﬁ t in that view. If users are allowed to see the physical table, or if they often \nﬁ lter on net proﬁ t and thus you’d want to index it, precomputing it and storing it \nphysically is preferable.\nSimilarly, if some facts need to be simultaneously presented with multiple units \nof measure, the same logic applies. If business users access the data through a view \nor OLAP database, then the various versions of the facts can effi  ciently be calculated \nat access time.\n Pipeline the Dimension Surrogate Key Lookup\nIt  is important that referential integrity (RI) is maintained between the fact table \nand dimension tables; you must never have a fact row that references a dimension \nmember that doesn’t exist. Therefore, you should not have a null value for any \nforeign key in the fact table nor should any fact row violate referential integrity to \nany dimension.\nThe surrogate key pipeline is the ﬁ nal operation before you load data into the \ntarget fact table. All other data cleaning, transformation, and processing should \nbe complete. The incoming fact data should look just like the target fact table in \nthe dimensional model, except it still contains the natural keys from the source \nsystem rather than the warehouse’s surrogate keys. The surrogate key pipeline is \nthe process that exchanges the natural keys for the surrogate keys and handles any \nreferential integrity errors.\nDimension table processing must complete before the fact data enters the sur-\nrogate key pipeline. Any new dimension members or type 2 changes to existing \ndimension members must have already been processed, so their keys are available \nto the surrogate key pipeline. \nFirst let’s discuss the referential integrity problem. It’s a simple matter to con-\nﬁ rm that each natural key in the historic fact data is represented in the dimension \ntables. This is a manual step. The historic load is paused at this point, so you can \ninvestigate and ﬁ x any referential integrity problems before proceeding. The dimen-\nsion table is either ﬁ xed, or the fact table extract is redesigned to ﬁ lter out spurious \nrows, as appropriate.\nNow that you’re conﬁ dent there will be no referential integrity violations, you can \ndesign the historic surrogate key pipeline, as shown in Figure 19-11 in the previous \nchapter. In this scenario, you need to include BETWEEN logic on any dimension \n",
      "content_length": 2911,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 547,
      "content": "ETL System Design and Development Process and Tasks 511\nwith type 2 changes to locate the dimension row that was in eff ect when the histori-\ncal fact measurement occurred.\nThere are several approaches for designing the historic load’s surrogate key pipe-\nline for best performance; the design depends on the features available in your ETL \ntool, the data volumes you’re processing, and your dimensional design. In theory, \nyou could deﬁ ne a query that joins the fact staging table and each dimension table \non the natural keys, returning the facts and surrogate keys from each dimension \ntable. If the historic data volumes are not huge, this can actually work quite well, \nassuming you staged the fact data in the relational database and indexed the dimen-\nsion tables to support this big query. This approach has several beneﬁ ts:\n \n■It leverages the power of the relational database.\n \n■It performs the surrogate key lookups on all dimensions in parallel.\n \n■It simpliﬁ es the problem of picking up the correct dimension key for type 2 \ndimensions. The join to type 2 dimensions must include a clause specifying \nthat the transaction date falls between the row eff ective date and row end date \nfor that image of the dimension member in the table.\nNo one would be eager to try this approach if the historic fact data volumes \nwere large in the hundreds of gigabytes to terabyte range. The complex join to \nthe type 2 dimension tables create the greatest demands on the system. Many \ndimensional designs include a fairly large number of (usually small) dimension \ntables that are fully type 1, and a smaller number of dimensions containing type \n2 attributes. You could use this relational technique to perform the surrogate key \nlookups for all the type 1 dimensions in one pass and then separately handle the \ntype 2 dimensions. You should ensure the eff ective date and end date columns \nare properly indexed.\nAn alternative to the database join technique described is to use the ETL tool’s \nlookup operator.\nWhen all the fact source keys have been replaced with surrogate keys, the fact \nrow is ready to load. The keys in the fact table row have been chosen to be proper \nforeign keys to the respective dimension tables, and the fact table is guaranteed to \nhave referential integrity with respect to the dimension tables.\n Assign Audit Dimension Key\nFact  tables often include an audit key on each fact row. The audit key points to an \naudit dimension that describes the characteristics of the load, including relatively \nstatic environment variables and measures of data quality. The audit dimension can \nbe quite small. An initial design of the audit dimension might have just two envi-\nronment variables (master ETL version number and proﬁ t allocation logic number), \nand only one quality indicator whose values are Quality Checks Passed and Quality \nProblems Encountered. Over time, these variables and diagnostic indicators can be \n",
      "content_length": 2947,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 548,
      "content": "Chapter 20\n512\nmade more detailed and more sophisticated. The audit dimension key is added to the \nfact table either immediately after or immediately before the surrogate key pipeline. \nFact Table Loading\nThe  main concern when loading the fact table is load performance. Some database \ntechnologies support fast loading with a speciﬁ ed batch size. Look at the docu-\nmentation for the fast-loading technology to see how to set this parameter. You \ncan experiment to ﬁ nd the ideal batch size for the size of the rows and the server’s \nmemory conﬁ guration. Most people don’t bother to get so precise and simply choose \na number like 10,000 or 100,000 or 1 million.\nAside from using the bulk loader and a reasonable batch size (if appropriate for \nthe database engine), the best way to improve the performance of the historic load \nis to load into a partitioned table, ideally loading multiple partitions in parallel. The \nsteps to loading into a partitioned table include:\n \n1. Disable foreign key (referential integrity) constraints between the fact table \nand each dimension table before loading data.\n 2. Drop or disable indexes on the fact table.\n \n3. Load the data using fast-loading techniques.\n \n4. Create or enable fact table indexes.\n \n5. If necessary, perform steps to stitch together the table’s partitions.\n \n6. Confirm each dimension table has a unique index on the surrogate \nkey column.\n \n7. Enable foreign key constraints between the fact table and dimension tables.\nDevelop Incremental ETL Processing\nOne of the biggest challenges with the incremental ETL process is identifying new, \nchanged, and deleted rows. After you have a stream of inserts, modiﬁ cations, and \ndeletions, the ETL system can apply transformations following virtually identical \nbusiness rules as for the historic data loads. \nThe historic load for dimensions and facts consisted largely or entirely of inserts. \nIn incremental processing, you primarily perform inserts, but updates for dimen-\nsions and some kinds of fact tables are inevitable. Updates and deletes are expensive \noperations in the data warehouse environment, so we’ll describe techniques to \nimprove the performance of these tasks.\nStep 7: Dimension Table Incremental Processing\nAs  you might expect, the incremental ETL system development begins with the \ndimension tables. Dimension incremental processing is very similar to the historic \nprocessing previously described. \n",
      "content_length": 2433,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 549,
      "content": "ETL System Design and Development Process and Tasks 513\nDimension Table Extracts\nIn  many cases, there is a customer master ﬁ le or product master ﬁ le that can serve \nas the single source for a dimension. In other cases, the raw source data is a mixture \nof dimensional and fact data.\nOften it’s easiest to pull the current snapshots of the dimension tables in their \nentirety and let the transformation step determine what has changed and how \nto handle it. If the dimension tables are large, you may need to use the fact table \ntechnique described in the section “Step 8: Fact Table Incremental Processing” for \nidentifying the changed record set. It can take a long time to look up each entry in \na large dimension table, even if it hasn’t changed from the existing entry.\nIf possible, construct the extract to pull only rows that have changed. This is \nparticularly easy and valuable if the source system maintains an indicator of the \ntype of change.\nIdentify New and Changed Dimension Rows\nThe  DW/BI team may not be successful in pushing the responsibility for identify-\ning new, updated, and deleted rows to the source system owners. In this case, the \nETL process needs to perform an expensive comparison operation to identify new \nand changed rows.\nWhen the incoming data is clean, it’s easy to ﬁ nd new dimension rows. The raw \ndata has an operational natural key, which must be matched to the same column in \nthe current dimension row. Remember, the natural key in the dimension table is an \nordinary dimensional attribute and is not the dimension’s surrogate primary key.\nYou can ﬁ nd new dimension members by performing a lookup from the incoming \nstream to the master dimension, comparing on the natural key. Any rows that fail \nthe lookup are new dimension members and should be inserted into the dimen-\nsion table.\nIf the dimension contains any type 2 attributes, set the row eff ective date column \nto the date the dimension member appeared in the system; this is usually yesterday \nif you are processing nightly. Set the row end date column to the default value for \ncurrent rows. This should be the largest date, very far in the future, supported by \nthe system. You should avoid using a null value in this second date column because \nrelational databases may generate an error or return the special value Unknown if \nyou attempt to compare a speciﬁ c value to a NULL.\nThe next step is to determine if the incoming dimension row has changed. The \nsimplest technique is to compare column by column between the incoming data and \nthe current corresponding member stored in the master dimension table.\nIf the dimension is large, with more than a million rows, the simple technique \nof column-wise comparison may be too slow, especially if there are many columns \n",
      "content_length": 2781,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 550,
      "content": "Chapter 20\n514\nin the dimension table. A popular alternative method is to use a hash or checksum \nfunction to speed the comparison process. You can add two new housekeeping \ncolumns to the dimension table: hash type1 and hash type2. You should place \na hash of a concatenation of the type 1 attributes in the hash type1 column and \nsimilarly for hash type2. Hashing algorithms convert a very long string into a \nmuch shorter string that is close to unique. The hashes are computed and stored \nin the dimension table. Then compute hashes on the incoming rowset in exactly \nthe same way, and compare them to the stored values. The comparison on a single, \nrelatively short string column is far more effi  cient than the pair-wise comparison \non dozens of separate columns. Alternatively, the relational database engine may \nhave syntax such as EXCEPT that enables a high-performance query to ﬁ nd the \nchanged rows. \nAs a general rule, you do not delete dimension rows that have been deleted in \nthe source system because these dimension members probably still have fact table \ndata associated with them in the data warehouse.\nProcess Changes to Dimension Attributes\nThe ETL application contains business rules to determine how to handle an attri-\nbute value that has changed from the value already stored in the data warehouse. \nIf the revised description is determined to be a legitimate and reliable update to \nprevious information, then the techniques of slowly changing dimensions must \nbe used.\nThe ﬁ rst step in preparing a dimension row is to decide if you already have \nthat row. If all the incoming dimensional information matches the correspond-\ning row in the dimension table, no further action is required. If the dimensional \ninformation has changed, then you can apply changes to the dimension, such as \ntype 1  or type 2. \nNOTE \nYou may recall from Chapter 5 that there are three primary methods for \ntracking changes in attribute values, as well as a set of advanced hybrid techniques. \nType 3 requires a change in the structure of the dimension table, creating a new \nset of columns to hold the “previous” versus “current” versions of the attributes. \nThis type of structural change is seldom automated in the ETL system; it’s more \nlikely to be handled as a one-time change in the data model.\nThe lookup and key assignment logic for handling a changed dimension record \nduring the extract process is shown in Figure 20-5. In this case, the logic ﬂ ow does \nnot assume the incoming data stream is limited only to new or changed rows.\n",
      "content_length": 2550,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 551,
      "content": "ETL System Design and Development Process and Tasks 515\nAdd to\ndimension\nEnd processing\nof row\nRow new?\nYES\nYES\nYES\nYES\nNO\nNO\nRow has Type\n2 changes?\nRow has Type\n1 changes?\nRow has any\nchanges?\nUpdate existing “Current”\nrow: Set Row_End_Date and\nIs_Row_Current\nAdd new dimension row for\nthis entity, with a new\nsurrogate key. Set\nRow_Start_Date = yesterday\nand Is_Row_Current = True\nUpdate Type 1\nattributes–usually\nall existing rows\nfor this entity\nFigure 20-5: Logic ﬂ ow for handling dimension updates.\nStep 8: Fact Table Incremental Processing\nMost  data warehouse databases are too large to entirely replace the fact tables in a \nsingle load window. Instead, new and updated fact rows are incrementally processed.\nNOTE \nIt is much more effi  cient to incrementally load only the records that \nhave been added or updated since the previous load. This is especially true in a \njournal-style system where history is never changed and only adjustments in the \ncurrent period are allowed.\nThe ETL process for fact table incremental processing diff ers from the historic \nload. The historic ETL process doesn’t need to be fully automated; you can stop the \n",
      "content_length": 1157,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 552,
      "content": "Chapter 20\n516\nprocess to examine the data and prepare for the next step. The incremental process-\ning, by contrast, must be fully automated.\nFact Table Extract and Data Quality Checkpoint\nAs  soon as the new and changed fact rows are extracted from the source system, a \ncopy of the untransformed data should be written to the staging area. At the same \ntime, measures of data quality on the raw extracted data are computed. The staged \ndata serves three purposes:\n \n■Archive for auditability\n \n■Provide a starting point after data quality veriﬁ cation\n \n■Provide a starting point for restarting the process\nFact Table Transformations and Surrogate Key Pipeline\nThe surrogate key pipeline for the incremental fact data is similar to that for the \nhistoric data. The key diff erence is that the error handling for referential integrity \nviolations must be automated. There are several methods for handling referential \nintegrity violations:\n \n■Halt the load. This is seldom a useful solution; although, it’s often the default \nin many ETL tools.\n \n■Throw away error rows. There are situations in which a missing dimen-\nsion value is a signal that the data is irrelevant to the business requirements \nunderlying the data warehouse.\n \n■Write error rows to a ﬁ le or table for later analysis. Design a mechanism \nfor moving corrected rows into a suspense ﬁ le. This approach is not a good \nchoice for a ﬁ nancial system, where it is vital that all rows be loaded.\n \n■Fix error rows by creating a dummy dimension row and returning its sur-\nrogate key to the pipeline.  The most attractive error handling for referential \nintegrity violations in the incremental surrogate key pipeline is to create a \ndummy dimension row on-the-ﬂ y for the unknown natural key. The natural \nkey is the only piece of information that you may have about the dimension \nmember; all the other attributes must be set to default values. This dummy \ndimension row will be corrected with type 1 updates when the detailed infor-\nmation about that dimension member becomes available.\n \n■Fix error rows by mapping to a single unknown member in each dimen-\nsion. This approach is not recommended. The problem is that all error rows \nare mapped to the same dimension member, for any unknown natural key \nvalues in the fact table extract.\n",
      "content_length": 2303,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 553,
      "content": "ETL System Design and Development Process and Tasks 517\nFor most systems, you perform the surrogate key lookups against a query, view, \nor physical table that subsets the dimension table. The dimension table rows are \nﬁ ltered, so the lookup works against only the current version of each dimension \nmember.\nLate Arriving Facts and the Surrogate Key Pipeline\nIn  most data warehouses, the incremental load process begins soon after midnight \nand processes all the transactions that occurred the previous day. However, there \nare scenarios in which some facts arrive late. This is most likely to happen when \nthe data sources are distributed across multiple machines or even worldwide, and \nconnectivity or latency problems prevent timely data collection.\nIf all the dimensions are managed completely as type 1 overwrites, late arriving \nfacts present no special challenges. But most systems have a mixture of type 1 and \ntype 2 attributes. The late arriving facts must be associated with the version of the \ndimension member that was in eff ect when the fact occurred. That requires a lookup \nin the dimension table using the row begin and end eff ective dates.\nIncremental Fact Table Load\nIn  the historic fact load, it’s important that data loads use fast-load techniques. In \nmost data warehouses, these fast-load techniques may not be available for the incre-\nmental load. The fast-load technologies often require stringent conditions on the \ntarget table (for example, empty or unindexed). For the incremental load, it’s usually \nfaster to use non-fast-load techniques than to fully populate or index the table. For \nsmall to medium systems, insert performance is usually adequate.\nIf your fact table is very large, you should already have partitioned the fact table \nfor manageability reasons. If incremental data is always loading into an empty \npartition, you should use fast-load techniques. With daily loads, you would create \n365 new fact table partitions each year. This is probably too many partitions for a \nfact table with long history, so consider implementing a process to consolidate daily \npartitions into weekly or monthly partitions.\nLoad Snapshot Fact Tables\nThe largest fact tables are usually transactional. Transaction fact tables are typically \nloaded only through inserts. Periodic snapshot fact tables are usually loaded at \nmonth end. Data for the current month is sometimes updated each day for current-\nmonth-to-date. In this scenario, monthly partitioning of the fact table makes it easy \nto reload the current month with excellent performance.\nAccumulating snapshot fact tables monitor relatively short-lived processes, such \nas ﬁ lling an order. The accumulating snapshot fact table is characterized by many \n",
      "content_length": 2743,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 554,
      "content": "Chapter 20\n518\nupdates for each fact row over the life of the process. This table is expensive to \nmaintain; although accumulating snapshots are almost always much smaller than \nthe other two types of fact tables.\nSpeed Up the Load Cycle\nProcessing only data that has been changed is one way to speed up the ETL cycle. \nThis section lists several additional techniques.\nMore Frequent Loading\nAlthough it is a huge leap to move from a monthly or weekly process to a nightly one, \nit is an eff ective way to shorten the load window. Every nightly process involves 1/30 \nthe data volume of a monthly one. Most data warehouses are on a nightly load cycle.\nIf nightly processing is too expensive, consider performing some preprocessing \non the data throughout the day. During the day, data is moved into a staging database \nor operational data store where data cleansing tasks are performed. After midnight, \nyou can consolidate multiple changes to dimension members, perform ﬁ nal data \nquality checks, assign surrogate keys, and move the data into the data warehouse.\nParallel Processing\nAnother  way to shorten the load time is to parallelize the ETL process. This can \nhappen in two ways: multiple steps running in parallel and a single step running \nin parallel.\n \n■Multiple load steps. The ETL job stream is divided into several independent \njobs submitted together. You need to think carefully about what goes into \neach job; the primary goal is to create independent jobs.\n \n■Parallel execution. The database itself can also identify certain tasks it can \nexecute in parallel. For example, creating an index can typically be parallel-\nized across as many processors as are available on the machine.\nNOTE \nThere are good ways and bad ways to break processing into parallel steps. \nOne simple way to parallelize is to extract all source data together, then load and \ntransform the dimensions, and then simultaneously check referential integrity \nbetween the fact table and all dimensions. Unfortunately, this approach is likely \nto be no faster—and possibly much slower—than the even simpler sequential \napproach because each step launches parallel processes that compete for the same \nsystem resources such as network bandwidth, I/O, and memory. To structure \nparallel jobs well, you need to account not just for logically sequential steps but \nalso for system resources.\n",
      "content_length": 2374,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 555,
      "content": "ETL System Design and Development Process and Tasks 519\nParallel Structures\nYou  can set up a three-way mirror or clustered conﬁ guration on two servers to \nmaintain a continuous load data warehouse, with one server managing the loads \nand the second handling the queries. The maintenance window is reduced to a \nfew minutes daily to swap the disks attached to each server. This is a great way to \nprovide high system availability.\nDepending on the requirements and available budget, there are several similar \ntechniques you can implement for tables, partitions, and databases. For example, you \ncan load into an offl  ine partition or table, and swap it into active duty with minimum \ndowntime. Other systems have two versions of the data warehouse database, one for \nloading and one for querying. These are less eff ective, but less expensive, versions \nof the functionality provided by clustered servers.\n Step 9: Aggregate Table and OLAP Loads\nAn  aggregate table is logically easy to build. It’s simply the results of a really big \naggregate query stored as a table. The problem with building aggregate tables from \na query on the fact table, of course, occurs when the fact table is just too big to \nprocess within the load window.\nIf the aggregate table includes an aggregation along the date dimension, perhaps \nto monthly grain, the aggregate maintenance process is more complex. The cur-\nrent month of data must be updated, or dropped and re-created, to incorporate the \ncurrent day’s data.\nA similar problem occurs if the aggregate table is deﬁ ned on a dimension attribute \nthat is overwritten as a type 1. Any type 1 change in a dimension attribute aff ects \nall fact table aggregates and OLAP cubes that are deﬁ ned on that attribute. An ETL \nprocess must “back out” the facts from the old aggregate level and move them to \nthe new one.\nIt is extremely important that the aggregate management system keep aggrega-\ntions in sync with the underlying fact data. You do not want to create a system that \nreturns a diff erent result set if the query is directed to the underlying detail facts \nor to a precomputed  aggregation.\nStep 10: ETL System Operation and Automation\nThe ideal ETL operation runs the regular load processes in a lights-out manner, \nwithout human intervention. Although this is a diffi  cult outcome to attain, it is \npossible to get close.\n",
      "content_length": 2372,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 556,
      "content": "Chapter 20\n520\nSchedule Jobs\nScheduling  jobs is usually straightforward. The ETL tool should contain function-\nality to schedule a job to kick off  at a certain time. Most ETL tools also contain \nfunctionality to conditionally execute a second task if the ﬁ rst task successfully \ncompleted. It’s common to set up an ETL job stream to launch at a certain time, and \nthen query a database or ﬁ lesystem to see if an event has occurred.\nYou can also write a script to perform this kind of job control. Every ETL tool \nhas a way to invoke a job from the operating system command line. Many orga-\nnizations are very comfortable using scripting languages, such as Perl, to manage \ntheir job schedules.\nAutomatically Handle Predictable Exceptions and Errors \nAlthough  it’s easy enough to launch jobs, it’s a harder task to make sure they run to \ncompletion, gracefully handling data errors and exceptions. Comprehensive error \nhandling is something that needs to be built into the ETL jobs from the outset.\n Gracefully Handle Unpredictable Errors \nSome errors are predictable, such as receiving an early arriving fact or a NULL value \nin a column that’s supposed to be populated. For these errors, you can generally \ndesign your ETL system to ﬁ x the data and continue processing. Other errors are \ncompletely unforeseen and range from receiving data that’s garbled to experiencing \na power outage during processing.\nWe look for ETL tool features and system design practices to help recover from \nthe unexpected. We generally recommend outﬁ tting fact tables with a single column \nsurrogate key that is assigned sequentially to new records that are being loaded. If \na large load job unexpectedly halts, the fact table surrogate key allows the load to \nresume from a reliable point, or back out the load by constraining on a contiguous \nrange of the surrogate  keys.\n Real-Time Implications\nReal-time  processing is an increasingly common requirement in data warehousing. \nThere is a strong possibility that your DW/BI system will have a real-time require-\nment. Some business users expect the data warehouse to be continuously updated \nthroughout the day and grow impatient with stale data. Building a real-time \nDW/BI system requires gathering a very precise understanding of the true business \nrequirements for real-time data and identifying an appropriate ETL architecture, \nincorporating a variety of technologies married with a solid platform.\n",
      "content_length": 2446,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 557,
      "content": "ETL System Design and Development Process and Tasks 521\nReal-Time Triage\nAsking business users if they want “real-time” delivery of data is a frustrating \nexercise for the DW/BI team. Faced with no constraints, most users will say, “That \nsounds good; go for it!” This kind of response is almost worthless.\nTo avoid this situation, we recommend dividing the real-time design challenge \ninto three categories, called instantaneous, intra-day, and daily. We use these terms \nwhen we talk to business users about their needs and then design our data delivery \npipelines diff erently for each option. Figure 20-6 summarizes the issues that arise \nas data is delivered faster.\nDaily\nBatch processing ETL\nMicro-batch ETL\nStreaming EII/ETL\nDrive user presentation from\nsource application\nSeparate from fact table\nProbe with queries or subscribe\nto message bus\nDaily hot fact table time partition\nProvisional\nProvisional\nIndividual transactions\nTransaction fragments\nWait for file ready\nConventional file table time partition\nReconciled\nComplete transaction set\nColumn screens\nColumn screens\nColumn screens\nStructure screens\nStructure screens\n--\n--\n--\nBusiness rule screens\nFinal results\nResults updated, corrected nightly\nResults updated, possibly\nrepudiated nightly\nIntra-Day\nInstantaneous\nFigure 20-6: Data quality trade-offs with low latency delivery.\nInstantaneous means the data visible on the screen represents the true state of \nthe source transaction system at every instant. When the source system status \nchanges, the screen instantly and synchronously responds. An instantaneous real-\ntime system is usually implemented as an enterprise information integration (EII) \nsolution, where the source system itself is responsible for supporting the update of \nremote users’ screens and servicing query requests. Obviously, such a system must \nlimit the complexity of the query requests because all the processing is done on the \nsource system. EII solutions typically involve no caching of data in the ETL pipeline \nbecause EII solutions by deﬁ nition have no delays between the source systems and \nthe users’ screens. Some situations are plausible candidates for an instantaneous \nreal-time solution. Inventory status tracking may be a good example, where the deci-\nsion maker has the right to commit available inventory to a customer in real time.\nIntra-day means the data visible on the screen is updated many times per day but \nis not guaranteed to be the absolute current truth. Most of us are familiar with stock \nmarket quote data that is current to within 15 minutes but is not instantaneous. \n",
      "content_length": 2600,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 558,
      "content": "Chapter 20\n522\nThe technology for delivering frequent real-time data (as well as the slower daily \ndata) is distinctly diff erent from instantaneous real-time delivery. Frequently deliv-\nered data is usually processed as micro-batches in a conventional ETL architecture. \nThis means the data undergoes the full gamut of change data capture, extract, stag-\ning to ﬁ le storage in the ETL back room of the data warehouse, cleaning and error \nchecking, conforming to enterprise data standards, assigning of surrogate keys, \nand possibly a host of other transformations to make the data ready to load into the \npresentation server. Almost all these steps must be omitted or drastically reduced \nin an EII solution. The big diff erence between intra-day and daily delivered data is \nin the ﬁ rst two steps: change data capture and extract. To capture data many times \nper day from the source system, the data warehouse usually must tap into a high \nbandwidth communications channel, such as message queue traffi  c between legacy \napplications, an accumulating transaction log ﬁ le, or low level database triggers \ncoming from the transaction system every time something happens.\nDaily means the data visible on the screen is valid as of a batch ﬁ le download or \nreconciliation from the source system at the end of the previous working day. There \nis a lot to recommend daily data. Quite often processes are run on the source system \nat the end of the working day that correct the raw data. When this reconciliation \nbecomes available, that signals the ETL system to perform a reliable and stable \ndownload of the data. If you have this situation, you should explain to the busi-\nness users what compromises they will experience if they demand instantaneous \nor intra-day updated data. Daily updated data usually involves reading a batch ﬁ le \nprepared by the source system or performing an extract query when a source system \nreadiness ﬂ ag is set. This, of course, is the simplest extract scenario because you \nwait for the source system to be ready and available.\nReal-Time Architecture Trade-Offs\nResponding  to real-time requirements means you need to change the DW/BI archi-\ntecture to get data to the business users’ screens faster. The architectural choices \ninvolve trade-off s that aff ect data quality and administration.\nYou can assume the overall goals for ETL system owners are not changed or com-\npromised by moving to real-time delivery. You can remain just as committed to data \nquality, integration, security, compliance, backup, recovery, and archiving as you \nwere before starting to design a real-time system. If you agree with this statement, \nthen read the following very carefully! The following sections discuss the typical \ntrade-off s that occur as you implement a more real-time architecture:\nReplace Batch Files\nConsider replacing a batch ﬁ le extract with reading from a message queue or trans-\naction log ﬁ le. A batch ﬁ le delivered from the source system may represent a clean \n",
      "content_length": 3007,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 559,
      "content": "ETL System Design and Development Process and Tasks 523\nand consistent view of the source data. The batch ﬁ le may contain only those records \nresulting from completed transactions. Foreign keys in the batch ﬁ les are prob-\nably resolved, such as when the ﬁ le contains an order from a new customer whose \ncomplete identity may be delivered with the batch ﬁ le. Message queue and log ﬁ le \ndata, on the other hand, is raw instantaneous data that may not be subject to any \ncorrective process or business rule enforcement in the source system. In the worst \ncase, this raw data may 1) be incorrect or incomplete because additional transac-\ntions may arrive later; 2) contain unresolved foreign keys that the DW/BI system \nhas not yet processed; and 3) require a parallel batch-oriented ETL data ﬂ ow to cor-\nrect or even replace the hot real-time data each 24 hours. And if the source system \nsubsequently applies complex business rules to the input transactions ﬁ rst seen in \nthe message queues or the log ﬁ les, then you really don’t want to recapitulate these \nbusiness rules in the ETL system!\nLimit Data Quality Screens\nConsider restricting data quality screening only to column screens and simple \ndecode lookups. As the time to process data moving through the ETL pipeline is \nreduced, it may be necessary to eliminate more costly data quality screening, espe-\ncially structure screens and business rule screens. Remember that column screens \ninvolve single ﬁ eld tests and/or simple lookups to replace or expand known values. \nEven in the most aggressive real-time applications, most column screens should \nsurvive. But structure screens and business rule screens by deﬁ nition require mul-\ntiple ﬁ elds, multiple records, and possibly multiple tables. You may not have time to \npass an address block of ﬁ elds to an address analyzer. You may not check referential \nintegrity between tables. You may not be able to perform a remote credit check \nthrough a web service. All this may require informing the users of the provisional \nand potentially unreliable state of the raw real-time data and may require that you \nimplement a parallel, batch-oriented ETL pipeline that overwrites the real-time data \nperiodically with properly checked data.\n Post Facts with Dimensions\nYou should allow early arriving facts to be posted with old copies of dimensions. In \nthe real-time world, it is common to receive transaction events before the context \n(such as the identity of the customer) of those transactions is updated. In other \nwords, the facts arrive before the dimensions. If the real-time system cannot wait \nfor the dimensions to be resolved, then old copies of the dimensions must be used if \nthey are available, or generic empty versions of the dimensions must be used other-\nwise. If and when revised versions of the dimensions are received, the data warehouse \nmay decide to post those into the hot partition or delay updating the dimension until \na batch process takes over, possibly at the end of the day. In any case, the users need \n",
      "content_length": 3048,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 560,
      "content": "Chapter 20\n524\nto understand there may be an ephemeral window of time where the dimensions \ndon’t exactly describe the facts.\nEliminate Data Staging\nSome real-time architectures, especially EII systems, stream data directly from the \nproduction source system to the users’ screens without writing the data to perma-\nnent storage in the ETL pipeline. If this kind of system is part of the DW/BI team’s \nresponsibility, then the team should have a serious talk with senior management \nabout whether backup, recovery, archiving, and compliance responsibilities can be \nmet, or whether those responsibilities are now the sole concern of the production \nsource system.\nReal-Time Partitions in the Presentation Server\nTo  support real-time requirements, the data warehouse must seamlessly extend its \nexisting historical time series right up to the current instant. If the customer has \nplaced an order in the last hour, you need to see this order in the context of the \nentire customer relationship. Furthermore, you need to track the hourly status of \nthis most current order as it changes during the day. Even though the gap between \nthe production transaction processing systems and the DW/BI system has shrunk \nin most cases to 24 hours, the insatiable needs of your business users require the \ndata warehouse to ﬁ ll this gap with real-time data.\nOne design solution for responding to this crunch is building a real-time parti-\ntion as an extension of the conventional, static data warehouse. To achieve real-time \nreporting, a special partition is built that is physically and administratively separated \nfrom the conventional data warehouse tables. Ideally, the real-time partition is a true \ndatabase partition where the fact table in question is partitioned by activity date.\nIn either case, the real-time partition ideally should meet the following tough \nset of requirements:\n \n■Contain all the activity that has occurred since the last update of the static \ndata warehouse.\n \n■Link as seamlessly as possible to the grain and content of the static data ware-\nhouse fact tables, ideally as a true physical partition of the fact table.\n \n■Be indexed so lightly that incoming data can continuously be “dribbled in.” \nIdeally, the real-time partition is completely unindexed; however, this may \nnot be possible in certain RDBMSs where indexes have been built that are not \nlogically aligned with the partitioning scheme.\n \n■Support highly responsive queries even in the absence of indexes by pinning \nthe real-time partition in memory.\n",
      "content_length": 2538,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 561,
      "content": "ETL System Design and Development Process and Tasks 525\nThe real-time partition can be used eff ectively with both transaction and periodic \nsnapshot fact tables. We have not found this approach needed with accumulating \nsnapshot fact tables.\nTransaction Real-Time Partition\nIf the static data warehouse fact table has a transaction grain, it contains exactly \none row for each individual transaction in the source system from the beginning \nof “recorded history.” The real-time partition has exactly the same dimensional \nstructure as its underlying static fact table. It contains only the transactions that \nhave occurred since midnight when you last loaded the regular fact tables. The real-\ntime partition may be completely unindexed, both because you need to maintain a \ncontinuously open window for loading and because there is no time series because \nonly today’s data is kept in this table.\nIn a relatively large retail environment experiencing 10 million transactions \nper day, the static fact table would be pretty big. Assuming each transaction grain \nrow is 40 bytes wide (seven dimensions plus three facts, all packed into 4-byte col-\numns), you accumulate 400 MB of data each day. Over a year, this would amount to \napproximately 150 GB of raw data. Such a fact table would be heavily indexed and \nsupported by aggregates. But the daily real-time slice of 400 MB should be pinned \nin memory. The real-time partition can remain biased toward very fast-loading \nperformance but at the same time provide speedy query performance.\nPeriodic Snapshot Real-Time Partition\nIf the static data warehouse fact table has a periodic grain (say, monthly), then the \nreal-time partition can be viewed as the current hot rolling month. Suppose you \nare a big retail bank with 15 million accounts. The static fact table has the grain of \naccount by month. A 36-month time series would result in 540 million fact table \nrows. Again, this table would be extensively indexed and supported by aggregates \nto provide query good performance. The real-time partition, on the other hand, is \njust an image of the current developing month, updated continuously as the month \nprogresses. Semi-additive balances and fully additive facts are adjusted as frequently \nas they are reported. In a retail bank, the supertype fact table spanning all account \ntypes is likely to be quite narrow, with perhaps four dimensions and four facts, \nresulting in a real-time partition of 480 MB. The real-time partition again can be \npinned in memory.\nOn the last day of the month, the periodic real-time partition can, with luck, \njust be merged onto the less volatile fact table as the most current month, and the \nprocess can start again with an empty real-time partition.\n",
      "content_length": 2746,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 562,
      "content": "Chapter 20\n526\nSummary\nThe previous chapter introduced 34 subsystems that are possible within a compre-\nhensive ETL implementation. In this chapter, we provided detailed practical advice \nfor actually building and deploying the ETL system. Perhaps the most interesting \nperspective is to separate the initial historical loads from the ongoing incremental \nloads. These processes are quite diff erent.\nIn general we recommend using a commercial ETL tool as opposed to maintaining \na library of scripts, even though the ETL tools can be expensive and have a signiﬁ -\ncant learning curve. ETL systems, more than any other part of the DW/BI ediﬁ ce, \nare legacy systems that need to be maintainable and scalable over long periods of \ntime and over changes of personnel.\nWe concluded this chapter with some design perspectives for real-time (low \nlatency) deli very of data. Not only are the real-time architectures diff erent from \nconventional batch processing, but data quality is compromised as the latency is \nprogressively lowered. Business users need to be thoughtful participants in this \ndesign trade-off .\n",
      "content_length": 1111,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 563,
      "content": "Big Data Analytics\nI\nn this chapter, we introduce big data in all its glory and show how it expands \nthe mission of the DW/BI system. We conclude with a comprehensive list of big \ndata best practices.\nChapter 21 discusses the following concepts:\n \n■Comparison of two architectural approaches for tackling big data analytics\n \n■Management, architecture, modeling, and governance best practices for deal-\ning with big data\nBig Data Overview\nWhat  is big data? Its bigness is actually not the most interesting characteristic. Big data \nis structured, semistructured, unstructured, and raw data in many diff erent formats, in \nsome cases looking totally diff erent than the clean scalar numbers and text you have \nstored in your data warehouses for the last 30 years. Much big data cannot be analyzed \nwith anything that looks like SQL. But most important, big data is a paradigm shift \nin how you think about data assets, where you collect them, how you analyze them, \nand how you monetize the insights from the analysis.\nThe big data movement has gathered momentum as a large number of use cases \nhave been recognized that fall into the category of big data analytics. These use \ncases include:\n \n■Search ranking\n \n■Ad tracking\n \n■Location and proximity tracking\n \n■Causal factor discovery\n \n■Social CRM\n \n■Document similarity testing\n21\n",
      "content_length": 1336,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 564,
      "content": "Chapter 21\n528\n \n■Genomics analysis\n \n■Cohort group discovery\n \n■In-ﬂ ight aircraft status\n \n■Smart utility meters\n \n■Building sensors\n \n■Satellite image comparison\n \n■CAT scan comparison\n \n■Financial account fraud detection and intervention\n \n■Computer system hacking detection and intervention\n \n■Online game gesture tracking\n \n■Big science data analysis\n \n■Generic name-value pair analysis\n \n■Loan risk analysis and insurance policy underwriting\n \n■Customer churn analysis\nGiven the breadth of potential use cases, this chapter focuses on the architectural \napproaches for tackling big data, along with our recommended best practices, but \nnot speciﬁ c dimensional designs for each use case.\nConventional RDBMSs and SQL simply cannot store or analyze this wide range \nof use cases. To fully address big data, a candidate system would have to be capable \nof the following:\n \n1. Scaling to easily support petabytes (thousands of terabytes) of data.\n 2. Being distributed across thousands of processors, potentially geographically \ndispersed and potentially heterogeneous.\n \n3. Storing the data in the original captured formats while supporting query and \nanalysis applications without converting or moving the data.\n \n4. Subsecond response time for highly constrained standard SQL queries.\n \n5. Embedding arbitrarily complex user-deﬁ ned functions (UDFs) within process-\ning requests.\n \n6. Implementing UDFs in a wide variety of industry-standard procedural \nlanguages.\n \n7. Assembling extensive libraries of reusable UDFs crossing most or all the use \ncases.\n \n8. Executing UDFs as relation scans over petabyte-sized data sets in a few \nminutes.\n \n9. Supporting a wide variety of data types growing to include images, waveforms, \narbitrarily hierarchical data structures, and collections of name-value pairs.\n 10. Loading data to be ready for analysis, at very high rates, at least gigabytes per \nsecond.\n",
      "content_length": 1907,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 565,
      "content": "Big Data Analytics 529\n 11. Integrating data from multiple sources during the load process at very high \nrates (GB/sec).\n 12. Loading data into the database before declaring or discovering its structure.\n 13. Executing certain streaming analytic queries in real time on incoming load \ndata.\n 14. Updating data in place at full load speeds.\n 15. Joining a billion-row dimension table to a trillion-row fact table without \npreclustering the dimension table with the fact table.\n 16. Scheduling and executing complex multi-hundred node workﬂ ows.\n 17. Being conﬁ gured without being subject to a single point of failure.\n 18. Having failover and process continuation when processing nodes fail.\n 19. Supporting extreme, mixed workloads including thousands of geographically \ndispersed online users and programs executing a variety of requests ranging \nfrom ad hoc queries to strategic analysis, while loading data in batch and \nstreaming fashion.\nIn response to these challenges, two architectures have emerged: extended \nRDBMSs and MapReduce/Hadoop.\nExtended RDBMS Architecture\nExisting  RDBMS vendors are extending the classic relational data types to include \nsome of the new data types required by big data, as shown by the arrows in the \nFigure 21-1.\n- Operational & ODS\n- ERP systems\n- User desktops\n- MDM systems\n- External suppliers\n- RDBMS\n- Flat files & XML docs\n- Message queues\n- Proprietary formats\n- Complex structures\n- Unstructured text\n- Images, video\n- Name-value pairs\n- Atomic business process\n  dimensional models with\n  aggregate navigation\n- Conformed dimensions/\n   facts driving EDW\n   integration\n- Specially crafted UDFs\n  embedded in DBMS\n  inner loop\nETL System\nSource Systems\nBI Applications\nMetadata\nInfrastructure and Security\nBack Room\nFront Room\nPresentation Server\n- Extract\n- Clean\n- Conform\n- Deliver\nETL Management Services\nETL Data Stores\n- Queries\n- Standard reports\n- Analytic applications\n- Dashboards\n- Operational BI\n- Data mining & models\n- Genl purpose programs\nBI Data Stores\nBI Management Services\nBI Portal\nFigure 21-1: Relational DBMS architecture showing big data extensions.\n",
      "content_length": 2124,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 566,
      "content": "Chapter 21\n530\nExisting  RDBMSs must open their doors to loading and processing a much \nbroader range of data types including complex structures such as vectors, matrices, \nand custom hyperstructured data. At the other end of the spectrum, the RDBMSs \nneed to load and process unstructured and semistructured text, as well as images, \nvideo, and collections of name-value pairs, sometimes called data bags.\nBut  it is not suffi  cient for RDBMSs to merely host the new data types as blobs to be \ndelivered at some later time to a BI application that can interpret the data, although \nthis alternative has always been possible. To really own big data, RDBMSs must \nallow the new data types to be processed within the DBMS inner loop by means of \nspecially crafted user-deﬁ ned functions (UDFs) written by business user analysts.\nFinally,  a valuable use case is to process the data twice through the RDBMS, \nwhere in the ﬁ rst pass the RDBMS is used as a fact extractor on the original data, \nand then in the second pass, these results are automatically fed back to the RDBMS \ninput as conventional relational rows, columns, and data types.\nMapReduce/Hadoop Architecture\nThe  alternative architecture, MapReduce/Hadoop, is an open source top-level Apache \nproject with many components. MapReduce is a processing framework originally \ndeveloped by Google in the early 2000s for performing web page searches across \nthousands of physically separated machines. The MapReduce approach is extremely \ngeneral. Complete MapReduce systems can be implemented in a variety of languages; \nthe most signiﬁ cant implementation is in Java. MapReduce is actually a UDF execu-\ntion framework, where the “F” can be extraordinarily complex. The most signiﬁ cant \nimplementation of MapReduce is Apache Hadoop, known simply as Hadoop. The \nHadoop project has thousands of contributors and a whole industry of diverse appli-\ncations. Hadoop runs natively on its own Hadoop distributed ﬁ le system (HDFS) and \ncan also read and write to Amazon S3 and others. Conventional database vendors \nare also implementing interfaces to allow Hadoop jobs to be run over massively \ndistributed instances of their databases.\nNOTE \nA full discussion of the MapReduce/Hadoop architecture is beyond the \nscope of this book. Interested readers are invited to study the in depth big data \nresources available on our website at www.kimballgroup.com.\nComparison of Big Data Architectures\nThe two big data architecture approaches have separate long-term advantages and \nare likely to coexist far into the future. At the time of this writing, the characteristics \nof the two architectures are summarized in the  Figure 21-2.\n",
      "content_length": 2680,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 567,
      "content": "Big Data Analytics 531\nExtended Relational DBMS\nProprietary, mostly\nOpen source\nExpensive\nData must be structured\nGreat for speedy indexed lookups\nDeep support for relational semantics\nIndirect support for complex data structures\nIndirect support for iteration, complex branching\nDeep support for transaction processing\nLess expensive\nData does not require structuring\nGreat for massive full data scans\nIndirect support for relational semantics, e.g., Hive\nDeep support for complex data structures\nDeep support for iteration, complex branching\nLittle or no support for transaction processing\nMapReduce/Hadoop\nFigure 21-2: Comparison of relational DBMS and MapReduce/Hadoop architectures.\nRecommended Best Practices for Big Data\nAlthough the big data marketplace is anything but mature, the industry now has a \ndecade of accumulated experience. In that time, a number of best practices speciﬁ c \nto big data have emerged. This section attempts to capture these best practices, \nsteering a middle ground between high-level motherhood admonitions versus down-\nin-the-weeds technical minutiae speciﬁ c to a single tool.\nHaving said that, one should recognize that the industry has a well-tested set of \nbest practices developed over the last 30 years for relationally-based data warehouses \nthat surely are relevant to big data. We list them brieﬂ y. They are to:\n \n■Drive the choice of data sources feeding the data warehouse from business \nneeds.\n \n■Focus incessantly on user interface simplicity and performance.\n \n■Think dimensionally: Divide the world into dimensions and facts.\n \n■Integrate separate data sources with conformed dimensions.\n \n■Track time variance with slowly changing dimensions (SCDs).\n \n■Anchor all dimensions with durable surrogate keys.\nIn the remainder of this section, we divide big data best practices into four cat-\negories: management, architecture, data modeling, and governance.\nManagement Best Practices for Big Data\nThe following best practices apply to the overall management of a big data \nenvironment.\nStructure Big Data Environments Around Analytics\nConsider  structuring big data environments around analytics and not ad hoc que-\nrying or standard reporting. Every step in the data pathway from original source \n",
      "content_length": 2248,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 568,
      "content": "Chapter 21\n532\nto analyst’s screen must support complex analytic routines implemented as user-\ndeﬁ ned functions (UDFs) or via a metadata-driven development environment that \ncan be programmed for each type of analysis. This includes loaders, cleansers, \nintegrators, user interfaces, and ﬁ nally BI tools, as further discussed in the archi-\ntectural best practices section.\nDelay Building Legacy Environments\nIt’s  not a good idea to attempt building a legacy big data environment at this time. \nThe big data environment is changing too rapidly to consider building a long-lasting \nlegacy foundation. Rather, plan for disruptive changes coming from every direc-\ntion: new data types, competitive challenges, programming approaches, hardware, \nnetworking technology, and services off ered by literally hundreds of new big data \nproviders. For the foreseeable future, maintain a balance among several imple-\nmentation approaches including Hadoop, traditional grid computing, pushdown \noptimization in an RDBMS, on-premise computing, cloud computing, and even \nthe mainframe. None of these approaches will be the single winner in the long \nrun. Platform as a service (PaaS) providers off er an attractive option that can help \nassemble a compatible set of tools. \nThink of Hadoop as a ﬂ exible, general purpose environment for many forms of \nETL processing, where the goal is to add suffi  cient structure and context to big data \nso that it can be loaded into an RDBMS. The same data in Hadoop can be accessed \nand transformed with Hive, Pig, HBase, and MapReduce code written in a variety \nof languages, even simultaneously.\nThis demands ﬂ exibility. Assume you will reprogram and rehost all your big \ndata applications within two years. Choose approaches that can be reprogramed \nand rehosted. Consider using a metadata-driven codeless development environment \nto increase productivity and help insulate from underlying technology  changes.\nBuild From Sandbox Results\nConsider  embracing sandbox silos and building a practice of productionizing sand-\nbox results. Allow data scientists to construct their data experiments and prototypes \nusing their preferred languages and programming environments. Then, after proof \nof concept, systematically reprogram these implementations with an IT turnover \nteam. Here are a couple of examples to illustrate this recommendation:\nThe production environment for custom analytic programming might be MatLab \nwithin PostgreSQL or SAS within a Teradata RDBMS, but the data scientists might be \nbuilding their proofs of concept in a wide variety of their own preferred languages \nand architectures. The key insight here: IT must be uncharacteristically tolerant \nof the range of technologies the data scientists use and be prepared in many cases \nto re-implement the data scientists’ work in a standard set of technologies that can \nbe supported over the long haul. The sandbox development environment might \n",
      "content_length": 2945,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 569,
      "content": "Big Data Analytics 533\nbe custom R code directly accessing Hadoop, but controlled by a metadata-driven \ndriven ETL tool. Then when the data scientist is ready to hand over the proof of \nconcept, much of the logic could immediately be redeployed under the ETL tool to \nrun in a grid computing environment that is scalable, highly available, and secure.\nTry Simple Applications First\nYou  can put your toe in the water with a simple big data application, such as backup \nand archiving. While starting with a big data program, and searching for valuable \nbusiness use cases with limited risk and when assembling the requisite big data \nskills, consider using Hadoop as a low-cost, ﬂ exible backup and archiving technol-\nogy. Hadoop can store and retrieve data in the full range of formats from totally \nunstructured to highly structured specialized formats. This approach may also \nenable you address the sunsetting challenge where original applications may not be \navailable in the distant future (perhaps because of licensing restrictions); you can \ndump data from those applications into your documented format.\nArchitecture Best Practices for Big Data\nThe following best practices aff ect the overall structure and organization of your \nbig data environment.\nPlan a Data Highway\nYou  should plan for a logical data highway with multiple caches of increasing latency. \nPhysically implement only those caches appropriate for your environment. The data \nhighway can have as many as ﬁ ve caches of increasing data latency, each with its \ndistinct analytic advantages and trade-off s, as shown in Figure 21-3.\nRaw Source\n(Immediate)\nReal Time\nCache\n(Seconds)\nBusiness Activity\nCache\n(Minutes)\nTop Line\nCache\n(24 Hours)\nDW and\nLong Time Series\n(Daily, Periodic, Annual)\nFigure 21-3: Big data caches of increasing latency and data quality.\nHere are potential examples of the ﬁ ve data caches:\n \n■Raw source applications: Credit card fraud detection, immediate complex \nevent processing (CEP) including network stability and cyber attack detection.\n \n■Real time applications: Web page ad selection, personalized price promotions, \non-line games monitoring.\n \n■Business activity applications: Low-latency KPI dashboards pushed to users, \ntrouble ticket tracking, process completion tracking, “fused” CEP reporting, \ncustomer service portals and dashboards, and mobile sales apps.\n",
      "content_length": 2372,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 570,
      "content": "Chapter 21\n534\n \n■Top line applications: Tactical reporting, promotion tracking, midcourse cor-\nrections based on social media buzz. Top line refers to the common practice \nby senior managers of seeing a quick top line review of what has happened \nin the enterprise over the past 24 hours.\n \n■Data warehouse and long time series applications: All forms of reporting, \nad hoc querying, historical analysis, master data management, large scale \ntemporal dynamics, and Markov chain analysis.\nEach cache that exists in a given environment is physical and distinct from the \nother caches. Data moves from the raw source down this highway through ETL \nprocesses. There may be multiple paths from the raw source to intermediate caches. \nFor instance, data could go to the real-time cache to drive a zero latency-style user \ninterface, but at the same time be extracted directly into a daily top line cache that \nwould look like a classic operational data store (ODS). Then the data from this ODS \ncould feed the data warehouse. Data also ﬂ ows in the reverse direction along the \nhighway. We’ll discuss implementing backﬂ ows later in this section.\nMuch of the data along this highway must remain in nonrelational formats rang-\ning from unstructured text to complex multistructured data, such as images, arrays, \ngraphs, links, matrices, and sets of name-value pairs.\nBuild a Fact Extractor from Big Data\nIt’s a  good idea to use big data analytics as a fact extractor to move data to the next \ncache. For example, the analysis of unstructured text tweets can produce a whole \nset of numerical, trendable sentiment measures including share of voice, audience \nengagement, conversation reach, active advocates, advocate inﬂ uence, advocacy \nimpact, resolution rate, resolution time, satisfaction score, topic trends, sentiment \nratio, and idea impact.\nBuild Comprehensive Ecosystems\nYou  can use big data integration to build comprehensive ecosystems that integrate \nconventional structured RDBMS data, documents, e-mails, and in-house, business-\noriented social networking. One of the potent messages from big data is the ability \nto integrate disparate data sources of diff erent modalities. You get streams of data \nfrom new data producing channels such as social networks, mobile devices, and \nautomated alert processes. Imagine a big ﬁ nancial institution handling millions of \naccounts, tens of millions of associated paper documents, and thousands of profes-\nsionals both within the organization and in the ﬁ eld as partners or customers. Now \nset up a secure social network of all the trusted parties to communicate as business \nis conducted. Much of this communication is signiﬁ cant and should be saved in a \nqueryable way. You could capture all this information in Hadoop, dimensionalize \nit (as you see in the following modeling best practices), use it in the course of busi-\nness, and then back it up and archive it.\n",
      "content_length": 2922,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 571,
      "content": "Big Data Analytics 535\nPlan for Data Quality\nYou  can plan for data quality to be better further along the data highway. This is \nthe classic trade-off  of latency versus quality. Analysts and business users must \naccept the reality that very low latency (that is, immediate) data is unavoidably \ndirty because there are limits to how much cleansing and diagnosing can be done \nin very short time intervals. Tests and corrections on individual ﬁ eld contents can \nbe performed at the fastest data transfer rates. Tests and corrections on structural \nrelationships among ﬁ elds and across data sources are necessarily slower. Tests \nand corrections involving complex business rules range from being instantaneous \n(such as a set of dates being in a certain order) to taking arbitrarily long times (such \nas waiting to see if a threshold of unusual events has been exceeded). And ﬁ nally, \nslower ETL processes, such as those feeding the daily top line cache, often are built \non fundamentally more complete data, for example where incomplete transaction \nsets and repudiated transactions have been eliminated. In this case, the instanta-\nneous data feeds simply do not have the correct  information.\nAdd Value to Data as Soon as Possible\nYou  should apply ﬁ ltering, cleansing, pruning, conforming, matching, joining, and \ndiagnosing at the earliest touch points possible. This is a corollary of the previous \nbest practice. Each step on the data highway provides more time to add value to \nthe data. Filtering, cleansing, and pruning the data reduces the amount transferred \nto the next cache and eliminates irrelevant or corrupted data. To be fair, there is \na school of thought that applies cleansing logic only at analysis run time because \ncleansing might delete “interesting outliers.” Conforming takes the active step of \nplacing highly administered enterprise attributes into major entities such as cus-\ntomer, product, and date. The existence of these conformed attributes allows high \nvalue joins to be made across separate application domains. A shorter name for this \nstep is “integration!” Diagnosing allows many interesting attributes to be added to \ndata, including special conﬁ dence tags and textual identiﬁ ers representing behavior \nclusters identiﬁ ed by a data mining professional.\nImplement Backﬂ ow to Earlier Caches\nYou  should implement backﬂ ows, especially from the data warehouse, to earlier caches \non the data highway. The highly administered dimensions in the data warehouse, such \nas customer, product, and date, should be connected back to data in earlier caches. \nIdeally, all that is needed are unique durable keys for these entities in all the caches. \nThe corollary here is that Job One in each ETL step from one cache to the next is to \nreplace idiosyncratic proprietary keys with the unique durable keys so that analysis \nin each cache can take advantage of the rich upstream content with a simple join on \nthe unique durable key. Can this ETL step be performed even when transferring raw \nsource data into the real time cache in less than a second? Maybe….\n",
      "content_length": 3096,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 572,
      "content": "Chapter 21\n536\nDimension data is not the only data to be transferred back down the highway \ntoward the source. Derived data from fact tables, such as historical summaries and \ncomplex data mining ﬁ ndings, can be packaged as simple indicators or grand totals \nand then transferred to earlier caches on the data highway.\nImplement Streaming Data\nYou  should implement streaming data analytics in selected data ﬂ ows. An interest-\ning angle on low latency data is the need to begin serious analysis on the data as \nit streams in, but possibly far before the data transfer process terminates. There is \nsigniﬁ cant interest in streaming analysis systems, which allow SQL-like queries to \nprocess the data as it ﬂ ows into the system. In some use cases, when the results of \na streaming query surpass a threshold, the analysis can be halted without running \nthe job to the bitter end. An academic eff ort, known as continuous query language \n(CQL), has made impressive progress in deﬁ ning the requirements for streaming \ndata processing including clever semantics for dynamically moving time windows \non the streaming data. Look for CQL language extensions and streaming data query \ncapabilities in the load programs for both RDBMSs and HDFS deployed data sets. \nAn ideal implementation would allow streaming data analysis to take place while \nthe data is loaded at gigabytes per second.\nAvoid Boundary Crashes\nYou  should implement far limits on scalability to avoid a boundary crash. In the early \ndays of computer programming, when machines had pathetically small hard drives \nand real memories, boundary crashes were common and were the bane of applica-\ntions development. When the application ran out of disk space or real memory, the \ndeveloper resorted to elaborate measures, usually requiring signiﬁ cant program-\nming that added nothing to the application’s primary function. Boundary crashes for \nnormal database applications have more or less been eliminated, but big data raises \nthis issue again. Hadoop is an architecture that dramatically reduces programming \nscalability concerns because you can, for the most part, indeﬁ nitely add commodity \nhardware. Of course, even commodity hardware must be provisioned, plugged in, \nand have high bandwidth network connections. The lesson is to plan far ahead for \nscaling out to huge volumes and throughputs.\nMove Prototypes to a Private Cloud\nConsider  performing big data prototyping on a public cloud and then moving to a \nprivate cloud. The advantage of a public cloud is it can be provisioned and scaled up \ninstantly. In those cases in which the sensitivity of the data allows quick in-and-out \nprototyping, this can be eff ective. Just remember not to leave a huge data set online with \nthe public cloud provider over the weekend when the programmers have gone home! \n",
      "content_length": 2829,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 573,
      "content": "Big Data Analytics 537\nHowever, keep in mind that in some cases in which you are trying to exploit data locality \nwith rack-aware MapReduce processes, you may not use a public cloud service because \nit may not provide the data storage control needed.\nStrive for Performance Improvements\nSearch for and expect tenfold to hundredfold performance improvements over time, \nrecognizing the paradigm shift for analysis at high speeds. The openness of the big \ndata marketplace has encouraged hundreds of special purpose tightly coded solu-\ntions for speciﬁ c kinds of analysis. This is a giant blessing and a curse. When free \nfrom being controlled by a big vendor’s RDBMS optimizer and inner loop, smart \ndevelopers can implement spot solutions that are truly 100 times as fast as standard \ntechniques. For instance, some impressive progress has been made on the infamous \n“big join” problem in which a billion-row dimension is joined to a trillion-row fact \ntable. The challenge is these individual spot solutions may not be part of a uniﬁ ed \nsingle architecture.\nOne very current big data theme is visualization of data sets. “Flying around” \na petabyte of data requires spectacular performance! Visualization of big data is \nan exciting new area of development that enables both analysis and discovery of \nunexpected features and data proﬁ ling.\nAnother  exciting application that imposes huge performance demands is “seman-\ntic zooming without pre-aggregations,” in which the analyst descends from a highly \naggregated level to progressively more detailed levels in unstructured or semistruc-\ntured data, analogous to zooming in on a map.\nThe important lesson behind this best practice is that revolutionary advances \nin your power to consume and analyze big data can result from 10x to 100x per-\nformance gains, and you have to be prepared to add these developments to your \nsuite of  tools.\nMonitor Compute Resources\nYou  should separate big data analytic workloads from the conventional data ware-\nhouse to preserve service level agreements. If your big data is hosted in Hadoop, \nit probably doesn’t compete for resources with your conventional RDBMS-based \ndata warehouse. However, be cautious if your big data analytics run on the data \nwarehouse machine because big data requirements change rapidly and inevitably \nin the direction of requiring more compute resources.\nExploit In-Database Analytics\nRemember  to exploit the unique capabilities of in-database analytics. The major \nRDBMS players all signiﬁ cantly invest in in-database analytics. After you pay the \nprice of loading data into relational tables, SQL can be combined with analytic \n",
      "content_length": 2653,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 574,
      "content": "Chapter 21\n538\nextensions in extremely powerful ways. In particular, PostgreSQL, an open source \ndatabase, has extensible syntax for adding powerful user deﬁ ned functions in the \ninner loop.\nData Modeling Best Practices for Big Data\nThe following best practices aff ect the logical and physical structures of the data.\nThink Dimensionally\nBy thinking dimensionally, we mean dividing the world into dimensions and facts. \nBusiness users ﬁ nd the concept of dimensions to be natural and obvious. No matter \nwhat the format of the data, the basic associated entities such as customer, product, \nservice, location, or time can always be found. In the following best practice you \nsee how, with a little discipline, dimensions can be used to integrate data sources. \nBut before getting to the integration ﬁ nish line, you must identify the dimensions \nin each data source and attach them to every low-level atomic data observation. \nThis process of dimensionalization is a good application for big data analytics. For \nexample, a single Twitter tweet “Wow! That is awesome!” may not seem to con-\ntain anything worth dimensionalizing, but with some analysis you often can get \ncustomer (or citizen or patient), location, product (or service or contract or event), \nmarketplace condition, provider, weather, cohort group (or demographic cluster), \nsession, triggering prior event, ﬁ nal outcome, and the list goes on. Some form of \nautomated dimensionalizing is required to stay ahead of the high-velocity streams \nof data. As we point out in a subsequent best practice, incoming data should be fully \ndimensionalized at the earliest extraction step in as close to real time as  possible.\nIntegrate Separate Data Sources with Conformed Dimensions\nConformed  dimensions are the glue that holds together separate data sources and \nenable them to be combined in a single analysis. Conformed dimensions are perhaps \nthe most powerful best practice from the conventional DW/BI world that should be \ninherited by big data.\nThe basic idea behind conformed dimensions is the presence of one or more \nenterprise attributes (ﬁ elds) in the versions of dimensions associated with separate \ndata sources. For instance, every customer-facing process in an enterprise will have \nsome variation of a customer dimension. These variations of the customer dimension \nmay have diff erent keys, diff erent ﬁ eld deﬁ nitions, and even diff erent granularity. \nBut even in the worst cases of incompatible data, one or more enterprise attributes \ncan be deﬁ ned that can be embedded in all the customer dimension variations. For \ninstance, a customer demographic category is a plausible choice. Such a descriptor \ncould be attached to nearly every customer dimension, even those at higher levels \nof aggregation. After this has been done, analyses on this customer demographic \n",
      "content_length": 2849,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 575,
      "content": "Big Data Analytics 539\ncategory can cross every participating data source with a simple sort-merge process \nafter separate queries are run against the diff erent data sources. Best of all, the step \nof introducing the enterprise attributes into the separate databases can be done in \nan incremental, agile, and nondisruptive way as described in Chapter 8: Customer \nRelationship Management and Chapter 19: ETL Subsystems and Techniques. All \nexisting analysis applications will continue to run as the conformed dimension \ncontent is rolled out.\nAnchor Dimensions with Durable Surrogate Keys\nIf  there is one lesson we have learned in the data warehouse world, it is not to anchor \nmajor entities such as customer, product, and time with the natural keys deﬁ ned by \na speciﬁ c application. These natural keys turn out to be a snare and a delusion in the \nreal world. They are incompatible across applications and are poorly administered, \nand they are administered by someone else who may not have the interests of the \ndata warehouse at heart. The ﬁ rst step in every data source is to augment the natural \nkey coming from a source with an enterprisewide durable surrogate key. Durable \nmeans there is no business rule that can change the key. The durable key belongs \nto the DW/BI system, not to the data source. Surrogate means the keys themselves \nare simple integers either assigned in sequence or generated by a robust hashing \nalgorithm that guarantees uniqueness. An isolated surrogate key has no applications \ncontent. It is just an identiﬁ er.\nThe big data world is ﬁ lled with obvious dimensions that must possess durable \nsurrogate keys. Earlier in this chapter when we proposed pushing data backward \ndown the data highway, we relied on the presence of the durable surrogate keys to \nmake this process work. We also stated that Job One on every data extraction from a \nraw source was to embed the durable surrogate keys in the appropriate dimensions.\nExpect to Integrate Structured and Unstructured Data\nBig  data considerably broadens the integration challenge. Much big data will never \nend up in a relational database; rather it will stay in Hadoop or a grid. But after you \nare armed with conformed dimensions and durable surrogate keys, all forms of data \ncan be combined in single analyses. For example, a medical study can select a group \nof patients with certain demographic and health status attributes and then combine \ntheir conventional DW/BI data with image data (photographs, X-rays, EKGs, and \nso on), free form text data (physician’s notes), social media sentiments (opinions of \ntreatment), and cohort group linkages (patients with similar situations), and doc-\ntors with similar patients.\nUse Slowly Changing Dimensions\nYou  should track time variance with slowly changing dimensions (SCDs). Tracking \ntime variance of dimensions is an old and venerable best practice from the data \n",
      "content_length": 2914,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 576,
      "content": "Chapter 21\n540\nwarehouse world. Chapter 5: Procurement makes a powerful case for using SCD \ntechniques for handling time variance. This is just as important in the big data \nworld as it is in the conventional data warehouse world.\nDeclare Data Structure at Analysis Time\nYou  must get used to not declaring data structures until analysis time. One of the \ncharms of big data is putting off  declaring data structures at the time of loading into \nHadoop or a data grid. This brings many advantages. The data structures may not \nbe understood at load time. The data may have such variable content that a single \ndata structure either makes no sense or forces you to modify the data to ﬁ t into \na structure. If you can load data into Hadoop, for instance, without declaring its \nstructure, you can avoid a resource intensive step. And ﬁ nally, diff erent analysts may \nlegitimately see the same data in diff erent ways. Of course, there is a penalty in some \ncases because data without a declared structure may be diffi  cult or impossible to \nindex for rapid access, as in an RDBMS. However, most big data analysis algorithms \nprocess entire data sets without expecting precise ﬁ ltering of subsets of the data.\nThis best practice conﬂ icts with traditional RDBMS methodologies, which puts a \nlot of emphasis on modeling the data carefully before loading. But this does not lead \nto a deadly conﬂ ict. For data destined for an RDBMS, the transfer from a Hadoop or \ndata grid environment and from a name-value pair structure into RDBMS named \ncolumns can be thought of as a valuable ETL step.\nLoad Data as Simple Name-Value Pairs\nConsider  building technology around name-value pair data sources. Big data \nsources are ﬁ lled with surprises. In many cases, you open the ﬁ re hose and discover \nunexpected or undocumented data content, which you must nevertheless load at \ngigabytes per second. The escape from this problem is to load this data as simple \nname-value pairs. For example, if an applicant were to disclose her ﬁ nancial assets, \nas illustrated with Figures 8-7 and 8-8, she might declare something unexpected \nsuch as “rare postage stamp = $10,000.” In a name-value pair data set, this would be \nloaded gracefully, even though you had never seen “rare postage stamp” and didn’t \nknow what to do with it at load time. Of course, this practice meshes nicely with the \nprevious practice of deferring the declaration of data structures until past load time.\nMany MapReduce programming frameworks require data to be presented as \nname-value pairs, which makes sense given the complete possible generality of \nbig data.\nRapidly Prototype Using Data Virtualization\nConsider  using data virtualization to allow rapid prototyping and schema altera-\ntions. Data virtualization is a powerful technique for declaring diff erent logical data \n",
      "content_length": 2841,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 577,
      "content": "Big Data Analytics 541\nstructures on underlying physical data. Standard view deﬁ nitions in SQL are a good \nexample of data virtualization. In theory, data virtualization can present a data \nsource in any format the analyst needs. But data virtualization trades off  the cost of \ncomputing at run time with the cost of ETL to build physical tables before run time. \nData virtualization is a powerful way to prototype data structures and make rapid \nalterations or provide distinct alternatives. The best data virtualization strategy is \nto expect to materialize the virtual schemas when they have been tested and vet-\nted and the analysts want the performance improvements of actual physical tables.\nData Governance Best Practices for Big Data\nThe following best practices apply to managing big data as a valuable enterprise \nasset.\nThere is No Such Thing as Big Data Governance\nNow  that we have your attention, the point is that data governance must be a com-\nprehensive approach for the entire data ecosystem, not a spot solution for big data \nin isolation. Data governance for big data should be an extension of the approach \nused to govern all the enterprise data. At a minimum, data governance embraces \nprivacy, security, compliance, data quality, metadata management, master data \nmanagement, and the business glossary that exposes deﬁ nitions and context to \nthe business community.\nDimensionalize the Data before Applying Governance\nHere  is an interesting challenge big data introduces: You must apply data gover-\nnance principles even when you don’t know what to expect from the content of the \ndata. You may receive data arriving at gigabytes per minute, often as name-value \npairs with unexpected content. The best chance at classifying data in ways that are \nimportant to your data governance responsibilities is to dimensionalize it as fully \nas possible at the earliest stage in the data pipeline. Parse it, match it, and apply \nidentity resolution on-the-ﬂ y. We made this same point when arguing for the ben-\neﬁ ts of data integration, but here we advocate against even using the data before \nthis dimensionalizing step.\nPrivacy is the Most Important Governance Perspective\nIf  you analyze data sets that include identifying information about individuals or \norganizations, privacy is the most important governance perspective. Although \nevery aspect of data governance looms as critically important, in these cases, privacy \ncarries the most responsibility and business risk. Egregious episodes of compro-\nmising the privacy of individuals or groups can damage your reputation, diminish \nmarketplace trust, expose you to civil lawsuits, and get you in trouble with the \n",
      "content_length": 2690,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 578,
      "content": "Chapter 21\n542\nlaw. At the least, for most forms of analysis, personal details must be masked, and \ndata aggregated enough to not allow identiﬁ cation of individuals. At the time of \nthis writing, special attention must be paid when storing sensitive data in Hadoop \nbecause after data is written to Hadoop, Hadoop doesn’t manage updates very well. \nData should either be masked or encrypted on write (persistent data masking) or \ndata should be masked on read (dynamic data masking).\nDon’t Choose Big Data over Governance\nDon’t put off  data governance completely in the rush to use big data. Even for \nexploratory big data prototype projects, maintain a checklist of issues to consider \nwhen going forward. You don’t want an ineff ective bureaucracy, but maybe you can \nstrive to deliver an agile bureaucracy! \nSummary\nBig data brings a host of changes and opportunities to IT, and it is easy to think that \na whole new set of rules must be created. But with the beneﬁ t of big data experience, \nmany best practices have emerged. Many of these practices are recognizable exten-\nsions from the DW/BI world, and admittedly quite a few are new and novel ways \nof thinking about data and the mission of IT. But the recognition that the mission \nhas expanded is welcome and is in some ways overdue. The current explosion of \ndata-collecting  channels, new data types, and new analytic opportunities mean the \nlist of best practices will continue to grow in interesting ways.\n",
      "content_length": 1472,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 579,
      "content": "Index\nSymbols\n3NF (third normal form) models, 7\nERDs (entity-relationship diagrams), 8\nnormalized 3NF structures, 8\n4-step dimensional design process, 38, 70–72\nA\nabnormal scenario indicators, 255–256\nabstract generic dimensions, 66\ngeographic location dimension, 310\naccessibility goals, 3\naccidents (insurance case study), factless fact \ntables, 396\naccounting case study, 202\nbudgeting, 210–213\nfact tables, consolidated, 224–225\nG/L (general ledger), 203\nchart of accounts, 203–204\ncurrencies, 206\nﬁ nancial statements, 209–210\nﬁ scal calendar, multiple, 208\nhierarchies, 209\njournal entries, 206–207\nperiod close, 204–206\nperiodic snapshot, 203\nyear-to-date facts, 206\nhierarchies\nﬁ xed depth, 214\nmodifying, ragged, 221\nragged, alternative modeling approaches, \n221–223\nragged, bridge table approach, 223\nragged, modifying, 220–221\nragged, shared ownership, 219\nragged, time varying, 220\nragged, variable depth, 215–217\nvariable depth, 214–215\nOLAP and, 226\naccumulating grain fact tables, 12\naccumulating snapshots, 44, 118–119, \n194–196\nclaims (insurance case study), 393\ncomplex workﬂ ows, 393–394\ntimespan accumulating snapshot, \n394–395\nETL systems, 475\nfact tables, 121, 326–329\ncomplementary fact tables, 122\nmilestones, 121\nOLAP cubes, 121–122\nupdates, 121–122\nhealthcare case study, 343\npolicy (insurance case study), 384–385\ntype 2 dimensions and, 196\nactivity-based costing measures, 184\nadditive facts, 11, 42\nadd mini dimension and type 1 outrigger \n(SCD type 5), 55\nadd mini-dimension (SCD type 4), 55\nmultiple, 156–159\nadd new attribute (SCD type 3), 55, 154–155\nmultiple, 156\nadd new row (SCD type 2), 54, 150–152\neff ective date, 152–153\nexpiration date, 152–153\ntype 1 in same dimension, 153\naddresses\nASCII, 236\nCRM and, customer dimension, 233–238\nUnicode, 236–238\nadd type 1 attributes to type 2 dimension \n(SCD type 6), 56\nadmissions events (education case study), \n330\naggregate builder, ETL system, 481\naggregated facts\nas attributes, 64\nCRM and, customer dimension, 239–240\n",
      "content_length": 2005,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 580,
      "content": "Index  \n544\naggregate fact tables, 45\nclickstream data, 366–367\naggregate OLAP cubes, 8, 45\naggregate tables, ETL system development, \n519\nagile development, 34–35\nconformed dimensions and, 137–138\nairline case study, 311\nbus matrix, 311–315\ncalendars as outriggers, 321–323\nclass of service ﬂ own dimension, 319–320\ndestination airport dimension, 320–321\nfact tables, granularity, 312–316\norigin dimension, 320–321\npassenger dimension, 314\nsales channel dimension, 315\nsegments, linking to trips, 315–316\ntime zones, multiple, 323\naliasing, 171\nallocated facts, 60\nallocating, 184–186\nallocations, proﬁ t and loss fact tables, 60\nALTER TABLE command, 17\nanalytics\nbig data management, 531\nGA (Google Analytics), 367\nin-database, big data and, 537\nanalytic solutions, packaged, 270–271\nAND queries, skill keywords bridge, 275\narchitecture\nbig data best practices\nbackﬂ ow, 535–536\nboundary crashes, 536\ncompute resources, 537\ndata highway planning, 533–534\ndata quality planning, 535\ndata value, 535\necosystems, 534\nfact extractor, 534\nin-database analytics, 537\nperformance improvements, 537\nprototypes, 536\nstreaming data, 536\nDW/BI alternatives, 26–29\nenterprise data warehouse bus architecture, \n22, 123–125\nhub-and-spoke CIF architecture, 28–29\nhybrid hub-and-spoke Kimball architecture, \n29\nindependent data mart architecture, 26–27\nMapReduce/Hadoop, 530\nRDBMS, extension, 529–530\nreal-time processing, 522–524\narchiving, 447–448, 485–486\nartiﬁ cial keys, 98\nASCII (American Standard Code for \nInformation Interchange), 236\natomic grain data, 17, 74\nattributes\naggregated facts as, 64\nbridge tables, CRM and, 247\nchanges, 514\ndetailed dimension model, 437\nexpiration, 266\nﬂ ags, 48\nindicators, 48\nnull, 48, 92\nnumeric values as, 59\npathstring, ragged/variable depth \nhierarchies, 57\nproduct dimensions, 132\nSCD type 3 (add new attribute), 154–155\nmultiple, 156\naudit columns, CDC (change data capture), \n452\naudit dimensions, 66, 192–193, 284, 495\nassembler, 460\ninsurance case study, 383\nkey assignment, 511–512\nautomation, ETL system development\nerrors, 520\nexceptions, 520\njob scheduling, 520\nB\nbackﬂ ow, big data and, 535–536\nbackups, 495\nbackup system, ETL systems, 485\narchiving, 485–486\ncompliance manager, 493–495\ndependency, 490–491\nhigh performance, 485\nlights-out operations, 485\nlineage, 490–491\nmetadata repository, 495\nparallelizing/pipelining system, 492\nproblem escalation system, 491–492\nrecovery and restart system, 486–488\nretrieval, 485–486\nsecurity system, 492–493\nsimple administration, 485\nsorting system, 490\nversion control system, 488\nversion migration system, 488\nworkﬂ ow monitor, 489–490\nbanking case study, 282\nbus matrix, 282–296\ndimensions\nhousehold, 286–287\n",
      "content_length": 2697,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 581,
      "content": "Index 545\nmini-dimensions, 289–291\nmultivalued, weighting, 287–289\ntoo few, 283–286\nfacts, value banding, 291–292\nheterogeneous products, 293–295\nhot swappable dimensions, 296\nuser perspective, 293\nbehavior\ncustomers, CRM and, 249–251\nsequential, step dimension and, 251–252\nstudy groups, 64, 249\nbehavior tags\nfacts, 241\ntime series, 63, 240–242\nBI application design/development \n(Lifecycle), 408, 423 –424\nBI applications, 22\nBI (business intelligence) delivery interfaces, \n448\nbig data\narchitecture best practices\nbackﬂ ow, 535–536\nboundary crashes, 536\ncompute resources, 537\ndata highway planning, 533–534\ndata quality planning, 535\ndata value, 535\necosystems, 534\nfact extractor, 534\nin-database analytics, 537\nperformance improvements, 537\nprototypes, 536\nstreaming data, 536\ndata governance best practices, 541\ndimensionalizing and, 541\nprivacy, 541–542\ndata modeling best practices\ndata structure declaration, 540\ndata virtualization, 540\ndimension anchoring, 539\nintegrating sources and conﬁ ned \ndimensions, 538\nname-value pairs, 540\nSCDs (slowly changing dimensions), 539\nstructured/unstructured data integration, \n539\nthinking dimensionally, 538\nmanagement best practices\nanalytics and, 531\nlegacy environments and, 532\nsandbox results and, 532–533\nsunsetting and, 533\noverview, 527–529\nblobs, 530\nboundary crashes, big data and, 536\nbridge tables\ncustomer contacts, CRM and, 248\nmini-dimensions, 290–291\nmultivalued\nCRM and, 245–246\ntime varying, 63\nmultivalued dimensions, 63, 477–478\nragged hierarchies and, 223\nragged/variable depth hierarchies, 57\nsparse attributes, CRM and, 247\nbubble chart, dimension modeling and, \n435–436\nbudget fact table, 210\nbudgeting process, 210–213\nbus architecture, 124–125\nenterprise data warehouse bus architecture, \n52\nbusiness analyst, 408\nBusiness Dimensional Lifecycle, 404\nbusiness-driven governance, 136–137\nbusiness driver, 408\nbusiness initiatives, 70\nbusiness lead, 408\nbusiness motivation, Lifecycle planning, 407\nbusiness processes\ncharacteristics, 70–71\ndimensional modeling, 39, 300\nretail sales case study, 74\nvalue chain, 111–112\nbusiness representatives, dimensional \nmodeling, 431–432\nbusiness requirements\ndimensional modeling, 432\nLifecycle, 405, 410\ndocumentation, 414\nforum selection, 410–411\ninterviews, 412–414\nlaunch, 412\nprioritization, 414–415\nrepresentatives, 411–412\nteam, 411\nbusiness rule screens, 458\nbusiness sponsor, 408\nLifecycle planning, 406\nbusiness users, 408\nperspectives, 293\nbus matrix\naccounting, 202\nairline, 311\nbanking, 282\ndetailed implementation bus matrix, 53\ndimensional modeling and, 439\nenterprise data warehouse bus matrix, 52\nhealthcare case study, 339–340\nHR (human resources), 268–269\ninsurance, 378–389\ndetailed implementation, 390\n",
      "content_length": 2739,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 582,
      "content": "Index  \n546\ninventory, 113–119\nopportunity/stakeholder matrix, 127\norder management, 168\nprocurement, 142–147\ntelecommunications, 297–299\nuniversity, 325–326\nweb retailers, clickstream integration, \n368–370\nC\ncalculation lag, 196–197\ncalendar date dimensions, 48\ncalendars, country-speciﬁ c as outriggers, \n321–323\ncannibalization, 90\ncargo shipper schema, 317\ncase studies\naccounting, 202\nbudgeting, 210–213\nconsolidated fact tables, 224–225\nG/L (general ledger), 203–210\nhierarchies, 214–223\nOLAP and, 226\nairline, 311\ncalendars as outriggers, 321–323\nclass of service ﬂ own dimension, 319–320\ndestination airport dimension, 320–321\nfact table granularity, 312–316\norigin dimension, 320–321\npassenger dimension, 314\nsales channel dimension, 315\ntime zones, multiple, 323\nCRM (customer relationship management)\nanalytic, 231–233\nbridge tables, 245–248\ncomplex customer behavior, 249–251\ncustomer data integration, 256–260\ncustomer dimension and, 233–245\nfact tables, abnormal scenario indicators, \n255–256\nfact tables, satisfaction indicators, \n254–255\nfact tables, timespan, 252–254\nlow latency data, 260–261\noperational, 231–233\nstep dimension, sequential behavior, \n251–252\neducation, 325–326\naccumulating snapshot fact table, \n326–329\nadditional uses, 336\nadmissions events, 330\napplicant pipeline, 326–329\nattendance, 335\nchange tracking, 330\ncourse registrations, 330–333\nfacility use, 334\ninstructors, multiple,  333\nmetrics, artiﬁ cial count, 331–332\nresearch grant proposal, 329\nstudent dimensions, 330\nterm dimensions, 330\nelectronic commerce\nclickstream data, 353–370\nproﬁ tability, sales transactions and, \n370–372\nﬁ nancial services, 282, 287–295\ndimensions, household, 286–287\ndimensions, too few, 283–286\nhealthcare, 339–340\nbilling, 342–344\nclaims, 342–344\ndate dimension, 345\ndiagnosis dimension, 345–347\nEMRs (electronic medical records), \n341–348\nHCPCS (Healthcare Common Procedure \nCoding System), 342\nHIPAA (Health Insurance Portability and \nAccountability Act), 341\nICD (International Classiﬁ cation of \nDiseases), 342\nimages, 350\ninventory, 351\nmeasure type dimension, 349–350\npayments, 342–344\nretroactive changes, 351–352\nsubtypes, 347–348\nsupertypes, 347–348\ntext comments, 350\nHR (Human Resources Management)\nbus matrix, 268\nemployee hierarchies, 271–272\nemployee proﬁ les, 263–267\nhierarchies, 273–274\nmanagers key, 272–273\npackaged data models, 270–271\nperiodic snapshots, 267–268\nskill keywords, 274–277\nsurvey questionnaire, 277–278\ninsurance, 375–377\naccident events factless fact table, 396\naccumulating snapshot, 384–385\nbus matrix, 378, 389–390\nclaim transactions, 390–396\nconformed dimensions, 386\nconformed facts, 386\ndegenerate dimension, 383\ndimensions, 380\n",
      "content_length": 2698,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 583,
      "content": "Index 547\ndimensions, audit, 383\ndimensions, low cardinality, 383\ndimensions, multivalued, 388\njunk dimensions, 392\nmini-dimensions, 381–382\nmultivalued dimensions, 382\nNAICS (North American Industry \nClassiﬁ cation System), 382\nnumeric attributes, 382\npay-in-advance facts, 386–387\nperiodic snapshot, 385\npolicy transaction fact table, 383\npolicy transactions, 379–380\npremiums, periodic snapshot, 386–388\nSCDs (slowly changing dimensions), \n380–381\nSIC (Standard Industry Classiﬁ cation), \n382\nsupertype/subtype products, 384, 387\ntimespan accumulating snapshot, 394\nvalue chain, 377–378\ninventory\naccumulating snapshot, 118–119\nfact tables, 115–116\nperiodic snapshot, 112–114\nsemi-additive facts, 114–115\ntransactions, 116–118\norder management, 167\naccumulating snapshots, 194–196\naudit dimension, 192–193\ncustomer dimension, 174–175\ndeal dimension, 177–179\nheader/line pattern, 186\nheader/line patterns, 181–182\ninvoice transactions, 187\njunk dimensions, 179–180\nlag calculations, 196\nmultiple currencies, 182–184\nproduct dimension, 172–173\nproﬁ t and loss facts, 189–191\ntransaction granularity, 184–186\ntransactions, 168–171\nunits of measure, multiple, 197–198\nprocurement, 141–142\nbus matrix, 142–143\ncomplementary procurement snapshot \nfact table, 147\ntransactions, 142–145\nretail sales, 72–73\nbusiness process selection, 74\ndimensions, selecting, 76\nfacts, 76–77\nfacts, derived, 77–78\nfacts, non-additive, 78\nfact tables, 79\nfrequent shopper program, 96\ngrain declaration, 74–75\nPOS schema, 94\nretail schema extensibility, 95–97\ntelecommunications, 297–299\ncausal dimension, 89–90, 284\nCDC (change data capture)\nETL system, 451\naudit columns, 452\ndiff  compare, 452\nlog scraping, 453\nmessage queue monitoring, 453\ntimed extracts, 452\ncentipede fact tables, 58, 108–109\nchange reasons, 266–267\nchange tracking, 147–148\neducation case study, 330\nHR (human resources) case study, \nembedded managers key, 272–273\nSCDs, 148\nchart of accounts (G/L), 203–204\nuniform chart of accounts, 204\ncheckpoints, data quality, 516\nCIF (Corporate Information Factory), 28–29\nCIO (chief information offi  cer), 377\nclaim transactions (insurance case study), \n390\nclaim accumulating snapshot, 393–394\njunk dimensions and, 392\nperiodic snapshot, 395–396\ntimespan accumulating snapshot, 394–395\nclass of service ﬂ own dimension (airline case \nstudy), 319–320\ncleaning and conforming, ETL systems, 450\naudit dimension assembler, 460\nconforming system, 461–463\ndata cleansing system, 456\nquality event responses, 458\nquality screens, 457–458\ndata quality improvement, 455–456\ndeduplication system, 460–461\nerror event schema, 458–460\nclickstream data, 353–354\ndimensional models, 357–358\naggregate fact tables, 366–367\ncustomer, 361–362\ndate, 361–362\nevent dimension, 359\nGA (Google Analytics), 367\npage dimension, 358–359\npage event fact table, 363–366\nreferral dimension, 360\nsession dimension, 359–360\nsession fact table, 361–363\nstep dimension, 366\ntime, 361–362\nsession IDs, 355–356\n",
      "content_length": 2973,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 584,
      "content": "Index  \n548\nvisitor identiﬁ cation, 356–357\nvisitor origins, 354–355\nweb retailer bus matrix integration, \n368–370\ncollaborative design workshops, 38\ncolumn screens, 457\ncomments, survey questionnaire (HR), 278\ncommon dimensions, 130\ncompliance, ETL system, 445\ncompliance manager, ETL system, 493–495\ncomposite keys, 12\ncomputer resources, big data and, 537\nconformed dimensions, 51, 130, 304\nagile movement and, 137–138\ndrill across, 130–131\ngrain, 132\nidentical, 131–132\ninsurance case study, 386\nlimited conformity, 135\nshrunken on bus matrix, 134\nshrunken rollup dimensions, 132\nshrunken with row subset, 132–134\nconformed facts, 42, 139\ninsurance case study, 386\ninventory case study, 138–139\nconforming system, ETL system, 461–463\nconsistency\nadaptability, 4\ngoals, 3\nconsolidated fact tables, 45\naccounting case study, 224–225\ncontacts, bridge tables, 248\ncontribution amount (P&L statement), 191\ncorrectly weighted reports, 288\ncost, activity-based costing measures, 184\nCOUNT DISTINCT, 243\ncountry-speciﬁ c calendars as outriggers, \n321–323\ncourse registrations (education case study), \n330\nCRM (customer relationship management), \n229\nanalytic, 231–233\nbridge tables\ncustomer contacts, 248\nmultivalued, 245–246\nsparse attributes, 247\ncomplex customer behavior, 249–251\ncustomer data integration, 256\nmultiple customer dimension conformity, \n258–259\nsingle customer dimension, 256–258\ncustomer dimension and, 233\naddresses, 233–236\naddresses, international, 236–238\ncounts with Type 2, 243\ndates, 238\nfacts, aggregated, 239–240\nhierarchies, 244–245\nnames, 233–236\nnames, international, 236–238\noutriggers, low cardinality attribute set \nand, 243–244\nscores, 240–243\nsegmentation, 240–243\nfacts\nabnormal scenario indicators, 255–256\nsatisfaction indicators, 254–255\ntimespan, 252–254\nlow latency data, 260–261\noperational, 231–233\noverview, 230–231\nsocial media and, 230\nstep dimension, sequential behavior, \n251–252\ncurrency, multiple\nfact tables, 60\nG/L (general ledger), 206\norder transactions, 182–184\ncurrent date attributes, dimension tables, \n82–83\ncustomer contacts, bridge tables, 248\ncustomer dimension, 158, 174–175\nclickstream data, 361–362\nCRM and, 233\naddresses, 233–236\naddresses, international, 236–238\ncounts with Type 2, 243\ndates, 238\nfacts, aggregated, 239–240\nhierarchies, 244–245\nnames, 233–236\nnames, international, 236–238\noutriggers, low cardinality attribute set \nand, 243–244\nscores, 240–243\nsegmentation, 240–243\nfactless fact tables, 176\nhierarchies, 174–175\nmultiple, partial conformity, 258–259\nsingle, 256–258\nsingle versus multiple dimension tables, \n175–176\ncustomer matching, 257\ncustomer relationship management. case \nstudy. See CRM, 230\nD\ndata architect/modeler, 409\ndata bags, 530\ndatabase administrator, 409\n",
      "content_length": 2757,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 585,
      "content": "Index 549\ndata cleansing system, ETL system, 456\nquality event responses, 458\nquality screens, 457–458\ndata compression, ETL system, 454\ndata governance, 135–136\nbig data best practices, 541\ndimensionalizing, 541\nprivacy, 541–542\nbusiness-driven governance, 136–137\nobjectives, 137\ndata handlers, late arriving, 478–479\ndata highway planning, 533–534\ndata integration\nconformed dimensions, 130–138\nCRM and, 256\nmultiple customer dimension conformity, \n258–259\nsingle customer dimension, 256–258\nETL system, 444–446\nMDM (master data management), 256\nstructure/unstructured data, 539\nvalue chain integration, 111–112\ndata latency, ETL system, 447\ndata mart, independent data mart \narchitecture, 26–27\ndata mining\nDW/BI system and, 242–243\nnull tracking, 92\ndata modeling, big data best practices\ndata structure declaration, 540\ndata virtualization, 540\ndimension anchoring, 539\nintegrating sources and conformed \ndimensions, 538\nname-value pairs, 540\nSCDs (slowly changing dimensions), 539\nstructured/unstructured data integration, \n539\nthinking dimensionally, 538\ndata models, packaged, 270–271\ndata proﬁ ling\nETL system, 450–451\ntools, 433\ndata propagation, ETL system, 482\ndata quality\ncheckpoints, 516\nETL system, 445\nimprovement, 455–456\nplanning, big data and, 535\ndata steward, 408\ndata structure, analysis time, 540\ndata value, big data and, 535\ndata virtualization, big data and, 540\ndata warehousing versus operational \nprocessing, 2\ndate dimension, 79–81, 284, 302\ncalendar date, 48\nclickstream data, 361–362\ncurrent date attributes, 82–83\nﬁ xed time series buckets and, 302–303\nhealthcare case study, 345\npopulating, 508\nrelative date attributes, 82–83\nrole playing, 171\nsmart keys, 101–102\ntextual attributes, 82\ntime-of-day, 83\ndates\nCRM and, customer dimension, 238\ndimension tables, 89\ntimespan fact tables, 252–254\ntransaction fact table, 170–171\nforeign key, 170\nrole playing, 171\ndate/time\nGMT (Greenwich Mean Time), 323\ntime zones, multiple, 323\nUTC (Coordinated Universal Time, 323\ndate/time dimensions, 470\ndate/time stamp dimensions, 284\ndeal dimensions, 177–178\ndecision-making goals, 4\ndecodes, dimensions, 303–304\ndecoding production codes, 504\ndeduplication system, 460–461\ndegenerate dimension, 47, 284, 303\ninsurance case study, 383\norder number, 178–179\nretail sales case study, 93–94\nsurrogate keys, 101\ntelecommunications case study, 303\ntransaction numbers, 93–94\ndemand planning, 142\ndemographics dimension, 291\nsize, 159\ndenormalized ﬂ attened dimensions, 47\ndependency analysis, 495\ndependency, ETL, 490–491\ndeployment\nLifecycle, 424\nOLAP, 9\nderived facts, 77–78\ndescriptions, dimensions, 303–304\ndescriptive context, dimensions for, 40\ndestination airport dimension (airline case \nstudy), 320–321\ndetailed implementation bus matrix, 53, 390\ndetailed table design documentation, \n437–439\ndiagnosis dimension (healthcare case study), \n345–347\n",
      "content_length": 2876,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 586,
      "content": "Index  \n550\ndiff  compare, CDC (change data capture), \n452\ndimensional modeling, 7\n3NF (third normal form) models, 7–8\n4-step design process\nbusiness process, 70–71\ndimensions, 72\nfacts, 72\ngrain, 71\natomic grain data, 17\nbeneﬁ ts of thinking dimensionally, 32–33\nbusiness processes, 300\nbusiness representatives, 431–432\ncalendar coordination, 433–434\nclickstream data, 357–367\ndata proﬁ ling tools, 433\ndesign\nbubble chart, 435–436\ndetailed model development, 436–439\ndocumentation ﬁ nalization, 441\nvalidation, 440–441\ndimension tables, 13\nattributes, 13–14\nhierarchical relationships, 15\nsnowﬂ aking, 15\nextensibility, 16\nfacts\nadditive facts, 11\ncomposite keys, 12\nFK (foreign keys), 12\ngrains, 10\nnumeric facts, 11\ntextual facts, 12\nfact tables, 10–12\ngrain categories, 12\nfundamentals\nbusiness processes, 39\nbusiness requirement gathering, 37–38\ncollaborative workshops, 38\ndata realities gathering, 37–38\ndescriptive context, 40\nfacts, 40\nfour-step dimensional design process, 38\ngrain, 39\nmodel extensions, 41\nstar schemas, 40\nLifecycle data track, 420\nmistakes to avoid, 397–401\nmyths, 30\ndepartmental versus enterprise, 31\nintegration, 32\npredictable use, 31–32\nscalability, 31\nsummary data, 30\nnaming conventions, 433\nOLAP (online analytical processing) cube, 8\ndeployment considerations, 9\noverview, 429–131\nparticipant identiﬁ cation, 431–432\nreports, 17\nsimplicity in, 16\nsources, 300\nstar schemas, 8\nterminology, 15\ntools, 432\ndimensional thinking, big data and, 538\ndimension manager system, 479–480\ndimensions\nanchoring, big data and, 539\nattributes, 514\naggregated facts as, 64\nbridge tables, CRM and, 247\nchanges, 514\ndetailed dimension model, 437\nexpiration, 266\nﬂ ags, 48\nindicators, 48\nnull, 48, 92\nnumeric values as, 59\npathstring, ragged/variable depth \nhierarchies, 57\nproduct dimensions, 132\nSCD type 3 (add new attribute), 154–156\nSee also attributes, 48\naudit dimension, 66, 192–193, 284\nassembler, 460\ninsurance case study, 383\naverage number in model, 284\ncalendar date, 48\ncausal, 89–90, 284\nchange reasons, 266–267\nclass of service ﬂ own (airline case study), \n319–320\nconformed, 51, 130, 304\nagile movement and, 137–138\ndrill across, 130–131\ngrain, 132\nidentical, 131–132\ninsurance case study, 386\nlimited conformity, 135\nshrunken, bus matrix and, 134\nshrunken rollup dimensions, 132\nshrunken with row subset, 132–134\ncustomer dimension, 158, 174–175\nconformity, 258–259\nCRM and, 233–245\nfactless fact tables, 176\nhierarchies, 174–175\nsingle, 256–258\nsingle versus multiple dimension tables, \n175–176\ndata governance, big data and, 541\ndate dimension, 48, 284, 302\n",
      "content_length": 2599,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 587,
      "content": "Index 551\nﬁ xed time series buckets and, 302–303\nhealthcare case study, 345\npopulating, 508\nrole playing, 171\ndate/time stamp, 284\ndeal dimension, 177–178\ndecodes, 303–304\ndegenerate, 47, 284, 303\norder number, 178–179\ndemographic, 291\nsize, 159\ndenormalized ﬂ attened, 47\ndescriptions, 303–304\ndestination airport (airline case study), \n320–321\ndetailed dimension model, 437\ndiagnosis (healthcare case study), 345–347\ndimensional design models, 72\ndrilling across, 51\nevent dimension, clickstream data, 359\ngeneric, abstract, 66\ngeographic location, 310\ngranularity, hierarchies and, 301–302\nhierarchies\nﬁ xed depth position hierarchies, 56\nragged/variable depth with hierarchy \nbridge tables, 57\nragged/variable depth with pathstring \nattributes, 57\nslightly ragged/variable depth, 57\nhot swappable, 66, 296\nhousehold, 286–287\ninsurance case study, 380\ndegenerate dimension, 383\nmini-dimensions, 381–382\nmultivalued dimensions, 382\nnumeric attributes, 382\nSCDs (slowly changing dimensions), \n380–381\njunk dimensions, 49, 179–180, 284\nkeys, natural, 162\nlate arriving, 67\nlow cardinality, insurance case study, 383\nmeasure type, 65\nhealthcare case study, 349–350\nmini-dimensions, 289–290\nbridge tables, 290–291\ninsurance case study, 381–382\ntype 5 SCD and, 160\nmultivalued\nbridge table builder, 477–478\nbridge tables and, 63\ninsurance case study, 382–388\nweighting, 287–289\norigin (airline case study), 320–321\noutrigger, 50\npage dimension, clickstream data, 358–359\npassenger (airline case study), 314\nproduct dimension\ncharacteristics, 172–173\noperational product master, 173\norder transactions, 172–173\nrapidly changing monster dimension, 55\nreferral dimension, clickstream data, 360\nretail sales case study, 76\nrole-playing, 284\nsales channel, airline case study, 315\nservice level performance, 188–189\nsession dimension, clickstream data, \n359–360\nshrunken, 51\nshrunken rollup, 132\nspecial dimensions manager, ETL systems, \n470\ndate/time dimensions, 470\njunk dimensions, 470\nmini-dimensions, 471\nshrunken subset, 472\nstatic, 472\nuser-maintained, 472–473\nstatic dimension, population, 508\nstatus, 284\nstep dimension, 65\nclickstream data, 366\nsequential behavior, 251–252\nstudent (education case study), 330\nterm (education case study), 330\ntext comments, 65\ntoo few, 283–286\ntransaction proﬁ le dimension, 49, 179\ntransformations\ncombine from separate sources, 504\ndecode production codes, 504\nrelationship validation, 504–505\nsimple data, 504\nsurrogate key assignment, 506\nvalue chain, 52\ndimension surrogate keys, 46\ndimension tables, 13\nattributes, 13–14\ncalendar date dimensions, 48\nchanged rows, 513–514\ndate dimension, 79–81\ncurrent date attributes, 82–83\nsmart keys, 101–102\ntextual attributes, 82\ntime-of-day, 83\ndates, 89\ndegenerate dimensions, 47\nsurrogate keys, 101\ntransaction numbers, 93–94\n",
      "content_length": 2809,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 588,
      "content": "Index  \n552\ndenormalized ﬂ attened dimensions, 47\ndrilling down, 47\ndurable keys, 46\nextracts, 513\nfact tables, centipede, 108–109\nﬂ ags, 48, 82\nhierarchical relationships, 15\nhierarchies, multiple, 48, 88–89\nhistoric data population, 503–506\nholiday indicator, 82\nindicators, 48, 82\njunk dimensions, 49\nloading, 506–507\nloading history, 507–508\nnatural keys, 46, 98–101\nnew rows, 513–514\nnull attributes, 48\noutrigger dimensions, 50\noutriggers, 106–107\nproduct dimension, 83–84\nattributes with embedded meaning, 85\ndrilling down, 86–87\nmany-to-one hierarchies, 84–85\nnumeric values, 85–86\npromotion dimension, 89–91\nnull items, 92\nrole-playing, 49\nsnowﬂ aking, 15, 50, 104–106\nstore dimension, 87–89\nstructure, 46\nsupernatural keys, 46, 101\nsurrogate keys, 46, 98–100\ntransaction proﬁ le dimensions, 49\nweekday indicator, 82\ndimension terminology, 15\ndimension-to-dimension table joins, 62\ndocumentation\ndetailed table design, 437–439\ndimensional modeling, 441\nETL development, 502–503\nsandbox source system, 503\nLifecycle architecture requirements, 417\nLifecycle business requirements, 414\ndraft design\nexercise discussion, 306–308\nremodeling existing structures, 309\ndrill across, 51, 130–131\ndrill down, 47, 86–87\nETL development, 500\nhierarchies, 501\ntable schematics, 501\nG/L (general ledger) hierarchy, 209\nmanagement hierarchies, 273–274\ndual date/time stamps, 254\ndual type 1 and type 2 dimensions \n(SCD type 7), 56\nduplication, deduplication system, 460–461\ndurable keys, 46\nsupernatural keys, 101\nDW/BI, 1\nalternative architecture, 26–29\ndata mining and, 242–243\ngoals, 3\ninternational goals, 237–238\nKimball architecture, 18\nBI applications, 22\nETL (extract, transformation, and load) \nsystem, 19–21\nhybrid hub-and-spoke Kimball, 29\noperational source systems, 18\npresentation area, 21–22\nrestaurant metaphor, 23–26\npublishing metaphor for DW/BI managers, \n5–7\nsystem users, 2\ndynamic value bands, 64, 291\nE\necosystems, big data and, 534\ncase study, 325–326\neducation\naccumulating snapshot fact table, 326–329\nadditional uses, 336\nadmissions events, 330\napplicant pipeline, 326–329\nattendance, 335\nbus matrix, 325–326\nchange tracking, 330\ncourse registrations, 330–333\nfacility use, 334\ninstructors, multiple, 333\nmetrics, artiﬁ cial count, 331–332\nresearch grant proposal, 329\nstudent dimension, 330–332\nterm dimension, 330\neff ective date, SCD type 2, 152–153\nEHR (electronic health record), 341\nelectronic commerce case study, 353–372\nembedded managers key (HR), 272–273\nembedding attribute meaning, 85\nemployee hierarchies, recursive, 271–272\nemployee proﬁ les, 263–265\ndimension change reasons, 266–267\neff ective time, 265–266\nexpiration, 265–266\nfact events, 267\ntype 2 attributes, 267\nEMRs (electronic medical records), \nhealthcare case study, 341, 348\n",
      "content_length": 2773,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 589,
      "content": "Index 553\nenterprise data warehouse bus architecture, \n22, 52, 123–125\nenterprise data warehouse bus matrix, 52, \n125–126\ncolumns, 126\nhierarchy levels, 129\ncommon mistakes, 128–129\nopportunity/stakeholder matrix, 127\nprocurement, 142–143\nretroﬁ tting existing models, 129–130\nrows\nnarrowly deﬁ ned, 128\noverly encompassing, 128\noverly generalized, 129\nshrunken conformed dimensions, 134\nuses, 126–127\nERDs (entity-relationship diagrams), 8\nerror event schema, ETL system, 458–460\nerror event schemas, 68\nETL (extract, transformation, and load) \nsystem, 19–21, 443\narchiving, 447–448\nBI, delivery, 448\nbusiness needs, 444\ncleaning and conforming, 450\naudit dimension assembler, 460\nconforming system, 461–463\ndata cleansing system, 456–458\ndata quality, improvement, 455–456\ndeduplication system, 460–461\nerror event schema, 458–460\ncompliance, 445\ndata integration, 446\ndata latency, 447\ndata propagation manager, 482\ndata quality, 445\ndelivering, 450, 463\naggregate builder, 481\ndimension manager system, 479–480\nfact provider system, 480–481\nfact table builders, 473–475\nhierarchy manager, 470\nlate arriving data handler, 478–479\nmultivalued dimension bridge table \nbuilder, 477–478\nSCD manager, 464–468\nspecial dimensions manager, 470–473\nsurrogate key generator, 469–470\nsurrogate key pipeline, 475–477\ndesign, 443\nLifecycle data track, 422\ndeveloper, 409\ndevelopment, 498\nactivities, 500\naggregate tables, 519\ndefault strategies, 500\ndrill down, 500–501\nhigh-level plan, 498\nincremental processing, 512–519\nOLAP loads, 519\none-time historic load data, 503–512\nspeciﬁ cation document, 502–503\nsystem operation and automation, 520\ntools, 499\nETL architect/designer, 409\nextracting, 450\nCDC (change data capture), 451–453\ndata proﬁ ling, 450–451\nextract system, 453–455\nlegacy licenses, 449\nlineage, 447–448\nmanaging, 450, 483\nbackup system, 485–495\njob scheduler, 483–484\nOLAP cube builder, 481–482\nprocess overview, 497\nsecurity, 446\nskills, 448\nsubsystems, 449\nevent dimension, clickstream data, 359\nexpiration date, type 2 SCD, 152–153\nextended allowance amount (P&L \nstatement), 190\nextended discount amount (P&L statement), \n190\nextended distribution cost (P&L statement), \n191\nextended ﬁ xed manufacturing cost (P&L \nstatement), 190\nextended gross amount (P&L statement), \n189\nextended net amount (P&L statement), 190\nextended storage cost (P&L statement), 191\nextended variable manufacturing cost (P&L \nstatement), 190\nextensibility in dimensional modeling, 16\nextracting, ETL systems, 450\nCDC (change data capture), 451\naudit columns, 452\ndiff  compare, 452\nlog scraping, 453\nmessage queue monitoring, 453\ntimed extracts, 452\ndata proﬁ ling, 450–451\nextract system, 453–455\nextraction, 19\nextract system, ETL system, 453–455\nF\nfact extractors, 530\nbig data and, 534\n",
      "content_length": 2778,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 590,
      "content": "Index  \n554\nfactless fact tables, 44, 97–98, 176\naccidents (insurance case study), 396\nadmissions (education case study), 330\nattendance (education case study), 335\ncourse registration (education case study), \n330–333\nfacility use (education case study), 334\norder management case study, 176\nfact provider system\nETL system, 480–481\nfacts, 10, 12, 72, 79\nabnormal scenario indicators, 255–256\naccumulating snapshots, 44, 121–122, \n326–329\nadditive facts, 11, 42\naggregate, 45\nas attributes, 64\nclickstream data, 366–367\nCRM and customer dimension, 239–240\nallocated facts, 60\nallocating, 184–186\nbehavior tags, 241\nbudget, 210\nbuilders, ETL systems, 473–475\ncentipede, 58, 108–109\ncompliance-enabled, 494\ncomposite keys, 12\nconformed, 42, 138–139\nconsolidated, 45\ncurrency, multiple, 60\nderived, 77–78\ndetailed dimension model, 437\ndimensional modeling process and, 40\ndrill across, 130–131\nemployee proﬁ les, 267\nenhanced, 115–116\nFK (foreign keys), 12\ngrains, 10, 12\ngranularity, airline bus matrix, 312–315\nheader/line fact tables, 59\nhistoric, 508\nincremental processing, 515, 519\ninvoice, 187–188\njoins, avoiding, 259–260\nlag/duration facts, 59\nlate arriving, 62\nloading, 512\nmini-dimension demographics key, 158\nmultiple units of measure, 61\nnon-additive, 42, 78\nnormalization, order transactions, 169–170\nnull, 42, 92\nnumeric facts, 11\nnumeric values, 59, 85–86\npage event, clickstream data, 363–366\npartitioning, smart keys, 102\npay-in-advance, insurance case study, \n386–387\nperiodic snapshots, 43, 120–122\npolicy transactions (insurance case study), \n383\nproﬁ tability, 370–372\nproﬁ t and loss, 189–192\nproﬁ t and loss, allocations and, 60\nreal-time, 68\nreferential integrity, 12\nreports, 17\nretail sales case study, identifying, 76–79\nsatisfaction indicators, 254–255\nsemi-additive, 42, 114–115\nservice level performance, 188–189\nsession, clickstream data, 361–363\nset diff erence, 97\nshrunken rollup dimensions, 132\nsingle granularity and, 301\nsnapshot, complementary procurement, \n147\nstructure, 41–42\nsubtype, 67, 293–295\nsupertype, 67, 293–295\nsurrogate keys, 58, 102–103\ntextual facts, 12\nterminology, 15\ntime-of-day, 83\ntimespan, 252–254\ntimespan tracking, 62\ntransactions, 43, 120\ndates, 170–171\nsingle versus multiple, 143–145\ntransformations, 509–512\nvalue banding, 291–292\nyear-to-date, 206\nYTD (year-to-date), 61\nfact-to-fact joins, avoiding with multipass \nSQL, 61\nfeasibility in Lifecycle planning, 407\nﬁ nancial services case study, 281\nbus matrix, 282\ndimensions\nhot-swappable, 296\nhousehold, 286–287\nmini-dimensions, 289–291\nmultivalued, weighting, 287–289\ntoo few, 283–286\nfacts, value banding, 291–292\nheterogeneous products, 293–295\nOLAP, 226\nuser perspective, 293\nﬁ nancial statements (G/L), 209–210\nﬁ scal calendar, G/L (general ledger), 208\nﬁ xed depth position hierarchies, 56, 214\nﬁ xed time series buckets, date dimensions \nand, 302–303\n",
      "content_length": 2873,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 591,
      "content": "Index 555\nFK (foreign keys). See foreign keys (FK), 12\nﬂ ags\nas textual attributes, 48\ndimension tables, 82\njunk dimensions and, 179–180\nﬂ attened dimensions, denormalized, 47\nﬂ exible access to information, 407\nforeign keys (FK)\ndemographics dimensions, 291\nfact tables, 12\nmanagers employee key as, 271–272\nmini-dimension keys, 158\nnull, 92\norder transactions, 170\nreferential integrity, 12\nforum, Lifecycle business requirements, \n410–411\nfrequent shopper program, retail sales \nschema, 96\nFROM clause, 18\nG\nGA (Google Analytics), 367\ngeneral ledger. See G/L (general ledger), 203\ngeneric dimensions, abstract, 66\ngeographic location dimension, 310\nG/L (general ledger), 203\nchart of accounts, 203–204\ncurrencies, multiple, 206\nﬁ nancial statements, 209–210\nﬁ scal calendar, multiple, 208\nhierarchies, drill down, 209\njournal entries, 206–207\nperiod close, 204–206\nperiodic snapshot, 203\nyear-to-date facts, 206\nGMT (Greenwich Mean Time), 323\ngoals of DW/BI, 3–4\nGoogle Analytics (GA), 367\ngovernance\nbusiness-driven, 136–137\nobjectives, 137\ngrain, 39\naccumulating snapshots, 44\natomic grain data, 74\nbudget fact table, 210\nconformed dimensions, 132\ndeclaration, 71\nretail sales case study, 74–75\ndimensions, hierarchies and, 301–302\nfact tables, 10\naccumulating snapshot, 12\nperiodic snapshot, 12\ntransaction, 12\nperiodic snapshots, 43\nsingle, facts and, 301\ntransaction fact tables, 43\ngranularity, 300\nGROUP BY clause, 18\ngrowth\nLifecycle, 425–426\nmarket growth, 90\nH\nHadoop, MapReduce/Hadoop, 530\nHCPCS (Healthcare Common Procedure \nCoding System), 342\nHDFS (Hadoop distributed ﬁ le system), 530\nheadcount periodic snapshot, 267–268\nheader/line fact tables, 59\nheader/line patterns, 181–182, 186\nhealthcare case study, 339–340\nbilling, 342–344\nclaims, 342–344\ndate dimension, 345\ndiagnosis dimension, 345–347\nEMRs (electronic medical records), 341, \n348\nHCPCS (Healthcare Common Procedure \nCoding System), 342\nHIPAA (Health Insurance Portability and \nAccountability Act), 341\nICD (International Classiﬁ cation of \nDiseases), 342\nimages, 350\ninventory, 351\nmeasure type dimension, 349–350\npayments, 342–344\nretroactive changes, 351–352\nsubtypes, 347–348\nsupertypes, 347–348\ntext comments, 350\nheterogeneous products, 293–295\nhierarchies\naccounting case study, 214–223\ncustomer dimension, 174–175, 244–245\ndimension granularity, 301–302\ndimension tables, multiple, 88–89\ndrill down, ETL development, 501\nemployees, 271–272\nETL systems, 470\nﬁ xed-depth positional hierarchies, 56\nG/L (general ledger), drill down, 209\nmanagement, drilling up/down, 273–274\nmany-to-one, 84–85\nmatrix columns, 129\nmultiple, 48\nnodes, 215\n",
      "content_length": 2623,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 592,
      "content": "Index  \n556\nragged/variable depth, 57\nslightly ragged/variable depth, 57\ntrees, 215–216\nhigh performance backup, 485\nHIPAA (Health Insurance Portability and \nAccountability Act), 341\nhistoric fact tables\nextracts, 508\nstatistics audit, 508\nhistoric load data, ETL development, \n503–512\ndimension table population, 503–506\nholiday indicator, 82\nhot response cache, 238\nhot swappable dimensions, 66, 296\nhousehold dimension, 286–287\nHR (human resources) case study, 263\nbus matrix, 268–269\nemployee proﬁ les, 263–265\ndimension change reasons, 266–267\neff ective time, 265–266\nexpiration, 265–266\nfact events, 267\ntype 2 attributes, 267\nhierarchies\nmanagement, 273–274\nrecursive, 271–272\nmanagers key\nas foreign key, 271–272\nembedded, 272–273\npackaged analytic solutions, 270–271\npackaged data models, 270–271\nperiodic snapshots, headcount, 267–268\nskill keywords, 274\nbridge, 275\ntext string, 276–277\nsurvey questionnaire, 277\ntext comments, 278\nHTTP (Hyper Text Transfer Protocol), \n355–356\nhub-and-spoke CIF architecture, 28–29\nhub-and-spoke Kimball hybrid architecture, \n29\nhuman resources management case study. \nSee HR (human resources), 263\nhybrid hub-and-spoke Kimball architecture, \n29\nhybrid techniques, SCDs, 159, 164\nSCD type 5 (add mini-dimension and type \n1 outrigger), 55, 160\nSCD type 6 (add type 1 attributes to type 2 \ndimension), 56, 160–162\nSCD type 7 (dual type 1 and type 2 \ndimension), 56, 162–163\nhyperstructured data, 530\nI\nICD (International Classiﬁ cation of \nDiseases), 342\nidentical conformed dimensions, 131–132\nimages, healthcare case study, 350\nimpact reports, 288\nincremental processing, ETL system \ndevelopment, 512\nchanged dimension rows, 513–514\ndimension attribute changes, 514\ndimension table extracts, 513\nfact tables, 515–519\nnew dimension rows, 513–514\nin-database analytics, big data and, 537\nindependent data mart architecture, 26–27\nindicators\nabnormal, fact tables, 255–256\nas textual attributes, 48\ndimension tables, 82\njunk dimensions and, 179–180\nsatisfaction, fact tables, 254–255\nInmon, Bill, 28–29\ninsurance case study, 375–377\naccidents, factless fact tables, 396\naccumulating snapshot, complementary \npolicy, 384–385\nbus matrix, 378–389\ndetailed implementation, 390\nclaim transactions, 390\nclaim accumulating snapshot, 393–394\njunk dimensions and, 392\nperiodic snapshot, 395–396\ntimespan accumulating snapshot, \n394–395\nconformed dimensions, 386\nconformed facts, 386\ndimensions, 380\naudit, 383\ndegenerate, 383\nlow cardinality, 383\nmini-dimensions, 381–382\nmultivalued, 382, 388\nSCDs (slowly changing dimensions), \n380–381\nNAICS (North American Industry \nClassiﬁ cation System), 382\nnumeric attributes, 382\npay-in-advance facts, 386–387\nperiodic snapshot, 385\npolicy transactions, 379–380, 383\npremiums, periodic snapshot, 386–388\nSIC (Standard Industry Classiﬁ cation), 382\nsupertype/subtype products, 384, 387\nvalue chain, 377–378\ninteger keys, 98\nsequential surrogate keys, 101\n",
      "content_length": 2930,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 593,
      "content": "Index 557\nintegration\nconformed dimensions, 130–138\ncustomer data, 256\ncustomer dimension conformity, 258–259\nsingle customer dimension, 256, 257, 258\ndimensional modeling myths, 32\nvalue chain, 122–123\ninternational names/addresses, customer \ndimension, 236–238\ninterviews, Lifecycle business requirements, \n412–413\ndata-centric, 413–414\ninventory case study, 112–114\naccumulating snapshot, 118–119\nfact tables, enhanced, 115–116\nperiodic snapshot, 112–114\nsemi-additive facts, 114–115\ntransactions, 116–118\ninventory, healthcare case study, 351\ninvoice transaction fact table, 187–188\nJ\njob scheduler, ETL systems, 483–484\njob scheduling, ETL operation and \nautomation, 520\njoins\ndimension-to-dimension table joins, 62\nfact tables, avoiding, 259–260\nmany-to-one-to-many, 259–260\nmultipass SQL to avoid fact-to-fact joins, 61\njournal entries (G/L), 206–207\njunk dimensions, 49, 179–180, 284\nairline case study, 320\nETL systems, 470\ninsurance case study, 392\norder management case study, 179–180\njustiﬁ cation for program/project planning, \n407\nK\nkeys\ndimension surrogate keys, 46\ndurable, 46\nforeign, 92, 291\nmanagers key (HR), 272–273\nnatural keys, 46, 98–101, 162\nsupernatural keys, 101\nsmart keys, 101–102\nsubtype tables, 294–295\nsupernatural, 46\nsupertype tables, 294–295\nsurrogate, 58, 98–100, 303\nassigning, 506\ndegenerate dimensions, 101\nETL system, 475–477\nfact tables, 102–103\ngenerator, 469–470\nlookup pipelining, 510–511\nkeywords, skill keywords, 274\nbridge, 275\ntext string, 276–277\nKimball Dimensional Modeling Techniques. \nSee dimensional modeling\nKimball DW/BI architecture, 18\nBI applications, 22\nETL (extract, transformation, and load) \nsystem, 19–21\nhub-and-spoke hybrid, 29\npresentation area, 21–22\nrestaurant metaphor, 23–26\nsource systems, operational source systems, \n18\nKimball Lifecycle, 404\nDW/BI initiative and, 404\nKPIs (key performance indicators), 139\nL\nlag calculations, 196–197\nlag/duration facts, 59\nlate arriving data handler, ETL system, \n478–479\nlate arriving dimensions, 67\nlate arriving facts, 62\nlaunch, Lifecycle business requirements, 412\nLaw of Too, 407\nlegacy environments, big data management, \n532\nlegacy licenses, ETL system, 449\nLifecycle\nBI applications, 406\ndevelopment, 423–424\nspeciﬁ cation, 423\nbusiness requirements, 405, 410\ndocumentation, 414\nforum selection, 410–411\ninterviews, 412–413\ninterviews, data-centric, 413–414\nlaunch, 412\nprioritization, 414–415\nrepresentatives, 411–412\nteam, 411\ndata, 405\ndimensional modeling, 420\nETL design/development, 422\nphysical design, 420–422\ndeployment, 424\ngrowth, 425–426\nmaintenance, 425–426\npitfalls, 426\n",
      "content_length": 2604,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 594,
      "content": "Index  \n558\nproducts\nevaluation matrix, 419\nmarket research, 419\nprotoypes, 419\nprogram/project planning, 405–406\nbusiness motivation, 407\nbusiness sponsor, 406\ndevelopment, 409–410\nfeasibility, 407\njustiﬁ cation, 407\nplanning, 409–410\nreadiness assessment, 406–407\nscoping, 407\nstaffi  ng, 408–409\ntechnical architecture, 405, 416–417\nimplementation phases, 418\nmodel creation, 417\nplan creation, 418\nrequirements, 417\nrequirements collection, 417\nsubsystems, 418\ntask force, 417\nlift, promotion, 89\nlights-out operations, backup, 485\nlimited conformed dimensions, 135\nlineage analysis, 495\nlineage, ETL system, 447–448, 490–491\nloading fact tables, incremental, 517\nlocalization, 237, 324\nlocation, geographic location dimension, 310\nlog scraping, CDC (change data capture), \n453\nlow cardinality dimensions, insurance case \nstudy, 383\nlow latency data, CRM and, 260–261\nM\nmaintenance, Lifecycle, 425–426\nmanagement\nETL systems, 450, 483\nbackup system, 485–495\njob scheduler, 483–484\nmanagement best practices, big data\nanalytics, 531\nlegacy environments, 532\nsandbox results, 532–533\nsunsetting and, 533\nmanagement hierarchies, drilling up/down, \n273–274\nmanagers, publishing metaphor, 5–7\nmany-to-one hierarchies, 84–85\nmany-to-one relationships, 175–176\nmany-to-one-to-many joins, 259–260\nMapReduce/Hadoop, 530\nmarket growth, 90\nmaster dimensions, 130\nMDM (master data management), 137, 256, \n446\nmeaningless keys, 98\nmeasurement, multiple, 61\nmeasure type dimension, 65\nhealthcare case study, 349–350\nmessage queue monitoring, CDC (change \ndata capture), 453\nmetadata coordinator, 409\nmetadata repository, ETL system, 495\nmigration, version migration system, ETL, \n488\nmilestones, accumulating snapshots, 121\nmini-dimension and type 1 outrigger (SCD \ntype 5), 160\nmini-dimensions, 289–290\nbridge tables, 290–291\nETL systems, 471\ninsurance case study, 381–382\ntype 4 SCD, 156–159\nmodeling\nbeneﬁ ts of thinking dimensionally, 32–33\ndimensional, 7–12\natomic grain data, 17\ndimension tables, 13–15\nextensibility, 16\nmyths, 30–32\nreports, 17\nsimplicity in, 16\nterminology, 15\nmultipass SQL, avoiding fact-to-fact table \njoins, 61\nmultiple customer dimension, partial \nconformity, 258–259\nmultiple units of measure, 61, 197–198\nmultivalued bridge tables\nCRM and, 245–246\ntime varying, 63\nmultivalued dimensions\nbridge table builder, 477–478\nbridge tables and, 63\nCRM and, 245–247\neducation case study, 325–333\nﬁ nancial services case study, 287–289\nhealthcare case study, 345–348\nHR (human resources) case study, 274–275\ninsurance case study, 382–388\nweighting factors, 287–289\nmyths about dimensional modeling, 30\ndepartmental versus enterprise, 31\nintegration, 32\npredictable use, 31–32\nscalability, 31\nsummary data, 30\n",
      "content_length": 2721,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 595,
      "content": "Index 559\nN\nnames\nASCII, 236\nCRM and, customer dimension, 233–238\nUnicode, 236–238\nname-value pairs, 540\nnaming conventions, 433\nnatural keys, 46, 98–101, 162\nsupernatural keys, 101\nNCOA (national change of address), 257\nnodes (hierarchies), 215\nnon-additive facts, 42, 78\nnon-natural keys, 98\nnormalization, 28, 301\nfacts\ncentipede, 108–109\norder transactions, 169–170\noutriggers, 106–107\nsnowﬂ aking, 104–106\nnormalized 3NF structures, 8\nnull attributes, 48\nnull fact values, 509\nnull values\nfact tables, 42\nforeign keys, 92\nnumber attributes, insurance case study, 382\nnumeric facts, 11\nnumeric values\nas attributes, 59, 85–86\nas facts, 59, 85–86\nO\noff -invoice allowance (P&L) statement, 190\nOLAP (online analytical processing) cube, \n8, 40\naccounting case study, 226\naccumulating snapshots, 121–122\naggregate, 45\ncube builder, ETL system, 481–482\ndeployment considerations, 9\nemployee data queries, 273\nﬁ nancial schemas, 226\nLifecycle data physical design, 421\nloads, ETL system, 519\nwhat didn’t happen, 335\none-to-one relationships, 175–176\noperational processing versus data \nwarehousing, 2\noperational product master, product \ndimensions, 173\noperational source systems, 18\noperational system users, 2\nopportunity/stakeholder matrix, 53, 127\norder management case study, 167–168\naccumulating snapshot, 194–196\ntype 2 dimensions and, 196\nallocating, 184–186\naudit dimension, 192–193\nbus matrix, 168\ncurrency, multiple, 182–184\ncustomer dimension, 174–175\nfactless fact tables, 176\nsingle versus multiple dimension tables, \n175–176\ndate, 170–171\nforeign keys, 170\nrole playing, 171\ndeal dimension, 177–178\ndegenerate dimension, order number and, \n178–179\nfact normalization, 169–170\nheader/line patterns, 181–186\njunk dimensions, 179–180\nproduct dimension, 172–173\norder number, degenerate dimensions, \n178–179\norder management case study, role playing, \n171\norigin dimension (airline case study), \n320–321\nOR, skill keywords bridge, 275\noutrigger dimensions, 50, 89, 106–107\ncalendars as, 321–323\nlow cardinality attribute set and, 243–244\ntype 5 and type 1 SCD, 160\noverwrite (type 1 SCD), 54, 149–150\nadd to type 2 attribute, 160–162\ntype 2 in same dimension, 153\nP\npackaged analytic solutions, 270–271\npackaged data models, 270–271\npage dimension, clickstream data, 358–359\npage event fact table, clickstream data, \n363–366\nparallelizing/pipelining system, 492\nparallel processing, fact tables, 518\nparallel structures, fact tables, 519\nparent/child schemas, 59\nparent/child tree structure hierarchy, 216\npartitioning\nfact tables, smart keys, 102\nreal-time processing, 524–525\npassenger dimension, airline case study, 314\npathstring, ragged/variable depth hierarches, \n57\npay-in-advance facts, insurance case study, \n386–387\npayment method, retail sales, 93\n",
      "content_length": 2769,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 596,
      "content": "Index  \n560\nperformance measurement, fact tables, 10, 12\nadditive facts, 11\ngrains, 10–12\nnumeric facts, 11\ntextual facts, 12\nperiod close (G/L), 204–206\nperiodic snapshots, 43, 112–114\neducation case study, 329, 333\nETL systems, 474\nfact tables, 120–121\ncomplementary fact tables, 122\nG/L (general ledger), 203\ngrain fact tables, 12\nheadcount, 267–268\nhealthcare case study, 342\ninsurance case study, 385\nclaims, 395–396\npremiums, 386–387\ninventory case study, 112–114\nprocurement case study, 147\nperspectives of business users, 293\nphysical design, Lifecycle data track, 420\naggregations, 421\ndatabase model, 421\ndatabase standards, 420\nindex plan, 421\nnaming standards, 420–421\nOLAP database, 421\nstorage, 422\npipelining system, 492\nplanning, demand planning, 142\nP&L (proﬁ t and loss) statement\ncontribution, 189–191\ngranularity, 191–192\npolicy transactions (insurance case study), \n379–380\nfact table, 383\nPO (purchase orders), 142\nPOS (point-of-sale) system, 73\nPOS schema, retail sales case study, 94\ntransaction numbers, 93–94\npresentation area, 21–22\nprioritization, Lifecycle business \nrequirements, 414–415\nprivacy, data governance and, 541–542\nproblem escalation system, 491–492\nprocurement case study, 141–142\nbus matrix, 142–143\nsnapshot fact table, 147\ntransactions, 142–145\nproduct dimension, 83–84\nattributes with embedded meaning, 85\ncharacteristics, 172–173\ndrilling down, 86–87\nmany-to-one hierarchies, 84–85\nnumeric values, 85–86\noperational product master, 173\norder transactions, 172–173\noperational product master, 173\nproduction codes, decoding, 504\nproducts\nheterogeneous, 293–295\nLifecycle\nevaluation matrix, 419\nmarket research, 419\nprototypes, 419\nproﬁ t and loss facts, 189–191, 370–372\nallocations and, 60\ngranularity, 191–192\nprogram/project planning (Lifecycle), \n405–406\nbusiness motivation, 407\nbusiness sponsor, 406\ndevelopment, 409–410\nfeasibility, 407\njustiﬁ cation, 407\nplanning, 409–410\nreadiness assessment, 406–407\nscoping, 407\nstaffi  ng, 408–409\ntask list, 409\nproject manager, 409\npromotion dimension, 89–91\nnull values, 92\npromotion lift, 89\nprototypes\nbig data and, 536\nLifecycle, 419\npublishing metaphor for DW/BI managers, \n5–7\nQ\nquality events, responses, 458\nquality screens, ETL systems, 457–458\nquestionnaire, HR (human resources), 277\ntext comments, 278\nR\nragged hierarchies\nalternative modeling approaches, 221–223\nbridge table approach, 223\nmodifying, 220–221\npathstring attributes, 57\nshared ownership, 219\ntime varying, 220\nvariable depth, 215–217\nrapidly changing monster dimension, 55\n",
      "content_length": 2545,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 597,
      "content": "Index 561\nRDBMS (relational database management \nsystem), 40\narchitecture extension, 529–530\nblobs, 530\nfact extractor, 530\nhyperstructured data, 530\nreal-time fact tables, 68\nreal-time processing, 520–522\narchitecture, 522–524\npartitions, 524–525\nrearview mirror metrics, 198\nrecovery and restart system, ETL system, \n486–488\nrecursive hierarchies, employees, 271–272\nreference dimensions, 130\nreferential integrity, 12\nreferral dimension, clickstream data, 360\nrelationships\ndimension tables, 15\nmany-to-one, 175–176\nmany-to-one-to-many joins, 259–260\none-to-one, 175–176\nvalidation, 504–505\nrelative date attributes, 82–83\nremodeling existing data structures, 309\nreports\ncorrectly weighted, 288\ndimensional models, 17\ndynamic value banding, 64\nfact tables, 17\nimpact, 288\nvalue band reporting, 291–292\nrequirements for dimensional modeling, 432\nrestaurant metaphor for Kimball architecture, \n23–26\nretail sales case study, 72–73, 92\nbusiness process selection, 74\ndimensions, selecting, 76\nfacts, 76–77\nderived, 77–78\nnon-additive, 78\nfact tables, 79\nfrequent shopper program, 96\ngrain declaration, 74–75\npayment method, 93\nPOS (point-of-sale) system, 73\nPOS schema, 94\nretail schema extensibility, 95–97\nSKUs, 73\nretain original (SCD type 0), 54, 148–149\nretrieval, 485–486\nretroactive changes, healthcare case study, \n351–352\nreviewing dimensional model, 440, 441\nRFI measures, 240\nRFP (request for proposal), 419\nrole playing, dimensions, 49, 89, 171, 284\nairline case study, 313\nbus matrix and, 171\nhealthcare case study, 345\ninsurance case study, 380\norder management case study, 170\nS\nsales channel dimension, airline case study, \n315\nsales reps, factless fact tables, 176\nsales transactions, web proﬁ tability and, \n370–372\nsandbox results, big data management, \n532–533\nsandbox source system, ETL development, \n503\nsatisfaction indicators in fact tables, 254–255\nscalability, dimensional modeling myths, 31\nSCDs (slowly changing dimensions), 53, 148, \n464–465\nbig data and, 539\ndetailed dimension model, 437\nhybrid techniques, 159–164\ninsurance case study, 380–381\ntype 0 (retain original), 54, 148–149\ntype 1 (overwrite), 54, 149–150\nETL systems, 465\ntype 2 in same dimension, 153\ntype 2 (add new row), 54, 150–152\naccumulating snapshots, 196\ncustomer counts, 243\neff ective date, 152–153\nETL systems, 465–466\nexpiration date, 152–153\ntype 1 in same dimension, 153\ntype 3 (add new attribute), 55, 154–155\nETL systems, 467\nmultiple, 156\ntype 4 (add mini-dimension), 55, 156–159\nETL systems, 467\ntype 5 (add mini-dimension and type 1 \noutrigger), 55, 160\nETL systems, 468\ntype 6 (add type 1 attributes to type 2 \ndimension), 56, 160–162\nETL systems, 468\ntype 7 (dual type 1 and type 2 dimension), \n56, 162–164\nETL systems, 468\nscheduling jobs, ETL operation and \nautomation, 520\nscoping for program/project planning, 407\n",
      "content_length": 2832,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 598,
      "content": "Index  \n562\nscoring, CRM and customer dimension, \n240–243\nscreening\nETL systems\nbusiness rule screens, 458\ncolumn screens, 457\nstructure screens, 457\nquality screens, 457–458\nsecurity, 495\nETL system, 446, 492–493\ngoals, 4\nsegmentation, CRM and customer dimension, \n240–243\nsegments, airline bus matrix granularity, 313\nlinking to trips, 315–316\nSELECT statement, 18\nsemi-additive facts, 42, 114–115\nsequential behavior, step dimension, 65, \n251–252\nsequential integers, surrogate keys, 101\nservice level performance, 188–189\nsession dimension, clickstream data, 359–360\nsession fact table, clickstream data, 361–363\nsession IDs, clickstream data, 355–356\nset diff erence, 97\nshared dimensions, 130\nshipment invoice fact table, 188\nshrunken dimensions, 51\nconformed\nattribute subset, 132\non bus matrix, 134\nrow subsets and, 132–134\nrollup, 132\nsubsets, ETL systems, 472\nsimple administration backup, 485\nsimple data transformation, dimensions, 504\nsingle customer dimension, data integration \nand, 256–258\nsingle granularity, facts and, 301\nsingle version of the truth, 407\nskill keywords, 274\nbridge, 275\nAND queries, 275\nOR queries, 275\ntext string, 276–277\nskills, ETL system, 448\nSKUs (stock keeping units), 73\nslightly ragged/variable depth hierarchies, 57\nslowly changing dimensions. See SCDs, 148\nsmart keys\ndate dimensions, 101–102\nfact tables, partitioning, 102\nsnapshots\naccumulating, 44, 118–119, 194–196\nclaims (insurance case study), 393–395\neducation case study, 326\nETL systems, 475\nfact tables, 121–122, 326–329\nfact tables, complementary, 122\nhealthcare case study, 343\ninventory case study, 118–119\norder management case study, 194–196\nprocurement case study, 147\ntype 2 dimensions and, 196\nincremental processing, 517\nperiodic, 43\neducation case study, 329\nETL systems, 474\nfact tables, 120–121\nfact tables, complementary, 122\nG/L (general ledger), 203\nheadcounts, 267–268\ninsurance case study, 385, 395–396\ninventory case study, 112–114\npremiums (insurance case study), \n386–388\nsnowﬂ aking, 15, 50, 104–106, 470\noutriggers, 106–107\nsocial media, CRM (customer relationship \nmanagement) and, 230\nsorting\nETL, 490\ninternational information, 237\nsource systems, operational, 18\nspecial dimensions manager, ETL systems, \n470\ndate/time dimensions, 470\njunk dimensions, 470\nmini-dimensions, 471\nshrunken subset, 472\nstatic, 472\nuser-maintained, 472–473\nspeciﬁ cation document, ETL development, \n502–503\nsandbox source system, 503\nSQL multipass to avoid fact-to-fact table \njoins, 61\nstaffi  ng for program/project planning, \n408–409\nstar joins, 16\nstar schemas, 8, 40\nstatic dimensions\nETL systems, 472\npopulation, 508\nstatistics, historic fact table audit, 508\nstatus dimensions, 284\nstep dimension, 65\nclickstream data, 366\nsequential behavior, 251–252\nstewardship, 135–136\n",
      "content_length": 2791,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 599,
      "content": "Index 563\nstorage, Lifecycle data, 422\nstore dimension, 87–89\nstrategic business initiatives, 70\nstreaming data, big data and, 536\nstrings, skill keywords, 276–277\nstructure screens, 457\nstudent dimension (education case study), \n330\nstudy groups, behavior, 64\nsubsets, shrunken subset dimensions, 472\nsubtypes, 293–294\nfact tables\nkeys, 294–295\nsupertype common facts, 295\nhealthcare case study, 347–348\ninsurance case study, 384, 387\nschemas, 67\nsummary data, dimensional modeling and, 30\nsunsetting, big data management, 533\nsupernatural keys, 46, 101\nsupertypes\nfact tables, 293–294\nkeys, 294–295\nsubtype common facts, 295\nhealthcare case study, 347–348\ninsurance case study, 384–387\nschemas, 67\nsurrogate keys, 58, 98–100, 303\nassignment, 506\ndegenerate dimensions, 101\ndimension tables, 98–100\nETL system, 475–477\ngenerator, 469–470\nfact tables, 102–103\nfact table transformations, 516\nlate arriving facts, 517\nlookup pipelining, 510–511\nsurvey questionnaire (HR), 277\ntext comments, 278\nsynthetic keys, 98\nT\ntags, behavior, in time series, 63\nteam building, Lifecycle business \nrequirements, 411\nrepresentatives, 411–412\ntechnical application design/development \n(Lifecycle), 406\ntechnical architect, 409\ntechnical architecture (Lifecycle), 405, \n416–417\narchitecture implementation phases, 418\nmodel creation, 417\nplan creation, 418\nrequirements\ncollection, 417\ndocumentation, 417\nrequirements collection, 417\nsubsystems, 418\ntask force, 417\ntelecommunications case study, 297–299\nterm dimension (education case study), 330\ntext comments\ndimensions, 65\nhealthcare case study, 350\ntext strings, skill keywords, 276–277\ntext, survey questionnaire (HR) comments, \n278\ntextual attributes, dimension tables, 82\ntextual facts, 12\nThe Data Warehouse Toolkit (Kimball), 2, 80\nthird normal form (3NF) models, 7\nentity-relationship diagrams (ERDs), 8\nnormalized 3NF structures, 8\ntime\nGMT (Greenwich Mean Time), 323\nUTC (Coordinated Universal Time), 323\ntimed extracts, CDC (change data capture), \n452\ntime dimension, 80\nclickstream data, 361–362\ntimeliness goals, 4\ntime-of-day\ndimension, 83\nfact, 83\ntime series\nbehavior tags, 63, 240–242\nﬁ xed time series buckets, date dimensions \nand, 302–303\ntime shifting, 90\ntimespan fact tables, 252–254\ndual date/time stamps, 254\ntimespan tracking in fact tables, 62\ntime varying multivalued bridge tables, 63\ntime zones\nairline case study, 323\nGMT (Greenwich Mean Time), 323\nmultiple, 65\nnumber of, 323\nUTC (Coordinated Universal Time), 323\ntools\ndimensional modeling, 432\ndata proﬁ ling tools, 433\nETL development, 499\ntransactions, 43, 120, 179\nclaim transactions (insurance case study), \n390\nclaim accumulating snapshot, 393–394\njunk dimensions and, 392\nperiodic snapshot, 395–396\ntimespan accumulating snapshot, \n394–395\n",
      "content_length": 2767,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 600,
      "content": "Index  \n564\nfact tables, 12, 143–145\nhealthcare case study, 342\ninventory transactions, 116–118\ninvoice transactions, 187–188\njournal entries (G/L), 206–207\nnumbers, degenerate dimensions, 93–94\norder management case study\nallocating, 184–186\ndate, 170–171\ndeal dimension, 177–178\ndegenerate dimension, 178–179\nheader/line patterns, 181–182, 186\njunk dimensions, 179–180\nproduct dimension, 172–173\norder transactions, 168\naudit dimension, 192–193\ncustomer dimension, 174–176\nfact normalization, 169–170\nmultiple currency, 182–184\npolicies (insurance case study), 379–380\nprocurement, 142–143\ntransaction proﬁ le dimension, 49, 179\ntransportation, 311\nairline case study, 311–323\ncargo shipper schema, 317\nlocalization and, 324\ntravel services ﬂ ight schema, 317\ntravel services ﬂ ight schema, 317\ntrees (hierarchies), 215\nparent/child structure, 216\ntype 0 (retain original) SCD, 54\nretain original, 148–149\ntype 1 (overwrite) SCD, 54\nadd to type 2 dimension, 160–162\nETL system, 465\noverwrite, 149–150\ntype 2 in same dimension, 153\ntype 2 (add new row) SCD, 54, 150–152\naccumulating snapshots, 196\ncustomer counts, 243\neff ective date, 152–153\nemployee proﬁ le changes, 267\nETL system, 465–466\nexpiration date, 152–153\ntype 1 in same dimension, 153\ntype 3 (add new attribute) SCD, 55, 154–155\nETL system, 467\nmultiple, 156\ntype 4 (add mini-dimension) SCD, 55, \n156–159\nETL system, 467\ntype 5 (add mini-dimension and type 1 \noutrigger) SCD, 55\ntype 5 (add mini-dimension and type \noutrigger) SCD, 160\nETL system, 468\ntype 6 (add type 1 attributes to type 2 \ndimension) SCD, 56, 160–162\nETL system, 468\ntype 7 (dual type 1 and type 2 dimension) \nSCD, 56, 162–163\nas of reporting, 164\nETL system, 468\nU\nUnicode, 236–238\nuniform chart of accounts, 204\nunits of measure, multiple, 197–198\nupdates, accumulating snapshots, 121–122\nuser-maintained dimensions, ETL systems, \n472–473\nUTC (Coordinated Universal Time), 323\nV\nvalidating dimension model, 440–441\nvalidation, relationships, 504–505\nvalue band reporting, 291–292\nvalue chain, 52\ninsurance case study, 377–378\nintegration, 122–123\ninventory case study, 111–112\nvariable depth hierarchies\npathstring attributes, 57\nragged, 215–217\nslightly ragged, 214–215\nvariable depth/ragged hierarchies with \nbridge tables, 57\nvariable depth/slightly ragged hierarchies, \n57\nversion control, 495\nETL system, 488\nversion migration system, ETL system, 488\nvisitor identiﬁ cation, web sites, 356–357\nW\nweekday indicator, 82\nWHERE clause, 18\nworkﬂ ow monitor, ETL system, 489–490\nworkshops, dimensional modeling, 38\nX–Y–Z\nYTD (year-to-date) facts, 61\nG/L (general ledger), 206 \n",
      "content_length": 2613,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 601,
      "content": "www.kimballgroup.com\nLearn More. Get More. Do More.\nThe Kimball Group is the source for dimensional data \nwarehouse and business intelligence consulting and education. \nAfter all, we wrote the books!\n■  Subscribe to Kimball DESIGN TIPS for practical, \n   reliable guidance\n■  Attend KIMBALL UNIVERSITY for courses consistent with\n   the instructors’ best-selling Toolkit books\n■  Work with Kimball CONSULTANTS to leverage our\n   decades of real-world experience\n  Visit www.kimballgroup.com for more information.\nK I M BA L L  GROUP\nConsulting | Kimball University\n",
      "content_length": 565,
      "extraction_method": "PyMuPDF_fallback"
    }
  ]
}