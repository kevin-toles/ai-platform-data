{
  "metadata": {
    "title": "Books Library Net - Unknown Title",
    "author": "Vladimir Khorikov",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 305,
    "conversion_date": "2025-12-25T18:10:58.851054",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "Books Library Net - Unknown Title.pdf",
    "extraction_method": "PyMuPDF_fallback (Unstructured failed)"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Segment 1 (pages 1-8)",
      "start_page": 1,
      "end_page": 8,
      "detection_method": "topic_boundary",
      "content": "M A N N I N G\nVladimir Khorikov\nPrinciples, Practices, and Patterns\n\n\nProtection against\nregressions\n(ch. 4)\nResistance to\nrefactoring\n(ch. 4)\nFast feedback\n(ch. 4)\nMaintainability\n(ch. 4)\nDeﬁned by\nDeﬁned by\nUnit tests\nIntegration tests\nMaximize\nMaximize\nMaximize\nMaximize\nManaged\ndependencies\n(ch. 8)\nUnmanaged\ndependencies\n(ch. 8)\nTest accuracy\n(ch. 4)\nFalse positives\n(ch. 4)\nFalse negatives\n(ch. 4)\nTackled by\nTackled by\nMocks\n(ch. 5)\nShould not be used for\nShould be used for\nDomain model and\nalgorithms\n(ch. 7)\nControllers\n(ch. 7)\nCover\nCover\nComplexity\n(ch. 7)\nCollaborators\n(ch. 2)\nHave large number of\nIn-process\ndependencies\n(ch. 2)\nOut-of-process\ndependencies\n(ch. 2)\nHave high\nAre\nAre\nAre\nAre\nUsed in\nDamage if used incorrectly\nChapter Map\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\nUnit Testing:\nPrinciples, Practices,\nand Patterns\nVLADIMIR KHORIKOV\nM A N N I N G\nSHELTER ISLAND\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\nFor online information and ordering of this and other Manning books, please visit\nwww.manning.com. The publisher offers discounts on this book when ordered in quantity. \nFor more information, please contact\nSpecial Sales Department\nManning Publications Co.\n20 Baldwin Road\nPO Box 761\nShelter Island, NY 11964\nEmail: orders@manning.com\n©2020 by Manning Publications Co. All rights reserved.\nNo part of this publication may be reproduced, stored in a retrieval system, or transmitted, in \nany form or by means electronic, mechanical, photocopying, or otherwise, without prior written \npermission of the publisher.\nMany of the designations used by manufacturers and sellers to distinguish their products are \nclaimed as trademarks. Where those designations appear in the book, and Manning Publications \nwas aware of a trademark claim, the designations have been printed in initial caps or all caps.\nRecognizing the importance of preserving what has been written, it is Manning’s policy to have \nthe books we publish printed on acid-free paper, and we exert our best efforts to that end. \nRecognizing also our responsibility to conserve the resources of our planet, Manning books\nare printed on paper that is at least 15 percent recycled and processed without the use of \nelemental chlorine.\nManning Publications Co.\nAcquisitions editor: Mike Stephens\n20 Baldwin Road\nDevelopment editor: Marina Michaels\nPO Box 761\nTechnical development editor: Sam Zaydel\nShelter Island, NY 11964\nReview editor: Aleksandar Dragosavljevic´\nProduction editor: Anthony Calcara\nCopy editor: Tiffany Taylor\nESL copyeditor: Frances Buran\nProofreader: Keri Hales\nTechnical proofreader: Alessandro Campeis\nTypesetter: Dennis Dalinnik\nCover designer: Marija Tudor\nISBN: 9781617296277\nPrinted in the United States of America\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n To my wife, Nina\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n \nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\nv\nbrief contents\nPART 1\nTHE BIGGER PICTURE....................................................1\n1\n■\nThe goal of unit testing\n3\n2\n■\nWhat is a unit test?\n20\n3\n■\nThe anatomy of a unit test\n41\nPART 2\nMAKING YOUR TESTS WORK FOR YOU...........................65\n4\n■\nThe four pillars of a good unit test\n67\n5\n■\nMocks and test fragility\n92\n6\n■\nStyles of unit testing\n119\n7\n■\nRefactoring toward valuable unit tests\n151\nPART 3\nINTEGRATION TESTING..............................................183\n8\n■\nWhy integration testing?\n185\n9\n■\nMocking best practices\n216\n10\n■\nTesting the database\n229\nPART 4\nUNIT TESTING ANTI-PATTERNS...................................257\n11\n■\nUnit testing anti-patterns\n259\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "page_number": 1
    },
    {
      "number": 2,
      "title": "Segment 2 (pages 9-16)",
      "start_page": 9,
      "end_page": 16,
      "detection_method": "topic_boundary",
      "content": "vii\ncontents\npreface\nxiv\nacknowledgments\nxv\nabout this book\nxvi\nabout the author\nxix\nabout the cover illustration\nxx\nPART 1\nTHE BIGGER PICTURE..........................................1\n1 \nThe goal of unit testing\n3\n1.1\nThe current state of unit testing\n4\n1.2\nThe goal of unit testing\n5\nWhat makes a good or bad test?\n7\n1.3\nUsing coverage metrics to measure test suite quality\n8\nUnderstanding the code coverage metric\n9\n■Understanding the \nbranch coverage metric\n10\n■Problems with coverage metrics\n12\nAiming at a particular coverage number\n15\n1.4\nWhat makes a successful test suite?\n15\nIt’s integrated into the development cycle\n16\n■It targets only the \nmost important parts of your code base\n16\n■It provides maximum \nvalue with minimum maintenance costs\n17\n1.5\nWhat you will learn in this book\n17\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\nCONTENTS\nviii\n2 \nWhat is a unit test?\n20\n2.1\nThe definition of “unit test”\n21\nThe isolation issue: The London take\n21\n■The isolation issue: \nThe classical take\n27\n2.2\nThe classical and London schools of unit testing\n30\nHow the classical and London schools handle dependencies\n30\n2.3\nContrasting the classical and London schools \nof unit testing\n34\nUnit testing one class at a time\n34\n■Unit testing a large graph of \ninterconnected classes\n35\n■Revealing the precise bug location\n36\nOther differences between the classical and London schools\n36\n2.4\nIntegration tests in the two schools\n37\nEnd-to-end tests are a subset of integration tests\n38\n3 \nThe anatomy of a unit test\n41\n3.1\nHow to structure a unit test\n42\nUsing the AAA pattern\n42\n■Avoid multiple arrange, act, \nand assert sections\n43\n■Avoid if statements in tests\n44\nHow large should each section be?\n45\n■How many assertions \nshould the assert section hold?\n47\n■What about the teardown \nphase?\n47\n■Differentiating the system under test\n47\nDropping the arrange, act, and assert comments from tests\n48\n3.2\nExploring the xUnit testing framework\n49\n3.3\nReusing test fixtures between tests\n50\nHigh coupling between tests is an anti-pattern\n52\n■The use of \nconstructors in tests diminishes test readability\n52\n■A better way \nto reuse test fixtures\n52\n3.4\nNaming a unit test\n54\nUnit test naming guidelines\n56\n■Example: Renaming a test \ntoward the guidelines\n56\n3.5\nRefactoring to parameterized tests\n58\nGenerating data for parameterized tests\n60\n3.6\nUsing an assertion library to further improve \ntest readability\n62\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\nCONTENTS\nix\nPART 2\nMAKING YOUR TESTS WORK FOR YOU.................65\n4 \nThe four pillars of a good unit test\n67\n4.1\nDiving into the four pillars of a good unit test\n68\nThe first pillar: Protection against regressions\n68\n■The second \npillar: Resistance to refactoring\n69\n■What causes false \npositives?\n71\n■Aim at the end result instead of \nimplementation details\n74\n4.2\nThe intrinsic connection between the first \ntwo attributes\n76\nMaximizing test accuracy\n76\n■The importance of false positives \nand false negatives: The dynamics\n78\n4.3\nThe third and fourth pillars: Fast feedback \nand maintainability\n79\n4.4\nIn search of an ideal test\n80\nIs it possible to create an ideal test?\n81\n■Extreme case #1: \nEnd-to-end tests\n81\n■Extreme case #2: Trivial tests\n82\nExtreme case #3: Brittle tests\n83\n■In search of an ideal test: \nThe results\n84\n4.5\nExploring well-known test automation concepts\n87\nBreaking down the Test Pyramid\n87\n■Choosing between black-box \nand white-box testing\n89\n5 \nMocks and test fragility\n92\n5.1\nDifferentiating mocks from stubs\n93\nThe types of test doubles\n93\n■Mock (the tool) vs. mock (the \ntest double)\n94\n■Don’t assert interactions with stubs\n96\nUsing mocks and stubs together\n97\n■How mocks and stubs \nrelate to commands and queries\n97\n5.2\nObservable behavior vs. implementation details\n99\nObservable behavior is not the same as a public API\n99\n■Leaking \nimplementation details: An example with an operation\n100\nWell-designed API and encapsulation\n103\n■Leaking \nimplementation details: An example with state\n104\n5.3\nThe relationship between mocks and test fragility\n106\nDefining hexagonal architecture\n106\n■Intra-system vs. inter-\nsystem communications\n110\n■Intra-system vs. inter-system \ncommunications: An example\n111\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\nCONTENTS\nx\n5.4\nThe classical vs. London schools of unit testing, \nrevisited\n114\nNot all out-of-process dependencies should be mocked out\n115\nUsing mocks to verify behavior\n116\n6 \nStyles of unit testing\n119\n6.1\nThe three styles of unit testing\n120\nDefining the output-based style\n120\n■Defining the state-based \nstyle\n121\n■Defining the communication-based style\n122\n6.2\nComparing the three styles of unit testing\n123\nComparing the styles using the metrics of protection against \nregressions and feedback speed\n124\n■Comparing the styles using \nthe metric of resistance to refactoring\n124\n■Comparing the styles \nusing the metric of maintainability\n125\n■Comparing the styles: \nThe results\n127\n6.3\nUnderstanding functional architecture\n128\nWhat is functional programming?\n128\n■What is functional \narchitecture?\n132\n■Comparing functional and hexagonal \narchitectures\n133\n6.4\nTransitioning to functional architecture and output-based \ntesting\n135\nIntroducing an audit system\n135\n■Using mocks to decouple tests \nfrom the filesystem\n137\n■Refactoring toward functional \narchitecture\n140\n■Looking forward to further developments\n146\n6.5\nUnderstanding the drawbacks of functional architecture\n146\nApplicability of functional architecture\n147\n■Performance \ndrawbacks\n148\n■Increase in the code base size\n149\n7 \nRefactoring toward valuable unit tests\n151\n7.1\nIdentifying the code to refactor\n152\nThe four types of code\n152\n■Using the Humble Object pattern to \nsplit overcomplicated code\n155\n7.2\nRefactoring toward valuable unit tests\n158\nIntroducing a customer management system\n158\n■Take 1: \nMaking implicit dependencies explicit\n160\n■Take 2: Introducing \nan application services layer\n160\n■Take 3: Removing complexity \nfrom the application service\n163\n■Take 4: Introducing a new \nCompany class\n164\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\nCONTENTS\nxi\n7.3\nAnalysis of optimal unit test coverage\n167\nTesting the domain layer and utility code\n167\n■Testing the code \nfrom the other three quadrants\n168\n■Should you test \npreconditions?\n169\n7.4\nHandling conditional logic in controllers\n169\nUsing the CanExecute/Execute pattern\n172\n■Using domain \nevents to track changes in the domain model\n175\n7.5\nConclusion\n178\nPART 3\nINTEGRATION TESTING....................................183\n8 \nWhy integration testing?\n185\n8.1\nWhat is an integration test?\n186\nThe role of integration tests\n186\n■The Test Pyramid \nrevisited\n187\n■Integration testing vs. failing fast\n188\n8.2\nWhich out-of-process dependencies to test directly\n190\nThe two types of out-of-process dependencies\n190\n■Working with \nboth managed and unmanaged dependencies\n191\n■What if you \ncan’t use a real database in integration tests?\n192\n8.3\nIntegration testing: An example\n193\nWhat scenarios to test?\n194\n■Categorizing the database and \nthe message bus\n195\n■What about end-to-end testing?\n195\nIntegration testing: The first try\n196\n8.4\nUsing interfaces to abstract dependencies\n197\nInterfaces and loose coupling\n198\n■Why use interfaces for \nout-of-process dependencies?\n199\n■Using interfaces for in-process \ndependencies\n199\n8.5\nIntegration testing best practices\n200\nMaking domain model boundaries explicit\n200\n■Reducing the \nnumber of layers\n200\n■Eliminating circular dependencies\n202\nUsing multiple act sections in a test\n204\n8.6\nHow to test logging functionality\n205\nShould you test logging?\n205\n■How should you test \nlogging?\n207\n■How much logging is enough?\n212\nHow do you pass around logger instances?\n212\n8.7\nConclusion\n213\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\nCONTENTS\nxii\n9 \nMocking best practices\n216\n9.1\nMaximizing mocks’ value\n217\nVerifying interactions at the system edges\n219\n■Replacing mocks \nwith spies\n222\n■What about IDomainLogger?\n224\n9.2\nMocking best practices\n225\nMocks are for integration tests only\n225\n■Not just one mock per \ntest\n225\n■Verifying the number of calls\n226\n■Only mock types \nthat you own\n227\n10 \nTesting the database\n229\n10.1\nPrerequisites for testing the database\n230\nKeeping the database in the source control system\n230\n■Reference \ndata is part of the database schema\n231\n■Separate instance for \nevery developer\n232\n■State-based vs. migration-based database \ndelivery\n232\n10.2\nDatabase transaction management\n234\nManaging database transactions in production code\n235\n■Managing \ndatabase transactions in integration tests\n242\n10.3\nTest data life cycle\n243\nParallel vs. sequential test execution\n243\n■Clearing data between \ntest runs\n244\n■Avoid in-memory databases\n246\n10.4\nReusing code in test sections\n246\nReusing code in arrange sections\n246\n■Reusing code in \nact sections\n249\n■Reusing code in assert sections\n250\nDoes the test create too many database transactions?\n251\n10.5\nCommon database testing questions\n252\nShould you test reads?\n252\n■Should you test repositories?\n253\n10.6\nConclusion\n254\nPART 3\nUNIT TESTING ANTI-PATTERNS.........................257\n11 \nUnit testing anti-patterns\n259\n11.1\nUnit testing private methods\n260\nPrivate methods and test fragility\n260\n■Private methods and \ninsufficient coverage\n260\n■When testing private methods is \nacceptable\n261\n11.2\nExposing private state\n263\n11.3\nLeaking domain knowledge to tests\n264\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\nCONTENTS\nxiii\n11.4\nCode pollution\n266\n11.5\nMocking concrete classes\n268\n11.6\nWorking with time\n271\nTime as an ambient context\n271\n■Time as an explicit \ndependency\n272\n11.7\nConclusion\n273\nindex\n275\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\nxiv\npreface\nI remember my first project where I tried out unit testing. It went relatively well; but after\nit was finished, I looked at the tests and thought that a lot of them were a pure waste of\ntime. Most of my unit tests spent a great deal of time setting up expectations and wiring\nup a complicated web of dependencies—all that, just to check that the three lines of\ncode in my controller were correct. I couldn’t pinpoint what exactly was wrong with the\ntests, but my sense of proportion sent me unambiguous signals that something was off.\n Luckily, I didn’t abandon unit testing and continued applying it in subsequent\nprojects. However, disagreement with common (at that time) unit testing practices\nhas been growing in me ever since. Throughout the years, I’ve written a lot about unit\ntesting. In those writings, I finally managed to crystallize what exactly was wrong with\nmy first tests and generalized this knowledge to broader areas of unit testing. This\nbook is a culmination of all my research, trial, and error during that period—compiled,\nrefined, and distilled.\n I come from a mathematical background and strongly believe that guidelines in\nprogramming, like theorems in math, should be derived from first principles. I’ve\ntried to structure this book in a similar way: start with a blank slate by not jumping to\nconclusions or throwing around unsubstantiated claims, and gradually build my case\nfrom the ground up. Interestingly enough, once you establish such first principles,\nguidelines and best practices often flow naturally as mere implications.\n I believe that unit testing is becoming a de facto requirement for software proj-\nects, and this book will give you everything you need to create valuable, highly main-\ntainable tests.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "page_number": 9
    },
    {
      "number": 3,
      "title": "Segment 3 (pages 17-24)",
      "start_page": 17,
      "end_page": 24,
      "detection_method": "topic_boundary",
      "content": "xv\nacknowledgments\nThis book was a lot of work. Even though I was prepared mentally, it was still much\nmore work than I could ever have imagined.\n A big “thank you” to Sam Zaydel, Alessandro Campeis, Frances Buran, Tiffany\nTaylor, and especially Marina Michaels, whose invaluable feedback helped shape the\nbook and made me a better writer along the way. Thanks also to everyone else at Man-\nning who worked on this book in production and behind the scenes.\n I’d also like to thank the reviewers who took the time to read my manuscript at var-\nious stages during its development and who provided valuable feedback: Aaron Barton,\nAlessandro Campeis, Conor Redmond, Dror Helper, Greg Wright, Hemant Koneru,\nJeremy Lange, Jorge Ezequiel Bo, Jort Rodenburg, Mark Nenadov, Marko Umek,\nMarkus Matzker, Srihari Sridharan, Stephen John Warnett, Sumant Tambe, Tim van\nDeurzen, and Vladimir Kuptsov.\n Above all, I would like to thank my wife Nina, who supported me during the whole\nprocess.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\nxvi\nabout this book\nUnit Testing: Principles, Practices, and Patterns provides insights into the best practices\nand common anti-patterns that surround the topic of unit testing. After reading this\nbook, armed with your newfound skills, you’ll have the knowledge needed to become\nan expert at delivering successful projects that are easy to maintain and extend,\nthanks to the tests you build along the way.\nWho should read this book\nMost online and print resources have one drawback: they focus on the basics of unit\ntesting but don’t go much beyond that. There’s a lot of value in such resources, but\nthe learning doesn’t end there. There’s a next level: not just writing tests, but doing it\nin a way that gives you the best return on your efforts. When you reach this point on\nthe learning curve, you’re pretty much left to your own devices to figure out how to\nget to the next level.\n This book takes you to that next level. It teaches a scientific, precise definition of\nthe ideal unit test. That definition provides a universal frame of reference, which will\nhelp you look at many of your tests in a new light and see which of them contribute to\nthe project and which must be refactored or removed.\n If you don’t have much experience with unit testing, you’ll learn a lot from this book.\nIf you’re an experienced programmer, you most likely already understand some of the\nideas taught in this book. The book will help you articulate why the techniques and best\npractices you’ve been using all along are so helpful. And don’t underestimate this skill:\nthe ability to clearly communicate your ideas to colleagues is priceless.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\nABOUT THIS BOOK\nxvii\nHow this book is organized: A roadmap\nThe book’s 11 chapters are divided into 4 parts. Part 1 introduces unit testing and\ngives a refresher on some of the more generic unit testing principles:\n■\nChapter 1 defines the goal of unit testing and gives an overview of how to differ-\nentiate a good test from a bad one.\n■\nChapter 2 explores the definition of unit test and discusses the two schools of\nunit testing.\n■\nChapter 3 provides a refresher on some basic topics, such as structuring of unit\ntests, reusing test fixtures, and test parameterization.\nPart 2 gets to the heart of the subject—it shows what makes a good unit test and pro-\nvides details about how to refactor your tests toward being more valuable:\n■\nChapter 4 defines the four pillars that form a good unit test and provide a com-\nmon frame of reference that is used throughout the book.\n■\nChapter 5 builds a case for mocks and explores their relation to test fragility.\n■\nChapter 6 examines the three styles of unit testing, along with which of those\nstyles produces tests of the best quality and why.\n■\nChapter 7 teaches you how to refactor away from bloated, overcomplicated\ntests and achieve tests that provide maximum value with minimum mainte-\nnance costs.\nPart 3 explores the topic of integration testing:\n■\nChapter 8 looks at integration testing in general along with its benefits and\ntrade-offs.\n■\nChapter 9 discusses mocks and how to use them in a way that benefits your tests\nthe most.\n■\nChapter 10 explores working with relational databases in tests.\nPart 4’s chapter 11 covers common unit testing anti-patterns, some of which you’ve\npossibly encountered before.\nAbout the Code\nThe code samples are written in C#, but the topics they illustrate are applicable to any\nobject-oriented language, such as Java or C++. C# is just the language that I happen to\nwork with the most.\n I tried not to use any C#-specific language features, and I made the sample code as\nsimple as possible, so you shouldn’t have any trouble understanding it. You can down-\nload all of the code samples online at www.manning.com/books/unit-testing.\n \n \nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\nABOUT THIS BOOK\nxviii\nliveBook discussion forum\nPurchase of Unit Testing: Principles, Practices, and Patterns includes free access to a private\nweb forum run by Manning Publications where you can make comments about the\nbook, ask technical questions, and receive help from the author and from other\nusers. To access the forum, go to https://livebook.manning.com/#!/book/unit-testing/\ndiscussion. You can also learn more about Manning’s forums and the rules of conduct\nat https://livebook.manning.com/#!/discussion.\n Manning’s commitment to our readers is to provide a venue where a meaningful\ndialogue between individual readers and between readers and the author can take\nplace. It is not a commitment to any specific amount of participation on the part of\nthe author, whose contribution to the forum remains voluntary (and unpaid). We sug-\ngest you try asking the author some challenging questions lest his interest stray! The\nforum and the archives of previous discussions will be accessible from the publisher’s\nwebsite as long as the book is in print.\nOther online resources\n■\nMy blog is at EnterpriseCraftsmanship.com.\n■\nI also have an online course about unit testing (in the works, as of this writing),\nwhich you can enroll in at UnitTestingCourse.com.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\nxix\nabout the author\nVLADIMIR KHORIKOV is a software engineer, Microsoft MVP, and Pluralsight author. He\nhas been professionally involved in software development for over 15 years, including\nmentoring teams on the ins and outs of unit testing. During the past several years,\nVladimir has written several popular blog post series and an online training course on\nthe topic of unit testing. The biggest advantage of his teaching style, and the one stu-\ndents often praise, is his tendency to have a strong theoretic background, which he\nthen applies to practical examples.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\nxx\nabout the cover illustration\nThe figure on the cover of Unit Testing: Principles, Practices, and Patterns is captioned\n“Esthinienne.” The illustration is taken from a collection of dress costumes from vari-\nous countries by Jacques Grasset de Saint-Sauveur (1757–1810), titled Costumes Civils\nActuels de Tous les Peuples Connus, published in France in 1788. Each illustration is\nfinely drawn and colored by hand. The rich variety of Grasset de Saint-Sauveur’s col-\nlection reminds us vividly of how culturally apart the world’s towns and regions were\njust 200 years ago. Isolated from each other, people spoke different dialects and lan-\nguages. In the streets or in the countryside, it was easy to identify where they lived and\nwhat their trade or station in life was just by their dress.\n The way we dress has changed since then and the diversity by region, so rich at the\ntime, has faded away. It is now hard to tell apart the inhabitants of different conti-\nnents, let alone different towns, regions, or countries. Perhaps we have traded cultural\ndiversity for a more varied personal life—certainly for a more varied and fast-paced\ntechnological life.\n At a time when it is hard to tell one computer book from another, Manning cele-\nbrates the inventiveness and initiative of the computer business with book covers\nbased on the rich diversity of regional life of two centuries ago, brought back to life by\nGrasset de Saint-Sauveur’s pictures.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\nPart 1\nThe bigger picture\nThis part of the book will get you up to speed with the current state of unit\ntesting. In chapter 1, I’ll define the goal of unit testing and give an overview of\nhow to differentiate a good test from a bad one. We’ll talk about coverage metrics\nand discuss properties of a good unit test in general.\n In chapter 2, we’ll look at the definition of unit test. A seemingly minor dis-\nagreement over this definition has led to the formation of two schools of unit test-\ning, which we’ll also dive into. Chapter 3 provides a refresher on some basic topics,\nsuch as structuring of unit tests, reusing test fixtures, and test parametrization.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n \nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "page_number": 17
    },
    {
      "number": 4,
      "title": "Segment 4 (pages 25-35)",
      "start_page": 25,
      "end_page": 35,
      "detection_method": "topic_boundary",
      "content": "3\nThe goal of unit testing\nLearning unit testing doesn’t stop at mastering the technical bits of it, such as\nyour favorite test framework, mocking library, and so on. There’s much more to\nunit testing than the act of writing tests. You always have to strive to achieve the\nbest return on the time you invest in unit testing, minimizing the effort you put\ninto tests and maximizing the benefits they provide. Achieving both things isn’t\nan easy task.\n It’s fascinating to watch projects that have achieved this balance: they grow\neffortlessly, don’t require much maintenance, and can quickly adapt to their cus-\ntomers’ ever-changing needs. It’s equally frustrating to see projects that failed to do\nso. Despite all the effort and an impressive number of unit tests, such projects drag\non slowly, with lots of bugs and upkeep costs.\nThis chapter covers\nThe state of unit testing\nThe goal of unit testing\nConsequences of having a bad test suite\nUsing coverage metrics to measure test \nsuite quality\nAttributes of a successful test suite\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n4\nCHAPTER 1\nThe goal of unit testing\n That’s the difference between various unit testing techniques. Some yield great\noutcomes and help maintain software quality. Others don’t: they result in tests that\ndon’t contribute much, break often, and require a lot of maintenance in general.\n What you learn in this book will help you differentiate between good and bad unit\ntesting techniques. You’ll learn how to do a cost-benefit analysis of your tests and apply\nproper testing techniques in your particular situation. You’ll also learn how to avoid\ncommon anti-patterns—patterns that may make sense at first but lead to trouble down\nthe road.\n But let’s start with the basics. This chapter gives a quick overview of the state of\nunit testing in the software industry, describes the goal behind writing and maintain-\ning tests, and provides you with the idea of what makes a test suite successful.\n1.1\nThe current state of unit testing\nFor the past two decades, there’s been a push toward adopting unit testing. The push\nhas been so successful that unit testing is now considered mandatory in most compa-\nnies. Most programmers practice unit testing and understand its importance. There’s\nno longer any dispute as to whether you should do it. Unless you’re working on a\nthrowaway project, the answer is, yes, you do.\n When it comes to enterprise application development, almost every project\nincludes at least some unit tests. A significant percentage of such projects go far\nbeyond that: they achieve good code coverage with lots and lots of unit and integra-\ntion tests. The ratio between the production code and the test code could be any-\nwhere between 1:1 and 1:3 (for each line of production code, there are one to\nthree lines of test code). Sometimes, this ratio goes much higher than that, to a\nwhopping 1:10.\n But as with all new technologies, unit testing continues to evolve. The discussion\nhas shifted from “Should we write unit tests?” to “What does it mean to write good unit\ntests?” This is where the main confusion still lies.\n You can see the results of this confusion in software projects. Many projects have\nautomated tests; they may even have a lot of them. But the existence of those tests\noften doesn’t provide the results the developers hope for. It can still take program-\nmers a lot of effort to make progress in such projects. New features take forever to\nimplement, new bugs constantly appear in the already implemented and accepted\nfunctionality, and the unit tests that are supposed to help don’t seem to mitigate this\nsituation at all. They can even make it worse.\n It’s a horrible situation for anyone to be in—and it’s the result of having unit tests\nthat don’t do their job properly. The difference between good and bad tests is not\nmerely a matter of taste or personal preference, it’s a matter of succeeding or failing\nat this critical project you’re working on.\n It’s hard to overestimate the importance of the discussion of what makes a good\nunit test. Still, this discussion isn’t occurring much in the software development industry\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n5\nThe goal of unit testing\ntoday. You’ll find a few articles and conference talks online, but I’ve yet to see any\ncomprehensive material on this topic.\n The situation in books isn’t any better; most of them focus on the basics of unit\ntesting but don’t go much beyond that. Don’t get me wrong. There’s a lot of value in\nsuch books, especially when you are just starting out with unit testing. However, the\nlearning doesn’t end with the basics. There’s a next level: not just writing tests, but\ndoing unit testing in a way that provides you with the best return on your efforts.\nWhen you reach this point, most books pretty much leave you to your own devices to\nfigure out how to get to that next level.\n This book takes you there. It teaches a precise, scientific definition of the ideal\nunit test. You’ll see how this definition can be applied to practical, real-world exam-\nples. My hope is that this book will help you understand why your particular project\nmay have gone sideways despite having a good number of tests, and how to correct its\ncourse for the better.\n You’ll get the most value out of this book if you work in enterprise application\ndevelopment, but the core ideas are applicable to any software project.\n1.2\nThe goal of unit testing\nBefore taking a deep dive into the topic of unit testing, let’s step back and consider\nthe goal that unit testing helps you to achieve. It’s often said that unit testing practices\nlead to a better design. And it’s true: the necessity to write unit tests for a code base\nnormally leads to a better design. But that’s not the main goal of unit testing; it’s\nmerely a pleasant side effect.\nWhat is an enterprise application?\nAn enterprise application is an application that aims at automating or assisting an\norganization’s inner processes. It can take many forms, but usually the characteris-\ntics of an enterprise software are\nHigh business logic complexity\nLong project lifespan\nModerate amounts of data\nLow or moderate performance requirements \nThe relationship between unit testing and code design\nThe ability to unit test a piece of code is a nice litmus test, but it only works in one\ndirection. It’s a good negative indicator—it points out poor-quality code with relatively\nhigh accuracy. If you find that code is hard to unit test, it’s a strong sign that the code\nneeds improvement. The poor quality usually manifests itself in tight coupling, which\nmeans different pieces of production code are not decoupled from each other\nenough, and it’s hard to test them separately.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n6\nCHAPTER 1\nThe goal of unit testing\nWhat is the goal of unit testing, then? The goal is to enable sustainable growth of the\nsoftware project. The term sustainable is key. It’s quite easy to grow a project, especially\nwhen you start from scratch. It’s much harder to sustain this growth over time.\n Figure 1.1 shows the growth dynamic of a typical project without tests. You start\noff quickly because there’s nothing dragging you down. No bad architectural deci-\nsions have been made yet, and there isn’t any existing code to worry about. As time\ngoes by, however, you have to put in more and more hours to make the same amount\nof progress you showed at the beginning. Eventually, the development speed slows\ndown significantly, sometimes even to the point where you can’t make any progress\nwhatsoever.\nThis phenomenon of quickly decreasing development speed is also known as software\nentropy. Entropy (the amount of disorder in a system) is a mathematical and scientific\nconcept that can also apply to software systems. (If you’re interested in the math and\nscience of entropy, look up the second law of thermodynamics.)\n In software, entropy manifests in the form of code that tends to deteriorate. Each\ntime you change something in a code base, the amount of disorder in it, or entropy,\nincreases. If left without proper care, such as constant cleaning and refactoring, the\nsystem becomes increasingly complex and disorganized. Fixing one bug introduces\nmore bugs, and modifying one part of the software breaks several others—it’s like a\n(continued)\nUnfortunately, the ability to unit test a piece of code is a bad positive indicator. The\nfact that you can easily unit test your code base doesn’t necessarily mean it’s of\ngood quality. The project can be a disaster even when it exhibits a high degree of\ndecoupling.\nWithout tests\nWith tests\nProgress\nhours\nspent\nWork\nFigure 1.1\nThe difference in growth \ndynamics between projects with and \nwithout tests. A project without tests \nhas a head start but quickly slows down \nto the point that it’s hard to make any \nprogress.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n7\nThe goal of unit testing\ndomino effect. Eventually, the code base becomes unreliable. And worst of all, it’s\nhard to bring it back to stability.\n Tests help overturn this tendency. They act as a safety net—a tool that provides\ninsurance against a vast majority of regressions. Tests help make sure the existing\nfunctionality works, even after you introduce new features or refactor the code to bet-\nter fit new requirements.\nDEFINITION\nA regression is when a feature stops working as intended after a cer-\ntain event (usually, a code modification). The terms regression and software bug\nare synonyms and can be used interchangeably.\nThe downside here is that tests require initial—sometimes significant—effort. But they\npay for themselves in the long run by helping the project to grow in the later stages.\nSoftware development without the help of tests that constantly verify the code base\nsimply doesn’t scale.\n Sustainability and scalability are the keys. They allow you to maintain development\nspeed in the long run.\n1.2.1\nWhat makes a good or bad test?\nAlthough unit testing helps maintain project growth, it’s not enough to just write tests.\nBadly written tests still result in the same picture.\n As shown in figure 1.2, bad tests do help to slow down code deterioration at the\nbeginning: the decline in development speed is less prominent compared to the situa-\ntion with no tests at all. But nothing really changes in the grand scheme of things. It\nmight take longer for such a project to enter the stagnation phase, but stagnation is\nstill inevitable.\nWithout tests\nWith good tests\nWith bad tests\nProgress\nWork\nhours\nspent\nFigure 1.2\nThe difference in \ngrowth dynamics between \nprojects with good and bad \ntests. A project with badly \nwritten tests exhibits the \nproperties of a project with \ngood tests at the beginning, \nbut it eventually falls into \nthe stagnation phase.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n8\nCHAPTER 1\nThe goal of unit testing\nRemember, not all tests are created equal. Some of them are valuable and contribute a lot\nto overall software quality. Others don’t. They raise false alarms, don’t help you catch\nregression errors, and are slow and difficult to maintain. It’s easy to fall into the trap\nof writing unit tests for the sake of unit testing without a clear picture of whether it\nhelps the project.\n You can’t achieve the goal of unit testing by just throwing more tests at the project.\nYou need to consider both the test’s value and its upkeep cost. The cost component is\ndetermined by the amount of time spent on various activities:\nRefactoring the test when you refactor the underlying code\nRunning the test on each code change\nDealing with false alarms raised by the test\nSpending time reading the test when you’re trying to understand how the\nunderlying code behaves\nIt’s easy to create tests whose net value is close to zero or even is negative due to high\nmaintenance costs. To enable sustainable project growth, you have to exclusively\nfocus on high-quality tests—those are the only type of tests that are worth keeping in\nthe test suite.\nIt’s crucial to learn how to differentiate between good and bad unit tests. I cover this\ntopic in chapter 4. \n1.3\nUsing coverage metrics to measure test suite quality\nIn this section, I talk about the two most popular coverage metrics—code coverage\nand branch coverage—how to calculate them, how they’re used, and problems with\nthem. I’ll show why it’s detrimental for programmers to aim at a particular coverage\nnumber and why you can’t just rely on coverage metrics to determine the quality of\nyour test suite.\nDEFINITION\nA coverage metric shows how much source code a test suite exe-\ncutes, from none to 100%.\nProduction code vs. test code \nPeople often think production code and test code are different. Tests are assumed\nto be an addition to production code and have no cost of ownership. By extension,\npeople often believe that the more tests, the better. This isn’t the case. Code is a\nliability, not an asset. The more code you introduce, the more you extend the surface\narea for potential bugs in your software, and the higher the project’s upkeep cost. It’s\nalways better to solve problems with as little code as possible.\nTests are code, too. You should view them as the part of your code base that aims at\nsolving a particular problem: ensuring the application’s correctness. Unit tests, just\nlike any other code, are also vulnerable to bugs and require maintenance.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n9\nUsing coverage metrics to measure test suite quality\nThere are different types of coverage metrics, and they’re often used to assess the\nquality of a test suite. The common belief is that the higher the coverage number,\nthe better.\n Unfortunately, it’s not that simple, and coverage metrics, while providing valuable\nfeedback, can’t be used to effectively measure the quality of a test suite. It’s the same\nsituation as with the ability to unit test the code: coverage metrics are a good negative\nindicator but a bad positive one.\n If a metric shows that there’s too little coverage in your code base—say, only 10%—\nthat’s a good indication that you are not testing enough. But the reverse isn’t true:\neven 100% coverage isn’t a guarantee that you have a good-quality test suite. A test\nsuite that provides high coverage can still be of poor quality.\n I already touched on why this is so—you can’t just throw random tests at your\nproject with the hope those tests will improve the situation. But let’s discuss this\nproblem in detail with respect to the code coverage metric.\n1.3.1\nUnderstanding the code coverage metric\nThe first and most-used coverage metric is code coverage, also known as test coverage; see\nfigure 1.3. This metric shows the ratio of the number of code lines executed by at least\none test and the total number of lines in the production code base.\nLet’s see an example to better understand how this works. Listing 1.1 shows an\nIsStringLong method and a test that covers it. The method determines whether a\nstring provided to it as an input parameter is long (here, the definition of long is any\nstring with the length greater than five characters). The test exercises the method\nusing \"abc\" and checks that this string is not considered long.\npublic static bool IsStringLong(string input)\n{\n           \nif (input.Length > 5)          \nreturn true;\n    \nreturn false;\n           \n}\n          \nListing 1.1\nA sample method partially covered by a test\nCode coverage (test coverage) =\nTotal number of lines\nLines of code executed\nFigure 1.3\nThe code coverage (test coverage) metric is \ncalculated as the ratio between the number of code lines \nexecuted by the test suite and the total number of lines in \nthe production code base.\nCovered \nby the \ntest\nNot\ncovered\nby the\ntest\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n10\nCHAPTER 1\nThe goal of unit testing\npublic void Test()\n{\nbool result = IsStringLong(\"abc\");\nAssert.Equal(false, result);\n}\nIt’s easy to calculate the code coverage here. The total number of lines in the method\nis five (curly braces count, too). The number of lines executed by the test is four—the\ntest goes through all the code lines except for the return true; statement. This gives\nus 4/5 = 0.8 = 80% code coverage.\n Now, what if I refactor the method and inline the unnecessary if statement, like this?\npublic static bool IsStringLong(string input)\n{\nreturn input.Length > 5;\n}\npublic void Test()\n{\nbool result = IsStringLong(\"abc\");\nAssert.Equal(false, result);\n}\nDoes the code coverage number change? Yes, it does. Because the test now exercises\nall three lines of code (the return statement plus two curly braces), the code coverage\nincreases to 100%.\n But did I improve the test suite with this refactoring? Of course not. I just shuffled the\ncode inside the method. The test still verifies the same number of possible outcomes.\n This simple example shows how easy it is to game the coverage numbers. The more\ncompact your code is, the better the test coverage metric becomes, because it only\naccounts for the raw line numbers. At the same time, squashing more code into less\nspace doesn’t (and shouldn’t) change the value of the test suite or the maintainability\nof the underlying code base. \n1.3.2\nUnderstanding the branch coverage metric\nAnother coverage metric is called branch coverage. Branch coverage provides more pre-\ncise results than code coverage because it helps cope with code coverage’s shortcom-\nings. Instead of using the raw number of code lines, this metric focuses on control\nstructures, such as if and switch statements. It shows how many of such control struc-\ntures are traversed by at least one test in the suite, as shown in figure 1.4.\nBranch coverage = Total number of branches\nBranches traversed\nFigure 1.4\nThe branch metric is calculated as the ratio of the \nnumber of code branches exercised by the test suite and the \ntotal number of branches in the production code base.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n11\nUsing coverage metrics to measure test suite quality\nTo calculate the branch coverage metric, you need to sum up all possible branches in\nyour code base and see how many of them are visited by tests. Let’s take our previous\nexample again:\npublic static bool IsStringLong(string input)\n{\nreturn input.Length > 5;\n}\npublic void Test()\n{\nbool result = IsStringLong(\"abc\");\nAssert.Equal(false, result);\n}\nThere are two branches in the IsStringLong method: one for the situation when the\nlength of the string argument is greater than five characters, and the other one when\nit’s not. The test covers only one of these branches, so the branch coverage metric is\n1/2 = 0.5 = 50%. And it doesn’t matter how we represent the code under test—\nwhether we use an if statement as before or use the shorter notation. The branch cov-\nerage metric only accounts for the number of branches; it doesn’t take into consider-\nation how many lines of code it took to implement those branches.\n Figure 1.5 shows a helpful way to visualize this metric. You can represent all pos-\nsible paths the code under test can take as a graph and see how many of them have\nbeen traversed. IsStringLong has two such paths, and the test exercises only one\nof them.\nStart\nLength <= 5\nEnd\nLength > 5\nFigure 1.5\nThe method IsStringLong represented as a graph of possible \ncode paths. Test covers only one of the two code paths, thus providing 50% \nbranch coverage.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n12\nCHAPTER 1\nThe goal of unit testing\n1.3.3\nProblems with coverage metrics\nAlthough the branch coverage metric yields better results than code coverage, you still\ncan’t rely on either of them to determine the quality of your test suite, for two reasons:\nYou can’t guarantee that the test verifies all the possible outcomes of the system\nunder test.\nNo coverage metric can take into account code paths in external libraries.\nLet’s look more closely at each of these reasons.\nYOU CAN’T GUARANTEE THAT THE TEST VERIFIES ALL THE POSSIBLE OUTCOMES\nFor the code paths to be actually tested and not just exercised, your unit tests must\nhave appropriate assertions. In other words, you need to check that the outcome the\nsystem under test produces is the exact outcome you expect it to produce. Moreover,\nthis outcome may have several components; and for the coverage metrics to be mean-\ningful, you need to verify all of them.\n The next listing shows another version of the IsStringLong method. It records the\nlast result into a public WasLastStringLong property.\npublic static bool WasLastStringLong { get; private set; }\npublic static bool IsStringLong(string input)\n{\nbool result = input.Length > 5;\nWasLastStringLong = result;         \nreturn result;\n     \n}\npublic void Test()\n{\nbool result = IsStringLong(\"abc\");\nAssert.Equal(false, result);   \n}\nThe IsStringLong method now has two outcomes: an explicit one, which is encoded\nby the return value; and an implicit one, which is the new value of the property. And\nin spite of not verifying the second, implicit outcome, the coverage metrics would still\nshow the same results: 100% for the code coverage and 50% for the branch coverage.\nAs you can see, the coverage metrics don’t guarantee that the underlying code is\ntested, only that it has been executed at some point.\n An extreme version of this situation with partially tested outcomes is assertion-free\ntesting, which is when you write tests that don’t have any assertion statements in them\nwhatsoever. Here’s an example of assertion-free testing.\n \n \nListing 1.2\nVersion of IsStringLong that records the last result\nFirst \noutcome\nSecond \noutcome\nThe test verifies only \nthe second outcome.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n13\nUsing coverage metrics to measure test suite quality\npublic void Test()\n{\nbool result1 = IsStringLong(\"abc\");   \nbool result2 = IsStringLong(\"abcdef\");   \n}\nThis test has both code and branch coverage metrics showing 100%. But at the same\ntime, it is completely useless because it doesn’t verify anything.\nBut let’s say that you thoroughly verify each outcome of the code under test. Does this,\nin combination with the branch coverage metric, provide a reliable mechanism, which\nyou can use to determine the quality of your test suite? Unfortunately, no. \nListing 1.3\nA test with no assertions always passes.\nA story from the trenches\nThe concept of assertion-free testing might look like a dumb idea, but it does happen\nin the wild.\nYears ago, I worked on a project where management imposed a strict requirement of\nhaving 100% code coverage for every project under development. This initiative had\nnoble intentions. It was during the time when unit testing wasn’t as prevalent as it is\ntoday. Few people in the organization practiced it, and even fewer did unit testing\nconsistently.\nA group of developers had gone to a conference where many talks were devoted to\nunit testing. After returning, they decided to put their new knowledge into practice.\nUpper management supported them, and the great conversion to better programming\ntechniques began. Internal presentations were given. New tools were installed. And,\nmore importantly, a new company-wide rule was imposed: all development teams had\nto focus on writing tests exclusively until they reached the 100% code coverage mark.\nAfter they reached this goal, any code check-in that lowered the metric had to be\nrejected by the build systems.\nAs you might guess, this didn’t play out well. Crushed by this severe limitation, devel-\nopers started to seek ways to game the system. Naturally, many of them came to the\nsame realization: if you wrap all tests with try/catch blocks and don’t introduce any\nassertions in them, those tests are guaranteed to pass. People started to mindlessly\ncreate tests for the sake of meeting the mandatory 100% coverage requirement.\nNeedless to say, those tests didn’t add any value to the projects. Moreover, they\ndamaged the projects because of all the effort and time they steered away from pro-\nductive activities, and because of the upkeep costs required to maintain the tests\nmoving forward.\nEventually, the requirement was lowered to 90% and then to 80%; after some period\nof time, it was retracted altogether (for the better!).\nReturns true\nReturns false\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "page_number": 25
    },
    {
      "number": 5,
      "title": "Segment 5 (pages 36-43)",
      "start_page": 36,
      "end_page": 43,
      "detection_method": "topic_boundary",
      "content": "14\nCHAPTER 1\nThe goal of unit testing\nNO COVERAGE METRIC CAN TAKE INTO ACCOUNT CODE PATHS IN EXTERNAL LIBRARIES\nThe second problem with all coverage metrics is that they don’t take into account\ncode paths that external libraries go through when the system under test calls meth-\nods on them. Let’s take the following example:\npublic static int Parse(string input)\n{\nreturn int.Parse(input);\n}\npublic void Test()\n{\nint result = Parse(\"5\");\nAssert.Equal(5, result);\n}\nThe branch coverage metric shows 100%, and the test verifies all components of the\nmethod’s outcome. It has a single such component anyway—the return value. At the\nsame time, this test is nowhere near being exhaustive. It doesn’t take into account\nthe code paths the .NET Framework’s int.Parse method may go through. And\nthere are quite a number of code paths, even in this simple method, as you can see\nin figure 1.6.\nThe built-in integer type has plenty of branches that are hidden from the test and\nthat might lead to different results, should you change the method’s input parameter.\nHere are just a few possible arguments that can’t be transformed into an integer:\nNull value\nAn empty string\n“Not an int”\nA string that’s too large\nHidden\npart\nStart\nint.Parse\nnull\n“ ”\n“5”\n“not an int”\nEnd\nFigure 1.6\nHidden code paths of external libraries. Coverage metrics have no way to see how \nmany of them there are and how many of them your tests exercise.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n15\nWhat makes a successful test suite?\nYou can fall into numerous edge cases, and there’s no way to see if your tests account\nfor all of them.\n This is not to say that coverage metrics should take into account code paths in\nexternal libraries (they shouldn’t), but rather to show you that you can’t rely on\nthose metrics to see how good or bad your unit tests are. Coverage metrics can’t\npossibly tell whether your tests are exhaustive; nor can they say if you have enough\ntests. \n1.3.4\nAiming at a particular coverage number\nAt this point, I hope you can see that relying on coverage metrics to determine the\nquality of your test suite is not enough. It can also lead to dangerous territory if you\nstart making a specific coverage number a target, be it 100%, 90%, or even a moder-\nate 70%. The best way to view a coverage metric is as an indicator, not a goal in and\nof itself.\n Think of a patient in a hospital. Their high temperature might indicate a fever and\nis a helpful observation. But the hospital shouldn’t make the proper temperature of\nthis patient a goal to target by any means necessary. Otherwise, the hospital might end\nup with the quick and “efficient” solution of installing an air conditioner next to the\npatient and regulating their temperature by adjusting the amount of cold air flowing\nonto their skin. Of course, this approach doesn’t make any sense.\n Likewise, targeting a specific coverage number creates a perverse incentive that\ngoes against the goal of unit testing. Instead of focusing on testing the things that\nmatter, people start to seek ways to attain this artificial target. Proper unit testing is dif-\nficult enough already. Imposing a mandatory coverage number only distracts develop-\ners from being mindful about what they test, and makes proper unit testing even\nharder to achieve.\nTIP\nIt’s good to have a high level of coverage in core parts of your system.\nIt’s bad to make this high level a requirement. The difference is subtle but\ncritical.\nLet me repeat myself: coverage metrics are a good negative indicator, but a bad posi-\ntive one. Low coverage numbers—say, below 60%—are a certain sign of trouble. They\nmean there’s a lot of untested code in your code base. But high numbers don’t mean\nanything. Thus, measuring the code coverage should be only a first step on the way to\na quality test suite. \n1.4\nWhat makes a successful test suite?\nI’ve spent most of this chapter discussing improper ways to measure the quality of a\ntest suite: using coverage metrics. What about a proper way? How should you mea-\nsure your test suite’s quality? The only reliable way is to evaluate each test in the\nsuite individually, one by one. Of course, you don’t have to evaluate all of them at\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n16\nCHAPTER 1\nThe goal of unit testing\nonce; that could be quite a large undertaking and require significant upfront effort.\nYou can perform this evaluation gradually. The point is that there’s no automated\nway to see how good your test suite is. You have to apply your personal judgment.\n Let’s look at a broader picture of what makes a test suite successful as a whole.\n(We’ll dive into the specifics of differentiating between good and bad tests in chapter 4.)\nA successful test suite has the following properties:\nIt’s integrated into the development cycle.\nIt targets only the most important parts of your code base.\nIt provides maximum value with minimum maintenance costs.\n1.4.1\nIt’s integrated into the development cycle\nThe only point in having automated tests is if you constantly use them. All tests should\nbe integrated into the development cycle. Ideally, you should execute them on every\ncode change, even the smallest one. \n1.4.2\nIt targets only the most important parts of your code base\nJust as all tests are not created equal, not all parts of your code base are worth the\nsame attention in terms of unit testing. The value the tests provide is not only in how\nthose tests themselves are structured, but also in the code they verify.\n It’s important to direct your unit testing efforts to the most critical parts of the sys-\ntem and verify the others only briefly or indirectly. In most applications, the most\nimportant part is the part that contains business logic—the domain model.1 Testing\nbusiness logic gives you the best return on your time investment.\n All other parts can be divided into three categories:\nInfrastructure code\nExternal services and dependencies, such as the database and third-party systems\nCode that glues everything together\nSome of these other parts may still need thorough unit testing, though. For example,\nthe infrastructure code may contain complex and important algorithms, so it would\nmake sense to cover them with a lot of tests, too. But in general, most of your attention\nshould be spent on the domain model.\n Some of your tests, such as integration tests, can go beyond the domain model and\nverify how the system works as a whole, including the noncritical parts of the code\nbase. And that’s fine. But the focus should remain on the domain model.\n Note that in order to follow this guideline, you should isolate the domain model\nfrom the non-essential parts of the code base. You have to keep the domain model\nseparated from all other application concerns so you can focus your unit testing\n1 See Domain-Driven Design: Tackling Complexity in the Heart of Software by Eric Evans (Addison-Wesley, 2003).\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n17\nWhat you will learn in this book\nefforts on that domain model exclusively. We talk about all this in detail in part 2 of\nthe book. \n1.4.3\nIt provides maximum value with minimum maintenance costs\nThe most difficult part of unit testing is achieving maximum value with minimum\nmaintenance costs. That’s the main focus of this book.\n It’s not enough to incorporate tests into a build system, and it’s not enough to\nmaintain high test coverage of the domain model. It’s also crucial to keep in the suite\nonly the tests whose value exceeds their upkeep costs by a good margin.\n This last attribute can be divided in two:\nRecognizing a valuable test (and, by extension, a test of low value)\nWriting a valuable test\nAlthough these skills may seem similar, they’re different by nature. To recognize a test\nof high value, you need a frame of reference. On the other hand, writing a valuable\ntest requires you to also know code design techniques. Unit tests and the underlying\ncode are highly intertwined, and it’s impossible to create valuable tests without put-\nting significant effort into the code base they cover.\n You can view it as the difference between recognizing a good song and being able\nto compose one. The amount of effort required to become a composer is asymmetri-\ncally larger than the effort required to differentiate between good and bad music. The\nsame is true for unit tests. Writing a new test requires more effort than examining an\nexisting one, mostly because you don’t write tests in a vacuum: you have to take into\naccount the underlying code. And so although I focus on unit tests, I also devote a sig-\nnificant portion of this book to discussing code design. \n1.5\nWhat you will learn in this book\nThis book teaches a frame of reference that you can use to analyze any test in your test\nsuite. This frame of reference is foundational. After learning it, you’ll be able to look\nat many of your tests in a new light and see which of them contribute to the project\nand which must be refactored or gotten rid of altogether.\n After setting this stage (chapter 4), the book analyzes the existing unit testing tech-\nniques and practices (chapters 4–6, and part of 7). It doesn’t matter whether you’re\nfamiliar with those techniques and practices. If you are familiar with them, you’ll see\nthem from a new angle. Most likely, you already get them at the intuitive level. This\nbook can help you articulate why the techniques and best practices you’ve been using\nall along are so helpful.\n Don’t underestimate this skill. The ability to clearly communicate your ideas to col-\nleagues is priceless. A software developer—even a great one—rarely gets full credit for\na design decision if they can’t explain why, exactly, that decision was made. This book\ncan help you transform your knowledge from the realm of the unconscious to some-\nthing you are able to talk about with anyone.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n18\nCHAPTER 1\nThe goal of unit testing\n If you don’t have much experience with unit testing techniques and best practices,\nyou’ll learn a lot. In addition to the frame of reference that you can use to analyze any\ntest in a test suite, the book teaches\nHow to refactor the test suite along with the production code it covers\nHow to apply different styles of unit testing\nUsing integration tests to verify the behavior of the system as a whole\nIdentifying and avoiding anti-patterns in unit tests\nIn addition to unit tests, this book covers the entire topic of automated testing, so\nyou’ll also learn about integration and end-to-end tests.\n I use C# and .NET in my code samples, but you don’t have to be a C# professional\nto read this book; C# is just the language that I happen to work with the most. All\nthe concepts I talk about are non-language-specific and can be applied to any other\nobject-oriented language, such as Java or C++.\nSummary\nCode tends to deteriorate. Each time you change something in a code base, the\namount of disorder in it, or entropy, increases. Without proper care, such as\nconstant cleaning and refactoring, the system becomes increasingly complex\nand disorganized. Tests help overturn this tendency. They act as a safety net— a\ntool that provides insurance against the vast majority of regressions.\nIt’s important to write unit tests. It’s equally important to write good unit tests.\nThe end result for projects with bad tests or no tests is the same: either stagna-\ntion or a lot of regressions with every new release.\nThe goal of unit testing is to enable sustainable growth of the software project.\nA good unit test suite helps avoid the stagnation phase and maintain the devel-\nopment pace over time. With such a suite, you’re confident that your changes\nwon’t lead to regressions. This, in turn, makes it easier to refactor the code or\nadd new features.\nAll tests are not created equal. Each test has a cost and a benefit component,\nand you need to carefully weigh one against the other. Keep only tests of posi-\ntive net value in the suite, and get rid of all others. Both the application code\nand the test code are liabilities, not assets.\nThe ability to unit test code is a good litmus test, but it only works in one direc-\ntion. It’s a good negative indicator (if you can’t unit test the code, it’s of poor\nquality) but a bad positive one (the ability to unit test the code doesn’t guaran-\ntee its quality).\nLikewise, coverage metrics are a good negative indicator but a bad positive one.\nLow coverage numbers are a certain sign of trouble, but a high coverage num-\nber doesn’t automatically mean your test suite is of high quality.\nBranch coverage provides better insight into the completeness of the test suite\nbut still can’t indicate whether the suite is good enough. It doesn’t take into\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n19\nSummary\naccount the presence of assertions, and it can’t account for code paths in third-\nparty libraries that your code base uses.\nImposing a particular coverage number creates a perverse incentive. It’s good\nto have a high level of coverage in core parts of your system, but it’s bad to make\nthis high level a requirement.\nA successful test suite exhibits the following attributes:\n– It is integrated into the development cycle.\n– It targets only the most important parts of your code base.\n– It provides maximum value with minimum maintenance costs.\nThe only way to achieve the goal of unit testing (that is, enabling sustainable\nproject growth) is to\n– Learn how to differentiate between a good and a bad test.\n– Be able to refactor a test to make it more valuable.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n20\nWhat is a unit test?\nAs mentioned in chapter 1, there are a surprising number of nuances in the defini-\ntion of a unit test. Those nuances are more important than you might think—so\nmuch so that the differences in interpreting them have led to two distinct views on\nhow to approach unit testing.\n These views are known as the classical and the London schools of unit testing.\nThe classical school is called “classical” because it’s how everyone originally\napproached unit testing and test-driven development. The London school takes\nroot in the programming community in London. The discussion in this chapter\nabout the differences between the classical and London styles lays the foundation\nfor chapter 5, where I cover the topic of mocks and test fragility in detail.\nThis chapter covers\nWhat a unit test is\nThe differences between shared, private, \nand volatile dependencies\nThe two schools of unit testing: classical \nand London\nThe differences between unit, integration, \nand end-to-end tests\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n21\nThe definition of “unit test”\n Let’s start by defining a unit test, with all due caveats and subtleties. This definition\nis the key to the difference between the classical and London schools.\n2.1\nThe definition of “unit test”\nThere are a lot of definitions of a unit test. Stripped of their non-essential bits, the\ndefinitions all have the following three most important attributes. A unit test is an\nautomated test that\nVerifies a small piece of code (also known as a unit),\nDoes it quickly,\nAnd does it in an isolated manner.\nThe first two attributes here are pretty non-controversial. There might be some dis-\npute as to what exactly constitutes a fast unit test because it’s a highly subjective mea-\nsure. But overall, it’s not that important. If your test suite’s execution time is good\nenough for you, it means your tests are quick enough.\n What people have vastly different opinions about is the third attribute. The isola-\ntion issue is the root of the differences between the classical and London schools of\nunit testing. As you will see in the next section, all other differences between the two\nschools flow naturally from this single disagreement on what exactly isolation means. I\nprefer the classical style for the reasons I describe in section 2.3.\n2.1.1\nThe isolation issue: The London take\nWhat does it mean to verify a piece of code—a unit—in an isolated manner? The Lon-\ndon school describes it as isolating the system under test from its collaborators. It\nmeans if a class has a dependency on another class, or several classes, you need to\nreplace all such dependencies with test doubles. This way, you can focus on the class\nunder test exclusively by separating its behavior from any external influence.\n \nThe classical and London schools of unit testing\nThe classical approach is also referred to as the Detroit and, sometimes, the classi-\ncist approach to unit testing. Probably the most canonical book on the classical\nschool is the one by Kent Beck: Test-Driven Development: By Example (Addison-Wesley\nProfessional, 2002).\nThe London style is sometimes referred to as mockist. Although the term mockist is\nwidespread, people who adhere to this style of unit testing generally don’t like it, so\nI call it the London style throughout this book. The most prominent proponents of this\napproach are Steve Freeman and Nat Pryce. I recommend their book, Growing Object-\nOriented Software, Guided by Tests (Addison-Wesley Professional, 2009), as a good\nsource on this subject.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "page_number": 36
    },
    {
      "number": 6,
      "title": "Segment 6 (pages 44-51)",
      "start_page": 44,
      "end_page": 51,
      "detection_method": "topic_boundary",
      "content": "22\nCHAPTER 2\nWhat is a unit test?\nDEFINITION\nA test double is an object that looks and behaves like its release-\nintended counterpart but is actually a simplified version that reduces the\ncomplexity and facilitates testing. This term was introduced by Gerard Mesza-\nros in his book, xUnit Test Patterns: Refactoring Test Code (Addison-Wesley, 2007).\nThe name itself comes from the notion of a stunt double in movies.\nFigure 2.1 shows how the isolation is usually achieved. A unit test that would otherwise\nverify the system under test along with all its dependencies now can do that separately\nfrom those dependencies.\nOne benefit of this approach is that if the test fails, you know for sure which part of\nthe code base is broken: it’s the system under test. There could be no other suspects,\nbecause all of the class’s neighbors are replaced with the test doubles.\n Another benefit is the ability to split the object graph—the web of communicating\nclasses solving the same problem. This web may become quite complicated: every class\nin it may have several immediate dependencies, each of which relies on dependencies\nof their own, and so on. Classes may even introduce circular dependencies, where the\nchain of dependency eventually comes back to where it started.\nTest double 2\nDependency 1\nDependency 2\nSystem under test\nSystem under test\nTest double 1\nFigure 2.1\nReplacing the dependencies \nof the system under test with test \ndoubles allows you to focus on verifying \nthe system under test exclusively, as \nwell as split the otherwise large \ninterconnected object graph.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n23\nThe definition of “unit test”\n Trying to test such an interconnected code base is hard without test doubles. Pretty\nmuch the only choice you are left with is re-creating the full object graph in the test,\nwhich might not be a feasible task if the number of classes in it is too high.\n With test doubles, you can put a stop to this. You can substitute the immediate\ndependencies of a class; and, by extension, you don’t have to deal with the dependen-\ncies of those dependencies, and so on down the recursion path. You are effectively\nbreaking up the graph—and that can significantly reduce the amount of preparations\nyou have to do in a unit test.\n And let’s not forget another small but pleasant side benefit of this approach to\nunit test isolation: it allows you to introduce a project-wide guideline of testing only\none class at a time, which establishes a simple structure in the whole unit test suite.\nYou no longer have to think much about how to cover your code base with tests.\nHave a class? Create a corresponding class with unit tests! Figure 2.2 shows how it\nusually looks.\nLet’s now look at some examples. Since the classical style probably looks more familiar\nto most people, I’ll show sample tests written in that style first and then rewrite them\nusing the London approach.\n Let’s say that we operate an online store. There’s just one simple use case in our\nsample application: a customer can purchase a product. When there’s enough inven-\ntory in the store, the purchase is deemed to be successful, and the amount of the\nproduct in the store is reduced by the purchase’s amount. If there’s not enough prod-\nuct, the purchase is not successful, and nothing happens in the store.\n Listing 2.1 shows two tests verifying that a purchase succeeds only when there’s\nenough inventory in the store. The tests are written in the classical style and use the\nClass 1\nClass 2\nClass 3\nUnit tests\nProduction code\nClass 1 Tests\nClass 2 Tests\nClass 3 Tests\nFigure 2.2\nIsolating the class under test from its dependencies helps establish a simple \ntest suite structure: one class with tests for each class in the production code.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n24\nCHAPTER 2\nWhat is a unit test?\ntypical three-phase sequence: arrange, act, and assert (AAA for short—I talk more\nabout this sequence in chapter 3).\n[Fact]\npublic void Purchase_succeeds_when_enough_inventory()\n{\n// Arrange\nvar store = new Store();\nstore.AddInventory(Product.Shampoo, 10);\nvar customer = new Customer();\n// Act\nbool success = customer.Purchase(store, Product.Shampoo, 5);\n// Assert\nAssert.True(success);\nAssert.Equal(5, store.GetInventory(Product.Shampoo));   \n}\n[Fact]\npublic void Purchase_fails_when_not_enough_inventory()\n{\n// Arrange\nvar store = new Store();\nstore.AddInventory(Product.Shampoo, 10);\nvar customer = new Customer();\n// Act\nbool success = customer.Purchase(store, Product.Shampoo, 15);\n// Assert\nAssert.False(success);\nAssert.Equal(10, store.GetInventory(Product.Shampoo));   \n}\npublic enum Product\n{\nShampoo,\nBook\n}\nAs you can see, the arrange part is where the tests make ready all dependencies and\nthe system under test. The call to customer.Purchase() is the act phase, where you\nexercise the behavior you want to verify. The assert statements are the verification\nstage, where you check to see if the behavior led to the expected results.\n During the arrange phase, the tests put together two kinds of objects: the system\nunder test (SUT) and one collaborator. In this case, Customer is the SUT and Store is\nthe collaborator. We need the collaborator for two reasons:\nListing 2.1\nTests written using the classical style of unit testing\nReduces the \nproduct amount in \nthe store by five\nThe product \namount in the \nstore remains \nunchanged.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n25\nThe definition of “unit test”\nTo get the method under test to compile, because customer.Purchase() requires\na Store instance as an argument\nFor the assertion phase, since one of the results of customer.Purchase() is a\npotential decrease in the product amount in the store \nProduct.Shampoo and the numbers 5 and 15 are constants.\nDEFINITION\nA method under test (MUT) is a method in the SUT called by the\ntest. The terms MUT and SUT are often used as synonyms, but normally, MUT\nrefers to a method while SUT refers to the whole class.\nThis code is an example of the classical style of unit testing: the test doesn’t replace\nthe collaborator (the Store class) but rather uses a production-ready instance of it.\nOne of the natural outcomes of this style is that the test now effectively verifies both\nCustomer and Store, not just Customer. Any bug in the inner workings of Store that\naffects Customer will lead to failing these unit tests, even if Customer still works cor-\nrectly. The two classes are not isolated from each other in the tests.\n Let’s now modify the example toward the London style. I’ll take the same tests and\nreplace the Store instances with test doubles—specifically, mocks.\n I use Moq (https://github.com/moq/moq4) as the mocking framework, but you\ncan find several equally good alternatives, such as NSubstitute (https://github.com/\nnsubstitute/NSubstitute). All object-oriented languages have analogous frameworks.\nFor instance, in the Java world, you can use Mockito, JMock, or EasyMock.\nDEFINITION\nA mock is a special kind of test double that allows you to examine\ninteractions between the system under test and its collaborators.\nWe’ll get back to the topic of mocks, stubs, and the differences between them in later\nchapters. For now, the main thing to remember is that mocks are a subset of test dou-\nbles. People often use the terms test double and mock as synonyms, but technically, they\nare not (more on this in chapter 5):\nTest double is an overarching term that describes all kinds of non-production-\nready, fake dependencies in a test.\nMock is just one kind of such dependencies.\nThe next listing shows how the tests look after isolating Customer from its collabora-\ntor, Store.\n[Fact]\npublic void Purchase_succeeds_when_enough_inventory()\n{\n// Arrange\nvar storeMock = new Mock<IStore>();\nstoreMock\nListing 2.2\nTests written using the London style of unit testing\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n26\nCHAPTER 2\nWhat is a unit test?\n.Setup(x => x.HasEnoughInventory(Product.Shampoo, 5))\n.Returns(true);\nvar customer = new Customer();\n// Act\nbool success = customer.Purchase(\nstoreMock.Object, Product.Shampoo, 5);\n// Assert\nAssert.True(success);\nstoreMock.Verify(\nx => x.RemoveInventory(Product.Shampoo, 5),\nTimes.Once);\n}\n[Fact]\npublic void Purchase_fails_when_not_enough_inventory()\n{\n// Arrange\nvar storeMock = new Mock<IStore>();\nstoreMock\n.Setup(x => x.HasEnoughInventory(Product.Shampoo, 5))\n.Returns(false);\nvar customer = new Customer();\n// Act\nbool success = customer.Purchase(\nstoreMock.Object, Product.Shampoo, 5);\n// Assert\nAssert.False(success);\nstoreMock.Verify(\nx => x.RemoveInventory(Product.Shampoo, 5),\nTimes.Never);\n}\nNote how different these tests are from those written in the classical style. In the\narrange phase, the tests no longer instantiate a production-ready instance of Store\nbut instead create a substitution for it, using Moq’s built-in class Mock<T>.\n Furthermore, instead of modifying the state of Store by adding a shampoo inven-\ntory to it, we directly tell the mock how to respond to calls to HasEnoughInventory().\nThe mock reacts to this request the way the tests need, regardless of the actual state of\nStore. In fact, the tests no longer use Store—we have introduced an IStore interface\nand are mocking that interface instead of the Store class.\n In chapter 8, I write in detail about working with interfaces. For now, just make a\nnote that interfaces are required for isolating the system under test from its collabora-\ntors. (You can also mock a concrete class, but that’s an anti-pattern; I cover this topic\nin chapter 11.)\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n27\nThe definition of “unit test”\n The assertion phase has changed too, and that’s where the key difference lies. We\nstill check the output from customer.Purchase as before, but the way we verify that\nthe customer did the right thing to the store is different. Previously, we did that by\nasserting against the store’s state. Now, we examine the interactions between Customer\nand Store: the tests check to see if the customer made the correct call on the store.\nWe do this by passing the method the customer should call on the store (x.Remove-\nInventory) as well as the number of times it should do that. If the purchases succeeds,\nthe customer should call this method once (Times.Once). If the purchases fails, the\ncustomer shouldn’t call it at all (Times.Never). \n2.1.2\nThe isolation issue: The classical take\nTo reiterate, the London style approaches the isolation requirement by segregating the\npiece of code under test from its collaborators with the help of test doubles: specifically,\nmocks. Interestingly enough, this point of view also affects your standpoint on what con-\nstitutes a small piece of code (a unit). Here are all the attributes of a unit test once again:\nA unit test verifies a small piece of code (a unit),\nDoes it quickly,\nAnd does it in an isolated manner.\nIn addition to the third attribute leaving room for interpretation, there’s some room\nin the possible interpretations of the first attribute as well. How small should a small\npiece of code be? As you saw from the previous section, if you adopt the position of\nisolating every individual class, then it’s natural to accept that the piece of code under\ntest should also be a single class, or a method inside that class. It can’t be more than\nthat due to the way you approach the isolation issue. In some cases, you might test a\ncouple of classes at once; but in general, you’ll always strive to maintain this guideline\nof unit testing one class at a time.\n As I mentioned earlier, there’s another way to interpret the isolation attribute—\nthe classical way. In the classical approach, it’s not the code that needs to be tested in\nan isolated manner. Instead, unit tests themselves should be run in isolation from\neach other. That way, you can run the tests in parallel, sequentially, and in any order,\nwhatever fits you best, and they still won’t affect each other’s outcome.\n Isolating tests from each other means it’s fine to exercise several classes at once as\nlong as they all reside in the memory and don’t reach out to a shared state, through\nwhich the tests can communicate and affect each other’s execution context. Typical\nexamples of such a shared state are out-of-process dependencies—the database, the\nfile system, and so on.\n For instance, one test could create a customer in the database as part of its arrange\nphase, and another test would delete it as part of its own arrange phase, before the\nfirst test completes executing. If you run these two tests in parallel, the first test will\nfail, not because the production code is broken, but rather because of the interfer-\nence from the second test.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n28\nCHAPTER 2\nWhat is a unit test?\nThis take on the isolation issue entails a much more modest view on the use of mocks\nand other test doubles. You can still use them, but you normally do that for only those\ndependencies that introduce a shared state between tests. Figure 2.3 shows how it looks.\n Note that shared dependencies are shared between unit tests, not between classes\nunder test (units). In that sense, a singleton dependency is not shared as long as you\nare able to create a new instance of it in each test. While there’s only one instance of a\nShared, private, and out-of-process dependencies \nA shared dependency is a dependency that is shared between tests and provides\nmeans for those tests to affect each other’s outcome. A typical example of shared\ndependencies is a static mutable field. A change to such a field is visible across all\nunit tests running within the same process. A database is another typical example of\na shared dependency.\nA private dependency is a dependency that is not shared.\nAn out-of-process dependency is a dependency that runs outside the application’s\nexecution process; it’s a proxy to data that is not yet in the memory. An out-of-process\ndependency corresponds to a shared dependency in the vast majority of cases, but\nnot always. For example, a database is both out-of-process and shared. But if you\nlaunch that database in a Docker container before each test run, that would make\nthis dependency out-of-process but not shared, since tests no longer work with the\nsame instance of it. Similarly, a read-only database is also out-of-process but not\nshared, even if it’s reused by tests. Tests can’t mutate data in such a database and\nthus can’t affect each other’s outcome.\nPrivate dependency; keep\nShared dependency; replace\nFile system\nSystem under test\nDatabase\nTest\nShared dependency; replace\nAnother class\nFigure 2.3\nIsolating unit tests from each other entails isolating the class under test \nfrom shared dependencies only. Private dependencies can be kept intact.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n29\nThe definition of “unit test”\nsingleton in the production code, tests may very well not follow this pattern and not\nreuse that singleton. Thus, such a dependency would be private.\n For example, there’s normally only one instance of a configuration class, which is\nreused across all production code. But if it’s injected into the SUT the way all other\ndependencies are, say, via a constructor, you can create a new instance of it in each\ntest; you don’t have to maintain a single instance throughout the test suite. You can’t\ncreate a new file system or a database, however; they must be either shared between\ntests or substituted away with test doubles.\nAnother reason for substituting shared dependencies is to increase the test execution\nspeed. Shared dependencies almost always reside outside the execution process, while\nprivate dependencies usually don’t cross that boundary. Because of that, calls to\nshared dependencies, such as a database or the file system, take more time than calls\nto private dependencies. And since the necessity to run quickly is the second attribute\nof the unit test definition, such calls push the tests with shared dependencies out of\nthe realm of unit testing and into the area of integration testing. I talk more about\nintegration testing later in this chapter.\n This alternative view of isolation also leads to a different take on what constitutes a\nunit (a small piece of code). A unit doesn’t necessarily have to be limited to a class.\nShared vs. volatile dependencies \nAnother term has a similar, yet not identical, meaning: volatile dependency. I recom-\nmend Dependency Injection: Principles, Practices, Patterns by Steven van Deursen and\nMark Seemann (Manning Publications, 2018) as a go-to book on the topic of depen-\ndency management.\nA volatile dependency is a dependency that exhibits one of the following properties:\nIt introduces a requirement to set up and configure a runtime environment in\naddition to what is installed on a developer’s machine by default. Databases\nand API services are good examples here. They require additional setup and\nare not installed on machines in your organization by default.\nIt contains nondeterministic behavior. An example would be a random num-\nber generator or a class returning the current date and time. These depen-\ndencies are non-deterministic because they provide different results on each\ninvocation.\nAs you can see, there’s an overlap between the notions of shared and volatile depen-\ndencies. For example, a dependency on the database is both shared and volatile. But\nthat’s not the case for the file system. The file system is not volatile because it is\ninstalled on every developer’s machine and it behaves deterministically in the vast\nmajority of cases. Still, the file system introduces a means by which the unit tests\ncan interfere with each other’s execution context; hence it is shared. Likewise, a ran-\ndom number generator is volatile, but because you can supply a separate instance\nof it to each test, it isn’t shared.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "page_number": 44
    },
    {
      "number": 7,
      "title": "Segment 7 (pages 52-61)",
      "start_page": 52,
      "end_page": 61,
      "detection_method": "topic_boundary",
      "content": "30\nCHAPTER 2\nWhat is a unit test?\nYou can just as well unit test a group of classes, as long as none of them is a shared\ndependency. \n2.2\nThe classical and London schools of unit testing\nAs you can see, the root of the differences between the London and classical schools is\nthe isolation attribute. The London school views it as isolation of the system under test\nfrom its collaborators, whereas the classical school views it as isolation of unit tests\nthemselves from each other.\n This seemingly minor difference has led to a vast disagreement about how to\napproach unit testing, which, as you already know, produced the two schools of thought.\nOverall, the disagreement between the schools spans three major topics:\nThe isolation requirement\nWhat constitutes a piece of code under test (a unit)\nHandling dependencies\nTable 2.1 sums it all up.\n2.2.1\nHow the classical and London schools handle dependencies\nNote that despite the ubiquitous use of test doubles, the London school still allows\nfor using some dependencies in tests as-is. The litmus test here is whether a depen-\ndency is mutable. It’s fine not to substitute objects that don’t ever change—\nimmutable objects.\n And you saw in the earlier examples that, when I refactored the tests toward the\nLondon style, I didn’t replace the Product instances with mocks but rather used\nthe real objects, as shown in the following code (repeated from listing 2.2 for your\nconvenience):\n[Fact]\npublic void Purchase_fails_when_not_enough_inventory()\n{\n// Arrange\nvar storeMock = new Mock<IStore>();\nstoreMock\n.Setup(x => x.HasEnoughInventory(Product.Shampoo, 5))\n.Returns(false);\nvar customer = new Customer();\nTable 2.1\nThe differences between the London and classical schools of unit testing, summed up by the\napproach to isolation, the size of a unit, and the use of test doubles\nIsolation of\nA unit is\nUses test doubles for\nLondon school\nUnits\nA class\nAll but immutable dependencies\nClassical school\nUnit tests\nA class or a set of classes\nShared dependencies\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n31\nThe classical and London schools of unit testing\n// Act\nbool success = customer.Purchase(storeMock.Object, Product.Shampoo, 5);\n// Assert\nAssert.False(success);\nstoreMock.Verify(\nx => x.RemoveInventory(Product.Shampoo, 5),\nTimes.Never);\n}\nOf the two dependencies of Customer, only Store contains an internal state that can\nchange over time. The Product instances are immutable (Product itself is a C#\nenum). Hence I substituted the Store instance only.\n It makes sense, if you think about it. You wouldn’t use a test double for the 5\nnumber in the previous test either, would you? That’s because it is also immutable—\nyou can’t possibly modify this number. Note that I’m not talking about a variable\ncontaining the number, but rather the number itself. In the statement Remove-\nInventory(Product.Shampoo, 5), we don’t even use a variable; 5 is declared right\naway. The same is true for Product.Shampoo.\n Such immutable objects are called value objects or values. Their main trait is that\nthey have no individual identity; they are identified solely by their content. As a corol-\nlary, if two such objects have the same content, it doesn’t matter which of them you’re\nworking with: these instances are interchangeable. For example, if you’ve got two 5\nintegers, you can use them in place of one another. The same is true for the products\nin our case: you can reuse a single Product.Shampoo instance or declare several of\nthem—it won’t make any difference. These instances will have the same content and\nthus can be used interchangeably.\n Note that the concept of a value object is language-agnostic and doesn’t require a\nparticular programming language or framework. You can read more about value\nobjects in my article “Entity vs. Value Object: The ultimate list of differences” at\nhttp://mng.bz/KE9O.\n Figure 2.4 shows the categorization of dependencies and how both schools of unit\ntesting treat them. A dependency can be either shared or private. A private dependency, in\nturn, can be either mutable or immutable. In the latter case, it is called a value object. For\nexample, a database is a shared dependency—its internal state is shared across all\nautomated tests (that don’t replace it with a test double). A Store instance is a private\ndependency that is mutable. And a Product instance (or an instance of a number 5,\nfor that matter) is an example of a private dependency that is immutable—a value\nobject. All shared dependencies are mutable, but for a mutable dependency to be\nshared, it has to be reused by tests.\n \n \n \n \nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n32\nCHAPTER 2\nWhat is a unit test?\nI’m repeating table 2.1 with the differences between the schools for your convenience.\nIsolation of\nA unit is\nUses test doubles for\nLondon school\nUnits\nA class\nAll but immutable dependencies\nClassical school\nUnit tests\nA class or a set of classes\nShared dependencies\nCollaborator vs. dependency\nA collaborator is a dependency that is either shared or mutable. For example, a class\nproviding access to the database is a collaborator since the database is a shared\ndependency. Store is a collaborator too, because its state can change over time.\nProduct and number 5 are also dependencies, but they’re not collaborators. They’re\nvalues or value objects.\nA typical class may work with dependencies of both types: collaborators and values.\nLook at this method call:\ncustomer.Purchase(store, Product.Shampoo, 5)\nHere we have three dependencies. One of them (store) is a collaborator, and the\nother two (Product.Shampoo, 5) are not.\nPrivate\nValue object\nMutable\nCollaborator,\nreplaced in the\nLondon school\nReplaced in the\nclassic school\nShared\nDependency\nFigure 2.4\nThe hierarchy of dependencies. The classical school advocates for \nreplacing shared dependencies with test doubles. The London school advocates for the \nreplacement of private dependencies as well, as long as they are mutable.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n33\nThe classical and London schools of unit testing\nAnd let me reiterate one point about the types of dependencies. Not all out-of-process\ndependencies fall into the category of shared dependencies. A shared dependency\nalmost always resides outside the application’s process, but the opposite isn’t true (see\nfigure 2.5). In order for an out-of-process dependency to be shared, it has to provide\nmeans for unit tests to communicate with each other. The communication is done\nthrough modifications of the dependency’s internal state. In that sense, an immutable\nout-of-process dependency doesn’t provide such a means. The tests simply can’t mod-\nify anything in it and thus can’t interfere with each other’s execution context.\nFor example, if there’s an API somewhere that returns a catalog of all products the orga-\nnization sells, this isn’t a shared dependency as long as the API doesn’t expose the\nfunctionality to change the catalog. It’s true that such a dependency is volatile and sits\noutside the application’s boundary, but since the tests can’t affect the data it returns, it\nisn’t shared. This doesn’t mean you have to include such a dependency in the testing\nscope. In most cases, you still need to replace it with a test double to keep the test fast.\nBut if the out-of-process dependency is quick enough and the connection to it is stable,\nyou can make a good case for using it as-is in the tests.\n Having that said, in this book, I use the terms shared dependency and out-of-process\ndependency interchangeably unless I explicitly state otherwise. In real-world projects,\nyou rarely have a shared dependency that isn’t out-of-process. If a dependency is in-\nprocess, you can easily supply a separate instance of it to each test; there’s no need to\nshare it between tests. Similarly, you normally don’t encounter an out-of-process\nShared\ndependencies\nOut-of-process\ndependencies\nSingleton\nDatabase\nRead-only API service\nFigure 2.5\nThe relation between shared and out-of-process dependencies. An example of a \ndependency that is shared but not out-of-process is a singleton (an instance that is reused by \nall tests) or a static field in a class. A database is shared and out-of-process—it resides outside \nthe main process and is mutable. A read-only API is out-of-process but not shared, since tests \ncan’t modify it and thus can’t affect each other’s execution flow.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n34\nCHAPTER 2\nWhat is a unit test?\ndependency that’s not shared. Most such dependencies are mutable and thus can be\nmodified by tests.\n With this foundation of definitions, let’s contrast the two schools on their merits. \n2.3\nContrasting the classical and London schools \nof unit testing\nTo reiterate, the main difference between the classical and London schools is in how\nthey treat the isolation issue in the definition of a unit test. This, in turn, spills over to\nthe treatment of a unit—the thing that should be put under test—and the approach\nto handling dependencies.\n As I mentioned previously, I prefer the classical school of unit testing. It tends to\nproduce tests of higher quality and thus is better suited for achieving the ultimate goal\nof unit testing, which is the sustainable growth of your project. The reason is fragility:\ntests that use mocks tend to be more brittle than classical tests (more on this in chap-\nter 5). For now, let’s take the main selling points of the London school and evaluate\nthem one by one.\n The London school’s approach provides the following benefits:\nBetter granularity. The tests are fine-grained and check only one class at a time.\nIt’s easier to unit test a larger graph of interconnected classes. Since all collaborators\nare replaced by test doubles, you don’t need to worry about them at the time of\nwriting the test.\nIf a test fails, you know for sure which functionality has failed. Without the class’s\ncollaborators, there could be no suspects other than the class under test itself.\nOf course, there may still be situations where the system under test uses a\nvalue object and it’s the change in this value object that makes the test fail.\nBut these cases aren’t that frequent because all other dependencies are elimi-\nnated in tests.\n2.3.1\nUnit testing one class at a time\nThe point about better granularity relates to the discussion about what constitutes a\nunit in unit testing. The London school considers a class as such a unit. Coming from\nan object-oriented programming background, developers usually regard classes as the\natomic building blocks that lie at the foundation of every code base. This naturally\nleads to treating classes as the atomic units to be verified in tests, too. This tendency is\nunderstandable but misleading.\nTIP\nTests shouldn’t verify units of code. Rather, they should verify units of\nbehavior: something that is meaningful for the problem domain and, ideally,\nsomething that a business person can recognize as useful. The number of\nclasses it takes to implement such a unit of behavior is irrelevant. The unit\ncould span across multiple classes or only one class, or even take up just a\ntiny method.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n35\nContrasting the classical and London schools of unit testing\nAnd so, aiming at better code granularity isn’t helpful. As long as the test checks a sin-\ngle unit of behavior, it’s a good test. Targeting something less than that can in fact\ndamage your unit tests, as it becomes harder to understand exactly what these tests\nverify. A test should tell a story about the problem your code helps to solve, and this story should\nbe cohesive and meaningful to a non-programmer.\n For instance, this is an example of a cohesive story:\nWhen I call my dog, he comes right to me.\nNow compare it to the following:\nWhen I call my dog, he moves his front left leg first, then the front right \nleg, his head turns, the tail start wagging...\nThe second story makes much less sense. What’s the purpose of all those movements?\nIs the dog coming to me? Or is he running away? You can’t tell. This is what your tests\nstart to look like when you target individual classes (the dog’s legs, head, and tail)\ninstead of the actual behavior (the dog coming to his master). I talk more about this\ntopic of observable behavior and how to differentiate it from internal implementation\ndetails in chapter 5. \n2.3.2\nUnit testing a large graph of interconnected classes\nThe use of mocks in place of real collaborators can make it easier to test a class—\nespecially when there’s a complicated dependency graph, where the class under test\nhas dependencies, each of which relies on dependencies of its own, and so on, several\nlayers deep. With test doubles, you can substitute the class’s immediate dependencies\nand thus break up the graph, which can significantly reduce the amount of prepara-\ntion you have to do in a unit test. If you follow the classical school, you have to re-create\nthe full object graph (with the exception of shared dependencies) just for the sake of\nsetting up the system under test, which can be a lot of work.\n Although this is all true, this line of reasoning focuses on the wrong problem.\nInstead of finding ways to test a large, complicated graph of interconnected classes,\nyou should focus on not having such a graph of classes in the first place. More often\nthan not, a large class graph is a result of a code design problem.\n It’s actually a good thing that the tests point out this problem. As we discussed in\nchapter 1, the ability to unit test a piece of code is a good negative indicator—it pre-\ndicts poor code quality with a relatively high precision. If you see that to unit test a\nclass, you need to extend the test’s arrange phase beyond all reasonable limits, it’s a\ncertain sign of trouble. The use of mocks only hides this problem; it doesn’t tackle the\nroot cause. I talk about how to fix the underlying code design problem in part 2. \n \nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n36\nCHAPTER 2\nWhat is a unit test?\n2.3.3\nRevealing the precise bug location\nIf you introduce a bug to a system with London-style tests, it normally causes only tests\nwhose SUT contains the bug to fail. However, with the classical approach, tests that\ntarget the clients of the malfunctioning class can also fail. This leads to a ripple effect\nwhere a single bug can cause test failures across the whole system. As a result, it\nbecomes harder to find the root of the issue. You might need to spend some time\ndebugging the tests to figure it out.\n It’s a valid concern, but I don’t see it as a big problem. If you run your tests regu-\nlarly (ideally, after each source code change), then you know what caused the bug—\nit’s what you edited last, so it’s not that difficult to find the issue. Also, you don’t have\nto look at all the failing tests. Fixing one automatically fixes all the others.\n Furthermore, there’s some value in failures cascading all over the test suite. If a\nbug leads to a fault in not only one test but a whole lot of them, it shows that the piece\nof code you have just broken is of great value—the entire system depends on it. That’s\nuseful information to keep in mind when working with the code. \n2.3.4\nOther differences between the classical and London schools\nTwo remaining differences between the classical and London schools are\nTheir approach to system design with test-driven development (TDD)\nThe issue of over-specification\nThe London style of unit testing leads to outside-in TDD, where you start from the\nhigher-level tests that set expectations for the whole system. By using mocks, you spec-\nify which collaborators the system should communicate with to achieve the expected\nresult. You then work your way through the graph of classes until you implement every\none of them. Mocks make this design process possible because you can focus on one\nTest-driven development\nTest-driven development is a software development process that relies on tests to\ndrive the project development. The process consists of three (some authors specify\nfour) stages, which you repeat for every test case:\n1\nWrite a failing test to indicate which functionality needs to be added and how\nit should behave.\n2\nWrite just enough code to make the test pass. At this stage, the code doesn’t\nhave to be elegant or clean.\n3\nRefactor the code. Under the protection of the passing test, you can safely\nclean up the code to make it more readable and maintainable.\nGood sources on this topic are the two books I recommended earlier: Kent Beck’s\nTest-Driven Development: By Example, and Growing Object-Oriented Software, Guided\nby Tests by Steve Freeman and Nat Pryce.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n37\nIntegration tests in the two schools\nclass at a time. You can cut off all of the SUT’s collaborators when testing it and thus\npostpone implementing those collaborators to a later time.\n The classical school doesn’t provide quite the same guidance since you have to\ndeal with the real objects in tests. Instead, you normally use the inside-out approach.\nIn this style, you start from the domain model and then put additional layers on top of\nit until the software becomes usable by the end user.\n But the most crucial distinction between the schools is the issue of over-specification:\nthat is, coupling the tests to the SUT’s implementation details. The London style\ntends to produce tests that couple to the implementation more often than the classi-\ncal style. And this is the main objection against the ubiquitous use of mocks and the\nLondon style in general.\n There’s much more to the topic of mocking. Starting with chapter 4, I gradually\ncover everything related to it. \n2.4\nIntegration tests in the two schools\nThe London and classical schools also diverge in their definition of an integration\ntest. This disagreement flows naturally from the difference in their views on the isola-\ntion issue.\n The London school considers any test that uses a real collaborator object an inte-\ngration test. Most of the tests written in the classical style would be deemed integra-\ntion tests by the London school proponents. For an example, see listing 1.4, in which I\nfirst introduced the two tests covering the customer purchase functionality. That code\nis a typical unit test from the classical perspective, but it’s an integration test for a fol-\nlower of the London school.\n In this book, I use the classical definitions of both unit and integration testing.\nAgain, a unit test is an automated test that has the following characteristics:\nIt verifies a small piece of code,\nDoes it quickly,\nAnd does it in an isolated manner.\nNow that I’ve clarified what the first and third attributes mean, I’ll redefine them\nfrom the point of view of the classical school. A unit test is a test that\nVerifies a single unit of behavior,\nDoes it quickly,\nAnd does it in isolation from other tests.\nAn integration test, then, is a test that doesn’t meet one of these criteria. For example,\na test that reaches out to a shared dependency—say, a database—can’t run in isolation\nfrom other tests. A change in the database’s state introduced by one test would alter\nthe outcome of all other tests that rely on the same database if run in parallel. You’d\nhave to take additional steps to avoid this interference. In particular, you would have\nto run such tests sequentially, so that each test would wait its turn to work with the\nshared dependency.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n38\nCHAPTER 2\nWhat is a unit test?\n Similarly, an outreach to an out-of-process dependency makes the test slow. A call\nto a database adds hundreds of milliseconds, potentially up to a second, of additional\nexecution time. Milliseconds might not seem like a big deal at first, but when your test\nsuite grows large enough, every second counts.\n In theory, you could write a slow test that works with in-memory objects only, but\nit’s not that easy to do. Communication between objects inside the same memory\nspace is much less expensive than between separate processes. Even if the test works\nwith hundreds of in-memory objects, the communication with them will still execute\nfaster than a call to a database.\n Finally, a test is an integration test when it verifies two or more units of behavior.\nThis is often a result of trying to optimize the test suite’s execution speed. When you\nhave two slow tests that follow similar steps but verify different units of behavior, it\nmight make sense to merge them into one: one test checking two similar things runs\nfaster than two more-granular tests. But then again, the two original tests would have\nbeen integration tests already (due to them being slow), so this characteristic usually\nisn’t decisive.\n An integration test can also verify how two or more modules developed by separate\nteams work together. This also falls into the third bucket of tests that verify multiple\nunits of behavior at once. But again, because such an integration normally requires an\nout-of-process dependency, the test will fail to meet all three criteria, not just one.\n Integration testing plays a significant part in contributing to software quality by\nverifying the system as a whole. I write about integration testing in detail in part 3.\n2.4.1\nEnd-to-end tests are a subset of integration tests\nIn short, an integration test is a test that verifies that your code works in integration with\nshared dependencies, out-of-process dependencies, or code developed by other teams\nin the organization. There’s also a separate notion of an end-to-end test. End-to-end\ntests are a subset of integration tests. They, too, check to see how your code works with\nout-of-process dependencies. The difference between an end-to-end test and an inte-\ngration test is that end-to-end tests usually include more of such dependencies.\n The line is blurred at times, but in general, an integration test works with only one\nor two out-of-process dependencies. On the other hand, an end-to-end test works with\nall out-of-process dependencies, or with the vast majority of them. Hence the name\nend-to-end, which means the test verifies the system from the end user’s point of view,\nincluding all the external applications this system integrates with (see figure 2.6).\n People also use such terms as UI tests (UI stands for user interface), GUI tests (GUI is\ngraphical user interface), and functional tests. The terminology is ill-defined, but in gen-\neral, these terms are all synonyms.\n Let’s say your application works with three out-of-process dependencies: a data-\nbase, the file system, and a payment gateway. A typical integration test would include\nonly the database and file system in scope and use a test double to replace the pay-\nment gateway. That’s because you have full control over the database and file system,\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n39\nSummary\nand thus can easily bring them to the required state in tests, whereas you don’t have\nthe same degree of control over the payment gateway. With the payment gateway, you\nmay need to contact the payment processor organization to set up a special test\naccount. You might also need to check that account from time to time to manually\nclean up all the payment charges left over from the past test executions.\n Since end-to-end tests are the most expensive in terms of maintenance, it’s better\nto run them late in the build process, after all the unit and integration tests have\npassed. You may possibly even run them only on the build server, not on individual\ndevelopers’ machines.\n Keep in mind that even with end-to-end tests, you might not be able to tackle all of\nthe out-of-process dependencies. There may be no test version of some dependencies,\nor it may be impossible to bring those dependencies to the required state automati-\ncally. So you may still need to use a test double, reinforcing the fact that there isn’t a\ndistinct line between integration and end-to-end tests. \nSummary\nThroughout this chapter, I’ve refined the definition of a unit test:\n– A unit test verifies a single unit of behavior,\n– Does it quickly,\n– And does it in isolation from other tests.\nAnother class\nUnit test\nPayment gateway\nEnd-to-end test\nDatabase\nSystem under test\nIntegration test\nFigure 2.6\nEnd-to-end tests normally include all or almost all out-of-process dependencies \nin the scope. Integration tests check only one or two such dependencies—those that are \neasier to set up automatically, such as the database or the file system.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "page_number": 52
    },
    {
      "number": 8,
      "title": "Segment 8 (pages 62-69)",
      "start_page": 62,
      "end_page": 69,
      "detection_method": "topic_boundary",
      "content": "40\nCHAPTER 2\nWhat is a unit test?\nThe isolation issue is disputed the most. The dispute led to the formation of two\nschools of unit testing: the classical (Detroit) school, and the London (mockist)\nschool. This difference of opinion affects the view of what constitutes a unit and\nthe treatment of the system under test’s (SUT’s) dependencies.\n– The London school states that the units under test should be isolated from\neach other. A unit under test is a unit of code, usually a class. All of its depen-\ndencies, except immutable dependencies, should be replaced with test dou-\nbles in tests.\n– The classical school states that the unit tests need to be isolated from each\nother, not units. Also, a unit under test is a unit of behavior, not a unit of code.\nThus, only shared dependencies should be replaced with test doubles.\nShared dependencies are dependencies that provide means for tests to affect\neach other’s execution flow.\nThe London school provides the benefits of better granularity, the ease of test-\ning large graphs of interconnected classes, and the ease of finding which func-\ntionality contains a bug after a test failure.\nThe benefits of the London school look appealing at first. However, they intro-\nduce several issues. First, the focus on classes under test is misplaced: tests\nshould verify units of behavior, not units of code. Furthermore, the inability to\nunit test a piece of code is a strong sign of a problem with the code design. The\nuse of test doubles doesn’t fix this problem, but rather only hides it. And finally,\nwhile the ease of determining which functionality contains a bug after a test fail-\nure is helpful, it’s not that big a deal because you often know what caused the\nbug anyway—it’s what you edited last.\nThe biggest issue with the London school of unit testing is the problem of over-\nspecification—coupling tests to the SUT’s implementation details.\nAn integration test is a test that doesn’t meet at least one of the criteria for a\nunit test. End-to-end tests are a subset of integration tests; they verify the system\nfrom the end user’s point of view. End-to-end tests reach out directly to all or\nalmost all out-of-process dependencies your application works with.\nFor a canonical book about the classical style, I recommend Kent Beck’s Test-\nDriven Development: By Example. For more on the London style, see Growing Object-\nOriented Software, Guided by Tests, by Steve Freeman and Nat Pryce. For further\nreading about working with dependencies, I recommend Dependency Injection:\nPrinciples, Practices, Patterns by Steven van Deursen and Mark Seemann.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n41\nThe anatomy of\na unit test\nIn this remaining chapter of part 1, I’ll give you a refresher on some basic topics.\nI’ll go over the structure of a typical unit test, which is usually represented by the\narrange, act, and assert (AAA) pattern. I’ll also show the unit testing framework of\nmy choice—xUnit—and explain why I’m using it and not one of its competitors.\n Along the way, we’ll talk about naming unit tests. There are quite a few compet-\ning pieces of advice on this topic, and unfortunately, most of them don’t do a good\nenough job improving your unit tests. In this chapter, I describe those less-useful\nnaming practices and show why they usually aren’t the best choice. Instead of those\npractices, I give you an alternative—a simple, easy-to-follow guideline for naming\ntests in a way that makes them readable not only to the programmer who wrote\nthem, but also to any other person familiar with the problem domain.\n Finally, I’ll talk about some features of the framework that help streamline the\nprocess of unit testing. Don’t worry about this information being too specific to C#\nThis chapter covers\nThe structure of a unit test\nUnit test naming best practices\nWorking with parameterized tests\nWorking with fluent assertions\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n42\nCHAPTER 3\nThe anatomy of a unit test\nand .NET; most unit testing frameworks exhibit similar functionality, regardless of\nthe programming language. If you learn one of them, you won’t have problems work-\ning with another.\n3.1\nHow to structure a unit test\nThis section shows how to structure unit tests using the arrange, act, and assert pat-\ntern, what pitfalls to avoid, and how to make your tests as readable as possible.\n3.1.1\nUsing the AAA pattern\nThe AAA pattern advocates for splitting each test into three parts: arrange, act, and\nassert. (This pattern is sometimes also called the 3A pattern.) Let’s take a Calculator\nclass with a single method that calculates a sum of two numbers:\npublic class Calculator\n{\npublic double Sum(double first, double second)\n{\nreturn first + second;\n}\n}\nThe following listing shows a test that verifies the class’s behavior. This test follows the\nAAA pattern.\npublic class CalculatorTests         \n{\n[Fact]    \npublic void Sum_of_two_numbers()   \n{\n// Arrange\ndouble first = 10;\n   \ndouble second = 20;\n   \nvar calculator = new Calculator();  \n// Act\ndouble result = calculator.Sum(first, second);    \n// Assert\nAssert.Equal(30, result);   \n}\n}\nThe AAA pattern provides a simple, uniform structure for all tests in the suite. This\nuniformity is one of the biggest advantages of this pattern: once you get used to it, you\ncan easily read and understand any test. That, in turn, reduces maintenance costs for\nyour entire test suite. The structure is as follows:\nListing 3.1\nA test covering the Sum method in calculator\nClass-container for a \ncohesive set of tests\nxUnit’s attribute \nindicating a test\nName of the\nunit test\nArrange \nsection\nAct section\nAssert section\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n43\nHow to structure a unit test\nIn the arrange section, you bring the system under test (SUT) and its dependen-\ncies to a desired state.\nIn the act section, you call methods on the SUT, pass the prepared dependen-\ncies, and capture the output value (if any).\nIn the assert section, you verify the outcome. The outcome may be represented\nby the return value, the final state of the SUT and its collaborators, or the meth-\nods the SUT called on those collaborators.\nThe natural inclination is to start writing a test with the arrange section. After all, it\ncomes before the other two. This approach works well in the vast majority of cases, but\nstarting with the assert section is a viable option too. When you practice Test-Driven\nDevelopment (TDD)—that is, when you create a failing test before developing a\nfeature—you don’t know enough about the feature’s behavior yet. So, it becomes\nadvantageous to first outline what you expect from the behavior and then figure out\nhow to develop the system to meet this expectation.\n Such a technique may look counterintuitive, but it’s how we approach problem\nsolving. We start by thinking about the objective: what a particular behavior should to\ndo for us. The actual solving of the problem comes after that. Writing down the asser-\ntions before everything else is merely a formalization of this thinking process. But\nagain, this guideline is only applicable when you follow TDD—when you write a test\nbefore the production code. If you write the production code before the test, by the\ntime you move on to the test, you already know what to expect from the behavior, so\nstarting with the arrange section is a better option. \n3.1.2\nAvoid multiple arrange, act, and assert sections\nOccasionally, you may encounter a test with multiple arrange, act, or assert sections. It\nusually works as shown in figure 3.1.\n When you see multiple act sections separated by assert and, possibly, arrange sec-\ntions, it means the test verifies multiple units of behavior. And, as we discussed in\nchapter 2, such a test is no longer a unit test but rather is an integration test. It’s best\nGiven-When-Then pattern\nYou might have heard of the Given-When-Then pattern, which is similar to AAA. This\npattern also advocates for breaking the test down into three parts:\nGiven—Corresponds to the arrange section\nWhen—Corresponds to the act section\nThen—Corresponds to the assert section\nThere’s no difference between the two patterns in terms of the test composition. The\nonly distinction is that the Given-When-Then structure is more readable to non-\nprogrammers. Thus, Given-When-Then is more suitable for tests that are shared with\nnon-technical people.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n44\nCHAPTER 3\nThe anatomy of a unit test\nto avoid such a test structure. A single action ensures that your tests remain within the\nrealm of unit testing, which means they are simple, fast, and easy to understand. If you\nsee a test containing a sequence of actions and assertions, refactor it. Extract each act\ninto a test of its own.\n It’s sometimes fine to have multiple act sections in integration tests. As you may\nremember from the previous chapter, integration tests can be slow. One way to speed\nthem up is to group several integration tests together into a single test with multiple\nacts and assertions. It’s especially helpful when system states naturally flow from one\nanother: that is, when an act simultaneously serves as an arrange for the subsequent act.\n But again, this optimization technique is only applicable to integration tests—and\nnot all of them, but rather those that are already slow and that you don’t want to\nbecome even slower. There’s no need for such an optimization in unit tests or integra-\ntion tests that are fast enough. It’s always better to split a multistep unit test into sev-\neral tests. \n3.1.3\nAvoid if statements in tests\nSimilar to multiple occurrences of the arrange, act, and assert sections, you may some-\ntimes encounter a unit test with an if statement. This is also an anti-pattern. A test—\nwhether a unit test or an integration test—should be a simple sequence of steps with\nno branching.\n An if statement indicates that the test verifies too many things at once. Such a test,\ntherefore, should be split into several tests. But unlike the situation with multiple AAA\nArrange the test\nAct\nAssert\nAct some more\nAssert again\nFigure 3.1\nMultiple arrange, act, and assert sections are a hint that the test verifies \ntoo many things at once. Such a test needs to be split into several tests to fix the \nproblem.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n45\nHow to structure a unit test\nsections, there’s no exception for integration tests. There are no benefits in branching\nwithin a test. You only gain additional maintenance costs: if statements make the tests\nharder to read and understand. \n3.1.4\nHow large should each section be?\nA common question people ask when starting out with the AAA pattern is, how large\nshould each section be? And what about the teardown section—the section that cleans\nup after the test? There are different guidelines regarding the size for each of the test\nsections.\nTHE ARRANGE SECTION IS THE LARGEST\nThe arrange section is usually the largest of the three. It can be as large as the act and\nassert sections combined. But if it becomes significantly larger than that, it’s better to\nextract the arrangements either into private methods within the same test class or to a\nseparate factory class. Two popular patterns can help you reuse the code in the arrange\nsections: Object Mother and Test Data Builder. \nWATCH OUT FOR ACT SECTIONS THAT ARE LARGER THAN A SINGLE LINE\nThe act section is normally just a single line of code. If the act consists of two or more\nlines, it could indicate a problem with the SUT’s public API.\n It’s best to express this point with an example, so let’s take one from chapter 2,\nwhich I repeat in the following listing. In this example, the customer makes a pur-\nchase from a store.\n[Fact]\npublic void Purchase_succeeds_when_enough_inventory()\n{\n// Arrange\nvar store = new Store();\nstore.AddInventory(Product.Shampoo, 10);\nvar customer = new Customer();\n// Act\nbool success = customer.Purchase(store, Product.Shampoo, 5);\n// Assert\nAssert.True(success);\nAssert.Equal(5, store.GetInventory(Product.Shampoo));\n}\nNotice that the act section in this test is a single method call, which is a sign of a well-\ndesigned class’s API. Now compare it to the version in listing 3.3: this act section con-\ntains two lines. And that’s a sign of a problem with the SUT: it requires the client to\nremember to make the second method call to finish the purchase and thus lacks\nencapsulation.\n \nListing 3.2\nA single-line act section \nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n46\nCHAPTER 3\nThe anatomy of a unit test\n[Fact]\npublic void Purchase_succeeds_when_enough_inventory()\n{\n// Arrange\nvar store = new Store();\nstore.AddInventory(Product.Shampoo, 10);\nvar customer = new Customer();\n// Act\nbool success = customer.Purchase(store, Product.Shampoo, 5);\nstore.RemoveInventory(success, Product.Shampoo, 5);\n// Assert\nAssert.True(success);\nAssert.Equal(5, store.GetInventory(Product.Shampoo));\n}\nHere’s what you can read from listing 3.3’s act section:\nIn the first line, the customer tries to acquire five units of shampoo from the\nstore.\nIn the second line, the inventory is removed from the store. The removal takes\nplace only if the preceding call to Purchase() returns a success.\nThe issue with the new version is that it requires two method calls to perform a single\noperation. Note that this is not an issue with the test itself. The test still verifies the\nsame unit of behavior: the process of making a purchase. The issue lies in the API sur-\nface of the Customer class. It shouldn’t require the client to make an additional\nmethod call.\n From a business perspective, a successful purchase has two outcomes: the acquisi-\ntion of a product by the customer and the reduction of the inventory in the store.\nBoth of these outcomes must be achieved together, which means there should be a\nsingle public method that does both things. Otherwise, there’s a room for inconsis-\ntency if the client code calls the first method but not the second, in which case the cus-\ntomer will acquire the product but its available amount won’t be reduced in the store.\n Such an inconsistency is called an invariant violation. The act of protecting your\ncode against potential inconsistencies is called encapsulation. When an inconsistency\npenetrates into the database, it becomes a big problem: now it’s impossible to reset\nthe state of your application by simply restarting it. You’ll have to deal with the cor-\nrupted data in the database and, potentially, contact customers and handle the situation\non a case-by-case basis. Just imagine what would happen if the application generated\nconfirmation receipts without actually reserving the inventory. It might issue claims\nto, and even charge for, more inventory than you could feasibly acquire in the near\nfuture.\n The remedy is to maintain code encapsulation at all times. In the previous exam-\nple, the customer should remove the acquired inventory from the store as part of its\nListing 3.3\nA two-line act section \nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n47\nHow to structure a unit test\nPurchase method and not rely on the client code to do so. When it comes to main-\ntaining invariants, you should eliminate any potential course of action that could lead\nto an invariant violation.\n This guideline of keeping the act section down to a single line holds true for the\nvast majority of code that contains business logic, but less so for utility or infrastruc-\nture code. Thus, I won’t say “never do it.” Be sure to examine each such case for a\npotential breach in encapsulation, though. \n3.1.5\nHow many assertions should the assert section hold?\nFinally, there’s the assert section. You may have heard about the guideline of having\none assertion per test. It takes root in the premise discussed in the previous chapter:\nthe premise of targeting the smallest piece of code possible.\n As you already know, this premise is incorrect. A unit in unit testing is a unit of\nbehavior, not a unit of code. A single unit of behavior can exhibit multiple outcomes,\nand it’s fine to evaluate them all in one test.\n Having that said, you need to watch out for assertion sections that grow too large:\nit could be a sign of a missing abstraction in the production code. For example,\ninstead of asserting all properties inside an object returned by the SUT, it may be bet-\nter to define proper equality members in the object’s class. You can then compare the\nobject to an expected value using a single assertion. \n3.1.6\nWhat about the teardown phase?\nSome people also distinguish a fourth section, teardown, which comes after arrange, act,\nand assert. For example, you can use this section to remove any files created by the\ntest, close a database connection, and so on. The teardown is usually represented by a\nseparate method, which is reused across all tests in the class. Thus, I don’t include this\nphase in the AAA pattern.\n Note that most unit tests don’t need teardown. Unit tests don’t talk to out-of-process\ndependencies and thus don’t leave side effects that need to be disposed of. That’s a\nrealm of integration testing. We’ll talk more about how to properly clean up after inte-\ngration tests in part 3. \n3.1.7\nDifferentiating the system under test\nThe SUT plays a significant role in tests. It provides an entry point for the behavior\nyou want to invoke in the application. As we discussed in the previous chapter, this\nbehavior can span across as many as several classes or as little as a single method. But\nthere can be only one entry point: one class that triggers that behavior.\n Thus it’s important to differentiate the SUT from its dependencies, especially\nwhen there are quite a few of them, so that you don’t need to spend too much time\nfiguring out who is who in the test. To do that, always name the SUT in tests sut. The\nfollowing listing shows how CalculatorTests would look after renaming the Calcu-\nlator instance.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "page_number": 62
    },
    {
      "number": 9,
      "title": "Segment 9 (pages 70-79)",
      "start_page": 70,
      "end_page": 79,
      "detection_method": "topic_boundary",
      "content": "48\nCHAPTER 3\nThe anatomy of a unit test\npublic class CalculatorTests\n{\n[Fact]\npublic void Sum_of_two_numbers()\n{\n// Arrange\ndouble first = 10;\ndouble second = 20;\nvar sut = new Calculator();    \n// Act\ndouble result = sut.Sum(first, second);\n// Assert\nAssert.Equal(30, result);\n}\n}\n3.1.8\nDropping the arrange, act, and assert comments from tests\nJust as it’s important to set the SUT apart from its dependencies, it’s also important to\ndifferentiate the three sections from each other, so that you don’t spend too much\ntime figuring out what section a particular line in the test belongs to. One way to do\nthat is to put // Arrange, // Act, and // Assert comments before the beginning of\neach section. Another way is to separate the sections with empty lines, as shown next.\npublic class CalculatorTests\n{\n[Fact]\npublic void Sum_of_two_numbers()\n{\ndouble first = 10;\n   \ndouble second = 20;\n  \nvar sut = new Calculator();  \ndouble result = sut.Sum(first, second);   \nAssert.Equal(30, result);   \n}\n}\nSeparating sections with empty lines works great in most unit tests. It allows you to\nkeep a balance between brevity and readability. It doesn’t work as well in large tests,\nthough, where you may want to put additional empty lines inside the arrange section\nto differentiate between configuration stages. This is often the case in integration\ntests—they frequently contain complicated setup logic. Therefore,\nListing 3.4\nDifferentiating the SUT from its dependencies\nListing 3.5\nCalculator with sections separated by empty lines\nThe calculator is \nnow called sut. \nArrange\nAct\nAssert\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n49\nExploring the xUnit testing framework\nDrop the section comments in tests that follow the AAA pattern and where you\ncan avoid additional empty lines inside the arrange and assert sections.\nKeep the section comments otherwise. \n3.2\nExploring the xUnit testing framework\nIn this section, I give a brief overview of unit testing tools available in .NET, and\ntheir features. I’m using xUnit (https://github.com/xunit/xunit) as the unit testing\nframework (note that you need to install the xunit.runner.visualstudio NuGet\npackage in order to run xUnit tests from Visual Studio). Although this framework\nworks in .NET only, every object-oriented language (Java, C++, JavaScript, and so\non) has unit testing frameworks, and all those frameworks look quite similar to each\nother. If you’ve worked with one of them, you won’t have any issues working with\nanother.\n In .NET alone, there are several alternatives to choose from, such as NUnit\n(https://github.com/nunit/nunit) and the built-in Microsoft MSTest. I personally\nprefer xUnit for the reasons I’ll describe shortly, but you can also use NUnit; these two\nframeworks are pretty much on par in terms of functionality. I don’t recommend\nMSTest, though; it doesn’t provide the same level of flexibility as xUnit and NUnit.\nAnd don’t take my word for it—even people inside Microsoft refrain from using\nMSTest. For example, the ASP.NET Core team uses xUnit.\n I prefer xUnit because it’s a cleaner, more concise version of NUnit. For example,\nyou may have noticed that in the tests I’ve brought up so far, there are no framework-\nrelated attributes other than [Fact], which marks the method as a unit test so the unit\ntesting framework knows to run it. There are no [TestFixture] attributes; any public\nclass can contain a unit test. There’s also no [SetUp] or [TearDown]. If you need to\nshare configuration logic between tests, you can put it inside the constructor. And if\nyou need to clean something up, you can implement the IDisposable interface, as\nshown in this listing.\npublic class CalculatorTests : IDisposable\n{\nprivate readonly Calculator _sut;\npublic CalculatorTests()\n   \n{\n   \n_sut = new Calculator();   \n}\n   \n[Fact]\npublic void Sum_of_two_numbers()\n{\n/* ... */\n}\nListing 3.6\nArrangement and teardown logic, shared by all tests\nCalled before \neach test in \nthe class\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n50\nCHAPTER 3\nThe anatomy of a unit test\npublic void Dispose()   \n{\n   \n_sut.CleanUp();\n   \n}\n   \n}\nAs you can see, the xUnit authors took significant steps toward simplifying the\nframework. A lot of notions that previously required additional configuration (like\n[TestFixture] or [SetUp] attributes) now rely on conventions or built-in language\nconstructs.\n I particularly like the [Fact] attribute, specifically because it’s called Fact and not\nTest. It emphasizes the rule of thumb I mentioned in the previous chapter: each test\nshould tell a story. This story is an individual, atomic scenario or fact about the problem\ndomain, and the passing test is a proof that this scenario or fact holds true. If the test\nfails, it means either the story is no longer valid and you need to rewrite it, or the sys-\ntem itself has to be fixed.\n I encourage you to adopt this way of thinking when you write unit tests. Your tests\nshouldn’t be a dull enumeration of what the production code does. Rather, they should\nprovide a higher-level description of the application’s behavior. Ideally, this description\nshould be meaningful not just to programmers but also to business people. \n3.3\nReusing test fixtures between tests\nIt’s important to know how and when to reuse code between tests. Reusing code\nbetween arrange sections is a good way to shorten and simplify your tests, and this sec-\ntion shows how to do that properly.\n I mentioned earlier that often, fixture arrangements take up too much space. It\nmakes sense to extract these arrangements into separate methods or classes that you\nthen reuse between tests. There are two ways you can perform such reuse, but only\none of them is beneficial; the other leads to increased maintenance costs.\nTest fixture\nThe term test fixture has two common meanings:\n1\nA test fixture is an object the test runs against. This object can be a regular\ndependency—an argument that is passed to the SUT. It can also be data in\nthe database or a file on the hard disk. Such an object needs to remain in a\nknown, fixed state before each test run, so it produces the same result.\nHence the word fixture.\n2\nThe other definition comes from the NUnit testing framework. In NUnit, Test-\nFixture is an attribute that marks a class containing tests.\nI use the first definition throughout this book.\nCalled after \neach test in \nthe class\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n51\nReusing test fixtures between tests\nThe first—incorrect—way to reuse test fixtures is to initialize them in the test’s con-\nstructor (or the method marked with a [SetUp] attribute if you are using NUnit), as\nshown next.\npublic class CustomerTests\n{\nprivate readonly Store _store;       \nprivate readonly Customer _sut;\npublic CustomerTests()\n     \n{\n   \n_store = new Store();\n   \n_store.AddInventory(Product.Shampoo, 10);   \n_sut = new Customer();\n   \n}\n   \n[Fact]\npublic void Purchase_succeeds_when_enough_inventory()\n{\nbool success = _sut.Purchase(_store, Product.Shampoo, 5);\nAssert.True(success);\nAssert.Equal(5, _store.GetInventory(Product.Shampoo));\n}\n[Fact]\npublic void Purchase_fails_when_not_enough_inventory()\n{\nbool success = _sut.Purchase(_store, Product.Shampoo, 15);\nAssert.False(success);\nAssert.Equal(10, _store.GetInventory(Product.Shampoo));\n}\n}\nThe two tests in listing 3.7 have common configuration logic. In fact, their arrange sec-\ntions are the same and thus can be fully extracted into CustomerTests’s constructor—\nwhich is precisely what I did here. The tests themselves no longer contain arrangements.\n With this approach, you can significantly reduce the amount of test code—you can\nget rid of most or even all test fixture configurations in tests. But this technique has\ntwo significant drawbacks:\nIt introduces high coupling between tests.\nIt diminishes test readability.\nLet’s discuss these drawbacks in more detail.\nListing 3.7\nExtracting the initialization code into the test constructor\nCommon test \nfixture\nRuns before \neach test in \nthe class\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n52\nCHAPTER 3\nThe anatomy of a unit test\n3.3.1\nHigh coupling between tests is an anti-pattern\nIn the new version, shown in listing 3.7, all tests are coupled to each other: a modifica-\ntion of one test’s arrangement logic will affect all tests in the class. For example, chang-\ning this line\n_store.AddInventory(Product.Shampoo, 10);\nto this\n_store.AddInventory(Product.Shampoo, 15);\nwould invalidate the assumption the tests make about the store’s initial state and there-\nfore would lead to unnecessary test failures.\n That’s a violation of an important guideline: a modification of one test should not affect\nother tests. This guideline is similar to what we discussed in chapter 2—that tests should\nrun in isolation from each other. It’s not the same, though. Here, we are talking about\nindependent modification of tests, not independent execution. Both are important\nattributes of a well-designed test.\n To follow this guideline, you need to avoid introducing shared state in test classes.\nThese two private fields are examples of such a shared state:\nprivate readonly Store _store;\nprivate readonly Customer _sut;\n3.3.2\nThe use of constructors in tests diminishes test readability\nThe other drawback to extracting the arrangement code into the constructor is\ndiminished test readability. You no longer see the full picture just by looking at the\ntest itself. You have to examine different places in the class to understand what the test\nmethod does.\n Even if there’s not much arrangement logic—say, only instantiation of the fixtures—\nyou are still better off moving it directly to the test method. Otherwise, you’ll wonder\nif it’s really just instantiation or something else being configured there, too. A self-con-\ntained test doesn’t leave you with such uncertainties. \n3.3.3\nA better way to reuse test fixtures\nThe use of the constructor is not the best approach when it comes to reusing test fix-\ntures. The second way—the beneficial one—is to introduce private factory methods in\nthe test class, as shown in the following listing.\npublic class CustomerTests\n{\n[Fact]\npublic void Purchase_succeeds_when_enough_inventory()\n{\nListing 3.8\nExtracting the common initialization code into private factory methods\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n53\nReusing test fixtures between tests\nStore store = CreateStoreWithInventory(Product.Shampoo, 10);\nCustomer sut = CreateCustomer();\nbool success = sut.Purchase(store, Product.Shampoo, 5);\nAssert.True(success);\nAssert.Equal(5, store.GetInventory(Product.Shampoo));\n}\n[Fact]\npublic void Purchase_fails_when_not_enough_inventory()\n{\nStore store = CreateStoreWithInventory(Product.Shampoo, 10);\nCustomer sut = CreateCustomer();\nbool success = sut.Purchase(store, Product.Shampoo, 15);\nAssert.False(success);\nAssert.Equal(10, store.GetInventory(Product.Shampoo));\n}\nprivate Store CreateStoreWithInventory(\nProduct product, int quantity)\n{\nStore store = new Store();\nstore.AddInventory(product, quantity);\nreturn store;\n}\nprivate static Customer CreateCustomer()\n{\nreturn new Customer();\n}\n}\nBy extracting the common initialization code into private factory methods, you can\nalso shorten the test code, but at the same time keep the full context of what’s going\non in the tests. Moreover, the private methods don’t couple tests to each other as long\nas you make them generic enough. That is, allow the tests to specify how they want the\nfixtures to be created.\n Look at this line, for example:\nStore store = CreateStoreWithInventory(Product.Shampoo, 10);\nThe test explicitly states that it wants the factory method to add 10 units of shampoo\nto the store. This is both highly readable and reusable. It’s readable because you don’t\nneed to examine the internals of the factory method to understand the attributes of\nthe created store. It’s reusable because you can use this method in other tests, too.\n Note that in this particular example, there’s no need to introduce factory meth-\nods, as the arrangement logic is quite simple. View it merely as a demonstration.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n54\nCHAPTER 3\nThe anatomy of a unit test\n There’s one exception to this rule of reusing test fixtures. You can instantiate a fix-\nture in the constructor if it’s used by all or almost all tests. This is often the case for\nintegration tests that work with a database. All such tests require a database connec-\ntion, which you can initialize once and then reuse everywhere. But even then, it would\nmake more sense to introduce a base class and initialize the database connection in\nthat class’s constructor, not in individual test classes. See the following listing for an\nexample of common initialization code in a base class.\npublic class CustomerTests : IntegrationTests\n{\n[Fact]\npublic void Purchase_succeeds_when_enough_inventory()\n{\n/* use _database here */\n}\n}\npublic abstract class IntegrationTests : IDisposable\n{\nprotected readonly Database _database;\nprotected IntegrationTests()\n{\n_database = new Database();\n}\npublic void Dispose()\n{\n_database.Dispose();\n}\n}\nNotice how CustomerTests remains constructor-less. It gets access to the _database\ninstance by inheriting from the IntegrationTests base class. \n3.4\nNaming a unit test\nIt’s important to give expressive names to your tests. Proper naming helps you under-\nstand what the test verifies and how the underlying system behaves.\n So, how should you name a unit test? I’ve seen and tried a lot of naming conven-\ntions over the past decade. One of the most prominent, and probably least helpful, is\nthe following convention:\n[MethodUnderTest]_[Scenario]_[ExpectedResult]\nwhere\n\nMethodUnderTest is the name of the method you are testing.\n\nScenario is the condition under which you test the method.\nListing 3.9\nCommon initialization code in a base class\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n55\nNaming a unit test\n\nExpectedResult is what you expect the method under test to do in the current\nscenario.\nIt’s unhelpful specifically because it encourages you to focus on implementation\ndetails instead of the behavior.\n Simple phrases in plain English do a much better job: they are more expressive\nand don’t box you in a rigid naming structure. With simple phrases, you can describe\nthe system behavior in a way that’s meaningful to a customer or a domain expert. To\ngive you an example of a test titled in plain English, here’s the test from listing 3.5\nonce again:\npublic class CalculatorTests\n{\n[Fact]\npublic void Sum_of_two_numbers()\n{\ndouble first = 10;\ndouble second = 20;\nvar sut = new Calculator();\ndouble result = sut.Sum(first, second);\nAssert.Equal(30, result);\n}\n}\nHow could the test’s name (Sum_of_two_numbers) be rewritten using the [MethodUnder-\nTest]_[Scenario]_[ExpectedResult] convention? Probably something like this:\npublic void Sum_TwoNumbers_ReturnsSum()\nThe method under test is Sum, the scenario includes two numbers, and the expected\nresult is a sum of those two numbers. The new name looks logical to a programmer’s\neye, but does it really help with test readability? Not at all. It’s Greek to an unin-\nformed person. Think about it: Why does Sum appear twice in the name of the test?\nAnd what is this Returns phrasing all about? Where is the sum returned to? You\ncan’t know.\n Some might argue that it doesn’t really matter what a non-programmer would\nthink of this name. After all, unit tests are written by programmers for programmers,\nnot domain experts. And programmers are good at deciphering cryptic names—it’s\ntheir job!\n This is true, but only to a degree. Cryptic names impose a cognitive tax on every-\none, programmers or not. They require additional brain capacity to figure out what\nexactly the test verifies and how it relates to business requirements. This may not seem\nlike much, but the mental burden adds up over time. It slowly but surely increases the\nmaintenance cost for the entire test suite. It’s especially noticeable if you return to the\ntest after you’ve forgotten about the feature’s specifics, or try to understand a test\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n56\nCHAPTER 3\nThe anatomy of a unit test\nwritten by a colleague. Reading someone else’s code is already difficult enough—any\nhelp understanding it is of considerable use.\n Here are the two versions again:\npublic void Sum_of_two_numbers()\npublic void Sum_TwoNumbers_ReturnsSum()\nThe initial name written in plain English is much simpler to read. It is a down-to-earth\ndescription of the behavior under test.\n3.4.1\nUnit test naming guidelines\nAdhere to the following guidelines to write expressive, easily readable test names:\nDon’t follow a rigid naming policy. You simply can’t fit a high-level description of a\ncomplex behavior into the narrow box of such a policy. Allow freedom of\nexpression.\nName the test as if you were describing the scenario to a non-programmer who is familiar\nwith the problem domain. A domain expert or a business analyst is a good example.\nSeparate words with underscores. Doing so helps improve readability, especially in\nlong names.\nNotice that I didn’t use underscores when naming the test class, CalculatorTests.\nNormally, the names of classes are not as long, so they read fine without underscores.\n Also notice that although I use the pattern [ClassName]Tests when naming test\nclasses, it doesn’t mean the tests are limited to verifying only that class. Remember, the\nunit in unit testing is a unit of behavior, not a class. This unit can span across one or sev-\neral classes; the actual size is irrelevant. Still, you have to start somewhere. View the\nclass in [ClassName]Tests as just that: an entry point, an API, using which you can\nverify a unit of behavior. \n3.4.2\nExample: Renaming a test toward the guidelines\nLet’s take a test as an example and try to gradually improve its name using the guide-\nlines I just outlined. In the following listing, you can see a test verifying that a delivery\nwith a past date is invalid. The test’s name is written using the rigid naming policy that\ndoesn’t help with the test readability.\n[Fact]\npublic void IsDeliveryValid_InvalidDate_ReturnsFalse()\n{\nDeliveryService sut = new DeliveryService();\nDateTime pastDate = DateTime.Now.AddDays(-1);\nDelivery delivery = new Delivery\n{\nDate = pastDate\n};\nListing 3.10\nA test named using the rigid naming policy\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n57\nNaming a unit test\nbool isValid = sut.IsDeliveryValid(delivery);\nAssert.False(isValid);\n}\nThis test checks that DeliveryService properly identifies a delivery with an incorrect\ndate as invalid. How would you rewrite the test’s name in plain English? The following\nwould be a good first try:\npublic void Delivery_with_invalid_date_should_be_considered_invalid()\nNotice two things in the new version:\nThe name now makes sense to a non-programmer, which means programmers\nwill have an easier time understanding it, too.\nThe name of the SUT’s method—IsDeliveryValid—is no longer part of the\ntest’s name.\nThe second point is a natural consequence of rewriting the test’s name in plain\nEnglish and thus can be easily overlooked. However, this consequence is important\nand can be elevated into a guideline of its own.\nBut let’s get back to the example. The new version of the test’s name is a good start,\nbut it can be improved further. What does it mean for a delivery date to be invalid,\nexactly? From the test in listing 3.10, we can see that an invalid date is any date in\nthe past. This makes sense—you should only be allowed to choose a delivery date\nin the future.\n So let’s be specific and reflect this knowledge in the test’s name:\npublic void Delivery_with_past_date_should_be_considered_invalid()\nMethod under test in the test’s name\nDon’t include the name of the SUT’s method in the test’s name.\nRemember, you don’t test code, you test application behavior. Therefore, it doesn’t\nmatter what the name of the method under test is. As I mentioned previously, the\nSUT is just an entry point: a means to invoke a behavior. You can decide to rename\nthe method under test to, say, IsDeliveryCorrect, and it will have no effect on the\nSUT’s behavior. On the other hand, if you follow the original naming convention, you’ll\nhave to rename the test. This once again shows that targeting code instead of behav-\nior couples tests to that code’s implementation details, which negatively affects the\ntest suite’s maintainability. More on this issue in chapter 5.\nThe only exception to this guideline is when you work on utility code. Such code\ndoesn’t contain business logic—its behavior doesn’t go much beyond simple auxil-\niary functionality and thus doesn’t mean anything to business people. It’s fine to use\nthe SUT’s method names there.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "page_number": 70
    },
    {
      "number": 10,
      "title": "Segment 10 (pages 80-87)",
      "start_page": 80,
      "end_page": 87,
      "detection_method": "topic_boundary",
      "content": "58\nCHAPTER 3\nThe anatomy of a unit test\nThis is better but still not ideal. It’s too verbose. We can get rid of the word consid-\nered without any loss of meaning:\npublic void Delivery_with_past_date_should_be_invalid()\nThe wording should be is another common anti-pattern. Earlier in this chapter, I men-\ntioned that a test is a single, atomic fact about a unit of behavior. There’s no place for\na wish or a desire when stating a fact. Name the test accordingly—replace should be\nwith is:\npublic void Delivery_with_past_date_is_invalid()\nAnd finally, there’s no need to avoid basic English grammar. Articles help the test read\nflawlessly. Add the article a to the test’s name:\npublic void Delivery_with_a_past_date_is_invalid()\nThere you go. This final version is a straight-to-the-point statement of a fact, which\nitself describes one of the aspects of the application behavior under test: in this partic-\nular case, the aspect of determining whether a delivery can be done. \n3.5\nRefactoring to parameterized tests\nOne test usually is not enough to fully describe a unit of behavior. Such a unit normally\nconsists of multiple components, each of which should be captured with its own test. If\nthe behavior is complex enough, the number of tests describing it can grow dramatically\nand may become unmanageable. Luckily, most unit testing frameworks provide func-\ntionality that allows you to group similar tests using parameterized tests (see figure 3.2).\nBehavior N\n…\n…\n…\n…\nBehavior 2\nBehavior 1\nCan be grouped\nFact N\nFact 2\nFact 1\nApplication\nFigure 3.2\nA typical application \nexhibits multiple behaviors. The \ngreater the complexity of the \nbehavior, the more facts are required \nto fully describe it. Each fact is \nrepresented by a test. Similar facts \ncan be grouped into a single test \nmethod using parameterized tests.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n59\nRefactoring to parameterized tests\nIn this section, I’ll first show each such behavior component described by a separate test\nand then demonstrate how these tests can be grouped together.\n Let’s say that our delivery functionality works in such a way that the soonest\nallowed delivery date is two days from now. Clearly, the one test we have isn’t enough.\nIn addition to the test that checks for a past delivery date, we’ll also need tests that\ncheck for today’s date, tomorrow’s date, and the date after that.\n The existing test is called Delivery_with_a_past_date_is_invalid. We could\nadd three more:\npublic void Delivery_for_today_is_invalid()\npublic void Delivery_for_tomorrow_is_invalid()\npublic void The_soonest_delivery_date_is_two_days_from_now()\nBut that would result in four test methods, with the only difference between them\nbeing the delivery date.\n A better approach is to group these tests into one in order to reduce the amount of\ntest code. xUnit (like most other test frameworks) has a feature called parameterized\ntests that allows you to do exactly that. The next listing shows how such grouping looks.\nEach InlineData attribute represents a separate fact about the system; it’s a test case\nin its own right.\npublic class DeliveryServiceTests\n{\n[InlineData(-1, false)]   \n[InlineData(0, false)]   \n[InlineData(1, false)]   \n[InlineData(2, true)]\n  \n[Theory]\npublic void Can_detect_an_invalid_delivery_date(\nint daysFromNow,       \nbool expected)\n      \n{\nDeliveryService sut = new DeliveryService();\nDateTime deliveryDate = DateTime.Now\n.AddDays(daysFromNow);                   \nDelivery delivery = new Delivery\n{\nDate = deliveryDate\n};\nbool isValid = sut.IsDeliveryValid(delivery);\nAssert.Equal(expected, isValid);              \n}\n}\nTIP\nNotice the use of the [Theory] attribute instead of [Fact]. A theory is a\nbunch of facts about the behavior.\nListing 3.11\nA test that encompasses several facts\nThe InlineData attribute sends a \nset of input values to the test \nmethod. Each line represents a \nseparate fact about the behavior.\nParameters to which the attributes \nattach the input values\nUses the \nparameters\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n60\nCHAPTER 3\nThe anatomy of a unit test\nEach fact is now represented by an [InlineData] line rather than a separate test. I\nalso renamed the test method something more generic: it no longer mentions what\nconstitutes a valid or invalid date.\n Using parameterized tests, you can significantly reduce the amount of test code,\nbut this benefit comes at a cost. It’s now hard to figure out what facts the test method\nrepresents. And the more parameters there are, the harder it becomes. As a compro-\nmise, you can extract the positive test case into its own test and benefit from the\ndescriptive naming where it matters the most—in determining what differentiates\nvalid and invalid delivery dates, as shown in the following listing.\npublic class DeliveryServiceTests\n{\n[InlineData(-1)]\n[InlineData(0)]\n[InlineData(1)]\n[Theory]\npublic void Detects_an_invalid_delivery_date(int daysFromNow)\n{\n/* ... */\n}\n[Fact]\npublic void The_soonest_delivery_date_is_two_days_from_now()\n{\n/* ... */\n}\n}\nThis approach also simplifies the negative test cases, since you can remove the\nexpected Boolean parameter from the test method. And, of course, you can trans-\nform the positive test method into a parameterized test as well, to test multiple dates.\n As you can see, there’s a trade-off between the amount of test code and the read-\nability of that code. As a rule of thumb, keep both positive and negative test cases\ntogether in a single method only when it’s self-evident from the input parameters\nwhich case stands for what. Otherwise, extract the positive test cases. And if the behav-\nior is too complicated, don’t use the parameterized tests at all. Represent each nega-\ntive and positive test case with its own test method.\n3.5.1\nGenerating data for parameterized tests\nThere are some caveats in using parameterized tests (at least, in .NET) that you need\nto be aware of. Notice that in listing 3.11, I used the daysFromNow parameter as an\ninput to the test method. Why not the actual date and time, you might ask? Unfortu-\nnately, the following code won’t work:\n[InlineData(DateTime.Now.AddDays(-1), false)]\n[InlineData(DateTime.Now, false)]\nListing 3.12\nTwo tests verifying the positive and negative scenarios\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n61\nRefactoring to parameterized tests\n[InlineData(DateTime.Now.AddDays(1), false)]\n[InlineData(DateTime.Now.AddDays(2), true)]\n[Theory]\npublic void Can_detect_an_invalid_delivery_date(\nDateTime deliveryDate,\nbool expected)\n{\nDeliveryService sut = new DeliveryService();\nDelivery delivery = new Delivery\n{\nDate = deliveryDate\n};\nbool isValid = sut.IsDeliveryValid(delivery);\nAssert.Equal(expected, isValid);\n}\nIn C#, the content of all attributes is evaluated at compile time. You have to use only\nthose values that the compiler can understand, which are as follows:\nConstants\nLiterals\n\ntypeof() expressions\nThe call to DateTime.Now relies on the .NET runtime and thus is not allowed.\n There is a way to overcome this problem. xUnit has another feature that you can\nuse to generate custom data to feed into the test method: [MemberData]. The next list-\ning shows how we can rewrite the previous test using this feature.\n[Theory]\n[MemberData(nameof(Data))]\npublic void Can_detect_an_invalid_delivery_date(\nDateTime deliveryDate,\nbool expected)\n{\n/* ... */\n}\npublic static List<object[]> Data()\n{\nreturn new List<object[]>\n{\nnew object[] { DateTime.Now.AddDays(-1), false },\nnew object[] { DateTime.Now, false },\nnew object[] { DateTime.Now.AddDays(1), false },\nnew object[] { DateTime.Now.AddDays(2), true }\n};\n}\nListing 3.13\nGenerating complex data for the parameterized test \nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n62\nCHAPTER 3\nThe anatomy of a unit test\nMemberData accepts the name of a static method that generates a collection of input\ndata (the compiler translates nameof(Data) into a \"Data\" literal). Each element of\nthe collection is itself a collection that is mapped into the two input parameters:\ndeliveryDate and expected. With this feature, you can overcome the compiler’s\nrestrictions and use parameters of any type in the parameterized tests. \n3.6\nUsing an assertion library to further improve \ntest readability\nOne more thing you can do to improve test readability is to use an assertion library. I\npersonally prefer Fluent Assertions (https://fluentassertions.com), but .NET has sev-\neral competing libraries in this area.\n The main benefit of using an assertion library is how you can restructure the asser-\ntions so that they are more readable. Here’s one of our earlier tests:\n[Fact]\npublic void Sum_of_two_numbers()\n{\nvar sut = new Calculator();\ndouble result = sut.Sum(10, 20);\nAssert.Equal(30, result);\n}\nNow compare it to the following, which uses a fluent assertion:\n[Fact]\npublic void Sum_of_two_numbers()\n{\nvar sut = new Calculator();\ndouble result = sut.Sum(10, 20);\nresult.Should().Be(30);\n}\nThe assertion from the second test reads as plain English, which is exactly how you\nwant all your code to read. We as humans prefer to absorb information in the form of\nstories. All stories adhere to this specific pattern:\n[Subject] [action] [object].\nFor example,\nBob opened the door.\nHere, Bob is a subject, opened is an action, and the door is an object. The same rule\napplies to code. result.Should().Be(30) reads better than Assert.Equal(30,\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n63\nSummary\nresult) precisely because it follows the story pattern. It’s a simple story in which\nresult is a subject, should be is an action, and 30 is an object.\nNOTE\nThe paradigm of object-oriented programming (OOP) has become a\nsuccess partly because of this readability benefit. With OOP, you, too, can\nstructure the code in a way that reads like a story.\nThe Fluent Assertions library also provides numerous helper methods to assert against\nnumbers, strings, collections, dates and times, and much more. The only drawback is\nthat such a library is an additional dependency you may not want to introduce to your\nproject (although it’s for development only and won’t be shipped to production). \nSummary\nAll unit tests should follow the AAA pattern: arrange, act, assert. If a test has mul-\ntiple arrange, act, or assert sections, that’s a sign that the test verifies multiple\nunits of behavior at once. If this test is meant to be a unit test, split it into several\ntests—one per each action.\nMore than one line in the act section is a sign of a problem with the SUT’s API.\nIt requires the client to remember to always perform these actions together,\nwhich can potentially lead to inconsistencies. Such inconsistencies are called\ninvariant violations. The act of protecting your code against potential invariant\nviolations is called encapsulation.\nDistinguish the SUT in tests by naming it sut. Differentiate the three test sec-\ntions either by putting Arrange, Act, and Assert comments before them or by\nintroducing empty lines between these sections.\nReuse test fixture initialization code by introducing factory methods, not by\nputting this initialization code to the constructor. Such reuse helps maintain a\nhigh degree of decoupling between tests and also provides better readability.\nDon’t use a rigid test naming policy. Name each test as if you were describing\nthe scenario in it to a non-programmer who is familiar with the problem\ndomain. Separate words in the test name by underscores, and don’t include the\nname of the method under test in the test name.\nParameterized tests help reduce the amount of code needed for similar tests.\nThe drawback is that the test names become less readable as you make them\nmore generic.\nAssertion libraries help you further improve test readability by restructuring the\nword order in assertions so that they read like plain English. \nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\nPart 2\nMaking your tests\nwork for you\nNow that you’re armed with the knowledge of what unit testing is for,\nyou’re ready to dive into the very crux of what makes a good test and learn how\nto refactor your tests toward being more valuable. In chapter 4, you’ll learn\nabout the four pillars that make up a good unit test. These four pillars set a foun-\ndation, a common frame of reference, which we’ll use to analyze unit tests and\ntesting approaches moving forward.\n Chapter 5 takes the frame of reference established in chapter 4 and builds\nthe case for mocks and their relation to test fragility.\n Chapter 6 uses the same the frame of reference to examine the three styles of\nunit testing. It shows which of those styles tends to produce tests of the best qual-\nity, and why.\n Chapter 7 puts the knowledge from chapters 4 to 6 into practice and teaches\nyou how to refactor away from bloated, overcomplicated tests to tests that pro-\nvide as much value with as little maintenance cost as possible.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "page_number": 80
    },
    {
      "number": 11,
      "title": "Segment 11 (pages 88-97)",
      "start_page": 88,
      "end_page": 97,
      "detection_method": "topic_boundary",
      "content": "Licensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n67\nThe four pillars\nof a good unit test\nNow we are getting to the heart of the matter. In chapter 1, you saw the properties\nof a good unit test suite:\nIt is integrated into the development cycle. You only get value from tests that you\nactively use; there’s no point in writing them otherwise.\nIt targets only the most important parts of your code base. Not all production code\ndeserves equal attention. It’s important to differentiate the heart of the\napplication (its domain model) from everything else. This topic is tackled in\nchapter 7.\nIt provides maximum value with minimum maintenance costs. To achieve this last\nattribute, you need to be able to\n– Recognize a valuable test (and, by extension, a test of low value)\n– Write a valuable test\nThis chapter covers\nExploring dichotomies between aspects of a \ngood unit test\nDefining an ideal test\nUnderstanding the Test Pyramid\nUsing black-box and white-box testing\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n68\nCHAPTER 4\nThe four pillars of a good unit test\nAs we discussed in chapter 1, recognizing a valuable test and writing a valuable test are two\nseparate skills. The latter skill requires the former one, though; so, in this chapter, I’ll\nshow how to recognize a valuable test. You’ll see a universal frame of reference with\nwhich you can analyze any test in the suite. We’ll then use this frame of reference to\ngo over some popular unit testing concepts: the Test Pyramid and black-box versus\nwhite-box testing.\n Buckle up: we are starting out.\n4.1\nDiving into the four pillars of a good unit test\nA good unit test has the following four attributes:\nProtection against regressions\nResistance to refactoring\nFast feedback\nMaintainability\nThese four attributes are foundational. You can use them to analyze any automated\ntest, be it unit, integration, or end-to-end. Every such test exhibits some degree of\neach attribute. In this section, I define the first two attributes; and in section 4.2, I\ndescribe the intrinsic connection between them.\n4.1.1\nThe first pillar: Protection against regressions\nLet’s start with the first attribute of a good unit test: protection against regressions. As you\nknow from chapter 1, a regression is a software bug. It’s when a feature stops working as\nintended after some code modification, usually after you roll out new functionality.\n Such regressions are annoying (to say the least), but that’s not the worst part about\nthem. The worst part is that the more features you develop, the more chances there are\nthat you’ll break one of those features with a new release. An unfortunate fact of pro-\ngramming life is that code is not an asset, it’s a liability. The larger the code base, the more\nexposure it has to potential bugs. That’s why it’s crucial to develop a good protection\nagainst regressions. Without such protection, you won’t be able to sustain the project\ngrowth in a long run—you’ll be buried under an ever-increasing number of bugs.\n To evaluate how well a test scores on the metric of protecting against regressions,\nyou need to take into account the following:\nThe amount of code that is executed during the test\nThe complexity of that code\nThe code’s domain significance\nGenerally, the larger the amount of code that gets executed, the higher the chance\nthat the test will reveal a regression. Of course, assuming that this test has a relevant\nset of assertions, you don’t want to merely execute the code. While it helps to know\nthat this code runs without throwing exceptions, you also need to validate the out-\ncome it produces.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n69\nDiving into the four pillars of a good unit test\n Note that it’s not only the amount of code that matters, but also its complexity and\ndomain significance. Code that represents complex business logic is more important\nthan boilerplate code—bugs in business-critical functionality are the most damaging.\n On the other hand, it’s rarely worthwhile to test trivial code. Such code is short and\ndoesn’t contain a substantial amount of business logic. Tests that cover trivial code\ndon’t have much of a chance of finding a regression error, because there’s not a lot of\nroom for a mistake. An example of trivial code is a single-line property like this:\npublic class User\n{\npublic string Name { get; set; }\n}\nFurthermore, in addition to your code, the code you didn’t write also counts: for\nexample, libraries, frameworks, and any external systems used in the project. That\ncode influences the working of your software almost as much as your own code. For\nthe best protection, the test must include those libraries, frameworks, and external sys-\ntems in the testing scope, in order to check that the assumptions your software makes\nabout these dependencies are correct.\nTIP\nTo maximize the metric of protection against regressions, the test needs\nto aim at exercising as much code as possible. \n4.1.2\nThe second pillar: Resistance to refactoring\nThe second attribute of a good unit test is resistance to refactoring—the degree to which\na test can sustain a refactoring of the underlying application code without turning red\n(failing).\nDEFINITION\nRefactoring means changing existing code without modifying its\nobservable behavior. The intention is usually to improve the code’s nonfunc-\ntional characteristics: increase readability and reduce complexity. Some exam-\nples of refactoring are renaming a method and extracting a piece of code into\na new class.\nPicture this situation. You developed a new feature, and everything works great. The\nfeature itself is doing its job, and all the tests are passing. Now you decide to clean up\nthe code. You do some refactoring here, a little bit of modification there, and every-\nthing looks even better than before. Except one thing—the tests are failing. You look\nmore closely to see exactly what you broke with the refactoring, but it turns out that\nyou didn’t break anything. The feature works perfectly, just as before. The problem is\nthat the tests are written in such a way that they turn red with any modification of the\nunderlying code. And they do that regardless of whether you actually break the func-\ntionality itself.\n This situation is called a false positive. A false positive is a false alarm. It’s a result\nindicating that the test fails, although in reality, the functionality it covers works as\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n70\nCHAPTER 4\nThe four pillars of a good unit test\nintended. Such false positives usually take place when you refactor the code—when\nyou modify the implementation but keep the observable behavior intact. Hence the\nname for this attribute of a good unit test: resistance to refactoring.\n To evaluate how well a test scores on the metric of resisting to refactoring, you\nneed to look at how many false positives the test generates. The fewer, the better.\n Why so much attention on false positives? Because they can have a devastating\neffect on your entire test suite. As you may recall from chapter 1, the goal of unit test-\ning is to enable sustainable project growth. The mechanism by which the tests enable\nsustainable growth is that they allow you to add new features and conduct regular\nrefactorings without introducing regressions. There are two specific benefits here:\nTests provide an early warning when you break existing functionality. Thanks to such\nearly warnings, you can fix an issue long before the faulty code is deployed to\nproduction, where dealing with it would require a significantly larger amount\nof effort.\nYou become confident that your code changes won’t lead to regressions. Without such\nconfidence, you will be much more hesitant to refactor and much more likely\nto leave the code base to deteriorate.\nFalse positives interfere with both of these benefits:\nIf tests fail with no good reason, they dilute your ability and willingness to react\nto problems in code. Over time, you get accustomed to such failures and stop\npaying as much attention. After a while, you start ignoring legitimate failures,\ntoo, allowing them to slip into production.\nOn the other hand, when false positives are frequent, you slowly lose trust in the\ntest suite. You no longer perceive it as a reliable safety net—the perception is\ndiminished by false alarms. This lack of trust leads to fewer refactorings,\nbecause you try to reduce code changes to a minimum in order to avoid regres-\nsions.\nA story from the trenches\nI once worked on a project with a rich history. The project wasn’t too old, maybe two\nor three years; but during that period of time, management significantly shifted the\ndirection they wanted to go with the project, and development changed direction\naccordingly. During this change, a problem emerged: the code base accumulated\nlarge chunks of leftover code that no one dared to delete or refactor. The company\nno longer needed the features that code provided, but some parts of it were used in\nnew functionality, so it was impossible to get rid of the old code completely.\nThe project had good test coverage. But every time someone tried to refactor the old\nfeatures and separate the bits that were still in use from everything else, the tests\nfailed. And not just the old tests—they had been disabled long ago—but the new\ntests, too. Some of the failures were legitimate, but most were not—they were false\npositives.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n71\nDiving into the four pillars of a good unit test\nThis story is typical of most projects with brittle tests. First, developers take test failures\nat face value and deal with them accordingly. After a while, people get tired of tests\ncrying “wolf” all the time and start to ignore them more and more. Eventually, there\ncomes a moment when a bunch of real bugs are released to production because devel-\nopers ignored the failures along with all the false positives.\n You don’t want to react to such a situation by ceasing all refactorings, though. The\ncorrect response is to re-evaluate the test suite and start reducing its brittleness. I\ncover this topic in chapter 7. \n4.1.3\nWhat causes false positives?\nSo, what causes false positives? And how can you avoid them?\n The number of false positives a test produces is directly related to the way the test\nis structured. The more the test is coupled to the implementation details of the system\nunder test (SUT), the more false alarms it generates. The only way to reduce the\nchance of getting a false positive is to decouple the test from those implementation\ndetails. You need to make sure the test verifies the end result the SUT delivers: its\nobservable behavior, not the steps it takes to do that. Tests should approach SUT veri-\nfication from the end user’s point of view and check only the outcome meaningful to\nthat end user. Everything else must be disregarded (more on this topic in chapter 5).\n The best way to structure a test is to make it tell a story about the problem domain.\nShould such a test fail, that failure would mean there’s a disconnect between the story\nand the actual application behavior. It’s the only type of test failure that benefits you—\nsuch failures are always on point and help you quickly understand what went wrong.\nAll other failures are just noise that steer your attention away from things that matter.\n Take a look at the following example. In it, the MessageRenderer class generates\nan HTML representation of a message containing a header, a body, and a footer.\npublic class Message\n{\npublic string Header { get; set; }\npublic string Body { get; set; }\npublic string Footer { get; set; }\n}\nAt first, the developers tried to deal with the test failures. However, since the vast\nmajority of them were false alarms, the situation got to the point where the develop-\ners ignored such failures and disabled the failing tests. The prevailing attitude was,\n“If it’s because of that old chunk of code, just disable the test; we’ll look at it later.”\nEverything worked fine for a while—until a major bug slipped into production. One of\nthe tests correctly identified the bug, but no one listened; the test was disabled along\nwith all the others. After that accident, the developers stopped touching the old code\nentirely.\nListing 4.1\nGenerating an HTML representation of a message\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n72\nCHAPTER 4\nThe four pillars of a good unit test\npublic interface IRenderer\n{\nstring Render(Message message);\n}\npublic class MessageRenderer : IRenderer\n{\npublic IReadOnlyList<IRenderer> SubRenderers { get; }\npublic MessageRenderer()\n{\nSubRenderers = new List<IRenderer>\n{\nnew HeaderRenderer(),\nnew BodyRenderer(),\nnew FooterRenderer()\n};\n}\npublic string Render(Message message)\n{\nreturn SubRenderers\n.Select(x => x.Render(message))\n.Aggregate(\"\", (str1, str2) => str1 + str2);\n}\n}\nThe MessageRenderer class contains several sub-renderers to which it delegates the\nactual work on parts of the message. It then combines the result into an HTML docu-\nment. The sub-renderers orchestrate the raw text with HTML tags. For example:\npublic class BodyRenderer : IRenderer\n{\npublic string Render(Message message)\n{\nreturn $\"<b>{message.Body}</b>\";\n}\n}\nHow can MessageRenderer be tested? One possible approach is to analyze the algo-\nrithm this class follows.\n[Fact]\npublic void MessageRenderer_uses_correct_sub_renderers()\n{\nvar sut = new MessageRenderer();\nIReadOnlyList<IRenderer> renderers = sut.SubRenderers;\nListing 4.2\nVerifying that MessageRenderer has the correct structure\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n73\nDiving into the four pillars of a good unit test\nAssert.Equal(3, renderers.Count);\nAssert.IsAssignableFrom<HeaderRenderer>(renderers[0]);\nAssert.IsAssignableFrom<BodyRenderer>(renderers[1]);\nAssert.IsAssignableFrom<FooterRenderer>(renderers[2]);\n}\nThis test checks to see if the sub-renderers are all of the expected types and appear in\nthe correct order, which presumes that the way MessageRenderer processes messages\nmust also be correct. The test might look good at first, but does it really verify Message-\nRenderer’s observable behavior? What if you rearrange the sub-renderers, or replace\none of them with a new one? Will that lead to a bug?\n Not necessarily. You could change a sub-renderer’s composition in such a way that\nthe resulting HTML document remains the same. For example, you could replace\nBodyRenderer with a BoldRenderer, which does the same job as BodyRenderer. Or you\ncould get rid of all the sub-renderers and implement the rendering directly in Message-\nRenderer.\n Still, the test will turn red if you do any of that, even though the end result won’t\nchange. That’s because the test couples to the SUT’s implementation details and not\nthe outcome the SUT produces. This test inspects the algorithm and expects to see\none particular implementation, without any consideration for equally applicable alter-\nnative implementations (see figure 4.1).\nAny substantial refactoring of the MessageRenderer class would lead to a test failure.\nMind you, the process of refactoring is changing the implementation without affecting\nthe application’s observable behavior. And it’s precisely because the test is concerned\nwith the implementation details that it turns red every time you change those details.\nStep 1\nStep 2\nStep 3\nClient\nSystem under test\nTest: “Are\nthese steps\ncorrect?”\nFigure 4.1\nA test that couples to the SUT’s algorithm. Such a test expects to see one particular \nimplementation (the specific steps the SUT must take to deliver the result) and therefore is \nbrittle. Any refactoring of the SUT’s implementation would lead to a test failure.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n74\nCHAPTER 4\nThe four pillars of a good unit test\nTherefore, tests that couple to the SUT’s implementation details are not resistant to refactoring.\nSuch tests exhibit all the shortcomings I described previously:\nThey don’t provide an early warning in the event of regressions—you simply\nignore those warnings due to little relevance.\nThey hinder your ability and willingness to refactor. It’s no wonder—who would\nlike to refactor, knowing that the tests can’t tell which way is up when it comes\nto finding bugs?\nThe next listing shows the most egregious example of brittleness in tests that I’ve ever\nencountered, in which the test reads the source code of the MessageRenderer class\nand compares it to the “correct” implementation.\n[Fact]\npublic void MessageRenderer_is_implemented_correctly()\n{\nstring sourceCode = File.ReadAllText(@\"[path]\\MessageRenderer.cs\");\nAssert.Equal(@\"\npublic class MessageRenderer : IRenderer\n{\npublic IReadOnlyList<<IRenderer> SubRenderers { get; }\npublic MessageRenderer()\n{\nSubRenderers = new List<<IRenderer>\n{\nnew HeaderRenderer(),\nnew BodyRenderer(),\nnew FooterRenderer()\n};\n}\npublic string Render(Message message) { /* ... */ }\n}\", sourceCode);\n}\nOf course, this test is just plain ridiculous; it will fail should you modify even the slight-\nest detail in the MessageRenderer class. At the same time, it’s not that different from\nthe test I brought up earlier. Both insist on a particular implementation without tak-\ning into consideration the SUT’s observable behavior. And both will turn red each\ntime you change that implementation. Admittedly, though, the test in listing 4.3 will\nbreak more often than the one in listing 4.2. \n4.1.4\nAim at the end result instead of implementation details\nAs I mentioned earlier, the only way to avoid brittleness in tests and increase their resis-\ntance to refactoring is to decouple them from the SUT’s implementation details—keep\nas much distance as possible between the test and the code’s inner workings, and\nListing 4.3\nVerifying the source code of the MessageRenderer class\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n75\nDiving into the four pillars of a good unit test\ninstead aim at verifying the end result. Let’s do that: let’s refactor the test from list-\ning 4.2 into something much less brittle.\n To start off, you need to ask yourself the following question: What is the final out-\ncome you get from MessageRenderer? Well, it’s the HTML representation of a mes-\nsage. And it’s the only thing that makes sense to check, since it’s the only observable\nresult you get out of the class. As long as this HTML representation stays the same,\nthere’s no need to worry about exactly how it’s generated. Such implementation\ndetails are irrelevant. The following code is the new version of the test.\n[Fact]\npublic void Rendering_a_message()\n{\nvar sut = new MessageRenderer();\nvar message = new Message\n{\nHeader = \"h\",\nBody = \"b\",\nFooter = \"f\"\n};\nstring html = sut.Render(message);\nAssert.Equal(\"<h1>h</h1><b>b</b><i>f</i>\", html);\n}\nThis test treats MessageRenderer as a black box and is only interested in its observable\nbehavior. As a result, the test is much more resistant to refactoring—it doesn’t care\nwhat changes you make to the SUT as long as the HTML output remains the same\n(figure 4.2).\n Notice the profound improvement in this test over the original version. It aligns\nitself with the business needs by verifying the only outcome meaningful to end users—\nListing 4.4\nVerifying the outcome that MessageRenderer produces\nStep 1\nStep 2\nStep 3\nClient\nSystem under test\nGood test: “Is\nthe end result\ncorrect?”\nStep 1\nStep 2\nStep 3\nClient\nSystem under test\nBad test: “Are\nthese steps\ncorrect?”\nFigure 4.2\nThe test on the left couples to the SUT’s observable behavior as opposed to implementation \ndetails. Such a test is resistant to refactoring—it will trigger few, if any, false positives.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "page_number": 88
    },
    {
      "number": 12,
      "title": "Segment 12 (pages 98-108)",
      "start_page": 98,
      "end_page": 108,
      "detection_method": "topic_boundary",
      "content": "76\nCHAPTER 4\nThe four pillars of a good unit test\nhow a message is displayed in the browser. Failures of such a test are always on point:\nthey communicate a change in the application behavior that can affect the customer\nand thus should be brought to the developer’s attention. This test will produce few, if\nany, false positives.\n Why few and not none at all? Because there could still be changes in Message-\nRenderer that would break the test. For example, you could introduce a new parame-\nter in the Render() method, causing a compilation error. And technically, such an\nerror counts as a false positive, too. After all, the test isn’t failing because of a change\nin the application’s behavior.\n But this kind of false positive is easy to fix. Just follow the compiler and add a new\nparameter to all tests that invoke the Render() method. The worse false positives are\nthose that don’t lead to compilation errors. Such false positives are the hardest to deal\nwith—they seem as though they point to a legitimate bug and require much more\ntime to investigate.\n4.2\nThe intrinsic connection between the first \ntwo attributes\nAs I mentioned earlier, there’s an intrinsic connection between the first two pillars of\na good unit test—protection against regressions and resistance to refactoring. They both con-\ntribute to the accuracy of the test suite, though from opposite perspectives. These two\nattributes also tend to influence the project differently over time: while it’s important\nto have good protection against regressions very soon after the project’s initiation, the\nneed for resistance to refactoring is not immediate.\n In this section, I talk about\nMaximizing test accuracy\nThe importance of false positives and false negatives\n4.2.1\nMaximizing test accuracy\nLet’s step back for a second and look at the broader picture with regard to test results.\nWhen it comes to code correctness and test results, there are four possible outcomes,\nas shown in figure 4.3. The test can either pass or fail (the rows of the table). And the\nfunctionality itself can be either correct or broken (the table’s columns).\n The situation when the test passes and the underlying functionality works as\nintended is a correct inference: the test correctly inferred the state of the system (there\nare no bugs in it). Another term for this combination of working functionality and a\npassing test is true negative.\n Similarly, when the functionality is broken and the test fails, it’s also a correct infer-\nence. That’s because you expect to see the test fail when the functionality is not work-\ning properly. That’s the whole point of unit testing. The corresponding term for this\nsituation is true positive.\n But when the test doesn’t catch an error, that’s a problem. This is the upper-right\nquadrant, a false negative. And this is what the first attribute of a good test—protection\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n77\nThe intrinsic connection between the first two attributes\nagainst regressions—helps you avoid. Tests with a good protection against regressions\nhelp you to minimize the number of false negatives—type II errors.\n On the other hand, there’s a symmetric situation when the functionality is correct\nbut the test still shows a failure. This is a false positive, a false alarm. And this is what the\nsecond attribute—resistance to refactoring—helps you with.\n All these terms (false positive, type I error and so on) have roots in statistics, but can\nalso be applied to analyzing a test suite. The best way to wrap your head around them\nis to think of a flu test. A flu test is positive when the person taking the test has the flu.\nThe term positive is a bit confusing because there’s nothing positive about having the\nflu. But the test doesn’t evaluate the situation as a whole. In the context of testing,\npositive means that some set of conditions is now true. Those are the conditions the\ncreators of the test have set it to react to. In this particular example, it’s the presence\nof the flu. Conversely, the lack of flu renders the flu test negative.\n Now, when you evaluate how accurate the flu test is, you bring up terms such as\nfalse positive or false negative. The probability of false positives and false negatives tells\nyou how good the flu test is: the lower that probability, the more accurate the test.\n This accuracy is what the first two pillars of a good unit test are all about. Protection\nagainst regressions and resistance to refactoring aim at maximizing the accuracy of the test\nsuite. The accuracy metric itself consists of two components:\nHow good the test is at indicating the presence of bugs (lack of false negatives,\nthe sphere of protection against regressions)\nHow good the test is at indicating the absence of bugs (lack of false positives,\nthe sphere of resistance to refactoring)\nAnother way to think of false positives and false negatives is in terms of signal-to-noise\nratio. As you can see from the formula in figure 4.4, there are two ways to improve test\nTable of error types\nType II error\n(false negative)\nCorrect inference\n(true positives)\nType I error\n(false positive)\nCorrect inference\n(true negatives)\nResistance to\nrefactoring\nTest\nresult\nTest fails\nTest passes\nCorrect\nFunctionality is\nBroken\nProtection\nagainst\nregressions\nFigure 4.3\nThe relationship between protection against regressions and resistance to \nrefactoring. Protection against regressions guards against false negatives (type II errors). \nResistance to refactoring minimizes the number of false positives (type I errors).\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n78\nCHAPTER 4\nThe four pillars of a good unit test\naccuracy. The first is to increase the numerator, signal: that is, make the test better at\nfinding regressions. The second is to reduce the denominator, noise: make the test bet-\nter at not raising false alarms.\n Both are critically important. There’s no use for a test that isn’t capable of finding\nany bugs, even if it doesn’t raise false alarms. Similarly, the test’s accuracy goes to zero\nwhen it generates a lot of noise, even if it’s capable of finding all the bugs in code.\nThese findings are simply lost in the sea of irrelevant information. \n4.2.2\nThe importance of false positives and false negatives: \nThe dynamics\nIn the short term, false positives are not as bad as false negatives. In the beginning of a\nproject, receiving a wrong warning is not that big a deal as opposed to not being\nwarned at all and running the risk of a bug slipping into production. But as the proj-\nect grows, false positives start to have an increasingly large effect on the test suite\n(figure 4.5).\nTest accuracy =\nNoise (number of false alarms raised)\nSignal (number of bugs found)\nFigure 4.4\nA test is accurate insofar as it generates a \nstrong signal (is capable of finding bugs) with as little \nnoise (false alarms) as possible.\nEﬀect on the\ntest suite\nProject duration\nFalse negatives\nFalse positives\nFigure 4.5\nFalse positives (false alarms) don’t have as much of a \nnegative effect in the beginning. But they become increasingly \nimportant as the project grows—as important as false negatives \n(unnoticed bugs).\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n79\nThe third and fourth pillars: Fast feedback and maintainability\nWhy are false positives not as important initially? Because the importance of refactor-\ning is also not immediate; it increases gradually over time. You don’t need to conduct\nmany code clean-ups in the beginning of the project. Newly written code is often shiny\nand flawless. It’s also still fresh in your memory, so you can easily refactor it even if\ntests raise false alarms.\n But as time goes on, the code base deteriorates. It becomes increasingly complex\nand disorganized. Thus you have to start conducting regular refactorings in order to\nmitigate this tendency. Otherwise, the cost of introducing new features eventually\nbecomes prohibitive.\n As the need for refactoring increases, the importance of resistance to refactoring in\ntests increases with it. As I explained earlier, you can’t refactor when the tests keep cry-\ning “wolf” and you keep getting warnings about bugs that don’t exist. You quickly lose\ntrust in such tests and stop viewing them as a reliable source of feedback.\n Despite the importance of protecting your code against false positives, especially in\nthe later project stages, few developers perceive false positives this way. Most people\ntend to focus solely on improving the first attribute of a good unit test—protection\nagainst regressions, which is not enough to build a valuable, highly accurate test suite\nthat helps sustain project growth.\n The reason, of course, is that far fewer projects get to those later stages, mostly\nbecause they are small and the development finishes before the project becomes too\nbig. Thus developers face the problem of unnoticed bugs more often than false\nalarms that swarm the project and hinder all refactoring undertakings. And so, people\noptimize accordingly. Nevertheless, if you work on a medium to large project, you\nhave to pay equal attention to both false negatives (unnoticed bugs) and false posi-\ntives (false alarms). \n4.3\nThe third and fourth pillars: Fast feedback \nand maintainability\nIn this section, I talk about the two remaining pillars of a good unit test:\nFast feedback\nMaintainability\nAs you may remember from chapter 2, fast feedback is an essential property of a unit\ntest. The faster the tests, the more of them you can have in the suite and the more\noften you can run them.\n With tests that run quickly, you can drastically shorten the feedback loop, to the\npoint where the tests begin to warn you about bugs as soon as you break the code, thus\nreducing the cost of fixing those bugs almost to zero. On the other hand, slow tests\ndelay the feedback and potentially prolong the period during which the bugs remain\nunnoticed, thus increasing the cost of fixing them. That’s because slow tests discour-\nage you from running them often, and therefore lead to wasting more time moving in\na wrong direction.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n80\nCHAPTER 4\nThe four pillars of a good unit test\n Finally, the fourth pillar of good units tests, the maintainability metric, evaluates\nmaintenance costs. This metric consists of two major components:\nHow hard it is to understand the test—This component is related to the size of the\ntest. The fewer lines of code in the test, the more readable the test is. It’s also\neasier to change a small test when needed. Of course, that’s assuming you don’t\ntry to compress the test code artificially just to reduce the line count. The qual-\nity of the test code matters as much as the production code. Don’t cut corners\nwhen writing tests; treat the test code as a first-class citizen.\nHow hard it is to run the test—If the test works with out-of-process dependencies,\nyou have to spend time keeping those dependencies operational: reboot the\ndatabase server, resolve network connectivity issues, and so on. \n4.4\nIn search of an ideal test\nHere are the four attributes of a good unit test once again:\nProtection against regressions\nResistance to refactoring\nFast feedback\nMaintainability\nThese four attributes, when multiplied together, determine the value of a test. And by\nmultiplied, I mean in a mathematical sense; that is, if a test gets zero in one of the attri-\nbutes, its value turns to zero as well:\nValue estimate = [0..1] * [0..1] * [0..1] * [0..1]\nTIP\nIn order to be valuable, the test needs to score at least something in all\nfour categories.\nOf course, it’s impossible to measure these attributes precisely. There’s no code analy-\nsis tool you can plug a test into and get the exact numbers. But you can still evaluate\nthe test pretty accurately to see where a test stands with regard to the four attributes.\nThis evaluation, in turn, gives you the test’s value estimate, which you can use to\ndecide whether to keep the test in the suite.\n Remember, all code, including test code, is a liability. Set a fairly high threshold\nfor the minimum required value, and only allow tests in the suite if they meet this\nthreshold. A small number of highly valuable tests will do a much better job sustain-\ning project growth than a large number of mediocre tests.\n I’ll show some examples shortly. For now, let’s examine whether it’s possible to cre-\nate an ideal test.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n81\nIn search of an ideal test\n4.4.1\nIs it possible to create an ideal test?\nAn ideal test is a test that scores the maximum in all four attributes. If you take the\nminimum and maximum values as 0 and 1 for each of the attributes, an ideal test must\nget 1 in all of them.\n Unfortunately, it’s impossible to create such an ideal test. The reason is that the\nfirst three attributes—protection against regressions, resistance to refactoring, and fast feedback—\nare mutually exclusive. It’s impossible to maximize them all: you have to sacrifice one\nof the three in order to max out the remaining two.\n Moreover, because of the multiplication principle (see the calculation of the value\nestimate in the previous section), it’s even trickier to keep the balance. You can’t just\nforgo one of the attributes in order to focus on the others. As I mentioned previously,\na test that scores zero in one of the four categories is worthless. Therefore, you have to\nmaximize these attributes in such a way that none of them is diminished too much.\nLet’s look at some examples of tests that aim at maximizing two out of three attributes\nat the expense of the third and, as a result, have a value that’s close to zero. \n4.4.2\nExtreme case #1: End-to-end tests\nThe first example is end-to-end tests. As you may remember from chapter 2, end-to-end\ntests look at the system from the end user’s perspective. They normally go through all of\nthe system’s components, including the UI, database, and external applications.\n Since end-to-end tests exercise a lot of code, they provide the best protection\nagainst regressions. In fact, of all types of tests, end-to-end tests exercise the most\ncode—both your code and the code you didn’t write but use in the project, such as\nexternal libraries, frameworks, and third-party applications.\n End-to-end tests are also immune to false positives and thus have a good resistance\nto refactoring. A refactoring, if done correctly, doesn’t change the system’s observable\nbehavior and therefore doesn’t affect the end-to-end tests. That’s another advantage\nof such tests: they don’t impose any particular implementation. The only thing end-to-\nend tests look at is how a feature behaves from the end user’s point of view. They are\nas removed from implementation details as tests could possibly be.\n However, despite these benefits, end-to-end tests have a major drawback: they are\nslow. Any system that relies solely on such tests would have a hard time getting rapid\nfeedback. And that is a deal-breaker for many development teams. This is why it’s\npretty much impossible to cover your code base with only end-to-end tests.\n Figure 4.6 shows where end-to-end tests stand with regard to the first three unit\ntesting metrics. Such tests provide great protection against both regression errors and\nfalse positives, but lack speed. \nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n82\nCHAPTER 4\nThe four pillars of a good unit test\n4.4.3\nExtreme case #2: Trivial tests\nAnother example of maximizing two out of three attributes at the expense of the third\nis a trivial test. Such tests cover a simple piece of code, something that is unlikely to\nbreak because it’s too trivial, as shown in the following listing.\npublic class User\n{\npublic string Name { get; set; }    \n}\n[Fact]\npublic void Test()\n{\nvar sut = new User();\nsut.Name = \"John Smith\";\nAssert.Equal(\"John Smith\", sut.Name);\n}\nUnlike end-to-end tests, trivial tests do provide fast feedback—they run very quickly.\nThey also have a fairly low chance of producing a false positive, so they have good\nresistance to refactoring. Trivial tests are unlikely to reveal any regressions, though,\nbecause there’s not much room for a mistake in the underlying code.\n Trivial tests taken to an extreme result in tautology tests. They don’t test anything\nbecause they are set up in such a way that they always pass or contain semantically\nmeaningless assertions.\nListing 4.5\nTrivial test covering a simple piece of code\nResistance to\nrefactoring\nFast\nfeedback\nProtection\nagainst\nregressions\nEnd-to-end tests\nFigure 4.6\nEnd-to-end tests \nprovide great protection against \nboth regression errors and false \npositives, but they fail at the \nmetric of fast feedback.\nOne-liners like \nthis are unlikely \nto contain bugs.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n83\nIn search of an ideal test\nFigure 4.7 shows where trivial tests stand. They have good resistance to refactoring\nand provide fast feedback, but they don’t protect you from regressions. \n4.4.4\nExtreme case #3: Brittle tests\nSimilarly, it’s pretty easy to write a test that runs fast and has a good chance of catching\na regression but does so with a lot of false positives. Such a test is called a brittle test: it\ncan’t withstand a refactoring and will turn red regardless of whether the underlying\nfunctionality is broken.\n You already saw an example of a brittle test in listing 4.2. Here’s another one.\npublic class UserRepository\n{\npublic User GetById(int id)\n{\n/* ... */\n}\npublic string LastExecutedSqlStatement { get; set; }\n}\n[Fact]\npublic void GetById_executes_correct_SQL_code()\n{\nvar sut = new UserRepository();\nUser user = sut.GetById(5);\nAssert.Equal(\n\"SELECT * FROM dbo.[User] WHERE UserID = 5\",\nsut.LastExecutedSqlStatement);\n}\nListing 4.6\nTest verifying which SQL statement is executed\nResistance to\nrefactoring\nFast\nfeedback\nProtection\nagainst\nregressions\nEnd-to-end tests\nTrivial tests\nFigure 4.7\nTrivial tests have good \nresistance to refactoring, and they \nprovide fast feedback, but such tests \ndon’t protect you from regressions.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n84\nCHAPTER 4\nThe four pillars of a good unit test\nThis test makes sure the UserRepository class generates a correct SQL statement\nwhen fetching a user from the database. Can this test catch a bug? It can. For exam-\nple, a developer can mess up the SQL code generation and mistakenly use ID instead\nof UserID, and the test will point that out by raising a failure. But does this test have\ngood resistance to refactoring? Absolutely not. Here are different variations of the\nSQL statement that lead to the same result:\nSELECT * FROM dbo.[User] WHERE UserID = 5\nSELECT * FROM dbo.User WHERE UserID = 5\nSELECT UserID, Name, Email FROM dbo.[User] WHERE UserID = 5\nSELECT * FROM dbo.[User] WHERE UserID = @UserID\nThe test in listing 4.6 will turn red if you change the SQL script to any of these varia-\ntions, even though the functionality itself will remain operational. This is once again\nan example of coupling the test to the SUT’s internal implementation details. The test\nis focusing on hows instead of whats and thus ingrains the SUT’s implementation\ndetails, preventing any further refactoring.\n Figure 4.8 shows that brittle tests fall into the third bucket. Such tests run fast and\nprovide good protection against regressions but have little resistance to refactoring. \n4.4.5\nIn search of an ideal test: The results\nThe first three attributes of a good unit test (protection against regressions, resistance to\nrefactoring, and fast feedback) are mutually exclusive. While it’s quite easy to come up\nwith a test that maximizes two out of these three attributes, you can only do that at the\nexpense of the third. Still, such a test would have a close-to-zero value due to the mul-\ntiplication rule. Unfortunately, it’s impossible to create an ideal test that has a perfect\nscore in all three attributes (figure 4.9).\nResistance to\nrefactoring\nFast\nfeedback\nProtection\nagainst\nregressions\nEnd-to-end tests\nTrivial tests\nBrittle tests\nFigure 4.8\nBrittle tests run fast and they \nprovide good protection against regressions, \nbut they have little resistance to refactoring.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n85\nIn search of an ideal test\nThe fourth attribute, maintainability, is not correlated to the first three, with the excep-\ntion of end-to-end tests. End-to-end tests are normally larger in size because of the\nnecessity to set up all the dependencies such tests reach out to. They also require addi-\ntional effort to keep those dependencies operational. Hence end-to-end tests tend to\nbe more expensive in terms of maintenance costs.\n It’s hard to keep a balance between the attributes of a good test. A test can’t have\nthe maximum score in each of the first three categories, and you also have to keep an\neye on the maintainability aspect so the test remains reasonably short and simple.\nTherefore, you have to make trade-offs. Moreover, you should make those trade-offs\nin such a way that no particular attribute turns to zero. The sacrifices have to be par-\ntial and strategic.\n What should those sacrifices look like? Because of the mutual exclusiveness of pro-\ntection against regressions, resistance to refactoring, and fast feedback, you may think that the\nbest strategy is to concede a little bit of each: just enough to make room for all three\nattributes.\n In reality, though, resistance to refactoring is non-negotiable. You should aim at gain-\ning as much of it as you can, provided that your tests remain reasonably quick and you\ndon’t resort to the exclusive use of end-to-end tests. The trade-off, then, comes down\nto the choice between how good your tests are at pointing out bugs and how fast they\ndo that: that is, between protection against regressions and fast feedback. You can view this\nchoice as a slider that can be freely moved between protection against regressions and\nfast feedback. The more you gain in one attribute, the more you lose on the other\n(see figure 4.10).\n The reason resistance to refactoring is non-negotiable is that whether a test possesses\nthis attribute is mostly a binary choice: the test either has resistance to refactoring or it\ndoesn’t. There are almost no intermediate stages in between. Thus you can’t concede\nResistance to\nrefactoring\nFast\nfeedback\nProtection\nagainst\nregressions\nUnreachable ideal\nFigure 4.9\nIt’s impossible to create an \nideal test that would have a perfect score \nin all three attributes.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n86\nCHAPTER 4\nThe four pillars of a good unit test\njust a little resistance to refactoring: you’ll have to lose it all. On the other hand, the metrics\nof protection against regressions and fast feedback are more malleable. You will see in the\nnext section what kind of trade-offs are possible when you choose one over the other.\nTIP\nEradicating brittleness (false positives) in tests is the first priority on the\npath to a robust test suite.\nThe CAP theorem\nThe trade-off between the first three attributes of a good unit test is similar to the\nCAP theorem. The CAP theorem states that it is impossible for a distributed data\nstore to simultaneously provide more than two of the following three guarantees:\nConsistency, which means every read receives the most recent write or an error.\nAvailability, which means every request receives a response (apart from out-\nages that affect all nodes in the system).\nPartition tolerance, which means the system continues to operate despite\nnetwork partitioning (losing connection between network nodes).\nThe similarity is two-fold:\nFirst, there is the two-out-of-three trade-off.\nSecond, the partition tolerance component in large-scale distributed systems is\nalso non-negotiable. A large application such as, for example, the Amazon web-\nsite can’t operate on a single machine. The option of preferring consistency and\navailability at the expense of partition tolerance simply isn’t on the table—Amazon\nhas too much data to store on a single server, however big that server is.\nrefactoring\nMax\nout\nProtection against\nregressions\nFast feedback\nMax\nout\nMaintainability\nChoose between the two\nResistance to\nFigure 4.10\nThe best tests exhibit maximum maintainability and resistance \nto refactoring; always try to max out these two attributes. The trade-off \ncomes down to the choice between protection against regressions and fast \nfeedback.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "page_number": 98
    },
    {
      "number": 13,
      "title": "Segment 13 (pages 109-120)",
      "start_page": 109,
      "end_page": 120,
      "detection_method": "topic_boundary",
      "content": "87\nExploring well-known test automation concepts\n4.5\nExploring well-known test automation concepts\nThe four attributes of a good unit test shown earlier are foundational. All existing,\nwell-known test automation concepts can be traced back to these four attributes. In\nthis section, we’ll look at two such concepts: the Test Pyramid and white-box versus\nblack-box testing.\n4.5.1\nBreaking down the Test Pyramid\nThe Test Pyramid is a concept that advocates for a certain ratio of different types of\ntests in the test suite (figure 4.11):\nUnit tests\nIntegration tests\nEnd-to-end tests\nThe Test Pyramid is often represented visually as a pyramid with those three types of\ntests in it. The width of the pyramid layers refers to the prevalence of a particular type\nThe choice, then, also boils down to a trade-off between consistency and availability.\nIn some parts of the system, it’s preferable to concede a little consistency to gain\nmore availability. For example, when displaying a product catalog, it’s generally fine\nif some parts of the catalog are out of date. Availability is of higher priority in this sce-\nnario. On the other hand, when updating a product description, consistency is more\nimportant than availability: network nodes must have a consensus on what the most\nrecent version of that description is, in order to avoid merge conflicts. \nEnd-\nto-end\nIntegration\ntests\nUnit tests\nTest count\nEmulating\nuser\nFigure 4.11\nThe Test Pyramid advocates for a certain ratio of unit, \nintegration, and end-to-end tests.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n88\nCHAPTER 4\nThe four pillars of a good unit test\nof test in the suite. The wider the layer, the greater the test count. The height of the\nlayer is a measure of how close these tests are to emulating the end user’s behavior.\nEnd-to-end tests are at the top—they are the closest to imitating the user experience.\nDifferent types of tests in the pyramid make different choices in the trade-off between\nfast feedback and protection against regressions. Tests in higher pyramid layers favor protec-\ntion against regressions, while lower layers emphasize execution speed (figure 4.12).\nNotice that neither layer gives up resistance to refactoring. Naturally, end-to-end and inte-\ngration tests score higher on this metric than unit tests, but only as a side effect of\nbeing more detached from the production code. Still, even unit tests should not con-\ncede resistance to refactoring. All tests should aim at producing as few false positives as\npossible, even when working directly with the production code. (How to do that is the\ntopic of the next chapter.)\n The exact mix between types of tests will be different for each team and project.\nBut in general, it should retain the pyramid shape: end-to-end tests should be the\nminority; unit tests, the majority; and integration tests somewhere in the middle.\n The reason end-to-end tests are the minority is, again, the multiplication rule\ndescribed in section 4.4. End-to-end tests score extremely low on the metric of fast feed-\nback. They also lack maintainability: they tend to be larger in size and require addi-\ntional effort to maintain the involved out-of-process dependencies. Thus, end-to-end\ntests only make sense when applied to the most critical functionality—features in\nrefactoring\nMax\nout\nProtection against\nregressions\nFast feedback\nEnd-to-end\nIntegration\nUnit tests\nResistance to\nFigure 4.12\nDifferent types of tests in the pyramid make different choices \nbetween fast feedback and protection against regressions. End-to-end tests \nfavor protection against regressions, unit tests emphasize fast feedback, and \nintegration tests lie in the middle.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n89\nExploring well-known test automation concepts\nwhich you don’t ever want to see any bugs—and only when you can’t get the same\ndegree of protection with unit or integration tests. The use of end-to-end tests for any-\nthing else shouldn’t pass your minimum required value threshold. Unit tests are usu-\nally more balanced, and hence you normally have many more of them.\n There are exceptions to the Test Pyramid. For example, if all your application does\nis basic create, read, update, and delete (CRUD) operations with very few business\nrules or any other complexity, your test “pyramid” will most likely look like a rectangle\nwith an equal number of unit and integration tests and no end-to-end tests.\n Unit tests are less useful in a setting without algorithmic or business complexity—\nthey quickly descend into trivial tests. At the same time, integration tests retain their\nvalue—it’s still important to verify how code, however simple it is, works in integration\nwith other subsystems, such as the database. As a result, you may end up with fewer\nunit tests and more integration tests. In the most trivial examples, the number of inte-\ngration tests may even be greater than the number of unit tests.\n Another exception to the Test Pyramid is an API that reaches out to a single out-of-\nprocess dependency—say, a database. Having more end-to-end tests may be a viable\noption for such an application. Since there’s no user interface, end-to-end tests will\nrun reasonably fast. The maintenance costs won’t be too high, either, because you\nonly work with the single external dependency, the database. Basically, end-to-end\ntests are indistinguishable from integration tests in this environment. The only thing\nthat differs is the entry point: end-to-end tests require the application to be hosted\nsomewhere to fully emulate the end user, while integration tests normally host the\napplication in the same process. We’ll get back to the Test Pyramid in chapter 8, when\nwe’ll be talking about integration testing. \n4.5.2\nChoosing between black-box and white-box testing\nThe other well-known test automation concept is black-box versus white-box testing.\nIn this section, I show when to use each of the two approaches:\nBlack-box testing is a method of software testing that examines the functionality\nof a system without knowing its internal structure. Such testing is normally built\naround specifications and requirements: what the application is supposed to do,\nrather than how it does it.\nWhite-box testing is the opposite of that. It’s a method of testing that verifies the\napplication’s inner workings. The tests are derived from the source code, not\nrequirements or specifications.\nThere are pros and cons to both of these methods. White-box testing tends to be more\nthorough. By analyzing the source code, you can uncover a lot of errors that you may\nmiss when relying solely on external specifications. On the other hand, tests resulting\nfrom white-box testing are often brittle, as they tend to tightly couple to the specific\nimplementation of the code under test. Such tests produce many false positives and\nthus fall short on the metric of resistance to refactoring. They also often can’t be traced\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n90\nCHAPTER 4\nThe four pillars of a good unit test\nback to a behavior that is meaningful to a business person, which is a strong sign that\nthese tests are fragile and don’t add much value. Black-box testing provides the oppo-\nsite set of pros and cons (table 4.1).\nAs you may remember from section 4.4.5, you can’t compromise on resistance to refac-\ntoring: a test either possesses resistance to refactoring or it doesn’t. Therefore, choose black-\nbox testing over white-box testing by default. Make all tests—be they unit, integration, or\nend-to-end—view the system as a black box and verify behavior meaningful to the\nproblem domain. If you can’t trace a test back to a business requirement, it’s an indi-\ncation of the test’s brittleness. Either restructure or delete this test; don’t let it into the\nsuite as-is. The only exception is when the test covers utility code with high algorith-\nmic complexity (more on this in chapter 7).\n Note that even though black-box testing is preferable when writing tests, you can\nstill use the white-box method when analyzing the tests. Use code coverage tools to see which\ncode branches are not exercised, but then turn around and test them as if you know nothing about\nthe code’s internal structure. Such a combination of the white-box and black-box meth-\nods works best. \nSummary\nA good unit test has four foundational attributes that you can use to analyze any\nautomated test, whether unit, integration, or end-to-end:\n– Protection against regressions\n– Resistance to refactoring\n– Fast feedback\n– Maintainability\nProtection against regressions is a measure of how good the test is at indicating the\npresence of bugs (regressions). The more code the test executes (both your\ncode and the code of libraries and frameworks used in the project), the higher\nthe chance this test will reveal a bug.\nResistance to refactoring is the degree to which a test can sustain application code\nrefactoring without producing a false positive.\nA false positive is a false alarm—a result indicating that the test fails, whereas\nthe functionality it covers works as intended. False positives can have a devastat-\ning effect on the test suite:\n– They dilute your ability and willingness to react to problems in code, because\nyou get accustomed to false alarms and stop paying attention to them.\nTable 4.1\nThe pros and cons of white-box and black-box testing\nProtection against regressions\nResistance to refactoring\nWhite-box testing\nGood\nBad\nBlack-box testing\nBad\nGood\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n91\nSummary\n– They diminish your perception of tests as a reliable safety net and lead to los-\ning trust in the test suite.\nFalse positives are a result of tight coupling between tests and the internal imple-\nmentation details of the system under test. To avoid such coupling, the test\nmust verify the end result the SUT produces, not the steps it took to do that.\nProtection against regressions and resistance to refactoring contribute to test accuracy.\nA test is accurate insofar as it generates a strong signal (is capable of finding\nbugs, the sphere of protection against regressions) with as little noise (false posi-\ntives) as possible (the sphere of resistance to refactoring).\nFalse positives don’t have as much of a negative effect in the beginning of the\nproject, but they become increasingly important as the project grows: as import-\nant as false negatives (unnoticed bugs).\nFast feedback is a measure of how quickly the test executes.\nMaintainability consists of two components:\n– How hard it is to understand the test. The smaller the test, the more read-\nable it is.\n– How hard it is to run the test. The fewer out-of-process dependencies the test\nreaches out to, the easier it is to keep them operational.\nA test’s value estimate is the product of scores the test gets in each of the four attri-\nbutes. If the test gets zero in one of the attributes, its value turns to zero as well.\nIt’s impossible to create a test that gets the maximum score in all four attri-\nbutes, because the first three—protection against regressions, resistance to refactor-\ning, and fast feedback—are mutually exclusive. The test can only maximize two\nout of the three.\nResistance to refactoring is non-negotiable because whether a test possess this attri-\nbute is mostly a binary choice: the test either has resistance to refactoring or it\ndoesn’t. The trade-off between the attributes comes down to the choice\nbetween protection against regressions and fast feedback.\nThe Test Pyramid advocates for a certain ratio of unit, integration, and end-to-\nend tests: end-to-end tests should be in the minority, unit tests in the majority,\nand integration tests somewhere in the middle.\nDifferent types of tests in the pyramid make different choices between fast feed-\nback and protection against regressions. End-to-end tests favor protection against\nregressions, while unit tests favor fast feedback.\nUse the black-box testing method when writing tests. Use the white-box method\nwhen analyzing the tests.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n92\nMocks and test fragility\nChapter 4 introduced a frame of reference that you can use to analyze specific tests\nand unit testing approaches. In this chapter, you’ll see that frame of reference in\naction; we’ll use it to dissect the topic of mocks.\n The use of mocks in tests is a controversial subject. Some people argue that\nmocks are a great tool and apply them in most of their tests. Others claim that mocks\nlead to test fragility and try not to use them at all. As the saying goes, the truth lies\nsomewhere in between. In this chapter, I’ll show that, indeed, mocks often result in\nfragile tests—tests that lack the metric of resistance to refactoring. But there are still\ncases where mocking is applicable and even preferable.\nThis chapter covers\nDifferentiating mocks from stubs\nDefining observable behavior and implementation \ndetails\nUnderstanding the relationship between mocks \nand test fragility\nUsing mocks without compromising resistance \nto refactoring\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n93\nDifferentiating mocks from stubs\n This chapter draws heavily on the discussion about the London versus classical\nschools of unit testing from chapter 2. In short, the disagreement between the schools\nstems from their views on the test isolation issue. The London school advocates isolat-\ning pieces of code under test from each other and using test doubles for all but\nimmutable dependencies to perform such isolation.\n The classical school stands for isolating unit tests themselves so that they can be\nrun in parallel. This school uses test doubles only for dependencies that are shared\nbetween tests.\n There’s a deep and almost inevitable connection between mocks and test fragility.\nIn the next several sections, I will gradually lay down the foundation for you to see why\nthat connection exists. You will also learn how to use mocks so that they don’t compro-\nmise a test’s resistance to refactoring.\n5.1\nDifferentiating mocks from stubs\nIn chapter 2, I briefly mentioned that a mock is a test double that allows you to exam-\nine interactions between the system under test (SUT) and its collaborators. There’s\nanother type of test double: a stub. Let’s take a closer look at what a mock is and how it\nis different from a stub.\n5.1.1\nThe types of test doubles\nA test double is an overarching term that describes all kinds of non-production-ready,\nfake dependencies in tests. The term comes from the notion of a stunt double in a\nmovie. The major use of test doubles is to facilitate testing; they are passed to the\nsystem under test instead of real dependencies, which could be hard to set up or\nmaintain.\n According to Gerard Meszaros, there are five variations of test doubles: dummy,\nstub, spy, mock, and fake.1 Such a variety can look intimidating, but in reality, they can all\nbe grouped together into just two types: mocks and stubs (figure 5.1).\n1 See xUnit Test Patterns: Refactoring Test Code (Addison-Wesley, 2007).\nTest double\nMock\n(mock, spy)\nStub\n(stub, dummy, fake)\nFigure 5.1\nAll variations of test \ndoubles can be categorized into \ntwo types: mocks and stubs.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n94\nCHAPTER 5\nMocks and test fragility\nThe difference between these two types boils down to the following:\nMocks help to emulate and examine outcoming interactions. These interactions\nare calls the SUT makes to its dependencies to change their state.\nStubs help to emulate incoming interactions. These interactions are calls the\nSUT makes to its dependencies to get input data (figure 5.2).\nAll other differences between the five variations are insignificant implementation\ndetails. For example, spies serve the same role as mocks. The distinction is that spies\nare written manually, whereas mocks are created with the help of a mocking frame-\nwork. Sometimes people refer to spies as handwritten mocks.\n On the other hand, the difference between a stub, a dummy, and a fake is in how\nintelligent they are. A dummy is a simple, hardcoded value such as a null value or a\nmade-up string. It’s used to satisfy the SUT’s method signature and doesn’t partici-\npate in producing the final outcome. A stub is more sophisticated. It’s a fully fledged\ndependency that you configure to return different values for different scenarios.\nFinally, a fake is the same as a stub for most purposes. The difference is in the ratio-\nnale for its creation: a fake is usually implemented to replace a dependency that\ndoesn’t yet exist.\n Notice the difference between mocks and stubs (aside from outcoming versus\nincoming interactions). Mocks help to emulate and examine interactions between the\nSUT and its dependencies, while stubs only help to emulate those interactions. This is\nan important distinction. You will see why shortly. \n5.1.2\nMock (the tool) vs. mock (the test double)\nThe term mock is overloaded and can mean different things in different circum-\nstances. I mentioned in chapter 2 that people often use this term to mean any test\ndouble, whereas mocks are only a subset of test doubles. But there’s another meaning\nSystem under test\nSMTP server\nSend an email\nRetrieve data\nDatabase\nStub\nMock\nFigure 5.2\nSending an email is \nan outcoming interaction: an inter-\naction that results in a side effect \nin the SMTP server. A test double \nemulating such an interaction is \na mock. Retrieving data from the \ndatabase is an incoming inter-\naction; it doesn’t result in a \nside effect. The corresponding \ntest double is a stub.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n95\nDifferentiating mocks from stubs\nfor the term mock. You can refer to the classes from mocking libraries as mocks, too.\nThese classes help you create actual mocks, but they themselves are not mocks per se.\nThe following listing shows an example.\n[Fact]\npublic void Sending_a_greetings_email()\n{\nvar mock = new Mock<IEmailGateway>();      \nvar sut = new Controller(mock.Object);\nsut.GreetUser(\"user@email.com\");\nmock.Verify(\n   \nx => x.SendGreetingsEmail(   \n\"user@email.com\"),\n   \nTimes.Once);\n   \n}\nThe test in listing 5.1 uses the Mock class from the mocking library of my choice\n(Moq). This class is a tool that enables you to create a test double—a mock. In other\nwords, the class Mock (or Mock<IEmailGateway>) is a mock (the tool), while the instance\nof that class, mock, is a mock (the test double). It’s important not to conflate a mock (the\ntool) with a mock (the test double) because you can use a mock (the tool) to create\nboth types of test doubles: mocks and stubs.\n The test in the following listing also uses the Mock class, but the instance of that\nclass is not a mock, it’s a stub.\n[Fact]\npublic void Creating_a_report()\n{\nvar stub = new Mock<IDatabase>();       \nstub.Setup(x => x.GetNumberOfUsers())     \n.Returns(10);\n     \nvar sut = new Controller(stub.Object);\nReport report = sut.CreateReport();\nAssert.Equal(10, report.NumberOfUsers);\n}\nThis test double emulates an incoming interaction—a call that provides the SUT with\ninput data. On the other hand, in the previous example (listing 5.1), the call to Send-\nGreetingsEmail() is an outcoming interaction. Its sole purpose is to incur a side\neffect—send an email. \nListing 5.1\nUsing the Mock class from a mocking library to create a mock\nListing 5.2\nUsing the Mock class to create a stub\nUses a mock (the \ntool) to create a mock \n(the test double)\nExamines the call \nfrom the SUT to \nthe test double\nUses a mock \n(the tool) to \ncreate a stub\nSets up a \ncanned answer\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n96\nCHAPTER 5\nMocks and test fragility\n5.1.3\nDon’t assert interactions with stubs\nAs I mentioned in section 5.1.1, mocks help to emulate and examine outcoming interac-\ntions between the SUT and its dependencies, while stubs only help to emulate incom-\ning interactions, not examine them. The difference between the two stems from the\nguideline of never asserting interactions with stubs. A call from the SUT to a stub is not\npart of the end result the SUT produces. Such a call is only a means to produce the\nend result: a stub provides input from which the SUT then generates the output.\nNOTE\nAsserting interactions with stubs is a common anti-pattern that leads to\nfragile tests.\nAs you might remember from chapter 4, the only way to avoid false positives and thus\nimprove resistance to refactoring in tests is to make those tests verify the end result\n(which, ideally, should be meaningful to a non-programmer), not implementation\ndetails. In listing 5.1, the check\nmock.Verify(x => x.SendGreetingsEmail(\"user@email.com\"))\ncorresponds to an actual outcome, and that outcome is meaningful to a domain\nexpert: sending a greetings email is something business people would want the system\nto do. At the same time, the call to GetNumberOfUsers() in listing 5.2 is not an out-\ncome at all. It’s an internal implementation detail regarding how the SUT gathers\ndata necessary for the report creation. Therefore, asserting this call would lead to test\nfragility: it shouldn’t matter how the SUT generates the end result, as long as that\nresult is correct. The following listing shows an example of such a brittle test.\n[Fact]\npublic void Creating_a_report()\n{\nvar stub = new Mock<IDatabase>();\nstub.Setup(x => x.GetNumberOfUsers()).Returns(10);\nvar sut = new Controller(stub.Object);\nReport report = sut.CreateReport();\nAssert.Equal(10, report.NumberOfUsers);\nstub.Verify(\n   \nx => x.GetNumberOfUsers(),   \nTimes.Once);\n   \n}\nThis practice of verifying things that aren’t part of the end result is also called over-\nspecification. Most commonly, overspecification takes place when examining interac-\ntions. Checking for interactions with stubs is a flaw that’s quite easy to spot because\ntests shouldn’t check for any interactions with stubs. Mocks are a more complicated sub-\nListing 5.3\nAsserting an interaction with a stub\nAsserts the \ninteraction \nwith the stub\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n97\nDifferentiating mocks from stubs\nject: not all uses of mocks lead to test fragility, but a lot of them do. You’ll see why later\nin this chapter. \n5.1.4\nUsing mocks and stubs together\nSometimes you need to create a test double that exhibits the properties of both a\nmock and a stub. For example, here’s a test from chapter 2 that I used to illustrate the\nLondon style of unit testing.\n[Fact]\npublic void Purchase_fails_when_not_enough_inventory()\n{\nvar storeMock = new Mock<IStore>();\nstoreMock\n    \n.Setup(x => x.HasEnoughInventory(    \nProduct.Shampoo, 5))\n    \n.Returns(false);\n    \nvar sut = new Customer();\nbool success = sut.Purchase(\nstoreMock.Object, Product.Shampoo, 5);\nAssert.False(success);\nstoreMock.Verify(\n   \nx => x.RemoveInventory(Product.Shampoo, 5),  \nTimes.Never);\n   \n}\nThis test uses storeMock for two purposes: it returns a canned answer and verifies a\nmethod call made by the SUT. Notice, though, that these are two different methods:\nthe test sets up the answer from HasEnoughInventory() but then verifies the call to\nRemoveInventory(). Thus, the rule of not asserting interactions with stubs is not vio-\nlated here.\n When a test double is both a mock and a stub, it’s still called a mock, not a stub.\nThat’s mostly the case because we need to pick one name, but also because being a\nmock is a more important fact than being a stub. \n5.1.5\nHow mocks and stubs relate to commands and queries\nThe notions of mocks and stubs tie to the command query separation (CQS) princi-\nple. The CQS principle states that every method should be either a command or a\nquery, but not both. As shown in figure 5.3, commands are methods that produce side\neffects and don’t return any value (return void). Examples of side effects include\nmutating an object’s state, changing a file in the file system, and so on. Queries are the\nopposite of that—they are side-effect free and return a value.\n To follow this principle, be sure that if a method produces a side effect, that\nmethod’s return type is void. And if the method returns a value, it must stay side-effect\nListing 5.4\nstoreMock: both a mock and a stub\nSets up a \ncanned \nanswer\nExamines a call \nfrom the SUT\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n98\nCHAPTER 5\nMocks and test fragility\nfree. In other words, asking a question should not change the answer. Code that main-\ntains such a clear separation becomes easier to read. You can tell what a method does\njust by looking at its signature, without diving into its implementation details.\n Of course, it’s not always possible to follow the CQS principle. There are always\nmethods for which it makes sense to both incur a side effect and return a value. A clas-\nsical example is stack.Pop(). This method both removes a top element from the\nstack and returns it to the caller. Still, it’s a good idea to adhere to the CQS principle\nwhenever you can.\n Test doubles that substitute commands become mocks. Similarly, test doubles that\nsubstitute queries are stubs. Look at the two tests from listings 5.1 and 5.2 again (I’m\nshowing their relevant parts here):\nvar mock = new Mock<IEmailGateway>();\nmock.Verify(x => x.SendGreetingsEmail(\"user@email.com\"));\nvar stub = new Mock<IDatabase>();\nstub.Setup(x => x.GetNumberOfUsers()).Returns(10);\nSendGreetingsEmail() is a command whose side effect is sending an email. The test\ndouble that substitutes this command is a mock. On the other hand, GetNumberOf-\nUsers() is a query that returns a value and doesn’t mutate the database state. The cor-\nresponding test double is a stub. \n \nMethods\nCommands\nIncur side effects\nNo return value\nMocks\nQueries\nSide-effect free\nReturns a value\nStubs\nFigure 5.3\nIn the command query \nseparation (CQS) principle, commands \ncorrespond to mocks, while queries are \nconsistent with stubs.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "page_number": 109
    },
    {
      "number": 14,
      "title": "Segment 14 (pages 121-131)",
      "start_page": 121,
      "end_page": 131,
      "detection_method": "topic_boundary",
      "content": "99\nObservable behavior vs. implementation details\n5.2\nObservable behavior vs. implementation details\nSection 5.1 showed what a mock is. The next step on the way to explaining the con-\nnection between mocks and test fragility is diving into what causes such fragility.\n As you might remember from chapter 4, test fragility corresponds to the second\nattribute of a good unit test: resistance to refactoring. (As a reminder, the four attri-\nbutes are protection against regressions, resistance to refactoring, fast feedback, and\nmaintainability.) The metric of resistance to refactoring is the most important\nbecause whether a unit test possesses this metric is mostly a binary choice. Thus, it’s\ngood to max out this metric to the extent that the test still remains in the realm of unit\ntesting and doesn’t transition to the category of end-to-end testing. The latter, despite\nbeing the best at resistance to refactoring, is generally much harder to maintain.\n In chapter 4, you also saw that the main reason tests deliver false positives (and thus\nfail at resistance to refactoring) is because they couple to the code’s implementation\ndetails. The only way to avoid such coupling is to verify the end result the code produces\n(its observable behavior) and distance tests from implementation details as much as pos-\nsible. In other words, tests must focus on the whats, not the hows. So, what exactly is an\nimplementation detail, and how is it different from an observable behavior?\n5.2.1\nObservable behavior is not the same as a public API\nAll production code can be categorized along two dimensions:\nPublic API vs. private API (where API means application programming interface)\nObservable behavior vs. implementation details \nThe categories in these dimensions don’t overlap. A method can’t belong to both a pub-\nlic and a private API; it’s either one or the other. Similarly, the code is either an internal\nimplementation detail or part of the system’s observable behavior, but not both.\n Most programming languages provide a simple mechanism to differentiate between\nthe code base’s public and private APIs. For example, in C#, you can mark any mem-\nber in a class with the private keyword, and that member will be hidden from the cli-\nent code, becoming part of the class’s private API. The same is true for classes: you can\neasily make them private by using the private or internal keyword.\n The distinction between observable behavior and internal implementation details\nis more nuanced. For a piece of code to be part of the system’s observable behavior, it\nhas to do one of the following things:\nExpose an operation that helps the client achieve one of its goals. An operation is\na method that performs a calculation or incurs a side effect or both.\nExpose a state that helps the client achieve one of its goals. State is the current\ncondition of the system.\nAny code that does neither of these two things is an implementation detail.\n Notice that whether the code is observable behavior depends on who its client is\nand what the goals of that client are. In order to be a part of observable behavior, the\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n100\nCHAPTER 5\nMocks and test fragility\ncode needs to have an immediate connection to at least one such goal. The word client\ncan refer to different things depending on where the code resides. The common\nexamples are client code from the same code base, an external application, or the\nuser interface.\n Ideally, the system’s public API surface should coincide with its observable behav-\nior, and all its implementation details should be hidden from the eyes of the clients.\nSuch a system has a well-designed API (figure 5.4).\nOften, though, the system’s public API extends beyond its observable behavior and\nstarts exposing implementation details. Such a system’s implementation details leak to\nits public API surface (figure 5.5). \n5.2.2\nLeaking implementation details: An example with an operation\nLet’s take a look at examples of code whose implementation details leak to the public\nAPI. Listing 5.5 shows a User class with a public API that consists of two members: a\nName property and a NormalizeName() method. The class also has an invariant: users’\nnames must not exceed 50 characters and should be truncated otherwise.\npublic class User\n{\npublic string Name { get; set; }\nListing 5.5\nUser class with leaking implementation details\nObservable behavior\nPublic API\nPrivate API\nImplementation detail\nFigure 5.4\nIn a well-designed API, the \nobservable behavior coincides with the public \nAPI, while all implementation details are \nhidden behind the private API.\nObservable behavior\nPublic API\nPrivate API\nLeaking implementation detail\nFigure 5.5\nA system leaks implementation \ndetails when its public API extends beyond \nthe observable behavior.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n101\nObservable behavior vs. implementation details\npublic string NormalizeName(string name)\n{\nstring result = (name ?? \"\").Trim();\nif (result.Length > 50)\nreturn result.Substring(0, 50);\nreturn result;\n}\n}\npublic class UserController\n{\npublic void RenameUser(int userId, string newName)\n{\nUser user = GetUserFromDatabase(userId);\nstring normalizedName = user.NormalizeName(newName);\nuser.Name = normalizedName;\nSaveUserToDatabase(user);\n}\n}\nUserController is client code. It uses the User class in its RenameUser method. The\ngoal of this method, as you have probably guessed, is to change a user’s name.\n So, why isn’t User’s API well-designed? Look at its members once again: the Name\nproperty and the NormalizeName method. Both of them are public. Therefore, in\norder for the class’s API to be well-designed, these members should be part of the\nobservable behavior. This, in turn, requires them to do one of the following two things\n(which I’m repeating here for convenience):\nExpose an operation that helps the client achieve one of its goals.\nExpose a state that helps the client achieve one of its goals.\nOnly the Name property meets this requirement. It exposes a setter, which is an opera-\ntion that allows UserController to achieve its goal of changing a user’s name. The\nNormalizeName method is also an operation, but it doesn’t have an immediate con-\nnection to the client’s goal. The only reason UserController calls this method is to\nsatisfy the invariant of User. NormalizeName is therefore an implementation detail that\nleaks to the class’s public API (figure 5.6).\n To fix the situation and make the class’s API well-designed, User needs to hide\nNormalizeName() and call it internally as part of the property’s setter without relying\non the client code to do so. Listing 5.6 shows this approach.\n \n \nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n102\nCHAPTER 5\nMocks and test fragility\n \npublic class User\n{\nprivate string _name;\npublic string Name\n{\nget => _name;\nset => _name = NormalizeName(value);\n}\nprivate string NormalizeName(string name)\n{\nstring result = (name ?? \"\").Trim();\nif (result.Length > 50)\nreturn result.Substring(0, 50);\nreturn result;\n}\n}\npublic class UserController\n{\npublic void RenameUser(int userId, string newName)\n{\nUser user = GetUserFromDatabase(userId);\nuser.Name = newName;\nSaveUserToDatabase(user);\n}\n}\nUser’s API in listing 5.6 is well-designed: only the observable behavior (the Name prop-\nerty) is made public, while the implementation details (the NormalizeName method)\nare hidden behind the private API (figure 5.7).\n \nListing 5.6\nA version of User with a well-designed API\nObservable behavior\nPublic API\nNormalize\nname\nName\nLeaking implementation detail\nFigure 5.6\nThe API of User is not well-\ndesigned: it exposes the NormalizeName \nmethod, which is not part of the observable \nbehavior.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n103\nObservable behavior vs. implementation details\nNOTE\nStrictly speaking, Name’s getter should also be made private, because\nit’s not used by UserController. In reality, though, you almost always want to\nread back changes you make. Therefore, in a real project, there will certainly be\nanother use case that requires seeing users’ current names via Name’s getter.\nThere’s a good rule of thumb that can help you determine whether a class leaks its\nimplementation details. If the number of operations the client has to invoke on the\nclass to achieve a single goal is greater than one, then that class is likely leaking imple-\nmentation details. Ideally, any individual goal should be achieved with a single operation. In\nlisting 5.5, for example, UserController has to use two operations from User:\nstring normalizedName = user.NormalizeName(newName);\nuser.Name = normalizedName;\nAfter the refactoring, the number of operations has been reduced to one:\nuser.Name = newName;\nIn my experience, this rule of thumb holds true for the vast majority of cases where\nbusiness logic is involved. There could very well be exceptions, though. Still, be sure\nto examine each situation where your code violates this rule for a potential leak of\nimplementation details. \n5.2.3\nWell-designed API and encapsulation\nMaintaining a well-designed API relates to the notion of encapsulation. As you might\nrecall from chapter 3, encapsulation is the act of protecting your code against inconsis-\ntencies, also known as invariant violations. An invariant is a condition that should be\nheld true at all times. The User class from the previous example had one such invari-\nant: no user could have a name that exceeded 50 characters.\n Exposing implementation details goes hand in hand with invariant violations—the\nformer often leads to the latter. Not only did the original version of User leak its\nimplementation details, but it also didn’t maintain proper encapsulation. It allowed\nthe client to bypass the invariant and assign a new name to a user without normalizing\nthat name first.\nObservable behavior\nPublic API\nNormalize\nname\nName\nPrivate API\nImplementation detail\nFigure 5.7\nUser with a well-designed API. \nOnly the observable behavior is public; the \nimplementation details are now private.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n104\nCHAPTER 5\nMocks and test fragility\n Encapsulation is crucial for code base maintainability in the long run. The reason\nwhy is complexity. Code complexity is one of the biggest challenges you’ll face in soft-\nware development. The more complex the code base becomes, the harder it is to work\nwith, which, in turn, results in slowing down development speed and increasing the\nnumber of bugs.\n Without encapsulation, you have no practical way to cope with ever-increasing\ncode complexity. When the code’s API doesn’t guide you through what is and what\nisn’t allowed to be done with that code, you have to keep a lot of information in mind\nto make sure you don’t introduce inconsistencies with new code changes. This brings\nan additional mental burden to the process of programming. Remove as much of that\nburden from yourself as possible. You cannot trust yourself to do the right thing all the\ntime—so, eliminate the very possibility of doing the wrong thing. The best way to do so is to\nmaintain proper encapsulation so that your code base doesn’t even provide an option\nfor you to do anything incorrectly. Encapsulation ultimately serves the same goal as\nunit testing: it enables sustainable growth of your software project.\n There’s a similar principle: tell-don’t-ask. It was coined by Martin Fowler (https://\nmartinfowler.com/bliki/TellDontAsk.html) and stands for bundling data with the\nfunctions that operate on that data. You can view this principle as a corollary to the\npractice of encapsulation. Code encapsulation is a goal, whereas bundling data and\nfunctions together, as well as hiding implementation details, are the means to achieve\nthat goal:\nHiding implementation details helps you remove the class’s internals from the eyes\nof its clients, so there’s less risk of corrupting those internals.\nBundling data and operations helps to make sure these operations don’t violate\nthe class’s invariants. \n5.2.4\nLeaking implementation details: An example with state\nThe example shown in listing 5.5 demonstrated an operation (the NormalizeName\nmethod) that was an implementation detail leaking to the public API. Let’s also look\nat an example with state. The following listing contains the MessageRenderer class you\nsaw in chapter 4. It uses a collection of sub-renderers to generate an HTML represen-\ntation of a message containing a header, a body, and a footer.\npublic class MessageRenderer : IRenderer\n{\npublic IReadOnlyList<IRenderer> SubRenderers { get; }\npublic MessageRenderer()\n{\nSubRenderers = new List<IRenderer>\n{\nnew HeaderRenderer(),\nnew BodyRenderer(),\nListing 5.7\nState as an implementation detail \nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n105\nObservable behavior vs. implementation details\nnew FooterRenderer()\n};\n}\npublic string Render(Message message)\n{\nreturn SubRenderers\n.Select(x => x.Render(message))\n.Aggregate(\"\", (str1, str2) => str1 + str2);\n}\n}\nThe sub-renderers collection is public. But is it part of observable behavior? Assuming\nthat the client’s goal is to render an HTML message, the answer is no. The only class\nmember such a client would need is the Render method itself. Thus SubRenderers is\nalso a leaking implementation detail.\n I bring up this example again for a reason. As you may remember, I used it to illus-\ntrate a brittle test. That test was brittle precisely because it was tied to this implementa-\ntion detail—it checked to see the collection’s composition. The brittleness was fixed by\nre-targeting the test at the Render method. The new version of the test verified the result-\ning message—the only output the client code cared about, the observable behavior.\n As you can see, there’s an intrinsic connection between good unit tests and a well-\ndesigned API. By making all implementation details private, you leave your tests no\nchoice other than to verify the code’s observable behavior, which automatically\nimproves their resistance to refactoring.\nTIP\nMaking the API well-designed automatically improves unit tests.\nAnother guideline flows from the definition of a well-designed API: you should expose\nthe absolute minimum number of operations and state. Only code that directly helps\nclients achieve their goals should be made public. Everything else is implementation\ndetails and thus must be hidden behind the private API.\n Note that there’s no such problem as leaking observable behavior, which would be\nsymmetric to the problem of leaking implementation details. While you can expose an\nimplementation detail (a method or a class that is not supposed to be used by the cli-\nent), you can’t hide an observable behavior. Such a method or class would no longer\nhave an immediate connection to the client goals, because the client wouldn’t be able\nto directly use it anymore. Thus, by definition, this code would cease to be part of\nobservable behavior. Table 5.1 sums it all up.\nTable 5.1\nThe relationship between the code’s publicity and purpose. Avoid making implementation\ndetails public.\nObservable behavior\nImplementation detail\nPublic\nGood\nBad\nPrivate\nN/A\nGood \nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n106\nCHAPTER 5\nMocks and test fragility\n5.3\nThe relationship between mocks and test fragility\nThe previous sections defined a mock and showed the difference between observable\nbehavior and an implementation detail. In this section, you will learn about hexago-\nnal architecture, the difference between internal and external communications, and\n(finally!) the relationship between mocks and test fragility.\n5.3.1\nDefining hexagonal architecture\nA typical application consists of two layers, domain and application services, as\nshown in figure 5.8. The domain layer resides in the middle of the diagram because\nit’s the central part of your application. It contains the business logic: the essential\nfunctionality your application is built for. The domain layer and its business logic\ndifferentiate this application from others and provide a competitive advantage for\nthe organization.\nThe application services layer sits on top of the domain layer and orchestrates com-\nmunication between that layer and the external world. For example, if your applica-\ntion is a RESTful API, all requests to this API hit the application services layer first.\nThis layer then coordinates the work between domain classes and out-of-process\ndependencies. Here’s an example of such coordination for the application service. It\ndoes the following:\nQueries the database and uses the data to materialize a domain class instance\nInvokes an operation on that instance\nSaves the results back to the database\nThe combination of the application services layer and the domain layer forms a hexa-\ngon, which itself represents your application. It can interact with other applications,\nwhich are represented with their own hexagons (see figure 5.9). These other applica-\ntions could be an SMTP service, a third-party system, a message bus, and so on. A set\nof interacting hexagons makes up a hexagonal architecture.\n \nDomain\n(business logic)\nApplication\nservices\nFigure 5.8\nA typical application consists of a \ndomain layer and an application services layer. \nThe domain layer contains the application’s \nbusiness logic; application services tie that \nlogic to business use cases.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n107\nThe relationship between mocks and test fragility\nThe term hexagonal architecture was introduced by Alistair Cockburn. Its purpose is to\nemphasize three important guidelines:\nThe separation of concerns between the domain and application services layers—Business\nlogic is the most important part of the application. Therefore, the domain layer\nshould be accountable only for that business logic and exempted from all other\nresponsibilities. Those responsibilities, such as communicating with external\napplications and retrieving data from the database, must be attributed to appli-\ncation services. Conversely, the application services shouldn’t contain any busi-\nness logic. Their responsibility is to adapt the domain layer by translating the\nincoming requests into operations on domain classes and then persisting the\nresults or returning them back to the caller. You can view the domain layer as a\ncollection of the application’s domain knowledge (how-to’s) and the application\nservices layer as a set of business use cases (what-to’s).\nCommunications inside your application—Hexagonal architecture prescribes a\none-way flow of dependencies: from the application services layer to the domain\nlayer. Classes inside the domain layer should only depend on each other; they\nshould not depend on classes from the application services layer. This guideline\nflows from the previous one. The separation of concerns between the applica-\ntion services layer and the domain layer means that the former knows about the\nlatter, but the opposite is not true. The domain layer should be fully isolated\nfrom the external world.\nCommunications between applications—External applications connect to your\napplication through a common interface maintained by the application services\nlayer. No one has a direct access to the domain layer. Each side in a hexagon\nrepresents a connection into or out of the application. Note that although a\nDomain\n(business logic)\nApplication\nservices\nThird-party\nsystem\nMessage\nbus\nSMTP\nservice\nFigure 5.9\nA hexagonal \narchitecture is a set of \ninteracting applications—\nhexagons.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n108\nCHAPTER 5\nMocks and test fragility\nhexagon has six sides, it doesn’t mean your application can only connect to six\nother applications. The number of connections is arbitrary. The point is that\nthere can be many such connections.\nEach layer of your application exhibits observable behavior and contains its own set of\nimplementation details. For example, observable behavior of the domain layer is the\nsum of this layer’s operations and state that helps the application service layer achieve\nat least one of its goals. The principles of a well-designed API have a fractal nature:\nthey apply equally to as much as a whole layer or as little as a single class.\n When you make each layer’s API well-designed (that is, hide its implementation\ndetails), your tests also start to have a fractal structure; they verify behavior that helps\nachieve the same goals but at different levels. A test covering an application service\nchecks to see how this service attains an overarching, coarse-grained goal posed by the\nexternal client. At the same time, a test working with a domain class verifies a subgoal\nthat is part of that greater goal (figure 5.10).\nYou might remember from previous chapters how I mentioned that you should be\nable to trace any test back to a particular business requirement. Each test should tell a\nstory that is meaningful to a domain expert, and if it doesn’t, that’s a strong indication\nthat the test couples to implementation details and therefore is brittle. I hope now you\ncan see why.\n Observable behavior flows inward from outer layers to the center. The overarching\ngoal posed by the external client gets translated into subgoals achieved by individual\nGoal\n(use case)\nSubgoal\nSubgoal\nTest 1\nTest 2\nTest 3\nExternal client\nApplication service\nDomain class 1\nDomain class 2\nFigure 5.10\nTests working with different layers have a fractal nature: they verify the \nsame behavior at different levels. A test of an application service checks to see how \nthe overall business use case is executed. A test working with a domain class verifies \nan intermediate subgoal on the way to use-case completion.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n109\nThe relationship between mocks and test fragility\ndomain classes. Each piece of observable behavior in the domain layer therefore pre-\nserves the connection to a particular business use case. You can trace this connection\nrecursively from the innermost (domain) layer outward to the application services\nlayer and then to the needs of the external client. This traceability follows from the\ndefinition of observable behavior. For a piece of code to be part of observable behav-\nior, it needs to help the client achieve one of its goals. For a domain class, the client is\nan application service; for the application service, it’s the external client itself.\n Tests that verify a code base with a well-designed API also have a connection to\nbusiness requirements because those tests tie to the observable behavior only. A good\nexample is the User and UserController classes from listing 5.6 (I’m repeating the\ncode here for convenience).\npublic class User\n{\nprivate string _name;\npublic string Name\n{\nget => _name;\nset => _name = NormalizeName(value);\n}\nprivate string NormalizeName(string name)\n{\n/* Trim name down to 50 characters */\n}\n}\npublic class UserController\n{\npublic void RenameUser(int userId, string newName)\n{\nUser user = GetUserFromDatabase(userId);\nuser.Name = newName;\nSaveUserToDatabase(user);\n}\n}\nUserController in this example is an application service. Assuming that the exter-\nnal client doesn’t have a specific goal of normalizing user names, and all names are\nnormalized solely due to restrictions from the application itself, the NormalizeName\nmethod in the User class can’t be traced to the client’s needs. Therefore, it’s an\nimplementation detail and should be made private (we already did that earlier in\nthis chapter). Moreover, tests shouldn’t check this method directly. They should ver-\nify it only as part of the class’s observable behavior—the Name property’s setter in\nthis example.\n This guideline of always tracing the code base’s public API to business require-\nments applies to the vast majority of domain classes and application services but less\nListing 5.8\nA domain class with an application service\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "page_number": 121
    },
    {
      "number": 15,
      "title": "Segment 15 (pages 132-139)",
      "start_page": 132,
      "end_page": 139,
      "detection_method": "topic_boundary",
      "content": "110\nCHAPTER 5\nMocks and test fragility\nso to utility and infrastructure code. The individual problems such code solves are\noften too low-level and fine-grained and can’t be traced to a specific business use case. \n5.3.2\nIntra-system vs. inter-system communications\nThere are two types of communications in a typical application: intra-system and inter-\nsystem. Intra-system communications are communications between classes inside your\napplication. Inter-system communications are when your application talks to other appli-\ncations (figure 5.11).\nNOTE\nIntra-system communications are implementation details; inter-system\ncommunications are not.\nIntra-system communications are implementation details because the collaborations\nyour domain classes go through in order to perform an operation are not part of their\nobservable behavior. These collaborations don’t have an immediate connection to the\nclient’s goal. Thus, coupling to such collaborations leads to fragile tests.\n Inter-system communications are a different matter. Unlike collaborations between\nclasses inside your application, the way your system talks to the external world forms\nthe observable behavior of that system as a whole. It’s part of the contract your appli-\ncation must hold at all times (figure 5.12).\n This attribute of inter-system communications stems from the way separate applica-\ntions evolve together. One of the main principles of such an evolution is maintaining\nbackward compatibility. Regardless of the refactorings you perform inside your sys-\ntem, the communication pattern it uses to talk to external applications should always\nstay in place, so that external applications can understand it. For example, messages\nyour application emits on a bus should preserve their structure, the calls issued to an\nSMTP service should have the same number and type of parameters, and so on.\nThird-party\nsystem\nSMTP service\nIntra-system\nInter-system\nInter-system\nFigure 5.11\nThere are two types \nof communications: intra-system \n(between classes inside the \napplication) and inter-system \n(between applications).\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n111\nThe relationship between mocks and test fragility\nThe use of mocks is beneficial when verifying the communication pattern between\nyour system and external applications. Conversely, using mocks to verify communica-\ntions between classes inside your system results in tests that couple to implementation\ndetails and therefore fall short of the resistance-to-refactoring metric.\n5.3.3\nIntra-system vs. inter-system communications: An example\nTo illustrate the difference between intra-system and inter-system communications, I’ll\nexpand on the example with the Customer and Store classes that I used in chapter 2\nand earlier in this chapter. Imagine the following business use case:\nA customer tries to purchase a product from a store.\nIf the amount of the product in the store is sufficient, then\n– The inventory is removed from the store.\n– An email receipt is sent to the customer.\n– A confirmation is returned.\nLet’s also assume that the application is an API with no user interface.\n In the following listing, the CustomerController class is an application service that\norchestrates the work between domain classes (Customer, Product, Store) and the\nexternal application (EmailGateway, which is a proxy to an SMTP service).\npublic class CustomerController\n{\npublic bool Purchase(int customerId, int productId, int quantity)\nListing 5.9\nConnecting the domain model with external applications\nThird-party\nsystem\nSMTP service\nImplementation detail\nObservable behavior (contract)\nObservable behavior (contract)\nFigure 5.12\nInter-system communications form the observable \nbehavior of your application as a whole. Intra-system communications \nare implementation details.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n112\nCHAPTER 5\nMocks and test fragility\n{\nCustomer customer = _customerRepository.GetById(customerId);\nProduct product = _productRepository.GetById(productId);\nbool isSuccess = customer.Purchase(\n_mainStore, product, quantity);\nif (isSuccess)\n{\n_emailGateway.SendReceipt(\ncustomer.Email, product.Name, quantity);\n}\nreturn isSuccess;\n}\n}\nValidation of input parameters is omitted for brevity. In the Purchase method, the\ncustomer checks to see if there’s enough inventory in the store and, if so, decreases\nthe product amount.\n The act of making a purchase is a business use case with both intra-system and\ninter-system communications. The inter-system communications are those between\nthe CustomerController application service and the two external systems: the third-\nparty application (which is also the client initiating the use case) and the email gate-\nway. The intra-system communication is between the Customer and the Store domain\nclasses (figure 5.13).\n In this example, the call to the SMTP service is a side effect that is visible to the\nexternal world and thus forms the observable behavior of the application as a whole.\nThird-party\nsystem\n(external\nclient)\nSMTP service\nSendReceipt()\nCustomer\nRemoveInventory()\nStore\nisSuccess\nFigure 5.13\nThe example in listing 5.9 represented using the hexagonal \narchitecture. The communications between the hexagons are inter-system \ncommunications. The communication inside the hexagon is intra-system.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n113\nThe relationship between mocks and test fragility\nIt also has a direct connection to the client’s goals. The client of the application is the\nthird-party system. This system’s goal is to make a purchase, and it expects the cus-\ntomer to receive a confirmation email as part of the successful outcome.\n The call to the SMTP service is a legitimate reason to do mocking. It doesn’t lead\nto test fragility because you want to make sure this type of communication stays in\nplace even after refactoring. The use of mocks helps you do exactly that.\n The next listing shows an example of a legitimate use of mocks.\n[Fact]\npublic void Successful_purchase()\n{\nvar mock = new Mock<IEmailGateway>();\nvar sut = new CustomerController(mock.Object);\nbool isSuccess = sut.Purchase(\ncustomerId: 1, productId: 2, quantity: 5);\nAssert.True(isSuccess);\nmock.Verify(\n  \nx => x.SendReceipt(\n  \n\"customer@email.com\", \"Shampoo\", 5),  \nTimes.Once);\n  \n}\nNote that the isSuccess flag is also observable by the external client and also needs\nverification. This flag doesn’t need mocking, though; a simple value comparison is\nenough.\n Let’s now look at a test that mocks the communication between Customer and\nStore.\n[Fact]\npublic void Purchase_succeeds_when_enough_inventory()\n{\nvar storeMock = new Mock<IStore>();\nstoreMock\n.Setup(x => x.HasEnoughInventory(Product.Shampoo, 5))\n.Returns(true);\nvar customer = new Customer();\nbool success = customer.Purchase(\nstoreMock.Object, Product.Shampoo, 5);\nAssert.True(success);\nstoreMock.Verify(\nx => x.RemoveInventory(Product.Shampoo, 5),\nTimes.Once);\n}\nListing 5.10\nMocking that doesn’t lead to fragile tests \nListing 5.11\nMocking that leads to fragile tests \nVerifies that the \nsystem sent a receipt \nabout the purchase\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n114\nCHAPTER 5\nMocks and test fragility\nUnlike the communication between CustomerController and the SMTP service, the\nRemoveInventory() method call from Customer to Store doesn’t cross the applica-\ntion boundary: both the caller and the recipient reside inside the application. Also,\nthis method is neither an operation nor a state that helps the client achieve its goals.\nThe client of these two domain classes is CustomerController with the goal of making\na purchase. The only two members that have an immediate connection to this goal are\ncustomer.Purchase() and store.GetInventory(). The Purchase() method initiates\nthe purchase, and GetInventory() shows the state of the system after the purchase is\ncompleted. The RemoveInventory() method call is an intermediate step on the way to\nthe client’s goal—an implementation detail. \n5.4\nThe classical vs. London schools of unit testing, \nrevisited\nAs a reminder from chapter 2 (table 2.1), table 5.2 sums up the differences between\nthe classical and London schools of unit testing.\nIn chapter 2, I mentioned that I prefer the classical school of unit testing over the\nLondon school. I hope now you can see why. The London school encourages the use\nof mocks for all but immutable dependencies and doesn’t differentiate between intra-\nsystem and inter-system communications. As a result, tests check communications\nbetween classes just as much as they check communications between your application\nand external systems.\n This indiscriminate use of mocks is why following the London school often results\nin tests that couple to implementation details and thus lack resistance to refactoring.\nAs you may remember from chapter 4, the metric of resistance to refactoring (unlike\nthe other three) is mostly a binary choice: a test either has resistance to refactoring or\nit doesn’t. Compromising on this metric renders the test nearly worthless.\n The classical school is much better at this issue because it advocates for substitut-\ning only dependencies that are shared between tests, which almost always translates\ninto out-of-process dependencies such as an SMTP service, a message bus, and so on.\nBut the classical school is not ideal in its treatment of inter-system communications,\neither. This school also encourages excessive use of mocks, albeit not as much as the\nLondon school.\nTable 5.2\nThe differences between the London and classical schools of unit testing\nIsolation of\nA unit is\nUses test doubles for\nLondon school\nUnits\nA class\nAll but immutable dependencies\nClassical school\nUnit tests\nA class or a set of classes\nShared dependencies\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n115\nThe classical vs. London schools of unit testing, revisited\n5.4.1\nNot all out-of-process dependencies should be mocked out\nBefore we discuss out-of-process dependencies and mocking, let me give you a quick\nrefresher on types of dependencies (refer to chapter 2 for more details):\nShared dependency—A dependency shared by tests (not production code)\nOut-of-process dependency—A dependency hosted by a process other than the pro-\ngram’s execution process (for example, a database, a message bus, or an SMTP\nservice)\nPrivate dependency—Any dependency that is not shared\nThe classical school recommends avoiding shared dependencies because they provide\nthe means for tests to interfere with each other’s execution context and thus prevent\nthose tests from running in parallel. The ability for tests to run in parallel, sequen-\ntially, and in any order is called test isolation.\n If a shared dependency is not out-of-process, then it’s easy to avoid reusing it in\ntests by providing a new instance of it on each test run. In cases where the shared\ndependency is out-of-process, testing becomes more complicated. You can’t instanti-\nate a new database or provision a new message bus before each test execution; that\nwould drastically slow down the test suite. The usual approach is to replace such\ndependencies with test doubles—mocks and stubs.\n Not all out-of-process dependencies should be mocked out, though. If an out-of-\nprocess dependency is only accessible through your application, then communications with such a\ndependency are not part of your system’s observable behavior. An out-of-process dependency\nthat can’t be observed externally, in effect, acts as part of your application (figure 5.14).\n Remember, the requirement to always preserve the communication pattern\nbetween your application and external systems stems from the necessity to maintain\nbackward compatibility. You have to maintain the way your application talks to external\nThird-party\nsystem\n(external\nclient)\nSMTP service\nObservable behavior (contract)\nApplication\ndatabase\n(accessible\nonly by the\napplication)\nImplementation details\nFigure 5.14\nCommunications with an out-of-process dependency that can’t be \nobserved externally are implementation details. They don’t have to stay in place \nafter refactoring and therefore shouldn’t be verified with mocks.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n116\nCHAPTER 5\nMocks and test fragility\nsystems. That’s because you can’t change those external systems simultaneously with\nyour application; they may follow a different deployment cycle, or you might simply\nnot have control over them.\n But when your application acts as a proxy to an external system, and no client can\naccess it directly, the backward-compatibility requirement vanishes. Now you can deploy\nyour application together with this external system, and it won’t affect the clients. The\ncommunication pattern with such a system becomes an implementation detail.\n A good example here is an application database: a database that is used only by\nyour application. No external system has access to this database. Therefore, you can\nmodify the communication pattern between your system and the application database\nin any way you like, as long as it doesn’t break existing functionality. Because that data-\nbase is completely hidden from the eyes of the clients, you can even replace it with an\nentirely different storage mechanism, and no one will notice.\n The use of mocks for out-of-process dependencies that you have a full control over\nalso leads to brittle tests. You don’t want your tests to turn red every time you split a\ntable in the database or modify the type of one of the parameters in a stored proce-\ndure. The database and your application must be treated as one system.\n This obviously poses an issue. How would you test the work with such a depen-\ndency without compromising the feedback speed, the third attribute of a good unit\ntest? You’ll see this subject covered in depth in the following two chapters. \n5.4.2\nUsing mocks to verify behavior\nMocks are often said to verify behavior. In the vast majority of cases, they don’t. The\nway each individual class interacts with neighboring classes in order to achieve some\ngoal has nothing to do with observable behavior; it’s an implementation detail.\n Verifying communications between classes is akin to trying to derive a person’s\nbehavior by measuring the signals that neurons in the brain pass among each other.\nSuch a level of detail is too granular. What matters is the behavior that can be traced\nback to the client goals. The client doesn’t care what neurons in your brain light up\nwhen they ask you to help. The only thing that matters is the help itself—provided by\nyou in a reliable and professional fashion, of course. Mocks have something to do with\nbehavior only when they verify interactions that cross the application boundary and\nonly when the side effects of those interactions are visible to the external world. \nSummary\nTest double is an overarching term that describes all kinds of non-production-\nready, fake dependencies in tests. There are five variations of test doubles—\ndummy, stub, spy, mock, and fake—that can be grouped in just two types: mocks\nand stubs. Spies are functionally the same as mocks; dummies and fakes serve\nthe same role as stubs.\nMocks help emulate and examine outcoming interactions: calls from the SUT to\nits dependencies that change the state of those dependencies. Stubs help\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n117\nSummary\nemulate incoming interactions: calls the SUT makes to its dependencies to get\ninput data.\nA mock (the tool) is a class from a mocking library that you can use to create a\nmock (the test double) or a stub.\nAsserting interactions with stubs leads to fragile tests. Such an interaction doesn’t\ncorrespond to the end result; it’s an intermediate step on the way to that result,\nan implementation detail.\nThe command query separation (CQS) principle states that every method\nshould be either a command or a query but not both. Test doubles that substi-\ntute commands are mocks. Test doubles that substitute queries are stubs.\nAll production code can be categorized along two dimensions: public API ver-\nsus private API, and observable behavior versus implementation details. Code\npublicity is controlled by access modifiers, such as private, public, and\ninternal keywords. Code is part of observable behavior when it meets one of\nthe following requirements (any other code is an implementation detail):\n– It exposes an operation that helps the client achieve one of its goals. An oper-\nation is a method that performs a calculation or incurs a side effect.\n– It exposes a state that helps the client achieve one of its goals. State is the cur-\nrent condition of the system.\nWell-designed code is code whose observable behavior coincides with the public\nAPI and whose implementation details are hidden behind the private API. A\ncode leaks implementation details when its public API extends beyond the\nobservable behavior.\nEncapsulation is the act of protecting your code against invariant violations.\nExposing implementation details often entails a breach in encapsulation\nbecause clients can use implementation details to bypass the code’s invariants.\nHexagonal architecture is a set of interacting applications represented as hexa-\ngons. Each hexagon consists of two layers: domain and application services.\nHexagonal architecture emphasizes three important aspects:\n– Separation of concerns between the domain and application services layers.\nThe domain layer should be responsible for the business logic, while the\napplication services should orchestrate the work between the domain layer\nand external applications.\n– A one-way flow of dependencies from the application services layer to the\ndomain layer. Classes inside the domain layer should only depend on each\nother; they should not depend on classes from the application services layer.\n– External applications connect to your application through a common inter-\nface maintained by the application services layer. No one has a direct access\nto the domain layer.\nEach layer in a hexagon exhibits observable behavior and contains its own set of\nimplementation details.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "page_number": 132
    },
    {
      "number": 16,
      "title": "Segment 16 (pages 140-147)",
      "start_page": 140,
      "end_page": 147,
      "detection_method": "topic_boundary",
      "content": "118\nCHAPTER 5\nMocks and test fragility\nThere are two types of communications in an application: intra-system and\ninter-system. Intra-system communications are communications between classes\ninside the application. Inter-system communication is when the application talks\nto external applications.\nIntra-system communications are implementation details. Inter-system commu-\nnications are part of observable behavior, with the exception of external systems\nthat are accessible only through your application. Interactions with such sys-\ntems are implementation details too, because the resulting side effects are not\nobserved externally.\nUsing mocks to assert intra-system communications leads to fragile tests. Mock-\ning is legitimate only when it’s used for inter-system communications—commu-\nnications that cross the application boundary—and only when the side effects\nof those communications are visible to the external world.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n119\nStyles of unit testing\nChapter 4 introduced the four attributes of a good unit test: protection against\nregressions, resistance to refactoring, fast feedback, and maintainability. These attri-\nbutes form a frame of reference that you can use to analyze specific tests and unit\ntesting approaches. We analyzed one such approach in chapter 5: the use of mocks.\n In this chapter, I apply the same frame of reference to the topic of unit testing\nstyles. There are three such styles: output-based, state-based, and communication-\nbased testing. Among the three, the output-based style produces tests of the highest\nquality, state-based testing is the second-best choice, and communication-based\ntesting should be used only occasionally.\n Unfortunately, you can’t use the output-based testing style everywhere. It’s only\napplicable to code written in a purely functional way. But don’t worry; there are\ntechniques that can help you transform more of your tests into the output-based\nstyle. For that, you’ll need to use functional programming principles to restructure\nthe underlying code toward a functional architecture.\nThis chapter covers\nComparing styles of unit testing\nThe relationship between functional and \nhexagonal architectures\nTransitioning to output-based testing\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n120\nCHAPTER 6\nStyles of unit testing\n Note that this chapter doesn’t provide a deep dive into the topic of functional pro-\ngramming. Still, by the end of this chapter, I hope you’ll have an intuitive understand-\ning of how functional programming relates to output-based testing. You’ll also learn\nhow to write more of your tests using the output-based style, as well as the limitations\nof functional programming and functional architecture.\n6.1\nThe three styles of unit testing\nAs I mentioned in the chapter introduction, there are three styles of unit testing:\nOutput-based testing \nState-based testing \nCommunication-based testing\nYou can employ one, two, or even all three styles together in a single test. This sec-\ntion lays the foundation for the whole chapter by defining (with examples) those\nthree styles of unit testing. You’ll see how they score against each other in the sec-\ntion after that.\n6.1.1\nDefining the output-based style\nThe first style of unit testing is the output-based style, where you feed an input to the sys-\ntem under test (SUT) and check the output it produces (figure 6.1). This style of unit\ntesting is only applicable to code that doesn’t change a global or internal state, so the\nonly component to verify is its return value.\nThe following listing shows an example of such code and a test covering it. The Price-\nEngine class accepts an array of products and calculates a discount.\npublic class PriceEngine\n{\npublic decimal CalculateDiscount(params Product[] products)\nListing 6.1\nOutput-based testing\nOutput\nProduction code\nInput\nOutput\nveriﬁcation\nFigure 6.1\nIn output-based testing, tests verify the output the system \ngenerates. This style of testing assumes there are no side effects and the only \nresult of the SUT’s work is the value it returns to the caller.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n121\nThe three styles of unit testing\n{\ndecimal discount = products.Length * 0.01m;\nreturn Math.Min(discount, 0.2m);\n}\n}\n[Fact]\npublic void Discount_of_two_products()\n{\nvar product1 = new Product(\"Hand wash\");\nvar product2 = new Product(\"Shampoo\");\nvar sut = new PriceEngine();\ndecimal discount = sut.CalculateDiscount(product1, product2);\nAssert.Equal(0.02m, discount);\n}\nPriceEngine multiplies the number of products by 1% and caps the result at 20%.\nThere’s nothing else to this class. It doesn’t add the products to any internal collec-\ntion, nor does it persist them in a database. The only outcome of the Calculate-\nDiscount() method is the discount it returns: the output value (figure 6.2).\nThe output-based style of unit testing is also known as functional. This name takes root\nin functional programming, a method of programming that emphasizes a preference for\nside-effect-free code. We’ll talk more about functional programming and functional\narchitecture later in this chapter. \n6.1.2\nDefining the state-based style\nThe state-based style is about verifying the state of the system after an operation is com-\nplete (figure 6.3). The term state in this style of testing can refer to the state of the\nSUT itself, of one of its collaborators, or of an out-of-process dependency, such as\nthe database or the filesystem.\nOutput\nveriﬁcation\nOutput\nPriceEngine\nInput\nProduct (“Hand wash”)\nProduct (“Shampoo”)\n2% discount\nFigure 6.2\nPriceEngine represented using input-output notation. Its \nCalculateDiscount() method accepts an array of products and \ncalculates a discount.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n122\nCHAPTER 6\nStyles of unit testing\nHere’s an example of state-based testing. The Order class allows the client to add a\nnew product.\npublic class Order\n{\nprivate readonly List<Product> _products = new List<Product>();\npublic IReadOnlyList<Product> Products => _products.ToList();\npublic void AddProduct(Product product)\n{\n_products.Add(product);\n}\n}\n[Fact]\npublic void Adding_a_product_to_an_order()\n{\nvar product = new Product(\"Hand wash\");\nvar sut = new Order();\nsut.AddProduct(product);\nAssert.Equal(1, sut.Products.Count);\nAssert.Equal(product, sut.Products[0]);\n}\nThe test verifies the Products collection after the addition is completed. Unlike\nthe example of output-based testing in listing 6.1, the outcome of AddProduct() is the\nchange made to the order’s state. \n6.1.3\nDefining the communication-based style\nFinally, the third style of unit testing is communication-based testing. This style uses\nmocks to verify communications between the system under test and its collaborators\n(figure 6.4).\nListing 6.2\nState-based testing\nState\nveriﬁcation\nState\nveriﬁcation\nProduction code\nInput\nFigure 6.3\nIn state-based testing, tests verify the final state of the \nsystem after an operation is complete. The dashed circles represent that \nfinal state.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n123\nComparing the three styles of unit testing\nThe following listing shows an example of communication-based testing.\n[Fact]\npublic void Sending_a_greetings_email()\n{\nvar emailGatewayMock = new Mock<IEmailGateway>();\nvar sut = new Controller(emailGatewayMock.Object);\nsut.GreetUser(\"user@email.com\");\nemailGatewayMock.Verify(\nx => x.SendGreetingsEmail(\"user@email.com\"),\nTimes.Once);\n}\n6.2\nComparing the three styles of unit testing\nThere’s nothing new about output-based, state-based, and communication-based\nstyles of unit testing. In fact, you already saw all of these styles previously in this book.\nWhat’s interesting is comparing them to each other using the four attributes of a good\nunit test. Here are those attributes again (refer to chapter 4 for more details):\nProtection against regressions\nResistance to refactoring\nFast feedback\nMaintainability\nIn our comparison, let’s look at each of the four separately.\nListing 6.3\nCommunication-based testing\nStyles and schools of unit testing\nThe classical school of unit testing prefers the state-based style over the communication-\nbased one. The London school makes the opposite choice. Both schools use output-\nbased testing. \nCollaboration\nveriﬁcation\nMocks\nProduction code\nInput\nFigure 6.4\nIn communication-based \ntesting, tests substitute the SUT’s \ncollaborators with mocks and verify \nthat the SUT calls those \ncollaborators correctly.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n124\nCHAPTER 6\nStyles of unit testing\n6.2.1\nComparing the styles using the metrics of protection against \nregressions and feedback speed\nLet’s first compare the three styles in terms of the protection against regressions\nand feedback speed attributes, as these attributes are the most straightforward in this\nparticular comparison. The metric of protection against regressions doesn’t depend\non a particular style of testing. This metric is a product of the following three\ncharacteristics:\nThe amount of code that is executed during the test\nThe complexity of that code\nIts domain significance\nGenerally, you can write a test that exercises as much or as little code as you like; no\nparticular style provides a benefit in this area. The same is true for the code’s com-\nplexity and domain significance. The only exception is the communication-based\nstyle: overusing it can result in shallow tests that verify only a thin slice of code and\nmock out everything else. Such shallowness is not a definitive feature of communication-\nbased testing, though, but rather is an extreme case of abusing this technique.\n There’s little correlation between the styles of testing and the test’s feedback speed.\nAs long as your tests don’t touch out-of-process dependencies and thus stay in the\nrealm of unit testing, all styles produce tests of roughly equal speed of execution.\nCommunication-based testing can be slightly worse because mocks tend to introduce\nadditional latency at runtime. But the difference is negligible, unless you have tens of\nthousands of such tests. \n6.2.2\nComparing the styles using the metric of resistance \nto refactoring\nWhen it comes to the metric of resistance to refactoring, the situation is different.\nResistance to refactoring is the measure of how many false positives (false alarms) tests gen-\nerate during refactorings. False positives, in turn, are a result of tests coupling to\ncode’s implementation details as opposed to observable behavior.\n Output-based testing provides the best protection against false positives because\nthe resulting tests couple only to the method under test. The only way for such tests to\ncouple to implementation details is when the method under test is itself an implemen-\ntation detail.\n State-based testing is usually more prone to false positives. In addition to the\nmethod under test, such tests also work with the class’s state. Probabilistically speak-\ning, the greater the coupling between the test and the production code, the greater\nthe chance for this test to tie to a leaking implementation detail. State-based tests tie\nto a larger API surface, and hence the chances of coupling them to implementation\ndetails are also higher.\n Communication-based testing is the most vulnerable to false alarms. As you may\nremember from chapter 5, the vast majority of tests that check interactions with test\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n125\nComparing the three styles of unit testing\ndoubles end up being brittle. This is always the case for interactions with stubs—you\nshould never check such interactions. Mocks are fine only when they verify interac-\ntions that cross the application boundary and only when the side effects of those\ninteractions are visible to the external world. As you can see, using communication-\nbased testing requires extra prudence in order to maintain proper resistance to\nrefactoring.\n But just like shallowness, brittleness is not a definitive feature of the communication-\nbased style, either. You can reduce the number of false positives to a minimum by\nmaintaining proper encapsulation and coupling tests to observable behavior only.\nAdmittedly, though, the amount of due diligence varies depending on the style of\nunit testing. \n6.2.3\nComparing the styles using the metric of maintainability\nFinally, the maintainability metric is highly correlated with the styles of unit testing;\nbut, unlike with resistance to refactoring, there’s not much you can do to mitigate\nthat. Maintainability evaluates the unit tests’ maintenance costs and is defined by the\nfollowing two characteristics:\nHow hard it is to understand the test, which is a function of the test’s size\nHow hard it is to run the test, which is a function of how many out-of-process\ndependencies the test works with directly\nLarger tests are less maintainable because they are harder to grasp or change when\nneeded. Similarly, a test that directly works with one or several out-of-process depen-\ndencies (such as the database) is less maintainable because you need to spend time\nkeeping those out-of-process dependencies operational: rebooting the database\nserver, resolving network connectivity issues, and so on.\nMAINTAINABILITY OF OUTPUT-BASED TESTS\nCompared with the other two types of testing, output-based testing is the most main-\ntainable. The resulting tests are almost always short and concise and thus are easier to\nmaintain. This benefit of the output-based style stems from the fact that this style boils\ndown to only two things: supplying an input to a method and verifying its output,\nwhich you can often do with just a couple lines of code.\n Because the underlying code in output-based testing must not change the global\nor internal state, these tests don’t deal with out-of-process dependencies. Hence,\noutput-based tests are best in terms of both maintainability characteristics. \nMAINTAINABILITY OF STATE-BASED TESTS\nState-based tests are normally less maintainable than output-based ones. This is\nbecause state verification often takes up more space than output verification. Here’s\nanother example of state-based testing.\n \n \nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "page_number": 140
    },
    {
      "number": 17,
      "title": "Segment 17 (pages 148-156)",
      "start_page": 148,
      "end_page": 156,
      "detection_method": "topic_boundary",
      "content": "126\nCHAPTER 6\nStyles of unit testing\n[Fact]\npublic void Adding_a_comment_to_an_article()\n{\nvar sut = new Article();\nvar text = \"Comment text\";\nvar author = \"John Doe\";\nvar now = new DateTime(2019, 4, 1);\nsut.AddComment(text, author, now);\nAssert.Equal(1, sut.Comments.Count);\n    \nAssert.Equal(text, sut.Comments[0].Text);\n    \nAssert.Equal(author, sut.Comments[0].Author);     \nAssert.Equal(now, sut.Comments[0].DateCreated);   \n}\nThis test adds a comment to an article and then checks to see if the comment\nappears in the article’s list of comments. Although this test is simplified and con-\ntains just a single comment, its assertion part already spans four lines. State-based\ntests often need to verify much more data than that and, therefore, can grow in size\nsignificantly.\n You can mitigate this issue by introducing helper methods that hide most of the\ncode and thus shorten the test (see listing 6.5), but these methods require significant\neffort to write and maintain. This effort is justified only when those methods are going\nto be reused across multiple tests, which is rarely the case. I’ll explain more about\nhelper methods in part 3 of this book.\n[Fact]\npublic void Adding_a_comment_to_an_article()\n{\nvar sut = new Article();\nvar text = \"Comment text\";\nvar author = \"John Doe\";\nvar now = new DateTime(2019, 4, 1);\nsut.AddComment(text, author, now);\nsut.ShouldContainNumberOfComments(1)    \n.WithComment(text, author, now);    \n}\nAnother way to shorten a state-based test is to define equality members in the class\nthat is being asserted. In listing 6.6, that’s the Comment class. You could turn it into a\nvalue object (a class whose instances are compared by value and not by reference), as\nshown next; this would also simplify the test, especially if you combined it with an\nassertion library like Fluent Assertions.\nListing 6.4\nState verification that takes up a lot of space\nListing 6.5\nUsing helper methods in assertions\nVerifies the state \nof the article\nHelper \nmethods\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n127\nComparing the three styles of unit testing\n[Fact]\npublic void Adding_a_comment_to_an_article()\n{\nvar sut = new Article();\nvar comment = new Comment(\n\"Comment text\",\n\"John Doe\",\nnew DateTime(2019, 4, 1));\nsut.AddComment(comment.Text, comment.Author, comment.DateCreated);\nsut.Comments.Should().BeEquivalentTo(comment);\n}\nThis test uses the fact that comments can be compared as whole values, without the\nneed to assert individual properties in them. It also uses the BeEquivalentTo method\nfrom Fluent Assertions, which can compare entire collections, thereby removing the\nneed to check the collection size.\n This is a powerful technique, but it works only when the class is inherently a value\nand can be converted into a value object. Otherwise, it leads to code pollution (pollut-\ning production code base with code whose sole purpose is to enable or, as in this case,\nsimplify unit testing). We’ll discuss code pollution along with other unit testing anti-\npatterns in chapter 11.\n As you can see, these two techniques—using helper methods and converting\nclasses into value objects—are applicable only occasionally. And even when these tech-\nniques are applicable, state-based tests still take up more space than output-based tests\nand thus remain less maintainable. \nMAINTAINABILITY OF COMMUNICATION-BASED TESTS\nCommunication-based tests score worse than output-based and state-based tests on\nthe maintainability metric. Communication-based testing requires setting up test dou-\nbles and interaction assertions, and that takes up a lot of space. Tests become even\nlarger and less maintainable when you have mock chains (mocks or stubs returning\nother mocks, which also return mocks, and so on, several layers deep). \n6.2.4\nComparing the styles: The results\nLet’s now compare the styles of unit testing using the attributes of a good unit test.\nTable 6.1 sums up the comparison results. As discussed in section 6.2.1, all three styles\nscore equally with the metrics of protection against regressions and feedback speed;\nhence, I’m omitting these metrics from the comparison.\n Output-based testing shows the best results. This style produces tests that rarely\ncouple to implementation details and thus don’t require much due diligence to main-\ntain proper resistance to refactoring. Such tests are also the most maintainable due to\ntheir conciseness and lack of out-of-process dependencies.\nListing 6.6\nComment compared by value\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n128\nCHAPTER 6\nStyles of unit testing\nState-based and communication-based tests are worse on both metrics. These are\nmore likely to couple to a leaking implementation detail, and they also incur higher\nmaintenance costs due to being larger in size.\n Always prefer output-based testing over everything else. Unfortunately, it’s easier\nsaid than done. This style of unit testing is only applicable to code that is written in a\nfunctional way, which is rarely the case for most object-oriented programming lan-\nguages. Still, there are techniques you can use to transition more of your tests toward\nthe output-based style.\n The rest of this chapter shows how to transition from state-based and collaboration-\nbased testing to output-based testing. The transition requires you to make your code\nmore purely functional, which, in turn, enables the use of output-based tests instead\nof state- or communication-based ones. \n6.3\nUnderstanding functional architecture\nSome groundwork is needed before I can show how to make the transition. In this sec-\ntion, you’ll see what functional programming and functional architecture are and\nhow the latter relates to the hexagonal architecture. Section 6.4 illustrates the transi-\ntion using an example.\n Note that this isn’t a deep dive into the topic of functional programming, but\nrather an explanation of the basic principles behind it. These basic principles should\nbe enough to understand the connection between functional programming and out-\nput-based testing. For a deeper look at functional programming, see Scott Wlaschin’s\nwebsite and books at https://fsharpforfunandprofit.com/books.\n6.3.1\nWhat is functional programming?\nAs I mentioned in section 6.1.1, the output-based unit testing style is also known as\nfunctional. That’s because it requires the underlying production code to be written in\na purely functional way, using functional programming. So, what is functional pro-\ngramming?\n Functional programming is programming with mathematical functions. A mathemati-\ncal function (also known as pure function) is a function (or method) that doesn’t have\nany hidden inputs or outputs. All inputs and outputs of a mathematical function must\nbe explicitly expressed in its method signature, which consists of the method’s name,\narguments, and return type. A mathematical function produces the same output for a\ngiven input regardless of how many times it is called.\nTable 6.1\nThe three styles of unit testing: The comparisons\nOutput-based\nState-based\nCommunication-based\nDue diligence to maintain \nresistance to refactoring\nLow\nMedium\nMedium\nMaintainability costs\nLow\nMedium\nHigh\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n129\nUnderstanding functional architecture\n Let’s take the CalculateDiscount() method from listing 6.1 as an example (I’m\ncopying it here for convenience):\npublic decimal CalculateDiscount(Product[] products)\n{\ndecimal discount = products.Length * 0.01m;\nreturn Math.Min(discount, 0.2m);\n}\nThis method has one input (a Product array) and one output (the decimal dis-\ncount), both of which are explicitly expressed in the method’s signature. There are\nno hidden inputs or outputs. This makes CalculateDiscount() a mathematical func-\ntion (figure 6.5).\nMethods with no hidden inputs and outputs are called mathematical functions\nbecause such methods adhere to the definition of a function in mathematics.\nDEFINITION\nIn mathematics, a function is a relationship between two sets that\nfor each element in the first set, finds exactly one element in the second set.\nFigure 6.6 shows how for each input number x, function f(x) = x + 1 finds a corre-\nsponding number y. Figure 6.7 displays the CalculateDiscount() method using the\nsame notation as in figure 6.6.\npublic         CalculateDiscount\ndecimal\n(Product[] products)\nMethod signature\nOutput\nName\nInput\nFigure 6.5\nCalculateDiscount() has one input (a Product array) and \none output (the decimal discount). Both the input and the output are explicitly \nexpressed in the method’s signature, which makes CalculateDiscount() \na mathematical function.\nY\n1\n2\n3\n4\n2\n3\n4\n5\nf(x) = x + 1\nX\nFigure 6.6\nA typical example of a function in \nmathematics is f(x) = x + 1. For each input \nnumber x in set X, the function finds a \ncorresponding number y in set Y.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n130\nCHAPTER 6\nStyles of unit testing\nExplicit inputs and outputs make mathematical functions extremely testable because\nthe resulting tests are short, simple, and easy to understand and maintain. Mathe-\nmatical functions are the only type of methods where you can apply output-based\ntesting, which has the best maintainability and the lowest chance of producing a\nfalse positive.\n On the other hand, hidden inputs and outputs make the code less testable (and\nless readable, too). Types of such hidden inputs and outputs include the following:\nSide effects—A side effect is an output that isn’t expressed in the method signature\nand, therefore, is hidden. An operation creates a side effect when it mutates the\nstate of a class instance, updates a file on the disk, and so on.\nExceptions—When a method throws an exception, it creates a path in the pro-\ngram flow that bypasses the contract established by the method’s signature. The\nthrown exception can be caught anywhere in the call stack, thus introducing an\nadditional output that the method signature doesn’t convey.\nA reference to an internal or external state—For example, a method can get the cur-\nrent date and time using a static property such as DateTime.Now. It can query\ndata from the database, or it can refer to a private mutable field. These are all\ninputs to the execution flow that aren’t present in the method signature and,\ntherefore, are hidden.\nA good rule of thumb when determining whether a method is a mathematical func-\ntion is to see if you can replace a call to that method with its return value without\nchanging the program’s behavior. The ability to replace a method call with the\ncorresponding value is known as referential transparency. Look at the following method,\nfor example:\nArrays of products\nProduct(“Soap”)\nProduct(“Hand wash”)\nProduct(“Shampoo”)\nProduct(“Soap”)\nProduct(“Sea salt”)\nDiscounts\n0.02\n0.01\nCalculateDiscount()\nFigure 6.7\nThe CalculateDiscount() method represented using the same \nnotation as the function f(x) = x + 1. For each input array of products, the \nmethod finds a corresponding discount as an output.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n131\nUnderstanding functional architecture\npublic int Increment(int x)\n{\nreturn x + 1;\n}\nThis method is a mathematical function. These two statements are equivalent to\neach other:\nint y = Increment(4);\nint y = 5;\nOn the other hand, the following method is not a mathematical function. You can’t\nreplace it with the return value because that return value doesn’t represent all of the\nmethod’s outputs. In this example, the hidden output is the change to field x (a side\neffect):\nint x = 0;\npublic int Increment()\n{\nx++;\nreturn x;\n}\nSide effects are the most prevalent type of hidden outputs. The following listing shows\nan AddComment method that looks like a mathematical function on the surface but\nactually isn’t one. Figure 6.8 shows the method graphically.\npublic Comment AddComment(string text)\n{\nvar comment = new Comment(text);\n_comments.Add(comment);\n   \nreturn comment;\n}\nListing 6.7\nModification of an internal state\nSide effect \nText\nComment\nSide effect\nMethod\nsignature\nHidden\npart\nf\nFigure 6.8\nMethod AddComment (shown as f) \nhas a text input and a Comment output, which \nare both expressed in the method signature. The \nside effect is an additional hidden output.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n132\nCHAPTER 6\nStyles of unit testing\n6.3.2\nWhat is functional architecture?\nYou can’t create an application that doesn’t incur any side effects whatsoever, of\ncourse. Such an application would be impractical. After all, side effects are what you\ncreate all applications for: updating the user’s information, adding a new order line to\nthe shopping cart, and so on.\n The goal of functional programming is not to eliminate side effects altogether but\nrather to introduce a separation between code that handles business logic and code\nthat incurs side effects. These two responsibilities are complex enough on their own;\nmixing them together multiplies the complexity and hinders code maintainability in\nthe long run. This is where functional architecture comes into play. It separates busi-\nness logic from side effects by pushing those side effects to the edges of a business operation.\nDEFINITION\nFunctional architecture maximizes the amount of code written in a\npurely functional (immutable) way, while minimizing code that deals with\nside effects. Immutable means unchangeable: once an object is created, its\nstate can’t be modified. This is in contrast to a mutable object (changeable\nobject), which can be modified after it is created.\nThe separation between business logic and side effects is done by segregating two\ntypes of code:\nCode that makes a decision—This code doesn’t require side effects and thus can\nbe written using mathematical functions.\nCode that acts upon that decision—This code converts all the decisions made by\nthe mathematical functions into visible bits, such as changes in the database or\nmessages sent to a bus.\nThe code that makes decisions is often referred to as a functional core (also known as an\nimmutable core). The code that acts upon those decisions is a mutable shell (figure 6.9).\nInput\nDecisions\nFunctional core\nMutable shell\nFigure 6.9\nIn functional architecture, \nthe functional core is implemented using \nmathematical functions and makes all \ndecisions in the application. The mutable \nshell provides the functional core with \ninput data and interprets its decisions by \napplying side effects to out-of-process \ndependencies such as a database.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n133\nUnderstanding functional architecture\nThe functional core and the mutable shell cooperate in the following way:\nThe mutable shell gathers all the inputs.\nThe functional core generates decisions.\nThe shell converts the decisions into side effects.\nTo maintain a proper separation between these two layers, you need to make sure the\nclasses representing the decisions contain enough information for the mutable shell\nto act upon them without additional decision-making. In other words, the mutable\nshell should be as dumb as possible. The goal is to cover the functional core exten-\nsively with output-based tests and leave the mutable shell to a much smaller number of\nintegration tests.\n6.3.3\nComparing functional and hexagonal architectures\nThere are a lot of similarities between functional and hexagonal architectures. Both\nof them are built around the idea of separation of concerns. The details of that sepa-\nration vary, though.\n As you may remember from chapter 5, the hexagonal architecture differentiates\nthe domain layer and the application services layer (figure 6.10). The domain layer is\naccountable for business logic while the application services layer, for communication with\nEncapsulation and immutability\nLike encapsulation, functional architecture (in general) and immutability (in particular)\nserve the same goal as unit testing: enabling sustainable growth of your software\nproject. In fact, there’s a deep connection between the concepts of encapsulation\nand immutability.\nAs you may remember from chapter 5, encapsulation is the act of protecting your\ncode against inconsistencies. Encapsulation safeguards the class’s internals from\ncorruption by\nReducing the API surface area that allows for data modification\nPutting the remaining APIs under scrutiny\nImmutability tackles this issue of preserving invariants from another angle. With\nimmutable classes, you don’t need to worry about state corruption because it’s impos-\nsible to corrupt something that cannot be changed in the first place. As a conse-\nquence, there’s no need for encapsulation in functional programming. You only need\nto validate the class’s state once, when you create an instance of it. After that, you\ncan freely pass this instance around. When all your data is immutable, the whole set\nof issues related to the lack of encapsulation simply vanishes.\nThere’s a great quote from Michael Feathers in that regard:\nObject-oriented programming makes code understandable by encapsulating mov-\ning parts. Functional programming makes code understandable by minimizing\nmoving parts.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n134\nCHAPTER 6\nStyles of unit testing\nexternal applications such as a database or an SMTP service. This is very similar to func-\ntional architecture, where you introduce the separation of decisions and actions.\n Another similarity is the one-way flow of dependencies. In the hexagonal architec-\nture, classes inside the domain layer should only depend on each other; they should\nnot depend on classes from the application services layer. Likewise, the immutable\ncore in functional architecture doesn’t depend on the mutable shell. It’s self-sufficient\nand can work in isolation from the outer layers. This is what makes functional archi-\ntecture so testable: you can strip the immutable core from the mutable shell entirely\nand simulate the inputs that the shell provides using simple values.\n The difference between the two is in their treatment of side effects. Functional\narchitecture pushes all side effects out of the immutable core to the edges of a busi-\nness operation. These edges are handled by the mutable shell. On the other hand, the\nhexagonal architecture is fine with side effects made by the domain layer, as long as\nthey are limited to that domain layer only. All modifications in hexagonal architecture\nshould be contained within the domain layer and not cross that layer’s boundary. For\nexample, a domain class instance can’t persist something to the database directly, but\nit can change its own state. An application service will then pick up this change and\napply it to the database.\nNOTE\nFunctional architecture is a subset of the hexagonal architecture. You\ncan view functional architecture as the hexagonal architecture taken to an\nextreme. \nDomain\n(business logic)\nApplication\nservices\nThird-party\nsystem\nMessage\nbus\nSMTP\nservice\nFigure 6.10\nHexagonal architecture is a set of interacting \napplications—hexagons. Your application consists of a domain \nlayer and an application services layer, which correspond to a \nfunctional core and a mutable shell in functional architecture.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "page_number": 148
    },
    {
      "number": 18,
      "title": "Segment 18 (pages 157-166)",
      "start_page": 157,
      "end_page": 166,
      "detection_method": "topic_boundary",
      "content": "135\nTransitioning to functional architecture and output-based testing\n6.4\nTransitioning to functional architecture and output-\nbased testing\nIn this section, we’ll take a sample application and refactor it toward functional archi-\ntecture. You’ll see two refactoring stages:\nMoving from using an out-of-process dependency to using mocks\nMoving from using mocks to using functional architecture\nThe transition affects test code, too! We’ll refactor state-based and communication-\nbased tests to the output-based style of unit testing. Before starting the refactoring,\nlet’s review the sample project and tests covering it.\n6.4.1\nIntroducing an audit system\nThe sample project is an audit system that keeps track of all visitors in an organization.\nIt uses flat text files as underlying storage with the structure shown in figure 6.11. The\nsystem appends the visitor’s name and the time of their visit to the end of the most\nrecent file. When the maximum number of entries per file is reached, a new file with\nan incremented index is created.\nThe following listing shows the initial version of the system.\npublic class AuditManager\n{\nprivate readonly int _maxEntriesPerFile;\nprivate readonly string _directoryName;\npublic AuditManager(int maxEntriesPerFile, string directoryName)\n{\n_maxEntriesPerFile = maxEntriesPerFile;\n_directoryName = directoryName;\n}\nListing 6.8\nInitial implementation of the audit system\nJane;\nJack;\nPeter; 2019-04-06T16:30:00\n2019-04-06T16:40:00\n2019-04-06T17:00:00\nMary;\n2019-04-06T17:30:00\nNew Person; Time of visit\naudit_01.txt\naudit_02.txt\nFigure 6.11\nThe audit system stores information \nabout visitors in text files with a specific format. \nWhen the maximum number of entries per file is \nreached, the system creates a new file.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n136\nCHAPTER 6\nStyles of unit testing\npublic void AddRecord(string visitorName, DateTime timeOfVisit)\n{\nstring[] filePaths = Directory.GetFiles(_directoryName);\n(int index, string path)[] sorted = SortByIndex(filePaths);\nstring newRecord = visitorName + ';' + timeOfVisit;\nif (sorted.Length == 0)\n{\nstring newFile = Path.Combine(_directoryName, \"audit_1.txt\");\nFile.WriteAllText(newFile, newRecord);\nreturn;\n}\n(int currentFileIndex, string currentFilePath) = sorted.Last();\nList<string> lines = File.ReadAllLines(currentFilePath).ToList();\nif (lines.Count < _maxEntriesPerFile)\n{\nlines.Add(newRecord);\nstring newContent = string.Join(\"\\r\\n\", lines);\nFile.WriteAllText(currentFilePath, newContent);\n}\nelse\n{\nint newIndex = currentFileIndex + 1;\nstring newName = $\"audit_{newIndex}.txt\";\nstring newFile = Path.Combine(_directoryName, newName);\nFile.WriteAllText(newFile, newRecord);\n}\n}\n}\nThe code might look a bit large, but it’s quite simple. AuditManager is the main class\nin the application. Its constructor accepts the maximum number of entries per file\nand the working directory as configuration parameters. The only public method in\nthe class is AddRecord, which does all the work of the audit system:\nRetrieves a full list of files from the working directory\nSorts them by index (all filenames follow the same pattern: audit_{index}.txt\n[for example, audit_1.txt])\nIf there are no audit files yet, creates a first one with a single record\nIf there are audit files, gets the most recent one and either appends the new\nrecord to it or creates a new file, depending on whether the number of entries\nin that file has reached the limit\nThe AuditManager class is hard to test as-is, because it’s tightly coupled to the file-\nsystem. Before the test, you’d need to put files in the right place, and after the test\nfinishes, you’d read those files, check their contents, and clear them out (figure 6.12).\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n137\nTransitioning to functional architecture and output-based testing\nYou won’t be able to parallelize such tests—at least, not without additional effort\nthat would significantly increase maintenance costs. The bottleneck is the filesys-\ntem: it’s a shared dependency through which tests can interfere with each other’s\nexecution flow.\n The filesystem also makes the tests slow. Maintainability suffers, too, because you\nhave to make sure the working directory exists and is accessible to tests—both on your\nlocal machine and on the build server. Table 6.2 sums up the scoring.\nBy the way, tests working directly with the filesystem don’t fit the definition of a unit\ntest. They don’t comply with the second and the third attributes of a unit test, thereby\nfalling into the category of integration tests (see chapter 2 for more details):\nA unit test verifies a single unit of behavior,\nDoes it quickly,\nAnd does it in isolation from other tests. \n6.4.2\nUsing mocks to decouple tests from the filesystem\nThe usual solution to the problem of tightly coupled tests is to mock the filesystem.\nYou can extract all operations on files into a separate class (IFileSystem) and inject\nthat class into AuditManager via the constructor. The tests will then mock this class\nand capture the writes the audit system do to the files (figure 6.13).\n \n \n \nTable 6.2\nThe initial version of the audit system scores badly on two out \nof the four attributes of a good test.\nInitial version\nProtection against regressions\nGood\nResistance to refactoring\nGood\nFast feedback\nBad\nMaintainability\nBad\nAudit system\nFilesystem\nTest\ninput\ninput\ninput\noutput\nassert\nFigure 6.12\nTests covering the initial version of the audit system would \nhave to work directly with the filesystem.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n138\nCHAPTER 6\nStyles of unit testing\nThe following listing shows how the filesystem is injected into AuditManager.\npublic class AuditManager\n{\nprivate readonly int _maxEntriesPerFile;\nprivate readonly string _directoryName;\nprivate readonly IFileSystem _fileSystem;    \npublic AuditManager(\nint maxEntriesPerFile,\nstring directoryName,\nIFileSystem fileSystem)\n{\n_maxEntriesPerFile = maxEntriesPerFile;\n_directoryName = directoryName;\n_fileSystem = fileSystem;                \n}\n}\nAnd next is the AddRecord method.\npublic void AddRecord(string visitorName, DateTime timeOfVisit)\n{\nstring[] filePaths = _fileSystem                                \n.GetFiles(_directoryName);                                  \n(int index, string path)[] sorted = SortByIndex(filePaths);\nstring newRecord = visitorName + ';' + timeOfVisit;\nif (sorted.Length == 0)\n{\nstring newFile = Path.Combine(_directoryName, \"audit_1.txt\");\n_fileSystem.WriteAllText(                                   \nnewFile, newRecord);                                    \nreturn;\n}\nListing 6.9\nInjecting the filesystem explicitly via the constructor\nListing 6.10\nUsing the new IFileSystem interface\nmock\nstub\ninput\nAudit system\nTest\nFilesystem\nFigure 6.13\nTests can mock the \nfilesystem and capture the writes \nthe audit system makes to the files.\nThe new interface \nrepresents the \nfilesystem.\nThe new\ninterface\nin action\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n139\nTransitioning to functional architecture and output-based testing\n(int currentFileIndex, string currentFilePath) = sorted.Last();\nList<string> lines = _fileSystem\n          \n.ReadAllLines(currentFilePath);          \nif (lines.Count < _maxEntriesPerFile)\n{\nlines.Add(newRecord);\nstring newContent = string.Join(\"\\r\\n\", lines);\n_fileSystem.WriteAllText(\n        \ncurrentFilePath, newContent);        \n}\nelse\n{\nint newIndex = currentFileIndex + 1;\nstring newName = $\"audit_{newIndex}.txt\";\nstring newFile = Path.Combine(_directoryName, newName);\n_fileSystem.WriteAllText(                \nnewFile, newRecord);                 \n}\n}\nIn listing 6.10, IFileSystem is a new custom interface that encapsulates the work with\nthe filesystem:\npublic interface IFileSystem\n{\nstring[] GetFiles(string directoryName);\nvoid WriteAllText(string filePath, string content);\nList<string> ReadAllLines(string filePath);\n}\nNow that AuditManager is decoupled from the filesystem, the shared dependency is\ngone, and tests can execute independently from each other. Here’s one such test.\n[Fact]\npublic void A_new_file_is_created_when_the_current_file_overflows()\n{\nvar fileSystemMock = new Mock<IFileSystem>();\nfileSystemMock\n.Setup(x => x.GetFiles(\"audits\"))\n.Returns(new string[]\n{\n@\"audits\\audit_1.txt\",\n@\"audits\\audit_2.txt\"\n});\nfileSystemMock\n.Setup(x => x.ReadAllLines(@\"audits\\audit_2.txt\"))\n.Returns(new List<string>\n{\n\"Peter; 2019-04-06T16:30:00\",\n\"Jane; 2019-04-06T16:40:00\",\nListing 6.11\nChecking the audit system’s behavior using a mock\nThe new\ninterface\nin action\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n140\nCHAPTER 6\nStyles of unit testing\n\"Jack; 2019-04-06T17:00:00\"\n});\nvar sut = new AuditManager(3, \"audits\", fileSystemMock.Object);\nsut.AddRecord(\"Alice\", DateTime.Parse(\"2019-04-06T18:00:00\"));\nfileSystemMock.Verify(x => x.WriteAllText(\n@\"audits\\audit_3.txt\",\n\"Alice;2019-04-06T18:00:00\"));\n}\nThis test verifies that when the number of entries in the current file reaches the limit\n(3, in this example), a new file with a single audit entry is created. Note that this is a\nlegitimate use of mocks. The application creates files that are visible to end users\n(assuming that those users use another program to read the files, be it specialized soft-\nware or a simple notepad.exe). Therefore, communications with the filesystem and\nthe side effects of these communications (that is, the changes in files) are part of the\napplication’s observable behavior. As you may remember from chapter 5, that’s the\nonly legitimate use case for mocking.\n This alternative implementation is an improvement over the initial version. Since\ntests no longer access the filesystem, they execute faster. And because you don’t need\nto look after the filesystem to keep the tests happy, the maintenance costs are also\nreduced. Protection against regressions and resistance to refactoring didn’t suffer\nfrom the refactoring either. Table 6.3 shows the differences between the two versions.\nWe can still do better, though. The test in listing 6.11 contains convoluted setups,\nwhich is less than ideal in terms of maintenance costs. Mocking libraries try their best\nto be helpful, but the resulting tests are still not as readable as those that rely on plain\ninput and output. \n6.4.3\nRefactoring toward functional architecture\nInstead of hiding side effects behind an interface and injecting that interface into\nAuditManager, you can move those side effects out of the class entirely. Audit-\nManager is then only responsible for making a decision about what to do with the\nfiles. A new class, Persister, acts on that decision and applies updates to the filesys-\ntem (figure 6.14).\nTable 6.3\nThe version with mocks compared to the initial version of the audit system\nInitial version\nWith mocks\nProtection against regressions\nGood\nGood\nResistance to refactoring\nGood\nGood\nFast feedback\nBad\nGood\nMaintainability\nBad\nModerate\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n141\nTransitioning to functional architecture and output-based testing\nPersister in this scenario acts as a mutable shell, while AuditManager becomes a func-\ntional (immutable) core. The following listing shows AuditManager after the refactoring.\npublic class AuditManager\n{\nprivate readonly int _maxEntriesPerFile;\npublic AuditManager(int maxEntriesPerFile)\n{\n_maxEntriesPerFile = maxEntriesPerFile;\n}\npublic FileUpdate AddRecord(\nFileContent[] files,\nstring visitorName,\nDateTime timeOfVisit)\n{\n(int index, FileContent file)[] sorted = SortByIndex(files);\nstring newRecord = visitorName + ';' + timeOfVisit;\nif (sorted.Length == 0)\n{\nreturn new FileUpdate(\n  \n\"audit_1.txt\", newRecord);  \n}\n(int currentFileIndex, FileContent currentFile) = sorted.Last();\nList<string> lines = currentFile.Lines.ToList();\nListing 6.12\nThe AuditManager class after refactoring\nFileContent\nFileUpdate\nAuditManager\n(functional core)\nPersister\n(mutable shell)\nFigure 6.14\nPersister and \nAuditManager form the functional \narchitecture. Persister gathers files \nand their contents from the working \ndirectory, feeds them to AuditManager, \nand then converts the return value into \nchanges in the filesystem.\nReturns an update \ninstruction\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n142\nCHAPTER 6\nStyles of unit testing\nif (lines.Count < _maxEntriesPerFile)\n{\nlines.Add(newRecord);\nstring newContent = string.Join(\"\\r\\n\", lines);\nreturn new FileUpdate(\n     \ncurrentFile.FileName, newContent);     \n}\nelse\n{\nint newIndex = currentFileIndex + 1;\nstring newName = $\"audit_{newIndex}.txt\";\nreturn new FileUpdate(\n                   \nnewName, newRecord);                   \n}\n}\n}\nInstead of the working directory path, AuditManager now accepts an array of File-\nContent. This class includes everything AuditManager needs to know about the filesys-\ntem to make a decision:\npublic class FileContent\n{\npublic readonly string FileName;\npublic readonly string[] Lines;\npublic FileContent(string fileName, string[] lines)\n{\nFileName = fileName;\nLines = lines;\n}\n}\nAnd, instead of mutating files in the working directory, AuditManager now returns an\ninstruction for the side effect it would like to perform:\npublic class FileUpdate\n{\npublic readonly string FileName;\npublic readonly string NewContent;\npublic FileUpdate(string fileName, string newContent)\n{\nFileName = fileName;\nNewContent = newContent;\n}\n}\nThe following listing shows the Persister class.\n \n \nReturns an \nupdate \ninstruction\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n143\nTransitioning to functional architecture and output-based testing\npublic class Persister\n{\npublic FileContent[] ReadDirectory(string directoryName)\n{\nreturn Directory\n.GetFiles(directoryName)\n.Select(x => new FileContent(\nPath.GetFileName(x),\nFile.ReadAllLines(x)))\n.ToArray();\n}\npublic void ApplyUpdate(string directoryName, FileUpdate update)\n{\nstring filePath = Path.Combine(directoryName, update.FileName);\nFile.WriteAllText(filePath, update.NewContent);\n}\n}\nNotice how trivial this class is. All it does is read content from the working directory\nand apply updates it receives from AuditManager back to that working directory. It has\nno branching (no if statements); all the complexity resides in the AuditManager\nclass. This is the separation between business logic and side effects in action.\n To maintain such a separation, you need to keep the interface of FileContent and\nFileUpdate as close as possible to that of the framework’s built-in file-interaction com-\nmands. All the parsing and preparation should be done in the functional core, so that\nthe code outside of that core remains trivial. For example, if .NET didn’t contain the\nbuilt-in File.ReadAllLines() method, which returns the file content as an array of\nlines, and only has File.ReadAllText(), which returns a single string, you’d need to\nreplace the Lines property in FileContent with a string too and do the parsing in\nAuditManager:\npublic class FileContent\n{\npublic readonly string FileName;\npublic readonly string Text; // previously, string[] Lines;\n}\nTo glue AuditManager and Persister together, you need another class: an applica-\ntion service in the hexagonal architecture taxonomy, as shown in the following listing.\npublic class ApplicationService\n{\nprivate readonly string _directoryName;\nprivate readonly AuditManager _auditManager;\nprivate readonly Persister _persister;\nListing 6.13\nThe mutable shell acting on AuditManager’s decision\nListing 6.14\nGluing together the functional core and mutable shell \nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n144\nCHAPTER 6\nStyles of unit testing\npublic ApplicationService(\nstring directoryName, int maxEntriesPerFile)\n{\n_directoryName = directoryName;\n_auditManager = new AuditManager(maxEntriesPerFile);\n_persister = new Persister();\n}\npublic void AddRecord(string visitorName, DateTime timeOfVisit)\n{\nFileContent[] files = _persister.ReadDirectory(_directoryName);\nFileUpdate update = _auditManager.AddRecord(\nfiles, visitorName, timeOfVisit);\n_persister.ApplyUpdate(_directoryName, update);\n}\n}\nAlong with gluing the functional core together with the mutable shell, the application\nservice also provides an entry point to the system for external clients (figure 6.15).\nWith this implementation, it becomes easy to check the audit system’s behavior. All\ntests now boil down to supplying a hypothetical state of the working directory and ver-\nifying the decision AuditManager makes.\n[Fact]\npublic void A_new_file_is_created_when_the_current_file_overflows()\n{\nvar sut = new AuditManager(3);\nvar files = new FileContent[]\n{\nnew FileContent(\"audit_1.txt\", new string[0]),\nListing 6.15\nThe test without mocks\nAudit manager\nPersister\nPersister\nApplication service\nExternal client\nFigure 6.15\nApplicationService glues the functional core (AuditManager) \nand the mutable shell (Persister) together and provides an entry point for external \nclients. In the hexagonal architecture taxonomy, ApplicationService and \nPersister are part of the application services layer, while AuditManager \nbelongs to the domain model.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "page_number": 157
    },
    {
      "number": 19,
      "title": "Segment 19 (pages 167-178)",
      "start_page": 167,
      "end_page": 178,
      "detection_method": "topic_boundary",
      "content": "145\nTransitioning to functional architecture and output-based testing\nnew FileContent(\"audit_2.txt\", new string[]\n{\n\"Peter; 2019-04-06T16:30:00\",\n\"Jane; 2019-04-06T16:40:00\",\n\"Jack; 2019-04-06T17:00:00\"\n})\n};\nFileUpdate update = sut.AddRecord(\nfiles, \"Alice\", DateTime.Parse(\"2019-04-06T18:00:00\"));\nAssert.Equal(\"audit_3.txt\", update.FileName);\nAssert.Equal(\"Alice;2019-04-06T18:00:00\", update.NewContent);\n}\nThis test retains the improvement the test with mocks made over the initial version\n(fast feedback) but also further improves on the maintainability metric. There’s no\nneed for complex mock setups anymore, only plain inputs and outputs, which helps\nthe test’s readability a lot. Table 6.4 compares the output-based test with the initial ver-\nsion and the version with mocks.\nNotice that the instructions generated by a functional core are always a value or a set of\nvalues. Two instances of such a value are interchangeable as long as their contents\nmatch. You can take advantage of this fact and improve test readability even further by\nturning FileUpdate into a value object. To do that in .NET, you need to either convert\nthe class into a struct or define custom equality members. That will give you compar-\nison by value, as opposed to the comparison by reference, which is the default behavior\nfor classes in C#. Comparison by value also allows you to compress the two assertions\nfrom listing 6.15 into one:\nAssert.Equal(\nnew FileUpdate(\"audit_3.txt\", \"Alice;2019-04-06T18:00:00\"),\nupdate);\nOr, using Fluent Assertions,\nupdate.Should().Be(\nnew FileUpdate(\"audit_3.txt\", \"Alice;2019-04-06T18:00:00\"));\nTable 6.4\nThe output-based test compared to the previous two versions\nInitial version\nWith mocks\nOutput-based\nProtection against regressions\nGood\nGood\nGood\nResistance to refactoring\nGood\nGood\nGood\nFast feedback\nBad\nGood\nGood\nMaintainability\nBad\nModerate\nGood\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n146\nCHAPTER 6\nStyles of unit testing\n6.4.4\nLooking forward to further developments\nLet’s step back for a minute and look at further developments that could be done in\nour sample project. The audit system I showed you is quite simple and contains only\nthree branches:\nCreating a new file in case of an empty working directory\nAppending a new record to an existing file\nCreating another file when the number of entries in the current file exceeds\nthe limit\nAlso, there’s only one use case: addition of a new entry to the audit log. What if\nthere were another use case, such as deleting all mentions of a particular visitor?\nAnd what if the system needed to do validations (say, for the maximum length of the\nvisitor’s name)?\n Deleting all mentions of a particular visitor could potentially affect several files, so\nthe new method would need to return multiple file instructions:\npublic FileUpdate[] DeleteAllMentions(\nFileContent[] files, string visitorName)\nFurthermore, business people might require that you not keep empty files in the\nworking directory. If the deleted entry was the last entry in an audit file, you would\nneed to remove that file altogether. To implement this requirement, you could\nrename FileUpdate to FileAction and introduce an additional ActionType enum\nfield to indicate whether it was an update or a deletion.\n Error handling also becomes simpler and more explicit with functional architec-\nture. You could embed errors into the method’s signature, either in the FileUpdate\nclass or as a separate component:\npublic (FileUpdate update, Error error) AddRecord(\nFileContent[] files,\nstring visitorName,\nDateTime timeOfVisit)\nThe application service would then check for this error. If it was there, the service\nwouldn’t pass the update instruction to the persister, instead propagating an error\nmessage to the user. \n6.5\nUnderstanding the drawbacks of functional \narchitecture\nUnfortunately, functional architecture isn’t always attainable. And even when it is, the\nmaintainability benefits are often offset by a performance impact and increase in\nthe size of the code base. In this section, we’ll explore the costs and the trade-offs\nattached to functional architecture.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n147\nUnderstanding the drawbacks of functional architecture\n6.5.1\nApplicability of functional architecture\nFunctional architecture worked for our audit system because this system could gather\nall the inputs up front, before making a decision. Often, though, the execution flow is\nless straightforward. You might need to query additional data from an out-of-process\ndependency, based on an intermediate result of the decision-making process.\n Here’s an example. Let’s say the audit system needs to check the visitor’s access\nlevel if the number of times they have visited during the last 24 hours exceeds some\nthreshold. And let’s also assume that all visitors’ access levels are stored in a database.\nYou can’t pass an IDatabase instance to AuditManager like this:\npublic FileUpdate AddRecord(\nFileContent[] files, string visitorName,\nDateTime timeOfVisit, IDatabase database\n)\nSuch an instance would introduce a hidden input to the AddRecord() method. This\nmethod would, therefore, cease to be a mathematical function (figure 6.16), which\nmeans you would no longer be able to apply output-based testing.\nThere are two solutions in such a situation:\nYou can gather the visitor’s access level in the application service up front,\nalong with the directory content.\nYou can introduce a new method such as IsAccessLevelCheckRequired() in\nAuditManager. The application service would call this method before Add-\nRecord(), and if it returned true, the service would get the access level from\nthe database and pass it to AddRecord().\nBoth approaches have drawbacks. The first one concedes performance—it uncondi-\ntionally queries the database, even in cases when the access level is not required. But this\napproach keeps the separation of business logic and communication with external\nApplication\nservice\nReadDirectory\nAudit manager\nFilesystem\nand database\nAdd\nrecord\nApplyUpdate\nGet\naccess\nlevel\nFigure 6.16\nA dependency on the database introduces a hidden input to \nAuditManager. Such a class is no longer purely functional, and the whole \napplication no longer follows the functional architecture.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n148\nCHAPTER 6\nStyles of unit testing\nsystems fully intact: all decision-making resides in AuditManager as before. The second\napproach concedes a degree of that separation for performance gains: the decision as\nto whether to call the database now goes to the application service, not AuditManager.\n Note that, unlike these two options, making the domain model (AuditManager)\ndepend on the database isn’t a good idea. I’ll explain more about keeping the balance\nbetween performance and separation of concerns in the next two chapters.\nNOTE\nA class from the functional core should work not with a collaborator,\nbut with the product of its work, a value. \n6.5.2\nPerformance drawbacks\nThe performance impact on the system as a whole is a common argument against\nfunctional architecture. Note that it’s not the performance of tests that suffers. The\noutput-based tests we ended up with work as fast as the tests with mocks. It’s that the\nsystem itself now has to do more calls to out-of-process dependencies and becomes\nless performant. The initial version of the audit system didn’t read all files from the\nworking directory, and neither did the version with mocks. But the final version does\nin order to comply with the read-decide-act approach.\n The choice between a functional architecture and a more traditional one is a\ntrade-off between performance and code maintainability (both production and test\ncode). In some systems where the performance impact is not as noticeable, it’s better\nto go with functional architecture for additional gains in maintainability. In others,\nyou might need to make the opposite choice. There’s no one-size-fits-all solution. \nCollaborators vs. values\nYou may have noticed that AuditManager’s AddRecord() method has a dependency\nthat’s not present in its signature: the _maxEntriesPerFile field. The audit man-\nager refers to this field to make a decision to either append an existing audit file or\ncreate a new one.\nAlthough this dependency isn’t present among the method’s arguments, it’s not hid-\nden. It can be derived from the class’s constructor signature. And because the _max-\nEntriesPerFile field is immutable, it stays the same between the class instantiation\nand the call to AddRecord(). In other words, that field is a value.\nThe situation with the IDatabase dependency is different because it’s a collaborator,\nnot a value like _maxEntriesPerFile. As you may remember from chapter 2, a col-\nlaborator is a dependency that is one or the other of the following:\nMutable (allows for modification of its state)\nA proxy to data that is not yet in memory (a shared dependency)\nThe IDatabase instance falls into the second category and, therefore, is a collabo-\nrator. It requires an additional call to an out-of-process dependency and thus pre-\ncludes the use of output-based testing.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n149\nSummary\n6.5.3\nIncrease in the code base size\nThe same is true for the size of the code base. Functional architecture requires a clear\nseparation between the functional (immutable) core and the mutable shell. This\nnecessitates additional coding initially, although it ultimately results in reduced code\ncomplexity and gains in maintainability.\n Not all projects exhibit a high enough degree of complexity to justify such an initial\ninvestment, though. Some code bases aren’t that significant from a business perspec-\ntive or are just plain too simple. It doesn’t make sense to use functional architecture\nin such projects because the initial investment will never pay off. Always apply func-\ntional architecture strategically, taking into account the complexity and importance of\nyour system.\n Finally, don’t go for purity of the functional approach if that purity comes at too\nhigh a cost. In most projects, you won’t be able to make the domain model fully\nimmutable and thus can’t rely solely on output-based tests, at least not when using an\nOOP language like C# or Java. In most cases, you’ll have a combination of output-\nbased and state-based styles, with a small mix of communication-based tests, and that’s\nfine. The goal of this chapter is not to incite you to transition all your tests toward the\noutput-based style; the goal is to transition as many of them as reasonably possible.\nThe difference is subtle but important. \nSummary\nOutput-based testing is a style of testing where you feed an input to the SUT and\ncheck the output it produces. This style of testing assumes there are no hidden\ninputs or outputs, and the only result of the SUT’s work is the value it returns.\nState-based testing verifies the state of the system after an operation is completed.\nIn communication-based testing, you use mocks to verify communications between\nthe system under test and its collaborators.\nThe classical school of unit testing prefers the state-based style over the\ncommunication-based one. The London school has the opposite preference.\nBoth schools use output-based testing.\nOutput-based testing produces tests of the highest quality. Such tests rarely cou-\nple to implementation details and thus are resistant to refactoring. They are\nalso small and concise and thus are more maintainable.\nState-based testing requires extra prudence to avoid brittleness: you need to\nmake sure you don’t expose a private state to enable unit testing. Because state-\nbased tests tend to be larger than output-based tests, they are also less maintain-\nable. Maintainability issues can sometimes be mitigated (but not eliminated)\nwith the use of helper methods and value objects.\nCommunication-based testing also requires extra prudence to avoid brittle-\nness. You should only verify communications that cross the application bound-\nary and whose side effects are visible to the external world. Maintainability of\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n150\nCHAPTER 6\nStyles of unit testing\ncommunication-based tests is worse compared to output-based and state-based\ntests. Mocks tend to occupy a lot of space, and that makes tests less readable.\nFunctional programming is programming with mathematical functions.\nA mathematical function is a function (or method) that doesn’t have any hidden\ninputs or outputs. Side effects and exceptions are hidden outputs. A reference\nto an internal or external state is a hidden input. Mathematical functions are\nexplicit, which makes them extremely testable.\nThe goal of functional programming is to introduce a separation between busi-\nness logic and side effects.\nFunctional architecture helps achieve that separation by pushing side effects\nto the edges of a business operation. This approach maximizes the amount of\ncode written in a purely functional way while minimizing code that deals with\nside effects.\nFunctional architecture divides all code into two categories: functional core\nand mutable shell. The functional core makes decisions. The mutable shell supplies\ninput data to the functional core and converts decisions the core makes into\nside effects.\nThe difference between functional and hexagonal architectures is in their treat-\nment of side effects. Functional architecture pushes all side effects out of the\ndomain layer. Conversely, hexagonal architecture is fine with side effects made\nby the domain layer, as long as they are limited to that domain layer only. Func-\ntional architecture is hexagonal architecture taken to an extreme.\nThe choice between a functional architecture and a more traditional one is a\ntrade-off between performance and code maintainability. Functional architec-\nture concedes performance for maintainability gains.\nNot all code bases are worth converting into functional architecture. Apply\nfunctional architecture strategically. Take into account the complexity and the\nimportance of your system. In code bases that are simple or not that important,\nthe initial investment required for functional architecture won’t pay off.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n151\nRefactoring toward\nvaluable unit tests\nIn chapter 1, I defined the properties of a good unit test suite:\nIt is integrated into the development cycle.\nIt targets only the most important parts of your code base.\nIt provides maximum value with minimum maintenance costs. To achieve\nthis last attribute, you need to be able to:\n– Recognize a valuable test (and, by extension, a test of low value).\n– Write a valuable test.\nChapter 4 covered the topic of recognizing a valuable test using the four attributes:\nprotection against regressions, resistance to refactoring, fast feedback, and main-\ntainability. And chapter 5 expanded on the most important one of the four: resis-\ntance to refactoring.\n As I mentioned earlier, it’s not enough to recognize valuable tests, you should also\nbe able to write such tests. The latter skill requires the former, but it also requires\nThis chapter covers\nRecognizing the four types of code\nUnderstanding the Humble Object pattern\nWriting valuable tests\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n152\nCHAPTER 7\nRefactoring toward valuable unit tests\nthat you know code design techniques. Unit tests and the underlying code are highly\nintertwined, and it’s impossible to create valuable tests without putting effort into the\ncode base they cover.\n You saw an example of a code base transformation in chapter 6, where we refac-\ntored an audit system toward a functional architecture and, as a result, were able to\napply output-based testing. This chapter generalizes this approach onto a wider spec-\ntrum of applications, including those that can’t use a functional architecture. You’ll\nsee practical guidelines on how to write valuable tests in almost any software project.\n7.1\nIdentifying the code to refactor\nIt’s rarely possible to significantly improve a test suite without refactoring the underly-\ning code. There’s no way around it—test and production code are intrinsically con-\nnected. In this section, you’ll see how to categorize your code into the four types in\norder to outline the direction of the refactoring. The subsequent sections show a com-\nprehensive example.\n7.1.1\nThe four types of code\nIn this section, I describe the four types of code that serve as a foundation for the rest\nof this chapter. \n All production code can be categorized along two dimensions:\nComplexity or domain significance\nThe number of collaborators\nCode complexity is defined by the number of decision-making (branching) points in the\ncode. The greater that number, the higher the complexity.\nHow to calculate cyclomatic complexity\nIn computer science, there’s a special term that describes code complexity: cyclo-\nmatic complexity. Cyclomatic complexity indicates the number of branches in a given\nprogram or method. This metric is calculated as\n1 + <number of branching points>\nThus, a method with no control flow statements (such as if statements or condi-\ntional loops) has a cyclomatic complexity of 1 + 0 = 1.\nThere’s another meaning to this metric. You can think of it in terms of the number of\nindependent paths through the method from an entry to an exit, or the number of tests\nneeded to get a 100% branch coverage.\nNote that the number of branching points is counted as the number of simplest pred-\nicates involved. For instance, a statement like IF condition1 AND condition2\nTHEN ... is equivalent to IF condition1 THEN IF condition2 THEN ... Therefore,\nits complexity would be 1 + 2 = 3.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n153\nIdentifying the code to refactor\nDomain significance shows how significant the code is for the problem domain of your\nproject. Normally, all code in the domain layer has a direct connection to the end\nusers’ goals and thus exhibits a high domain significance. On the other hand, utility\ncode doesn’t have such a connection.\n Complex code and code that has domain significance benefit from unit testing the\nmost because the corresponding tests have great protection against regressions. Note\nthat the domain code doesn’t have to be complex, and complex code doesn’t have to\nexhibit domain significance to be test-worthy. The two components are independent\nof each other. For example, a method calculating an order price can contain no con-\nditional statements and thus have the cyclomatic complexity of 1. Still, it’s important\nto test such a method because it represents business-critical functionality.\n The second dimension is the number of collaborators a class or a method has. As\nyou may remember from chapter 2, a collaborator is a dependency that is either\nmutable or out-of-process (or both). Code with a large number of collaborators is\nexpensive to test. That’s due to the maintainability metric, which depends on the size\nof the test. It takes space to bring collaborators to an expected condition and then\ncheck their state or interactions with them afterward. And the more collaborators\nthere are, the larger the test becomes.\n The type of the collaborators also matters. Out-of-process collaborators are a no-go\nwhen it comes to the domain model. They add additional maintenance costs due to\nthe necessity to maintain complicated mock machinery in tests. You also have to be\nextra prudent and only use mocks to verify interactions that cross the application\nboundary in order to maintain proper resistance to refactoring (refer to chapter 5 for\nmore details). It’s better to delegate all communications with out-of-process depen-\ndencies to classes outside the domain layer. The domain classes then will only work\nwith in-process dependencies.\n Notice that both implicit and explicit collaborators count toward this number. It\ndoesn’t matter if the system under test (SUT) accepts a collaborator as an argument\nor refers to it implicitly via a static method, you still have to set up this collaborator in\ntests. Conversely, immutable dependencies (values or value objects) don’t count. Such\ndependencies are much easier to set up and assert against.\n The combination of code complexity, its domain significance, and the number of\ncollaborators give us the four types of code shown in figure 7.1:\nDomain model and algorithms (figure 7.1, top left)—Complex code is often part of\nthe domain model but not in 100% of all cases. You might have a complex algo-\nrithm that’s not directly related to the problem domain.\nTrivial code (figure 7.1, bottom left)—Examples of such code in C# are parameter-\nless constructors and one-line properties: they have few (if any) collaborators\nand exhibit little complexity or domain significance.\nControllers (figure 7.1, bottom right)—This code doesn’t do complex or business-\ncritical work by itself but coordinates the work of other components like domain\nclasses and external applications.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n154\nCHAPTER 7\nRefactoring toward valuable unit tests\nOvercomplicated code (figure 7.1, top right)—Such code scores highly on both\nmetrics: it has a lot of collaborators, and it’s also complex or important. An\nexample here are fat controllers (controllers that don’t delegate complex work\nanywhere and do everything themselves).\nUnit testing the top-left quadrant (domain model and algorithms) gives you the best\nreturn for your efforts. The resulting unit tests are highly valuable and cheap. They’re\nvaluable because the underlying code carries out complex or important logic, thus\nincreasing tests’ protection against regressions. And they’re cheap because the code\nhas few collaborators (ideally, none), thus decreasing tests’ maintenance costs.\n Trivial code shouldn’t be tested at all; such tests have a close-to-zero value. As for\ncontrollers, you should test them briefly as part of a much smaller set of the overarch-\ning integration tests (I cover this topic in part 3).\n The most problematic type of code is the overcomplicated quadrant. It’s hard to\nunit test but too risky to leave without test coverage. Such code is one of the main rea-\nsons many people struggle with unit testing. This whole chapter is primarily devoted\nto how you can bypass this dilemma. The general idea is to split overcomplicated code\ninto two parts: algorithms and controllers (figure 7.2), although the actual implemen-\ntation can be tricky at times.\nTIP\nThe more important or complex the code, the fewer collaborators it\nshould have.\nGetting rid of the overcomplicated code and unit testing only the domain model and\nalgorithms is the path to a highly valuable, easily maintainable test suite. With this\napproach, you won’t have 100% test coverage, but you don’t need to—100% coverage\nshouldn’t ever be your goal. Your goal is a test suite where each test adds significant\nvalue to the project. Refactor or get rid of all other tests. Don’t allow them to inflate\nthe size of your test suite.\nComplexity,\ndomain\nsigniﬁcance\nDomain model,\nalgorithms\nOvercomplicated\ncode\nTrivial code\nNumber of\ncollaborators\nControllers\nFigure 7.1\nThe four types of code, \ncategorized by code complexity and \ndomain significance (the vertical \naxis) and the number of collaborators \n(the horizontal axis).\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n155\nIdentifying the code to refactor\nNOTE\nRemember that it’s better to not write a test at all than to write a\nbad test.\nOf course, getting rid of overcomplicated code is easier said than done. Still, there are\ntechniques that can help you do that. I’ll first explain the theory behind those tech-\nniques and then demonstrate them using a close-to-real-world example. \n7.1.2\nUsing the Humble Object pattern to split overcomplicated code\nTo split overcomplicated code, you need to use the Humble Object design pattern.\nThis pattern was introduced by Gerard Meszaros in his book xUnit Test Patterns: Refac-\ntoring Test Code (Addison-Wesley, 2007) as one of the ways to battle code coupling, but\nit has a much broader application. You’ll see why shortly.\n We often find that code is hard to test because it’s coupled to a framework depen-\ndency (see figure 7.3). Examples include asynchronous or multi-threaded execution,\nuser interfaces, communication with out-of-process dependencies, and so on.\nTo bring the logic of this code under test, you need to extract a testable part out of it.\nAs a result, the code becomes a thin, humble wrapper around that testable part: it glues\nComplexity,\ndomain\nsigniﬁcance\nDomain model,\nalgorithms\nOvercomplicated\ncode\nTrivial code\nNumber of\ncollaborators\nControllers\nFigure 7.2\nRefactor overcomplicated \ncode by splitting it into algorithms and \ncontrollers. Ideally, you should have no \ncode in the top-right quadrant.\nOvercomplicated code\nHard-to-test\ndependency\nLogic\nTest\nFigure 7.3\nIt’s hard to test \ncode that couples to a difficult \ndependency. Tests have to deal \nwith that dependency, too, which \nincreases their maintenance cost.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n156\nCHAPTER 7\nRefactoring toward valuable unit tests\nthe hard-to-test dependency and the newly extracted component together, but itself\ncontains little or no logic and thus doesn’t need to be tested (figure 7.4).\n If this approach looks familiar, it’s because you already saw it in this book. In fact,\nboth hexagonal and functional architectures implement this exact pattern. As you\nmay remember from previous chapters, hexagonal architecture advocates for the sep-\naration of business logic and communications with out-of-process dependencies. This\nis what the domain and application services layers are responsible for, respectively.\n Functional architecture goes even further and separates business logic from com-\nmunications with all collaborators, not just out-of-process ones. This is what makes\nfunctional architecture so testable: its functional core has no collaborators. All depen-\ndencies in a functional core are immutable, which brings it very close to the vertical\naxis on the types-of-code diagram (figure 7.5).\nHumble object\nHard-to-test\ndependency\nTest\nLogic\nFigure 7.4\nThe Humble Object \npattern extracts the logic out of the \novercomplicated code, making that \ncode so humble that it doesn’t need to \nbe tested. The extracted logic is \nmoved into another class, decoupled \nfrom the hard-to-test dependency.\nComplexity,\ndomain\nsigniﬁcance\nNumber of\ncollaborators\nDomain model,\nalgorithms\nOvercomplicated\ncode\nTrivial code\nControllers\nDomain layer\nMutable shell and\napplication services layer\nFunctional core\nFigure 7.5\nThe functional core in a functional architecture and the domain layer in \na hexagonal architecture reside in the top-left quadrant: they have few collaborators \nand exhibit high complexity and domain significance. The functional core is closer \nto the vertical axis because it has no collaborators. The mutable shell (functional \narchitecture) and the application services layer (hexagonal architecture) belong \nto the controllers’ quadrant.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "page_number": 167
    },
    {
      "number": 20,
      "title": "Segment 20 (pages 179-189)",
      "start_page": 179,
      "end_page": 189,
      "detection_method": "topic_boundary",
      "content": "157\nIdentifying the code to refactor\nAnother way to view the Humble Object pattern is as a means to adhere to the Single\nResponsibility principle, which states that each class should have only a single respon-\nsibility.1 One such responsibility is always business logic; the pattern can be applied to\nsegregate that logic from pretty much anything.\n In our particular situation, we are interested in the separation of business logic\nand orchestration. You can think of these two responsibilities in terms of code depth\nversus code width. Your code can be either deep (complex or important) or wide (work\nwith many collaborators), but never both (figure 7.6).\nI can’t stress enough how important this separation is. In fact, many well-known princi-\nples and patterns can be described as a form of the Humble Object pattern: they are\ndesigned specifically to segregate complex code from the code that does orchestration.\n You already saw the relationship between this pattern and hexagonal and func-\ntional architectures. Other examples include the Model-View-Presenter (MVP) and\nthe Model-View-Controller (MVC) patterns. These two patterns help you decouple\nbusiness logic (the Model part), UI concerns (the View), and the coordination between\nthem (Presenter or Controller). The Presenter and Controller components are humble\nobjects: they glue the view and the model together.\n Another example is the Aggregate pattern from Domain-Driven Design.2 One of its\ngoals is to reduce connectivity between classes by grouping them into clusters—\naggregates. The classes are highly connected inside those clusters, but the clusters them-\nselves are loosely coupled. Such a structure decreases the total number of communica-\ntions in the code base. The reduced connectivity, in turn, improves testability.\n1 See Agile Principles, Patterns, and Practices in C# by Robert C. Martin and Micah Martin (Prentice Hall, 2006).\n2 See Domain-Driven Design: Tackling Complexity in the Heart of Software by Eric Evans (Addison-Wesley, 2003).\nControllers\nDomain layer,\nalgorithms\nFigure 7.6\nCode depth versus code width is \na useful metaphor to apply when you think of \nthe separation between the business logic \nand orchestration responsibilities. Controllers \norchestrate many dependencies (represented as \narrows in the figure) but aren’t complex on their \nown (complexity is represented as block height). \nDomain classes are the opposite of that.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n158\nCHAPTER 7\nRefactoring toward valuable unit tests\n Note that improved testability is not the only reason to maintain the separation\nbetween business logic and orchestration. Such a separation also helps tackle code\ncomplexity, which is crucial for project growth, too, especially in the long run. I per-\nsonally always find it fascinating how a testable design is not only testable but also easy\nto maintain. \n7.2\nRefactoring toward valuable unit tests\nIn this section, I’ll show a comprehensive example of splitting overcomplicated code\ninto algorithms and controllers. You saw a similar example in the previous chapter,\nwhere we talked about output-based testing and functional architecture. This time, I’ll\ngeneralize this approach to all enterprise-level applications, with the help of the Hum-\nble Object pattern. I’ll use this project not only in this chapter but also in the subse-\nquent chapters of part 3.\n7.2.1\nIntroducing a customer management system\nThe sample project is a customer management system (CRM) that handles user\nregistrations. All users are stored in a database. The system currently supports only\none use case: changing a user’s email. There are three business rules involved in this\noperation:\nIf the user’s email belongs to the company’s domain, that user is marked as an\nemployee. Otherwise, they are treated as a customer.\nThe system must track the number of employees in the company. If the user’s\ntype changes from employee to customer, or vice versa, this number must\nchange, too.\nWhen the email changes, the system must notify external systems by sending a\nmessage to a message bus.\nThe following listing shows the initial implementation of the CRM system.\npublic class User\n{\npublic int UserId { get; private set; }\npublic string Email { get; private set; }\npublic UserType Type { get; private set; }\npublic void ChangeEmail(int userId, string newEmail)\n{\nobject[] data = Database.GetUserById(userId);    \nUserId = userId;\nEmail = (string)data[1];\nType = (UserType)data[2];\nif (Email == newEmail)\nreturn;\nListing 7.1\nInitial implementation of the CRM system\nRetrieves the user’s \ncurrent email and \ntype from the \ndatabase\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n159\nRefactoring toward valuable unit tests\nobject[] companyData = Database.GetCompany();       \nstring companyDomainName = (string)companyData[0];\nint numberOfEmployees = (int)companyData[1];\nstring emailDomain = newEmail.Split('@')[1];\nbool isEmailCorporate = emailDomain == companyDomainName;\nUserType newType = isEmailCorporate                       \n? UserType.Employee\n                       \n: UserType.Customer;\n                       \nif (Type != newType)\n{\nint delta = newType == UserType.Employee ? 1 : -1;\nint newNumber = numberOfEmployees + delta;\nDatabase.SaveCompany(newNumber);        \n}\nEmail = newEmail;\nType = newType;\nDatabase.SaveUser(this);              \nMessageBus.SendEmailChangedMessage(UserId, newEmail);       \n}\n}\npublic enum UserType\n{\nCustomer = 1,\nEmployee = 2\n}\nThe User class changes a user email. Note that, for brevity, I omitted simple valida-\ntions such as checks for email correctness and user existence in the database. Let’s\nanalyze this implementation from the perspective of the types-of-code diagram.\n The code’s complexity is not too high. The ChangeEmail method contains only a\ncouple of explicit decision-making points: whether to identify the user as an employee\nor a customer, and how to update the company’s number of employees. Despite being\nsimple, these decisions are important: they are the application’s core business logic.\nHence, the class scores highly on the complexity and domain significance dimension.\n On the other hand, the User class has four dependencies, two of which are explicit\nand the other two of which are implicit. The explicit dependencies are the userId\nand newEmail arguments. These are values, though, and thus don’t count toward the\nclass’s number of collaborators. The implicit ones are Database and MessageBus.\nThese two are out-of-process collaborators. As I mentioned earlier, out-of-process col-\nlaborators are a no-go for code with high domain significance. Hence, the User class\nscores highly on the collaborators dimension, which puts this class into the overcom-\nplicated category (figure 7.7).\n This approach—when a domain class retrieves and persists itself to the database—\nis called the Active Record pattern. It works fine in simple or short-lived projects but\nRetrieves the organization’s \ndomain name and the \nnumber of employees \nfrom the database\nSets the user type \ndepending on the new \nemail’s domain name\nUpdates the number \nof employees in the \norganization, if needed\nPersists the user \nin the database\nSends a notification\nto the message bus\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n160\nCHAPTER 7\nRefactoring toward valuable unit tests\noften fails to scale as the code base grows. The reason is precisely this lack of separa-\ntion between these two responsibilities: business logic and communication with out-of-\nprocess dependencies. \n7.2.2\nTake 1: Making implicit dependencies explicit\nThe usual approach to improve testability is to make implicit dependencies explicit:\nthat is, introduce interfaces for Database and MessageBus, inject those interfaces into\nUser, and then mock them in tests. This approach does help, and that’s exactly what\nwe did in the previous chapter when we introduced the implementation with mocks\nfor the audit system. However, it’s not enough.\n From the perspective of the types-of-code diagram, it doesn’t matter if the domain\nmodel refers to out-of-process dependencies directly or via an interface. Such depen-\ndencies are still out-of-process; they are proxies to data that is not yet in memory. You\nstill need to maintain complicated mock machinery in order to test such classes,\nwhich increases the tests’ maintenance costs. Moreover, using mocks for the database\ndependency would lead to test fragility (we’ll discuss this in the next chapter).\n Overall, it’s much cleaner for the domain model not to depend on out-of-process\ncollaborators at all, directly or indirectly (via an interface). That’s what the hexagonal\narchitecture advocates as well—the domain model shouldn’t be responsible for com-\nmunications with external systems. \n7.2.3\nTake 2: Introducing an application services layer\nTo overcome the problem of the domain model directly communicating with external\nsystems, we need to shift this responsibility to another class, a humble controller (an\napplication service, in the hexagonal architecture taxonomy). As a general rule, domain\nclasses should only depend on in-process dependencies, such as other domain classes,\nor plain values. Here’s what the first version of that application service looks like.\nDomain model,\nalgorithms\nOvercomplicated\ncode\nTrivial code\nControllers\nComplexity,\ndomain\nsigniﬁcance\nNumber of\ncollaborators\nUser class\nFigure 7.7\nThe initial \nimplementation of the User \nclass scores highly on both \ndimensions and thus falls \ninto the category of \novercomplicated code.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n161\nRefactoring toward valuable unit tests\npublic class UserController\n{\nprivate readonly Database _database = new Database();\nprivate readonly MessageBus _messageBus = new MessageBus();\npublic void ChangeEmail(int userId, string newEmail)\n{\nobject[] data = _database.GetUserById(userId);\nstring email = (string)data[1];\nUserType type = (UserType)data[2];\nvar user = new User(userId, email, type);\nobject[] companyData = _database.GetCompany();\nstring companyDomainName = (string)companyData[0];\nint numberOfEmployees = (int)companyData[1];\nint newNumberOfEmployees = user.ChangeEmail(\nnewEmail, companyDomainName, numberOfEmployees);\n_database.SaveCompany(newNumberOfEmployees);\n_database.SaveUser(user);\n_messageBus.SendEmailChangedMessage(userId, newEmail);\n}\n}\nThis is a good first try; the application service helped offload the work with out-of-\nprocess dependencies from the User class. But there are some issues with this imple-\nmentation:\nThe out-of-process dependencies (Database and MessageBus) are instantiated\ndirectly, not injected. That’s going to be a problem for the integration tests we’ll\nbe writing for this class.\nThe controller reconstructs a User instance from the raw data it receives from\nthe database. This is complex logic and thus shouldn’t belong to the applica-\ntion service, whose sole role is orchestration, not logic of any complexity or\ndomain significance.\nThe same is true for the company’s data. The other problem with that data is\nthat User now returns an updated number of employees, which doesn’t look\nright. The number of company employees has nothing to do with a specific\nuser. This responsibility should belong elsewhere.\nThe controller persists modified data and sends notifications to the message\nbus unconditionally, regardless of whether the new email is different than the\nprevious one.\nThe User class has become quite easy to test because it no longer has to communicate\nwith out-of-process dependencies. In fact, it has no collaborators whatsoever—out-of-\nprocess or not. Here’s the new version of User’s ChangeEmail method:\nListing 7.2\nApplication service, version 1\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n162\nCHAPTER 7\nRefactoring toward valuable unit tests\npublic int ChangeEmail(string newEmail,\nstring companyDomainName, int numberOfEmployees)\n{\nif (Email == newEmail)\nreturn numberOfEmployees;\nstring emailDomain = newEmail.Split('@')[1];\nbool isEmailCorporate = emailDomain == companyDomainName;\nUserType newType = isEmailCorporate\n? UserType.Employee\n: UserType.Customer;\nif (Type != newType)\n{\nint delta = newType == UserType.Employee ? 1 : -1;\nint newNumber = numberOfEmployees + delta;\nnumberOfEmployees = newNumber;\n}\nEmail = newEmail;\nType = newType;\nreturn numberOfEmployees;\n}\nFigure 7.8 shows where User and UserController currently stand in our diagram.\nUser has moved to the domain model quadrant, close to the vertical axis, because it\nno longer has to deal with collaborators. UserController is more problematic.\nAlthough I’ve put it into the controllers quadrant, it almost crosses the boundary into\novercomplicated code because it contains logic that is quite complex. \nDomain model,\nalgorithms\nOvercomplicated\ncode\nTrivial code\nControllers\nComplexity,\ndomain\nsigniﬁcance\nNumber of\ncollaborators\nUserController\nUser\nFigure 7.8\nTake 2 puts User in the domain model quadrant, close to the vertical \naxis. UserController almost crosses the boundary with the overcomplicated \nquadrant because it contains complex logic.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n163\nRefactoring toward valuable unit tests\n7.2.4\nTake 3: Removing complexity from the application service\nTo put UserController firmly into the controllers quadrant, we need to extract the\nreconstruction logic from it. If you use an object-relational mapping (ORM) library\nto map the database into the domain model, that would be a good place to which to\nattribute the reconstruction logic. Each ORM library has a dedicated place where you\ncan specify how your database tables should be mapped to domain classes, such as\nattributes on top of those domain classes, XML files, or files with fluent mappings.\n If you don’t want to or can’t use an ORM, create a factory in the domain model\nthat will instantiate the domain classes using raw database data. This factory can be a\nseparate class or, for simpler cases, a static method in the existing domain classes. The\nreconstruction logic in our sample application is not too complicated, but it’s good to\nkeep such things separated, so I’m putting it in a separate UserFactory class as shown\nin the following listing.\npublic class UserFactory\n{\npublic static User Create(object[] data)\n{\nPrecondition.Requires(data.Length >= 3);\nint id = (int)data[0];\nstring email = (string)data[1];\nUserType type = (UserType)data[2];\nreturn new User(id, email, type);\n}\n}\nThis code is now fully isolated from all collaborators and therefore easily testable.\nNotice that I’ve put a safeguard in this method: a requirement to have at least three\nelements in the data array. Precondition is a simple custom class that throws an\nexception if the Boolean argument is false. The reason for this class is the more\nsuccinct code and the condition inversion: affirmative statements are more read-\nable than negative ones. In our example, the data.Length >= 3 requirement reads\nbetter than\nif (data.Length < 3)\nthrow new Exception();\nNote that while this reconstruction logic is somewhat complex, it doesn’t have domain\nsignificance: it isn’t directly related to the client’s goal of changing the user email. It’s\nan example of the utility code I refer to in previous chapters.\n \nListing 7.3\nUser factory\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n164\nCHAPTER 7\nRefactoring toward valuable unit tests\n7.2.5\nTake 4: Introducing a new Company class\nLook at this code in the controller once again:\nobject[] companyData = _database.GetCompany();\nstring companyDomainName = (string)companyData[0];\nint numberOfEmployees = (int)companyData[1];\nint newNumberOfEmployees = user.ChangeEmail(\nnewEmail, companyDomainName, numberOfEmployees);\nThe awkwardness of returning an updated number of employees from User is a sign\nof a misplaced responsibility, which itself is a sign of a missing abstraction. To fix this,\nwe need to introduce another domain class, Company, that bundles the company-\nrelated logic and data together, as shown in the following listing.\npublic class Company\n{\npublic string DomainName { get; private set; }\npublic int NumberOfEmployees { get; private set; }\npublic void ChangeNumberOfEmployees(int delta)\n{\nPrecondition.Requires(NumberOfEmployees + delta >= 0);\nNumberOfEmployees += delta;\n}\npublic bool IsEmailCorporate(string email)\n{\nstring emailDomain = email.Split('@')[1];\nreturn emailDomain == DomainName;\n}\n}\nHow is the reconstruction logic complex?\nHow is the reconstruction logic complex, given that there’s only a single branching\npoint in the UserFactory.Create() method? As I mentioned in chapter 1, there\ncould be a lot of hidden branching points in the underlying libraries used by the code\nand thus a lot of potential for something to go wrong. This is exactly the case for the\nUserFactory.Create() method.\nReferring to an array element by index (data[0]) entails an internal decision made\nby the .NET Framework as to what data element to access. The same is true for the\nconversion from object to int or string. Internally, the .NET Framework decides\nwhether to throw a cast exception or allow the conversion to proceed. All these hid-\nden branches make the reconstruction logic test-worthy, despite the lack of decision\npoints in it. \nListing 7.4\nThe new class in the domain layer\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n165\nRefactoring toward valuable unit tests\nThere are two methods in this class: ChangeNumberOfEmployees() and IsEmail-\nCorporate(). These methods help adhere to the tell-don’t-ask principle I mentioned\nin chapter 5. This principle advocates for bundling together data and operations on\nthat data. A User instance will tell the company to change its number of employees or\nfigure out whether a particular email is corporate; it won’t ask for the raw data and do\neverything on its own.\n There’s also a new CompanyFactory class, which is responsible for the reconstruc-\ntion of Company objects, similar to UserFactory. This is how the controller now looks.\npublic class UserController\n{\nprivate readonly Database _database = new Database();\nprivate readonly MessageBus _messageBus = new MessageBus();\npublic void ChangeEmail(int userId, string newEmail)\n{\nobject[] userData = _database.GetUserById(userId);\nUser user = UserFactory.Create(userData);\nobject[] companyData = _database.GetCompany();\nCompany company = CompanyFactory.Create(companyData);\nuser.ChangeEmail(newEmail, company);\n_database.SaveCompany(company);\n_database.SaveUser(user);\n_messageBus.SendEmailChangedMessage(userId, newEmail);\n}\n}\nAnd here’s the User class.\npublic class User\n{\npublic int UserId { get; private set; }\npublic string Email { get; private set; }\npublic UserType Type { get; private set; }\npublic void ChangeEmail(string newEmail, Company company)\n{\nif (Email == newEmail)\nreturn;\nUserType newType = company.IsEmailCorporate(newEmail)\n? UserType.Employee\n: UserType.Customer;\nListing 7.5\nController after refactoring \nListing 7.6\nUser after refactoring \nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n166\nCHAPTER 7\nRefactoring toward valuable unit tests\nif (Type != newType)\n{\nint delta = newType == UserType.Employee ? 1 : -1;\ncompany.ChangeNumberOfEmployees(delta);\n}\nEmail = newEmail;\nType = newType;\n}\n}\nNotice how the removal of the misplaced responsibility made User much cleaner.\nInstead of operating on company data, it accepts a Company instance and delegates\ntwo important pieces of work to that instance: determining whether an email is corpo-\nrate and changing the number of employees in the company.\n Figure 7.9 shows where each class stands in the diagram. The factories and both\ndomain classes reside in the domain model and algorithms quadrant. User has moved\nto the right because it now has one collaborator, Company, whereas previously it had\nnone. That has made User less testable, but not much.\nUserController now firmly stands in the controllers quadrant because all of its com-\nplexity has moved to the factories. The only thing this class is responsible for is gluing\ntogether all the collaborating parties.\nDomain model,\nalgorithms\nOvercomplicated\ncode\nTrivial code\nControllers\nComplexity,\ndomain\nsigniﬁcance\nNumber of\ncollaborators\nUserController\nUser\nCompany,\nUserFactory,\nCompanyFactory\nFigure 7.9\nUser has shifted to the right because it now has the Company \ncollaborator. UserController firmly stands in the controllers quadrant; all \nits complexity has moved to the factories.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n167\nAnalysis of optimal unit test coverage\n Note the similarities between this implementation and the functional architecture\nfrom the previous chapter. Neither the functional core in the audit system nor the\ndomain layer in this CRM (the User and Company classes) communicates with out-of-\nprocess dependencies. In both implementations, the application services layer is\nresponsible for such communication: it gets the raw data from the filesystem or from\nthe database, passes that data to stateless algorithms or the domain model, and then\npersists the results back to the data storage.\n The difference between the two implementations is in their treatment of side\neffects. The functional core doesn’t incur any side effects whatsoever. The CRM’s\ndomain model does, but all those side effects remain inside the domain model in the\nform of the changed user email and the number of employees. The side effects only\ncross the domain model’s boundary when the controller persists the User and Company\nobjects in the database.\n The fact that all side effects are contained in memory until the very last moment\nimproves testability a lot. Your tests don’t need to examine out-of-process dependen-\ncies, nor do they need to resort to communication-based testing. All the verification\ncan be done using output-based and state-based testing of objects in memory. \n7.3\nAnalysis of optimal unit test coverage\nNow that we’ve completed the refactoring with the help of the Humble Object pat-\ntern, let’s analyze which parts of the project fall into which code category and how\nthose parts should be tested. Table 7.1 shows all the code from the sample project\ngrouped by position in the types-of-code diagram.\nWith the full separation of business logic and orchestration at hand, it’s easy to decide\nwhich parts of the code base to unit test.\n7.3.1\nTesting the domain layer and utility code\nTesting methods in the top-left quadrant in table 7.1 provides the best results in cost-\nbenefit terms. The code’s high complexity or domain significance guarantees great\nprotection against regressions, while having few collaborators ensures the lowest mainte-\nnance costs. This is an example of how User could be tested:\nTable 7.1\nTypes of code in the sample project after refactoring using the Humble Object pattern\nFew collaborators\nMany collaborators\nHigh complexity or \ndomain significance\nChangeEmail(newEmail, company) in User;\nChangeNumberOfEmployees(delta) and \nIsEmailCorporate(email) in Company; \nand Create(data) in UserFactory and \nCompanyFactory\nLow complexity and \ndomain significance\nConstructors in User and Company\nChangeEmail(userId, \nnewEmail) in \nUserController\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "page_number": 179
    },
    {
      "number": 21,
      "title": "Segment 21 (pages 190-200)",
      "start_page": 190,
      "end_page": 200,
      "detection_method": "topic_boundary",
      "content": "168\nCHAPTER 7\nRefactoring toward valuable unit tests\n[Fact]\npublic void Changing_email_from_non_corporate_to_corporate()\n{\nvar company = new Company(\"mycorp.com\", 1);\nvar sut = new User(1, \"user@gmail.com\", UserType.Customer);\nsut.ChangeEmail(\"new@mycorp.com\", company);\nAssert.Equal(2, company.NumberOfEmployees);\nAssert.Equal(\"new@mycorp.com\", sut.Email);\nAssert.Equal(UserType.Employee, sut.Type);\n}\nTo achieve full coverage, you’d need another three such tests:\npublic void Changing_email_from_corporate_to_non_corporate()\npublic void Changing_email_without_changing_user_type()\npublic void Changing_email_to_the_same_one()\nTests for the other three classes would be even shorter, and you could use parameter-\nized tests to group several test cases together:\n[InlineData(\"mycorp.com\", \"email@mycorp.com\", true)]\n[InlineData(\"mycorp.com\", \"email@gmail.com\", false)]\n[Theory]\npublic void Differentiates_a_corporate_email_from_non_corporate(\nstring domain, string email, bool expectedResult)\n{\nvar sut = new Company(domain, 0);\nbool isEmailCorporate = sut.IsEmailCorporate(email);\nAssert.Equal(expectedResult, isEmailCorporate);\n}\n7.3.2\nTesting the code from the other three quadrants\nCode with low complexity and few collaborators (bottom-left quadrant in table 7.1) is\nrepresented by the constructors in User and Company, such as\npublic User(int userId, string email, UserType type)\n{\nUserId = userId;\nEmail = email;\nType = type;\n}\nThese constructors are trivial and aren’t worth the effort. The resulting tests wouldn’t\nprovide great enough protection against regressions.\n The refactoring has eliminated all code with high complexity and a large number\nof collaborators (top-right quadrant in table 7.1), so we have nothing to test there,\neither. As for the controllers quadrant (bottom-right in table 7.1), we’ll discuss testing\nit in the next chapter. \nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n169\nHandling conditional logic in controllers\n7.3.3\nShould you test preconditions?\nLet’s take a look at a special kind of branching points—preconditions—and see whether\nyou should test them. For example, look at this method from Company once again:\npublic void ChangeNumberOfEmployees(int delta)\n{\nPrecondition.Requires(NumberOfEmployees + delta >= 0);\nNumberOfEmployees += delta;\n}\nIt has a precondition stating that the number of employees in the company should\nnever become negative. This precondition is a safeguard that’s activated only in\nexceptional cases. Such exceptional cases are usually the result of bugs. The only pos-\nsible reason for the number of employees to go below zero is if there’s an error in\ncode. The safeguard provides a mechanism for your software to fail fast and to prevent\nthe error from spreading and being persisted in the database, where it would be much\nharder to deal with. Should you test such preconditions? In other words, would such\ntests be valuable enough to have in the test suite?\n There’s no hard rule here, but the general guideline I recommend is to test all pre-\nconditions that have domain significance. The requirement for the non-negative\nnumber of employees is such a precondition. It’s part of the Company class’s invariants:\nconditions that should be held true at all times. But don’t spend time testing precon-\nditions that don’t have domain significance. For example, UserFactory has the follow-\ning safeguard in its Create method:\npublic static User Create(object[] data)\n{\nPrecondition.Requires(data.Length >= 3);\n/* Extract id, email, and type out of data */\n}\nThere’s no domain meaning to this precondition and therefore not much value in\ntesting it. \n7.4\nHandling conditional logic in controllers\nHandling conditional logic and simultaneously maintaining the domain layer free of\nout-of-process collaborators is often tricky and involves trade-offs. In this section, I’ll\nshow what those trade-offs are and how to decide which of them to choose in your\nown project.\n The separation between business logic and orchestration works best when a busi-\nness operation has three distinct stages:\nRetrieving data from storage\nExecuting business logic\nPersisting data back to the storage (figure 7.10)\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n170\nCHAPTER 7\nRefactoring toward valuable unit tests\nThere are a lot of situations where these stages aren’t as clearcut, though. As we discussed\nin chapter 6, you might need to query additional data from an out-of-process depen-\ndency based on an intermediate result of the decision-making process (figure 7.11). Writ-\ning to the out-of-process dependency often depends on that result, too.\nAs also discussed in the previous chapter, you have three options in such a situation:\nPush all external reads and writes to the edges anyway. This approach preserves the\nread-decide-act structure but concedes performance: the controller will call\nout-of-process dependencies even when there’s no need for that.\nInject the out-of-process dependencies into the domain model and allow the business\nlogic to directly decide when to call those dependencies.\nSplit the decision-making process into more granular steps and have the controller act\non each of those steps separately.\nRead\nInvoke\nWrite\nOut-of-process\ndependencies:\nﬁlesystem,\ndatabase, etc.\nApplication\nservice\n(controller)\nBusiness logic\n(domain\nmodel)\nFigure 7.10\nHexagonal and functional architectures work best when all \nreferences to out-of-process dependencies can be pushed to the edges of \nbusiness operations.\nRead\nRead\nInvoke 1\nWrite\nOut-of-process\ndependencies:\nﬁlesystem,\ndatabase, etc.\nApplication\nservice\n(controller)\nBusiness logic\n(domain\nmodel)\nInvoke 2\nFigure 7.11\nA hexagonal architecture doesn’t work as well when you need to refer to \nout-of-process dependencies in the middle of the business operation.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n171\nHandling conditional logic in controllers\nThe challenge is to balance the following three attributes:\nDomain model testability, which is a function of the number and type of collabora-\ntors in domain classes\nController simplicity, which depends on the presence of decision-making (branch-\ning) points in the controller\nPerformance, as defined by the number of calls to out-of-process dependencies\nEach option only gives you two out of the three attributes (figure 7.12):\nPushing all external reads and writes to the edges of a business operation—Preserves\ncontroller simplicity and keeps the domain model isolated from out-of-process\ndependencies (thus allowing it to remain testable) but concedes performance.\nInjecting out-of-process dependencies into the domain model—Keeps performance and\nthe controller’s simplicity intact but damages domain model testability.\nSplitting the decision-making process into more granular steps—Helps with both per-\nformance and domain model testability but concedes controller simplicity.\nYou’ll need to introduce decision-making points in the controller in order to\nmanage these granular steps.\nIn most software projects, performance is important, so the first approach (pushing\nexternal reads and writes to the edges of a business operation) is out of the question.\nThe second option (injecting out-of-process dependencies into the domain model)\nbrings most of your code into the overcomplicated quadrant on the types-of-code dia-\ngram. This is exactly what we refactored the initial CRM implementation away from. I\nrecommend that you avoid this approach: such code no longer preserves the separation\nDomain model\ntestability\nPerformance\nPushing all external reads\nand writes to the edges of\nthe business operation\nInjecting out-of-process\ndependencies into the\ndomain model\nSplitting the decision-making\nprocess into more granular steps\nController simplicity\nFigure 7.12\nThere’s no single solution that satisfies all three attributes: controller simplicity, \ndomain model testability, and performance. You have to choose two out of the three.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n172\nCHAPTER 7\nRefactoring toward valuable unit tests\nbetween business logic and communication with out-of-process dependencies and\nthus becomes much harder to test and maintain.\n That leaves you with the third option: splitting the decision-making process into\nsmaller steps. With this approach, you will have to make your controllers more com-\nplex, which will also push them closer to the overcomplicated quadrant. But there are\nways to mitigate this problem. Although you will rarely be able to factor all the com-\nplexity out of controllers as we did previously in the sample project, you can keep that\ncomplexity manageable.\n7.4.1\nUsing the CanExecute/Execute pattern\nThe first way to mitigate the growth of the controllers’ complexity is to use the Can-\nExecute/Execute pattern, which helps avoid leaking of business logic from the\ndomain model to controllers. This pattern is best explained with an example, so let’s\nexpand on our sample project.\n Let’s say that a user can change their email only until they confirm it. If a user tries\nto change the email after the confirmation, they should be shown an error message.\nTo accommodate this new requirement, we’ll add a new property to the User class.\npublic class User\n{\npublic int UserId { get; private set; }\npublic string Email { get; private set; }\npublic UserType Type { get; private set; }\npublic bool IsEmailConfirmed               \n{ get; private set; }\n               \n/* ChangeEmail(newEmail, company) method */\n}\nThere are two options for where to put this check. First, you could put it in User’s\nChangeEmail method:\npublic string ChangeEmail(string newEmail, Company company)\n{\nif (IsEmailConfirmed)\nreturn \"Can't change a confirmed email\";\n/* the rest of the method */\n}\nThen you could make the controller either return an error or incur all necessary side\neffects, depending on this method’s output.\npublic string ChangeEmail(int userId, string newEmail)\n{\nListing 7.7\nUser with a new property\nListing 7.8\nThe controller, still stripped of all decision-making\nNew property\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n173\nHandling conditional logic in controllers\nobject[] userData = _database.GetUserById(userId);\n  \nUser user = UserFactory.Create(userData);\n  \nobject[] companyData = _database.GetCompany();\n  \nCompany company = CompanyFactory.Create(companyData);   \nstring error = user.ChangeEmail(newEmail, company);\n     \nif (error != null)\n    \nreturn error;\n     \n_database.SaveCompany(company);\n      \n_database.SaveUser(user);\n \n_messageBus.SendEmailChangedMessage(userId, newEmail); \nreturn \"OK\";\n \n}\nThis implementation keeps the controller free of decision-making, but it does so at\nthe expense of a performance drawback. The Company instance is retrieved from the\ndatabase unconditionally, even when the email is confirmed and thus can’t be changed.\nThis is an example of pushing all external reads and writes to the edges of a business\noperation.\nNOTE\nI don’t consider the new if statement analyzing the error string an\nincrease in complexity because it belongs to the acting phase; it’s not part of\nthe decision-making process. All the decisions are made by the User class, and\nthe controller merely acts on those decisions.\nThe second option is to move the check for IsEmailConfirmed from User to the\ncontroller.\npublic string ChangeEmail(int userId, string newEmail)\n{\nobject[] userData = _database.GetUserById(userId);\nUser user = UserFactory.Create(userData);\nif (user.IsEmailConfirmed)\n   \nreturn \"Can't change a confirmed email\";  \nobject[] companyData = _database.GetCompany();\nCompany company = CompanyFactory.Create(companyData);\nuser.ChangeEmail(newEmail, company);\n_database.SaveCompany(company);\n_database.SaveUser(user);\n_messageBus.SendEmailChangedMessage(userId, newEmail);\nreturn \"OK\";\n}\nListing 7.9\nController deciding whether to change the user’s email\nPrepares \nthe data\nMakes a\ndecision\nActs on the \ndecision\nDecision-making \nmoved here from User.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n174\nCHAPTER 7\nRefactoring toward valuable unit tests\nWith this implementation, the performance stays intact: the Company instance is\nretrieved from the database only after it is certain that the email can be changed. But\nnow the decision-making process is split into two parts:\nWhether to proceed with the change of email (performed by the controller)\nWhat to do during that change (performed by User)\nNow it’s also possible to change the email without verifying the IsEmailConfirmed\nflag first, which diminishes the domain model’s encapsulation. Such fragmentation\nhinders the separation between business logic and orchestration and moves the con-\ntroller closer to the overcomplicated danger zone.\n To prevent this fragmentation, you can introduce a new method in User, CanChange-\nEmail(), and make its successful execution a precondition for changing an email. The\nmodified version in the following listing follows the CanExecute/Execute pattern.\npublic string CanChangeEmail()\n{\nif (IsEmailConfirmed)\nreturn \"Can't change a confirmed email\";\nreturn null;\n}\npublic void ChangeEmail(string newEmail, Company company)\n{\nPrecondition.Requires(CanChangeEmail() == null);\n/* the rest of the method */\n}\nThis approach provides two important benefits:\nThe controller no longer needs to know anything about the process of chang-\ning emails. All it needs to do is call the CanChangeEmail() method to see if the\noperation can be done. Notice that this method can contain multiple valida-\ntions, all encapsulated away from the controller.\nThe additional precondition in ChangeEmail() guarantees that the email won’t\never be changed without checking for the confirmation first.\nThis pattern helps you to consolidate all decisions in the domain layer. The controller\nno longer has an option not to check for the email confirmation, which essentially\neliminates the new decision-making point from that controller. Thus, although the\ncontroller still contains the if statement calling CanChangeEmail(), you don’t need to\ntest that if statement. Unit testing the precondition in the User class itself is enough.\nNOTE\nFor simplicity’s sake, I’m using a string to denote an error. In a real-\nworld project, you may want to introduce a custom Result class to indicate\nthe success or failure of an operation. \nListing 7.10\nChanging an email using the CanExecute/Execute pattern\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n175\nHandling conditional logic in controllers\n7.4.2\nUsing domain events to track changes in the domain model\nIt’s sometimes hard to deduct what steps led the domain model to the current state.\nStill, it might be important to know these steps because you need to inform external\nsystems about what exactly has happened in your application. Putting this responsibil-\nity on the controllers would make them more complicated. To avoid that, you can\ntrack important changes in the domain model and then convert those changes into\ncalls to out-of-process dependencies after the business operation is complete. Domain\nevents help you implement such tracking.\nDEFINITION\nA domain event describes an event in the application that is mean-\ningful to domain experts. The meaningfulness for domain experts is what\ndifferentiates domain events from regular events (such as button clicks).\nDomain events are often used to inform external applications about import-\nant changes that have happened in your system.\nOur CRM has a tracking requirement, too: it has to notify external systems about\nchanged user emails by sending messages to the message bus. The current implemen-\ntation has a flaw in the notification functionality: it sends messages even when the\nemail is not changed, as shown in the following listing.\n// User\npublic void ChangeEmail(string newEmail, Company company)\n{\nPrecondition.Requires(CanChangeEmail() == null);\nif (Email == newEmail)   \nreturn;\n/* the rest of the method */\n}\n// Controller\npublic string ChangeEmail(int userId, string newEmail)\n{\n/* preparations */\nuser.ChangeEmail(newEmail, company);\n_database.SaveCompany(company);\n_database.SaveUser(user);\n_messageBus.SendEmailChangedMessage(  \nuserId, newEmail);\n  \nreturn \"OK\";\n}\nYou could resolve this bug by moving the check for email sameness to the controller,\nbut then again, there are issues with the business logic fragmentation. And you can’t\nListing 7.11\nSends a notification even when the email has not changed\nUser email may \nnot change.\nThe controller sends \na message anyway.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n176\nCHAPTER 7\nRefactoring toward valuable unit tests\nput this check to CanChangeEmail() because the application shouldn’t return an\nerror if the new email is the same as the old one.\n Note that this particular check probably doesn’t introduce too much business logic\nfragmentation, so I personally wouldn’t consider the controller overcomplicated if it\ncontained that check. But you may find yourself in a more difficult situation in which\nit’s hard to prevent your application from making unnecessary calls to out-of-process\ndependencies without passing those dependencies to the domain model, thus over-\ncomplicating that domain model. The only way to prevent such overcomplication is\nthe use of domain events.\n From an implementation standpoint, a domain event is a class that contains data\nneeded to notify external systems. In our specific example, it is the user’s ID and\nemail:\npublic class EmailChangedEvent\n{\npublic int UserId { get; }\npublic string NewEmail { get; }\n}\nNOTE\nDomain events should always be named in the past tense because they\nrepresent things that already happened. Domain events are values—they are\nimmutable and interchangeable.\nUser will have a collection of such events to which it will add a new element when the\nemail changes. This is how its ChangeEmail() method looks after the refactoring.\npublic void ChangeEmail(string newEmail, Company company)\n{\nPrecondition.Requires(CanChangeEmail() == null);\nif (Email == newEmail)\nreturn;\nUserType newType = company.IsEmailCorporate(newEmail)\n? UserType.Employee\n: UserType.Customer;\nif (Type != newType)\n{\nint delta = newType == UserType.Employee ? 1 : -1;\ncompany.ChangeNumberOfEmployees(delta);\n}\nEmail = newEmail;\nType = newType;\nEmailChangedEvents.Add(\n  \nnew EmailChangedEvent(UserId, newEmail));  \n}\nListing 7.12\nUser adding an event when the email changes\nA new event indicates \nthe change of email.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n177\nHandling conditional logic in controllers\nThe controller then will convert the events into messages on the bus.\npublic string ChangeEmail(int userId, string newEmail)\n{\nobject[] userData = _database.GetUserById(userId);\nUser user = UserFactory.Create(userData);\nstring error = user.CanChangeEmail();\nif (error != null)\nreturn error;\nobject[] companyData = _database.GetCompany();\nCompany company = CompanyFactory.Create(companyData);\nuser.ChangeEmail(newEmail, company);\n_database.SaveCompany(company);\n_database.SaveUser(user);\nforeach (var ev in user.EmailChangedEvents)  \n{\n  \n_messageBus.SendEmailChangedMessage(\n  \nev.UserId, ev.NewEmail);\n  \n}\n  \nreturn \"OK\";\n}\nNotice that the Company and User instances are still persisted in the database uncondi-\ntionally: the persistence logic doesn’t depend on domain events. This is due to the dif-\nference between changes in the database and messages in the bus.\n Assuming that no application has access to the database other than the CRM, com-\nmunications with that database are not part of the CRM’s observable behavior—they\nare implementation details. As long as the final state of the database is correct, it\ndoesn’t matter how many calls your application makes to that database. On the other\nhand, communications with the message bus are part of the application’s observable\nbehavior. In order to maintain the contract with external systems, the CRM should put\nmessages on the bus only when the email changes.\n There are performance implications to persisting data in the database uncondi-\ntionally, but they are relatively insignificant. The chances that after all the validations\nthe new email is the same as the old one are quite small. The use of an ORM can also\nhelp. Most ORMs won’t make a round trip to the database if there are no changes to\nthe object state.\n You can generalize the solution with domain events: extract a DomainEvent base\nclass and introduce a base class for all domain classes, which would contain a collec-\ntion of such events: List<DomainEvent> events. You can also write a separate event\ndispatcher instead of dispatching domain events manually in controllers. Finally, in\nlarger projects, you might need a mechanism for merging domain events before\nListing 7.13\nThe controller processing domain events\nDomain event \nprocessing\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n178\nCHAPTER 7\nRefactoring toward valuable unit tests\ndispatching them. That topic is outside the scope of this book, though. You can read\nabout it in my article “Merging domain events before dispatching” at http://mng\n.bz/YeVe.\n Domain events remove the decision-making responsibility from the controller and\nput that responsibility into the domain model, thus simplifying unit testing communi-\ncations with external systems. Instead of verifying the controller itself and using mocks\nto substitute out-of-process dependencies, you can test the domain event creation\ndirectly in unit tests, as shown next.\n[Fact]\npublic void Changing_email_from_corporate_to_non_corporate()\n{\nvar company = new Company(\"mycorp.com\", 1);\nvar sut = new User(1, \"user@mycorp.com\", UserType.Employee, false);\nsut.ChangeEmail(\"new@gmail.com\", company);\ncompany.NumberOfEmployees.Should().Be(0);\nsut.Email.Should().Be(\"new@gmail.com\");\nsut.Type.Should().Be(UserType.Customer);\nsut.EmailChangedEvents.Should().Equal(\n   \nnew EmailChangedEvent(1, \"new@gmail.com\"));  \n}\nOf course, you’ll still need to test the controller to make sure it does the orchestration\ncorrectly, but doing so requires a much smaller set of tests. That’s the topic of the next\nchapter. \n7.5\nConclusion\nNotice a theme that has been present throughout this chapter: abstracting away the\napplication of side effects to external systems. You achieve such abstraction by keeping\nthose side effects in memory until the very end of the business operation, so that they\ncan be tested with plain unit tests without involving out-of-process dependencies.\nDomain events are abstractions on top of upcoming messages in the bus. Changes in\ndomain classes are abstractions on top of upcoming modifications in the database.\nNOTE\nIt’s easier to test abstractions than the things they abstract.\nAlthough we were able to successfully contain all the decision-making in the domain\nmodel with the help of domain events and the CanExecute/Execute pattern, you\nwon’t be able to always do that. There are situations where business logic fragmenta-\ntion is inevitable.\n For example, there’s no way to verify email uniqueness outside the controller with-\nout introducing out-of-process dependencies in the domain model. Another example\nis failures in out-of-process dependencies that should alter the course of the business\nListing 7.14\nTesting the creation of a domain event\nSimultaneously asserts \nthe collection size and the \nelement in the collection\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "page_number": 190
    },
    {
      "number": 22,
      "title": "Segment 22 (pages 201-210)",
      "start_page": 201,
      "end_page": 210,
      "detection_method": "topic_boundary",
      "content": "179\nConclusion\noperation. The decision about which way to go can’t reside in the domain layer\nbecause it’s not the domain layer that calls those out-of-process dependencies. You will\nhave to put this logic into controllers and then cover it with integration tests. Still,\neven with the potential fragmentation, there’s a lot of value in separating business\nlogic from orchestration because this separation drastically simplifies the unit test-\ning process.\n Just as you can’t avoid having some business logic in controllers, you will rarely be\nable to remove all collaborators from domain classes. And that’s fine. One, two, or\neven three collaborators won’t turn a domain class into overcomplicated code, as long\nas these collaborators don’t refer to out-of-process dependencies.\n Don’t use mocks to verify interactions with such collaborators, though. These\ninteractions have nothing to do with the domain model’s observable behavior. Only\nthe very first call, which goes from a controller to a domain class, has an immediate\nconnection to that controller’s goal. All the subsequent calls the domain class\nmakes to its neighbor domain classes within the same operation are implementa-\ntion details.\n Figure 7.13 illustrates this idea. It shows the communications between components\nin the CRM and their relationship to observable behavior. As you may remember from\nchapter 5, whether a method is part of the class’s observable behavior depends on\nwhom the client is and what the goals of that client are. To be part of the observable\nbehavior, the method must meet one of the following two criteria:\nHave an immediate connection to one of the client’s goals\nIncur a side effect in an out-of-process dependency that is visible to external\napplications\nThe controller’s ChangeEmail() method is part of its observable behavior, and so is\nthe call it makes to the message bus. The first method is the entry point for the exter-\nnal client, thereby meeting the first criterion. The call to the bus sends messages to\nexternal applications, thereby meeting the second criterion. You should verify both of\nExternal client\nApplication\nservice\n(controller)\nMessage bus\nUser\nCompany\nObservable behavior\nfor external client\nObservable behavior\nfor controller\nObservable\nbehavior for user\nFigure 7.13\nA map that shows communications among components in the CRM and the \nrelationship between these communications and observable behavior\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n180\nCHAPTER 7\nRefactoring toward valuable unit tests\nthese method calls (which is the topic of the next chapter). However, the subsequent\ncall from the controller to User doesn’t have an immediate connection to the goals of\nthe external client. That client doesn’t care how the controller decides to implement\nthe change of email as long as the final state of the system is correct and the call to the\nmessage bus is in place. Therefore, you shouldn’t verify calls the controller makes to\nUser when testing that controller’s behavior.\n When you step one level down the call stack, you get a similar situation. Now it’s\nthe controller who is the client, and the ChangeEmail method in User has an immedi-\nate connection to that client’s goal of changing the user email and thus should be\ntested. But the subsequent calls from User to Company are implementation details\nfrom the controller’s point of view. Therefore, the test that covers the ChangeEmail\nmethod in User shouldn’t verify what methods User calls on Company. The same line\nof reasoning applies when you step one more level down and test the two methods in\nCompany from User’s point of view.\n Think of the observable behavior and implementation details as onion layers. Test\neach layer from the outer layer’s point of view, and disregard how that layer talks to\nthe underlying layers. As you peel these layers one by one, you switch perspective:\nwhat previously was an implementation detail now becomes an observable behavior,\nwhich you then cover with another set of tests. \nSummary\nCode complexity is defined by the number of decision-making points in the\ncode, both explicit (made by the code itself) and implicit (made by the libraries\nthe code uses).\nDomain significance shows how significant the code is for the problem domain\nof your project. Complex code often has high domain significance and vice\nversa, but not in 100% of all cases.\nComplex code and code that has domain significance benefit from unit test-\ning the most because the corresponding tests have greater protection against\nregressions.\nUnit tests that cover code with a large number of collaborators have high\nmaintenance costs. Such tests require a lot of space to bring collaborators to\nan expected condition and then check their state or interactions with them\nafterward.\nAll production code can be categorized into four types of code by its complexity\nor domain significance and the number of collaborators:\n– Domain model and algorithms (high complexity or domain significance, few\ncollaborators) provide the best return on unit testing efforts.\n– Trivial code (low complexity and domain significance, few collaborators)\nisn’t worth testing at all.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n181\nSummary\n– Controllers (low complexity and domain significance, large number of col-\nlaborators) should be tested briefly by integration tests.\n– Overcomplicated code (high complexity or domain significance, large num-\nber of collaborators) should be split into controllers and complex code.\nThe more important or complex the code is, the fewer collaborators it should\nhave.\nThe Humble Object pattern helps make overcomplicated code testable by\nextracting business logic out of that code into a separate class. As a result, the\nremaining code becomes a controller—a thin, humble wrapper around the busi-\nness logic.\nThe hexagonal and functional architectures implement the Humble Object\npattern. Hexagonal architecture advocates for the separation of business logic and\ncommunications with out-of-process dependencies. Functional architecture sepa-\nrates business logic from communications with all collaborators, not just out-of-\nprocess ones.\nThink of the business logic and orchestration responsibilities in terms of code\ndepth versus code width. Your code can be either deep (complex or important)\nor wide (work with many collaborators), but never both.\nTest preconditions if they have a domain significance; don’t test them otherwise.\nThere are three important attributes when it comes to separating business logic\nfrom orchestration:\n– Domain model testability—A function of the number and the type of collabora-\ntors in domain classes\n– Controller simplicity—Depends on the presence of decision-making points in\nthe controller\n– Performance—Defined by the number of calls to out-of-process dependencies\nYou can have a maximum of two of these three attributes at any given moment:\n– Pushing all external reads and writes to the edges of a business operation—Preserves\ncontroller simplicity and keeps the domain model testability, but concedes\nperformance\n– Injecting out-of-process dependencies into the domain model—Keeps performance\nand the controller’s simplicity, but damages domain model testability\n– Splitting the decision-making process into more granular steps—Preserves perfor-\nmance and domain model testability, but gives up controller simplicity\nSplitting the decision-making process into more granular steps—Is a trade-off with the\nbest set of pros and cons. You can mitigate the growth of controller complexity\nusing the following two patterns:\n– The CanExecute/Execute pattern introduces a CanDo() for each Do() method\nand makes its successful execution a precondition for Do(). This pattern\nessentially eliminates the controller’s decision-making because there’s no\noption not to call CanDo() before Do().\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n182\nCHAPTER 7\nRefactoring toward valuable unit tests\n– Domain events help track important changes in the domain model, and then\nconvert those changes to calls to out-of-process dependencies. This pattern\nremoves the tracking responsibility from the controller.\nIt’s easier to test abstractions than the things they abstract. Domain events are\nabstractions on top of upcoming calls to out-of-process dependencies. Changes\nin domain classes are abstractions on top of upcoming modifications in the\ndata storage.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\nPart 3\nIntegration testing\nHave you ever been in a situation where all the unit tests pass but the\napplication still doesn’t work? Validating software components in isolation from\neach other is important, but it’s equally important to check how those compo-\nnents work in integration with external systems. This is where integration testing\ncomes into play.\n In chapter 8, we’ll look at integration testing in general and revisit the Test\nPyramid concept. You’ll learn the trade-offs inherent to integration testing and\nhow to navigate them. Chapters 9 and 10 will then discuss more specific topics.\nChapter 9 will teach you how to get the most out of your mocks. Chapter 10 is a\ndeep dive into working with relational databases in tests.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n \nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n185\nWhy integration testing?\nYou can never be sure your system works as a whole if you rely on unit tests exclu-\nsively. Unit tests are great at verifying business logic, but it’s not enough to check\nthat logic in a vacuum. You have to validate how different parts of it integrate with\neach other and external systems: the database, the message bus, and so on.\n In this chapter, you’ll learn the role of integration tests: when you should apply\nthem and when it’s better to rely on plain old unit tests or even other techniques\nsuch as the Fail Fast principle. You will see which out-of-process dependencies to\nuse as-is in integration tests and which to replace with mocks. You will also see inte-\ngration testing best practices that will help improve the health of your code base in\ngeneral: making domain model boundaries explicit, reducing the number of layers\nin the application, and eliminating circular dependencies. Finally, you’ll learn why\ninterfaces with a single implementation should be used sporadically, and how and\nwhen to test logging functionality.\nThis chapter covers\nUnderstanding the role of integration testing\nDiving deeper into the Test Pyramid concept\nWriting valuable integration tests\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n186\nCHAPTER 8\nWhy integration testing?\n8.1\nWhat is an integration test?\nIntegration tests play an important role in your test suite. It’s also crucial to balance\nthe number of unit and integration tests. You will see shortly what that role is and how\nto maintain the balance, but first, let me give you a refresher on what differentiates an\nintegration test from a unit test.\n8.1.1\nThe role of integration tests\nAs you may remember from chapter 2, a unit test is a test that meets the following three\nrequirements:\nVerifies a single unit of behavior,\nDoes it quickly,\nAnd does it in isolation from other tests.\nA test that doesn’t meet at least one of these three requirements falls into the category\nof integration tests. An integration test then is any test that is not a unit test.\n In practice, integration tests almost always verify how your system works in integra-\ntion with out-of-process dependencies. In other words, these tests cover the code from\nthe controllers quadrant (see chapter 7 for more details about code quadrants). The\ndiagram in figure 8.1 shows the typical responsibilities of unit and integration tests.\nUnit tests cover the domain model, while integration tests check the code that glues\nthat domain model with out-of-process dependencies.\nDomain model,\nalgorithms\nOvercomplicated\ncode\nTrivial code\nControllers\nComplexity,\ndomain\nsigniﬁcance\nNumber of\ncollaborators\nIntegration\ntests\nUnit tests\nFigure 8.1\nIntegration tests cover controllers, while unit tests cover the domain \nmodel and algorithms. Trivial and overcomplicated code shouldn’t be tested at all.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n187\nWhat is an integration test?\nNote that tests covering the controllers quadrant can sometimes be unit tests too. If all\nout-of-process dependencies are replaced with mocks, there will be no dependencies\nshared between tests, which will allow those tests to remain fast and maintain their iso-\nlation from each other. Most applications do have an out-of-process dependency that\ncan’t be replaced with a mock, though. It’s usually a database—a dependency that is\nnot visible to other applications.\n As you may also remember from chapter 7, the other two quadrants from figure 8.1\n(trivial code and overcomplicated code) shouldn’t be tested at all. Trivial code isn’t\nworth the effort, while overcomplicated code should be refactored into algorithms\nand controllers. Thus, all your tests must focus on the domain model and the control-\nlers quadrants exclusively. \n8.1.2\nThe Test Pyramid revisited\nIt’s important to maintain a balance between unit and integration tests. Working\ndirectly with out-of-process dependencies makes integration tests slow. Such tests are\nalso more expensive to maintain. The increase in maintainability costs is due to\nThe necessity to keep the out-of-process dependencies operational\nThe greater number of collaborators involved, which inflates the test’s size\nOn the other hand, integration tests go through a larger amount of code (both your\ncode and the code of the libraries used by the application), which makes them better\nthan unit tests at protecting against regressions. They are also more detached from\nthe production code and therefore have better resistance to refactoring.\n The ratio between unit and integration tests can differ depending on the project’s\nspecifics, but the general rule of thumb is the following: check as many of the business\nscenario’s edge cases as possible with unit tests; use integration tests to cover one\nhappy path, as well as any edge cases that can’t be covered by unit tests.\nDEFINITION\nA happy path is a successful execution of a business scenario. An\nedge case is when the business scenario execution results in an error.\nShifting the majority of the workload to unit tests helps keep maintenance costs low.\nAt the same time, having one or two overarching integration tests per business sce-\nnario ensures the correctness of your system as a whole. This guideline forms the pyr-\namid-like ratio between unit and integration tests, as shown in figure 8.2 (as discussed\nin chapter 2, end-to-end tests are a subset of integration tests).\n The Test Pyramid can take different shapes depending on the project’s complexity.\nSimple applications have little (if any) code in the domain model and algorithms\nquadrant. As a result, tests form a rectangle instead of a pyramid, with an equal num-\nber of unit and integration tests (figure 8.3). In the most trivial cases, you might have\nno unit tests whatsoever.\n Note that integration tests retain their value even in simple applications. Regard-\nless of how simple your code is, it’s still important to verify how it works in integration\nwith other subsystems. \nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n188\nCHAPTER 8\nWhy integration testing?\n8.1.3\nIntegration testing vs. failing fast\nThis section elaborates on the guideline of using integration tests to cover one happy\npath per business scenario and any edge cases that can’t be covered by unit tests. \n For an integration test, select the longest happy path in order to verify interactions\nwith all out-of-process dependencies. If there’s no one path that goes through all such\ninteractions, write additional integration tests—as many as needed to capture commu-\nnications with every external system.\n As with the edge cases that can’t be covered by unit tests, there are exceptions to\nthis part of the guideline, too. There’s no need to test an edge case if an incorrect\nexecution of that edge case immediately fails the entire application. For example, you\nsaw in chapter 7 how User from the sample CRM system implemented a CanChange-\nEmail method and made its successful execution a precondition for ChangeEmail():\nEnd-\nto-end\nIntegration\ntests\nUnit tests\nTest count\nProtection against\nregressions,\nresistance to\nrefactoring\nFast feedback,\nmaintainability\nFigure 8.2\nThe Test Pyramid represents a trade-off that works best for most \napplications. Fast, cheap unit tests cover the majority of edge cases, while a \nsmaller number of slow, more expensive integration tests ensure the correctness \nof the system as a whole.\nFigure 8.3\nThe Test Pyramid of a simple project. \nLittle complexity requires a smaller number of unit \ntests compared to a normal pyramid.\nUnit tests\nIntegration tests\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "page_number": 201
    },
    {
      "number": 23,
      "title": "Segment 23 (pages 211-219)",
      "start_page": 211,
      "end_page": 219,
      "detection_method": "topic_boundary",
      "content": "189\nWhat is an integration test?\npublic void ChangeEmail(string newEmail, Company company)\n{\nPrecondition.Requires(CanChangeEmail() == null);\n/* the rest of the method */\n}\nThe controller invokes CanChangeEmail() and interrupts the operation if that\nmethod returns an error:\n// UserController\npublic string ChangeEmail(int userId, string newEmail)\n{\nobject[] userData = _database.GetUserById(userId);\nUser user = UserFactory.Create(userData);\nstring error = user.CanChangeEmail();\nif (error != null)                    \nreturn error;                     \n/* the rest of the method */\n}\nThis example shows the edge case you could theoretically cover with an integration\ntest. Such a test doesn’t provide a significant enough value, though. If the controller\ntries to change the email without consulting with CanChangeEmail() first, the applica-\ntion crashes. This bug reveals itself with the first execution and thus is easy to notice\nand fix. It also doesn’t lead to data corruption.\nTIP\nIt’s better to not write a test at all than to write a bad test. A test that\ndoesn’t provide significant value is a bad test.\nUnlike the call from the controller to CanChangeEmail(), the presence of the precon-\ndition in User should be tested. But that is better done with a unit test; there’s no need\nfor an integration test.\n Making bugs manifest themselves quickly is called the Fail Fast principle, and it’s a\nviable alternative to integration testing.\nThe Fail Fast principle \nThe Fail Fast principle stands for stopping the current operation as soon as any unex-\npected error occurs. This principle makes your application more stable by\nShortening the feedback loop—The sooner you detect a bug, the easier it is\nto fix. A bug that is already in production is orders of magnitude more expen-\nsive to fix compared to a bug found during development.\nProtecting the persistence state—Bugs lead to corruption of the application’s\nstate. Once that state penetrates into the database, it becomes much harder\nto fix. Failing fast helps you prevent the corruption from spreading.\nEdge case\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n190\nCHAPTER 8\nWhy integration testing?\n8.2\nWhich out-of-process dependencies to test directly\nAs I mentioned earlier, integration tests verify how your system integrates with out-of-\nprocess dependencies. There are two ways to implement such verification: use the real\nout-of-process dependency, or replace that dependency with a mock. This section\nshows when to apply each of the two approaches.\n8.2.1\nThe two types of out-of-process dependencies\nAll out-of-process dependencies fall into two categories:\nManaged dependencies (out-of-process dependencies you have full control over)—These\ndependencies are only accessible through your application; interactions with\nthem aren’t visible to the external world. A typical example is a database. Exter-\nnal systems normally don’t access your database directly; they do that through\nthe API your application provides.\nUnmanaged dependencies (out-of-process dependencies you don’t have full control over)—\nInteractions with such dependencies are observable externally. Examples include\nan SMTP server and a message bus: both produce side effects visible to other\napplications.\nI mentioned in chapter 5 that communications with managed dependencies are\nimplementation details. Conversely, communications with unmanaged dependencies\nare part of your system’s observable behavior (figure 8.4). This distinction leads to the\ndifference in treatment of out-of-process dependencies in integration tests.\nIMPORTANT\nUse real instances of managed dependencies; replace unman-\naged dependencies with mocks.\nAs discussed in chapter 5, the requirement to preserve the communication pattern\nwith unmanaged dependencies stems from the necessity to maintain backward com-\npatibility with those dependencies. Mocks are perfect for this task. With mocks, you\ncan ensure communication pattern permanence in light of any possible refactorings.\n(continued)\nStopping the current operation is normally done by throwing exceptions, because\nexceptions have semantics that are perfectly suited for the Fail Fast principle: they\ninterrupt the program flow and pop up to the highest level of the execution stack,\nwhere you can log them and shut down or restart the operation.\nPreconditions are one example of the Fail Fast principle in action. A failing precondi-\ntion signifies an incorrect assumption made about the application state, which is\nalways a bug. Another example is reading data from a configuration file. You can\narrange the reading logic such that it will throw an exception if the data in the config-\nuration file is incomplete or incorrect. You can also put this logic close to the appli-\ncation startup, so that the application doesn’t launch if there’s a problem with its\nconfiguration. \nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n191\nWhich out-of-process dependencies to test directly\nHowever, there’s no need to maintain backward compatibility in communications with\nmanaged dependencies, because your application is the only one that talks to them.\nExternal clients don’t care how you organize your database; the only thing that mat-\nters is the final state of your system. Using real instances of managed dependencies in\nintegration tests helps you verify that final state from the external client’s point of\nview. It also helps during database refactorings, such as renaming a column or even\nmigrating from one database to another. \n8.2.2\nWorking with both managed and unmanaged dependencies\nSometimes you’ll encounter an out-of-process dependency that exhibits attributes of\nboth managed and unmanaged dependencies. A good example is a database that\nother applications have access to.\n The story usually goes like this. A system begins with its own dedicated database. After\na while, another system begins to require data from the same database. And so the team\ndecides to share access to a limited number of tables just for ease of integration with that\nother system. As a result, the database becomes a dependency that is both managed and\nunmanaged. It still contains parts that are visible to your application only; but, in addi-\ntion to those parts, it also has a number of tables accessible by other applications.\n The use of a database is a poor way to implement integration between systems\nbecause it couples these systems to each other and complicates their further develop-\nment. Only resort to this approach when all other options are exhausted. A better way\nto do the integration is via an API (for synchronous communications) or a message\nbus (for asynchronous communications).\n But what do you do when you already have a shared database and can’t do any-\nthing about it in the foreseeable future? In this case, treat tables that are visible to\nSMTP service\n(unmanaged\ndependency)\nObservable behavior (contract)\nImplementation details\nApplication\ndatabase\n(managed\ndependency)\nThird-party\nsystem\n(external\nclient)\nFigure 8.4\nCommunications with managed dependencies are implementation \ndetails; use such dependencies as-is in integration tests. Communications \nwith unmanaged dependencies are part of your system’s observable behavior. \nSuch dependencies should be mocked out.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n192\nCHAPTER 8\nWhy integration testing?\nother applications as an unmanaged dependency. Such tables in effect act as a mes-\nsage bus, with their rows playing the role of messages. Use mocks to make sure the\ncommunication pattern with these tables remains unchanged. At the same time, treat\nthe rest of your database as a managed dependency and verify its final state, not the\ninteractions with it (figure 8.5).\nIt’s important to differentiate these two parts of your database because, again, the\nshared tables are observable externally, and you need to be careful about how your\napplication communicates with them. Don’t change the way your system interacts with\nthose tables unless absolutely necessary! You never know how other applications will\nreact to such a change. \n8.2.3\nWhat if you can’t use a real database in integration tests?\nSometimes, for reasons outside of your control, you just can’t use a real version of a\nmanaged dependency in integration tests. An example would be a legacy database\nthat you can’t deploy to a test automation environment, not to mention a developer\nmachine, because of some IT security policy, or because the cost of setting up and\nmaintaining a test database instance is prohibitive.\n What should you do in such a situation? Should you mock out the database anyway,\ndespite it being a managed dependency? No, because mocking out a managed depen-\ndency compromises the integration tests’ resistance to refactoring. Furthermore, such\nExternal applications\nTable\nTable\nTable\nTable\nManaged part\nTable\nTable\nUnmanaged part\nTest directly\nReplace with mocks\nDatabase\nYour application\nFigure 8.5\nTreat the part of the database that is visible to external \napplications as an unmanaged dependency. Replace it with mocks in \nintegration tests. Treat the rest of the database as a managed dependency. \nVerify its final state, not interactions with it.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n193\nIntegration testing: An example\ntests no longer provide as good protection against regressions. And if the database is\nthe only out-of-process dependency in your project, the resulting integration tests\nwould deliver no additional protection compared to the existing set of unit tests (assum-\ning these unit tests follow the guidelines from chapter 7).\n The only thing such integration tests would do, in addition to unit tests, is check\nwhat repository methods the controller calls. In other words, you wouldn’t really gain\nconfidence about anything other than those three lines of code in your controller\nbeing correct, while still having to do a lot of plumbing.\n If you can’t test the database as-is, don’t write integration tests at all, and instead,\nfocus exclusively on unit testing of the domain model. Remember to always put all\nyour tests under close scrutiny. Tests that don’t provide a high enough value should\nhave no place in your test suite. \n8.3\nIntegration testing: An example\nLet’s get back to the sample CRM system from chapter 7 and see how it can be cov-\nered with integration tests. As you may recall, this system implements one feature:\nchanging the user’s email. It retrieves the user and the company from the database,\ndelegates the decision-making to the domain model, and then saves the results back\nto the database and puts a message on the bus if needed (figure 8.6).\nThe following listing shows how the controller currently looks.\npublic class UserController\n{\nprivate readonly Database _database = new Database();\nprivate readonly MessageBus _messageBus = new MessageBus();\npublic string ChangeEmail(int userId, string newEmail)\n{\nListing 8.1\nThe user controller \nApplication\nservice\n(controller)\nBusiness logic\n(domain model)\nDatabase\nMessage bus\nGetUserById\nCanChangeEmail\nSaveCompany\nGetCompany\nChangeEmail\nSaveUser\nSendMessage\nFigure 8.6\nThe use case of changing the user’s email. The controller orchestrates the work between \nthe database, the message bus, and the domain model.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n194\nCHAPTER 8\nWhy integration testing?\nobject[] userData = _database.GetUserById(userId);\nUser user = UserFactory.Create(userData);\nstring error = user.CanChangeEmail();\nif (error != null)\nreturn error;\nobject[] companyData = _database.GetCompany();\nCompany company = CompanyFactory.Create(companyData);\nuser.ChangeEmail(newEmail, company);\n_database.SaveCompany(company);\n_database.SaveUser(user);\nforeach (EmailChangedEvent ev in user.EmailChangedEvents)\n{\n_messageBus.SendEmailChangedMessage(ev.UserId, ev.NewEmail);\n}\nreturn \"OK\";\n}\n}\nIn the following section, I’ll first outline scenarios to verify using integration tests.\nThen I’ll show you how to work with the database and the message bus in tests.\n8.3.1\nWhat scenarios to test?\nAs I mentioned earlier, the general guideline for integration testing is to cover the\nlongest happy path and any edge cases that can’t be exercised by unit tests. The longest\nhappy path is the one that goes through all out-of-process dependencies.\n In the CRM project, the longest happy path is a change from a corporate to a non-\ncorporate email. Such a change leads to the maximum number of side effects:\nIn the database, both the user and the company are updated: the user changes\nits type (from corporate to non-corporate) and email, and the company changes\nits number of employees.\nA message is sent to the message bus.\nAs for the edge cases that aren’t tested by unit tests, there’s only one such edge case:\nthe scenario where the email can’t be changed. There’s no need to test this scenario,\nthough, because the application will fail fast if this check isn’t present in the control-\nler. That leaves us with a single integration test:\npublic void Changing_email_from_corporate_to_non_corporate()\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n195\nIntegration testing: An example\n8.3.2\nCategorizing the database and the message bus\nBefore writing the integration test, you need to categorize the two out-of-process\ndependencies and decide which of them to test directly and which to replace with a\nmock. The application database is a managed dependency because no other system\ncan access it. Therefore, you should use a real instance of it. The integration test will\nInsert a user and a company into the database.\nRun the change of email scenario on that database.\nVerify the database state.\nOn the other hand, the message bus is an unmanaged dependency—its sole pur-\npose is to enable communication with other systems. The integration test will mock\nout the message bus and verify the interactions between the controller and the\nmock afterward. \n8.3.3\nWhat about end-to-end testing?\nThere will be no end-to-end tests in our sample project. An end-to-end test in a sce-\nnario with an API would be a test running against a deployed, fully functioning ver-\nsion of that API, which means no mocks for any of the out-of-process dependencies\n(figure 8.7). On the other hand, integration tests host the application within the same\nprocess and substitute unmanaged dependencies with mocks (figure 8.8).\n As I mentioned in chapter 2, whether to use end-to-end tests is a judgment call. For\nthe most part, when you include managed dependencies in the integration testing\nscope and mock out only unmanaged dependencies, integration tests provide a level\nEnd-to-end test\nApplication\nMessage bus\nDatabase\nOut-of-process\nIn-process\nFigure 8.7\nEnd-to-end tests emulate the external client and therefore test a \ndeployed version of the application with all out-of-process dependencies included \nin the testing scope. End-to-end tests shouldn’t check managed dependencies \n(such as the database) directly, only indirectly through the application.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n196\nCHAPTER 8\nWhy integration testing?\nof protection that is close enough to that of end-to-end tests, so you can skip end-to-\nend testing. However, you could still create one or two overarching end-to-end tests\nthat would provide a sanity check for the project after deployment. Make such tests go\nthrough the longest happy path, too, to ensure that your application communicates\nwith all out-of-process dependencies properly. To emulate the external client’s behav-\nior, check the message bus directly, but verify the database’s state through the applica-\ntion itself. \n8.3.4\nIntegration testing: The first try\nHere’s the first version of the integration test.\n[Fact]\npublic void Changing_email_from_corporate_to_non_corporate()\n{\n// Arrange\nvar db = new Database(ConnectionString);\n    \nUser user = CreateUser(\n   \n\"user@mycorp.com\", UserType.Employee, db);   \nCreateCompany(\"mycorp.com\", 1, db);\n   \nvar messageBusMock = new Mock<IMessageBus>();         \nvar sut = new UserController(db, messageBusMock.Object);\n// Act\nstring result = sut.ChangeEmail(user.UserId, \"new@gmail.com\");\nListing 8.2\nThe integration test\nIntegration test\nApplication\nMessage bus\nmock\nDatabase\nOut-of-process\nIn-process\nFigure 8.8\nIntegration tests host the application within the same process. Unlike \nend-to-end tests, integration tests substitute unmanaged dependencies with \nmocks. The only out-of-process components for integration tests are managed \ndependencies.\nDatabase \nrepository\nCreates the user \nand company in \nthe database\nSets up a \nmock for the \nmessage bus\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n197\nUsing interfaces to abstract dependencies\n// Assert\nAssert.Equal(\"OK\", result);\nobject[] userData = db.GetUserById(user.UserId);   \nUser userFromDb = UserFactory.Create(userData);    \nAssert.Equal(\"new@gmail.com\", userFromDb.Email);   \nAssert.Equal(UserType.Customer, userFromDb.Type);  \nobject[] companyData = db.GetCompany();\n   \nCompany companyFromDb = CompanyFactory\n   \n.Create(companyData);\n   \nAssert.Equal(0, companyFromDb.NumberOfEmployees);  \nmessageBusMock.Verify(\n    \nx => x.SendEmailChangedMessage(\n    \nuser.UserId, \"new@gmail.com\"),    \nTimes.Once);\n     \n}\nTIP\nNotice that in the arrange section, the test doesn’t insert the user and\nthe company into the database on its own but instead calls the CreateUser\nand CreateCompany helper methods. These methods can be reused across\nmultiple integration tests.\nIt’s important to check the state of the database independently of the data used as\ninput parameters. To do that, the integration test queries the user and company data\nseparately in the assert section, creates new userFromDb and companyFromDb instances,\nand only then asserts their state. This approach ensures that the test exercises both\nwrites to and reads from the database and thus provides the maximum protection\nagainst regressions. The reading itself must be implemented using the same code the\ncontroller uses internally: in this example, using the Database, UserFactory, and\nCompanyFactory classes.\n This integration test, while it gets the job done, can still benefit from some\nimprovement. For instance, you could use helper methods in the assertion section, too,\nin order to reduce this section’s size. Also, messageBusMock doesn’t provide as good\nprotection against regressions as it potentially could. We’ll talk about these improve-\nments in the subsequent two chapters where we discuss mocking and database testing\nbest practices. \n8.4\nUsing interfaces to abstract dependencies\nOne of the most misunderstood subjects in the sphere of unit testing is the use of\ninterfaces. Developers often ascribe invalid reasons to why they introduce interfaces\nand, as a result, tend to overuse them. In this section, I’ll expand on those invalid\nreasons and show in what circumstances the use of interfaces is and isn’t preferable.\n \nAsserts the \nuser’s state\nAsserts the \ncompany’s \nstate\nChecks the \ninteractions \nwith the mock\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "page_number": 211
    },
    {
      "number": 24,
      "title": "Segment 24 (pages 220-231)",
      "start_page": 220,
      "end_page": 231,
      "detection_method": "topic_boundary",
      "content": "198\nCHAPTER 8\nWhy integration testing?\n8.4.1\nInterfaces and loose coupling\nMany developers introduce interfaces for out-of-process dependencies, such as the\ndatabase or the message bus, even when these interfaces have only one implementation.\nThis practice has become so widespread nowadays that hardly anyone questions it.\nYou’ll often see class-interface pairs similar to the following:\npublic interface IMessageBus\npublic class MessageBus : IMessageBus\npublic interface IUserRepository\npublic class UserRepository : IUserRepository\nThe common reasoning behind the use of such interfaces is that they help to\nAbstract out-of-process dependencies, thus achieving loose coupling\nAdd new functionality without changing the existing code, thus adhering to the\nOpen-Closed principle (OCP)\nBoth of these reasons are misconceptions. Interfaces with a single implementation are\nnot abstractions and don’t provide loose coupling any more than concrete classes that\nimplement those interfaces. Genuine abstractions are discovered, not invented. The dis-\ncovery, by definition, takes place post factum, when the abstraction already exists but\nis not yet clearly defined in the code. Thus, for an interface to be a genuine abstrac-\ntion, it must have at least two implementations.\n The second reason (the ability to add new functionality without changing the exist-\ning code) is a misconception because it violates a more foundational principle:\nYAGNI. YAGNI stands for “You aren’t gonna need it” and advocates against investing\ntime in functionality that’s not needed right now. You shouldn’t develop this function-\nality, nor should you modify your existing code to account for the appearance of such\nfunctionality in the future. The two major reasons are as follows:\nOpportunity cost—If you spend time on a feature that business people don’t need\nat the moment, you steer that time away from features they do need right now.\nMoreover, when the business people finally come to require the developed func-\ntionality, their view on it will most likely have evolved, and you will still need to\nadjust the already-written code. Such activity is wasteful. It’s more beneficial to\nimplement the functionality from scratch when the actual need for it emerges.\nThe less code in the project, the better. Introducing code just in case without an imme-\ndiate need unnecessarily increases your code base’s cost of ownership. It’s bet-\nter to postpone introducing new functionality until as late a stage of your\nproject as possible.\nTIP\nWriting code is an expensive way to solve problems. The less code the\nsolution requires and the simpler that code is, the better.\nThere are exceptional cases where YAGNI doesn’t apply, but these are few and far\nbetween. For those cases, see my article “OCP vs YAGNI,” at https://enterprise-\ncraftsmanship.com/posts/ocp-vs-yagni. \nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n199\nUsing interfaces to abstract dependencies\n8.4.2\nWhy use interfaces for out-of-process dependencies?\nSo, why use interfaces for out-of-process dependencies at all, assuming that each of\nthose interfaces has only one implementation? The real reason is much more practi-\ncal and down-to-earth. It’s to enable mocking—as simple as that. Without an interface,\nyou can’t create a test double and thus can’t verify interactions between the system\nunder test and the out-of-process dependency.\n Therefore, don’t introduce interfaces for out-of-process dependencies unless you need to mock\nout those dependencies. You only mock out unmanaged dependencies, so the guideline\ncan be boiled down to this: use interfaces for unmanaged dependencies only. Still inject\nmanaged dependencies into the controller explicitly, but use concrete classes for that.\n Note that genuine abstractions (abstractions that have more than one implementa-\ntion) can be represented with interfaces regardless of whether you mock them out.\nIntroducing an interface with a single implementation for reasons other than mock-\ning is a violation of YAGNI, however.\n And you might have noticed in listing 8.2 that UserController now accepts both\nthe message bus and the database explicitly via the constructor, but only the message\nbus has a corresponding interface. The database is a managed dependency and thus\ndoesn’t require such an interface. Here’s the controller:\npublic class UserController\n{\nprivate readonly Database _database;   \nprivate readonly IMessageBus _messageBus;    \npublic UserController(Database database, IMessageBus messageBus)\n{\n_database = database;\n_messageBus = messageBus;\n}\npublic string ChangeEmail(int userId, string newEmail)\n{\n/* the method uses _database and _messageBus */\n}\n}\nNOTE\nYou can mock out a dependency without resorting to an interface by\nmaking methods in that dependency virtual and using the class itself as a base\nfor the mock. This approach is inferior to the one with interfaces, though. I\nexplain more on this topic of interfaces versus base classes in chapter 11. \n8.4.3\nUsing interfaces for in-process dependencies\nYou sometimes see code bases where interfaces back not only out-of-process depen-\ndencies but in-process dependencies as well. For example:\npublic interface IUser\n{\nint UserId { get; set; }\nA concrete \nclass\nThe interface\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n200\nCHAPTER 8\nWhy integration testing?\nstring Email { get; }\nstring CanChangeEmail();\nvoid ChangeEmail(string newEmail, Company company);\n}\npublic class User : IUser\n{\n/* ... */\n}\nAssuming that IUser has only one implementation (and such specific interfaces always\nhave only one implementation), this is a huge red flag. Just like with out-of-process\ndependencies, the only reason to introduce an interface with a single implementation\nfor a domain class is to enable mocking. But unlike out-of-process dependencies, you\nshould never check interactions between domain classes, because doing so results in\nbrittle tests: tests that couple to implementation details and thus fail on the metric of\nresisting to refactoring (see chapter 5 for more details about mocks and test fragility). \n8.5\nIntegration testing best practices\nThere are some general guidelines that can help you get the most out of your integra-\ntion tests:\nMaking domain model boundaries explicit\nReducing the number of layers in the application\nEliminating circular dependencies\nAs usual, best practices that are beneficial for tests also tend to improve the health of\nyour code base in general.\n8.5.1\nMaking domain model boundaries explicit\nTry to always have an explicit, well-known place for the domain model in your code\nbase. The domain model is the collection of domain knowledge about the problem your\nproject is meant to solve. Assigning the domain model an explicit boundary helps you\nbetter visualize and reason about that part of your code.\n This practice also helps with testing. As I mentioned earlier in this chapter, unit\ntests target the domain model and algorithms, while integration tests target control-\nlers. The explicit boundary between domain classes and controllers makes it easier to\ntell the difference between unit and integration tests.\n The boundary itself can take the form of a separate assembly or a namespace. The\nparticulars aren’t that important as long as all of the domain logic is put under a sin-\ngle, distinct umbrella and not scattered across the code base. \n8.5.2\nReducing the number of layers\nMost programmers naturally gravitate toward abstracting and generalizing the code\nby introducing additional layers of indirection. In a typical enterprise-level applica-\ntion, you can easily observe several such layers (figure 8.9).\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n201\nIntegration testing best practices\nIn extreme cases, an application gets so many abstraction layers that it becomes too\nhard to navigate the code base and understand the logic behind even the simplest\noperations. At some point, you just want to get to the specific solution of the problem\nat hand, not some generalization of that solution in a vacuum.\nAll problems in computer science can be solved by another layer of indirection, except for\nthe problem of too many layers of indirection.\n                                                                   \n—David J. Wheeler\nLayers of indirection negatively affect your ability to reason about the code. When\nevery feature has a representation in each of those layers, you have to expend signifi-\ncant effort assembling all the pieces into a cohesive picture. This creates an additional\nmental burden that handicaps the entire development process.\n An excessive number of abstractions doesn’t help unit or integration testing,\neither. Code bases with many layers of indirections tend not to have a clear boundary\nbetween controllers and the domain model (which, as you might remember from\nchapter 7, is a precondition for effective tests). There’s also a much stronger tendency\nto verify each layer separately. This tendency results in a lot of low-value integration\ntests, each of which exercises only the code from a specific layer and mocks out layers\nApplication\nservices layer\nBusiness logic\nimplementation layer\nAbstractions layer\nPersistence layer\nOrder checkout\nChanging user email\nResetting password\nFigure 8.9\nVarious application concerns are often addressed by \nseparate layers of indirection. A typical feature takes up a small \nportion of each layer.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n202\nCHAPTER 8\nWhy integration testing?\nunderneath. The end result is always the same: insufficient protection against regres-\nsions combined with low resistance to refactoring.\n Try to have as few layers of indirection as possible. In most backend systems, you\ncan get away with just three: the domain model, application services layer (control-\nlers), and infrastructure layer. The infrastructure layer typically consists of algorithms\nthat don’t belong to the domain model, as well as code that enables access to out-of-\nprocess dependencies (figure 8.10). \n8.5.3\nEliminating circular dependencies\nAnother practice that can drastically improve the maintainability of your code base\nand make testing easier is eliminating circular dependencies.\nDEFINITION\nA circular dependency (also known as cyclic dependency) is two or\nmore classes that directly or indirectly depend on each other to function\nproperly.\nA typical example of a circular dependency is a callback:\npublic class CheckOutService\n{\npublic void CheckOut(int orderId)\n{\nvar service = new ReportGenerationService();\nservice.GenerateReport(orderId, this);\nApplication\nservices layer\nDomain layer\nInfrastructure layer\nOrder checkout\nChanging user email\nResetting password\nFigure 8.10\nYou can get away with just three layers: the domain layer (contains \ndomain logic), application services layers (provides an entry point for the external \nclient, and coordinates the work between domain classes and out-of-process \ndependencies), and infrastructure layer (works with out-of-process dependencies; \ndatabase repositories, ORM mappings, and SMTP gateways reside in this layer).\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n203\nIntegration testing best practices\n/* other code */\n}\n}\npublic class ReportGenerationService\n{\npublic void GenerateReport(\nint orderId,\nCheckOutService checkOutService)\n{\n/* calls checkOutService when generation is completed */\n}\n}\nHere, CheckOutService creates an instance of ReportGenerationService and passes\nitself to that instance as an argument. ReportGenerationService calls CheckOut-\nService back to notify it about the result of the report generation.\n Just like an excessive number of abstraction layers, circular dependencies add tre-\nmendous cognitive load when you try to read and understand the code. The reason is\nthat circular dependencies don’t give you a clear starting point from which you can\nbegin exploring the solution. To understand just one class, you have to read and\nunderstand the whole graph of its siblings all at once. Even a small set of interdepen-\ndent classes can quickly become too hard to grasp.\n Circular dependencies also interfere with testing. You often have to resort to inter-\nfaces and mocking in order to split the class graph and isolate a single unit of behav-\nior, which, again, is a no-go when it comes to testing the domain model (more on that\nin chapter 5).\n Note that the use of interfaces only masks the problem of circular dependencies. If\nyou introduce an interface for CheckOutService and make ReportGenerationService\ndepend on that interface instead of the concrete class, you remove the circular depen-\ndency at compile time (figure 8.11), but the cycle still persists at runtime. Even\nthough the compiler no longer regards this class composition as a circular reference,\nthe cognitive load required to understand the code doesn’t become any smaller. If\nanything, it increases due to the additional interface.\nCheckOutService\nICheckOutService\nReportGenerationService\nFigure 8.11\nWith an interface, you remove the circular dependency \nat compile time, but not at runtime. The cognitive load required to \nunderstand the code doesn’t become any smaller.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n204\nCHAPTER 8\nWhy integration testing?\nA better approach to handle circular dependencies is to get rid of them. Refactor\nReportGenerationService such that it depends on neither CheckOutService nor the\nICheckOutService interface, and make ReportGenerationService return the result\nof its work as a plain value instead of calling CheckOutService:\npublic class CheckOutService\n{\npublic void CheckOut(int orderId)\n{\nvar service = new ReportGenerationService();\nReport report = service.GenerateReport(orderId);\n/* other work */\n}\n}\npublic class ReportGenerationService\n{\npublic Report GenerateReport(int orderId)\n{\n/* ... */\n}\n}\nIt’s rarely possible to eliminate all circular dependencies in your code base. But even\nthen, you can minimize the damage by making the remaining graphs of interdepen-\ndent classes as small as possible. \n8.5.4\nUsing multiple act sections in a test\nAs you might remember from chapter 3, having more than one arrange, act, or assert\nsection in a test is a code smell. It’s a sign that this test checks multiple units of behav-\nior, which, in turn, hinders the test’s maintainability. For example, if you have two\nrelated use cases—say, user registration and user deletion—it might be tempting to\ncheck both of these use cases in a single integration test. Such a test could have the\nfollowing structure:\nArrange—Prepare data with which to register a user.\nAct—Call UserController.RegisterUser().\nAssert—Query the database to see if the registration is completed successfully.\nAct—Call UserController.DeleteUser().\nAssert—Query the database to make sure the user is deleted.\nThis approach is compelling because the user states naturally flow from one another,\nand the first act (registering a user) can simultaneously serve as an arrange phase for\nthe subsequent act (user deletion). The problem is that such tests lose focus and can\nquickly become too bloated.\n It’s best to split the test by extracting each act into a test of its own. It may seem like\nunnecessary work (after all, why create two tests where one would suffice?), but this\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n205\nHow to test logging functionality\nwork pays off in the long run. Having each test focus on a single unit of behavior\nmakes those tests easier to understand and modify when necessary.\n The exception to this guideline is tests working with out-of-process dependencies\nthat are hard to bring to a desirable state. Let’s say for example that registering a user\nresults in creating a bank account in an external banking system. The bank has provi-\nsioned a sandbox for your organization, and you want to use that sandbox in an end-\nto-end test. The problem is that the sandbox is too slow, or maybe the bank limits the\nnumber of calls you can make to that sandbox. In such a scenario, it becomes benefi-\ncial to combine multiple acts into a single test and thus reduce the number of interac-\ntions with the problematic out-of-process dependency.\n Hard-to-manage out-of-process dependencies are the only legitimate reason to\nwrite a test with more than one act section. This is why you should never have multiple\nacts in a unit test—unit tests don’t work with out-of-process dependencies. Even inte-\ngration tests should rarely have several acts. In practice, multistep tests almost always\nbelong to the category of end-to-end tests. \n8.6\nHow to test logging functionality\nLogging is a gray area, and it isn’t obvious what to do with it when it comes to testing.\nThis is a complex topic that I’ll split into the following questions:\nShould you test logging at all?\nIf so, how should you test it?\nHow much logging is enough?\nHow do you pass around logger instances?\nWe’ll use our sample CRM project as an example.\n8.6.1\nShould you test logging?\nLogging is a cross-cutting functionality, which you can require in any part of your code\nbase. Here’s an example of logging in the User class.\npublic class User\n{\npublic void ChangeEmail(string newEmail, Company company)\n{\n_logger.Info(    \n$\"Changing email for user {UserId} to {newEmail}\");\nPrecondition.Requires(CanChangeEmail() == null);\nif (Email == newEmail)\nreturn;\nUserType newType = company.IsEmailCorporate(newEmail)\n? UserType.Employee\n: UserType.Customer;\nListing 8.3\nAn example of logging in User\nStart of the\nmethod\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n206\nCHAPTER 8\nWhy integration testing?\nif (Type != newType)\n{\nint delta = newType == UserType.Employee ? 1 : -1;\ncompany.ChangeNumberOfEmployees(delta);\n_logger.Info(\n   \n$\"User {UserId} changed type \" +\n   \n$\"from {Type} to {newType}\");\n   \n}\nEmail = newEmail;\nType = newType;\nEmailChangedEvents.Add(new EmailChangedEvent(UserId, newEmail));\n_logger.Info(\n    \n$\"Email is changed for user {UserId}\");\n}\n}\nThe User class records in a log file each beginning and ending of the ChangeEmail\nmethod, as well as the change of the user type. Should you test this functionality?\n On the one hand, logging generates important information about the applica-\ntion’s behavior. But on the other hand, logging can be so ubiquitous that it’s not obvi-\nous whether this functionality is worth the additional, quite significant, testing effort.\nThe answer to the question of whether you should test logging comes down to this: Is\nlogging part of the application’s observable behavior, or is it an implementation detail?\n In that sense, it isn’t different from any other functionality. Logging ultimately\nresults in side effects in an out-of-process dependency such as a text file or a database.\nIf these side effects are meant to be observed by your customer, the application’s cli-\nents, or anyone else other than the developers themselves, then logging is an observ-\nable behavior and thus must be tested. If the only audience is the developers, then it’s\nan implementation detail that can be freely modified without anyone noticing, in\nwhich case it shouldn’t be tested.\n For example, if you write a logging library, then the logs this library produces are\nthe most important (and the only) part of its observable behavior. Another example is\nwhen business people insist on logging key application workflows. In this case, logs\nalso become a business requirement and thus have to be covered by tests. However, in\nthe latter example, you might also have separate logging just for developers.\n Steve Freeman and Nat Pryce, in their book Growing Object-Oriented Software, Guided\nby Tests (Addison-Wesley Professional, 2009), call these two types of logging support\nlogging and diagnostic logging:\nSupport logging produces messages that are intended to be tracked by support\nstaff or system administrators.\nDiagnostic logging helps developers understand what’s going on inside the\napplication. \nChanges the \nuser type\nEnd of the\nmethod\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n207\nHow to test logging functionality\n8.6.2\nHow should you test logging?\nBecause logging involves out-of-process dependencies, when it comes to testing it, the\nsame rules apply as with any other functionality that touches out-of-process dependen-\ncies. You need to use mocks to verify interactions between your application and the\nlog storage.\nINTRODUCING A WRAPPER ON TOP OF ILOGGER\nBut don’t just mock out the ILogger interface. Because support logging is a business\nrequirement, reflect that requirement explicitly in your code base. Create a special\nDomainLogger class where you explicitly list all the support logging needed for the\nbusiness; verify interactions with that class instead of the raw ILogger.\n For example, let’s say that business people require you to log all changes of the\nusers’ types, but the logging at the beginning and the end of the method is there just\nfor debugging purposes. The next listing shows the User class after introducing a\nDomainLogger class.\npublic void ChangeEmail(string newEmail, Company company)\n{\n_logger.Info(\n     \n$\"Changing email for user {UserId} to {newEmail}\");\nPrecondition.Requires(CanChangeEmail() == null);\nif (Email == newEmail)\nreturn;\nUserType newType = company.IsEmailCorporate(newEmail)\n? UserType.Employee\n: UserType.Customer;\nif (Type != newType)\n{\nint delta = newType == UserType.Employee ? 1 : -1;\ncompany.ChangeNumberOfEmployees(delta);\n_domainLogger.UserTypeHasChanged(         \nUserId, Type, newType);\n         \n}\nEmail = newEmail;\nType = newType;\nEmailChangedEvents.Add(new EmailChangedEvent(UserId, newEmail));\n_logger.Info(\n   \n$\"Email is changed for user {UserId}\");\n}\nThe diagnostic logging still uses the old logger (which is of type ILogger), but the\nsupport logging now uses the new domainLogger instance of type IDomainLogger. The\nfollowing listing shows the implementation of IDomainLogger.\nListing 8.4\nExtracting support logging into the DomainLogger class\nDiagnostic\nlogging\nSupport \nlogging\nDiagnostic\nlogging\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n208\nCHAPTER 8\nWhy integration testing?\npublic class DomainLogger : IDomainLogger\n{\nprivate readonly ILogger _logger;\npublic DomainLogger(ILogger logger)\n{\n_logger = logger;\n}\npublic void UserTypeHasChanged(\nint userId, UserType oldType, UserType newType)\n{\n_logger.Info(\n$\"User {userId} changed type \" +\n$\"from {oldType} to {newType}\");\n}\n}\nDomainLogger works on top of ILogger: it uses the domain language to declare spe-\ncific log entries required by the business, thus making support logging easier to\nunderstand and maintain. In fact, this implementation is very similar to the concept\nof structured logging, which enables great flexibility when it comes to log file post-\nprocessing and analysis. \nUNDERSTANDING STRUCTURED LOGGING\nStructured logging is a logging technique where capturing log data is decoupled from\nthe rendering of that data. Traditional logging works with simple text. A call like\nlogger.Info(\"User Id is \" + 12);\nfirst forms a string and then writes that string to a log storage. The problem with this\napproach is that the resulting log files are hard to analyze due to the lack of structure.\nFor example, it’s not easy to see how many messages of a particular type there are and\nhow many of those relate to a specific user ID. You’d need to use (or even write your\nown) special tooling for that.\n On the other hand, structured logging introduces structure to your log storage.\nThe use of a structured logging library looks similar on the surface:\nlogger.Info(\"User Id is {UserId}\", 12);\nBut its underlying behavior differs significantly. Behind the scenes, this method com-\nputes a hash of the message template (the message itself is stored in a lookup storage\nfor space efficiency) and combines that hash with the input parameters to form a set\nof captured data. The next step is the rendering of that data. You can still have a flat log\nfile, as with traditional logging, but that’s just one possible rendering. You could also\nconfigure the logging library to render the captured data as a JSON or a CSV file,\nwhere it would be easier to analyze (figure 8.12).\nListing 8.5\nDomainLogger as a wrapper on top of ILogger\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n209\nHow to test logging functionality\nDomainLogger in listing 8.5 isn’t a structured logger per se, but it operates in the same\nspirit. Look at this method once again:\npublic void UserTypeHasChanged(\nint userId, UserType oldType, UserType newType)\n{\n_logger.Info(\n$\"User {userId} changed type \" +\n$\"from {oldType} to {newType}\");\n}\nYou can view UserTypeHasChanged() as the message template’s hash. Together with\nthe userId, oldType, and newType parameters, that hash forms the log data. The\nmethod’s implementation renders the log data into a flat log file. And you can easily\ncreate additional renderings by also writing the log data into a JSON or a CSV file. \nWRITING TESTS FOR SUPPORT AND DIAGNOSTIC LOGGING\nAs I mentioned earlier, DomainLogger represents an out-of-process dependency—the\nlog storage. This poses a problem: User now interacts with that dependency and thus\nviolates the separation between business logic and communication with out-of-process\ndependencies. The use of DomainLogger has transitioned User to the category of\nLog data\nlogger.Info(\"User Id is {UserId}\", 12)\nMessageTemplate\nUserId\nUser Id is {UserId}\n12\nUser Id is 12\nFlat log ﬁle\n{ “MessageTemplate”: “…”,\n“UserId” : 12 }\nMessageTemplate,UserId\nUser Id is {UserId},12\nJSON ﬁle\nCSV ﬁle\nRendering\nFigure 8.12\nStructured logging decouples log data from renderings of that data. You can set up \nmultiple renderings, such as a flat log file, JSON, or CSV file.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "page_number": 220
    },
    {
      "number": 25,
      "title": "Segment 25 (pages 232-239)",
      "start_page": 232,
      "end_page": 239,
      "detection_method": "topic_boundary",
      "content": "210\nCHAPTER 8\nWhy integration testing?\novercomplicated code, making it harder to test and maintain (refer to chapter 7 for\nmore details about code categories).\n This problem can be solved the same way we implemented the notification of\nexternal systems about changed user emails: with the help of domain events (again,\nsee chapter 7 for details). You can introduce a separate domain event to track changes\nin the user type. The controller will then convert those changes into calls to Domain-\nLogger, as shown in the following listing.\npublic void ChangeEmail(string newEmail, Company company)\n{\n_logger.Info(\n$\"Changing email for user {UserId} to {newEmail}\");\nPrecondition.Requires(CanChangeEmail() == null);\nif (Email == newEmail)\nreturn;\nUserType newType = company.IsEmailCorporate(newEmail)\n? UserType.Employee\n: UserType.Customer;\nif (Type != newType)\n{\nint delta = newType == UserType.Employee ? 1 : -1;\ncompany.ChangeNumberOfEmployees(delta);\nAddDomainEvent(\n        \nnew UserTypeChangedEvent(\n        \nUserId, Type, newType));        \n}\nEmail = newEmail;\nType = newType;\nAddDomainEvent(new EmailChangedEvent(UserId, newEmail));\n_logger.Info($\"Email is changed for user {UserId}\");\n}\nNotice that there are now two domain events: UserTypeChangedEvent and Email-\nChangedEvent. Both of them implement the same interface (IDomainEvent) and thus\ncan be stored in the same collection.\n And here is how the controller looks.\npublic string ChangeEmail(int userId, string newEmail)\n{\nobject[] userData = _database.GetUserById(userId);\nUser user = UserFactory.Create(userData);\nListing 8.6\nReplacing DomainLogger in User with a domain event\nListing 8.7\nLatest version of UserController\nUses a domain \nevent instead of \nDomainLogger\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n211\nHow to test logging functionality\nstring error = user.CanChangeEmail();\nif (error != null)\nreturn error;\nobject[] companyData = _database.GetCompany();\nCompany company = CompanyFactory.Create(companyData);\nuser.ChangeEmail(newEmail, company);\n_database.SaveCompany(company);\n_database.SaveUser(user);\n_eventDispatcher.Dispatch(user.DomainEvents);   \nreturn \"OK\";\n}\nEventDispatcher is a new class that converts domain events into calls to out-of-process\ndependencies:\n\nEmailChangedEvent translates into _messageBus.SendEmailChangedMessage().\n\nUserTypeChangedEvent translates into _domainLogger.UserTypeHasChanged().\nThe use of UserTypeChangedEvent has restored the separation between the two\nresponsibilities: domain logic and communication with out-of-process dependencies.\nTesting support logging now isn’t any different from testing the other unmanaged\ndependency, the message bus:\nUnit tests should check an instance of UserTypeChangedEvent in the User\nunder test.\nThe single integration test should use a mock to ensure the interaction with\nDomainLogger is in place.\nNote that if you need to do support logging in the controller and not one of the\ndomain classes, there’s no need to use domain events. As you may remember from\nchapter 7, controllers orchestrate the collaboration between the domain model and\nout-of-process dependencies. DomainLogger is one of such dependencies, and thus\nUserController can use that logger directly.\n Also notice that I didn’t change the way the User class does diagnostic logging.\nUser still uses the logger instance directly in the beginning and at the end of its Chan-\ngeEmail method. This is by design. Diagnostic logging is for developers only; you\ndon’t need to unit test this functionality and thus don’t have to keep it out of the\ndomain model.\n Still, refrain from the use of diagnostic logging in User or other domain classes\nwhen possible. I explain why in the next section. \nDispatches user \ndomain events\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n212\nCHAPTER 8\nWhy integration testing?\n8.6.3\nHow much logging is enough?\nAnother important question is about the optimum amount of logging. How much log-\nging is enough? Support logging is out of the question here because it’s a business\nrequirement. You do have control over diagnostic logging, though.\n It’s important not to overuse diagnostic logging, for the following two reasons:\nExcessive logging clutters the code. This is especially true for the domain model.\nThat’s why I don’t recommend using diagnostic logging in User even though\nsuch a use is fine from a unit testing perspective: it obscures the code.\nLogs’ signal-to-noise ratio is key. The more you log, the harder it is to find relevant\ninformation. Maximize the signal; minimize the noise.\nTry not to use diagnostic logging in the domain model at all. In most cases, you can\nsafely move that logging from domain classes to controllers. And even then, resort to\ndiagnostic logging only temporarily when you need to debug something. Remove it\nonce you finish debugging. Ideally, you should use diagnostic logging for unhandled\nexceptions only. \n8.6.4\nHow do you pass around logger instances?\nFinally, the last question is how to pass logger instances in the code. One way to\nresolve these instances is using static methods, as shown in the following listing.\npublic class User\n{\nprivate static readonly ILogger _logger =   \nLogManager.GetLogger(typeof(User));     \npublic void ChangeEmail(string newEmail, Company company)\n{\n_logger.Info(\n$\"Changing email for user {UserId} to {newEmail}\");\n/* ... */\n_logger.Info($\"Email is changed for user {UserId}\");\n}\n}\nSteven van Deursen and Mark Seeman, in their book Dependency Injection Principles,\nPractices, Patterns (Manning Publications, 2018), call this type of dependency acquisi-\ntion ambient context. This is an anti-pattern. Two of their arguments are that\nThe dependency is hidden and hard to change.\nTesting becomes more difficult.\nI fully agree with this analysis. To me, though, the main drawback of ambient con-\ntext is that it masks potential problems in code. If injecting a logger explicitly into a\nListing 8.8\nStoring ILogger in a static field\nResolves ILogger through a \nstatic method, and stores it \nin a private static field\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n213\nSummary\ndomain class becomes so inconvenient that you have to resort to ambient context,\nthat’s a certain sign of trouble. You either log too much or use too many layers of indi-\nrection. In any case, ambient context is not a solution. Instead, tackle the root cause\nof the problem.\n The following listing shows one way to explicitly inject the logger: as a method\nargument. Another way is through the class constructor.\npublic void ChangeEmail(\nstring newEmail,      \nCompany company,      \nILogger logger)       \n{\nlogger.Info(\n$\"Changing email for user {UserId} to {newEmail}\");\n/* ... */\nlogger.Info($\"Email is changed for user {UserId}\");\n}\n8.7\nConclusion\nView communications with all out-of-process dependencies through the lens of whether\nthis communication is part of the application’s observable behavior or an imple-\nmentation detail. The log storage isn’t any different in that regard. Mock logging\nfunctionality if the logs are observable by non-programmers; don’t test it otherwise.\nIn the next chapter, we’ll dive deeper into the topic of mocking and best practices\nrelated to it. \nSummary\nAn integration test is any test that is not a unit test. Integration tests verify how\nyour system works in integration with out-of-process dependencies:\n– Integration tests cover controllers; unit tests cover algorithms and the domain\nmodel.\n– Integration tests provide better protection against regressions and resistance\nto refactoring; unit tests have better maintainability and feedback speed.\nThe bar for integration tests is higher than for unit tests: the score they have in\nthe metrics of protection against regressions and resistance to refactoring must\nbe higher than that of a unit test to offset the worse maintainability and feed-\nback speed. The Test Pyramid represents this trade-off: the majority of tests\nshould be fast and cheap unit tests, with a smaller number of slow and more\nexpensive integration tests that check correctness of the system as a whole:\n– Check as many of the business scenario’s edge cases as possible with unit\ntests. Use integration tests to cover one happy path, as well as any edge cases\nthat can’t be covered by unit tests.\nListing 8.9\nInjecting the logger explicitly\nMethod \ninjection \nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n214\nCHAPTER 8\nWhy integration testing?\n– The shape of the Test Pyramid depends on the project’s complexity. Simple\nprojects have little code in the domain model and thus can have an equal\nnumber of unit and integration tests. In the most trivial cases, there might be\nno unit tests.\nThe Fail Fast principle advocates for making bugs manifest themselves quickly\nand is a viable alternative to integration testing.\nManaged dependencies are out-of-process dependencies that are only accessible\nthrough your application. Interactions with managed dependencies aren’t\nobservable externally. A typical example is the application database.\nUnmanaged dependencies are out-of-process dependencies that other applications\nhave access to. Interactions with unmanaged dependencies are observable exter-\nnally. Typical examples include an SMTP server and a message bus.\nCommunications with managed dependencies are implementation details; com-\nmunications with unmanaged dependencies are part of your system’s observ-\nable behavior.\nUse real instances of managed dependencies in integration tests; replace unman-\naged dependencies with mocks.\nSometimes an out-of-process dependency exhibits attributes of both managed and\nunmanaged dependencies. A typical example is a database that other applications\nhave access to. Treat the observable part of the dependency as an unmanaged\ndependency: replace that part with mocks in tests. Treat the rest of the depen-\ndency as a managed dependency: verify its final state, not interactions with it.\nAn integration test must go through all layers that work with a managed depen-\ndency. In an example with a database, this means checking the state of that\ndatabase independently of the data used as input parameters.\nInterfaces with a single implementation are not abstractions and don’t provide\nloose coupling any more than the concrete classes that implement those inter-\nfaces. Trying to anticipate future implementations for such interfaces violates\nthe YAGNI (you aren’t gonna need it) principle.\nThe only legitimate reason to use interfaces with a single implementation is to\nenable mocking. Use such interfaces only for unmanaged dependencies. Use\nconcrete classes for managed dependencies.\nInterfaces with a single implementation used for in-process dependencies are\na red flag. Such interfaces hint at using mocks to check interactions between\ndomain classes, which leads to coupling tests to the code’s implementation\ndetails.\nHave an explicit and well-known place for the domain model in your code base.\nThe explicit boundary between domain classes and controllers makes it easier\nto tell unit and integration tests apart.\nAn excessive number of layers of indirection negatively affects your ability to\nreason about the code. Have as few layers of indirections as possible. In most\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n215\nSummary\nbackend systems, you can get away with just three of them: the domain model,\nan application services layer (controllers), and an infrastructure layer.\nCircular dependencies add cognitive load when you try to understand the code.\nA typical example is a callback (when a callee notifies the caller about the result\nof its work). Break the cycle by introducing a value object; use that value object\nto return the result from the callee to the caller.\nMultiple act sections in a test are only justified when that test works with out-of-\nprocess dependencies that are hard to bring into a desirable state. You should\nnever have multiple acts in a unit test, because unit tests don’t work with out-of-\nprocess dependencies. Multistep tests almost always belong to the category of\nend-to-end tests.\nSupport logging is intended for support staff and system administrators; it’s\npart of the application’s observable behavior. Diagnostic logging helps devel-\nopers understand what’s going on inside the application: it’s an implementa-\ntion detail.\nBecause support logging is a business requirement, reflect that requirement\nexplicitly in your code base. Introduce a special DomainLogger class where you\nlist all the support logging needed for the business.\nTreat support logging like any other functionality that works with an out-of-pro-\ncess dependency. Use domain events to track changes in the domain model;\nconvert those domain events into calls to DomainLogger in controllers.\nDon’t test diagnostic logging. Unlike support logging, you can do diagnostic\nlogging directly in the domain model.\nUse diagnostic logging sporadically. Excessive diagnostic logging clutters the\ncode and damages the logs’ signal-to-noise ratio. Ideally, you should only use\ndiagnostic logging for unhandled exceptions.\nAlways inject all dependencies explicitly (including loggers), either via the con-\nstructor or as a method argument.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n216\nMocking best practices\nAs you might remember from chapter 5, a mock is a test double that helps to emu-\nlate and examine interactions between the system under test and its dependencies.\nAs you might also remember from chapter 8, mocks should only be applied to\nunmanaged dependencies (interactions with such dependencies are observable by\nexternal applications). Using mocks for anything else results in brittle tests (tests that\nlack the metric of resistance to refactoring). When it comes to mocks, adhering to\nthis one guideline will get you about two-thirds of the way to success.\n This chapter shows the remaining guidelines that will help you develop inte-\ngration tests that have the greatest possible value by maxing out mocks’ resistance\nto refactoring and protection against regressions. I’ll first show a typical use of\nmocks, describe its drawbacks, and then demonstrate how you can overcome\nthose drawbacks.\nThis chapter covers\nMaximizing the value of mocks\nReplacing mocks with spies\nMocking best practices\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n217\nMaximizing mocks’ value\n9.1\nMaximizing mocks’ value\nIt’s important to limit the use of mocks to unmanaged dependencies, but that’s only\nthe first step on the way to maximizing the value of mocks. This topic is best explained\nwith an example, so I’ll continue using the CRM system from earlier chapters as a sam-\nple project. I’ll remind you of its functionality and show the integration test we ended\nup with. After that, you’ll see how that test can be improved with regard to mocking.\n As you might recall, the CRM system currently supports only one use case: chang-\ning a user’s email. The following listing shows where we left off with the controller.\npublic class UserController\n{\nprivate readonly Database _database;\nprivate readonly EventDispatcher _eventDispatcher;\npublic UserController(\nDatabase database,\nIMessageBus messageBus,\nIDomainLogger domainLogger)\n{\n_database = database;\n_eventDispatcher = new EventDispatcher(\nmessageBus, domainLogger);\n}\npublic string ChangeEmail(int userId, string newEmail)\n{\nobject[] userData = _database.GetUserById(userId);\nUser user = UserFactory.Create(userData);\nstring error = user.CanChangeEmail();\nif (error != null)\nreturn error;\nobject[] companyData = _database.GetCompany();\nCompany company = CompanyFactory.Create(companyData);\nuser.ChangeEmail(newEmail, company);\n_database.SaveCompany(company);\n_database.SaveUser(user);\n_eventDispatcher.Dispatch(user.DomainEvents);\nreturn \"OK\";\n}\n}\nNote that there’s no longer any diagnostic logging, but support logging (the IDomain-\nLogger interface) is still in place (see chapter 8 for more details). Also, listing 9.1\nintroduces a new class: the EventDispatcher. It converts domain events generated by\nListing 9.1\nUser controller\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "page_number": 232
    },
    {
      "number": 26,
      "title": "Segment 26 (pages 240-247)",
      "start_page": 240,
      "end_page": 247,
      "detection_method": "topic_boundary",
      "content": "218\nCHAPTER 9\nMocking best practices\nthe domain model into calls to unmanaged dependencies (something that the control-\nler previously did by itself), as shown next.\npublic class EventDispatcher\n{\nprivate readonly IMessageBus _messageBus;\nprivate readonly IDomainLogger _domainLogger;\npublic EventDispatcher(\nIMessageBus messageBus,\nIDomainLogger domainLogger)\n{\n_domainLogger = domainLogger;\n_messageBus = messageBus;\n}\npublic void Dispatch(List<IDomainEvent> events)\n{\nforeach (IDomainEvent ev in events)\n{\nDispatch(ev);\n}\n}\nprivate void Dispatch(IDomainEvent ev)\n{\nswitch (ev)\n{\ncase EmailChangedEvent emailChangedEvent:\n_messageBus.SendEmailChangedMessage(\nemailChangedEvent.UserId,\nemailChangedEvent.NewEmail);\nbreak;\ncase UserTypeChangedEvent userTypeChangedEvent:\n_domainLogger.UserTypeHasChanged(\nuserTypeChangedEvent.UserId,\nuserTypeChangedEvent.OldType,\nuserTypeChangedEvent.NewType);\nbreak;\n}\n}\n}\nFinally, the following listing shows the integration test. This test goes through all out-\nof-process dependencies (both managed and unmanaged).\n[Fact]\npublic void Changing_email_from_corporate_to_non_corporate()\n{\nListing 9.2\nEvent dispatcher\nListing 9.3\nIntegration test\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n219\nMaximizing mocks’ value\n// Arrange\nvar db = new Database(ConnectionString);\nUser user = CreateUser(\"user@mycorp.com\", UserType.Employee, db);\nCreateCompany(\"mycorp.com\", 1, db);\nvar messageBusMock = new Mock<IMessageBus>();   \nvar loggerMock = new Mock<IDomainLogger>();     \nvar sut = new UserController(\ndb, messageBusMock.Object, loggerMock.Object);\n// Act\nstring result = sut.ChangeEmail(user.UserId, \"new@gmail.com\");\n// Assert\nAssert.Equal(\"OK\", result);\nobject[] userData = db.GetUserById(user.UserId);\nUser userFromDb = UserFactory.Create(userData);\nAssert.Equal(\"new@gmail.com\", userFromDb.Email);\nAssert.Equal(UserType.Customer, userFromDb.Type);\nobject[] companyData = db.GetCompany();\nCompany companyFromDb = CompanyFactory.Create(companyData);\nAssert.Equal(0, companyFromDb.NumberOfEmployees);\nmessageBusMock.Verify(\n  \nx => x.SendEmailChangedMessage(\n  \nuser.UserId, \"new@gmail.com\"),  \nTimes.Once);\n  \nloggerMock.Verify(\n  \nx => x.UserTypeHasChanged(\n  \nuser.UserId,\n  \nUserType.Employee,\n  \nUserType.Customer),\n  \nTimes.Once);\n  \n}\nThis test mocks out two unmanaged dependencies: IMessageBus and IDomainLogger.\nI’ll focus on IMessageBus first. We’ll discuss IDomainLogger later in this chapter.\n9.1.1\nVerifying interactions at the system edges\nLet’s discuss why the mocks used by the integration test in listing 9.3 aren’t ideal in\nterms of their protection against regressions and resistance to refactoring and how we\ncan fix that.\nTIP\nWhen mocking, always adhere to the following guideline: verify interac-\ntions with unmanaged dependencies at the very edges of your system.\nThe problem with messageBusMock in listing 9.3 is that the IMessageBus interface\ndoesn’t reside at the system’s edge. Look at that interface’s implementation.\n \nSets up the \nmocks\nVerifies the \ninteractions \nwith the mocks\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n220\nCHAPTER 9\nMocking best practices\npublic interface IMessageBus\n{\nvoid SendEmailChangedMessage(int userId, string newEmail);\n}\npublic class MessageBus : IMessageBus\n{\nprivate readonly IBus _bus;\npublic void SendEmailChangedMessage(\nint userId, string newEmail)\n{\n_bus.Send(\"Type: USER EMAIL CHANGED; \" +\n$\"Id: {userId}; \" +\n$\"NewEmail: {newEmail}\");\n}\n}\npublic interface IBus\n{\nvoid Send(string message);\n}\nBoth the IMessageBus and IBus interfaces (and the classes implementing them) belong\nto our project’s code base. IBus is a wrapper on top of the message bus SDK library (pro-\nvided by the company that develops that message bus). This wrapper encapsulates non-\nessential technical details, such as connection credentials, and exposes a nice, clean\ninterface for sending arbitrary text messages to the bus. IMessageBus is a wrapper on\ntop of IBus; it defines messages specific to your domain. IMessageBus helps you keep all\nsuch messages in one place and reuse them across the application.\n It’s possible to merge the IBus and IMessageBus interfaces together, but that\nwould be a suboptimal solution. These two responsibilities—hiding the external\nlibrary’s complexity and holding all application messages in one place—are best kept\nseparated. This is the same situation as with ILogger and IDomainLogger, which you\nsaw in chapter 8. IDomainLogger implements specific logging functionality required\nby the business, and it does that by using the generic ILogger behind the scenes.\n Figure 9.1 shows where IBus and IMessageBus stand from a hexagonal architec-\nture perspective: IBus is the last link in the chain of types between the controller and\nthe message bus, while IMessageBus is only an intermediate step on the way.\n Mocking IBus instead of IMessageBus maximizes the mock’s protection against\nregressions. As you might remember from chapter 4, protection against regressions is\na function of the amount of code that is executed during the test. Mocking the very\nlast type that communicates with the unmanaged dependency increases the number\nof classes the integration test goes through and thus improves the protection. This\nguideline is also the reason you don’t want to mock EventDispatcher. It resides even\nfurther away from the edge of the system, compared to IMessageBus.\nListing 9.4\nMessage bus \nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n221\nMaximizing mocks’ value\nHere’s the integration test after retargeting it from IMessageBus to IBus. I’m omitting\nthe parts that didn’t change from listing 9.3.\n[Fact]\npublic void Changing_email_from_corporate_to_non_corporate()\n{\nvar busMock = new Mock<IBus>();\nvar messageBus = new MessageBus(busMock.Object);     \nvar loggerMock = new Mock<IDomainLogger>();\nvar sut = new UserController(db, messageBus, loggerMock.Object);\n/* ... */\nbusMock.Verify(\nx => x.Send(\n\"Type: USER EMAIL CHANGED; \" +  \n$\"Id: {user.UserId}; \" +\n  \n\"NewEmail: new@gmail.com\"),\n  \nTimes.Once);\n}\nListing 9.5\nIntegration test targeting IBus\nExternal client\nMessage bus\nIMessageBus\nController\nDomain model\nIBus\nFigure 9.1\nIBus resides at the system’s edge; IMessageBus is only an intermediate \nlink in the chain of types between the controller and the message bus. Mocking IBus \ninstead of IMessageBus achieves the best protection against regressions.\nUses a concrete \nclass instead of \nthe interface\nVerifies the actual \nmessage sent to \nthe bus\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n222\nCHAPTER 9\nMocking best practices\nNotice how the test now uses the concrete MessageBus class and not the correspond-\ning IMessageBus interface. IMessageBus is an interface with a single implementation,\nand, as you’ll remember from chapter 8, mocking is the only legitimate reason to have\nsuch interfaces. Because we no longer mock IMessageBus, this interface can be\ndeleted and its usages replaced with MessageBus.\n Also notice how the test in listing 9.5 checks the text message sent to the bus. Com-\npare it to the previous version:\nmessageBusMock.Verify(\nx => x.SendEmailChangedMessage(user.UserId, \"new@gmail.com\"),\nTimes.Once);\nThere’s a huge difference between verifying a call to a custom class that you wrote and\nthe actual text sent to external systems. External systems expect text messages from your\napplication, not calls to classes like MessageBus. In fact, text messages are the only side\neffect observable externally; classes that participate in producing those messages are\nmere implementation details. Thus, in addition to the increased protection against\nregressions, verifying interactions at the very edges of your system also improves resis-\ntance to refactoring. The resulting tests are less exposed to potential false positives; no\nmatter what refactorings take place, such tests won’t turn red as long as the message’s\nstructure is preserved.\n The same mechanism is at play here as the one that gives integration and end-to-end\ntests additional resistance to refactoring compared to unit tests. They are more detached\nfrom the code base and, therefore, aren’t affected as much during low-level refactorings.\nTIP\nA call to an unmanaged dependency goes through several stages before\nit leaves your application. Pick the last such stage. It is the best way to ensure\nbackward compatibility with external systems, which is the goal that mocks\nhelp you achieve. \n9.1.2\nReplacing mocks with spies\nAs you may remember from chapter 5, a spy is a variation of a test double that serves\nthe same purpose as a mock. The only difference is that spies are written manually,\nwhereas mocks are created with the help of a mocking framework. Indeed, spies are\noften called handwritten mocks.\n It turns out that, when it comes to classes residing at the system edges, spies are supe-\nrior to mocks. Spies help you reuse code in the assertion phase, thereby reducing the\ntest’s size and improving readability. The next listing shows an example of a spy that\nworks on top of IBus.\npublic interface IBus\n{\nvoid Send(string message);\n}\nListing 9.6\nA spy (also known as a handwritten mock)\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n223\nMaximizing mocks’ value\npublic class BusSpy : IBus\n{\nprivate List<string> _sentMessages =    \nnew List<string>();\n    \npublic void Send(string message)\n{\n_sentMessages.Add(message);\n    \n}\npublic BusSpy ShouldSendNumberOfMessages(int number)\n{\nAssert.Equal(number, _sentMessages.Count);\nreturn this;\n}\npublic BusSpy WithEmailChangedMessage(int userId, string newEmail)\n{\nstring message = \"Type: USER EMAIL CHANGED; \" +\n$\"Id: {userId}; \" +\n$\"NewEmail: {newEmail}\";\nAssert.Contains(\n   \n_sentMessages, x => x == message);   \nreturn this;\n}\n}\nThe following listing is a new version of the integration test. Again, I’m showing only\nthe relevant parts.\n[Fact]\npublic void Changing_email_from_corporate_to_non_corporate()\n{\nvar busSpy = new BusSpy();\nvar messageBus = new MessageBus(busSpy);\nvar loggerMock = new Mock<IDomainLogger>();\nvar sut = new UserController(db, messageBus, loggerMock.Object);\n/* ... */\nbusSpy.ShouldSendNumberOfMessages(1)\n.WithEmailChangedMessage(user.UserId, \"new@gmail.com\");\n}\nVerifying the interactions with the message bus is now succinct and expressive, thanks\nto the fluent interface that BusSpy provides. With that fluent interface, you can chain\ntogether several assertions, thus forming cohesive, almost plain-English sentences.\nTIP\nYou can rename BusSpy into BusMock. As I mentioned earlier, the differ-\nence between a mock and a spy is an implementation detail. Most programmers\nListing 9.7\nUsing the spy from listing 6.43\nStores all sent \nmessages \nlocally\nAsserts that the \nmessage has been sent\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n224\nCHAPTER 9\nMocking best practices\naren’t familiar with the term spy, though, so renaming the spy as BusMock can\nsave your colleagues unnecessary confusion.\nThere’s a reasonable question to be asked here: didn’t we just make a full circle and\ncome back to where we started? The version of the test in listing 9.7 looks a lot like the\nearlier version that mocked IMessageBus:\nmessageBusMock.Verify(\nx => x.SendEmailChangedMessage(\n  \nuser.UserId, \"new@gmail.com\"),  \nTimes.Once);      \nThese assertions are similar because both BusSpy and MessageBus are wrappers on\ntop of IBus. But there’s a crucial difference between the two: BusSpy is part of the test\ncode, whereas MessageBus belongs to the production code. This difference is import-\nant because you shouldn’t rely on the production code when making assertions in tests.\n Think of your tests as auditors. A good auditor wouldn’t just take the auditee’s\nwords at face value; they would double-check everything. The same is true with the\nspy: it provides an independent checkpoint that raises an alarm when the message\nstructure is changed. On the other hand, a mock on IMessageBus puts too much trust\nin the production code. \n9.1.3\nWhat about IDomainLogger?\nThe mock that previously verified interactions with IMessageBus is now targeted at\nIBus, which resides at the system’s edge. Here are the current mock assertions in the\nintegration test.\nbusSpy.ShouldSendNumberOfMessages(1)\n  \n.WithEmailChangedMessage(\n  \nuser.UserId, \"new@gmail.com\");  \nloggerMock.Verify(\n    \nx => x.UserTypeHasChanged(\n  \nuser.UserId,\n  \nUserType.Employee,\n  \nUserType.Customer),\n  \nTimes.Once);\n  \nNote that just as MessageBus is a wrapper on top of IBus, DomainLogger is a wrapper\non top of ILogger (see chapter 8 for more details). Shouldn’t the test be retargeted at\nILogger, too, because this interface also resides at the application boundary?\n In most projects, such retargeting isn’t necessary. While the logger and the mes-\nsage bus are unmanaged dependencies and, therefore, both require maintaining\nbackward compatibility, the accuracy of that compatibility doesn’t have to be the\nsame. With the message bus, it’s important not to allow any changes to the structure of\nListing 9.8\nMock assertions\nSame as WithEmailChanged-\nMessage(user.UserId, \n\"new@gmail.com\")\nSame as \nShouldSendNumberOfMessages(1)\nChecks \ninteractions \nwith IBus\nChecks \ninteractions with \nIDomainLogger\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n225\nMocking best practices\nthe messages, because you never know how external systems will react to such\nchanges. But the exact structure of text logs is not that important for the intended\naudience (support staff and system administrators). What’s important is the existence\nof those logs and the information they carry. Thus, mocking IDomainLogger alone\nprovides the necessary level of protection. \n9.2\nMocking best practices\nYou’ve learned two major mocking best practices so far:\nApplying mocks to unmanaged dependencies only\nVerifying the interactions with those dependencies at the very edges of your\nsystem\nIn this section, I explain the remaining best practices:\nUsing mocks in integration tests only, not in unit tests\nAlways verifying the number of calls made to the mock\nMocking only types that you own\n9.2.1\nMocks are for integration tests only\nThe guideline saying that mocks are for integration tests only, and that you shouldn’t\nuse mocks in unit tests, stems from the foundational principle described in chapter 7:\nthe separation of business logic and orchestration. Your code should either communi-\ncate with out-of-process dependencies or be complex, but never both. This principle\nnaturally leads to the formation of two distinct layers: the domain model (that handles\ncomplexity) and controllers (that handle the communication).\n Tests on the domain model fall into the category of unit tests; tests covering con-\ntrollers are integration tests. Because mocks are for unmanaged dependencies only,\nand because controllers are the only code working with such dependencies, you\nshould only apply mocking when testing controllers—in integration tests. \n9.2.2\nNot just one mock per test\nYou might sometimes hear the guideline of having only one mock per test. According\nto this guideline, if you have more than one mock, you are likely testing several things\nat a time.\n This is a misconception that follows from a more foundational misunderstanding\ncovered in chapter 2: that a unit in a unit test refers to a unit of code, and all such units\nmust be tested in isolation from each other. On the contrary: the term unit means\na unit of behavior, not a unit of code. The amount of code it takes to implement such a\nunit of behavior is irrelevant. It could span across multiple classes, a single class, or\ntake up just a tiny method.\n With mocks, the same principle is at play: it’s irrelevant how many mocks it takes to ver-\nify a unit of behavior. Earlier in this chapter, it took us two mocks to check the scenario\nof changing the user email from corporate to non-corporate: one for the logger and\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "page_number": 240
    },
    {
      "number": 27,
      "title": "Segment 27 (pages 248-256)",
      "start_page": 248,
      "end_page": 256,
      "detection_method": "topic_boundary",
      "content": "226\nCHAPTER 9\nMocking best practices\nthe other for the message bus. That number could have been larger. In fact, you don’t\nhave control over how many mocks to use in an integration test. The number of\nmocks depends solely on the number of unmanaged dependencies participating in\nthe operation. \n9.2.3\nVerifying the number of calls\nWhen it comes to communications with unmanaged dependencies, it’s important to\nensure both of the following:\nThe existence of expected calls\nThe absence of unexpected calls\nThis requirement, once again, stems from the need to maintain backward compatibil-\nity with unmanaged dependencies. The compatibility must go both ways: your appli-\ncation shouldn’t omit messages that external systems expect, and it also shouldn’t\nproduce unexpected messages. It’s not enough to check that the system under test\nsends a message like this:\nmessageBusMock.Verify(\nx => x.SendEmailChangedMessage(user.UserId, \"new@gmail.com\"));\nYou also need to ensure that this message is sent exactly once:\nmessageBusMock.Verify(\nx => x.SendEmailChangedMessage(user.UserId, \"new@gmail.com\"),\nTimes.Once);      \nWith most mocking libraries, you can also explicitly verify that no other calls are\nmade on the mock. In Moq (the mocking library of my choice), this verification\nlooks as follows:\nmessageBusMock.Verify(\nx => x.SendEmailChangedMessage(user.UserId, \"new@gmail.com\"),\nTimes.Once);\nmessageBusMock.VerifyNoOtherCalls();     \nBusSpy implements this functionality, too:\nbusSpy\n.ShouldSendNumberOfMessages(1)\n.WithEmailChangedMessage(user.UserId, \"new@gmail.com\");\nThe spy’s check ShouldSendNumberOfMessages(1) encompasses both Times.Once and\nVerifyNoOtherCalls() verifications from the mock. \nEnsures that the method \nis called only once\nThe additional \ncheck\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n227\nSummary\n9.2.4\nOnly mock types that you own\nThe last guideline I’d like to talk about is mocking only types that you own. It was first\nintroduced by Steve Freeman and Nat Pryce.1 The guideline states that you should\nalways write your own adapters on top of third-party libraries and mock those adapters\ninstead of the underlying types. A few of their arguments are as follows:\nYou often don’t have a deep understanding of how the third-party code works.\nEven if that code already provides built-in interfaces, it’s risky to mock those\ninterfaces, because you have to be sure the behavior you mock matches what\nthe external library actually does.\nAdapters abstract non-essential technical details of the third-party code and\ndefine the relationship with the library in your application’s terms.\nI fully agree with this analysis. Adapters, in effect, act as an anti-corruption layer\nbetween your code and the external world.2 These help you to\nAbstract the underlying library’s complexity\nOnly expose features you need from the library\nDo that using your project’s domain language\nThe IBus interface in our sample CRM project serves exactly that purpose. Even if the\nunderlying message bus’s library provides as nice and clean an interface as IBus, you\nare still better off introducing your own wrapper on top of it. You never know how the\nthird-party code will change when you upgrade the library. Such an upgrade could\ncause a ripple effect across the whole code base! The additional abstraction layer\nrestricts that ripple effect to just one class: the adapter itself.\n Note that the “mock your own types” guideline doesn’t apply to in-process depen-\ndencies. As I explained previously, mocks are for unmanaged dependencies only.\nThus, there’s no need to abstract in-memory or managed dependencies. For instance,\nif a library provides a date and time API, you can use that API as-is, because it doesn’t\nreach out to unmanaged dependencies. Similarly, there’s no need to abstract an ORM\nas long as it’s used for accessing a database that isn’t visible to external applications.\nOf course, you can introduce your own wrapper on top of any library, but it’s rarely\nworth the effort for anything other than unmanaged dependencies. \nSummary\nVerify interactions with an unmanaged dependency at the very edges of your\nsystem. Mock the last type in the chain of types between the controller and the\nunmanaged dependency. This helps you increase both protection against\nregressions (due to more code being validated by the integration test) and\n1 See page 69 in Growing Object-Oriented Software, Guided by Tests by Steve Freeman and Nat Pryce (Addison-Wesley\nProfessional, 2009).\n2 See Domain-Driven Design: Tackling Complexity in the Heart of Software by Eric Evans (Addison-Wesley, 2003).\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n228\nCHAPTER 9\nMocking best practices\nresistance to refactoring (due to detaching the mock from the code’s imple-\nmentation details).\nSpies are handwritten mocks. When it comes to classes residing at the system’s\nedges, spies are superior to mocks. They help you reuse code in the assertion\nphase, thereby reducing the test’s size and improving readability.\nDon’t rely on production code when making assertions. Use a separate set of lit-\nerals and constants in tests. Duplicate those literals and constants from the pro-\nduction code if necessary. Tests should provide a checkpoint independent of\nthe production code. Otherwise, you risk producing tautology tests (tests that\ndon’t verify anything and contain semantically meaningless assertions).\nNot all unmanaged dependencies require the same level of backward compati-\nbility. If the exact structure of the message isn’t important, and you only want to\nverify the existence of that message and the information it carries, you can\nignore the guideline of verifying interactions with unmanaged dependencies at\nthe very edges of your system. The typical example is logging.\nBecause mocks are for unmanaged dependencies only, and because controllers\nare the only code working with such dependencies, you should only apply mock-\ning when testing controllers—in integration tests. Don’t use mocks in unit tests.\nThe number of mocks used in a test is irrelevant. That number depends solely\non the number of unmanaged dependencies participating in the operation.\nEnsure both the existence of expected calls and the absence of unexpected calls\nto mocks.\nOnly mock types that you own. Write your own adapters on top of third-party\nlibraries that provide access to unmanaged dependencies. Mock those adapters\ninstead of the underlying types.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n229\nTesting the database\nThe last piece of the puzzle in integration testing is managed out-of-process depen-\ndencies. The most common example of a managed dependency is an application\ndatabase—a database no other application has access to.\n Running tests against a real database provides bulletproof protection against\nregressions, but those tests aren’t easy to set up. This chapter shows the preliminary\nsteps you need to take before you can start testing your database: it covers keeping\ntrack of the database schema, explains the difference between the state-based and\nmigration-based database delivery approaches, and demonstrates why you should\nchoose the latter over the former.\n After learning the basics, you’ll see how to manage transactions during the test,\nclean up leftover data, and keep tests small by eliminating insignificant parts and\namplifying the essentials. This chapter focuses on relational databases, but many of\nThis chapter covers\nPrerequisites for testing the database\nDatabase testing best practices\nTest data life cycle\nManaging database transactions in tests\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n230\nCHAPTER 10\nTesting the database\nthe same principles are applicable to other types of data stores such as document-ori-\nented databases or even plain text file storages.\n10.1\nPrerequisites for testing the database\nAs you might recall from chapter 8, managed dependencies should be included as-is\nin integration tests. That makes working with those dependencies more laborious\nthan unmanaged ones because using a mock is out of the question. But even before\nyou start writing tests, you must take preparatory steps to enable integration testing. In\nthis section, you’ll see these prerequisites:\nKeeping the database in the source control system\nUsing a separate database instance for every developer\nApplying the migration-based approach to database delivery\nLike almost everything in testing, though, practices that facilitate testing also improve\nthe health of your database in general. You’ll get value out of those practices even if\nyou don’t write integration tests.\n10.1.1 Keeping the database in the source control system\nThe first step on the way to testing the database is treating the database schema as reg-\nular code. Just as with regular code, a database schema is best stored in a source con-\ntrol system such as Git.\n I’ve worked on projects where programmers maintained a dedicated database\ninstance, which served as a reference point (a model database). During development,\nall schema changes accumulated in that instance. Upon production deployments, the\nteam compared the production and model databases, used a special tool to generate\nupgrade scripts, and ran those scripts in production (figure 10.1).\nModel\ndatabase\nProduction\ndatabase\nCompare\nModiﬁcations by\nprogrammers\nUpgrade\nscripts\nGenerate\nApply\nComparison\ntool\nFigure 10.1\nHaving a dedicated instance as a model database is an anti-pattern. The database \nschema is best stored in a source control system.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n231\nPrerequisites for testing the database\nUsing a model database is a horrible way to maintain database schema. That’s because\nthere’s\nNo change history—You can’t trace the database schema back to some point in\nthe past, which might be important when reproducing bugs in production.\nNo single source of truth—The model database becomes a competing source of\ntruth about the state of development. Maintaining two such sources (Git and\nthe model database) creates an additional burden.\nOn the other hand, keeping all the database schema updates in the source control sys-\ntem helps you to maintain a single source of truth and also to track database changes\nalong with the changes of regular code. No modifications to the database structure\nshould be made outside of the source control. \n10.1.2 Reference data is part of the database schema\nWhen it comes to the database schema, the usual suspects are tables, views, indexes,\nstored procedures, and anything else that forms a blueprint of how the database is\nconstructed. The schema itself is represented in the form of SQL scripts. You\nshould be able to use those scripts to create a fully functional, up-to-date database\ninstance of your own at any time during development. However, there’s another\npart of the database that belongs to the database schema but is rarely viewed as\nsuch: reference data.\nDEFINITION\nReference data is data that must be prepopulated in order for the\napplication to operate properly.\nTake the CRM system from the earlier chapters, for example. Its users can be either of\ntype Customer or type Employee. Let’s say that you want to create a table with all user\ntypes and introduce a foreign key constraint from User to that table. Such a constraint\nwould provide an additional guarantee that the application won’t ever assign a user a\nnonexistent type. In this scenario, the content of the UserType table would be refer-\nence data because the application relies on its existence in order to persist users in the\ndatabase.\nTIP\nThere’s a simple way to differentiate reference data from regular data.\nIf your application can modify the data, it’s regular data; if not, it’s refer-\nence data.\nBecause reference data is essential for your application, you should keep it in the\nsource control system along with tables, views, and other parts of the database schema,\nin the form of SQL INSERT statements.\n Note that although reference data is normally stored separately from regular data,\nthe two can sometimes coexist in the same table. To make this work, you need to intro-\nduce a flag differentiating data that can be modified (regular data) from data that can’t\nbe modified (reference data) and forbid your application from changing the latter. \nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n232\nCHAPTER 10\nTesting the database\n10.1.3 Separate instance for every developer\nIt’s difficult enough to run tests against a real database. It becomes even more difficult\nif you have to share that database with other developers. The use of a shared database\nhinders the development process because\nTests run by different developers interfere with each other.\nNon-backward-compatible changes can block the work of other developers.\nKeep a separate database instance for every developer, preferably on that developer’s\nown machine in order to maximize test execution speed. \n10.1.4 State-based vs. migration-based database delivery\nThere are two major approaches to database delivery: state-based and migration-based.\nThe migration-based approach is more difficult to implement and maintain initially,\nbut it works much better than the state-based approach in the long run.\nTHE STATE-BASED APPROACH\nThe state-based approach to database delivery is similar to what I described in figure\n10.1. You also have a model database that you maintain throughout development.\nDuring deployments, a comparison tool generates scripts for the production database\nto bring it up to date with the model database. The difference is that with the state-\nbased approach, you don’t actually have a physical model database as a source of\ntruth. Instead, you have SQL scripts that you can use to create that database. The\nscripts are stored in the source control.\n In the state-based approach, the comparison tool does all the hard lifting. What-\never the state of the production database, the tool does everything needed to get it in\nsync with the model database: delete unnecessary tables, create new ones, rename col-\numns, and so on. \nTHE MIGRATION-BASED APPROACH\nOn the other hand, the migration-based approach emphasizes the use of explicit\nmigrations that transition the database from one version to another (figure 10.2).\nWith this approach, you don’t use tools to automatically synchronize the production\nand development databases; you come up with upgrade scripts yourself. However, a\ndatabase comparison tool can still be useful when detecting undocumented changes\nin the production database schema.\nCREATE TABLE\ndbo.Customer (…)\nALTER TABLE\ndbo.Customer (…)\nCREATE TABLE\ndbo.User (…)\nMigration 1\nMigration 2\nMigration 3\nFigure 10.2\nThe migration-based approach to database delivery emphasizes the use of explicit \nmigrations that transition the database from one version to another.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n233\nPrerequisites for testing the database\nIn the migration-based approach, migrations and not the database state become the\nartifacts you store in the source control. Migrations are usually represented with\nplain SQL scripts (popular tools include Flyway [https://flywaydb.org] and Liquibase\n[https://liquibase.org]), but they can also be written using a DSL-like language that\ngets translated into SQL. The following example shows a C# class that represents a\ndatabase migration with the help of the FluentMigrator library (https://github.com/\nfluentmigrator/fluentmigrator):\n[Migration(1)]\n          \npublic class CreateUserTable : Migration\n{\npublic override void Up()       \n{\nCreate.Table(\"Users\");\n}\npublic override void Down()    \n{\nDelete.Table(\"Users\");\n}\n}\nPREFER THE MIGRATION-BASED APPROACH OVER THE STATE-BASED ONE\nThe difference between the state-based and migration-based approaches to database\ndelivery comes down to (as their names imply) state versus migrations (see figure 10.3):\nThe state-based approach makes the state explicit (by virtue of storing that\nstate in the source control) and lets the comparison tool implicitly control the\nmigrations.\nThe migration-based approach makes the migrations explicit but leaves the state\nimplicit. It’s impossible to view the database state directly; you have to assemble\nit from the migrations.\nMigration \nnumber\nForward \nmigration\nBackward migration (helpful \nwhen downgrading to an \nearlier database version to \nreproduce a bug)\nState-based\napproach\nState of the database\nMigration mechanism\nMigration-based\napproach\nImplicit\nImplicit\nExplicit\nExplicit\nFigure 10.3\nThe state-based approach makes the state explicit and \nmigrations implicit; the migration-based approach makes the opposite choice.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n234\nCHAPTER 10\nTesting the database\nSuch a distinction leads to different sets of trade-offs. The explicitness of the database\nstate makes it easier to handle merge conflicts, while explicit migrations help to tackle\ndata motion.\nDEFINITION\nData motion is the process of changing the shape of existing data\nso that it conforms to the new database schema.\nAlthough the alleviation of merge conflicts and the ease of data motion might look\nlike equally important benefits, in the vast majority of projects, data motion is much more\nimportant than merge conflicts. Unless you haven’t yet released your application to pro-\nduction, you always have data that you can’t simply discard.\n For example, when splitting a Name column into FirstName and LastName, you not\nonly have to drop the Name column and create the new FirstName and LastName col-\numns, but you also have to write a script to split all existing names into two pieces.\nThere is no easy way to implement this change using the state-driven approach; com-\nparison tools are awful when it comes to managing data. The reason is that while the\ndatabase schema itself is objective, meaning there is only one way to interpret it, data\nis context-dependent. No tool can make reliable assumptions about data when gener-\nating upgrade scripts. You have to apply domain-specific rules in order to implement\nproper transformations.\n As a result, the state-based approach is impractical in the vast majority of projects.\nYou can use it temporarily, though, while the project still has not been released to pro-\nduction. After all, test data isn’t that important, and you can re-create it every time you\nchange the database. But once you release the first version, you will have to switch to\nthe migration-based approach in order to handle data motion properly.\nTIP\nApply every modification to the database schema (including reference\ndata) through migrations. Don’t modify migrations once they are committed\nto the source control. If a migration is incorrect, create a new migration\ninstead of fixing the old one. Make exceptions to this rule only when the\nincorrect migration can lead to data loss. \n10.2\nDatabase transaction management\nDatabase transaction management is a topic that’s important for both production and\ntest code. Proper transaction management in production code helps you avoid data\ninconsistencies. In tests, it helps you verify integration with the database in a close-to-\nproduction setting.\n In this section, I’ll first show how to handle transactions in the production code\n(the controller) and then demonstrate how to use them in integration tests. I’ll con-\ntinue using the same CRM project you saw in the earlier chapters as an example.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "page_number": 248
    },
    {
      "number": 28,
      "title": "Segment 28 (pages 257-265)",
      "start_page": 257,
      "end_page": 265,
      "detection_method": "topic_boundary",
      "content": "235\nDatabase transaction management\n10.2.1 Managing database transactions in production code\nOur sample CRM project uses the Database class to work with User and Company.\nDatabase creates a separate SQL connection on each method call. Every such connec-\ntion implicitly opens an independent transaction behind the scenes, as the following\nlisting shows.\npublic class Database\n{\nprivate readonly string _connectionString;\npublic Database(string connectionString)\n{\n_connectionString = connectionString;\n}\npublic void SaveUser(User user)\n{\nbool isNewUser = user.UserId == 0;\nusing (var connection =\nnew SqlConnection(_connectionString))      \n{\n/* Insert or update the user depending on isNewUser */\n}\n}\npublic void SaveCompany(Company company)\n{\nusing (var connection =\nnew SqlConnection(_connectionString))      \n{\n/* Update only; there's only one company */\n}\n}\n}\nAs a result, the user controller creates a total of four database transactions during a\nsingle business operation, as shown in the following listing.\npublic string ChangeEmail(int userId, string newEmail)\n{\nobject[] userData = _database.GetUserById(userId);    \nUser user = UserFactory.Create(userData);\nstring error = user.CanChangeEmail();\nif (error != null)\nreturn error;\nListing 10.1\nClass that enables access to the database\nListing 10.2\nUser controller\nOpens a\ndatabase\ntransaction\nOpens a new \ndatabase \ntransaction\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n236\nCHAPTER 10\nTesting the database\nobject[] companyData = _database.GetCompany();        \nCompany company = CompanyFactory.Create(companyData);\nuser.ChangeEmail(newEmail, company);\n_database.SaveCompany(company);                       \n_database.SaveUser(user);                             \n_eventDispatcher.Dispatch(user.DomainEvents);\nreturn \"OK\";\n}\nIt’s fine to open multiple transactions during read-only operations: for example, when\nreturning user information to the external client. But if the business operation\ninvolves data mutation, all updates taking place during that operation should be\natomic in order to avoid inconsistencies. For example, the controller can successfully\npersist the company but then fail when saving the user due to a database connectivity\nissue. As a result, the company’s NumberOfEmployees can become inconsistent with\nthe total number of Employee users in the database.\nDEFINITION\nAtomic updates are executed in an all-or-nothing manner. Each\nupdate in the set of atomic updates must either be complete in its entirety or\nhave no effect whatsoever.\nSEPARATING DATABASE CONNECTIONS FROM DATABASE TRANSACTIONS\nTo avoid potential inconsistencies, you need to introduce a separation between two\ntypes of decisions:\nWhat data to update\nWhether to keep the updates or roll them back\nSuch a separation is important because the controller can’t make these decisions\nsimultaneously. It only knows whether the updates can be kept when all the steps in\nthe business operation have succeeded. And it can only take those steps by accessing\nthe database and trying to make the updates. You can implement the separation\nbetween these responsibilities by splitting the Database class into repositories and a\ntransaction:\nRepositories are classes that enable access to and modification of the data in the\ndatabase. There will be two repositories in our sample project: one for User and\nthe other for Company.\nA transaction is a class that either commits or rolls back data updates in full. This\nwill be a custom class relying on the underlying database’s transactions to pro-\nvide atomicity of data modification.\nNot only do repositories and transactions have different responsibilities, but they also\nhave different lifespans. A transaction lives during the whole business operation and is\ndisposed of at the very end of it. A repository, on the other hand, is short-lived. You\nOpens a new \ndatabase \ntransaction\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n237\nDatabase transaction management\ncan dispose of a repository as soon as the call to the database is completed. As a result,\nrepositories always work on top of the current transaction. When connecting to the\ndatabase, a repository enlists itself into the transaction so that any data modifications\nmade during that connection can later be rolled back by the transaction.\n Figure 10.4 shows how the communication between the controller and the data-\nbase looks in listing 10.2. Each database call is wrapped into its own transaction;\nupdates are not atomic.\nFigure 10.5 shows the application after the introduction of explicit transactions. The\ntransaction mediates interactions between the controller and the database. All four\ndatabase calls are still there, but now data modifications are either committed or\nrolled back in full.\nThe following listing shows the controller after introducing a transaction and repositories.\npublic class UserController\n{\nprivate readonly Transaction _transaction;\nprivate readonly UserRepository _userRepository;\nListing 10.3\nUser controller, repositories, and a transaction\nDatabase\nGetUserById\nController\nSaveCompany\nGetCompany\nSaveUser\nFigure 10.4\nWrapping each \ndatabase call into a separate \ntransaction introduces a risk of \ninconsistencies due to hardware or \nsoftware failures. For example, the \napplication can update the number of \nemployees in the company but not \nthe employees themselves.\nTransaction\nDatabase\nController\nCommit tran\nCommit tran\nSaveUser\nSaveUser\nSaveCompany\nSaveCompany\nGetCompany\nGetCompany\nGetUserById\nGetUserById\nOpen tran\nOpen tran\nFigure 10.5\nThe transaction mediates interactions between the controller and the database and \nthus enables atomic data modification.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n238\nCHAPTER 10\nTesting the database\nprivate readonly CompanyRepository _companyRepository;\nprivate readonly EventDispatcher _eventDispatcher;\npublic UserController(\nTransaction transaction,     \nMessageBus messageBus,\nIDomainLogger domainLogger)\n{\n_transaction = transaction;\n_userRepository = new UserRepository(transaction);\n_companyRepository = new CompanyRepository(transaction);\n_eventDispatcher = new EventDispatcher(\nmessageBus, domainLogger);\n}\npublic string ChangeEmail(int userId, string newEmail)\n{\nobject[] userData = _userRepository           \n.GetUserById(userId);\n           \nUser user = UserFactory.Create(userData);\nstring error = user.CanChangeEmail();\nif (error != null)\nreturn error;\nobject[] companyData = _companyRepository     \n.GetCompany();\n      \nCompany company = CompanyFactory.Create(companyData);\nuser.ChangeEmail(newEmail, company);\n_companyRepository.SaveCompany(company);      \n_userRepository.SaveUser(user);\n      \n_eventDispatcher.Dispatch(user.DomainEvents);\n_transaction.Commit();     \nreturn \"OK\";\n}\n}\npublic class UserRepository\n{\nprivate readonly Transaction _transaction;\npublic UserRepository(Transaction transaction)    \n{\n_transaction = transaction;\n}\n/* ... */\n}\npublic class Transaction : IDisposable\n{\nAccepts a \ntransaction\nUses the\nrepositories\ninstead\nof the\nDatabase\nclass\nCommits the \ntransaction \non success\nInjects a \ntransaction into \na repository\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n239\nDatabase transaction management\npublic void Commit() { /* ... */ }\npublic void Dispose() { /* ... */ }\n}\nThe internals of the Transaction class aren’t important, but if you’re curious, I’m\nusing .NET’s standard TransactionScope behind the scenes. The important part\nabout Transaction is that it contains two methods:\n\nCommit()marks the transaction as successful. This is only called when the busi-\nness operation itself has succeeded and all data modifications are ready to be\npersisted.\n\nDispose()ends the transaction. This is called indiscriminately at the end of the\nbusiness operation. If Commit() was previously invoked, Dispose() persists all\ndata updates; otherwise, it rolls them back.\nSuch a combination of Commit() and Dispose() guarantees that the database is\naltered only during happy paths (the successful execution of the business scenario).\nThat’s why Commit() resides at the very end of the ChangeEmail() method. In the\nevent of any error, be it a validation error or an unhandled exception, the execution\nflow returns early and thereby prevents the transaction from being committed.\n Commit() is invoked by the controller because this method call requires decision-\nmaking. There’s no decision-making involved in calling Dispose(), though, so you\ncan delegate that method call to a class from the infrastructure layer. The same class\nthat instantiates the controller and provides it with the necessary dependencies\nshould also dispose of the transaction once the controller is done working.\n Notice how UserRepository requires Transaction as a constructor parameter.\nThis explicitly shows that repositories always work on top of transactions; a repository\ncan’t call the database on its own. \nUPGRADING THE TRANSACTION TO A UNIT OF WORK\nThe introduction of repositories and a transaction is a good way to avoid potential\ndata inconsistencies, but there’s an even better approach. You can upgrade the\nTransaction class to a unit of work.\nDEFINITION\nA unit of work maintains a list of objects affected by a business\noperation. Once the operation is completed, the unit of work figures out all\nupdates that need to be done to alter the database and executes those\nupdates as a single unit (hence the pattern name).\nThe main advantage of a unit of work over a plain transaction is the deferral of\nupdates. Unlike a transaction, a unit of work executes all updates at the end of the\nbusiness operation, thus minimizing the duration of the underlying database transac-\ntion and reducing data congestion (see figure 10.6). Often, this pattern also helps to\nreduce the number of database calls.\nNOTE\nDatabase transactions also implement the unit-of-work pattern.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n240\nCHAPTER 10\nTesting the database\nMaintaining a list of modified objects and then figuring out what SQL script to gener-\nate can look like a lot of work. In reality, though, you don’t need to do that work your-\nself. Most object-relational mapping (ORM) libraries implement the unit-of-work\npattern for you. In .NET, for example, you can use NHibernate or Entity Framework,\nboth of which provide classes that do all the hard lifting (those classes are ISession\nand DbContext, respectively). The following listing shows how UserController looks\nin combination with Entity Framework.\npublic class UserController\n{\nprivate readonly CrmContext _context;\nprivate readonly UserRepository _userRepository;\nprivate readonly CompanyRepository _companyRepository;\nprivate readonly EventDispatcher _eventDispatcher;\npublic UserController(\nCrmContext context,                     \nMessageBus messageBus,\nIDomainLogger domainLogger)\n{\n_context = context;\n_userRepository = new UserRepository(\ncontext);                           \n_companyRepository = new CompanyRepository(\ncontext);                           \n_eventDispatcher = new EventDispatcher(\nmessageBus, domainLogger);\n}\npublic string ChangeEmail(int userId, string newEmail)\n{\nUser user = _userRepository.GetUserById(userId);\nListing 10.4\nUser controller with Entity Framework\nUnit of work\nGetUserById\nDatabase\nSaveCompany\nGetCompany\nController\nSaveUser\nCreate\nSaveChanges\nGetUserById\nGetCompany\nSave all\nFigure 10.6\nA unit of work executes all updates at the end of the business operation. The updates \nare still wrapped in a database transaction, but that transaction lives for a shorter period of time, \nthus reducing data congestion.\nCrmContext\nreplaces\nTransaction.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n241\nDatabase transaction management\nstring error = user.CanChangeEmail();\nif (error != null)\nreturn error;\nCompany company = _companyRepository.GetCompany();\nuser.ChangeEmail(newEmail, company);\n_companyRepository.SaveCompany(company);\n_userRepository.SaveUser(user);\n_eventDispatcher.Dispatch(user.DomainEvents);\n_context.SaveChanges();  \nreturn \"OK\";\n}\n}\nCrmContext is a custom class that contains mapping between the domain model and\nthe database (it inherits from Entity Framework’s DbContext). The controller in list-\ning 10.4 uses CrmContext instead of Transaction. As a result,\nBoth repositories now work on top of CrmContext, just as they worked on top of\nTransaction in the previous version.\nThe controller commits changes to the database via context.SaveChanges()\ninstead of transaction.Commit().\nNotice that there’s no need for UserFactory and CompanyFactory anymore because\nEntity Framework now serves as a mapper between the raw database data and\ndomain objects.\nData inconsistencies in non-relational databases\nIt’s easy to avoid data inconsistencies when using a relational database: all major\nrelational databases provide atomic updates that can span as many rows as needed.\nBut how do you achieve the same level of protection with a non-relational database\nsuch as MongoDB?\nThe problem with most non-relational databases is the lack of transactions in the\nclassical sense; atomic updates are guaranteed only within a single document. If a\nbusiness operation affects multiple documents, it becomes prone to inconsisten-\ncies. (In non-relational databases, a document is the equivalent of a row.)\nNon-relational databases approach inconsistencies from a different angle: they\nrequire you to design your documents such that no business operation modifies more\nthan one of those documents at a time. This is possible because documents are\nmore flexible than rows in relational databases. A single document can store data of\nany shape and complexity and thus capture side effects of even the most sophisti-\ncated business operations.\nCrmContext \nreplaces \nTransaction.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n242\nCHAPTER 10\nTesting the database\n10.2.2 Managing database transactions in integration tests\nWhen it comes to managing database transactions in integration tests, adhere to the\nfollowing guideline: don’t reuse database transactions or units of work between sections of the\ntest. The following listing shows an example of reusing CrmContext in the integration\ntest after switching that test to Entity Framework.\n[Fact]\npublic void Changing_email_from_corporate_to_non_corporate()\n{\nusing (var context =\n   \nnew CrmContext(ConnectionString))   \n{\n// Arrange\nvar userRepository =\n         \nnew UserRepository(context);\n         \nvar companyRepository =\n         \nnew CompanyRepository(context);         \nvar user = new User(0, \"user@mycorp.com\",\nUserType.Employee, false);\nuserRepository.SaveUser(user);\nvar company = new Company(\"mycorp.com\", 1);\ncompanyRepository.SaveCompany(company);\ncontext.SaveChanges();                      \nvar busSpy = new BusSpy();\nvar messageBus = new MessageBus(busSpy);\nvar loggerMock = new Mock<IDomainLogger>();\nvar sut = new UserController(\ncontext,                     \nmessageBus,\nloggerMock.Object);\n// Act\nstring result = sut.ChangeEmail(user.UserId, \"new@gmail.com\");\n// Assert\nAssert.Equal(\"OK\", result);\nUser userFromDb = userRepository     \n.GetUserById(user.UserId);       \n(continued)\nIn domain-driven design, there’s a guideline saying that you shouldn’t modify more\nthan one aggregate per business operation. This guideline serves the same goal: pro-\ntecting you from data inconsistencies. The guideline is only applicable to systems\nthat work with document databases, though, where each document corresponds to\none aggregate. \nListing 10.5\nIntegration test reusing CrmContext\nCreates a \ncontext\nUses the context \nin the arrange \nsection . . .\n. . . in act . . .\n. . . and in assert\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n243\nTest data life cycle\nAssert.Equal(\"new@gmail.com\", userFromDb.Email);\nAssert.Equal(UserType.Customer, userFromDb.Type);\nCompany companyFromDb = companyRepository     \n.GetCompany();\n     \nAssert.Equal(0, companyFromDb.NumberOfEmployees);\nbusSpy.ShouldSendNumberOfMessages(1)\n.WithEmailChangedMessage(user.UserId, \"new@gmail.com\");\nloggerMock.Verify(\nx => x.UserTypeHasChanged(\nuser.UserId, UserType.Employee, UserType.Customer),\nTimes.Once);\n}\n}\nThis test uses the same instance of CrmContext in all three sections: arrange, act, and\nassert. This is a problem because such reuse of the unit of work creates an environment\nthat doesn’t match what the controller experiences in production. In production, each\nbusiness operation has an exclusive instance of CrmContext. That instance is created\nright before the controller method invocation and is disposed of immediately after.\n To avoid the risk of inconsistent behavior, integration tests should replicate the\nproduction environment as closely as possible, which means the act section must not\nshare CrmContext with anyone else. The arrange and assert sections must get their\nown instances of CrmContext too, because, as you might remember from chapter 8,\nit’s important to check the state of the database independently of the data used as\ninput parameters. And although the assert section does query the user and the com-\npany independently of the arrange section, these sections still share the same database\ncontext. That context can (and many ORMs do) cache the requested data for perfor-\nmance improvements.\nTIP\nUse at least three transactions or units of work in an integration test: one\nper each arrange, act, and assert section. \n10.3\nTest data life cycle\nThe shared database raises the problem of isolating integration tests from each other.\nTo solve this problem, you need to\nExecute integration tests sequentially.\nRemove leftover data between test runs.\nOverall, your tests shouldn’t depend on the state of the database. Your tests should\nbring that state to the required condition on their own.\n10.3.1 Parallel vs. sequential test execution\nParallel execution of integration tests involves significant effort. You have to ensure\nthat all test data is unique so no database constraints are violated and tests don’t acci-\ndentally pick up input data after each other. Cleaning up leftover data also becomes\n. . . and in assert\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "page_number": 257
    },
    {
      "number": 29,
      "title": "Segment 29 (pages 266-273)",
      "start_page": 266,
      "end_page": 273,
      "detection_method": "topic_boundary",
      "content": "244\nCHAPTER 10\nTesting the database\ntrickier. It’s more practical to run integration tests sequentially rather than spend time\ntrying to squeeze additional performance out of them.\n Most unit testing frameworks allow you to define separate test collections and\nselectively disable parallelization in them. Create two such collections (for unit and\nintegration tests), and then disable test parallelization in the collection with the inte-\ngration tests.\n As an alternative, you could parallelize tests using containers. For example, you\ncould put the model database on a Docker image and instantiate a new container\nfrom that image for each integration test. In practice, though, this approach creates\ntoo much of an additional maintenance burden. With Docker, you not only have to\nkeep track of the database itself, but you also need to\nMaintain Docker images\nMake sure each test gets its own container instance\nBatch integration tests (because you most likely won’t be able to create all con-\ntainer instances at once)\nDispose of used-up containers\nI don’t recommend using containers unless you absolutely need to minimize your\nintegration tests’ execution time. Again, it’s more practical to have just one database\ninstance per developer. You can run that single instance in Docker, though. I advocate\nagainst premature parallelization, not the use of Docker per se. \n10.3.2 Clearing data between test runs\nThere are four options to clean up leftover data between test runs:\nRestoring a database backup before each test—This approach addresses the problem\nof data cleanup but is much slower than the other three options. Even with con-\ntainers, the removal of a container instance and creation of a new one usually\ntakes several seconds, which quickly adds to the total test suite execution time.\nCleaning up data at the end of a test—This method is fast but susceptible to skip-\nping the cleanup phase. If the build server crashes in the middle of the test, or\nyou shut down the test in the debugger, the input data remains in the database\nand affects further test runs.\nWrapping each test in a database transaction and never committing it—In this case, all\nchanges made by the test and the SUT are rolled back automatically. This\napproach solves the problem of skipping the cleanup phase but poses another\nissue: the introduction of an overarching transaction can lead to inconsistent\nbehavior between the production and test environments. It’s the same problem\nas with reusing a unit of work: the additional transaction creates a setup that’s\ndifferent than that in production.\nCleaning up data at the beginning of a test—This is the best option. It works fast,\ndoesn’t result in inconsistent behavior, and isn’t prone to accidentally skipping\nthe cleanup phase.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n245\nTest data life cycle\nTIP\nThere’s no need for a separate teardown phase; implement that phase as\npart of the arrange section.\nThe data removal itself must be done in a particular order, to honor the database’s\nforeign key constraints. I sometimes see people use sophisticated algorithms to figure\nout relationships between tables and automatically generate the deletion script or\neven disable all integrity constraints and re-enable them afterward. This is unneces-\nsary. Write the SQL script manually: it’s simpler and gives you more granular control\nover the deletion process.\n Introduce a base class for all integration tests, and put the deletion script there. With\nsuch a base class, you will have the script run automatically at the start of each test, as\nshown in the following listing.\npublic abstract class IntegrationTests\n{\nprivate const string ConnectionString = \"...\";\nprotected IntegrationTests()\n{\nClearDatabase();\n}\nprivate void ClearDatabase()\n{\nstring query =\n\"DELETE FROM dbo.[User];\" +    \n\"DELETE FROM dbo.Company;\";    \nusing (var connection = new SqlConnection(ConnectionString))\n{\nvar command = new SqlCommand(query, connection)\n{\nCommandType = CommandType.Text\n};\nconnection.Open();\ncommand.ExecuteNonQuery();\n}\n}\n}\nTIP\nThe deletion script must remove all regular data but none of the refer-\nence data. Reference data, along with the rest of the database schema, should\nbe controlled solely by migrations. \nListing 10.6\nBase class for integration tests\nDeletion \nscript\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n246\nCHAPTER 10\nTesting the database\n10.3.3 Avoid in-memory databases\nAnother way to isolate integration tests from each other is by replacing the database\nwith an in-memory analog, such as SQLite. In-memory databases can seem beneficial\nbecause they\nDon’t require removal of test data\nWork faster\nCan be instantiated for each test run\nBecause in-memory databases aren’t shared dependencies, integration tests in effect\nbecome unit tests (assuming the database is the only managed dependency in the\nproject), similar to the approach with containers described in section 10.3.1.\n In spite of all these benefits, I don’t recommend using in-memory databases\nbecause they aren’t consistent functionality-wise with regular databases. This is, once\nagain, the problem of a mismatch between production and test environments. Your\ntests can easily run into false positives or (worse!) false negatives due to the differ-\nences between the regular and in-memory databases. You’ll never gain good protec-\ntion with such tests and will have to do a lot of regression testing manually anyway.\nTIP\nUse the same database management system (DBMS) in tests as in pro-\nduction. It’s usually fine for the version or edition to differ, but the vendor\nmust remain the same. \n10.4\nReusing code in test sections\nIntegration tests can quickly grow too large and thus lose ground on the maintainabil-\nity metric. It’s important to keep integration tests as short as possible but without cou-\npling them to each other or affecting readability. Even the shortest tests shouldn’t\ndepend on one another. They also should preserve the full context of the test scenario\nand shouldn’t require you to examine different parts of the test class to understand\nwhat’s going on.\n The best way to shorten integration is by extracting technical, non-business-related\nbits into private methods or helper classes. As a side bonus, you’ll get to reuse those\nbits. In this section, I’ll show how to shorten all three sections of the test: arrange, act,\nand assert.\n10.4.1 Reusing code in arrange sections\nThe following listing shows how our integration test looks after providing a separate\ndatabase context (unit of work) for each of its sections.\n[Fact]\npublic void Changing_email_from_corporate_to_non_corporate()\n{\n// Arrange\nUser user;\nListing 10.7\nIntegration test with three database contexts\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n247\nReusing code in test sections\nusing (var context = new CrmContext(ConnectionString))\n{\nvar userRepository = new UserRepository(context);\nvar companyRepository = new CompanyRepository(context);\nuser = new User(0, \"user@mycorp.com\",\nUserType.Employee, false);\nuserRepository.SaveUser(user);\nvar company = new Company(\"mycorp.com\", 1);\ncompanyRepository.SaveCompany(company);\ncontext.SaveChanges();\n}\nvar busSpy = new BusSpy();\nvar messageBus = new MessageBus(busSpy);\nvar loggerMock = new Mock<IDomainLogger>();\nstring result;\nusing (var context = new CrmContext(ConnectionString))\n{\nvar sut = new UserController(\ncontext, messageBus, loggerMock.Object);\n// Act\nresult = sut.ChangeEmail(user.UserId, \"new@gmail.com\");\n}\n// Assert\nAssert.Equal(\"OK\", result);\nusing (var context = new CrmContext(ConnectionString))\n{\nvar userRepository = new UserRepository(context);\nvar companyRepository = new CompanyRepository(context);\nUser userFromDb = userRepository.GetUserById(user.UserId);\nAssert.Equal(\"new@gmail.com\", userFromDb.Email);\nAssert.Equal(UserType.Customer, userFromDb.Type);\nCompany companyFromDb = companyRepository.GetCompany();\nAssert.Equal(0, companyFromDb.NumberOfEmployees);\nbusSpy.ShouldSendNumberOfMessages(1)\n.WithEmailChangedMessage(user.UserId, \"new@gmail.com\");\nloggerMock.Verify(\nx => x.UserTypeHasChanged(\nuser.UserId, UserType.Employee, UserType.Customer),\nTimes.Once);\n}\n}\nAs you might remember from chapter 3, the best way to reuse code between the tests’\narrange sections is to introduce private factory methods. For example, the following\nlisting creates a user.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n248\nCHAPTER 10\nTesting the database\nprivate User CreateUser(\nstring email, UserType type, bool isEmailConfirmed)\n{\nusing (var context = new CrmContext(ConnectionString))\n{\nvar user = new User(0, email, type, isEmailConfirmed);\nvar repository = new UserRepository(context);\nrepository.SaveUser(user);\ncontext.SaveChanges();\nreturn user;\n}\n}\nYou can also define default values for the method’s arguments, as shown next.\nprivate User CreateUser(\nstring email = \"user@mycorp.com\",\nUserType type = UserType.Employee,\nbool isEmailConfirmed = false)\n{\n/* ... */\n}\nWith default values, you can specify arguments selectively and thus shorten the test\neven further. The selective use of arguments also emphasizes which of those argu-\nments are relevant to the test scenario.\nUser user = CreateUser(\nemail: \"user@mycorp.com\",\ntype: UserType.Employee);\nListing 10.8\nA separate method that creates a user\nListing 10.9\nAdding default values to the factory\nListing 10.10\nUsing the factory method\nObject Mother vs. Test Data Builder\nThe pattern shown in listings 10.9 and 10.10 is called the Object Mother. The Object\nMother is a class or method that helps create test fixtures (objects the test runs\nagainst).\nThere’s another pattern that helps achieve the same goal of reusing code in arrange\nsections: Test Data Builder. It works similarly to Object Mother but exposes a fluent\ninterface instead of plain methods. Here’s a Test Data Builder usage example:\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n249\nReusing code in test sections\nWHERE TO PUT FACTORY METHODS\nWhen you start distilling the tests’ essentials and move the technicalities out to fac-\ntory methods, you face the question of where to put those methods. Should they\nreside in the same class as the tests? The base IntegrationTests class? Or in a sepa-\nrate helper class?\n Start simple. Place the factory methods in the same class by default. Move them\ninto separate helper classes only when code duplication becomes a significant issue.\nDon’t put the factory methods in the base class; reserve that class for code that has to\nrun in every test, such as data cleanup. \n10.4.2 Reusing code in act sections\nEvery act section in integration tests involves the creation of a database transaction or\na unit of work. This is how the act section currently looks in listing 10.7:\nstring result;\nusing (var context = new CrmContext(ConnectionString))\n{\nvar sut = new UserController(\ncontext, messageBus, loggerMock.Object);\n// Act\nresult = sut.ChangeEmail(user.UserId, \"new@gmail.com\");\n}\nThis section can also be reduced. You can introduce a method accepting a delegate\nwith the information of what controller function needs to be invoked. The method\nwill then decorate the controller invocation with the creation of a database context, as\nshown in the following listing.\nprivate string Execute(\nFunc<UserController, string> func,   \nMessageBus messageBus,\nIDomainLogger logger)\n{\nusing (var context = new CrmContext(ConnectionString))\n{\nvar controller = new UserController(\nUser user = new UserBuilder()\n.WithEmail(\"user@mycorp.com\")\n.WithType(UserType.Employee)\n.Build();\nTest Data Builder slightly improves test readability but requires too much boilerplate.\nFor that reason, I recommend sticking to the Object Mother (at least in C#, where you\nhave optional arguments as a language feature).\nListing 10.11\nDecorator method\nDelegate defines \na controller \nfunction.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n250\nCHAPTER 10\nTesting the database\ncontext, messageBus, logger);\nreturn func(controller);\n}\n}\nWith this decorator method, you can boil down the test’s act section to just a couple\nof lines:\nstring result = Execute(\nx => x.ChangeEmail(user.UserId, \"new@gmail.com\"),\nmessageBus, loggerMock.Object);\n10.4.3 Reusing code in assert sections\nFinally, the assert section can be shortened, too. The easiest way to do that is to intro-\nduce helper methods similar to CreateUser and CreateCompany, as shown in the fol-\nlowing listing.\nUser userFromDb = QueryUser(user.UserId);         \nAssert.Equal(\"new@gmail.com\", userFromDb.Email);\nAssert.Equal(UserType.Customer, userFromDb.Type);\nCompany companyFromDb = QueryCompany();           \nAssert.Equal(0, companyFromDb.NumberOfEmployees);\nYou can take a step further and create a fluent interface for these data assertions, sim-\nilar to what you saw in chapter 9 with BusSpy. In C#, a fluent interface on top of exist-\ning domain classes can be implemented using extension methods, as shown in the\nfollowing listing.\npublic static class UserExternsions\n{\npublic static User ShouldExist(this User user)\n{\nAssert.NotNull(user);\nreturn user;\n}\npublic static User WithEmail(this User user, string email)\n{\nAssert.Equal(email, user.Email);\nreturn user;\n}\n}\nWith this fluent interface, the assertions become much easier to read:\nUser userFromDb = QueryUser(user.UserId);\nuserFromDb\n.ShouldExist()\nListing 10.12\nData assertions after extracting the querying logic\nListing 10.13\nFluent interface for data assertions\nNew helper \nmethods\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n251\nReusing code in test sections\n.WithEmail(\"new@gmail.com\")\n.WithType(UserType.Customer);\nCompany companyFromDb = QueryCompany();\ncompanyFromDb\n.ShouldExist()\n.WithNumberOfEmployees(0);\n10.4.4 Does the test create too many database transactions?\nAfter all the simplifications made earlier, the integration test has become more read-\nable and, therefore, more maintainable. There’s one drawback, though: the test now\nuses a total of five database transactions (units of work), where before it used only\nthree, as shown in the following listing.\npublic class UserControllerTests : IntegrationTests\n{\n[Fact]\npublic void Changing_email_from_corporate_to_non_corporate()\n{\n// Arrange\nUser user = CreateUser(\n                 \nemail: \"user@mycorp.com\",\ntype: UserType.Employee);\nCreateCompany(\"mycorp.com\", 1);                 \nvar busSpy = new BusSpy();\nvar messageBus = new MessageBus(busSpy);\nvar loggerMock = new Mock<IDomainLogger>();\n// Act\nstring result = Execute(                        \nx => x.ChangeEmail(user.UserId, \"new@gmail.com\"),\nmessageBus, loggerMock.Object);\n// Assert\nAssert.Equal(\"OK\", result);\nUser userFromDb = QueryUser(user.UserId);       \nuserFromDb\n.ShouldExist()\n.WithEmail(\"new@gmail.com\")\n.WithType(UserType.Customer);\nCompany companyFromDb = QueryCompany();         \ncompanyFromDb\n.ShouldExist()\n.WithNumberOfEmployees(0);\nbusSpy.ShouldSendNumberOfMessages(1)\n.WithEmailChangedMessage(user.UserId, \"new@gmail.com\");\nloggerMock.Verify(\nListing 10.14\nIntegration test after moving all technicalities out of it\nInstantiates a\nnew database\ncontext\nbehind the\nscenes\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "page_number": 266
    },
    {
      "number": 30,
      "title": "Segment 30 (pages 274-281)",
      "start_page": 274,
      "end_page": 281,
      "detection_method": "topic_boundary",
      "content": "252\nCHAPTER 10\nTesting the database\nx => x.UserTypeHasChanged(\nuser.UserId, UserType.Employee, UserType.Customer),\nTimes.Once);\n}\n}\nIs the increased number of database transactions a problem? And, if so, what can\nyou do about it? The additional database contexts are a problem to some degree\nbecause they make the test slower, but there’s not much that can be done about it.\nIt’s another example of a trade-off between different aspects of a valuable test: this\ntime between fast feedback and maintainability. It’s worth it to make that trade-off\nand exchange performance for maintainability in this particular case. The perfor-\nmance degradation shouldn’t be that significant, especially when the database is\nlocated on the developer’s machine. At the same time, the gains in maintainability\nare quite substantial. \n10.5\nCommon database testing questions\nIn this last section of the chapter, I’d like to answer common questions related to\ndatabase testing, as well as briefly reiterate some important points made in chapters 8\nand 9.\n10.5.1 Should you test reads?\nThroughout the last several chapters, we’ve worked with a sample scenario of chang-\ning a user email. This scenario is an example of a write operation (an operation that\nleaves a side effect in the database and other out-of-process dependencies). Most\napplications contain both write and read operations. An example of a read operation\nwould be returning the user information to the external client. Should you test both\nwrites and reads?\n It’s crucial to thoroughly test writes, because the stakes are high. Mistakes in write\noperations often lead to data corruption, which can affect not only your database but\nalso external applications. Tests that cover writes are highly valuable due to the protec-\ntion they provide against such mistakes.\n This is not the case for reads: a bug in a read operation usually doesn’t have conse-\nquences that are as detrimental. Therefore, the threshold for testing reads should be\nhigher than that for writes. Test only the most complex or important read operations;\ndisregard the rest.\n Note that there’s also no need for a domain model in reads. One of the main goals\nof domain modeling is encapsulation. And, as you might remember from chapters 5\nand 6, encapsulation is about preserving data consistency in light of any changes. The\nlack of data changes makes encapsulation of reads pointless. In fact, you don’t need a\nfully fledged ORM such as NHibernate or Entity Framework in reads, either. You are\nbetter off using plain SQL, which is superior to an ORM performance-wise, thanks to\nbypassing unnecessary layers of abstraction (figure 10.7).\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n253\nCommon database testing questions\nBecause there are hardly any abstraction layers in reads (the domain model is one\nsuch layer), unit tests aren’t of any use there. If you decide to test your reads, do so\nusing integration tests on a real database. \n10.5.2 Should you test repositories?\nRepositories provide a useful abstraction on top of the database. Here’s a usage exam-\nple from our sample CRM project:\nUser user = _userRepository.GetUserById(userId);\n_userRepository.SaveUser(user);\nShould you test repositories independently of other integration tests? It might seem\nbeneficial to test how repositories map domain objects to the database. After all,\nthere’s significant room for a mistake in this functionality. Still, such tests are a net loss\nto your test suite due to high maintenance costs and inferior protection against\nregressions. Let’s discuss these two drawbacks in more detail.\nHIGH MAINTENANCE COSTS\nRepositories fall into the controllers quadrant on the types-of-code diagram from\nchapter 7 (figure 10.8). They exhibit little complexity and communicate with an out-\nof-process dependency: the database. The presence of that out-of-process dependency\nis what inflates the tests’ maintenance costs.\n When it comes to maintenance costs, testing repositories carries the same burden\nas regular integration tests. But does such testing provide an equal amount of benefits\nin return? Unfortunately, it doesn’t.\nWrites\nDatabase\nClient\nReads\nApplication\n. . . not here\nDomain model goes here . . .\nFigure 10.7\nThere’s no need for a domain model in reads. And because the cost of a \nmistake in reads is lower than it is in writes, there’s also not as much need for integration \ntesting.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n254\nCHAPTER 10\nTesting the database\nINFERIOR PROTECTION AGAINST REGRESSIONS\nRepositories don’t carry that much complexity, and a lot of the gains in protection\nagainst regressions overlap with the gains provided by regular integration tests. Thus,\ntests on repositories don’t add significant enough value.\n The best course of action in testing a repository is to extract the little complexity it\nhas into a self-contained algorithm and test that algorithm exclusively. That’s what\nUserFactory and CompanyFactory were for in earlier chapters. These two classes per-\nformed all the mappings without taking on any collaborators, out-of-process or other-\nwise. The repositories (the Database class) only contained simple SQL queries.\n Unfortunately, such a separation between data mapping (formerly performed by\nthe factories) and interactions with the database (formerly performed by Database) is\nimpossible when using an ORM. You can’t test your ORM mappings without calling\nthe database, at least not without compromising resistance to refactoring. Therefore,\nadhere to the following guideline: don’t test repositories directly, only as part of the overarch-\ning integration test suite.\n Don’t test EventDispatcher separately, either (this class converts domain events\ninto calls to unmanaged dependencies). There are too few gains in protection against\nregressions in exchange for the too-high costs required to maintain the complicated\nmock machinery. \n10.6\nConclusion\nWell-crafted tests against the database provide bulletproof protection from bugs. In\nmy experience, they are one of the most effective tools, without which it’s impossible\nDomain model,\nalgorithms\nOvercomplicated\ncode\nTrivial code\nControllers\nComplexity,\ndomain\nsigniﬁcance\nNumber of\ncollaborators\nRepositories\nFigure 10.8\nRepositories exhibit little complexity and communicate with the \nout-of-process dependency, thus falling into the controllers quadrant on the \ntypes-of-code diagram.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n255\nSummary\nto gain full confidence in your software. Such tests help enormously when you refac-\ntor the database, switch the ORM, or change the database vendor.\n In fact, our sample project transitioned to the Entity Framework ORM earlier in\nthis chapter, and I only needed to modify a couple of lines of code in the integration\ntest to make sure the transition was successful. Integration tests working directly with\nmanaged dependencies are the most efficient way to protect against bugs resulting\nfrom large-scale refactorings. \nSummary\nStore database schema in a source control system, along with your source code.\nDatabase schema consists of tables, views, indexes, stored procedures, and any-\nthing else that forms a blueprint of how the database is constructed.\nReference data is also part of the database schema. It is data that must be pre-\npopulated in order for the application to operate properly. To differentiate\nbetween reference and regular data, look at whether your application can mod-\nify that data. If so, it’s regular data; otherwise, it’s reference data.\nHave a separate database instance for every developer. Better yet, host that\ninstance on the developer’s own machine for maximum test execution speed.\nThe state-based approach to database delivery makes the state explicit and lets a\ncomparison tool implicitly control migrations. The migration-based approach\nemphasizes the use of explicit migrations that transition the database from one\nstate to another. The explicitness of the database state makes it easier to handle\nmerge conflicts, while explicit migrations help tackle data motion.\nPrefer the migration-based approach over state-based, because handling data\nmotion is much more important than merge conflicts. Apply every modification\nto the database schema (including reference data) through migrations.\nBusiness operations must update data atomically. To achieve atomicity, rely on\nthe underlying database’s transaction mechanism.\nUse the unit of work pattern when possible. A unit of work relies on the under-\nlying database’s transactions; it also defers all updates to the end of the business\noperation, thus improving performance.\nDon’t reuse database transactions or units of work between sections of the\ntest. Each arrange, act, and assert section should have its own transaction or\nunit of work.\nExecute integration tests sequentially. Parallel execution involves significant\neffort and usually is not worth it.\nClean up leftover data at the start of a test. This approach works fast, doesn’t\nresult in inconsistent behavior, and isn’t prone to accidentally skipping the\ncleanup phase. With this approach, you don’t have to introduce a separate tear-\ndown phase, either.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n256\nCHAPTER 10\nTesting the database\nAvoid in-memory databases such as SQLite. You’ll never gain good protection if\nyour tests run against a database from a different vendor. Use the same database\nmanagement system in tests as in production.\nShorten tests by extracting non-essential parts into private methods or helper\nclasses:\n– For the arrange section, choose Object Mother over Test Data Builder.\n– For act, create decorator methods.\n– For assert, introduce a fluent interface.\nThe threshold for testing reads should be higher than that for writes. Test only\nthe most complex or important read operations; disregard the rest.\nDon’t test repositories directly, but only as part of the overarching integration\ntest suite. Tests on repositories introduce too high maintenance costs for too\nfew additional gains in protection against regressions.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\nPart 4\nUnit testing anti-patterns\nThis final part of the book covers common unit testing anti-patterns. You’ve\nmost likely encountered some of them in the past. Still, it’s interesting to look at\nthis topic using the four attributes of a good unit test defined in chapter 4. You\ncan use those attributes to analyze any unit testing concepts or patterns; anti-\npatterns aren’t an exception.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n \nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n259\nUnit testing anti-patterns\nThis chapter is an aggregation of lesser related topics (mostly anti-patterns) that\ndidn’t fit in earlier in the book and are better served on their own. An anti-pattern is\na common solution to a recurring problem that looks appropriate on the surface\nbut leads to problems further down the road.\n You will learn how to work with time in tests, how to identify and avoid such anti-\npatterns as unit testing of private methods, code pollution, mocking concrete\nclasses, and more. Most of these topics follow from the first principles described in\npart 2. Still, they are well worth spelling out explicitly. You’ve probably heard of at\nleast some of these anti-patterns in the past, but this chapter will help you connect\nthe dots, so to speak, and see the foundations they are based on.\nThis chapter covers\nUnit testing private methods\nExposing private state to enable unit testing\nLeaking domain knowledge to tests\nMocking concrete classes\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "page_number": 274
    },
    {
      "number": 31,
      "title": "Segment 31 (pages 282-294)",
      "start_page": 282,
      "end_page": 294,
      "detection_method": "topic_boundary",
      "content": "260\nCHAPTER 11\nUnit testing anti-patterns\n11.1\nUnit testing private methods\nWhen it comes to unit testing, one of the most commonly asked questions is how to\ntest a private method. The short answer is that you shouldn’t do so at all, but there’s\nquite a bit of nuance to this topic.\n11.1.1 Private methods and test fragility\nExposing methods that you would otherwise keep private just to enable unit testing\nviolates one of the foundational principles we discussed in chapter 5: testing observ-\nable behavior only. Exposing private methods leads to coupling tests to implementa-\ntion details and, ultimately, damaging your tests’ resistance to refactoring—the most\nimportant metric of the four. (All four metrics, once again, are protection against\nregressions, resistance to refactoring, fast feedback, and maintainability.) Instead of\ntesting private methods directly, test them indirectly, as part of the overarching observ-\nable behavior. \n11.1.2 Private methods and insufficient coverage\nSometimes, the private method is too complex, and testing it as part of the observable\nbehavior doesn’t provide sufficient coverage. Assuming the observable behavior\nalready has reasonable test coverage, there can be two issues at play:\nThis is dead code. If the uncovered code isn’t being used, this is likely some extra-\nneous code left after a refactoring. It’s best to delete this code.\nThere’s a missing abstraction. If the private method is too complex (and thus is\nhard to test via the class’s public API), it’s an indication of a missing abstraction\nthat should be extracted into a separate class.\nLet’s illustrate the second issue with an example.\npublic class Order\n{\nprivate Customer _customer;\nprivate List<Product> _products;\npublic string GenerateDescription()\n{\nreturn $\"Customer name: {_customer.Name}, \" +\n$\"total number of products: {_products.Count}, \" +\n$\"total price: {GetPrice()}\";             \n}\nprivate decimal GetPrice()     \n{\ndecimal basePrice = /* Calculate based on _products */;\ndecimal discounts = /* Calculate based on _customer */;\ndecimal taxes = /* Calculate based on _products */;\nListing 11.1\nA class with a complex private method\nThe complex private\nmethod is used by a\nmuch simpler public\nmethod.\nComplex private \nmethod\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n261\nUnit testing private methods\nreturn basePrice - discounts + taxes;\n}\n}\nThe GenerateDescription() method is quite simple: it returns a generic description\nof the order. But it uses the private GetPrice() method, which is much more com-\nplex: it contains important business logic and needs to be thoroughly tested. That\nlogic is a missing abstraction. Instead of exposing the GetPrice method, make this\nabstraction explicit by extracting it into a separate class, as shown in the next listing.\npublic class Order\n{\nprivate Customer _customer;\nprivate List<Product> _products;\npublic string GenerateDescription()\n{\nvar calc = new PriceCalculator();\nreturn $\"Customer name: {_customer.Name}, \" +\n$\"total number of products: {_products.Count}, \" +\n$\"total price: {calc.Calculate(_customer, _products)}\";\n}\n}\npublic class PriceCalculator\n{\npublic decimal Calculate(Customer customer, List<Product> products)\n{\ndecimal basePrice = /* Calculate based on products */;\ndecimal discounts = /* Calculate based on customer */;\ndecimal taxes = /* Calculate based on products */;\nreturn basePrice - discounts + taxes;\n}\n}\nNow you can test PriceCalculator independently of Order. You can also use the\noutput-based (functional) style of unit testing, because PriceCalculator doesn’t\nhave any hidden inputs or outputs. See chapter 6 for more information about styles\nof unit testing. \n11.1.3 When testing private methods is acceptable\nThere are exceptions to the rule of never testing private methods. To understand\nthose exceptions, we need to revisit the relationship between the code’s publicity and\npurpose from chapter 5. Table 11.1 sums up that relationship (you already saw this\ntable in chapter 5; I’m copying it here for convenience).\nListing 11.2\nExtracting the complex private method\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n262\nCHAPTER 11\nUnit testing anti-patterns\nAs you might remember from chapter 5, making the observable behavior public and\nimplementation details private results in a well-designed API. On the other hand,\nleaking implementation details damages the code’s encapsulation. The intersection of\nobservable behavior and private methods is marked N/A in the table because for a\nmethod to become part of observable behavior, it has to be used by the client code,\nwhich is impossible if that method is private.\n Note that testing private methods isn’t bad in and of itself. It’s only bad because\nthose private methods are a proxy for implementation details. Testing implementa-\ntion details is what ultimately leads to test brittleness. Having that said, there are rare\ncases where a method is both private and part of observable behavior (and thus the\nN/A marking in table 11.1 isn’t entirely correct).\n Let’s take a system that manages credit inquiries as an example. New inquiries are\nbulk-loaded directly into the database once a day. Administrators then review those\ninquiries one by one and decide whether to approve them. Here’s how the Inquiry\nclass might look in that system.\npublic class Inquiry\n{\npublic bool IsApproved { get; private set; }\npublic DateTime? TimeApproved { get; private set; }\nprivate Inquiry(\n  \nbool isApproved, DateTime? timeApproved)  \n{\nif (isApproved && !timeApproved.HasValue)\nthrow new Exception();\nIsApproved = isApproved;\nTimeApproved = timeApproved;\n}\npublic void Approve(DateTime now)\n{\nif (IsApproved)\nreturn;\nIsApproved = true;\nTimeApproved = now;\n}\n}\nTable 11.1\nThe relationship between the code’s publicity and purpose\nObservable behavior\nImplementation detail\nPublic\nGood\nBad\nPrivate\nN/A\nGood\nListing 11.3\nA class with a private constructor\nPrivate \nconstructor\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n263\nExposing private state\nThe private constructor is private because the class is restored from the database by an\nobject-relational mapping (ORM) library. That ORM doesn’t need a public construc-\ntor; it may well work with a private one. At the same time, our system doesn’t need a\nconstructor, either, because it’s not responsible for the creation of those inquiries.\n How do you test the Inquiry class given that you can’t instantiate its objects? On\nthe one hand, the approval logic is clearly important and thus should be unit tested.\nBut on the other, making the constructor public would violate the rule of not expos-\ning private methods.\n Inquiry’s constructor is an example of a method that is both private and part of\nthe observable behavior. This constructor fulfills the contract with the ORM, and the\nfact that it’s private doesn’t make that contract less important: the ORM wouldn’t be\nable to restore inquiries from the database without it.\n And so, making Inquiry’s constructor public won’t lead to test brittleness in this par-\nticular case. In fact, it will arguably bring the class’s API closer to being well-designed.\nJust make sure the constructor contains all the preconditions required to maintain its\nencapsulation. In listing 11.3, such a precondition is the requirement to have the\napproval time in all approved inquiries.\n Alternatively, if you prefer to keep the class’s public API surface as small as possi-\nble, you can instantiate Inquiry via reflection in tests. Although this looks like a hack,\nyou are just following the ORM, which also uses reflection behind the scenes. \n11.2\nExposing private state\nAnother common anti-pattern is exposing private state for the sole purpose of unit\ntesting. The guideline here is the same as with private methods: don’t expose state\nthat you would otherwise keep private—test observable behavior only. Let’s take a\nlook at the following listing.\npublic class Customer\n{\nprivate CustomerStatus _status =   \nCustomerStatus.Regular;\n   \npublic void Promote()\n{\n_status = CustomerStatus.Preferred;\n}\npublic decimal GetDiscount()\n{\nreturn _status == CustomerStatus.Preferred ? 0.05m : 0m;\n}\n}\npublic enum CustomerStatus\n{\nListing 11.4\nA class with private state\nPrivate \nstate\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n264\nCHAPTER 11\nUnit testing anti-patterns\nRegular,\nPreferred\n}\nThis example shows a Customer class. Each customer is created in the Regular status\nand then can be promoted to Preferred, at which point they get a 5% discount on\neverything.\n How would you test the Promote() method? This method’s side effect is a change\nof the _status field, but the field itself is private and thus not available in tests. A\ntempting solution would be to make this field public. After all, isn’t the change of sta-\ntus the ultimate goal of calling Promote()?\n That would be an anti-pattern, however. Remember, your tests should interact with the\nsystem under test (SUT) exactly the same way as the production code and shouldn’t have any spe-\ncial privileges. In listing 11.4, the _status field is hidden from the production code and\nthus is not part of the SUT’s observable behavior. Exposing that field would result in\ncoupling tests to implementation details. How to test Promote(), then?\n What you should do, instead, is look at how the production code uses this class. In\nthis particular example, the production code doesn’t care about the customer’s status;\notherwise, that field would be public. The only information the production code does\ncare about is the discount the customer gets after the promotion. And so that’s what\nyou need to verify in tests. You need to check that\nA newly created customer has no discount.\nOnce the customer is promoted, the discount becomes 5%.\nLater, if the production code starts using the customer status field, you’d be able to\ncouple to that field in tests too, because it would officially become part of the SUT’s\nobservable behavior.\nNOTE\nWidening the public API surface for the sake of testability is a bad practice. \n11.3\nLeaking domain knowledge to tests\nLeaking domain knowledge to tests is another quite common anti-pattern. It usually\ntakes place in tests that cover complex algorithms. Let’s take the following (admit-\ntedly, not that complex) calculation algorithm as an example:\npublic static class Calculator\n{\npublic static int Add(int value1, int value2)\n{\nreturn value1 + value2;\n}\n}\nThis listing shows an incorrect way to test it.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n265\nLeaking domain knowledge to tests\npublic class CalculatorTests\n{\n[Fact]\npublic void Adding_two_numbers()\n{\nint value1 = 1;\nint value2 = 3;\nint expected = value1 + value2;      \nint actual = Calculator.Add(value1, value2);\nAssert.Equal(expected, actual);\n}\n}\nYou could also parameterize the test to throw in a couple more test cases at almost no\nadditional cost.\npublic class CalculatorTests\n{\n[Theory]\n[InlineData(1, 3)]\n[InlineData(11, 33)]\n[InlineData(100, 500)]\npublic void Adding_two_numbers(int value1, int value2)\n{\nint expected = value1 + value2;    \nint actual = Calculator.Add(value1, value2);\nAssert.Equal(expected, actual);\n}\n}\nListings 11.5 and 11.6 look fine at first, but they are, in fact, examples of the anti-pattern:\nthese tests duplicate the algorithm implementation from the production code. Of\ncourse, it might not seem like a big deal. After all, it’s just one line. But that’s only\nbecause the example is rather simplified. I’ve seen tests that covered complex algo-\nrithms and did nothing but reimplement those algorithms in the arrange part. They\nwere basically a copy-paste from the production code.\n These tests are another example of coupling to implementation details. They score\nalmost zero on the metric of resistance to refactoring and are worthless as a result.\nSuch tests don’t have a chance of differentiating legitimate failures from false posi-\ntives. Should a change in the algorithm make those tests fail, the team would most\nlikely just copy the new version of that algorithm to the test without even trying to\nListing 11.5\nLeaking algorithm implementation\nListing 11.6\nA parameterized version of the same test\nThe leakage\nThe leakage\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n266\nCHAPTER 11\nUnit testing anti-patterns\nidentify the root cause (which is understandable, because the tests were a mere dupli-\ncation of the algorithm in the first place).\n How to test the algorithm properly, then? Don’t imply any specific implementation when\nwriting tests. Instead of duplicating the algorithm, hard-code its results into the test, as\nshown in the following listing.\npublic class CalculatorTests\n{\n[Theory]\n[InlineData(1, 3, 4)]\n[InlineData(11, 33, 44)]\n[InlineData(100, 500, 600)]\npublic void Adding_two_numbers(int value1, int value2, int expected)\n{\nint actual = Calculator.Add(value1, value2);\nAssert.Equal(expected, actual);\n}\n}\nIt can seem counterintuitive at first, but hardcoding the expected result is a good\npractice when it comes to unit testing. The important part with the hardcoded values\nis to precalculate them using something other than the SUT, ideally with the help of a\ndomain expert. Of course, that’s only if the algorithm is complex enough (we are all\nexperts at summing up two numbers). Alternatively, if you refactor a legacy applica-\ntion, you can have the legacy code produce those results and then use them as expected\nvalues in tests. \n11.4\nCode pollution\nThe next anti-pattern is code pollution.\nDEFINITION\nCode pollution is adding production code that’s only needed for\ntesting.\nCode pollution often takes the form of various types of switches. Let’s take a logger as\nan example.\npublic class Logger\n{\nprivate readonly bool _isTestEnvironment;\npublic Logger(bool isTestEnvironment)    \n{\n_isTestEnvironment = isTestEnvironment;\n}\nListing 11.7\nTest with no domain knowledge\nListing 11.8\nLogger with a Boolean switch \nThe switch\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n267\nCode pollution\npublic void Log(string text)\n{\nif (_isTestEnvironment)     \nreturn;\n/* Log the text */\n}\n}\npublic class Controller\n{\npublic void SomeMethod(Logger logger)\n{\nlogger.Log(\"SomeMethod is called\");\n}\n}\nIn this example, Logger has a constructor parameter that indicates whether the class\nruns in production. If so, the logger records the message into the file; otherwise, it\ndoes nothing. With such a Boolean switch, you can disable the logger during test runs,\nas shown in the following listing.\n[Fact]\npublic void Some_test()\n{\nvar logger = new Logger(true);    \nvar sut = new Controller();\nsut.SomeMethod(logger);\n/* assert */\n}\nThe problem with code pollution is that it mixes up test and production code and\nthereby increases the maintenance costs of the latter. To avoid this anti-pattern, keep\nthe test code out of the production code base.\n In the example with Logger, introduce an ILogger interface and create two imple-\nmentations of it: a real one for production and a fake one for testing purposes. After\nthat, re-target Controller to accept the interface instead of the concrete class, as\nshown in the following listing.\npublic interface ILogger\n{\nvoid Log(string text);\n}\nListing 11.9\nA test using the Boolean switch\nListing 11.10\nA version without the switch\nThe switch\nSets the parameter to \ntrue to indicate the \ntest environment\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n268\nCHAPTER 11\nUnit testing anti-patterns\npublic class Logger : ILogger\n  \n{\n  \npublic void Log(string text)  \n{\n  \n/* Log the text */\n  \n}\n  \n}\n  \npublic class FakeLogger : ILogger   \n{\n   \npublic void Log(string text)    \n{\n   \n/* Do nothing */\n   \n}\n   \n}\n   \npublic class Controller\n{\npublic void SomeMethod(ILogger logger)\n{\nlogger.Log(\"SomeMethod is called\");\n}\n}\nSuch a separation helps keep the production logger simple because it no longer has\nto account for different environments. Note that ILogger itself is arguably a form of\ncode pollution: it resides in the production code base but is only needed for testing.\nSo how is the new implementation better?\n The kind of pollution ILogger introduces is less damaging and easier to deal\nwith. Unlike the initial Logger implementation, with the new version, you can’t acci-\ndentally invoke a code path that isn’t intended for production use. You can’t have\nbugs in interfaces, either, because they are just contracts with no code in them. In\ncontrast to Boolean switches, interfaces don’t introduce additional surface area for\npotential bugs. \n11.5\nMocking concrete classes\nSo far, this book has shown mocking examples using interfaces, but there’s an alterna-\ntive approach: you can mock concrete classes instead and thus preserve part of the\noriginal classes’ functionality, which can be useful at times. This alternative has a sig-\nnificant drawback, though: it violates the Single Responsibility principle. The next list-\ning illustrates this idea.\npublic class StatisticsCalculator\n{\npublic (double totalWeight, double totalCost) Calculate(\nint customerId)\n{\nList<DeliveryRecord> records = GetDeliveries(customerId);\nListing 11.11\nA class that calculates statistics\nBelongs in the \nproduction code\nBelongs in \nthe test code\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n269\nMocking concrete classes\ndouble totalWeight = records.Sum(x => x.Weight);\ndouble totalCost = records.Sum(x => x.Cost);\nreturn (totalWeight, totalCost);\n}\npublic List<DeliveryRecord> GetDeliveries(int customerId)\n{\n/* Call an out-of-process dependency\nto get the list of deliveries */\n}\n}\nStatisticsCalculator gathers and calculates customer statistics: the weight and cost\nof all deliveries sent to a particular customer. The class does the calculation based on\nthe list of deliveries retrieved from an external service (the GetDeliveries method).\nLet’s also say there’s a controller that uses StatisticsCalculator, as shown in the fol-\nlowing listing.\npublic class CustomerController\n{\nprivate readonly StatisticsCalculator _calculator;\npublic CustomerController(StatisticsCalculator calculator)\n{\n_calculator = calculator;\n}\npublic string GetStatistics(int customerId)\n{\n(double totalWeight, double totalCost) = _calculator\n.Calculate(customerId);\nreturn\n$\"Total weight delivered: {totalWeight}. \" +\n$\"Total cost: {totalCost}\";\n}\n}\nHow would you test this controller? You can’t supply it with a real Statistics-\nCalculator instance, because that instance refers to an unmanaged out-of-process\ndependency. The unmanaged dependency has to be substituted with a stub. At the\nsame time, you don’t want to replace StatisticsCalculator entirely, either. This\nclass contains important calculation functionality, which needs to be left intact.\n One way to overcome this dilemma is to mock the StatisticsCalculator class\nand override only the GetDeliveries() method, which can be done by making that\nmethod virtual, as shown in the following listing.\n \nListing 11.12\nA controller using StatisticsCalculator\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n270\nCHAPTER 11\nUnit testing anti-patterns\n[Fact]\npublic void Customer_with_no_deliveries()\n{\n// Arrange\nvar stub = new Mock<StatisticsCalculator> { CallBase = true };\nstub.Setup(x => x.GetDeliveries(1))         \n.Returns(new List<DeliveryRecord>());\nvar sut = new CustomerController(stub.Object);\n// Act\nstring result = sut.GetStatistics(1);\n// Assert\nAssert.Equal(\"Total weight delivered: 0. Total cost: 0\", result);\n}\nThe CallBase = true setting tells the mock to preserve the base class’s behavior unless\nit’s explicitly overridden. With this approach, you can substitute only a part of the class\nwhile keeping the rest as-is. As I mentioned earlier, this is an anti-pattern.\nNOTE\nThe necessity to mock a concrete class in order to preserve part of its\nfunctionality is a result of violating the Single Responsibility principle.\nStatisticsCalculator combines two unrelated responsibilities: communicating with\nthe unmanaged dependency and calculating statistics. Look at listing 11.11 again. The\nCalculate() method is where the domain logic lies. GetDeliveries() just gathers\nthe inputs for that logic. Instead of mocking StatisticsCalculator, split this class in\ntwo, as the following listing shows.\npublic class DeliveryGateway : IDeliveryGateway\n{\npublic List<DeliveryRecord> GetDeliveries(int customerId)\n{\n/* Call an out-of-process dependency\nto get the list of deliveries */\n}\n}\npublic class StatisticsCalculator\n{\npublic (double totalWeight, double totalCost) Calculate(\nList<DeliveryRecord> records)\n{\ndouble totalWeight = records.Sum(x => x.Weight);\ndouble totalCost = records.Sum(x => x.Cost);\nreturn (totalWeight, totalCost);\n}\n}\nListing 11.13\nTest that mocks the concrete class\nListing 11.14\nSplitting StatisticsCalculator into two classes\nGetDeliveries() must \nbe made virtual.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n271\nWorking with time\nThe next listing shows the controller after the refactoring.\npublic class CustomerController\n{\nprivate readonly StatisticsCalculator _calculator;\nprivate readonly IDeliveryGateway _gateway;\npublic CustomerController(\nStatisticsCalculator calculator,   \nIDeliveryGateway gateway)\n   \n{\n_calculator = calculator;\n_gateway = gateway;\n}\npublic string GetStatistics(int customerId)\n{\nvar records = _gateway.GetDeliveries(customerId);\n(double totalWeight, double totalCost) = _calculator\n.Calculate(records);\nreturn\n$\"Total weight delivered: {totalWeight}. \" +\n$\"Total cost: {totalCost}\";\n}\n}\nThe responsibility of communicating with the unmanaged dependency has transi-\ntioned to DeliveryGateway. Notice how this gateway is backed by an interface, which\nyou can now use for mocking instead of the concrete class. The code in listing 11.15 is\nan example of the Humble Object design pattern in action. Refer to chapter 7 to\nlearn more about this pattern. \n11.6\nWorking with time\nMany application features require access to the current date and time. Testing func-\ntionality that depends on time can result in false positives, though: the time during\nthe act phase might not be the same as in the assert. There are three options for stabi-\nlizing this dependency. One of these options is an anti-pattern; and of the other two,\none is preferable to the other.\n11.6.1 Time as an ambient context\nThe first option is to use the ambient context pattern. You already saw this pattern in\nchapter 8 in the section about testing loggers. In the context of time, the ambient con-\ntext would be a custom class that you’d use in code instead of the framework’s built-in\nDateTime.Now, as shown in the next listing.\n \nListing 11.15\nController after the refactoring\nTwo separate \ndependencies\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n272\nCHAPTER 11\nUnit testing anti-patterns\npublic static class DateTimeServer\n{\nprivate static Func<DateTime> _func;\npublic static DateTime Now => _func();\npublic static void Init(Func<DateTime> func)\n{\n_func = func;\n}\n}\nDateTimeServer.Init(() => DateTime.Now);     \nDateTimeServer.Init(() => new DateTime(2020, 1, 1));      \nJust as with the logger functionality, using an ambient context for time is also an anti-\npattern. The ambient context pollutes the production code and makes testing more\ndifficult. Also, the static field introduces a dependency shared between tests, thus tran-\nsitioning those tests into the sphere of integration testing. \n11.6.2 Time as an explicit dependency\nA better approach is to inject the time dependency explicitly (instead of referring to it\nvia a static method in an ambient context), either as a service or as a plain value, as\nshown in the following listing.\npublic interface IDateTimeServer\n{\nDateTime Now { get; }\n}\npublic class DateTimeServer : IDateTimeServer\n{\npublic DateTime Now => DateTime.Now;\n}\npublic class InquiryController\n{\nprivate readonly DateTimeServer _dateTimeServer;\npublic InquiryController(\nDateTimeServer dateTimeServer)    \n{\n_dateTimeServer = dateTimeServer;\n}\npublic void ApproveInquiry(int id)\n{\nInquiry inquiry = GetById(id);\nListing 11.16\nCurrent date and time as an ambient context\nListing 11.17\nCurrent date and time as an explicit dependency\nInitialization code \nfor production\nInitialization code \nfor unit tests\nInjects time as \na service\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "page_number": 282
    },
    {
      "number": 32,
      "title": "Segment 32 (pages 295-302)",
      "start_page": 295,
      "end_page": 302,
      "detection_method": "topic_boundary",
      "content": "273\nSummary\ninquiry.Approve(_dateTimeServer.Now);      \nSaveInquiry(inquiry);\n}\n}\nOf these two options, prefer injecting the time as a value rather than as a service. It’s\neasier to work with plain values in production code, and it’s also easier to stub those\nvalues in tests.\n Most likely, you won’t be able to always inject the time as a plain value, because\ndependency injection frameworks don’t play well with value objects. A good compro-\nmise is to inject the time as a service at the start of a business operation and then\npass it as a value in the remainder of that operation. You can see this approach in\nlisting 11.17: the controller accepts DateTimeServer (the service) but then passes a\nDateTime value to the Inquiry domain class. \n11.7\nConclusion\nIn this chapter, we looked at some of the most prominent real-world unit testing use\ncases and analyzed them using the four attributes of a good test. I understand that it\nmay be overwhelming to start applying all the ideas and guidelines from this book at\nonce. Also, your situation might not be as clear-cut. I publish reviews of other people’s\ncode and answer questions (related to unit testing and code design in general) on my\nblog at https://enterprisecraftsmanship.com. You can also submit your own question\nat https://enterprisecraftsmanship.com/about. You might also be interested in taking\nmy online course, where I show how to build an application from the ground up,\napplying all the principles described in this book in practice, at https://unittesting-\ncourse.com.\n You can always catch me on twitter at @vkhorikov, or contact me directly through\nhttps://enterprisecraftsmanship.com/about. I look forward to hearing from you!\nSummary\nExposing private methods to enable unit testing leads to coupling tests to\nimplementation and, ultimately, damaging the tests’ resistance to refactoring.\nInstead of testing private methods directly, test them indirectly as part of the\noverarching observable behavior.\nIf the private method is too complex to be tested as part of the public API that\nuses it, that’s an indication of a missing abstraction. Extract this abstraction into\na separate class instead of making the private method public.\nIn rare cases, private methods do belong to the class’s observable behavior.\nSuch methods usually implement a non-public contract between the class and\nan ORM or a factory.\nDon’t expose state that you would otherwise keep private for the sole purpose\nof unit testing. Your tests should interact with the system under test exactly the\nsame way as the production code; they shouldn’t have any special privileges.\nInjects time as \na plain value\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n274\nCHAPTER 11\nUnit testing anti-patterns\nDon’t imply any specific implementation when writing tests. Verify the produc-\ntion code from a black-box perspective; avoid leaking domain knowledge to\ntests (see chapter 4 for more details about black-box and white-box testing).\nCode pollution is adding production code that’s only needed for testing. It’s an\nanti-pattern because it mixes up test and production code and increases the\nmaintenance costs of the latter.\nThe necessity to mock a concrete class in order to preserve part of its function-\nality is a result of violating the Single Responsibility principle. Separate that\nclass into two classes: one with the domain logic, and the other one communi-\ncating with the out-of-process dependency.\nRepresenting the current time as an ambient context pollutes the production\ncode and makes testing more difficult. Inject time as an explicit dependency—\neither as a service or as a plain value. Prefer the plain value whenever possible.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\n275\nindex\nA\nAAA (arrange, act, and assert) pattern 42–49\navoiding if statements 44–45\navoiding multiple AAA sections 43–44\ndifferentiating system under test 47–48\ndropping AAA comments 48–49\noverview 42–43\nreusing code in test sections 246–252\nin act sections 249–250\nin arrange sections 246–249\nin assert sections 250\nsection size 45–47\narrange section 45\nnumber of assertions in assert \nsection 47\nsections larger than a single line 45–47\nteardown phase 47\nabstractions 198, 260\nActive Record pattern 159\nadapters 227\naggregates 157\nambient context 212\nanti-patterns 212\ncode pollution 266–268\nexposing private state 263–264\nleaking domain knowledge to tests\n264–266\nmocking concrete classes 268–271\nprivate methods 260–263\nacceptability of testing 261–263\ninsufficient coverage 260–261\ntest fragility 260\ntime 271–273\nas ambient context 271–272\nas explicit dependency 272–273\nAPI (application programming interface) 104, \n111, 133, 191, 195, 227, 264\nmissing abstractions 260\npublic vs. private 99\nwell-designed 100–101, 105, 108, 262\napplication behavior 57\napplication services layer 133–134\narrange, act, and assert pattern. See AAA \npattern\nassertion libraries, using to improve test \nreadability 62–63\nassertion-free testing 12–13\nasynchronous communications 191\natomic updates 236\nautomation concepts 87–90\nblack-box vs. white-box testing 89–90\nTest Pyramid 87–89\nB\nbackward migration 233\nbad tests 189\nblack-box testing 68, 89–90\nBoolean switches 266–268\nbranch coverage metric 10–11\nbrittle tests 83–84, 116, 216\nbrittleness 86, 125\nbugs 68, 79, 104, 175, 189\nbusiness logic 106–107, 156, 169, \n179\nC\nCanExecute/Execute pattern 172, 174\nCAP theorem 86–87\ncaptured data 208\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\nINDEX\n276\ncircular dependencies 203\ndefined 202\neliminating 202–204\nclassical school of unit testing 30–37\ndependencies 30–34\nend-to-end tests 38–39\nintegration tests 37–39\nisolation issue 27–30\nmocks 114–116\nmocking out out-of-process dependencies\n115–116\nusing mocks to verify behavior 116\nprecise bug location 36\ntesting large graph of interconnected classes 35\ntesting one class at a time 34–35\ncleanup phase 244\nclusters, grouping into aggregates 157\ncode complexity 104, 152\ncode coverage metric 9–10\ncode coverage tools 90\ncode depth 157\ncode pollution 127, 266–268, 272\ncode width 157\ncollaborators 32, 148, 153\ncommand query separation. See CQS principle\ncommands 97\ncommunication-based testing 122–123, 128\nfeedback speed 124\nmaintainability 127\noveruse of 124\nprotection against regressions and feedback \nspeed 124\nresistance to refactoring 124–125\nvulnerability to false alarms 124\ncommunications\nbetween applications 107, 110\nbetween classes in application 110, 116\nconditional logic 169–180\nCanExecute/Execute pattern 172–174\ndomain events for tracking changes in the \ndomain model 175–178\nconstructors, reusing test fixtures between \ntests 52\ncontainers 244\ncontrollers 153, 225\nsimplicity 171\ncoverage metrics, measuring test suite quality \nwith 8–15\naiming for particular coverage number 15\nbranch coverage metric 10–11\ncode coverage metric 9–10\nproblems with 12–15\ncode paths in external libraries 14–15\nimpossible to verify all possible outcomes\n12–13\nCQS (command query separation) principle\n97–98\nCRUD (create, read, update, and delete) \noperations 89\nCSV files 208–209\ncyclic dependency 202\ncyclomatic complexity 152\nD\ndata inconsistencies 241\ndata mapping 254\ndata motion 234\ndata, bundling 104\ndatabase backup, restoring 244\ndatabase management system (DBMS) 246\ndatabase testing\ncommon questions 252–255\ntesting reads 252–253\ntesting repositories 253–254\ndatabase transaction management 234–243\nin integration tests 242–243\nin production code 235–242\nprerequisites for 230–234\nkeeping database in source control \nsystem 230–231\nreference data as part of database \nschema 231\nseparate instances for every developer\n232\nstate-based vs. migration-based database \ndelivery 232–234\nreusing code in test sections 246–252\ncreating too many database \ntransactions 251–252\nin act sections 249–250\nin arrange sections 246–249\nin assert sections 250\ntest data life cycle 243–246\navoiding in-memory databases 246\nclearing data between test runs 244–245\nparallel vs. sequential test execution\n243–244\ndatabase transaction management 234–243\nin integration tests 242–243\nin production code 235–242\nseparating connections from transactions\n236–239\nupgrading transaction to unit of work\n239–242\ndatabase transactions 244\ndaysFromNow parameter 60\nDBMS (database management system) 246\ndead code 260\ndeliveryDate parameter 62\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\nINDEX\n277\ndependencies 28–29, 35\nclassical school of unit testing 30–34\nLondon school of unit testing 30–34\nout-of-process 161, 190\nshared 29, 31\ntypes of 115\nDetroit approach, unit testing 21\ndiagnostic logging 206, 212\ndiscovered abstractions 198\nDocker container 28\ndomain events, tracking changes in domain \nmodel 175–178\ndomain layers 106–107, 109, 133–134\ndomain model 16, 153, 225\nconnecting with external applications 111\ntestability 171\ndomain significance 153\ndummy test double 93–94\nE\nEasyMock 25\nedge cases 187, 189, 194\nencapsulation 46, 252\nend-to-end tests 88–89, 195–196, 205, 222\nclassical school of unit testing 38–39\nLondon school of unit testing 38–39\npossibility of creating ideal tests 81\nenterprise applications 5\nEntity Framework 240–242, 255\nentropy 6\nerror handling 146\nexceptions 130\nexpected parameter 62\nexplicit inputs and outputs 130\nexternal libraries 81\nexternal reads 170–171, 173\nexternal state 130\nexternal writes 170–171, 173\nF\nFail Fast principle 185, 189\nfailing preconditions 190\nfake dependencies 93\nfake test double 93–94\nfalse negatives 76–77\nfalse positives 69–70, 77, 82, 86, 96, 99, 124\ncauses of 71–74\nimportance of 78–79\nfast feedback 81–86, 88, 99, 123, 252, 260\nfat controllers 154\nfeedback loop, shortening 189\nfeedback speed 79–80, 124\nfixed state 50\nFluent Assertions 62\nfragile tests 96, 113\nframeworks 81\nfunctional architecture 128–134\ndefined 132–133\ndrawbacks of 146–149\napplicability of 147–148\ncode base size increases 149\nperformance drawbacks 148\nfunctional programming 128–131\nhexagonal architecture 133–134\ntransitioning to output-based testing 135–146\naudit system 135–137\nrefactoring toward functional \narchitecture 140–145\nusing mocks to decouple tests from \nfilesystem 137–140\nfunctional core 132–133, 143–144, 156\nfunctional programming 121\nfunctional testing 38, 121, 128\nG\nGit 230–231\nGiven-When-Then pattern 43\nGUI (graphical user interface) tests 38\nH\nhandwritten mocks 94, 222\nhappy paths 187, 194, 239\nhelper methods 126–127\nhexagonal architecture 106–107, 128, 156\ndefining 106–110\nfunctional architecture 133–134\npurpose of 107\nhexagons 106, 108, 134\nhidden outputs 131\nhigh coupling, reusing test fixtures between \ntests 52\nHTML tags 72\nhumble controller 160\nHumble Object pattern 155, 157–158, 167, 271\nhumble objects 157\nhumble wrappers 155\nI\nideal tests 80–87\nbrittle tests 83–84\nend-to-end tests 81\npossibility of creating 81\ntrivial tests 82–83\nif statements 10–11, 44–45, 143, 152, 173–174\nimmutability 133\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\nINDEX\n278\nimmutable classes 133\nimmutable core 132, 134\nimmutable events 176\nimmutable objects 30, 132\nimplementation details 99–105\nincoming interactions 94–95\ninfrastructure code 16\ninfrastructure layer 202\nin-memory databases 246\nin-process dependencies 199–200\nINSERT statements 231\ninteger type 14\nintegration testing\nbest practices 200–205\neliminating circular dependencies\n202–204\nmaking domain model boundaries \nexplicit 200\nmultiple act sections 204–205\nreducing number of layers 200–202\nclassical school of unit testing 37–39\ndatabase transaction management in\n242–243\ndefined 186–190\nexample of 193–197\ncategorizing database and message bus 195\nend-to-end testing 195–196\nfirst version 196–197\nscenarios 194\nfailing fast 188–190\ninterfaces for abstracting dependencies\n197–200\nin-process dependencies 199–200\nloose coupling and 198\nout-of-process dependencies 199\nlogging functionality 205–213\namount of logging 212\nintroducing wrapper on top of ILogger\n207–208\npassing around logger instances 212–213\nstructured logging 208–209\nwhether to test or not 205–206\nwriting tests for support and diagnostic \nlogging 209–211\nLondon school of unit testing 37–39\nout-of-process dependencies 190–193\ntypes of 190–191\nwhen real databases are unavailable\n192–193\nworking with both 191–192\nrole of 186–187\nTest Pyramid 187\ninterconnected classes 34\ninternal keyword 99\ninvariant violations 46, 103\ninvariants 100, 103\nisolation issue\nclassical school of unit testing 27–30\nLondon school of unit testing 21–27\nisSuccess flag 113\nJ\nJMock 25\nJSON files 208–209\nL\nlogging functionality testing 205–213\namount of logging 212\nintroducing wrapper on top of ILogger\n207–208\npassing around logger instances 212–213\nstructured logging 208–209\nwhether to test or not 205–206\nwriting tests for support and diagnostic \nlogging 209–211\nLondon school of unit testing 30–37\ndependencies 30–34\nend-to-end tests 38–39\nintegration tests 37–39\nisolation issue 21–27\nmocks 114–116\nmocking out out-of-process dependencies\n115–116\nusing mocks to verify behavior 116\nprecise bug location 36\ntesting large graph of interconnected classes 35\ntesting one class at a time 34–35\nloose coupling, interfaces for abstracting depen-\ndencies and 198\nM\nmaintainability 79–80, 85, 88, 99, 137, 148, \n252, 260\ncomparing testing styles 125–127\ncommunication-based tests 127\noutput-based tests 125\nstate-based tests 125–127\nmanaged dependencies 190, 192, 246\nmathematical functions 128–131\nmerging domain events 177\nmessage bus 190–192, 199, 220, 224\nmethod signatures 128\nmethod under test (MUT) 25\nMicrosoft MSTest 49\nmigration-based database delivery 232–234\nmissing abstractions 260\nmock chains 127\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\nINDEX\n279\nmocking frameworks 25\nmockist style, unit testing 21\nMockito 25\nmocks 25, 254\nbest practices 225–227\nfor integration tests only 225\nnot just one mock per test 225–226\nonly mock types that you own 227\nverifying number of calls 226\ndecoupling tests from filesystem 137–140\ndefined 25\nLondon school vs. classical school 114–116\nmocking out out-of-process \ndependencies 115–116\nusing mocks to verify behavior 116\nmaximizing value of 217–225\nIDomainLogger 224–225\nreplacing mocks with spies 222–224\nverifying interactions at system edges\n219–222\nmocking concrete classes 268–271\nobservable behavior vs. implementation \ndetails 99–105\nleaking implementation details 100–105\nobservable behavior vs. public API 99–100\nwell-designed API and encapsulation\n103–104\nstubs 93–98\nasserting interactions with stubs 96–97\ncommands and queries 97–98\nmock (tool) vs. mock (test double) 94–95\ntypes of test doubles 93–94\nusing mocks and stubs together 97\ntest doubles 25\ntest fragility 106–114\ndefining hexagonal architecture 106–110\nintra-system vs. inter-system \ncommunications 110–114\nmodel database 230\nModel-View-Controller (MVC) pattern 157\nMoq 25, 95, 226\nMSTest 49\nMUT (method under test) 25\nmutable objects 132\nmutable shell 132–133, 143–144\nMVC (Model-View-Controller) pattern 157\nN\nnaming tests 54–58\nguidelines for 56\nrenaming tests to meet guidelines 56–58\nNHibernate 240\nnoise, reducing 78\nNSubstitute 25\nNuGet package 49\nNUnit 49, 51\nO\nobject graphs 22–23\nObject Mother 248\nobject-oriented programming (OOP) 63, 133\nobject-relational mapping (ORM) 163, 177, \n227, 240, 243, 254–255, 263\nobservable behavior 99, 105, 108, 115, 263\nleaking implementation details 100–105\npublic API 99–100\nwell-designed API and encapsulation 103–104\nOCP (Open-Closed principle) 198\nOOP (object-oriented programming) 63, 133\nOpen-Closed principle (OCP) 198\noperations 99, 104\norchestration, separating business logic from\n169, 179\nORM (object-relational mapping) 163, 177, \n227, 240, 243, 254–255, 263\noutcoming interactions 94–95\nout-of-process collaborators 159–160\nout-of-process dependencies 28, 33, 38–39, \n115, 125, 148, 160–161, 167, 170, 176, \n186, 200, 229\nintegration testing 190–193\ninterfaces for abstracting dependencies 199\ntypes of 190–191\nwhen real databases are unavailable\n192–193\nworking with both 191–192\noutput value 121\noutput-based testing 120–121, 124, 128\nfeedback speed 124\nmaintainability 125\nprotection against regressions and feedback \nspeed 124\nresistance to refactoring 124–125\ntransitioning to functional architecture \nand 135–146\naudit system 135–137\nrefactoring toward functional \narchitecture 140–145\nusing mocks to decouple tests from \nfilesystem 137–140\novercomplicated code 154\noverspecification 96\nP\nparallel test execution 243–244\nparameterized tests 59, 61\npartition tolerance 86\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\nINDEX\n280\nperformance 171\npersistence state 189\npreconditions 190\nprivate APIs 99\nprivate constructors 263\nprivate dependencies 28–29, 31, 115\nprivate keyword 99\nprivate methods 260–263\nacceptability of testing 261–263\ninsufficient coverage and 260–261\nreusing test fixtures between tests 52–54\ntest fragility and 260\nProduct array 129\nproduction code 8\nprotection against regressions 68–69, 81, 84–86, \n88, 99, 260\ncomparing testing styles 124\nimportance of false positives and false \nnegatives 78–79\nmaximizing test accuracy 76–78\nPublic API 99, 109\npure functions 128\nQ\nqueries 97\nR\nrandom number generators 29\nread operations 252\nreadability 53\nread-decide-act approach 148\nrefactoring 165\nanalysis of optimal test coverage 167–169\ntesting domain layer and utility code 167–168\ntesting from other three quadrants 168\ntesting preconditions 169\nconditional logic in controllers 169–180\nCanExecute/Execute pattern 172–174\ndomain events for tracking changes in the \ndomain model 175–178\nidentifying code to refactor 152–158\nfour types of code 152–155\nHumble Object pattern for splitting overcom-\nplicated code 155–158\nresistance to 69–71\ncomparing testing styles 124–125\nimportance of false positives and false \nnegatives 78–79\nmaximizing test accuracy 76–78\nto parameterized tests\ngeneral discussion 58–62\ngenerating data for parameterized tests\n60–62\ntoward valuable unit tests 158–167\napplication services layer 160–162\nCompany class 164–167\ncustomer management system 158–160\nmaking implicit dependencies explicit 160\nremoving complexity from application \nservice 163–164\nreference data 231, 234, 245\nreferential transparency 130\nregression errors 8, 69, 82\nregressions 7, 229\nrepositories 236–237, 241, 253\nresistance to refactoring 69–71, 79–81, 83–85, \n88–90, 92–93, 99, 123, 260, 265\ncomparing testing styles 124–125\nimportance of false positives and false \nnegatives 78–79\nmaximizing test accuracy 76–78\nreturn statement 10\nreturn true statement 10\nreusability 53\nS\nscalability 7\nsequential test execution 243–244\nshallowness 124–125\nshared dependencies 28–29, 31, 33, 115, 148, 246\nside effects 130–134, 190\nsignal-to-noise ratio 212\nSingle Responsibility principle 157, 268, 270\nsingle-line act section 45\nSMTP service 110, 112–115, 134, 190\nsoftware bugs 7, 68\nsoftware entropy 6\nsource of truth 231\nspies 94, 222–224\nspy test double 93\nSQL scripts 231–232, 240, 245\nSQLite 246\nstate 99, 101\nstate verification 125\nstate-based database delivery 232\nstate-based testing 120–122, 124, 128, 135\nfeedback speed 124\nmaintainability 125–127\nprotection against regressions and feedback \nspeed 124\nresistance to refactoring 124–125\nstubs, mocks 93–98\nasserting interactions with stubs 96–97\ncommands and queries 97–98\nmock (tool) vs. mock (test double) 94–95\ntypes of test doubles 93–94\nusing mocks and stubs together 97\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "page_number": 295
    },
    {
      "number": 33,
      "title": "Segment 33 (pages 303-305)",
      "start_page": 303,
      "end_page": 305,
      "detection_method": "topic_boundary",
      "content": "INDEX\n281\nsub-renderers collection 105\nsupport logging 206, 212\nsustainability 7\nsustainable growth 6\nSUT (system under test) 24–25, 29, 36–37, 43, \n45, 47–48, 57, 71, 73–75, 84, 93–94, 96–97, \n120–121, 123, 153, 244, 264, 266\nswitch statement 10\nsynchronous communications 191\nsystem leaks 100\nT\ntables 191\ntautology tests 82\nTDD (test-driven development) 36, 43\ntell-don’t-ask principle 104\ntest code 8\ntest coverage 9\nTest Data Builder 248\ntest data life cycle 243–246\navoiding in-memory databases 246\nclearing data between test runs 244–245\nparallel vs. sequential test execution\n243–244\ntest doubles 22–23, 25, 28, 93–94, 98, 199\ntest fixtures 248\ndefined 50\nreusing between tests\nconstructors 52\nhigh coupling 52\nprivate factory methods 52–54\nreusing between tests 50–54\ntest fragility, mocks and 106–114\ndefining hexagonal architecture 106–110\nintra-system vs. inter-system \ncommunications 110–114\ntest isolation 115\nTest Pyramid\ngeneral discussion 87–89\nintegration testing 187\ntest suites\ncharacteristics of successful suites 15–17\nintegration into development cycle 16\nmaximum value with minimum maintenance \ncosts 17\ntargeting most important parts of code \nbase 16–17\ncoverage metrics, measuring test suite quality \nwith 8–15\naiming for particular coverage number 15\nbranch coverage metric 10–11\ncode coverage metric 9–10\nproblems with 12–15\nthird-party applications 81, 112\ntight coupling 5\ntime 271–273\nas ambient context 271–272\nas explicit dependency 272–273\ntrivial code 153–154\ntrivial tests 82–83\ntrue negative 76\ntrue positive 76\ntwo-line act section 46\nU\nUI (user interface) tests 38\nunit of behavior 56, 225\nunit of work 239, 242\nunit testing\nanatomy of 41–63\nAAA pattern 42–49\nassertion libraries, using to improve test \nreadability 62–63\nnaming tests 54–58\nrefactoring to parameterized tests 58–62\nreusing test fixtures between tests 50–54\nxUnit testing framework 49–50\nautomation concepts 87–90\nblack-box vs. white-box testing 89–90\nTest Pyramid 87–89\ncharacteristics of successful test suites 15–17\nintegration into development cycle 16\nmaximum value with minimum maintenance \ncosts 17\ntargeting most important parts of code \nbase 16–17\nclassical school of 30–37\ndependencies 30–34\nend-to-end tests 38–39\nintegration tests 37–39\nisolation issue 27–30\nprecise bug location 36\ntesting large graph of interconnected \nclasses 35\ntesting one class at a time 34–35\ncoverage metrics, measuring test suite quality \nwith 8–15\naiming for particular coverage number 15\nbranch coverage metric 10–11\ncode coverage metric 9–10\nproblems with 12–15\ncurrent state of 4–5\ndefined 21–30\nfour pillars of 68–80\nfeedback speed 79–80\nmaintainability 79–80\nprotection against regressions 68–69\nresistance to refactoring 69–71\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\nINDEX\n282\nunit testing (continued)\nfunctional architecture 128–134\ndefined 132–133\ndrawbacks of 146–149\nfunctional programming 128–131\nhexagonal architecture 133–134\ntransitioning to output-based testing\n135–146\ngoal of 5–8\ngood vs. bad tests 7–8\nideal tests 80–87\nbrittle tests 83–84\nend-to-end tests 81\npossibility of creating 81\ntrivial tests 82–83\nLondon school of 30–37\ndependencies 30–34\nend-to-end tests 38–39\nintegration tests 37–39\nisolation issue 21–27\nprecise bug location 36\ntesting large graph of interconnected \nclasses 35\ntesting one class at a time 34–35\nstyles of 120–123\ncommunication-based testing\n122–123\ncomparing 123–128\noutput-based testing 120–121\nstate-based testing 121–122\nunits of behavior 34\nunits of code 21, 27–29, 34, 47, 225\nunmanaged dependencies 190, 199, 211, 216, \n218, 220, 222, 226, 254\nuser controller 193\nuser interface (UI) tests 38\nV\nvalue objects 31, 126–127\nvoid type 97\nvolatile dependencies 29\nW\nwhite-box testing 89–90\nwrite operation 252\nX\nxUnit testing framework 49–50\nY\nYAGNI (You aren’t gonna need it) principle\n198–199\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n\n\nVladimir Khorikov\nG\nreat testing practices will help maximize your project \nquality and delivery speed. Wrong tests will break your \ncode, multiply bugs, and increase time and costs. You \nowe it to yourself—and your projects—to learn how to do \nexcellent unit testing to increase your productivity and the \nend-to-end quality of your software.\nUnit Testing: Principles, Practices, and Patterns teaches you to \ndesign and write tests that target the domain model and \nother key areas of your code base. In this clearly written \nguide, you learn to develop professional-quality test suites, \nsafely automate your testing process, and integrate testing \nthroughout the application life cycle. As you adopt a testing \nmindset, you’ll be amazed at how better tests cause you to \nwrite better code. \nWhat’s Inside\n● Universal guidelines to assess any unit test\n● Testing to identify and avoid anti-patterns\n● Refactoring tests along with the production code\n● Using integration tests to verify the whole system\nFor readers who know the basics of unit testing. The C# \nexamples apply to any language.\nVladimir Khorikov is an author, blogger, and Microsoft MVP. \nHe has mentored numerous teams on the ins and outs of \nunit testing.\nTo download their free eBook in PDF, ePub, and Kindle formats, owners \nof this book should visit www.manning.com/books/unit-testing\n$49.99 / Can $65.99  [INCLUDING eBOOK]\nUnit Testing Principles, Practices, and Patterns\nTESTING/SOFTWARE DEVELOPMENT\nM A N N I N G\n“\nThis book is an\n indispensable resource.”\n \n—Greg Wright\nKainos Software Ltd.\n“\nServes as a valuable and \nhumbling encouragement \nto double down and test \nwell, something we need \nno matter how experienced \n  we may be.”\n \n—Mark Nenadov, BorderConnect\n“\nI wish I had this book \ntwenty years ago when I was \nstarting my career in \n  software development.”\n—Conor Redmond\nIncomm Product Control \n“\nThis is the kind of book \non unit testing I have been \n waiting on for a long time.”\n \n—Jeremy Lange, G2\nSee first page\nISBN-13: 978-1-61729-627-7\nISBN-10: 1-61729-627-9\n",
      "page_number": 303
    }
  ],
  "pages": [
    {
      "page_number": 1,
      "content": "M A N N I N G\nVladimir Khorikov\nPrinciples, Practices, and Patterns\n",
      "content_length": 68,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 2,
      "content": "Protection against\nregressions\n(ch. 4)\nResistance to\nrefactoring\n(ch. 4)\nFast feedback\n(ch. 4)\nMaintainability\n(ch. 4)\nDeﬁned by\nDeﬁned by\nUnit tests\nIntegration tests\nMaximize\nMaximize\nMaximize\nMaximize\nManaged\ndependencies\n(ch. 8)\nUnmanaged\ndependencies\n(ch. 8)\nTest accuracy\n(ch. 4)\nFalse positives\n(ch. 4)\nFalse negatives\n(ch. 4)\nTackled by\nTackled by\nMocks\n(ch. 5)\nShould not be used for\nShould be used for\nDomain model and\nalgorithms\n(ch. 7)\nControllers\n(ch. 7)\nCover\nCover\nComplexity\n(ch. 7)\nCollaborators\n(ch. 2)\nHave large number of\nIn-process\ndependencies\n(ch. 2)\nOut-of-process\ndependencies\n(ch. 2)\nHave high\nAre\nAre\nAre\nAre\nUsed in\nDamage if used incorrectly\nChapter Map\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 734,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 3,
      "content": "Unit Testing:\nPrinciples, Practices,\nand Patterns\nVLADIMIR KHORIKOV\nM A N N I N G\nSHELTER ISLAND\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 148,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 4,
      "content": "For online information and ordering of this and other Manning books, please visit\nwww.manning.com. The publisher offers discounts on this book when ordered in quantity. \nFor more information, please contact\nSpecial Sales Department\nManning Publications Co.\n20 Baldwin Road\nPO Box 761\nShelter Island, NY 11964\nEmail: orders@manning.com\n©2020 by Manning Publications Co. All rights reserved.\nNo part of this publication may be reproduced, stored in a retrieval system, or transmitted, in \nany form or by means electronic, mechanical, photocopying, or otherwise, without prior written \npermission of the publisher.\nMany of the designations used by manufacturers and sellers to distinguish their products are \nclaimed as trademarks. Where those designations appear in the book, and Manning Publications \nwas aware of a trademark claim, the designations have been printed in initial caps or all caps.\nRecognizing the importance of preserving what has been written, it is Manning’s policy to have \nthe books we publish printed on acid-free paper, and we exert our best efforts to that end. \nRecognizing also our responsibility to conserve the resources of our planet, Manning books\nare printed on paper that is at least 15 percent recycled and processed without the use of \nelemental chlorine.\nManning Publications Co.\nAcquisitions editor: Mike Stephens\n20 Baldwin Road\nDevelopment editor: Marina Michaels\nPO Box 761\nTechnical development editor: Sam Zaydel\nShelter Island, NY 11964\nReview editor: Aleksandar Dragosavljevic´\nProduction editor: Anthony Calcara\nCopy editor: Tiffany Taylor\nESL copyeditor: Frances Buran\nProofreader: Keri Hales\nTechnical proofreader: Alessandro Campeis\nTypesetter: Dennis Dalinnik\nCover designer: Marija Tudor\nISBN: 9781617296277\nPrinted in the United States of America\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1846,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 5,
      "content": " To my wife, Nina\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 69,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 6,
      "content": " \nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 53,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 7,
      "content": "v\nbrief contents\nPART 1\nTHE BIGGER PICTURE....................................................1\n1\n■\nThe goal of unit testing\n3\n2\n■\nWhat is a unit test?\n20\n3\n■\nThe anatomy of a unit test\n41\nPART 2\nMAKING YOUR TESTS WORK FOR YOU...........................65\n4\n■\nThe four pillars of a good unit test\n67\n5\n■\nMocks and test fragility\n92\n6\n■\nStyles of unit testing\n119\n7\n■\nRefactoring toward valuable unit tests\n151\nPART 3\nINTEGRATION TESTING..............................................183\n8\n■\nWhy integration testing?\n185\n9\n■\nMocking best practices\n216\n10\n■\nTesting the database\n229\nPART 4\nUNIT TESTING ANTI-PATTERNS...................................257\n11\n■\nUnit testing anti-patterns\n259\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 739,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 8,
      "content": "Licensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 51,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 9,
      "content": "vii\ncontents\npreface\nxiv\nacknowledgments\nxv\nabout this book\nxvi\nabout the author\nxix\nabout the cover illustration\nxx\nPART 1\nTHE BIGGER PICTURE..........................................1\n1 \nThe goal of unit testing\n3\n1.1\nThe current state of unit testing\n4\n1.2\nThe goal of unit testing\n5\nWhat makes a good or bad test?\n7\n1.3\nUsing coverage metrics to measure test suite quality\n8\nUnderstanding the code coverage metric\n9\n■Understanding the \nbranch coverage metric\n10\n■Problems with coverage metrics\n12\nAiming at a particular coverage number\n15\n1.4\nWhat makes a successful test suite?\n15\nIt’s integrated into the development cycle\n16\n■It targets only the \nmost important parts of your code base\n16\n■It provides maximum \nvalue with minimum maintenance costs\n17\n1.5\nWhat you will learn in this book\n17\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 849,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 10,
      "content": "CONTENTS\nviii\n2 \nWhat is a unit test?\n20\n2.1\nThe definition of “unit test”\n21\nThe isolation issue: The London take\n21\n■The isolation issue: \nThe classical take\n27\n2.2\nThe classical and London schools of unit testing\n30\nHow the classical and London schools handle dependencies\n30\n2.3\nContrasting the classical and London schools \nof unit testing\n34\nUnit testing one class at a time\n34\n■Unit testing a large graph of \ninterconnected classes\n35\n■Revealing the precise bug location\n36\nOther differences between the classical and London schools\n36\n2.4\nIntegration tests in the two schools\n37\nEnd-to-end tests are a subset of integration tests\n38\n3 \nThe anatomy of a unit test\n41\n3.1\nHow to structure a unit test\n42\nUsing the AAA pattern\n42\n■Avoid multiple arrange, act, \nand assert sections\n43\n■Avoid if statements in tests\n44\nHow large should each section be?\n45\n■How many assertions \nshould the assert section hold?\n47\n■What about the teardown \nphase?\n47\n■Differentiating the system under test\n47\nDropping the arrange, act, and assert comments from tests\n48\n3.2\nExploring the xUnit testing framework\n49\n3.3\nReusing test fixtures between tests\n50\nHigh coupling between tests is an anti-pattern\n52\n■The use of \nconstructors in tests diminishes test readability\n52\n■A better way \nto reuse test fixtures\n52\n3.4\nNaming a unit test\n54\nUnit test naming guidelines\n56\n■Example: Renaming a test \ntoward the guidelines\n56\n3.5\nRefactoring to parameterized tests\n58\nGenerating data for parameterized tests\n60\n3.6\nUsing an assertion library to further improve \ntest readability\n62\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1616,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 11,
      "content": "CONTENTS\nix\nPART 2\nMAKING YOUR TESTS WORK FOR YOU.................65\n4 \nThe four pillars of a good unit test\n67\n4.1\nDiving into the four pillars of a good unit test\n68\nThe first pillar: Protection against regressions\n68\n■The second \npillar: Resistance to refactoring\n69\n■What causes false \npositives?\n71\n■Aim at the end result instead of \nimplementation details\n74\n4.2\nThe intrinsic connection between the first \ntwo attributes\n76\nMaximizing test accuracy\n76\n■The importance of false positives \nand false negatives: The dynamics\n78\n4.3\nThe third and fourth pillars: Fast feedback \nand maintainability\n79\n4.4\nIn search of an ideal test\n80\nIs it possible to create an ideal test?\n81\n■Extreme case #1: \nEnd-to-end tests\n81\n■Extreme case #2: Trivial tests\n82\nExtreme case #3: Brittle tests\n83\n■In search of an ideal test: \nThe results\n84\n4.5\nExploring well-known test automation concepts\n87\nBreaking down the Test Pyramid\n87\n■Choosing between black-box \nand white-box testing\n89\n5 \nMocks and test fragility\n92\n5.1\nDifferentiating mocks from stubs\n93\nThe types of test doubles\n93\n■Mock (the tool) vs. mock (the \ntest double)\n94\n■Don’t assert interactions with stubs\n96\nUsing mocks and stubs together\n97\n■How mocks and stubs \nrelate to commands and queries\n97\n5.2\nObservable behavior vs. implementation details\n99\nObservable behavior is not the same as a public API\n99\n■Leaking \nimplementation details: An example with an operation\n100\nWell-designed API and encapsulation\n103\n■Leaking \nimplementation details: An example with state\n104\n5.3\nThe relationship between mocks and test fragility\n106\nDefining hexagonal architecture\n106\n■Intra-system vs. inter-\nsystem communications\n110\n■Intra-system vs. inter-system \ncommunications: An example\n111\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1789,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 12,
      "content": "CONTENTS\nx\n5.4\nThe classical vs. London schools of unit testing, \nrevisited\n114\nNot all out-of-process dependencies should be mocked out\n115\nUsing mocks to verify behavior\n116\n6 \nStyles of unit testing\n119\n6.1\nThe three styles of unit testing\n120\nDefining the output-based style\n120\n■Defining the state-based \nstyle\n121\n■Defining the communication-based style\n122\n6.2\nComparing the three styles of unit testing\n123\nComparing the styles using the metrics of protection against \nregressions and feedback speed\n124\n■Comparing the styles using \nthe metric of resistance to refactoring\n124\n■Comparing the styles \nusing the metric of maintainability\n125\n■Comparing the styles: \nThe results\n127\n6.3\nUnderstanding functional architecture\n128\nWhat is functional programming?\n128\n■What is functional \narchitecture?\n132\n■Comparing functional and hexagonal \narchitectures\n133\n6.4\nTransitioning to functional architecture and output-based \ntesting\n135\nIntroducing an audit system\n135\n■Using mocks to decouple tests \nfrom the filesystem\n137\n■Refactoring toward functional \narchitecture\n140\n■Looking forward to further developments\n146\n6.5\nUnderstanding the drawbacks of functional architecture\n146\nApplicability of functional architecture\n147\n■Performance \ndrawbacks\n148\n■Increase in the code base size\n149\n7 \nRefactoring toward valuable unit tests\n151\n7.1\nIdentifying the code to refactor\n152\nThe four types of code\n152\n■Using the Humble Object pattern to \nsplit overcomplicated code\n155\n7.2\nRefactoring toward valuable unit tests\n158\nIntroducing a customer management system\n158\n■Take 1: \nMaking implicit dependencies explicit\n160\n■Take 2: Introducing \nan application services layer\n160\n■Take 3: Removing complexity \nfrom the application service\n163\n■Take 4: Introducing a new \nCompany class\n164\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1835,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 13,
      "content": "CONTENTS\nxi\n7.3\nAnalysis of optimal unit test coverage\n167\nTesting the domain layer and utility code\n167\n■Testing the code \nfrom the other three quadrants\n168\n■Should you test \npreconditions?\n169\n7.4\nHandling conditional logic in controllers\n169\nUsing the CanExecute/Execute pattern\n172\n■Using domain \nevents to track changes in the domain model\n175\n7.5\nConclusion\n178\nPART 3\nINTEGRATION TESTING....................................183\n8 \nWhy integration testing?\n185\n8.1\nWhat is an integration test?\n186\nThe role of integration tests\n186\n■The Test Pyramid \nrevisited\n187\n■Integration testing vs. failing fast\n188\n8.2\nWhich out-of-process dependencies to test directly\n190\nThe two types of out-of-process dependencies\n190\n■Working with \nboth managed and unmanaged dependencies\n191\n■What if you \ncan’t use a real database in integration tests?\n192\n8.3\nIntegration testing: An example\n193\nWhat scenarios to test?\n194\n■Categorizing the database and \nthe message bus\n195\n■What about end-to-end testing?\n195\nIntegration testing: The first try\n196\n8.4\nUsing interfaces to abstract dependencies\n197\nInterfaces and loose coupling\n198\n■Why use interfaces for \nout-of-process dependencies?\n199\n■Using interfaces for in-process \ndependencies\n199\n8.5\nIntegration testing best practices\n200\nMaking domain model boundaries explicit\n200\n■Reducing the \nnumber of layers\n200\n■Eliminating circular dependencies\n202\nUsing multiple act sections in a test\n204\n8.6\nHow to test logging functionality\n205\nShould you test logging?\n205\n■How should you test \nlogging?\n207\n■How much logging is enough?\n212\nHow do you pass around logger instances?\n212\n8.7\nConclusion\n213\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1692,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 14,
      "content": "CONTENTS\nxii\n9 \nMocking best practices\n216\n9.1\nMaximizing mocks’ value\n217\nVerifying interactions at the system edges\n219\n■Replacing mocks \nwith spies\n222\n■What about IDomainLogger?\n224\n9.2\nMocking best practices\n225\nMocks are for integration tests only\n225\n■Not just one mock per \ntest\n225\n■Verifying the number of calls\n226\n■Only mock types \nthat you own\n227\n10 \nTesting the database\n229\n10.1\nPrerequisites for testing the database\n230\nKeeping the database in the source control system\n230\n■Reference \ndata is part of the database schema\n231\n■Separate instance for \nevery developer\n232\n■State-based vs. migration-based database \ndelivery\n232\n10.2\nDatabase transaction management\n234\nManaging database transactions in production code\n235\n■Managing \ndatabase transactions in integration tests\n242\n10.3\nTest data life cycle\n243\nParallel vs. sequential test execution\n243\n■Clearing data between \ntest runs\n244\n■Avoid in-memory databases\n246\n10.4\nReusing code in test sections\n246\nReusing code in arrange sections\n246\n■Reusing code in \nact sections\n249\n■Reusing code in assert sections\n250\nDoes the test create too many database transactions?\n251\n10.5\nCommon database testing questions\n252\nShould you test reads?\n252\n■Should you test repositories?\n253\n10.6\nConclusion\n254\nPART 3\nUNIT TESTING ANTI-PATTERNS.........................257\n11 \nUnit testing anti-patterns\n259\n11.1\nUnit testing private methods\n260\nPrivate methods and test fragility\n260\n■Private methods and \ninsufficient coverage\n260\n■When testing private methods is \nacceptable\n261\n11.2\nExposing private state\n263\n11.3\nLeaking domain knowledge to tests\n264\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1666,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 15,
      "content": "CONTENTS\nxiii\n11.4\nCode pollution\n266\n11.5\nMocking concrete classes\n268\n11.6\nWorking with time\n271\nTime as an ambient context\n271\n■Time as an explicit \ndependency\n272\n11.7\nConclusion\n273\nindex\n275\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 248,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 16,
      "content": "xiv\npreface\nI remember my first project where I tried out unit testing. It went relatively well; but after\nit was finished, I looked at the tests and thought that a lot of them were a pure waste of\ntime. Most of my unit tests spent a great deal of time setting up expectations and wiring\nup a complicated web of dependencies—all that, just to check that the three lines of\ncode in my controller were correct. I couldn’t pinpoint what exactly was wrong with the\ntests, but my sense of proportion sent me unambiguous signals that something was off.\n Luckily, I didn’t abandon unit testing and continued applying it in subsequent\nprojects. However, disagreement with common (at that time) unit testing practices\nhas been growing in me ever since. Throughout the years, I’ve written a lot about unit\ntesting. In those writings, I finally managed to crystallize what exactly was wrong with\nmy first tests and generalized this knowledge to broader areas of unit testing. This\nbook is a culmination of all my research, trial, and error during that period—compiled,\nrefined, and distilled.\n I come from a mathematical background and strongly believe that guidelines in\nprogramming, like theorems in math, should be derived from first principles. I’ve\ntried to structure this book in a similar way: start with a blank slate by not jumping to\nconclusions or throwing around unsubstantiated claims, and gradually build my case\nfrom the ground up. Interestingly enough, once you establish such first principles,\nguidelines and best practices often flow naturally as mere implications.\n I believe that unit testing is becoming a de facto requirement for software proj-\nects, and this book will give you everything you need to create valuable, highly main-\ntainable tests.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1810,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 17,
      "content": "xv\nacknowledgments\nThis book was a lot of work. Even though I was prepared mentally, it was still much\nmore work than I could ever have imagined.\n A big “thank you” to Sam Zaydel, Alessandro Campeis, Frances Buran, Tiffany\nTaylor, and especially Marina Michaels, whose invaluable feedback helped shape the\nbook and made me a better writer along the way. Thanks also to everyone else at Man-\nning who worked on this book in production and behind the scenes.\n I’d also like to thank the reviewers who took the time to read my manuscript at var-\nious stages during its development and who provided valuable feedback: Aaron Barton,\nAlessandro Campeis, Conor Redmond, Dror Helper, Greg Wright, Hemant Koneru,\nJeremy Lange, Jorge Ezequiel Bo, Jort Rodenburg, Mark Nenadov, Marko Umek,\nMarkus Matzker, Srihari Sridharan, Stephen John Warnett, Sumant Tambe, Tim van\nDeurzen, and Vladimir Kuptsov.\n Above all, I would like to thank my wife Nina, who supported me during the whole\nprocess.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1031,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 18,
      "content": "xvi\nabout this book\nUnit Testing: Principles, Practices, and Patterns provides insights into the best practices\nand common anti-patterns that surround the topic of unit testing. After reading this\nbook, armed with your newfound skills, you’ll have the knowledge needed to become\nan expert at delivering successful projects that are easy to maintain and extend,\nthanks to the tests you build along the way.\nWho should read this book\nMost online and print resources have one drawback: they focus on the basics of unit\ntesting but don’t go much beyond that. There’s a lot of value in such resources, but\nthe learning doesn’t end there. There’s a next level: not just writing tests, but doing it\nin a way that gives you the best return on your efforts. When you reach this point on\nthe learning curve, you’re pretty much left to your own devices to figure out how to\nget to the next level.\n This book takes you to that next level. It teaches a scientific, precise definition of\nthe ideal unit test. That definition provides a universal frame of reference, which will\nhelp you look at many of your tests in a new light and see which of them contribute to\nthe project and which must be refactored or removed.\n If you don’t have much experience with unit testing, you’ll learn a lot from this book.\nIf you’re an experienced programmer, you most likely already understand some of the\nideas taught in this book. The book will help you articulate why the techniques and best\npractices you’ve been using all along are so helpful. And don’t underestimate this skill:\nthe ability to clearly communicate your ideas to colleagues is priceless.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1680,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 19,
      "content": "ABOUT THIS BOOK\nxvii\nHow this book is organized: A roadmap\nThe book’s 11 chapters are divided into 4 parts. Part 1 introduces unit testing and\ngives a refresher on some of the more generic unit testing principles:\n■\nChapter 1 defines the goal of unit testing and gives an overview of how to differ-\nentiate a good test from a bad one.\n■\nChapter 2 explores the definition of unit test and discusses the two schools of\nunit testing.\n■\nChapter 3 provides a refresher on some basic topics, such as structuring of unit\ntests, reusing test fixtures, and test parameterization.\nPart 2 gets to the heart of the subject—it shows what makes a good unit test and pro-\nvides details about how to refactor your tests toward being more valuable:\n■\nChapter 4 defines the four pillars that form a good unit test and provide a com-\nmon frame of reference that is used throughout the book.\n■\nChapter 5 builds a case for mocks and explores their relation to test fragility.\n■\nChapter 6 examines the three styles of unit testing, along with which of those\nstyles produces tests of the best quality and why.\n■\nChapter 7 teaches you how to refactor away from bloated, overcomplicated\ntests and achieve tests that provide maximum value with minimum mainte-\nnance costs.\nPart 3 explores the topic of integration testing:\n■\nChapter 8 looks at integration testing in general along with its benefits and\ntrade-offs.\n■\nChapter 9 discusses mocks and how to use them in a way that benefits your tests\nthe most.\n■\nChapter 10 explores working with relational databases in tests.\nPart 4’s chapter 11 covers common unit testing anti-patterns, some of which you’ve\npossibly encountered before.\nAbout the Code\nThe code samples are written in C#, but the topics they illustrate are applicable to any\nobject-oriented language, such as Java or C++. C# is just the language that I happen to\nwork with the most.\n I tried not to use any C#-specific language features, and I made the sample code as\nsimple as possible, so you shouldn’t have any trouble understanding it. You can down-\nload all of the code samples online at www.manning.com/books/unit-testing.\n \n \nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2172,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 20,
      "content": "ABOUT THIS BOOK\nxviii\nliveBook discussion forum\nPurchase of Unit Testing: Principles, Practices, and Patterns includes free access to a private\nweb forum run by Manning Publications where you can make comments about the\nbook, ask technical questions, and receive help from the author and from other\nusers. To access the forum, go to https://livebook.manning.com/#!/book/unit-testing/\ndiscussion. You can also learn more about Manning’s forums and the rules of conduct\nat https://livebook.manning.com/#!/discussion.\n Manning’s commitment to our readers is to provide a venue where a meaningful\ndialogue between individual readers and between readers and the author can take\nplace. It is not a commitment to any specific amount of participation on the part of\nthe author, whose contribution to the forum remains voluntary (and unpaid). We sug-\ngest you try asking the author some challenging questions lest his interest stray! The\nforum and the archives of previous discussions will be accessible from the publisher’s\nwebsite as long as the book is in print.\nOther online resources\n■\nMy blog is at EnterpriseCraftsmanship.com.\n■\nI also have an online course about unit testing (in the works, as of this writing),\nwhich you can enroll in at UnitTestingCourse.com.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1312,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 21,
      "content": "xix\nabout the author\nVLADIMIR KHORIKOV is a software engineer, Microsoft MVP, and Pluralsight author. He\nhas been professionally involved in software development for over 15 years, including\nmentoring teams on the ins and outs of unit testing. During the past several years,\nVladimir has written several popular blog post series and an online training course on\nthe topic of unit testing. The biggest advantage of his teaching style, and the one stu-\ndents often praise, is his tendency to have a strong theoretic background, which he\nthen applies to practical examples.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 622,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 22,
      "content": "xx\nabout the cover illustration\nThe figure on the cover of Unit Testing: Principles, Practices, and Patterns is captioned\n“Esthinienne.” The illustration is taken from a collection of dress costumes from vari-\nous countries by Jacques Grasset de Saint-Sauveur (1757–1810), titled Costumes Civils\nActuels de Tous les Peuples Connus, published in France in 1788. Each illustration is\nfinely drawn and colored by hand. The rich variety of Grasset de Saint-Sauveur’s col-\nlection reminds us vividly of how culturally apart the world’s towns and regions were\njust 200 years ago. Isolated from each other, people spoke different dialects and lan-\nguages. In the streets or in the countryside, it was easy to identify where they lived and\nwhat their trade or station in life was just by their dress.\n The way we dress has changed since then and the diversity by region, so rich at the\ntime, has faded away. It is now hard to tell apart the inhabitants of different conti-\nnents, let alone different towns, regions, or countries. Perhaps we have traded cultural\ndiversity for a more varied personal life—certainly for a more varied and fast-paced\ntechnological life.\n At a time when it is hard to tell one computer book from another, Manning cele-\nbrates the inventiveness and initiative of the computer business with book covers\nbased on the rich diversity of regional life of two centuries ago, brought back to life by\nGrasset de Saint-Sauveur’s pictures.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1501,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 23,
      "content": "Part 1\nThe bigger picture\nThis part of the book will get you up to speed with the current state of unit\ntesting. In chapter 1, I’ll define the goal of unit testing and give an overview of\nhow to differentiate a good test from a bad one. We’ll talk about coverage metrics\nand discuss properties of a good unit test in general.\n In chapter 2, we’ll look at the definition of unit test. A seemingly minor dis-\nagreement over this definition has led to the formation of two schools of unit test-\ning, which we’ll also dive into. Chapter 3 provides a refresher on some basic topics,\nsuch as structuring of unit tests, reusing test fixtures, and test parametrization.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 713,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 24,
      "content": " \nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 53,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 25,
      "content": "3\nThe goal of unit testing\nLearning unit testing doesn’t stop at mastering the technical bits of it, such as\nyour favorite test framework, mocking library, and so on. There’s much more to\nunit testing than the act of writing tests. You always have to strive to achieve the\nbest return on the time you invest in unit testing, minimizing the effort you put\ninto tests and maximizing the benefits they provide. Achieving both things isn’t\nan easy task.\n It’s fascinating to watch projects that have achieved this balance: they grow\neffortlessly, don’t require much maintenance, and can quickly adapt to their cus-\ntomers’ ever-changing needs. It’s equally frustrating to see projects that failed to do\nso. Despite all the effort and an impressive number of unit tests, such projects drag\non slowly, with lots of bugs and upkeep costs.\nThis chapter covers\nThe state of unit testing\nThe goal of unit testing\nConsequences of having a bad test suite\nUsing coverage metrics to measure test \nsuite quality\nAttributes of a successful test suite\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1091,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 26,
      "content": "4\nCHAPTER 1\nThe goal of unit testing\n That’s the difference between various unit testing techniques. Some yield great\noutcomes and help maintain software quality. Others don’t: they result in tests that\ndon’t contribute much, break often, and require a lot of maintenance in general.\n What you learn in this book will help you differentiate between good and bad unit\ntesting techniques. You’ll learn how to do a cost-benefit analysis of your tests and apply\nproper testing techniques in your particular situation. You’ll also learn how to avoid\ncommon anti-patterns—patterns that may make sense at first but lead to trouble down\nthe road.\n But let’s start with the basics. This chapter gives a quick overview of the state of\nunit testing in the software industry, describes the goal behind writing and maintain-\ning tests, and provides you with the idea of what makes a test suite successful.\n1.1\nThe current state of unit testing\nFor the past two decades, there’s been a push toward adopting unit testing. The push\nhas been so successful that unit testing is now considered mandatory in most compa-\nnies. Most programmers practice unit testing and understand its importance. There’s\nno longer any dispute as to whether you should do it. Unless you’re working on a\nthrowaway project, the answer is, yes, you do.\n When it comes to enterprise application development, almost every project\nincludes at least some unit tests. A significant percentage of such projects go far\nbeyond that: they achieve good code coverage with lots and lots of unit and integra-\ntion tests. The ratio between the production code and the test code could be any-\nwhere between 1:1 and 1:3 (for each line of production code, there are one to\nthree lines of test code). Sometimes, this ratio goes much higher than that, to a\nwhopping 1:10.\n But as with all new technologies, unit testing continues to evolve. The discussion\nhas shifted from “Should we write unit tests?” to “What does it mean to write good unit\ntests?” This is where the main confusion still lies.\n You can see the results of this confusion in software projects. Many projects have\nautomated tests; they may even have a lot of them. But the existence of those tests\noften doesn’t provide the results the developers hope for. It can still take program-\nmers a lot of effort to make progress in such projects. New features take forever to\nimplement, new bugs constantly appear in the already implemented and accepted\nfunctionality, and the unit tests that are supposed to help don’t seem to mitigate this\nsituation at all. They can even make it worse.\n It’s a horrible situation for anyone to be in—and it’s the result of having unit tests\nthat don’t do their job properly. The difference between good and bad tests is not\nmerely a matter of taste or personal preference, it’s a matter of succeeding or failing\nat this critical project you’re working on.\n It’s hard to overestimate the importance of the discussion of what makes a good\nunit test. Still, this discussion isn’t occurring much in the software development industry\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 3117,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 27,
      "content": "5\nThe goal of unit testing\ntoday. You’ll find a few articles and conference talks online, but I’ve yet to see any\ncomprehensive material on this topic.\n The situation in books isn’t any better; most of them focus on the basics of unit\ntesting but don’t go much beyond that. Don’t get me wrong. There’s a lot of value in\nsuch books, especially when you are just starting out with unit testing. However, the\nlearning doesn’t end with the basics. There’s a next level: not just writing tests, but\ndoing unit testing in a way that provides you with the best return on your efforts.\nWhen you reach this point, most books pretty much leave you to your own devices to\nfigure out how to get to that next level.\n This book takes you there. It teaches a precise, scientific definition of the ideal\nunit test. You’ll see how this definition can be applied to practical, real-world exam-\nples. My hope is that this book will help you understand why your particular project\nmay have gone sideways despite having a good number of tests, and how to correct its\ncourse for the better.\n You’ll get the most value out of this book if you work in enterprise application\ndevelopment, but the core ideas are applicable to any software project.\n1.2\nThe goal of unit testing\nBefore taking a deep dive into the topic of unit testing, let’s step back and consider\nthe goal that unit testing helps you to achieve. It’s often said that unit testing practices\nlead to a better design. And it’s true: the necessity to write unit tests for a code base\nnormally leads to a better design. But that’s not the main goal of unit testing; it’s\nmerely a pleasant side effect.\nWhat is an enterprise application?\nAn enterprise application is an application that aims at automating or assisting an\norganization’s inner processes. It can take many forms, but usually the characteris-\ntics of an enterprise software are\nHigh business logic complexity\nLong project lifespan\nModerate amounts of data\nLow or moderate performance requirements \nThe relationship between unit testing and code design\nThe ability to unit test a piece of code is a nice litmus test, but it only works in one\ndirection. It’s a good negative indicator—it points out poor-quality code with relatively\nhigh accuracy. If you find that code is hard to unit test, it’s a strong sign that the code\nneeds improvement. The poor quality usually manifests itself in tight coupling, which\nmeans different pieces of production code are not decoupled from each other\nenough, and it’s hard to test them separately.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2587,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 28,
      "content": "6\nCHAPTER 1\nThe goal of unit testing\nWhat is the goal of unit testing, then? The goal is to enable sustainable growth of the\nsoftware project. The term sustainable is key. It’s quite easy to grow a project, especially\nwhen you start from scratch. It’s much harder to sustain this growth over time.\n Figure 1.1 shows the growth dynamic of a typical project without tests. You start\noff quickly because there’s nothing dragging you down. No bad architectural deci-\nsions have been made yet, and there isn’t any existing code to worry about. As time\ngoes by, however, you have to put in more and more hours to make the same amount\nof progress you showed at the beginning. Eventually, the development speed slows\ndown significantly, sometimes even to the point where you can’t make any progress\nwhatsoever.\nThis phenomenon of quickly decreasing development speed is also known as software\nentropy. Entropy (the amount of disorder in a system) is a mathematical and scientific\nconcept that can also apply to software systems. (If you’re interested in the math and\nscience of entropy, look up the second law of thermodynamics.)\n In software, entropy manifests in the form of code that tends to deteriorate. Each\ntime you change something in a code base, the amount of disorder in it, or entropy,\nincreases. If left without proper care, such as constant cleaning and refactoring, the\nsystem becomes increasingly complex and disorganized. Fixing one bug introduces\nmore bugs, and modifying one part of the software breaks several others—it’s like a\n(continued)\nUnfortunately, the ability to unit test a piece of code is a bad positive indicator. The\nfact that you can easily unit test your code base doesn’t necessarily mean it’s of\ngood quality. The project can be a disaster even when it exhibits a high degree of\ndecoupling.\nWithout tests\nWith tests\nProgress\nhours\nspent\nWork\nFigure 1.1\nThe difference in growth \ndynamics between projects with and \nwithout tests. A project without tests \nhas a head start but quickly slows down \nto the point that it’s hard to make any \nprogress.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2127,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 29,
      "content": "7\nThe goal of unit testing\ndomino effect. Eventually, the code base becomes unreliable. And worst of all, it’s\nhard to bring it back to stability.\n Tests help overturn this tendency. They act as a safety net—a tool that provides\ninsurance against a vast majority of regressions. Tests help make sure the existing\nfunctionality works, even after you introduce new features or refactor the code to bet-\nter fit new requirements.\nDEFINITION\nA regression is when a feature stops working as intended after a cer-\ntain event (usually, a code modification). The terms regression and software bug\nare synonyms and can be used interchangeably.\nThe downside here is that tests require initial—sometimes significant—effort. But they\npay for themselves in the long run by helping the project to grow in the later stages.\nSoftware development without the help of tests that constantly verify the code base\nsimply doesn’t scale.\n Sustainability and scalability are the keys. They allow you to maintain development\nspeed in the long run.\n1.2.1\nWhat makes a good or bad test?\nAlthough unit testing helps maintain project growth, it’s not enough to just write tests.\nBadly written tests still result in the same picture.\n As shown in figure 1.2, bad tests do help to slow down code deterioration at the\nbeginning: the decline in development speed is less prominent compared to the situa-\ntion with no tests at all. But nothing really changes in the grand scheme of things. It\nmight take longer for such a project to enter the stagnation phase, but stagnation is\nstill inevitable.\nWithout tests\nWith good tests\nWith bad tests\nProgress\nWork\nhours\nspent\nFigure 1.2\nThe difference in \ngrowth dynamics between \nprojects with good and bad \ntests. A project with badly \nwritten tests exhibits the \nproperties of a project with \ngood tests at the beginning, \nbut it eventually falls into \nthe stagnation phase.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1937,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 30,
      "content": "8\nCHAPTER 1\nThe goal of unit testing\nRemember, not all tests are created equal. Some of them are valuable and contribute a lot\nto overall software quality. Others don’t. They raise false alarms, don’t help you catch\nregression errors, and are slow and difficult to maintain. It’s easy to fall into the trap\nof writing unit tests for the sake of unit testing without a clear picture of whether it\nhelps the project.\n You can’t achieve the goal of unit testing by just throwing more tests at the project.\nYou need to consider both the test’s value and its upkeep cost. The cost component is\ndetermined by the amount of time spent on various activities:\nRefactoring the test when you refactor the underlying code\nRunning the test on each code change\nDealing with false alarms raised by the test\nSpending time reading the test when you’re trying to understand how the\nunderlying code behaves\nIt’s easy to create tests whose net value is close to zero or even is negative due to high\nmaintenance costs. To enable sustainable project growth, you have to exclusively\nfocus on high-quality tests—those are the only type of tests that are worth keeping in\nthe test suite.\nIt’s crucial to learn how to differentiate between good and bad unit tests. I cover this\ntopic in chapter 4. \n1.3\nUsing coverage metrics to measure test suite quality\nIn this section, I talk about the two most popular coverage metrics—code coverage\nand branch coverage—how to calculate them, how they’re used, and problems with\nthem. I’ll show why it’s detrimental for programmers to aim at a particular coverage\nnumber and why you can’t just rely on coverage metrics to determine the quality of\nyour test suite.\nDEFINITION\nA coverage metric shows how much source code a test suite exe-\ncutes, from none to 100%.\nProduction code vs. test code \nPeople often think production code and test code are different. Tests are assumed\nto be an addition to production code and have no cost of ownership. By extension,\npeople often believe that the more tests, the better. This isn’t the case. Code is a\nliability, not an asset. The more code you introduce, the more you extend the surface\narea for potential bugs in your software, and the higher the project’s upkeep cost. It’s\nalways better to solve problems with as little code as possible.\nTests are code, too. You should view them as the part of your code base that aims at\nsolving a particular problem: ensuring the application’s correctness. Unit tests, just\nlike any other code, are also vulnerable to bugs and require maintenance.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2597,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 31,
      "content": "9\nUsing coverage metrics to measure test suite quality\nThere are different types of coverage metrics, and they’re often used to assess the\nquality of a test suite. The common belief is that the higher the coverage number,\nthe better.\n Unfortunately, it’s not that simple, and coverage metrics, while providing valuable\nfeedback, can’t be used to effectively measure the quality of a test suite. It’s the same\nsituation as with the ability to unit test the code: coverage metrics are a good negative\nindicator but a bad positive one.\n If a metric shows that there’s too little coverage in your code base—say, only 10%—\nthat’s a good indication that you are not testing enough. But the reverse isn’t true:\neven 100% coverage isn’t a guarantee that you have a good-quality test suite. A test\nsuite that provides high coverage can still be of poor quality.\n I already touched on why this is so—you can’t just throw random tests at your\nproject with the hope those tests will improve the situation. But let’s discuss this\nproblem in detail with respect to the code coverage metric.\n1.3.1\nUnderstanding the code coverage metric\nThe first and most-used coverage metric is code coverage, also known as test coverage; see\nfigure 1.3. This metric shows the ratio of the number of code lines executed by at least\none test and the total number of lines in the production code base.\nLet’s see an example to better understand how this works. Listing 1.1 shows an\nIsStringLong method and a test that covers it. The method determines whether a\nstring provided to it as an input parameter is long (here, the definition of long is any\nstring with the length greater than five characters). The test exercises the method\nusing \"abc\" and checks that this string is not considered long.\npublic static bool IsStringLong(string input)\n{\n           \nif (input.Length > 5)          \nreturn true;\n    \nreturn false;\n           \n}\n          \nListing 1.1\nA sample method partially covered by a test\nCode coverage (test coverage) =\nTotal number of lines\nLines of code executed\nFigure 1.3\nThe code coverage (test coverage) metric is \ncalculated as the ratio between the number of code lines \nexecuted by the test suite and the total number of lines in \nthe production code base.\nCovered \nby the \ntest\nNot\ncovered\nby the\ntest\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2345,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 32,
      "content": "10\nCHAPTER 1\nThe goal of unit testing\npublic void Test()\n{\nbool result = IsStringLong(\"abc\");\nAssert.Equal(false, result);\n}\nIt’s easy to calculate the code coverage here. The total number of lines in the method\nis five (curly braces count, too). The number of lines executed by the test is four—the\ntest goes through all the code lines except for the return true; statement. This gives\nus 4/5 = 0.8 = 80% code coverage.\n Now, what if I refactor the method and inline the unnecessary if statement, like this?\npublic static bool IsStringLong(string input)\n{\nreturn input.Length > 5;\n}\npublic void Test()\n{\nbool result = IsStringLong(\"abc\");\nAssert.Equal(false, result);\n}\nDoes the code coverage number change? Yes, it does. Because the test now exercises\nall three lines of code (the return statement plus two curly braces), the code coverage\nincreases to 100%.\n But did I improve the test suite with this refactoring? Of course not. I just shuffled the\ncode inside the method. The test still verifies the same number of possible outcomes.\n This simple example shows how easy it is to game the coverage numbers. The more\ncompact your code is, the better the test coverage metric becomes, because it only\naccounts for the raw line numbers. At the same time, squashing more code into less\nspace doesn’t (and shouldn’t) change the value of the test suite or the maintainability\nof the underlying code base. \n1.3.2\nUnderstanding the branch coverage metric\nAnother coverage metric is called branch coverage. Branch coverage provides more pre-\ncise results than code coverage because it helps cope with code coverage’s shortcom-\nings. Instead of using the raw number of code lines, this metric focuses on control\nstructures, such as if and switch statements. It shows how many of such control struc-\ntures are traversed by at least one test in the suite, as shown in figure 1.4.\nBranch coverage = Total number of branches\nBranches traversed\nFigure 1.4\nThe branch metric is calculated as the ratio of the \nnumber of code branches exercised by the test suite and the \ntotal number of branches in the production code base.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2164,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 33,
      "content": "11\nUsing coverage metrics to measure test suite quality\nTo calculate the branch coverage metric, you need to sum up all possible branches in\nyour code base and see how many of them are visited by tests. Let’s take our previous\nexample again:\npublic static bool IsStringLong(string input)\n{\nreturn input.Length > 5;\n}\npublic void Test()\n{\nbool result = IsStringLong(\"abc\");\nAssert.Equal(false, result);\n}\nThere are two branches in the IsStringLong method: one for the situation when the\nlength of the string argument is greater than five characters, and the other one when\nit’s not. The test covers only one of these branches, so the branch coverage metric is\n1/2 = 0.5 = 50%. And it doesn’t matter how we represent the code under test—\nwhether we use an if statement as before or use the shorter notation. The branch cov-\nerage metric only accounts for the number of branches; it doesn’t take into consider-\nation how many lines of code it took to implement those branches.\n Figure 1.5 shows a helpful way to visualize this metric. You can represent all pos-\nsible paths the code under test can take as a graph and see how many of them have\nbeen traversed. IsStringLong has two such paths, and the test exercises only one\nof them.\nStart\nLength <= 5\nEnd\nLength > 5\nFigure 1.5\nThe method IsStringLong represented as a graph of possible \ncode paths. Test covers only one of the two code paths, thus providing 50% \nbranch coverage.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1479,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 34,
      "content": "12\nCHAPTER 1\nThe goal of unit testing\n1.3.3\nProblems with coverage metrics\nAlthough the branch coverage metric yields better results than code coverage, you still\ncan’t rely on either of them to determine the quality of your test suite, for two reasons:\nYou can’t guarantee that the test verifies all the possible outcomes of the system\nunder test.\nNo coverage metric can take into account code paths in external libraries.\nLet’s look more closely at each of these reasons.\nYOU CAN’T GUARANTEE THAT THE TEST VERIFIES ALL THE POSSIBLE OUTCOMES\nFor the code paths to be actually tested and not just exercised, your unit tests must\nhave appropriate assertions. In other words, you need to check that the outcome the\nsystem under test produces is the exact outcome you expect it to produce. Moreover,\nthis outcome may have several components; and for the coverage metrics to be mean-\ningful, you need to verify all of them.\n The next listing shows another version of the IsStringLong method. It records the\nlast result into a public WasLastStringLong property.\npublic static bool WasLastStringLong { get; private set; }\npublic static bool IsStringLong(string input)\n{\nbool result = input.Length > 5;\nWasLastStringLong = result;         \nreturn result;\n     \n}\npublic void Test()\n{\nbool result = IsStringLong(\"abc\");\nAssert.Equal(false, result);   \n}\nThe IsStringLong method now has two outcomes: an explicit one, which is encoded\nby the return value; and an implicit one, which is the new value of the property. And\nin spite of not verifying the second, implicit outcome, the coverage metrics would still\nshow the same results: 100% for the code coverage and 50% for the branch coverage.\nAs you can see, the coverage metrics don’t guarantee that the underlying code is\ntested, only that it has been executed at some point.\n An extreme version of this situation with partially tested outcomes is assertion-free\ntesting, which is when you write tests that don’t have any assertion statements in them\nwhatsoever. Here’s an example of assertion-free testing.\n \n \nListing 1.2\nVersion of IsStringLong that records the last result\nFirst \noutcome\nSecond \noutcome\nThe test verifies only \nthe second outcome.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2248,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 35,
      "content": "13\nUsing coverage metrics to measure test suite quality\npublic void Test()\n{\nbool result1 = IsStringLong(\"abc\");   \nbool result2 = IsStringLong(\"abcdef\");   \n}\nThis test has both code and branch coverage metrics showing 100%. But at the same\ntime, it is completely useless because it doesn’t verify anything.\nBut let’s say that you thoroughly verify each outcome of the code under test. Does this,\nin combination with the branch coverage metric, provide a reliable mechanism, which\nyou can use to determine the quality of your test suite? Unfortunately, no. \nListing 1.3\nA test with no assertions always passes.\nA story from the trenches\nThe concept of assertion-free testing might look like a dumb idea, but it does happen\nin the wild.\nYears ago, I worked on a project where management imposed a strict requirement of\nhaving 100% code coverage for every project under development. This initiative had\nnoble intentions. It was during the time when unit testing wasn’t as prevalent as it is\ntoday. Few people in the organization practiced it, and even fewer did unit testing\nconsistently.\nA group of developers had gone to a conference where many talks were devoted to\nunit testing. After returning, they decided to put their new knowledge into practice.\nUpper management supported them, and the great conversion to better programming\ntechniques began. Internal presentations were given. New tools were installed. And,\nmore importantly, a new company-wide rule was imposed: all development teams had\nto focus on writing tests exclusively until they reached the 100% code coverage mark.\nAfter they reached this goal, any code check-in that lowered the metric had to be\nrejected by the build systems.\nAs you might guess, this didn’t play out well. Crushed by this severe limitation, devel-\nopers started to seek ways to game the system. Naturally, many of them came to the\nsame realization: if you wrap all tests with try/catch blocks and don’t introduce any\nassertions in them, those tests are guaranteed to pass. People started to mindlessly\ncreate tests for the sake of meeting the mandatory 100% coverage requirement.\nNeedless to say, those tests didn’t add any value to the projects. Moreover, they\ndamaged the projects because of all the effort and time they steered away from pro-\nductive activities, and because of the upkeep costs required to maintain the tests\nmoving forward.\nEventually, the requirement was lowered to 90% and then to 80%; after some period\nof time, it was retracted altogether (for the better!).\nReturns true\nReturns false\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2600,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 36,
      "content": "14\nCHAPTER 1\nThe goal of unit testing\nNO COVERAGE METRIC CAN TAKE INTO ACCOUNT CODE PATHS IN EXTERNAL LIBRARIES\nThe second problem with all coverage metrics is that they don’t take into account\ncode paths that external libraries go through when the system under test calls meth-\nods on them. Let’s take the following example:\npublic static int Parse(string input)\n{\nreturn int.Parse(input);\n}\npublic void Test()\n{\nint result = Parse(\"5\");\nAssert.Equal(5, result);\n}\nThe branch coverage metric shows 100%, and the test verifies all components of the\nmethod’s outcome. It has a single such component anyway—the return value. At the\nsame time, this test is nowhere near being exhaustive. It doesn’t take into account\nthe code paths the .NET Framework’s int.Parse method may go through. And\nthere are quite a number of code paths, even in this simple method, as you can see\nin figure 1.6.\nThe built-in integer type has plenty of branches that are hidden from the test and\nthat might lead to different results, should you change the method’s input parameter.\nHere are just a few possible arguments that can’t be transformed into an integer:\nNull value\nAn empty string\n“Not an int”\nA string that’s too large\nHidden\npart\nStart\nint.Parse\nnull\n“ ”\n“5”\n“not an int”\nEnd\nFigure 1.6\nHidden code paths of external libraries. Coverage metrics have no way to see how \nmany of them there are and how many of them your tests exercise.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1473,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 37,
      "content": "15\nWhat makes a successful test suite?\nYou can fall into numerous edge cases, and there’s no way to see if your tests account\nfor all of them.\n This is not to say that coverage metrics should take into account code paths in\nexternal libraries (they shouldn’t), but rather to show you that you can’t rely on\nthose metrics to see how good or bad your unit tests are. Coverage metrics can’t\npossibly tell whether your tests are exhaustive; nor can they say if you have enough\ntests. \n1.3.4\nAiming at a particular coverage number\nAt this point, I hope you can see that relying on coverage metrics to determine the\nquality of your test suite is not enough. It can also lead to dangerous territory if you\nstart making a specific coverage number a target, be it 100%, 90%, or even a moder-\nate 70%. The best way to view a coverage metric is as an indicator, not a goal in and\nof itself.\n Think of a patient in a hospital. Their high temperature might indicate a fever and\nis a helpful observation. But the hospital shouldn’t make the proper temperature of\nthis patient a goal to target by any means necessary. Otherwise, the hospital might end\nup with the quick and “efficient” solution of installing an air conditioner next to the\npatient and regulating their temperature by adjusting the amount of cold air flowing\nonto their skin. Of course, this approach doesn’t make any sense.\n Likewise, targeting a specific coverage number creates a perverse incentive that\ngoes against the goal of unit testing. Instead of focusing on testing the things that\nmatter, people start to seek ways to attain this artificial target. Proper unit testing is dif-\nficult enough already. Imposing a mandatory coverage number only distracts develop-\ners from being mindful about what they test, and makes proper unit testing even\nharder to achieve.\nTIP\nIt’s good to have a high level of coverage in core parts of your system.\nIt’s bad to make this high level a requirement. The difference is subtle but\ncritical.\nLet me repeat myself: coverage metrics are a good negative indicator, but a bad posi-\ntive one. Low coverage numbers—say, below 60%—are a certain sign of trouble. They\nmean there’s a lot of untested code in your code base. But high numbers don’t mean\nanything. Thus, measuring the code coverage should be only a first step on the way to\na quality test suite. \n1.4\nWhat makes a successful test suite?\nI’ve spent most of this chapter discussing improper ways to measure the quality of a\ntest suite: using coverage metrics. What about a proper way? How should you mea-\nsure your test suite’s quality? The only reliable way is to evaluate each test in the\nsuite individually, one by one. Of course, you don’t have to evaluate all of them at\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2775,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 38,
      "content": "16\nCHAPTER 1\nThe goal of unit testing\nonce; that could be quite a large undertaking and require significant upfront effort.\nYou can perform this evaluation gradually. The point is that there’s no automated\nway to see how good your test suite is. You have to apply your personal judgment.\n Let’s look at a broader picture of what makes a test suite successful as a whole.\n(We’ll dive into the specifics of differentiating between good and bad tests in chapter 4.)\nA successful test suite has the following properties:\nIt’s integrated into the development cycle.\nIt targets only the most important parts of your code base.\nIt provides maximum value with minimum maintenance costs.\n1.4.1\nIt’s integrated into the development cycle\nThe only point in having automated tests is if you constantly use them. All tests should\nbe integrated into the development cycle. Ideally, you should execute them on every\ncode change, even the smallest one. \n1.4.2\nIt targets only the most important parts of your code base\nJust as all tests are not created equal, not all parts of your code base are worth the\nsame attention in terms of unit testing. The value the tests provide is not only in how\nthose tests themselves are structured, but also in the code they verify.\n It’s important to direct your unit testing efforts to the most critical parts of the sys-\ntem and verify the others only briefly or indirectly. In most applications, the most\nimportant part is the part that contains business logic—the domain model.1 Testing\nbusiness logic gives you the best return on your time investment.\n All other parts can be divided into three categories:\nInfrastructure code\nExternal services and dependencies, such as the database and third-party systems\nCode that glues everything together\nSome of these other parts may still need thorough unit testing, though. For example,\nthe infrastructure code may contain complex and important algorithms, so it would\nmake sense to cover them with a lot of tests, too. But in general, most of your attention\nshould be spent on the domain model.\n Some of your tests, such as integration tests, can go beyond the domain model and\nverify how the system works as a whole, including the noncritical parts of the code\nbase. And that’s fine. But the focus should remain on the domain model.\n Note that in order to follow this guideline, you should isolate the domain model\nfrom the non-essential parts of the code base. You have to keep the domain model\nseparated from all other application concerns so you can focus your unit testing\n1 See Domain-Driven Design: Tackling Complexity in the Heart of Software by Eric Evans (Addison-Wesley, 2003).\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2713,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 39,
      "content": "17\nWhat you will learn in this book\nefforts on that domain model exclusively. We talk about all this in detail in part 2 of\nthe book. \n1.4.3\nIt provides maximum value with minimum maintenance costs\nThe most difficult part of unit testing is achieving maximum value with minimum\nmaintenance costs. That’s the main focus of this book.\n It’s not enough to incorporate tests into a build system, and it’s not enough to\nmaintain high test coverage of the domain model. It’s also crucial to keep in the suite\nonly the tests whose value exceeds their upkeep costs by a good margin.\n This last attribute can be divided in two:\nRecognizing a valuable test (and, by extension, a test of low value)\nWriting a valuable test\nAlthough these skills may seem similar, they’re different by nature. To recognize a test\nof high value, you need a frame of reference. On the other hand, writing a valuable\ntest requires you to also know code design techniques. Unit tests and the underlying\ncode are highly intertwined, and it’s impossible to create valuable tests without put-\nting significant effort into the code base they cover.\n You can view it as the difference between recognizing a good song and being able\nto compose one. The amount of effort required to become a composer is asymmetri-\ncally larger than the effort required to differentiate between good and bad music. The\nsame is true for unit tests. Writing a new test requires more effort than examining an\nexisting one, mostly because you don’t write tests in a vacuum: you have to take into\naccount the underlying code. And so although I focus on unit tests, I also devote a sig-\nnificant portion of this book to discussing code design. \n1.5\nWhat you will learn in this book\nThis book teaches a frame of reference that you can use to analyze any test in your test\nsuite. This frame of reference is foundational. After learning it, you’ll be able to look\nat many of your tests in a new light and see which of them contribute to the project\nand which must be refactored or gotten rid of altogether.\n After setting this stage (chapter 4), the book analyzes the existing unit testing tech-\nniques and practices (chapters 4–6, and part of 7). It doesn’t matter whether you’re\nfamiliar with those techniques and practices. If you are familiar with them, you’ll see\nthem from a new angle. Most likely, you already get them at the intuitive level. This\nbook can help you articulate why the techniques and best practices you’ve been using\nall along are so helpful.\n Don’t underestimate this skill. The ability to clearly communicate your ideas to col-\nleagues is priceless. A software developer—even a great one—rarely gets full credit for\na design decision if they can’t explain why, exactly, that decision was made. This book\ncan help you transform your knowledge from the realm of the unconscious to some-\nthing you are able to talk about with anyone.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2943,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 40,
      "content": "18\nCHAPTER 1\nThe goal of unit testing\n If you don’t have much experience with unit testing techniques and best practices,\nyou’ll learn a lot. In addition to the frame of reference that you can use to analyze any\ntest in a test suite, the book teaches\nHow to refactor the test suite along with the production code it covers\nHow to apply different styles of unit testing\nUsing integration tests to verify the behavior of the system as a whole\nIdentifying and avoiding anti-patterns in unit tests\nIn addition to unit tests, this book covers the entire topic of automated testing, so\nyou’ll also learn about integration and end-to-end tests.\n I use C# and .NET in my code samples, but you don’t have to be a C# professional\nto read this book; C# is just the language that I happen to work with the most. All\nthe concepts I talk about are non-language-specific and can be applied to any other\nobject-oriented language, such as Java or C++.\nSummary\nCode tends to deteriorate. Each time you change something in a code base, the\namount of disorder in it, or entropy, increases. Without proper care, such as\nconstant cleaning and refactoring, the system becomes increasingly complex\nand disorganized. Tests help overturn this tendency. They act as a safety net— a\ntool that provides insurance against the vast majority of regressions.\nIt’s important to write unit tests. It’s equally important to write good unit tests.\nThe end result for projects with bad tests or no tests is the same: either stagna-\ntion or a lot of regressions with every new release.\nThe goal of unit testing is to enable sustainable growth of the software project.\nA good unit test suite helps avoid the stagnation phase and maintain the devel-\nopment pace over time. With such a suite, you’re confident that your changes\nwon’t lead to regressions. This, in turn, makes it easier to refactor the code or\nadd new features.\nAll tests are not created equal. Each test has a cost and a benefit component,\nand you need to carefully weigh one against the other. Keep only tests of posi-\ntive net value in the suite, and get rid of all others. Both the application code\nand the test code are liabilities, not assets.\nThe ability to unit test code is a good litmus test, but it only works in one direc-\ntion. It’s a good negative indicator (if you can’t unit test the code, it’s of poor\nquality) but a bad positive one (the ability to unit test the code doesn’t guaran-\ntee its quality).\nLikewise, coverage metrics are a good negative indicator but a bad positive one.\nLow coverage numbers are a certain sign of trouble, but a high coverage num-\nber doesn’t automatically mean your test suite is of high quality.\nBranch coverage provides better insight into the completeness of the test suite\nbut still can’t indicate whether the suite is good enough. It doesn’t take into\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2891,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 41,
      "content": "19\nSummary\naccount the presence of assertions, and it can’t account for code paths in third-\nparty libraries that your code base uses.\nImposing a particular coverage number creates a perverse incentive. It’s good\nto have a high level of coverage in core parts of your system, but it’s bad to make\nthis high level a requirement.\nA successful test suite exhibits the following attributes:\n– It is integrated into the development cycle.\n– It targets only the most important parts of your code base.\n– It provides maximum value with minimum maintenance costs.\nThe only way to achieve the goal of unit testing (that is, enabling sustainable\nproject growth) is to\n– Learn how to differentiate between a good and a bad test.\n– Be able to refactor a test to make it more valuable.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 827,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 42,
      "content": "20\nWhat is a unit test?\nAs mentioned in chapter 1, there are a surprising number of nuances in the defini-\ntion of a unit test. Those nuances are more important than you might think—so\nmuch so that the differences in interpreting them have led to two distinct views on\nhow to approach unit testing.\n These views are known as the classical and the London schools of unit testing.\nThe classical school is called “classical” because it’s how everyone originally\napproached unit testing and test-driven development. The London school takes\nroot in the programming community in London. The discussion in this chapter\nabout the differences between the classical and London styles lays the foundation\nfor chapter 5, where I cover the topic of mocks and test fragility in detail.\nThis chapter covers\nWhat a unit test is\nThe differences between shared, private, \nand volatile dependencies\nThe two schools of unit testing: classical \nand London\nThe differences between unit, integration, \nand end-to-end tests\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1055,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 43,
      "content": "21\nThe definition of “unit test”\n Let’s start by defining a unit test, with all due caveats and subtleties. This definition\nis the key to the difference between the classical and London schools.\n2.1\nThe definition of “unit test”\nThere are a lot of definitions of a unit test. Stripped of their non-essential bits, the\ndefinitions all have the following three most important attributes. A unit test is an\nautomated test that\nVerifies a small piece of code (also known as a unit),\nDoes it quickly,\nAnd does it in an isolated manner.\nThe first two attributes here are pretty non-controversial. There might be some dis-\npute as to what exactly constitutes a fast unit test because it’s a highly subjective mea-\nsure. But overall, it’s not that important. If your test suite’s execution time is good\nenough for you, it means your tests are quick enough.\n What people have vastly different opinions about is the third attribute. The isola-\ntion issue is the root of the differences between the classical and London schools of\nunit testing. As you will see in the next section, all other differences between the two\nschools flow naturally from this single disagreement on what exactly isolation means. I\nprefer the classical style for the reasons I describe in section 2.3.\n2.1.1\nThe isolation issue: The London take\nWhat does it mean to verify a piece of code—a unit—in an isolated manner? The Lon-\ndon school describes it as isolating the system under test from its collaborators. It\nmeans if a class has a dependency on another class, or several classes, you need to\nreplace all such dependencies with test doubles. This way, you can focus on the class\nunder test exclusively by separating its behavior from any external influence.\n \nThe classical and London schools of unit testing\nThe classical approach is also referred to as the Detroit and, sometimes, the classi-\ncist approach to unit testing. Probably the most canonical book on the classical\nschool is the one by Kent Beck: Test-Driven Development: By Example (Addison-Wesley\nProfessional, 2002).\nThe London style is sometimes referred to as mockist. Although the term mockist is\nwidespread, people who adhere to this style of unit testing generally don’t like it, so\nI call it the London style throughout this book. The most prominent proponents of this\napproach are Steve Freeman and Nat Pryce. I recommend their book, Growing Object-\nOriented Software, Guided by Tests (Addison-Wesley Professional, 2009), as a good\nsource on this subject.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2551,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 44,
      "content": "22\nCHAPTER 2\nWhat is a unit test?\nDEFINITION\nA test double is an object that looks and behaves like its release-\nintended counterpart but is actually a simplified version that reduces the\ncomplexity and facilitates testing. This term was introduced by Gerard Mesza-\nros in his book, xUnit Test Patterns: Refactoring Test Code (Addison-Wesley, 2007).\nThe name itself comes from the notion of a stunt double in movies.\nFigure 2.1 shows how the isolation is usually achieved. A unit test that would otherwise\nverify the system under test along with all its dependencies now can do that separately\nfrom those dependencies.\nOne benefit of this approach is that if the test fails, you know for sure which part of\nthe code base is broken: it’s the system under test. There could be no other suspects,\nbecause all of the class’s neighbors are replaced with the test doubles.\n Another benefit is the ability to split the object graph—the web of communicating\nclasses solving the same problem. This web may become quite complicated: every class\nin it may have several immediate dependencies, each of which relies on dependencies\nof their own, and so on. Classes may even introduce circular dependencies, where the\nchain of dependency eventually comes back to where it started.\nTest double 2\nDependency 1\nDependency 2\nSystem under test\nSystem under test\nTest double 1\nFigure 2.1\nReplacing the dependencies \nof the system under test with test \ndoubles allows you to focus on verifying \nthe system under test exclusively, as \nwell as split the otherwise large \ninterconnected object graph.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1628,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 45,
      "content": "23\nThe definition of “unit test”\n Trying to test such an interconnected code base is hard without test doubles. Pretty\nmuch the only choice you are left with is re-creating the full object graph in the test,\nwhich might not be a feasible task if the number of classes in it is too high.\n With test doubles, you can put a stop to this. You can substitute the immediate\ndependencies of a class; and, by extension, you don’t have to deal with the dependen-\ncies of those dependencies, and so on down the recursion path. You are effectively\nbreaking up the graph—and that can significantly reduce the amount of preparations\nyou have to do in a unit test.\n And let’s not forget another small but pleasant side benefit of this approach to\nunit test isolation: it allows you to introduce a project-wide guideline of testing only\none class at a time, which establishes a simple structure in the whole unit test suite.\nYou no longer have to think much about how to cover your code base with tests.\nHave a class? Create a corresponding class with unit tests! Figure 2.2 shows how it\nusually looks.\nLet’s now look at some examples. Since the classical style probably looks more familiar\nto most people, I’ll show sample tests written in that style first and then rewrite them\nusing the London approach.\n Let’s say that we operate an online store. There’s just one simple use case in our\nsample application: a customer can purchase a product. When there’s enough inven-\ntory in the store, the purchase is deemed to be successful, and the amount of the\nproduct in the store is reduced by the purchase’s amount. If there’s not enough prod-\nuct, the purchase is not successful, and nothing happens in the store.\n Listing 2.1 shows two tests verifying that a purchase succeeds only when there’s\nenough inventory in the store. The tests are written in the classical style and use the\nClass 1\nClass 2\nClass 3\nUnit tests\nProduction code\nClass 1 Tests\nClass 2 Tests\nClass 3 Tests\nFigure 2.2\nIsolating the class under test from its dependencies helps establish a simple \ntest suite structure: one class with tests for each class in the production code.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2183,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 46,
      "content": "24\nCHAPTER 2\nWhat is a unit test?\ntypical three-phase sequence: arrange, act, and assert (AAA for short—I talk more\nabout this sequence in chapter 3).\n[Fact]\npublic void Purchase_succeeds_when_enough_inventory()\n{\n// Arrange\nvar store = new Store();\nstore.AddInventory(Product.Shampoo, 10);\nvar customer = new Customer();\n// Act\nbool success = customer.Purchase(store, Product.Shampoo, 5);\n// Assert\nAssert.True(success);\nAssert.Equal(5, store.GetInventory(Product.Shampoo));   \n}\n[Fact]\npublic void Purchase_fails_when_not_enough_inventory()\n{\n// Arrange\nvar store = new Store();\nstore.AddInventory(Product.Shampoo, 10);\nvar customer = new Customer();\n// Act\nbool success = customer.Purchase(store, Product.Shampoo, 15);\n// Assert\nAssert.False(success);\nAssert.Equal(10, store.GetInventory(Product.Shampoo));   \n}\npublic enum Product\n{\nShampoo,\nBook\n}\nAs you can see, the arrange part is where the tests make ready all dependencies and\nthe system under test. The call to customer.Purchase() is the act phase, where you\nexercise the behavior you want to verify. The assert statements are the verification\nstage, where you check to see if the behavior led to the expected results.\n During the arrange phase, the tests put together two kinds of objects: the system\nunder test (SUT) and one collaborator. In this case, Customer is the SUT and Store is\nthe collaborator. We need the collaborator for two reasons:\nListing 2.1\nTests written using the classical style of unit testing\nReduces the \nproduct amount in \nthe store by five\nThe product \namount in the \nstore remains \nunchanged.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1632,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 47,
      "content": "25\nThe definition of “unit test”\nTo get the method under test to compile, because customer.Purchase() requires\na Store instance as an argument\nFor the assertion phase, since one of the results of customer.Purchase() is a\npotential decrease in the product amount in the store \nProduct.Shampoo and the numbers 5 and 15 are constants.\nDEFINITION\nA method under test (MUT) is a method in the SUT called by the\ntest. The terms MUT and SUT are often used as synonyms, but normally, MUT\nrefers to a method while SUT refers to the whole class.\nThis code is an example of the classical style of unit testing: the test doesn’t replace\nthe collaborator (the Store class) but rather uses a production-ready instance of it.\nOne of the natural outcomes of this style is that the test now effectively verifies both\nCustomer and Store, not just Customer. Any bug in the inner workings of Store that\naffects Customer will lead to failing these unit tests, even if Customer still works cor-\nrectly. The two classes are not isolated from each other in the tests.\n Let’s now modify the example toward the London style. I’ll take the same tests and\nreplace the Store instances with test doubles—specifically, mocks.\n I use Moq (https://github.com/moq/moq4) as the mocking framework, but you\ncan find several equally good alternatives, such as NSubstitute (https://github.com/\nnsubstitute/NSubstitute). All object-oriented languages have analogous frameworks.\nFor instance, in the Java world, you can use Mockito, JMock, or EasyMock.\nDEFINITION\nA mock is a special kind of test double that allows you to examine\ninteractions between the system under test and its collaborators.\nWe’ll get back to the topic of mocks, stubs, and the differences between them in later\nchapters. For now, the main thing to remember is that mocks are a subset of test dou-\nbles. People often use the terms test double and mock as synonyms, but technically, they\nare not (more on this in chapter 5):\nTest double is an overarching term that describes all kinds of non-production-\nready, fake dependencies in a test.\nMock is just one kind of such dependencies.\nThe next listing shows how the tests look after isolating Customer from its collabora-\ntor, Store.\n[Fact]\npublic void Purchase_succeeds_when_enough_inventory()\n{\n// Arrange\nvar storeMock = new Mock<IStore>();\nstoreMock\nListing 2.2\nTests written using the London style of unit testing\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2453,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 48,
      "content": "26\nCHAPTER 2\nWhat is a unit test?\n.Setup(x => x.HasEnoughInventory(Product.Shampoo, 5))\n.Returns(true);\nvar customer = new Customer();\n// Act\nbool success = customer.Purchase(\nstoreMock.Object, Product.Shampoo, 5);\n// Assert\nAssert.True(success);\nstoreMock.Verify(\nx => x.RemoveInventory(Product.Shampoo, 5),\nTimes.Once);\n}\n[Fact]\npublic void Purchase_fails_when_not_enough_inventory()\n{\n// Arrange\nvar storeMock = new Mock<IStore>();\nstoreMock\n.Setup(x => x.HasEnoughInventory(Product.Shampoo, 5))\n.Returns(false);\nvar customer = new Customer();\n// Act\nbool success = customer.Purchase(\nstoreMock.Object, Product.Shampoo, 5);\n// Assert\nAssert.False(success);\nstoreMock.Verify(\nx => x.RemoveInventory(Product.Shampoo, 5),\nTimes.Never);\n}\nNote how different these tests are from those written in the classical style. In the\narrange phase, the tests no longer instantiate a production-ready instance of Store\nbut instead create a substitution for it, using Moq’s built-in class Mock<T>.\n Furthermore, instead of modifying the state of Store by adding a shampoo inven-\ntory to it, we directly tell the mock how to respond to calls to HasEnoughInventory().\nThe mock reacts to this request the way the tests need, regardless of the actual state of\nStore. In fact, the tests no longer use Store—we have introduced an IStore interface\nand are mocking that interface instead of the Store class.\n In chapter 8, I write in detail about working with interfaces. For now, just make a\nnote that interfaces are required for isolating the system under test from its collabora-\ntors. (You can also mock a concrete class, but that’s an anti-pattern; I cover this topic\nin chapter 11.)\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1719,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 49,
      "content": "27\nThe definition of “unit test”\n The assertion phase has changed too, and that’s where the key difference lies. We\nstill check the output from customer.Purchase as before, but the way we verify that\nthe customer did the right thing to the store is different. Previously, we did that by\nasserting against the store’s state. Now, we examine the interactions between Customer\nand Store: the tests check to see if the customer made the correct call on the store.\nWe do this by passing the method the customer should call on the store (x.Remove-\nInventory) as well as the number of times it should do that. If the purchases succeeds,\nthe customer should call this method once (Times.Once). If the purchases fails, the\ncustomer shouldn’t call it at all (Times.Never). \n2.1.2\nThe isolation issue: The classical take\nTo reiterate, the London style approaches the isolation requirement by segregating the\npiece of code under test from its collaborators with the help of test doubles: specifically,\nmocks. Interestingly enough, this point of view also affects your standpoint on what con-\nstitutes a small piece of code (a unit). Here are all the attributes of a unit test once again:\nA unit test verifies a small piece of code (a unit),\nDoes it quickly,\nAnd does it in an isolated manner.\nIn addition to the third attribute leaving room for interpretation, there’s some room\nin the possible interpretations of the first attribute as well. How small should a small\npiece of code be? As you saw from the previous section, if you adopt the position of\nisolating every individual class, then it’s natural to accept that the piece of code under\ntest should also be a single class, or a method inside that class. It can’t be more than\nthat due to the way you approach the isolation issue. In some cases, you might test a\ncouple of classes at once; but in general, you’ll always strive to maintain this guideline\nof unit testing one class at a time.\n As I mentioned earlier, there’s another way to interpret the isolation attribute—\nthe classical way. In the classical approach, it’s not the code that needs to be tested in\nan isolated manner. Instead, unit tests themselves should be run in isolation from\neach other. That way, you can run the tests in parallel, sequentially, and in any order,\nwhatever fits you best, and they still won’t affect each other’s outcome.\n Isolating tests from each other means it’s fine to exercise several classes at once as\nlong as they all reside in the memory and don’t reach out to a shared state, through\nwhich the tests can communicate and affect each other’s execution context. Typical\nexamples of such a shared state are out-of-process dependencies—the database, the\nfile system, and so on.\n For instance, one test could create a customer in the database as part of its arrange\nphase, and another test would delete it as part of its own arrange phase, before the\nfirst test completes executing. If you run these two tests in parallel, the first test will\nfail, not because the production code is broken, but rather because of the interfer-\nence from the second test.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 3147,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 50,
      "content": "28\nCHAPTER 2\nWhat is a unit test?\nThis take on the isolation issue entails a much more modest view on the use of mocks\nand other test doubles. You can still use them, but you normally do that for only those\ndependencies that introduce a shared state between tests. Figure 2.3 shows how it looks.\n Note that shared dependencies are shared between unit tests, not between classes\nunder test (units). In that sense, a singleton dependency is not shared as long as you\nare able to create a new instance of it in each test. While there’s only one instance of a\nShared, private, and out-of-process dependencies \nA shared dependency is a dependency that is shared between tests and provides\nmeans for those tests to affect each other’s outcome. A typical example of shared\ndependencies is a static mutable field. A change to such a field is visible across all\nunit tests running within the same process. A database is another typical example of\na shared dependency.\nA private dependency is a dependency that is not shared.\nAn out-of-process dependency is a dependency that runs outside the application’s\nexecution process; it’s a proxy to data that is not yet in the memory. An out-of-process\ndependency corresponds to a shared dependency in the vast majority of cases, but\nnot always. For example, a database is both out-of-process and shared. But if you\nlaunch that database in a Docker container before each test run, that would make\nthis dependency out-of-process but not shared, since tests no longer work with the\nsame instance of it. Similarly, a read-only database is also out-of-process but not\nshared, even if it’s reused by tests. Tests can’t mutate data in such a database and\nthus can’t affect each other’s outcome.\nPrivate dependency; keep\nShared dependency; replace\nFile system\nSystem under test\nDatabase\nTest\nShared dependency; replace\nAnother class\nFigure 2.3\nIsolating unit tests from each other entails isolating the class under test \nfrom shared dependencies only. Private dependencies can be kept intact.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2070,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 51,
      "content": "29\nThe definition of “unit test”\nsingleton in the production code, tests may very well not follow this pattern and not\nreuse that singleton. Thus, such a dependency would be private.\n For example, there’s normally only one instance of a configuration class, which is\nreused across all production code. But if it’s injected into the SUT the way all other\ndependencies are, say, via a constructor, you can create a new instance of it in each\ntest; you don’t have to maintain a single instance throughout the test suite. You can’t\ncreate a new file system or a database, however; they must be either shared between\ntests or substituted away with test doubles.\nAnother reason for substituting shared dependencies is to increase the test execution\nspeed. Shared dependencies almost always reside outside the execution process, while\nprivate dependencies usually don’t cross that boundary. Because of that, calls to\nshared dependencies, such as a database or the file system, take more time than calls\nto private dependencies. And since the necessity to run quickly is the second attribute\nof the unit test definition, such calls push the tests with shared dependencies out of\nthe realm of unit testing and into the area of integration testing. I talk more about\nintegration testing later in this chapter.\n This alternative view of isolation also leads to a different take on what constitutes a\nunit (a small piece of code). A unit doesn’t necessarily have to be limited to a class.\nShared vs. volatile dependencies \nAnother term has a similar, yet not identical, meaning: volatile dependency. I recom-\nmend Dependency Injection: Principles, Practices, Patterns by Steven van Deursen and\nMark Seemann (Manning Publications, 2018) as a go-to book on the topic of depen-\ndency management.\nA volatile dependency is a dependency that exhibits one of the following properties:\nIt introduces a requirement to set up and configure a runtime environment in\naddition to what is installed on a developer’s machine by default. Databases\nand API services are good examples here. They require additional setup and\nare not installed on machines in your organization by default.\nIt contains nondeterministic behavior. An example would be a random num-\nber generator or a class returning the current date and time. These depen-\ndencies are non-deterministic because they provide different results on each\ninvocation.\nAs you can see, there’s an overlap between the notions of shared and volatile depen-\ndencies. For example, a dependency on the database is both shared and volatile. But\nthat’s not the case for the file system. The file system is not volatile because it is\ninstalled on every developer’s machine and it behaves deterministically in the vast\nmajority of cases. Still, the file system introduces a means by which the unit tests\ncan interfere with each other’s execution context; hence it is shared. Likewise, a ran-\ndom number generator is volatile, but because you can supply a separate instance\nof it to each test, it isn’t shared.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 3079,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 52,
      "content": "30\nCHAPTER 2\nWhat is a unit test?\nYou can just as well unit test a group of classes, as long as none of them is a shared\ndependency. \n2.2\nThe classical and London schools of unit testing\nAs you can see, the root of the differences between the London and classical schools is\nthe isolation attribute. The London school views it as isolation of the system under test\nfrom its collaborators, whereas the classical school views it as isolation of unit tests\nthemselves from each other.\n This seemingly minor difference has led to a vast disagreement about how to\napproach unit testing, which, as you already know, produced the two schools of thought.\nOverall, the disagreement between the schools spans three major topics:\nThe isolation requirement\nWhat constitutes a piece of code under test (a unit)\nHandling dependencies\nTable 2.1 sums it all up.\n2.2.1\nHow the classical and London schools handle dependencies\nNote that despite the ubiquitous use of test doubles, the London school still allows\nfor using some dependencies in tests as-is. The litmus test here is whether a depen-\ndency is mutable. It’s fine not to substitute objects that don’t ever change—\nimmutable objects.\n And you saw in the earlier examples that, when I refactored the tests toward the\nLondon style, I didn’t replace the Product instances with mocks but rather used\nthe real objects, as shown in the following code (repeated from listing 2.2 for your\nconvenience):\n[Fact]\npublic void Purchase_fails_when_not_enough_inventory()\n{\n// Arrange\nvar storeMock = new Mock<IStore>();\nstoreMock\n.Setup(x => x.HasEnoughInventory(Product.Shampoo, 5))\n.Returns(false);\nvar customer = new Customer();\nTable 2.1\nThe differences between the London and classical schools of unit testing, summed up by the\napproach to isolation, the size of a unit, and the use of test doubles\nIsolation of\nA unit is\nUses test doubles for\nLondon school\nUnits\nA class\nAll but immutable dependencies\nClassical school\nUnit tests\nA class or a set of classes\nShared dependencies\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2066,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 53,
      "content": "31\nThe classical and London schools of unit testing\n// Act\nbool success = customer.Purchase(storeMock.Object, Product.Shampoo, 5);\n// Assert\nAssert.False(success);\nstoreMock.Verify(\nx => x.RemoveInventory(Product.Shampoo, 5),\nTimes.Never);\n}\nOf the two dependencies of Customer, only Store contains an internal state that can\nchange over time. The Product instances are immutable (Product itself is a C#\nenum). Hence I substituted the Store instance only.\n It makes sense, if you think about it. You wouldn’t use a test double for the 5\nnumber in the previous test either, would you? That’s because it is also immutable—\nyou can’t possibly modify this number. Note that I’m not talking about a variable\ncontaining the number, but rather the number itself. In the statement Remove-\nInventory(Product.Shampoo, 5), we don’t even use a variable; 5 is declared right\naway. The same is true for Product.Shampoo.\n Such immutable objects are called value objects or values. Their main trait is that\nthey have no individual identity; they are identified solely by their content. As a corol-\nlary, if two such objects have the same content, it doesn’t matter which of them you’re\nworking with: these instances are interchangeable. For example, if you’ve got two 5\nintegers, you can use them in place of one another. The same is true for the products\nin our case: you can reuse a single Product.Shampoo instance or declare several of\nthem—it won’t make any difference. These instances will have the same content and\nthus can be used interchangeably.\n Note that the concept of a value object is language-agnostic and doesn’t require a\nparticular programming language or framework. You can read more about value\nobjects in my article “Entity vs. Value Object: The ultimate list of differences” at\nhttp://mng.bz/KE9O.\n Figure 2.4 shows the categorization of dependencies and how both schools of unit\ntesting treat them. A dependency can be either shared or private. A private dependency, in\nturn, can be either mutable or immutable. In the latter case, it is called a value object. For\nexample, a database is a shared dependency—its internal state is shared across all\nautomated tests (that don’t replace it with a test double). A Store instance is a private\ndependency that is mutable. And a Product instance (or an instance of a number 5,\nfor that matter) is an example of a private dependency that is immutable—a value\nobject. All shared dependencies are mutable, but for a mutable dependency to be\nshared, it has to be reused by tests.\n \n \n \n \nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2585,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 54,
      "content": "32\nCHAPTER 2\nWhat is a unit test?\nI’m repeating table 2.1 with the differences between the schools for your convenience.\nIsolation of\nA unit is\nUses test doubles for\nLondon school\nUnits\nA class\nAll but immutable dependencies\nClassical school\nUnit tests\nA class or a set of classes\nShared dependencies\nCollaborator vs. dependency\nA collaborator is a dependency that is either shared or mutable. For example, a class\nproviding access to the database is a collaborator since the database is a shared\ndependency. Store is a collaborator too, because its state can change over time.\nProduct and number 5 are also dependencies, but they’re not collaborators. They’re\nvalues or value objects.\nA typical class may work with dependencies of both types: collaborators and values.\nLook at this method call:\ncustomer.Purchase(store, Product.Shampoo, 5)\nHere we have three dependencies. One of them (store) is a collaborator, and the\nother two (Product.Shampoo, 5) are not.\nPrivate\nValue object\nMutable\nCollaborator,\nreplaced in the\nLondon school\nReplaced in the\nclassic school\nShared\nDependency\nFigure 2.4\nThe hierarchy of dependencies. The classical school advocates for \nreplacing shared dependencies with test doubles. The London school advocates for the \nreplacement of private dependencies as well, as long as they are mutable.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1372,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 55,
      "content": "33\nThe classical and London schools of unit testing\nAnd let me reiterate one point about the types of dependencies. Not all out-of-process\ndependencies fall into the category of shared dependencies. A shared dependency\nalmost always resides outside the application’s process, but the opposite isn’t true (see\nfigure 2.5). In order for an out-of-process dependency to be shared, it has to provide\nmeans for unit tests to communicate with each other. The communication is done\nthrough modifications of the dependency’s internal state. In that sense, an immutable\nout-of-process dependency doesn’t provide such a means. The tests simply can’t mod-\nify anything in it and thus can’t interfere with each other’s execution context.\nFor example, if there’s an API somewhere that returns a catalog of all products the orga-\nnization sells, this isn’t a shared dependency as long as the API doesn’t expose the\nfunctionality to change the catalog. It’s true that such a dependency is volatile and sits\noutside the application’s boundary, but since the tests can’t affect the data it returns, it\nisn’t shared. This doesn’t mean you have to include such a dependency in the testing\nscope. In most cases, you still need to replace it with a test double to keep the test fast.\nBut if the out-of-process dependency is quick enough and the connection to it is stable,\nyou can make a good case for using it as-is in the tests.\n Having that said, in this book, I use the terms shared dependency and out-of-process\ndependency interchangeably unless I explicitly state otherwise. In real-world projects,\nyou rarely have a shared dependency that isn’t out-of-process. If a dependency is in-\nprocess, you can easily supply a separate instance of it to each test; there’s no need to\nshare it between tests. Similarly, you normally don’t encounter an out-of-process\nShared\ndependencies\nOut-of-process\ndependencies\nSingleton\nDatabase\nRead-only API service\nFigure 2.5\nThe relation between shared and out-of-process dependencies. An example of a \ndependency that is shared but not out-of-process is a singleton (an instance that is reused by \nall tests) or a static field in a class. A database is shared and out-of-process—it resides outside \nthe main process and is mutable. A read-only API is out-of-process but not shared, since tests \ncan’t modify it and thus can’t affect each other’s execution flow.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2431,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 56,
      "content": "34\nCHAPTER 2\nWhat is a unit test?\ndependency that’s not shared. Most such dependencies are mutable and thus can be\nmodified by tests.\n With this foundation of definitions, let’s contrast the two schools on their merits. \n2.3\nContrasting the classical and London schools \nof unit testing\nTo reiterate, the main difference between the classical and London schools is in how\nthey treat the isolation issue in the definition of a unit test. This, in turn, spills over to\nthe treatment of a unit—the thing that should be put under test—and the approach\nto handling dependencies.\n As I mentioned previously, I prefer the classical school of unit testing. It tends to\nproduce tests of higher quality and thus is better suited for achieving the ultimate goal\nof unit testing, which is the sustainable growth of your project. The reason is fragility:\ntests that use mocks tend to be more brittle than classical tests (more on this in chap-\nter 5). For now, let’s take the main selling points of the London school and evaluate\nthem one by one.\n The London school’s approach provides the following benefits:\nBetter granularity. The tests are fine-grained and check only one class at a time.\nIt’s easier to unit test a larger graph of interconnected classes. Since all collaborators\nare replaced by test doubles, you don’t need to worry about them at the time of\nwriting the test.\nIf a test fails, you know for sure which functionality has failed. Without the class’s\ncollaborators, there could be no suspects other than the class under test itself.\nOf course, there may still be situations where the system under test uses a\nvalue object and it’s the change in this value object that makes the test fail.\nBut these cases aren’t that frequent because all other dependencies are elimi-\nnated in tests.\n2.3.1\nUnit testing one class at a time\nThe point about better granularity relates to the discussion about what constitutes a\nunit in unit testing. The London school considers a class as such a unit. Coming from\nan object-oriented programming background, developers usually regard classes as the\natomic building blocks that lie at the foundation of every code base. This naturally\nleads to treating classes as the atomic units to be verified in tests, too. This tendency is\nunderstandable but misleading.\nTIP\nTests shouldn’t verify units of code. Rather, they should verify units of\nbehavior: something that is meaningful for the problem domain and, ideally,\nsomething that a business person can recognize as useful. The number of\nclasses it takes to implement such a unit of behavior is irrelevant. The unit\ncould span across multiple classes or only one class, or even take up just a\ntiny method.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2741,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 57,
      "content": "35\nContrasting the classical and London schools of unit testing\nAnd so, aiming at better code granularity isn’t helpful. As long as the test checks a sin-\ngle unit of behavior, it’s a good test. Targeting something less than that can in fact\ndamage your unit tests, as it becomes harder to understand exactly what these tests\nverify. A test should tell a story about the problem your code helps to solve, and this story should\nbe cohesive and meaningful to a non-programmer.\n For instance, this is an example of a cohesive story:\nWhen I call my dog, he comes right to me.\nNow compare it to the following:\nWhen I call my dog, he moves his front left leg first, then the front right \nleg, his head turns, the tail start wagging...\nThe second story makes much less sense. What’s the purpose of all those movements?\nIs the dog coming to me? Or is he running away? You can’t tell. This is what your tests\nstart to look like when you target individual classes (the dog’s legs, head, and tail)\ninstead of the actual behavior (the dog coming to his master). I talk more about this\ntopic of observable behavior and how to differentiate it from internal implementation\ndetails in chapter 5. \n2.3.2\nUnit testing a large graph of interconnected classes\nThe use of mocks in place of real collaborators can make it easier to test a class—\nespecially when there’s a complicated dependency graph, where the class under test\nhas dependencies, each of which relies on dependencies of its own, and so on, several\nlayers deep. With test doubles, you can substitute the class’s immediate dependencies\nand thus break up the graph, which can significantly reduce the amount of prepara-\ntion you have to do in a unit test. If you follow the classical school, you have to re-create\nthe full object graph (with the exception of shared dependencies) just for the sake of\nsetting up the system under test, which can be a lot of work.\n Although this is all true, this line of reasoning focuses on the wrong problem.\nInstead of finding ways to test a large, complicated graph of interconnected classes,\nyou should focus on not having such a graph of classes in the first place. More often\nthan not, a large class graph is a result of a code design problem.\n It’s actually a good thing that the tests point out this problem. As we discussed in\nchapter 1, the ability to unit test a piece of code is a good negative indicator—it pre-\ndicts poor code quality with a relatively high precision. If you see that to unit test a\nclass, you need to extend the test’s arrange phase beyond all reasonable limits, it’s a\ncertain sign of trouble. The use of mocks only hides this problem; it doesn’t tackle the\nroot cause. I talk about how to fix the underlying code design problem in part 2. \n \nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2804,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 58,
      "content": "36\nCHAPTER 2\nWhat is a unit test?\n2.3.3\nRevealing the precise bug location\nIf you introduce a bug to a system with London-style tests, it normally causes only tests\nwhose SUT contains the bug to fail. However, with the classical approach, tests that\ntarget the clients of the malfunctioning class can also fail. This leads to a ripple effect\nwhere a single bug can cause test failures across the whole system. As a result, it\nbecomes harder to find the root of the issue. You might need to spend some time\ndebugging the tests to figure it out.\n It’s a valid concern, but I don’t see it as a big problem. If you run your tests regu-\nlarly (ideally, after each source code change), then you know what caused the bug—\nit’s what you edited last, so it’s not that difficult to find the issue. Also, you don’t have\nto look at all the failing tests. Fixing one automatically fixes all the others.\n Furthermore, there’s some value in failures cascading all over the test suite. If a\nbug leads to a fault in not only one test but a whole lot of them, it shows that the piece\nof code you have just broken is of great value—the entire system depends on it. That’s\nuseful information to keep in mind when working with the code. \n2.3.4\nOther differences between the classical and London schools\nTwo remaining differences between the classical and London schools are\nTheir approach to system design with test-driven development (TDD)\nThe issue of over-specification\nThe London style of unit testing leads to outside-in TDD, where you start from the\nhigher-level tests that set expectations for the whole system. By using mocks, you spec-\nify which collaborators the system should communicate with to achieve the expected\nresult. You then work your way through the graph of classes until you implement every\none of them. Mocks make this design process possible because you can focus on one\nTest-driven development\nTest-driven development is a software development process that relies on tests to\ndrive the project development. The process consists of three (some authors specify\nfour) stages, which you repeat for every test case:\n1\nWrite a failing test to indicate which functionality needs to be added and how\nit should behave.\n2\nWrite just enough code to make the test pass. At this stage, the code doesn’t\nhave to be elegant or clean.\n3\nRefactor the code. Under the protection of the passing test, you can safely\nclean up the code to make it more readable and maintainable.\nGood sources on this topic are the two books I recommended earlier: Kent Beck’s\nTest-Driven Development: By Example, and Growing Object-Oriented Software, Guided\nby Tests by Steve Freeman and Nat Pryce.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2719,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 59,
      "content": "37\nIntegration tests in the two schools\nclass at a time. You can cut off all of the SUT’s collaborators when testing it and thus\npostpone implementing those collaborators to a later time.\n The classical school doesn’t provide quite the same guidance since you have to\ndeal with the real objects in tests. Instead, you normally use the inside-out approach.\nIn this style, you start from the domain model and then put additional layers on top of\nit until the software becomes usable by the end user.\n But the most crucial distinction between the schools is the issue of over-specification:\nthat is, coupling the tests to the SUT’s implementation details. The London style\ntends to produce tests that couple to the implementation more often than the classi-\ncal style. And this is the main objection against the ubiquitous use of mocks and the\nLondon style in general.\n There’s much more to the topic of mocking. Starting with chapter 4, I gradually\ncover everything related to it. \n2.4\nIntegration tests in the two schools\nThe London and classical schools also diverge in their definition of an integration\ntest. This disagreement flows naturally from the difference in their views on the isola-\ntion issue.\n The London school considers any test that uses a real collaborator object an inte-\ngration test. Most of the tests written in the classical style would be deemed integra-\ntion tests by the London school proponents. For an example, see listing 1.4, in which I\nfirst introduced the two tests covering the customer purchase functionality. That code\nis a typical unit test from the classical perspective, but it’s an integration test for a fol-\nlower of the London school.\n In this book, I use the classical definitions of both unit and integration testing.\nAgain, a unit test is an automated test that has the following characteristics:\nIt verifies a small piece of code,\nDoes it quickly,\nAnd does it in an isolated manner.\nNow that I’ve clarified what the first and third attributes mean, I’ll redefine them\nfrom the point of view of the classical school. A unit test is a test that\nVerifies a single unit of behavior,\nDoes it quickly,\nAnd does it in isolation from other tests.\nAn integration test, then, is a test that doesn’t meet one of these criteria. For example,\na test that reaches out to a shared dependency—say, a database—can’t run in isolation\nfrom other tests. A change in the database’s state introduced by one test would alter\nthe outcome of all other tests that rely on the same database if run in parallel. You’d\nhave to take additional steps to avoid this interference. In particular, you would have\nto run such tests sequentially, so that each test would wait its turn to work with the\nshared dependency.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2786,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 60,
      "content": "38\nCHAPTER 2\nWhat is a unit test?\n Similarly, an outreach to an out-of-process dependency makes the test slow. A call\nto a database adds hundreds of milliseconds, potentially up to a second, of additional\nexecution time. Milliseconds might not seem like a big deal at first, but when your test\nsuite grows large enough, every second counts.\n In theory, you could write a slow test that works with in-memory objects only, but\nit’s not that easy to do. Communication between objects inside the same memory\nspace is much less expensive than between separate processes. Even if the test works\nwith hundreds of in-memory objects, the communication with them will still execute\nfaster than a call to a database.\n Finally, a test is an integration test when it verifies two or more units of behavior.\nThis is often a result of trying to optimize the test suite’s execution speed. When you\nhave two slow tests that follow similar steps but verify different units of behavior, it\nmight make sense to merge them into one: one test checking two similar things runs\nfaster than two more-granular tests. But then again, the two original tests would have\nbeen integration tests already (due to them being slow), so this characteristic usually\nisn’t decisive.\n An integration test can also verify how two or more modules developed by separate\nteams work together. This also falls into the third bucket of tests that verify multiple\nunits of behavior at once. But again, because such an integration normally requires an\nout-of-process dependency, the test will fail to meet all three criteria, not just one.\n Integration testing plays a significant part in contributing to software quality by\nverifying the system as a whole. I write about integration testing in detail in part 3.\n2.4.1\nEnd-to-end tests are a subset of integration tests\nIn short, an integration test is a test that verifies that your code works in integration with\nshared dependencies, out-of-process dependencies, or code developed by other teams\nin the organization. There’s also a separate notion of an end-to-end test. End-to-end\ntests are a subset of integration tests. They, too, check to see how your code works with\nout-of-process dependencies. The difference between an end-to-end test and an inte-\ngration test is that end-to-end tests usually include more of such dependencies.\n The line is blurred at times, but in general, an integration test works with only one\nor two out-of-process dependencies. On the other hand, an end-to-end test works with\nall out-of-process dependencies, or with the vast majority of them. Hence the name\nend-to-end, which means the test verifies the system from the end user’s point of view,\nincluding all the external applications this system integrates with (see figure 2.6).\n People also use such terms as UI tests (UI stands for user interface), GUI tests (GUI is\ngraphical user interface), and functional tests. The terminology is ill-defined, but in gen-\neral, these terms are all synonyms.\n Let’s say your application works with three out-of-process dependencies: a data-\nbase, the file system, and a payment gateway. A typical integration test would include\nonly the database and file system in scope and use a test double to replace the pay-\nment gateway. That’s because you have full control over the database and file system,\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 3380,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 61,
      "content": "39\nSummary\nand thus can easily bring them to the required state in tests, whereas you don’t have\nthe same degree of control over the payment gateway. With the payment gateway, you\nmay need to contact the payment processor organization to set up a special test\naccount. You might also need to check that account from time to time to manually\nclean up all the payment charges left over from the past test executions.\n Since end-to-end tests are the most expensive in terms of maintenance, it’s better\nto run them late in the build process, after all the unit and integration tests have\npassed. You may possibly even run them only on the build server, not on individual\ndevelopers’ machines.\n Keep in mind that even with end-to-end tests, you might not be able to tackle all of\nthe out-of-process dependencies. There may be no test version of some dependencies,\nor it may be impossible to bring those dependencies to the required state automati-\ncally. So you may still need to use a test double, reinforcing the fact that there isn’t a\ndistinct line between integration and end-to-end tests. \nSummary\nThroughout this chapter, I’ve refined the definition of a unit test:\n– A unit test verifies a single unit of behavior,\n– Does it quickly,\n– And does it in isolation from other tests.\nAnother class\nUnit test\nPayment gateway\nEnd-to-end test\nDatabase\nSystem under test\nIntegration test\nFigure 2.6\nEnd-to-end tests normally include all or almost all out-of-process dependencies \nin the scope. Integration tests check only one or two such dependencies—those that are \neasier to set up automatically, such as the database or the file system.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1687,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 62,
      "content": "40\nCHAPTER 2\nWhat is a unit test?\nThe isolation issue is disputed the most. The dispute led to the formation of two\nschools of unit testing: the classical (Detroit) school, and the London (mockist)\nschool. This difference of opinion affects the view of what constitutes a unit and\nthe treatment of the system under test’s (SUT’s) dependencies.\n– The London school states that the units under test should be isolated from\neach other. A unit under test is a unit of code, usually a class. All of its depen-\ndencies, except immutable dependencies, should be replaced with test dou-\nbles in tests.\n– The classical school states that the unit tests need to be isolated from each\nother, not units. Also, a unit under test is a unit of behavior, not a unit of code.\nThus, only shared dependencies should be replaced with test doubles.\nShared dependencies are dependencies that provide means for tests to affect\neach other’s execution flow.\nThe London school provides the benefits of better granularity, the ease of test-\ning large graphs of interconnected classes, and the ease of finding which func-\ntionality contains a bug after a test failure.\nThe benefits of the London school look appealing at first. However, they intro-\nduce several issues. First, the focus on classes under test is misplaced: tests\nshould verify units of behavior, not units of code. Furthermore, the inability to\nunit test a piece of code is a strong sign of a problem with the code design. The\nuse of test doubles doesn’t fix this problem, but rather only hides it. And finally,\nwhile the ease of determining which functionality contains a bug after a test fail-\nure is helpful, it’s not that big a deal because you often know what caused the\nbug anyway—it’s what you edited last.\nThe biggest issue with the London school of unit testing is the problem of over-\nspecification—coupling tests to the SUT’s implementation details.\nAn integration test is a test that doesn’t meet at least one of the criteria for a\nunit test. End-to-end tests are a subset of integration tests; they verify the system\nfrom the end user’s point of view. End-to-end tests reach out directly to all or\nalmost all out-of-process dependencies your application works with.\nFor a canonical book about the classical style, I recommend Kent Beck’s Test-\nDriven Development: By Example. For more on the London style, see Growing Object-\nOriented Software, Guided by Tests, by Steve Freeman and Nat Pryce. For further\nreading about working with dependencies, I recommend Dependency Injection:\nPrinciples, Practices, Patterns by Steven van Deursen and Mark Seemann.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2661,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 63,
      "content": "41\nThe anatomy of\na unit test\nIn this remaining chapter of part 1, I’ll give you a refresher on some basic topics.\nI’ll go over the structure of a typical unit test, which is usually represented by the\narrange, act, and assert (AAA) pattern. I’ll also show the unit testing framework of\nmy choice—xUnit—and explain why I’m using it and not one of its competitors.\n Along the way, we’ll talk about naming unit tests. There are quite a few compet-\ning pieces of advice on this topic, and unfortunately, most of them don’t do a good\nenough job improving your unit tests. In this chapter, I describe those less-useful\nnaming practices and show why they usually aren’t the best choice. Instead of those\npractices, I give you an alternative—a simple, easy-to-follow guideline for naming\ntests in a way that makes them readable not only to the programmer who wrote\nthem, but also to any other person familiar with the problem domain.\n Finally, I’ll talk about some features of the framework that help streamline the\nprocess of unit testing. Don’t worry about this information being too specific to C#\nThis chapter covers\nThe structure of a unit test\nUnit test naming best practices\nWorking with parameterized tests\nWorking with fluent assertions\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1294,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 64,
      "content": "42\nCHAPTER 3\nThe anatomy of a unit test\nand .NET; most unit testing frameworks exhibit similar functionality, regardless of\nthe programming language. If you learn one of them, you won’t have problems work-\ning with another.\n3.1\nHow to structure a unit test\nThis section shows how to structure unit tests using the arrange, act, and assert pat-\ntern, what pitfalls to avoid, and how to make your tests as readable as possible.\n3.1.1\nUsing the AAA pattern\nThe AAA pattern advocates for splitting each test into three parts: arrange, act, and\nassert. (This pattern is sometimes also called the 3A pattern.) Let’s take a Calculator\nclass with a single method that calculates a sum of two numbers:\npublic class Calculator\n{\npublic double Sum(double first, double second)\n{\nreturn first + second;\n}\n}\nThe following listing shows a test that verifies the class’s behavior. This test follows the\nAAA pattern.\npublic class CalculatorTests         \n{\n[Fact]    \npublic void Sum_of_two_numbers()   \n{\n// Arrange\ndouble first = 10;\n   \ndouble second = 20;\n   \nvar calculator = new Calculator();  \n// Act\ndouble result = calculator.Sum(first, second);    \n// Assert\nAssert.Equal(30, result);   \n}\n}\nThe AAA pattern provides a simple, uniform structure for all tests in the suite. This\nuniformity is one of the biggest advantages of this pattern: once you get used to it, you\ncan easily read and understand any test. That, in turn, reduces maintenance costs for\nyour entire test suite. The structure is as follows:\nListing 3.1\nA test covering the Sum method in calculator\nClass-container for a \ncohesive set of tests\nxUnit’s attribute \nindicating a test\nName of the\nunit test\nArrange \nsection\nAct section\nAssert section\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1757,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 65,
      "content": "43\nHow to structure a unit test\nIn the arrange section, you bring the system under test (SUT) and its dependen-\ncies to a desired state.\nIn the act section, you call methods on the SUT, pass the prepared dependen-\ncies, and capture the output value (if any).\nIn the assert section, you verify the outcome. The outcome may be represented\nby the return value, the final state of the SUT and its collaborators, or the meth-\nods the SUT called on those collaborators.\nThe natural inclination is to start writing a test with the arrange section. After all, it\ncomes before the other two. This approach works well in the vast majority of cases, but\nstarting with the assert section is a viable option too. When you practice Test-Driven\nDevelopment (TDD)—that is, when you create a failing test before developing a\nfeature—you don’t know enough about the feature’s behavior yet. So, it becomes\nadvantageous to first outline what you expect from the behavior and then figure out\nhow to develop the system to meet this expectation.\n Such a technique may look counterintuitive, but it’s how we approach problem\nsolving. We start by thinking about the objective: what a particular behavior should to\ndo for us. The actual solving of the problem comes after that. Writing down the asser-\ntions before everything else is merely a formalization of this thinking process. But\nagain, this guideline is only applicable when you follow TDD—when you write a test\nbefore the production code. If you write the production code before the test, by the\ntime you move on to the test, you already know what to expect from the behavior, so\nstarting with the arrange section is a better option. \n3.1.2\nAvoid multiple arrange, act, and assert sections\nOccasionally, you may encounter a test with multiple arrange, act, or assert sections. It\nusually works as shown in figure 3.1.\n When you see multiple act sections separated by assert and, possibly, arrange sec-\ntions, it means the test verifies multiple units of behavior. And, as we discussed in\nchapter 2, such a test is no longer a unit test but rather is an integration test. It’s best\nGiven-When-Then pattern\nYou might have heard of the Given-When-Then pattern, which is similar to AAA. This\npattern also advocates for breaking the test down into three parts:\nGiven—Corresponds to the arrange section\nWhen—Corresponds to the act section\nThen—Corresponds to the assert section\nThere’s no difference between the two patterns in terms of the test composition. The\nonly distinction is that the Given-When-Then structure is more readable to non-\nprogrammers. Thus, Given-When-Then is more suitable for tests that are shared with\nnon-technical people.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2732,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 66,
      "content": "44\nCHAPTER 3\nThe anatomy of a unit test\nto avoid such a test structure. A single action ensures that your tests remain within the\nrealm of unit testing, which means they are simple, fast, and easy to understand. If you\nsee a test containing a sequence of actions and assertions, refactor it. Extract each act\ninto a test of its own.\n It’s sometimes fine to have multiple act sections in integration tests. As you may\nremember from the previous chapter, integration tests can be slow. One way to speed\nthem up is to group several integration tests together into a single test with multiple\nacts and assertions. It’s especially helpful when system states naturally flow from one\nanother: that is, when an act simultaneously serves as an arrange for the subsequent act.\n But again, this optimization technique is only applicable to integration tests—and\nnot all of them, but rather those that are already slow and that you don’t want to\nbecome even slower. There’s no need for such an optimization in unit tests or integra-\ntion tests that are fast enough. It’s always better to split a multistep unit test into sev-\neral tests. \n3.1.3\nAvoid if statements in tests\nSimilar to multiple occurrences of the arrange, act, and assert sections, you may some-\ntimes encounter a unit test with an if statement. This is also an anti-pattern. A test—\nwhether a unit test or an integration test—should be a simple sequence of steps with\nno branching.\n An if statement indicates that the test verifies too many things at once. Such a test,\ntherefore, should be split into several tests. But unlike the situation with multiple AAA\nArrange the test\nAct\nAssert\nAct some more\nAssert again\nFigure 3.1\nMultiple arrange, act, and assert sections are a hint that the test verifies \ntoo many things at once. Such a test needs to be split into several tests to fix the \nproblem.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1905,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 67,
      "content": "45\nHow to structure a unit test\nsections, there’s no exception for integration tests. There are no benefits in branching\nwithin a test. You only gain additional maintenance costs: if statements make the tests\nharder to read and understand. \n3.1.4\nHow large should each section be?\nA common question people ask when starting out with the AAA pattern is, how large\nshould each section be? And what about the teardown section—the section that cleans\nup after the test? There are different guidelines regarding the size for each of the test\nsections.\nTHE ARRANGE SECTION IS THE LARGEST\nThe arrange section is usually the largest of the three. It can be as large as the act and\nassert sections combined. But if it becomes significantly larger than that, it’s better to\nextract the arrangements either into private methods within the same test class or to a\nseparate factory class. Two popular patterns can help you reuse the code in the arrange\nsections: Object Mother and Test Data Builder. \nWATCH OUT FOR ACT SECTIONS THAT ARE LARGER THAN A SINGLE LINE\nThe act section is normally just a single line of code. If the act consists of two or more\nlines, it could indicate a problem with the SUT’s public API.\n It’s best to express this point with an example, so let’s take one from chapter 2,\nwhich I repeat in the following listing. In this example, the customer makes a pur-\nchase from a store.\n[Fact]\npublic void Purchase_succeeds_when_enough_inventory()\n{\n// Arrange\nvar store = new Store();\nstore.AddInventory(Product.Shampoo, 10);\nvar customer = new Customer();\n// Act\nbool success = customer.Purchase(store, Product.Shampoo, 5);\n// Assert\nAssert.True(success);\nAssert.Equal(5, store.GetInventory(Product.Shampoo));\n}\nNotice that the act section in this test is a single method call, which is a sign of a well-\ndesigned class’s API. Now compare it to the version in listing 3.3: this act section con-\ntains two lines. And that’s a sign of a problem with the SUT: it requires the client to\nremember to make the second method call to finish the purchase and thus lacks\nencapsulation.\n \nListing 3.2\nA single-line act section \nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2174,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 68,
      "content": "46\nCHAPTER 3\nThe anatomy of a unit test\n[Fact]\npublic void Purchase_succeeds_when_enough_inventory()\n{\n// Arrange\nvar store = new Store();\nstore.AddInventory(Product.Shampoo, 10);\nvar customer = new Customer();\n// Act\nbool success = customer.Purchase(store, Product.Shampoo, 5);\nstore.RemoveInventory(success, Product.Shampoo, 5);\n// Assert\nAssert.True(success);\nAssert.Equal(5, store.GetInventory(Product.Shampoo));\n}\nHere’s what you can read from listing 3.3’s act section:\nIn the first line, the customer tries to acquire five units of shampoo from the\nstore.\nIn the second line, the inventory is removed from the store. The removal takes\nplace only if the preceding call to Purchase() returns a success.\nThe issue with the new version is that it requires two method calls to perform a single\noperation. Note that this is not an issue with the test itself. The test still verifies the\nsame unit of behavior: the process of making a purchase. The issue lies in the API sur-\nface of the Customer class. It shouldn’t require the client to make an additional\nmethod call.\n From a business perspective, a successful purchase has two outcomes: the acquisi-\ntion of a product by the customer and the reduction of the inventory in the store.\nBoth of these outcomes must be achieved together, which means there should be a\nsingle public method that does both things. Otherwise, there’s a room for inconsis-\ntency if the client code calls the first method but not the second, in which case the cus-\ntomer will acquire the product but its available amount won’t be reduced in the store.\n Such an inconsistency is called an invariant violation. The act of protecting your\ncode against potential inconsistencies is called encapsulation. When an inconsistency\npenetrates into the database, it becomes a big problem: now it’s impossible to reset\nthe state of your application by simply restarting it. You’ll have to deal with the cor-\nrupted data in the database and, potentially, contact customers and handle the situation\non a case-by-case basis. Just imagine what would happen if the application generated\nconfirmation receipts without actually reserving the inventory. It might issue claims\nto, and even charge for, more inventory than you could feasibly acquire in the near\nfuture.\n The remedy is to maintain code encapsulation at all times. In the previous exam-\nple, the customer should remove the acquired inventory from the store as part of its\nListing 3.3\nA two-line act section \nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2531,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 69,
      "content": "47\nHow to structure a unit test\nPurchase method and not rely on the client code to do so. When it comes to main-\ntaining invariants, you should eliminate any potential course of action that could lead\nto an invariant violation.\n This guideline of keeping the act section down to a single line holds true for the\nvast majority of code that contains business logic, but less so for utility or infrastruc-\nture code. Thus, I won’t say “never do it.” Be sure to examine each such case for a\npotential breach in encapsulation, though. \n3.1.5\nHow many assertions should the assert section hold?\nFinally, there’s the assert section. You may have heard about the guideline of having\none assertion per test. It takes root in the premise discussed in the previous chapter:\nthe premise of targeting the smallest piece of code possible.\n As you already know, this premise is incorrect. A unit in unit testing is a unit of\nbehavior, not a unit of code. A single unit of behavior can exhibit multiple outcomes,\nand it’s fine to evaluate them all in one test.\n Having that said, you need to watch out for assertion sections that grow too large:\nit could be a sign of a missing abstraction in the production code. For example,\ninstead of asserting all properties inside an object returned by the SUT, it may be bet-\nter to define proper equality members in the object’s class. You can then compare the\nobject to an expected value using a single assertion. \n3.1.6\nWhat about the teardown phase?\nSome people also distinguish a fourth section, teardown, which comes after arrange, act,\nand assert. For example, you can use this section to remove any files created by the\ntest, close a database connection, and so on. The teardown is usually represented by a\nseparate method, which is reused across all tests in the class. Thus, I don’t include this\nphase in the AAA pattern.\n Note that most unit tests don’t need teardown. Unit tests don’t talk to out-of-process\ndependencies and thus don’t leave side effects that need to be disposed of. That’s a\nrealm of integration testing. We’ll talk more about how to properly clean up after inte-\ngration tests in part 3. \n3.1.7\nDifferentiating the system under test\nThe SUT plays a significant role in tests. It provides an entry point for the behavior\nyou want to invoke in the application. As we discussed in the previous chapter, this\nbehavior can span across as many as several classes or as little as a single method. But\nthere can be only one entry point: one class that triggers that behavior.\n Thus it’s important to differentiate the SUT from its dependencies, especially\nwhen there are quite a few of them, so that you don’t need to spend too much time\nfiguring out who is who in the test. To do that, always name the SUT in tests sut. The\nfollowing listing shows how CalculatorTests would look after renaming the Calcu-\nlator instance.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2920,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 70,
      "content": "48\nCHAPTER 3\nThe anatomy of a unit test\npublic class CalculatorTests\n{\n[Fact]\npublic void Sum_of_two_numbers()\n{\n// Arrange\ndouble first = 10;\ndouble second = 20;\nvar sut = new Calculator();    \n// Act\ndouble result = sut.Sum(first, second);\n// Assert\nAssert.Equal(30, result);\n}\n}\n3.1.8\nDropping the arrange, act, and assert comments from tests\nJust as it’s important to set the SUT apart from its dependencies, it’s also important to\ndifferentiate the three sections from each other, so that you don’t spend too much\ntime figuring out what section a particular line in the test belongs to. One way to do\nthat is to put // Arrange, // Act, and // Assert comments before the beginning of\neach section. Another way is to separate the sections with empty lines, as shown next.\npublic class CalculatorTests\n{\n[Fact]\npublic void Sum_of_two_numbers()\n{\ndouble first = 10;\n   \ndouble second = 20;\n  \nvar sut = new Calculator();  \ndouble result = sut.Sum(first, second);   \nAssert.Equal(30, result);   \n}\n}\nSeparating sections with empty lines works great in most unit tests. It allows you to\nkeep a balance between brevity and readability. It doesn’t work as well in large tests,\nthough, where you may want to put additional empty lines inside the arrange section\nto differentiate between configuration stages. This is often the case in integration\ntests—they frequently contain complicated setup logic. Therefore,\nListing 3.4\nDifferentiating the SUT from its dependencies\nListing 3.5\nCalculator with sections separated by empty lines\nThe calculator is \nnow called sut. \nArrange\nAct\nAssert\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1635,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 71,
      "content": "49\nExploring the xUnit testing framework\nDrop the section comments in tests that follow the AAA pattern and where you\ncan avoid additional empty lines inside the arrange and assert sections.\nKeep the section comments otherwise. \n3.2\nExploring the xUnit testing framework\nIn this section, I give a brief overview of unit testing tools available in .NET, and\ntheir features. I’m using xUnit (https://github.com/xunit/xunit) as the unit testing\nframework (note that you need to install the xunit.runner.visualstudio NuGet\npackage in order to run xUnit tests from Visual Studio). Although this framework\nworks in .NET only, every object-oriented language (Java, C++, JavaScript, and so\non) has unit testing frameworks, and all those frameworks look quite similar to each\nother. If you’ve worked with one of them, you won’t have any issues working with\nanother.\n In .NET alone, there are several alternatives to choose from, such as NUnit\n(https://github.com/nunit/nunit) and the built-in Microsoft MSTest. I personally\nprefer xUnit for the reasons I’ll describe shortly, but you can also use NUnit; these two\nframeworks are pretty much on par in terms of functionality. I don’t recommend\nMSTest, though; it doesn’t provide the same level of flexibility as xUnit and NUnit.\nAnd don’t take my word for it—even people inside Microsoft refrain from using\nMSTest. For example, the ASP.NET Core team uses xUnit.\n I prefer xUnit because it’s a cleaner, more concise version of NUnit. For example,\nyou may have noticed that in the tests I’ve brought up so far, there are no framework-\nrelated attributes other than [Fact], which marks the method as a unit test so the unit\ntesting framework knows to run it. There are no [TestFixture] attributes; any public\nclass can contain a unit test. There’s also no [SetUp] or [TearDown]. If you need to\nshare configuration logic between tests, you can put it inside the constructor. And if\nyou need to clean something up, you can implement the IDisposable interface, as\nshown in this listing.\npublic class CalculatorTests : IDisposable\n{\nprivate readonly Calculator _sut;\npublic CalculatorTests()\n   \n{\n   \n_sut = new Calculator();   \n}\n   \n[Fact]\npublic void Sum_of_two_numbers()\n{\n/* ... */\n}\nListing 3.6\nArrangement and teardown logic, shared by all tests\nCalled before \neach test in \nthe class\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2379,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 72,
      "content": "50\nCHAPTER 3\nThe anatomy of a unit test\npublic void Dispose()   \n{\n   \n_sut.CleanUp();\n   \n}\n   \n}\nAs you can see, the xUnit authors took significant steps toward simplifying the\nframework. A lot of notions that previously required additional configuration (like\n[TestFixture] or [SetUp] attributes) now rely on conventions or built-in language\nconstructs.\n I particularly like the [Fact] attribute, specifically because it’s called Fact and not\nTest. It emphasizes the rule of thumb I mentioned in the previous chapter: each test\nshould tell a story. This story is an individual, atomic scenario or fact about the problem\ndomain, and the passing test is a proof that this scenario or fact holds true. If the test\nfails, it means either the story is no longer valid and you need to rewrite it, or the sys-\ntem itself has to be fixed.\n I encourage you to adopt this way of thinking when you write unit tests. Your tests\nshouldn’t be a dull enumeration of what the production code does. Rather, they should\nprovide a higher-level description of the application’s behavior. Ideally, this description\nshould be meaningful not just to programmers but also to business people. \n3.3\nReusing test fixtures between tests\nIt’s important to know how and when to reuse code between tests. Reusing code\nbetween arrange sections is a good way to shorten and simplify your tests, and this sec-\ntion shows how to do that properly.\n I mentioned earlier that often, fixture arrangements take up too much space. It\nmakes sense to extract these arrangements into separate methods or classes that you\nthen reuse between tests. There are two ways you can perform such reuse, but only\none of them is beneficial; the other leads to increased maintenance costs.\nTest fixture\nThe term test fixture has two common meanings:\n1\nA test fixture is an object the test runs against. This object can be a regular\ndependency—an argument that is passed to the SUT. It can also be data in\nthe database or a file on the hard disk. Such an object needs to remain in a\nknown, fixed state before each test run, so it produces the same result.\nHence the word fixture.\n2\nThe other definition comes from the NUnit testing framework. In NUnit, Test-\nFixture is an attribute that marks a class containing tests.\nI use the first definition throughout this book.\nCalled after \neach test in \nthe class\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2404,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 73,
      "content": "51\nReusing test fixtures between tests\nThe first—incorrect—way to reuse test fixtures is to initialize them in the test’s con-\nstructor (or the method marked with a [SetUp] attribute if you are using NUnit), as\nshown next.\npublic class CustomerTests\n{\nprivate readonly Store _store;       \nprivate readonly Customer _sut;\npublic CustomerTests()\n     \n{\n   \n_store = new Store();\n   \n_store.AddInventory(Product.Shampoo, 10);   \n_sut = new Customer();\n   \n}\n   \n[Fact]\npublic void Purchase_succeeds_when_enough_inventory()\n{\nbool success = _sut.Purchase(_store, Product.Shampoo, 5);\nAssert.True(success);\nAssert.Equal(5, _store.GetInventory(Product.Shampoo));\n}\n[Fact]\npublic void Purchase_fails_when_not_enough_inventory()\n{\nbool success = _sut.Purchase(_store, Product.Shampoo, 15);\nAssert.False(success);\nAssert.Equal(10, _store.GetInventory(Product.Shampoo));\n}\n}\nThe two tests in listing 3.7 have common configuration logic. In fact, their arrange sec-\ntions are the same and thus can be fully extracted into CustomerTests’s constructor—\nwhich is precisely what I did here. The tests themselves no longer contain arrangements.\n With this approach, you can significantly reduce the amount of test code—you can\nget rid of most or even all test fixture configurations in tests. But this technique has\ntwo significant drawbacks:\nIt introduces high coupling between tests.\nIt diminishes test readability.\nLet’s discuss these drawbacks in more detail.\nListing 3.7\nExtracting the initialization code into the test constructor\nCommon test \nfixture\nRuns before \neach test in \nthe class\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1634,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 74,
      "content": "52\nCHAPTER 3\nThe anatomy of a unit test\n3.3.1\nHigh coupling between tests is an anti-pattern\nIn the new version, shown in listing 3.7, all tests are coupled to each other: a modifica-\ntion of one test’s arrangement logic will affect all tests in the class. For example, chang-\ning this line\n_store.AddInventory(Product.Shampoo, 10);\nto this\n_store.AddInventory(Product.Shampoo, 15);\nwould invalidate the assumption the tests make about the store’s initial state and there-\nfore would lead to unnecessary test failures.\n That’s a violation of an important guideline: a modification of one test should not affect\nother tests. This guideline is similar to what we discussed in chapter 2—that tests should\nrun in isolation from each other. It’s not the same, though. Here, we are talking about\nindependent modification of tests, not independent execution. Both are important\nattributes of a well-designed test.\n To follow this guideline, you need to avoid introducing shared state in test classes.\nThese two private fields are examples of such a shared state:\nprivate readonly Store _store;\nprivate readonly Customer _sut;\n3.3.2\nThe use of constructors in tests diminishes test readability\nThe other drawback to extracting the arrangement code into the constructor is\ndiminished test readability. You no longer see the full picture just by looking at the\ntest itself. You have to examine different places in the class to understand what the test\nmethod does.\n Even if there’s not much arrangement logic—say, only instantiation of the fixtures—\nyou are still better off moving it directly to the test method. Otherwise, you’ll wonder\nif it’s really just instantiation or something else being configured there, too. A self-con-\ntained test doesn’t leave you with such uncertainties. \n3.3.3\nA better way to reuse test fixtures\nThe use of the constructor is not the best approach when it comes to reusing test fix-\ntures. The second way—the beneficial one—is to introduce private factory methods in\nthe test class, as shown in the following listing.\npublic class CustomerTests\n{\n[Fact]\npublic void Purchase_succeeds_when_enough_inventory()\n{\nListing 3.8\nExtracting the common initialization code into private factory methods\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2268,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 75,
      "content": "53\nReusing test fixtures between tests\nStore store = CreateStoreWithInventory(Product.Shampoo, 10);\nCustomer sut = CreateCustomer();\nbool success = sut.Purchase(store, Product.Shampoo, 5);\nAssert.True(success);\nAssert.Equal(5, store.GetInventory(Product.Shampoo));\n}\n[Fact]\npublic void Purchase_fails_when_not_enough_inventory()\n{\nStore store = CreateStoreWithInventory(Product.Shampoo, 10);\nCustomer sut = CreateCustomer();\nbool success = sut.Purchase(store, Product.Shampoo, 15);\nAssert.False(success);\nAssert.Equal(10, store.GetInventory(Product.Shampoo));\n}\nprivate Store CreateStoreWithInventory(\nProduct product, int quantity)\n{\nStore store = new Store();\nstore.AddInventory(product, quantity);\nreturn store;\n}\nprivate static Customer CreateCustomer()\n{\nreturn new Customer();\n}\n}\nBy extracting the common initialization code into private factory methods, you can\nalso shorten the test code, but at the same time keep the full context of what’s going\non in the tests. Moreover, the private methods don’t couple tests to each other as long\nas you make them generic enough. That is, allow the tests to specify how they want the\nfixtures to be created.\n Look at this line, for example:\nStore store = CreateStoreWithInventory(Product.Shampoo, 10);\nThe test explicitly states that it wants the factory method to add 10 units of shampoo\nto the store. This is both highly readable and reusable. It’s readable because you don’t\nneed to examine the internals of the factory method to understand the attributes of\nthe created store. It’s reusable because you can use this method in other tests, too.\n Note that in this particular example, there’s no need to introduce factory meth-\nods, as the arrangement logic is quite simple. View it merely as a demonstration.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1811,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 76,
      "content": "54\nCHAPTER 3\nThe anatomy of a unit test\n There’s one exception to this rule of reusing test fixtures. You can instantiate a fix-\nture in the constructor if it’s used by all or almost all tests. This is often the case for\nintegration tests that work with a database. All such tests require a database connec-\ntion, which you can initialize once and then reuse everywhere. But even then, it would\nmake more sense to introduce a base class and initialize the database connection in\nthat class’s constructor, not in individual test classes. See the following listing for an\nexample of common initialization code in a base class.\npublic class CustomerTests : IntegrationTests\n{\n[Fact]\npublic void Purchase_succeeds_when_enough_inventory()\n{\n/* use _database here */\n}\n}\npublic abstract class IntegrationTests : IDisposable\n{\nprotected readonly Database _database;\nprotected IntegrationTests()\n{\n_database = new Database();\n}\npublic void Dispose()\n{\n_database.Dispose();\n}\n}\nNotice how CustomerTests remains constructor-less. It gets access to the _database\ninstance by inheriting from the IntegrationTests base class. \n3.4\nNaming a unit test\nIt’s important to give expressive names to your tests. Proper naming helps you under-\nstand what the test verifies and how the underlying system behaves.\n So, how should you name a unit test? I’ve seen and tried a lot of naming conven-\ntions over the past decade. One of the most prominent, and probably least helpful, is\nthe following convention:\n[MethodUnderTest]_[Scenario]_[ExpectedResult]\nwhere\n\nMethodUnderTest is the name of the method you are testing.\n\nScenario is the condition under which you test the method.\nListing 3.9\nCommon initialization code in a base class\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1765,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 77,
      "content": "55\nNaming a unit test\n\nExpectedResult is what you expect the method under test to do in the current\nscenario.\nIt’s unhelpful specifically because it encourages you to focus on implementation\ndetails instead of the behavior.\n Simple phrases in plain English do a much better job: they are more expressive\nand don’t box you in a rigid naming structure. With simple phrases, you can describe\nthe system behavior in a way that’s meaningful to a customer or a domain expert. To\ngive you an example of a test titled in plain English, here’s the test from listing 3.5\nonce again:\npublic class CalculatorTests\n{\n[Fact]\npublic void Sum_of_two_numbers()\n{\ndouble first = 10;\ndouble second = 20;\nvar sut = new Calculator();\ndouble result = sut.Sum(first, second);\nAssert.Equal(30, result);\n}\n}\nHow could the test’s name (Sum_of_two_numbers) be rewritten using the [MethodUnder-\nTest]_[Scenario]_[ExpectedResult] convention? Probably something like this:\npublic void Sum_TwoNumbers_ReturnsSum()\nThe method under test is Sum, the scenario includes two numbers, and the expected\nresult is a sum of those two numbers. The new name looks logical to a programmer’s\neye, but does it really help with test readability? Not at all. It’s Greek to an unin-\nformed person. Think about it: Why does Sum appear twice in the name of the test?\nAnd what is this Returns phrasing all about? Where is the sum returned to? You\ncan’t know.\n Some might argue that it doesn’t really matter what a non-programmer would\nthink of this name. After all, unit tests are written by programmers for programmers,\nnot domain experts. And programmers are good at deciphering cryptic names—it’s\ntheir job!\n This is true, but only to a degree. Cryptic names impose a cognitive tax on every-\none, programmers or not. They require additional brain capacity to figure out what\nexactly the test verifies and how it relates to business requirements. This may not seem\nlike much, but the mental burden adds up over time. It slowly but surely increases the\nmaintenance cost for the entire test suite. It’s especially noticeable if you return to the\ntest after you’ve forgotten about the feature’s specifics, or try to understand a test\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2234,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 78,
      "content": "56\nCHAPTER 3\nThe anatomy of a unit test\nwritten by a colleague. Reading someone else’s code is already difficult enough—any\nhelp understanding it is of considerable use.\n Here are the two versions again:\npublic void Sum_of_two_numbers()\npublic void Sum_TwoNumbers_ReturnsSum()\nThe initial name written in plain English is much simpler to read. It is a down-to-earth\ndescription of the behavior under test.\n3.4.1\nUnit test naming guidelines\nAdhere to the following guidelines to write expressive, easily readable test names:\nDon’t follow a rigid naming policy. You simply can’t fit a high-level description of a\ncomplex behavior into the narrow box of such a policy. Allow freedom of\nexpression.\nName the test as if you were describing the scenario to a non-programmer who is familiar\nwith the problem domain. A domain expert or a business analyst is a good example.\nSeparate words with underscores. Doing so helps improve readability, especially in\nlong names.\nNotice that I didn’t use underscores when naming the test class, CalculatorTests.\nNormally, the names of classes are not as long, so they read fine without underscores.\n Also notice that although I use the pattern [ClassName]Tests when naming test\nclasses, it doesn’t mean the tests are limited to verifying only that class. Remember, the\nunit in unit testing is a unit of behavior, not a class. This unit can span across one or sev-\neral classes; the actual size is irrelevant. Still, you have to start somewhere. View the\nclass in [ClassName]Tests as just that: an entry point, an API, using which you can\nverify a unit of behavior. \n3.4.2\nExample: Renaming a test toward the guidelines\nLet’s take a test as an example and try to gradually improve its name using the guide-\nlines I just outlined. In the following listing, you can see a test verifying that a delivery\nwith a past date is invalid. The test’s name is written using the rigid naming policy that\ndoesn’t help with the test readability.\n[Fact]\npublic void IsDeliveryValid_InvalidDate_ReturnsFalse()\n{\nDeliveryService sut = new DeliveryService();\nDateTime pastDate = DateTime.Now.AddDays(-1);\nDelivery delivery = new Delivery\n{\nDate = pastDate\n};\nListing 3.10\nA test named using the rigid naming policy\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2281,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 79,
      "content": "57\nNaming a unit test\nbool isValid = sut.IsDeliveryValid(delivery);\nAssert.False(isValid);\n}\nThis test checks that DeliveryService properly identifies a delivery with an incorrect\ndate as invalid. How would you rewrite the test’s name in plain English? The following\nwould be a good first try:\npublic void Delivery_with_invalid_date_should_be_considered_invalid()\nNotice two things in the new version:\nThe name now makes sense to a non-programmer, which means programmers\nwill have an easier time understanding it, too.\nThe name of the SUT’s method—IsDeliveryValid—is no longer part of the\ntest’s name.\nThe second point is a natural consequence of rewriting the test’s name in plain\nEnglish and thus can be easily overlooked. However, this consequence is important\nand can be elevated into a guideline of its own.\nBut let’s get back to the example. The new version of the test’s name is a good start,\nbut it can be improved further. What does it mean for a delivery date to be invalid,\nexactly? From the test in listing 3.10, we can see that an invalid date is any date in\nthe past. This makes sense—you should only be allowed to choose a delivery date\nin the future.\n So let’s be specific and reflect this knowledge in the test’s name:\npublic void Delivery_with_past_date_should_be_considered_invalid()\nMethod under test in the test’s name\nDon’t include the name of the SUT’s method in the test’s name.\nRemember, you don’t test code, you test application behavior. Therefore, it doesn’t\nmatter what the name of the method under test is. As I mentioned previously, the\nSUT is just an entry point: a means to invoke a behavior. You can decide to rename\nthe method under test to, say, IsDeliveryCorrect, and it will have no effect on the\nSUT’s behavior. On the other hand, if you follow the original naming convention, you’ll\nhave to rename the test. This once again shows that targeting code instead of behav-\nior couples tests to that code’s implementation details, which negatively affects the\ntest suite’s maintainability. More on this issue in chapter 5.\nThe only exception to this guideline is when you work on utility code. Such code\ndoesn’t contain business logic—its behavior doesn’t go much beyond simple auxil-\niary functionality and thus doesn’t mean anything to business people. It’s fine to use\nthe SUT’s method names there.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2390,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 80,
      "content": "58\nCHAPTER 3\nThe anatomy of a unit test\nThis is better but still not ideal. It’s too verbose. We can get rid of the word consid-\nered without any loss of meaning:\npublic void Delivery_with_past_date_should_be_invalid()\nThe wording should be is another common anti-pattern. Earlier in this chapter, I men-\ntioned that a test is a single, atomic fact about a unit of behavior. There’s no place for\na wish or a desire when stating a fact. Name the test accordingly—replace should be\nwith is:\npublic void Delivery_with_past_date_is_invalid()\nAnd finally, there’s no need to avoid basic English grammar. Articles help the test read\nflawlessly. Add the article a to the test’s name:\npublic void Delivery_with_a_past_date_is_invalid()\nThere you go. This final version is a straight-to-the-point statement of a fact, which\nitself describes one of the aspects of the application behavior under test: in this partic-\nular case, the aspect of determining whether a delivery can be done. \n3.5\nRefactoring to parameterized tests\nOne test usually is not enough to fully describe a unit of behavior. Such a unit normally\nconsists of multiple components, each of which should be captured with its own test. If\nthe behavior is complex enough, the number of tests describing it can grow dramatically\nand may become unmanageable. Luckily, most unit testing frameworks provide func-\ntionality that allows you to group similar tests using parameterized tests (see figure 3.2).\nBehavior N\n…\n…\n…\n…\nBehavior 2\nBehavior 1\nCan be grouped\nFact N\nFact 2\nFact 1\nApplication\nFigure 3.2\nA typical application \nexhibits multiple behaviors. The \ngreater the complexity of the \nbehavior, the more facts are required \nto fully describe it. Each fact is \nrepresented by a test. Similar facts \ncan be grouped into a single test \nmethod using parameterized tests.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1877,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 81,
      "content": "59\nRefactoring to parameterized tests\nIn this section, I’ll first show each such behavior component described by a separate test\nand then demonstrate how these tests can be grouped together.\n Let’s say that our delivery functionality works in such a way that the soonest\nallowed delivery date is two days from now. Clearly, the one test we have isn’t enough.\nIn addition to the test that checks for a past delivery date, we’ll also need tests that\ncheck for today’s date, tomorrow’s date, and the date after that.\n The existing test is called Delivery_with_a_past_date_is_invalid. We could\nadd three more:\npublic void Delivery_for_today_is_invalid()\npublic void Delivery_for_tomorrow_is_invalid()\npublic void The_soonest_delivery_date_is_two_days_from_now()\nBut that would result in four test methods, with the only difference between them\nbeing the delivery date.\n A better approach is to group these tests into one in order to reduce the amount of\ntest code. xUnit (like most other test frameworks) has a feature called parameterized\ntests that allows you to do exactly that. The next listing shows how such grouping looks.\nEach InlineData attribute represents a separate fact about the system; it’s a test case\nin its own right.\npublic class DeliveryServiceTests\n{\n[InlineData(-1, false)]   \n[InlineData(0, false)]   \n[InlineData(1, false)]   \n[InlineData(2, true)]\n  \n[Theory]\npublic void Can_detect_an_invalid_delivery_date(\nint daysFromNow,       \nbool expected)\n      \n{\nDeliveryService sut = new DeliveryService();\nDateTime deliveryDate = DateTime.Now\n.AddDays(daysFromNow);                   \nDelivery delivery = new Delivery\n{\nDate = deliveryDate\n};\nbool isValid = sut.IsDeliveryValid(delivery);\nAssert.Equal(expected, isValid);              \n}\n}\nTIP\nNotice the use of the [Theory] attribute instead of [Fact]. A theory is a\nbunch of facts about the behavior.\nListing 3.11\nA test that encompasses several facts\nThe InlineData attribute sends a \nset of input values to the test \nmethod. Each line represents a \nseparate fact about the behavior.\nParameters to which the attributes \nattach the input values\nUses the \nparameters\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2186,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 82,
      "content": "60\nCHAPTER 3\nThe anatomy of a unit test\nEach fact is now represented by an [InlineData] line rather than a separate test. I\nalso renamed the test method something more generic: it no longer mentions what\nconstitutes a valid or invalid date.\n Using parameterized tests, you can significantly reduce the amount of test code,\nbut this benefit comes at a cost. It’s now hard to figure out what facts the test method\nrepresents. And the more parameters there are, the harder it becomes. As a compro-\nmise, you can extract the positive test case into its own test and benefit from the\ndescriptive naming where it matters the most—in determining what differentiates\nvalid and invalid delivery dates, as shown in the following listing.\npublic class DeliveryServiceTests\n{\n[InlineData(-1)]\n[InlineData(0)]\n[InlineData(1)]\n[Theory]\npublic void Detects_an_invalid_delivery_date(int daysFromNow)\n{\n/* ... */\n}\n[Fact]\npublic void The_soonest_delivery_date_is_two_days_from_now()\n{\n/* ... */\n}\n}\nThis approach also simplifies the negative test cases, since you can remove the\nexpected Boolean parameter from the test method. And, of course, you can trans-\nform the positive test method into a parameterized test as well, to test multiple dates.\n As you can see, there’s a trade-off between the amount of test code and the read-\nability of that code. As a rule of thumb, keep both positive and negative test cases\ntogether in a single method only when it’s self-evident from the input parameters\nwhich case stands for what. Otherwise, extract the positive test cases. And if the behav-\nior is too complicated, don’t use the parameterized tests at all. Represent each nega-\ntive and positive test case with its own test method.\n3.5.1\nGenerating data for parameterized tests\nThere are some caveats in using parameterized tests (at least, in .NET) that you need\nto be aware of. Notice that in listing 3.11, I used the daysFromNow parameter as an\ninput to the test method. Why not the actual date and time, you might ask? Unfortu-\nnately, the following code won’t work:\n[InlineData(DateTime.Now.AddDays(-1), false)]\n[InlineData(DateTime.Now, false)]\nListing 3.12\nTwo tests verifying the positive and negative scenarios\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2251,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 83,
      "content": "61\nRefactoring to parameterized tests\n[InlineData(DateTime.Now.AddDays(1), false)]\n[InlineData(DateTime.Now.AddDays(2), true)]\n[Theory]\npublic void Can_detect_an_invalid_delivery_date(\nDateTime deliveryDate,\nbool expected)\n{\nDeliveryService sut = new DeliveryService();\nDelivery delivery = new Delivery\n{\nDate = deliveryDate\n};\nbool isValid = sut.IsDeliveryValid(delivery);\nAssert.Equal(expected, isValid);\n}\nIn C#, the content of all attributes is evaluated at compile time. You have to use only\nthose values that the compiler can understand, which are as follows:\nConstants\nLiterals\n\ntypeof() expressions\nThe call to DateTime.Now relies on the .NET runtime and thus is not allowed.\n There is a way to overcome this problem. xUnit has another feature that you can\nuse to generate custom data to feed into the test method: [MemberData]. The next list-\ning shows how we can rewrite the previous test using this feature.\n[Theory]\n[MemberData(nameof(Data))]\npublic void Can_detect_an_invalid_delivery_date(\nDateTime deliveryDate,\nbool expected)\n{\n/* ... */\n}\npublic static List<object[]> Data()\n{\nreturn new List<object[]>\n{\nnew object[] { DateTime.Now.AddDays(-1), false },\nnew object[] { DateTime.Now, false },\nnew object[] { DateTime.Now.AddDays(1), false },\nnew object[] { DateTime.Now.AddDays(2), true }\n};\n}\nListing 3.13\nGenerating complex data for the parameterized test \nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1430,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 84,
      "content": "62\nCHAPTER 3\nThe anatomy of a unit test\nMemberData accepts the name of a static method that generates a collection of input\ndata (the compiler translates nameof(Data) into a \"Data\" literal). Each element of\nthe collection is itself a collection that is mapped into the two input parameters:\ndeliveryDate and expected. With this feature, you can overcome the compiler’s\nrestrictions and use parameters of any type in the parameterized tests. \n3.6\nUsing an assertion library to further improve \ntest readability\nOne more thing you can do to improve test readability is to use an assertion library. I\npersonally prefer Fluent Assertions (https://fluentassertions.com), but .NET has sev-\neral competing libraries in this area.\n The main benefit of using an assertion library is how you can restructure the asser-\ntions so that they are more readable. Here’s one of our earlier tests:\n[Fact]\npublic void Sum_of_two_numbers()\n{\nvar sut = new Calculator();\ndouble result = sut.Sum(10, 20);\nAssert.Equal(30, result);\n}\nNow compare it to the following, which uses a fluent assertion:\n[Fact]\npublic void Sum_of_two_numbers()\n{\nvar sut = new Calculator();\ndouble result = sut.Sum(10, 20);\nresult.Should().Be(30);\n}\nThe assertion from the second test reads as plain English, which is exactly how you\nwant all your code to read. We as humans prefer to absorb information in the form of\nstories. All stories adhere to this specific pattern:\n[Subject] [action] [object].\nFor example,\nBob opened the door.\nHere, Bob is a subject, opened is an action, and the door is an object. The same rule\napplies to code. result.Should().Be(30) reads better than Assert.Equal(30,\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1702,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 85,
      "content": "63\nSummary\nresult) precisely because it follows the story pattern. It’s a simple story in which\nresult is a subject, should be is an action, and 30 is an object.\nNOTE\nThe paradigm of object-oriented programming (OOP) has become a\nsuccess partly because of this readability benefit. With OOP, you, too, can\nstructure the code in a way that reads like a story.\nThe Fluent Assertions library also provides numerous helper methods to assert against\nnumbers, strings, collections, dates and times, and much more. The only drawback is\nthat such a library is an additional dependency you may not want to introduce to your\nproject (although it’s for development only and won’t be shipped to production). \nSummary\nAll unit tests should follow the AAA pattern: arrange, act, assert. If a test has mul-\ntiple arrange, act, or assert sections, that’s a sign that the test verifies multiple\nunits of behavior at once. If this test is meant to be a unit test, split it into several\ntests—one per each action.\nMore than one line in the act section is a sign of a problem with the SUT’s API.\nIt requires the client to remember to always perform these actions together,\nwhich can potentially lead to inconsistencies. Such inconsistencies are called\ninvariant violations. The act of protecting your code against potential invariant\nviolations is called encapsulation.\nDistinguish the SUT in tests by naming it sut. Differentiate the three test sec-\ntions either by putting Arrange, Act, and Assert comments before them or by\nintroducing empty lines between these sections.\nReuse test fixture initialization code by introducing factory methods, not by\nputting this initialization code to the constructor. Such reuse helps maintain a\nhigh degree of decoupling between tests and also provides better readability.\nDon’t use a rigid test naming policy. Name each test as if you were describing\nthe scenario in it to a non-programmer who is familiar with the problem\ndomain. Separate words in the test name by underscores, and don’t include the\nname of the method under test in the test name.\nParameterized tests help reduce the amount of code needed for similar tests.\nThe drawback is that the test names become less readable as you make them\nmore generic.\nAssertion libraries help you further improve test readability by restructuring the\nword order in assertions so that they read like plain English. \nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2439,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 86,
      "content": "Licensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 51,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 87,
      "content": "Part 2\nMaking your tests\nwork for you\nNow that you’re armed with the knowledge of what unit testing is for,\nyou’re ready to dive into the very crux of what makes a good test and learn how\nto refactor your tests toward being more valuable. In chapter 4, you’ll learn\nabout the four pillars that make up a good unit test. These four pillars set a foun-\ndation, a common frame of reference, which we’ll use to analyze unit tests and\ntesting approaches moving forward.\n Chapter 5 takes the frame of reference established in chapter 4 and builds\nthe case for mocks and their relation to test fragility.\n Chapter 6 uses the same the frame of reference to examine the three styles of\nunit testing. It shows which of those styles tends to produce tests of the best qual-\nity, and why.\n Chapter 7 puts the knowledge from chapters 4 to 6 into practice and teaches\nyou how to refactor away from bloated, overcomplicated tests to tests that pro-\nvide as much value with as little maintenance cost as possible.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1049,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 88,
      "content": "Licensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 51,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 89,
      "content": "67\nThe four pillars\nof a good unit test\nNow we are getting to the heart of the matter. In chapter 1, you saw the properties\nof a good unit test suite:\nIt is integrated into the development cycle. You only get value from tests that you\nactively use; there’s no point in writing them otherwise.\nIt targets only the most important parts of your code base. Not all production code\ndeserves equal attention. It’s important to differentiate the heart of the\napplication (its domain model) from everything else. This topic is tackled in\nchapter 7.\nIt provides maximum value with minimum maintenance costs. To achieve this last\nattribute, you need to be able to\n– Recognize a valuable test (and, by extension, a test of low value)\n– Write a valuable test\nThis chapter covers\nExploring dichotomies between aspects of a \ngood unit test\nDefining an ideal test\nUnderstanding the Test Pyramid\nUsing black-box and white-box testing\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 976,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 90,
      "content": "68\nCHAPTER 4\nThe four pillars of a good unit test\nAs we discussed in chapter 1, recognizing a valuable test and writing a valuable test are two\nseparate skills. The latter skill requires the former one, though; so, in this chapter, I’ll\nshow how to recognize a valuable test. You’ll see a universal frame of reference with\nwhich you can analyze any test in the suite. We’ll then use this frame of reference to\ngo over some popular unit testing concepts: the Test Pyramid and black-box versus\nwhite-box testing.\n Buckle up: we are starting out.\n4.1\nDiving into the four pillars of a good unit test\nA good unit test has the following four attributes:\nProtection against regressions\nResistance to refactoring\nFast feedback\nMaintainability\nThese four attributes are foundational. You can use them to analyze any automated\ntest, be it unit, integration, or end-to-end. Every such test exhibits some degree of\neach attribute. In this section, I define the first two attributes; and in section 4.2, I\ndescribe the intrinsic connection between them.\n4.1.1\nThe first pillar: Protection against regressions\nLet’s start with the first attribute of a good unit test: protection against regressions. As you\nknow from chapter 1, a regression is a software bug. It’s when a feature stops working as\nintended after some code modification, usually after you roll out new functionality.\n Such regressions are annoying (to say the least), but that’s not the worst part about\nthem. The worst part is that the more features you develop, the more chances there are\nthat you’ll break one of those features with a new release. An unfortunate fact of pro-\ngramming life is that code is not an asset, it’s a liability. The larger the code base, the more\nexposure it has to potential bugs. That’s why it’s crucial to develop a good protection\nagainst regressions. Without such protection, you won’t be able to sustain the project\ngrowth in a long run—you’ll be buried under an ever-increasing number of bugs.\n To evaluate how well a test scores on the metric of protecting against regressions,\nyou need to take into account the following:\nThe amount of code that is executed during the test\nThe complexity of that code\nThe code’s domain significance\nGenerally, the larger the amount of code that gets executed, the higher the chance\nthat the test will reveal a regression. Of course, assuming that this test has a relevant\nset of assertions, you don’t want to merely execute the code. While it helps to know\nthat this code runs without throwing exceptions, you also need to validate the out-\ncome it produces.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2641,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 91,
      "content": "69\nDiving into the four pillars of a good unit test\n Note that it’s not only the amount of code that matters, but also its complexity and\ndomain significance. Code that represents complex business logic is more important\nthan boilerplate code—bugs in business-critical functionality are the most damaging.\n On the other hand, it’s rarely worthwhile to test trivial code. Such code is short and\ndoesn’t contain a substantial amount of business logic. Tests that cover trivial code\ndon’t have much of a chance of finding a regression error, because there’s not a lot of\nroom for a mistake. An example of trivial code is a single-line property like this:\npublic class User\n{\npublic string Name { get; set; }\n}\nFurthermore, in addition to your code, the code you didn’t write also counts: for\nexample, libraries, frameworks, and any external systems used in the project. That\ncode influences the working of your software almost as much as your own code. For\nthe best protection, the test must include those libraries, frameworks, and external sys-\ntems in the testing scope, in order to check that the assumptions your software makes\nabout these dependencies are correct.\nTIP\nTo maximize the metric of protection against regressions, the test needs\nto aim at exercising as much code as possible. \n4.1.2\nThe second pillar: Resistance to refactoring\nThe second attribute of a good unit test is resistance to refactoring—the degree to which\na test can sustain a refactoring of the underlying application code without turning red\n(failing).\nDEFINITION\nRefactoring means changing existing code without modifying its\nobservable behavior. The intention is usually to improve the code’s nonfunc-\ntional characteristics: increase readability and reduce complexity. Some exam-\nples of refactoring are renaming a method and extracting a piece of code into\na new class.\nPicture this situation. You developed a new feature, and everything works great. The\nfeature itself is doing its job, and all the tests are passing. Now you decide to clean up\nthe code. You do some refactoring here, a little bit of modification there, and every-\nthing looks even better than before. Except one thing—the tests are failing. You look\nmore closely to see exactly what you broke with the refactoring, but it turns out that\nyou didn’t break anything. The feature works perfectly, just as before. The problem is\nthat the tests are written in such a way that they turn red with any modification of the\nunderlying code. And they do that regardless of whether you actually break the func-\ntionality itself.\n This situation is called a false positive. A false positive is a false alarm. It’s a result\nindicating that the test fails, although in reality, the functionality it covers works as\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2803,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 92,
      "content": "70\nCHAPTER 4\nThe four pillars of a good unit test\nintended. Such false positives usually take place when you refactor the code—when\nyou modify the implementation but keep the observable behavior intact. Hence the\nname for this attribute of a good unit test: resistance to refactoring.\n To evaluate how well a test scores on the metric of resisting to refactoring, you\nneed to look at how many false positives the test generates. The fewer, the better.\n Why so much attention on false positives? Because they can have a devastating\neffect on your entire test suite. As you may recall from chapter 1, the goal of unit test-\ning is to enable sustainable project growth. The mechanism by which the tests enable\nsustainable growth is that they allow you to add new features and conduct regular\nrefactorings without introducing regressions. There are two specific benefits here:\nTests provide an early warning when you break existing functionality. Thanks to such\nearly warnings, you can fix an issue long before the faulty code is deployed to\nproduction, where dealing with it would require a significantly larger amount\nof effort.\nYou become confident that your code changes won’t lead to regressions. Without such\nconfidence, you will be much more hesitant to refactor and much more likely\nto leave the code base to deteriorate.\nFalse positives interfere with both of these benefits:\nIf tests fail with no good reason, they dilute your ability and willingness to react\nto problems in code. Over time, you get accustomed to such failures and stop\npaying as much attention. After a while, you start ignoring legitimate failures,\ntoo, allowing them to slip into production.\nOn the other hand, when false positives are frequent, you slowly lose trust in the\ntest suite. You no longer perceive it as a reliable safety net—the perception is\ndiminished by false alarms. This lack of trust leads to fewer refactorings,\nbecause you try to reduce code changes to a minimum in order to avoid regres-\nsions.\nA story from the trenches\nI once worked on a project with a rich history. The project wasn’t too old, maybe two\nor three years; but during that period of time, management significantly shifted the\ndirection they wanted to go with the project, and development changed direction\naccordingly. During this change, a problem emerged: the code base accumulated\nlarge chunks of leftover code that no one dared to delete or refactor. The company\nno longer needed the features that code provided, but some parts of it were used in\nnew functionality, so it was impossible to get rid of the old code completely.\nThe project had good test coverage. But every time someone tried to refactor the old\nfeatures and separate the bits that were still in use from everything else, the tests\nfailed. And not just the old tests—they had been disabled long ago—but the new\ntests, too. Some of the failures were legitimate, but most were not—they were false\npositives.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2994,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 93,
      "content": "71\nDiving into the four pillars of a good unit test\nThis story is typical of most projects with brittle tests. First, developers take test failures\nat face value and deal with them accordingly. After a while, people get tired of tests\ncrying “wolf” all the time and start to ignore them more and more. Eventually, there\ncomes a moment when a bunch of real bugs are released to production because devel-\nopers ignored the failures along with all the false positives.\n You don’t want to react to such a situation by ceasing all refactorings, though. The\ncorrect response is to re-evaluate the test suite and start reducing its brittleness. I\ncover this topic in chapter 7. \n4.1.3\nWhat causes false positives?\nSo, what causes false positives? And how can you avoid them?\n The number of false positives a test produces is directly related to the way the test\nis structured. The more the test is coupled to the implementation details of the system\nunder test (SUT), the more false alarms it generates. The only way to reduce the\nchance of getting a false positive is to decouple the test from those implementation\ndetails. You need to make sure the test verifies the end result the SUT delivers: its\nobservable behavior, not the steps it takes to do that. Tests should approach SUT veri-\nfication from the end user’s point of view and check only the outcome meaningful to\nthat end user. Everything else must be disregarded (more on this topic in chapter 5).\n The best way to structure a test is to make it tell a story about the problem domain.\nShould such a test fail, that failure would mean there’s a disconnect between the story\nand the actual application behavior. It’s the only type of test failure that benefits you—\nsuch failures are always on point and help you quickly understand what went wrong.\nAll other failures are just noise that steer your attention away from things that matter.\n Take a look at the following example. In it, the MessageRenderer class generates\nan HTML representation of a message containing a header, a body, and a footer.\npublic class Message\n{\npublic string Header { get; set; }\npublic string Body { get; set; }\npublic string Footer { get; set; }\n}\nAt first, the developers tried to deal with the test failures. However, since the vast\nmajority of them were false alarms, the situation got to the point where the develop-\ners ignored such failures and disabled the failing tests. The prevailing attitude was,\n“If it’s because of that old chunk of code, just disable the test; we’ll look at it later.”\nEverything worked fine for a while—until a major bug slipped into production. One of\nthe tests correctly identified the bug, but no one listened; the test was disabled along\nwith all the others. After that accident, the developers stopped touching the old code\nentirely.\nListing 4.1\nGenerating an HTML representation of a message\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2914,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 94,
      "content": "72\nCHAPTER 4\nThe four pillars of a good unit test\npublic interface IRenderer\n{\nstring Render(Message message);\n}\npublic class MessageRenderer : IRenderer\n{\npublic IReadOnlyList<IRenderer> SubRenderers { get; }\npublic MessageRenderer()\n{\nSubRenderers = new List<IRenderer>\n{\nnew HeaderRenderer(),\nnew BodyRenderer(),\nnew FooterRenderer()\n};\n}\npublic string Render(Message message)\n{\nreturn SubRenderers\n.Select(x => x.Render(message))\n.Aggregate(\"\", (str1, str2) => str1 + str2);\n}\n}\nThe MessageRenderer class contains several sub-renderers to which it delegates the\nactual work on parts of the message. It then combines the result into an HTML docu-\nment. The sub-renderers orchestrate the raw text with HTML tags. For example:\npublic class BodyRenderer : IRenderer\n{\npublic string Render(Message message)\n{\nreturn $\"<b>{message.Body}</b>\";\n}\n}\nHow can MessageRenderer be tested? One possible approach is to analyze the algo-\nrithm this class follows.\n[Fact]\npublic void MessageRenderer_uses_correct_sub_renderers()\n{\nvar sut = new MessageRenderer();\nIReadOnlyList<IRenderer> renderers = sut.SubRenderers;\nListing 4.2\nVerifying that MessageRenderer has the correct structure\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1226,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 95,
      "content": "73\nDiving into the four pillars of a good unit test\nAssert.Equal(3, renderers.Count);\nAssert.IsAssignableFrom<HeaderRenderer>(renderers[0]);\nAssert.IsAssignableFrom<BodyRenderer>(renderers[1]);\nAssert.IsAssignableFrom<FooterRenderer>(renderers[2]);\n}\nThis test checks to see if the sub-renderers are all of the expected types and appear in\nthe correct order, which presumes that the way MessageRenderer processes messages\nmust also be correct. The test might look good at first, but does it really verify Message-\nRenderer’s observable behavior? What if you rearrange the sub-renderers, or replace\none of them with a new one? Will that lead to a bug?\n Not necessarily. You could change a sub-renderer’s composition in such a way that\nthe resulting HTML document remains the same. For example, you could replace\nBodyRenderer with a BoldRenderer, which does the same job as BodyRenderer. Or you\ncould get rid of all the sub-renderers and implement the rendering directly in Message-\nRenderer.\n Still, the test will turn red if you do any of that, even though the end result won’t\nchange. That’s because the test couples to the SUT’s implementation details and not\nthe outcome the SUT produces. This test inspects the algorithm and expects to see\none particular implementation, without any consideration for equally applicable alter-\nnative implementations (see figure 4.1).\nAny substantial refactoring of the MessageRenderer class would lead to a test failure.\nMind you, the process of refactoring is changing the implementation without affecting\nthe application’s observable behavior. And it’s precisely because the test is concerned\nwith the implementation details that it turns red every time you change those details.\nStep 1\nStep 2\nStep 3\nClient\nSystem under test\nTest: “Are\nthese steps\ncorrect?”\nFigure 4.1\nA test that couples to the SUT’s algorithm. Such a test expects to see one particular \nimplementation (the specific steps the SUT must take to deliver the result) and therefore is \nbrittle. Any refactoring of the SUT’s implementation would lead to a test failure.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2125,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 96,
      "content": "74\nCHAPTER 4\nThe four pillars of a good unit test\nTherefore, tests that couple to the SUT’s implementation details are not resistant to refactoring.\nSuch tests exhibit all the shortcomings I described previously:\nThey don’t provide an early warning in the event of regressions—you simply\nignore those warnings due to little relevance.\nThey hinder your ability and willingness to refactor. It’s no wonder—who would\nlike to refactor, knowing that the tests can’t tell which way is up when it comes\nto finding bugs?\nThe next listing shows the most egregious example of brittleness in tests that I’ve ever\nencountered, in which the test reads the source code of the MessageRenderer class\nand compares it to the “correct” implementation.\n[Fact]\npublic void MessageRenderer_is_implemented_correctly()\n{\nstring sourceCode = File.ReadAllText(@\"[path]\\MessageRenderer.cs\");\nAssert.Equal(@\"\npublic class MessageRenderer : IRenderer\n{\npublic IReadOnlyList<<IRenderer> SubRenderers { get; }\npublic MessageRenderer()\n{\nSubRenderers = new List<<IRenderer>\n{\nnew HeaderRenderer(),\nnew BodyRenderer(),\nnew FooterRenderer()\n};\n}\npublic string Render(Message message) { /* ... */ }\n}\", sourceCode);\n}\nOf course, this test is just plain ridiculous; it will fail should you modify even the slight-\nest detail in the MessageRenderer class. At the same time, it’s not that different from\nthe test I brought up earlier. Both insist on a particular implementation without tak-\ning into consideration the SUT’s observable behavior. And both will turn red each\ntime you change that implementation. Admittedly, though, the test in listing 4.3 will\nbreak more often than the one in listing 4.2. \n4.1.4\nAim at the end result instead of implementation details\nAs I mentioned earlier, the only way to avoid brittleness in tests and increase their resis-\ntance to refactoring is to decouple them from the SUT’s implementation details—keep\nas much distance as possible between the test and the code’s inner workings, and\nListing 4.3\nVerifying the source code of the MessageRenderer class\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2108,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 97,
      "content": "75\nDiving into the four pillars of a good unit test\ninstead aim at verifying the end result. Let’s do that: let’s refactor the test from list-\ning 4.2 into something much less brittle.\n To start off, you need to ask yourself the following question: What is the final out-\ncome you get from MessageRenderer? Well, it’s the HTML representation of a mes-\nsage. And it’s the only thing that makes sense to check, since it’s the only observable\nresult you get out of the class. As long as this HTML representation stays the same,\nthere’s no need to worry about exactly how it’s generated. Such implementation\ndetails are irrelevant. The following code is the new version of the test.\n[Fact]\npublic void Rendering_a_message()\n{\nvar sut = new MessageRenderer();\nvar message = new Message\n{\nHeader = \"h\",\nBody = \"b\",\nFooter = \"f\"\n};\nstring html = sut.Render(message);\nAssert.Equal(\"<h1>h</h1><b>b</b><i>f</i>\", html);\n}\nThis test treats MessageRenderer as a black box and is only interested in its observable\nbehavior. As a result, the test is much more resistant to refactoring—it doesn’t care\nwhat changes you make to the SUT as long as the HTML output remains the same\n(figure 4.2).\n Notice the profound improvement in this test over the original version. It aligns\nitself with the business needs by verifying the only outcome meaningful to end users—\nListing 4.4\nVerifying the outcome that MessageRenderer produces\nStep 1\nStep 2\nStep 3\nClient\nSystem under test\nGood test: “Is\nthe end result\ncorrect?”\nStep 1\nStep 2\nStep 3\nClient\nSystem under test\nBad test: “Are\nthese steps\ncorrect?”\nFigure 4.2\nThe test on the left couples to the SUT’s observable behavior as opposed to implementation \ndetails. Such a test is resistant to refactoring—it will trigger few, if any, false positives.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1829,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 98,
      "content": "76\nCHAPTER 4\nThe four pillars of a good unit test\nhow a message is displayed in the browser. Failures of such a test are always on point:\nthey communicate a change in the application behavior that can affect the customer\nand thus should be brought to the developer’s attention. This test will produce few, if\nany, false positives.\n Why few and not none at all? Because there could still be changes in Message-\nRenderer that would break the test. For example, you could introduce a new parame-\nter in the Render() method, causing a compilation error. And technically, such an\nerror counts as a false positive, too. After all, the test isn’t failing because of a change\nin the application’s behavior.\n But this kind of false positive is easy to fix. Just follow the compiler and add a new\nparameter to all tests that invoke the Render() method. The worse false positives are\nthose that don’t lead to compilation errors. Such false positives are the hardest to deal\nwith—they seem as though they point to a legitimate bug and require much more\ntime to investigate.\n4.2\nThe intrinsic connection between the first \ntwo attributes\nAs I mentioned earlier, there’s an intrinsic connection between the first two pillars of\na good unit test—protection against regressions and resistance to refactoring. They both con-\ntribute to the accuracy of the test suite, though from opposite perspectives. These two\nattributes also tend to influence the project differently over time: while it’s important\nto have good protection against regressions very soon after the project’s initiation, the\nneed for resistance to refactoring is not immediate.\n In this section, I talk about\nMaximizing test accuracy\nThe importance of false positives and false negatives\n4.2.1\nMaximizing test accuracy\nLet’s step back for a second and look at the broader picture with regard to test results.\nWhen it comes to code correctness and test results, there are four possible outcomes,\nas shown in figure 4.3. The test can either pass or fail (the rows of the table). And the\nfunctionality itself can be either correct or broken (the table’s columns).\n The situation when the test passes and the underlying functionality works as\nintended is a correct inference: the test correctly inferred the state of the system (there\nare no bugs in it). Another term for this combination of working functionality and a\npassing test is true negative.\n Similarly, when the functionality is broken and the test fails, it’s also a correct infer-\nence. That’s because you expect to see the test fail when the functionality is not work-\ning properly. That’s the whole point of unit testing. The corresponding term for this\nsituation is true positive.\n But when the test doesn’t catch an error, that’s a problem. This is the upper-right\nquadrant, a false negative. And this is what the first attribute of a good test—protection\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2922,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 99,
      "content": "77\nThe intrinsic connection between the first two attributes\nagainst regressions—helps you avoid. Tests with a good protection against regressions\nhelp you to minimize the number of false negatives—type II errors.\n On the other hand, there’s a symmetric situation when the functionality is correct\nbut the test still shows a failure. This is a false positive, a false alarm. And this is what the\nsecond attribute—resistance to refactoring—helps you with.\n All these terms (false positive, type I error and so on) have roots in statistics, but can\nalso be applied to analyzing a test suite. The best way to wrap your head around them\nis to think of a flu test. A flu test is positive when the person taking the test has the flu.\nThe term positive is a bit confusing because there’s nothing positive about having the\nflu. But the test doesn’t evaluate the situation as a whole. In the context of testing,\npositive means that some set of conditions is now true. Those are the conditions the\ncreators of the test have set it to react to. In this particular example, it’s the presence\nof the flu. Conversely, the lack of flu renders the flu test negative.\n Now, when you evaluate how accurate the flu test is, you bring up terms such as\nfalse positive or false negative. The probability of false positives and false negatives tells\nyou how good the flu test is: the lower that probability, the more accurate the test.\n This accuracy is what the first two pillars of a good unit test are all about. Protection\nagainst regressions and resistance to refactoring aim at maximizing the accuracy of the test\nsuite. The accuracy metric itself consists of two components:\nHow good the test is at indicating the presence of bugs (lack of false negatives,\nthe sphere of protection against regressions)\nHow good the test is at indicating the absence of bugs (lack of false positives,\nthe sphere of resistance to refactoring)\nAnother way to think of false positives and false negatives is in terms of signal-to-noise\nratio. As you can see from the formula in figure 4.4, there are two ways to improve test\nTable of error types\nType II error\n(false negative)\nCorrect inference\n(true positives)\nType I error\n(false positive)\nCorrect inference\n(true negatives)\nResistance to\nrefactoring\nTest\nresult\nTest fails\nTest passes\nCorrect\nFunctionality is\nBroken\nProtection\nagainst\nregressions\nFigure 4.3\nThe relationship between protection against regressions and resistance to \nrefactoring. Protection against regressions guards against false negatives (type II errors). \nResistance to refactoring minimizes the number of false positives (type I errors).\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2681,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 100,
      "content": "78\nCHAPTER 4\nThe four pillars of a good unit test\naccuracy. The first is to increase the numerator, signal: that is, make the test better at\nfinding regressions. The second is to reduce the denominator, noise: make the test bet-\nter at not raising false alarms.\n Both are critically important. There’s no use for a test that isn’t capable of finding\nany bugs, even if it doesn’t raise false alarms. Similarly, the test’s accuracy goes to zero\nwhen it generates a lot of noise, even if it’s capable of finding all the bugs in code.\nThese findings are simply lost in the sea of irrelevant information. \n4.2.2\nThe importance of false positives and false negatives: \nThe dynamics\nIn the short term, false positives are not as bad as false negatives. In the beginning of a\nproject, receiving a wrong warning is not that big a deal as opposed to not being\nwarned at all and running the risk of a bug slipping into production. But as the proj-\nect grows, false positives start to have an increasingly large effect on the test suite\n(figure 4.5).\nTest accuracy =\nNoise (number of false alarms raised)\nSignal (number of bugs found)\nFigure 4.4\nA test is accurate insofar as it generates a \nstrong signal (is capable of finding bugs) with as little \nnoise (false alarms) as possible.\nEﬀect on the\ntest suite\nProject duration\nFalse negatives\nFalse positives\nFigure 4.5\nFalse positives (false alarms) don’t have as much of a \nnegative effect in the beginning. But they become increasingly \nimportant as the project grows—as important as false negatives \n(unnoticed bugs).\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1610,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 101,
      "content": "79\nThe third and fourth pillars: Fast feedback and maintainability\nWhy are false positives not as important initially? Because the importance of refactor-\ning is also not immediate; it increases gradually over time. You don’t need to conduct\nmany code clean-ups in the beginning of the project. Newly written code is often shiny\nand flawless. It’s also still fresh in your memory, so you can easily refactor it even if\ntests raise false alarms.\n But as time goes on, the code base deteriorates. It becomes increasingly complex\nand disorganized. Thus you have to start conducting regular refactorings in order to\nmitigate this tendency. Otherwise, the cost of introducing new features eventually\nbecomes prohibitive.\n As the need for refactoring increases, the importance of resistance to refactoring in\ntests increases with it. As I explained earlier, you can’t refactor when the tests keep cry-\ning “wolf” and you keep getting warnings about bugs that don’t exist. You quickly lose\ntrust in such tests and stop viewing them as a reliable source of feedback.\n Despite the importance of protecting your code against false positives, especially in\nthe later project stages, few developers perceive false positives this way. Most people\ntend to focus solely on improving the first attribute of a good unit test—protection\nagainst regressions, which is not enough to build a valuable, highly accurate test suite\nthat helps sustain project growth.\n The reason, of course, is that far fewer projects get to those later stages, mostly\nbecause they are small and the development finishes before the project becomes too\nbig. Thus developers face the problem of unnoticed bugs more often than false\nalarms that swarm the project and hinder all refactoring undertakings. And so, people\noptimize accordingly. Nevertheless, if you work on a medium to large project, you\nhave to pay equal attention to both false negatives (unnoticed bugs) and false posi-\ntives (false alarms). \n4.3\nThe third and fourth pillars: Fast feedback \nand maintainability\nIn this section, I talk about the two remaining pillars of a good unit test:\nFast feedback\nMaintainability\nAs you may remember from chapter 2, fast feedback is an essential property of a unit\ntest. The faster the tests, the more of them you can have in the suite and the more\noften you can run them.\n With tests that run quickly, you can drastically shorten the feedback loop, to the\npoint where the tests begin to warn you about bugs as soon as you break the code, thus\nreducing the cost of fixing those bugs almost to zero. On the other hand, slow tests\ndelay the feedback and potentially prolong the period during which the bugs remain\nunnoticed, thus increasing the cost of fixing them. That’s because slow tests discour-\nage you from running them often, and therefore lead to wasting more time moving in\na wrong direction.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2915,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 102,
      "content": "80\nCHAPTER 4\nThe four pillars of a good unit test\n Finally, the fourth pillar of good units tests, the maintainability metric, evaluates\nmaintenance costs. This metric consists of two major components:\nHow hard it is to understand the test—This component is related to the size of the\ntest. The fewer lines of code in the test, the more readable the test is. It’s also\neasier to change a small test when needed. Of course, that’s assuming you don’t\ntry to compress the test code artificially just to reduce the line count. The qual-\nity of the test code matters as much as the production code. Don’t cut corners\nwhen writing tests; treat the test code as a first-class citizen.\nHow hard it is to run the test—If the test works with out-of-process dependencies,\nyou have to spend time keeping those dependencies operational: reboot the\ndatabase server, resolve network connectivity issues, and so on. \n4.4\nIn search of an ideal test\nHere are the four attributes of a good unit test once again:\nProtection against regressions\nResistance to refactoring\nFast feedback\nMaintainability\nThese four attributes, when multiplied together, determine the value of a test. And by\nmultiplied, I mean in a mathematical sense; that is, if a test gets zero in one of the attri-\nbutes, its value turns to zero as well:\nValue estimate = [0..1] * [0..1] * [0..1] * [0..1]\nTIP\nIn order to be valuable, the test needs to score at least something in all\nfour categories.\nOf course, it’s impossible to measure these attributes precisely. There’s no code analy-\nsis tool you can plug a test into and get the exact numbers. But you can still evaluate\nthe test pretty accurately to see where a test stands with regard to the four attributes.\nThis evaluation, in turn, gives you the test’s value estimate, which you can use to\ndecide whether to keep the test in the suite.\n Remember, all code, including test code, is a liability. Set a fairly high threshold\nfor the minimum required value, and only allow tests in the suite if they meet this\nthreshold. A small number of highly valuable tests will do a much better job sustain-\ning project growth than a large number of mediocre tests.\n I’ll show some examples shortly. For now, let’s examine whether it’s possible to cre-\nate an ideal test.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2322,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 103,
      "content": "81\nIn search of an ideal test\n4.4.1\nIs it possible to create an ideal test?\nAn ideal test is a test that scores the maximum in all four attributes. If you take the\nminimum and maximum values as 0 and 1 for each of the attributes, an ideal test must\nget 1 in all of them.\n Unfortunately, it’s impossible to create such an ideal test. The reason is that the\nfirst three attributes—protection against regressions, resistance to refactoring, and fast feedback—\nare mutually exclusive. It’s impossible to maximize them all: you have to sacrifice one\nof the three in order to max out the remaining two.\n Moreover, because of the multiplication principle (see the calculation of the value\nestimate in the previous section), it’s even trickier to keep the balance. You can’t just\nforgo one of the attributes in order to focus on the others. As I mentioned previously,\na test that scores zero in one of the four categories is worthless. Therefore, you have to\nmaximize these attributes in such a way that none of them is diminished too much.\nLet’s look at some examples of tests that aim at maximizing two out of three attributes\nat the expense of the third and, as a result, have a value that’s close to zero. \n4.4.2\nExtreme case #1: End-to-end tests\nThe first example is end-to-end tests. As you may remember from chapter 2, end-to-end\ntests look at the system from the end user’s perspective. They normally go through all of\nthe system’s components, including the UI, database, and external applications.\n Since end-to-end tests exercise a lot of code, they provide the best protection\nagainst regressions. In fact, of all types of tests, end-to-end tests exercise the most\ncode—both your code and the code you didn’t write but use in the project, such as\nexternal libraries, frameworks, and third-party applications.\n End-to-end tests are also immune to false positives and thus have a good resistance\nto refactoring. A refactoring, if done correctly, doesn’t change the system’s observable\nbehavior and therefore doesn’t affect the end-to-end tests. That’s another advantage\nof such tests: they don’t impose any particular implementation. The only thing end-to-\nend tests look at is how a feature behaves from the end user’s point of view. They are\nas removed from implementation details as tests could possibly be.\n However, despite these benefits, end-to-end tests have a major drawback: they are\nslow. Any system that relies solely on such tests would have a hard time getting rapid\nfeedback. And that is a deal-breaker for many development teams. This is why it’s\npretty much impossible to cover your code base with only end-to-end tests.\n Figure 4.6 shows where end-to-end tests stand with regard to the first three unit\ntesting metrics. Such tests provide great protection against both regression errors and\nfalse positives, but lack speed. \nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2895,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 104,
      "content": "82\nCHAPTER 4\nThe four pillars of a good unit test\n4.4.3\nExtreme case #2: Trivial tests\nAnother example of maximizing two out of three attributes at the expense of the third\nis a trivial test. Such tests cover a simple piece of code, something that is unlikely to\nbreak because it’s too trivial, as shown in the following listing.\npublic class User\n{\npublic string Name { get; set; }    \n}\n[Fact]\npublic void Test()\n{\nvar sut = new User();\nsut.Name = \"John Smith\";\nAssert.Equal(\"John Smith\", sut.Name);\n}\nUnlike end-to-end tests, trivial tests do provide fast feedback—they run very quickly.\nThey also have a fairly low chance of producing a false positive, so they have good\nresistance to refactoring. Trivial tests are unlikely to reveal any regressions, though,\nbecause there’s not much room for a mistake in the underlying code.\n Trivial tests taken to an extreme result in tautology tests. They don’t test anything\nbecause they are set up in such a way that they always pass or contain semantically\nmeaningless assertions.\nListing 4.5\nTrivial test covering a simple piece of code\nResistance to\nrefactoring\nFast\nfeedback\nProtection\nagainst\nregressions\nEnd-to-end tests\nFigure 4.6\nEnd-to-end tests \nprovide great protection against \nboth regression errors and false \npositives, but they fail at the \nmetric of fast feedback.\nOne-liners like \nthis are unlikely \nto contain bugs.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1431,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 105,
      "content": "83\nIn search of an ideal test\nFigure 4.7 shows where trivial tests stand. They have good resistance to refactoring\nand provide fast feedback, but they don’t protect you from regressions. \n4.4.4\nExtreme case #3: Brittle tests\nSimilarly, it’s pretty easy to write a test that runs fast and has a good chance of catching\na regression but does so with a lot of false positives. Such a test is called a brittle test: it\ncan’t withstand a refactoring and will turn red regardless of whether the underlying\nfunctionality is broken.\n You already saw an example of a brittle test in listing 4.2. Here’s another one.\npublic class UserRepository\n{\npublic User GetById(int id)\n{\n/* ... */\n}\npublic string LastExecutedSqlStatement { get; set; }\n}\n[Fact]\npublic void GetById_executes_correct_SQL_code()\n{\nvar sut = new UserRepository();\nUser user = sut.GetById(5);\nAssert.Equal(\n\"SELECT * FROM dbo.[User] WHERE UserID = 5\",\nsut.LastExecutedSqlStatement);\n}\nListing 4.6\nTest verifying which SQL statement is executed\nResistance to\nrefactoring\nFast\nfeedback\nProtection\nagainst\nregressions\nEnd-to-end tests\nTrivial tests\nFigure 4.7\nTrivial tests have good \nresistance to refactoring, and they \nprovide fast feedback, but such tests \ndon’t protect you from regressions.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1303,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 106,
      "content": "84\nCHAPTER 4\nThe four pillars of a good unit test\nThis test makes sure the UserRepository class generates a correct SQL statement\nwhen fetching a user from the database. Can this test catch a bug? It can. For exam-\nple, a developer can mess up the SQL code generation and mistakenly use ID instead\nof UserID, and the test will point that out by raising a failure. But does this test have\ngood resistance to refactoring? Absolutely not. Here are different variations of the\nSQL statement that lead to the same result:\nSELECT * FROM dbo.[User] WHERE UserID = 5\nSELECT * FROM dbo.User WHERE UserID = 5\nSELECT UserID, Name, Email FROM dbo.[User] WHERE UserID = 5\nSELECT * FROM dbo.[User] WHERE UserID = @UserID\nThe test in listing 4.6 will turn red if you change the SQL script to any of these varia-\ntions, even though the functionality itself will remain operational. This is once again\nan example of coupling the test to the SUT’s internal implementation details. The test\nis focusing on hows instead of whats and thus ingrains the SUT’s implementation\ndetails, preventing any further refactoring.\n Figure 4.8 shows that brittle tests fall into the third bucket. Such tests run fast and\nprovide good protection against regressions but have little resistance to refactoring. \n4.4.5\nIn search of an ideal test: The results\nThe first three attributes of a good unit test (protection against regressions, resistance to\nrefactoring, and fast feedback) are mutually exclusive. While it’s quite easy to come up\nwith a test that maximizes two out of these three attributes, you can only do that at the\nexpense of the third. Still, such a test would have a close-to-zero value due to the mul-\ntiplication rule. Unfortunately, it’s impossible to create an ideal test that has a perfect\nscore in all three attributes (figure 4.9).\nResistance to\nrefactoring\nFast\nfeedback\nProtection\nagainst\nregressions\nEnd-to-end tests\nTrivial tests\nBrittle tests\nFigure 4.8\nBrittle tests run fast and they \nprovide good protection against regressions, \nbut they have little resistance to refactoring.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2124,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 107,
      "content": "85\nIn search of an ideal test\nThe fourth attribute, maintainability, is not correlated to the first three, with the excep-\ntion of end-to-end tests. End-to-end tests are normally larger in size because of the\nnecessity to set up all the dependencies such tests reach out to. They also require addi-\ntional effort to keep those dependencies operational. Hence end-to-end tests tend to\nbe more expensive in terms of maintenance costs.\n It’s hard to keep a balance between the attributes of a good test. A test can’t have\nthe maximum score in each of the first three categories, and you also have to keep an\neye on the maintainability aspect so the test remains reasonably short and simple.\nTherefore, you have to make trade-offs. Moreover, you should make those trade-offs\nin such a way that no particular attribute turns to zero. The sacrifices have to be par-\ntial and strategic.\n What should those sacrifices look like? Because of the mutual exclusiveness of pro-\ntection against regressions, resistance to refactoring, and fast feedback, you may think that the\nbest strategy is to concede a little bit of each: just enough to make room for all three\nattributes.\n In reality, though, resistance to refactoring is non-negotiable. You should aim at gain-\ning as much of it as you can, provided that your tests remain reasonably quick and you\ndon’t resort to the exclusive use of end-to-end tests. The trade-off, then, comes down\nto the choice between how good your tests are at pointing out bugs and how fast they\ndo that: that is, between protection against regressions and fast feedback. You can view this\nchoice as a slider that can be freely moved between protection against regressions and\nfast feedback. The more you gain in one attribute, the more you lose on the other\n(see figure 4.10).\n The reason resistance to refactoring is non-negotiable is that whether a test possesses\nthis attribute is mostly a binary choice: the test either has resistance to refactoring or it\ndoesn’t. There are almost no intermediate stages in between. Thus you can’t concede\nResistance to\nrefactoring\nFast\nfeedback\nProtection\nagainst\nregressions\nUnreachable ideal\nFigure 4.9\nIt’s impossible to create an \nideal test that would have a perfect score \nin all three attributes.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2312,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 108,
      "content": "86\nCHAPTER 4\nThe four pillars of a good unit test\njust a little resistance to refactoring: you’ll have to lose it all. On the other hand, the metrics\nof protection against regressions and fast feedback are more malleable. You will see in the\nnext section what kind of trade-offs are possible when you choose one over the other.\nTIP\nEradicating brittleness (false positives) in tests is the first priority on the\npath to a robust test suite.\nThe CAP theorem\nThe trade-off between the first three attributes of a good unit test is similar to the\nCAP theorem. The CAP theorem states that it is impossible for a distributed data\nstore to simultaneously provide more than two of the following three guarantees:\nConsistency, which means every read receives the most recent write or an error.\nAvailability, which means every request receives a response (apart from out-\nages that affect all nodes in the system).\nPartition tolerance, which means the system continues to operate despite\nnetwork partitioning (losing connection between network nodes).\nThe similarity is two-fold:\nFirst, there is the two-out-of-three trade-off.\nSecond, the partition tolerance component in large-scale distributed systems is\nalso non-negotiable. A large application such as, for example, the Amazon web-\nsite can’t operate on a single machine. The option of preferring consistency and\navailability at the expense of partition tolerance simply isn’t on the table—Amazon\nhas too much data to store on a single server, however big that server is.\nrefactoring\nMax\nout\nProtection against\nregressions\nFast feedback\nMax\nout\nMaintainability\nChoose between the two\nResistance to\nFigure 4.10\nThe best tests exhibit maximum maintainability and resistance \nto refactoring; always try to max out these two attributes. The trade-off \ncomes down to the choice between protection against regressions and fast \nfeedback.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1934,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 109,
      "content": "87\nExploring well-known test automation concepts\n4.5\nExploring well-known test automation concepts\nThe four attributes of a good unit test shown earlier are foundational. All existing,\nwell-known test automation concepts can be traced back to these four attributes. In\nthis section, we’ll look at two such concepts: the Test Pyramid and white-box versus\nblack-box testing.\n4.5.1\nBreaking down the Test Pyramid\nThe Test Pyramid is a concept that advocates for a certain ratio of different types of\ntests in the test suite (figure 4.11):\nUnit tests\nIntegration tests\nEnd-to-end tests\nThe Test Pyramid is often represented visually as a pyramid with those three types of\ntests in it. The width of the pyramid layers refers to the prevalence of a particular type\nThe choice, then, also boils down to a trade-off between consistency and availability.\nIn some parts of the system, it’s preferable to concede a little consistency to gain\nmore availability. For example, when displaying a product catalog, it’s generally fine\nif some parts of the catalog are out of date. Availability is of higher priority in this sce-\nnario. On the other hand, when updating a product description, consistency is more\nimportant than availability: network nodes must have a consensus on what the most\nrecent version of that description is, in order to avoid merge conflicts. \nEnd-\nto-end\nIntegration\ntests\nUnit tests\nTest count\nEmulating\nuser\nFigure 4.11\nThe Test Pyramid advocates for a certain ratio of unit, \nintegration, and end-to-end tests.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1577,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 110,
      "content": "88\nCHAPTER 4\nThe four pillars of a good unit test\nof test in the suite. The wider the layer, the greater the test count. The height of the\nlayer is a measure of how close these tests are to emulating the end user’s behavior.\nEnd-to-end tests are at the top—they are the closest to imitating the user experience.\nDifferent types of tests in the pyramid make different choices in the trade-off between\nfast feedback and protection against regressions. Tests in higher pyramid layers favor protec-\ntion against regressions, while lower layers emphasize execution speed (figure 4.12).\nNotice that neither layer gives up resistance to refactoring. Naturally, end-to-end and inte-\ngration tests score higher on this metric than unit tests, but only as a side effect of\nbeing more detached from the production code. Still, even unit tests should not con-\ncede resistance to refactoring. All tests should aim at producing as few false positives as\npossible, even when working directly with the production code. (How to do that is the\ntopic of the next chapter.)\n The exact mix between types of tests will be different for each team and project.\nBut in general, it should retain the pyramid shape: end-to-end tests should be the\nminority; unit tests, the majority; and integration tests somewhere in the middle.\n The reason end-to-end tests are the minority is, again, the multiplication rule\ndescribed in section 4.4. End-to-end tests score extremely low on the metric of fast feed-\nback. They also lack maintainability: they tend to be larger in size and require addi-\ntional effort to maintain the involved out-of-process dependencies. Thus, end-to-end\ntests only make sense when applied to the most critical functionality—features in\nrefactoring\nMax\nout\nProtection against\nregressions\nFast feedback\nEnd-to-end\nIntegration\nUnit tests\nResistance to\nFigure 4.12\nDifferent types of tests in the pyramid make different choices \nbetween fast feedback and protection against regressions. End-to-end tests \nfavor protection against regressions, unit tests emphasize fast feedback, and \nintegration tests lie in the middle.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2161,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 111,
      "content": "89\nExploring well-known test automation concepts\nwhich you don’t ever want to see any bugs—and only when you can’t get the same\ndegree of protection with unit or integration tests. The use of end-to-end tests for any-\nthing else shouldn’t pass your minimum required value threshold. Unit tests are usu-\nally more balanced, and hence you normally have many more of them.\n There are exceptions to the Test Pyramid. For example, if all your application does\nis basic create, read, update, and delete (CRUD) operations with very few business\nrules or any other complexity, your test “pyramid” will most likely look like a rectangle\nwith an equal number of unit and integration tests and no end-to-end tests.\n Unit tests are less useful in a setting without algorithmic or business complexity—\nthey quickly descend into trivial tests. At the same time, integration tests retain their\nvalue—it’s still important to verify how code, however simple it is, works in integration\nwith other subsystems, such as the database. As a result, you may end up with fewer\nunit tests and more integration tests. In the most trivial examples, the number of inte-\ngration tests may even be greater than the number of unit tests.\n Another exception to the Test Pyramid is an API that reaches out to a single out-of-\nprocess dependency—say, a database. Having more end-to-end tests may be a viable\noption for such an application. Since there’s no user interface, end-to-end tests will\nrun reasonably fast. The maintenance costs won’t be too high, either, because you\nonly work with the single external dependency, the database. Basically, end-to-end\ntests are indistinguishable from integration tests in this environment. The only thing\nthat differs is the entry point: end-to-end tests require the application to be hosted\nsomewhere to fully emulate the end user, while integration tests normally host the\napplication in the same process. We’ll get back to the Test Pyramid in chapter 8, when\nwe’ll be talking about integration testing. \n4.5.2\nChoosing between black-box and white-box testing\nThe other well-known test automation concept is black-box versus white-box testing.\nIn this section, I show when to use each of the two approaches:\nBlack-box testing is a method of software testing that examines the functionality\nof a system without knowing its internal structure. Such testing is normally built\naround specifications and requirements: what the application is supposed to do,\nrather than how it does it.\nWhite-box testing is the opposite of that. It’s a method of testing that verifies the\napplication’s inner workings. The tests are derived from the source code, not\nrequirements or specifications.\nThere are pros and cons to both of these methods. White-box testing tends to be more\nthorough. By analyzing the source code, you can uncover a lot of errors that you may\nmiss when relying solely on external specifications. On the other hand, tests resulting\nfrom white-box testing are often brittle, as they tend to tightly couple to the specific\nimplementation of the code under test. Such tests produce many false positives and\nthus fall short on the metric of resistance to refactoring. They also often can’t be traced\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 3262,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 112,
      "content": "90\nCHAPTER 4\nThe four pillars of a good unit test\nback to a behavior that is meaningful to a business person, which is a strong sign that\nthese tests are fragile and don’t add much value. Black-box testing provides the oppo-\nsite set of pros and cons (table 4.1).\nAs you may remember from section 4.4.5, you can’t compromise on resistance to refac-\ntoring: a test either possesses resistance to refactoring or it doesn’t. Therefore, choose black-\nbox testing over white-box testing by default. Make all tests—be they unit, integration, or\nend-to-end—view the system as a black box and verify behavior meaningful to the\nproblem domain. If you can’t trace a test back to a business requirement, it’s an indi-\ncation of the test’s brittleness. Either restructure or delete this test; don’t let it into the\nsuite as-is. The only exception is when the test covers utility code with high algorith-\nmic complexity (more on this in chapter 7).\n Note that even though black-box testing is preferable when writing tests, you can\nstill use the white-box method when analyzing the tests. Use code coverage tools to see which\ncode branches are not exercised, but then turn around and test them as if you know nothing about\nthe code’s internal structure. Such a combination of the white-box and black-box meth-\nods works best. \nSummary\nA good unit test has four foundational attributes that you can use to analyze any\nautomated test, whether unit, integration, or end-to-end:\n– Protection against regressions\n– Resistance to refactoring\n– Fast feedback\n– Maintainability\nProtection against regressions is a measure of how good the test is at indicating the\npresence of bugs (regressions). The more code the test executes (both your\ncode and the code of libraries and frameworks used in the project), the higher\nthe chance this test will reveal a bug.\nResistance to refactoring is the degree to which a test can sustain application code\nrefactoring without producing a false positive.\nA false positive is a false alarm—a result indicating that the test fails, whereas\nthe functionality it covers works as intended. False positives can have a devastat-\ning effect on the test suite:\n– They dilute your ability and willingness to react to problems in code, because\nyou get accustomed to false alarms and stop paying attention to them.\nTable 4.1\nThe pros and cons of white-box and black-box testing\nProtection against regressions\nResistance to refactoring\nWhite-box testing\nGood\nBad\nBlack-box testing\nBad\nGood\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2547,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 113,
      "content": "91\nSummary\n– They diminish your perception of tests as a reliable safety net and lead to los-\ning trust in the test suite.\nFalse positives are a result of tight coupling between tests and the internal imple-\nmentation details of the system under test. To avoid such coupling, the test\nmust verify the end result the SUT produces, not the steps it took to do that.\nProtection against regressions and resistance to refactoring contribute to test accuracy.\nA test is accurate insofar as it generates a strong signal (is capable of finding\nbugs, the sphere of protection against regressions) with as little noise (false posi-\ntives) as possible (the sphere of resistance to refactoring).\nFalse positives don’t have as much of a negative effect in the beginning of the\nproject, but they become increasingly important as the project grows: as import-\nant as false negatives (unnoticed bugs).\nFast feedback is a measure of how quickly the test executes.\nMaintainability consists of two components:\n– How hard it is to understand the test. The smaller the test, the more read-\nable it is.\n– How hard it is to run the test. The fewer out-of-process dependencies the test\nreaches out to, the easier it is to keep them operational.\nA test’s value estimate is the product of scores the test gets in each of the four attri-\nbutes. If the test gets zero in one of the attributes, its value turns to zero as well.\nIt’s impossible to create a test that gets the maximum score in all four attri-\nbutes, because the first three—protection against regressions, resistance to refactor-\ning, and fast feedback—are mutually exclusive. The test can only maximize two\nout of the three.\nResistance to refactoring is non-negotiable because whether a test possess this attri-\nbute is mostly a binary choice: the test either has resistance to refactoring or it\ndoesn’t. The trade-off between the attributes comes down to the choice\nbetween protection against regressions and fast feedback.\nThe Test Pyramid advocates for a certain ratio of unit, integration, and end-to-\nend tests: end-to-end tests should be in the minority, unit tests in the majority,\nand integration tests somewhere in the middle.\nDifferent types of tests in the pyramid make different choices between fast feed-\nback and protection against regressions. End-to-end tests favor protection against\nregressions, while unit tests favor fast feedback.\nUse the black-box testing method when writing tests. Use the white-box method\nwhen analyzing the tests.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2555,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 114,
      "content": "92\nMocks and test fragility\nChapter 4 introduced a frame of reference that you can use to analyze specific tests\nand unit testing approaches. In this chapter, you’ll see that frame of reference in\naction; we’ll use it to dissect the topic of mocks.\n The use of mocks in tests is a controversial subject. Some people argue that\nmocks are a great tool and apply them in most of their tests. Others claim that mocks\nlead to test fragility and try not to use them at all. As the saying goes, the truth lies\nsomewhere in between. In this chapter, I’ll show that, indeed, mocks often result in\nfragile tests—tests that lack the metric of resistance to refactoring. But there are still\ncases where mocking is applicable and even preferable.\nThis chapter covers\nDifferentiating mocks from stubs\nDefining observable behavior and implementation \ndetails\nUnderstanding the relationship between mocks \nand test fragility\nUsing mocks without compromising resistance \nto refactoring\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1024,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 115,
      "content": "93\nDifferentiating mocks from stubs\n This chapter draws heavily on the discussion about the London versus classical\nschools of unit testing from chapter 2. In short, the disagreement between the schools\nstems from their views on the test isolation issue. The London school advocates isolat-\ning pieces of code under test from each other and using test doubles for all but\nimmutable dependencies to perform such isolation.\n The classical school stands for isolating unit tests themselves so that they can be\nrun in parallel. This school uses test doubles only for dependencies that are shared\nbetween tests.\n There’s a deep and almost inevitable connection between mocks and test fragility.\nIn the next several sections, I will gradually lay down the foundation for you to see why\nthat connection exists. You will also learn how to use mocks so that they don’t compro-\nmise a test’s resistance to refactoring.\n5.1\nDifferentiating mocks from stubs\nIn chapter 2, I briefly mentioned that a mock is a test double that allows you to exam-\nine interactions between the system under test (SUT) and its collaborators. There’s\nanother type of test double: a stub. Let’s take a closer look at what a mock is and how it\nis different from a stub.\n5.1.1\nThe types of test doubles\nA test double is an overarching term that describes all kinds of non-production-ready,\nfake dependencies in tests. The term comes from the notion of a stunt double in a\nmovie. The major use of test doubles is to facilitate testing; they are passed to the\nsystem under test instead of real dependencies, which could be hard to set up or\nmaintain.\n According to Gerard Meszaros, there are five variations of test doubles: dummy,\nstub, spy, mock, and fake.1 Such a variety can look intimidating, but in reality, they can all\nbe grouped together into just two types: mocks and stubs (figure 5.1).\n1 See xUnit Test Patterns: Refactoring Test Code (Addison-Wesley, 2007).\nTest double\nMock\n(mock, spy)\nStub\n(stub, dummy, fake)\nFigure 5.1\nAll variations of test \ndoubles can be categorized into \ntwo types: mocks and stubs.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2134,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 116,
      "content": "94\nCHAPTER 5\nMocks and test fragility\nThe difference between these two types boils down to the following:\nMocks help to emulate and examine outcoming interactions. These interactions\nare calls the SUT makes to its dependencies to change their state.\nStubs help to emulate incoming interactions. These interactions are calls the\nSUT makes to its dependencies to get input data (figure 5.2).\nAll other differences between the five variations are insignificant implementation\ndetails. For example, spies serve the same role as mocks. The distinction is that spies\nare written manually, whereas mocks are created with the help of a mocking frame-\nwork. Sometimes people refer to spies as handwritten mocks.\n On the other hand, the difference between a stub, a dummy, and a fake is in how\nintelligent they are. A dummy is a simple, hardcoded value such as a null value or a\nmade-up string. It’s used to satisfy the SUT’s method signature and doesn’t partici-\npate in producing the final outcome. A stub is more sophisticated. It’s a fully fledged\ndependency that you configure to return different values for different scenarios.\nFinally, a fake is the same as a stub for most purposes. The difference is in the ratio-\nnale for its creation: a fake is usually implemented to replace a dependency that\ndoesn’t yet exist.\n Notice the difference between mocks and stubs (aside from outcoming versus\nincoming interactions). Mocks help to emulate and examine interactions between the\nSUT and its dependencies, while stubs only help to emulate those interactions. This is\nan important distinction. You will see why shortly. \n5.1.2\nMock (the tool) vs. mock (the test double)\nThe term mock is overloaded and can mean different things in different circum-\nstances. I mentioned in chapter 2 that people often use this term to mean any test\ndouble, whereas mocks are only a subset of test doubles. But there’s another meaning\nSystem under test\nSMTP server\nSend an email\nRetrieve data\nDatabase\nStub\nMock\nFigure 5.2\nSending an email is \nan outcoming interaction: an inter-\naction that results in a side effect \nin the SMTP server. A test double \nemulating such an interaction is \na mock. Retrieving data from the \ndatabase is an incoming inter-\naction; it doesn’t result in a \nside effect. The corresponding \ntest double is a stub.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2366,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 117,
      "content": "95\nDifferentiating mocks from stubs\nfor the term mock. You can refer to the classes from mocking libraries as mocks, too.\nThese classes help you create actual mocks, but they themselves are not mocks per se.\nThe following listing shows an example.\n[Fact]\npublic void Sending_a_greetings_email()\n{\nvar mock = new Mock<IEmailGateway>();      \nvar sut = new Controller(mock.Object);\nsut.GreetUser(\"user@email.com\");\nmock.Verify(\n   \nx => x.SendGreetingsEmail(   \n\"user@email.com\"),\n   \nTimes.Once);\n   \n}\nThe test in listing 5.1 uses the Mock class from the mocking library of my choice\n(Moq). This class is a tool that enables you to create a test double—a mock. In other\nwords, the class Mock (or Mock<IEmailGateway>) is a mock (the tool), while the instance\nof that class, mock, is a mock (the test double). It’s important not to conflate a mock (the\ntool) with a mock (the test double) because you can use a mock (the tool) to create\nboth types of test doubles: mocks and stubs.\n The test in the following listing also uses the Mock class, but the instance of that\nclass is not a mock, it’s a stub.\n[Fact]\npublic void Creating_a_report()\n{\nvar stub = new Mock<IDatabase>();       \nstub.Setup(x => x.GetNumberOfUsers())     \n.Returns(10);\n     \nvar sut = new Controller(stub.Object);\nReport report = sut.CreateReport();\nAssert.Equal(10, report.NumberOfUsers);\n}\nThis test double emulates an incoming interaction—a call that provides the SUT with\ninput data. On the other hand, in the previous example (listing 5.1), the call to Send-\nGreetingsEmail() is an outcoming interaction. Its sole purpose is to incur a side\neffect—send an email. \nListing 5.1\nUsing the Mock class from a mocking library to create a mock\nListing 5.2\nUsing the Mock class to create a stub\nUses a mock (the \ntool) to create a mock \n(the test double)\nExamines the call \nfrom the SUT to \nthe test double\nUses a mock \n(the tool) to \ncreate a stub\nSets up a \ncanned answer\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1992,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 118,
      "content": "96\nCHAPTER 5\nMocks and test fragility\n5.1.3\nDon’t assert interactions with stubs\nAs I mentioned in section 5.1.1, mocks help to emulate and examine outcoming interac-\ntions between the SUT and its dependencies, while stubs only help to emulate incom-\ning interactions, not examine them. The difference between the two stems from the\nguideline of never asserting interactions with stubs. A call from the SUT to a stub is not\npart of the end result the SUT produces. Such a call is only a means to produce the\nend result: a stub provides input from which the SUT then generates the output.\nNOTE\nAsserting interactions with stubs is a common anti-pattern that leads to\nfragile tests.\nAs you might remember from chapter 4, the only way to avoid false positives and thus\nimprove resistance to refactoring in tests is to make those tests verify the end result\n(which, ideally, should be meaningful to a non-programmer), not implementation\ndetails. In listing 5.1, the check\nmock.Verify(x => x.SendGreetingsEmail(\"user@email.com\"))\ncorresponds to an actual outcome, and that outcome is meaningful to a domain\nexpert: sending a greetings email is something business people would want the system\nto do. At the same time, the call to GetNumberOfUsers() in listing 5.2 is not an out-\ncome at all. It’s an internal implementation detail regarding how the SUT gathers\ndata necessary for the report creation. Therefore, asserting this call would lead to test\nfragility: it shouldn’t matter how the SUT generates the end result, as long as that\nresult is correct. The following listing shows an example of such a brittle test.\n[Fact]\npublic void Creating_a_report()\n{\nvar stub = new Mock<IDatabase>();\nstub.Setup(x => x.GetNumberOfUsers()).Returns(10);\nvar sut = new Controller(stub.Object);\nReport report = sut.CreateReport();\nAssert.Equal(10, report.NumberOfUsers);\nstub.Verify(\n   \nx => x.GetNumberOfUsers(),   \nTimes.Once);\n   \n}\nThis practice of verifying things that aren’t part of the end result is also called over-\nspecification. Most commonly, overspecification takes place when examining interac-\ntions. Checking for interactions with stubs is a flaw that’s quite easy to spot because\ntests shouldn’t check for any interactions with stubs. Mocks are a more complicated sub-\nListing 5.3\nAsserting an interaction with a stub\nAsserts the \ninteraction \nwith the stub\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2410,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 119,
      "content": "97\nDifferentiating mocks from stubs\nject: not all uses of mocks lead to test fragility, but a lot of them do. You’ll see why later\nin this chapter. \n5.1.4\nUsing mocks and stubs together\nSometimes you need to create a test double that exhibits the properties of both a\nmock and a stub. For example, here’s a test from chapter 2 that I used to illustrate the\nLondon style of unit testing.\n[Fact]\npublic void Purchase_fails_when_not_enough_inventory()\n{\nvar storeMock = new Mock<IStore>();\nstoreMock\n    \n.Setup(x => x.HasEnoughInventory(    \nProduct.Shampoo, 5))\n    \n.Returns(false);\n    \nvar sut = new Customer();\nbool success = sut.Purchase(\nstoreMock.Object, Product.Shampoo, 5);\nAssert.False(success);\nstoreMock.Verify(\n   \nx => x.RemoveInventory(Product.Shampoo, 5),  \nTimes.Never);\n   \n}\nThis test uses storeMock for two purposes: it returns a canned answer and verifies a\nmethod call made by the SUT. Notice, though, that these are two different methods:\nthe test sets up the answer from HasEnoughInventory() but then verifies the call to\nRemoveInventory(). Thus, the rule of not asserting interactions with stubs is not vio-\nlated here.\n When a test double is both a mock and a stub, it’s still called a mock, not a stub.\nThat’s mostly the case because we need to pick one name, but also because being a\nmock is a more important fact than being a stub. \n5.1.5\nHow mocks and stubs relate to commands and queries\nThe notions of mocks and stubs tie to the command query separation (CQS) princi-\nple. The CQS principle states that every method should be either a command or a\nquery, but not both. As shown in figure 5.3, commands are methods that produce side\neffects and don’t return any value (return void). Examples of side effects include\nmutating an object’s state, changing a file in the file system, and so on. Queries are the\nopposite of that—they are side-effect free and return a value.\n To follow this principle, be sure that if a method produces a side effect, that\nmethod’s return type is void. And if the method returns a value, it must stay side-effect\nListing 5.4\nstoreMock: both a mock and a stub\nSets up a \ncanned \nanswer\nExamines a call \nfrom the SUT\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2224,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 120,
      "content": "98\nCHAPTER 5\nMocks and test fragility\nfree. In other words, asking a question should not change the answer. Code that main-\ntains such a clear separation becomes easier to read. You can tell what a method does\njust by looking at its signature, without diving into its implementation details.\n Of course, it’s not always possible to follow the CQS principle. There are always\nmethods for which it makes sense to both incur a side effect and return a value. A clas-\nsical example is stack.Pop(). This method both removes a top element from the\nstack and returns it to the caller. Still, it’s a good idea to adhere to the CQS principle\nwhenever you can.\n Test doubles that substitute commands become mocks. Similarly, test doubles that\nsubstitute queries are stubs. Look at the two tests from listings 5.1 and 5.2 again (I’m\nshowing their relevant parts here):\nvar mock = new Mock<IEmailGateway>();\nmock.Verify(x => x.SendGreetingsEmail(\"user@email.com\"));\nvar stub = new Mock<IDatabase>();\nstub.Setup(x => x.GetNumberOfUsers()).Returns(10);\nSendGreetingsEmail() is a command whose side effect is sending an email. The test\ndouble that substitutes this command is a mock. On the other hand, GetNumberOf-\nUsers() is a query that returns a value and doesn’t mutate the database state. The cor-\nresponding test double is a stub. \n \nMethods\nCommands\nIncur side effects\nNo return value\nMocks\nQueries\nSide-effect free\nReturns a value\nStubs\nFigure 5.3\nIn the command query \nseparation (CQS) principle, commands \ncorrespond to mocks, while queries are \nconsistent with stubs.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1616,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 121,
      "content": "99\nObservable behavior vs. implementation details\n5.2\nObservable behavior vs. implementation details\nSection 5.1 showed what a mock is. The next step on the way to explaining the con-\nnection between mocks and test fragility is diving into what causes such fragility.\n As you might remember from chapter 4, test fragility corresponds to the second\nattribute of a good unit test: resistance to refactoring. (As a reminder, the four attri-\nbutes are protection against regressions, resistance to refactoring, fast feedback, and\nmaintainability.) The metric of resistance to refactoring is the most important\nbecause whether a unit test possesses this metric is mostly a binary choice. Thus, it’s\ngood to max out this metric to the extent that the test still remains in the realm of unit\ntesting and doesn’t transition to the category of end-to-end testing. The latter, despite\nbeing the best at resistance to refactoring, is generally much harder to maintain.\n In chapter 4, you also saw that the main reason tests deliver false positives (and thus\nfail at resistance to refactoring) is because they couple to the code’s implementation\ndetails. The only way to avoid such coupling is to verify the end result the code produces\n(its observable behavior) and distance tests from implementation details as much as pos-\nsible. In other words, tests must focus on the whats, not the hows. So, what exactly is an\nimplementation detail, and how is it different from an observable behavior?\n5.2.1\nObservable behavior is not the same as a public API\nAll production code can be categorized along two dimensions:\nPublic API vs. private API (where API means application programming interface)\nObservable behavior vs. implementation details \nThe categories in these dimensions don’t overlap. A method can’t belong to both a pub-\nlic and a private API; it’s either one or the other. Similarly, the code is either an internal\nimplementation detail or part of the system’s observable behavior, but not both.\n Most programming languages provide a simple mechanism to differentiate between\nthe code base’s public and private APIs. For example, in C#, you can mark any mem-\nber in a class with the private keyword, and that member will be hidden from the cli-\nent code, becoming part of the class’s private API. The same is true for classes: you can\neasily make them private by using the private or internal keyword.\n The distinction between observable behavior and internal implementation details\nis more nuanced. For a piece of code to be part of the system’s observable behavior, it\nhas to do one of the following things:\nExpose an operation that helps the client achieve one of its goals. An operation is\na method that performs a calculation or incurs a side effect or both.\nExpose a state that helps the client achieve one of its goals. State is the current\ncondition of the system.\nAny code that does neither of these two things is an implementation detail.\n Notice that whether the code is observable behavior depends on who its client is\nand what the goals of that client are. In order to be a part of observable behavior, the\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 3169,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 122,
      "content": "100\nCHAPTER 5\nMocks and test fragility\ncode needs to have an immediate connection to at least one such goal. The word client\ncan refer to different things depending on where the code resides. The common\nexamples are client code from the same code base, an external application, or the\nuser interface.\n Ideally, the system’s public API surface should coincide with its observable behav-\nior, and all its implementation details should be hidden from the eyes of the clients.\nSuch a system has a well-designed API (figure 5.4).\nOften, though, the system’s public API extends beyond its observable behavior and\nstarts exposing implementation details. Such a system’s implementation details leak to\nits public API surface (figure 5.5). \n5.2.2\nLeaking implementation details: An example with an operation\nLet’s take a look at examples of code whose implementation details leak to the public\nAPI. Listing 5.5 shows a User class with a public API that consists of two members: a\nName property and a NormalizeName() method. The class also has an invariant: users’\nnames must not exceed 50 characters and should be truncated otherwise.\npublic class User\n{\npublic string Name { get; set; }\nListing 5.5\nUser class with leaking implementation details\nObservable behavior\nPublic API\nPrivate API\nImplementation detail\nFigure 5.4\nIn a well-designed API, the \nobservable behavior coincides with the public \nAPI, while all implementation details are \nhidden behind the private API.\nObservable behavior\nPublic API\nPrivate API\nLeaking implementation detail\nFigure 5.5\nA system leaks implementation \ndetails when its public API extends beyond \nthe observable behavior.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1699,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 123,
      "content": "101\nObservable behavior vs. implementation details\npublic string NormalizeName(string name)\n{\nstring result = (name ?? \"\").Trim();\nif (result.Length > 50)\nreturn result.Substring(0, 50);\nreturn result;\n}\n}\npublic class UserController\n{\npublic void RenameUser(int userId, string newName)\n{\nUser user = GetUserFromDatabase(userId);\nstring normalizedName = user.NormalizeName(newName);\nuser.Name = normalizedName;\nSaveUserToDatabase(user);\n}\n}\nUserController is client code. It uses the User class in its RenameUser method. The\ngoal of this method, as you have probably guessed, is to change a user’s name.\n So, why isn’t User’s API well-designed? Look at its members once again: the Name\nproperty and the NormalizeName method. Both of them are public. Therefore, in\norder for the class’s API to be well-designed, these members should be part of the\nobservable behavior. This, in turn, requires them to do one of the following two things\n(which I’m repeating here for convenience):\nExpose an operation that helps the client achieve one of its goals.\nExpose a state that helps the client achieve one of its goals.\nOnly the Name property meets this requirement. It exposes a setter, which is an opera-\ntion that allows UserController to achieve its goal of changing a user’s name. The\nNormalizeName method is also an operation, but it doesn’t have an immediate con-\nnection to the client’s goal. The only reason UserController calls this method is to\nsatisfy the invariant of User. NormalizeName is therefore an implementation detail that\nleaks to the class’s public API (figure 5.6).\n To fix the situation and make the class’s API well-designed, User needs to hide\nNormalizeName() and call it internally as part of the property’s setter without relying\non the client code to do so. Listing 5.6 shows this approach.\n \n \nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1868,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 124,
      "content": "102\nCHAPTER 5\nMocks and test fragility\n \npublic class User\n{\nprivate string _name;\npublic string Name\n{\nget => _name;\nset => _name = NormalizeName(value);\n}\nprivate string NormalizeName(string name)\n{\nstring result = (name ?? \"\").Trim();\nif (result.Length > 50)\nreturn result.Substring(0, 50);\nreturn result;\n}\n}\npublic class UserController\n{\npublic void RenameUser(int userId, string newName)\n{\nUser user = GetUserFromDatabase(userId);\nuser.Name = newName;\nSaveUserToDatabase(user);\n}\n}\nUser’s API in listing 5.6 is well-designed: only the observable behavior (the Name prop-\nerty) is made public, while the implementation details (the NormalizeName method)\nare hidden behind the private API (figure 5.7).\n \nListing 5.6\nA version of User with a well-designed API\nObservable behavior\nPublic API\nNormalize\nname\nName\nLeaking implementation detail\nFigure 5.6\nThe API of User is not well-\ndesigned: it exposes the NormalizeName \nmethod, which is not part of the observable \nbehavior.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1031,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 125,
      "content": "103\nObservable behavior vs. implementation details\nNOTE\nStrictly speaking, Name’s getter should also be made private, because\nit’s not used by UserController. In reality, though, you almost always want to\nread back changes you make. Therefore, in a real project, there will certainly be\nanother use case that requires seeing users’ current names via Name’s getter.\nThere’s a good rule of thumb that can help you determine whether a class leaks its\nimplementation details. If the number of operations the client has to invoke on the\nclass to achieve a single goal is greater than one, then that class is likely leaking imple-\nmentation details. Ideally, any individual goal should be achieved with a single operation. In\nlisting 5.5, for example, UserController has to use two operations from User:\nstring normalizedName = user.NormalizeName(newName);\nuser.Name = normalizedName;\nAfter the refactoring, the number of operations has been reduced to one:\nuser.Name = newName;\nIn my experience, this rule of thumb holds true for the vast majority of cases where\nbusiness logic is involved. There could very well be exceptions, though. Still, be sure\nto examine each situation where your code violates this rule for a potential leak of\nimplementation details. \n5.2.3\nWell-designed API and encapsulation\nMaintaining a well-designed API relates to the notion of encapsulation. As you might\nrecall from chapter 3, encapsulation is the act of protecting your code against inconsis-\ntencies, also known as invariant violations. An invariant is a condition that should be\nheld true at all times. The User class from the previous example had one such invari-\nant: no user could have a name that exceeded 50 characters.\n Exposing implementation details goes hand in hand with invariant violations—the\nformer often leads to the latter. Not only did the original version of User leak its\nimplementation details, but it also didn’t maintain proper encapsulation. It allowed\nthe client to bypass the invariant and assign a new name to a user without normalizing\nthat name first.\nObservable behavior\nPublic API\nNormalize\nname\nName\nPrivate API\nImplementation detail\nFigure 5.7\nUser with a well-designed API. \nOnly the observable behavior is public; the \nimplementation details are now private.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2326,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 126,
      "content": "104\nCHAPTER 5\nMocks and test fragility\n Encapsulation is crucial for code base maintainability in the long run. The reason\nwhy is complexity. Code complexity is one of the biggest challenges you’ll face in soft-\nware development. The more complex the code base becomes, the harder it is to work\nwith, which, in turn, results in slowing down development speed and increasing the\nnumber of bugs.\n Without encapsulation, you have no practical way to cope with ever-increasing\ncode complexity. When the code’s API doesn’t guide you through what is and what\nisn’t allowed to be done with that code, you have to keep a lot of information in mind\nto make sure you don’t introduce inconsistencies with new code changes. This brings\nan additional mental burden to the process of programming. Remove as much of that\nburden from yourself as possible. You cannot trust yourself to do the right thing all the\ntime—so, eliminate the very possibility of doing the wrong thing. The best way to do so is to\nmaintain proper encapsulation so that your code base doesn’t even provide an option\nfor you to do anything incorrectly. Encapsulation ultimately serves the same goal as\nunit testing: it enables sustainable growth of your software project.\n There’s a similar principle: tell-don’t-ask. It was coined by Martin Fowler (https://\nmartinfowler.com/bliki/TellDontAsk.html) and stands for bundling data with the\nfunctions that operate on that data. You can view this principle as a corollary to the\npractice of encapsulation. Code encapsulation is a goal, whereas bundling data and\nfunctions together, as well as hiding implementation details, are the means to achieve\nthat goal:\nHiding implementation details helps you remove the class’s internals from the eyes\nof its clients, so there’s less risk of corrupting those internals.\nBundling data and operations helps to make sure these operations don’t violate\nthe class’s invariants. \n5.2.4\nLeaking implementation details: An example with state\nThe example shown in listing 5.5 demonstrated an operation (the NormalizeName\nmethod) that was an implementation detail leaking to the public API. Let’s also look\nat an example with state. The following listing contains the MessageRenderer class you\nsaw in chapter 4. It uses a collection of sub-renderers to generate an HTML represen-\ntation of a message containing a header, a body, and a footer.\npublic class MessageRenderer : IRenderer\n{\npublic IReadOnlyList<IRenderer> SubRenderers { get; }\npublic MessageRenderer()\n{\nSubRenderers = new List<IRenderer>\n{\nnew HeaderRenderer(),\nnew BodyRenderer(),\nListing 5.7\nState as an implementation detail \nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2680,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 127,
      "content": "105\nObservable behavior vs. implementation details\nnew FooterRenderer()\n};\n}\npublic string Render(Message message)\n{\nreturn SubRenderers\n.Select(x => x.Render(message))\n.Aggregate(\"\", (str1, str2) => str1 + str2);\n}\n}\nThe sub-renderers collection is public. But is it part of observable behavior? Assuming\nthat the client’s goal is to render an HTML message, the answer is no. The only class\nmember such a client would need is the Render method itself. Thus SubRenderers is\nalso a leaking implementation detail.\n I bring up this example again for a reason. As you may remember, I used it to illus-\ntrate a brittle test. That test was brittle precisely because it was tied to this implementa-\ntion detail—it checked to see the collection’s composition. The brittleness was fixed by\nre-targeting the test at the Render method. The new version of the test verified the result-\ning message—the only output the client code cared about, the observable behavior.\n As you can see, there’s an intrinsic connection between good unit tests and a well-\ndesigned API. By making all implementation details private, you leave your tests no\nchoice other than to verify the code’s observable behavior, which automatically\nimproves their resistance to refactoring.\nTIP\nMaking the API well-designed automatically improves unit tests.\nAnother guideline flows from the definition of a well-designed API: you should expose\nthe absolute minimum number of operations and state. Only code that directly helps\nclients achieve their goals should be made public. Everything else is implementation\ndetails and thus must be hidden behind the private API.\n Note that there’s no such problem as leaking observable behavior, which would be\nsymmetric to the problem of leaking implementation details. While you can expose an\nimplementation detail (a method or a class that is not supposed to be used by the cli-\nent), you can’t hide an observable behavior. Such a method or class would no longer\nhave an immediate connection to the client goals, because the client wouldn’t be able\nto directly use it anymore. Thus, by definition, this code would cease to be part of\nobservable behavior. Table 5.1 sums it all up.\nTable 5.1\nThe relationship between the code’s publicity and purpose. Avoid making implementation\ndetails public.\nObservable behavior\nImplementation detail\nPublic\nGood\nBad\nPrivate\nN/A\nGood \nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2420,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 128,
      "content": "106\nCHAPTER 5\nMocks and test fragility\n5.3\nThe relationship between mocks and test fragility\nThe previous sections defined a mock and showed the difference between observable\nbehavior and an implementation detail. In this section, you will learn about hexago-\nnal architecture, the difference between internal and external communications, and\n(finally!) the relationship between mocks and test fragility.\n5.3.1\nDefining hexagonal architecture\nA typical application consists of two layers, domain and application services, as\nshown in figure 5.8. The domain layer resides in the middle of the diagram because\nit’s the central part of your application. It contains the business logic: the essential\nfunctionality your application is built for. The domain layer and its business logic\ndifferentiate this application from others and provide a competitive advantage for\nthe organization.\nThe application services layer sits on top of the domain layer and orchestrates com-\nmunication between that layer and the external world. For example, if your applica-\ntion is a RESTful API, all requests to this API hit the application services layer first.\nThis layer then coordinates the work between domain classes and out-of-process\ndependencies. Here’s an example of such coordination for the application service. It\ndoes the following:\nQueries the database and uses the data to materialize a domain class instance\nInvokes an operation on that instance\nSaves the results back to the database\nThe combination of the application services layer and the domain layer forms a hexa-\ngon, which itself represents your application. It can interact with other applications,\nwhich are represented with their own hexagons (see figure 5.9). These other applica-\ntions could be an SMTP service, a third-party system, a message bus, and so on. A set\nof interacting hexagons makes up a hexagonal architecture.\n \nDomain\n(business logic)\nApplication\nservices\nFigure 5.8\nA typical application consists of a \ndomain layer and an application services layer. \nThe domain layer contains the application’s \nbusiness logic; application services tie that \nlogic to business use cases.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2203,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 129,
      "content": "107\nThe relationship between mocks and test fragility\nThe term hexagonal architecture was introduced by Alistair Cockburn. Its purpose is to\nemphasize three important guidelines:\nThe separation of concerns between the domain and application services layers—Business\nlogic is the most important part of the application. Therefore, the domain layer\nshould be accountable only for that business logic and exempted from all other\nresponsibilities. Those responsibilities, such as communicating with external\napplications and retrieving data from the database, must be attributed to appli-\ncation services. Conversely, the application services shouldn’t contain any busi-\nness logic. Their responsibility is to adapt the domain layer by translating the\nincoming requests into operations on domain classes and then persisting the\nresults or returning them back to the caller. You can view the domain layer as a\ncollection of the application’s domain knowledge (how-to’s) and the application\nservices layer as a set of business use cases (what-to’s).\nCommunications inside your application—Hexagonal architecture prescribes a\none-way flow of dependencies: from the application services layer to the domain\nlayer. Classes inside the domain layer should only depend on each other; they\nshould not depend on classes from the application services layer. This guideline\nflows from the previous one. The separation of concerns between the applica-\ntion services layer and the domain layer means that the former knows about the\nlatter, but the opposite is not true. The domain layer should be fully isolated\nfrom the external world.\nCommunications between applications—External applications connect to your\napplication through a common interface maintained by the application services\nlayer. No one has a direct access to the domain layer. Each side in a hexagon\nrepresents a connection into or out of the application. Note that although a\nDomain\n(business logic)\nApplication\nservices\nThird-party\nsystem\nMessage\nbus\nSMTP\nservice\nFigure 5.9\nA hexagonal \narchitecture is a set of \ninteracting applications—\nhexagons.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2155,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 130,
      "content": "108\nCHAPTER 5\nMocks and test fragility\nhexagon has six sides, it doesn’t mean your application can only connect to six\nother applications. The number of connections is arbitrary. The point is that\nthere can be many such connections.\nEach layer of your application exhibits observable behavior and contains its own set of\nimplementation details. For example, observable behavior of the domain layer is the\nsum of this layer’s operations and state that helps the application service layer achieve\nat least one of its goals. The principles of a well-designed API have a fractal nature:\nthey apply equally to as much as a whole layer or as little as a single class.\n When you make each layer’s API well-designed (that is, hide its implementation\ndetails), your tests also start to have a fractal structure; they verify behavior that helps\nachieve the same goals but at different levels. A test covering an application service\nchecks to see how this service attains an overarching, coarse-grained goal posed by the\nexternal client. At the same time, a test working with a domain class verifies a subgoal\nthat is part of that greater goal (figure 5.10).\nYou might remember from previous chapters how I mentioned that you should be\nable to trace any test back to a particular business requirement. Each test should tell a\nstory that is meaningful to a domain expert, and if it doesn’t, that’s a strong indication\nthat the test couples to implementation details and therefore is brittle. I hope now you\ncan see why.\n Observable behavior flows inward from outer layers to the center. The overarching\ngoal posed by the external client gets translated into subgoals achieved by individual\nGoal\n(use case)\nSubgoal\nSubgoal\nTest 1\nTest 2\nTest 3\nExternal client\nApplication service\nDomain class 1\nDomain class 2\nFigure 5.10\nTests working with different layers have a fractal nature: they verify the \nsame behavior at different levels. A test of an application service checks to see how \nthe overall business use case is executed. A test working with a domain class verifies \nan intermediate subgoal on the way to use-case completion.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2170,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 131,
      "content": "109\nThe relationship between mocks and test fragility\ndomain classes. Each piece of observable behavior in the domain layer therefore pre-\nserves the connection to a particular business use case. You can trace this connection\nrecursively from the innermost (domain) layer outward to the application services\nlayer and then to the needs of the external client. This traceability follows from the\ndefinition of observable behavior. For a piece of code to be part of observable behav-\nior, it needs to help the client achieve one of its goals. For a domain class, the client is\nan application service; for the application service, it’s the external client itself.\n Tests that verify a code base with a well-designed API also have a connection to\nbusiness requirements because those tests tie to the observable behavior only. A good\nexample is the User and UserController classes from listing 5.6 (I’m repeating the\ncode here for convenience).\npublic class User\n{\nprivate string _name;\npublic string Name\n{\nget => _name;\nset => _name = NormalizeName(value);\n}\nprivate string NormalizeName(string name)\n{\n/* Trim name down to 50 characters */\n}\n}\npublic class UserController\n{\npublic void RenameUser(int userId, string newName)\n{\nUser user = GetUserFromDatabase(userId);\nuser.Name = newName;\nSaveUserToDatabase(user);\n}\n}\nUserController in this example is an application service. Assuming that the exter-\nnal client doesn’t have a specific goal of normalizing user names, and all names are\nnormalized solely due to restrictions from the application itself, the NormalizeName\nmethod in the User class can’t be traced to the client’s needs. Therefore, it’s an\nimplementation detail and should be made private (we already did that earlier in\nthis chapter). Moreover, tests shouldn’t check this method directly. They should ver-\nify it only as part of the class’s observable behavior—the Name property’s setter in\nthis example.\n This guideline of always tracing the code base’s public API to business require-\nments applies to the vast majority of domain classes and application services but less\nListing 5.8\nA domain class with an application service\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2194,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 132,
      "content": "110\nCHAPTER 5\nMocks and test fragility\nso to utility and infrastructure code. The individual problems such code solves are\noften too low-level and fine-grained and can’t be traced to a specific business use case. \n5.3.2\nIntra-system vs. inter-system communications\nThere are two types of communications in a typical application: intra-system and inter-\nsystem. Intra-system communications are communications between classes inside your\napplication. Inter-system communications are when your application talks to other appli-\ncations (figure 5.11).\nNOTE\nIntra-system communications are implementation details; inter-system\ncommunications are not.\nIntra-system communications are implementation details because the collaborations\nyour domain classes go through in order to perform an operation are not part of their\nobservable behavior. These collaborations don’t have an immediate connection to the\nclient’s goal. Thus, coupling to such collaborations leads to fragile tests.\n Inter-system communications are a different matter. Unlike collaborations between\nclasses inside your application, the way your system talks to the external world forms\nthe observable behavior of that system as a whole. It’s part of the contract your appli-\ncation must hold at all times (figure 5.12).\n This attribute of inter-system communications stems from the way separate applica-\ntions evolve together. One of the main principles of such an evolution is maintaining\nbackward compatibility. Regardless of the refactorings you perform inside your sys-\ntem, the communication pattern it uses to talk to external applications should always\nstay in place, so that external applications can understand it. For example, messages\nyour application emits on a bus should preserve their structure, the calls issued to an\nSMTP service should have the same number and type of parameters, and so on.\nThird-party\nsystem\nSMTP service\nIntra-system\nInter-system\nInter-system\nFigure 5.11\nThere are two types \nof communications: intra-system \n(between classes inside the \napplication) and inter-system \n(between applications).\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2141,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 133,
      "content": "111\nThe relationship between mocks and test fragility\nThe use of mocks is beneficial when verifying the communication pattern between\nyour system and external applications. Conversely, using mocks to verify communica-\ntions between classes inside your system results in tests that couple to implementation\ndetails and therefore fall short of the resistance-to-refactoring metric.\n5.3.3\nIntra-system vs. inter-system communications: An example\nTo illustrate the difference between intra-system and inter-system communications, I’ll\nexpand on the example with the Customer and Store classes that I used in chapter 2\nand earlier in this chapter. Imagine the following business use case:\nA customer tries to purchase a product from a store.\nIf the amount of the product in the store is sufficient, then\n– The inventory is removed from the store.\n– An email receipt is sent to the customer.\n– A confirmation is returned.\nLet’s also assume that the application is an API with no user interface.\n In the following listing, the CustomerController class is an application service that\norchestrates the work between domain classes (Customer, Product, Store) and the\nexternal application (EmailGateway, which is a proxy to an SMTP service).\npublic class CustomerController\n{\npublic bool Purchase(int customerId, int productId, int quantity)\nListing 5.9\nConnecting the domain model with external applications\nThird-party\nsystem\nSMTP service\nImplementation detail\nObservable behavior (contract)\nObservable behavior (contract)\nFigure 5.12\nInter-system communications form the observable \nbehavior of your application as a whole. Intra-system communications \nare implementation details.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1725,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 134,
      "content": "112\nCHAPTER 5\nMocks and test fragility\n{\nCustomer customer = _customerRepository.GetById(customerId);\nProduct product = _productRepository.GetById(productId);\nbool isSuccess = customer.Purchase(\n_mainStore, product, quantity);\nif (isSuccess)\n{\n_emailGateway.SendReceipt(\ncustomer.Email, product.Name, quantity);\n}\nreturn isSuccess;\n}\n}\nValidation of input parameters is omitted for brevity. In the Purchase method, the\ncustomer checks to see if there’s enough inventory in the store and, if so, decreases\nthe product amount.\n The act of making a purchase is a business use case with both intra-system and\ninter-system communications. The inter-system communications are those between\nthe CustomerController application service and the two external systems: the third-\nparty application (which is also the client initiating the use case) and the email gate-\nway. The intra-system communication is between the Customer and the Store domain\nclasses (figure 5.13).\n In this example, the call to the SMTP service is a side effect that is visible to the\nexternal world and thus forms the observable behavior of the application as a whole.\nThird-party\nsystem\n(external\nclient)\nSMTP service\nSendReceipt()\nCustomer\nRemoveInventory()\nStore\nisSuccess\nFigure 5.13\nThe example in listing 5.9 represented using the hexagonal \narchitecture. The communications between the hexagons are inter-system \ncommunications. The communication inside the hexagon is intra-system.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1505,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 135,
      "content": "113\nThe relationship between mocks and test fragility\nIt also has a direct connection to the client’s goals. The client of the application is the\nthird-party system. This system’s goal is to make a purchase, and it expects the cus-\ntomer to receive a confirmation email as part of the successful outcome.\n The call to the SMTP service is a legitimate reason to do mocking. It doesn’t lead\nto test fragility because you want to make sure this type of communication stays in\nplace even after refactoring. The use of mocks helps you do exactly that.\n The next listing shows an example of a legitimate use of mocks.\n[Fact]\npublic void Successful_purchase()\n{\nvar mock = new Mock<IEmailGateway>();\nvar sut = new CustomerController(mock.Object);\nbool isSuccess = sut.Purchase(\ncustomerId: 1, productId: 2, quantity: 5);\nAssert.True(isSuccess);\nmock.Verify(\n  \nx => x.SendReceipt(\n  \n\"customer@email.com\", \"Shampoo\", 5),  \nTimes.Once);\n  \n}\nNote that the isSuccess flag is also observable by the external client and also needs\nverification. This flag doesn’t need mocking, though; a simple value comparison is\nenough.\n Let’s now look at a test that mocks the communication between Customer and\nStore.\n[Fact]\npublic void Purchase_succeeds_when_enough_inventory()\n{\nvar storeMock = new Mock<IStore>();\nstoreMock\n.Setup(x => x.HasEnoughInventory(Product.Shampoo, 5))\n.Returns(true);\nvar customer = new Customer();\nbool success = customer.Purchase(\nstoreMock.Object, Product.Shampoo, 5);\nAssert.True(success);\nstoreMock.Verify(\nx => x.RemoveInventory(Product.Shampoo, 5),\nTimes.Once);\n}\nListing 5.10\nMocking that doesn’t lead to fragile tests \nListing 5.11\nMocking that leads to fragile tests \nVerifies that the \nsystem sent a receipt \nabout the purchase\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1795,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 136,
      "content": "114\nCHAPTER 5\nMocks and test fragility\nUnlike the communication between CustomerController and the SMTP service, the\nRemoveInventory() method call from Customer to Store doesn’t cross the applica-\ntion boundary: both the caller and the recipient reside inside the application. Also,\nthis method is neither an operation nor a state that helps the client achieve its goals.\nThe client of these two domain classes is CustomerController with the goal of making\na purchase. The only two members that have an immediate connection to this goal are\ncustomer.Purchase() and store.GetInventory(). The Purchase() method initiates\nthe purchase, and GetInventory() shows the state of the system after the purchase is\ncompleted. The RemoveInventory() method call is an intermediate step on the way to\nthe client’s goal—an implementation detail. \n5.4\nThe classical vs. London schools of unit testing, \nrevisited\nAs a reminder from chapter 2 (table 2.1), table 5.2 sums up the differences between\nthe classical and London schools of unit testing.\nIn chapter 2, I mentioned that I prefer the classical school of unit testing over the\nLondon school. I hope now you can see why. The London school encourages the use\nof mocks for all but immutable dependencies and doesn’t differentiate between intra-\nsystem and inter-system communications. As a result, tests check communications\nbetween classes just as much as they check communications between your application\nand external systems.\n This indiscriminate use of mocks is why following the London school often results\nin tests that couple to implementation details and thus lack resistance to refactoring.\nAs you may remember from chapter 4, the metric of resistance to refactoring (unlike\nthe other three) is mostly a binary choice: a test either has resistance to refactoring or\nit doesn’t. Compromising on this metric renders the test nearly worthless.\n The classical school is much better at this issue because it advocates for substitut-\ning only dependencies that are shared between tests, which almost always translates\ninto out-of-process dependencies such as an SMTP service, a message bus, and so on.\nBut the classical school is not ideal in its treatment of inter-system communications,\neither. This school also encourages excessive use of mocks, albeit not as much as the\nLondon school.\nTable 5.2\nThe differences between the London and classical schools of unit testing\nIsolation of\nA unit is\nUses test doubles for\nLondon school\nUnits\nA class\nAll but immutable dependencies\nClassical school\nUnit tests\nA class or a set of classes\nShared dependencies\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2645,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 137,
      "content": "115\nThe classical vs. London schools of unit testing, revisited\n5.4.1\nNot all out-of-process dependencies should be mocked out\nBefore we discuss out-of-process dependencies and mocking, let me give you a quick\nrefresher on types of dependencies (refer to chapter 2 for more details):\nShared dependency—A dependency shared by tests (not production code)\nOut-of-process dependency—A dependency hosted by a process other than the pro-\ngram’s execution process (for example, a database, a message bus, or an SMTP\nservice)\nPrivate dependency—Any dependency that is not shared\nThe classical school recommends avoiding shared dependencies because they provide\nthe means for tests to interfere with each other’s execution context and thus prevent\nthose tests from running in parallel. The ability for tests to run in parallel, sequen-\ntially, and in any order is called test isolation.\n If a shared dependency is not out-of-process, then it’s easy to avoid reusing it in\ntests by providing a new instance of it on each test run. In cases where the shared\ndependency is out-of-process, testing becomes more complicated. You can’t instanti-\nate a new database or provision a new message bus before each test execution; that\nwould drastically slow down the test suite. The usual approach is to replace such\ndependencies with test doubles—mocks and stubs.\n Not all out-of-process dependencies should be mocked out, though. If an out-of-\nprocess dependency is only accessible through your application, then communications with such a\ndependency are not part of your system’s observable behavior. An out-of-process dependency\nthat can’t be observed externally, in effect, acts as part of your application (figure 5.14).\n Remember, the requirement to always preserve the communication pattern\nbetween your application and external systems stems from the necessity to maintain\nbackward compatibility. You have to maintain the way your application talks to external\nThird-party\nsystem\n(external\nclient)\nSMTP service\nObservable behavior (contract)\nApplication\ndatabase\n(accessible\nonly by the\napplication)\nImplementation details\nFigure 5.14\nCommunications with an out-of-process dependency that can’t be \nobserved externally are implementation details. They don’t have to stay in place \nafter refactoring and therefore shouldn’t be verified with mocks.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2389,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 138,
      "content": "116\nCHAPTER 5\nMocks and test fragility\nsystems. That’s because you can’t change those external systems simultaneously with\nyour application; they may follow a different deployment cycle, or you might simply\nnot have control over them.\n But when your application acts as a proxy to an external system, and no client can\naccess it directly, the backward-compatibility requirement vanishes. Now you can deploy\nyour application together with this external system, and it won’t affect the clients. The\ncommunication pattern with such a system becomes an implementation detail.\n A good example here is an application database: a database that is used only by\nyour application. No external system has access to this database. Therefore, you can\nmodify the communication pattern between your system and the application database\nin any way you like, as long as it doesn’t break existing functionality. Because that data-\nbase is completely hidden from the eyes of the clients, you can even replace it with an\nentirely different storage mechanism, and no one will notice.\n The use of mocks for out-of-process dependencies that you have a full control over\nalso leads to brittle tests. You don’t want your tests to turn red every time you split a\ntable in the database or modify the type of one of the parameters in a stored proce-\ndure. The database and your application must be treated as one system.\n This obviously poses an issue. How would you test the work with such a depen-\ndency without compromising the feedback speed, the third attribute of a good unit\ntest? You’ll see this subject covered in depth in the following two chapters. \n5.4.2\nUsing mocks to verify behavior\nMocks are often said to verify behavior. In the vast majority of cases, they don’t. The\nway each individual class interacts with neighboring classes in order to achieve some\ngoal has nothing to do with observable behavior; it’s an implementation detail.\n Verifying communications between classes is akin to trying to derive a person’s\nbehavior by measuring the signals that neurons in the brain pass among each other.\nSuch a level of detail is too granular. What matters is the behavior that can be traced\nback to the client goals. The client doesn’t care what neurons in your brain light up\nwhen they ask you to help. The only thing that matters is the help itself—provided by\nyou in a reliable and professional fashion, of course. Mocks have something to do with\nbehavior only when they verify interactions that cross the application boundary and\nonly when the side effects of those interactions are visible to the external world. \nSummary\nTest double is an overarching term that describes all kinds of non-production-\nready, fake dependencies in tests. There are five variations of test doubles—\ndummy, stub, spy, mock, and fake—that can be grouped in just two types: mocks\nand stubs. Spies are functionally the same as mocks; dummies and fakes serve\nthe same role as stubs.\nMocks help emulate and examine outcoming interactions: calls from the SUT to\nits dependencies that change the state of those dependencies. Stubs help\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 3150,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 139,
      "content": "117\nSummary\nemulate incoming interactions: calls the SUT makes to its dependencies to get\ninput data.\nA mock (the tool) is a class from a mocking library that you can use to create a\nmock (the test double) or a stub.\nAsserting interactions with stubs leads to fragile tests. Such an interaction doesn’t\ncorrespond to the end result; it’s an intermediate step on the way to that result,\nan implementation detail.\nThe command query separation (CQS) principle states that every method\nshould be either a command or a query but not both. Test doubles that substi-\ntute commands are mocks. Test doubles that substitute queries are stubs.\nAll production code can be categorized along two dimensions: public API ver-\nsus private API, and observable behavior versus implementation details. Code\npublicity is controlled by access modifiers, such as private, public, and\ninternal keywords. Code is part of observable behavior when it meets one of\nthe following requirements (any other code is an implementation detail):\n– It exposes an operation that helps the client achieve one of its goals. An oper-\nation is a method that performs a calculation or incurs a side effect.\n– It exposes a state that helps the client achieve one of its goals. State is the cur-\nrent condition of the system.\nWell-designed code is code whose observable behavior coincides with the public\nAPI and whose implementation details are hidden behind the private API. A\ncode leaks implementation details when its public API extends beyond the\nobservable behavior.\nEncapsulation is the act of protecting your code against invariant violations.\nExposing implementation details often entails a breach in encapsulation\nbecause clients can use implementation details to bypass the code’s invariants.\nHexagonal architecture is a set of interacting applications represented as hexa-\ngons. Each hexagon consists of two layers: domain and application services.\nHexagonal architecture emphasizes three important aspects:\n– Separation of concerns between the domain and application services layers.\nThe domain layer should be responsible for the business logic, while the\napplication services should orchestrate the work between the domain layer\nand external applications.\n– A one-way flow of dependencies from the application services layer to the\ndomain layer. Classes inside the domain layer should only depend on each\nother; they should not depend on classes from the application services layer.\n– External applications connect to your application through a common inter-\nface maintained by the application services layer. No one has a direct access\nto the domain layer.\nEach layer in a hexagon exhibits observable behavior and contains its own set of\nimplementation details.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2793,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 140,
      "content": "118\nCHAPTER 5\nMocks and test fragility\nThere are two types of communications in an application: intra-system and\ninter-system. Intra-system communications are communications between classes\ninside the application. Inter-system communication is when the application talks\nto external applications.\nIntra-system communications are implementation details. Inter-system commu-\nnications are part of observable behavior, with the exception of external systems\nthat are accessible only through your application. Interactions with such sys-\ntems are implementation details too, because the resulting side effects are not\nobserved externally.\nUsing mocks to assert intra-system communications leads to fragile tests. Mock-\ning is legitimate only when it’s used for inter-system communications—commu-\nnications that cross the application boundary—and only when the side effects\nof those communications are visible to the external world.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 982,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 141,
      "content": "119\nStyles of unit testing\nChapter 4 introduced the four attributes of a good unit test: protection against\nregressions, resistance to refactoring, fast feedback, and maintainability. These attri-\nbutes form a frame of reference that you can use to analyze specific tests and unit\ntesting approaches. We analyzed one such approach in chapter 5: the use of mocks.\n In this chapter, I apply the same frame of reference to the topic of unit testing\nstyles. There are three such styles: output-based, state-based, and communication-\nbased testing. Among the three, the output-based style produces tests of the highest\nquality, state-based testing is the second-best choice, and communication-based\ntesting should be used only occasionally.\n Unfortunately, you can’t use the output-based testing style everywhere. It’s only\napplicable to code written in a purely functional way. But don’t worry; there are\ntechniques that can help you transform more of your tests into the output-based\nstyle. For that, you’ll need to use functional programming principles to restructure\nthe underlying code toward a functional architecture.\nThis chapter covers\nComparing styles of unit testing\nThe relationship between functional and \nhexagonal architectures\nTransitioning to output-based testing\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1330,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 142,
      "content": "120\nCHAPTER 6\nStyles of unit testing\n Note that this chapter doesn’t provide a deep dive into the topic of functional pro-\ngramming. Still, by the end of this chapter, I hope you’ll have an intuitive understand-\ning of how functional programming relates to output-based testing. You’ll also learn\nhow to write more of your tests using the output-based style, as well as the limitations\nof functional programming and functional architecture.\n6.1\nThe three styles of unit testing\nAs I mentioned in the chapter introduction, there are three styles of unit testing:\nOutput-based testing \nState-based testing \nCommunication-based testing\nYou can employ one, two, or even all three styles together in a single test. This sec-\ntion lays the foundation for the whole chapter by defining (with examples) those\nthree styles of unit testing. You’ll see how they score against each other in the sec-\ntion after that.\n6.1.1\nDefining the output-based style\nThe first style of unit testing is the output-based style, where you feed an input to the sys-\ntem under test (SUT) and check the output it produces (figure 6.1). This style of unit\ntesting is only applicable to code that doesn’t change a global or internal state, so the\nonly component to verify is its return value.\nThe following listing shows an example of such code and a test covering it. The Price-\nEngine class accepts an array of products and calculates a discount.\npublic class PriceEngine\n{\npublic decimal CalculateDiscount(params Product[] products)\nListing 6.1\nOutput-based testing\nOutput\nProduction code\nInput\nOutput\nveriﬁcation\nFigure 6.1\nIn output-based testing, tests verify the output the system \ngenerates. This style of testing assumes there are no side effects and the only \nresult of the SUT’s work is the value it returns to the caller.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1856,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 143,
      "content": "121\nThe three styles of unit testing\n{\ndecimal discount = products.Length * 0.01m;\nreturn Math.Min(discount, 0.2m);\n}\n}\n[Fact]\npublic void Discount_of_two_products()\n{\nvar product1 = new Product(\"Hand wash\");\nvar product2 = new Product(\"Shampoo\");\nvar sut = new PriceEngine();\ndecimal discount = sut.CalculateDiscount(product1, product2);\nAssert.Equal(0.02m, discount);\n}\nPriceEngine multiplies the number of products by 1% and caps the result at 20%.\nThere’s nothing else to this class. It doesn’t add the products to any internal collec-\ntion, nor does it persist them in a database. The only outcome of the Calculate-\nDiscount() method is the discount it returns: the output value (figure 6.2).\nThe output-based style of unit testing is also known as functional. This name takes root\nin functional programming, a method of programming that emphasizes a preference for\nside-effect-free code. We’ll talk more about functional programming and functional\narchitecture later in this chapter. \n6.1.2\nDefining the state-based style\nThe state-based style is about verifying the state of the system after an operation is com-\nplete (figure 6.3). The term state in this style of testing can refer to the state of the\nSUT itself, of one of its collaborators, or of an out-of-process dependency, such as\nthe database or the filesystem.\nOutput\nveriﬁcation\nOutput\nPriceEngine\nInput\nProduct (“Hand wash”)\nProduct (“Shampoo”)\n2% discount\nFigure 6.2\nPriceEngine represented using input-output notation. Its \nCalculateDiscount() method accepts an array of products and \ncalculates a discount.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1629,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 144,
      "content": "122\nCHAPTER 6\nStyles of unit testing\nHere’s an example of state-based testing. The Order class allows the client to add a\nnew product.\npublic class Order\n{\nprivate readonly List<Product> _products = new List<Product>();\npublic IReadOnlyList<Product> Products => _products.ToList();\npublic void AddProduct(Product product)\n{\n_products.Add(product);\n}\n}\n[Fact]\npublic void Adding_a_product_to_an_order()\n{\nvar product = new Product(\"Hand wash\");\nvar sut = new Order();\nsut.AddProduct(product);\nAssert.Equal(1, sut.Products.Count);\nAssert.Equal(product, sut.Products[0]);\n}\nThe test verifies the Products collection after the addition is completed. Unlike\nthe example of output-based testing in listing 6.1, the outcome of AddProduct() is the\nchange made to the order’s state. \n6.1.3\nDefining the communication-based style\nFinally, the third style of unit testing is communication-based testing. This style uses\nmocks to verify communications between the system under test and its collaborators\n(figure 6.4).\nListing 6.2\nState-based testing\nState\nveriﬁcation\nState\nveriﬁcation\nProduction code\nInput\nFigure 6.3\nIn state-based testing, tests verify the final state of the \nsystem after an operation is complete. The dashed circles represent that \nfinal state.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1306,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 145,
      "content": "123\nComparing the three styles of unit testing\nThe following listing shows an example of communication-based testing.\n[Fact]\npublic void Sending_a_greetings_email()\n{\nvar emailGatewayMock = new Mock<IEmailGateway>();\nvar sut = new Controller(emailGatewayMock.Object);\nsut.GreetUser(\"user@email.com\");\nemailGatewayMock.Verify(\nx => x.SendGreetingsEmail(\"user@email.com\"),\nTimes.Once);\n}\n6.2\nComparing the three styles of unit testing\nThere’s nothing new about output-based, state-based, and communication-based\nstyles of unit testing. In fact, you already saw all of these styles previously in this book.\nWhat’s interesting is comparing them to each other using the four attributes of a good\nunit test. Here are those attributes again (refer to chapter 4 for more details):\nProtection against regressions\nResistance to refactoring\nFast feedback\nMaintainability\nIn our comparison, let’s look at each of the four separately.\nListing 6.3\nCommunication-based testing\nStyles and schools of unit testing\nThe classical school of unit testing prefers the state-based style over the communication-\nbased one. The London school makes the opposite choice. Both schools use output-\nbased testing. \nCollaboration\nveriﬁcation\nMocks\nProduction code\nInput\nFigure 6.4\nIn communication-based \ntesting, tests substitute the SUT’s \ncollaborators with mocks and verify \nthat the SUT calls those \ncollaborators correctly.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1454,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 146,
      "content": "124\nCHAPTER 6\nStyles of unit testing\n6.2.1\nComparing the styles using the metrics of protection against \nregressions and feedback speed\nLet’s first compare the three styles in terms of the protection against regressions\nand feedback speed attributes, as these attributes are the most straightforward in this\nparticular comparison. The metric of protection against regressions doesn’t depend\non a particular style of testing. This metric is a product of the following three\ncharacteristics:\nThe amount of code that is executed during the test\nThe complexity of that code\nIts domain significance\nGenerally, you can write a test that exercises as much or as little code as you like; no\nparticular style provides a benefit in this area. The same is true for the code’s com-\nplexity and domain significance. The only exception is the communication-based\nstyle: overusing it can result in shallow tests that verify only a thin slice of code and\nmock out everything else. Such shallowness is not a definitive feature of communication-\nbased testing, though, but rather is an extreme case of abusing this technique.\n There’s little correlation between the styles of testing and the test’s feedback speed.\nAs long as your tests don’t touch out-of-process dependencies and thus stay in the\nrealm of unit testing, all styles produce tests of roughly equal speed of execution.\nCommunication-based testing can be slightly worse because mocks tend to introduce\nadditional latency at runtime. But the difference is negligible, unless you have tens of\nthousands of such tests. \n6.2.2\nComparing the styles using the metric of resistance \nto refactoring\nWhen it comes to the metric of resistance to refactoring, the situation is different.\nResistance to refactoring is the measure of how many false positives (false alarms) tests gen-\nerate during refactorings. False positives, in turn, are a result of tests coupling to\ncode’s implementation details as opposed to observable behavior.\n Output-based testing provides the best protection against false positives because\nthe resulting tests couple only to the method under test. The only way for such tests to\ncouple to implementation details is when the method under test is itself an implemen-\ntation detail.\n State-based testing is usually more prone to false positives. In addition to the\nmethod under test, such tests also work with the class’s state. Probabilistically speak-\ning, the greater the coupling between the test and the production code, the greater\nthe chance for this test to tie to a leaking implementation detail. State-based tests tie\nto a larger API surface, and hence the chances of coupling them to implementation\ndetails are also higher.\n Communication-based testing is the most vulnerable to false alarms. As you may\nremember from chapter 5, the vast majority of tests that check interactions with test\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2914,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 147,
      "content": "125\nComparing the three styles of unit testing\ndoubles end up being brittle. This is always the case for interactions with stubs—you\nshould never check such interactions. Mocks are fine only when they verify interac-\ntions that cross the application boundary and only when the side effects of those\ninteractions are visible to the external world. As you can see, using communication-\nbased testing requires extra prudence in order to maintain proper resistance to\nrefactoring.\n But just like shallowness, brittleness is not a definitive feature of the communication-\nbased style, either. You can reduce the number of false positives to a minimum by\nmaintaining proper encapsulation and coupling tests to observable behavior only.\nAdmittedly, though, the amount of due diligence varies depending on the style of\nunit testing. \n6.2.3\nComparing the styles using the metric of maintainability\nFinally, the maintainability metric is highly correlated with the styles of unit testing;\nbut, unlike with resistance to refactoring, there’s not much you can do to mitigate\nthat. Maintainability evaluates the unit tests’ maintenance costs and is defined by the\nfollowing two characteristics:\nHow hard it is to understand the test, which is a function of the test’s size\nHow hard it is to run the test, which is a function of how many out-of-process\ndependencies the test works with directly\nLarger tests are less maintainable because they are harder to grasp or change when\nneeded. Similarly, a test that directly works with one or several out-of-process depen-\ndencies (such as the database) is less maintainable because you need to spend time\nkeeping those out-of-process dependencies operational: rebooting the database\nserver, resolving network connectivity issues, and so on.\nMAINTAINABILITY OF OUTPUT-BASED TESTS\nCompared with the other two types of testing, output-based testing is the most main-\ntainable. The resulting tests are almost always short and concise and thus are easier to\nmaintain. This benefit of the output-based style stems from the fact that this style boils\ndown to only two things: supplying an input to a method and verifying its output,\nwhich you can often do with just a couple lines of code.\n Because the underlying code in output-based testing must not change the global\nor internal state, these tests don’t deal with out-of-process dependencies. Hence,\noutput-based tests are best in terms of both maintainability characteristics. \nMAINTAINABILITY OF STATE-BASED TESTS\nState-based tests are normally less maintainable than output-based ones. This is\nbecause state verification often takes up more space than output verification. Here’s\nanother example of state-based testing.\n \n \nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2756,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 148,
      "content": "126\nCHAPTER 6\nStyles of unit testing\n[Fact]\npublic void Adding_a_comment_to_an_article()\n{\nvar sut = new Article();\nvar text = \"Comment text\";\nvar author = \"John Doe\";\nvar now = new DateTime(2019, 4, 1);\nsut.AddComment(text, author, now);\nAssert.Equal(1, sut.Comments.Count);\n    \nAssert.Equal(text, sut.Comments[0].Text);\n    \nAssert.Equal(author, sut.Comments[0].Author);     \nAssert.Equal(now, sut.Comments[0].DateCreated);   \n}\nThis test adds a comment to an article and then checks to see if the comment\nappears in the article’s list of comments. Although this test is simplified and con-\ntains just a single comment, its assertion part already spans four lines. State-based\ntests often need to verify much more data than that and, therefore, can grow in size\nsignificantly.\n You can mitigate this issue by introducing helper methods that hide most of the\ncode and thus shorten the test (see listing 6.5), but these methods require significant\neffort to write and maintain. This effort is justified only when those methods are going\nto be reused across multiple tests, which is rarely the case. I’ll explain more about\nhelper methods in part 3 of this book.\n[Fact]\npublic void Adding_a_comment_to_an_article()\n{\nvar sut = new Article();\nvar text = \"Comment text\";\nvar author = \"John Doe\";\nvar now = new DateTime(2019, 4, 1);\nsut.AddComment(text, author, now);\nsut.ShouldContainNumberOfComments(1)    \n.WithComment(text, author, now);    \n}\nAnother way to shorten a state-based test is to define equality members in the class\nthat is being asserted. In listing 6.6, that’s the Comment class. You could turn it into a\nvalue object (a class whose instances are compared by value and not by reference), as\nshown next; this would also simplify the test, especially if you combined it with an\nassertion library like Fluent Assertions.\nListing 6.4\nState verification that takes up a lot of space\nListing 6.5\nUsing helper methods in assertions\nVerifies the state \nof the article\nHelper \nmethods\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2043,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 149,
      "content": "127\nComparing the three styles of unit testing\n[Fact]\npublic void Adding_a_comment_to_an_article()\n{\nvar sut = new Article();\nvar comment = new Comment(\n\"Comment text\",\n\"John Doe\",\nnew DateTime(2019, 4, 1));\nsut.AddComment(comment.Text, comment.Author, comment.DateCreated);\nsut.Comments.Should().BeEquivalentTo(comment);\n}\nThis test uses the fact that comments can be compared as whole values, without the\nneed to assert individual properties in them. It also uses the BeEquivalentTo method\nfrom Fluent Assertions, which can compare entire collections, thereby removing the\nneed to check the collection size.\n This is a powerful technique, but it works only when the class is inherently a value\nand can be converted into a value object. Otherwise, it leads to code pollution (pollut-\ning production code base with code whose sole purpose is to enable or, as in this case,\nsimplify unit testing). We’ll discuss code pollution along with other unit testing anti-\npatterns in chapter 11.\n As you can see, these two techniques—using helper methods and converting\nclasses into value objects—are applicable only occasionally. And even when these tech-\nniques are applicable, state-based tests still take up more space than output-based tests\nand thus remain less maintainable. \nMAINTAINABILITY OF COMMUNICATION-BASED TESTS\nCommunication-based tests score worse than output-based and state-based tests on\nthe maintainability metric. Communication-based testing requires setting up test dou-\nbles and interaction assertions, and that takes up a lot of space. Tests become even\nlarger and less maintainable when you have mock chains (mocks or stubs returning\nother mocks, which also return mocks, and so on, several layers deep). \n6.2.4\nComparing the styles: The results\nLet’s now compare the styles of unit testing using the attributes of a good unit test.\nTable 6.1 sums up the comparison results. As discussed in section 6.2.1, all three styles\nscore equally with the metrics of protection against regressions and feedback speed;\nhence, I’m omitting these metrics from the comparison.\n Output-based testing shows the best results. This style produces tests that rarely\ncouple to implementation details and thus don’t require much due diligence to main-\ntain proper resistance to refactoring. Such tests are also the most maintainable due to\ntheir conciseness and lack of out-of-process dependencies.\nListing 6.6\nComment compared by value\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2484,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 150,
      "content": "128\nCHAPTER 6\nStyles of unit testing\nState-based and communication-based tests are worse on both metrics. These are\nmore likely to couple to a leaking implementation detail, and they also incur higher\nmaintenance costs due to being larger in size.\n Always prefer output-based testing over everything else. Unfortunately, it’s easier\nsaid than done. This style of unit testing is only applicable to code that is written in a\nfunctional way, which is rarely the case for most object-oriented programming lan-\nguages. Still, there are techniques you can use to transition more of your tests toward\nthe output-based style.\n The rest of this chapter shows how to transition from state-based and collaboration-\nbased testing to output-based testing. The transition requires you to make your code\nmore purely functional, which, in turn, enables the use of output-based tests instead\nof state- or communication-based ones. \n6.3\nUnderstanding functional architecture\nSome groundwork is needed before I can show how to make the transition. In this sec-\ntion, you’ll see what functional programming and functional architecture are and\nhow the latter relates to the hexagonal architecture. Section 6.4 illustrates the transi-\ntion using an example.\n Note that this isn’t a deep dive into the topic of functional programming, but\nrather an explanation of the basic principles behind it. These basic principles should\nbe enough to understand the connection between functional programming and out-\nput-based testing. For a deeper look at functional programming, see Scott Wlaschin’s\nwebsite and books at https://fsharpforfunandprofit.com/books.\n6.3.1\nWhat is functional programming?\nAs I mentioned in section 6.1.1, the output-based unit testing style is also known as\nfunctional. That’s because it requires the underlying production code to be written in\na purely functional way, using functional programming. So, what is functional pro-\ngramming?\n Functional programming is programming with mathematical functions. A mathemati-\ncal function (also known as pure function) is a function (or method) that doesn’t have\nany hidden inputs or outputs. All inputs and outputs of a mathematical function must\nbe explicitly expressed in its method signature, which consists of the method’s name,\narguments, and return type. A mathematical function produces the same output for a\ngiven input regardless of how many times it is called.\nTable 6.1\nThe three styles of unit testing: The comparisons\nOutput-based\nState-based\nCommunication-based\nDue diligence to maintain \nresistance to refactoring\nLow\nMedium\nMedium\nMaintainability costs\nLow\nMedium\nHigh\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2676,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 151,
      "content": "129\nUnderstanding functional architecture\n Let’s take the CalculateDiscount() method from listing 6.1 as an example (I’m\ncopying it here for convenience):\npublic decimal CalculateDiscount(Product[] products)\n{\ndecimal discount = products.Length * 0.01m;\nreturn Math.Min(discount, 0.2m);\n}\nThis method has one input (a Product array) and one output (the decimal dis-\ncount), both of which are explicitly expressed in the method’s signature. There are\nno hidden inputs or outputs. This makes CalculateDiscount() a mathematical func-\ntion (figure 6.5).\nMethods with no hidden inputs and outputs are called mathematical functions\nbecause such methods adhere to the definition of a function in mathematics.\nDEFINITION\nIn mathematics, a function is a relationship between two sets that\nfor each element in the first set, finds exactly one element in the second set.\nFigure 6.6 shows how for each input number x, function f(x) = x + 1 finds a corre-\nsponding number y. Figure 6.7 displays the CalculateDiscount() method using the\nsame notation as in figure 6.6.\npublic         CalculateDiscount\ndecimal\n(Product[] products)\nMethod signature\nOutput\nName\nInput\nFigure 6.5\nCalculateDiscount() has one input (a Product array) and \none output (the decimal discount). Both the input and the output are explicitly \nexpressed in the method’s signature, which makes CalculateDiscount() \na mathematical function.\nY\n1\n2\n3\n4\n2\n3\n4\n5\nf(x) = x + 1\nX\nFigure 6.6\nA typical example of a function in \nmathematics is f(x) = x + 1. For each input \nnumber x in set X, the function finds a \ncorresponding number y in set Y.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1646,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 152,
      "content": "130\nCHAPTER 6\nStyles of unit testing\nExplicit inputs and outputs make mathematical functions extremely testable because\nthe resulting tests are short, simple, and easy to understand and maintain. Mathe-\nmatical functions are the only type of methods where you can apply output-based\ntesting, which has the best maintainability and the lowest chance of producing a\nfalse positive.\n On the other hand, hidden inputs and outputs make the code less testable (and\nless readable, too). Types of such hidden inputs and outputs include the following:\nSide effects—A side effect is an output that isn’t expressed in the method signature\nand, therefore, is hidden. An operation creates a side effect when it mutates the\nstate of a class instance, updates a file on the disk, and so on.\nExceptions—When a method throws an exception, it creates a path in the pro-\ngram flow that bypasses the contract established by the method’s signature. The\nthrown exception can be caught anywhere in the call stack, thus introducing an\nadditional output that the method signature doesn’t convey.\nA reference to an internal or external state—For example, a method can get the cur-\nrent date and time using a static property such as DateTime.Now. It can query\ndata from the database, or it can refer to a private mutable field. These are all\ninputs to the execution flow that aren’t present in the method signature and,\ntherefore, are hidden.\nA good rule of thumb when determining whether a method is a mathematical func-\ntion is to see if you can replace a call to that method with its return value without\nchanging the program’s behavior. The ability to replace a method call with the\ncorresponding value is known as referential transparency. Look at the following method,\nfor example:\nArrays of products\nProduct(“Soap”)\nProduct(“Hand wash”)\nProduct(“Shampoo”)\nProduct(“Soap”)\nProduct(“Sea salt”)\nDiscounts\n0.02\n0.01\nCalculateDiscount()\nFigure 6.7\nThe CalculateDiscount() method represented using the same \nnotation as the function f(x) = x + 1. For each input array of products, the \nmethod finds a corresponding discount as an output.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2166,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 153,
      "content": "131\nUnderstanding functional architecture\npublic int Increment(int x)\n{\nreturn x + 1;\n}\nThis method is a mathematical function. These two statements are equivalent to\neach other:\nint y = Increment(4);\nint y = 5;\nOn the other hand, the following method is not a mathematical function. You can’t\nreplace it with the return value because that return value doesn’t represent all of the\nmethod’s outputs. In this example, the hidden output is the change to field x (a side\neffect):\nint x = 0;\npublic int Increment()\n{\nx++;\nreturn x;\n}\nSide effects are the most prevalent type of hidden outputs. The following listing shows\nan AddComment method that looks like a mathematical function on the surface but\nactually isn’t one. Figure 6.8 shows the method graphically.\npublic Comment AddComment(string text)\n{\nvar comment = new Comment(text);\n_comments.Add(comment);\n   \nreturn comment;\n}\nListing 6.7\nModification of an internal state\nSide effect \nText\nComment\nSide effect\nMethod\nsignature\nHidden\npart\nf\nFigure 6.8\nMethod AddComment (shown as f) \nhas a text input and a Comment output, which \nare both expressed in the method signature. The \nside effect is an additional hidden output.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1227,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 154,
      "content": "132\nCHAPTER 6\nStyles of unit testing\n6.3.2\nWhat is functional architecture?\nYou can’t create an application that doesn’t incur any side effects whatsoever, of\ncourse. Such an application would be impractical. After all, side effects are what you\ncreate all applications for: updating the user’s information, adding a new order line to\nthe shopping cart, and so on.\n The goal of functional programming is not to eliminate side effects altogether but\nrather to introduce a separation between code that handles business logic and code\nthat incurs side effects. These two responsibilities are complex enough on their own;\nmixing them together multiplies the complexity and hinders code maintainability in\nthe long run. This is where functional architecture comes into play. It separates busi-\nness logic from side effects by pushing those side effects to the edges of a business operation.\nDEFINITION\nFunctional architecture maximizes the amount of code written in a\npurely functional (immutable) way, while minimizing code that deals with\nside effects. Immutable means unchangeable: once an object is created, its\nstate can’t be modified. This is in contrast to a mutable object (changeable\nobject), which can be modified after it is created.\nThe separation between business logic and side effects is done by segregating two\ntypes of code:\nCode that makes a decision—This code doesn’t require side effects and thus can\nbe written using mathematical functions.\nCode that acts upon that decision—This code converts all the decisions made by\nthe mathematical functions into visible bits, such as changes in the database or\nmessages sent to a bus.\nThe code that makes decisions is often referred to as a functional core (also known as an\nimmutable core). The code that acts upon those decisions is a mutable shell (figure 6.9).\nInput\nDecisions\nFunctional core\nMutable shell\nFigure 6.9\nIn functional architecture, \nthe functional core is implemented using \nmathematical functions and makes all \ndecisions in the application. The mutable \nshell provides the functional core with \ninput data and interprets its decisions by \napplying side effects to out-of-process \ndependencies such as a database.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2242,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 155,
      "content": "133\nUnderstanding functional architecture\nThe functional core and the mutable shell cooperate in the following way:\nThe mutable shell gathers all the inputs.\nThe functional core generates decisions.\nThe shell converts the decisions into side effects.\nTo maintain a proper separation between these two layers, you need to make sure the\nclasses representing the decisions contain enough information for the mutable shell\nto act upon them without additional decision-making. In other words, the mutable\nshell should be as dumb as possible. The goal is to cover the functional core exten-\nsively with output-based tests and leave the mutable shell to a much smaller number of\nintegration tests.\n6.3.3\nComparing functional and hexagonal architectures\nThere are a lot of similarities between functional and hexagonal architectures. Both\nof them are built around the idea of separation of concerns. The details of that sepa-\nration vary, though.\n As you may remember from chapter 5, the hexagonal architecture differentiates\nthe domain layer and the application services layer (figure 6.10). The domain layer is\naccountable for business logic while the application services layer, for communication with\nEncapsulation and immutability\nLike encapsulation, functional architecture (in general) and immutability (in particular)\nserve the same goal as unit testing: enabling sustainable growth of your software\nproject. In fact, there’s a deep connection between the concepts of encapsulation\nand immutability.\nAs you may remember from chapter 5, encapsulation is the act of protecting your\ncode against inconsistencies. Encapsulation safeguards the class’s internals from\ncorruption by\nReducing the API surface area that allows for data modification\nPutting the remaining APIs under scrutiny\nImmutability tackles this issue of preserving invariants from another angle. With\nimmutable classes, you don’t need to worry about state corruption because it’s impos-\nsible to corrupt something that cannot be changed in the first place. As a conse-\nquence, there’s no need for encapsulation in functional programming. You only need\nto validate the class’s state once, when you create an instance of it. After that, you\ncan freely pass this instance around. When all your data is immutable, the whole set\nof issues related to the lack of encapsulation simply vanishes.\nThere’s a great quote from Michael Feathers in that regard:\nObject-oriented programming makes code understandable by encapsulating mov-\ning parts. Functional programming makes code understandable by minimizing\nmoving parts.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2631,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 156,
      "content": "134\nCHAPTER 6\nStyles of unit testing\nexternal applications such as a database or an SMTP service. This is very similar to func-\ntional architecture, where you introduce the separation of decisions and actions.\n Another similarity is the one-way flow of dependencies. In the hexagonal architec-\nture, classes inside the domain layer should only depend on each other; they should\nnot depend on classes from the application services layer. Likewise, the immutable\ncore in functional architecture doesn’t depend on the mutable shell. It’s self-sufficient\nand can work in isolation from the outer layers. This is what makes functional archi-\ntecture so testable: you can strip the immutable core from the mutable shell entirely\nand simulate the inputs that the shell provides using simple values.\n The difference between the two is in their treatment of side effects. Functional\narchitecture pushes all side effects out of the immutable core to the edges of a busi-\nness operation. These edges are handled by the mutable shell. On the other hand, the\nhexagonal architecture is fine with side effects made by the domain layer, as long as\nthey are limited to that domain layer only. All modifications in hexagonal architecture\nshould be contained within the domain layer and not cross that layer’s boundary. For\nexample, a domain class instance can’t persist something to the database directly, but\nit can change its own state. An application service will then pick up this change and\napply it to the database.\nNOTE\nFunctional architecture is a subset of the hexagonal architecture. You\ncan view functional architecture as the hexagonal architecture taken to an\nextreme. \nDomain\n(business logic)\nApplication\nservices\nThird-party\nsystem\nMessage\nbus\nSMTP\nservice\nFigure 6.10\nHexagonal architecture is a set of interacting \napplications—hexagons. Your application consists of a domain \nlayer and an application services layer, which correspond to a \nfunctional core and a mutable shell in functional architecture.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2055,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 157,
      "content": "135\nTransitioning to functional architecture and output-based testing\n6.4\nTransitioning to functional architecture and output-\nbased testing\nIn this section, we’ll take a sample application and refactor it toward functional archi-\ntecture. You’ll see two refactoring stages:\nMoving from using an out-of-process dependency to using mocks\nMoving from using mocks to using functional architecture\nThe transition affects test code, too! We’ll refactor state-based and communication-\nbased tests to the output-based style of unit testing. Before starting the refactoring,\nlet’s review the sample project and tests covering it.\n6.4.1\nIntroducing an audit system\nThe sample project is an audit system that keeps track of all visitors in an organization.\nIt uses flat text files as underlying storage with the structure shown in figure 6.11. The\nsystem appends the visitor’s name and the time of their visit to the end of the most\nrecent file. When the maximum number of entries per file is reached, a new file with\nan incremented index is created.\nThe following listing shows the initial version of the system.\npublic class AuditManager\n{\nprivate readonly int _maxEntriesPerFile;\nprivate readonly string _directoryName;\npublic AuditManager(int maxEntriesPerFile, string directoryName)\n{\n_maxEntriesPerFile = maxEntriesPerFile;\n_directoryName = directoryName;\n}\nListing 6.8\nInitial implementation of the audit system\nJane;\nJack;\nPeter; 2019-04-06T16:30:00\n2019-04-06T16:40:00\n2019-04-06T17:00:00\nMary;\n2019-04-06T17:30:00\nNew Person; Time of visit\naudit_01.txt\naudit_02.txt\nFigure 6.11\nThe audit system stores information \nabout visitors in text files with a specific format. \nWhen the maximum number of entries per file is \nreached, the system creates a new file.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1810,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 158,
      "content": "136\nCHAPTER 6\nStyles of unit testing\npublic void AddRecord(string visitorName, DateTime timeOfVisit)\n{\nstring[] filePaths = Directory.GetFiles(_directoryName);\n(int index, string path)[] sorted = SortByIndex(filePaths);\nstring newRecord = visitorName + ';' + timeOfVisit;\nif (sorted.Length == 0)\n{\nstring newFile = Path.Combine(_directoryName, \"audit_1.txt\");\nFile.WriteAllText(newFile, newRecord);\nreturn;\n}\n(int currentFileIndex, string currentFilePath) = sorted.Last();\nList<string> lines = File.ReadAllLines(currentFilePath).ToList();\nif (lines.Count < _maxEntriesPerFile)\n{\nlines.Add(newRecord);\nstring newContent = string.Join(\"\\r\\n\", lines);\nFile.WriteAllText(currentFilePath, newContent);\n}\nelse\n{\nint newIndex = currentFileIndex + 1;\nstring newName = $\"audit_{newIndex}.txt\";\nstring newFile = Path.Combine(_directoryName, newName);\nFile.WriteAllText(newFile, newRecord);\n}\n}\n}\nThe code might look a bit large, but it’s quite simple. AuditManager is the main class\nin the application. Its constructor accepts the maximum number of entries per file\nand the working directory as configuration parameters. The only public method in\nthe class is AddRecord, which does all the work of the audit system:\nRetrieves a full list of files from the working directory\nSorts them by index (all filenames follow the same pattern: audit_{index}.txt\n[for example, audit_1.txt])\nIf there are no audit files yet, creates a first one with a single record\nIf there are audit files, gets the most recent one and either appends the new\nrecord to it or creates a new file, depending on whether the number of entries\nin that file has reached the limit\nThe AuditManager class is hard to test as-is, because it’s tightly coupled to the file-\nsystem. Before the test, you’d need to put files in the right place, and after the test\nfinishes, you’d read those files, check their contents, and clear them out (figure 6.12).\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1957,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 159,
      "content": "137\nTransitioning to functional architecture and output-based testing\nYou won’t be able to parallelize such tests—at least, not without additional effort\nthat would significantly increase maintenance costs. The bottleneck is the filesys-\ntem: it’s a shared dependency through which tests can interfere with each other’s\nexecution flow.\n The filesystem also makes the tests slow. Maintainability suffers, too, because you\nhave to make sure the working directory exists and is accessible to tests—both on your\nlocal machine and on the build server. Table 6.2 sums up the scoring.\nBy the way, tests working directly with the filesystem don’t fit the definition of a unit\ntest. They don’t comply with the second and the third attributes of a unit test, thereby\nfalling into the category of integration tests (see chapter 2 for more details):\nA unit test verifies a single unit of behavior,\nDoes it quickly,\nAnd does it in isolation from other tests. \n6.4.2\nUsing mocks to decouple tests from the filesystem\nThe usual solution to the problem of tightly coupled tests is to mock the filesystem.\nYou can extract all operations on files into a separate class (IFileSystem) and inject\nthat class into AuditManager via the constructor. The tests will then mock this class\nand capture the writes the audit system do to the files (figure 6.13).\n \n \n \nTable 6.2\nThe initial version of the audit system scores badly on two out \nof the four attributes of a good test.\nInitial version\nProtection against regressions\nGood\nResistance to refactoring\nGood\nFast feedback\nBad\nMaintainability\nBad\nAudit system\nFilesystem\nTest\ninput\ninput\ninput\noutput\nassert\nFigure 6.12\nTests covering the initial version of the audit system would \nhave to work directly with the filesystem.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1806,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 160,
      "content": "138\nCHAPTER 6\nStyles of unit testing\nThe following listing shows how the filesystem is injected into AuditManager.\npublic class AuditManager\n{\nprivate readonly int _maxEntriesPerFile;\nprivate readonly string _directoryName;\nprivate readonly IFileSystem _fileSystem;    \npublic AuditManager(\nint maxEntriesPerFile,\nstring directoryName,\nIFileSystem fileSystem)\n{\n_maxEntriesPerFile = maxEntriesPerFile;\n_directoryName = directoryName;\n_fileSystem = fileSystem;                \n}\n}\nAnd next is the AddRecord method.\npublic void AddRecord(string visitorName, DateTime timeOfVisit)\n{\nstring[] filePaths = _fileSystem                                \n.GetFiles(_directoryName);                                  \n(int index, string path)[] sorted = SortByIndex(filePaths);\nstring newRecord = visitorName + ';' + timeOfVisit;\nif (sorted.Length == 0)\n{\nstring newFile = Path.Combine(_directoryName, \"audit_1.txt\");\n_fileSystem.WriteAllText(                                   \nnewFile, newRecord);                                    \nreturn;\n}\nListing 6.9\nInjecting the filesystem explicitly via the constructor\nListing 6.10\nUsing the new IFileSystem interface\nmock\nstub\ninput\nAudit system\nTest\nFilesystem\nFigure 6.13\nTests can mock the \nfilesystem and capture the writes \nthe audit system makes to the files.\nThe new interface \nrepresents the \nfilesystem.\nThe new\ninterface\nin action\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1426,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 161,
      "content": "139\nTransitioning to functional architecture and output-based testing\n(int currentFileIndex, string currentFilePath) = sorted.Last();\nList<string> lines = _fileSystem\n          \n.ReadAllLines(currentFilePath);          \nif (lines.Count < _maxEntriesPerFile)\n{\nlines.Add(newRecord);\nstring newContent = string.Join(\"\\r\\n\", lines);\n_fileSystem.WriteAllText(\n        \ncurrentFilePath, newContent);        \n}\nelse\n{\nint newIndex = currentFileIndex + 1;\nstring newName = $\"audit_{newIndex}.txt\";\nstring newFile = Path.Combine(_directoryName, newName);\n_fileSystem.WriteAllText(                \nnewFile, newRecord);                 \n}\n}\nIn listing 6.10, IFileSystem is a new custom interface that encapsulates the work with\nthe filesystem:\npublic interface IFileSystem\n{\nstring[] GetFiles(string directoryName);\nvoid WriteAllText(string filePath, string content);\nList<string> ReadAllLines(string filePath);\n}\nNow that AuditManager is decoupled from the filesystem, the shared dependency is\ngone, and tests can execute independently from each other. Here’s one such test.\n[Fact]\npublic void A_new_file_is_created_when_the_current_file_overflows()\n{\nvar fileSystemMock = new Mock<IFileSystem>();\nfileSystemMock\n.Setup(x => x.GetFiles(\"audits\"))\n.Returns(new string[]\n{\n@\"audits\\audit_1.txt\",\n@\"audits\\audit_2.txt\"\n});\nfileSystemMock\n.Setup(x => x.ReadAllLines(@\"audits\\audit_2.txt\"))\n.Returns(new List<string>\n{\n\"Peter; 2019-04-06T16:30:00\",\n\"Jane; 2019-04-06T16:40:00\",\nListing 6.11\nChecking the audit system’s behavior using a mock\nThe new\ninterface\nin action\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1606,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 162,
      "content": "140\nCHAPTER 6\nStyles of unit testing\n\"Jack; 2019-04-06T17:00:00\"\n});\nvar sut = new AuditManager(3, \"audits\", fileSystemMock.Object);\nsut.AddRecord(\"Alice\", DateTime.Parse(\"2019-04-06T18:00:00\"));\nfileSystemMock.Verify(x => x.WriteAllText(\n@\"audits\\audit_3.txt\",\n\"Alice;2019-04-06T18:00:00\"));\n}\nThis test verifies that when the number of entries in the current file reaches the limit\n(3, in this example), a new file with a single audit entry is created. Note that this is a\nlegitimate use of mocks. The application creates files that are visible to end users\n(assuming that those users use another program to read the files, be it specialized soft-\nware or a simple notepad.exe). Therefore, communications with the filesystem and\nthe side effects of these communications (that is, the changes in files) are part of the\napplication’s observable behavior. As you may remember from chapter 5, that’s the\nonly legitimate use case for mocking.\n This alternative implementation is an improvement over the initial version. Since\ntests no longer access the filesystem, they execute faster. And because you don’t need\nto look after the filesystem to keep the tests happy, the maintenance costs are also\nreduced. Protection against regressions and resistance to refactoring didn’t suffer\nfrom the refactoring either. Table 6.3 shows the differences between the two versions.\nWe can still do better, though. The test in listing 6.11 contains convoluted setups,\nwhich is less than ideal in terms of maintenance costs. Mocking libraries try their best\nto be helpful, but the resulting tests are still not as readable as those that rely on plain\ninput and output. \n6.4.3\nRefactoring toward functional architecture\nInstead of hiding side effects behind an interface and injecting that interface into\nAuditManager, you can move those side effects out of the class entirely. Audit-\nManager is then only responsible for making a decision about what to do with the\nfiles. A new class, Persister, acts on that decision and applies updates to the filesys-\ntem (figure 6.14).\nTable 6.3\nThe version with mocks compared to the initial version of the audit system\nInitial version\nWith mocks\nProtection against regressions\nGood\nGood\nResistance to refactoring\nGood\nGood\nFast feedback\nBad\nGood\nMaintainability\nBad\nModerate\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2347,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 163,
      "content": "141\nTransitioning to functional architecture and output-based testing\nPersister in this scenario acts as a mutable shell, while AuditManager becomes a func-\ntional (immutable) core. The following listing shows AuditManager after the refactoring.\npublic class AuditManager\n{\nprivate readonly int _maxEntriesPerFile;\npublic AuditManager(int maxEntriesPerFile)\n{\n_maxEntriesPerFile = maxEntriesPerFile;\n}\npublic FileUpdate AddRecord(\nFileContent[] files,\nstring visitorName,\nDateTime timeOfVisit)\n{\n(int index, FileContent file)[] sorted = SortByIndex(files);\nstring newRecord = visitorName + ';' + timeOfVisit;\nif (sorted.Length == 0)\n{\nreturn new FileUpdate(\n  \n\"audit_1.txt\", newRecord);  \n}\n(int currentFileIndex, FileContent currentFile) = sorted.Last();\nList<string> lines = currentFile.Lines.ToList();\nListing 6.12\nThe AuditManager class after refactoring\nFileContent\nFileUpdate\nAuditManager\n(functional core)\nPersister\n(mutable shell)\nFigure 6.14\nPersister and \nAuditManager form the functional \narchitecture. Persister gathers files \nand their contents from the working \ndirectory, feeds them to AuditManager, \nand then converts the return value into \nchanges in the filesystem.\nReturns an update \ninstruction\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1267,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 164,
      "content": "142\nCHAPTER 6\nStyles of unit testing\nif (lines.Count < _maxEntriesPerFile)\n{\nlines.Add(newRecord);\nstring newContent = string.Join(\"\\r\\n\", lines);\nreturn new FileUpdate(\n     \ncurrentFile.FileName, newContent);     \n}\nelse\n{\nint newIndex = currentFileIndex + 1;\nstring newName = $\"audit_{newIndex}.txt\";\nreturn new FileUpdate(\n                   \nnewName, newRecord);                   \n}\n}\n}\nInstead of the working directory path, AuditManager now accepts an array of File-\nContent. This class includes everything AuditManager needs to know about the filesys-\ntem to make a decision:\npublic class FileContent\n{\npublic readonly string FileName;\npublic readonly string[] Lines;\npublic FileContent(string fileName, string[] lines)\n{\nFileName = fileName;\nLines = lines;\n}\n}\nAnd, instead of mutating files in the working directory, AuditManager now returns an\ninstruction for the side effect it would like to perform:\npublic class FileUpdate\n{\npublic readonly string FileName;\npublic readonly string NewContent;\npublic FileUpdate(string fileName, string newContent)\n{\nFileName = fileName;\nNewContent = newContent;\n}\n}\nThe following listing shows the Persister class.\n \n \nReturns an \nupdate \ninstruction\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1250,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 165,
      "content": "143\nTransitioning to functional architecture and output-based testing\npublic class Persister\n{\npublic FileContent[] ReadDirectory(string directoryName)\n{\nreturn Directory\n.GetFiles(directoryName)\n.Select(x => new FileContent(\nPath.GetFileName(x),\nFile.ReadAllLines(x)))\n.ToArray();\n}\npublic void ApplyUpdate(string directoryName, FileUpdate update)\n{\nstring filePath = Path.Combine(directoryName, update.FileName);\nFile.WriteAllText(filePath, update.NewContent);\n}\n}\nNotice how trivial this class is. All it does is read content from the working directory\nand apply updates it receives from AuditManager back to that working directory. It has\nno branching (no if statements); all the complexity resides in the AuditManager\nclass. This is the separation between business logic and side effects in action.\n To maintain such a separation, you need to keep the interface of FileContent and\nFileUpdate as close as possible to that of the framework’s built-in file-interaction com-\nmands. All the parsing and preparation should be done in the functional core, so that\nthe code outside of that core remains trivial. For example, if .NET didn’t contain the\nbuilt-in File.ReadAllLines() method, which returns the file content as an array of\nlines, and only has File.ReadAllText(), which returns a single string, you’d need to\nreplace the Lines property in FileContent with a string too and do the parsing in\nAuditManager:\npublic class FileContent\n{\npublic readonly string FileName;\npublic readonly string Text; // previously, string[] Lines;\n}\nTo glue AuditManager and Persister together, you need another class: an applica-\ntion service in the hexagonal architecture taxonomy, as shown in the following listing.\npublic class ApplicationService\n{\nprivate readonly string _directoryName;\nprivate readonly AuditManager _auditManager;\nprivate readonly Persister _persister;\nListing 6.13\nThe mutable shell acting on AuditManager’s decision\nListing 6.14\nGluing together the functional core and mutable shell \nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2046,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 166,
      "content": "144\nCHAPTER 6\nStyles of unit testing\npublic ApplicationService(\nstring directoryName, int maxEntriesPerFile)\n{\n_directoryName = directoryName;\n_auditManager = new AuditManager(maxEntriesPerFile);\n_persister = new Persister();\n}\npublic void AddRecord(string visitorName, DateTime timeOfVisit)\n{\nFileContent[] files = _persister.ReadDirectory(_directoryName);\nFileUpdate update = _auditManager.AddRecord(\nfiles, visitorName, timeOfVisit);\n_persister.ApplyUpdate(_directoryName, update);\n}\n}\nAlong with gluing the functional core together with the mutable shell, the application\nservice also provides an entry point to the system for external clients (figure 6.15).\nWith this implementation, it becomes easy to check the audit system’s behavior. All\ntests now boil down to supplying a hypothetical state of the working directory and ver-\nifying the decision AuditManager makes.\n[Fact]\npublic void A_new_file_is_created_when_the_current_file_overflows()\n{\nvar sut = new AuditManager(3);\nvar files = new FileContent[]\n{\nnew FileContent(\"audit_1.txt\", new string[0]),\nListing 6.15\nThe test without mocks\nAudit manager\nPersister\nPersister\nApplication service\nExternal client\nFigure 6.15\nApplicationService glues the functional core (AuditManager) \nand the mutable shell (Persister) together and provides an entry point for external \nclients. In the hexagonal architecture taxonomy, ApplicationService and \nPersister are part of the application services layer, while AuditManager \nbelongs to the domain model.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1553,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 167,
      "content": "145\nTransitioning to functional architecture and output-based testing\nnew FileContent(\"audit_2.txt\", new string[]\n{\n\"Peter; 2019-04-06T16:30:00\",\n\"Jane; 2019-04-06T16:40:00\",\n\"Jack; 2019-04-06T17:00:00\"\n})\n};\nFileUpdate update = sut.AddRecord(\nfiles, \"Alice\", DateTime.Parse(\"2019-04-06T18:00:00\"));\nAssert.Equal(\"audit_3.txt\", update.FileName);\nAssert.Equal(\"Alice;2019-04-06T18:00:00\", update.NewContent);\n}\nThis test retains the improvement the test with mocks made over the initial version\n(fast feedback) but also further improves on the maintainability metric. There’s no\nneed for complex mock setups anymore, only plain inputs and outputs, which helps\nthe test’s readability a lot. Table 6.4 compares the output-based test with the initial ver-\nsion and the version with mocks.\nNotice that the instructions generated by a functional core are always a value or a set of\nvalues. Two instances of such a value are interchangeable as long as their contents\nmatch. You can take advantage of this fact and improve test readability even further by\nturning FileUpdate into a value object. To do that in .NET, you need to either convert\nthe class into a struct or define custom equality members. That will give you compar-\nison by value, as opposed to the comparison by reference, which is the default behavior\nfor classes in C#. Comparison by value also allows you to compress the two assertions\nfrom listing 6.15 into one:\nAssert.Equal(\nnew FileUpdate(\"audit_3.txt\", \"Alice;2019-04-06T18:00:00\"),\nupdate);\nOr, using Fluent Assertions,\nupdate.Should().Be(\nnew FileUpdate(\"audit_3.txt\", \"Alice;2019-04-06T18:00:00\"));\nTable 6.4\nThe output-based test compared to the previous two versions\nInitial version\nWith mocks\nOutput-based\nProtection against regressions\nGood\nGood\nGood\nResistance to refactoring\nGood\nGood\nGood\nFast feedback\nBad\nGood\nGood\nMaintainability\nBad\nModerate\nGood\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1926,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 168,
      "content": "146\nCHAPTER 6\nStyles of unit testing\n6.4.4\nLooking forward to further developments\nLet’s step back for a minute and look at further developments that could be done in\nour sample project. The audit system I showed you is quite simple and contains only\nthree branches:\nCreating a new file in case of an empty working directory\nAppending a new record to an existing file\nCreating another file when the number of entries in the current file exceeds\nthe limit\nAlso, there’s only one use case: addition of a new entry to the audit log. What if\nthere were another use case, such as deleting all mentions of a particular visitor?\nAnd what if the system needed to do validations (say, for the maximum length of the\nvisitor’s name)?\n Deleting all mentions of a particular visitor could potentially affect several files, so\nthe new method would need to return multiple file instructions:\npublic FileUpdate[] DeleteAllMentions(\nFileContent[] files, string visitorName)\nFurthermore, business people might require that you not keep empty files in the\nworking directory. If the deleted entry was the last entry in an audit file, you would\nneed to remove that file altogether. To implement this requirement, you could\nrename FileUpdate to FileAction and introduce an additional ActionType enum\nfield to indicate whether it was an update or a deletion.\n Error handling also becomes simpler and more explicit with functional architec-\nture. You could embed errors into the method’s signature, either in the FileUpdate\nclass or as a separate component:\npublic (FileUpdate update, Error error) AddRecord(\nFileContent[] files,\nstring visitorName,\nDateTime timeOfVisit)\nThe application service would then check for this error. If it was there, the service\nwouldn’t pass the update instruction to the persister, instead propagating an error\nmessage to the user. \n6.5\nUnderstanding the drawbacks of functional \narchitecture\nUnfortunately, functional architecture isn’t always attainable. And even when it is, the\nmaintainability benefits are often offset by a performance impact and increase in\nthe size of the code base. In this section, we’ll explore the costs and the trade-offs\nattached to functional architecture.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2249,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 169,
      "content": "147\nUnderstanding the drawbacks of functional architecture\n6.5.1\nApplicability of functional architecture\nFunctional architecture worked for our audit system because this system could gather\nall the inputs up front, before making a decision. Often, though, the execution flow is\nless straightforward. You might need to query additional data from an out-of-process\ndependency, based on an intermediate result of the decision-making process.\n Here’s an example. Let’s say the audit system needs to check the visitor’s access\nlevel if the number of times they have visited during the last 24 hours exceeds some\nthreshold. And let’s also assume that all visitors’ access levels are stored in a database.\nYou can’t pass an IDatabase instance to AuditManager like this:\npublic FileUpdate AddRecord(\nFileContent[] files, string visitorName,\nDateTime timeOfVisit, IDatabase database\n)\nSuch an instance would introduce a hidden input to the AddRecord() method. This\nmethod would, therefore, cease to be a mathematical function (figure 6.16), which\nmeans you would no longer be able to apply output-based testing.\nThere are two solutions in such a situation:\nYou can gather the visitor’s access level in the application service up front,\nalong with the directory content.\nYou can introduce a new method such as IsAccessLevelCheckRequired() in\nAuditManager. The application service would call this method before Add-\nRecord(), and if it returned true, the service would get the access level from\nthe database and pass it to AddRecord().\nBoth approaches have drawbacks. The first one concedes performance—it uncondi-\ntionally queries the database, even in cases when the access level is not required. But this\napproach keeps the separation of business logic and communication with external\nApplication\nservice\nReadDirectory\nAudit manager\nFilesystem\nand database\nAdd\nrecord\nApplyUpdate\nGet\naccess\nlevel\nFigure 6.16\nA dependency on the database introduces a hidden input to \nAuditManager. Such a class is no longer purely functional, and the whole \napplication no longer follows the functional architecture.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2147,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 170,
      "content": "148\nCHAPTER 6\nStyles of unit testing\nsystems fully intact: all decision-making resides in AuditManager as before. The second\napproach concedes a degree of that separation for performance gains: the decision as\nto whether to call the database now goes to the application service, not AuditManager.\n Note that, unlike these two options, making the domain model (AuditManager)\ndepend on the database isn’t a good idea. I’ll explain more about keeping the balance\nbetween performance and separation of concerns in the next two chapters.\nNOTE\nA class from the functional core should work not with a collaborator,\nbut with the product of its work, a value. \n6.5.2\nPerformance drawbacks\nThe performance impact on the system as a whole is a common argument against\nfunctional architecture. Note that it’s not the performance of tests that suffers. The\noutput-based tests we ended up with work as fast as the tests with mocks. It’s that the\nsystem itself now has to do more calls to out-of-process dependencies and becomes\nless performant. The initial version of the audit system didn’t read all files from the\nworking directory, and neither did the version with mocks. But the final version does\nin order to comply with the read-decide-act approach.\n The choice between a functional architecture and a more traditional one is a\ntrade-off between performance and code maintainability (both production and test\ncode). In some systems where the performance impact is not as noticeable, it’s better\nto go with functional architecture for additional gains in maintainability. In others,\nyou might need to make the opposite choice. There’s no one-size-fits-all solution. \nCollaborators vs. values\nYou may have noticed that AuditManager’s AddRecord() method has a dependency\nthat’s not present in its signature: the _maxEntriesPerFile field. The audit man-\nager refers to this field to make a decision to either append an existing audit file or\ncreate a new one.\nAlthough this dependency isn’t present among the method’s arguments, it’s not hid-\nden. It can be derived from the class’s constructor signature. And because the _max-\nEntriesPerFile field is immutable, it stays the same between the class instantiation\nand the call to AddRecord(). In other words, that field is a value.\nThe situation with the IDatabase dependency is different because it’s a collaborator,\nnot a value like _maxEntriesPerFile. As you may remember from chapter 2, a col-\nlaborator is a dependency that is one or the other of the following:\nMutable (allows for modification of its state)\nA proxy to data that is not yet in memory (a shared dependency)\nThe IDatabase instance falls into the second category and, therefore, is a collabo-\nrator. It requires an additional call to an out-of-process dependency and thus pre-\ncludes the use of output-based testing.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2876,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 171,
      "content": "149\nSummary\n6.5.3\nIncrease in the code base size\nThe same is true for the size of the code base. Functional architecture requires a clear\nseparation between the functional (immutable) core and the mutable shell. This\nnecessitates additional coding initially, although it ultimately results in reduced code\ncomplexity and gains in maintainability.\n Not all projects exhibit a high enough degree of complexity to justify such an initial\ninvestment, though. Some code bases aren’t that significant from a business perspec-\ntive or are just plain too simple. It doesn’t make sense to use functional architecture\nin such projects because the initial investment will never pay off. Always apply func-\ntional architecture strategically, taking into account the complexity and importance of\nyour system.\n Finally, don’t go for purity of the functional approach if that purity comes at too\nhigh a cost. In most projects, you won’t be able to make the domain model fully\nimmutable and thus can’t rely solely on output-based tests, at least not when using an\nOOP language like C# or Java. In most cases, you’ll have a combination of output-\nbased and state-based styles, with a small mix of communication-based tests, and that’s\nfine. The goal of this chapter is not to incite you to transition all your tests toward the\noutput-based style; the goal is to transition as many of them as reasonably possible.\nThe difference is subtle but important. \nSummary\nOutput-based testing is a style of testing where you feed an input to the SUT and\ncheck the output it produces. This style of testing assumes there are no hidden\ninputs or outputs, and the only result of the SUT’s work is the value it returns.\nState-based testing verifies the state of the system after an operation is completed.\nIn communication-based testing, you use mocks to verify communications between\nthe system under test and its collaborators.\nThe classical school of unit testing prefers the state-based style over the\ncommunication-based one. The London school has the opposite preference.\nBoth schools use output-based testing.\nOutput-based testing produces tests of the highest quality. Such tests rarely cou-\nple to implementation details and thus are resistant to refactoring. They are\nalso small and concise and thus are more maintainable.\nState-based testing requires extra prudence to avoid brittleness: you need to\nmake sure you don’t expose a private state to enable unit testing. Because state-\nbased tests tend to be larger than output-based tests, they are also less maintain-\nable. Maintainability issues can sometimes be mitigated (but not eliminated)\nwith the use of helper methods and value objects.\nCommunication-based testing also requires extra prudence to avoid brittle-\nness. You should only verify communications that cross the application bound-\nary and whose side effects are visible to the external world. Maintainability of\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2965,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 172,
      "content": "150\nCHAPTER 6\nStyles of unit testing\ncommunication-based tests is worse compared to output-based and state-based\ntests. Mocks tend to occupy a lot of space, and that makes tests less readable.\nFunctional programming is programming with mathematical functions.\nA mathematical function is a function (or method) that doesn’t have any hidden\ninputs or outputs. Side effects and exceptions are hidden outputs. A reference\nto an internal or external state is a hidden input. Mathematical functions are\nexplicit, which makes them extremely testable.\nThe goal of functional programming is to introduce a separation between busi-\nness logic and side effects.\nFunctional architecture helps achieve that separation by pushing side effects\nto the edges of a business operation. This approach maximizes the amount of\ncode written in a purely functional way while minimizing code that deals with\nside effects.\nFunctional architecture divides all code into two categories: functional core\nand mutable shell. The functional core makes decisions. The mutable shell supplies\ninput data to the functional core and converts decisions the core makes into\nside effects.\nThe difference between functional and hexagonal architectures is in their treat-\nment of side effects. Functional architecture pushes all side effects out of the\ndomain layer. Conversely, hexagonal architecture is fine with side effects made\nby the domain layer, as long as they are limited to that domain layer only. Func-\ntional architecture is hexagonal architecture taken to an extreme.\nThe choice between a functional architecture and a more traditional one is a\ntrade-off between performance and code maintainability. Functional architec-\nture concedes performance for maintainability gains.\nNot all code bases are worth converting into functional architecture. Apply\nfunctional architecture strategically. Take into account the complexity and the\nimportance of your system. In code bases that are simple or not that important,\nthe initial investment required for functional architecture won’t pay off.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2117,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 173,
      "content": "151\nRefactoring toward\nvaluable unit tests\nIn chapter 1, I defined the properties of a good unit test suite:\nIt is integrated into the development cycle.\nIt targets only the most important parts of your code base.\nIt provides maximum value with minimum maintenance costs. To achieve\nthis last attribute, you need to be able to:\n– Recognize a valuable test (and, by extension, a test of low value).\n– Write a valuable test.\nChapter 4 covered the topic of recognizing a valuable test using the four attributes:\nprotection against regressions, resistance to refactoring, fast feedback, and main-\ntainability. And chapter 5 expanded on the most important one of the four: resis-\ntance to refactoring.\n As I mentioned earlier, it’s not enough to recognize valuable tests, you should also\nbe able to write such tests. The latter skill requires the former, but it also requires\nThis chapter covers\nRecognizing the four types of code\nUnderstanding the Humble Object pattern\nWriting valuable tests\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1046,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 174,
      "content": "152\nCHAPTER 7\nRefactoring toward valuable unit tests\nthat you know code design techniques. Unit tests and the underlying code are highly\nintertwined, and it’s impossible to create valuable tests without putting effort into the\ncode base they cover.\n You saw an example of a code base transformation in chapter 6, where we refac-\ntored an audit system toward a functional architecture and, as a result, were able to\napply output-based testing. This chapter generalizes this approach onto a wider spec-\ntrum of applications, including those that can’t use a functional architecture. You’ll\nsee practical guidelines on how to write valuable tests in almost any software project.\n7.1\nIdentifying the code to refactor\nIt’s rarely possible to significantly improve a test suite without refactoring the underly-\ning code. There’s no way around it—test and production code are intrinsically con-\nnected. In this section, you’ll see how to categorize your code into the four types in\norder to outline the direction of the refactoring. The subsequent sections show a com-\nprehensive example.\n7.1.1\nThe four types of code\nIn this section, I describe the four types of code that serve as a foundation for the rest\nof this chapter. \n All production code can be categorized along two dimensions:\nComplexity or domain significance\nThe number of collaborators\nCode complexity is defined by the number of decision-making (branching) points in the\ncode. The greater that number, the higher the complexity.\nHow to calculate cyclomatic complexity\nIn computer science, there’s a special term that describes code complexity: cyclo-\nmatic complexity. Cyclomatic complexity indicates the number of branches in a given\nprogram or method. This metric is calculated as\n1 + <number of branching points>\nThus, a method with no control flow statements (such as if statements or condi-\ntional loops) has a cyclomatic complexity of 1 + 0 = 1.\nThere’s another meaning to this metric. You can think of it in terms of the number of\nindependent paths through the method from an entry to an exit, or the number of tests\nneeded to get a 100% branch coverage.\nNote that the number of branching points is counted as the number of simplest pred-\nicates involved. For instance, a statement like IF condition1 AND condition2\nTHEN ... is equivalent to IF condition1 THEN IF condition2 THEN ... Therefore,\nits complexity would be 1 + 2 = 3.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2449,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 175,
      "content": "153\nIdentifying the code to refactor\nDomain significance shows how significant the code is for the problem domain of your\nproject. Normally, all code in the domain layer has a direct connection to the end\nusers’ goals and thus exhibits a high domain significance. On the other hand, utility\ncode doesn’t have such a connection.\n Complex code and code that has domain significance benefit from unit testing the\nmost because the corresponding tests have great protection against regressions. Note\nthat the domain code doesn’t have to be complex, and complex code doesn’t have to\nexhibit domain significance to be test-worthy. The two components are independent\nof each other. For example, a method calculating an order price can contain no con-\nditional statements and thus have the cyclomatic complexity of 1. Still, it’s important\nto test such a method because it represents business-critical functionality.\n The second dimension is the number of collaborators a class or a method has. As\nyou may remember from chapter 2, a collaborator is a dependency that is either\nmutable or out-of-process (or both). Code with a large number of collaborators is\nexpensive to test. That’s due to the maintainability metric, which depends on the size\nof the test. It takes space to bring collaborators to an expected condition and then\ncheck their state or interactions with them afterward. And the more collaborators\nthere are, the larger the test becomes.\n The type of the collaborators also matters. Out-of-process collaborators are a no-go\nwhen it comes to the domain model. They add additional maintenance costs due to\nthe necessity to maintain complicated mock machinery in tests. You also have to be\nextra prudent and only use mocks to verify interactions that cross the application\nboundary in order to maintain proper resistance to refactoring (refer to chapter 5 for\nmore details). It’s better to delegate all communications with out-of-process depen-\ndencies to classes outside the domain layer. The domain classes then will only work\nwith in-process dependencies.\n Notice that both implicit and explicit collaborators count toward this number. It\ndoesn’t matter if the system under test (SUT) accepts a collaborator as an argument\nor refers to it implicitly via a static method, you still have to set up this collaborator in\ntests. Conversely, immutable dependencies (values or value objects) don’t count. Such\ndependencies are much easier to set up and assert against.\n The combination of code complexity, its domain significance, and the number of\ncollaborators give us the four types of code shown in figure 7.1:\nDomain model and algorithms (figure 7.1, top left)—Complex code is often part of\nthe domain model but not in 100% of all cases. You might have a complex algo-\nrithm that’s not directly related to the problem domain.\nTrivial code (figure 7.1, bottom left)—Examples of such code in C# are parameter-\nless constructors and one-line properties: they have few (if any) collaborators\nand exhibit little complexity or domain significance.\nControllers (figure 7.1, bottom right)—This code doesn’t do complex or business-\ncritical work by itself but coordinates the work of other components like domain\nclasses and external applications.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 3297,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 176,
      "content": "154\nCHAPTER 7\nRefactoring toward valuable unit tests\nOvercomplicated code (figure 7.1, top right)—Such code scores highly on both\nmetrics: it has a lot of collaborators, and it’s also complex or important. An\nexample here are fat controllers (controllers that don’t delegate complex work\nanywhere and do everything themselves).\nUnit testing the top-left quadrant (domain model and algorithms) gives you the best\nreturn for your efforts. The resulting unit tests are highly valuable and cheap. They’re\nvaluable because the underlying code carries out complex or important logic, thus\nincreasing tests’ protection against regressions. And they’re cheap because the code\nhas few collaborators (ideally, none), thus decreasing tests’ maintenance costs.\n Trivial code shouldn’t be tested at all; such tests have a close-to-zero value. As for\ncontrollers, you should test them briefly as part of a much smaller set of the overarch-\ning integration tests (I cover this topic in part 3).\n The most problematic type of code is the overcomplicated quadrant. It’s hard to\nunit test but too risky to leave without test coverage. Such code is one of the main rea-\nsons many people struggle with unit testing. This whole chapter is primarily devoted\nto how you can bypass this dilemma. The general idea is to split overcomplicated code\ninto two parts: algorithms and controllers (figure 7.2), although the actual implemen-\ntation can be tricky at times.\nTIP\nThe more important or complex the code, the fewer collaborators it\nshould have.\nGetting rid of the overcomplicated code and unit testing only the domain model and\nalgorithms is the path to a highly valuable, easily maintainable test suite. With this\napproach, you won’t have 100% test coverage, but you don’t need to—100% coverage\nshouldn’t ever be your goal. Your goal is a test suite where each test adds significant\nvalue to the project. Refactor or get rid of all other tests. Don’t allow them to inflate\nthe size of your test suite.\nComplexity,\ndomain\nsigniﬁcance\nDomain model,\nalgorithms\nOvercomplicated\ncode\nTrivial code\nNumber of\ncollaborators\nControllers\nFigure 7.1\nThe four types of code, \ncategorized by code complexity and \ndomain significance (the vertical \naxis) and the number of collaborators \n(the horizontal axis).\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2329,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 177,
      "content": "155\nIdentifying the code to refactor\nNOTE\nRemember that it’s better to not write a test at all than to write a\nbad test.\nOf course, getting rid of overcomplicated code is easier said than done. Still, there are\ntechniques that can help you do that. I’ll first explain the theory behind those tech-\nniques and then demonstrate them using a close-to-real-world example. \n7.1.2\nUsing the Humble Object pattern to split overcomplicated code\nTo split overcomplicated code, you need to use the Humble Object design pattern.\nThis pattern was introduced by Gerard Meszaros in his book xUnit Test Patterns: Refac-\ntoring Test Code (Addison-Wesley, 2007) as one of the ways to battle code coupling, but\nit has a much broader application. You’ll see why shortly.\n We often find that code is hard to test because it’s coupled to a framework depen-\ndency (see figure 7.3). Examples include asynchronous or multi-threaded execution,\nuser interfaces, communication with out-of-process dependencies, and so on.\nTo bring the logic of this code under test, you need to extract a testable part out of it.\nAs a result, the code becomes a thin, humble wrapper around that testable part: it glues\nComplexity,\ndomain\nsigniﬁcance\nDomain model,\nalgorithms\nOvercomplicated\ncode\nTrivial code\nNumber of\ncollaborators\nControllers\nFigure 7.2\nRefactor overcomplicated \ncode by splitting it into algorithms and \ncontrollers. Ideally, you should have no \ncode in the top-right quadrant.\nOvercomplicated code\nHard-to-test\ndependency\nLogic\nTest\nFigure 7.3\nIt’s hard to test \ncode that couples to a difficult \ndependency. Tests have to deal \nwith that dependency, too, which \nincreases their maintenance cost.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1725,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 178,
      "content": "156\nCHAPTER 7\nRefactoring toward valuable unit tests\nthe hard-to-test dependency and the newly extracted component together, but itself\ncontains little or no logic and thus doesn’t need to be tested (figure 7.4).\n If this approach looks familiar, it’s because you already saw it in this book. In fact,\nboth hexagonal and functional architectures implement this exact pattern. As you\nmay remember from previous chapters, hexagonal architecture advocates for the sep-\naration of business logic and communications with out-of-process dependencies. This\nis what the domain and application services layers are responsible for, respectively.\n Functional architecture goes even further and separates business logic from com-\nmunications with all collaborators, not just out-of-process ones. This is what makes\nfunctional architecture so testable: its functional core has no collaborators. All depen-\ndencies in a functional core are immutable, which brings it very close to the vertical\naxis on the types-of-code diagram (figure 7.5).\nHumble object\nHard-to-test\ndependency\nTest\nLogic\nFigure 7.4\nThe Humble Object \npattern extracts the logic out of the \novercomplicated code, making that \ncode so humble that it doesn’t need to \nbe tested. The extracted logic is \nmoved into another class, decoupled \nfrom the hard-to-test dependency.\nComplexity,\ndomain\nsigniﬁcance\nNumber of\ncollaborators\nDomain model,\nalgorithms\nOvercomplicated\ncode\nTrivial code\nControllers\nDomain layer\nMutable shell and\napplication services layer\nFunctional core\nFigure 7.5\nThe functional core in a functional architecture and the domain layer in \na hexagonal architecture reside in the top-left quadrant: they have few collaborators \nand exhibit high complexity and domain significance. The functional core is closer \nto the vertical axis because it has no collaborators. The mutable shell (functional \narchitecture) and the application services layer (hexagonal architecture) belong \nto the controllers’ quadrant.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2031,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 179,
      "content": "157\nIdentifying the code to refactor\nAnother way to view the Humble Object pattern is as a means to adhere to the Single\nResponsibility principle, which states that each class should have only a single respon-\nsibility.1 One such responsibility is always business logic; the pattern can be applied to\nsegregate that logic from pretty much anything.\n In our particular situation, we are interested in the separation of business logic\nand orchestration. You can think of these two responsibilities in terms of code depth\nversus code width. Your code can be either deep (complex or important) or wide (work\nwith many collaborators), but never both (figure 7.6).\nI can’t stress enough how important this separation is. In fact, many well-known princi-\nples and patterns can be described as a form of the Humble Object pattern: they are\ndesigned specifically to segregate complex code from the code that does orchestration.\n You already saw the relationship between this pattern and hexagonal and func-\ntional architectures. Other examples include the Model-View-Presenter (MVP) and\nthe Model-View-Controller (MVC) patterns. These two patterns help you decouple\nbusiness logic (the Model part), UI concerns (the View), and the coordination between\nthem (Presenter or Controller). The Presenter and Controller components are humble\nobjects: they glue the view and the model together.\n Another example is the Aggregate pattern from Domain-Driven Design.2 One of its\ngoals is to reduce connectivity between classes by grouping them into clusters—\naggregates. The classes are highly connected inside those clusters, but the clusters them-\nselves are loosely coupled. Such a structure decreases the total number of communica-\ntions in the code base. The reduced connectivity, in turn, improves testability.\n1 See Agile Principles, Patterns, and Practices in C# by Robert C. Martin and Micah Martin (Prentice Hall, 2006).\n2 See Domain-Driven Design: Tackling Complexity in the Heart of Software by Eric Evans (Addison-Wesley, 2003).\nControllers\nDomain layer,\nalgorithms\nFigure 7.6\nCode depth versus code width is \na useful metaphor to apply when you think of \nthe separation between the business logic \nand orchestration responsibilities. Controllers \norchestrate many dependencies (represented as \narrows in the figure) but aren’t complex on their \nown (complexity is represented as block height). \nDomain classes are the opposite of that.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2481,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 180,
      "content": "158\nCHAPTER 7\nRefactoring toward valuable unit tests\n Note that improved testability is not the only reason to maintain the separation\nbetween business logic and orchestration. Such a separation also helps tackle code\ncomplexity, which is crucial for project growth, too, especially in the long run. I per-\nsonally always find it fascinating how a testable design is not only testable but also easy\nto maintain. \n7.2\nRefactoring toward valuable unit tests\nIn this section, I’ll show a comprehensive example of splitting overcomplicated code\ninto algorithms and controllers. You saw a similar example in the previous chapter,\nwhere we talked about output-based testing and functional architecture. This time, I’ll\ngeneralize this approach to all enterprise-level applications, with the help of the Hum-\nble Object pattern. I’ll use this project not only in this chapter but also in the subse-\nquent chapters of part 3.\n7.2.1\nIntroducing a customer management system\nThe sample project is a customer management system (CRM) that handles user\nregistrations. All users are stored in a database. The system currently supports only\none use case: changing a user’s email. There are three business rules involved in this\noperation:\nIf the user’s email belongs to the company’s domain, that user is marked as an\nemployee. Otherwise, they are treated as a customer.\nThe system must track the number of employees in the company. If the user’s\ntype changes from employee to customer, or vice versa, this number must\nchange, too.\nWhen the email changes, the system must notify external systems by sending a\nmessage to a message bus.\nThe following listing shows the initial implementation of the CRM system.\npublic class User\n{\npublic int UserId { get; private set; }\npublic string Email { get; private set; }\npublic UserType Type { get; private set; }\npublic void ChangeEmail(int userId, string newEmail)\n{\nobject[] data = Database.GetUserById(userId);    \nUserId = userId;\nEmail = (string)data[1];\nType = (UserType)data[2];\nif (Email == newEmail)\nreturn;\nListing 7.1\nInitial implementation of the CRM system\nRetrieves the user’s \ncurrent email and \ntype from the \ndatabase\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2215,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 181,
      "content": "159\nRefactoring toward valuable unit tests\nobject[] companyData = Database.GetCompany();       \nstring companyDomainName = (string)companyData[0];\nint numberOfEmployees = (int)companyData[1];\nstring emailDomain = newEmail.Split('@')[1];\nbool isEmailCorporate = emailDomain == companyDomainName;\nUserType newType = isEmailCorporate                       \n? UserType.Employee\n                       \n: UserType.Customer;\n                       \nif (Type != newType)\n{\nint delta = newType == UserType.Employee ? 1 : -1;\nint newNumber = numberOfEmployees + delta;\nDatabase.SaveCompany(newNumber);        \n}\nEmail = newEmail;\nType = newType;\nDatabase.SaveUser(this);              \nMessageBus.SendEmailChangedMessage(UserId, newEmail);       \n}\n}\npublic enum UserType\n{\nCustomer = 1,\nEmployee = 2\n}\nThe User class changes a user email. Note that, for brevity, I omitted simple valida-\ntions such as checks for email correctness and user existence in the database. Let’s\nanalyze this implementation from the perspective of the types-of-code diagram.\n The code’s complexity is not too high. The ChangeEmail method contains only a\ncouple of explicit decision-making points: whether to identify the user as an employee\nor a customer, and how to update the company’s number of employees. Despite being\nsimple, these decisions are important: they are the application’s core business logic.\nHence, the class scores highly on the complexity and domain significance dimension.\n On the other hand, the User class has four dependencies, two of which are explicit\nand the other two of which are implicit. The explicit dependencies are the userId\nand newEmail arguments. These are values, though, and thus don’t count toward the\nclass’s number of collaborators. The implicit ones are Database and MessageBus.\nThese two are out-of-process collaborators. As I mentioned earlier, out-of-process col-\nlaborators are a no-go for code with high domain significance. Hence, the User class\nscores highly on the collaborators dimension, which puts this class into the overcom-\nplicated category (figure 7.7).\n This approach—when a domain class retrieves and persists itself to the database—\nis called the Active Record pattern. It works fine in simple or short-lived projects but\nRetrieves the organization’s \ndomain name and the \nnumber of employees \nfrom the database\nSets the user type \ndepending on the new \nemail’s domain name\nUpdates the number \nof employees in the \norganization, if needed\nPersists the user \nin the database\nSends a notification\nto the message bus\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2595,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 182,
      "content": "160\nCHAPTER 7\nRefactoring toward valuable unit tests\noften fails to scale as the code base grows. The reason is precisely this lack of separa-\ntion between these two responsibilities: business logic and communication with out-of-\nprocess dependencies. \n7.2.2\nTake 1: Making implicit dependencies explicit\nThe usual approach to improve testability is to make implicit dependencies explicit:\nthat is, introduce interfaces for Database and MessageBus, inject those interfaces into\nUser, and then mock them in tests. This approach does help, and that’s exactly what\nwe did in the previous chapter when we introduced the implementation with mocks\nfor the audit system. However, it’s not enough.\n From the perspective of the types-of-code diagram, it doesn’t matter if the domain\nmodel refers to out-of-process dependencies directly or via an interface. Such depen-\ndencies are still out-of-process; they are proxies to data that is not yet in memory. You\nstill need to maintain complicated mock machinery in order to test such classes,\nwhich increases the tests’ maintenance costs. Moreover, using mocks for the database\ndependency would lead to test fragility (we’ll discuss this in the next chapter).\n Overall, it’s much cleaner for the domain model not to depend on out-of-process\ncollaborators at all, directly or indirectly (via an interface). That’s what the hexagonal\narchitecture advocates as well—the domain model shouldn’t be responsible for com-\nmunications with external systems. \n7.2.3\nTake 2: Introducing an application services layer\nTo overcome the problem of the domain model directly communicating with external\nsystems, we need to shift this responsibility to another class, a humble controller (an\napplication service, in the hexagonal architecture taxonomy). As a general rule, domain\nclasses should only depend on in-process dependencies, such as other domain classes,\nor plain values. Here’s what the first version of that application service looks like.\nDomain model,\nalgorithms\nOvercomplicated\ncode\nTrivial code\nControllers\nComplexity,\ndomain\nsigniﬁcance\nNumber of\ncollaborators\nUser class\nFigure 7.7\nThe initial \nimplementation of the User \nclass scores highly on both \ndimensions and thus falls \ninto the category of \novercomplicated code.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2313,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 183,
      "content": "161\nRefactoring toward valuable unit tests\npublic class UserController\n{\nprivate readonly Database _database = new Database();\nprivate readonly MessageBus _messageBus = new MessageBus();\npublic void ChangeEmail(int userId, string newEmail)\n{\nobject[] data = _database.GetUserById(userId);\nstring email = (string)data[1];\nUserType type = (UserType)data[2];\nvar user = new User(userId, email, type);\nobject[] companyData = _database.GetCompany();\nstring companyDomainName = (string)companyData[0];\nint numberOfEmployees = (int)companyData[1];\nint newNumberOfEmployees = user.ChangeEmail(\nnewEmail, companyDomainName, numberOfEmployees);\n_database.SaveCompany(newNumberOfEmployees);\n_database.SaveUser(user);\n_messageBus.SendEmailChangedMessage(userId, newEmail);\n}\n}\nThis is a good first try; the application service helped offload the work with out-of-\nprocess dependencies from the User class. But there are some issues with this imple-\nmentation:\nThe out-of-process dependencies (Database and MessageBus) are instantiated\ndirectly, not injected. That’s going to be a problem for the integration tests we’ll\nbe writing for this class.\nThe controller reconstructs a User instance from the raw data it receives from\nthe database. This is complex logic and thus shouldn’t belong to the applica-\ntion service, whose sole role is orchestration, not logic of any complexity or\ndomain significance.\nThe same is true for the company’s data. The other problem with that data is\nthat User now returns an updated number of employees, which doesn’t look\nright. The number of company employees has nothing to do with a specific\nuser. This responsibility should belong elsewhere.\nThe controller persists modified data and sends notifications to the message\nbus unconditionally, regardless of whether the new email is different than the\nprevious one.\nThe User class has become quite easy to test because it no longer has to communicate\nwith out-of-process dependencies. In fact, it has no collaborators whatsoever—out-of-\nprocess or not. Here’s the new version of User’s ChangeEmail method:\nListing 7.2\nApplication service, version 1\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2174,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 184,
      "content": "162\nCHAPTER 7\nRefactoring toward valuable unit tests\npublic int ChangeEmail(string newEmail,\nstring companyDomainName, int numberOfEmployees)\n{\nif (Email == newEmail)\nreturn numberOfEmployees;\nstring emailDomain = newEmail.Split('@')[1];\nbool isEmailCorporate = emailDomain == companyDomainName;\nUserType newType = isEmailCorporate\n? UserType.Employee\n: UserType.Customer;\nif (Type != newType)\n{\nint delta = newType == UserType.Employee ? 1 : -1;\nint newNumber = numberOfEmployees + delta;\nnumberOfEmployees = newNumber;\n}\nEmail = newEmail;\nType = newType;\nreturn numberOfEmployees;\n}\nFigure 7.8 shows where User and UserController currently stand in our diagram.\nUser has moved to the domain model quadrant, close to the vertical axis, because it\nno longer has to deal with collaborators. UserController is more problematic.\nAlthough I’ve put it into the controllers quadrant, it almost crosses the boundary into\novercomplicated code because it contains logic that is quite complex. \nDomain model,\nalgorithms\nOvercomplicated\ncode\nTrivial code\nControllers\nComplexity,\ndomain\nsigniﬁcance\nNumber of\ncollaborators\nUserController\nUser\nFigure 7.8\nTake 2 puts User in the domain model quadrant, close to the vertical \naxis. UserController almost crosses the boundary with the overcomplicated \nquadrant because it contains complex logic.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1382,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 185,
      "content": "163\nRefactoring toward valuable unit tests\n7.2.4\nTake 3: Removing complexity from the application service\nTo put UserController firmly into the controllers quadrant, we need to extract the\nreconstruction logic from it. If you use an object-relational mapping (ORM) library\nto map the database into the domain model, that would be a good place to which to\nattribute the reconstruction logic. Each ORM library has a dedicated place where you\ncan specify how your database tables should be mapped to domain classes, such as\nattributes on top of those domain classes, XML files, or files with fluent mappings.\n If you don’t want to or can’t use an ORM, create a factory in the domain model\nthat will instantiate the domain classes using raw database data. This factory can be a\nseparate class or, for simpler cases, a static method in the existing domain classes. The\nreconstruction logic in our sample application is not too complicated, but it’s good to\nkeep such things separated, so I’m putting it in a separate UserFactory class as shown\nin the following listing.\npublic class UserFactory\n{\npublic static User Create(object[] data)\n{\nPrecondition.Requires(data.Length >= 3);\nint id = (int)data[0];\nstring email = (string)data[1];\nUserType type = (UserType)data[2];\nreturn new User(id, email, type);\n}\n}\nThis code is now fully isolated from all collaborators and therefore easily testable.\nNotice that I’ve put a safeguard in this method: a requirement to have at least three\nelements in the data array. Precondition is a simple custom class that throws an\nexception if the Boolean argument is false. The reason for this class is the more\nsuccinct code and the condition inversion: affirmative statements are more read-\nable than negative ones. In our example, the data.Length >= 3 requirement reads\nbetter than\nif (data.Length < 3)\nthrow new Exception();\nNote that while this reconstruction logic is somewhat complex, it doesn’t have domain\nsignificance: it isn’t directly related to the client’s goal of changing the user email. It’s\nan example of the utility code I refer to in previous chapters.\n \nListing 7.3\nUser factory\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2178,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 186,
      "content": "164\nCHAPTER 7\nRefactoring toward valuable unit tests\n7.2.5\nTake 4: Introducing a new Company class\nLook at this code in the controller once again:\nobject[] companyData = _database.GetCompany();\nstring companyDomainName = (string)companyData[0];\nint numberOfEmployees = (int)companyData[1];\nint newNumberOfEmployees = user.ChangeEmail(\nnewEmail, companyDomainName, numberOfEmployees);\nThe awkwardness of returning an updated number of employees from User is a sign\nof a misplaced responsibility, which itself is a sign of a missing abstraction. To fix this,\nwe need to introduce another domain class, Company, that bundles the company-\nrelated logic and data together, as shown in the following listing.\npublic class Company\n{\npublic string DomainName { get; private set; }\npublic int NumberOfEmployees { get; private set; }\npublic void ChangeNumberOfEmployees(int delta)\n{\nPrecondition.Requires(NumberOfEmployees + delta >= 0);\nNumberOfEmployees += delta;\n}\npublic bool IsEmailCorporate(string email)\n{\nstring emailDomain = email.Split('@')[1];\nreturn emailDomain == DomainName;\n}\n}\nHow is the reconstruction logic complex?\nHow is the reconstruction logic complex, given that there’s only a single branching\npoint in the UserFactory.Create() method? As I mentioned in chapter 1, there\ncould be a lot of hidden branching points in the underlying libraries used by the code\nand thus a lot of potential for something to go wrong. This is exactly the case for the\nUserFactory.Create() method.\nReferring to an array element by index (data[0]) entails an internal decision made\nby the .NET Framework as to what data element to access. The same is true for the\nconversion from object to int or string. Internally, the .NET Framework decides\nwhether to throw a cast exception or allow the conversion to proceed. All these hid-\nden branches make the reconstruction logic test-worthy, despite the lack of decision\npoints in it. \nListing 7.4\nThe new class in the domain layer\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2016,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 187,
      "content": "165\nRefactoring toward valuable unit tests\nThere are two methods in this class: ChangeNumberOfEmployees() and IsEmail-\nCorporate(). These methods help adhere to the tell-don’t-ask principle I mentioned\nin chapter 5. This principle advocates for bundling together data and operations on\nthat data. A User instance will tell the company to change its number of employees or\nfigure out whether a particular email is corporate; it won’t ask for the raw data and do\neverything on its own.\n There’s also a new CompanyFactory class, which is responsible for the reconstruc-\ntion of Company objects, similar to UserFactory. This is how the controller now looks.\npublic class UserController\n{\nprivate readonly Database _database = new Database();\nprivate readonly MessageBus _messageBus = new MessageBus();\npublic void ChangeEmail(int userId, string newEmail)\n{\nobject[] userData = _database.GetUserById(userId);\nUser user = UserFactory.Create(userData);\nobject[] companyData = _database.GetCompany();\nCompany company = CompanyFactory.Create(companyData);\nuser.ChangeEmail(newEmail, company);\n_database.SaveCompany(company);\n_database.SaveUser(user);\n_messageBus.SendEmailChangedMessage(userId, newEmail);\n}\n}\nAnd here’s the User class.\npublic class User\n{\npublic int UserId { get; private set; }\npublic string Email { get; private set; }\npublic UserType Type { get; private set; }\npublic void ChangeEmail(string newEmail, Company company)\n{\nif (Email == newEmail)\nreturn;\nUserType newType = company.IsEmailCorporate(newEmail)\n? UserType.Employee\n: UserType.Customer;\nListing 7.5\nController after refactoring \nListing 7.6\nUser after refactoring \nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1688,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 188,
      "content": "166\nCHAPTER 7\nRefactoring toward valuable unit tests\nif (Type != newType)\n{\nint delta = newType == UserType.Employee ? 1 : -1;\ncompany.ChangeNumberOfEmployees(delta);\n}\nEmail = newEmail;\nType = newType;\n}\n}\nNotice how the removal of the misplaced responsibility made User much cleaner.\nInstead of operating on company data, it accepts a Company instance and delegates\ntwo important pieces of work to that instance: determining whether an email is corpo-\nrate and changing the number of employees in the company.\n Figure 7.9 shows where each class stands in the diagram. The factories and both\ndomain classes reside in the domain model and algorithms quadrant. User has moved\nto the right because it now has one collaborator, Company, whereas previously it had\nnone. That has made User less testable, but not much.\nUserController now firmly stands in the controllers quadrant because all of its com-\nplexity has moved to the factories. The only thing this class is responsible for is gluing\ntogether all the collaborating parties.\nDomain model,\nalgorithms\nOvercomplicated\ncode\nTrivial code\nControllers\nComplexity,\ndomain\nsigniﬁcance\nNumber of\ncollaborators\nUserController\nUser\nCompany,\nUserFactory,\nCompanyFactory\nFigure 7.9\nUser has shifted to the right because it now has the Company \ncollaborator. UserController firmly stands in the controllers quadrant; all \nits complexity has moved to the factories.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1457,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 189,
      "content": "167\nAnalysis of optimal unit test coverage\n Note the similarities between this implementation and the functional architecture\nfrom the previous chapter. Neither the functional core in the audit system nor the\ndomain layer in this CRM (the User and Company classes) communicates with out-of-\nprocess dependencies. In both implementations, the application services layer is\nresponsible for such communication: it gets the raw data from the filesystem or from\nthe database, passes that data to stateless algorithms or the domain model, and then\npersists the results back to the data storage.\n The difference between the two implementations is in their treatment of side\neffects. The functional core doesn’t incur any side effects whatsoever. The CRM’s\ndomain model does, but all those side effects remain inside the domain model in the\nform of the changed user email and the number of employees. The side effects only\ncross the domain model’s boundary when the controller persists the User and Company\nobjects in the database.\n The fact that all side effects are contained in memory until the very last moment\nimproves testability a lot. Your tests don’t need to examine out-of-process dependen-\ncies, nor do they need to resort to communication-based testing. All the verification\ncan be done using output-based and state-based testing of objects in memory. \n7.3\nAnalysis of optimal unit test coverage\nNow that we’ve completed the refactoring with the help of the Humble Object pat-\ntern, let’s analyze which parts of the project fall into which code category and how\nthose parts should be tested. Table 7.1 shows all the code from the sample project\ngrouped by position in the types-of-code diagram.\nWith the full separation of business logic and orchestration at hand, it’s easy to decide\nwhich parts of the code base to unit test.\n7.3.1\nTesting the domain layer and utility code\nTesting methods in the top-left quadrant in table 7.1 provides the best results in cost-\nbenefit terms. The code’s high complexity or domain significance guarantees great\nprotection against regressions, while having few collaborators ensures the lowest mainte-\nnance costs. This is an example of how User could be tested:\nTable 7.1\nTypes of code in the sample project after refactoring using the Humble Object pattern\nFew collaborators\nMany collaborators\nHigh complexity or \ndomain significance\nChangeEmail(newEmail, company) in User;\nChangeNumberOfEmployees(delta) and \nIsEmailCorporate(email) in Company; \nand Create(data) in UserFactory and \nCompanyFactory\nLow complexity and \ndomain significance\nConstructors in User and Company\nChangeEmail(userId, \nnewEmail) in \nUserController\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2714,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 190,
      "content": "168\nCHAPTER 7\nRefactoring toward valuable unit tests\n[Fact]\npublic void Changing_email_from_non_corporate_to_corporate()\n{\nvar company = new Company(\"mycorp.com\", 1);\nvar sut = new User(1, \"user@gmail.com\", UserType.Customer);\nsut.ChangeEmail(\"new@mycorp.com\", company);\nAssert.Equal(2, company.NumberOfEmployees);\nAssert.Equal(\"new@mycorp.com\", sut.Email);\nAssert.Equal(UserType.Employee, sut.Type);\n}\nTo achieve full coverage, you’d need another three such tests:\npublic void Changing_email_from_corporate_to_non_corporate()\npublic void Changing_email_without_changing_user_type()\npublic void Changing_email_to_the_same_one()\nTests for the other three classes would be even shorter, and you could use parameter-\nized tests to group several test cases together:\n[InlineData(\"mycorp.com\", \"email@mycorp.com\", true)]\n[InlineData(\"mycorp.com\", \"email@gmail.com\", false)]\n[Theory]\npublic void Differentiates_a_corporate_email_from_non_corporate(\nstring domain, string email, bool expectedResult)\n{\nvar sut = new Company(domain, 0);\nbool isEmailCorporate = sut.IsEmailCorporate(email);\nAssert.Equal(expectedResult, isEmailCorporate);\n}\n7.3.2\nTesting the code from the other three quadrants\nCode with low complexity and few collaborators (bottom-left quadrant in table 7.1) is\nrepresented by the constructors in User and Company, such as\npublic User(int userId, string email, UserType type)\n{\nUserId = userId;\nEmail = email;\nType = type;\n}\nThese constructors are trivial and aren’t worth the effort. The resulting tests wouldn’t\nprovide great enough protection against regressions.\n The refactoring has eliminated all code with high complexity and a large number\nof collaborators (top-right quadrant in table 7.1), so we have nothing to test there,\neither. As for the controllers quadrant (bottom-right in table 7.1), we’ll discuss testing\nit in the next chapter. \nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1911,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 191,
      "content": "169\nHandling conditional logic in controllers\n7.3.3\nShould you test preconditions?\nLet’s take a look at a special kind of branching points—preconditions—and see whether\nyou should test them. For example, look at this method from Company once again:\npublic void ChangeNumberOfEmployees(int delta)\n{\nPrecondition.Requires(NumberOfEmployees + delta >= 0);\nNumberOfEmployees += delta;\n}\nIt has a precondition stating that the number of employees in the company should\nnever become negative. This precondition is a safeguard that’s activated only in\nexceptional cases. Such exceptional cases are usually the result of bugs. The only pos-\nsible reason for the number of employees to go below zero is if there’s an error in\ncode. The safeguard provides a mechanism for your software to fail fast and to prevent\nthe error from spreading and being persisted in the database, where it would be much\nharder to deal with. Should you test such preconditions? In other words, would such\ntests be valuable enough to have in the test suite?\n There’s no hard rule here, but the general guideline I recommend is to test all pre-\nconditions that have domain significance. The requirement for the non-negative\nnumber of employees is such a precondition. It’s part of the Company class’s invariants:\nconditions that should be held true at all times. But don’t spend time testing precon-\nditions that don’t have domain significance. For example, UserFactory has the follow-\ning safeguard in its Create method:\npublic static User Create(object[] data)\n{\nPrecondition.Requires(data.Length >= 3);\n/* Extract id, email, and type out of data */\n}\nThere’s no domain meaning to this precondition and therefore not much value in\ntesting it. \n7.4\nHandling conditional logic in controllers\nHandling conditional logic and simultaneously maintaining the domain layer free of\nout-of-process collaborators is often tricky and involves trade-offs. In this section, I’ll\nshow what those trade-offs are and how to decide which of them to choose in your\nown project.\n The separation between business logic and orchestration works best when a busi-\nness operation has three distinct stages:\nRetrieving data from storage\nExecuting business logic\nPersisting data back to the storage (figure 7.10)\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2308,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 192,
      "content": "170\nCHAPTER 7\nRefactoring toward valuable unit tests\nThere are a lot of situations where these stages aren’t as clearcut, though. As we discussed\nin chapter 6, you might need to query additional data from an out-of-process depen-\ndency based on an intermediate result of the decision-making process (figure 7.11). Writ-\ning to the out-of-process dependency often depends on that result, too.\nAs also discussed in the previous chapter, you have three options in such a situation:\nPush all external reads and writes to the edges anyway. This approach preserves the\nread-decide-act structure but concedes performance: the controller will call\nout-of-process dependencies even when there’s no need for that.\nInject the out-of-process dependencies into the domain model and allow the business\nlogic to directly decide when to call those dependencies.\nSplit the decision-making process into more granular steps and have the controller act\non each of those steps separately.\nRead\nInvoke\nWrite\nOut-of-process\ndependencies:\nﬁlesystem,\ndatabase, etc.\nApplication\nservice\n(controller)\nBusiness logic\n(domain\nmodel)\nFigure 7.10\nHexagonal and functional architectures work best when all \nreferences to out-of-process dependencies can be pushed to the edges of \nbusiness operations.\nRead\nRead\nInvoke 1\nWrite\nOut-of-process\ndependencies:\nﬁlesystem,\ndatabase, etc.\nApplication\nservice\n(controller)\nBusiness logic\n(domain\nmodel)\nInvoke 2\nFigure 7.11\nA hexagonal architecture doesn’t work as well when you need to refer to \nout-of-process dependencies in the middle of the business operation.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1629,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 193,
      "content": "171\nHandling conditional logic in controllers\nThe challenge is to balance the following three attributes:\nDomain model testability, which is a function of the number and type of collabora-\ntors in domain classes\nController simplicity, which depends on the presence of decision-making (branch-\ning) points in the controller\nPerformance, as defined by the number of calls to out-of-process dependencies\nEach option only gives you two out of the three attributes (figure 7.12):\nPushing all external reads and writes to the edges of a business operation—Preserves\ncontroller simplicity and keeps the domain model isolated from out-of-process\ndependencies (thus allowing it to remain testable) but concedes performance.\nInjecting out-of-process dependencies into the domain model—Keeps performance and\nthe controller’s simplicity intact but damages domain model testability.\nSplitting the decision-making process into more granular steps—Helps with both per-\nformance and domain model testability but concedes controller simplicity.\nYou’ll need to introduce decision-making points in the controller in order to\nmanage these granular steps.\nIn most software projects, performance is important, so the first approach (pushing\nexternal reads and writes to the edges of a business operation) is out of the question.\nThe second option (injecting out-of-process dependencies into the domain model)\nbrings most of your code into the overcomplicated quadrant on the types-of-code dia-\ngram. This is exactly what we refactored the initial CRM implementation away from. I\nrecommend that you avoid this approach: such code no longer preserves the separation\nDomain model\ntestability\nPerformance\nPushing all external reads\nand writes to the edges of\nthe business operation\nInjecting out-of-process\ndependencies into the\ndomain model\nSplitting the decision-making\nprocess into more granular steps\nController simplicity\nFigure 7.12\nThere’s no single solution that satisfies all three attributes: controller simplicity, \ndomain model testability, and performance. You have to choose two out of the three.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2142,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 194,
      "content": "172\nCHAPTER 7\nRefactoring toward valuable unit tests\nbetween business logic and communication with out-of-process dependencies and\nthus becomes much harder to test and maintain.\n That leaves you with the third option: splitting the decision-making process into\nsmaller steps. With this approach, you will have to make your controllers more com-\nplex, which will also push them closer to the overcomplicated quadrant. But there are\nways to mitigate this problem. Although you will rarely be able to factor all the com-\nplexity out of controllers as we did previously in the sample project, you can keep that\ncomplexity manageable.\n7.4.1\nUsing the CanExecute/Execute pattern\nThe first way to mitigate the growth of the controllers’ complexity is to use the Can-\nExecute/Execute pattern, which helps avoid leaking of business logic from the\ndomain model to controllers. This pattern is best explained with an example, so let’s\nexpand on our sample project.\n Let’s say that a user can change their email only until they confirm it. If a user tries\nto change the email after the confirmation, they should be shown an error message.\nTo accommodate this new requirement, we’ll add a new property to the User class.\npublic class User\n{\npublic int UserId { get; private set; }\npublic string Email { get; private set; }\npublic UserType Type { get; private set; }\npublic bool IsEmailConfirmed               \n{ get; private set; }\n               \n/* ChangeEmail(newEmail, company) method */\n}\nThere are two options for where to put this check. First, you could put it in User’s\nChangeEmail method:\npublic string ChangeEmail(string newEmail, Company company)\n{\nif (IsEmailConfirmed)\nreturn \"Can't change a confirmed email\";\n/* the rest of the method */\n}\nThen you could make the controller either return an error or incur all necessary side\neffects, depending on this method’s output.\npublic string ChangeEmail(int userId, string newEmail)\n{\nListing 7.7\nUser with a new property\nListing 7.8\nThe controller, still stripped of all decision-making\nNew property\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2096,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 195,
      "content": "173\nHandling conditional logic in controllers\nobject[] userData = _database.GetUserById(userId);\n  \nUser user = UserFactory.Create(userData);\n  \nobject[] companyData = _database.GetCompany();\n  \nCompany company = CompanyFactory.Create(companyData);   \nstring error = user.ChangeEmail(newEmail, company);\n     \nif (error != null)\n    \nreturn error;\n     \n_database.SaveCompany(company);\n      \n_database.SaveUser(user);\n \n_messageBus.SendEmailChangedMessage(userId, newEmail); \nreturn \"OK\";\n \n}\nThis implementation keeps the controller free of decision-making, but it does so at\nthe expense of a performance drawback. The Company instance is retrieved from the\ndatabase unconditionally, even when the email is confirmed and thus can’t be changed.\nThis is an example of pushing all external reads and writes to the edges of a business\noperation.\nNOTE\nI don’t consider the new if statement analyzing the error string an\nincrease in complexity because it belongs to the acting phase; it’s not part of\nthe decision-making process. All the decisions are made by the User class, and\nthe controller merely acts on those decisions.\nThe second option is to move the check for IsEmailConfirmed from User to the\ncontroller.\npublic string ChangeEmail(int userId, string newEmail)\n{\nobject[] userData = _database.GetUserById(userId);\nUser user = UserFactory.Create(userData);\nif (user.IsEmailConfirmed)\n   \nreturn \"Can't change a confirmed email\";  \nobject[] companyData = _database.GetCompany();\nCompany company = CompanyFactory.Create(companyData);\nuser.ChangeEmail(newEmail, company);\n_database.SaveCompany(company);\n_database.SaveUser(user);\n_messageBus.SendEmailChangedMessage(userId, newEmail);\nreturn \"OK\";\n}\nListing 7.9\nController deciding whether to change the user’s email\nPrepares \nthe data\nMakes a\ndecision\nActs on the \ndecision\nDecision-making \nmoved here from User.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1917,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 196,
      "content": "174\nCHAPTER 7\nRefactoring toward valuable unit tests\nWith this implementation, the performance stays intact: the Company instance is\nretrieved from the database only after it is certain that the email can be changed. But\nnow the decision-making process is split into two parts:\nWhether to proceed with the change of email (performed by the controller)\nWhat to do during that change (performed by User)\nNow it’s also possible to change the email without verifying the IsEmailConfirmed\nflag first, which diminishes the domain model’s encapsulation. Such fragmentation\nhinders the separation between business logic and orchestration and moves the con-\ntroller closer to the overcomplicated danger zone.\n To prevent this fragmentation, you can introduce a new method in User, CanChange-\nEmail(), and make its successful execution a precondition for changing an email. The\nmodified version in the following listing follows the CanExecute/Execute pattern.\npublic string CanChangeEmail()\n{\nif (IsEmailConfirmed)\nreturn \"Can't change a confirmed email\";\nreturn null;\n}\npublic void ChangeEmail(string newEmail, Company company)\n{\nPrecondition.Requires(CanChangeEmail() == null);\n/* the rest of the method */\n}\nThis approach provides two important benefits:\nThe controller no longer needs to know anything about the process of chang-\ning emails. All it needs to do is call the CanChangeEmail() method to see if the\noperation can be done. Notice that this method can contain multiple valida-\ntions, all encapsulated away from the controller.\nThe additional precondition in ChangeEmail() guarantees that the email won’t\never be changed without checking for the confirmation first.\nThis pattern helps you to consolidate all decisions in the domain layer. The controller\nno longer has an option not to check for the email confirmation, which essentially\neliminates the new decision-making point from that controller. Thus, although the\ncontroller still contains the if statement calling CanChangeEmail(), you don’t need to\ntest that if statement. Unit testing the precondition in the User class itself is enough.\nNOTE\nFor simplicity’s sake, I’m using a string to denote an error. In a real-\nworld project, you may want to introduce a custom Result class to indicate\nthe success or failure of an operation. \nListing 7.10\nChanging an email using the CanExecute/Execute pattern\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2416,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 197,
      "content": "175\nHandling conditional logic in controllers\n7.4.2\nUsing domain events to track changes in the domain model\nIt’s sometimes hard to deduct what steps led the domain model to the current state.\nStill, it might be important to know these steps because you need to inform external\nsystems about what exactly has happened in your application. Putting this responsibil-\nity on the controllers would make them more complicated. To avoid that, you can\ntrack important changes in the domain model and then convert those changes into\ncalls to out-of-process dependencies after the business operation is complete. Domain\nevents help you implement such tracking.\nDEFINITION\nA domain event describes an event in the application that is mean-\ningful to domain experts. The meaningfulness for domain experts is what\ndifferentiates domain events from regular events (such as button clicks).\nDomain events are often used to inform external applications about import-\nant changes that have happened in your system.\nOur CRM has a tracking requirement, too: it has to notify external systems about\nchanged user emails by sending messages to the message bus. The current implemen-\ntation has a flaw in the notification functionality: it sends messages even when the\nemail is not changed, as shown in the following listing.\n// User\npublic void ChangeEmail(string newEmail, Company company)\n{\nPrecondition.Requires(CanChangeEmail() == null);\nif (Email == newEmail)   \nreturn;\n/* the rest of the method */\n}\n// Controller\npublic string ChangeEmail(int userId, string newEmail)\n{\n/* preparations */\nuser.ChangeEmail(newEmail, company);\n_database.SaveCompany(company);\n_database.SaveUser(user);\n_messageBus.SendEmailChangedMessage(  \nuserId, newEmail);\n  \nreturn \"OK\";\n}\nYou could resolve this bug by moving the check for email sameness to the controller,\nbut then again, there are issues with the business logic fragmentation. And you can’t\nListing 7.11\nSends a notification even when the email has not changed\nUser email may \nnot change.\nThe controller sends \na message anyway.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2106,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 198,
      "content": "176\nCHAPTER 7\nRefactoring toward valuable unit tests\nput this check to CanChangeEmail() because the application shouldn’t return an\nerror if the new email is the same as the old one.\n Note that this particular check probably doesn’t introduce too much business logic\nfragmentation, so I personally wouldn’t consider the controller overcomplicated if it\ncontained that check. But you may find yourself in a more difficult situation in which\nit’s hard to prevent your application from making unnecessary calls to out-of-process\ndependencies without passing those dependencies to the domain model, thus over-\ncomplicating that domain model. The only way to prevent such overcomplication is\nthe use of domain events.\n From an implementation standpoint, a domain event is a class that contains data\nneeded to notify external systems. In our specific example, it is the user’s ID and\nemail:\npublic class EmailChangedEvent\n{\npublic int UserId { get; }\npublic string NewEmail { get; }\n}\nNOTE\nDomain events should always be named in the past tense because they\nrepresent things that already happened. Domain events are values—they are\nimmutable and interchangeable.\nUser will have a collection of such events to which it will add a new element when the\nemail changes. This is how its ChangeEmail() method looks after the refactoring.\npublic void ChangeEmail(string newEmail, Company company)\n{\nPrecondition.Requires(CanChangeEmail() == null);\nif (Email == newEmail)\nreturn;\nUserType newType = company.IsEmailCorporate(newEmail)\n? UserType.Employee\n: UserType.Customer;\nif (Type != newType)\n{\nint delta = newType == UserType.Employee ? 1 : -1;\ncompany.ChangeNumberOfEmployees(delta);\n}\nEmail = newEmail;\nType = newType;\nEmailChangedEvents.Add(\n  \nnew EmailChangedEvent(UserId, newEmail));  \n}\nListing 7.12\nUser adding an event when the email changes\nA new event indicates \nthe change of email.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1935,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 199,
      "content": "177\nHandling conditional logic in controllers\nThe controller then will convert the events into messages on the bus.\npublic string ChangeEmail(int userId, string newEmail)\n{\nobject[] userData = _database.GetUserById(userId);\nUser user = UserFactory.Create(userData);\nstring error = user.CanChangeEmail();\nif (error != null)\nreturn error;\nobject[] companyData = _database.GetCompany();\nCompany company = CompanyFactory.Create(companyData);\nuser.ChangeEmail(newEmail, company);\n_database.SaveCompany(company);\n_database.SaveUser(user);\nforeach (var ev in user.EmailChangedEvents)  \n{\n  \n_messageBus.SendEmailChangedMessage(\n  \nev.UserId, ev.NewEmail);\n  \n}\n  \nreturn \"OK\";\n}\nNotice that the Company and User instances are still persisted in the database uncondi-\ntionally: the persistence logic doesn’t depend on domain events. This is due to the dif-\nference between changes in the database and messages in the bus.\n Assuming that no application has access to the database other than the CRM, com-\nmunications with that database are not part of the CRM’s observable behavior—they\nare implementation details. As long as the final state of the database is correct, it\ndoesn’t matter how many calls your application makes to that database. On the other\nhand, communications with the message bus are part of the application’s observable\nbehavior. In order to maintain the contract with external systems, the CRM should put\nmessages on the bus only when the email changes.\n There are performance implications to persisting data in the database uncondi-\ntionally, but they are relatively insignificant. The chances that after all the validations\nthe new email is the same as the old one are quite small. The use of an ORM can also\nhelp. Most ORMs won’t make a round trip to the database if there are no changes to\nthe object state.\n You can generalize the solution with domain events: extract a DomainEvent base\nclass and introduce a base class for all domain classes, which would contain a collec-\ntion of such events: List<DomainEvent> events. You can also write a separate event\ndispatcher instead of dispatching domain events manually in controllers. Finally, in\nlarger projects, you might need a mechanism for merging domain events before\nListing 7.13\nThe controller processing domain events\nDomain event \nprocessing\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2365,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 200,
      "content": "178\nCHAPTER 7\nRefactoring toward valuable unit tests\ndispatching them. That topic is outside the scope of this book, though. You can read\nabout it in my article “Merging domain events before dispatching” at http://mng\n.bz/YeVe.\n Domain events remove the decision-making responsibility from the controller and\nput that responsibility into the domain model, thus simplifying unit testing communi-\ncations with external systems. Instead of verifying the controller itself and using mocks\nto substitute out-of-process dependencies, you can test the domain event creation\ndirectly in unit tests, as shown next.\n[Fact]\npublic void Changing_email_from_corporate_to_non_corporate()\n{\nvar company = new Company(\"mycorp.com\", 1);\nvar sut = new User(1, \"user@mycorp.com\", UserType.Employee, false);\nsut.ChangeEmail(\"new@gmail.com\", company);\ncompany.NumberOfEmployees.Should().Be(0);\nsut.Email.Should().Be(\"new@gmail.com\");\nsut.Type.Should().Be(UserType.Customer);\nsut.EmailChangedEvents.Should().Equal(\n   \nnew EmailChangedEvent(1, \"new@gmail.com\"));  \n}\nOf course, you’ll still need to test the controller to make sure it does the orchestration\ncorrectly, but doing so requires a much smaller set of tests. That’s the topic of the next\nchapter. \n7.5\nConclusion\nNotice a theme that has been present throughout this chapter: abstracting away the\napplication of side effects to external systems. You achieve such abstraction by keeping\nthose side effects in memory until the very end of the business operation, so that they\ncan be tested with plain unit tests without involving out-of-process dependencies.\nDomain events are abstractions on top of upcoming messages in the bus. Changes in\ndomain classes are abstractions on top of upcoming modifications in the database.\nNOTE\nIt’s easier to test abstractions than the things they abstract.\nAlthough we were able to successfully contain all the decision-making in the domain\nmodel with the help of domain events and the CanExecute/Execute pattern, you\nwon’t be able to always do that. There are situations where business logic fragmenta-\ntion is inevitable.\n For example, there’s no way to verify email uniqueness outside the controller with-\nout introducing out-of-process dependencies in the domain model. Another example\nis failures in out-of-process dependencies that should alter the course of the business\nListing 7.14\nTesting the creation of a domain event\nSimultaneously asserts \nthe collection size and the \nelement in the collection\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2531,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 201,
      "content": "179\nConclusion\noperation. The decision about which way to go can’t reside in the domain layer\nbecause it’s not the domain layer that calls those out-of-process dependencies. You will\nhave to put this logic into controllers and then cover it with integration tests. Still,\neven with the potential fragmentation, there’s a lot of value in separating business\nlogic from orchestration because this separation drastically simplifies the unit test-\ning process.\n Just as you can’t avoid having some business logic in controllers, you will rarely be\nable to remove all collaborators from domain classes. And that’s fine. One, two, or\neven three collaborators won’t turn a domain class into overcomplicated code, as long\nas these collaborators don’t refer to out-of-process dependencies.\n Don’t use mocks to verify interactions with such collaborators, though. These\ninteractions have nothing to do with the domain model’s observable behavior. Only\nthe very first call, which goes from a controller to a domain class, has an immediate\nconnection to that controller’s goal. All the subsequent calls the domain class\nmakes to its neighbor domain classes within the same operation are implementa-\ntion details.\n Figure 7.13 illustrates this idea. It shows the communications between components\nin the CRM and their relationship to observable behavior. As you may remember from\nchapter 5, whether a method is part of the class’s observable behavior depends on\nwhom the client is and what the goals of that client are. To be part of the observable\nbehavior, the method must meet one of the following two criteria:\nHave an immediate connection to one of the client’s goals\nIncur a side effect in an out-of-process dependency that is visible to external\napplications\nThe controller’s ChangeEmail() method is part of its observable behavior, and so is\nthe call it makes to the message bus. The first method is the entry point for the exter-\nnal client, thereby meeting the first criterion. The call to the bus sends messages to\nexternal applications, thereby meeting the second criterion. You should verify both of\nExternal client\nApplication\nservice\n(controller)\nMessage bus\nUser\nCompany\nObservable behavior\nfor external client\nObservable behavior\nfor controller\nObservable\nbehavior for user\nFigure 7.13\nA map that shows communications among components in the CRM and the \nrelationship between these communications and observable behavior\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2478,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 202,
      "content": "180\nCHAPTER 7\nRefactoring toward valuable unit tests\nthese method calls (which is the topic of the next chapter). However, the subsequent\ncall from the controller to User doesn’t have an immediate connection to the goals of\nthe external client. That client doesn’t care how the controller decides to implement\nthe change of email as long as the final state of the system is correct and the call to the\nmessage bus is in place. Therefore, you shouldn’t verify calls the controller makes to\nUser when testing that controller’s behavior.\n When you step one level down the call stack, you get a similar situation. Now it’s\nthe controller who is the client, and the ChangeEmail method in User has an immedi-\nate connection to that client’s goal of changing the user email and thus should be\ntested. But the subsequent calls from User to Company are implementation details\nfrom the controller’s point of view. Therefore, the test that covers the ChangeEmail\nmethod in User shouldn’t verify what methods User calls on Company. The same line\nof reasoning applies when you step one more level down and test the two methods in\nCompany from User’s point of view.\n Think of the observable behavior and implementation details as onion layers. Test\neach layer from the outer layer’s point of view, and disregard how that layer talks to\nthe underlying layers. As you peel these layers one by one, you switch perspective:\nwhat previously was an implementation detail now becomes an observable behavior,\nwhich you then cover with another set of tests. \nSummary\nCode complexity is defined by the number of decision-making points in the\ncode, both explicit (made by the code itself) and implicit (made by the libraries\nthe code uses).\nDomain significance shows how significant the code is for the problem domain\nof your project. Complex code often has high domain significance and vice\nversa, but not in 100% of all cases.\nComplex code and code that has domain significance benefit from unit test-\ning the most because the corresponding tests have greater protection against\nregressions.\nUnit tests that cover code with a large number of collaborators have high\nmaintenance costs. Such tests require a lot of space to bring collaborators to\nan expected condition and then check their state or interactions with them\nafterward.\nAll production code can be categorized into four types of code by its complexity\nor domain significance and the number of collaborators:\n– Domain model and algorithms (high complexity or domain significance, few\ncollaborators) provide the best return on unit testing efforts.\n– Trivial code (low complexity and domain significance, few collaborators)\nisn’t worth testing at all.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2743,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 203,
      "content": "181\nSummary\n– Controllers (low complexity and domain significance, large number of col-\nlaborators) should be tested briefly by integration tests.\n– Overcomplicated code (high complexity or domain significance, large num-\nber of collaborators) should be split into controllers and complex code.\nThe more important or complex the code is, the fewer collaborators it should\nhave.\nThe Humble Object pattern helps make overcomplicated code testable by\nextracting business logic out of that code into a separate class. As a result, the\nremaining code becomes a controller—a thin, humble wrapper around the busi-\nness logic.\nThe hexagonal and functional architectures implement the Humble Object\npattern. Hexagonal architecture advocates for the separation of business logic and\ncommunications with out-of-process dependencies. Functional architecture sepa-\nrates business logic from communications with all collaborators, not just out-of-\nprocess ones.\nThink of the business logic and orchestration responsibilities in terms of code\ndepth versus code width. Your code can be either deep (complex or important)\nor wide (work with many collaborators), but never both.\nTest preconditions if they have a domain significance; don’t test them otherwise.\nThere are three important attributes when it comes to separating business logic\nfrom orchestration:\n– Domain model testability—A function of the number and the type of collabora-\ntors in domain classes\n– Controller simplicity—Depends on the presence of decision-making points in\nthe controller\n– Performance—Defined by the number of calls to out-of-process dependencies\nYou can have a maximum of two of these three attributes at any given moment:\n– Pushing all external reads and writes to the edges of a business operation—Preserves\ncontroller simplicity and keeps the domain model testability, but concedes\nperformance\n– Injecting out-of-process dependencies into the domain model—Keeps performance\nand the controller’s simplicity, but damages domain model testability\n– Splitting the decision-making process into more granular steps—Preserves perfor-\nmance and domain model testability, but gives up controller simplicity\nSplitting the decision-making process into more granular steps—Is a trade-off with the\nbest set of pros and cons. You can mitigate the growth of controller complexity\nusing the following two patterns:\n– The CanExecute/Execute pattern introduces a CanDo() for each Do() method\nand makes its successful execution a precondition for Do(). This pattern\nessentially eliminates the controller’s decision-making because there’s no\noption not to call CanDo() before Do().\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2691,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 204,
      "content": "182\nCHAPTER 7\nRefactoring toward valuable unit tests\n– Domain events help track important changes in the domain model, and then\nconvert those changes to calls to out-of-process dependencies. This pattern\nremoves the tracking responsibility from the controller.\nIt’s easier to test abstractions than the things they abstract. Domain events are\nabstractions on top of upcoming calls to out-of-process dependencies. Changes\nin domain classes are abstractions on top of upcoming modifications in the\ndata storage.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 562,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 205,
      "content": "Part 3\nIntegration testing\nHave you ever been in a situation where all the unit tests pass but the\napplication still doesn’t work? Validating software components in isolation from\neach other is important, but it’s equally important to check how those compo-\nnents work in integration with external systems. This is where integration testing\ncomes into play.\n In chapter 8, we’ll look at integration testing in general and revisit the Test\nPyramid concept. You’ll learn the trade-offs inherent to integration testing and\nhow to navigate them. Chapters 9 and 10 will then discuss more specific topics.\nChapter 9 will teach you how to get the most out of your mocks. Chapter 10 is a\ndeep dive into working with relational databases in tests.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 790,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 206,
      "content": " \nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 53,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 207,
      "content": "185\nWhy integration testing?\nYou can never be sure your system works as a whole if you rely on unit tests exclu-\nsively. Unit tests are great at verifying business logic, but it’s not enough to check\nthat logic in a vacuum. You have to validate how different parts of it integrate with\neach other and external systems: the database, the message bus, and so on.\n In this chapter, you’ll learn the role of integration tests: when you should apply\nthem and when it’s better to rely on plain old unit tests or even other techniques\nsuch as the Fail Fast principle. You will see which out-of-process dependencies to\nuse as-is in integration tests and which to replace with mocks. You will also see inte-\ngration testing best practices that will help improve the health of your code base in\ngeneral: making domain model boundaries explicit, reducing the number of layers\nin the application, and eliminating circular dependencies. Finally, you’ll learn why\ninterfaces with a single implementation should be used sporadically, and how and\nwhen to test logging functionality.\nThis chapter covers\nUnderstanding the role of integration testing\nDiving deeper into the Test Pyramid concept\nWriting valuable integration tests\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1266,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 208,
      "content": "186\nCHAPTER 8\nWhy integration testing?\n8.1\nWhat is an integration test?\nIntegration tests play an important role in your test suite. It’s also crucial to balance\nthe number of unit and integration tests. You will see shortly what that role is and how\nto maintain the balance, but first, let me give you a refresher on what differentiates an\nintegration test from a unit test.\n8.1.1\nThe role of integration tests\nAs you may remember from chapter 2, a unit test is a test that meets the following three\nrequirements:\nVerifies a single unit of behavior,\nDoes it quickly,\nAnd does it in isolation from other tests.\nA test that doesn’t meet at least one of these three requirements falls into the category\nof integration tests. An integration test then is any test that is not a unit test.\n In practice, integration tests almost always verify how your system works in integra-\ntion with out-of-process dependencies. In other words, these tests cover the code from\nthe controllers quadrant (see chapter 7 for more details about code quadrants). The\ndiagram in figure 8.1 shows the typical responsibilities of unit and integration tests.\nUnit tests cover the domain model, while integration tests check the code that glues\nthat domain model with out-of-process dependencies.\nDomain model,\nalgorithms\nOvercomplicated\ncode\nTrivial code\nControllers\nComplexity,\ndomain\nsigniﬁcance\nNumber of\ncollaborators\nIntegration\ntests\nUnit tests\nFigure 8.1\nIntegration tests cover controllers, while unit tests cover the domain \nmodel and algorithms. Trivial and overcomplicated code shouldn’t be tested at all.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1643,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 209,
      "content": "187\nWhat is an integration test?\nNote that tests covering the controllers quadrant can sometimes be unit tests too. If all\nout-of-process dependencies are replaced with mocks, there will be no dependencies\nshared between tests, which will allow those tests to remain fast and maintain their iso-\nlation from each other. Most applications do have an out-of-process dependency that\ncan’t be replaced with a mock, though. It’s usually a database—a dependency that is\nnot visible to other applications.\n As you may also remember from chapter 7, the other two quadrants from figure 8.1\n(trivial code and overcomplicated code) shouldn’t be tested at all. Trivial code isn’t\nworth the effort, while overcomplicated code should be refactored into algorithms\nand controllers. Thus, all your tests must focus on the domain model and the control-\nlers quadrants exclusively. \n8.1.2\nThe Test Pyramid revisited\nIt’s important to maintain a balance between unit and integration tests. Working\ndirectly with out-of-process dependencies makes integration tests slow. Such tests are\nalso more expensive to maintain. The increase in maintainability costs is due to\nThe necessity to keep the out-of-process dependencies operational\nThe greater number of collaborators involved, which inflates the test’s size\nOn the other hand, integration tests go through a larger amount of code (both your\ncode and the code of the libraries used by the application), which makes them better\nthan unit tests at protecting against regressions. They are also more detached from\nthe production code and therefore have better resistance to refactoring.\n The ratio between unit and integration tests can differ depending on the project’s\nspecifics, but the general rule of thumb is the following: check as many of the business\nscenario’s edge cases as possible with unit tests; use integration tests to cover one\nhappy path, as well as any edge cases that can’t be covered by unit tests.\nDEFINITION\nA happy path is a successful execution of a business scenario. An\nedge case is when the business scenario execution results in an error.\nShifting the majority of the workload to unit tests helps keep maintenance costs low.\nAt the same time, having one or two overarching integration tests per business sce-\nnario ensures the correctness of your system as a whole. This guideline forms the pyr-\namid-like ratio between unit and integration tests, as shown in figure 8.2 (as discussed\nin chapter 2, end-to-end tests are a subset of integration tests).\n The Test Pyramid can take different shapes depending on the project’s complexity.\nSimple applications have little (if any) code in the domain model and algorithms\nquadrant. As a result, tests form a rectangle instead of a pyramid, with an equal num-\nber of unit and integration tests (figure 8.3). In the most trivial cases, you might have\nno unit tests whatsoever.\n Note that integration tests retain their value even in simple applications. Regard-\nless of how simple your code is, it’s still important to verify how it works in integration\nwith other subsystems. \nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 3131,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 210,
      "content": "188\nCHAPTER 8\nWhy integration testing?\n8.1.3\nIntegration testing vs. failing fast\nThis section elaborates on the guideline of using integration tests to cover one happy\npath per business scenario and any edge cases that can’t be covered by unit tests. \n For an integration test, select the longest happy path in order to verify interactions\nwith all out-of-process dependencies. If there’s no one path that goes through all such\ninteractions, write additional integration tests—as many as needed to capture commu-\nnications with every external system.\n As with the edge cases that can’t be covered by unit tests, there are exceptions to\nthis part of the guideline, too. There’s no need to test an edge case if an incorrect\nexecution of that edge case immediately fails the entire application. For example, you\nsaw in chapter 7 how User from the sample CRM system implemented a CanChange-\nEmail method and made its successful execution a precondition for ChangeEmail():\nEnd-\nto-end\nIntegration\ntests\nUnit tests\nTest count\nProtection against\nregressions,\nresistance to\nrefactoring\nFast feedback,\nmaintainability\nFigure 8.2\nThe Test Pyramid represents a trade-off that works best for most \napplications. Fast, cheap unit tests cover the majority of edge cases, while a \nsmaller number of slow, more expensive integration tests ensure the correctness \nof the system as a whole.\nFigure 8.3\nThe Test Pyramid of a simple project. \nLittle complexity requires a smaller number of unit \ntests compared to a normal pyramid.\nUnit tests\nIntegration tests\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1593,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 211,
      "content": "189\nWhat is an integration test?\npublic void ChangeEmail(string newEmail, Company company)\n{\nPrecondition.Requires(CanChangeEmail() == null);\n/* the rest of the method */\n}\nThe controller invokes CanChangeEmail() and interrupts the operation if that\nmethod returns an error:\n// UserController\npublic string ChangeEmail(int userId, string newEmail)\n{\nobject[] userData = _database.GetUserById(userId);\nUser user = UserFactory.Create(userData);\nstring error = user.CanChangeEmail();\nif (error != null)                    \nreturn error;                     \n/* the rest of the method */\n}\nThis example shows the edge case you could theoretically cover with an integration\ntest. Such a test doesn’t provide a significant enough value, though. If the controller\ntries to change the email without consulting with CanChangeEmail() first, the applica-\ntion crashes. This bug reveals itself with the first execution and thus is easy to notice\nand fix. It also doesn’t lead to data corruption.\nTIP\nIt’s better to not write a test at all than to write a bad test. A test that\ndoesn’t provide significant value is a bad test.\nUnlike the call from the controller to CanChangeEmail(), the presence of the precon-\ndition in User should be tested. But that is better done with a unit test; there’s no need\nfor an integration test.\n Making bugs manifest themselves quickly is called the Fail Fast principle, and it’s a\nviable alternative to integration testing.\nThe Fail Fast principle \nThe Fail Fast principle stands for stopping the current operation as soon as any unex-\npected error occurs. This principle makes your application more stable by\nShortening the feedback loop—The sooner you detect a bug, the easier it is\nto fix. A bug that is already in production is orders of magnitude more expen-\nsive to fix compared to a bug found during development.\nProtecting the persistence state—Bugs lead to corruption of the application’s\nstate. Once that state penetrates into the database, it becomes much harder\nto fix. Failing fast helps you prevent the corruption from spreading.\nEdge case\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2128,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 212,
      "content": "190\nCHAPTER 8\nWhy integration testing?\n8.2\nWhich out-of-process dependencies to test directly\nAs I mentioned earlier, integration tests verify how your system integrates with out-of-\nprocess dependencies. There are two ways to implement such verification: use the real\nout-of-process dependency, or replace that dependency with a mock. This section\nshows when to apply each of the two approaches.\n8.2.1\nThe two types of out-of-process dependencies\nAll out-of-process dependencies fall into two categories:\nManaged dependencies (out-of-process dependencies you have full control over)—These\ndependencies are only accessible through your application; interactions with\nthem aren’t visible to the external world. A typical example is a database. Exter-\nnal systems normally don’t access your database directly; they do that through\nthe API your application provides.\nUnmanaged dependencies (out-of-process dependencies you don’t have full control over)—\nInteractions with such dependencies are observable externally. Examples include\nan SMTP server and a message bus: both produce side effects visible to other\napplications.\nI mentioned in chapter 5 that communications with managed dependencies are\nimplementation details. Conversely, communications with unmanaged dependencies\nare part of your system’s observable behavior (figure 8.4). This distinction leads to the\ndifference in treatment of out-of-process dependencies in integration tests.\nIMPORTANT\nUse real instances of managed dependencies; replace unman-\naged dependencies with mocks.\nAs discussed in chapter 5, the requirement to preserve the communication pattern\nwith unmanaged dependencies stems from the necessity to maintain backward com-\npatibility with those dependencies. Mocks are perfect for this task. With mocks, you\ncan ensure communication pattern permanence in light of any possible refactorings.\n(continued)\nStopping the current operation is normally done by throwing exceptions, because\nexceptions have semantics that are perfectly suited for the Fail Fast principle: they\ninterrupt the program flow and pop up to the highest level of the execution stack,\nwhere you can log them and shut down or restart the operation.\nPreconditions are one example of the Fail Fast principle in action. A failing precondi-\ntion signifies an incorrect assumption made about the application state, which is\nalways a bug. Another example is reading data from a configuration file. You can\narrange the reading logic such that it will throw an exception if the data in the config-\nuration file is incomplete or incorrect. You can also put this logic close to the appli-\ncation startup, so that the application doesn’t launch if there’s a problem with its\nconfiguration. \nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2778,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 213,
      "content": "191\nWhich out-of-process dependencies to test directly\nHowever, there’s no need to maintain backward compatibility in communications with\nmanaged dependencies, because your application is the only one that talks to them.\nExternal clients don’t care how you organize your database; the only thing that mat-\nters is the final state of your system. Using real instances of managed dependencies in\nintegration tests helps you verify that final state from the external client’s point of\nview. It also helps during database refactorings, such as renaming a column or even\nmigrating from one database to another. \n8.2.2\nWorking with both managed and unmanaged dependencies\nSometimes you’ll encounter an out-of-process dependency that exhibits attributes of\nboth managed and unmanaged dependencies. A good example is a database that\nother applications have access to.\n The story usually goes like this. A system begins with its own dedicated database. After\na while, another system begins to require data from the same database. And so the team\ndecides to share access to a limited number of tables just for ease of integration with that\nother system. As a result, the database becomes a dependency that is both managed and\nunmanaged. It still contains parts that are visible to your application only; but, in addi-\ntion to those parts, it also has a number of tables accessible by other applications.\n The use of a database is a poor way to implement integration between systems\nbecause it couples these systems to each other and complicates their further develop-\nment. Only resort to this approach when all other options are exhausted. A better way\nto do the integration is via an API (for synchronous communications) or a message\nbus (for asynchronous communications).\n But what do you do when you already have a shared database and can’t do any-\nthing about it in the foreseeable future? In this case, treat tables that are visible to\nSMTP service\n(unmanaged\ndependency)\nObservable behavior (contract)\nImplementation details\nApplication\ndatabase\n(managed\ndependency)\nThird-party\nsystem\n(external\nclient)\nFigure 8.4\nCommunications with managed dependencies are implementation \ndetails; use such dependencies as-is in integration tests. Communications \nwith unmanaged dependencies are part of your system’s observable behavior. \nSuch dependencies should be mocked out.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2415,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 214,
      "content": "192\nCHAPTER 8\nWhy integration testing?\nother applications as an unmanaged dependency. Such tables in effect act as a mes-\nsage bus, with their rows playing the role of messages. Use mocks to make sure the\ncommunication pattern with these tables remains unchanged. At the same time, treat\nthe rest of your database as a managed dependency and verify its final state, not the\ninteractions with it (figure 8.5).\nIt’s important to differentiate these two parts of your database because, again, the\nshared tables are observable externally, and you need to be careful about how your\napplication communicates with them. Don’t change the way your system interacts with\nthose tables unless absolutely necessary! You never know how other applications will\nreact to such a change. \n8.2.3\nWhat if you can’t use a real database in integration tests?\nSometimes, for reasons outside of your control, you just can’t use a real version of a\nmanaged dependency in integration tests. An example would be a legacy database\nthat you can’t deploy to a test automation environment, not to mention a developer\nmachine, because of some IT security policy, or because the cost of setting up and\nmaintaining a test database instance is prohibitive.\n What should you do in such a situation? Should you mock out the database anyway,\ndespite it being a managed dependency? No, because mocking out a managed depen-\ndency compromises the integration tests’ resistance to refactoring. Furthermore, such\nExternal applications\nTable\nTable\nTable\nTable\nManaged part\nTable\nTable\nUnmanaged part\nTest directly\nReplace with mocks\nDatabase\nYour application\nFigure 8.5\nTreat the part of the database that is visible to external \napplications as an unmanaged dependency. Replace it with mocks in \nintegration tests. Treat the rest of the database as a managed dependency. \nVerify its final state, not interactions with it.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1930,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 215,
      "content": "193\nIntegration testing: An example\ntests no longer provide as good protection against regressions. And if the database is\nthe only out-of-process dependency in your project, the resulting integration tests\nwould deliver no additional protection compared to the existing set of unit tests (assum-\ning these unit tests follow the guidelines from chapter 7).\n The only thing such integration tests would do, in addition to unit tests, is check\nwhat repository methods the controller calls. In other words, you wouldn’t really gain\nconfidence about anything other than those three lines of code in your controller\nbeing correct, while still having to do a lot of plumbing.\n If you can’t test the database as-is, don’t write integration tests at all, and instead,\nfocus exclusively on unit testing of the domain model. Remember to always put all\nyour tests under close scrutiny. Tests that don’t provide a high enough value should\nhave no place in your test suite. \n8.3\nIntegration testing: An example\nLet’s get back to the sample CRM system from chapter 7 and see how it can be cov-\nered with integration tests. As you may recall, this system implements one feature:\nchanging the user’s email. It retrieves the user and the company from the database,\ndelegates the decision-making to the domain model, and then saves the results back\nto the database and puts a message on the bus if needed (figure 8.6).\nThe following listing shows how the controller currently looks.\npublic class UserController\n{\nprivate readonly Database _database = new Database();\nprivate readonly MessageBus _messageBus = new MessageBus();\npublic string ChangeEmail(int userId, string newEmail)\n{\nListing 8.1\nThe user controller \nApplication\nservice\n(controller)\nBusiness logic\n(domain model)\nDatabase\nMessage bus\nGetUserById\nCanChangeEmail\nSaveCompany\nGetCompany\nChangeEmail\nSaveUser\nSendMessage\nFigure 8.6\nThe use case of changing the user’s email. The controller orchestrates the work between \nthe database, the message bus, and the domain model.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2070,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 216,
      "content": "194\nCHAPTER 8\nWhy integration testing?\nobject[] userData = _database.GetUserById(userId);\nUser user = UserFactory.Create(userData);\nstring error = user.CanChangeEmail();\nif (error != null)\nreturn error;\nobject[] companyData = _database.GetCompany();\nCompany company = CompanyFactory.Create(companyData);\nuser.ChangeEmail(newEmail, company);\n_database.SaveCompany(company);\n_database.SaveUser(user);\nforeach (EmailChangedEvent ev in user.EmailChangedEvents)\n{\n_messageBus.SendEmailChangedMessage(ev.UserId, ev.NewEmail);\n}\nreturn \"OK\";\n}\n}\nIn the following section, I’ll first outline scenarios to verify using integration tests.\nThen I’ll show you how to work with the database and the message bus in tests.\n8.3.1\nWhat scenarios to test?\nAs I mentioned earlier, the general guideline for integration testing is to cover the\nlongest happy path and any edge cases that can’t be exercised by unit tests. The longest\nhappy path is the one that goes through all out-of-process dependencies.\n In the CRM project, the longest happy path is a change from a corporate to a non-\ncorporate email. Such a change leads to the maximum number of side effects:\nIn the database, both the user and the company are updated: the user changes\nits type (from corporate to non-corporate) and email, and the company changes\nits number of employees.\nA message is sent to the message bus.\nAs for the edge cases that aren’t tested by unit tests, there’s only one such edge case:\nthe scenario where the email can’t be changed. There’s no need to test this scenario,\nthough, because the application will fail fast if this check isn’t present in the control-\nler. That leaves us with a single integration test:\npublic void Changing_email_from_corporate_to_non_corporate()\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1795,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 217,
      "content": "195\nIntegration testing: An example\n8.3.2\nCategorizing the database and the message bus\nBefore writing the integration test, you need to categorize the two out-of-process\ndependencies and decide which of them to test directly and which to replace with a\nmock. The application database is a managed dependency because no other system\ncan access it. Therefore, you should use a real instance of it. The integration test will\nInsert a user and a company into the database.\nRun the change of email scenario on that database.\nVerify the database state.\nOn the other hand, the message bus is an unmanaged dependency—its sole pur-\npose is to enable communication with other systems. The integration test will mock\nout the message bus and verify the interactions between the controller and the\nmock afterward. \n8.3.3\nWhat about end-to-end testing?\nThere will be no end-to-end tests in our sample project. An end-to-end test in a sce-\nnario with an API would be a test running against a deployed, fully functioning ver-\nsion of that API, which means no mocks for any of the out-of-process dependencies\n(figure 8.7). On the other hand, integration tests host the application within the same\nprocess and substitute unmanaged dependencies with mocks (figure 8.8).\n As I mentioned in chapter 2, whether to use end-to-end tests is a judgment call. For\nthe most part, when you include managed dependencies in the integration testing\nscope and mock out only unmanaged dependencies, integration tests provide a level\nEnd-to-end test\nApplication\nMessage bus\nDatabase\nOut-of-process\nIn-process\nFigure 8.7\nEnd-to-end tests emulate the external client and therefore test a \ndeployed version of the application with all out-of-process dependencies included \nin the testing scope. End-to-end tests shouldn’t check managed dependencies \n(such as the database) directly, only indirectly through the application.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1941,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 218,
      "content": "196\nCHAPTER 8\nWhy integration testing?\nof protection that is close enough to that of end-to-end tests, so you can skip end-to-\nend testing. However, you could still create one or two overarching end-to-end tests\nthat would provide a sanity check for the project after deployment. Make such tests go\nthrough the longest happy path, too, to ensure that your application communicates\nwith all out-of-process dependencies properly. To emulate the external client’s behav-\nior, check the message bus directly, but verify the database’s state through the applica-\ntion itself. \n8.3.4\nIntegration testing: The first try\nHere’s the first version of the integration test.\n[Fact]\npublic void Changing_email_from_corporate_to_non_corporate()\n{\n// Arrange\nvar db = new Database(ConnectionString);\n    \nUser user = CreateUser(\n   \n\"user@mycorp.com\", UserType.Employee, db);   \nCreateCompany(\"mycorp.com\", 1, db);\n   \nvar messageBusMock = new Mock<IMessageBus>();         \nvar sut = new UserController(db, messageBusMock.Object);\n// Act\nstring result = sut.ChangeEmail(user.UserId, \"new@gmail.com\");\nListing 8.2\nThe integration test\nIntegration test\nApplication\nMessage bus\nmock\nDatabase\nOut-of-process\nIn-process\nFigure 8.8\nIntegration tests host the application within the same process. Unlike \nend-to-end tests, integration tests substitute unmanaged dependencies with \nmocks. The only out-of-process components for integration tests are managed \ndependencies.\nDatabase \nrepository\nCreates the user \nand company in \nthe database\nSets up a \nmock for the \nmessage bus\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1606,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 219,
      "content": "197\nUsing interfaces to abstract dependencies\n// Assert\nAssert.Equal(\"OK\", result);\nobject[] userData = db.GetUserById(user.UserId);   \nUser userFromDb = UserFactory.Create(userData);    \nAssert.Equal(\"new@gmail.com\", userFromDb.Email);   \nAssert.Equal(UserType.Customer, userFromDb.Type);  \nobject[] companyData = db.GetCompany();\n   \nCompany companyFromDb = CompanyFactory\n   \n.Create(companyData);\n   \nAssert.Equal(0, companyFromDb.NumberOfEmployees);  \nmessageBusMock.Verify(\n    \nx => x.SendEmailChangedMessage(\n    \nuser.UserId, \"new@gmail.com\"),    \nTimes.Once);\n     \n}\nTIP\nNotice that in the arrange section, the test doesn’t insert the user and\nthe company into the database on its own but instead calls the CreateUser\nand CreateCompany helper methods. These methods can be reused across\nmultiple integration tests.\nIt’s important to check the state of the database independently of the data used as\ninput parameters. To do that, the integration test queries the user and company data\nseparately in the assert section, creates new userFromDb and companyFromDb instances,\nand only then asserts their state. This approach ensures that the test exercises both\nwrites to and reads from the database and thus provides the maximum protection\nagainst regressions. The reading itself must be implemented using the same code the\ncontroller uses internally: in this example, using the Database, UserFactory, and\nCompanyFactory classes.\n This integration test, while it gets the job done, can still benefit from some\nimprovement. For instance, you could use helper methods in the assertion section, too,\nin order to reduce this section’s size. Also, messageBusMock doesn’t provide as good\nprotection against regressions as it potentially could. We’ll talk about these improve-\nments in the subsequent two chapters where we discuss mocking and database testing\nbest practices. \n8.4\nUsing interfaces to abstract dependencies\nOne of the most misunderstood subjects in the sphere of unit testing is the use of\ninterfaces. Developers often ascribe invalid reasons to why they introduce interfaces\nand, as a result, tend to overuse them. In this section, I’ll expand on those invalid\nreasons and show in what circumstances the use of interfaces is and isn’t preferable.\n \nAsserts the \nuser’s state\nAsserts the \ncompany’s \nstate\nChecks the \ninteractions \nwith the mock\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2412,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 220,
      "content": "198\nCHAPTER 8\nWhy integration testing?\n8.4.1\nInterfaces and loose coupling\nMany developers introduce interfaces for out-of-process dependencies, such as the\ndatabase or the message bus, even when these interfaces have only one implementation.\nThis practice has become so widespread nowadays that hardly anyone questions it.\nYou’ll often see class-interface pairs similar to the following:\npublic interface IMessageBus\npublic class MessageBus : IMessageBus\npublic interface IUserRepository\npublic class UserRepository : IUserRepository\nThe common reasoning behind the use of such interfaces is that they help to\nAbstract out-of-process dependencies, thus achieving loose coupling\nAdd new functionality without changing the existing code, thus adhering to the\nOpen-Closed principle (OCP)\nBoth of these reasons are misconceptions. Interfaces with a single implementation are\nnot abstractions and don’t provide loose coupling any more than concrete classes that\nimplement those interfaces. Genuine abstractions are discovered, not invented. The dis-\ncovery, by definition, takes place post factum, when the abstraction already exists but\nis not yet clearly defined in the code. Thus, for an interface to be a genuine abstrac-\ntion, it must have at least two implementations.\n The second reason (the ability to add new functionality without changing the exist-\ning code) is a misconception because it violates a more foundational principle:\nYAGNI. YAGNI stands for “You aren’t gonna need it” and advocates against investing\ntime in functionality that’s not needed right now. You shouldn’t develop this function-\nality, nor should you modify your existing code to account for the appearance of such\nfunctionality in the future. The two major reasons are as follows:\nOpportunity cost—If you spend time on a feature that business people don’t need\nat the moment, you steer that time away from features they do need right now.\nMoreover, when the business people finally come to require the developed func-\ntionality, their view on it will most likely have evolved, and you will still need to\nadjust the already-written code. Such activity is wasteful. It’s more beneficial to\nimplement the functionality from scratch when the actual need for it emerges.\nThe less code in the project, the better. Introducing code just in case without an imme-\ndiate need unnecessarily increases your code base’s cost of ownership. It’s bet-\nter to postpone introducing new functionality until as late a stage of your\nproject as possible.\nTIP\nWriting code is an expensive way to solve problems. The less code the\nsolution requires and the simpler that code is, the better.\nThere are exceptional cases where YAGNI doesn’t apply, but these are few and far\nbetween. For those cases, see my article “OCP vs YAGNI,” at https://enterprise-\ncraftsmanship.com/posts/ocp-vs-yagni. \nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2901,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 221,
      "content": "199\nUsing interfaces to abstract dependencies\n8.4.2\nWhy use interfaces for out-of-process dependencies?\nSo, why use interfaces for out-of-process dependencies at all, assuming that each of\nthose interfaces has only one implementation? The real reason is much more practi-\ncal and down-to-earth. It’s to enable mocking—as simple as that. Without an interface,\nyou can’t create a test double and thus can’t verify interactions between the system\nunder test and the out-of-process dependency.\n Therefore, don’t introduce interfaces for out-of-process dependencies unless you need to mock\nout those dependencies. You only mock out unmanaged dependencies, so the guideline\ncan be boiled down to this: use interfaces for unmanaged dependencies only. Still inject\nmanaged dependencies into the controller explicitly, but use concrete classes for that.\n Note that genuine abstractions (abstractions that have more than one implementa-\ntion) can be represented with interfaces regardless of whether you mock them out.\nIntroducing an interface with a single implementation for reasons other than mock-\ning is a violation of YAGNI, however.\n And you might have noticed in listing 8.2 that UserController now accepts both\nthe message bus and the database explicitly via the constructor, but only the message\nbus has a corresponding interface. The database is a managed dependency and thus\ndoesn’t require such an interface. Here’s the controller:\npublic class UserController\n{\nprivate readonly Database _database;   \nprivate readonly IMessageBus _messageBus;    \npublic UserController(Database database, IMessageBus messageBus)\n{\n_database = database;\n_messageBus = messageBus;\n}\npublic string ChangeEmail(int userId, string newEmail)\n{\n/* the method uses _database and _messageBus */\n}\n}\nNOTE\nYou can mock out a dependency without resorting to an interface by\nmaking methods in that dependency virtual and using the class itself as a base\nfor the mock. This approach is inferior to the one with interfaces, though. I\nexplain more on this topic of interfaces versus base classes in chapter 11. \n8.4.3\nUsing interfaces for in-process dependencies\nYou sometimes see code bases where interfaces back not only out-of-process depen-\ndencies but in-process dependencies as well. For example:\npublic interface IUser\n{\nint UserId { get; set; }\nA concrete \nclass\nThe interface\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2407,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 222,
      "content": "200\nCHAPTER 8\nWhy integration testing?\nstring Email { get; }\nstring CanChangeEmail();\nvoid ChangeEmail(string newEmail, Company company);\n}\npublic class User : IUser\n{\n/* ... */\n}\nAssuming that IUser has only one implementation (and such specific interfaces always\nhave only one implementation), this is a huge red flag. Just like with out-of-process\ndependencies, the only reason to introduce an interface with a single implementation\nfor a domain class is to enable mocking. But unlike out-of-process dependencies, you\nshould never check interactions between domain classes, because doing so results in\nbrittle tests: tests that couple to implementation details and thus fail on the metric of\nresisting to refactoring (see chapter 5 for more details about mocks and test fragility). \n8.5\nIntegration testing best practices\nThere are some general guidelines that can help you get the most out of your integra-\ntion tests:\nMaking domain model boundaries explicit\nReducing the number of layers in the application\nEliminating circular dependencies\nAs usual, best practices that are beneficial for tests also tend to improve the health of\nyour code base in general.\n8.5.1\nMaking domain model boundaries explicit\nTry to always have an explicit, well-known place for the domain model in your code\nbase. The domain model is the collection of domain knowledge about the problem your\nproject is meant to solve. Assigning the domain model an explicit boundary helps you\nbetter visualize and reason about that part of your code.\n This practice also helps with testing. As I mentioned earlier in this chapter, unit\ntests target the domain model and algorithms, while integration tests target control-\nlers. The explicit boundary between domain classes and controllers makes it easier to\ntell the difference between unit and integration tests.\n The boundary itself can take the form of a separate assembly or a namespace. The\nparticulars aren’t that important as long as all of the domain logic is put under a sin-\ngle, distinct umbrella and not scattered across the code base. \n8.5.2\nReducing the number of layers\nMost programmers naturally gravitate toward abstracting and generalizing the code\nby introducing additional layers of indirection. In a typical enterprise-level applica-\ntion, you can easily observe several such layers (figure 8.9).\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2390,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 223,
      "content": "201\nIntegration testing best practices\nIn extreme cases, an application gets so many abstraction layers that it becomes too\nhard to navigate the code base and understand the logic behind even the simplest\noperations. At some point, you just want to get to the specific solution of the problem\nat hand, not some generalization of that solution in a vacuum.\nAll problems in computer science can be solved by another layer of indirection, except for\nthe problem of too many layers of indirection.\n                                                                   \n—David J. Wheeler\nLayers of indirection negatively affect your ability to reason about the code. When\nevery feature has a representation in each of those layers, you have to expend signifi-\ncant effort assembling all the pieces into a cohesive picture. This creates an additional\nmental burden that handicaps the entire development process.\n An excessive number of abstractions doesn’t help unit or integration testing,\neither. Code bases with many layers of indirections tend not to have a clear boundary\nbetween controllers and the domain model (which, as you might remember from\nchapter 7, is a precondition for effective tests). There’s also a much stronger tendency\nto verify each layer separately. This tendency results in a lot of low-value integration\ntests, each of which exercises only the code from a specific layer and mocks out layers\nApplication\nservices layer\nBusiness logic\nimplementation layer\nAbstractions layer\nPersistence layer\nOrder checkout\nChanging user email\nResetting password\nFigure 8.9\nVarious application concerns are often addressed by \nseparate layers of indirection. A typical feature takes up a small \nportion of each layer.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1770,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 224,
      "content": "202\nCHAPTER 8\nWhy integration testing?\nunderneath. The end result is always the same: insufficient protection against regres-\nsions combined with low resistance to refactoring.\n Try to have as few layers of indirection as possible. In most backend systems, you\ncan get away with just three: the domain model, application services layer (control-\nlers), and infrastructure layer. The infrastructure layer typically consists of algorithms\nthat don’t belong to the domain model, as well as code that enables access to out-of-\nprocess dependencies (figure 8.10). \n8.5.3\nEliminating circular dependencies\nAnother practice that can drastically improve the maintainability of your code base\nand make testing easier is eliminating circular dependencies.\nDEFINITION\nA circular dependency (also known as cyclic dependency) is two or\nmore classes that directly or indirectly depend on each other to function\nproperly.\nA typical example of a circular dependency is a callback:\npublic class CheckOutService\n{\npublic void CheckOut(int orderId)\n{\nvar service = new ReportGenerationService();\nservice.GenerateReport(orderId, this);\nApplication\nservices layer\nDomain layer\nInfrastructure layer\nOrder checkout\nChanging user email\nResetting password\nFigure 8.10\nYou can get away with just three layers: the domain layer (contains \ndomain logic), application services layers (provides an entry point for the external \nclient, and coordinates the work between domain classes and out-of-process \ndependencies), and infrastructure layer (works with out-of-process dependencies; \ndatabase repositories, ORM mappings, and SMTP gateways reside in this layer).\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1685,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 225,
      "content": "203\nIntegration testing best practices\n/* other code */\n}\n}\npublic class ReportGenerationService\n{\npublic void GenerateReport(\nint orderId,\nCheckOutService checkOutService)\n{\n/* calls checkOutService when generation is completed */\n}\n}\nHere, CheckOutService creates an instance of ReportGenerationService and passes\nitself to that instance as an argument. ReportGenerationService calls CheckOut-\nService back to notify it about the result of the report generation.\n Just like an excessive number of abstraction layers, circular dependencies add tre-\nmendous cognitive load when you try to read and understand the code. The reason is\nthat circular dependencies don’t give you a clear starting point from which you can\nbegin exploring the solution. To understand just one class, you have to read and\nunderstand the whole graph of its siblings all at once. Even a small set of interdepen-\ndent classes can quickly become too hard to grasp.\n Circular dependencies also interfere with testing. You often have to resort to inter-\nfaces and mocking in order to split the class graph and isolate a single unit of behav-\nior, which, again, is a no-go when it comes to testing the domain model (more on that\nin chapter 5).\n Note that the use of interfaces only masks the problem of circular dependencies. If\nyou introduce an interface for CheckOutService and make ReportGenerationService\ndepend on that interface instead of the concrete class, you remove the circular depen-\ndency at compile time (figure 8.11), but the cycle still persists at runtime. Even\nthough the compiler no longer regards this class composition as a circular reference,\nthe cognitive load required to understand the code doesn’t become any smaller. If\nanything, it increases due to the additional interface.\nCheckOutService\nICheckOutService\nReportGenerationService\nFigure 8.11\nWith an interface, you remove the circular dependency \nat compile time, but not at runtime. The cognitive load required to \nunderstand the code doesn’t become any smaller.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2064,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 226,
      "content": "204\nCHAPTER 8\nWhy integration testing?\nA better approach to handle circular dependencies is to get rid of them. Refactor\nReportGenerationService such that it depends on neither CheckOutService nor the\nICheckOutService interface, and make ReportGenerationService return the result\nof its work as a plain value instead of calling CheckOutService:\npublic class CheckOutService\n{\npublic void CheckOut(int orderId)\n{\nvar service = new ReportGenerationService();\nReport report = service.GenerateReport(orderId);\n/* other work */\n}\n}\npublic class ReportGenerationService\n{\npublic Report GenerateReport(int orderId)\n{\n/* ... */\n}\n}\nIt’s rarely possible to eliminate all circular dependencies in your code base. But even\nthen, you can minimize the damage by making the remaining graphs of interdepen-\ndent classes as small as possible. \n8.5.4\nUsing multiple act sections in a test\nAs you might remember from chapter 3, having more than one arrange, act, or assert\nsection in a test is a code smell. It’s a sign that this test checks multiple units of behav-\nior, which, in turn, hinders the test’s maintainability. For example, if you have two\nrelated use cases—say, user registration and user deletion—it might be tempting to\ncheck both of these use cases in a single integration test. Such a test could have the\nfollowing structure:\nArrange—Prepare data with which to register a user.\nAct—Call UserController.RegisterUser().\nAssert—Query the database to see if the registration is completed successfully.\nAct—Call UserController.DeleteUser().\nAssert—Query the database to make sure the user is deleted.\nThis approach is compelling because the user states naturally flow from one another,\nand the first act (registering a user) can simultaneously serve as an arrange phase for\nthe subsequent act (user deletion). The problem is that such tests lose focus and can\nquickly become too bloated.\n It’s best to split the test by extracting each act into a test of its own. It may seem like\nunnecessary work (after all, why create two tests where one would suffice?), but this\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2118,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 227,
      "content": "205\nHow to test logging functionality\nwork pays off in the long run. Having each test focus on a single unit of behavior\nmakes those tests easier to understand and modify when necessary.\n The exception to this guideline is tests working with out-of-process dependencies\nthat are hard to bring to a desirable state. Let’s say for example that registering a user\nresults in creating a bank account in an external banking system. The bank has provi-\nsioned a sandbox for your organization, and you want to use that sandbox in an end-\nto-end test. The problem is that the sandbox is too slow, or maybe the bank limits the\nnumber of calls you can make to that sandbox. In such a scenario, it becomes benefi-\ncial to combine multiple acts into a single test and thus reduce the number of interac-\ntions with the problematic out-of-process dependency.\n Hard-to-manage out-of-process dependencies are the only legitimate reason to\nwrite a test with more than one act section. This is why you should never have multiple\nacts in a unit test—unit tests don’t work with out-of-process dependencies. Even inte-\ngration tests should rarely have several acts. In practice, multistep tests almost always\nbelong to the category of end-to-end tests. \n8.6\nHow to test logging functionality\nLogging is a gray area, and it isn’t obvious what to do with it when it comes to testing.\nThis is a complex topic that I’ll split into the following questions:\nShould you test logging at all?\nIf so, how should you test it?\nHow much logging is enough?\nHow do you pass around logger instances?\nWe’ll use our sample CRM project as an example.\n8.6.1\nShould you test logging?\nLogging is a cross-cutting functionality, which you can require in any part of your code\nbase. Here’s an example of logging in the User class.\npublic class User\n{\npublic void ChangeEmail(string newEmail, Company company)\n{\n_logger.Info(    \n$\"Changing email for user {UserId} to {newEmail}\");\nPrecondition.Requires(CanChangeEmail() == null);\nif (Email == newEmail)\nreturn;\nUserType newType = company.IsEmailCorporate(newEmail)\n? UserType.Employee\n: UserType.Customer;\nListing 8.3\nAn example of logging in User\nStart of the\nmethod\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2227,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 228,
      "content": "206\nCHAPTER 8\nWhy integration testing?\nif (Type != newType)\n{\nint delta = newType == UserType.Employee ? 1 : -1;\ncompany.ChangeNumberOfEmployees(delta);\n_logger.Info(\n   \n$\"User {UserId} changed type \" +\n   \n$\"from {Type} to {newType}\");\n   \n}\nEmail = newEmail;\nType = newType;\nEmailChangedEvents.Add(new EmailChangedEvent(UserId, newEmail));\n_logger.Info(\n    \n$\"Email is changed for user {UserId}\");\n}\n}\nThe User class records in a log file each beginning and ending of the ChangeEmail\nmethod, as well as the change of the user type. Should you test this functionality?\n On the one hand, logging generates important information about the applica-\ntion’s behavior. But on the other hand, logging can be so ubiquitous that it’s not obvi-\nous whether this functionality is worth the additional, quite significant, testing effort.\nThe answer to the question of whether you should test logging comes down to this: Is\nlogging part of the application’s observable behavior, or is it an implementation detail?\n In that sense, it isn’t different from any other functionality. Logging ultimately\nresults in side effects in an out-of-process dependency such as a text file or a database.\nIf these side effects are meant to be observed by your customer, the application’s cli-\nents, or anyone else other than the developers themselves, then logging is an observ-\nable behavior and thus must be tested. If the only audience is the developers, then it’s\nan implementation detail that can be freely modified without anyone noticing, in\nwhich case it shouldn’t be tested.\n For example, if you write a logging library, then the logs this library produces are\nthe most important (and the only) part of its observable behavior. Another example is\nwhen business people insist on logging key application workflows. In this case, logs\nalso become a business requirement and thus have to be covered by tests. However, in\nthe latter example, you might also have separate logging just for developers.\n Steve Freeman and Nat Pryce, in their book Growing Object-Oriented Software, Guided\nby Tests (Addison-Wesley Professional, 2009), call these two types of logging support\nlogging and diagnostic logging:\nSupport logging produces messages that are intended to be tracked by support\nstaff or system administrators.\nDiagnostic logging helps developers understand what’s going on inside the\napplication. \nChanges the \nuser type\nEnd of the\nmethod\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2472,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 229,
      "content": "207\nHow to test logging functionality\n8.6.2\nHow should you test logging?\nBecause logging involves out-of-process dependencies, when it comes to testing it, the\nsame rules apply as with any other functionality that touches out-of-process dependen-\ncies. You need to use mocks to verify interactions between your application and the\nlog storage.\nINTRODUCING A WRAPPER ON TOP OF ILOGGER\nBut don’t just mock out the ILogger interface. Because support logging is a business\nrequirement, reflect that requirement explicitly in your code base. Create a special\nDomainLogger class where you explicitly list all the support logging needed for the\nbusiness; verify interactions with that class instead of the raw ILogger.\n For example, let’s say that business people require you to log all changes of the\nusers’ types, but the logging at the beginning and the end of the method is there just\nfor debugging purposes. The next listing shows the User class after introducing a\nDomainLogger class.\npublic void ChangeEmail(string newEmail, Company company)\n{\n_logger.Info(\n     \n$\"Changing email for user {UserId} to {newEmail}\");\nPrecondition.Requires(CanChangeEmail() == null);\nif (Email == newEmail)\nreturn;\nUserType newType = company.IsEmailCorporate(newEmail)\n? UserType.Employee\n: UserType.Customer;\nif (Type != newType)\n{\nint delta = newType == UserType.Employee ? 1 : -1;\ncompany.ChangeNumberOfEmployees(delta);\n_domainLogger.UserTypeHasChanged(         \nUserId, Type, newType);\n         \n}\nEmail = newEmail;\nType = newType;\nEmailChangedEvents.Add(new EmailChangedEvent(UserId, newEmail));\n_logger.Info(\n   \n$\"Email is changed for user {UserId}\");\n}\nThe diagnostic logging still uses the old logger (which is of type ILogger), but the\nsupport logging now uses the new domainLogger instance of type IDomainLogger. The\nfollowing listing shows the implementation of IDomainLogger.\nListing 8.4\nExtracting support logging into the DomainLogger class\nDiagnostic\nlogging\nSupport \nlogging\nDiagnostic\nlogging\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2044,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 230,
      "content": "208\nCHAPTER 8\nWhy integration testing?\npublic class DomainLogger : IDomainLogger\n{\nprivate readonly ILogger _logger;\npublic DomainLogger(ILogger logger)\n{\n_logger = logger;\n}\npublic void UserTypeHasChanged(\nint userId, UserType oldType, UserType newType)\n{\n_logger.Info(\n$\"User {userId} changed type \" +\n$\"from {oldType} to {newType}\");\n}\n}\nDomainLogger works on top of ILogger: it uses the domain language to declare spe-\ncific log entries required by the business, thus making support logging easier to\nunderstand and maintain. In fact, this implementation is very similar to the concept\nof structured logging, which enables great flexibility when it comes to log file post-\nprocessing and analysis. \nUNDERSTANDING STRUCTURED LOGGING\nStructured logging is a logging technique where capturing log data is decoupled from\nthe rendering of that data. Traditional logging works with simple text. A call like\nlogger.Info(\"User Id is \" + 12);\nfirst forms a string and then writes that string to a log storage. The problem with this\napproach is that the resulting log files are hard to analyze due to the lack of structure.\nFor example, it’s not easy to see how many messages of a particular type there are and\nhow many of those relate to a specific user ID. You’d need to use (or even write your\nown) special tooling for that.\n On the other hand, structured logging introduces structure to your log storage.\nThe use of a structured logging library looks similar on the surface:\nlogger.Info(\"User Id is {UserId}\", 12);\nBut its underlying behavior differs significantly. Behind the scenes, this method com-\nputes a hash of the message template (the message itself is stored in a lookup storage\nfor space efficiency) and combines that hash with the input parameters to form a set\nof captured data. The next step is the rendering of that data. You can still have a flat log\nfile, as with traditional logging, but that’s just one possible rendering. You could also\nconfigure the logging library to render the captured data as a JSON or a CSV file,\nwhere it would be easier to analyze (figure 8.12).\nListing 8.5\nDomainLogger as a wrapper on top of ILogger\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2196,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 231,
      "content": "209\nHow to test logging functionality\nDomainLogger in listing 8.5 isn’t a structured logger per se, but it operates in the same\nspirit. Look at this method once again:\npublic void UserTypeHasChanged(\nint userId, UserType oldType, UserType newType)\n{\n_logger.Info(\n$\"User {userId} changed type \" +\n$\"from {oldType} to {newType}\");\n}\nYou can view UserTypeHasChanged() as the message template’s hash. Together with\nthe userId, oldType, and newType parameters, that hash forms the log data. The\nmethod’s implementation renders the log data into a flat log file. And you can easily\ncreate additional renderings by also writing the log data into a JSON or a CSV file. \nWRITING TESTS FOR SUPPORT AND DIAGNOSTIC LOGGING\nAs I mentioned earlier, DomainLogger represents an out-of-process dependency—the\nlog storage. This poses a problem: User now interacts with that dependency and thus\nviolates the separation between business logic and communication with out-of-process\ndependencies. The use of DomainLogger has transitioned User to the category of\nLog data\nlogger.Info(\"User Id is {UserId}\", 12)\nMessageTemplate\nUserId\nUser Id is {UserId}\n12\nUser Id is 12\nFlat log ﬁle\n{ “MessageTemplate”: “…”,\n“UserId” : 12 }\nMessageTemplate,UserId\nUser Id is {UserId},12\nJSON ﬁle\nCSV ﬁle\nRendering\nFigure 8.12\nStructured logging decouples log data from renderings of that data. You can set up \nmultiple renderings, such as a flat log file, JSON, or CSV file.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1489,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 232,
      "content": "210\nCHAPTER 8\nWhy integration testing?\novercomplicated code, making it harder to test and maintain (refer to chapter 7 for\nmore details about code categories).\n This problem can be solved the same way we implemented the notification of\nexternal systems about changed user emails: with the help of domain events (again,\nsee chapter 7 for details). You can introduce a separate domain event to track changes\nin the user type. The controller will then convert those changes into calls to Domain-\nLogger, as shown in the following listing.\npublic void ChangeEmail(string newEmail, Company company)\n{\n_logger.Info(\n$\"Changing email for user {UserId} to {newEmail}\");\nPrecondition.Requires(CanChangeEmail() == null);\nif (Email == newEmail)\nreturn;\nUserType newType = company.IsEmailCorporate(newEmail)\n? UserType.Employee\n: UserType.Customer;\nif (Type != newType)\n{\nint delta = newType == UserType.Employee ? 1 : -1;\ncompany.ChangeNumberOfEmployees(delta);\nAddDomainEvent(\n        \nnew UserTypeChangedEvent(\n        \nUserId, Type, newType));        \n}\nEmail = newEmail;\nType = newType;\nAddDomainEvent(new EmailChangedEvent(UserId, newEmail));\n_logger.Info($\"Email is changed for user {UserId}\");\n}\nNotice that there are now two domain events: UserTypeChangedEvent and Email-\nChangedEvent. Both of them implement the same interface (IDomainEvent) and thus\ncan be stored in the same collection.\n And here is how the controller looks.\npublic string ChangeEmail(int userId, string newEmail)\n{\nobject[] userData = _database.GetUserById(userId);\nUser user = UserFactory.Create(userData);\nListing 8.6\nReplacing DomainLogger in User with a domain event\nListing 8.7\nLatest version of UserController\nUses a domain \nevent instead of \nDomainLogger\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1781,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 233,
      "content": "211\nHow to test logging functionality\nstring error = user.CanChangeEmail();\nif (error != null)\nreturn error;\nobject[] companyData = _database.GetCompany();\nCompany company = CompanyFactory.Create(companyData);\nuser.ChangeEmail(newEmail, company);\n_database.SaveCompany(company);\n_database.SaveUser(user);\n_eventDispatcher.Dispatch(user.DomainEvents);   \nreturn \"OK\";\n}\nEventDispatcher is a new class that converts domain events into calls to out-of-process\ndependencies:\n\nEmailChangedEvent translates into _messageBus.SendEmailChangedMessage().\n\nUserTypeChangedEvent translates into _domainLogger.UserTypeHasChanged().\nThe use of UserTypeChangedEvent has restored the separation between the two\nresponsibilities: domain logic and communication with out-of-process dependencies.\nTesting support logging now isn’t any different from testing the other unmanaged\ndependency, the message bus:\nUnit tests should check an instance of UserTypeChangedEvent in the User\nunder test.\nThe single integration test should use a mock to ensure the interaction with\nDomainLogger is in place.\nNote that if you need to do support logging in the controller and not one of the\ndomain classes, there’s no need to use domain events. As you may remember from\nchapter 7, controllers orchestrate the collaboration between the domain model and\nout-of-process dependencies. DomainLogger is one of such dependencies, and thus\nUserController can use that logger directly.\n Also notice that I didn’t change the way the User class does diagnostic logging.\nUser still uses the logger instance directly in the beginning and at the end of its Chan-\ngeEmail method. This is by design. Diagnostic logging is for developers only; you\ndon’t need to unit test this functionality and thus don’t have to keep it out of the\ndomain model.\n Still, refrain from the use of diagnostic logging in User or other domain classes\nwhen possible. I explain why in the next section. \nDispatches user \ndomain events\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2015,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 234,
      "content": "212\nCHAPTER 8\nWhy integration testing?\n8.6.3\nHow much logging is enough?\nAnother important question is about the optimum amount of logging. How much log-\nging is enough? Support logging is out of the question here because it’s a business\nrequirement. You do have control over diagnostic logging, though.\n It’s important not to overuse diagnostic logging, for the following two reasons:\nExcessive logging clutters the code. This is especially true for the domain model.\nThat’s why I don’t recommend using diagnostic logging in User even though\nsuch a use is fine from a unit testing perspective: it obscures the code.\nLogs’ signal-to-noise ratio is key. The more you log, the harder it is to find relevant\ninformation. Maximize the signal; minimize the noise.\nTry not to use diagnostic logging in the domain model at all. In most cases, you can\nsafely move that logging from domain classes to controllers. And even then, resort to\ndiagnostic logging only temporarily when you need to debug something. Remove it\nonce you finish debugging. Ideally, you should use diagnostic logging for unhandled\nexceptions only. \n8.6.4\nHow do you pass around logger instances?\nFinally, the last question is how to pass logger instances in the code. One way to\nresolve these instances is using static methods, as shown in the following listing.\npublic class User\n{\nprivate static readonly ILogger _logger =   \nLogManager.GetLogger(typeof(User));     \npublic void ChangeEmail(string newEmail, Company company)\n{\n_logger.Info(\n$\"Changing email for user {UserId} to {newEmail}\");\n/* ... */\n_logger.Info($\"Email is changed for user {UserId}\");\n}\n}\nSteven van Deursen and Mark Seeman, in their book Dependency Injection Principles,\nPractices, Patterns (Manning Publications, 2018), call this type of dependency acquisi-\ntion ambient context. This is an anti-pattern. Two of their arguments are that\nThe dependency is hidden and hard to change.\nTesting becomes more difficult.\nI fully agree with this analysis. To me, though, the main drawback of ambient con-\ntext is that it masks potential problems in code. If injecting a logger explicitly into a\nListing 8.8\nStoring ILogger in a static field\nResolves ILogger through a \nstatic method, and stores it \nin a private static field\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2310,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 235,
      "content": "213\nSummary\ndomain class becomes so inconvenient that you have to resort to ambient context,\nthat’s a certain sign of trouble. You either log too much or use too many layers of indi-\nrection. In any case, ambient context is not a solution. Instead, tackle the root cause\nof the problem.\n The following listing shows one way to explicitly inject the logger: as a method\nargument. Another way is through the class constructor.\npublic void ChangeEmail(\nstring newEmail,      \nCompany company,      \nILogger logger)       \n{\nlogger.Info(\n$\"Changing email for user {UserId} to {newEmail}\");\n/* ... */\nlogger.Info($\"Email is changed for user {UserId}\");\n}\n8.7\nConclusion\nView communications with all out-of-process dependencies through the lens of whether\nthis communication is part of the application’s observable behavior or an imple-\nmentation detail. The log storage isn’t any different in that regard. Mock logging\nfunctionality if the logs are observable by non-programmers; don’t test it otherwise.\nIn the next chapter, we’ll dive deeper into the topic of mocking and best practices\nrelated to it. \nSummary\nAn integration test is any test that is not a unit test. Integration tests verify how\nyour system works in integration with out-of-process dependencies:\n– Integration tests cover controllers; unit tests cover algorithms and the domain\nmodel.\n– Integration tests provide better protection against regressions and resistance\nto refactoring; unit tests have better maintainability and feedback speed.\nThe bar for integration tests is higher than for unit tests: the score they have in\nthe metrics of protection against regressions and resistance to refactoring must\nbe higher than that of a unit test to offset the worse maintainability and feed-\nback speed. The Test Pyramid represents this trade-off: the majority of tests\nshould be fast and cheap unit tests, with a smaller number of slow and more\nexpensive integration tests that check correctness of the system as a whole:\n– Check as many of the business scenario’s edge cases as possible with unit\ntests. Use integration tests to cover one happy path, as well as any edge cases\nthat can’t be covered by unit tests.\nListing 8.9\nInjecting the logger explicitly\nMethod \ninjection \nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2292,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 236,
      "content": "214\nCHAPTER 8\nWhy integration testing?\n– The shape of the Test Pyramid depends on the project’s complexity. Simple\nprojects have little code in the domain model and thus can have an equal\nnumber of unit and integration tests. In the most trivial cases, there might be\nno unit tests.\nThe Fail Fast principle advocates for making bugs manifest themselves quickly\nand is a viable alternative to integration testing.\nManaged dependencies are out-of-process dependencies that are only accessible\nthrough your application. Interactions with managed dependencies aren’t\nobservable externally. A typical example is the application database.\nUnmanaged dependencies are out-of-process dependencies that other applications\nhave access to. Interactions with unmanaged dependencies are observable exter-\nnally. Typical examples include an SMTP server and a message bus.\nCommunications with managed dependencies are implementation details; com-\nmunications with unmanaged dependencies are part of your system’s observ-\nable behavior.\nUse real instances of managed dependencies in integration tests; replace unman-\naged dependencies with mocks.\nSometimes an out-of-process dependency exhibits attributes of both managed and\nunmanaged dependencies. A typical example is a database that other applications\nhave access to. Treat the observable part of the dependency as an unmanaged\ndependency: replace that part with mocks in tests. Treat the rest of the depen-\ndency as a managed dependency: verify its final state, not interactions with it.\nAn integration test must go through all layers that work with a managed depen-\ndency. In an example with a database, this means checking the state of that\ndatabase independently of the data used as input parameters.\nInterfaces with a single implementation are not abstractions and don’t provide\nloose coupling any more than the concrete classes that implement those inter-\nfaces. Trying to anticipate future implementations for such interfaces violates\nthe YAGNI (you aren’t gonna need it) principle.\nThe only legitimate reason to use interfaces with a single implementation is to\nenable mocking. Use such interfaces only for unmanaged dependencies. Use\nconcrete classes for managed dependencies.\nInterfaces with a single implementation used for in-process dependencies are\na red flag. Such interfaces hint at using mocks to check interactions between\ndomain classes, which leads to coupling tests to the code’s implementation\ndetails.\nHave an explicit and well-known place for the domain model in your code base.\nThe explicit boundary between domain classes and controllers makes it easier\nto tell unit and integration tests apart.\nAn excessive number of layers of indirection negatively affects your ability to\nreason about the code. Have as few layers of indirections as possible. In most\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2881,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 237,
      "content": "215\nSummary\nbackend systems, you can get away with just three of them: the domain model,\nan application services layer (controllers), and an infrastructure layer.\nCircular dependencies add cognitive load when you try to understand the code.\nA typical example is a callback (when a callee notifies the caller about the result\nof its work). Break the cycle by introducing a value object; use that value object\nto return the result from the callee to the caller.\nMultiple act sections in a test are only justified when that test works with out-of-\nprocess dependencies that are hard to bring into a desirable state. You should\nnever have multiple acts in a unit test, because unit tests don’t work with out-of-\nprocess dependencies. Multistep tests almost always belong to the category of\nend-to-end tests.\nSupport logging is intended for support staff and system administrators; it’s\npart of the application’s observable behavior. Diagnostic logging helps devel-\nopers understand what’s going on inside the application: it’s an implementa-\ntion detail.\nBecause support logging is a business requirement, reflect that requirement\nexplicitly in your code base. Introduce a special DomainLogger class where you\nlist all the support logging needed for the business.\nTreat support logging like any other functionality that works with an out-of-pro-\ncess dependency. Use domain events to track changes in the domain model;\nconvert those domain events into calls to DomainLogger in controllers.\nDon’t test diagnostic logging. Unlike support logging, you can do diagnostic\nlogging directly in the domain model.\nUse diagnostic logging sporadically. Excessive diagnostic logging clutters the\ncode and damages the logs’ signal-to-noise ratio. Ideally, you should only use\ndiagnostic logging for unhandled exceptions.\nAlways inject all dependencies explicitly (including loggers), either via the con-\nstructor or as a method argument.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1980,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 238,
      "content": "216\nMocking best practices\nAs you might remember from chapter 5, a mock is a test double that helps to emu-\nlate and examine interactions between the system under test and its dependencies.\nAs you might also remember from chapter 8, mocks should only be applied to\nunmanaged dependencies (interactions with such dependencies are observable by\nexternal applications). Using mocks for anything else results in brittle tests (tests that\nlack the metric of resistance to refactoring). When it comes to mocks, adhering to\nthis one guideline will get you about two-thirds of the way to success.\n This chapter shows the remaining guidelines that will help you develop inte-\ngration tests that have the greatest possible value by maxing out mocks’ resistance\nto refactoring and protection against regressions. I’ll first show a typical use of\nmocks, describe its drawbacks, and then demonstrate how you can overcome\nthose drawbacks.\nThis chapter covers\nMaximizing the value of mocks\nReplacing mocks with spies\nMocking best practices\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1079,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 239,
      "content": "217\nMaximizing mocks’ value\n9.1\nMaximizing mocks’ value\nIt’s important to limit the use of mocks to unmanaged dependencies, but that’s only\nthe first step on the way to maximizing the value of mocks. This topic is best explained\nwith an example, so I’ll continue using the CRM system from earlier chapters as a sam-\nple project. I’ll remind you of its functionality and show the integration test we ended\nup with. After that, you’ll see how that test can be improved with regard to mocking.\n As you might recall, the CRM system currently supports only one use case: chang-\ning a user’s email. The following listing shows where we left off with the controller.\npublic class UserController\n{\nprivate readonly Database _database;\nprivate readonly EventDispatcher _eventDispatcher;\npublic UserController(\nDatabase database,\nIMessageBus messageBus,\nIDomainLogger domainLogger)\n{\n_database = database;\n_eventDispatcher = new EventDispatcher(\nmessageBus, domainLogger);\n}\npublic string ChangeEmail(int userId, string newEmail)\n{\nobject[] userData = _database.GetUserById(userId);\nUser user = UserFactory.Create(userData);\nstring error = user.CanChangeEmail();\nif (error != null)\nreturn error;\nobject[] companyData = _database.GetCompany();\nCompany company = CompanyFactory.Create(companyData);\nuser.ChangeEmail(newEmail, company);\n_database.SaveCompany(company);\n_database.SaveUser(user);\n_eventDispatcher.Dispatch(user.DomainEvents);\nreturn \"OK\";\n}\n}\nNote that there’s no longer any diagnostic logging, but support logging (the IDomain-\nLogger interface) is still in place (see chapter 8 for more details). Also, listing 9.1\nintroduces a new class: the EventDispatcher. It converts domain events generated by\nListing 9.1\nUser controller\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1782,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 240,
      "content": "218\nCHAPTER 9\nMocking best practices\nthe domain model into calls to unmanaged dependencies (something that the control-\nler previously did by itself), as shown next.\npublic class EventDispatcher\n{\nprivate readonly IMessageBus _messageBus;\nprivate readonly IDomainLogger _domainLogger;\npublic EventDispatcher(\nIMessageBus messageBus,\nIDomainLogger domainLogger)\n{\n_domainLogger = domainLogger;\n_messageBus = messageBus;\n}\npublic void Dispatch(List<IDomainEvent> events)\n{\nforeach (IDomainEvent ev in events)\n{\nDispatch(ev);\n}\n}\nprivate void Dispatch(IDomainEvent ev)\n{\nswitch (ev)\n{\ncase EmailChangedEvent emailChangedEvent:\n_messageBus.SendEmailChangedMessage(\nemailChangedEvent.UserId,\nemailChangedEvent.NewEmail);\nbreak;\ncase UserTypeChangedEvent userTypeChangedEvent:\n_domainLogger.UserTypeHasChanged(\nuserTypeChangedEvent.UserId,\nuserTypeChangedEvent.OldType,\nuserTypeChangedEvent.NewType);\nbreak;\n}\n}\n}\nFinally, the following listing shows the integration test. This test goes through all out-\nof-process dependencies (both managed and unmanaged).\n[Fact]\npublic void Changing_email_from_corporate_to_non_corporate()\n{\nListing 9.2\nEvent dispatcher\nListing 9.3\nIntegration test\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1232,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 241,
      "content": "219\nMaximizing mocks’ value\n// Arrange\nvar db = new Database(ConnectionString);\nUser user = CreateUser(\"user@mycorp.com\", UserType.Employee, db);\nCreateCompany(\"mycorp.com\", 1, db);\nvar messageBusMock = new Mock<IMessageBus>();   \nvar loggerMock = new Mock<IDomainLogger>();     \nvar sut = new UserController(\ndb, messageBusMock.Object, loggerMock.Object);\n// Act\nstring result = sut.ChangeEmail(user.UserId, \"new@gmail.com\");\n// Assert\nAssert.Equal(\"OK\", result);\nobject[] userData = db.GetUserById(user.UserId);\nUser userFromDb = UserFactory.Create(userData);\nAssert.Equal(\"new@gmail.com\", userFromDb.Email);\nAssert.Equal(UserType.Customer, userFromDb.Type);\nobject[] companyData = db.GetCompany();\nCompany companyFromDb = CompanyFactory.Create(companyData);\nAssert.Equal(0, companyFromDb.NumberOfEmployees);\nmessageBusMock.Verify(\n  \nx => x.SendEmailChangedMessage(\n  \nuser.UserId, \"new@gmail.com\"),  \nTimes.Once);\n  \nloggerMock.Verify(\n  \nx => x.UserTypeHasChanged(\n  \nuser.UserId,\n  \nUserType.Employee,\n  \nUserType.Customer),\n  \nTimes.Once);\n  \n}\nThis test mocks out two unmanaged dependencies: IMessageBus and IDomainLogger.\nI’ll focus on IMessageBus first. We’ll discuss IDomainLogger later in this chapter.\n9.1.1\nVerifying interactions at the system edges\nLet’s discuss why the mocks used by the integration test in listing 9.3 aren’t ideal in\nterms of their protection against regressions and resistance to refactoring and how we\ncan fix that.\nTIP\nWhen mocking, always adhere to the following guideline: verify interac-\ntions with unmanaged dependencies at the very edges of your system.\nThe problem with messageBusMock in listing 9.3 is that the IMessageBus interface\ndoesn’t reside at the system’s edge. Look at that interface’s implementation.\n \nSets up the \nmocks\nVerifies the \ninteractions \nwith the mocks\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1871,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 242,
      "content": "220\nCHAPTER 9\nMocking best practices\npublic interface IMessageBus\n{\nvoid SendEmailChangedMessage(int userId, string newEmail);\n}\npublic class MessageBus : IMessageBus\n{\nprivate readonly IBus _bus;\npublic void SendEmailChangedMessage(\nint userId, string newEmail)\n{\n_bus.Send(\"Type: USER EMAIL CHANGED; \" +\n$\"Id: {userId}; \" +\n$\"NewEmail: {newEmail}\");\n}\n}\npublic interface IBus\n{\nvoid Send(string message);\n}\nBoth the IMessageBus and IBus interfaces (and the classes implementing them) belong\nto our project’s code base. IBus is a wrapper on top of the message bus SDK library (pro-\nvided by the company that develops that message bus). This wrapper encapsulates non-\nessential technical details, such as connection credentials, and exposes a nice, clean\ninterface for sending arbitrary text messages to the bus. IMessageBus is a wrapper on\ntop of IBus; it defines messages specific to your domain. IMessageBus helps you keep all\nsuch messages in one place and reuse them across the application.\n It’s possible to merge the IBus and IMessageBus interfaces together, but that\nwould be a suboptimal solution. These two responsibilities—hiding the external\nlibrary’s complexity and holding all application messages in one place—are best kept\nseparated. This is the same situation as with ILogger and IDomainLogger, which you\nsaw in chapter 8. IDomainLogger implements specific logging functionality required\nby the business, and it does that by using the generic ILogger behind the scenes.\n Figure 9.1 shows where IBus and IMessageBus stand from a hexagonal architec-\nture perspective: IBus is the last link in the chain of types between the controller and\nthe message bus, while IMessageBus is only an intermediate step on the way.\n Mocking IBus instead of IMessageBus maximizes the mock’s protection against\nregressions. As you might remember from chapter 4, protection against regressions is\na function of the amount of code that is executed during the test. Mocking the very\nlast type that communicates with the unmanaged dependency increases the number\nof classes the integration test goes through and thus improves the protection. This\nguideline is also the reason you don’t want to mock EventDispatcher. It resides even\nfurther away from the edge of the system, compared to IMessageBus.\nListing 9.4\nMessage bus \nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2367,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 243,
      "content": "221\nMaximizing mocks’ value\nHere’s the integration test after retargeting it from IMessageBus to IBus. I’m omitting\nthe parts that didn’t change from listing 9.3.\n[Fact]\npublic void Changing_email_from_corporate_to_non_corporate()\n{\nvar busMock = new Mock<IBus>();\nvar messageBus = new MessageBus(busMock.Object);     \nvar loggerMock = new Mock<IDomainLogger>();\nvar sut = new UserController(db, messageBus, loggerMock.Object);\n/* ... */\nbusMock.Verify(\nx => x.Send(\n\"Type: USER EMAIL CHANGED; \" +  \n$\"Id: {user.UserId}; \" +\n  \n\"NewEmail: new@gmail.com\"),\n  \nTimes.Once);\n}\nListing 9.5\nIntegration test targeting IBus\nExternal client\nMessage bus\nIMessageBus\nController\nDomain model\nIBus\nFigure 9.1\nIBus resides at the system’s edge; IMessageBus is only an intermediate \nlink in the chain of types between the controller and the message bus. Mocking IBus \ninstead of IMessageBus achieves the best protection against regressions.\nUses a concrete \nclass instead of \nthe interface\nVerifies the actual \nmessage sent to \nthe bus\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1074,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 244,
      "content": "222\nCHAPTER 9\nMocking best practices\nNotice how the test now uses the concrete MessageBus class and not the correspond-\ning IMessageBus interface. IMessageBus is an interface with a single implementation,\nand, as you’ll remember from chapter 8, mocking is the only legitimate reason to have\nsuch interfaces. Because we no longer mock IMessageBus, this interface can be\ndeleted and its usages replaced with MessageBus.\n Also notice how the test in listing 9.5 checks the text message sent to the bus. Com-\npare it to the previous version:\nmessageBusMock.Verify(\nx => x.SendEmailChangedMessage(user.UserId, \"new@gmail.com\"),\nTimes.Once);\nThere’s a huge difference between verifying a call to a custom class that you wrote and\nthe actual text sent to external systems. External systems expect text messages from your\napplication, not calls to classes like MessageBus. In fact, text messages are the only side\neffect observable externally; classes that participate in producing those messages are\nmere implementation details. Thus, in addition to the increased protection against\nregressions, verifying interactions at the very edges of your system also improves resis-\ntance to refactoring. The resulting tests are less exposed to potential false positives; no\nmatter what refactorings take place, such tests won’t turn red as long as the message’s\nstructure is preserved.\n The same mechanism is at play here as the one that gives integration and end-to-end\ntests additional resistance to refactoring compared to unit tests. They are more detached\nfrom the code base and, therefore, aren’t affected as much during low-level refactorings.\nTIP\nA call to an unmanaged dependency goes through several stages before\nit leaves your application. Pick the last such stage. It is the best way to ensure\nbackward compatibility with external systems, which is the goal that mocks\nhelp you achieve. \n9.1.2\nReplacing mocks with spies\nAs you may remember from chapter 5, a spy is a variation of a test double that serves\nthe same purpose as a mock. The only difference is that spies are written manually,\nwhereas mocks are created with the help of a mocking framework. Indeed, spies are\noften called handwritten mocks.\n It turns out that, when it comes to classes residing at the system edges, spies are supe-\nrior to mocks. Spies help you reuse code in the assertion phase, thereby reducing the\ntest’s size and improving readability. The next listing shows an example of a spy that\nworks on top of IBus.\npublic interface IBus\n{\nvoid Send(string message);\n}\nListing 9.6\nA spy (also known as a handwritten mock)\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2645,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 245,
      "content": "223\nMaximizing mocks’ value\npublic class BusSpy : IBus\n{\nprivate List<string> _sentMessages =    \nnew List<string>();\n    \npublic void Send(string message)\n{\n_sentMessages.Add(message);\n    \n}\npublic BusSpy ShouldSendNumberOfMessages(int number)\n{\nAssert.Equal(number, _sentMessages.Count);\nreturn this;\n}\npublic BusSpy WithEmailChangedMessage(int userId, string newEmail)\n{\nstring message = \"Type: USER EMAIL CHANGED; \" +\n$\"Id: {userId}; \" +\n$\"NewEmail: {newEmail}\";\nAssert.Contains(\n   \n_sentMessages, x => x == message);   \nreturn this;\n}\n}\nThe following listing is a new version of the integration test. Again, I’m showing only\nthe relevant parts.\n[Fact]\npublic void Changing_email_from_corporate_to_non_corporate()\n{\nvar busSpy = new BusSpy();\nvar messageBus = new MessageBus(busSpy);\nvar loggerMock = new Mock<IDomainLogger>();\nvar sut = new UserController(db, messageBus, loggerMock.Object);\n/* ... */\nbusSpy.ShouldSendNumberOfMessages(1)\n.WithEmailChangedMessage(user.UserId, \"new@gmail.com\");\n}\nVerifying the interactions with the message bus is now succinct and expressive, thanks\nto the fluent interface that BusSpy provides. With that fluent interface, you can chain\ntogether several assertions, thus forming cohesive, almost plain-English sentences.\nTIP\nYou can rename BusSpy into BusMock. As I mentioned earlier, the differ-\nence between a mock and a spy is an implementation detail. Most programmers\nListing 9.7\nUsing the spy from listing 6.43\nStores all sent \nmessages \nlocally\nAsserts that the \nmessage has been sent\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1585,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 246,
      "content": "224\nCHAPTER 9\nMocking best practices\naren’t familiar with the term spy, though, so renaming the spy as BusMock can\nsave your colleagues unnecessary confusion.\nThere’s a reasonable question to be asked here: didn’t we just make a full circle and\ncome back to where we started? The version of the test in listing 9.7 looks a lot like the\nearlier version that mocked IMessageBus:\nmessageBusMock.Verify(\nx => x.SendEmailChangedMessage(\n  \nuser.UserId, \"new@gmail.com\"),  \nTimes.Once);      \nThese assertions are similar because both BusSpy and MessageBus are wrappers on\ntop of IBus. But there’s a crucial difference between the two: BusSpy is part of the test\ncode, whereas MessageBus belongs to the production code. This difference is import-\nant because you shouldn’t rely on the production code when making assertions in tests.\n Think of your tests as auditors. A good auditor wouldn’t just take the auditee’s\nwords at face value; they would double-check everything. The same is true with the\nspy: it provides an independent checkpoint that raises an alarm when the message\nstructure is changed. On the other hand, a mock on IMessageBus puts too much trust\nin the production code. \n9.1.3\nWhat about IDomainLogger?\nThe mock that previously verified interactions with IMessageBus is now targeted at\nIBus, which resides at the system’s edge. Here are the current mock assertions in the\nintegration test.\nbusSpy.ShouldSendNumberOfMessages(1)\n  \n.WithEmailChangedMessage(\n  \nuser.UserId, \"new@gmail.com\");  \nloggerMock.Verify(\n    \nx => x.UserTypeHasChanged(\n  \nuser.UserId,\n  \nUserType.Employee,\n  \nUserType.Customer),\n  \nTimes.Once);\n  \nNote that just as MessageBus is a wrapper on top of IBus, DomainLogger is a wrapper\non top of ILogger (see chapter 8 for more details). Shouldn’t the test be retargeted at\nILogger, too, because this interface also resides at the application boundary?\n In most projects, such retargeting isn’t necessary. While the logger and the mes-\nsage bus are unmanaged dependencies and, therefore, both require maintaining\nbackward compatibility, the accuracy of that compatibility doesn’t have to be the\nsame. With the message bus, it’s important not to allow any changes to the structure of\nListing 9.8\nMock assertions\nSame as WithEmailChanged-\nMessage(user.UserId, \n\"new@gmail.com\")\nSame as \nShouldSendNumberOfMessages(1)\nChecks \ninteractions \nwith IBus\nChecks \ninteractions with \nIDomainLogger\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2471,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 247,
      "content": "225\nMocking best practices\nthe messages, because you never know how external systems will react to such\nchanges. But the exact structure of text logs is not that important for the intended\naudience (support staff and system administrators). What’s important is the existence\nof those logs and the information they carry. Thus, mocking IDomainLogger alone\nprovides the necessary level of protection. \n9.2\nMocking best practices\nYou’ve learned two major mocking best practices so far:\nApplying mocks to unmanaged dependencies only\nVerifying the interactions with those dependencies at the very edges of your\nsystem\nIn this section, I explain the remaining best practices:\nUsing mocks in integration tests only, not in unit tests\nAlways verifying the number of calls made to the mock\nMocking only types that you own\n9.2.1\nMocks are for integration tests only\nThe guideline saying that mocks are for integration tests only, and that you shouldn’t\nuse mocks in unit tests, stems from the foundational principle described in chapter 7:\nthe separation of business logic and orchestration. Your code should either communi-\ncate with out-of-process dependencies or be complex, but never both. This principle\nnaturally leads to the formation of two distinct layers: the domain model (that handles\ncomplexity) and controllers (that handle the communication).\n Tests on the domain model fall into the category of unit tests; tests covering con-\ntrollers are integration tests. Because mocks are for unmanaged dependencies only,\nand because controllers are the only code working with such dependencies, you\nshould only apply mocking when testing controllers—in integration tests. \n9.2.2\nNot just one mock per test\nYou might sometimes hear the guideline of having only one mock per test. According\nto this guideline, if you have more than one mock, you are likely testing several things\nat a time.\n This is a misconception that follows from a more foundational misunderstanding\ncovered in chapter 2: that a unit in a unit test refers to a unit of code, and all such units\nmust be tested in isolation from each other. On the contrary: the term unit means\na unit of behavior, not a unit of code. The amount of code it takes to implement such a\nunit of behavior is irrelevant. It could span across multiple classes, a single class, or\ntake up just a tiny method.\n With mocks, the same principle is at play: it’s irrelevant how many mocks it takes to ver-\nify a unit of behavior. Earlier in this chapter, it took us two mocks to check the scenario\nof changing the user email from corporate to non-corporate: one for the logger and\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2669,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 248,
      "content": "226\nCHAPTER 9\nMocking best practices\nthe other for the message bus. That number could have been larger. In fact, you don’t\nhave control over how many mocks to use in an integration test. The number of\nmocks depends solely on the number of unmanaged dependencies participating in\nthe operation. \n9.2.3\nVerifying the number of calls\nWhen it comes to communications with unmanaged dependencies, it’s important to\nensure both of the following:\nThe existence of expected calls\nThe absence of unexpected calls\nThis requirement, once again, stems from the need to maintain backward compatibil-\nity with unmanaged dependencies. The compatibility must go both ways: your appli-\ncation shouldn’t omit messages that external systems expect, and it also shouldn’t\nproduce unexpected messages. It’s not enough to check that the system under test\nsends a message like this:\nmessageBusMock.Verify(\nx => x.SendEmailChangedMessage(user.UserId, \"new@gmail.com\"));\nYou also need to ensure that this message is sent exactly once:\nmessageBusMock.Verify(\nx => x.SendEmailChangedMessage(user.UserId, \"new@gmail.com\"),\nTimes.Once);      \nWith most mocking libraries, you can also explicitly verify that no other calls are\nmade on the mock. In Moq (the mocking library of my choice), this verification\nlooks as follows:\nmessageBusMock.Verify(\nx => x.SendEmailChangedMessage(user.UserId, \"new@gmail.com\"),\nTimes.Once);\nmessageBusMock.VerifyNoOtherCalls();     \nBusSpy implements this functionality, too:\nbusSpy\n.ShouldSendNumberOfMessages(1)\n.WithEmailChangedMessage(user.UserId, \"new@gmail.com\");\nThe spy’s check ShouldSendNumberOfMessages(1) encompasses both Times.Once and\nVerifyNoOtherCalls() verifications from the mock. \nEnsures that the method \nis called only once\nThe additional \ncheck\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1821,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 249,
      "content": "227\nSummary\n9.2.4\nOnly mock types that you own\nThe last guideline I’d like to talk about is mocking only types that you own. It was first\nintroduced by Steve Freeman and Nat Pryce.1 The guideline states that you should\nalways write your own adapters on top of third-party libraries and mock those adapters\ninstead of the underlying types. A few of their arguments are as follows:\nYou often don’t have a deep understanding of how the third-party code works.\nEven if that code already provides built-in interfaces, it’s risky to mock those\ninterfaces, because you have to be sure the behavior you mock matches what\nthe external library actually does.\nAdapters abstract non-essential technical details of the third-party code and\ndefine the relationship with the library in your application’s terms.\nI fully agree with this analysis. Adapters, in effect, act as an anti-corruption layer\nbetween your code and the external world.2 These help you to\nAbstract the underlying library’s complexity\nOnly expose features you need from the library\nDo that using your project’s domain language\nThe IBus interface in our sample CRM project serves exactly that purpose. Even if the\nunderlying message bus’s library provides as nice and clean an interface as IBus, you\nare still better off introducing your own wrapper on top of it. You never know how the\nthird-party code will change when you upgrade the library. Such an upgrade could\ncause a ripple effect across the whole code base! The additional abstraction layer\nrestricts that ripple effect to just one class: the adapter itself.\n Note that the “mock your own types” guideline doesn’t apply to in-process depen-\ndencies. As I explained previously, mocks are for unmanaged dependencies only.\nThus, there’s no need to abstract in-memory or managed dependencies. For instance,\nif a library provides a date and time API, you can use that API as-is, because it doesn’t\nreach out to unmanaged dependencies. Similarly, there’s no need to abstract an ORM\nas long as it’s used for accessing a database that isn’t visible to external applications.\nOf course, you can introduce your own wrapper on top of any library, but it’s rarely\nworth the effort for anything other than unmanaged dependencies. \nSummary\nVerify interactions with an unmanaged dependency at the very edges of your\nsystem. Mock the last type in the chain of types between the controller and the\nunmanaged dependency. This helps you increase both protection against\nregressions (due to more code being validated by the integration test) and\n1 See page 69 in Growing Object-Oriented Software, Guided by Tests by Steve Freeman and Nat Pryce (Addison-Wesley\nProfessional, 2009).\n2 See Domain-Driven Design: Tackling Complexity in the Heart of Software by Eric Evans (Addison-Wesley, 2003).\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2844,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 250,
      "content": "228\nCHAPTER 9\nMocking best practices\nresistance to refactoring (due to detaching the mock from the code’s imple-\nmentation details).\nSpies are handwritten mocks. When it comes to classes residing at the system’s\nedges, spies are superior to mocks. They help you reuse code in the assertion\nphase, thereby reducing the test’s size and improving readability.\nDon’t rely on production code when making assertions. Use a separate set of lit-\nerals and constants in tests. Duplicate those literals and constants from the pro-\nduction code if necessary. Tests should provide a checkpoint independent of\nthe production code. Otherwise, you risk producing tautology tests (tests that\ndon’t verify anything and contain semantically meaningless assertions).\nNot all unmanaged dependencies require the same level of backward compati-\nbility. If the exact structure of the message isn’t important, and you only want to\nverify the existence of that message and the information it carries, you can\nignore the guideline of verifying interactions with unmanaged dependencies at\nthe very edges of your system. The typical example is logging.\nBecause mocks are for unmanaged dependencies only, and because controllers\nare the only code working with such dependencies, you should only apply mock-\ning when testing controllers—in integration tests. Don’t use mocks in unit tests.\nThe number of mocks used in a test is irrelevant. That number depends solely\non the number of unmanaged dependencies participating in the operation.\nEnsure both the existence of expected calls and the absence of unexpected calls\nto mocks.\nOnly mock types that you own. Write your own adapters on top of third-party\nlibraries that provide access to unmanaged dependencies. Mock those adapters\ninstead of the underlying types.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1843,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 251,
      "content": "229\nTesting the database\nThe last piece of the puzzle in integration testing is managed out-of-process depen-\ndencies. The most common example of a managed dependency is an application\ndatabase—a database no other application has access to.\n Running tests against a real database provides bulletproof protection against\nregressions, but those tests aren’t easy to set up. This chapter shows the preliminary\nsteps you need to take before you can start testing your database: it covers keeping\ntrack of the database schema, explains the difference between the state-based and\nmigration-based database delivery approaches, and demonstrates why you should\nchoose the latter over the former.\n After learning the basics, you’ll see how to manage transactions during the test,\nclean up leftover data, and keep tests small by eliminating insignificant parts and\namplifying the essentials. This chapter focuses on relational databases, but many of\nThis chapter covers\nPrerequisites for testing the database\nDatabase testing best practices\nTest data life cycle\nManaging database transactions in tests\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1146,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 252,
      "content": "230\nCHAPTER 10\nTesting the database\nthe same principles are applicable to other types of data stores such as document-ori-\nented databases or even plain text file storages.\n10.1\nPrerequisites for testing the database\nAs you might recall from chapter 8, managed dependencies should be included as-is\nin integration tests. That makes working with those dependencies more laborious\nthan unmanaged ones because using a mock is out of the question. But even before\nyou start writing tests, you must take preparatory steps to enable integration testing. In\nthis section, you’ll see these prerequisites:\nKeeping the database in the source control system\nUsing a separate database instance for every developer\nApplying the migration-based approach to database delivery\nLike almost everything in testing, though, practices that facilitate testing also improve\nthe health of your database in general. You’ll get value out of those practices even if\nyou don’t write integration tests.\n10.1.1 Keeping the database in the source control system\nThe first step on the way to testing the database is treating the database schema as reg-\nular code. Just as with regular code, a database schema is best stored in a source con-\ntrol system such as Git.\n I’ve worked on projects where programmers maintained a dedicated database\ninstance, which served as a reference point (a model database). During development,\nall schema changes accumulated in that instance. Upon production deployments, the\nteam compared the production and model databases, used a special tool to generate\nupgrade scripts, and ran those scripts in production (figure 10.1).\nModel\ndatabase\nProduction\ndatabase\nCompare\nModiﬁcations by\nprogrammers\nUpgrade\nscripts\nGenerate\nApply\nComparison\ntool\nFigure 10.1\nHaving a dedicated instance as a model database is an anti-pattern. The database \nschema is best stored in a source control system.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1941,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 253,
      "content": "231\nPrerequisites for testing the database\nUsing a model database is a horrible way to maintain database schema. That’s because\nthere’s\nNo change history—You can’t trace the database schema back to some point in\nthe past, which might be important when reproducing bugs in production.\nNo single source of truth—The model database becomes a competing source of\ntruth about the state of development. Maintaining two such sources (Git and\nthe model database) creates an additional burden.\nOn the other hand, keeping all the database schema updates in the source control sys-\ntem helps you to maintain a single source of truth and also to track database changes\nalong with the changes of regular code. No modifications to the database structure\nshould be made outside of the source control. \n10.1.2 Reference data is part of the database schema\nWhen it comes to the database schema, the usual suspects are tables, views, indexes,\nstored procedures, and anything else that forms a blueprint of how the database is\nconstructed. The schema itself is represented in the form of SQL scripts. You\nshould be able to use those scripts to create a fully functional, up-to-date database\ninstance of your own at any time during development. However, there’s another\npart of the database that belongs to the database schema but is rarely viewed as\nsuch: reference data.\nDEFINITION\nReference data is data that must be prepopulated in order for the\napplication to operate properly.\nTake the CRM system from the earlier chapters, for example. Its users can be either of\ntype Customer or type Employee. Let’s say that you want to create a table with all user\ntypes and introduce a foreign key constraint from User to that table. Such a constraint\nwould provide an additional guarantee that the application won’t ever assign a user a\nnonexistent type. In this scenario, the content of the UserType table would be refer-\nence data because the application relies on its existence in order to persist users in the\ndatabase.\nTIP\nThere’s a simple way to differentiate reference data from regular data.\nIf your application can modify the data, it’s regular data; if not, it’s refer-\nence data.\nBecause reference data is essential for your application, you should keep it in the\nsource control system along with tables, views, and other parts of the database schema,\nin the form of SQL INSERT statements.\n Note that although reference data is normally stored separately from regular data,\nthe two can sometimes coexist in the same table. To make this work, you need to intro-\nduce a flag differentiating data that can be modified (regular data) from data that can’t\nbe modified (reference data) and forbid your application from changing the latter. \nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2774,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 254,
      "content": "232\nCHAPTER 10\nTesting the database\n10.1.3 Separate instance for every developer\nIt’s difficult enough to run tests against a real database. It becomes even more difficult\nif you have to share that database with other developers. The use of a shared database\nhinders the development process because\nTests run by different developers interfere with each other.\nNon-backward-compatible changes can block the work of other developers.\nKeep a separate database instance for every developer, preferably on that developer’s\nown machine in order to maximize test execution speed. \n10.1.4 State-based vs. migration-based database delivery\nThere are two major approaches to database delivery: state-based and migration-based.\nThe migration-based approach is more difficult to implement and maintain initially,\nbut it works much better than the state-based approach in the long run.\nTHE STATE-BASED APPROACH\nThe state-based approach to database delivery is similar to what I described in figure\n10.1. You also have a model database that you maintain throughout development.\nDuring deployments, a comparison tool generates scripts for the production database\nto bring it up to date with the model database. The difference is that with the state-\nbased approach, you don’t actually have a physical model database as a source of\ntruth. Instead, you have SQL scripts that you can use to create that database. The\nscripts are stored in the source control.\n In the state-based approach, the comparison tool does all the hard lifting. What-\never the state of the production database, the tool does everything needed to get it in\nsync with the model database: delete unnecessary tables, create new ones, rename col-\numns, and so on. \nTHE MIGRATION-BASED APPROACH\nOn the other hand, the migration-based approach emphasizes the use of explicit\nmigrations that transition the database from one version to another (figure 10.2).\nWith this approach, you don’t use tools to automatically synchronize the production\nand development databases; you come up with upgrade scripts yourself. However, a\ndatabase comparison tool can still be useful when detecting undocumented changes\nin the production database schema.\nCREATE TABLE\ndbo.Customer (…)\nALTER TABLE\ndbo.Customer (…)\nCREATE TABLE\ndbo.User (…)\nMigration 1\nMigration 2\nMigration 3\nFigure 10.2\nThe migration-based approach to database delivery emphasizes the use of explicit \nmigrations that transition the database from one version to another.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2525,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 255,
      "content": "233\nPrerequisites for testing the database\nIn the migration-based approach, migrations and not the database state become the\nartifacts you store in the source control. Migrations are usually represented with\nplain SQL scripts (popular tools include Flyway [https://flywaydb.org] and Liquibase\n[https://liquibase.org]), but they can also be written using a DSL-like language that\ngets translated into SQL. The following example shows a C# class that represents a\ndatabase migration with the help of the FluentMigrator library (https://github.com/\nfluentmigrator/fluentmigrator):\n[Migration(1)]\n          \npublic class CreateUserTable : Migration\n{\npublic override void Up()       \n{\nCreate.Table(\"Users\");\n}\npublic override void Down()    \n{\nDelete.Table(\"Users\");\n}\n}\nPREFER THE MIGRATION-BASED APPROACH OVER THE STATE-BASED ONE\nThe difference between the state-based and migration-based approaches to database\ndelivery comes down to (as their names imply) state versus migrations (see figure 10.3):\nThe state-based approach makes the state explicit (by virtue of storing that\nstate in the source control) and lets the comparison tool implicitly control the\nmigrations.\nThe migration-based approach makes the migrations explicit but leaves the state\nimplicit. It’s impossible to view the database state directly; you have to assemble\nit from the migrations.\nMigration \nnumber\nForward \nmigration\nBackward migration (helpful \nwhen downgrading to an \nearlier database version to \nreproduce a bug)\nState-based\napproach\nState of the database\nMigration mechanism\nMigration-based\napproach\nImplicit\nImplicit\nExplicit\nExplicit\nFigure 10.3\nThe state-based approach makes the state explicit and \nmigrations implicit; the migration-based approach makes the opposite choice.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1815,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 256,
      "content": "234\nCHAPTER 10\nTesting the database\nSuch a distinction leads to different sets of trade-offs. The explicitness of the database\nstate makes it easier to handle merge conflicts, while explicit migrations help to tackle\ndata motion.\nDEFINITION\nData motion is the process of changing the shape of existing data\nso that it conforms to the new database schema.\nAlthough the alleviation of merge conflicts and the ease of data motion might look\nlike equally important benefits, in the vast majority of projects, data motion is much more\nimportant than merge conflicts. Unless you haven’t yet released your application to pro-\nduction, you always have data that you can’t simply discard.\n For example, when splitting a Name column into FirstName and LastName, you not\nonly have to drop the Name column and create the new FirstName and LastName col-\numns, but you also have to write a script to split all existing names into two pieces.\nThere is no easy way to implement this change using the state-driven approach; com-\nparison tools are awful when it comes to managing data. The reason is that while the\ndatabase schema itself is objective, meaning there is only one way to interpret it, data\nis context-dependent. No tool can make reliable assumptions about data when gener-\nating upgrade scripts. You have to apply domain-specific rules in order to implement\nproper transformations.\n As a result, the state-based approach is impractical in the vast majority of projects.\nYou can use it temporarily, though, while the project still has not been released to pro-\nduction. After all, test data isn’t that important, and you can re-create it every time you\nchange the database. But once you release the first version, you will have to switch to\nthe migration-based approach in order to handle data motion properly.\nTIP\nApply every modification to the database schema (including reference\ndata) through migrations. Don’t modify migrations once they are committed\nto the source control. If a migration is incorrect, create a new migration\ninstead of fixing the old one. Make exceptions to this rule only when the\nincorrect migration can lead to data loss. \n10.2\nDatabase transaction management\nDatabase transaction management is a topic that’s important for both production and\ntest code. Proper transaction management in production code helps you avoid data\ninconsistencies. In tests, it helps you verify integration with the database in a close-to-\nproduction setting.\n In this section, I’ll first show how to handle transactions in the production code\n(the controller) and then demonstrate how to use them in integration tests. I’ll con-\ntinue using the same CRM project you saw in the earlier chapters as an example.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2761,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 257,
      "content": "235\nDatabase transaction management\n10.2.1 Managing database transactions in production code\nOur sample CRM project uses the Database class to work with User and Company.\nDatabase creates a separate SQL connection on each method call. Every such connec-\ntion implicitly opens an independent transaction behind the scenes, as the following\nlisting shows.\npublic class Database\n{\nprivate readonly string _connectionString;\npublic Database(string connectionString)\n{\n_connectionString = connectionString;\n}\npublic void SaveUser(User user)\n{\nbool isNewUser = user.UserId == 0;\nusing (var connection =\nnew SqlConnection(_connectionString))      \n{\n/* Insert or update the user depending on isNewUser */\n}\n}\npublic void SaveCompany(Company company)\n{\nusing (var connection =\nnew SqlConnection(_connectionString))      \n{\n/* Update only; there's only one company */\n}\n}\n}\nAs a result, the user controller creates a total of four database transactions during a\nsingle business operation, as shown in the following listing.\npublic string ChangeEmail(int userId, string newEmail)\n{\nobject[] userData = _database.GetUserById(userId);    \nUser user = UserFactory.Create(userData);\nstring error = user.CanChangeEmail();\nif (error != null)\nreturn error;\nListing 10.1\nClass that enables access to the database\nListing 10.2\nUser controller\nOpens a\ndatabase\ntransaction\nOpens a new \ndatabase \ntransaction\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1439,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 258,
      "content": "236\nCHAPTER 10\nTesting the database\nobject[] companyData = _database.GetCompany();        \nCompany company = CompanyFactory.Create(companyData);\nuser.ChangeEmail(newEmail, company);\n_database.SaveCompany(company);                       \n_database.SaveUser(user);                             \n_eventDispatcher.Dispatch(user.DomainEvents);\nreturn \"OK\";\n}\nIt’s fine to open multiple transactions during read-only operations: for example, when\nreturning user information to the external client. But if the business operation\ninvolves data mutation, all updates taking place during that operation should be\natomic in order to avoid inconsistencies. For example, the controller can successfully\npersist the company but then fail when saving the user due to a database connectivity\nissue. As a result, the company’s NumberOfEmployees can become inconsistent with\nthe total number of Employee users in the database.\nDEFINITION\nAtomic updates are executed in an all-or-nothing manner. Each\nupdate in the set of atomic updates must either be complete in its entirety or\nhave no effect whatsoever.\nSEPARATING DATABASE CONNECTIONS FROM DATABASE TRANSACTIONS\nTo avoid potential inconsistencies, you need to introduce a separation between two\ntypes of decisions:\nWhat data to update\nWhether to keep the updates or roll them back\nSuch a separation is important because the controller can’t make these decisions\nsimultaneously. It only knows whether the updates can be kept when all the steps in\nthe business operation have succeeded. And it can only take those steps by accessing\nthe database and trying to make the updates. You can implement the separation\nbetween these responsibilities by splitting the Database class into repositories and a\ntransaction:\nRepositories are classes that enable access to and modification of the data in the\ndatabase. There will be two repositories in our sample project: one for User and\nthe other for Company.\nA transaction is a class that either commits or rolls back data updates in full. This\nwill be a custom class relying on the underlying database’s transactions to pro-\nvide atomicity of data modification.\nNot only do repositories and transactions have different responsibilities, but they also\nhave different lifespans. A transaction lives during the whole business operation and is\ndisposed of at the very end of it. A repository, on the other hand, is short-lived. You\nOpens a new \ndatabase \ntransaction\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2490,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 259,
      "content": "237\nDatabase transaction management\ncan dispose of a repository as soon as the call to the database is completed. As a result,\nrepositories always work on top of the current transaction. When connecting to the\ndatabase, a repository enlists itself into the transaction so that any data modifications\nmade during that connection can later be rolled back by the transaction.\n Figure 10.4 shows how the communication between the controller and the data-\nbase looks in listing 10.2. Each database call is wrapped into its own transaction;\nupdates are not atomic.\nFigure 10.5 shows the application after the introduction of explicit transactions. The\ntransaction mediates interactions between the controller and the database. All four\ndatabase calls are still there, but now data modifications are either committed or\nrolled back in full.\nThe following listing shows the controller after introducing a transaction and repositories.\npublic class UserController\n{\nprivate readonly Transaction _transaction;\nprivate readonly UserRepository _userRepository;\nListing 10.3\nUser controller, repositories, and a transaction\nDatabase\nGetUserById\nController\nSaveCompany\nGetCompany\nSaveUser\nFigure 10.4\nWrapping each \ndatabase call into a separate \ntransaction introduces a risk of \ninconsistencies due to hardware or \nsoftware failures. For example, the \napplication can update the number of \nemployees in the company but not \nthe employees themselves.\nTransaction\nDatabase\nController\nCommit tran\nCommit tran\nSaveUser\nSaveUser\nSaveCompany\nSaveCompany\nGetCompany\nGetCompany\nGetUserById\nGetUserById\nOpen tran\nOpen tran\nFigure 10.5\nThe transaction mediates interactions between the controller and the database and \nthus enables atomic data modification.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1787,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 260,
      "content": "238\nCHAPTER 10\nTesting the database\nprivate readonly CompanyRepository _companyRepository;\nprivate readonly EventDispatcher _eventDispatcher;\npublic UserController(\nTransaction transaction,     \nMessageBus messageBus,\nIDomainLogger domainLogger)\n{\n_transaction = transaction;\n_userRepository = new UserRepository(transaction);\n_companyRepository = new CompanyRepository(transaction);\n_eventDispatcher = new EventDispatcher(\nmessageBus, domainLogger);\n}\npublic string ChangeEmail(int userId, string newEmail)\n{\nobject[] userData = _userRepository           \n.GetUserById(userId);\n           \nUser user = UserFactory.Create(userData);\nstring error = user.CanChangeEmail();\nif (error != null)\nreturn error;\nobject[] companyData = _companyRepository     \n.GetCompany();\n      \nCompany company = CompanyFactory.Create(companyData);\nuser.ChangeEmail(newEmail, company);\n_companyRepository.SaveCompany(company);      \n_userRepository.SaveUser(user);\n      \n_eventDispatcher.Dispatch(user.DomainEvents);\n_transaction.Commit();     \nreturn \"OK\";\n}\n}\npublic class UserRepository\n{\nprivate readonly Transaction _transaction;\npublic UserRepository(Transaction transaction)    \n{\n_transaction = transaction;\n}\n/* ... */\n}\npublic class Transaction : IDisposable\n{\nAccepts a \ntransaction\nUses the\nrepositories\ninstead\nof the\nDatabase\nclass\nCommits the \ntransaction \non success\nInjects a \ntransaction into \na repository\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1455,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 261,
      "content": "239\nDatabase transaction management\npublic void Commit() { /* ... */ }\npublic void Dispose() { /* ... */ }\n}\nThe internals of the Transaction class aren’t important, but if you’re curious, I’m\nusing .NET’s standard TransactionScope behind the scenes. The important part\nabout Transaction is that it contains two methods:\n\nCommit()marks the transaction as successful. This is only called when the busi-\nness operation itself has succeeded and all data modifications are ready to be\npersisted.\n\nDispose()ends the transaction. This is called indiscriminately at the end of the\nbusiness operation. If Commit() was previously invoked, Dispose() persists all\ndata updates; otherwise, it rolls them back.\nSuch a combination of Commit() and Dispose() guarantees that the database is\naltered only during happy paths (the successful execution of the business scenario).\nThat’s why Commit() resides at the very end of the ChangeEmail() method. In the\nevent of any error, be it a validation error or an unhandled exception, the execution\nflow returns early and thereby prevents the transaction from being committed.\n Commit() is invoked by the controller because this method call requires decision-\nmaking. There’s no decision-making involved in calling Dispose(), though, so you\ncan delegate that method call to a class from the infrastructure layer. The same class\nthat instantiates the controller and provides it with the necessary dependencies\nshould also dispose of the transaction once the controller is done working.\n Notice how UserRepository requires Transaction as a constructor parameter.\nThis explicitly shows that repositories always work on top of transactions; a repository\ncan’t call the database on its own. \nUPGRADING THE TRANSACTION TO A UNIT OF WORK\nThe introduction of repositories and a transaction is a good way to avoid potential\ndata inconsistencies, but there’s an even better approach. You can upgrade the\nTransaction class to a unit of work.\nDEFINITION\nA unit of work maintains a list of objects affected by a business\noperation. Once the operation is completed, the unit of work figures out all\nupdates that need to be done to alter the database and executes those\nupdates as a single unit (hence the pattern name).\nThe main advantage of a unit of work over a plain transaction is the deferral of\nupdates. Unlike a transaction, a unit of work executes all updates at the end of the\nbusiness operation, thus minimizing the duration of the underlying database transac-\ntion and reducing data congestion (see figure 10.6). Often, this pattern also helps to\nreduce the number of database calls.\nNOTE\nDatabase transactions also implement the unit-of-work pattern.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2729,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 262,
      "content": "240\nCHAPTER 10\nTesting the database\nMaintaining a list of modified objects and then figuring out what SQL script to gener-\nate can look like a lot of work. In reality, though, you don’t need to do that work your-\nself. Most object-relational mapping (ORM) libraries implement the unit-of-work\npattern for you. In .NET, for example, you can use NHibernate or Entity Framework,\nboth of which provide classes that do all the hard lifting (those classes are ISession\nand DbContext, respectively). The following listing shows how UserController looks\nin combination with Entity Framework.\npublic class UserController\n{\nprivate readonly CrmContext _context;\nprivate readonly UserRepository _userRepository;\nprivate readonly CompanyRepository _companyRepository;\nprivate readonly EventDispatcher _eventDispatcher;\npublic UserController(\nCrmContext context,                     \nMessageBus messageBus,\nIDomainLogger domainLogger)\n{\n_context = context;\n_userRepository = new UserRepository(\ncontext);                           \n_companyRepository = new CompanyRepository(\ncontext);                           \n_eventDispatcher = new EventDispatcher(\nmessageBus, domainLogger);\n}\npublic string ChangeEmail(int userId, string newEmail)\n{\nUser user = _userRepository.GetUserById(userId);\nListing 10.4\nUser controller with Entity Framework\nUnit of work\nGetUserById\nDatabase\nSaveCompany\nGetCompany\nController\nSaveUser\nCreate\nSaveChanges\nGetUserById\nGetCompany\nSave all\nFigure 10.6\nA unit of work executes all updates at the end of the business operation. The updates \nare still wrapped in a database transaction, but that transaction lives for a shorter period of time, \nthus reducing data congestion.\nCrmContext\nreplaces\nTransaction.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1771,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 263,
      "content": "241\nDatabase transaction management\nstring error = user.CanChangeEmail();\nif (error != null)\nreturn error;\nCompany company = _companyRepository.GetCompany();\nuser.ChangeEmail(newEmail, company);\n_companyRepository.SaveCompany(company);\n_userRepository.SaveUser(user);\n_eventDispatcher.Dispatch(user.DomainEvents);\n_context.SaveChanges();  \nreturn \"OK\";\n}\n}\nCrmContext is a custom class that contains mapping between the domain model and\nthe database (it inherits from Entity Framework’s DbContext). The controller in list-\ning 10.4 uses CrmContext instead of Transaction. As a result,\nBoth repositories now work on top of CrmContext, just as they worked on top of\nTransaction in the previous version.\nThe controller commits changes to the database via context.SaveChanges()\ninstead of transaction.Commit().\nNotice that there’s no need for UserFactory and CompanyFactory anymore because\nEntity Framework now serves as a mapper between the raw database data and\ndomain objects.\nData inconsistencies in non-relational databases\nIt’s easy to avoid data inconsistencies when using a relational database: all major\nrelational databases provide atomic updates that can span as many rows as needed.\nBut how do you achieve the same level of protection with a non-relational database\nsuch as MongoDB?\nThe problem with most non-relational databases is the lack of transactions in the\nclassical sense; atomic updates are guaranteed only within a single document. If a\nbusiness operation affects multiple documents, it becomes prone to inconsisten-\ncies. (In non-relational databases, a document is the equivalent of a row.)\nNon-relational databases approach inconsistencies from a different angle: they\nrequire you to design your documents such that no business operation modifies more\nthan one of those documents at a time. This is possible because documents are\nmore flexible than rows in relational databases. A single document can store data of\nany shape and complexity and thus capture side effects of even the most sophisti-\ncated business operations.\nCrmContext \nreplaces \nTransaction.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2134,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 264,
      "content": "242\nCHAPTER 10\nTesting the database\n10.2.2 Managing database transactions in integration tests\nWhen it comes to managing database transactions in integration tests, adhere to the\nfollowing guideline: don’t reuse database transactions or units of work between sections of the\ntest. The following listing shows an example of reusing CrmContext in the integration\ntest after switching that test to Entity Framework.\n[Fact]\npublic void Changing_email_from_corporate_to_non_corporate()\n{\nusing (var context =\n   \nnew CrmContext(ConnectionString))   \n{\n// Arrange\nvar userRepository =\n         \nnew UserRepository(context);\n         \nvar companyRepository =\n         \nnew CompanyRepository(context);         \nvar user = new User(0, \"user@mycorp.com\",\nUserType.Employee, false);\nuserRepository.SaveUser(user);\nvar company = new Company(\"mycorp.com\", 1);\ncompanyRepository.SaveCompany(company);\ncontext.SaveChanges();                      \nvar busSpy = new BusSpy();\nvar messageBus = new MessageBus(busSpy);\nvar loggerMock = new Mock<IDomainLogger>();\nvar sut = new UserController(\ncontext,                     \nmessageBus,\nloggerMock.Object);\n// Act\nstring result = sut.ChangeEmail(user.UserId, \"new@gmail.com\");\n// Assert\nAssert.Equal(\"OK\", result);\nUser userFromDb = userRepository     \n.GetUserById(user.UserId);       \n(continued)\nIn domain-driven design, there’s a guideline saying that you shouldn’t modify more\nthan one aggregate per business operation. This guideline serves the same goal: pro-\ntecting you from data inconsistencies. The guideline is only applicable to systems\nthat work with document databases, though, where each document corresponds to\none aggregate. \nListing 10.5\nIntegration test reusing CrmContext\nCreates a \ncontext\nUses the context \nin the arrange \nsection . . .\n. . . in act . . .\n. . . and in assert\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1879,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 265,
      "content": "243\nTest data life cycle\nAssert.Equal(\"new@gmail.com\", userFromDb.Email);\nAssert.Equal(UserType.Customer, userFromDb.Type);\nCompany companyFromDb = companyRepository     \n.GetCompany();\n     \nAssert.Equal(0, companyFromDb.NumberOfEmployees);\nbusSpy.ShouldSendNumberOfMessages(1)\n.WithEmailChangedMessage(user.UserId, \"new@gmail.com\");\nloggerMock.Verify(\nx => x.UserTypeHasChanged(\nuser.UserId, UserType.Employee, UserType.Customer),\nTimes.Once);\n}\n}\nThis test uses the same instance of CrmContext in all three sections: arrange, act, and\nassert. This is a problem because such reuse of the unit of work creates an environment\nthat doesn’t match what the controller experiences in production. In production, each\nbusiness operation has an exclusive instance of CrmContext. That instance is created\nright before the controller method invocation and is disposed of immediately after.\n To avoid the risk of inconsistent behavior, integration tests should replicate the\nproduction environment as closely as possible, which means the act section must not\nshare CrmContext with anyone else. The arrange and assert sections must get their\nown instances of CrmContext too, because, as you might remember from chapter 8,\nit’s important to check the state of the database independently of the data used as\ninput parameters. And although the assert section does query the user and the com-\npany independently of the arrange section, these sections still share the same database\ncontext. That context can (and many ORMs do) cache the requested data for perfor-\nmance improvements.\nTIP\nUse at least three transactions or units of work in an integration test: one\nper each arrange, act, and assert section. \n10.3\nTest data life cycle\nThe shared database raises the problem of isolating integration tests from each other.\nTo solve this problem, you need to\nExecute integration tests sequentially.\nRemove leftover data between test runs.\nOverall, your tests shouldn’t depend on the state of the database. Your tests should\nbring that state to the required condition on their own.\n10.3.1 Parallel vs. sequential test execution\nParallel execution of integration tests involves significant effort. You have to ensure\nthat all test data is unique so no database constraints are violated and tests don’t acci-\ndentally pick up input data after each other. Cleaning up leftover data also becomes\n. . . and in assert\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2446,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 266,
      "content": "244\nCHAPTER 10\nTesting the database\ntrickier. It’s more practical to run integration tests sequentially rather than spend time\ntrying to squeeze additional performance out of them.\n Most unit testing frameworks allow you to define separate test collections and\nselectively disable parallelization in them. Create two such collections (for unit and\nintegration tests), and then disable test parallelization in the collection with the inte-\ngration tests.\n As an alternative, you could parallelize tests using containers. For example, you\ncould put the model database on a Docker image and instantiate a new container\nfrom that image for each integration test. In practice, though, this approach creates\ntoo much of an additional maintenance burden. With Docker, you not only have to\nkeep track of the database itself, but you also need to\nMaintain Docker images\nMake sure each test gets its own container instance\nBatch integration tests (because you most likely won’t be able to create all con-\ntainer instances at once)\nDispose of used-up containers\nI don’t recommend using containers unless you absolutely need to minimize your\nintegration tests’ execution time. Again, it’s more practical to have just one database\ninstance per developer. You can run that single instance in Docker, though. I advocate\nagainst premature parallelization, not the use of Docker per se. \n10.3.2 Clearing data between test runs\nThere are four options to clean up leftover data between test runs:\nRestoring a database backup before each test—This approach addresses the problem\nof data cleanup but is much slower than the other three options. Even with con-\ntainers, the removal of a container instance and creation of a new one usually\ntakes several seconds, which quickly adds to the total test suite execution time.\nCleaning up data at the end of a test—This method is fast but susceptible to skip-\nping the cleanup phase. If the build server crashes in the middle of the test, or\nyou shut down the test in the debugger, the input data remains in the database\nand affects further test runs.\nWrapping each test in a database transaction and never committing it—In this case, all\nchanges made by the test and the SUT are rolled back automatically. This\napproach solves the problem of skipping the cleanup phase but poses another\nissue: the introduction of an overarching transaction can lead to inconsistent\nbehavior between the production and test environments. It’s the same problem\nas with reusing a unit of work: the additional transaction creates a setup that’s\ndifferent than that in production.\nCleaning up data at the beginning of a test—This is the best option. It works fast,\ndoesn’t result in inconsistent behavior, and isn’t prone to accidentally skipping\nthe cleanup phase.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2828,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 267,
      "content": "245\nTest data life cycle\nTIP\nThere’s no need for a separate teardown phase; implement that phase as\npart of the arrange section.\nThe data removal itself must be done in a particular order, to honor the database’s\nforeign key constraints. I sometimes see people use sophisticated algorithms to figure\nout relationships between tables and automatically generate the deletion script or\neven disable all integrity constraints and re-enable them afterward. This is unneces-\nsary. Write the SQL script manually: it’s simpler and gives you more granular control\nover the deletion process.\n Introduce a base class for all integration tests, and put the deletion script there. With\nsuch a base class, you will have the script run automatically at the start of each test, as\nshown in the following listing.\npublic abstract class IntegrationTests\n{\nprivate const string ConnectionString = \"...\";\nprotected IntegrationTests()\n{\nClearDatabase();\n}\nprivate void ClearDatabase()\n{\nstring query =\n\"DELETE FROM dbo.[User];\" +    \n\"DELETE FROM dbo.Company;\";    \nusing (var connection = new SqlConnection(ConnectionString))\n{\nvar command = new SqlCommand(query, connection)\n{\nCommandType = CommandType.Text\n};\nconnection.Open();\ncommand.ExecuteNonQuery();\n}\n}\n}\nTIP\nThe deletion script must remove all regular data but none of the refer-\nence data. Reference data, along with the rest of the database schema, should\nbe controlled solely by migrations. \nListing 10.6\nBase class for integration tests\nDeletion \nscript\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1549,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 268,
      "content": "246\nCHAPTER 10\nTesting the database\n10.3.3 Avoid in-memory databases\nAnother way to isolate integration tests from each other is by replacing the database\nwith an in-memory analog, such as SQLite. In-memory databases can seem beneficial\nbecause they\nDon’t require removal of test data\nWork faster\nCan be instantiated for each test run\nBecause in-memory databases aren’t shared dependencies, integration tests in effect\nbecome unit tests (assuming the database is the only managed dependency in the\nproject), similar to the approach with containers described in section 10.3.1.\n In spite of all these benefits, I don’t recommend using in-memory databases\nbecause they aren’t consistent functionality-wise with regular databases. This is, once\nagain, the problem of a mismatch between production and test environments. Your\ntests can easily run into false positives or (worse!) false negatives due to the differ-\nences between the regular and in-memory databases. You’ll never gain good protec-\ntion with such tests and will have to do a lot of regression testing manually anyway.\nTIP\nUse the same database management system (DBMS) in tests as in pro-\nduction. It’s usually fine for the version or edition to differ, but the vendor\nmust remain the same. \n10.4\nReusing code in test sections\nIntegration tests can quickly grow too large and thus lose ground on the maintainabil-\nity metric. It’s important to keep integration tests as short as possible but without cou-\npling them to each other or affecting readability. Even the shortest tests shouldn’t\ndepend on one another. They also should preserve the full context of the test scenario\nand shouldn’t require you to examine different parts of the test class to understand\nwhat’s going on.\n The best way to shorten integration is by extracting technical, non-business-related\nbits into private methods or helper classes. As a side bonus, you’ll get to reuse those\nbits. In this section, I’ll show how to shorten all three sections of the test: arrange, act,\nand assert.\n10.4.1 Reusing code in arrange sections\nThe following listing shows how our integration test looks after providing a separate\ndatabase context (unit of work) for each of its sections.\n[Fact]\npublic void Changing_email_from_corporate_to_non_corporate()\n{\n// Arrange\nUser user;\nListing 10.7\nIntegration test with three database contexts\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2409,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 269,
      "content": "247\nReusing code in test sections\nusing (var context = new CrmContext(ConnectionString))\n{\nvar userRepository = new UserRepository(context);\nvar companyRepository = new CompanyRepository(context);\nuser = new User(0, \"user@mycorp.com\",\nUserType.Employee, false);\nuserRepository.SaveUser(user);\nvar company = new Company(\"mycorp.com\", 1);\ncompanyRepository.SaveCompany(company);\ncontext.SaveChanges();\n}\nvar busSpy = new BusSpy();\nvar messageBus = new MessageBus(busSpy);\nvar loggerMock = new Mock<IDomainLogger>();\nstring result;\nusing (var context = new CrmContext(ConnectionString))\n{\nvar sut = new UserController(\ncontext, messageBus, loggerMock.Object);\n// Act\nresult = sut.ChangeEmail(user.UserId, \"new@gmail.com\");\n}\n// Assert\nAssert.Equal(\"OK\", result);\nusing (var context = new CrmContext(ConnectionString))\n{\nvar userRepository = new UserRepository(context);\nvar companyRepository = new CompanyRepository(context);\nUser userFromDb = userRepository.GetUserById(user.UserId);\nAssert.Equal(\"new@gmail.com\", userFromDb.Email);\nAssert.Equal(UserType.Customer, userFromDb.Type);\nCompany companyFromDb = companyRepository.GetCompany();\nAssert.Equal(0, companyFromDb.NumberOfEmployees);\nbusSpy.ShouldSendNumberOfMessages(1)\n.WithEmailChangedMessage(user.UserId, \"new@gmail.com\");\nloggerMock.Verify(\nx => x.UserTypeHasChanged(\nuser.UserId, UserType.Employee, UserType.Customer),\nTimes.Once);\n}\n}\nAs you might remember from chapter 3, the best way to reuse code between the tests’\narrange sections is to introduce private factory methods. For example, the following\nlisting creates a user.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1639,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 270,
      "content": "248\nCHAPTER 10\nTesting the database\nprivate User CreateUser(\nstring email, UserType type, bool isEmailConfirmed)\n{\nusing (var context = new CrmContext(ConnectionString))\n{\nvar user = new User(0, email, type, isEmailConfirmed);\nvar repository = new UserRepository(context);\nrepository.SaveUser(user);\ncontext.SaveChanges();\nreturn user;\n}\n}\nYou can also define default values for the method’s arguments, as shown next.\nprivate User CreateUser(\nstring email = \"user@mycorp.com\",\nUserType type = UserType.Employee,\nbool isEmailConfirmed = false)\n{\n/* ... */\n}\nWith default values, you can specify arguments selectively and thus shorten the test\neven further. The selective use of arguments also emphasizes which of those argu-\nments are relevant to the test scenario.\nUser user = CreateUser(\nemail: \"user@mycorp.com\",\ntype: UserType.Employee);\nListing 10.8\nA separate method that creates a user\nListing 10.9\nAdding default values to the factory\nListing 10.10\nUsing the factory method\nObject Mother vs. Test Data Builder\nThe pattern shown in listings 10.9 and 10.10 is called the Object Mother. The Object\nMother is a class or method that helps create test fixtures (objects the test runs\nagainst).\nThere’s another pattern that helps achieve the same goal of reusing code in arrange\nsections: Test Data Builder. It works similarly to Object Mother but exposes a fluent\ninterface instead of plain methods. Here’s a Test Data Builder usage example:\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1494,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 271,
      "content": "249\nReusing code in test sections\nWHERE TO PUT FACTORY METHODS\nWhen you start distilling the tests’ essentials and move the technicalities out to fac-\ntory methods, you face the question of where to put those methods. Should they\nreside in the same class as the tests? The base IntegrationTests class? Or in a sepa-\nrate helper class?\n Start simple. Place the factory methods in the same class by default. Move them\ninto separate helper classes only when code duplication becomes a significant issue.\nDon’t put the factory methods in the base class; reserve that class for code that has to\nrun in every test, such as data cleanup. \n10.4.2 Reusing code in act sections\nEvery act section in integration tests involves the creation of a database transaction or\na unit of work. This is how the act section currently looks in listing 10.7:\nstring result;\nusing (var context = new CrmContext(ConnectionString))\n{\nvar sut = new UserController(\ncontext, messageBus, loggerMock.Object);\n// Act\nresult = sut.ChangeEmail(user.UserId, \"new@gmail.com\");\n}\nThis section can also be reduced. You can introduce a method accepting a delegate\nwith the information of what controller function needs to be invoked. The method\nwill then decorate the controller invocation with the creation of a database context, as\nshown in the following listing.\nprivate string Execute(\nFunc<UserController, string> func,   \nMessageBus messageBus,\nIDomainLogger logger)\n{\nusing (var context = new CrmContext(ConnectionString))\n{\nvar controller = new UserController(\nUser user = new UserBuilder()\n.WithEmail(\"user@mycorp.com\")\n.WithType(UserType.Employee)\n.Build();\nTest Data Builder slightly improves test readability but requires too much boilerplate.\nFor that reason, I recommend sticking to the Object Mother (at least in C#, where you\nhave optional arguments as a language feature).\nListing 10.11\nDecorator method\nDelegate defines \na controller \nfunction.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1975,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 272,
      "content": "250\nCHAPTER 10\nTesting the database\ncontext, messageBus, logger);\nreturn func(controller);\n}\n}\nWith this decorator method, you can boil down the test’s act section to just a couple\nof lines:\nstring result = Execute(\nx => x.ChangeEmail(user.UserId, \"new@gmail.com\"),\nmessageBus, loggerMock.Object);\n10.4.3 Reusing code in assert sections\nFinally, the assert section can be shortened, too. The easiest way to do that is to intro-\nduce helper methods similar to CreateUser and CreateCompany, as shown in the fol-\nlowing listing.\nUser userFromDb = QueryUser(user.UserId);         \nAssert.Equal(\"new@gmail.com\", userFromDb.Email);\nAssert.Equal(UserType.Customer, userFromDb.Type);\nCompany companyFromDb = QueryCompany();           \nAssert.Equal(0, companyFromDb.NumberOfEmployees);\nYou can take a step further and create a fluent interface for these data assertions, sim-\nilar to what you saw in chapter 9 with BusSpy. In C#, a fluent interface on top of exist-\ning domain classes can be implemented using extension methods, as shown in the\nfollowing listing.\npublic static class UserExternsions\n{\npublic static User ShouldExist(this User user)\n{\nAssert.NotNull(user);\nreturn user;\n}\npublic static User WithEmail(this User user, string email)\n{\nAssert.Equal(email, user.Email);\nreturn user;\n}\n}\nWith this fluent interface, the assertions become much easier to read:\nUser userFromDb = QueryUser(user.UserId);\nuserFromDb\n.ShouldExist()\nListing 10.12\nData assertions after extracting the querying logic\nListing 10.13\nFluent interface for data assertions\nNew helper \nmethods\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1617,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 273,
      "content": "251\nReusing code in test sections\n.WithEmail(\"new@gmail.com\")\n.WithType(UserType.Customer);\nCompany companyFromDb = QueryCompany();\ncompanyFromDb\n.ShouldExist()\n.WithNumberOfEmployees(0);\n10.4.4 Does the test create too many database transactions?\nAfter all the simplifications made earlier, the integration test has become more read-\nable and, therefore, more maintainable. There’s one drawback, though: the test now\nuses a total of five database transactions (units of work), where before it used only\nthree, as shown in the following listing.\npublic class UserControllerTests : IntegrationTests\n{\n[Fact]\npublic void Changing_email_from_corporate_to_non_corporate()\n{\n// Arrange\nUser user = CreateUser(\n                 \nemail: \"user@mycorp.com\",\ntype: UserType.Employee);\nCreateCompany(\"mycorp.com\", 1);                 \nvar busSpy = new BusSpy();\nvar messageBus = new MessageBus(busSpy);\nvar loggerMock = new Mock<IDomainLogger>();\n// Act\nstring result = Execute(                        \nx => x.ChangeEmail(user.UserId, \"new@gmail.com\"),\nmessageBus, loggerMock.Object);\n// Assert\nAssert.Equal(\"OK\", result);\nUser userFromDb = QueryUser(user.UserId);       \nuserFromDb\n.ShouldExist()\n.WithEmail(\"new@gmail.com\")\n.WithType(UserType.Customer);\nCompany companyFromDb = QueryCompany();         \ncompanyFromDb\n.ShouldExist()\n.WithNumberOfEmployees(0);\nbusSpy.ShouldSendNumberOfMessages(1)\n.WithEmailChangedMessage(user.UserId, \"new@gmail.com\");\nloggerMock.Verify(\nListing 10.14\nIntegration test after moving all technicalities out of it\nInstantiates a\nnew database\ncontext\nbehind the\nscenes\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1640,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 274,
      "content": "252\nCHAPTER 10\nTesting the database\nx => x.UserTypeHasChanged(\nuser.UserId, UserType.Employee, UserType.Customer),\nTimes.Once);\n}\n}\nIs the increased number of database transactions a problem? And, if so, what can\nyou do about it? The additional database contexts are a problem to some degree\nbecause they make the test slower, but there’s not much that can be done about it.\nIt’s another example of a trade-off between different aspects of a valuable test: this\ntime between fast feedback and maintainability. It’s worth it to make that trade-off\nand exchange performance for maintainability in this particular case. The perfor-\nmance degradation shouldn’t be that significant, especially when the database is\nlocated on the developer’s machine. At the same time, the gains in maintainability\nare quite substantial. \n10.5\nCommon database testing questions\nIn this last section of the chapter, I’d like to answer common questions related to\ndatabase testing, as well as briefly reiterate some important points made in chapters 8\nand 9.\n10.5.1 Should you test reads?\nThroughout the last several chapters, we’ve worked with a sample scenario of chang-\ning a user email. This scenario is an example of a write operation (an operation that\nleaves a side effect in the database and other out-of-process dependencies). Most\napplications contain both write and read operations. An example of a read operation\nwould be returning the user information to the external client. Should you test both\nwrites and reads?\n It’s crucial to thoroughly test writes, because the stakes are high. Mistakes in write\noperations often lead to data corruption, which can affect not only your database but\nalso external applications. Tests that cover writes are highly valuable due to the protec-\ntion they provide against such mistakes.\n This is not the case for reads: a bug in a read operation usually doesn’t have conse-\nquences that are as detrimental. Therefore, the threshold for testing reads should be\nhigher than that for writes. Test only the most complex or important read operations;\ndisregard the rest.\n Note that there’s also no need for a domain model in reads. One of the main goals\nof domain modeling is encapsulation. And, as you might remember from chapters 5\nand 6, encapsulation is about preserving data consistency in light of any changes. The\nlack of data changes makes encapsulation of reads pointless. In fact, you don’t need a\nfully fledged ORM such as NHibernate or Entity Framework in reads, either. You are\nbetter off using plain SQL, which is superior to an ORM performance-wise, thanks to\nbypassing unnecessary layers of abstraction (figure 10.7).\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2703,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 275,
      "content": "253\nCommon database testing questions\nBecause there are hardly any abstraction layers in reads (the domain model is one\nsuch layer), unit tests aren’t of any use there. If you decide to test your reads, do so\nusing integration tests on a real database. \n10.5.2 Should you test repositories?\nRepositories provide a useful abstraction on top of the database. Here’s a usage exam-\nple from our sample CRM project:\nUser user = _userRepository.GetUserById(userId);\n_userRepository.SaveUser(user);\nShould you test repositories independently of other integration tests? It might seem\nbeneficial to test how repositories map domain objects to the database. After all,\nthere’s significant room for a mistake in this functionality. Still, such tests are a net loss\nto your test suite due to high maintenance costs and inferior protection against\nregressions. Let’s discuss these two drawbacks in more detail.\nHIGH MAINTENANCE COSTS\nRepositories fall into the controllers quadrant on the types-of-code diagram from\nchapter 7 (figure 10.8). They exhibit little complexity and communicate with an out-\nof-process dependency: the database. The presence of that out-of-process dependency\nis what inflates the tests’ maintenance costs.\n When it comes to maintenance costs, testing repositories carries the same burden\nas regular integration tests. But does such testing provide an equal amount of benefits\nin return? Unfortunately, it doesn’t.\nWrites\nDatabase\nClient\nReads\nApplication\n. . . not here\nDomain model goes here . . .\nFigure 10.7\nThere’s no need for a domain model in reads. And because the cost of a \nmistake in reads is lower than it is in writes, there’s also not as much need for integration \ntesting.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1752,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 276,
      "content": "254\nCHAPTER 10\nTesting the database\nINFERIOR PROTECTION AGAINST REGRESSIONS\nRepositories don’t carry that much complexity, and a lot of the gains in protection\nagainst regressions overlap with the gains provided by regular integration tests. Thus,\ntests on repositories don’t add significant enough value.\n The best course of action in testing a repository is to extract the little complexity it\nhas into a self-contained algorithm and test that algorithm exclusively. That’s what\nUserFactory and CompanyFactory were for in earlier chapters. These two classes per-\nformed all the mappings without taking on any collaborators, out-of-process or other-\nwise. The repositories (the Database class) only contained simple SQL queries.\n Unfortunately, such a separation between data mapping (formerly performed by\nthe factories) and interactions with the database (formerly performed by Database) is\nimpossible when using an ORM. You can’t test your ORM mappings without calling\nthe database, at least not without compromising resistance to refactoring. Therefore,\nadhere to the following guideline: don’t test repositories directly, only as part of the overarch-\ning integration test suite.\n Don’t test EventDispatcher separately, either (this class converts domain events\ninto calls to unmanaged dependencies). There are too few gains in protection against\nregressions in exchange for the too-high costs required to maintain the complicated\nmock machinery. \n10.6\nConclusion\nWell-crafted tests against the database provide bulletproof protection from bugs. In\nmy experience, they are one of the most effective tools, without which it’s impossible\nDomain model,\nalgorithms\nOvercomplicated\ncode\nTrivial code\nControllers\nComplexity,\ndomain\nsigniﬁcance\nNumber of\ncollaborators\nRepositories\nFigure 10.8\nRepositories exhibit little complexity and communicate with the \nout-of-process dependency, thus falling into the controllers quadrant on the \ntypes-of-code diagram.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2010,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 277,
      "content": "255\nSummary\nto gain full confidence in your software. Such tests help enormously when you refac-\ntor the database, switch the ORM, or change the database vendor.\n In fact, our sample project transitioned to the Entity Framework ORM earlier in\nthis chapter, and I only needed to modify a couple of lines of code in the integration\ntest to make sure the transition was successful. Integration tests working directly with\nmanaged dependencies are the most efficient way to protect against bugs resulting\nfrom large-scale refactorings. \nSummary\nStore database schema in a source control system, along with your source code.\nDatabase schema consists of tables, views, indexes, stored procedures, and any-\nthing else that forms a blueprint of how the database is constructed.\nReference data is also part of the database schema. It is data that must be pre-\npopulated in order for the application to operate properly. To differentiate\nbetween reference and regular data, look at whether your application can mod-\nify that data. If so, it’s regular data; otherwise, it’s reference data.\nHave a separate database instance for every developer. Better yet, host that\ninstance on the developer’s own machine for maximum test execution speed.\nThe state-based approach to database delivery makes the state explicit and lets a\ncomparison tool implicitly control migrations. The migration-based approach\nemphasizes the use of explicit migrations that transition the database from one\nstate to another. The explicitness of the database state makes it easier to handle\nmerge conflicts, while explicit migrations help tackle data motion.\nPrefer the migration-based approach over state-based, because handling data\nmotion is much more important than merge conflicts. Apply every modification\nto the database schema (including reference data) through migrations.\nBusiness operations must update data atomically. To achieve atomicity, rely on\nthe underlying database’s transaction mechanism.\nUse the unit of work pattern when possible. A unit of work relies on the under-\nlying database’s transactions; it also defers all updates to the end of the business\noperation, thus improving performance.\nDon’t reuse database transactions or units of work between sections of the\ntest. Each arrange, act, and assert section should have its own transaction or\nunit of work.\nExecute integration tests sequentially. Parallel execution involves significant\neffort and usually is not worth it.\nClean up leftover data at the start of a test. This approach works fast, doesn’t\nresult in inconsistent behavior, and isn’t prone to accidentally skipping the\ncleanup phase. With this approach, you don’t have to introduce a separate tear-\ndown phase, either.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2778,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 278,
      "content": "256\nCHAPTER 10\nTesting the database\nAvoid in-memory databases such as SQLite. You’ll never gain good protection if\nyour tests run against a database from a different vendor. Use the same database\nmanagement system in tests as in production.\nShorten tests by extracting non-essential parts into private methods or helper\nclasses:\n– For the arrange section, choose Object Mother over Test Data Builder.\n– For act, create decorator methods.\n– For assert, introduce a fluent interface.\nThe threshold for testing reads should be higher than that for writes. Test only\nthe most complex or important read operations; disregard the rest.\nDon’t test repositories directly, but only as part of the overarching integration\ntest suite. Tests on repositories introduce too high maintenance costs for too\nfew additional gains in protection against regressions.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 902,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 279,
      "content": "Part 4\nUnit testing anti-patterns\nThis final part of the book covers common unit testing anti-patterns. You’ve\nmost likely encountered some of them in the past. Still, it’s interesting to look at\nthis topic using the four attributes of a good unit test defined in chapter 4. You\ncan use those attributes to analyze any unit testing concepts or patterns; anti-\npatterns aren’t an exception.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 441,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 280,
      "content": " \nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 53,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 281,
      "content": "259\nUnit testing anti-patterns\nThis chapter is an aggregation of lesser related topics (mostly anti-patterns) that\ndidn’t fit in earlier in the book and are better served on their own. An anti-pattern is\na common solution to a recurring problem that looks appropriate on the surface\nbut leads to problems further down the road.\n You will learn how to work with time in tests, how to identify and avoid such anti-\npatterns as unit testing of private methods, code pollution, mocking concrete\nclasses, and more. Most of these topics follow from the first principles described in\npart 2. Still, they are well worth spelling out explicitly. You’ve probably heard of at\nleast some of these anti-patterns in the past, but this chapter will help you connect\nthe dots, so to speak, and see the foundations they are based on.\nThis chapter covers\nUnit testing private methods\nExposing private state to enable unit testing\nLeaking domain knowledge to tests\nMocking concrete classes\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1026,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 282,
      "content": "260\nCHAPTER 11\nUnit testing anti-patterns\n11.1\nUnit testing private methods\nWhen it comes to unit testing, one of the most commonly asked questions is how to\ntest a private method. The short answer is that you shouldn’t do so at all, but there’s\nquite a bit of nuance to this topic.\n11.1.1 Private methods and test fragility\nExposing methods that you would otherwise keep private just to enable unit testing\nviolates one of the foundational principles we discussed in chapter 5: testing observ-\nable behavior only. Exposing private methods leads to coupling tests to implementa-\ntion details and, ultimately, damaging your tests’ resistance to refactoring—the most\nimportant metric of the four. (All four metrics, once again, are protection against\nregressions, resistance to refactoring, fast feedback, and maintainability.) Instead of\ntesting private methods directly, test them indirectly, as part of the overarching observ-\nable behavior. \n11.1.2 Private methods and insufficient coverage\nSometimes, the private method is too complex, and testing it as part of the observable\nbehavior doesn’t provide sufficient coverage. Assuming the observable behavior\nalready has reasonable test coverage, there can be two issues at play:\nThis is dead code. If the uncovered code isn’t being used, this is likely some extra-\nneous code left after a refactoring. It’s best to delete this code.\nThere’s a missing abstraction. If the private method is too complex (and thus is\nhard to test via the class’s public API), it’s an indication of a missing abstraction\nthat should be extracted into a separate class.\nLet’s illustrate the second issue with an example.\npublic class Order\n{\nprivate Customer _customer;\nprivate List<Product> _products;\npublic string GenerateDescription()\n{\nreturn $\"Customer name: {_customer.Name}, \" +\n$\"total number of products: {_products.Count}, \" +\n$\"total price: {GetPrice()}\";             \n}\nprivate decimal GetPrice()     \n{\ndecimal basePrice = /* Calculate based on _products */;\ndecimal discounts = /* Calculate based on _customer */;\ndecimal taxes = /* Calculate based on _products */;\nListing 11.1\nA class with a complex private method\nThe complex private\nmethod is used by a\nmuch simpler public\nmethod.\nComplex private \nmethod\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2306,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 283,
      "content": "261\nUnit testing private methods\nreturn basePrice - discounts + taxes;\n}\n}\nThe GenerateDescription() method is quite simple: it returns a generic description\nof the order. But it uses the private GetPrice() method, which is much more com-\nplex: it contains important business logic and needs to be thoroughly tested. That\nlogic is a missing abstraction. Instead of exposing the GetPrice method, make this\nabstraction explicit by extracting it into a separate class, as shown in the next listing.\npublic class Order\n{\nprivate Customer _customer;\nprivate List<Product> _products;\npublic string GenerateDescription()\n{\nvar calc = new PriceCalculator();\nreturn $\"Customer name: {_customer.Name}, \" +\n$\"total number of products: {_products.Count}, \" +\n$\"total price: {calc.Calculate(_customer, _products)}\";\n}\n}\npublic class PriceCalculator\n{\npublic decimal Calculate(Customer customer, List<Product> products)\n{\ndecimal basePrice = /* Calculate based on products */;\ndecimal discounts = /* Calculate based on customer */;\ndecimal taxes = /* Calculate based on products */;\nreturn basePrice - discounts + taxes;\n}\n}\nNow you can test PriceCalculator independently of Order. You can also use the\noutput-based (functional) style of unit testing, because PriceCalculator doesn’t\nhave any hidden inputs or outputs. See chapter 6 for more information about styles\nof unit testing. \n11.1.3 When testing private methods is acceptable\nThere are exceptions to the rule of never testing private methods. To understand\nthose exceptions, we need to revisit the relationship between the code’s publicity and\npurpose from chapter 5. Table 11.1 sums up that relationship (you already saw this\ntable in chapter 5; I’m copying it here for convenience).\nListing 11.2\nExtracting the complex private method\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1832,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 284,
      "content": "262\nCHAPTER 11\nUnit testing anti-patterns\nAs you might remember from chapter 5, making the observable behavior public and\nimplementation details private results in a well-designed API. On the other hand,\nleaking implementation details damages the code’s encapsulation. The intersection of\nobservable behavior and private methods is marked N/A in the table because for a\nmethod to become part of observable behavior, it has to be used by the client code,\nwhich is impossible if that method is private.\n Note that testing private methods isn’t bad in and of itself. It’s only bad because\nthose private methods are a proxy for implementation details. Testing implementa-\ntion details is what ultimately leads to test brittleness. Having that said, there are rare\ncases where a method is both private and part of observable behavior (and thus the\nN/A marking in table 11.1 isn’t entirely correct).\n Let’s take a system that manages credit inquiries as an example. New inquiries are\nbulk-loaded directly into the database once a day. Administrators then review those\ninquiries one by one and decide whether to approve them. Here’s how the Inquiry\nclass might look in that system.\npublic class Inquiry\n{\npublic bool IsApproved { get; private set; }\npublic DateTime? TimeApproved { get; private set; }\nprivate Inquiry(\n  \nbool isApproved, DateTime? timeApproved)  \n{\nif (isApproved && !timeApproved.HasValue)\nthrow new Exception();\nIsApproved = isApproved;\nTimeApproved = timeApproved;\n}\npublic void Approve(DateTime now)\n{\nif (IsApproved)\nreturn;\nIsApproved = true;\nTimeApproved = now;\n}\n}\nTable 11.1\nThe relationship between the code’s publicity and purpose\nObservable behavior\nImplementation detail\nPublic\nGood\nBad\nPrivate\nN/A\nGood\nListing 11.3\nA class with a private constructor\nPrivate \nconstructor\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1848,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 285,
      "content": "263\nExposing private state\nThe private constructor is private because the class is restored from the database by an\nobject-relational mapping (ORM) library. That ORM doesn’t need a public construc-\ntor; it may well work with a private one. At the same time, our system doesn’t need a\nconstructor, either, because it’s not responsible for the creation of those inquiries.\n How do you test the Inquiry class given that you can’t instantiate its objects? On\nthe one hand, the approval logic is clearly important and thus should be unit tested.\nBut on the other, making the constructor public would violate the rule of not expos-\ning private methods.\n Inquiry’s constructor is an example of a method that is both private and part of\nthe observable behavior. This constructor fulfills the contract with the ORM, and the\nfact that it’s private doesn’t make that contract less important: the ORM wouldn’t be\nable to restore inquiries from the database without it.\n And so, making Inquiry’s constructor public won’t lead to test brittleness in this par-\nticular case. In fact, it will arguably bring the class’s API closer to being well-designed.\nJust make sure the constructor contains all the preconditions required to maintain its\nencapsulation. In listing 11.3, such a precondition is the requirement to have the\napproval time in all approved inquiries.\n Alternatively, if you prefer to keep the class’s public API surface as small as possi-\nble, you can instantiate Inquiry via reflection in tests. Although this looks like a hack,\nyou are just following the ORM, which also uses reflection behind the scenes. \n11.2\nExposing private state\nAnother common anti-pattern is exposing private state for the sole purpose of unit\ntesting. The guideline here is the same as with private methods: don’t expose state\nthat you would otherwise keep private—test observable behavior only. Let’s take a\nlook at the following listing.\npublic class Customer\n{\nprivate CustomerStatus _status =   \nCustomerStatus.Regular;\n   \npublic void Promote()\n{\n_status = CustomerStatus.Preferred;\n}\npublic decimal GetDiscount()\n{\nreturn _status == CustomerStatus.Preferred ? 0.05m : 0m;\n}\n}\npublic enum CustomerStatus\n{\nListing 11.4\nA class with private state\nPrivate \nstate\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2293,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 286,
      "content": "264\nCHAPTER 11\nUnit testing anti-patterns\nRegular,\nPreferred\n}\nThis example shows a Customer class. Each customer is created in the Regular status\nand then can be promoted to Preferred, at which point they get a 5% discount on\neverything.\n How would you test the Promote() method? This method’s side effect is a change\nof the _status field, but the field itself is private and thus not available in tests. A\ntempting solution would be to make this field public. After all, isn’t the change of sta-\ntus the ultimate goal of calling Promote()?\n That would be an anti-pattern, however. Remember, your tests should interact with the\nsystem under test (SUT) exactly the same way as the production code and shouldn’t have any spe-\ncial privileges. In listing 11.4, the _status field is hidden from the production code and\nthus is not part of the SUT’s observable behavior. Exposing that field would result in\ncoupling tests to implementation details. How to test Promote(), then?\n What you should do, instead, is look at how the production code uses this class. In\nthis particular example, the production code doesn’t care about the customer’s status;\notherwise, that field would be public. The only information the production code does\ncare about is the discount the customer gets after the promotion. And so that’s what\nyou need to verify in tests. You need to check that\nA newly created customer has no discount.\nOnce the customer is promoted, the discount becomes 5%.\nLater, if the production code starts using the customer status field, you’d be able to\ncouple to that field in tests too, because it would officially become part of the SUT’s\nobservable behavior.\nNOTE\nWidening the public API surface for the sake of testability is a bad practice. \n11.3\nLeaking domain knowledge to tests\nLeaking domain knowledge to tests is another quite common anti-pattern. It usually\ntakes place in tests that cover complex algorithms. Let’s take the following (admit-\ntedly, not that complex) calculation algorithm as an example:\npublic static class Calculator\n{\npublic static int Add(int value1, int value2)\n{\nreturn value1 + value2;\n}\n}\nThis listing shows an incorrect way to test it.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2226,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 287,
      "content": "265\nLeaking domain knowledge to tests\npublic class CalculatorTests\n{\n[Fact]\npublic void Adding_two_numbers()\n{\nint value1 = 1;\nint value2 = 3;\nint expected = value1 + value2;      \nint actual = Calculator.Add(value1, value2);\nAssert.Equal(expected, actual);\n}\n}\nYou could also parameterize the test to throw in a couple more test cases at almost no\nadditional cost.\npublic class CalculatorTests\n{\n[Theory]\n[InlineData(1, 3)]\n[InlineData(11, 33)]\n[InlineData(100, 500)]\npublic void Adding_two_numbers(int value1, int value2)\n{\nint expected = value1 + value2;    \nint actual = Calculator.Add(value1, value2);\nAssert.Equal(expected, actual);\n}\n}\nListings 11.5 and 11.6 look fine at first, but they are, in fact, examples of the anti-pattern:\nthese tests duplicate the algorithm implementation from the production code. Of\ncourse, it might not seem like a big deal. After all, it’s just one line. But that’s only\nbecause the example is rather simplified. I’ve seen tests that covered complex algo-\nrithms and did nothing but reimplement those algorithms in the arrange part. They\nwere basically a copy-paste from the production code.\n These tests are another example of coupling to implementation details. They score\nalmost zero on the metric of resistance to refactoring and are worthless as a result.\nSuch tests don’t have a chance of differentiating legitimate failures from false posi-\ntives. Should a change in the algorithm make those tests fail, the team would most\nlikely just copy the new version of that algorithm to the test without even trying to\nListing 11.5\nLeaking algorithm implementation\nListing 11.6\nA parameterized version of the same test\nThe leakage\nThe leakage\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1730,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 288,
      "content": "266\nCHAPTER 11\nUnit testing anti-patterns\nidentify the root cause (which is understandable, because the tests were a mere dupli-\ncation of the algorithm in the first place).\n How to test the algorithm properly, then? Don’t imply any specific implementation when\nwriting tests. Instead of duplicating the algorithm, hard-code its results into the test, as\nshown in the following listing.\npublic class CalculatorTests\n{\n[Theory]\n[InlineData(1, 3, 4)]\n[InlineData(11, 33, 44)]\n[InlineData(100, 500, 600)]\npublic void Adding_two_numbers(int value1, int value2, int expected)\n{\nint actual = Calculator.Add(value1, value2);\nAssert.Equal(expected, actual);\n}\n}\nIt can seem counterintuitive at first, but hardcoding the expected result is a good\npractice when it comes to unit testing. The important part with the hardcoded values\nis to precalculate them using something other than the SUT, ideally with the help of a\ndomain expert. Of course, that’s only if the algorithm is complex enough (we are all\nexperts at summing up two numbers). Alternatively, if you refactor a legacy applica-\ntion, you can have the legacy code produce those results and then use them as expected\nvalues in tests. \n11.4\nCode pollution\nThe next anti-pattern is code pollution.\nDEFINITION\nCode pollution is adding production code that’s only needed for\ntesting.\nCode pollution often takes the form of various types of switches. Let’s take a logger as\nan example.\npublic class Logger\n{\nprivate readonly bool _isTestEnvironment;\npublic Logger(bool isTestEnvironment)    \n{\n_isTestEnvironment = isTestEnvironment;\n}\nListing 11.7\nTest with no domain knowledge\nListing 11.8\nLogger with a Boolean switch \nThe switch\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1729,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 289,
      "content": "267\nCode pollution\npublic void Log(string text)\n{\nif (_isTestEnvironment)     \nreturn;\n/* Log the text */\n}\n}\npublic class Controller\n{\npublic void SomeMethod(Logger logger)\n{\nlogger.Log(\"SomeMethod is called\");\n}\n}\nIn this example, Logger has a constructor parameter that indicates whether the class\nruns in production. If so, the logger records the message into the file; otherwise, it\ndoes nothing. With such a Boolean switch, you can disable the logger during test runs,\nas shown in the following listing.\n[Fact]\npublic void Some_test()\n{\nvar logger = new Logger(true);    \nvar sut = new Controller();\nsut.SomeMethod(logger);\n/* assert */\n}\nThe problem with code pollution is that it mixes up test and production code and\nthereby increases the maintenance costs of the latter. To avoid this anti-pattern, keep\nthe test code out of the production code base.\n In the example with Logger, introduce an ILogger interface and create two imple-\nmentations of it: a real one for production and a fake one for testing purposes. After\nthat, re-target Controller to accept the interface instead of the concrete class, as\nshown in the following listing.\npublic interface ILogger\n{\nvoid Log(string text);\n}\nListing 11.9\nA test using the Boolean switch\nListing 11.10\nA version without the switch\nThe switch\nSets the parameter to \ntrue to indicate the \ntest environment\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1411,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 290,
      "content": "268\nCHAPTER 11\nUnit testing anti-patterns\npublic class Logger : ILogger\n  \n{\n  \npublic void Log(string text)  \n{\n  \n/* Log the text */\n  \n}\n  \n}\n  \npublic class FakeLogger : ILogger   \n{\n   \npublic void Log(string text)    \n{\n   \n/* Do nothing */\n   \n}\n   \n}\n   \npublic class Controller\n{\npublic void SomeMethod(ILogger logger)\n{\nlogger.Log(\"SomeMethod is called\");\n}\n}\nSuch a separation helps keep the production logger simple because it no longer has\nto account for different environments. Note that ILogger itself is arguably a form of\ncode pollution: it resides in the production code base but is only needed for testing.\nSo how is the new implementation better?\n The kind of pollution ILogger introduces is less damaging and easier to deal\nwith. Unlike the initial Logger implementation, with the new version, you can’t acci-\ndentally invoke a code path that isn’t intended for production use. You can’t have\nbugs in interfaces, either, because they are just contracts with no code in them. In\ncontrast to Boolean switches, interfaces don’t introduce additional surface area for\npotential bugs. \n11.5\nMocking concrete classes\nSo far, this book has shown mocking examples using interfaces, but there’s an alterna-\ntive approach: you can mock concrete classes instead and thus preserve part of the\noriginal classes’ functionality, which can be useful at times. This alternative has a sig-\nnificant drawback, though: it violates the Single Responsibility principle. The next list-\ning illustrates this idea.\npublic class StatisticsCalculator\n{\npublic (double totalWeight, double totalCost) Calculate(\nint customerId)\n{\nList<DeliveryRecord> records = GetDeliveries(customerId);\nListing 11.11\nA class that calculates statistics\nBelongs in the \nproduction code\nBelongs in \nthe test code\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1837,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 291,
      "content": "269\nMocking concrete classes\ndouble totalWeight = records.Sum(x => x.Weight);\ndouble totalCost = records.Sum(x => x.Cost);\nreturn (totalWeight, totalCost);\n}\npublic List<DeliveryRecord> GetDeliveries(int customerId)\n{\n/* Call an out-of-process dependency\nto get the list of deliveries */\n}\n}\nStatisticsCalculator gathers and calculates customer statistics: the weight and cost\nof all deliveries sent to a particular customer. The class does the calculation based on\nthe list of deliveries retrieved from an external service (the GetDeliveries method).\nLet’s also say there’s a controller that uses StatisticsCalculator, as shown in the fol-\nlowing listing.\npublic class CustomerController\n{\nprivate readonly StatisticsCalculator _calculator;\npublic CustomerController(StatisticsCalculator calculator)\n{\n_calculator = calculator;\n}\npublic string GetStatistics(int customerId)\n{\n(double totalWeight, double totalCost) = _calculator\n.Calculate(customerId);\nreturn\n$\"Total weight delivered: {totalWeight}. \" +\n$\"Total cost: {totalCost}\";\n}\n}\nHow would you test this controller? You can’t supply it with a real Statistics-\nCalculator instance, because that instance refers to an unmanaged out-of-process\ndependency. The unmanaged dependency has to be substituted with a stub. At the\nsame time, you don’t want to replace StatisticsCalculator entirely, either. This\nclass contains important calculation functionality, which needs to be left intact.\n One way to overcome this dilemma is to mock the StatisticsCalculator class\nand override only the GetDeliveries() method, which can be done by making that\nmethod virtual, as shown in the following listing.\n \nListing 11.12\nA controller using StatisticsCalculator\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1755,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 292,
      "content": "270\nCHAPTER 11\nUnit testing anti-patterns\n[Fact]\npublic void Customer_with_no_deliveries()\n{\n// Arrange\nvar stub = new Mock<StatisticsCalculator> { CallBase = true };\nstub.Setup(x => x.GetDeliveries(1))         \n.Returns(new List<DeliveryRecord>());\nvar sut = new CustomerController(stub.Object);\n// Act\nstring result = sut.GetStatistics(1);\n// Assert\nAssert.Equal(\"Total weight delivered: 0. Total cost: 0\", result);\n}\nThe CallBase = true setting tells the mock to preserve the base class’s behavior unless\nit’s explicitly overridden. With this approach, you can substitute only a part of the class\nwhile keeping the rest as-is. As I mentioned earlier, this is an anti-pattern.\nNOTE\nThe necessity to mock a concrete class in order to preserve part of its\nfunctionality is a result of violating the Single Responsibility principle.\nStatisticsCalculator combines two unrelated responsibilities: communicating with\nthe unmanaged dependency and calculating statistics. Look at listing 11.11 again. The\nCalculate() method is where the domain logic lies. GetDeliveries() just gathers\nthe inputs for that logic. Instead of mocking StatisticsCalculator, split this class in\ntwo, as the following listing shows.\npublic class DeliveryGateway : IDeliveryGateway\n{\npublic List<DeliveryRecord> GetDeliveries(int customerId)\n{\n/* Call an out-of-process dependency\nto get the list of deliveries */\n}\n}\npublic class StatisticsCalculator\n{\npublic (double totalWeight, double totalCost) Calculate(\nList<DeliveryRecord> records)\n{\ndouble totalWeight = records.Sum(x => x.Weight);\ndouble totalCost = records.Sum(x => x.Cost);\nreturn (totalWeight, totalCost);\n}\n}\nListing 11.13\nTest that mocks the concrete class\nListing 11.14\nSplitting StatisticsCalculator into two classes\nGetDeliveries() must \nbe made virtual.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1845,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 293,
      "content": "271\nWorking with time\nThe next listing shows the controller after the refactoring.\npublic class CustomerController\n{\nprivate readonly StatisticsCalculator _calculator;\nprivate readonly IDeliveryGateway _gateway;\npublic CustomerController(\nStatisticsCalculator calculator,   \nIDeliveryGateway gateway)\n   \n{\n_calculator = calculator;\n_gateway = gateway;\n}\npublic string GetStatistics(int customerId)\n{\nvar records = _gateway.GetDeliveries(customerId);\n(double totalWeight, double totalCost) = _calculator\n.Calculate(records);\nreturn\n$\"Total weight delivered: {totalWeight}. \" +\n$\"Total cost: {totalCost}\";\n}\n}\nThe responsibility of communicating with the unmanaged dependency has transi-\ntioned to DeliveryGateway. Notice how this gateway is backed by an interface, which\nyou can now use for mocking instead of the concrete class. The code in listing 11.15 is\nan example of the Humble Object design pattern in action. Refer to chapter 7 to\nlearn more about this pattern. \n11.6\nWorking with time\nMany application features require access to the current date and time. Testing func-\ntionality that depends on time can result in false positives, though: the time during\nthe act phase might not be the same as in the assert. There are three options for stabi-\nlizing this dependency. One of these options is an anti-pattern; and of the other two,\none is preferable to the other.\n11.6.1 Time as an ambient context\nThe first option is to use the ambient context pattern. You already saw this pattern in\nchapter 8 in the section about testing loggers. In the context of time, the ambient con-\ntext would be a custom class that you’d use in code instead of the framework’s built-in\nDateTime.Now, as shown in the next listing.\n \nListing 11.15\nController after the refactoring\nTwo separate \ndependencies\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1843,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 294,
      "content": "272\nCHAPTER 11\nUnit testing anti-patterns\npublic static class DateTimeServer\n{\nprivate static Func<DateTime> _func;\npublic static DateTime Now => _func();\npublic static void Init(Func<DateTime> func)\n{\n_func = func;\n}\n}\nDateTimeServer.Init(() => DateTime.Now);     \nDateTimeServer.Init(() => new DateTime(2020, 1, 1));      \nJust as with the logger functionality, using an ambient context for time is also an anti-\npattern. The ambient context pollutes the production code and makes testing more\ndifficult. Also, the static field introduces a dependency shared between tests, thus tran-\nsitioning those tests into the sphere of integration testing. \n11.6.2 Time as an explicit dependency\nA better approach is to inject the time dependency explicitly (instead of referring to it\nvia a static method in an ambient context), either as a service or as a plain value, as\nshown in the following listing.\npublic interface IDateTimeServer\n{\nDateTime Now { get; }\n}\npublic class DateTimeServer : IDateTimeServer\n{\npublic DateTime Now => DateTime.Now;\n}\npublic class InquiryController\n{\nprivate readonly DateTimeServer _dateTimeServer;\npublic InquiryController(\nDateTimeServer dateTimeServer)    \n{\n_dateTimeServer = dateTimeServer;\n}\npublic void ApproveInquiry(int id)\n{\nInquiry inquiry = GetById(id);\nListing 11.16\nCurrent date and time as an ambient context\nListing 11.17\nCurrent date and time as an explicit dependency\nInitialization code \nfor production\nInitialization code \nfor unit tests\nInjects time as \na service\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1563,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 295,
      "content": "273\nSummary\ninquiry.Approve(_dateTimeServer.Now);      \nSaveInquiry(inquiry);\n}\n}\nOf these two options, prefer injecting the time as a value rather than as a service. It’s\neasier to work with plain values in production code, and it’s also easier to stub those\nvalues in tests.\n Most likely, you won’t be able to always inject the time as a plain value, because\ndependency injection frameworks don’t play well with value objects. A good compro-\nmise is to inject the time as a service at the start of a business operation and then\npass it as a value in the remainder of that operation. You can see this approach in\nlisting 11.17: the controller accepts DateTimeServer (the service) but then passes a\nDateTime value to the Inquiry domain class. \n11.7\nConclusion\nIn this chapter, we looked at some of the most prominent real-world unit testing use\ncases and analyzed them using the four attributes of a good test. I understand that it\nmay be overwhelming to start applying all the ideas and guidelines from this book at\nonce. Also, your situation might not be as clear-cut. I publish reviews of other people’s\ncode and answer questions (related to unit testing and code design in general) on my\nblog at https://enterprisecraftsmanship.com. You can also submit your own question\nat https://enterprisecraftsmanship.com/about. You might also be interested in taking\nmy online course, where I show how to build an application from the ground up,\napplying all the principles described in this book in practice, at https://unittesting-\ncourse.com.\n You can always catch me on twitter at @vkhorikov, or contact me directly through\nhttps://enterprisecraftsmanship.com/about. I look forward to hearing from you!\nSummary\nExposing private methods to enable unit testing leads to coupling tests to\nimplementation and, ultimately, damaging the tests’ resistance to refactoring.\nInstead of testing private methods directly, test them indirectly as part of the\noverarching observable behavior.\nIf the private method is too complex to be tested as part of the public API that\nuses it, that’s an indication of a missing abstraction. Extract this abstraction into\na separate class instead of making the private method public.\nIn rare cases, private methods do belong to the class’s observable behavior.\nSuch methods usually implement a non-public contract between the class and\nan ORM or a factory.\nDon’t expose state that you would otherwise keep private for the sole purpose\nof unit testing. Your tests should interact with the system under test exactly the\nsame way as the production code; they shouldn’t have any special privileges.\nInjects time as \na plain value\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2702,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 296,
      "content": "274\nCHAPTER 11\nUnit testing anti-patterns\nDon’t imply any specific implementation when writing tests. Verify the produc-\ntion code from a black-box perspective; avoid leaking domain knowledge to\ntests (see chapter 4 for more details about black-box and white-box testing).\nCode pollution is adding production code that’s only needed for testing. It’s an\nanti-pattern because it mixes up test and production code and increases the\nmaintenance costs of the latter.\nThe necessity to mock a concrete class in order to preserve part of its function-\nality is a result of violating the Single Responsibility principle. Separate that\nclass into two classes: one with the domain logic, and the other one communi-\ncating with the out-of-process dependency.\nRepresenting the current time as an ambient context pollutes the production\ncode and makes testing more difficult. Inject time as an explicit dependency—\neither as a service or as a plain value. Prefer the plain value whenever possible.\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1040,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 297,
      "content": "275\nindex\nA\nAAA (arrange, act, and assert) pattern 42–49\navoiding if statements 44–45\navoiding multiple AAA sections 43–44\ndifferentiating system under test 47–48\ndropping AAA comments 48–49\noverview 42–43\nreusing code in test sections 246–252\nin act sections 249–250\nin arrange sections 246–249\nin assert sections 250\nsection size 45–47\narrange section 45\nnumber of assertions in assert \nsection 47\nsections larger than a single line 45–47\nteardown phase 47\nabstractions 198, 260\nActive Record pattern 159\nadapters 227\naggregates 157\nambient context 212\nanti-patterns 212\ncode pollution 266–268\nexposing private state 263–264\nleaking domain knowledge to tests\n264–266\nmocking concrete classes 268–271\nprivate methods 260–263\nacceptability of testing 261–263\ninsufficient coverage 260–261\ntest fragility 260\ntime 271–273\nas ambient context 271–272\nas explicit dependency 272–273\nAPI (application programming interface) 104, \n111, 133, 191, 195, 227, 264\nmissing abstractions 260\npublic vs. private 99\nwell-designed 100–101, 105, 108, 262\napplication behavior 57\napplication services layer 133–134\narrange, act, and assert pattern. See AAA \npattern\nassertion libraries, using to improve test \nreadability 62–63\nassertion-free testing 12–13\nasynchronous communications 191\natomic updates 236\nautomation concepts 87–90\nblack-box vs. white-box testing 89–90\nTest Pyramid 87–89\nB\nbackward migration 233\nbad tests 189\nblack-box testing 68, 89–90\nBoolean switches 266–268\nbranch coverage metric 10–11\nbrittle tests 83–84, 116, 216\nbrittleness 86, 125\nbugs 68, 79, 104, 175, 189\nbusiness logic 106–107, 156, 169, \n179\nC\nCanExecute/Execute pattern 172, 174\nCAP theorem 86–87\ncaptured data 208\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1735,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 298,
      "content": "INDEX\n276\ncircular dependencies 203\ndefined 202\neliminating 202–204\nclassical school of unit testing 30–37\ndependencies 30–34\nend-to-end tests 38–39\nintegration tests 37–39\nisolation issue 27–30\nmocks 114–116\nmocking out out-of-process dependencies\n115–116\nusing mocks to verify behavior 116\nprecise bug location 36\ntesting large graph of interconnected classes 35\ntesting one class at a time 34–35\ncleanup phase 244\nclusters, grouping into aggregates 157\ncode complexity 104, 152\ncode coverage metric 9–10\ncode coverage tools 90\ncode depth 157\ncode pollution 127, 266–268, 272\ncode width 157\ncollaborators 32, 148, 153\ncommand query separation. See CQS principle\ncommands 97\ncommunication-based testing 122–123, 128\nfeedback speed 124\nmaintainability 127\noveruse of 124\nprotection against regressions and feedback \nspeed 124\nresistance to refactoring 124–125\nvulnerability to false alarms 124\ncommunications\nbetween applications 107, 110\nbetween classes in application 110, 116\nconditional logic 169–180\nCanExecute/Execute pattern 172–174\ndomain events for tracking changes in the \ndomain model 175–178\nconstructors, reusing test fixtures between \ntests 52\ncontainers 244\ncontrollers 153, 225\nsimplicity 171\ncoverage metrics, measuring test suite quality \nwith 8–15\naiming for particular coverage number 15\nbranch coverage metric 10–11\ncode coverage metric 9–10\nproblems with 12–15\ncode paths in external libraries 14–15\nimpossible to verify all possible outcomes\n12–13\nCQS (command query separation) principle\n97–98\nCRUD (create, read, update, and delete) \noperations 89\nCSV files 208–209\ncyclic dependency 202\ncyclomatic complexity 152\nD\ndata inconsistencies 241\ndata mapping 254\ndata motion 234\ndata, bundling 104\ndatabase backup, restoring 244\ndatabase management system (DBMS) 246\ndatabase testing\ncommon questions 252–255\ntesting reads 252–253\ntesting repositories 253–254\ndatabase transaction management 234–243\nin integration tests 242–243\nin production code 235–242\nprerequisites for 230–234\nkeeping database in source control \nsystem 230–231\nreference data as part of database \nschema 231\nseparate instances for every developer\n232\nstate-based vs. migration-based database \ndelivery 232–234\nreusing code in test sections 246–252\ncreating too many database \ntransactions 251–252\nin act sections 249–250\nin arrange sections 246–249\nin assert sections 250\ntest data life cycle 243–246\navoiding in-memory databases 246\nclearing data between test runs 244–245\nparallel vs. sequential test execution\n243–244\ndatabase transaction management 234–243\nin integration tests 242–243\nin production code 235–242\nseparating connections from transactions\n236–239\nupgrading transaction to unit of work\n239–242\ndatabase transactions 244\ndaysFromNow parameter 60\nDBMS (database management system) 246\ndead code 260\ndeliveryDate parameter 62\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2884,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 299,
      "content": "INDEX\n277\ndependencies 28–29, 35\nclassical school of unit testing 30–34\nLondon school of unit testing 30–34\nout-of-process 161, 190\nshared 29, 31\ntypes of 115\nDetroit approach, unit testing 21\ndiagnostic logging 206, 212\ndiscovered abstractions 198\nDocker container 28\ndomain events, tracking changes in domain \nmodel 175–178\ndomain layers 106–107, 109, 133–134\ndomain model 16, 153, 225\nconnecting with external applications 111\ntestability 171\ndomain significance 153\ndummy test double 93–94\nE\nEasyMock 25\nedge cases 187, 189, 194\nencapsulation 46, 252\nend-to-end tests 88–89, 195–196, 205, 222\nclassical school of unit testing 38–39\nLondon school of unit testing 38–39\npossibility of creating ideal tests 81\nenterprise applications 5\nEntity Framework 240–242, 255\nentropy 6\nerror handling 146\nexceptions 130\nexpected parameter 62\nexplicit inputs and outputs 130\nexternal libraries 81\nexternal reads 170–171, 173\nexternal state 130\nexternal writes 170–171, 173\nF\nFail Fast principle 185, 189\nfailing preconditions 190\nfake dependencies 93\nfake test double 93–94\nfalse negatives 76–77\nfalse positives 69–70, 77, 82, 86, 96, 99, 124\ncauses of 71–74\nimportance of 78–79\nfast feedback 81–86, 88, 99, 123, 252, 260\nfat controllers 154\nfeedback loop, shortening 189\nfeedback speed 79–80, 124\nfixed state 50\nFluent Assertions 62\nfragile tests 96, 113\nframeworks 81\nfunctional architecture 128–134\ndefined 132–133\ndrawbacks of 146–149\napplicability of 147–148\ncode base size increases 149\nperformance drawbacks 148\nfunctional programming 128–131\nhexagonal architecture 133–134\ntransitioning to output-based testing 135–146\naudit system 135–137\nrefactoring toward functional \narchitecture 140–145\nusing mocks to decouple tests from \nfilesystem 137–140\nfunctional core 132–133, 143–144, 156\nfunctional programming 121\nfunctional testing 38, 121, 128\nG\nGit 230–231\nGiven-When-Then pattern 43\nGUI (graphical user interface) tests 38\nH\nhandwritten mocks 94, 222\nhappy paths 187, 194, 239\nhelper methods 126–127\nhexagonal architecture 106–107, 128, 156\ndefining 106–110\nfunctional architecture 133–134\npurpose of 107\nhexagons 106, 108, 134\nhidden outputs 131\nhigh coupling, reusing test fixtures between \ntests 52\nHTML tags 72\nhumble controller 160\nHumble Object pattern 155, 157–158, 167, 271\nhumble objects 157\nhumble wrappers 155\nI\nideal tests 80–87\nbrittle tests 83–84\nend-to-end tests 81\npossibility of creating 81\ntrivial tests 82–83\nif statements 10–11, 44–45, 143, 152, 173–174\nimmutability 133\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2542,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 300,
      "content": "INDEX\n278\nimmutable classes 133\nimmutable core 132, 134\nimmutable events 176\nimmutable objects 30, 132\nimplementation details 99–105\nincoming interactions 94–95\ninfrastructure code 16\ninfrastructure layer 202\nin-memory databases 246\nin-process dependencies 199–200\nINSERT statements 231\ninteger type 14\nintegration testing\nbest practices 200–205\neliminating circular dependencies\n202–204\nmaking domain model boundaries \nexplicit 200\nmultiple act sections 204–205\nreducing number of layers 200–202\nclassical school of unit testing 37–39\ndatabase transaction management in\n242–243\ndefined 186–190\nexample of 193–197\ncategorizing database and message bus 195\nend-to-end testing 195–196\nfirst version 196–197\nscenarios 194\nfailing fast 188–190\ninterfaces for abstracting dependencies\n197–200\nin-process dependencies 199–200\nloose coupling and 198\nout-of-process dependencies 199\nlogging functionality 205–213\namount of logging 212\nintroducing wrapper on top of ILogger\n207–208\npassing around logger instances 212–213\nstructured logging 208–209\nwhether to test or not 205–206\nwriting tests for support and diagnostic \nlogging 209–211\nLondon school of unit testing 37–39\nout-of-process dependencies 190–193\ntypes of 190–191\nwhen real databases are unavailable\n192–193\nworking with both 191–192\nrole of 186–187\nTest Pyramid 187\ninterconnected classes 34\ninternal keyword 99\ninvariant violations 46, 103\ninvariants 100, 103\nisolation issue\nclassical school of unit testing 27–30\nLondon school of unit testing 21–27\nisSuccess flag 113\nJ\nJMock 25\nJSON files 208–209\nL\nlogging functionality testing 205–213\namount of logging 212\nintroducing wrapper on top of ILogger\n207–208\npassing around logger instances 212–213\nstructured logging 208–209\nwhether to test or not 205–206\nwriting tests for support and diagnostic \nlogging 209–211\nLondon school of unit testing 30–37\ndependencies 30–34\nend-to-end tests 38–39\nintegration tests 37–39\nisolation issue 21–27\nmocks 114–116\nmocking out out-of-process dependencies\n115–116\nusing mocks to verify behavior 116\nprecise bug location 36\ntesting large graph of interconnected classes 35\ntesting one class at a time 34–35\nloose coupling, interfaces for abstracting depen-\ndencies and 198\nM\nmaintainability 79–80, 85, 88, 99, 137, 148, \n252, 260\ncomparing testing styles 125–127\ncommunication-based tests 127\noutput-based tests 125\nstate-based tests 125–127\nmanaged dependencies 190, 192, 246\nmathematical functions 128–131\nmerging domain events 177\nmessage bus 190–192, 199, 220, 224\nmethod signatures 128\nmethod under test (MUT) 25\nMicrosoft MSTest 49\nmigration-based database delivery 232–234\nmissing abstractions 260\nmock chains 127\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2713,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 301,
      "content": "INDEX\n279\nmocking frameworks 25\nmockist style, unit testing 21\nMockito 25\nmocks 25, 254\nbest practices 225–227\nfor integration tests only 225\nnot just one mock per test 225–226\nonly mock types that you own 227\nverifying number of calls 226\ndecoupling tests from filesystem 137–140\ndefined 25\nLondon school vs. classical school 114–116\nmocking out out-of-process \ndependencies 115–116\nusing mocks to verify behavior 116\nmaximizing value of 217–225\nIDomainLogger 224–225\nreplacing mocks with spies 222–224\nverifying interactions at system edges\n219–222\nmocking concrete classes 268–271\nobservable behavior vs. implementation \ndetails 99–105\nleaking implementation details 100–105\nobservable behavior vs. public API 99–100\nwell-designed API and encapsulation\n103–104\nstubs 93–98\nasserting interactions with stubs 96–97\ncommands and queries 97–98\nmock (tool) vs. mock (test double) 94–95\ntypes of test doubles 93–94\nusing mocks and stubs together 97\ntest doubles 25\ntest fragility 106–114\ndefining hexagonal architecture 106–110\nintra-system vs. inter-system \ncommunications 110–114\nmodel database 230\nModel-View-Controller (MVC) pattern 157\nMoq 25, 95, 226\nMSTest 49\nMUT (method under test) 25\nmutable objects 132\nmutable shell 132–133, 143–144\nMVC (Model-View-Controller) pattern 157\nN\nnaming tests 54–58\nguidelines for 56\nrenaming tests to meet guidelines 56–58\nNHibernate 240\nnoise, reducing 78\nNSubstitute 25\nNuGet package 49\nNUnit 49, 51\nO\nobject graphs 22–23\nObject Mother 248\nobject-oriented programming (OOP) 63, 133\nobject-relational mapping (ORM) 163, 177, \n227, 240, 243, 254–255, 263\nobservable behavior 99, 105, 108, 115, 263\nleaking implementation details 100–105\npublic API 99–100\nwell-designed API and encapsulation 103–104\nOCP (Open-Closed principle) 198\nOOP (object-oriented programming) 63, 133\nOpen-Closed principle (OCP) 198\noperations 99, 104\norchestration, separating business logic from\n169, 179\nORM (object-relational mapping) 163, 177, \n227, 240, 243, 254–255, 263\noutcoming interactions 94–95\nout-of-process collaborators 159–160\nout-of-process dependencies 28, 33, 38–39, \n115, 125, 148, 160–161, 167, 170, 176, \n186, 200, 229\nintegration testing 190–193\ninterfaces for abstracting dependencies 199\ntypes of 190–191\nwhen real databases are unavailable\n192–193\nworking with both 191–192\noutput value 121\noutput-based testing 120–121, 124, 128\nfeedback speed 124\nmaintainability 125\nprotection against regressions and feedback \nspeed 124\nresistance to refactoring 124–125\ntransitioning to functional architecture \nand 135–146\naudit system 135–137\nrefactoring toward functional \narchitecture 140–145\nusing mocks to decouple tests from \nfilesystem 137–140\novercomplicated code 154\noverspecification 96\nP\nparallel test execution 243–244\nparameterized tests 59, 61\npartition tolerance 86\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2858,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 302,
      "content": "INDEX\n280\nperformance 171\npersistence state 189\npreconditions 190\nprivate APIs 99\nprivate constructors 263\nprivate dependencies 28–29, 31, 115\nprivate keyword 99\nprivate methods 260–263\nacceptability of testing 261–263\ninsufficient coverage and 260–261\nreusing test fixtures between tests 52–54\ntest fragility and 260\nProduct array 129\nproduction code 8\nprotection against regressions 68–69, 81, 84–86, \n88, 99, 260\ncomparing testing styles 124\nimportance of false positives and false \nnegatives 78–79\nmaximizing test accuracy 76–78\nPublic API 99, 109\npure functions 128\nQ\nqueries 97\nR\nrandom number generators 29\nread operations 252\nreadability 53\nread-decide-act approach 148\nrefactoring 165\nanalysis of optimal test coverage 167–169\ntesting domain layer and utility code 167–168\ntesting from other three quadrants 168\ntesting preconditions 169\nconditional logic in controllers 169–180\nCanExecute/Execute pattern 172–174\ndomain events for tracking changes in the \ndomain model 175–178\nidentifying code to refactor 152–158\nfour types of code 152–155\nHumble Object pattern for splitting overcom-\nplicated code 155–158\nresistance to 69–71\ncomparing testing styles 124–125\nimportance of false positives and false \nnegatives 78–79\nmaximizing test accuracy 76–78\nto parameterized tests\ngeneral discussion 58–62\ngenerating data for parameterized tests\n60–62\ntoward valuable unit tests 158–167\napplication services layer 160–162\nCompany class 164–167\ncustomer management system 158–160\nmaking implicit dependencies explicit 160\nremoving complexity from application \nservice 163–164\nreference data 231, 234, 245\nreferential transparency 130\nregression errors 8, 69, 82\nregressions 7, 229\nrepositories 236–237, 241, 253\nresistance to refactoring 69–71, 79–81, 83–85, \n88–90, 92–93, 99, 123, 260, 265\ncomparing testing styles 124–125\nimportance of false positives and false \nnegatives 78–79\nmaximizing test accuracy 76–78\nreturn statement 10\nreturn true statement 10\nreusability 53\nS\nscalability 7\nsequential test execution 243–244\nshallowness 124–125\nshared dependencies 28–29, 31, 33, 115, 148, 246\nside effects 130–134, 190\nsignal-to-noise ratio 212\nSingle Responsibility principle 157, 268, 270\nsingle-line act section 45\nSMTP service 110, 112–115, 134, 190\nsoftware bugs 7, 68\nsoftware entropy 6\nsource of truth 231\nspies 94, 222–224\nspy test double 93\nSQL scripts 231–232, 240, 245\nSQLite 246\nstate 99, 101\nstate verification 125\nstate-based database delivery 232\nstate-based testing 120–122, 124, 128, 135\nfeedback speed 124\nmaintainability 125–127\nprotection against regressions and feedback \nspeed 124\nresistance to refactoring 124–125\nstubs, mocks 93–98\nasserting interactions with stubs 96–97\ncommands and queries 97–98\nmock (tool) vs. mock (test double) 94–95\ntypes of test doubles 93–94\nusing mocks and stubs together 97\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2876,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 303,
      "content": "INDEX\n281\nsub-renderers collection 105\nsupport logging 206, 212\nsustainability 7\nsustainable growth 6\nSUT (system under test) 24–25, 29, 36–37, 43, \n45, 47–48, 57, 71, 73–75, 84, 93–94, 96–97, \n120–121, 123, 153, 244, 264, 266\nswitch statement 10\nsynchronous communications 191\nsystem leaks 100\nT\ntables 191\ntautology tests 82\nTDD (test-driven development) 36, 43\ntell-don’t-ask principle 104\ntest code 8\ntest coverage 9\nTest Data Builder 248\ntest data life cycle 243–246\navoiding in-memory databases 246\nclearing data between test runs 244–245\nparallel vs. sequential test execution\n243–244\ntest doubles 22–23, 25, 28, 93–94, 98, 199\ntest fixtures 248\ndefined 50\nreusing between tests\nconstructors 52\nhigh coupling 52\nprivate factory methods 52–54\nreusing between tests 50–54\ntest fragility, mocks and 106–114\ndefining hexagonal architecture 106–110\nintra-system vs. inter-system \ncommunications 110–114\ntest isolation 115\nTest Pyramid\ngeneral discussion 87–89\nintegration testing 187\ntest suites\ncharacteristics of successful suites 15–17\nintegration into development cycle 16\nmaximum value with minimum maintenance \ncosts 17\ntargeting most important parts of code \nbase 16–17\ncoverage metrics, measuring test suite quality \nwith 8–15\naiming for particular coverage number 15\nbranch coverage metric 10–11\ncode coverage metric 9–10\nproblems with 12–15\nthird-party applications 81, 112\ntight coupling 5\ntime 271–273\nas ambient context 271–272\nas explicit dependency 272–273\ntrivial code 153–154\ntrivial tests 82–83\ntrue negative 76\ntrue positive 76\ntwo-line act section 46\nU\nUI (user interface) tests 38\nunit of behavior 56, 225\nunit of work 239, 242\nunit testing\nanatomy of 41–63\nAAA pattern 42–49\nassertion libraries, using to improve test \nreadability 62–63\nnaming tests 54–58\nrefactoring to parameterized tests 58–62\nreusing test fixtures between tests 50–54\nxUnit testing framework 49–50\nautomation concepts 87–90\nblack-box vs. white-box testing 89–90\nTest Pyramid 87–89\ncharacteristics of successful test suites 15–17\nintegration into development cycle 16\nmaximum value with minimum maintenance \ncosts 17\ntargeting most important parts of code \nbase 16–17\nclassical school of 30–37\ndependencies 30–34\nend-to-end tests 38–39\nintegration tests 37–39\nisolation issue 27–30\nprecise bug location 36\ntesting large graph of interconnected \nclasses 35\ntesting one class at a time 34–35\ncoverage metrics, measuring test suite quality \nwith 8–15\naiming for particular coverage number 15\nbranch coverage metric 10–11\ncode coverage metric 9–10\nproblems with 12–15\ncurrent state of 4–5\ndefined 21–30\nfour pillars of 68–80\nfeedback speed 79–80\nmaintainability 79–80\nprotection against regressions 68–69\nresistance to refactoring 69–71\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 2778,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 304,
      "content": "INDEX\n282\nunit testing (continued)\nfunctional architecture 128–134\ndefined 132–133\ndrawbacks of 146–149\nfunctional programming 128–131\nhexagonal architecture 133–134\ntransitioning to output-based testing\n135–146\ngoal of 5–8\ngood vs. bad tests 7–8\nideal tests 80–87\nbrittle tests 83–84\nend-to-end tests 81\npossibility of creating 81\ntrivial tests 82–83\nLondon school of 30–37\ndependencies 30–34\nend-to-end tests 38–39\nintegration tests 37–39\nisolation issue 21–27\nprecise bug location 36\ntesting large graph of interconnected \nclasses 35\ntesting one class at a time 34–35\nstyles of 120–123\ncommunication-based testing\n122–123\ncomparing 123–128\noutput-based testing 120–121\nstate-based testing 121–122\nunits of behavior 34\nunits of code 21, 27–29, 34, 47, 225\nunmanaged dependencies 190, 199, 211, 216, \n218, 220, 222, 226, 254\nuser controller 193\nuser interface (UI) tests 38\nV\nvalue objects 31, 126–127\nvoid type 97\nvolatile dependencies 29\nW\nwhite-box testing 89–90\nwrite operation 252\nX\nxUnit testing framework 49–50\nY\nYAGNI (You aren’t gonna need it) principle\n198–199\nLicensed to Jorge Cavaco <jorgemccavaco@gmail.com>\n",
      "content_length": 1123,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 305,
      "content": "Vladimir Khorikov\nG\nreat testing practices will help maximize your project \nquality and delivery speed. Wrong tests will break your \ncode, multiply bugs, and increase time and costs. You \nowe it to yourself—and your projects—to learn how to do \nexcellent unit testing to increase your productivity and the \nend-to-end quality of your software.\nUnit Testing: Principles, Practices, and Patterns teaches you to \ndesign and write tests that target the domain model and \nother key areas of your code base. In this clearly written \nguide, you learn to develop professional-quality test suites, \nsafely automate your testing process, and integrate testing \nthroughout the application life cycle. As you adopt a testing \nmindset, you’ll be amazed at how better tests cause you to \nwrite better code. \nWhat’s Inside\n● Universal guidelines to assess any unit test\n● Testing to identify and avoid anti-patterns\n● Refactoring tests along with the production code\n● Using integration tests to verify the whole system\nFor readers who know the basics of unit testing. The C# \nexamples apply to any language.\nVladimir Khorikov is an author, blogger, and Microsoft MVP. \nHe has mentored numerous teams on the ins and outs of \nunit testing.\nTo download their free eBook in PDF, ePub, and Kindle formats, owners \nof this book should visit www.manning.com/books/unit-testing\n$49.99 / Can $65.99  [INCLUDING eBOOK]\nUnit Testing Principles, Practices, and Patterns\nTESTING/SOFTWARE DEVELOPMENT\nM A N N I N G\n“\nThis book is an\n indispensable resource.”\n \n—Greg Wright\nKainos Software Ltd.\n“\nServes as a valuable and \nhumbling encouragement \nto double down and test \nwell, something we need \nno matter how experienced \n  we may be.”\n \n—Mark Nenadov, BorderConnect\n“\nI wish I had this book \ntwenty years ago when I was \nstarting my career in \n  software development.”\n—Conor Redmond\nIncomm Product Control \n“\nThis is the kind of book \non unit testing I have been \n waiting on for a long time.”\n \n—Jeremy Lange, G2\nSee first page\nISBN-13: 978-1-61729-627-7\nISBN-10: 1-61729-627-9\n",
      "content_length": 2055,
      "extraction_method": "PyMuPDF_fallback"
    }
  ]
}