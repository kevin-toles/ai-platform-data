{
  "metadata": {
    "title": "Designing Distributed Systems 2e - Brendan Burns",
    "author": "Brendan Burns",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 395,
    "conversion_date": "2025-12-19T17:26:26.422019",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "Designing Distributed Systems 2e - Brendan Burns.pdf",
    "extraction_method": "Unstructured"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Introduction",
      "start_page": 20,
      "end_page": 31,
      "detection_method": "regex_chapter_title",
      "content": "A Brief History of Systems Development\n\nIn the beginning, there were machines built for specific purposes, such as\n\ncalculating artillery tables or the tides, breaking codes, or other precise,\n\ncomplicated but rote mathematical applications. Eventually these purpose-\n\nbuilt machines evolved into general-purpose programmable machines. And\n\neventually they evolved from running one program at a time to running\n\nmultiple programs on a single machine via time-sharing operating systems,\n\nbut these machines were still disjoint from each other.\n\nGradually, machines came to be networked together, and client-server\n\narchitectures were born so that a relatively low-powered machine on\n\nsomeone’s desk could be used to harness the greater power of a mainframe\n\nin another room or building. While this sort of client-server programming\n\nwas somewhat more complicated than writing a program for a single\n\nmachine, it was still fairly straightforward to understand. The client(s) made\n\nrequests; the server(s) serviced those requests.\n\nIn the early 2000s, the rise of the internet and large-scale data centers\n\nconsisting of thousands of relatively low-cost commodity computers\n\nnetworked together gave rise to the widespread development of distributed\n\nsystems. Unlike client-server architectures, distributed system applications\n\nare made up of multiple different applications running on different\n\nmachines, or many replicas running across different machines, all\n\ncommunicating together to implement a system like web search or a retail\n\nsales platform.\n\nBecause of their distributed nature, when structured properly, distributed\n\nsystems are inherently more reliable. And when architected correctly, they\n\ncan lead to much more scalable organizational models for the teams of\n\nsoftware engineers that built these systems. Unfortunately, these advantages\n\ncome at a cost. These distributed systems can be significantly more\n\ncomplicated to design, build, and debug correctly. The engineering skills\n\nneeded to build a reliable distributed system are significantly higher than\n\nthose needed to build single-machine applications like mobile or web\n\nfrontends. Regardless, the need for reliable distributed systems only\n\ncontinues to grow. Thus, there is a corresponding need for the tools,\n\npatterns, and practices for building them.\n\nFortunately, technology has also increased the ease with which you can\n\nbuild distributed systems. Containers, container images, and container\n\norchestrators have all become popular in recent years because they are the\n\nfoundation and building blocks for reliable distributed systems. Using\n\ncontainers and container orchestration as a foundation, we can establish a\n\ncollection of patterns and reusable components. These patterns and\n\ncomponents are a toolkit that we can use to build our systems more reliably\n\nand efficiently.\n\nA Brief History of Patterns in Software\n\nDevelopment\n\nThis is not the first time such a transformation has occurred in the software\n\nindustry. For a better context on how patterns, practices, and reusable\n\ncomponents have previously reshaped systems development, it is helpful to\n\nlook at past moments when similar transformations have taken place.\n\nFormalization of Algorithmic Programming\n\nThough people had been programming for more than a decade before its\n\npublication in 1962, Donald Knuth’s collection, The Art of Computer\n\nProgramming (Addison-Wesley Professional), marks an important chapter\n\nin the development of computer science. In particular, the books contain\n\nalgorithms not designed for any specific computer, but rather to educate the\n\nreader on the algorithms themselves. These algorithms then could be\n\nadapted to the specific architecture of the machine being used or the\n\nspecific problem that the reader was solving. This formalization was\n\nimportant because it provided users with a shared toolkit for building their\n\nprograms, but also because it showed that there was a general-purpose\n\nconcept that programmers should learn and then subsequently apply in a\n\nvariety of different contexts. The algorithms themselves, independent of\n\nany specific problem to solve, were worth understanding for their own sake.\n\nPatterns for Object-Oriented Programming\n\nKnuth’s books represent an important landmark in the thinking about\n\ncomputer programming, and algorithms represent an important component\n\nin the development of computer programming. However, as the complexity\n\nof programs grew, and the number of people writing a single program grew\n\nfrom the single digits to the double digits and eventually to the thousands, it\n\nbecame clear that procedural programming languages and algorithms were\n\ninsufficient for the tasks of modern-day programming. These changes in\n\ncomputer programming led to the development of object-oriented\n\nprogramming languages, which elevated data, reusability, and extensibility\n\nto peers of the algorithm in the development of computer programs.\n\nIn response to these changes to computer programming, there were changes\n\nto the patterns and practices for programming as well. Throughout the early\n\nto mid-1990s, there was an explosion of books on patterns for object-\n\noriented programming. The most famous of these is the “gang of four”\n\nbook, Design Patterns: Elements of Reusable Object-Oriented\n\nProgramming by Erich Gamma et al. (Addison-Wesley Professional).\n\nDesign Patterns gave a common language and framework to the task of\n\nprogramming. It described a series of interface-based patterns that could be\n\nreused in a variety of contexts. Because of advances in object-oriented\n\nprogramming and specifically interfaces, these patterns could also be\n\nimplemented as generic reusable libraries. These libraries could be written\n\nonce by a community of developers and reused repeatedly, saving time and\n\nimproving reliability.\n\nThe Rise of Open Source Software\n\nThough the concept of developers sharing source code has been around\n\nnearly since the beginning of computing, and formal free software\n\norganizations have been in existence since the mid-1980s, the very late\n\n1990s and the 2000s saw a dramatic increase in the development and\n\ndistribution of open source software. Though open source is only\n\ntangentially related to the development of patterns for distributed systems, it\n\nis important in the sense that it was through the open source communities\n\nthat it became increasingly clear that software development in general and\n\ndistributed systems development in particular are community endeavors. It\n\nis important to note that all of the container technology that forms the\n\nfoundation of the patterns described in this book has been developed and\n\nreleased as open source software. The value of patterns for both describing\n\nand improving the practice of distributed development is especially clear\n\nwhen you look at it from this community perspective.\n\nNOTE\n\nWhat is a pattern for a distributed system? There are plenty of instructions out there that will tell you\n\nhow to install specific distributed systems (such as a NoSQL database). Likewise, there are recipes\n\nfor a specific collection of systems (like a MEAN stack). But when I speak of patterns, I’m referring\n\nto general blueprints for organizing distributed systems, without mandating any specific technology\n\nor application choices. The purpose of a pattern is to provide general advice or structure to guide\n\nyour design. The hope is that such patterns will guide your thinking and also be generally applicable\n\nto a wide variety of applications and environments.\n\nThe Value of Patterns, Practices, and\n\nComponents\n\nBefore spending any of your valuable time reading about a series of\n\npatterns that I claim will improve your development practices, teach you\n\nnew skills, and—let’s face it—change your life, it’s reasonable to ask:\n\n“Why?” What is it about the design patterns and practices that can change\n\nthe way that we design and build software? In this section, I’ll lay out the\n\nreasons I think this is an important topic, and hopefully convince you to\n\nstick with me for the rest of the book.\n\nStanding on the Shoulders of Giants\n\nAs a starting point, the value that patterns for distributed systems offer is\n\nthe opportunity to figuratively stand on the shoulders of giants. It’s rarely\n\nthe case that the problems we solve or the systems we build are truly\n\nunique. Ultimately, the combination of pieces that we put together and the\n\noverall business model that the software enables may be something that the\n\nworld has never seen before. But the way the system is built and the\n\nproblems it encounters as it aspires to be reliable, agile, and scalable are not\n\nnew.\n\nThis, then, is the first value of patterns: they allow us to learn from the\n\nmistakes of others. Perhaps you have never built a distributed system\n\nbefore, or perhaps you have never built this type of distributed system.\n\nRather than hoping that a colleague has some experience in this area or\n\nlearning by making the same mistakes that others have already made, you\n\ncan turn to patterns as your guide. Learning about patterns for distributed\n\nsystem development is the same as learning about any other best practice in\n\ncomputer programming. It accelerates your ability to build software without\n\nrequiring that you have direct experience with the systems, mistakes, and\n\nfirsthand learning that led to the codification of the pattern in the first place.\n\nA Shared Language for Discussing Our Practice\n\nLearning about and accelerating our understanding of distributed systems is\n\nonly the first value of having a shared set of patterns. Patterns have value\n\neven for experienced distributed system developers who already understand\n\nthem well. Patterns provide a shared vocabulary that enables us to\n\nunderstand each other quickly. This understanding forms the basis for\n\nknowledge sharing and further learning.\n\nTo better understand this, imagine that we both are using the same object to\n\nbuild our house. I call that object a “Foo” while you call that object a “Bar.”\n\nHow long will we spend arguing about the value of a Foo versus that of a\n\nBar, or trying to explain the differing properties of Foo and Bar until we\n\nfigure out that we’re speaking about the same object? Only once we\n\ndetermine that Foo and Bar are the same can we truly start learning from\n\neach other’s experience.\n\nWithout a common vocabulary, we waste time in arguments of “violent\n\nagreement” or in explaining concepts that others understand but know by\n\nanother name. Consequently, another significant value of patterns is to\n\nprovide a common set of names and definitions so that we don’t waste time\n\nworrying about naming, and instead get right down to discussing the details\n\nand implementation of the core concepts.\n\nI have seen this happen in my short time working on containers. Along the\n\nway, the notion of a sidecar container (described in Chapter 3 of this book)\n\ntook hold within the container community. Because of this, we no longer\n\nhave to spend time defining what it means to be a sidecar and can instead\n\njump immediately to how the concept can be used to solve a particular\n\nproblem. “If we just use a sidecar” … “Yeah, and I know just the container\n\nwe can use for that.” This example leads to the third value of patterns: the\n\nconstruction of reusable components.\n\nShared Components for Easy Reuse\n\nBeyond enabling people to learn from others and providing a shared\n\nvocabulary for discussing the art of building systems, patterns provide\n\nanother important tool for computer programming: the ability to identify\n\ncommon components that can be implemented once.\n\nIf we had to create all of the code that our programs use ourselves, we\n\nwould never get done. Indeed, we would barely get started. Today, every\n\nsystem ever written stands on the shoulders of thousands if not hundreds of\n\nthousands of years of human effort. Code for operating systems, printer\n\ndrivers, distributed databases, container runtimes, and container\n\norchestrators—indeed, the entirety of applications that we build today are\n\nbuilt with reusable shared libraries and components.\n\nPatterns are the basis for the definition and development of such reusable\n\ncomponents. The formalization of algorithms led to reusable\n\nimplementations of sorting and other canonical algorithms. The\n\nidentification of interface-based patterns gave rise to a collection of generic\n\nobject-oriented libraries implementing those patterns.\n\nIdentifying core patterns for distributed systems enables us to build shared\n\ncommon components. Implementing these patterns as container images\n\nwith HTTP-based interfaces means they can be reused across many\n\ndifferent programming languages. And, of course, building reusable\n\ncomponents improves the quality of each component because the shared\n\ncodebase gets sufficient usage to identify bugs and weaknesses, and\n\nsufficient attention to ensure that they get fixed.\n\nMore recently, a series of software supply chain attacks has made\n\ndependencies and dependency management a critical part of securing our\n\napplications. In the context of a secure software supply chain, these shared\n\ncomponents take on even more importance. Every library or application that\n\nwe use brings in more dependencies—and consequently, more risk. Relying\n\non a single shared implementation of a core idea reduces the total amount of\n\nsoftware that the world needs to depend on, and by focusing attention on a\n\nfew dependencies, significantly improves the chances that they are properly\n\nmaintained and protected from software supply chain attacks.\n\nSummary\n\nDistributed systems are required to implement the level of reliability,\n\nagility, and scale expected of modern computer programs. Distributed\n\nsystem design continues to be more of a black art practiced by wizards than\n\na science applied by laypeople. The identification of common patterns and\n\npractices has regularized and improved the practice of algorithmic\n\ndevelopment and object-oriented programming. It is this book’s goal to do\n\nthe same for distributed systems. Enjoy!\n\nOceanofPDF.com\n\nChapter 2. Important Distributed System\n\nConcepts\n\nBefore we get too deep into the design of distributed systems, it is valuable\n\nto ground ourselves in some key concepts that underpin the development of\n\nthese distributed systems. If you are already experienced in distributed\n\nsystems or systems design in general, some of these ideas may be familiar\n\nto you, and you may want to skip ahead, but hopefully at the completion of\n\nthis chapter, you will have a core foundation for the patterns and designs\n\npresented throughout the rest of the book.\n\nAPIs and RPCs\n\nApplication programming interfaces, or APIs, form the core of any\n\ndistributed system. You may have seen APIs in the context of programming\n\nlanguage libraries or calls to operating system APIs. In distributed systems\n\nthe idea is similar, but the API calls take place over the network between\n\nremote services. These calls are referred to as “RPCs” or “remote procedure\n\ncalls.” RPCs rely on an underlying network transport, commonly, though\n\nnot exclusively, the HTTP protocol and the JSON object format. Numerous\n\nother more structured protocols have existed over the years, from CORBA\n\nto gRPC as well as database-specific protocols. Though such systems\n\nprovide capabilities beyond HTTP + JSON (also referred to as REST or",
      "page_number": 20
    },
    {
      "number": 2,
      "title": "Important Distributed System",
      "start_page": 32,
      "end_page": 50,
      "detection_method": "regex_chapter_title",
      "content": "RESTful), these continue to be the most popular because of their wide\n\nsupport, simplicity, and ease of integration with web-based frontends.\n\nAPIs can be synchronous or asynchronous. Synchronous APIs return only\n\nafter the request has been fully processed, while asynchronous APIs accept\n\nthe request and return an operation ID for a background process that can be\n\nmonitored via subsequent status requests. Synchronous APIs are the easiest\n\nto implement, but long-running operations are a bad match for HTTP or\n\nother network protocols because of timeouts and other limitations. Thus, if\n\nthe length of your operation is measured in minutes, it is better off being\n\nmodeled as an asynchronous operation.\n\nAPIs are generally not very useful without clients that can call those APIs.\n\nHaving every developer hand-roll HTTP calls to your API is a recipe for\n\nunhappy developers and badly designed systems. In smaller systems, which\n\nmay only be required to support individual languages, it is possible to\n\nhandwrite these client libraries. But larger systems rely on IDLs, or\n\ninterface description languages, such as OpenAPI, which can describe your\n\nAPI’s structure in such a way that clients can be automatically generated for\n\nnearly any programming language.\n\nBeyond the structure of your API, the core functionality of an API is\n\ndescribed in terms of its SLO, its service level objectives, which describe\n\nhow the API performs in terms of latency and reliability. Having formal\n\nSLOs for latency, reliability, and throughput (requests per second) enables\n\nclients to successfully develop distributed systems.\n\nLatency\n\nOne of the most important measures of your system’s performance is the\n\ntime that it takes to do something. This measure is commonly referred to as\n\nlatency. Generally speaking, latency is measured in milliseconds, though\n\ndepending on your system, it could be measured in microseconds (very fast)\n\nor seconds (very slow). When measuring latency, it is important to ensure\n\nthat it is actually representative of end-user experience (see “Percentiles”)\n\nand that it is attributable, meaning you understand where the latency is\n\ncoming from. Without proper attribution, you may understand that your\n\nsystem is slow, but you will have little idea about how to make it better.\n\nReliability\n\nIn addition to latency, reliability is obviously a key metric for any\n\ndistributed system. At its simplest, reliability is just the fraction of\n\nsuccessful requests relative to the total number of requests. But as with all\n\nthings engineering, in the real world, the devil is in the details. Consider\n\nrequests that are based on the HTTP(S) protocol. Using this protocol, you\n\ncould measure the number of requests failed (i.e., requests that didn’t\n\nactually complete the HTTP protocol). However, for any reasonable web\n\nservice, this number rapidly trends to zero. The reason for this is that HTTP\n\nitself has rich error semantics in the form of the HTTP response code. Thus,\n\nnearly every HTTP request succeeds at the protocol level, but often the\n\nresponse code indicates an error. From this, it is clear that our measure of\n\nreliability should not be at the HTTP protocol level, but rather at the HTTP\n\nresponse code level. However, HTTP defines a large number of response\n\ncodes. Which should we treat as errors? It’s fairly obvious that the 50x\n\nseries of codes are true errors. Most of them represent either internal server\n\nerrors (500) or network transmission errors (e.g., 502). Similarly it is fairly\n\nclear that the 20x series codes are successful requests. Unfortunately, in\n\nbetween 20x and 50x is a gray area. Typically we can consider the 30x\n\n(mostly redirects) status codes to be “successful,” though an excessive\n\nnumber of redirects can be an error in some conditions. Most of the\n\nuncertainty lies in the 40x collection of errors. The 40x collection of status\n\ncodes are the land of ambiguity. Is that 404 (not found) an indication that a\n\nclient simply made a mistake? Or is your service somehow misconfigured\n\nto look for files in the wrong location? Similarly, it may seem obvious that\n\n403 (unauthorized) represents a “client error,” but what if your\n\nauthorization service is having an outage?\n\nUltimately, though the examples above are drawn from the HTTP protocol,\n\nthe important takeaway is that measuring reliability is both critical and\n\ncomplicated. In practice, treating 404 (not found) as a client error can mask\n\ntrue reliability problems and prevent you from alerting when there is a\n\ncustomer-facing outage in your system. At the same time, a spike in 404\n\nerrors can also be from a single misconfigured client, causing noise and\n\nfalse alerts, which will plague your on-call engineers. In this gray area,\n\nproper monitoring will be application dependent. Further AI-based\n\noperations and anomaly detection become critical in differentiating between\n\ntrue problems and false alerts.\n\nPercentiles\n\nThroughout this book there are discussions of percentiles. The 99th\n\npercentile latency, for example. Percentiles are key to understanding\n\ndistributed systems because distributed systems are inherently about serving\n\nlarge numbers of users and large numbers of requests. Of course, in serving\n\nlots of users, we want to understand the operation of our services. By\n\ndefault it is easy to use the average as the way in which we measure user\n\nexperience. It makes sense. We want to understand the experience of the\n\naverage user, so it makes sense to look at the average experience of our\n\nvarious metrics. Unfortunately, averages are often really distorted in\n\nconsidering the experience of a system. There are several reasons for this,\n\nbut the two largest have to do with the calculation of the average as well as\n\nthe ways in which individual metrics (e.g., request latency) come together\n\nin an end-to-end user experience. When you consider the arithmetic\n\naverage, it is heavily skewed by either large or small outliers. Consider\n\nmeasuring latency, a relatively small number of users with very fast latency\n\nwill distort the average latency of your system and make you believe that\n\nyour “average” performance is much better than it actually is.\n\nThe other reason that the average is problematic is that it considers\n\nindividual requests separately; however, most user experiences are the\n\ncombination of multiple requests which must succeed. The combining of all\n\nof these experiences means that almost no one actually has an “average”\n\nexperience in terms of latency; instead, the probability that every user has at\n\nleast one exceptionally slow request rapidly approaches 100% as the\n\nnumber of requests in an end-to-end experience increases.\n\nFor both of these reasons, to truly understand the behavior of your system,\n\nit is far more useful to consider the 90th or 99th percentile of your user\n\nexperience. These percentiles represent the place in the distribution of\n\nlatency where 90% or 99% of your users experience lower latency. For\n\nexample, if your 99th percentile latency is one second, then 99% of your\n\nusers have latency less than one second. This gives you a much better sense\n\nfor the operation of your service, but importantly, even the 99th percentile\n\nsuffers from the problems of multiple requests in a single end-to-end\n\nexperience. Taken together, the 99th percentile latency of a single request\n\nrapidly becomes the 90th (or even 50th) percentile of an end-to-end\n\nexperience with enough requests in that experience. Because of this, it is\n\nalways worth understanding exactly which experiences you are measuring.\n\nIdempotency\n\nDesigning distributed systems is, in many ways, the study of, and\n\nanticipation of, how systems can fail. Of course, we don’t want our systems\n\nto fail, and thus, in order to compensate for when the underlying systems do\n\nfail, we introduce ideas that make it easier for us to restore our services. The\n\nfirst of these is the idea of idempotency. Idempotency is the notion that\n\nwhether an action is performed a single time or many times, the result is\n\nidentical. An example of an idempotent operation is the statement X = 5. No\n\nmatter how many times you execute that operation, the result is the same. X\n\nalways is equal to 5. Compare that with the statement X = X + 1. This\n\noperation is not idempotent. Each time it’s executed, the value of X\n\nincrements by one. It is clear that the eventual value of X is dependent on\n\nthe number of times that instruction is executed. If you are familiar with the\n\nideas of infrastructure as code and declarative configuration, idempotency\n\nmaps directly to declarative versus imperative configuration.\n\nIdempotency is an important concept for distributed systems because it\n\nmakes error and failure handling significantly easier. If we know that an\n\noperation is idempotent, in the face of failures, we can simply repeat it until\n\nit succeeds. No matter how many times it takes to get a successful outcome,\n\nwe know that that outcome will always be the same. Idempotent operations\n\nmake designing reliable systems easier.\n\nDelivery Semantics\n\nThough idempotency allows us to ignore the number of times a particular\n\nrequest has been made, sometimes it is necessary to actually control the\n\nnumber of times a particular request has been made in the presence of error\n\nconditions.\n\nGenerally, delivery of a message (or the success of a request) is divided into\n\ntwo categories. The first is “at least once” delivery. As its name suggests, at\n\nleast once ensures that a message or a request will be delivered at least once\n\nto the recipient. The “at least” part is the important part, because with at\n\nleast once delivery there are no guarantees about how many times a\n\nmessage will be received.\n\nIn contrast to “at least once” delivery, the alternative is “at most once”\n\ndelivery, which ensures that a message can never be delivered more than\n\nonce. As with at least once, the important part of this is the “at most”\n\nphrase, because with at most once delivery it is perfectly acceptable for a\n\nmessage to never be delivered. A good example of where you might want at\n\nmost once delivery is the charging of a credit card. Clearly multiple\n\nidentical charges to the same card are not allowable; “at most once”\n\ndelivery assumes that in the presence of failure (e.g., the absence of\n\npayment) someone will try again.\n\nYou may notice that “exactly once” delivery is missing from the options\n\ndescribed. This is because, in practice, exactly once delivery is\n\nexceptionally hard to achieve. You are far better designing systems which\n\ncan handle either at most or at least once delivery.\n\nRelational Integrity\n\nThe reason that message delivery semantics become so important is that\n\nthey ultimately affect the data that is the core of our application. If the data\n\nis your bank account, the integrity (e.g., reliability) of that data is clearly a\n\nvery important concern. Ensuring the reliability of a single data entry (e.g.,\n\nthe balance in your bank account) is important, but relational integrity is an\n\nequally important concern for distributed systems. Relational integrity\n\nrefers to the integrity or accuracy of data stored in multiple different rows\n\nor stores within a system. For example, your bank balance might be stored\n\nas a pair of (user-id, balance) , and your address might be stored\n\nin a different store as (user-id, address) . The integrity of that data\n\nrefers to whether there are strong guarantees that for every user-id in\n\nthe balances table, there is a corresponding entry in the addresses table. If\n\nthe code in your application relies on there being consistency between\n\nvarious data stores, then your code relies on strong relational integrity. If\n\ninstead your code can handle cases in which data in multiple stores may not\n\nbe fully in sync, your code does not rely on relational integrity. The reason\n\nthat this becomes important is that maintaining relational integrity requires\n\ncarefully synchronizing changes to multiple data stores. This\n\nsynchronization has significant performance and reliability implications. In\n\nparticular, it requires distributed transactions across multiple different data\n\nstores, which requires distributed locks that make your system more\n\ncomplicated and less parallel. Relational integrity can generally be seen as\n\nthe distinction between SQL and NoSQL data stores. Traditional databases\n\nand SQL go to great pains to ensure relational consistency between various\n\ntables, including transactions and rollback semantics. NoSQL data stores\n\nlike Azure’s CosmosDB and the open source Cassandra database favor\n\nmore relaxed relational semantics, where everything is a key/value store\n\nand consistency is a problem for application code. In general, relational\n\nconsistency can be seen as a trade-off between performance and code\n\ncomplexity. If you don’t need performance, the consistency offered by a\n\ntraditional SQL database can make your code much easier, while looser\n\napproaches facilitate increased parallelism and corresponding performance.\n\nData Consistency\n\nBeyond relational consistency, there is another related, important dimension\n\nof consistency to consider, and that is data consistency. In any distributed\n\nsystem, data is replicated to ensure the availability of data in the face of\n\nerrors. This data replication has performance implications, and it is\n\nimportant to decide whether your data requires strong or eventual\n\nconsistency.\n\nClearly, consistency, the notion that the data written is the same in all\n\nlocations, is a critically important part of data storage. But how quickly\n\nmust that data be the same? That is what we are considering when we talk\n\nabout data consistency. In a strongly consistent system, the data is\n\nguaranteed to be present everywhere before the writing of the data is\n\nconsidered to be “complete.” In an eventually consistent system, the data is\n\nguaranteed to be the same everywhere “eventually,” where eventually can\n\nactually mean a long-ish time.\n\nTo make this a little more concrete, consider a system which replicates its\n\ndata to all of the data centers it maintains in the world. In a strongly\n\nconsistent system, a user requests a write of some data item, and that data is\n\nimmediately written to all of the different data centers. Until all of these\n\ndata centers successfully acknowledge that the write has completed, the\n\nuser request to write is not considered successful. At first blush, this seems\n\nlike the right approach—until the data is everywhere, the user should not be\n\ntold that their request is successful. Unfortunately, the result of this is\n\nsignificantly decreased performance. As the number of data centers grows,\n\nthe number of required writes also grows, up until a point where every\n\nrequest is basically guaranteed to hit a data center which is running slowly\n\nor even failing. Thus, while strong consistency ensures data reliability, it\n\nsignificantly reduces write throughput.\n\nIn the previous example, an eventually consistent system might return as\n\nsoon as a single data center has acknowledged the write. Replication to all\n\nof the data centers is performed by an asynchronous background process,\n\nand the user doesn’t have to wait around for it to complete. The system says\n\n“I got this,” and the user trusts the system to eventually make the data\n\nconsistent. Waiting only for a single write makes the system both more\n\nreliable and more performant. You are significantly less likely to hit a slow\n\nor failing data center with a single write.\n\nUnfortunately, in many use cases eventual consistency gives the impression\n\nto the end user that the system is less reliable. The reason for this is the\n\n“read your own writes” problem. Distributed systems replicate themselves\n\nacross multiple data centers for reliability and performance. Most of the\n\ntime, a single user’s requests will all go to the same data center, but\n\noccasionally, under load, failures, or other specific network conditions,\n\ndifferent requests from the same user will go to different data centers. In an\n\neventually consistent system, this means that after a user writes something\n\nto the system, a subsequent read that is load balanced to a different data\n\ncenter may not see the recent write. This can be quite distressing to a user.\n\nImagine if you ordered something on a retail website, clicked on “view my\n\norders,” and couldn’t see your order in the returned list. It’s highly likely\n\nthat you would never order from that site again.\n\nBecause of these complications, the choice between strong consistency and\n\neventual consistency is never a very clean one. It depends on the use case as\n\nwell as the performance requirements of the system. Be careful when\n\ndeciding which approach to take with your system, though. The decision to\n\nuse strong or eventual consistency will permeate your system design and\n\nyour application’s code. Once written, it is very hard to change the storage\n\nmodel that is used by a distributed system, and the implications of the\n\ndecision often take years to show up.\n\nThe trade-offs above between consistency, availability, and partition\n\ntolerance is known as the CAP theorem, and it states that you can build a\n\nsystem with two of the three attributes, e.g., a system that is consistent and\n\navailable but not partition tolerant, but you can’t build a system with all\n\nthree. It’s worth noting that the CAP theorem is not a draconian rule. You\n\nmay not be able to build a system that guarantees all three, but it is possible\n\nto build a system where each are reasonable for your application. For some\n\napplications, 99% availability is sufficient, and thus, you can use that trade-\n\noff to ensure the consistency and partition tolerance you need as well.\n\nEffectively, the CAP theorem says that whenever you are optimizing one of\n\nthe system characteristics, you are inherently trading off against other goals.\n\nOrchestration and Kubernetes\n\nDistributed systems defined today are not deployed in a vacuum or using\n\nonly code written by the system’s developers. Instead, the ubiquity of\n\ndistributed systems today has meant that shared infrastructure has been built\n\nto support the deployment and operation of distributed systems. This\n\ninfrastructure is commonly referred to as an orchestrator, because it\n\norchestrates the operation of the distributed system without implementing\n\nthe details of the systems logic. The most common orchestrator is the open\n\nsource Kubernetes system for orchestrating deployments of containerized\n\napplications. The orchestrator is responsible for taking the desired state of\n\nthe application (e.g., I want three replicas of my frontend and five replicas\n\nof my backend) and ensuring that the real state of the system matches the\n\ndesired state that has been declared.\n\nHealth Checks\n\nTo achieve successful orchestration, the orchestration system needs to\n\nunderstand the health of the pieces that make up the distributed system.\n\nHealth is used to understand when an application is ready to be added to a\n\nload balancer, when an application is unhealthy and needs to be restarted, or\n\nwhen a rollout should be halted because the health of the application is\n\nimpacted. While there are some basic aspects of health (e.g., is the program\n\nrunning) that the orchestrator can observe from outside the system, true\n\nunderstanding of your application requires that you implement health\n\nchecks that are specific to your application. When providing health\n\ninformation to the orchestrator, there are two different notions of\n\napplication health. The first is the simplest to understand, in that it simply\n\nrepresents whether or not your application is alive. Applications which are\n\nno longer responding to a liveness check should be terminated. And if a\n\nrollout is causing a significant degradation in the number of alive\n\napplications, that rollout should be halted. The second notion of health is\n\nreadiness. Readiness indicates that an application is ready to be used. At\n\nfirst blush, readiness and liveness would seem to be the same thing, but in\n\nmany circumstances an application can be alive but not ready. The easiest\n\nexample to understand is when an application has just started up and is\n\ninitializing its state, perhaps by downloading and caching large files over\n\nthe network. Such an application is alive, it is doing useful work and should\n\nnot be terminated, but it is not ready until it has successfully downloaded\n\nand made ready to serve all of the files in its cache. By properly\n\nimplementing both liveness and readiness checks, you can ensure that your\n\ndistributed systems interact well with the orchestrator on which they run.\n\nSummary\n\nThis chapter has described a number of key concepts that are foundational\n\nto the design and understanding of distributed systems. These concepts\n\nform the basis for both understanding the requirements and describing the\n\ncharacteristics of the systems that we build. Before going further into the\n\nbook, make sure that they all feel intuitive and familiar to you.\n\nOceanofPDF.com\n\nPart II. Single-Node Patterns\n\nThis book concerns itself with distributed systems, which are applications\n\nmade up of many different components running on many different\n\nmachines. However, the first section of this book is devoted to patterns that\n\nexist on a single node. The motivation for this is straightforward.\n\nContainers are the foundational building block for the patterns in this book,\n\nbut in the end, groups of containers colocated on a single machine make up\n\nthe atomic elements of distributed system patterns.\n\nThough it is clear as to why you might want to break your distributed\n\napplication into a collection of different containers running on different\n\nmachines, it is perhaps somewhat less clear as to why you might also want\n\nto break up the components running on a single machine into different\n\ncontainers. To understand the motivation for these groups of containers, it is\n\nworth considering the goals behind containerization. In general, the goal of\n\na container is to establish boundaries around specific resources (e.g., this\n\napplication needs two cores and 8 GB of memory). Likewise, the boundary\n\ndelineates team ownership (e.g., this team owns this image). Finally, the\n\nboundary is intended to provide separation of concerns (e.g., this image\n\ndoes this one thing).\n\nAll of these reasons provide motivation for splitting up an application on a\n\nsingle machine into a group of containers. Consider resource isolation first.\n\nYour application may be made up of two components: one is a user-facing\n\napplication server and the other is a background configuration file loader.\n\nClearly, end-user-facing request latency is the highest priority, so the user-\n\nfacing application needs to have sufficient resources to ensure that it is\n\nhighly responsive. On the other hand, the background configuration loader\n\nis mostly a best-effort service; if it is delayed slightly during times of high\n\nuser-request volume, the system will be OK. Likewise, the background\n\nconfiguration loader should not impact the quality of service that end users\n\nreceive. For all of these reasons, you want to separate the user-facing\n\nservice and the background shard loader into different containers. This\n\nallows you to attach different resource requirements and priorities to the\n\ntwo different containers and, for example, ensure that the background\n\nloader opportunistically steals cycles from the user-facing service whenever\n\nit is lightly loaded and the cycles are free. Likewise, separate resource\n\nrequirements for the two containers ensure that the background loader will\n\nbe terminated before the user-facing service if there is a resource contention\n\nissue caused by a memory leak or other overcommitment of memory\n\nresources.\n\nIn addition to this resource isolation, there are other reasons to split your\n\nsingle-node application into multiple containers. Consider the task of\n\nscaling a team. There is good reason to believe that the ideal team size is six\n\nto eight people. In order to structure teams in this manner and yet still build\n\nsignificant systems, we need to have small, focused pieces for each team to\n\nown. Additionally, often some of the components, if factored properly, are\n\nreusable modules that can be used by many teams.\n\nConsider, for example, the task of keeping a local filesystem synchronized\n\nwith a Git source code repository. If you build this Git sync tool as a\n\nseparate container, you can reuse it with PHP, HTML, JavaScript, Python,\n\nand numerous other web-serving environments. If you instead factor each\n\nenvironment as a single container where, for example, the Python runtime\n\nand the Git synchronization are inextricably bound, then this sort of\n\nmodular reuse (and the corresponding small team that owns that reusable\n\nmodule) is impossible.\n\nFinally, even if your application is small and all of your containers are\n\nowned by a single team, the separation of concerns ensures that your\n\napplication is well understood and can easily be tested, updated, and\n\ndeployed. Small, focused applications are easier to understand and have\n\nfewer couplings to other systems. This means, for example, that you can\n\ndeploy the Git synchronization container without having to also redeploy\n\nyour application server. This leads to rollouts with fewer dependencies and\n\nsmaller scope. That, in turn, leads to more reliable rollouts (and rollbacks),\n\nwhich leads to greater agility and flexibility when deploying your\n\napplication.\n\nI hope that all of these examples have motivated you to think about\n\nbreaking up your applications, even those on a single node, into multiple\n\n1\n\ncontainers. The following chapters describe some patterns that can help\n\nguide you as you build modular groups of containers. In contrast to later\n\npatterns we’ll discuss, all of the patterns in this section assume tight\n\ndependencies between all containers. In particular, they assume that all of\n\nthe containers in the pattern can be reliably coscheduled onto a single\n\nmachine.\n\nThey also assume that all of the containers in the pattern can optionally\n\nshare volumes or parts of their filesystems as well as other key container\n\nresources like network namespaces and shared memory. This tight grouping\n\n1\n\nis called a pod in Kubernetes, but the concept is generally applicable to\n\ndifferent container orchestrators, though some support it more natively than\n\nothers.\n\nKubernetes is an open source system for automating deployment, scaling, and management of\n\ncontainerized applications. Check out my book, Kubernetes: Up and Running (O’Reilly).\n\nOceanofPDF.com\n\nChapter 3. The Sidecar Pattern\n\nThe first single-node pattern is the sidecar pattern. The sidecar pattern is a\n\nsingle-node pattern made up of two containers. The first is the application\n\ncontainer. It contains the core logic for the application. Without this\n\ncontainer, the application would not exist. In addition to the application\n\ncontainer, there is a sidecar container. The role of the sidecar is to augment\n\nand improve the application container, often without the application\n\ncontainer’s knowledge. In its simplest form, a sidecar container can be used\n\nto add functionality to a container that might otherwise be difficult to\n\nimprove. Sidecar containers are coscheduled onto the same machine via an\n\natomic container group, such as the pod API object in Kubernetes. In\n\naddition to being scheduled on the same machine, the application container\n\nand sidecar container share a number of resources, including parts of the\n\nfilesystem, hostname, and network, and many other namespaces. A generic\n\nimage of this sidecar pattern is shown in Figure 3-1.",
      "page_number": 32
    },
    {
      "number": 3,
      "title": "The Sidecar Pattern",
      "start_page": 51,
      "end_page": 70,
      "detection_method": "regex_chapter_title",
      "content": "Figure 3-1. The generic sidecar pattern\n\nAn Example Sidecar: Adding HTTPS to\n\na Legacy Service\n\nConsider, for example, a legacy web service. Years ago, when it was built,\n\ninternal network security was not as high a priority for the company, and\n\nthus, the application only services requests over unencrypted HTTP, not\n\nHTTPS. Due to recent security incidents, the company has mandated the\n\nuse of HTTPS for all company websites. To compound the misery of the\n\nteam sent to update this particular web service, the source code for this\n\napplication was built with an old version of the company’s build system,\n\nwhich no longer functions.\n\nContainerizing this HTTP application is simple enough: the binary can run\n\nin a container with a version of an old Linux distribution on top of a more\n\nmodern kernel being run by the team’s container orchestrator. However, the\n\ntask of adding HTTPS to this application is significantly more challenging.\n\nThe team is trying to decide between resurrecting the old build system\n\nversus porting the application’s source code to the new build system, when\n\none of the team members suggests that they use the sidecar pattern to\n\nresolve the situation more easily.\n\nThe application of the sidecar pattern to this situation is straightforward.\n\nThe legacy web service can be configured to serve exclusively on localhost\n\n(127.0.0.1), which means that only services that share the local network\n\nwith the server will be able to access the service. Normally, this wouldn’t be\n\na practical choice because it would mean that no one could access the web\n\nservice. However, using the sidecar pattern, in addition to the legacy\n\ncontainer, we will add an nginx sidecar container. This nginx\n\ncontainer lives in the same network namespace as the legacy web\n\napplication, so it can access the service that is running on localhost.\n\nAt the same time, this nginx service can terminate HTTPS traffic on the\n\nexternal IP address of the pod and proxy that traffic to the legacy web\n\napplication (see Figure 3-2). Since this unencrypted traffic is only sent via\n\nthe local loopback adapter inside the container group, the network security\n\nteam is satisfied that the data is safe. Likewise, by using the sidecar pattern,\n\nthe team has modernized a legacy application without having to figure out\n\nhow to rebuild a new application to serve HTTPS. A similar form of this\n\npattern can also be used to add automatic certificate rotation, or even\n\nauthentication and authorization to legacy web applications that may not be\n\neasy to modify.\n\nFigure 3-2. The HTTPS sidecar\n\nDynamic Configuration with Sidecars\n\nSimply proxying traffic into an existing application is not the only use for a\n\nsidecar. Another common example is configuration synchronization. Many\n\nolder applications use a configuration file for parameterizing the\n\napplication; this may be a raw text file or something more structured like\n\nTOML, XML, JSON, or YAML. Many preexisting applications were\n\nwritten to assume that this file was present on the filesystem and read their\n\nconfiguration from there. However, in a cloud native environment it is often\n\nquite useful to use an API for updating configuration stored elsewhere. This\n\nallows you to do a dynamic push of configuration information via an API\n\ninstead of manually logging in to every server and updating the\n\nconfiguration file using imperative commands. The desire for such an API\n\nis driven both by ease of use as well as the ability to add automation like\n\nrollback, which makes configuring (and reconfiguring) safer and easier.\n\nKubernetes supports ConfigMap resources for exactly this purpose.\n\nSimilar to the case of HTTPS, new applications can be written with the\n\nexpectation that configuration is a dynamic property that should be obtained\n\nusing a cloud API, but adapting and updating an existing application can be\n\nsignificantly more challenging. Fortunately, the sidecar pattern again can be\n\nused to provide new functionality that augments a legacy application\n\nwithout changing the existing application. For the sidecar pattern shown in\n\nFigure 3-3, there again are two containers: the container that is the serving\n\napplication and the container that is the configuration manager. The two\n\ncontainers are grouped together into a pod, where they share a directory\n\nbetween themselves. This shared directory is where the configuration file is\n\nmaintained.\n\nFigure 3-3. A sidecar example of managing a dynamic configuration\n\nWhen the legacy application starts, it loads its configuration from the\n\nfilesystem, as expected. This configuration is placed into the filesystem\n\nusing a ConfigMap volume which places the current contents of a\n\nConfigMap resource at a particular location in the filesystem. When the\n\nconfiguration manager starts, it examines the configuration API and looks\n\nfor differences between the local filesystem and the configuration stored in\n\nthe API. If there are differences, the configuration manager downloads the\n\nnew configuration to the local filesystem and signals to the legacy\n\napplication that it should reconfigure itself with this new configuration. The\n\nactual mechanism for this notification varies by application. Some\n\napplications actually watch the configuration file for changes, while others\n\nrespond to a SIGHUP signal. In extreme cases, the configuration manager\n\nmay send a SIGKILL signal to abort the legacy application. Once aborted,\n\nthe container orchestration system will restart the legacy application, at\n\nwhich point it will load its new configuration. As with adding HTTPS to an\n\nexisting application, this pattern illustrates how the sidecar pattern can help\n\nadapt pre-existing applications to more cloud native scenarios.\n\nModular Application Containers\n\nAt this point, you might be forgiven if you thought that the sole reason for\n\nthe sidecar pattern to exist was to adapt legacy applications where you no\n\nlonger wanted to make modifications to the original source code. While this\n\nis a common use case for the pattern, there are many other motivations for\n\ndesigning things using sidecars. One of the other main advantages of using\n\nthe sidecar pattern is modularity and reuse of the components used as\n\nsidecars. In deploying any real-world, reliable application, there is\n\nfunctionality that you need for debugging or other management of the\n\napplication, such as giving a readout of all of the different processes using\n\nresources in the container, similar to the top command-line tool.\n\nOne approach to providing this introspection is to require that each\n\ndeveloper implement an HTTP /topz interface that provides a readout of\n\nresource usage. To make this easier, you might implement this webhook as\n\na language-specific plug-in that the developer could simply link into their\n\napplication. But even if done this way, the developer would be forced to\n\nchoose to link it in, and your organization would be forced to implement the\n\ninterface for every language it wants to support. Unless done with extreme\n\nrigor, this approach is bound to lead to variations among languages as well\n\nas a lack of support for the functionality when using new languages.\n\nInstead, this topz functionality can be deployed as a sidecar container\n\nthat shares the process-ID (PID) namespace with the application container.\n\nThis topz container can introspect all running processes and provide a\n\nconsistent user interface. Moreover, you can use the orchestration system to\n\nautomatically add this container to all applications deployed via the\n\norchestration system to ensure that there is a consistent set of tools available\n\nfor all applications running in your infrastructure.\n\nOf course, as with any technical choice, there are trade-offs between this\n\nmodular container-based pattern and rolling your own code into your\n\napplication. The library-based approach is always going to be somewhat\n\nless tailored to the specifics of your application. This means that it may be\n\nless efficient in terms of performance, or that the API may require some\n\nadaptation to fit into your environment. I would compare these trade-offs to\n\nthe difference between buying off-the-rack clothing versus bespoke fashion.\n\nThe bespoke fashion will always fit you better, but it will take longer to\n\narrive and cost more to acquire. As with clothes, for most of us it makes\n\nsense to buy the more general-purpose solution when it comes to coding. Of\n\ncourse, if your application demands extremes in terms of performance, you\n\ncan always choose the handwritten solution.\n\nHands On: Deploying the topz Container\n\nTo see the topz sidecar in action, you first need to deploy some other\n\ncontainer to act as the application container. Choose an existing application\n\nthat you are running and deploy it using Docker:\n\n$ docker run -d <my-app-image>\n\n<container-hash-value>\n\nAfter you run that image, you will receive the identifier for that specific\n\ncontainer. It will look something like: cccf82b85000… If you don’t\n\nhave it, you can always look it up using the docker ps command,\n\nwhich will show all currently running containers. Assuming you have\n\nstashed that value in an environment variable named APP_ID , you can\n\nthen run the topz container in the same PID namespace using:\n\n$ docker run --pid=container:${APP_ID} \\\n\np 8080:8080 \\\n\nbrendanburns/topz:db0fa58 \\\n\n/server --addr=0.0.0.0:8080\n\nThis will launch the topz sidecar in the same PID namespace as the\n\napplication container. Note that you may need to change the port that the\n\nsidecar uses for serving if your application container is also running on port\n\n8080 . Once the sidecar is running, you can visit\n\nhttp://localhost:8080/topz to get a complete readout of the processes that\n\nare running in the application container and their resource usage.\n\nYou can mix this sidecar with any other existing container to easily get a\n\nview into how the container is using its resources via a web interface.\n\nBuilding a Simple PaaS with Sidecars\n\nThe sidecar pattern can be used for more than adaptation and monitoring. It\n\ncan also be used as a means to implement the complete logic for your\n\napplication in a simplified, modular manner. As an example, imagine\n\nbuilding a simple platform as a service (PaaS) built around the git\n\nworkflow. Once you deploy this PaaS, simply pushing new code up to a Git\n\nrepository results in that code being deployed to the running servers. We’ll\n\nsee how the sidecar pattern makes building this PaaS remarkably\n\nstraightforward.\n\nAs previously stated, in the sidecar pattern there are two containers: the\n\nmain application container and the sidecar. In our simple PaaS application,\n\nthe main container is a Node.js server that implements a web server. The\n\nNode.js server is instrumented so that it automatically reloads the server\n\nwhen new files are updated. This is accomplished with the nodemon tool.\n\nThe sidecar container shares a filesystem with the main application\n\ncontainer and runs a simple loop that synchronizes the filesystem with an\n\nexisting Git repository:\n\n#!/bin/bash\n\nwhile true; do\n\ngit pull\n\nsleep 10\n\ndone\n\nObviously, this script could be more complex, pulling from a specific\n\nbranch instead of simply from HEAD. It is left purposefully simple to\n\nimprove the readability of this example.\n\nThe Node.js application and Git synchronization sidecar are coscheduled\n\nand deployed together to implement our simple PaaS (Figure 3-4). Once\n\ndeployed, every time new code is pushed to a Git repository, the code is\n\nautomatically updated by the sidecar and reloaded by the server.\n\nFigure 3-4. A simple sidecar-based PaaS\n\nDesigning Sidecars for Modularity and\n\nReusability\n\nIn all of the examples of sidecars that we have detailed throughout this\n\nchapter, one of the most important themes is that every one was a modular,\n\nreusable artifact. To be successful, the sidecar should be reusable across a\n\nwide variety of applications and deployments. By achieving modular reuse,\n\nsidecars can dramatically speed up the building of your application.\n\nHowever, this modularity and reusability, just like achieving modularity in\n\nhigh-quality software development, requires focus and discipline. In\n\nparticular, you need to focus on developing three areas:\n\nParameterizing your containers\n\nCreating the API surface of your container\n\nDocumenting the operation of your container\n\nOnce you follow these common approaches, you will develop sidecars that\n\ncan be re-used across a wide variety of applications.\n\nParameterized Containers\n\nParameterizing your containers is the most important thing you can do to\n\nmake your containers modular and reusable regardless of whether they are\n\nsidecars or not, though sidecars and other add-on containers are especially\n\nimportant to parameterize.\n\nWhat do I mean when I say “parameterize”? Consider your container as a\n\nfunction in your program. How many parameters does it have? Each\n\nparameter represents an input that can customize a generic container to a\n\nspecific situation. Consider, for example, the SSL add-on sidecar deployed\n\npreviously. To be generally useful, it likely needs at least two parameters:\n\nthe first is the name of the certificate being used to provide SSL, and the\n\nother is the port of the “legacy” application server running on localhost.\n\nWithout these parameters, it is hard to imagine this sidecar container being\n\nusable for a broad array of applications. Similar parameters exist for all of\n\nthe other sidecars described in this chapter.\n\nNow that we know the parameters we want to expose, how do we actually\n\nexpose them to users, and how do we consume them inside the container.\n\nThere are two ways in which such parameters can be passed to your\n\ncontainer: through environment variables or the command line. Though\n\neither is feasible, I have a general preference for passing parameters via\n\nenvironment variables. An example of passing such parameters to a sidecar\n\ncontainer is:\n\ndocker run -e=PORT=<port> -d <image>\n\nOf course, delivering values into the container is only part of the battle. The\n\nother part is actually using these variables inside the container. Typically, to\n\ndo that, a simple shell script is used that loads the environment variables\n\nsupplied with the sidecar container and either adjusts the configuration files\n\nor parameterizes the underlying application.\n\nFor example, you might pass in the certificate path and port as environment\n\nvariables:\n\ndocker run -e=PROXY_PORT=8080 -e=CERTIFICATE_PATH\n\nIn your container, you would use those variables to configure an\n\nnginx.conf file that points the web server to the correct file and proxy\n\nlocation.\n\nDefine Each Container’s API\n\nGiven that you are parameterizing your containers, it is obvious that your\n\ncontainers are defining a “function” that is called whenever the container is\n\nexecuted. This function is clearly a part of the API that is defined by your\n\ncontainer, but there are other parts to this API as well, including calls that\n\nthe container will make to other services as well as traditional HTTP or\n\nother APIs that the container provides.\n\nAs you think about defining modular, reusable containers, it is important to\n\nrealize that all aspects of how your container interacts with its world are\n\npart of the API defined by that reusable container. As in the world of\n\nmicroservices, these microcontainers rely on APIs to ensure that there is a\n\nclean separation between the main application container and the sidecar.\n\nAdditionally, the API exists to ensure that all consumers of the sidecar will\n\ncontinue to work correctly as subsequent versions are released. Likewise,\n\nhaving a clean API for a sidecar enables the sidecar developer to move\n\nmore quickly since they have a clear definition (and hopefully unit tests) for\n\nthe services they provide as a part of the sidecar.\n\nTo see a concrete example of why this API surface area is important,\n\nconsider the configuration management sidecar we discussed earlier. A\n\nuseful configuration for this sidecar might be UPDATE_FREQUENCY ,\n\nwhich indicates how often the configuration should be synchronized with\n\nthe filesystem. It is clear that if, at some later time, the name of the\n\nparameter is changed to UPDATE_PERIOD , this change would be a\n\nviolation of the sidecar’s API and clearly would break it for some users.\n\nWhile that example is obvious, even more subtle changes can break the\n\nsidecar API. Imagine, for example, that UPDATE_FREQUENCY originally\n\ntook a number in seconds. Over time and with feedback from users, the\n\nsidecar developer determined that specifying seconds for large time values\n\n(e.g., minutes) was annoying users and changed the parameter to accept\n\nstrings (10 m, 5 s, etc.). Because old parameter values (e.g., 10, for 10\n\nseconds) won’t parse in this new scheme, this is a breaking API change.\n\nSuppose still that the developer anticipated this but made values without\n\nunits parse to milliseconds where they had previously parsed to seconds.\n\nEven this change, despite not leading to an error, is a breaking API change\n\nfor the sidecar since it will lead to significantly more frequent configuration\n\nchecks and correspondingly more load on the cloud configuration server.\n\nI hope this discussion has shown you that for true modularity, you need to\n\nbe very conscious of the API that your sidecar provides, and that “breaking”\n\nchanges to that API may not always be as obvious as changing the name of\n\na parameter.\n\nDocumenting Your Containers\n\nBy now, you’ve seen how you can parameterize your sidecar containers to\n\nmake them modular and reuseable. You’ve learned about the importance of\n\nmaintaining a stable API to ensure that you don’t break sidecars for your\n\nusers. But there’s one final step to building modular, reusable containers:\n\nensuring that people can use them in the first place.\n\nAs with software libraries, the key to building something truly useful is\n\nexplaining how to use it. There is little good in building a flexible, reliable\n\nmodular container if no one can figure out how to use it. Sadly, there are\n\nfew formal tools available for documenting container images, but there are\n\nsome best practices you can use to accomplish this.\n\nFor every container image, the most obvious place to look for\n\ndocumentation is the Dockerfile from which the container was built.\n\nThere are some parts of the Dockerfile that already document how the\n\ncontainer works. One example of this is the EXPOSE directive, which\n\nindicates the ports that the image listens on. Even though EXPOSE is not\n\nnecessary, it is a good practice to include it in your Dockerfile and\n\nalso to add a comment that explains what exactly is listening on that port.\n\nFor example:\n\n...\n\n# Main web server runs on port 8080\n\nEXPOSE 8080\n\n...\n\nAdditionally, if you use environment variables to parameterize your\n\ncontainer, you can use the ENV directive to set default values for those\n\nparameters as well as document their usage:\n\n...\n\n# The PROXY_PORT parameter indicates the port on # traffic to.\n\nENV PROXY_PORT 8000\n\n...\n\nFinally, you should always use the LABEL directive to add metadata for\n\nyour image; for example, the maintainer’s email address, web page, and\n\nversion of the image:\n\n...\n\nLABEL \"org.opencontainers.image.vendor\"=\"name@com\n\nLABEL \"org.opencontainers.image.url\"=\"http://imag LABEL \"org.opencontainers.image.version\"=\"1.0.3\"\n\n...\n\nThe names for these labels are drawn from the schema established by the\n\nOpen Containers Initiative. The Open Containers Initiative (OCI) is the\n\nspecification body which maintains the definition of what it means to be a\n\ncontainer image. A part of that specification is the definition of a shared set\n\nof annotations/labels. By using a common taxonomy of image labels,\n\nmultiple different tools can rely on the same meta information in order to\n\nvisualize, monitor, and correctly use an application. By adopting shared\n\nterms, you can use the set of tools developed in the community without\n\nmodifying your image. Of course, you can also add whatever additional\n\nlabels make sense in the context of your image.\n\nSummary\n\nOver the course of this chapter, we’ve introduced the sidecar pattern for\n\ncombining containers on a single machine. In the sidecar pattern, a sidecar\n\ncontainer augments and extends an application container to add\n\nfunctionality. Sidecars can be used to update existing legacy applications\n\nwhen changing the application is too costly. Likewise, they can be used to\n\ncreate modular utility containers that standardize implementations of\n\ncommon functionality. These utility containers can be reused in a large\n\nnumber of applications, increasing consistency and reducing the cost of\n\ndeveloping each application. Subsequent chapters introduce other single-\n\nnode patterns that demonstrate other uses for modular, reusable containers.\n\nOceanofPDF.com\n\nChapter 4. Ambassadors\n\nChapter 3 introduced the sidecar pattern, where one container augments a\n\npre-existing container to add functionality. This chapter introduces the\n\nambassador pattern, where an ambassador container brokers interactions\n\nbetween the application container and the rest of the world. As with other\n\nsingle-node patterns, the two containers are tightly linked in a symbiotic\n\npairing that is scheduled to a single machine. A canonical diagram of this\n\npattern is shown in Figure 4-1.\n\nFigure 4-1. Generic ambassador pattern\n\nThe value of the ambassador pattern is twofold. First, as with the other\n\nsingle-node patterns, there is inherent value in building modular, reusable\n\ncontainers. The separation of concerns makes the containers easier to build\n\nand maintain. Likewise, the ambassador container can be reused with a\n\nnumber of different application containers. This reuse speeds up application\n\ndevelopment because the container’s code can be reused in a number of",
      "page_number": 51
    },
    {
      "number": 4,
      "title": "Ambassadors",
      "start_page": 71,
      "end_page": 87,
      "detection_method": "regex_chapter_title",
      "content": "places. Additionally, the implementation is both more consistent and of a\n\nhigher quality because it is built once and used in many different contexts.\n\nThe rest of this chapter provides a number of examples of using the\n\nambassador pattern to implement a series of real-world applications.\n\nUsing an Ambassador to Shard a Service\n\nSometimes the data that you want to store in a storage layer becomes too\n\nbig for a single machine to handle. In such situations, you need to shard\n\nyour storage layer. Sharding splits up the layer into multiple disjoint pieces,\n\neach hosted by a separate machine. This chapter focuses on a single-node\n\npattern for adapting an existing service to talk to a sharded service that\n\nexists somewhere in the world. It does not discuss how the sharded service\n\ncame to exist. Sharding and a multinode sharded service design pattern are\n\ndiscussed in great detail in Chapter 7. A diagram of a sharded service is\n\nshown in Figure 4-2.\n\nFigure 4-2. A generic sharded service\n\nWhen deploying a sharded service, one question that arises is how to\n\nintegrate it with the frontend or middleware code that stores data. Clearly\n\nthere needs to be logic that routes a particular request to a particular shard,\n\nbut often it is difficult to retrofit such a sharded client into existing source\n\ncode that expects to connect to a single storage backend. Additionally,\n\nsharded services make it difficult to share configuration between\n\ndevelopment environments (where there is often only a single storage\n\nshard) and production environments (where there are often many storage\n\nshards).\n\nOne approach is to build all of the sharding logic into the sharded service\n\nitself. In this approach, the sharded service also has a stateless load balancer\n\nthat directs traffic to the appropriate shard. Effectively, this load balancer is\n\na distributed ambassador as a service. This makes a client-side ambassador\n\nunnecessary at the expense of a more complicated deployment for the\n\nsharded service. The alternative is to integrate a single-node ambassador on\n\nthe client side to route traffic to the appropriate shard. This makes\n\ndeploying the client somewhat more complicated but simplifies the\n\ndeployment of the sharded service. As is always the case with trade-offs, it\n\nis up to the particulars of your specific application to determine which\n\napproach makes the most sense. Some factors to consider include where\n\nteam lines fall in your architecture, as well as where you are writing code\n\nversus simply deploying off-the-shelf software. Ultimately, either choice is\n\nvalid. The section “Hands On: Implementing a Sharded Redis” describes\n\nhow to use the single-node ambassador pattern for client-side sharding.\n\nWhen adapting an existing application to a sharded backend, you can\n\nintroduce an ambassador container that contains all of the logic needed to\n\nroute requests to the appropriate storage shard. Thus, your frontend or\n\nmiddleware application only connects to what appears to be a single storage\n\nbackend running on localhost. However, this server is in fact actually a\n\nsharding ambassador proxy, which receives all of the requests from your\n\napplication code, sends a request to the appropriate storage shard, and then\n\nreturns the result to your application. This use of an ambassador is\n\ndiagrammed in Figure 4-3.\n\nThe net result of applying the ambassador pattern to sharded services is a\n\nseparation of concerns between the application container, which simply\n\nknows it needs to talk to a storage service and discovers that service on\n\nlocalhost, and the sharding ambassador proxy, which only contains the code\n\nnecessary to perform appropriate sharding. As with all good single-node\n\npatterns, this ambassador can be reused between many different\n\napplications. Or, as we’ll see in the following hands-on example, an off-the\n\nshelf open source implementation can be used for the ambassador, speeding\n\nup the development of the overall distributed system.\n\nHands On: Implementing a Sharded Redis\n\nRedis is a fast key-value store that can be used as a cache or for more\n\npersistent storage. In this example, we’ll be using it as a cache. We’ll begin\n\nby deploying a sharded Redis service to a Kubernetes cluster. We’ll use the\n\nStatefulSet API object to deploy it, since it will give us unique DNS\n\nnames for each shard that we can use when configuring the proxy.\n\nThe StatefulSet for Redis looks like this:\n\napiVersion: apps/v1beta1 kind: StatefulSet\n\nmetadata:\n\nname: sharded-redis\n\nspec: serviceName: \"redis\"\n\nreplicas: 3\n\ntemplate:\n\nmetadata: labels:\n\napp: redis\n\nspec: terminationGracePeriodSeconds: 10\n\ncontainers:\n\nname: redis\n\nimage: redis ports:\n\ncontainerPort: 6379\n\nname: redis\n\nSave this to a file named redis-shards.yaml, and you can deploy this with\n\nkubectl create -f redis-shards.yaml . This will create three\n\ncontainers running redis. You can see these by running kubectl get\n\npods ; you should see sharded-redis-[0,1,2] .\n\nOf course, just running the replicas isn’t sufficient; we also need names by\n\nwhich we can refer to the replicas. In this case, we’ll use a Kubernetes\n\nService , which will create DNS names for the replicas we have created.\n\nThe Service looks like this:\n\napiVersion: v1\n\nkind: Service metadata:\n\nname: redis\n\nlabels: app: redis\n\nspec:\n\nports: - port: 6379\n\nname: redis\n\nclusterIP: None\n\nselector: app: redis\n\nSave this to a file named redis-service.yaml and deploy with kubectl\n\ncreate -f redis-service.yaml . You should now have DNS\n\nentries for sharded-redis-0.redis , sharded-redis-\n\n1.redis , etc. We can use these names to configure twemproxy .\n\ntwemproxy is a lightweight, highly performant proxy for memcached\n\nand Redis , which was originally developed by Twitter and is open source\n\nand available on GitHub. We can configure twemproxy to point to the\n\nreplicas we created by using the following configuration:\n\nredis: listen: 127.0.0.1:6379\n\nhash: fnv1a_64\n\ndistribution: ketama\n\nauto_eject_hosts: true\n\nredis: true\n\ntimeout: 400\n\nserver_retry_timeout: 2000\n\nserver_failure_limit: 1\n\nservers:\n\nsharded-redis-0.redis:6379:1\n\nsharded-redis-1.redis:6379:1\n\nsharded-redis-2.redis:6379:1\n\nIn this config, you can see that we are serving the Redis protocol on\n\nlocalhost:6379 so that the application container can access the\n\nambassador. We will deploy this into our ambassador pod using a\n\nKubernetes ConfigMap object that we can create with:\n\nkubectl create configmap twem-config --from-file=\n\nFinally, all of the preparations are done and we can deploy our ambassador\n\nexample. We define a pod that looks like:\n\napiVersion: v1 kind: Pod\n\nmetadata: name: ambassador-example\n\nspec: containers: # This is where the application container wou # - name: nginx\n\n# image: nginx\n\n# This is the ambassador container - name: twemproxy\n\nimage: ganomede/twemproxy\n\ncommand:\n\n\"nutcracker\"\n\n\"-c\" - \"/etc/config/nutcracker.yaml\"\n\n\"-v\"\n\n\"7\"\n\n\"-s\"\n\n\"6222\"\n\nvolumeMounts:\n\nname: config-volume\n\nmountPath: /etc/config\n\nvolumes:\n\nname: config-volume\n\nconfigMap:\n\nname: twem-config\n\nThis pod defines the ambassador; then the specific user’s application\n\ncontainer can be injected to complete the pod.\n\nUsing an Ambassador for Service\n\nBrokering\n\nWhen trying to render an application portable across multiple environments\n\n(e.g., public cloud, physical data center, or private cloud), one of the\n\nprimary challenges is service discovery and configuration. To understand\n\nwhat this means, imagine a frontend that relies on a MySQL database to\n\nstore its data. In the public cloud, this MySQL service might be provided as\n\nsoftware as a service (SaaS), whereas in a private cloud it might be\n\nnecessary to dynamically spin up a new virtual machine or container\n\nrunning MySQL.\n\nConsequently, building a portable application requires that the application\n\nknow how to introspect its environment and find the appropriate MySQL\n\nservice to connect to. This process is called service discovery, and the\n\nsystem that performs this discovery and linking is commonly called a\n\nservice broker. As with previous examples, the ambassador pattern enables\n\na system to separate the logic of the application container from the logic of\n\nthe service broker ambassador. The application simply always connects to\n\nan instance of the service (e.g., MySQL) running on localhost. It is the\n\nresponsibility of the service broker ambassador to introspect its\n\nenvironment and broker the appropriate connection. This process is shown\n\nin Figure 4-3.\n\nFigure 4-3. A service broker ambassador creating a MySQL service\n\nUsing an Ambassador to Do\n\nExperimentation or Request Splitting\n\nA final example application of the ambassador pattern is to perform\n\nexperimentation or other forms of request splitting. In many production\n\nsystems, it is advantageous to be able to perform request splitting, where\n\nsome fraction of all requests are not serviced by the main production\n\nservice but rather are redirected to a different implementation of the service.\n\nMost often, this is used to perform experiments with new beta versions of\n\nthe service to determine if the new version of the software is reliable or\n\ncomparable in performance to the currently deployed version.\n\nAdditionally, request splitting is sometimes used to tee or split traffic\n\nsuch that all traffic goes to both the production system as well as a newer,\n\nundeployed version. The responses from the production system are returned\n\nto the user, while the responses from the tee-d service are ignored. Most\n\noften, this form of request splitting is used to simulate production load on\n\nthe new version of the service without risking impact to existing production\n\nusers.\n\nGiven the previous examples, it is straightforward to see how a request-\n\nsplitting ambassador can interact with an application container to\n\nimplement request splitting. As before, the application container simply\n\nconnects to the service on localhost, while the ambassador container\n\nreceives the requests, proxies responses to both the production and\n\nexperimental systems, and then returns the production responses back as if\n\nit had performed the work itself.\n\nThis separation of concerns keeps the code in each container slim and\n\nfocused, and the modular factoring of the application ensures that the\n\nrequest-splitting ambassador can be reused for a variety of different\n\napplications and settings.\n\nHands On: Implementing 10% Experiments\n\nTo implement our request-splitting experiment, we’re going to use the\n\nnginx web server. nginx is a powerful, richly featured open source\n\nserver. To configure nginx as the ambassador, we’ll use the following\n\nconfiguration (note that this is for HTTP but it could easily be adapted for\n\nHTTPS as well):\n\nworker_processes 5;\n\nerror_log error.log;\n\npid nginx.pid;\n\nworker_rlimit_nofile 8192;\n\nevents {\n\nworker_connections 1024;\n\n}\n\nhttp {\n\nupstream backend { ip_hash;\n\nserver web weight=9;\n\nserver experiment;\n\n}\n\nserver {\n\nlisten localhost:80;\n\nlocation / {\n\nproxy_pass http://backend;\n\n}\n\n}\n\n}\n\nNOTE\n\nAs with the previous discussion of sharded services, it’s also possible to deploy the experiment\n\nframework as a separate microservice in front of your application instead of integrating it as a part of\n\nyour client pods. Of course, by doing this you are introducing another service that needs to be\n\nmaintained, scaled, monitored, etc. If experimentation is likely to be a longstanding component in\n\nyour architecture, this might be worthwhile. If it is used more occasionally, then a client-side\n\nambassador might make more sense.\n\nYou’ll note that I’m using IP hashing in this configuration. This is important\n\nbecause it ensures that the user doesn’t flip-flop back and forth between the\n\nexperiment and the main site. This assures that every user has a consistent\n\nexperience with the application.\n\nThe weight parameter is used to send 90% of the traffic to the main\n\nexisting application, while 10% of the traffic is redirected to the\n\nexperiment.\n\nAs with other examples, we’ll deploy this configuration as a ConfigMap\n\nobject in Kubernetes:\n\nkubectl create configmap experiment-config --from\n\nOf course, this assumes that you have both a web and experiment\n\nservice defined. If you don’t, you need to create them now before you try to\n\ncreate the ambassador container, since nginx doesn’t like to start if it\n\ncan’t find the services it is proxying to. Here are some example service\n\nconfigs:\n\n# This is the 'experiment' service\n\napiVersion: v1\n\nkind: Service\n\nmetadata: name: experiment\n\nlabels:\n\napp: experiment\n\nspec:\n\nports:\n\nport: 80\n\nname: web\n\nselector:\n\n# Change this selector to match your applicat\n\napp: experiment\n\n---\n\n# This is the 'prod' service\n\napiVersion: v1 kind: Service\n\nmetadata: name: web\n\nlabels: app: web spec:\n\nports:\n\nport: 80\n\nname: web\n\nselector: # Change this selector to match your applicat\n\napp: web\n\nAnd then we will deploy nginx itself as the ambassador container within\n\na pod:\n\napiVersion: v1\n\nkind: Pod\n\nmetadata:\n\nname: experiment-example\n\nspec:\n\ncontainers:\n\n# This is where the application container wou\n\n# - name: some-name\n\n# image: some-image\n\n# This is the ambassador container\n\nname: nginx\n\nimage: nginx volumeMounts:\n\nname: config-volume mountPath: /etc/nginx volumes:\n\nname: config-volume\n\nconfigMap:\n\nname: experiment-config\n\nYou can add a second (or third, or fourth) container to the pod to take\n\nadvantage of the ambassador.\n\nSummary\n\nAmbassadors are a key way to simplify life for application developers.\n\nThey can encapsulate complex logic that is necessary for scale or reliability,\n\nsuch as sharding, and provide a simplified interface which makes such a\n\ncomplex system easy to use. For platform engineers, ambassadors can be an\n\nimportant tool in constructing a powerful platform that is easy to use.\n\nOceanofPDF.com\n\nChapter 5. Adapters\n\nIn the preceding chapters, we saw how the sidecar pattern can extend and\n\naugment existing application containers. We also saw how ambassadors can\n\nalter and broker how an application container communicates with the\n\nexternal world. This chapter describes the final single-node pattern: the\n\nadapter pattern. In the adapter pattern, the adapter container is used to\n\nmodify the interface of the application container so that it conforms to\n\nsome predefined interface that is expected of all applications. For example,\n\nan adapter might ensure that an application implements a consistent\n\nmonitoring interface. Or it might ensure that log files are always written to\n\nstdout or any number of other conventions.\n\nReal-world application development is a heterogeneous, hybrid exercise.\n\nSome parts of your application might be written from scratch by your team,\n\nsome supplied by vendors, and some might consist entirely of off-the-shelf\n\nopen source or proprietary software that you consume as precompiled\n\nbinary. The net effect of this heterogeneity is that any real-world application\n\nyou deploy will have been written in a variety of languages, with a variety\n\nof conventions for logging, monitoring, and other common services.\n\nYet, to effectively monitor and operate your application, you need common\n\ninterfaces. When each application provides metrics using a different format\n\nand interface, it is very difficult to collect all of those metrics in a single",
      "page_number": 71
    },
    {
      "number": 5,
      "title": "Adapters",
      "start_page": 88,
      "end_page": 109,
      "detection_method": "regex_chapter_title",
      "content": "place for visualization and alerting. This is where the adapter pattern is\n\nrelevant. Like other single-node patterns, the adapter pattern is made up of\n\nmodular containers. Different application containers can present many\n\ndifferent monitoring interfaces while the adapter container adapts this\n\nheterogeneity to present a consistent interface. This enables you to deploy a\n\nsingle tool that expects this single interface. Figure 5-1 illustrates this\n\ngeneral pattern.\n\nFigure 5-1. The generic adapter pattern\n\nThe remainder of this chapter gives several different applications of the\n\nadapter pattern.\n\nMonitoring\n\nWhen monitoring your software, you want a single solution that can\n\nautomatically discover and monitor any application that is deployed into\n\nyour environment. To make this feasible, every application has to\n\nimplement the same monitoring interface. There are numerous examples of\n\nstandardized monitoring interfaces, such as syslog , Event Tracing for\n\nWindows (ETW), JMX for Java applications, and many, many other\n\nprotocols and interfaces. However, each of these is unique in both protocol\n\nfor communication as well as the style of communication (push versus\n\npull).\n\nSadly, applications in your distributed system are likely to span the gamut\n\nfrom code that you have written yourself to off-the-shelf open source\n\ncomponents. As a result, you will find yourself with a wide range of\n\ndifferent monitoring interfaces that you need to integrate into a single well-\n\nunderstood system.\n\nFortunately, most monitoring solutions understand that they need to be\n\nwidely applicable, and thus they have implemented a variety of plug-ins\n\nthat can adapt one monitoring format to a common interface. Given this set\n\nof tools, how can we deploy and manage our applications in an agile and\n\nstable manner? Fortunately, the adapter pattern can provide us with the\n\nanswers. Applying the adapter pattern to monitoring, we see that the\n\napplication container is simply the application that we want to monitor. The\n\nadapter container contains the tools for transforming the monitoring\n\ninterface exposed by the application container into the interface expected by\n\nthe general-purpose monitoring system.\n\nDecoupling the system in this fashion makes for a more comprehensible,\n\nmaintainable system. Rolling out new versions of the application doesn’t\n\nrequire a rollout of the monitoring adapter. Additionally, the monitoring\n\ncontainer can be reused with multiple different application containers. The\n\nmonitoring container may even have been supplied by the monitoring\n\nsystem maintainers independent of the application developers. Finally,\n\ndeploying the monitoring adapter as a separate container ensures that each\n\ncontainer gets its own dedicated resources in terms of both CPU and\n\nmemory. This ensures that a misbehaving monitoring adapter cannot cause\n\nproblems with a user-facing service.\n\nHands On: Using Prometheus for Monitoring\n\nAs an example, consider monitoring your containers via the Prometheus\n\nopen source project. Prometheus is a monitoring aggregator, which collects\n\nmetrics and aggregates them into a single time-series database. On top of\n\nthis database, Prometheus provides visualization and query language for\n\nintrospecting the collected metrics. To collect metrics from a variety of\n\ndifferent systems, Prometheus expects every container to expose a specific\n\nmetrics API. This enables Prometheus to monitor a wide variety of\n\ndifferent programs through a single interface.\n\nHowever, many popular programs, such as the Redis key-value store, do not\n\nexport metrics in a format that is compatible with Prometheus.\n\nConsequently, the adapter pattern is quite useful for taking an existing\n\nservice like Redis and adapting it to the Prometheus metrics-collection\n\ninterface.\n\nConsider a simple Kubernetes pod definition for a Redis server:\n\napiVersion: v1\n\nkind: Pod\n\nmetadata: name: adapter-example\n\nnamespace: default\n\nspec:\n\ncontainers: - image: redis\n\nname: redis\n\nAt this point, this container is not capable of being monitored by\n\nPrometheus because it does not export the right interface. However, if we\n\nsimply add an adapter container (in this case, an open source Prometheus\n\nexporter), we can modify this pod to export the correct interface and thus\n\nadapt it to fit Prometheus’s expectations:\n\napiVersion: v1\n\nkind: Pod\n\nmetadata: name: adapter-example\n\nnamespace: default spec: containers: - image: redis\n\nname: redis\n\n# Provide an adapter that implements the Prometh - image: oliver006/redis_exporter\n\nname: adapter\n\nThis example illustrates not only the value of the adapter pattern for\n\nensuring a consistent interface, but also the value of container patterns in\n\ngeneral for modular container reuse. In this case, the example shown\n\ncombines an existing Redis container with an existing Prometheus adapter.\n\nThe net effect is a monitorable Redis server, with little work on our part to\n\ndeploy it. In the absence of the adapter pattern, the same deployment would\n\nhave required significantly more custom work and would have resulted in a\n\nmuch less operable solution, since any updates to either Redis or the adapter\n\nwould have required work to apply the update.\n\nLogging\n\nMuch like monitoring, there is a wide variety of heterogeneity in how\n\nsystems log data to an output stream. Systems might divide their logs into\n\ndifferent levels (such as debug, info, warning, and error) with each level\n\ngoing into a different file. Some might simply log to stdout and\n\nstderr . This is especially problematic in the world of containerized\n\napplications, where there is a general expectation that your containers will\n\nlog to stdout , because that is what is available via commands like\n\ndocker logs or kubectl logs .\n\nAdding further complexity, the information logged generally has structured\n\ninformation (e.g., the date/time of the log), but this information varies\n\nwidely between different logging libraries (e.g., Java’s built-in logging\n\nversus the glog package for Go).\n\nOf course, when you are storing and querying the logs for your distributed\n\nsystem, you don’t really care about these differences in logging format. You\n\nwant to ensure that despite different structures for the data, every log ends\n\nup with the appropriate timestamp.\n\nFortunately, as with monitoring, the adapter pattern can help provide a\n\nmodular, re-usable design for both of these situations. While the application\n\ncontainer may log to a file, the adapter container can redirect that file to\n\nstdout . Different application containers can log information in different\n\nformats, but the adapter container can transform that data into a single\n\nstructured representation that can be consumed by your log aggregator.\n\nAgain, the adapter is taking a heterogeneous world of applications and\n\ncreating a homogenous world of common interfaces.\n\nNOTE\n\nOne question that often comes up when considering adapter patterns is: why not simply modify the\n\napplication container itself? If you are the developer responsible for the application container, then\n\nthis might actually be a good solution. Adapting your code or your container to implement a\n\nconsistent interface can work well. However, in many cases we are reusing a container produced by\n\nanother party. In such cases, deriving a slightly modified image that we have to maintain (patch,\n\nrebase, etc.) is significantly more expensive than developing an adapter container that can run\n\nalongside the other party’s image. Additionally, decoupling the adapter into its own container allows\n\nfor the possibility of sharing and reuse, which isn’t possible when you modify the application\n\ncontainer.\n\nHands On: Normalizing Different Logging\n\nFormats with fluentd\n\nOne common task for an adapter is to normalize log metrics into a standard\n\nset of events. Many different applications have different output formats, but\n\nyou can use a standard logging tool deployed as an adapter to normalize\n\nthem all to a consistent format. In this example, we will use the fluentd\n\nmonitoring agent as well as some community-supported plug-ins to obtain\n\nlogs from a variety of different sources.\n\nfluentd is one of the more popular open source logging agents\n\navailable. One of its major features is a rich set of community-supported\n\nplug-ins that enable a great deal of flexibility in monitoring a variety of\n\napplications.\n\nThe first application that we will monitor is Redis. Redis is a popular key-\n\nvalue store; one of the commands it offers is the SLOWLOG command.\n\nThis command lists recent queries that exceeded a particular time interval.\n\nSuch information is quite useful in debugging your application’s\n\nperformance. Unfortunately, SLOWLOG is only available as a command on\n\nthe Redis server, which means that it is difficult to use retrospectively if a\n\nproblem happens when someone isn’t available to debug the server. To fix\n\nthis limitation, we can use fluentd and the adapter pattern to add slow-\n\nquery logging to Redis.\n\nTo do this, we use the adapter pattern with a redis container as the main\n\napplication container, and the fluentd container as our adapter\n\ncontainer. In this case, we will also use the fluent-plugin-redis-\n\nslowlog fluentd plug-in to listen to the slow queries. We can\n\nconfigure this plug-in by using the following snippet:\n\n<source>\n\ntype redis_slowlog\n\nhost localhost\n\nport 6379 tag redis.slowlog\n\n</source>\n\nBecause we are using an adapter and the containers both share a network\n\nnamespace, configuring the logging simply uses localhost and the\n\ndefault Redis port (6379). Given this application of the adapter pattern,\n\nlogging will always be available whenever we want to debug slow Redis\n\nqueries.\n\nA similar exercise can be done to monitor logs from the Apache Storm\n\nsystem. Again, Storm provides data via a RESTful API, which is useful but\n\nhas limitations if we are not currently monitoring the system when a\n\nproblem occurs. Like Redis, we can use a fluentd adapter to transform\n\nthe Storm process into a time series of queryable logs. To do this, we deploy\n\na fluentd adapter with the fluent-plugin-storm plug-in\n\nenabled. We can configure this plug-in with a fluentd config pointed at\n\nlocalhost (because again, we are running as a container group with a shared\n\nlocalhost); the config for the plug-in looks like:\n\n<source>\n\ntype storm tag storm\n\nurl http://localhost:8080\n\nwindow 600\n\nsys 0\n\n</source>\n\nThis adapter builds a bridge between Storm and the time series of logs.\n\nAdding a Health Monitor\n\nOne last example of applying the adapter pattern is derived from monitoring\n\nthe health of an application container. Consider the task of monitoring the\n\nhealth of an off-the-shelf database container. In this case, the container for\n\nthe database is supplied by the database project, and we would rather not\n\nmodify that container simply to add health checks. Of course, a container\n\norchestrator will allow us to add simple health checks to ensure that the\n\nprocess is running and that it is listening on a particular port, but what if we\n\nwant to add richer health checks that actually run queries against the\n\ndatabase?\n\nContainer orchestration systems like Kubernetes enable us to use shell\n\nscripts as health checks as well. Given this capability, we can write a rich\n\nshell script that runs a number of different diagnostic queries against the\n\ndatabase to determine its health. But where can we store such a script and\n\nhow can we version it?\n\nThe answer to these problems should be easy to guess by now: we can use\n\nan adapter container. The database runs in the application container and\n\nshares a network interface with the adapter container. The adapter container\n\nis a simple container that only contains the shell script for determining the\n\nhealth of the database. This script can then be set up as the health check for\n\nthe database container and can perform whatever rich health checks our\n\napplication requires. If these checks ever fail, the database will be\n\nautomatically restarted.\n\nHands On: Adding Rich Health Monitoring for MySQL\n\nSuppose, then, that you want to add deep monitoring on a MySQL database\n\nwhere you actually run a query that is representative of your workload. In\n\nthis case, one option would be to update the MySQL container to contain a\n\nhealth check that is specific to your application. However, this is generally\n\nan unattractive idea because it requires that you both modify some existing\n\nMySQL base image as well as update that image as new MySQL images are\n\nreleased.\n\nUsing the adapter pattern is a much more attractive approach to adding\n\nhealth checks to your database container. Instead of modifying the existing\n\nMySQL container, you can add an additional adapter container to the\n\npreexisting MySQL container, which runs the appropriate query to test the\n\ndatabase health. Given that this adapter container implements the expected\n\nHTTP health check, it is simply a case of defining the MySQL database\n\nprocess’s health check in terms of the interface exposed by this database\n\nadapter.\n\nThe source code for this adapter is relatively straightforward and looks like\n\nthis in Go (though clearly other language implementations are possible as\n\nwell):\n\npackage main\n\nimport (\n\n\"database/sql\" \"flag\"\n\n\"fmt\"\n\n\"net/http\"\n\n_ \"github.com/go-sql-driver/mysql\"\n\n)\n\nvar (\n\nuser = flag.String(\"user\", \"\", \"The dat\n\npasswd = flag.String(\"password\", \"\", \"The\n\ndb = flag.String(\"database\", \"\", \"The\n\nquery = flag.String(\"query\", \"\", \"The te\n\naddr = flag.String(\"address\", \"localhos\n\n\"The address to liste\n\n)\n\n// Basic usage: // db-check --query=\"SELECT * from my-cool-tabl // --user=bdburns \\\n\n// --passwd=\"you wish\"\n\n//\n\nfunc main() {\n\nflag.Parse()\n\ndb, err := sql.Open(\"localhost\", fmt.Sprintf(\"%s:%s@/%\n\nif err != nil {\n\nfmt.Printf(\"Error opening databas\n\n}\n\n// Simple web handler that runs the query\n\nhttp.HandleFunc(\"\", func(res http.Respons\n\n_, err := db.Exec(*query)\n\nif err != nil {\n\nres.WriteHeader(http.Stat\n\nres.Write([]byte(err.Erro\n\nreturn\n\n}\n\nres.WriteHeader(http.StatusOK)\n\nres.Write([]byte(\"OK\"))\n\nreturn\n\n})\n\n// Startup the server\n\nhttp.ListenAndServe(*addr, nil)\n\n}\n\nWe can then build this into a container image and pull it into a pod that\n\nlooks like:\n\napiVersion: v1\n\nkind: Pod\n\nmetadata: name: adapter-example-health\n\nnamespace: default\n\nspec:\n\ncontainers:\n\nimage: mysql\n\nname: mysql\n\nimage: brendanburns/mysql-adapter\n\nname: adapter\n\nThat way, the mysql container is unchanged, but the desired feedback\n\nabout the health of the MySQL server can still be obtained from the adapter\n\ncontainer.\n\nWhen looking at this application of the adapter pattern, it may seem like\n\napplying the pattern is superfluous. Clearly we could have built our own\n\ncustom image that knew how to health check the mysql instance itself.\n\nWhile this is true, this method ignores the strong benefits that derive from\n\nmodularity. If every developer implements their own specific container with\n\nhealth checking built in, there are no opportunities for reuse or sharing.\n\nIn contrast, if we use patterns like the adapter to develop modular solutions\n\ncomprised of multiple containers, the work is inherently decoupled and\n\nmore easily shared. An adapter that is developed to health check mysql is\n\na module that can be shared and reused by a variety of people. Further,\n\npeople can apply the adapter pattern using this shared health-checking\n\ncontainer, without having deep knowledge of how to health check a\n\nmysql database. Thus the modularity and adapter pattern serve not to just\n\nfacilitate sharing, but also to empower people to take advantage of the\n\nknowledge of others.\n\nSometimes design patterns aren’t just for the developers who apply them,\n\nbut lead to the development of communities that can collaborate and share\n\nsolutions between members of the community as well as the broader\n\ndeveloper ecosystem.\n\nSummary\n\nSometimes the world isn’t perfectly aligned. The components that we need\n\nto use to build our systems can have different interfaces or different\n\nprotocols for interactions like logging, monitoring, and RPCs. Adapters can\n\nbe a critical tool to add consistency to our systems, enabling them to share a\n\nconsistent protocol and way of connecting. Using an adapter is often\n\nsignificantly easier than trying to change the upstream project.\n\nOceanofPDF.com\n\nPart III. Serving Patterns\n\nChapter 5 described patterns for grouping collections of containers that are\n\nscheduled on the same machine. These groups are tightly coupled,\n\nsymbiotic systems. They depend on local, shared resources like disk,\n\nnetwork interface, or inter-process communications. Such collections of\n\ncontainers are important patterns, but they are also building blocks for\n\nlarger systems. Reliability, scalability, and separation of concerns dictate\n\nthat real-world systems are built out of many different components, spread\n\nacross multiple machines. In contrast to single-node patterns, the multinode\n\ndistributed patterns are more loosely coupled. While the patterns dictate\n\npatterns of communication between the components, this communication is\n\nbased on network calls. Furthermore, many calls are issued in parallel, and\n\nsystems coordinate via loose synchronization rather than tight constraints.\n\nRecently, the term microservices has become a common term for describing\n\nmultinode distributed software architectures. Microservices describe a\n\nsystem built out of many different components running in different\n\nprocesses and communicating over defined APIs. Microservices stand in\n\ncontrast to monolithic systems, which tend to place all of the functionality\n\nfor a service within a single, tightly coordinated application. These two\n\ndifferent architectural approaches are shown in Figures III-1 and III-2.\n\nFigure III-1. A monolithic service with all functions in a single container\n\nFigure III-2. A microservice architecture with each function broken out as a separate microservice\n\nThere are numerous benefits to the microservices approach—most of them\n\nare centered around reliability and agility. Microservices break down an\n\napplication into small pieces, each focused on providing a single service.\n\nThis reduced scope enables each service to be built and maintained by a\n\nsingle “two pizza” team. Reduced team size also reduces the overhead\n\nassociated with keeping a team focused and moving in one direction.\n\nAdditionally, the introduction of formal APIs in between different\n\nmicroservices decouples the teams from one another and provides a reliable\n\ncontract between the different services. This formal contract reduces the\n\nneed for tight synchronization among the teams because the team providing\n\nthe API understands the surface area that it needs to keep stable, and the\n\nteam consuming the API can rely on a stable service without worrying\n\nabout its details. This decoupling enables teams to independently manage\n\ntheir code and release schedules, which in turn improves each team’s ability\n\nto iterate and improve their code.\n\nThe design of a service architecture is much like the design of a class\n\nhierarchy—its goals are to introduce abstraction, encapsulation, and\n\nmodularity. It is important to note that in each of these categories the\n\n“right” design has as much to do with the humans involved in building the\n\ndesign as it does with the core application being developed.\n\nAbstraction and encapsulation can often be seen as foils of each other.\n\nAbstraction enables teams to not worry about the details of how a complex\n\npiece of functionality is implemented and focus instead on what they can do\n\nwith that capability. The programming required to recognize objects in an\n\nimage using machine learning is quite complicated, but the abstract service\n\nfindObjectsInImage(image): objects is quite easy to use.\n\nDistributed systems are quite complex, and trying to keep all of the details\n\nin our heads rapidly leads to information overload. Abstraction enables\n\nteams to focus on higher-level capabilities, not lower-level implementation\n\ndetails.\n\nEncapsulation is the same but in reverse; encapsulation allows teams to\n\ndeliver a service while hiding the implementation details from the user.\n\nHiding these details is critical because it enables the implementation team\n\nto make changes without breaking their users. This flexibility makes teams\n\nmore agile because they are decoupled from each other. Taken together,\n\nencapsulation and abstraction enable us to build optimally sized teams for\n\nexecution and delivery. Big enough to take on hard problems with a sense\n\nof purpose, but small enough to move quickly without the communication\n\nbaggage that arises in larger teams.\n\nFinally, the decoupling of microservices enables better scaling. Because\n\neach component has been broken out into its own service, it can be scaled\n\nindependently. It is rare for each service within a larger application to grow\n\nat the same rate, or have the same way of scaling. Some systems are\n\nstateless and can simply scale horizontally, whereas other systems maintain\n\nstate and require sharding or other approaches to scale. By separating each\n\nservice out, each service can use the approach to scaling that suits it best.\n\nThis is not possible when all services are part of a single monolith.\n\nBut of course there are downsides to the microservices approach to system\n\ndesign as well. As the system has become more loosely coupled, debugging\n\nthe system when failures occur is significantly more difficult. You can no\n\nlonger simply load a single application into a debugger and determine what\n\nwent wrong. Any errors are the by-products of a large number of systems\n\noften running on different machines. This environment is quite challenging\n\nto reproduce in a debugger. As a corollary, microservices-based systems are\n\nalso difficult to design and architect. A microservices-based system uses\n\nmultiple methods of communicating between services; different patterns\n\n(e.g., synchronous, asynchronous, message-passing, etc.); and multiple\n\ndifferent patterns of coordination and control among the services.\n\nAdditionally, there can be a tendency within teams to take a microservice\n\npattern to an extreme and end up with far too many microservices that are\n\nfar too small. This over-segmentation not only adds complexity, but it also\n\ncan add significant inefficiency in terms of both network overhead (since\n\ncommunication between microservices occurs on the network) and also\n\ncosts (since each instance of a microservice has a fixed amount of overhead\n\nin terms of CPU and memory). As a general rule of thumb, if you have\n\nmore microservices than you have engineers on your team, you either have\n\na very small team or you are overdoing the microservice approach.\n\nThese challenges are the motivation for distributed patterns. If a\n\nmicroservices architecture is made up of well-known patterns, then it is\n\neasier to design because many of the design practices are specified by the\n\npatterns. Additionally, patterns make the systems easier to debug because\n\nthey enable developers to apply lessons learned across a number of different\n\nsystems that use the same patterns.\n\nWith that in mind, this section introduces a number of multinode patterns\n\nfor building distributed systems. These patterns are not mutually exclusive.\n\nAny real-world system will be built from a collection of these patterns\n\nworking together to produce a single higher-level application.\n\nOceanofPDF.com\n\nChapter 6. Replicated Load-Balanced\n\nServices\n\nThe simplest distributed pattern, and one that most are familiar with, is a\n\nreplicated load-balanced service. In such a service, every server is identical\n\nto every other server, and all are capable of supporting traffic from any\n\nclient. The pattern consists of a scalable number of servers with a load\n\nbalancer in front of them. The load balancer is typically either completely\n\nround-robin or uses some form of session stickiness. The chapter will give a\n\nconcrete example of how to deploy such a service in Kubernetes.\n\nStateless Services\n\nStateless services are ones that don’t require saved state to operate correctly.\n\nIn the simplest stateless applications, even individual requests may be\n\nrouted to separate instances of the service (see Figure 6-1). Examples of\n\nstateless services include things like static content servers and complex\n\nmiddleware systems that receive and aggregate responses from numerous\n\ndifferent backend systems.",
      "page_number": 88
    },
    {
      "number": 6,
      "title": "Replicated Load-Balanced",
      "start_page": 110,
      "end_page": 134,
      "detection_method": "regex_chapter_title",
      "content": "Figure 6-1. Basic replicated stateless service\n\nStateless systems are replicated to provide redundancy and scale. No matter\n\nhow small your service is, you need at least two replicas to provide a\n\nservice with a “highly available” service level agreement (SLA). To\n\nunderstand why this is true, consider trying to deliver a three-nines (99.9%\n\navailability). In a three-nines service, you get 1.4 minutes of downtime per\n\nday (24 × 60 × 0.001). Assuming that you have a service that never crashes,\n\nthat still means you need to be able to do a software upgrade in less than 1.4\n\nminutes in order to hit your SLA with a single instance. And that’s\n\nassuming that you do daily software rollouts. If your team is really\n\nembracing continuous delivery and you’re pushing a new version of\n\nsoftware every hour, you need to be able to do a software rollout in 3.6\n\nseconds to achieve your 99.9% uptime SLA with a single instance. Any\n\nlonger than that, and you will have more than 0.01% downtime from those\n\n3.6 seconds. And all of this is assuming that your code never fails due to\n\nbugs, which is unrealistic in any real-world codebase.\n\nOf course, instead of all of that work, you could just have two replicas of\n\nyour service with a load balancer in front of them. That way, while you are\n\ndoing a rollout, or in the—unlikely, I’m sure—event that your software\n\ncrashes, your users will be served by the other replica of the service and\n\nnever know anything was going on.\n\nAs services grow larger, they are also replicated to support additional users.\n\nHorizontally scalable systems handle more and more users by adding more\n\nreplicas; see Figure 6-2. They achieve this with the load-balanced replicated\n\nserving pattern.\n\nFigure 6-2. Horizontal scaling of a replicated stateless application\n\nReadiness Probes for Load Balancing\n\nOf course, simply replicating your service and adding a load balancer is\n\nonly part of a complete pattern for stateless replicated serving. When\n\ndesigning a replicated service, it is equally important to build and deploy a\n\nreadiness probe to inform the load balancer. We have discussed how health\n\nprobes can be used by a container orchestration system to determine when\n\nan application needs to be restarted. In contrast, a readiness probe\n\ndetermines when an application is ready to serve user requests. The reason\n\nfor the differentiation is that many applications require some time to\n\nbecome initialized before they are ready to serve. They may need to connect\n\nto databases, load plugins, or download serving files from the network. In\n\nall of these cases, the containers are alive, but they are not ready. When\n\nbuilding an application for a replicated service pattern, be sure to include a\n\nspecial URL that implements this readiness check.\n\nHands On: Creating a Replicated Service in\n\nKubernetes\n\nThe instructions below give a concrete example of how to deploy a stateless\n\nreplicated service behind a load balancer. These directions use the\n\nKubernetes container orchestrator, but the pattern can be implemented on\n\ntop of a number of different container orchestrators.\n\nTo begin with, we will create a small Node.js application that serves\n\ndefinitions of words from the dictionary.\n\nTo try this service out, you can run it using a container image:\n\ndocker run -p 8080:8080 brendanburns/dictionary-s\n\nThis runs a simple dictionary server on your local machine. For example,\n\nyou can visit http://localhost:8080/dog to see the definition for dog.\n\nIf you look at the logs for the container, you’ll see that it starts serving\n\nimmediately but only reports readiness after the dictionary (which is\n\napproximately 8 MB) has been downloaded over the network.\n\nTo deploy this in Kubernetes, you create a Deployment :\n\napiVersion: extensions/v1beta1\n\nkind: Deployment metadata:\n\nname: dictionary-server\n\nspec: replicas: 3\n\ntemplate:\n\nmetadata: labels:\n\napp: dictionary-server\n\nspec: containers:\n\nname: server\n\nimage: brendanburns/dictionary-server ports:\n\ncontainerPort: 8080\n\nreadinessProbe:\n\nhttpGet: path: /ready\n\nport: 8080\n\ninitialDelaySeconds: 5 periodSeconds: 5\n\nYou can create this replicated stateless service with:\n\nkubectl create -f dictionary-deploy.yaml\n\nNow that you have a number of replicas, you need a load balancer to bring\n\nrequests to your replicas. The load balancer serves to distribute the load as\n\nwell as to provide an abstraction to separate the replicated service from the\n\nconsumers of the service. The load balancer also provides a resolvable\n\nname that is independent of any of the specific replicas.\n\nWith Kubernetes, you can create this load balancer with a Service\n\nobject:\n\nkind: Service apiVersion: v1\n\nmetadata:\n\nname: dictionary-server-service spec:\n\nselector:\n\napp: dictionary-server ports:\n\nprotocol: TCP\n\nport: 8080\n\ntargetPort: 8080\n\nOnce you have the configuration file, you can create the dictionary service\n\nwith:\n\nkubectl create -f dictionary-service.yaml\n\nSession Tracked Services\n\nThe previous examples of the stateless replicated pattern routed requests\n\nfrom all users to all replicas of a service. While this ensures an even\n\ndistribution of load and fault tolerance, it is not always the preferred\n\nsolution. Often there are reasons for wanting to ensure that a particular\n\nuser’s requests always end up on the same machine. Sometimes this is\n\nbecause you are caching that user’s data in memory, so landing on the same\n\nmachine ensures a higher cache hit rate. Sometimes it is because the\n\ninteraction is long-running in nature, so some amount of state is maintained\n\nbetween requests. Regardless of the reason, an adaption of the stateless\n\nreplicated service pattern is to use session tracked services, which ensure\n\nthat all requests for a single user map to the same replica, as illustrated in\n\nFigure 6-3.\n\nFigure 6-3. A session tracked service where all requests for a specific user are routed to a single instance\n\nGenerally speaking, this session tracking is performed by hashing the\n\nsource and destination IP addresses and using that key to identify the server\n\nthat should service the requests. So long as the source and destination IP\n\naddresses remain constant, all requests are sent to the same replica.\n\nNOTE\n\nIP-based session tracking works within a cluster (internal IPs) but generally doesn’t work well with\n\nexternal IP addresses because of network address translation (NAT). For external session tracking,\n\napplication-level tracking (e.g., via cookies) is preferred.\n\nOften, session tracking is accomplished via a consistent hashing function.\n\nThe benefit of a consistent hashing function becomes evident when the\n\nservice is scaled up or down. Obviously, when the number of replicas\n\nchanges, the mapping of a particular user to a replica may change.\n\nConsistent hashing functions minimize the number of users that actually\n\nchange which replica they are mapped to, reducing the impact of scaling on\n\nyour application.\n\nApplication-Layer Replicated Services\n\nIn all of the preceding examples, the replication and load balancing takes\n\nplace in the network layer of the service. The load balancing is independent\n\nof the actual protocol that is being spoken over the network, beyond\n\nTCP/IP. However, many applications use HTTP as the protocol for speaking\n\nwith each other, and knowledge of the application protocol that is being\n\nspoken enables further refinements to the replicated stateless serving pattern\n\nfor additional functionality.\n\nIntroducing a Caching Layer\n\nSometimes the code in your stateless service is still expensive despite being\n\nstateless. It might make queries to a database to service requests or do a\n\nsignificant amount of rendering or data mixing to service the request. In\n\nsuch a world, a caching layer can make a great deal of sense. A cache exists\n\nbetween your stateless application and the end-user request. The simplest\n\nform of caching for web applications is a caching web proxy. The caching\n\nproxy is simply an HTTP server that maintains the correct responses to user\n\nrequests in memory. If two users request the same web page or API call,\n\nonly one request will go to your backend; the other will be serviced out of\n\nmemory in the cache. This is illustrated in Figure 6-4.\n\nFigure 6-4. The operation of a cache server\n\nFor our purposes, we will use Varnish, an open source web cache.\n\nDeploying Your Cache\n\nThe simplest way to deploy the web cache is alongside each instance of\n\nyour web server using the sidecar pattern (see Figure 6-5).\n\nFigure 6-5. Adding the web cache server as a sidecar\n\nThough this approach is simple, it has some disadvantages, namely that you\n\nwill have to scale your cache at the same scale as your web servers. This is\n\noften not the approach you want. For your cache, you want as few replicas\n\nas possible with lots of resources for each replica (e.g., rather than 10\n\nreplicas with 1 GB of RAM each, you’d want two replicas with 5 GB of\n\nRAM each). To understand why this is preferable, consider that every page\n\nwill be stored in every replica. With 10 replicas, you will store every page\n\n10 times, reducing the overall set of pages that you can keep in memory in\n\nthe cache. This causes a reduction in the hit rate, the fraction of the time\n\nthat a request can be served out of cache, which in turn decreases the utility\n\nof the cache. Though you do want a few large caches, you might also want\n\nlots of small replicas of your web servers. Many languages (e.g., Node.js)\n\ncan really only utilize a single core, and thus you want many replicas to be\n\nable to take advantage of multiple cores, even on the same machine.\n\nTherefore, it makes the most sense to configure your caching layer as a\n\nsecond stateless replicated serving tier above your web-serving tier, as\n\nillustrated in Figure 6-6.\n\nFigure 6-6. Adding the caching layer to our replicated service\n\nNOTE\n\nUnless you are careful, caching can break session tracking. The reason for this is that if you use\n\ndefault IP address affinity and load balancing, all requests will be sent from the IP addresses of the\n\ncache, not the end user of your service. If you’ve followed the advice previously given and deployed\n\na few large caches, your IP-address-based affinity may in fact mean that some replicas of your web\n\nlayer see no traffic. Instead, you need to use something like a cookie or HTTP header for session\n\ntracking.\n\nHands On: Deploying the Caching Layer\n\nThe dictionary-server service we built earlier distributes traffic to\n\nthe dictionary server and is discoverable as the DNS name dictionary-\n\nserver . This pattern is illustrated in Figure 6-7.\n\nWe can begin building this with the following Varnish cache configuration:\n\nvcl 4.0;\n\nbackend default {\n\n.host = \"dictionary-server-service\";\n\n.port = \"8080\"; }\n\nCreate a ConfigMap object to hold this configuration:\n\nkubectl create configmap varnish-config --from-fi\n\nFigure 6-7. Adding a caching layer to the dictionary server\n\nNow we can deploy the replicated Varnish cache, which will load this\n\nconfiguration:\n\napiVersion: extensions/v1beta1\n\nkind: Deployment\n\nmetadata:\n\nname: varnish-cache spec:\n\nreplicas: 2\n\ntemplate:\n\nmetadata:\n\nlabels:\n\napp: varnish-cache\n\nspec:\n\ncontainers:\n\nname: cache\n\nresources:\n\nrequests:\n\n# We'll use two gigabytes for each va\n\nmemory: 2Gi image: brendanburns/varnish\n\ncommand: - varnishd\n\n-F - -f - /etc/varnish-config/default.vcl\n\n-a\n\n0.0.0.0:8080\n\n-s\n\n# This memory allocation should match the - malloc,2G\n\nports:\n\ncontainerPort: 8080\n\nvolumeMounts:\n\nname: varnish mountPath: /etc/varnish-config volumes:\n\nname: varnish\n\nconfigMap:\n\nname: varnish-config\n\nYou can deploy the replicated Varnish servers with:\n\nkubectl create -f varnish-deploy.yaml\n\nAnd then finally deploy a load balancer for this Varnish cache:\n\nkind: Service\n\napiVersion: v1\n\nmetadata:\n\nname: varnish-service\n\nspec:\n\nselector:\n\napp: varnish-cache\n\nports:\n\nprotocol: TCP\n\nport: 80\n\ntargetPort: 8080\n\nwhich you can create with:\n\nkubectl create -f varnish-service.yaml\n\nExpanding the Caching Layer\n\nNow that we have inserted a caching layer into our stateless replicated\n\nservice, let’s look at what this layer can provide beyond standard caching.\n\nHTTP reverse proxies like Varnish are generally pluggable and can provide\n\na number of advanced features that are useful beyond caching.\n\nRate Limiting and Denial-of-Service Defense\n\nFew of us build sites with the expectation that we will encounter a denial-\n\nof-service attack. But as more and more of us build APIs, a denial of service\n\ncan come simply from a developer misconfiguring a client or a site-\n\nreliability engineer accidentally running a load test against a production\n\ninstallation. Thus, it makes sense to add general denial-of-service defense\n\nvia rate limiting to the caching layer. Most HTTP reverse proxies like\n\nVarnish have capabilities along this line. In particular, Varnish has a\n\nthrottle module that can be configured to provide throttling based on\n\nIP address and request path, as well as whether or not a user is logged in.\n\nIf you are deploying an API, it is generally a best practice to have a\n\nrelatively small rate limit for anonymous access and then force users to log\n\nin to obtain a higher rate limit. Requiring a login provides auditing to\n\ndetermine who is responsible for the unexpected load, and also offers a\n\nbarrier to would-be attackers who need to obtain multiple identities to\n\nlaunch a successful attack.\n\nWhen a user hits the rate limit, the server will return the 429 error code\n\nindicating that too many requests have been issued. However, many users\n\nwant to understand how many requests they have left before hitting that\n\nlimit. To that end, you will likely also want to populate an HTTP header\n\nwith the remaining-calls information. Though there isn’t a standard header\n\nfor returning this data, many APIs return some variation of X-\n\nRateLimit-Remaining .\n\nNOTE\n\nWhile Varnish can provide some basic protection against DDoS attacks, in the real world attackers\n\nare often sophisticated organizations with large amounts of resources to bring to bear in ever-\n\nevolving attacks against organizations. If you are planning on deploying large-scale or mission-\n\ncritical infrastructure, you will probably also want to add cloud-based denial of service protection\n\nfrom a public cloud provider to your application. Cloud-based DDoS systems can handle\n\nsignificantly more load and also are constantly evolving to meet the changing landscape of threat\n\nactors.\n\nSSL Termination\n\nIn addition to performing caching for performance, one of the other\n\ncommon tasks performed by the edge layer is SSL termination. Even if you\n\nplan on using SSL for communication between layers in your cluster, you\n\nshould still use different certificates for the edge and your internal services.\n\nIndeed, each individual internal service should use its own certificate to\n\nensure that each layer can be rolled out independently. Unfortunately, the\n\nVarnish web cache can’t be used for SSL termination, but fortunately, the\n\nnginx application can. Thus, we want to add a third layer to our stateless\n\napplication pattern, which will be a replicated layer of nginx servers that\n\nwill handle SSL termination for HTTPS traffic and forward traffic on to our\n\nVarnish cache. HTTP traffic continues to travel to the Varnish web cache,\n\nand Varnish forwards traffic on to our web application, as shown in\n\nFigure 6-8.\n\nFigure 6-8. Complete replicated stateless serving example\n\nHands On: Deploying nginx and SSL Termination\n\nThe following instructions describe how to add a replicated SSL\n\nterminating nginx to the replicated service and cache that we previously\n\ndeployed.\n\nNOTE\n\nThese instructions assume that you have a certificate. If you need to obtain a certificate, the easiest\n\nway to do that is via the tools at Let’s Encrypt. Alternatively, you can use the openssl tool to\n\ncreate them. The following instructions assume that you’ve named them server.crt (public\n\ncertificate) and server.key (private key on the server). Such self-signed certificates will cause\n\nsecurity alerts in modern web browsers and should never be used for production.\n\nThe first step is to upload your certificate as a secret to Kubernetes:\n\nkubectl create secret tls ssl --cert=server.crt -\n\nOnce you have uploaded your certificate as a secret, you need to create an\n\nnginx configuration to serve SSL:\n\nevents {\n\nworker_connections 1024;\n\n}\n\nhttp { server { listen 443 ssl; server_name my-domain.com www.my-domain.com; ssl on; ssl_certificate /etc/certs/tls.crt; ssl_certificate_key /etc/certs/tls.key;\n\nlocation / {\n\nproxy_pass http://varnish-service:80;\n\nproxy_set_header Host $host; proxy_set_header X-Forwarded-For $proxy_a\n\nproxy_set_header X-Forwarded-Proto $schem\n\nproxy_set_header X-Real-IP $remote_addr;\n\n}\n\n} }\n\nAs with Varnish, you need to transform this into a ConfigMap object:\n\nkubectl create configmap nginx-conf --from-file=n\n\nNow that you have a secret and an nginx configuration, it’s time to\n\ncreate the replicated stateless nginx layer:\n\napiVersion: extensions/v1beta1\n\nkind: Deployment\n\nmetadata: name: nginx-ssl\n\nspec: replicas: 4 template: metadata: labels: app: nginx-ssl\n\nspec:\n\ncontainers: - name: nginx\n\nimage: nginx\n\nports:\n\ncontainerPort: 443\n\nvolumeMounts: - name: conf\n\nmountPath: /etc/nginx\n\nname: certs\n\nmountPath: /etc/certs\n\nvolumes:\n\nname: conf\n\nconfigMap:\n\n# This is the ConfigMap for nginx we cr\n\nname: nginx-conf\n\nname: certs\n\nsecret:\n\n# This is the secret we created above\n\nsecretName: ssl\n\nTo create the replicated nginx servers, you use:\n\nkubectl create -f nginx-deploy.yaml\n\nFinally, you can expose this nginx SSL server with a service:\n\nkind: Service\n\napiVersion: v1\n\nmetadata:\n\nname: nginx-service spec:\n\nselector:\n\napp: nginx-ssl\n\ntype: LoadBalancer\n\nports:\n\nprotocol: TCP\n\nport: 443\n\ntargetPort: 443\n\nTo create this load-balancing service, run:\n\nkubectl create -f nginx-service.yaml\n\nIf you create this service on a Kubernetes cluster that supports external load\n\nbalancers, this will create an externalized, public service that services traffic\n\non a public IP address.\n\nTo get this IP address, you can run:\n\nkubectl get services\n\nYou should then be able to access the service with your web browser.\n\nSummary\n\nThis chapter began with a simple pattern for replicated stateless services.\n\nThen we saw how this pattern grows with two additional replicated load-\n\nbalanced layers to provide caching for performance and SSL termination for\n\nsecure web serving. This complete pattern for stateless replicated serving is\n\nshown in Figure 6-8.\n\nThis complete pattern can be deployed into Kubernetes using three\n\nDeployments and Service load balancers to connect the layers\n\nshown in Figure 6-8. The complete source for these examples can be found\n\nat this book’s GitHub repository.\n\nOceanofPDF.com\n\nChapter 7. Sharded Services\n\nIn Chapter 6, we saw the value of replicating stateless services for\n\nreliability, redundancy, and scaling. This chapter considers sharded services.\n\nWith the replicated services that we introduced in Chapter 6, each replica\n\nwas entirely homogeneous and capable of serving every request. In contrast\n\nto replicated services, with sharded services, each replica, or shard, is only\n\ncapable of serving a subset of all requests. A load-balancing node, or root, is\n\nresponsible for examining each request and distributing each request to the\n\nappropriate shard or shards for processing. The contrast between replicated\n\nand sharded services is represented in Figure 7-1.\n\nFigure 7-1. Replicated service versus sharded service\n\nReplicated services are generally used for building stateless services,\n\nwhereas sharded services are generally used for building stateful services.\n\nThe primary reason for sharding the data is because the size of the state is",
      "page_number": 110
    },
    {
      "number": 7,
      "title": "Sharded Services",
      "start_page": 135,
      "end_page": 161,
      "detection_method": "regex_chapter_title",
      "content": "too large to be served by a single machine. Sharding enables you to scale a\n\nservice in response to the size of the state that needs to be served.\n\nSharding is not exclusively for stateful services, though. Sometimes\n\nsharding can also be useful for stateless services for the purpose of\n\nisolation. Failures in distributed systems can sometimes come from the\n\ninput requests to the system (“poison requests”) in a replicated system—if\n\nthe poison requests come in a large enough volume, they can take down all\n\nof the replicas. In such systems, sharding can form an isolation boundary\n\nthat protects the majority of the requests from the poison ones. Though the\n\npoison requests may take out the entire shard, that will only be a fraction of\n\nthe requests that the system processes.\n\nSharded Caching\n\nTo completely illustrate the design of a sharded system, this section\n\nprovides a deep dive into the design of a sharded caching system. A sharded\n\ncache is a cache that sits between the user requests and the actual frontend\n\nimplementation. A high-level diagram of the system is shown in Figure 7-2.\n\nFigure 7-2. A sharded cache\n\nIn Chapter 4, we discussed how an ambassador could be used to distribute\n\ndata to a sharded service. This section discusses how to build that service.\n\nWhen designing a sharded cache, there are a number of design aspects to\n\nconsider:\n\nWhy you might need a sharded cache\n\nThe role of the cache in your architecture\n\nReplicated sharded caches\n\nThe sharding function\n\nWhy You Might Need a Sharded Cache\n\nReminder: the primary reason for sharding any service is to increase the\n\nsize of the data being stored in the service. To understand how this helps a\n\ncaching system, imagine the following system: each cache has 10 GB of\n\nRAM available to store results, and can serve 100 requests per second\n\n(RPS). Suppose then that our service has a total of 200 GB possible results\n\nthat could be returned, and an expected 1,000 RPS. Clearly, we need 10\n\nreplicas of the cache in order to satisfy 1,000 RPS (10 replicas × 100\n\nrequests per second per replica). The simplest way to deploy this service\n\nwould be as a replicated service, as described in Chapter 6. But deployed\n\nthis way, the distributed cache can only hold a maximum of 5% (10 GB/200\n\nGB) of the total data set that we are serving. This is because each cache\n\nreplica is independent, and thus each cache replica stores roughly the exact\n\nsame data in the cache. This is great for redundancy, but pretty terrible for\n\nmaximizing memory utilization. If instead, we deploy a 10-way sharded\n\ncache, we can still serve the appropriate number of RPS (10 × 100 is still\n\n1,000), but because each cache serves a completely unique set of data, we\n\nare able to store 50% (10 × 10 GB/200 GB) of the total data set. This\n\ntenfold increase in cache storage means that the memory for the cache is\n\nmuch better utilized, since each key exists only in a single cache. The more\n\ndata we store in the cache, the faster the overall system will be able to\n\noperate since more requests can be served from the cache.\n\nThe Role of the Cache in System Performance\n\nIn Chapter 6 we discussed how caches can be used to optimize end-user\n\nperformance and latency, but one thing that wasn’t covered was the\n\ncriticality of the cache to your application’s performance, reliability, and\n\nstability.\n\nPut simply, the important question for you to consider is: if the cache were\n\nto fail, what would the impact be for your users and your service?\n\nWhen we discussed the replicated cache, this question was less relevant\n\nbecause the cache itself was horizontally scalable, and failures of specific\n\nreplicas would only lead to transient failures. Likewise, the cache could be\n\nhorizontally scaled in response to increased load without impacting the end\n\nuser.\n\nThis changes when you consider sharded caches. Because a specific user or\n\nrequest is always mapped to the same shard, if that shard fails, that user or\n\nrequest will always miss the cache until the shard is restored. Given the\n\nnature of a cache as transient data, this miss is not inherently a problem, and\n\nyour system must know how to recalculate the data. However, this\n\nrecalculation is inherently slower than using the cache directly, and thus it\n\nhas performance implications for your end users.\n\nThe performance of your cache is defined in terms of its hit rate. The hit\n\nrate is the percentage of the time that your cache contains the data for a user\n\nrequest. Ultimately, the hit rate determines the overall capacity of your\n\ndistributed system and affects the overall capacity and performance of your\n\nsystem.\n\nImagine, if you will, that you have a request-serving layer that can handle\n\n1,000 RPS. After 1,000 RPS, the system starts to return HTTP 500 errors\n\nto users. If you place a cache with a 50% hit rate in front of this request-\n\nserving layer, adding this cache increases your maximum RPS from 1,000\n\nRPS to 2,000 RPS. To understand why this is true, you can see that of the\n\n2,000 inbound requests, 1,000 (50%) can be serviced by the cache, leaving\n\n1,000 requests to be serviced by your serving layer. In this instance, the\n\ncache is fairly critical to your service, because if the cache fails, then the\n\nserving layer will be overloaded and half of all your user requests will fail.\n\nGiven this, it likely makes sense to rate your service at a maximum of 1,500\n\nRPS rather than the full 2,000 RPS. If you do this, then you can sustain a\n\nfailure of half of your cache replicas and still keep your service stable.\n\nBut the performance of your system isn’t just defined in terms of the\n\nnumber of requests that it can process. Your system’s end-user performance\n\nis defined in terms of the latency of requests as well. A result from a cache\n\nis generally significantly faster than calculating that result from scratch.\n\nConsequently, a cache can improve the speed of requests as well as the total\n\nnumber of requests processed. To see why this is true, imagine that your\n\nsystem can serve a request from a user in 100 milliseconds. You add a cache\n\nwith a 25% hit rate that can return a result in 10 milliseconds. Thus, the\n\naverage latency for a request in your system is now 77.5 milliseconds.\n\nUnlike maximum requests per second, the cache simply makes your\n\nrequests faster, so there is somewhat less need to worry about the fact that\n\nrequests will slow down if the cache fails or is being upgraded. However, in\n\nsome cases, the performance impact can cause too many user requests to\n\npile up in request queues and ultimately time out. It’s always recommended\n\nthat you load test your system both with and without caches to understand\n\nthe impact of the cache on the overall performance of your system.\n\nFinally, it isn’t just failures that you need to think about. If you need to\n\nupgrade or redeploy a sharded cache, you cannot just deploy a new replica\n\nand assume it will immediately take the existing load. A newly created\n\ncache shard doesn’t have any data stored in memory. Responses are cached\n\nas they are received in response to user requests. A newly deployed cache\n\nneeds to be “warmed up” or filled with responses before it can provide\n\nmaximal benefit. Deploying a new version of a sharded cache will generally\n\nresult in temporarily losing some capacity. Another, more advanced option\n\nis to replicate your shards.\n\nReplicated Sharded Caches\n\nSometimes your system is so dependent on a cache for latency or load that\n\nit is not acceptable to lose an entire cache shard if there is a failure or you\n\nare doing a rollout. Alternatively, you may have so much load on a\n\nparticular cache shard that you need to scale it to handle the load. For these\n\nreasons, you may choose to deploy a sharded replicated service. A sharded\n\nreplicated service combines the replicated service pattern described in\n\nChapter 6 with the sharded pattern described in previous sections. In a\n\nnutshell, rather than having a single server implement each shard in the\n\ncache, a replicated service is used to implement each cache shard.\n\nThis design is obviously more complicated to implement and deploy, but it\n\nhas several advantages over a simple sharded service. Most importantly, by\n\nreplacing a single server with a replicated service, each cache shard is\n\nresilient to failures and is always present during failures. Rather than\n\ndesigning your system to be tolerant to performance degradation resulting\n\nfrom cache shard failures, you can rely on the performance improvements\n\nthat the cache provides. Assuming that you are willing to over-provision\n\nshard capacity, this means that it is safe for you to do a cache rollout during\n\npeak traffic, rather than waiting for a quiet period for your service.\n\nAdditionally, because each replicated cache shard is an independent\n\nreplicated service, you can scale each cache shard in response to its load;\n\nthis sort of “hot sharding” is discussed at the end of this chapter.\n\nHands On: Deploying an Ambassador and Memcache for a Sharded Cache\n\nIn Chapter 4 we saw how to deploy a sharded Redis service. Deploying a\n\nsharded memcache is similar.\n\nFirst, we will deploy memcache as a Kubernetes StatefulSet :\n\napiVersion: apps/v1\n\nkind: StatefulSet\n\nmetadata: name: sharded-memcache\n\nspec:\n\nserviceName: \"memcache\"\n\nreplicas: 3\n\ntemplate:\n\nmetadata: labels:\n\napp: memcache\n\nspec:\n\nterminationGracePeriodSeconds: 10\n\ncontainers:\n\nname: memcache\n\nimage: memcached ports:\n\ncontainerPort: 11211\n\nname: memcache\n\nSave this to a file named memcached-shards.yaml, and you can deploy this\n\nwith kubectl create -f memcached-shards.yaml . This will\n\ncreate three containers running memcached.\n\nAs with the sharded Redis example, we also need to create a Kubernetes\n\nService that will create DNS names for the replicas we have created.\n\nThe service looks like this:\n\napiVersion: v1 kind: Service\n\nmetadata:\n\nname: memcache\n\nlabels:\n\napp: memcache\n\nspec:\n\nports: - port: 11211\n\nname: memcache\n\nclusterIP: None\n\nselector:\n\napp: memcache\n\nSave this to a file named memcached-service.yaml and deploy it with\n\nkubectl create -f memcached-service.yaml . You should\n\nnow have DNS entries for memcache-0.memcache , memcache-\n\n1.memcache , etc. As with Redis, we can use these names to configure\n\ntwemproxy :\n\nmemcache:\n\nlisten: 127.0.0.1:11211\n\nhash: fnv1a_64\n\ndistribution: ketama\n\nauto_eject_hosts: true\n\ntimeout: 400 server_retry_timeout: 2000\n\nserver_failure_limit: 1\n\nservers:\n\nmemcache-0.memcache:11211:1\n\nmemcache-1.memcache:11211:1\n\nmemcache-2.memcache:11211:1\n\nIn this config, you can see that we are serving the memcache protocol on\n\nlocalhost:11211 so that the application container can access the\n\nambassador. We will deploy this into our ambassador pod using a\n\nKubernetes ConfigMap object that we can create with: kubectl\n\ncreate configmap --from-file=nutcracker.yaml twem-\n\nconfig .\n\nFinally, all of the preparations are done, and we can deploy our ambassador\n\nexample. We define a pod that looks like this:\n\napiVersion: v1\n\nkind: Pod\n\nmetadata:\n\nname: sharded-memcache-ambassador spec:\n\ncontainers:\n\n# This is where the application container wou\n\n# - name: nginx\n\n# image: nginx\n\n# This is the ambassador container\n\nname: twemproxy\n\nimage: ganomede/twemproxy\n\ncommand:\n\nnutcracker\n\n-c\n\n/etc/config/nutcracker.yaml\n\n-v - 7\n\n-s - 6222\n\nvolumeMounts: - name: config-volume mountPath: /etc/config\n\nvolumes:\n\nname: config-volume\n\nconfigMap:\n\nname: twem-config\n\nYou can save this to a file named memcached-ambassador-pod.yaml, and\n\nthen deploy it with:\n\nkubectl create -f memcached-ambassador-pod.yaml\n\nOf course, we don’t have to use the ambassador pattern if we don’t want to.\n\nAn alternative is to deploy a replicated shard router service. There are\n\ntrade-offs between using an ambassador versus using a shard routing\n\nservice. The value of the service is a reduction of complexity. You don’t\n\nhave to deploy the ambassador with every pod that wants to access the\n\nsharded memcache service; it can be accessed via a named and load-\n\nbalanced service. The downside of a shared service is twofold. First,\n\nbecause it is a shared service, you will have to scale it larger as demand\n\nload increases. Second, using the shared service introduces an extra\n\nnetwork hop that will add some latency to requests and contribute network\n\nbandwidth to the overall distributed system.\n\nTo deploy a shared routing service, you need to change the twemproxy\n\nconfiguration slightly so that it listens on all interfaces, not just localhost:\n\nmemcache:\n\nlisten: 0.0.0.0:11211\n\nhash: fnv1a_64 distribution: ketama\n\nauto_eject_hosts: true\n\ntimeout: 400\n\nserver_retry_timeout: 2000\n\nserver_failure_limit: 1\n\nservers:\n\nmemcache-0.memcache:11211:1\n\nmemcache-1.memcache:11211:1\n\nmemcache-2.memcache:11211:1\n\nYou can save this to a file named shared-nutcracker.yaml, and then create a\n\ncorresponding ConfigMap using kubectl :\n\nkubectl create configmap --from-file=shared-nutcr\n\nThen you can turn up the replicated shard routing service as a\n\nDeployment :\n\napiVersion: extensions/v1\n\nkind: Deployment\n\nmetadata:\n\nname: shared-twemproxy\n\nspec:\n\nreplicas: 3\n\ntemplate:\n\nmetadata: labels:\n\napp: shared-twemproxy\n\nspec:\n\ncontainers:\n\nname: twemproxy\n\nimage: ganomede/twemproxy\n\ncommand:\n\nnutcracker\n\n-c\n\n/etc/config/shared-nutcracker.yaml\n\n-v\n\n7\n\n-s - 6222\n\nvolumeMounts:\n\nname: config-volume\n\nmountPath: /etc/config\n\nvolumes:\n\nname: config-volume\n\nconfigMap:\n\nname: shared-twem-config\n\nIf you save this to shared-twemproxy-deploy.yaml, you can create the\n\nreplicated shard router using kubectl :\n\nkubectl create -f shared-twemproxy-deploy.yaml\n\nTo complete the shard router, we have to declare a load balancer to process\n\nrequests:\n\nkind: Service\n\napiVersion: v1\n\nmetadata:\n\nname: shard-router-service\n\nspec:\n\nselector:\n\napp: shared-twemproxy\n\nports:\n\nprotocol: TCP\n\nport: 11211\n\ntargetPort: 11211\n\nThis load balancer can be created using kubectl create -f\n\nshard-router-service.yaml .\n\nAn Examination of Sharding Functions\n\nSo far we’ve discussed the design and deployment of both simple sharded\n\nand replicated sharded caches, but we haven’t spent very much time\n\nconsidering how traffic is routed to different shards. Consider a sharded\n\nservice where you have 10 independent shards. Given some specific user\n\nrequest Req, how do you determine which shard S in the range from zero to\n\nnine should be used for the request? This mapping is the responsibility of\n\nthe sharding function. A sharding function is very similar to a hashing\n\nfunction, which you may have encountered when learning about hashtable\n\ndata structures. Indeed, a bucket-based hashtable could be considered an\n\nexample of a sharded service. Given both Req and Shard, then the role of\n\nthe sharding function is to relate them together, specifically:\n\nShard = ShardingFunction(Req)\n\nCommonly, the sharding function is defined using a hashing function and\n\nthe modulo (%) operator. Hashing functions are functions that transform an\n\narbitrary object into an integer hash. The hash function has two important\n\ncharacteristics for our sharding:\n\nDeterminism\n\nThe output should always be the same for a unique input.\n\nUniformity\n\nThe distribution of outputs across the output space should be equal.\n\nFor our sharded service, determinism and uniformity are the most important\n\ncharacteristics. Determinism is important because it ensures that a particular\n\nrequest R always goes to the same shard in the service. Uniformity is\n\nimportant because it ensures that load is evenly spread between the different\n\nshards.\n\nFortunately for us, modern programming languages include a wide variety\n\nof high-quality hash functions. However, the outputs of these hash functions\n\nare often significantly larger than the number of shards in a sharded service.\n\nConsequently, we use the modulo operator (%) to reduce a hash function to\n\nthe appropriate range. Returning to our sharded service with 10 shards, we\n\ncan see that we can define our sharding function as:\n\nShard = hash(Req) % 10\n\nIf the output of the hash function has the appropriate properties in terms of\n\ndeterminism and uniformity, those properties will be preserved by the\n\nmodulo operator.\n\nSelecting a Key\n\nGiven this sharding function, it might be tempting to simply use the hashing\n\nfunction that is built into the programming language, hash the entire object,\n\nand call it a day. The result of this, however, will not be a very good\n\nsharding function.\n\nTo understand this, consider a simple HTTP request that contains three\n\nthings:\n\nThe time of the request\n\nThe source IP address from the client\n\nThe HTTP request path (e.g., /some/page.html)\n\nIf we use a simple object-based hashing function, shard ( request ),\n\nthen it is clear that {12:00, 1.2.3.4, /some/file.html} has a\n\ndifferent shard value than {12:01, 5.6.7.8,\n\n/some/file.html} . The output of the sharding function is different\n\nbecause the client’s IP address and the time of the request are different\n\nbetween the two requests. But of course, in most cases, the IP address of the\n\nclient and the time of the request don’t impact the response to the HTTP\n\nrequest. Consequently, instead of hashing the entire request object, a much\n\nbetter sharding function would be shard ( request.path ). When we\n\nuse request.path as the shard key, then we map both requests to the\n\nsame shard, and thus the response to one request can be served out of the\n\ncache to service the other.\n\nOf course, sometimes client IP is important to the response that is returned\n\nfrom the frontend. For example, client IP may be used to look up the\n\ngeographic region that the user is located in, and different content (e.g.,\n\ndifferent languages) may be returned to different IP addresses. In such\n\ncases, the previous sharding function shard ( request.path ) will\n\nactually result in errors, since a cache request from a French IP address may\n\nbe served a result page from the cache in English. In such cases, the cache\n\nfunction is too general, as it groups together requests that do not have\n\nidentical responses.\n\nGiven this problem, it would be tempting then to define our sharding\n\nfunction as shard ( request.ip, request.path ), but this\n\nsharding function has problems as well. It will cause two different French\n\nIP addresses to map to different shards, thus resulting in inefficient\n\nsharding. This shard function is too specific, as it fails to group together\n\nrequests that are identical. A better sharding function for this situation\n\nwould be:\n\nshard ( country ( request.ip ), request.path )\n\nThis first determines the country from the IP address, and then uses that\n\ncountry as part of the key for the sharding function. Thus, multiple requests\n\nfrom France will be routed to one shard, while requests from the United\n\nStates will be routed to a different shard.\n\nDetermining the appropriate key for your sharding function is vital to the\n\noptimal design of a sharded system. Determining the correct shard key\n\nrequires an understanding of the requests that you expect to see.\n\nConsistent Hashing Functions\n\nSetting up the initial shards for a new service is relatively straightforward:\n\nyou set up the appropriate shards and the roots to perform the sharding, and\n\nyou are off to the races. However, what happens when you need to change\n\nthe number of shards in your sharded service? Such “re-sharding” is often a\n\ncomplicated process.\n\nTo understand why this is true, consider the sharded cache previously\n\nexamined. Certainly, scaling the cache from 10 to 11 replicas is\n\nstraightforward to do with a container orchestrator, but consider the effect\n\nof changing the scaling function from hash(Req) % 10 to hash(Req) % 11.\n\nWhen you deploy this new scaling function, a large number of requests are\n\ngoing to be mapped to a different shard than the one they were previously\n\nmapped to. In a sharded cache, this is going to temporarily cause a dramatic\n\nincrease in your miss rate until the cache is repopulated with responses for\n\nthe new requests that have been mapped to that cache shard by the new\n\nsharding function. In the worst case, rolling out a new sharding function for\n\nyour sharded cache will be equivalent to a complete cache failure.\n\nTo resolve these kinds of problems, many sharding functions use consistent\n\nhashing functions. Consistent hashing functions are special hash functions\n\nthat are guaranteed to only remap # keys / # shards, when being resized to #\n\nshards. For example, if we use a consistent hashing function for our sharded\n\ncache, moving from 10 to 11 shards will only result in remapping < 10% (K\n\n/ 11) keys. This is dramatically better than losing the entire sharded service.\n\nHands On: Building a Consistent HTTP Sharding Proxy\n\nTo shard HTTP requests, the first question to answer is what to use as the\n\nkey for the sharding function. Though there are several options, a good\n\ngeneral-purpose key is the request path as well as the fragment and query\n\nparameters (i.e., everything that makes the request unique). Note that this\n\ndoes not include cookies from the user or the language/location (e.g.,\n\nEN_US). If your service provides extensive customization to users or their\n\nlocation, you will need to include them in the hash key as well.\n\nWe can use the versatile nginx HTTP server for our sharding proxy:\n\nworker_processes 5; error_log error.log;\n\npid nginx.pid;\n\nworker_rlimit_nofile 8192;\n\nevents {\n\nworker_connections 1024;\n\n}\n\nhttp {\n\n# define a named 'backend' that we can use in\n\n# below.\n\nupstream backend {\n\n# Has the full URI of the request and use\n\nhash $request_uri consistent\n\nserver web-shard-1.web;\n\nserver web-shard-2.web;\n\nserver web-shard-3.web;\n\n}\n\nserver {\n\nlisten localhost:80;\n\nlocation / {\n\nproxy_pass http://backend; }\n\n}\n\n}\n\nNote that we chose to use the full request URI as the key for the hash and\n\nuse the key word consistent to indicate that we want to use a\n\nconsistent hashing function.\n\nSharded Replicated Serving\n\nMost of the examples in this chapter so far have described sharding in terms\n\nof cache serving. But, of course, caches are not the only kinds of services\n\nthat can benefit from sharding. Sharding is useful when considering any\n\nsort of service where there is more data than can fit on a single machine. In\n\ncontrast to previous examples, the key and sharding function are not a part\n\nof the HTTP request, but rather some context for the user.\n\nFor example, consider implementing a large-scale multiplayer game. Such a\n\ngame world is likely to be far too large to fit on a single machine. However,\n\nplayers who are distant from each other in this virtual world are unlikely to\n\ninteract. Consequently, the world of the game can be sharded across many\n\ndifferent machines. The sharding function is keyed off of the player’s\n\nlocation so that all players in a particular location land on the same set of\n\nservers.\n\nHot Sharding Systems\n\nIdeally the load on a sharded cache will be perfectly even, but in many\n\ncases this isn’t true, and “hot shards” appear because organic load patterns\n\ndrive more traffic to one particular shard.\n\nAs an example of this, consider a sharded cache for a user’s photos; when a\n\nparticular photo goes viral and suddenly receives a disproportionate amount\n\nof traffic, the cache shard containing that photo will become “hot.” When\n\nthis happens, with a replicated sharded cache, you can scale the cache shard\n\nto respond to the increased load. Indeed, if you set up autoscaling for each\n\ncache shard, you can dynamically grow and shrink each replicated shard as\n\nthe organic traffic to your service shifts around. An illustration of this\n\nprocess is shown in Figure 7-3. Initially the sharded service receives equal\n\ntraffic to all three shards. Then the traffic shifts so that Shard A is receiving\n\nfour times as much traffic as Shard B and Shard C. The hot sharding system\n\nmoves Shard B to the same machine as Shard C, and replicates Shard A to a\n\nsecond machine. Traffic is now, once again, equally shared among replicas.\n\nFigure 7-3. An example of a hot sharded system: initially the shards are evenly distributed, but when extra traffic comes to shard A, it is replicated to two machines, and shards B and C are combined on a\n\nsingle machine\n\nSummary\n\nThis chapter introduced the concept of sharding, which directs particular\n\nrequests to particular groups of machines instead of load-balancing all\n\ntraffic to all replicas of a service. An example was given of how sharding\n\ncan significantly improve the efficiency and performance of a caching\n\nsystem attached to a serving API. Sharding is not just useful for caching,\n\nhowever. Systems like large-scale open world game servers and databases\n\ncan scale beyond the size limits of a single machine by sharding their data\n\nacross multiple different shard servers. Sharding also helps with isolation,\n\nlimiting the blast radius of poison requests to a subset of all servers. Setting\n\nup a sharded system requires a careful understanding and design of both the\n\nshard key and the key hashing function. Identifying the right way to\n\npartition your data can often make the difference between a scalable system\n\nand one which runs into bottlenecks as it scales. Understanding the details\n\nof sharding is critical to building larger and more complex distributed\n\nsystems.\n\nOceanofPDF.com\n\nChapter 8. Scatter/Gather\n\nSo far we’ve examined systems that replicate for scalability in terms of the\n\nnumber of requests processed per second (the stateless replicated pattern),\n\nas well as scalability for the size of the data (the sharded data pattern). In\n\nthis chapter we introduce the scatter/gather pattern, which uses replication\n\nfor scalability in terms of time. Specifically, the scatter/gather pattern\n\nallows you to achieve parallelism in servicing requests, enabling you to\n\nservice them significantly faster than you could if you had to service them\n\nsequentially.\n\nLike replicated and sharded systems, the scatter/gather pattern is a tree\n\npattern with a root that distributes requests and leaves that process those\n\nrequests. However, in contrast to replicated and sharded systems,\n\nscatter/gather requests are simultaneously farmed out to all of the replicas in\n\nthe system. Each replica does a small amount of processing and then returns\n\na fraction of the result to the root. The root server then combines the various\n\npartial results together to form a single complete response to the request and\n\nthen sends this request back out to the client. The scatter/gather pattern is\n\nillustrated in Figure 8-1.",
      "page_number": 135
    },
    {
      "number": 8,
      "title": "Scatter/Gather",
      "start_page": 162,
      "end_page": 176,
      "detection_method": "regex_chapter_title",
      "content": "Figure 8-1. A scatter/gather pattern\n\nScatter/gather is quite useful when you have a large amount of mostly\n\nindependent processing that is needed to handle a particular request. With\n\nsuch problems, it is relatively easy to spread the computation required\n\nacross multiple different independent processors.\n\nWhereas the previous examples of caching sharded the data to maximize\n\nthe efficiency of memory usage, the scatter/gather algorithm shards the\n\ncomputation necessary to service the request, reducing the latency via\n\nparallel processing. Though sometimes, as we will see, the data is sharded\n\nas well.\n\nScatter/Gather with Root Distribution\n\nThe simplest form of scatter/gather is one in which each leaf is entirely\n\nhomogenous but the work is distributed to a number of different leaves in\n\norder to improve the performance of the request. This pattern is equivalent\n\nto solving an “embarrassingly parallel” problem. The problem can be\n\nbroken up into many different pieces, and each piece can be put back\n\ntogether with all of the other pieces to form a complete answer.\n\nTo understand this in more concrete terms, imagine that you need to service\n\na user request R, and it takes one minute for a single core to produce the\n\nanswer A to this request. If we program a multithreaded application, we can\n\nparallelize this request on a single machine by using multiple cores. Given\n\nthis approach and a 30-core processor (yes, typically it would be a 32-core\n\nprocessor, but 30 makes the math cleaner), we can reduce the time that it\n\ntakes to process a single request down to 2 seconds (60 seconds of\n\ncomputation split across 30 threads for computation is equal to 2 seconds).\n\nBut even 2 seconds is pretty slow to service a user’s web request.\n\nAdditionally, truly achieving a completely parallel speed-up on a single\n\nprocess is going to be tricky, as things like memory, network, or disk\n\nbandwidth start to become the bottleneck. Instead of (or in addition to)\n\nparallelizing an application across cores on a single machine, we can use\n\nthe scatter/gather pattern to parallelize requests across multiple processes on\n\nmany different machines. In this way, we can improve our overall latency\n\nrequests, since we are no longer bound by the number of cores we can get\n\non a single machine, as well as ensure that the bottleneck in our process\n\ncontinues to be CPU, since the memory, network, and disk bandwidth are\n\nall spread across a number of different machines. Additionally, because\n\nevery machine in the scatter/gather tree is capable of handling every\n\nrequest, the root of the tree can dynamically dispatch load to different nodes\n\nat different times depending on their responsiveness. If, for some reason, a\n\nparticular leaf node is responding more slowly than other machines (e.g., it\n\nhas a noisy neighbor process that is interfering with resources), then the\n\nroot can dynamically redistribute load to assure a fast response.\n\nHands On: Distributed Document Search\n\nTo see an example of scatter/gather in action, consider the task of searching\n\nacross a large database of documents for all documents that contain the\n\nwords “cat” and “dog.” One way to perform this search would be to open\n\nup all of the documents, read through the entire set, searching for the words\n\nin each document, and then return to the user the set of documents that\n\ncontain both words.\n\nAs you might imagine, this is quite a slow process because it requires\n\nopening and reading through a large number of files for each request. To\n\nmake request processing faster, you can build an index. Just like in the back\n\nof a reference book, the index is effectively a hashtable, where the keys are\n\nindividual words (e.g., “cat”) and the values are a list of documents (or\n\npages) containing that word.\n\nNow, instead of searching through every document, finding the documents\n\nthat match any one word is as easy as doing a lookup in this hashtable.\n\nHowever, we have lost one important ability. Remember that we were\n\nlooking for all documents that contained “cat” and “dog.” Since the index\n\nonly has single words, not conjunctions of words, we still need to find the\n\ndocuments that contain both words. Luckily, this is just an intersection of\n\nthe sets of documents returned for each word.\n\nWe could implement this algorithm on a single machine. Step one would be\n\nto use the index to look up the list of documents containing cat. Step two\n\nwould be to use the index to look up the list of documents containing dog.\n\nAnd the final step would be to find the intersection, the documents which\n\nare present in both lists. Looking at these steps, it’s clear that the lookup for\n\ncat and the lookup for dog are independent; they can easily be performed in\n\nparallel and can even be performed on different machines. Calculating the\n\nintersection, however, requires both lists of documents, and thus is\n\ndependent on the data from the first two steps.\n\nGiven this understanding, we can implement this document search as an\n\nexample of the scatter/gather pattern. When a request comes in to the\n\ndocument search root, it parses the request and farms out to two leaf\n\nmachines (one for the word “cat” and one for the word “dog”). Each of\n\nthese machines returns a list of documents that match one of the words, and\n\nthe root node returns the list of documents containing both “cat” and “dog.”\n\nA diagram of this process is shown in Figure 8-2: the leaf returns {doc1,\n\ndoc2, doc4} for “cat” and {doc1, doc3, doc4} for “dog,” so\n\nthe root finds the intersection and returns {doc1, doc4} .\n\nFigure 8-2. Example of a term-sharded scatter/gather system\n\nScatter/Gather with Leaf Sharding\n\nWhile applying the replicated data scatter/gather pattern allows you to\n\nreduce the processing time required for handling user requests, it doesn’t\n\nallow you to scale beyond an amount of data that can be held in the memory\n\nor disk of a single machine. Much like the replicated serving pattern that\n\nwas previously described, it is simple to build a replicated scatter/gather\n\nsystem. But at a certain data size, it is necessary to introduce sharding in\n\norder to build a system that can hold more data than can be stored on a\n\nsingle machine.\n\nPreviously, when sharding was introduced to scale replicated systems, the\n\nsharding was done at a per-request level. Some part of the request was used\n\nto determine where the request was sent. That replica then handled all of the\n\nprocessing for the request, and the response was handed back to the user.\n\nInstead, with scatter/gather sharding, the request is sent to all of the leaf\n\nnodes (or shards) in the system. Each leaf node processes the request using\n\nthe data that it has loaded in its shard. This partial response is then returned\n\nto the root node that requested data, and that root node merges all of the\n\nresponses together to form a comprehensive response for the user.\n\nAs a concrete example of this sort of architecture, consider implementing\n\nsearch across a very large document set (all patents in the world, for\n\nexample); in such a case, the data is too large to fit in the memory of a\n\nsingle machine, so instead the data is sharded across multiple replicas. For\n\nexample, patents 0–100,000 might be on the first machine, 100,001–\n\n200,000 on the next machine, and so forth. (Note that this is not actually a\n\ngood sharding scheme since it will continually force us to add new shards\n\nas new patents are registered. In practice, we’d likely use the patent number\n\nmodulo the total number of shards.)\n\nWhen a user submits a request to find a particular word (e.g., “rockets”) in\n\nall of the patents in the index, that request is sent to each shard, which\n\nsearches through its patent shard for patents that match the word in the\n\nquery. Any matches that are found are returned to the root node in response\n\nto the shard request. The root node then collates all of these responses\n\ntogether into a single response that contains all the patents that match the\n\nparticular word. The operation of this search index is illustrated in Figure 8-\n\n3.\n\nHands On: Sharded Document Search\n\nThe previous example scattered the different term requests across the\n\ncluster, but this only works if all of the documents are present on all of the\n\nmachines in the scatter/gather tree. If there is not enough room for all of the\n\ndocuments in all of the leaves in the tree, then sharding must be used to put\n\ndifferent sets of documents onto different leaves.\n\nThis means that when a user makes a request for all documents that match\n\nthe words “cat” and “dog,” the request is actually sent out to every leaf in\n\nthe scatter/gather system. Each leaf node returns the set of documents that it\n\nknows about that matches “cat” and “dog.” Previously, the root node was\n\nresponsible for performing the intersection of the two sets of documents\n\nreturned for two different words. In the sharded case, the root node is\n\nresponsible for generating the union of all of the documents returned by all\n\nof the different shards and returning this complete set of documents back up\n\nto the user.\n\nIn Figure 8-3, the first leaf serves documents 1 through 10 and returns\n\n{doc1, doc5} . The second leaf serves documents 11 through 20 and\n\nreturns {doc15} . The third leaf serves documents 21 through 30 and\n\nreturns {doc22, doc28} . The root combines all of these responses\n\ntogether into a single response and returns {doc1, doc5, doc15,\n\ndoc22, doc28} .\n\nFigure 8-3. Conjunctive query executing in a scatter/gather search system\n\nChoosing the Right Number of Leaves\n\nIt might seem that in the scatter/gather pattern, replicating out to a very\n\nlarge number of leaves would always be a good idea. You parallelize your\n\ncomputation and consequently reduce the clock time required to process\n\nany particular request. However, increased parallelization comes at a cost,\n\nand thus choosing the right number of leaf nodes in the scatter/gather\n\npattern is critical to designing a performant distributed system.\n\nTo understand how this can happen, it’s worth considering two things. The\n\nfirst is that processing any particular request has a certain amount of\n\noverhead. This includes the time spent parsing a request, sending requests\n\nacross the network, and performing encryption. In general, the overhead\n\ndue to system request handling is constant and significantly less than the\n\ntime spent in user code processing the request. Consequently, this overhead\n\ncan generally be ignored when assessing the performance of the\n\nscatter/gather pattern. However, it is important to understand that you pay\n\nthe cost of this overhead on every leaf node in the scatter/gather system.\n\nEach leaf node you add to increase parallelism increases the total overhead\n\ncost. Additionally, increasing the parallelism reduces the time spent\n\nperforming “useful” computation on each leaf node, until eventually the\n\ntime spent on the computational overhead is significantly greater than the\n\ntime spent performing “useful” computation. When this happens, you can\n\nno longer improve performance by adding additional leaf nodes. This\n\nmeans that the gains of parallelization are asymptotic. You cannot\n\ncontinually add nodes and expect to improve performance.\n\nIn addition to the fact that adding more leaf nodes may not actually speed\n\nup processing, scatter/gather systems also suffer from the “straggler”\n\nproblem, which you may have encountered in MapReduce big-data systems\n\nlike Hadoop. To understand how this works, it is important to remember\n\nthat in a scatter/gather system, the root node waits for requests from all of\n\nthe leaf nodes to return before sending a response back to the end user.\n\nSince data from every leaf node is required, the overall time it takes to\n\nprocess a user request is defined by the slowest leaf node that sends a\n\nresponse. To understand the impact of this, imagine that we have a service\n\nthat has a 99th percentile latency of 2 seconds. This means that on average\n\none request out of every 100 has a latency of 2 seconds, or put another way,\n\nthere is a 1% chance that a request will take 2 seconds. This may be totally\n\nacceptable at first glance: a single user out of 100 has a slow request.\n\nHowever, consider how this actually works in a scatter/gather system. Since\n\nthe time of the user request is defined by the slowest response, we need to\n\nconsider not a single request but all requests scattered out to the various leaf\n\nnodes.\n\nLet’s see what happens when we scatter out to five leaf nodes. In this\n\nsituation, there is a 5% chance that one of these five scatter requests has a\n\nlatency of 2 seconds (0.99 × 0.99 × 0.99 × 0.99 × 0.99 = 0.95). This means\n\nthat our 99th percentile latency for individual requests becomes a 95th\n\npercentile latency for our complete scatter/gather system. And it only gets\n\nworse from there: if we scatter out to 100 leaves, then we are more or less\n\nguaranteeing that our overall latency for all requests will be 2 seconds.\n\nTogether, these complications of scatter/gather systems lead us to some\n\nconclusions:\n\nIncreased parallelism doesn’t always speed things up because of\n\noverhead on each node.\n\nIncreased parallelism doesn’t always speed things up because of the\n\nstraggler problem.\n\nThe performance of the 99th percentile is more important than in other\n\nsystems because each user request actually becomes numerous requests\n\nto the service.\n\nThe same straggler problem applies to availability. If you issue a request to\n\n100 leaf nodes, and the probability that any leaf node failing is 1 in 100,\n\nyou are again practically guaranteed to fail every single user request.\n\nScaling Scatter/Gather for Reliability and\n\nScale\n\nOf course, just as with a sharded system, having a single replica of a\n\nsharded scatter/gather system is likely not the desirable design choice. A\n\nsingle replica means that if it fails, all scatter/gather requests will fail for the\n\nduration that the shard is unavailable because all requests are required to be\n\nprocessed by all leaf nodes in the scatter/gather pattern. Likewise, upgrades\n\nwill take out a percentage of your shards, so an upgrade while under user-\n\nfacing load is no longer possible. Finally, the computational scale of your\n\nsystem will be limited by the load that any single node is capable of\n\nachieving. Ultimately, this limits your scale, and as we have seen in\n\nprevious sections, you cannot simply increase the number of shards in order\n\nto improve the computational power of a scatter/gather pattern.\n\nGiven these challenges of reliability and scale, the correct approach is to\n\nreplicate each of the individual shards so that instead of a single instance at\n\neach leaf node, there is a replicated service that implements each leaf shard.\n\nThis replicated sharded scatter/gather pattern is shown in Figure 8-4.\n\nFigure 8-4. A sharded replicated scatter/gather system\n\nBuilt this way, each leaf request from the root is actually load balanced\n\nacross all healthy replicas of the shard. This means that if there are any\n\nfailures, they won’t result in a user-visible outage for your system.\n\nLikewise, you can safely perform an upgrade under load, since each\n\nreplicated shard can be upgraded one replica at a time. Indeed, you can\n\nperform the upgrade across multiple shards simultaneously, depending on\n\nhow quickly you want to perform the upgrade.\n\nSummary\n\nThis chapter describes the scatter/gather pattern, a distributed system for\n\nimproving computation speed and reducing the latency of handling any\n\nparticular request. Whereas previous serving patterns focused on scaling for\n\nadditional load or scaling for additional data, scatter/gather scales to\n\nincrease responsiveness, to reduce the user-perceived time spent processing\n\na request. We also saw how compute sharding with scatter/gather can be\n\ncombined with data sharding for large indexes, as well as adding replication\n\nto improve reliability and redundancy. Any large-scale distributed system is\n\noften a combination of multiple different serving patterns.\n\nOceanofPDF.com\n\nChapter 9. Functions and Event-Driven\n\nProcessing\n\nSo far, we have examined design for systems with long-running\n\ncomputation. The servers that handle user requests are always up and\n\nrunning. This pattern is the right one for many applications that are under\n\nheavy load, keep a large amount of data in memory, or require some sort of\n\nbackground processing. However, there is a class of applications that might\n\nonly need to temporarily come into existence to handle a single request, or\n\nsimply need to respond to a specific event. This style of request or event-\n\ndriven application design has flourished recently as large-scale public cloud\n\nproviders have developed function-as-a-service (FaaS) products. More\n\nrecently, FaaS implementations have also emerged running on top of cluster\n\norchestrators in private cloud or physical environments. This chapter\n\ndescribes emerging architectures for this new style of computing. In many\n\ncases, FaaS is a component in a broader architecture rather than a complete\n\nsolution.",
      "page_number": 162
    },
    {
      "number": 9,
      "title": "Functions and Event-Driven",
      "start_page": 177,
      "end_page": 197,
      "detection_method": "regex_chapter_title",
      "content": "NOTE\n\nOftentimes, FaaS is referred to as serverless computing. And while this is true (you don’t see the\n\nservers in FaaS), it’s worth differentiating between event-driven FaaS and the broader notion of\n\nserverless computing. Indeed, serverless computing can apply to a wide variety of computing\n\nservices; for example, a multitenant container orchestrator (container-as-a-service) is serverless but\n\nnot event-driven. Conversely, an open source FaaS running on a cluster of physical machines that\n\nyou own and administer is event-driven but not serverless. Understanding this distinction enables you\n\nto determine when event-driven, serverless, or both is the right choice for your application.\n\nDetermining When FaaS Makes Sense\n\nAs with many tools for developing a distributed system, it can be tempting\n\nto see a particular solution like event-driven processing as a universal\n\nhammer. However, the truth is that it is best suited to a particular set of\n\nproblems. Within a particular context it is a powerful tool, but stretching it\n\nto fit all applications or systems will lead to overly complicated, brittle\n\ndesigns. Especially since FaaS is a relatively new computing tool, before\n\ndiscussing specific design patterns, it is worth discussing the benefits,\n\nlimitations, and optimal situations for employing event-driven computing.\n\nThe Benefits of FaaS\n\nThe benefits of FaaS are primarily for the developer. It dramatically\n\nsimplifies the distance from code to running service. Because there is no\n\nartifact to create or push beyond the source code itself, FaaS makes it\n\nsimple to go from code on a laptop or web browser to running code in the\n\ncloud.\n\nLikewise, the code that is deployed is managed and scaled automatically. As\n\nmore traffic is loaded onto the service, more instances of the function are\n\ncreated to handle that increase in traffic. If a function fails due to\n\napplication or machine failures, it is automatically restarted on some other\n\nmachine.\n\nFinally, much like containers, functions are an even more granular building\n\nblock for designing distributed systems. Functions are stateless, and thus\n\nany system you build on top of functions is inherently more modular and\n\ndecoupled than a similar system built into a single binary. But, of course,\n\nthis is also the challenge of developing systems in FaaS. The decoupling is\n\nboth a strength and a weakness. The following section describes some of\n\nthe challenges that come from developing systems using FaaS.\n\nThe Challenges of FaaS\n\nAs described in “The Benefits of FaaS”, developing systems using FaaS\n\nforces you to strongly decouple each piece of your service. Each function is\n\nentirely independent. The only communication is across the network, and\n\neach function instance cannot have local memory, requiring all states to be\n\nstored in a storage service. This forced decoupling can improve the agility\n\nand speed with which you can develop services, but it can also significantly\n\ncomplicate the operations of the same service.\n\nIn particular, it is often quite difficult to obtain a comprehensive view of\n\nyour service, determine how the various functions integrate with one\n\nanother, and understand when things go wrong, and why they go wrong.\n\nAdditionally, the request-based and serverless nature of functions means\n\nthat certain problems are quite difficult to detect. As an example, consider\n\nthe following functions:\n\nfunctionA() which calls functionB()\n\nfunctionB() which calls functionC()\n\nfunctionC() which calls back to functionA()\n\nNow consider what happens when a request comes into any of these\n\nfunctions: it kicks off an infinite loop that only terminates when the original\n\nrequest times out (and possibly not even then) or when you run out of\n\nmoney to pay for requests in the system. Obviously, the above example is\n\nquite contrived, but it is actually quite difficult to detect in your code. Since\n\neach function is radically decoupled from the other functions, there is no\n\nreal representation of the dependencies or interactions between different\n\nfunctions. These problems are not unsolvable, and I expect that as FaaSs\n\nmature, more analysis and debugging tools will provide a richer experience\n\nto understand how and why an application comprised of FaaS is performing\n\nthe way that it does.\n\nFor now, when adopting FaaS, you must be vigilant to adopt rigorous\n\nmonitoring and alerting for how your system is behaving so that you can\n\ndetect situations and correct them before they become significant problems.\n\nOf course, the complexity introduced by monitoring flies somewhat in the\n\nface of the simplicity of deploying to FaaS, which is friction that your\n\ndevelopers must overcome.\n\nThe Need for Background Processing\n\nFaaS is inherently an event-based application model. Functions are\n\nexecuted in response to discrete events that occur and trigger the execution\n\nof the functions. Additionally, because of the serverless nature of the\n\nimplementation of these services, the runtime of any particular function\n\ninstance is generally time bounded. This means that FaaS is usually a poor\n\nfit for situations that require processing. Examples of such background\n\nprocessing might be transcoding a video, compressing log files, or other\n\nsorts of low-priority, long-running computations. In many cases, it is\n\npossible to set up a scheduled trigger that synthetically generates events in\n\nyour functions on a particular schedule. Though this is a good fit for\n\nresponding to temporal events (e.g., firing a text-message alarm to wake\n\nsomeone up), it is still not sufficient infrastructure for generic background\n\nprocessing. To achieve that, you need to launch your code in an\n\nenvironment that supports long-running processes. And this generally\n\nmeans switching to a pay-per-consumption rather than pay-per-request\n\nmodel for the parts of your application that do background processing.\n\nThe Need to Hold Data in Memory\n\nIn addition to the operational challenges, there are some architectural\n\nlimitations that make FaaS ill-suited for some types of applications. The\n\nfirst of these limitations is the need to have a significant amount of data\n\nloaded into memory in order to process user requests. There are a variety of\n\nservices (e.g., serving a search index of documents) that require a great deal\n\nof data to be loaded in memory in order to service user requests. Even with\n\na relatively fast storage layer, loading such data can take significantly\n\nlonger than the desired time to service a user request. Because with FaaS,\n\nthe function itself may be dynamically spun up in response to a user request\n\nwhile the user is waiting, the need to load a lot of detail may significantly\n\nimpact the latency that the user perceives while interacting with your\n\nservice. Of course, once your FaaS has been created, it may handle a large\n\nnumber of requests, so this loading cost can be amortized across a large\n\nnumber of requests. But if you have a sufficient number of requests to keep\n\na function active, then it’s likely you are overpaying for the requests you are\n\nprocessing.\n\nThe Costs of Sustained Request-Based Processing\n\nThe cost model of public cloud FaaS is based on per-request pricing. This\n\napproach is great if you only have a few requests per minute or hour. In\n\nsuch a situation, you are idle most of the time, and given a pay-per-request\n\nmodel, you are only paying for the time when your service is actively\n\nserving requests. In contrast, if you service requests via a long-running\n\nservice, either in a container or a virtual machine, then you are always\n\npaying for processor cycles that are largely sitting around waiting for a user\n\nrequest.\n\nHowever, as a service grows, the number of requests that you are servicing\n\ngrows to the point where you can keep a processor continuously active\n\nservicing user requests. At this point, the economics of a pay-per-request\n\nmodel start to become bad, and only get worse because the cost of cloud\n\nvirtual machines generally decreases as you add more cores (and also via\n\ncommitted resources like reservations or sustained use discounts), whereas\n\nthe cost per-request largely grows linearly with the number of requests.\n\nConsequently, as your service grows and evolves, it’s highly likely that your\n\nuse of FaaS will evolve as well. One ideal way to scale FaaS is to run an\n\nopen source FaaS that runs on a container orchestrator like Kubernetes.\n\nThat way, you can still take advantage of the developer benefits of FaaS,\n\nwhile taking advantage of the pricing models of virtual machines.\n\nPatterns for FaaS\n\nIn addition to understanding the trade-offs in deploying event-driven or\n\nFaaS architectures as part of your distributed system, understanding the best\n\nways to deploy FaaS is critical to the design of a successful system. This\n\nsection describes some canonical patterns for incorporating FaaS.\n\nThe Decorator Pattern: Request or Response Transformation\n\nFaaS is ideal for deploying simple functions that can take an input,\n\ntransform it into an output, and then pass it on to a different service. This\n\ngeneral pattern can be used to augment or decorate HTTP requests to or\n\nfrom a different service. A basic illustration of this pattern is shown in\n\nFigure 9-1.\n\nFigure 9-1. The decorator pattern applied to HTTP APIs\n\nInterestingly, there are several analogies to this pattern in programming\n\nlanguages. In particular, the decorator pattern from Python is a close\n\nanalogue for the services that a request or response decorator can perform.\n\nBecause decoration transformations are generally stateless, and also\n\nbecause they are often added after the fact to existing code as the service\n\nevolves, they are ideal services to implement via FaaS. Additionally, the\n\nlightness of FaaS means that you can experiment with a variety of different\n\ndecorators before finally adopting one and pulling it more completely into\n\nthe implementation of the service.\n\nA great example of the value of the decorator pattern is adding defaults to\n\nthe input to an HTTP RESTful API. In many cases in the API, there are\n\nfields whose values should have sane defaults if they are left empty. For\n\nexample, you may want a field to default to true , but it’s difficult to\n\naccomplish this in classical JSON, because the default value for a field is\n\nnull , which is generally understood to be false . To resolve this, we\n\ncan add defaulting logic either in the front of the API server or within the\n\napplication code itself (e.g., if (field == null) field =\n\ntrue ). However, both of these solutions are somewhat unappealing since\n\nthe defaulting mechanism is fairly conceptually independent from the\n\nhandling of the request. Instead, we can use the FaaS decorator pattern to\n\ntransform the request in between the user and the service implementation.\n\nGiven the previous discussion of the adapter pattern in the single-node\n\nsection, you may be wondering why we don’t simply package this\n\ndefaulting as an adapter container. And this is a totally reasonable approach,\n\nbut it does mean that we are going to couple the scale of the defaulting\n\nservice with the API service itself. The defaulting is actually a lightweight\n\noperation, and we are likely to need far fewer instances of it than the service\n\nitself to handle the load.\n\nNOTE\n\nFor the examples in this chapter, we are going to use the kubeless FaaS framework. Kubeless is\n\ndeployed on top of the Kubernetes container orchestration service. Assuming that you have\n\nprovisioned a Kubernetes cluster, you can install Kubeless from its releases page. Once you have the\n\nkubeless binary installed, you can install it into your cluster with the following commands:\n\nkubeless install .\n\nKubeless installs itself as a native Kubernetes third-party API. This means that once it is installed,\n\nyou can use the native kubectl command-line tool. For example, you can see deployed functions\n\nusing kubectl get functions . Currently, you should have no functions deployed.\n\nHands On: Adding Request Defaulting Prior to\n\nRequest Processing\n\nTo demonstrate the utility of the decorator pattern for FaaS, consider the\n\ntask of adding default values to a RESTful function call if the values are\n\nmissing. This is quite straightforward to do using FaaS. We’ll write the\n\ndefaulting function using the Python programming language:\n\n# Simple handler function for adding default valu def handler(context):\n\n# Get the input value obj = context.json\n\n# If the 'name' field is not present, set it ra if obj.get(\"name\", None) is None: obj[\"name\"] = random_name() # If the 'color' field is not present, set it t\n\nif obj.get(\"color\", None) is None:\n\nobj[\"color\"] = \"blue\" # Call the actual API, potentially with the new\n\n# values, and return the result\n\nreturn call_my_api(obj)\n\nSave this function in a file named defaults.py. You obviously will want to\n\nupdate the call_my_api code so that it points to the actual API you\n\nwant to call. Once you have finished writing the code, this defaulting\n\nfunction can be installed as a kubeless function using:\n\nkubeless function deploy add-defaults \\ --runtime python27 \\\n\n--handler defaults.handler \\\n\n--from-file defaults.py \\\n\n--trigger-http\n\nIf you want to test the handling of this function, you can also use the\n\nkubeless tool:\n\nkubeless function call add-defaults --data '{\"nam\n\nThe decorator pattern shows just how easy it is to adapt and extend existing\n\nAPIs with additional features like validation or defaulting.\n\nHandling Events\n\nWhile most systems are request-driven, handling a steady stream of user\n\nand API requests, many other systems are more event-driven in nature. The\n\ndifferentiation, in my mind at least, between a request and an event has to\n\ndo with the notion of session. Requests are part of a larger series of\n\ninteractions or sessions; generally each user request is part of a larger\n\ninteraction with a complete web application or API. Events, as I see them,\n\ninstead tend to be single-instance and asynchronous in nature. Events are\n\nimportant and need to be properly handled, but they are fired off from a\n\nmain interaction and responded to some time later. Examples of events\n\ninclude a user signing up for a new service (which might trigger a welcome\n\nemail), someone uploading a file to a shared folder (which might send\n\nnotifications to everyone who has access to the folder), or even a machine\n\nbeing about to reboot (which might notify an operator or automated system\n\nto take appropriate action).\n\nBecause these events tend to be largely independent and stateless in nature,\n\nand because the rate of events can be highly variable, they are ideal\n\ncandidates for event-driven and FaaS architectures. In this role, they are\n\noften deployed alongside a production application server as augmentation to\n\nthe main user experience, or to handle some sort of reactive, background\n\nprocessing. Additionally, because new events are often dynamically added\n\nto the service, the lightweight nature of deploying functions is a good match\n\nfor defining new event handlers. Likewise, because each event is\n\nconceptually independent, the forced decoupling of a functions-based\n\nsystem actually helps reduce the conceptual complexity by enabling a\n\ndeveloper to focus on the steps required to handle just a single type of\n\nevent.\n\nA concrete example of integrating an event-based component to an existing\n\nservice is implementing two-factor authentication. In this case, the event is\n\nthe user logging into a service. The service can generate an event for this\n\naction, fire it into a function-based handler that takes the code and the user’s\n\ncontact information, and sends the two-factor code via text message.\n\nHands On: Implementing Two-Factor Authentication\n\nTwo-factor authentication requires that the user both have something that\n\nthey know (e.g., a password) as well as something that they possess (e.g., a\n\nphone) to be able to log in to the system. Two-factor authentication is\n\nsignificantly more secure than passwords alone since it requires two\n\ndifferent security compromises (a thief learning your password and a thief\n\nstealing your phone) to enable a true security problem.\n\nWhen considering how to implement two-factor authentication, one of the\n\nchallenges is how to handle the request to generate a random code and\n\nregister it with the login service as well as send the text message. It is\n\npossible to add this code to the main login web server. But it is complicated\n\nand monolithic, and forces the act of sending a text message, which can\n\nhave some latency, to be inline with the code that renders the login web\n\npage. This latency produces a substandard user experience.\n\nA better option is to register a FaaS to asynchronously generate the random\n\nnumber, register it with the login service, and send the number to the user’s\n\nphone. In this way, the login server can simply fire an asynchronous web-\n\nhook request to a FaaS, and that FaaS can handle the somewhat slow and\n\nasynchronous task of registering the two-factor code and sending the text\n\nmessage.\n\nTo see how this works in practice, consider the following code:\n\ndef two_factor(context):\n\n# Generate a random six-digit code\n\ncode = random.randint(100000, 999999)\n\n# Register the code with the login service\n\nuser = context.json[\"user\"]\n\nregister_code_with_login_service(user, code)\n\n# Use the twillio library to send texts account = \"my-account-sid\" token = \"my-token\"\n\nclient = twilio.rest.Client(account, token)\n\nuser_number = context.json[\"phoneNumber\"]\n\nmsg = \"Hello {} your authentication code is: {}\n\nmessage = client.api.account.messages.create(to fr\n\nbo\n\nreturn {\"status\": \"ok\"}\n\nWe can then register this FaaS with kubeless :\n\nkubeless function deploy add-two-factor \\\n\n--runtime python27 \\\n\n--handler two_factor.two_factor \\\n\n--from-file two_factor.py \\\n\n--trigger-http\n\nThis function can then be made asynchronously from client-side JavaScript\n\nwhenever the user successfully provides their password. The web UX can\n\nthen immediately display a page to enter the code, and the user (once they\n\nreceive the code as a text message) can supply it to the service, where the\n\ncode has already been registered via our FaaS.\n\nAgain, developing a simple, asynchronous, event-based service that is\n\ntriggered whenever a user logs in is made dramatically simpler using FaaS.\n\nEvent-Based Pipelines\n\nThere are some applications that are inherently easier to think about in\n\nterms of a pipeline of decoupled events. These event pipelines often\n\nresemble the flowcharts of old. They can be represented as a directed graph\n\nof connected event syncs. In the event pipeline pattern, each node is a\n\ndifferent function or webhook, and the edges linking the graph together are\n\nHTTP or other network calls to the function/webhook. In general, there is\n\nno shared state between the different pieces of the pipeline, but there may\n\nbe a context or other reference point that can be used to look up information\n\nin shared storage.\n\nSo, what is the difference between this type of pipeline and a\n\n“microservices” architecture? There are two central differences. The first is\n\nthe main difference between functions in general and long-running services,\n\nwhich is that an event-based pipeline is by its very nature event-driven.\n\nConversely, a microservices architecture features a collection of long-\n\nrunning services. Additionally, event-driven pipelines may be highly\n\nasynchronous and diverse in the things that they connect together. For\n\nexample, while it is difficult to see how a human approving a ticket in a\n\nticketing system like Jira could be integrated into a microservices\n\napplication, it’s quite easy to see how that event could be incorporated into\n\nan event-driven pipeline.\n\nAs an example of this, imagine a pipeline in which the original event is\n\ncode being submitted into a source control system. This event then triggers\n\na build. The build may take multiple minutes to complete, and when it does,\n\nit fires an event to a build analysis function. This function takes different\n\nactions if the build is successful or fails. If the build succeeds, a ticket is\n\ncreated for a human to approve it to be pushed to production. Once the\n\nticket is closed, the act of closing is an event that triggers the actual push to\n\nproduction. If the build fails, a bug is filed on the failure, and the event\n\npipeline terminates.\n\nHands On: Implementing a Pipeline for New User Signup\n\nConsider the task of implementing a new user signup flow. When a new\n\nuser account is created, there are certain things that are always done, such\n\nas sending a welcome email. And there are some things that are optionally\n\ndone, such as registering a user to receive product updates (sometimes\n\nknown as “spam”) via their email.\n\nOne approach to implementing this logic would be to place everything into\n\na single monolithic user-creation server. However, this factoring means that\n\na single team owns the entirety of the user-creation service, and that the\n\nentire experience is deployed as a single service. Both of these mean that it\n\nis more difficult to perform experiments or make changes to the user\n\nexperience.\n\nConsider, instead, implementing the user login experience as an event\n\npipeline with a series of FaaS. In this factoring, the user-creation function is\n\nactually unaware of the details of what happens on user login. Instead, the\n\nmain user-creation service simply has two lists:\n\nA list of required actions (e.g., sending a welcome mail)\n\nA list of optional actions (e.g., subscribing the user to a mailing list)\n\nEach of these actions is also implemented as a FaaS, and the list of actions\n\nis actually just a list of webhooks. Consequently, the main user creation\n\nfunction looks like this:\n\ndef create_user(context):\n\n# For required event handlers, call them univer\n\nfor key, value in required.items():\n\ncall_function(value.webhook, context.json)\n\n# For optional event handlers, check and call t # conditionally for key, value in optional.items(): if context.json.get(key, None) is not None: call_function(value.webhook, context.json)\n\nNow we can also use FaaS to implement each of these handlers:\n\ndef email_user(context):\n\n# Get the user name\n\nuser = context.json['username']\n\nmsg = 'Hello {} thanks for joining my awesome s\n\nsend_email(msg, contex.json['email])\n\ndef subscribe_user(context):\n\n# Get the user name\n\nemail = context.json['email']\n\nsubscribe_user(email)\n\nFactored in this way, each FaaS is simple, containing only a few lines of\n\ncode and focused on implementing one specific piece of functionality. This\n\nmicroservices-based approach is simple to write but might lead to\n\ncomplexity if we actually had to deploy and manage three different\n\nmicroservices. This is where FaaS can shine, since it makes it trivially easy\n\nto host these small code snippets. Additionally, by visualizing our user-\n\ncreation flow as an event-driven pipeline, it is also straightforward to have a\n\nhigh-level understanding of what exactly happens on user login, simply by\n\nfollowing the flow of the context through the various functions in the\n\npipeline.\n\nSummary\n\nFunctions as a service, or FaaS, is a powerful tool for easily hosting simple,\n\nscalable applications. As with any tool, the key to using it for maximal\n\nvalue lies in understanding what it is best at. The simplicity of functions as\n\na service comes with restrictions that may limit ways in which your\n\napplication can develop or libraries that your application can use. However,\n\nFaaS can also radically reduce your time to market. As you gain experience\n\nwith distributed systems, you will see that the ideal design is often a\n\ncombination of different pieces. Combining FaaS with more flexible\n\ninfrastructure or IaaS primitives, like the core of the Kubernetes API, often\n\nallows you to gain the best of both possible worlds and blend power and\n\nflexibility with simplicity and automation for the optimal system design.\n\nOceanofPDF.com\n\nChapter 10. Ownership Election\n\nThe previous patterns that we have seen have been about distributing\n\nrequests in order to scale requests per second, the state being served, or the\n\ntime to process a request. This final chapter on multinode serving patterns is\n\nabout how you scale assignment. In many different systems, there is a\n\nnotion of ownership where a specific process owns a specific task. We have\n\npreviously seen this in the context of sharded and hot-sharded systems\n\nwhere specific instances owned specific sections of the sharded key space.\n\nIn the context of a single process on a single server, you have likely already\n\nlearned how to manage concurrency using primitives like locks or mutexes\n\nthat are present in modern programming languages. A lock establishes\n\nownership within the context of a single application running on a single\n\nmachine because it uses storage in the memory of that machine and\n\nprimitives in the processor and operating system to establish exclusive\n\naccess. Even in the context of a single machine, concurrency can be\n\nchallenging to implement, and mistakes in concurrency are at the root of\n\nsome of the most challenging bugs you may encounter.\n\nUnfortunately, restricting ownership to a single application limits\n\nscalability, since the task can’t be replicated, and reliability, since if the task\n\nfails, it is unavailable for a period of time. If you want to build a large-scale\n\nreliable system that requires ownership, you need to build distributed locks.",
      "page_number": 177
    },
    {
      "number": 10,
      "title": "Ownership Election",
      "start_page": 198,
      "end_page": 223,
      "detection_method": "regex_chapter_title",
      "content": "Distributed locks introduce additional complexity since you cannot rely on\n\nshared memory or processors between different containers in your\n\ndistributed system.\n\nA general diagram of distributed ownership is shown in Figure 10-1. In the\n\ndiagram, there are three replicas that could be the owner or leader. Initially,\n\nthe first replica is the leader. Then that replica fails, and replica number\n\nthree then becomes the leader. Finally, replica number one recovers and\n\nreturns to the group, but replica three remains as the leader/owner.\n\nFigure 10-1. A leader election protocol in operation: initially the first leader is selected, but when it fails, the third leader takes over\n\nOften, establishing distributed ownership is both the most complicated and\n\nmost important part of designing a reliable distributed system.\n\nDetermining If You Even Need Leader\n\nElection\n\nThe simplest form of ownership is to just have a single replica of the\n\nservice. Since there is only one instance running at a time, that instance\n\nimplicitly owns everything without any need for election. This has\n\nadvantages of simplifying your application and deployment, but it has\n\ndisadvantages in terms of downtime and reliability. However, for many\n\napplications, the simplicity of this singleton pattern may be worth the\n\nreliability trade-off. Let’s look at this further.\n\nAssuming that you run your singleton in a container orchestration system\n\nlike Kubernetes, you have the following guarantees:\n\nIf the container crashes, it will automatically be restarted.\n\nIf the container hangs, and you implement a health check, it will\n\nautomatically be restarted.\n\nIf the machine fails, the container will be moved to a different machine.\n\nBecause of these guarantees, a singleton of a service running in a container\n\norchestrator has pretty good uptime. To take the definition of “pretty good”\n\na little further, let’s examine what happens in each of these failure modes. If\n\nthe container process fails or the container hangs, your application will be\n\nrestarted in a few seconds. Assuming your container crashes once a day, this\n\nis roughly three to four nines of uptime (2 seconds of downtime / day ~=\n\n99.99% uptime). If your container crashes less often, it’s even better than\n\nthat. If your machine fails, it takes a while for Kubernetes to decide that the\n\nmachine has failed and move your workload over to a different machine;\n\nlet’s assume that takes around 5 minutes. Given that, if every machine in\n\nyour cluster fails every day, then your service will have two nines of\n\nuptime. And honestly, if every machine in your cluster fails every day, then\n\nyou have way worse problems than the uptime of your leader-elected\n\nservice.\n\nIt’s worth considering, of course, that there are more reasons for downtime\n\nthan just failures. When you are rolling out new software, it takes time to\n\ndownload and start the new version. With a singleton, you cannot have both\n\nold and new versions running at the same time, so you will need to take\n\ndown the old version for the duration of the upgrade, which may be several\n\nminutes if your image is large. Consequently, if you deploy daily and it\n\ntakes two minutes to upgrade your software, you will only be able to run a\n\ntwo nines service, and if you deploy hourly, it won’t even be a single nine\n\nservice. Of course, there are ways that you can speed up your deployment\n\nby pre-pulling the new image onto the machine before you run the update.\n\nThis can reduce the time it takes to deploy a new version to a few seconds,\n\nbut the trade-off is added complexity, which was what we were trying to\n\navoid in the first place.\n\nRegardless, there are many applications (e.g., background asynchronous\n\nprocessing) where such an SLA is an acceptable trade-off for application\n\nsimplicity. One of the key components of designing a distributed system is\n\ndeciding when the “distributed” part is actually unnecessarily complex. But\n\nthere are certainly situations where high availability (four+ nines) is a\n\ncritical component of the application, and in such systems, you need to run\n\nmultiple replicas of the service, where only one replica is the designated\n\nowner. The design of these types of systems is described in the sections that\n\nfollow.\n\nThe significant downside of just using a singleton is that for the period that\n\nthe leader is down, your entire application will be unable to process\n\nrequests. While you may be able to achieve a reasonable uptime measured\n\nacross a day, for the seconds or minutes that you might be down, your\n\nservice is completely down. For a system which is interactive, like a game,\n\nretail, or banking system, being completely down even for a short period of\n\ntime will likely lead to significant customer dissatisfaction.\n\nThe Basics of Leader Election\n\nImagine that there is a service Foo with three replicas: Foo-1, Foo-2, and\n\nFoo-3. There is also some object Bar that must only be “owned” by one of\n\nthe replicas (e.g., Foo-1) at a time. Often this replica is called the leader,\n\nhence the term leader election used to describe the process of how this\n\nleader is selected as well as how a new leader is selected if that leader fails.\n\nThere are two ways to implement this leader election. This first is to\n\nimplement a distributed consensus algorithm like Paxos or RAFT, but the\n\ncomplexity of these algorithms make them beyond the scope of this book\n\nand not worthwhile to implement. Implementing one of these algorithms is\n\nakin to implementing locks on top of assembly code compare-and-swap\n\ninstructions. It’s an interesting exercise for an undergraduate computer\n\nscience course, but it is not something that is generally worth doing in\n\npractice.\n\nFortunately, there are a large number of distributed key-value stores that\n\nhave implemented such consensus algorithms for you. At a general level,\n\nthese systems provide a replicated, reliable data store and the primitives\n\nnecessary to build more complicated locking and election abstractions on\n\ntop. Examples of these distributed stores include etcd, ZooKeeper, and\n\nConsul. The basic primitives that these systems provide is the ability to\n\nperform a compare-and-swap operation for a particular key. Compare-and-\n\nswap atomically writes a new value if the existing value matches the\n\nexpected value. If the value doesn’t match, it returns false . If the value\n\ndoesn’t exist and currentValue is not null, it returns an error. If you\n\nhaven’t seen compare-and-swap before, it is basically an atomic operation\n\nthat looks like this:\n\nlock := sync.Mutex{}\n\nstore := map[string]string{}\n\nfunc compareAndSwap(key, nextValue, currentValue\n\nlock.Lock()\n\ndefer lock.Unlock()\n\n_, containsKey := store[key]\n\nif !containsKey {\n\nif len(currentValue) == 0 {\n\nstore[key] = nextValue\n\nreturn true, nil\n\n}\n\nreturn false, fmt.Errorf(\n\n\"Expected value %s for key %s, but found em }\n\nif store[key] == currentValue {\n\nstore[key] = nextValue return true, nil\n\n}\n\nreturn false, nil\n\n}\n\nIn addition to compare-and-swap, the key-value stores allow you to set a\n\ntime-to-live (TTL) for a key. Once the TTL expires, the key is set back to\n\nempty.\n\nPut together, these functions are sufficient to implement a variety of\n\ndistributed synchronization primitives.\n\nHands On: Deploying etcd\n\netcd is a distributed lock server originally developed by CoreOS and\n\nnow donated to the Cloud Native Computing Foundation (CNCF) and\n\nmaintained by the community. It is robust and proven in production at high\n\nscale, and is used by a variety of projects, including Kubernetes.\n\nThe easiest way to run etcd is to run it locally. The project makes\n\navailable binaries for Linux, MacOS X, and Windows. You can download\n\nthose binaries from GitHub; version 3.5.16 was the latest at the time of\n\nwriting, but you may want to check for later versions.\n\nOnce you have downloaded the archive for the release, you can unpack it,\n\nand you will find three executable binaries, etcd, etcdctl, and etcdutl, as\n\nwell as some documentation. For the purposes of this exercise, you can just\n\nrun etcd locally by running the command etcd ; this will start etcd\n\nrunning on localhost (127.0.0.1).\n\nObviously, running etcd locally is not a very robust or permanent\n\nsolution. If you would rather, you can run etcd inside a Kubernetes\n\ncluster using the open source pacakge manager Helm. If you don’t already\n\nhave Helm installed, you can do so from the Helm website.\n\nOnce you have the helm tool installed in your environment, you can\n\ninstall the etcd operator using helm , as follows:\n\n# Initialize helm\n\nhelm init\n\n# Install etcd using helm\n\nhelm install my-release oci://registry-1.docker.i\n\nOnce the etcd chart is installed, you can use kubectl get pods to\n\nsee the replicas in your etcd cluster.\n\nImplementing Locks\n\nThe simplest form of synchronization is the mutual exclusion lock (a.k.a.\n\nmutex). Anyone who has done concurrent programming on a single\n\nmachine is familiar with locks, and the same concept can be applied to\n\ndistributed replicas. Instead of local memory and assembly instructions,\n\nthese distributed locks can be implemented in terms of the distributed key-\n\nvalue stores described previously.\n\nAs with locks in memory, the first step is to acquire the lock:\n\nfunc (Lock l) simpleLock() boolean {\n\n// compare and swap \"1\" for \"0\"\n\nlocked, _ = compareAndSwap(l.lockName, \"1\", \"0\"\n\nreturn locked\n\n}\n\nBut of course, it’s possible that the lock doesn’t already exist, because we\n\nare the first to claim it, so we need to handle that case, too:\n\nfunc (Lock l) simpleLock() boolean {\n\n// compare and swap \"1\" for \"0\"\n\nlocked, error = compareAndSwap(l.lockName, \"1\",\n\n// lock doesn't exist, try to write \"1\" with a\n\n// non-existent if error != nil { locked, _ = compareAndSwap(l.lockName, \"1\", n } return locked }\n\nTraditional lock implementations block until the lock is acquired, so we\n\nactually want something like this:\n\nfunc (Lock l) lock() {\n\nwhile (!l.simpleLock()) {\n\nsleep(2)\n\n} }\n\nThis implementation, though simple, has the problem that you will always\n\nwait at least a second after the lock is released before you acquire the lock.\n\nFortunately, many key-value stores let you watch for changes instead of\n\npolling, so you can implement:\n\nfunc (Lock l) lock() {\n\nwhile (!l.simpleLock()) {\n\nwaitForChanges(l.lockName)\n\n}\n\n}\n\nGiven this locking function, we can also implement unlock:\n\nfunc (Lock l) unlock() {\n\ncompareAndSwap(l.lockName, \"0\", \"1\")\n\n}\n\nYou might now think that we are done, but remember that we are building\n\nthis for a distributed system. A process could fail in the middle of holding\n\nthe lock, and at that point there is no one left to release it. In such a\n\nsituation, our system will become stuck. To resolve this, we take advantage\n\nof the TTL functionality of the key-value store. We change our\n\nsimpleLock function so that it always writes with a TTL, so if we don’t\n\nunlock within a given time, the lock will automatically unlock:\n\nfunc (Lock l) simpleLock() boolean {\n\n// compare and swap \"1\" for \"0\"\n\nlocked, error = compareAndSwap(l.lockName, \"1\",\n\n// lock doesn't exist, try to write \"1\" with a\n\n// non-existent\n\nif error != nil {\n\nlocked, _ = compareAndSwap(l.lockName, \"1\", n\n\n}\n\nreturn locked\n\n}\n\nNOTE\n\nWhen using distributed locks, it is critical to ensure that any processing you do doesn’t last longer\n\nthan the TTL of the lock. One good practice is to set a watchdog timer when you acquire the lock.\n\nThe watchdog contains an assertion that will crash your program if the TTL of the lock expires\n\nbefore you have called unlock . If you don’t do this, you may perform computation when you have\n\nlost the lock.\n\nBy adding TTL to our locks, we have actually introduced a bug into our\n\nunlock function. Consider the following scenario:\n\n1. Process-1 obtains the lock with TTL t.\n\n2. Process-1 runs really slowly for some reason, for longer than t.\n\n3. The lock expires.\n\n4. Process-2 acquires the lock, since Process-1 has lost it due to TTL.\n\n5. Process-1 finishes and calls unlock.\n\n6. Process-3 acquires the lock.\n\nAt this point, Process-1 believes that it has unlocked the lock that it held at\n\nthe beginning; it doesn’t understand that it has actually lost the lock due to\n\nTTL, and in fact unlocked the lock held by Process-2. Then Process-3\n\ncomes along and also grabs the lock. Now both Process-2 and Process-3\n\nbelieve they own the lock, which means that your system likely has\n\nunexpected failures or data corruption.\n\nFortunately, the key-value store provides a resource version for every write\n\nthat is performed. Whenever a new write is performed, the resource version\n\nchanges. We can use this resource version as a precondition for any\n\nsubsequent write to ensure ordering. Our lock function stores the resource\n\nversion and uses it in compareAndSwap to ensure that not only is the\n\nvalue as expected, but the resource version is the same as when the lock\n\noperation occurred. This changes our simple Lock function to look like\n\nthis:\n\nfunc (Lock l) simpleLock() boolean {\n\n// compare and swap \"1\" for \"0\"\n\nlocked, l.version, error = compareAndSwap(l.loc\n\n// lock doesn't exist, try to write \"1\" with a // non-existent\n\nif error != null {\n\nlocked, l.version, _ = compareAndSwap(l.lockN\n\n} return locked\n\n}\n\nAnd the unlock function then looks like this:\n\nfunc (Lock l) unlock() {\n\ncompareAndSwap(l.lockName, \"0\", \"1\", l.version)\n\n}\n\nThis ensures that the lock is only unlocked if the TTL has not expired. The\n\nlock cannot be unlocked accidentally if someone else has acquired it after\n\nthe TTL expired. In such a case, the new owner of the lock has to have\n\nwritten a value into the storage system which results in a new resource\n\nversion, and our attempt to unlock using the old resource version will fail.\n\nHands On: Implementing Locks in etcd\n\nTo implement locks in etcd , you can use a key as the name of the lock\n\nand pre-condition writes to ensure that only one lock holder is allowed at a\n\ntime. For simplicity, we’ll use the etcdctl command line to lock and\n\nunlock the lock. In reality, of course, you would want to use a programming\n\nlanguage; there are etcd clients for most popular programming\n\nlanguages.\n\nLet’s start by creating a lock named my-lock :\n\nkubectl exec my-etcd-cluster-0000 -- sh -c \\\n\n\"ETCD_API=3 etcdctl --endpoints=${ETCD_ENDPOINT\n\nThis creates a key in etcd named my-lock and sets the initial value to\n\nunlocked .\n\nNow let’s suppose that Alice and Bob both want to take ownership of my-\n\nlock . Alice and Bob both try to write their name to the lock, using a\n\nprecondition that the value of the lock is unlocked .\n\nAlice first runs:\n\nkubectl exec my-etcd-cluster-0000 -- sh -c \\ \"ETCD_API=3 etcdctl --endpoints=${ETCD_ENDPOINT\n\nset --swap-with-value unlocked my-lock alic\n\nAnd obtains the lock. Now Bob attempts to obtain the lock:\n\nkubectl exec my-etcd-cluster-0000 -- sh -c \\\n\n\"ETCD_API=3 etcdctl --endpoints=${ETCD_ENDPOINT\n\nset --swap-with-value unlocked my-lock bob\"\n\nError: 101: Compare failed ([unlocked != alice])\n\nYou can see that Bob’s attempt to claim the lock has failed, since Alice\n\ncurrently owns the lock.\n\nTo unlock the lock, Alice writes unlocked with a precondition value of\n\nalice :\n\nkubectl exec my-etcd-cluster-0000 -- sh -c \\\n\n\"ETCD_API=3 etcdctl --endpoints=${ETCD_ENDPOINT\n\nset --swap-with-value alice my-lock unlocke\n\nImplementing Ownership\n\nWhile locks are great for establishing temporary ownership of some critical\n\ncomponent, sometimes you want to take ownership for the duration of the\n\ntime that the component is running. For example, in a highly available\n\ndeployment of Kubernetes, there are multiple replicas of the scheduler but\n\nonly one replica is actively making scheduling decisions. Further, once it\n\nbecomes the active scheduler, it remains the active scheduler until that\n\nprocess fails for some reason.\n\nObviously, one way to do this would be to extend the TTL for the lock to a\n\nvery long period (say a week or longer), but this has the significant\n\ndownside that if the current lock owner fails, a new lock owner wouldn’t be\n\nchosen until the TTL expired a week later.\n\nInstead, we need to create a renewable lock, which can be periodically\n\nrenewed by the owner so that the lock can be retained for an arbitrary\n\nperiod of time.\n\nWe can extend the existing Lock that we defined previously to create a\n\nrenewable lock, which enables the lock holder to renew the lock:\n\nfunc (Lock l) renew() boolean {\n\nlocked, _ = compareAndSwap(l.lockName, \"1\", \"1\"\n\nreturn locked }\n\nOf course, you probably want to do this repeatedly in a separate thread so\n\nthat you hold onto the lock indefinitely. Notice that the lock is renewed\n\nevery ttl/2 seconds; that way there is significantly less risk that the lock\n\nwill accidentally expire due to timing subtleties:\n\nfor {\n\nif !l.renew() {\n\nhandleLockLost()\n\n}\n\nsleep(ttl/2)\n\n}\n\nOf course, you need to implement the handleLockLost() function so\n\nthat it terminates all activity that required the lock in the first place. In a\n\ncontainer orchestration system, the easiest way to do this may simply be to\n\nterminate the application and let the orchestrator restart it. This is safe,\n\nbecause some other replica has grabbed the lock in the interim, and when\n\nthe restarted application comes back online it will become a secondary\n\nlistener waiting for the lock to become free.\n\nHands On: Implementing Leases in etcd\n\nTo see how we implement leases using etcd , we will return to our earlier\n\nlocking example and add the --ttl=<seconds> flag to our lock create\n\nand update calls. The ttl flag defines a time after which the lock that we\n\ncreate is deleted. Because the lock disappears after the ttl expires,\n\ninstead of creating with the value of unlocked, we will assume that the\n\nabsence of the lock means that it is unlocked. To do this, we use the mk\n\ncommand instead of the set command. etcdctl mk only succeeds if\n\nthe key does not currently exist.\n\nThus, to lock a leased lock, Alice executes:\n\nkubectl exec my-etcd-cluster-0000 -- \\\n\nsh -c \"ETCD_API=3 etcdctl --endpoints=${ETCD_\n\n--ttl=10 mk my-lock alice\"\n\nThis creates a leased lock with a duration of 10 seconds.\n\nFor Alice to continue to hold the lock, she needs to execute:\n\nkubectl exec my-etcd-cluster-0000 -- \\\n\nsh -c \"ETCD_API=3 etcdctl --endpoints=${ETCD_\n\nset --ttl=10 --swap-with-value alice my-l\n\nIt may seem odd that Alice is continually rewriting her own name into the\n\nlock, but this is the way the lock lease is extended beyond the 10-second\n\nTTL.\n\nIf, for some reason, the TTL expires, then the lock update will fail, and\n\nAlice will go back to creating the lock using the etcd mk command, or\n\nBob may also use the mk command to obtain the lock for himself. Bob\n\nwill likewise need to set and update the lock every 10 seconds to maintain\n\nownership.\n\nHandling Concurrent Data Manipulation\n\nEven with all of the locking mechanisms we have described, it is still\n\npossible for two replicas to simultaneously believe they hold the lock for a\n\nvery brief period of time. To understand how this can happen, imagine that\n\nthe original lock holder becomes so overwhelmed that its processor stops\n\nrunning for minutes at a time. This can happen on extremely overscheduled\n\nmachines. In such a case, the lock will time out and some other replica will\n\nown the lock. Now the processor frees up the replica that was the original\n\nlock holder. Obviously, the handleLockLost() function will quickly\n\nbe called, but there will be a brief period where the replica still believes it\n\nholds the lock. Although such an event is fairly unlikely, systems need to be\n\nbuilt to be robust to such occurrences. The first step to take is to double-\n\ncheck that the lock is still held, using a function like this:\n\nfunc (Lock l) isLocked() boolean { return l.locked && l.lockTime + 0.75 * l.ttl >\n\n}\n\nIf this function executes prior to any code that needs to be protected by a\n\nlock, then the probability of two leaders being active is significantly\n\nreduced, but—it is important to note—it is not completely eliminated. The\n\nlock timeout could always occur between the time that the lock was\n\nchecked and the guarded code was executed. To protect against these\n\nscenarios, the system that is being called from the replica needs to validate\n\nthat the replica sending a request is actually still the leader. To do this, the\n\nhostname of the replica holding the lock is stored in the key-value store in\n\naddition to the state of the lock. That way, others can double-check that a\n\nreplica asserting that it is the leader is in fact the leader.\n\nThis system diagram is shown in Figure 10-2. In the image, shard2 is\n\nthe owner of the lock, and when a request is sent to the worker, the worker\n\ndouble-checks with the lock server and validates that shard2 is actually\n\nthe current owner.\n\nFigure 10-2. A worker double-checking to validate that the requester who sent a message is actually the current owner of the shard\n\nIn the second case, shard2 has lost ownership of the lock, but it has not\n\nyet realized this, so it continues to send requests to the worker node. This\n\ntime, when the worker node receives a request from shard2 , it double-\n\nchecks with the lock service and realizes that shard2 is no longer the\n\nlock owner, and thus the requests are rejected.\n\nTo add one final further complicating wrinkle, it’s always possible that\n\nownership could be obtained, lost, and then re-obtained by the system,\n\nwhich could actually cause a request to succeed when it should actually be\n\nrejected. To understand how this is possible, consider the following\n\nsequence of events:\n\n1. shard1 obtains ownership to become leader.\n\n2. shard1 sends a request R1 as leader at time T1 .\n\n3. The network hiccups and delivery of R1 is delayed.\n\n4. shard1 fails TTL because of the network and loses lock to shard2 .\n\n5. shard2 becomes leader and sends a request R2 at time T2 .\n\n6. Request R2 is received and processed.\n\n7. shard2 crashes and loses ownership back to shard1 .\n\n8. Request R1 finally arrives, and shard1 is the current leader, so it is\n\naccepted, but this is bad because R2 has already been processed.\n\nSuch sequences of events seem byzantine, but in reality, in any large system\n\nthey occur with disturbing frequency. Fortunately, this is similar to the case\n\ndescribed previously, which we resolved with the resource version in\n\netcd . We can do the same thing here. In addition to storing the name of\n\nthe current owner in etcd , we also send the resource version along with\n\neach request. So in the previous example, R1 becomes (R1,\n\nVersion1) . Now when the request is received, the double-check\n\nvalidates both the current owner and the resource version of the request. If\n\neither match fails, the request is rejected. This patches up this example.\n\nUltimately, in designing a distributed ownership system, you have to choose\n\nbetween at-most once processing, which is possible, but requires you to\n\npossibly lose availability for a period of time. Or you can have at least once\n\nprocessing, which can provide higher availability, but will occasionally\n\nresult in double processing of some request. Depending on the type of\n\nsystem you are building, duplicate processing may simply waste some\n\nresources (e.g., compute time on a GPU); in which case, if it is infrequent, it\n\nis likely acceptable. However, some systems may involve actual\n\ntransactions (e.g., transferring money between accounts) where you would\n\nrather lose availability but preserve consistency. Understanding these trade-\n\noffs is a critical part of distributed system design.\n\nSummary\n\nIn this chapter we have explored one of the most complicated parts of\n\ndistributed system design, distributed ownership and locks. While parallel\n\nconcurrent processing is a key part of building reliable, scalable systems,\n\nthe concurrency also introduces challenges in systems where ownership of\n\nspecific work is important. Distributed ownership or locks enable pieces of\n\nyour distributed system to maintain exclusive access to a particular part of\n\nthe data store or compute operations. Implementing locks involves\n\ninteracting with consensus-based distributed storage, like etcd or\n\nZooKeeper, with very specific coding patterns. In many languages, code for\n\ndistributed locks have already been implemented. Since writing this code\n\nyourself is a challenge, if there’s already a library for your programming\n\nlanguage, it’s a best practice to rely on pre-existing production-grade code.\n\nFinally, some simple systems don’t need locks and can get away with using\n\nSingletons and in-process concurrency. Such systems trade availability for\n\nsimplicity.\n\nOceanofPDF.com\n\nPart IV. Batch Computational Patterns\n\nThe preceding chapters described patterns for reliable, long-running server\n\napplications. This section describes patterns for batch processing. In\n\ncontrast to long-running applications, batch processes are expected to only\n\nrun for a short period of time. Examples of a batch process include\n\ngenerating aggregation of user telemetry data, analyzing sales data for daily\n\nor weekly reporting, or transcoding video files. Batch processes are\n\ngenerally characterized by the need to process large amounts of data\n\nquickly using parallelism to speed up the processing. The most famous\n\npattern for distributed batch processing is the MapReduce pattern, which\n\nhas become an entire industry in itself. However, there are several other\n\npatterns that are useful for batch processing, which are described in the\n\nfollowing chapters.\n\nOceanofPDF.com\n\nChapter 11. Work Queue Systems\n\nThe simplest form of batch processing is a work queue. In a work queue\n\nsystem, there is a batch of work to be performed. Each piece of work is\n\nwholly independent of the other and can be processed without any\n\ninteractions. Generally, the goals of the work queue system are to ensure\n\nthat each piece of work is processed within a certain amount of time.\n\nWorkers are scaled up or scaled down to ensure that the work can be\n\nhandled. An illustration of a generic work queue is shown in Figure 11-1.\n\nFigure 11-1. A generic work queue\n\nA Generic Work Queue System\n\nThe work queue is an ideal way to demonstrate the power of distributed\n\nsystem patterns. Most of the logic in the work queue is wholly independent\n\nof the actual work being done, and in many cases the delivery of the work",
      "page_number": 198
    },
    {
      "number": 11,
      "title": "Work Queue Systems",
      "start_page": 224,
      "end_page": 244,
      "detection_method": "regex_chapter_title",
      "content": "can be performed in an independent manner as well. To illustrate this point,\n\nconsider the work queue illustrated in Figure 11-1. If we take a look at it\n\nagain and identify functionality that can be provided by a shared set of\n\nlibrary containers, it becomes apparent that most of the implementation of\n\na containerized work queue can be shared across a wide variety of users, as\n\nshown in Figure 11-2.\n\nFigure 11-2. The same work queue as shown in Figure 11-1, but this time using reusable containers; the reusable system containers are shown in white, while the user-supplied container is shown in the darker shaded boxes\n\nBuilding a reusable container-based work queue requires the definition of\n\ninterfaces between the generic library containers and the user-defined\n\napplication logic. In the containerized work queue, there are two interfaces:\n\nthe source container interface, which provides a stream of work items that\n\nneed processing, and the worker container interface, which knows how to\n\nactually process a work item.\n\nThe Source Container Interface\n\nTo operate, every work queue needs a collection of work items that need\n\nprocessing. There are many different sources of work items for the work\n\nqueue, depending on the specific application of the work queue. However,\n\nonce the set of items has been obtained, the actual operation of the work\n\nqueue is quite generic. Consequently, we can separate the application-\n\nspecific queue source logic from the generic queue processing logic. Given\n\nthe previously defined patterns of container groups, this can be seen as an\n\nexample of the ambassador pattern defined previously. The generic work\n\nqueue container is the primary application container, and the application-\n\nspecific source container is the ambassador that proxies the generic work\n\nqueue’s requests out to the concrete definition of the work queue out in the\n\nreal world. This container group is illustrated in Figure 11-3.\n\nFigure 11-3. The work queue container group\n\nInterestingly, while the ambassador container is clearly application-specific,\n\nthere is also a variety of generic implementations of the work queue source\n\nAPI. For example, a source might be a list of pictures stored in a cloud\n\nstorage API, a collection of files stored on network storage, or a queue in a\n\npub/sub system like Kafka or Redis. In these cases, though the user chooses\n\nthe particular work queue ambassador that fits their scenario, they should be\n\nreusing a single generic “library” implementation of the container itself.\n\nThis minimizes work and maximizes code reuse.\n\nWork Queue API\n\nGiven this coordination between the generic work-queue manager and the\n\napplication-specific ambassador, we need a formal definition of the\n\ninterface between the two containers. Though there are a variety of different\n\nprotocols, an HTTP RESTful API is both the easiest to implement as well as\n\nthe de facto standard for such an interface. The coordinating work queue\n\nexpects the ambassador to implement the following URLs:\n\nGET http://localhost/api/v1/items\n\nGET http://localhost/api/v1/items/<item-name>\n\nNOTE\n\nYou might wonder why we include a v1 in the API definition. Will there ever be a v2 of this\n\ninterface? It may not seem logical, but it costs very little to version your API when you initially\n\ndefine it. Refactoring versioning onto an API without it, on the other hand, is very expensive.\n\nConsequently, it is a best practice to always add versions to your APIs even if you’re not sure they\n\nwill ever change. Better safe than sorry.\n\nThe /items/ URL returns a complete list of all items:\n\n{\n\nkind: ItemList,\n\napiVersion: v1,\n\nitems: [ \"item-1\",\n\n\"item-2\",\n\n…. ]\n\n}\n\nThe /items/<item-name> URL provides the details for a specific item:\n\n{ kind: Item,\n\napiVersion: v1,\n\ndata: { \"some\": \"json\",\n\n\"object\": \"here\",\n\n} }\n\nImportantly, you will notice that this API doesn’t have any affordances for\n\nrecording that a work item has been processed. We could have designed a\n\nmore complicated API and then pushed more implementation into the\n\nambassador container, but remember, the goal of this effort is to place as\n\nmuch of the generic implementation inside of the generic work queue\n\nmanager as possible. To that end, the work queue manager itself is\n\nresponsible for tracking which items have been processed and which items\n\nremain to be processed.\n\nThe item details are obtained from this API, and the item.data field is\n\npassed along to the worker interface for processing.\n\nThe Worker Container Interface\n\nOnce a particular work item has been obtained by the work queue manager,\n\nit needs to be processed by a worker. This is the second container interface\n\nin our generic work queue. This container and interface are slightly\n\ndifferent than the previous work queue source interface for a few reasons.\n\nThe first is that it is a one-off API: a single call is made to begin the work,\n\nand no other API calls are made throughout the life of the worker container.\n\nSecondly, the worker container is not inside a container group with the\n\nwork queue manager. Instead, it is launched via a container orchestration\n\nAPI and scheduled to its own container group. This means that the work\n\nqueue manager has to make a remote call to the worker container in order to\n\nstart work. It also means that we may need to be more careful about security\n\nto prevent a malicious user in our cluster from injecting extra work into the\n\nsystem.\n\nWith the work queue source API, we used a simple HTTP-based API for\n\nsending items back to the work queue manager. This was because we\n\nneeded to make repeated calls to the API, and security wasn’t a concern\n\nsince everything was running on localhost. With the worker container, we\n\nonly need to make a single call, and we want to ensure that other users in\n\nthe system can’t accidentally or maliciously add work to our workers.\n\nConsequently, for the worker container, we will use a file-based API.\n\nNamely, when the worker container is created, it will receive an\n\nenvironment variable named WORK_ITEM_FILE ; this will point to a file\n\nin the container’s local filesystem, where the data field from a work item\n\nhas been written to a file. Concretely, as you will see below, this API can be\n\nimplemented via a Kubernetes ConfigMap object that can be mounted\n\ninto a container group as a file, as illustrated in Figure 11-4.\n\nFigure 11-4. The work queue worker API\n\nThis file-based API pattern is also easier for the container to implement.\n\nOften a work queue worker is simply a shell script across a few command-\n\nline tools. In that context, spinning up a web server to manage the work to\n\nperform is an unnecessary complexity. As was true with the work queue\n\nsource implementation, most of the worker containers will be special-\n\npurpose container images built for specific work queue applications, but\n\nthere are also generic workers that can be applied to multiple different work\n\nqueue applications.\n\nConsider the example of a work queue worker that downloads a file from\n\ncloud storage and runs a shell script with that file as input, and then copies\n\nits output back up to cloud storage. Such a container can be mostly generic\n\nbut then have the specific script to execute supplied to it as a runtime\n\nparameter. In this way, most of the work of file handling can be shared by\n\nmultiple users/work queues and only the specifics of file processing need to\n\nbe supplied by the end user.\n\nThe Shared Work Queue Infrastructure\n\nGiven implementations of the two container interfaces described previously,\n\nwhat is left to implement our reusable work queue implementation? The\n\nbasic algorithm for the work queue is fairly straightforward:\n\n1. Load the available work by calling into source container interface.\n\n2. Consult with work queue state to determine which work items have been\n\nprocessed or are being processed currently.\n\n3. For these items, spawn jobs that use the worker container interface to\n\nprocess the work item.\n\n4. When one of these worker containers finishes successfully, record that\n\nthe work item has been completed.\n\nWhile this algorithm is simple to express in words, it is somewhat more\n\ncomplicated to implement in reality. Fortunately for us, the Kubernetes\n\ncontainer orchestrator contains a number of features that make it\n\nsignificantly easier to implement. Namely, Kubernetes contains a Job\n\nobject that allows for the reliable execution of the work queue. The Job\n\ncan be configured to either run the worker container once or to run it until it\n\ncompletes successfully. If the worker container is set to run to completion,\n\nthen even if a machine in the cluster fails, the job will eventually be run to\n\nsuccess. This dramatically simplifies the task of building a work queue\n\nbecause the orchestrator takes on responsibility for the reliable operation of\n\neach work item.\n\nAdditionally, Kubernetes has annotations for each Job object that enable\n\nus to mark each job with the work item it is processing. This enables us to\n\nunderstand which items are being processed as well as those that have\n\ncompleted in either failure or success.\n\nPut together, this means that we can implement a work queue on top of the\n\nKubernetes orchestration layer without using any storage of our own.\n\nThus, the expanded operation of our work queue container looks like this:\n\nRepeat forever\n\nGet the list of work items from the work sour\n\nGet the list of all jobs that have been creat\n\nqueue.\n\nDifference these lists to find the set of wor\n\nbeen processed.\n\nFor these unprocessed items, create new Job o\n\nappropriate worker container.\n\nHere is a simple Python script that implements this work queue:\n\nimport requests import json\n\nfrom kubernetes import client, config import time\n\nnamespace = \"default\"\n\ndef make_container(item, obj):\n\ncontainer = client.V1Container()\n\ncontainer.image = \"my/worker-image\"\n\ncontainer.name = \"worker\" return container\n\ndef make_job(item):\n\nresponse = requests.get(\"http://localhost:800\n\nobj = json.loads(response.text)\n\njob = client.V1Job()\n\njob.metadata = client.V1ObjectMeta()\n\njob.metadata.name = item\n\njob.spec = client.V1JobSpec()\n\njob.spec.template = client.V1PodTemplate()\n\njob.spec.template.spec = client.V1PodTemplate\n\njob.spec.template.spec.restart_policy = \"Neve\n\njob.spec.template.spec.containers = [\n\nmake_container(item, obj)\n\n]\n\nreturn job\n\ndef update_queue(batch):\n\nresponse = requests.get(\"http://localhost:800\n\nobj = json.loads(response.text)\n\nitems = obj['items']\n\nret = batch.list_namespaced_job(namespace, wa\n\nfor item in items:\n\nfound = False\n\nfor i in ret.items: if i.metadata.name == item:\n\nfound = True\n\nif not found:\n\n# This function creates the job objec\n\n# brevity\n\njob = make_job(item)\n\nbatch.create_namespaced_job(namespace\n\nconfig.load_kube_config()\n\nbatch = client.BatchV1Api()\n\nwhile True:\n\nupdate_queue(batch)\n\ntime.sleep(10)\n\nHands On: Implementing a Video\n\nThumbnailer\n\nTo provide a concrete example of how we might use a work queue, consider\n\nthe task of generating thumbnails for videos. These thumbnails help users\n\ndetermine which videos they want to watch.\n\nTo implement this video thumbnailer, we need two different user containers.\n\nThe first is the work item source container. The simplest way for this to\n\nwork is for the work items to appear on a shared disk, such as a Network\n\nFile System (NFS) share. The work item source simply lists the files in this\n\ndirectory and returns them to the caller. Here’s a simple node program\n\nthat does this:\n\nconst http = require('http');\n\nconst fs = require('fs');\n\nconst port = 8080;\n\nconst path = process.env.MEDIA_PATH;\n\nconst requestHandler = (request, response) => {\n\nconsole.log(request.url); fs.readdir(path + '/*.mp4', (err, items)\n\nvar msg = {\n\n'kind': 'ItemList',\n\n'apiVersion': 'v1',\n\n'items': []\n\n};\n\nif (!items) {\n\nreturn msg;\n\n}\n\nfor (var i = 0; i < items.length; msg.items.push(items[i]);\n\n}\n\nresponse.end(JSON.stringify(msg))\n\n});\n\n}\n\nconst server = http.createServer(requestHandler);\n\nserver.listen(port, (err) => {\n\nif (err) {\n\nreturn console.log('Error startin\n\n}\n\nconsole.log(`server is active on ${port}`\n\n});\n\nThis source of defines the queue of movies to thumbnail. We use the\n\nffmpeg utility to actually perform the thumbnailing work.\n\nYou can create a container that uses the following as its command line:\n\nffmpeg -i ${INPUT_FILE} -frames:v 100 thumb.png\n\nThis command will take one frame every 100 frames (that’s the -\n\nframes:v 100 flag) and output it as a PNG file (e.g., thumb1.png ,\n\nthumb2.png , etc.).\n\nYou can create the ffmpeg container using a base image of your choice\n\nand installing the ffmpeg executable in that container. There is also an\n\nopen source project with Dockerfiles for many popular operating systems\n\non GitHub. You can use those scripts as a starting point for your container\n\nimage.\n\nBy defining a simple source container as well as an even simpler worker\n\ncontainer, we can clearly see the power and utility of a generic container-\n\nbased queuing system. It dramatically reduces the time/distance between an\n\nidea for implementing a work queue and the corresponding concrete\n\nimplementation.\n\nDynamic Scaling of the Workers\n\nThe previously described work queue is great for processing work items as\n\nquickly as they arrive in the work queue, but this can lead to bursty resource\n\nloads being placed onto a container orchestrator cluster. This is good if you\n\nhave a lot of different workloads that will burst at different times and thus\n\nkeep your infrastructure evenly utilized. But if you don’t have a sufficient\n\nnumber of different workloads, this feast or famine approach to scaling your\n\nwork queue might require that you over-provision resources to support the\n\nbursts that will lie idle (and cost too much money) while you don’t have\n\nwork to perform.\n\nTo address this problem, you can limit the overall number of Job objects\n\nthat your work queue is willing to create. This will naturally serve to limit\n\nthe number of work items you process in parallel and consequentially limit\n\nthe maximum amount of resources that you use at a particular time.\n\nHowever, doing this will increase the time to completion (latency) for each\n\nwork item being completed when under heavy load. If the load is bursty,\n\nthen this is probably OK because you can use the slack times to catch up\n\nwith the backlog that developed during a burst of usage. However, if your\n\nsteady-state usage is too high, your work queue may never be able to catch\n\nup, and the time to completion will simply get longer and longer.\n\nWhen your work queue is faced with this situation, you need to have it\n\ndynamically adjust itself to increase the parallelism that it is willing to\n\ncreate (and correspondingly the resources it is willing to use) so that it can\n\nkeep up with the incoming work. Fortunately, there are mathematical\n\nformulas that we can use to determine when we need to dynamically scale\n\nup our work queue.\n\nConsider a work queue where a new work item arrives an average of once\n\nevery minute, and each work item takes an average of 30 seconds to\n\ncomplete. Such a system is capable of keeping up with all of the work it\n\nreceives. Even if a large batch of work arrives all at once and creates a\n\nbacklog, on average the work queue processes two work items for every\n\none work item that arrives, and thus it will be able to gradually work\n\nthrough its backlog.\n\nIf, instead, a new work item arrives on average once every minute and it\n\ntakes an average of one minute to process each work item, then the system\n\nis perfectly balanced, but it does not handle variance well. It can catch up\n\nwith bursts—but it will take a while, and it has no slack or capacity to\n\nabsorb a sustained increase in the rate at which new work items arrive. This\n\nis probably not an ideal way to run, as some safety margin for growth and\n\nother sustained increases in work (or unexpected slowdowns in processing)\n\nis needed to preserve a stable system.\n\nFinally, consider a system in which a work item arrives every minute and\n\neach item takes two minutes to process. In such a system, we are always\n\nlosing ground. The queue of work will grow without bound, and the latency\n\nof any one item in the queue will grow toward infinity (and our users will\n\nbecome very frustrated).\n\nThus, we can keep track of both of these metrics for our work queue, and\n\nthe average time between work items over an extended period of time (#\n\nwork items / 24 hours) will give us the interarrival time for new work. We\n\ncan also keep track of the average latency, the time to process any one item\n\nonce we start working on it, not counting any time spent waiting in the\n\nqueue. To have a stable work queue, we need to adjust the number of\n\nresources so that latency is less than the interarrival time of new items. If\n\nwe are processing work items in parallel, we also divide the processing time\n\nfor a work item by the parallelism. For example, if each item takes one\n\nminute to process but we process four items in parallel, the effective time to\n\nprocess one item is 15 seconds, and thus we can sustain an interarrival\n\nperiod of 16 or more seconds.\n\nThis approach makes it fairly straightforward to build an autoscaler to\n\ndynamically size up our work queue. Sizing down the work queue is\n\nsomewhat trickier, but you can use the same math as well as a heuristic for\n\nthe amount of spare capacity for the safety margin you want to maintain.\n\nFor example, you can reduce the parallelism until the processing time for an\n\nitem is 90% of the interarrival time for new items. While in some cases it is\n\nmore efficient to build your own scaler, many times it is better to use an\n\nopen source solution. That is where the Kubernetes Event-Driven\n\nAutoscaling (KEDA) project is useful. KEDA provides numerous different\n\nevent-based scalers, which can be used to scale your application. KEDA\n\nsupports event sources ranging from files on disk to events in pub/sub\n\nsystems and many more.\n\nThe Multiworker Pattern\n\nOne of the themes of this book has been the use of containers for\n\nencapsulation and reuse of code. The same holds true for the work queue\n\npatterns described in this chapter. In addition to the patterns for reusing\n\ncontainers for driving the work queue itself, you can also reuse multiple\n\ndifferent containers to compose a worker implementation. Suppose, for\n\nexample, that you have three different types of work that you want to\n\nperform on a particular work queue item. For example, you might want to\n\ndetect faces in an image, tag those faces with identities, and then blur the\n\nfaces in the image. You could write a single worker to perform this\n\ncomplete set of tasks, but this would be a bespoke solution that would not\n\nbe reusable the next time you want to identify something else, such as cars,\n\nyet still provide the same blurring.\n\nTo achieve this kind of code reuse, the multiworker pattern is something of\n\na specialization of the adapter pattern described in previous chapters. In this\n\ncase, the multiworker pattern transforms a collection of different worker\n\ncontainers into a single unified container that implements the worker\n\ninterface, yet delegates the actual work to a collection of different reusable\n\ncontainers. This process is illustrated in Figure 11-5.\n\nFigure 11-5. The multiworker aggregator pattern as a group of containers\n\nBecause of this code reuse, the composition of multiple different worker\n\ncontainers means an increase in the reuse of code and a reduction in effort\n\nfor people designing batch-oriented distributed systems.\n\nSummary\n\nThis chapter begins the discussion of batch systems by introducing\n\nworkflows. A workflow is a sequence of discrete actions which, when put\n\ntogether in a sequence, achieve some goal like the processing of an order or\n\nthe machine-learning analysis of a video. This chapter showed how\n\ncontainers can be used to build modular workflows with sidecar containers\n\nproviding workflow orchestration while application containers supply the\n\nspecific processing necessary for your workflow. Workflow is a key part of\n\nmost distributed batch systems. Its generic pattern is easy to adapt and\n\nextend to meet the specific needs of your workload.\n\nOceanofPDF.com\n\nChapter 12. Event-Driven Batch\n\nProcessing\n\nIn Chapter 11, we saw a generic framework for work queue processing, as\n\nwell as a number of example applications of simple work queue processing.\n\nWork queues are great for enabling individual transformations of one input\n\nto one output. However, there are a number of batch applications where you\n\nwant to perform more than a single action, or you may need to generate\n\nmultiple different outputs from a single data input. In these cases, you start\n\nto link work queues together so that the output of one work queue becomes\n\nthe input to one or more other work queues, and so on. This forms a series\n\nof processing steps that respond to events, with the events being the\n\ncompletion of the preceding step in the work queue that came before it.\n\nThese sort of event-driven processing systems are often called workflow\n\nsystems, since there is a flow of work through a directed, acyclic graph that\n\ndescribes the various stages and their coordination. A basic illustration of\n\nsuch a system is shown in Figure 12-1.\n\nThe most straightforward application of this type of system simply chains\n\nthe output of one queue to the input of the next queue. But as systems\n\nbecome more complicated, there are a series of different patterns that\n\nemerge for linking a series of work queues together. Understanding and\n\ndesigning in terms of these patterns is important for comprehending how",
      "page_number": 224
    },
    {
      "number": 12,
      "title": "Event-Driven Batch",
      "start_page": 245,
      "end_page": 272,
      "detection_method": "regex_chapter_title",
      "content": "the system is working. The operation of an event-driven batch processor is\n\nsimilar to event-driven FaaS. Consequently, without an overall blueprint for\n\nhow the different event queues relate to each other, it can be hard to fully\n\nunderstand how the system is operating. Furthermore, though the initial\n\ndesign of a work queue can be straightforward, correctly handling errors\n\nand other unexpected conditions rapidly makes a simple work queue much\n\nmore complicated.\n\nFigure 12-1. This workflow combines copying work into multiple queues (Stage 2a, 2b), parallel processing of those queues, and combining the result back into a single queue (Stage 3)\n\nPatterns of Event-Driven Processing\n\nBeyond the simple work queue described in Chapter 11, there are a number\n\nof patterns for linking work queues together. The simplest pattern—one\n\nwhere the output of a single queue becomes the input to a second queue—is\n\nstraightforward enough that we won’t cover it here. We will describe\n\npatterns that involve the coordination of multiple different queues or the\n\nmodification of the output of one or more work queues.\n\nCopier\n\nThe first pattern for coordinating work queues is a copier. The job of a\n\ncopier is to take a single stream of work items and duplicate it out into two\n\nor more identical streams. This pattern is useful when there are multiple\n\ndifferent pieces of work to be done on the same work item. An example of\n\nthis might be rendering a video. When rendering a video, there are a variety\n\nof different formats that are useful, depending on where the video is\n\nintended to be shown. There might be a 4K high-resolution format for\n\nplaying off of a hard drive, a 1080-pixel rendering for digital streaming, a\n\nlow-resolution format for streaming to mobile users on slow networks, and\n\nan animated GIF thumbnail for displaying in a movie-picking user\n\ninterface. All of these work items can be modeled as separate work queues\n\nfor each render, but the input to each work item is identical. An illustration\n\nof the copier pattern applied to transcoding is shown in Figure 12-2.\n\nFigure 12-2. The copier batch pattern for transcoding\n\nThe copy pattern can also be used to trade additional computational power\n\nfor increased reliability or performance. Imagine that you have a worker\n\nprocessing work that usually takes one second to run, but occasionally takes\n\none minute (60x slowdown). If you process every work item twice and take\n\nwhichever result completes first, you are much less likely to be impacted by\n\nthis slowdown. Similarly, if your worker crashes 1% of the time, you can go\n\nfrom a two-nines (99% reliable) system to a four-nines (99.99% reliable)\n\nsimply by processing every item twice. Obviously, in both cases, doing the\n\nengineering work to improve the system is a better long-term solution. But\n\nsometimes as a temporary workaround, or in situations where the\n\nengineering work isn’t possible, it’s useful to be able to just throw resources\n\nat the problem.\n\nFilter\n\nThe second pattern for event-driven batch processing is a filter. The role of\n\na filter is to reduce a stream of work items to a smaller stream of work\n\nitems by filtering out work items that don’t meet particular criteria. As an\n\nexample of this, consider setting up a batch workflow that handles new\n\nusers signing up for a service. Some set of those users will have ticked the\n\ncheckbox that indicates that they wish to be contacted via email for\n\npromotions and other information. In such a workflow, you can filter the set\n\nof newly signed-up users to only be those who have explicitly opted into\n\nbeing contacted.\n\nIdeally you would compose a filter work queue source as an ambassador\n\nthat wraps up an existing work queue source. The original source container\n\nprovides the complete list of items to be worked on, and the filter container\n\nthen adjusts that list based on the filter criteria and only returns those\n\nfiltered results to the work queue infrastructure. An illustration of this use of\n\nthe adapter pattern is shown in Figure 12-3.\n\nFigure 12-3. An example of a filter pattern that removes all odd-numbered work items\n\nSplitter\n\nSometimes you don’t want to just filter things out by dropping them on the\n\nfloor, but rather you have two different kinds of input present in your set of\n\nwork items, and you want to divide them into two separate work queues\n\nwithout dropping any of them. For this task, you want to use a splitter. The\n\nrole of a splitter is to evaluate some criteria—just like a filter—but instead\n\nof eliminating input, the splitter sends different inputs to different queues\n\nbased on that criteria.\n\nAn example of an application of the splitter pattern is processing online\n\norders where people can receive shipping notifications either by email or\n\ntext message. Given a work queue of items that have been shipped, the\n\nsplitter divides it into two different queues: one that is responsible for\n\nsending emails and another devoted to sending text messages. A splitter can\n\nalso be a copier if it sends the same output to multiple queues, such as when\n\na user selects both text messages and email notifications in the previous\n\nexample. It is interesting to note that a splitter can actually also be\n\nimplemented by a copier and two different filters. But the splitter pattern is\n\na more compact representation that captures the job of the splitter more\n\nsuccinctly. This is a little like the fact that an XOR (exclusive or) logic gate\n\ncan actually be implemented with OR/NOT/AND gates, but it is convenient\n\nto think of it as an XOR. An example of using the splitter pattern to send\n\nshipping notifications to users is shown in Figure 12-4.\n\nFigure 12-4. An example of the batch splitter pattern splitting shipping notifications into two different queues\n\nSharder\n\nA slightly more generic form of splitter is a sharder. Much like the sharded\n\nserver that we saw in earlier chapters, the role of a sharder in a workflow is\n\nto divide up a single queue into an evenly divided collection of work items\n\nbased upon some sort of sharding function. There are several different\n\nreasons why you might consider sharding your workflow. One of the first is\n\nfor reliability. If you shard your work queue, then the failure of a single\n\nworkflow due to a bad update, infrastructure failure, or other problem only\n\naffects a fraction of your service.\n\nFor example, imagine that you push a bad update to your worker container,\n\nwhich causes your workers to crash and your queue to stop processing work\n\nitems. If you only have a single work queue that is processing items, then\n\nyou will have a complete outage for your service with all users affected. If,\n\ninstead, you have sharded your work queue into four different shards, you\n\nhave the opportunity to do a staged rollout of your new worker container.\n\nAssuming you catch the failure in the first phase of the staged rollout,\n\nsharding your queue into four different shards means that only one-quarter\n\nof your users would be affected.\n\nAn additional reason to shard your work queue is to more evenly distribute\n\nwork across different resources. If you don’t really care which region or\n\ndata center is used to process a particular set of work items, you can use a\n\nsharder to evenly spread work across multiple data centers to even out\n\nutilization of all data centers/regions. As with updates, spreading your work\n\nqueue across multiple failure regions also has the benefit of providing\n\nreliability against data center or region failures. An illustration of a sharded\n\nqueue when everything is working correctly is shown in Figure 12-5.\n\nFigure 12-5. An example of the sharding pattern in a healthy operation\n\nWhen the number of healthy shards is reduced due to failures, the sharding\n\nalgorithm dynamically adjusts to send work to the remaining healthy work\n\nqueues, even if only a single queue remains.\n\nThis is illustrated in Figure 12-6.\n\nFigure 12-6. When one work queue is unhealthy, the remaining work spills over to a different queue\n\nMerger\n\nThe last pattern for event-driven or workflow batch systems is a merger. A\n\nmerger is the opposite of a copier; the job of a merger is to take two\n\ndifferent work queues and turn them into a single work queue. Suppose, for\n\nexample, that you have a large number of different source repositories all\n\nadding new commits at the same time. You want to take each of these\n\ncommits and perform a build-and-test for it. It is not scalable to create a\n\nseparate build infrastructure for each source repository. We can model each\n\nof the different source repositories as a separate work queue source that\n\nprovides a set of commits. We can transform all of these different work\n\nqueue inputs into a single merged set of inputs using a merger adapter. This\n\nmerged stream of commits is then the single source to the build system that\n\nperforms the actual build. The merger is another great example of the\n\nadapter pattern, though in this case, the adapter is actually adapting multiple\n\nrunning source containers into a single merged source.\n\nThis multiadapter pattern is shown in Figure 12-7.\n\nFigure 12-7. Using multiple levels of containers to adapt multiple independent work queues into a single shared queue\n\nHands On: Building an Event-Driven\n\nFlow for New User Signup\n\nA concrete example of a workflow helps show how these patterns can be\n\nput together to form a complete operating system. The problem this\n\nexample will consider is a new user signup flow.\n\nImagine that our user acquisition funnel has two stages. The first is user\n\nverification. After a new user signs up, the user then has to receive an email\n\nnotification to validate their email. Once the user validates their email, they\n\nare sent a confirmation email. Then they are optionally registered for email,\n\ntext message, both, or neither for notifications.\n\nThe first step in the event-driven workflow is the generation of the\n\nverification email. To achieve this reliably, we will use the shard pattern to\n\nshard users across multiple different geographic failure zones. This ensures\n\nthat we will continue to process new user signups, even in the presence of\n\npartial failures. Each work queue shard sends a verification email to the end\n\nuser. At this point, this substage of the workflow is complete. This first\n\nstage of the flow is illustrated in Figure 12-8.\n\nFigure 12-8. The first stage of the workflow for user signup\n\nThe workflow begins again when we receive a verification email from the\n\nend user. These emails become new events in a separate (but clearly related)\n\nworkflow that sends welcome emails and sets up notifications. The first\n\nstage of this workflow is an example of the copier pattern, where the user is\n\ncopied into two work queues. The first work queue is responsible for\n\nsending the welcome email, and the second work queue is responsible for\n\nsetting up user notifications.\n\nOnce the work items have been duplicated between the queues, the email-\n\nsending queue simply takes care of sending an email message, and the\n\nworkflow exits. But remember that because of the use of the copier pattern,\n\nthere is still an additional copy of the event active in our workflow. This\n\ncopier triggers an additional work queue to handle notification settings.\n\nThis work queue feeds into an example of the filter pattern, which splits the\n\nwork queue into separate email and text message notification queues. These\n\nspecific queues register the user for email, text, or both notifications.\n\nThe remainder of this workflow is shown in Figure 12-9.\n\nFigure 12-9. The user notification and welcome email work queue\n\nPublisher/Subscriber Infrastructure\n\nWe have seen a variety of abstract patterns for linking together different\n\nevent-driven batch processing patterns. But when it comes time to actually\n\nbuild such a system, we need to figure out how to manage the stream of\n\ndata that passes through the event-driven workflow. The simplest thing to\n\ndo would be to simply write each element in the work queue to a particular\n\ndirectory on a local filesystem, and then have each stage monitor that\n\ndirectory for input. But of course doing this with a local filesystem limits\n\nour workflow to operating on a single node. We can introduce a network\n\nfilesystem to distribute files to multiple nodes, but this introduces\n\nincreasing complexity both in our code and in the deployment of the batch\n\nworkflow.\n\nInstead, a popular approach to building a workflow like this is to use a\n\npublisher/subscriber (pub/sub) API or service. A pub/sub API allows a user\n\nto define a collection of queues (sometimes called topics). One or more\n\npublishers publishes messages to these queues. Likewise, one or more\n\nsubscribers is listening to these queues for new messages. When a message\n\nis published, it is reliably stored by the queue and subsequently delivered to\n\nsubscribers in a reliable manner.\n\nAt this point, most public clouds feature a pub/sub API such as Azure’s\n\nEventGrid or Amazon’s Simple Queue Service. Additionally, the open\n\nsource Kafka project provides a very popular pub/sub implementation that\n\nyou can run on your own hardware as well as on cloud virtual machines.\n\nFor the remainder of this overview of pub/sub APIs, we’ll use Kafka for our\n\nexamples, but they are relatively simple to port to alternate pub/sub APIs.\n\nHands On: Deploying Kafka\n\nThere are obviously many ways to deploy Kafka, and one of the easiest\n\nways is to run it as a container using a Kubernetes cluster and the Helm\n\npackage manager.\n\nHelm is a package manager for Kubernetes that makes it easy to deploy and\n\nmanage prepackaged off-the-shelf applications like Kafka. If you don’t\n\nalready have the helm command-line tool installed, you can install it from\n\nthe Helm website.\n\nOnce the helm tool is on your machine, you need to initialize it.\n\nInitializing Helm deploys a cluster-side component named tiller to\n\nyour cluster and installs some templates to your local filesystem:\n\nhelm init\n\nNow that Helm is initialized, you can install Kafka using this command:\n\nhelm repo add incubator http://storage.googleapis\n\nhelm install --name kafka-service incubator/kafka\n\nNOTE\n\nHelm templates have different levels of production hardening and support. stable templates are\n\nthe most strictly vetted and supported, whereas incubator templates like Kafka are more\n\nexperimental and have less production mileage. Regardless, incubator templates are useful for quick\n\nproof of concepts as well as a place to start from when implementing a production deployment of a\n\nKubernetes-based service.\n\nOnce you have Kafka up and running, you can create a topic to publish to.\n\nGenerally in batch processing, you’re going to use a topic to represent the\n\noutput of one module in your workflow. This output is likely to be the input\n\nfor another module in the workflow.\n\nFor example, if you are using the Sharder pattern described previously,\n\nyou would have a topic for each of the output shards. If you called your\n\noutput Photos and you chose to have three shards, then you would have\n\nthree topics: Photos-1 , Photos-2 , and Photos-3 . Your sharder\n\nmodule would output messages to the appropriate topic after applying the\n\nsharding function.\n\nHere’s how you create a topic. First, create a container in the cluster so that\n\nyou can access Kafka:\n\nfor x in 0 1 2; do\n\nkubectl run kafka --image=solsson/kafka:0.11.0\n\n./bin/kafka-topics.sh --create --zookeeper ka\n\n--replication-factor 3 --partitions 10 --to done\n\nNote that there are two interesting parameters in addition to the topic name\n\nand the ZooKeeper service. They are --replication-factor and -\n\npartitions . The replication factor is how many different machines\n\nmessages in the topic will be replicated to. This is the redundancy that is\n\navailable in case things crash. A value of 3 or 5 is recommended. The\n\nsecond parameter is the number of partitions for the topic. The number of\n\npartitions represents the maximum distribution of the topic onto multiple\n\nmachines for purposes of load balancing. In this case, since there are 10\n\npartitions, there can be at most 10 different replicas of the topic for load\n\nbalancing.\n\nNow that we have created a topic, we can send messages to that topic:\n\nkubectl run kafka-producer --image=solsson/kafka ./bin/kafka-console-producer.sh --broker-list\n\n--topic photos-1\n\nOnce that command is up and connected, you should see the Kafka prompt,\n\nand you can then send messages to the topic(s). To receive messages, you\n\ncan run:\n\nkubectl run kafka-consumer --image=solsson/kafka\n\n./bin/kafka-console-consumer.sh --bootstrap-s\n\n--topic photos-1 \\\n\n--from-beginning\n\nOf course, running these command lines only gives you a taste of how to\n\ncommunicate via Kafka messages. To build a real-world event-driven batch\n\nprocessing system, you would likely use a proper programming language\n\nand Kafka SDK to access the service. But on the other hand, never\n\nunderestimate the power of a good bash script!\n\nThis section has shown how installing Kafka into your Kubernetes cluster\n\ncan dramatically simplify the task of building a work queue-based system.\n\nResiliency and Performance in Work\n\nQueues\n\nSo far in this chapter, we have designed workflows to optimize the design\n\nof the work queue to match the problem being addressed by the work\n\nqueue. This is an important part of engineering. The clearer the design of\n\nthe system matches the task, the easier it is to understand, update, and\n\nextend. However, an additional critical part of engineering is anticipating\n\nand designing systems for both the performance and reliability requirements\n\nof the users.\n\nSo far, in all of the designs we have assumed that all of the work in the\n\nsystem is roughly identical and each piece of work is equally reliable.\n\nUnfortunately, in the real world, neither one of these assumptions is\n\ntypically true. Unless your system is very, very unusual, it contains a mix of\n\ndifferent work. Different videos, different emails, different builds—work\n\nqueues are constantly dealing with similar but different individual pieces of\n\nwork.\n\nWork Stealing\n\nThe net result of this is that some pieces of work take longer than other\n\npieces of work. Sometimes this is inherent (that movie is just larger), but\n\noften it is because of design decisions in our code which are better or worse\n\nfits for a specific work item. We may have optimized our code for lots of\n\nfiles and few directories, and the work item might be the opposite: deep\n\ndirectory trees with relatively few files. Oftentimes the latency of individual\n\nwork items might not matter; if they generally land on different queues via\n\nsomething like sharding, the overall average times generally work out. But\n\nunfortunately, sometimes this average performance doesn’t work out, and\n\nall of the slow items end up on one queue. When this happens, the inputs to\n\nthe queue pile up and latency increases, much like a slow checkout lane in a\n\ngrocery store. Just like in the grocery store, the best option is to move from\n\nthe slow lane to a lane with a shorter line. This algorithm is called “work\n\nstealing.”\n\nThe basic algorithm for work stealing says that for any worker, as long as\n\nthat worker has work on their queue, the work on the queue is performed as\n\nyou would expect. When there is no work on a worker’s queue, instead of\n\nsleeping until more work arrives, the worker takes work off of the back of\n\nthe longest queue of any other worker. In this way, the maximum length of\n\nany work queue is kept to a minimum since workers with shorter work\n\nqueues remove work from workers with longer queues. Work is taken from\n\nthe back of the queue because that removal can be done safely without\n\nworrying about whether the work may be performed by multiple workers.\n\nErrors, Priority, and Retry\n\nThe other problems that can occur in a work queue system come when there\n\nare errors processing the work itself. The first problem that can happen is if\n\nyour worker crashes in the middle of processing the work. In the worst case,\n\nthe work that was being removed from the queue is lost. Fortunately, most\n\nqueue systems have semantics where the message being processed is\n\ninitially removed from the queue for processing. This initial removal hides\n\nthe message from other workers, but it doesn’t actually remove it from the\n\nqueue. When the work has been processed, a second “completed” message\n\nis sent to the queue, which permanently removes the work from the queue.\n\nTypically a message queue has a timeout after which, if the work has not\n\nbeen “completed,” the work item is returned to the head of the queue for\n\nother workers to pick it up.\n\nUnfortunately, recovering from this error results in an additional\n\ncomplication. What if the first worker is merely running slowly, not\n\ncrashed? Now we have two workers processing the same work item. The\n\neasiest way to deal with this situation is to make each work item\n\nidempotent. That way the additional processing is wasted computation, but\n\nit does not cause any additional errors. If it isn’t possible to make your work\n\nidempotent, there are various distributed locking methods discussed in\n\nprevious chapters that can be used. The important takeaway is that when\n\nwriting a worker, you must assume that multiple workers could be operating\n\non the same work item.\n\nAssuming idempotent work, we’re now safe from workers crashing, right?\n\nSadly, it’s not that easy. Imagine if the crashing worker is not due to random\n\nbugs in the worker’s code, but something about the work item itself.\n\nSuppose there is a “poison” work item which crashes a worker 100% of the\n\ntime. In such a situation, where one worker is recovering from the crash, the\n\npoison work is picked up by another worker, which itself crashes, causing\n\nanother worker to pick up the poison and so on and so forth until all of your\n\nworkers are crashing. Even if you are lucky and your workers recover\n\nquickly, poison work can cause you to lose a significant percentage of your\n\nworker capacity and slow down your work queue significantly.\n\nMuch as with error handling, the first solution to poison work is to keep\n\ntrack of the number of times that a particular piece of work has been\n\nremoved from the message queue. We can then use exponential backoff to\n\nincrease the timeout for the poison message larger and larger until we\n\nremove it from processing entirely. Generally, exponential backoff will\n\nsolve the problem of poison work.\n\nUnfortunately, requeuing work for processing introduces additional\n\nproblems. To understand why, consider the case where workers are crashing\n\nfor reasons unrelated to the work. Perhaps a downstream dependency is\n\nreturning bad data and causing the crash. In this situation, the work queue\n\neffectively stalls, and more and more work piles up in the queue. Since no\n\nworkers can make progress, even work stealing described previously won’t\n\nhelp. Suppose the situation resolves itself and the workers can start making\n\nprogress, like a traffic jam long after an accident; all of the work that has\n\npiled up in the queue means that new requests are still experiencing\n\nsignificant latency. This is often made especially bad in user-facing\n\nsituations where the old work items are often no longer relevant, but the\n\nnew work has value if it can be completed in time.\n\nTo address this problem of a backlog, it is often useful to actually have two\n\nqueues of work items. The first queue is used for all work items which have\n\nnever had an attempt made at processing. The second queue is the “retry”\n\nqueue, which is used for any message which has had at least one\n\n(unsuccessful) attempt at processing. Using this dual queue approach, in the\n\nabove situation, there will be a lot of work piled up in the retry queue\n\nwaiting for processing, but new work is no longer standing at the back of\n\nthe line of retries. It has an equal chance of being processed along with the\n\nretry work. This two-queue approach is an example of a general priority\n\nqueue approach, where different work queues have different priority and\n\nwork is taken off the higher priority queue until it is empty, and then the\n\nnext highest priority queue, etc. This can ensure that performance goes to\n\nwhere it is needed most to achieve latency goals.\n\nSummary\n\nThis chapter has introduced the idea of a workflow. Workflows combine\n\nmultiple work queues together into an orchestration of work to achieve\n\nsome more complex objective. Multiple patterns for building workflows,\n\nfrom copying, to filtering, sharding, and merging were introduced. Finally,\n\nthe important considerations of both performance and reliability in the face\n\nof slow work and worker crashes was examined. Using these patterns and\n\nerror handling approaches, any complex work can be broken into an easily\n\nreasoned-about and constructed workflow for processing data reliably and\n\nefficiently.\n\nOceanofPDF.com\n\nChapter 13. Coordinated Batch\n\nProcessing\n\nChapter 12 described a number of patterns for splitting and chaining queues\n\ntogether to achieve more complex batch processing. Duplicating and\n\nproducing multiple different outputs is often an important part of batch\n\nprocessing, but sometimes it is equally important to pull multiple outputs\n\nback together in order to generate some sort of aggregate output. A generic\n\nillustration of such a pattern is shown in Figure 13-1.",
      "page_number": 245
    },
    {
      "number": 13,
      "title": "Coordinated Batch",
      "start_page": 273,
      "end_page": 292,
      "detection_method": "regex_chapter_title",
      "content": "Figure 13-1. A generic parallel work distribution and result aggregation batch system\n\nProbably the most canonical example of this aggregation is the reduce part\n\nof the MapReduce pattern. It’s easy to see that the map step is an example\n\nof sharding a work queue, and the reduce step is an example of coordinated\n\nprocessing that eventually reduces a large number of outputs down to a\n\nsingle aggregate response. However, there are a number of different\n\naggregate patterns for batch processing, and this chapter discusses a number\n\nof them in addition to real-world applications.\n\nJoin (or Barrier Synchronization)\n\nIn previous chapters, we saw patterns for breaking up work and distributing\n\nit in parallel on multiple nodes. In particular, we saw how a sharded work\n\nqueue could distribute work in parallel to a number of different work queue\n\nshards. However, sometimes when processing a workflow, it is necessary to\n\nhave the complete set of work available to you before you move on to the\n\nnext stage of the workflow.\n\nOne option for doing this was shown in Chapter 12, which was to merge\n\nmultiple queues together. However, merge simply blends the output of two\n\nwork queues into a single work queue for additional processing. While the\n\nmerge pattern is sufficient in some cases, it does not ensure that a complete\n\ndata set is present prior to the beginning of processing. This means that\n\nthere can be no guarantees about the completeness of the processing being\n\nperformed, as well as no opportunity to compute aggregate statistics for all\n\nof the elements that have been processed.\n\nTo give a concrete example of the necessity of the join pattern, consider the\n\ncreation of a digitally animated film. Each frame of the film is an\n\nindependent work item; it can be rendered in parallel without reference to\n\nany of the other frames in the movie. However, consider transforming each\n\nframe of the movie into one second of video. You clearly need to have all\n\nthirty frames that make up that one second present before they are\n\ntransformed into video.\n\nWe need a stronger, coordinated primitive for batch data processing, and\n\nthat primitive is the join pattern. Join is similar to joining a thread. The\n\nbasic idea is that all of the work is happening in parallel, but work items\n\naren’t released out of the join until all of the work items that are processed\n\nin parallel are completed. This is also generally known as barrier\n\nsynchronization in concurrent programming. An illustration of the join\n\npattern for a coordinated batch is shown in Figure 13-2.\n\nCoordination through join ensures that no data is missing before some sort\n\nof aggregation phase is performed (e.g., finding the sum of some value in a\n\nset). The value of the join is that it ensures that all of the data in the set is\n\npresent. The downside of the join pattern is that it requires that all data be\n\nprocessed by a previous stage before subsequent computation can begin.\n\nThis reduces the parallelism that is possible in the batch workflow, and thus\n\nincreases the overall latency of running the workflow, as discussed in\n\nChapter 12. This latency increase can be compensated for by trading\n\ncompute for time and duplicating the processing of all of the work items to\n\nreduce the impact of tail latency.\n\nFigure 13-2. The join pattern for batch processing\n\nReduce\n\nIf sharding a work queue is an example of the map phase of the canonical\n\nMapReduce algorithm, then what remains is the reduce phase. Reduce is an\n\nexample of a coordinated batch processing pattern because it can happen\n\nregardless of how the input is split up, and it is used similar to join; that is,\n\nto group together the parallel output of a number of different batch\n\noperations on different pieces of data.\n\nHowever, in contrast to the join pattern described previously, the goal of\n\nreduce is not to wait until all data has been processed, but rather to\n\noptimistically merge together all of the parallel data items into a single\n\ncomprehensive representation of the full set.\n\nWith the reduce pattern, each step in the reduce merges several different\n\noutputs into a single output. This stage is called “reduce” because it reduces\n\nthe total number of outputs. Additionally, it reduces the data from a\n\ncomplete data item to simply the representative data necessary for\n\nproducing the answer to a specific batch computation. Because the reduce\n\nphase operates on a range of input and produces a similar output, the reduce\n\nphase can be repeated as many or as few times as necessary in order to\n\nsuccessfully reduce the output down to a single output for the entire data\n\nset.\n\nThis is a fortunate contrast to the join pattern, because unlike join, it means\n\nthat reduce can be started in parallel while there is still processing going on\n\nas part of the map/shard phase. Of course, in order to produce a complete\n\noutput, all of the data must be processed eventually, but the ability to begin\n\nearly means that the batch computation executes more quickly overall.\n\nHands On: Count\n\nTo understand how the reduce pattern works, consider the task of counting\n\nthe number of instances of a particular word in a book. We can first use\n\nsharding to divide up the work of counting words into a number of different\n\nwork queues. As an example, we could create 10 different sharded work\n\nqueues with 10 different people responsible for counting words in each\n\nqueue. We can shard the book among these 10 work queues by looking at\n\nthe page number. All pages that end in the number 1 will go to the first\n\nqueue, all pages that end in the number 2 will go to the second, and so forth.\n\nOnce all of the people have finished processing their pages, they write\n\ndown their results on a piece of paper. For example, they might write:\n\na: 50\n\nthe: 17 cat: 2\n\nairplane: 1\n\n...\n\nThis can be output to the reduce phase. Remember that the reduce pattern\n\nreduces by combining two or more outputs into a single output.\n\nGiven a second output:\n\na: 30\n\nthe: 25\n\ndog: 4\n\nairplane: 2\n\n...\n\nThe reduction proceeds by summing up all of the counts for the various\n\nwords, in this example producing:\n\na: 80\n\nthe 42\n\ndog: 4\n\ncat: 2\n\nairplane: 3\n\n...\n\nIt’s clear to see that this reduction phase can be repeated on the output of\n\nprevious reduce phases until there is only a single reduced output left. This\n\nis valuable since this means that reductions can be performed in parallel.\n\nUltimately, in this example you can see that the output of the reduction will\n\nbe a single output with the count of all of the various words that are present\n\nin the book. Of course, the number of times reduce is run has an impact on\n\nthe overall performance of the batch processing. Too many reduce\n\noperations cause significant wasted computation, while too few begin to\n\nresemble the synchronization of the join pattern.\n\nSum\n\nA similar but slightly different form of reduction is the summation of a\n\ncollection of different values. This is like counting, but rather than simply\n\ncounting one for every value, you actually add together a value that is\n\npresent in the original output data.\n\nSuppose, for example, you want to sum the total population of the United\n\nStates. Assume that you will do this by measuring the population in every\n\ntown and then summing them all together.\n\nA first step might be to shard the work into work queues of towns, sharded\n\nby state. This is a great first sharding, but it’s clear that even when\n\ndistributed in parallel, it would take a single person a long time to count the\n\nnumber of people in every town. Consequently, we perform a second\n\nsharding to another set of work queues, this time by county.\n\nAt this point, we have parallelized first to the level of states, then to the\n\nlevel of counties, and then each work queue in each county produces a\n\nstream of outputs of (town, population) tuples.\n\nNow that we are producing output, the reduce pattern can kick in.\n\nIn this case, the reduce doesn’t even really need to be aware of the two-\n\nlevel sharding that we performed. It is sufficient for the reduce to simply\n\ngrab two or more output items, such as (Seattle, 4,000,000) and\n\n(Northampton, 25,000) , and sum them together to produce a new\n\noutput (Seattle-Northampton, 4,025,000) . It’s clear to see\n\nthat, like counting, this reduction can be performed an arbitrary number of\n\ntimes with the same code running at each interval, and at the end, there will\n\nonly be a single output containing the complete population of the United\n\nStates. Importantly, again, nearly all of the computation required is\n\nhappening in parallel.\n\nHistogram\n\nAs a final example of the reduce pattern, consider that while we are\n\ncounting the population of the United States via parallel sharding/mapping\n\nand reducing, we also want to build a model of the average American\n\nfamily. To do this, we want to develop a histogram of family size; that is, a\n\nmodel that estimates the total number of families with zero to 10 children.\n\nWe will perform our multilevel sharding exactly as before (indeed, we can\n\nlikely use the same workers).\n\nHowever, this time, the output of the data collection phase is a histogram\n\nper town:\n\n0: 15% 1: 25%\n\n2: 45%\n\n3: 10%\n\n4: 5%\n\nFrom the previous examples, we can see that if we apply the reduce pattern,\n\nwe should be able to combine all of these histograms to develop a\n\ncomprehensive picture of the United States. At first blush, it may seem\n\nquite difficult to understand how to merge these histograms, but when\n\ncombined with the population data from the summation example, we can\n\nsee that if we multiply each histogram by its relative population, then we\n\ncan obtain the total population for each item being merged. If we then\n\ndivide this new total by the sum of the merged populations, it is clear that\n\nwe can merge and update multiple different histograms into a single output.\n\nGiven this, we can apply the reduce pattern as many times as necessary\n\nuntil a single output is produced.\n\nHands On: An Image Tagging and Processing Pipeline\n\nTo see how coordinated batch processing can be used to accomplish a larger\n\nbatch task, consider the job of tagging and processing a set of images. Let\n\nus assume that we have a large collection of images of highways at rush\n\nhour, and we want to count both the numbers of cars, trucks, and\n\nmotorcycles, as well as distribution of the colors of each of the vehicles. Let\n\nus also suppose that there is a preliminary step to blur the license plates of\n\nall of the vehicles to preserve anonymity.\n\nThe images are delivered to us as a series of HTTPS URLs where each\n\nURL points to a raw image. The first stage in the pipeline is to find and blur\n\nthe license plates. To simplify each task in the work queue, we will have\n\none worker that detects a license plate, and a second worker that blurs that\n\nlocation in the image. We will combine these two different worker\n\ncontainers into a single container group using the multiworker pattern\n\ndescribed in Chapter 12.\n\nThis separation of concerns may seem unnecessary, but it facilitates\n\ncontainer image reuse. Imagine if the API to our container is an image and a\n\nbox to blur. The same container that blurs license plates can be reused to\n\nblur people’s faces in a different pipeline.\n\nAdditionally, to ensure reliability and to maximize parallel processing, we\n\nwill shard the images across multiple worker queues. This complete\n\nworkflow for sharded image blurring is shown in Figure 13-3.\n\nFigure 13-3. The sharded work queue and the multiple blurring shards\n\nOnce each image has been successfully blurred, we will upload it to a\n\ndifferent location, and we will then delete the originals. However, we don’t\n\nwant to delete the original until all of the images have been successfully\n\nblurred in case there is some sort of catastrophic failure and we need to\n\nrerun this entire pipeline. Thus, to wait for all of the blurring to complete,\n\nwe use the join pattern to merge the output of all of the sharded blurring\n\nwork queues into a single queue that will only release its items after all of\n\nthe shards have completed the work.\n\nNow we are ready to delete the original images as well as begin work on\n\nvehicle model and color detection. Again, we want to maximize the\n\nthroughput of this pipeline, so we will use the copier pattern from\n\nChapter 12 to duplicate the work queue items to two different queues:\n\nA work queue that deletes the original images\n\nA work queue that identifies the type of vehicle (car, truck, motorcycle)\n\nand the color of the vehicle\n\nFigure 13-4 shows these stages of the processing pipeline.\n\nFigure 13-4. The output join, copier, deletion, and image recognition parts of the pipeline\n\nFinally, we need to design the queue that identifies vehicles and colors and\n\naggregates these statistics into a final count. To do this, we first again apply\n\nthe shard pattern to distribute the work out to a number of queues. Each of\n\nthese queues has two different workers: one that identifies the location and\n\ntype of each vehicle and one that identifies the color of a region.\n\nWe will again join these together using the multiworker pattern described in\n\nChapter 12. As before, the separation of code into different containers\n\nenables us to reuse the color detection container for multiple tasks beyond\n\nidentifying the color of the vehicles.\n\nThe output of this work queue is a JSON tuple that looks like this:\n\n{\n\n\"vehicles\": {\n\n\"car\": 12,\n\n\"truck\": 7,\n\n\"motorcycle\": 4\n\n}, \"colors\": {\n\n\"white\": 8,\n\n\"black\": 3,\n\n\"blue\": 6,\n\n\"red\": 6\n\n}\n\n}\n\nThis data represents the information found in a single image. To aggregate\n\nall of this data together, we will use the reduce pattern described previously\n\nand made famous by MapReduce to sum everything together just as we did\n\nin the count example above. At the end, this reduce pipeline stage produces\n\nthe final count of images and colors found in the complete set of images.\n\nSummary\n\nThis chapter describes coordinated batch processing, which is used to\n\nsynchronize workflows for preconditions or to combine output from\n\nprevious workflows into a merged output. The join pattern is used as a\n\nbarrier synchronizer to ensure that all of the necessary output is ready\n\nbefore proceeding to the next stage. Reduce combines output from multiple\n\nshards into an output that can be reduced further until the final output is\n\nready. Sum and histogram are specific types of reduce operations that are\n\nuseful for obtaining specific statistical results from your data. Taken\n\ntogether, these patterns for coordination in batch processing can ensure that\n\nyou can build the workflow that is necessary for any batch operation or\n\ncalculation you need to perform.\n\nOceanofPDF.com\n\nPart V. Universal Concepts\n\nSo far we have focused on patterns for building your applications, all the\n\nway from single nodes to complex distributed systems. In addition to these\n\npatterns, there are parts of our systems that are universal to all applications\n\nthat you build. The following chapters in this section cover some of these\n\nuniversal concepts. Chapter 14 discusses observability for your application\n\nvia logging, monitoring and alerting. Observability is critical for you to\n\nunderstand if your application is operating correctly. Chapter 15 covers AI,\n\nwhich is revolutionizing the way that we think of applications and user\n\ninterfaces. Finally, Chapter 16 ends this section by including common\n\nmistakes and errors that occur with unfortunate regularity in distributed\n\nsystems that people build. No matter your application, all of these chapters\n\nwill enable you to make it more intelligent and more reliable.\n\nOceanofPDF.com\n\nChapter 14. Monitoring and\n\nObservability Patterns\n\nOne of the core differences between client applications and distributed\n\nsystems is that generally distributed systems implement services. Services\n\nare always on, always available for users around the world in all time zones\n\nand ways of working. Because of the 24/7 nature of these systems,\n\nmonitoring and observability become critical to building reliable systems.\n\nTo deliver reliability, you must notice a problem before the customer\n\nnotices a problem; and to solve any problems you find, you need to be able\n\nto understand how your system is operating. This chapter focuses on best\n\npractices for such monitoring and observability.\n\nMonitoring and Observability Basics\n\nBefore we get into the details of implementing monitoring and\n\nobservability, it is useful to ground ourselves in the core set of concepts that\n\nmake up any monitoring and observability solution.\n\nIn any system, there are four key concepts which make up our solutions:\n\nLogging\n\nMetrics\n\nAlerting",
      "page_number": 273
    },
    {
      "number": 14,
      "title": "Monitoring and",
      "start_page": 293,
      "end_page": 314,
      "detection_method": "regex_chapter_title",
      "content": "Tracing\n\nWe’ll step through each of these in a little more detail.\n\nIt’s highly likely that anyone who has built even the smallest system has\n\nimplemented logging, even if they don’t realize that they have. The simplest\n\nversion of logging is the humble printf statement. Of course, there are\n\nmany more sophisticated ways to do logging, but ultimately they all serve\n\nthe same purpose as that print statement. Namely, they show us that a\n\nparticular place in our code has executed, and they record data associated\n\nwith that place in the code. Logging records data that helps us understand\n\nspecific executions of our code.\n\nThe contrast to logging are metrics or monitoring. While logging is\n\nassociated with a specific execution of our source code, a metric is\n\ngenerally related to aggregate data across multiple requests. Examples of\n\nmetrics could be the number of times this particular function has been\n\ncalled in the last minute, or the average length of time spent in that function\n\nacross all of these calls. While logging provides us a perspective to\n\nunderstand a specific execution of our service, metrics provide us a global\n\nperspective of how our service is executing in general.\n\nThe purpose of logging and metrics are to help us understand our systems\n\nwhen we are looking at them, or when we know that there is a problem. If\n\nwe spent all of our time staring at graphs of metrics and logging interfaces,\n\nthat might be all we need; but of course, in real life we spend much of our\n\ntime in other environments. Consequently, we need alerting to detect\n\nproblems and notify us that we need to interrupt our lives and go look at the\n\nlogging or metrics. Effectively, alerts are rules that are applied to either logs\n\nor metrics which trigger an event. Generally, this event triggers a\n\nnotification which is intended to draw our attention to a problem (or\n\npotential problem) in our systems. Crafting high-quality alerts is a key part\n\nof building a reliable system.\n\nThe final component in our system relates to the distributed part of our\n\ndistributed system. One of the hardest problems in building out a distributed\n\nsystem is understanding how all of the pieces work together, or even harder,\n\nunderstanding why they may fail in certain circumstances. From a naive\n\npoint of view, every individual request to a microservice is unique and\n\nindividual. But the truth is that from the broader perspective of the\n\napplication, there is often a larger request, which is being serviced by a\n\nchain of calls through all of the microservices. Tracing is the act of\n\nrebuilding this context so that instead of seeing individual, unique requests\n\nat each microservice, we can understand that all of these individual pieces\n\nare part of a broader user request. This global perspective, in turn, enables\n\nus to understand (for example) why a particular user’s request is running\n\nslowly or failing to work at all. Tracing takes all of the pieces of a\n\ndistributed system and reconstructs the user’s perspective.\n\nIn the coming sections, we’ll dive deeper into each of these components and\n\nsee how you can use them to build a reliable system.\n\nLogging\n\nOnce you figure out how to log information, one of the first things that you\n\nnotice in a distributed system is just how much information gets logged.\n\nThe volume of logs, or “log spam” can often be a significant problem in\n\nterms of understanding a problem, or even just the raw cost of storing and\n\nsearching all of those logs.\n\nThe truth is that the value of logging is very contextual. When you are\n\ndebugging a specific problem in a specific component, then verbose,\n\ndetailed logging can be critical to identifying the problem. On the other\n\nhand, during standard execution, where everything is working correctly,\n\nmost of the logging is just wasted noise, except those logs which trigger\n\nalerts because of system errors.\n\nConsequently, only the simplest distributed systems use logging like basic\n\nprinting. Nearly every system uses some sort of logging library. These\n\nlogging libraries provide important capabilities that make it much easier to\n\ndeal with lots and lots of logs. The most basic capability provided by such\n\nlibraries is basic timestamping. Oftentimes we are trying to understand what\n\nhappened and in what sequence; and without a shared notion of time in\n\nevery log line, it is difficult to know (for example) the amount of time that\n\npassed between different logs. In addition to timestamp, most logging\n\nlibraries enable the addition of other types of context to the logs. In any\n\nmultithreaded system, many requests may be processed in parallel, and thus\n\nmany different identical logs may be issued. To differentiate between these\n\nlogs and understand how they relate to a specific request, context like the\n\nthread identifier, user ID, or request ID help differentiate between otherwise\n\nidentical logs on the same server and filter to the context of a specific\n\nrequest. The final, and often most valuable, service provided by logging\n\nlibraries is leveled logging. Leveled logs allow you to express the type of\n\nlogs being emitted. The specific number of different levels often vary by\n\nlibrary, but in general they include:\n\nDebug\n\nVerbose logs, generally only useful when doing detailed debugging.\n\nInfo\n\nOften useful but not exceptional messages tied to the operation of the\n\nservice.\n\nWarning\n\nSomething that is concerning, but not necessarily fatal to the entire\n\napplication.\n\nError\n\nSomething very bad has happened, and someone should take a look\n\nas soon as possible.\n\nFatal\n\nSomething unrecoverable has happened, and the application will\n\nterminate after writing this log.\n\nLeveled logging enables you to filter only to certain types of messages, and\n\nit allows you to leave detailed logging in your code, for when it is needed,\n\nwithout filling up your log streams with many, many spammy log messages.\n\nGenerally, your log level for an application is set to Info or Warning\n\ndepending on your logging style. Debug messages are reserved for\n\nconditional activation to debug specific problems.\n\nIn addition to the request context, a valuable context for the logging library\n\nis the location of the log call itself. Typically, logging systems understand\n\nwhich class or component contains the log. This enables conditionally\n\nlogging where you dynamically activate detailed debug logging for specific\n\nclasses or other components on running servers. Conditional logging\n\nenables you to respond to a reported incident or a problem seen by a\n\nspecific user without requiring the storage or latency of detailed logging for\n\nevery request. Some care should be used when performing conditional\n\nlogging, as it can significantly increase the latency of end-user requests.\n\nNothing is worse than causing a worse incident or outage in the process of\n\ninvestigating an outage.\n\nOf course, to make logging useful, you must have a clear sense of what the\n\npurpose of logging is in the first place. Ultimately, logging provides insight\n\ninto the operation of your service or application. One common joke is that\n\nthe way to know what to log is to remember what you were missing when\n\nyou initially debugged the problem. That is, logging is often a retrospective\n\nexercise “I wish I had logged… .” It goes without saying that the most\n\nuseful things to log are exceptional or erroneous situations. Every error or\n\nexception should probably be logged unless you are completely confident in\n\nthe error-handling code (and sometimes even then). Beyond errors, often\n\nthe most interesting things to log are the things that are weird. For example,\n\nsome requests are just slow. You can stare at metrics (see “Metrics”)\n\nshowing that your 99th-percentile requests take a ridiculous amount of time,\n\nbut without logs indicating which parts of request handling are\n\nexceptionally slow, it is hard to take action. On the other hand, just logging\n\nthe play by play (“function foo was called”) is rarely going to be that useful,\n\nand it is definitely going to spam your logs. When logging, try to imagine\n\nthe hardest problem you might need to debug in that particular part of the\n\ncode, and write down in the log whatever you would need to successfully\n\nfigure it out.\n\nMetrics\n\nIn contrast to logging, with its focus on individual executions, monitoring\n\nmetrics are designed to collect statistics over time which characterize the\n\ngeneral execution of your services. The focus of this metrics section will be\n\nthe Prometheus monitoring application, which has become the de facto\n\nindustry standard. Other monitoring solutions offer similar capabilities.\n\nMonitoring data is generally recorded over time; from this view you can see\n\nboth the instantaneous state of your application and also how that state is\n\nchanging over time. When monitoring your application there are three basic\n\ntypes of data that are interesting to record:\n\nHistograms\n\nCounts\n\nValues\n\nHistograms represent a distribution of values over some interval.\n\nHistograms enable you to understand both the average and the extreme\n\nexperiences of your users. For example, a histogram of latency can show\n\nyou the best 10% average and worst 10% latency that your users\n\nexperienced. Such information is useful to give you an “at-a-glance” sense\n\nof how your application is performing.\n\nCounts are monotonically increasing values that keep track of some value\n\nthat can only increase. One of the most common examples of a count is the\n\ntotal number of requests served by your service. This number can clearly\n\nonly increase over time, as there are more and more requests. Counts are\n\nalso often transformed into rates, which are the first-order derivative of the\n\ncount and represent the rate of change of that count over time. Given a\n\ncount that tracks the total number of requests, the rate could give you the\n\nnumber of requests per second. Counts are inherently integral (whole\n\nnumbers) in nature.\n\nFinally, there are values; these are numbers which can take on any value\n\nand can both increase and decrease over time. Common examples of values\n\ninclude CPU and memory usage for a service. Clearly, the amount of CPU\n\nand memory use can vary over time and increase or decrease based on the\n\noperation of the service. Values are most useful when viewed over time\n\nbecause they can indicate the overall behavior of the system. Steadily\n\nincreasing memory usage can indicate a memory leak. Periodically spikey\n\nCPU can indicate some background process which is unnecessarily\n\nexpensive.\n\nIt should be clear at this point that the main value of monitoring data is the\n\nperspective that it gives over time. Indeed, most monitoring is referred to as\n\na time series representing exactly these values over time. Dedicated\n\ndatabases for this time series data have been developed to efficiently store\n\nand query this data. The Prometheus system comes with a simple time-\n\nseries database, but most people will integrate Prometheus with a dedicated\n\ntime series database. Often, a good option is to integrate Prometheus\n\nmonitoring with a cloud-based monitoring as a service. At this point, most\n\ncloud-based monitoring services, for example Azure Monitor, support the\n\ningestion of Prometheus metrics.\n\nWhen it comes to ingesting metrics, Prometheus is a pull-based or scraping\n\nmetrics system. Periodically the Prometheus application sends requests out\n\nto your application for the current state of all of the metrics. These metrics\n\nare stored into the time-series database for later query and recording. Pull-\n\nbased monitoring works well for applications that are always running, but\n\nhow can you record metrics for batch and other transient jobs that may not\n\nbe around long enough to be scraped? For such jobs, you need to implement\n\npush-based metrics instead. Fortunately, Prometheus has a sister project, the\n\nPrometheus push gateway, which implements exactly this pattern. When\n\nyou run the push gateway, it runs a dedicated server that Prometheus can\n\nscrape. Your applications can send or “push” metrics into this gateway so\n\nthat they are recorded even if your application is no longer running.\n\nCombining pull-based monitoring for your long-running applications and\n\npush-based metrics for transient jobs allows you to have a complete\n\nperspective on the operation of your application.\n\nBasic Request Monitoring\n\nOne of the first types of monitoring that most people add is for the number\n\nof requests served by your application. This is a basic statistic that\n\ndemonstrates how many people (or other services) are using your service\n\nover time. The number of requests is an example of a count metric. The\n\nnumber of requests cannot decrease since there is no such thing as a\n\nnegative request.\n\nHowever, the total number of requests is actually not that interesting, what\n\nis more interesting is the rate of change of the total number of requests.\n\nRate of change is a secondary metric defined as the difference between the\n\nnumber of requests at time t and the number of requests at time t + 1. To\n\nmake this concrete, the rate of requests per minute is the total number of\n\nrequests right now, minus the total number of requests one minute ago. To\n\nturn this into a graph, the rate is calculated for each previous minute\n\nmoving backward and producing a rate at one-minute intervals. Of course,\n\nin this example, one minute is arbitrary; you can do equivalent calculations\n\nfor whatever time interval that you want, from a millisecond to a day (or\n\nmore).\n\nThinking about calculating the rate of requests for a millisecond\n\ndemonstrates one of the challenges with rate metrics. If your server serves\n\nfewer than 1,000 requests per second, calculating the rate of requests at\n\nmillisecond granularity will lead to a very spikey metric, which oscillates\n\nbetween zero and one (and possibly more if there is a brief surge of\n\nrequests). Rates are variable and highly dependent on external factors.\n\nConsequently, it is often more useful to use the average of the rate of\n\nrequests per time interval, rather than just the raw value. Averaging the rate\n\nover time smooths the peaks and valleys and gives you a more realistic\n\npicture of how your service is being accessed.\n\nThe total number of requests is a good metric to start with, but to truly\n\nunderstand the operation of your system, it is useful to look at more details\n\nassociated with each request. One of the most common features of a request\n\nis the HTTP response code which it returned to the users. If you are not\n\nalready familiar with HTTP response codes, they are a part of the HTTP\n\nprotocol that indicates how the server responded to the request. Some of the\n\nmost common are 200 (OK), 404 (Not found), and 500 (Internal Server\n\nError), but there are many more response codes for different situations.\n\nWith Prometheus metrics, values can have labels which add detail to the\n\nmetric. Using this capability, we can view the response code as a label\n\nassociated with the request count. Using this label, we can query for the\n\ntotal number of requests which were served successfully ( code==200 ) or\n\nthe total number of requests which resulted in errors ( code==500 ); we\n\ncan also calculate the rate of each of these types of requests using the\n\nmethods described previously. Subdividing the total request count by the\n\nresponse code gives us a much richer view into the operation of our\n\napplication. We can also group error codes together and view how many\n\nrequests were errors (codes starting with 5xx) and how many requests were\n\nsuccessful (codes starting with 2xx).\n\nIn addition to the response code for a server request, the next most\n\nimportant characteristic of the request is the latency of the request, or how\n\nmuch time it took to process. It might be tempting to think that latency\n\ncould be a label on the request count as well, but this doesn’t work. Labels\n\nneed to be a limited set of discrete values, while latency is a value that can\n\ntake on many different arbitrary values. Request latency is a separate\n\nmetric, and it is generally modeled as a histogram. Using the histogram\n\nmetric for request latency, you can understand the average experience of the\n\nuser as well as the experience of users who have the slowest requests. As\n\nwith request count, the request latency can also be labeled using the\n\nresponse code. That way, not only can you see the average experience of\n\nany request, but you can also calculate the average latency of an error, of a\n\nnot found request, or any other HTTP response code.\n\nAdvanced Request Monitoring\n\nThough total request count, the rate of requests, and latency are the most\n\ncommon and most generally useful metrics for monitoring requests to your\n\nserver or service, there are other more specialized metrics, which can be\n\nquite useful to have a deep understanding of how your service is\n\nperforming. A good example of this might be the request and response sizes\n\nin bytes for your service. Request size is useful because you can use it to\n\ncorrelate to slow requests (larger requests may take longer to parse and\n\nprocess) and also to look at it over time to detect patterns like increasing\n\nrequests or response sizes over time. Like latency, request and response size\n\nare examples of histogram metrics.\n\nSometimes useful metrics are recorded by adding detail to how you\n\ncalculate and report metrics in your application. For example, it is\n\nsometimes useful to understand the amount of time a request has spent in a\n\nqueue, versus how much time was spent processing a request. This sort of\n\ninformation is especially useful when you are debugging why your service\n\nis slow in processing requests. Simply seeing that latency is high will show\n\nyou that there is a problem and that users are likely unhappy, but having\n\nmetrics for queue time and processing time will help you understand\n\nwhether the problem is that you need to scale out (queue time is too long) or\n\nif you need to optimize your code (processing time is too long). To get these\n\nmetrics, you actually need to add additional metrics to your code. In this\n\ncase, you would add two new histogram metrics, one measuring the time\n\nfrom when a request is received to when processing starts, and one\n\nmeasuring the time from when processing starts to when the request is\n\ncompleted. This system now has three histogram metrics measuring latency:\n\ntotal latency, queue latency, and processing latency. Metrics are cheap to\n\nadd, and the additional insight that they provide is often invaluable in\n\ndebugging your service. Of course, as with anything, it is very possible to\n\noverdo things, and if you are monitoring the latency of each line of code (or\n\neven each function), you’re probably taking things a little too far.\n\nAnother advanced metric for requests might be to break them down by\n\ngeography or customer. This monitoring can give you business insights that\n\ncan help you build better products, or possibly identify abusive requests\n\ncoming from specific users or locations. If you are willing to only look at\n\ncoarse-grained geography (e.g., country), then it is possible to view\n\ngeography as another label that you can apply to the request count metric;\n\nbut more often, this sort of data needs to be obtained by querying across\n\nlogs instead of looking at metrics directly. For example, in any large system\n\nthere are far too many customers to use a customer ID as a label for the\n\nrequest count metric. Instead, the system logs the customer ID whenever a\n\nrequest is processed. This example demonstrates the interplay between\n\nlogging and monitoring. Monitoring and metrics can provide aggregate data\n\nthat is quick to query. For detailed information, logging and log search is an\n\nessential technique that provides slower, but more rich, access to data. In\n\nthis case, because the business analysis can likely stand extra latency in\n\nprocessing, obtaining customer usage information from log search can meet\n\nthe business requirements.\n\nAlerting\n\nSo far we’ve discussed all of the different ways to record information from\n\nyour application’s logs and metrics. This data can be critical for\n\nunderstanding why your system isn’t behaving correctly during an outage or\n\nother incident. But how can you figure out something is happening in the\n\nfirst place?\n\nObviously, the first way we find out that there is a problem in our system is\n\nwhen a customer or user complains. But waiting for such customer reported\n\nincidents, or CRIs, is a bad experience for both the user and the person\n\nreceiving the alert. Nothing hurts customer confidence like being told that\n\nthey were the first one to notice a problem. At this point with our systems,\n\nevery user expects that we will notice (and fix) problems in our services\n\nbefore the user notices that anything is amiss.\n\nThus, the best way to find out about a problem is via alerting. Alerting is\n\nthe practice of establishing conditions under which the team responsible for\n\na service is notified, typically through a pager alert, that there is a problem\n\nwith the system.\n\nBasic alerting\n\nThe most basic form of alerting is based on metric queries and static\n\nthresholds. For example, you might choose to alert if the latency at the 90th\n\npercentile in your system is greater than half a second. You might also\n\nchose to alert if the number of HTTP 500 errors reported by your system is\n\ngreater than 1% of all requests. Given these rules, it’s easy to see that if\n\neither occurred it would be a good idea to let the engineers know; but how\n\ndo you go about determining the right alerts for your system?\n\nThe most basic way to think about writing alerts is that they define the\n\nservice level objective (SLO) for your application. If you don’t alert, you\n\nwon’t notice and so your alert thresholds define the difference between\n\nnominal and exceptional operation. Every application is different, but it is\n\ngood to write down what the goals are for a “normal” user experience of\n\nyour service and right alerts that fire if those goals aren’t being achieved.\n\nAnother consideration is the experience of your on-call engineers. Firing\n\nalerts constantly is a recipe for burnout and people quitting. Similarly, firing\n\nalerts that are false alarms (“crying wolf”) will cause your on-call engineers\n\nto stop responding. Therefore, you need to balance the goals of your service\n\nwith the needs of your on-call engineers to determine the right alerts. The\n\ndevelopment of alerts is an ongoing activity throughout the lifespan of your\n\nservice. As you add more capabilities and, thus, more monitoring, you\n\nshould also be adding more alerting; as you get more experience with the\n\nsystem, you can tune the alerts to be tighter or looser depending on the\n\nneeds of your users or your on-call engineers.\n\nAlerting on anomolies\n\nBasic alerting is great for many use cases, but sometimes it is insufficient. A\n\ngreat example of this is an error that causes a 100% outage for a small (1%)\n\nsubset of your users. From a static alerting perspective, your system is\n\nfunctioning within bounds, but for those poor users in that subset your\n\nservice is completely down. Static alerting assumes that all metrics are\n\nuniform across all users and service calls. However, this isn’t always the\n\ncase. Anomoly detection is a form of AI that can identify anmolies: for\n\nexample, customer A is 100% down while customer B is working perfectly.\n\nThough alerting on anomolies is an advanced topic, it is often the only way\n\nto find a balance between alerting when there is a problem and firing lots of\n\nfalse alarms. As your system gets larger and more complex, anomoly\n\ndetection alerting will become a critical requirement.\n\nMonitoring and logging provides visibility into your application. Alerting\n\ntells you when you should be paying attention. Importantly, not every\n\nmetric requires an alert; some are useful only for debugging and\n\nvisualization. However, the performance of your application will be defined\n\nby the quality of your alerts. Invest in alerting with the same energy you\n\nbring to designing customer-facing features.\n\nTracing\n\nAs we have seen in the preceding chapters, most modern applications\n\ninvolve requests that span multiple machines and processes. From the\n\ntechniques above, you can learn how to monitor all of these components\n\nindividually. But unfortunately, this service-by-service view shows you all\n\nof the different requests flowing through your system at the same time, but\n\nit doesn’t do a good job showing you the path of a single request through all\n\nthe microservices in your system. Request tracing correlates all of the\n\nservice metrics together and gives you that end-to-end perspective.\n\nThe easiest way to achieve tracing is to create a unique correlation ID for\n\nevery request as it enters your system. Effectively, this correlation ID is a\n\nnumber that can be used to group together logs, metrics, and other\n\ninformation across your application. The correlation ID can be anything you\n\nwant as long as it uniquely identifies each request. The simplest correlation\n\nID would be a large random number; a better one would be a hash function\n\napplied to unique characteristics of the original request like its RESTful\n\npath and source IP address.\n\nOnce you have a correlation ID, whenever you emit a log or a metric, you\n\nemit the correlation ID. For example, in a logging system, in addition to the\n\ntimestamp and the log level, you would also log the request correlation ID.\n\nTo enable you to log that ID within all components in your system, you\n\nneed to pass it along with the API calls you make throughout your system.\n\nAssuming that you are making API requests using the HTTP protocol, you\n\ncan embed your correlation ID in an application-specific header, for\n\nexample, x-my-apps-correlation-id , and modify your code to\n\nread from that header and populate a request context object with that ID.\n\nOnce you have correlation IDs populated throughout your logging and\n\nmetrics, you can group that information back together to visualize the path\n\nof a single request through your entire system.\n\nIf this sounds like a lot of work to implement and get right, you’re right, it\n\nis. Fortunately, the OpenTelemetry project has taken care of much of the\n\nheavy lifting for you. OpenTelemetry has SDKs for all popular languages\n\nthat make it easy to integrate request tracing into your application. It has a\n\nvendor-agnostic philosophy that allows it to integrate with numerous\n\nbackends for metrics and logging. Additionally, numerous visualization and\n\ndebugging tools have also integrated with OpenTelemetry to provide rich\n\nintrospection capabilities. In nearly all cases, integrating with a community-\n\ndriven open source project like OpenTelemetry for request tracing is a\n\nbetter idea than building your own.\n\nAggregating Information\n\nOnce you start monitoring, you will discover that monitoring can generate a\n\ntremendous amount of information. Efficiently maintaining necessary\n\ninformation as well as being able to quickly sift through it to find the right\n\ninformation for your current issue becomes an important challenge.\n\nAggregation enables you to group information together so that it is easier to\n\nunderstand or more efficient to store.\n\nThe first form of aggregation that most people begin with is querying logs\n\nacross multiple processes. In “Tracing” we discussed how OpenTelemetry\n\nprovides a request-oriented view of your data, but that’s not the only\n\naggregation you might want to perform. If you have a component that has\n\nbeen replicated for scale, you may want to search for instances of a specific\n\nerror or failure across all of the replicas.\n\nFortunately, there are numerous tools that make it easier to group logs\n\nacross multiple containers in Kubernetes. One of the easiest to use is a tool\n\nlike ktail , which extends the traditional kubectl log interface to\n\ncombine logs across multiple pods. More sophisticated tools ingest logs into\n\na search index like Elasticsearch, which makes it possible to run arbitrary\n\nqueries over your logs, just like you would with web search.\n\nSetting and maintaining a log index is complicated and error-prone, but\n\nfortunately, at this point all public clouds and many startups offer log\n\ningestion and search as a service. Similar services are also available for\n\ntime series metrics. Whether you are building in the public cloud or your\n\nown infrastructure, using a service for something as critical as your logs is\n\nthe right choice more often than not.\n\nIn addition to storing your logs and metrics in systems designed for efficient\n\nstorage and querying, sometimes it is necessary to reduce the information\n\nbeing stored. This can be referred to as downsampling or aggregation. The\n\nsimplest form of downsampling for metrics is to just store the metric less\n\noften. For example, if you are recording the metric every second, after a\n\nweek you might reduce that to once per minute. Even this simple change\n\nreduces the space necessary by 60 times. Of course, simply dropping the\n\nsample rate can cause you to lose information. Another option would be to\n\nuse the average value over those sixty seconds instead of an arbitrary\n\ninstantaneous value. This simple kind of downsampling can be used for\n\nlogs. The simplest approach is to keep only a certain class of logs, such as\n\nerrors, while dropping less important information. You can also just keep a\n\nfraction of the logs, though this can be very lossy in terms of information.\n\nFinally, with more effort, you can develop application-specific aggregations\n\nthat maintain the information that is relevant to your application while\n\ncompressing the data used to store it.\n\nThough downsampling is often a good solution, if you want to reduce costs\n\nwhile maintaining full data fidelity, perhaps for compliance or security, you\n\ncan instead change the accessibility of the data. Uploading logs to cloud-\n\nbased “cold” storage can significantly reduce the costs of storing the data,\n\nwhile making it slower and harder to query information from those logs.\n\nThis can often be a good solution when data needs to be retained for long\n\nperiods of time, but is infrequently used. Some logging as a service\n\nsolutions will implement this sort of data tiering for you automatically.\n\nSummary\n\nMonitoring is a critical part of any distributed system. Though it doesn’t\n\ncontribute to making the system operate correctly, when the system is not\n\noperating correctly, it is the key component which enables us to understand\n\nwhat is failing and how to correct it. Learning how to correctly monitor and\n\nobserve your systems is critical to ensuring that you can rapidly restore\n\nreliable operation in the face of a problem or outage.\n\nOceanofPDF.com\n\nChapter 15. AI Inference and Serving\n\nIn the last few years, AI has become a key part of many different types of\n\napplications. Though the basics of neural networks and machine learning\n\nhave been around for decades, in the last decade advances in deep learning\n\nand large language models (LLMs) have created a phase shift in the quality\n\nof models and the applications that are possible for AI. More crucially,\n\nthese systems have captured the imagination of application developers all\n\nover the world who see limitless ways to apply LLMs to their particular\n\nbusiness domains.\n\nAI and machine learning is a complex topic that can take years to master,\n\nbut fortunately, with the assistance of libraries and pre-built models, it takes\n\nsignificantly less time to begin to incorporate intelligence into your\n\napplication. This chapter does not attempt to make you an AI expert, but it\n\ncan serve as an introduction to the concepts and approaches for using AI in\n\nyour system.\n\nThe Basics of AI Systems\n\nBefore we get started on the details of using AI in your system, it is useful\n\nto get a grounding in the core concepts that make up AI application. The\n\nplace that most people start is with a model. A model is a collection of\n\nnumeric weights that encode the knowledge in a neural network. In modern",
      "page_number": 293
    },
    {
      "number": 15,
      "title": "AI Inference and Serving",
      "start_page": 315,
      "end_page": 328,
      "detection_method": "regex_chapter_title",
      "content": "LLMs, there are trillions of these weights. As a rough definition, you can\n\nthink of the model as a function that takes a collection of inputs and\n\ntransforms them into some output.\n\nUnlike a traditional function in a programming language, however, a neural\n\nnetwork model is learned via training. Training is the act of taking large\n\namounts of training data and training the model from this training data. The\n\ntraining data contains pairs of both inputs and outputs. When training\n\nhappens, the input is fed into the existing model, and its current output is\n\nobserved. The current output is compared to the expected output in the\n\ntraining data. The error between the expected output and the observed\n\noutput is fed back through the model in a single training iteration. This\n\nprocess is repeated over and over again until the model correctly outputs the\n\nexpected training outputs. This training process is computationally\n\nexpensive and uses dedicated hardware.\n\nBecause of the cost and complexity of training these LLMs, more recently\n\npeople have used fine-tuning to refine foundational models like ChatGPT.\n\nFine-tuning is the process of taking a general-purpose model and refining it\n\nto better match your application. Fine-tuning generally uses smaller data\n\nsets and less computation so it is more efficient from a cost perspective.\n\nOnce you have a model trained, it can be put to use within your application.\n\nWhile training evaluated the model with known inputs and expected\n\noutputs, inference is the process of evaluating the model on user inputs\n\nwhere the output is unknown. Inference is typically implemented as a\n\nRESTful API, which you can call with the user’s input.\n\nMany cloud providers provide inference as a service. Such solutions can\n\nmake it significantly easier to get started incorporating AI into your\n\napplication and often allow access to models that may not be available for\n\nyour own usage. However, some applications have data privacy or security\n\nrequirements that mean they need to host inference on their own servers.\n\nWe’ll discuss how this is done in later sections.\n\nFor LLMs, the final part of inference is the prompt itself. When you chat\n\nwith an LLM, it may seem as if your query text is sent directly to the\n\nmodel, but in most cases it is wrapped as part of a larger text that is called\n\nthe prompt. The prompt provides context and instructions to the model that\n\nare not included in your specific query. For example, a prompt might add\n\nyour location or other personalization information that is known about you\n\nto your query. The structure of the prompt is critical in getting good\n\nresponses from an LLM. The act of creating a good prompt is known as\n\nprompt engineering. In addition to the examples above, prompt engineering\n\ncan be used to add additional data from traditional data sources and to\n\nensure that the responses from the model adhere to your application’s\n\nrequirements for accuracy and stability.\n\n“The Basics of AI Systems” described the components of an AI application.\n\nThe following sections will get into more details about how the application\n\ncan be built.\n\nHosting a Model\n\nBuilding a model is a large, complex task that could fill an entire book (or\n\nmore) and is thus outside of the scope for this chapter. Even if you are\n\ninterested in doing your own training or fine-tuning, you are encouraged to\n\nbegin by using pretrained models so that you can get a sense for how well\n\nAI works with your application. Not every application is a great fit for the\n\ncurrent state of AI models, and thus, before investing the time, energy, and\n\nmoney in training, it makes sense to evaluate your overall approach.\n\nWhen you are looking to host a model, the first thing to consider is the\n\ncomputational requirements of the model and your application. If your\n\napplication is going to be used interactively by a user, the latency of the\n\nresponses is critical to the quality of the application. Generally, users begin\n\nto notice latency at 250 milliseconds and start turning away and doing other\n\ntasks when latency reaches more than a few seconds. Thus, in interactive\n\ninference, performance is critical.\n\nTypically, LLMs require high-performance dedicated GPUs to achieve the\n\nrequired latency. Such GPUs are expensive to purchase for your own\n\nhardware or rent from the cloud; further, given the intense interest in AI, the\n\nchips themselves have been hard to obtain from time to time. LLMs\n\ntypically cannot run on the hardware profile of a mobile device such as a\n\nphone or tablet. If your application is intended for a mobile use, you will\n\nneed to run the inference in the cloud and access it from the mobile device\n\nover the network.\n\nThe challenge of the cost of inference has led to the development so-called\n\n“small language models.” These models, like Microsoft’s Phi family of\n\nmodels, use much more focused training data to achieve reasonable\n\nperformance with many fewer weights (billions versus trillions). Though\n\ntheir scores on AI benchmarks cannot compete with larger models like\n\nChatGPT, the reduced size means that both the cost and latency of inference\n\ncan be dramatically reduced. For many applications, the trade-off is worth\n\nit, and small language models (SLMs) can deliver acceptable quality at a\n\nmuch lower cost. Furthermore, since SLMs can run on a mobile device,\n\nthey also can help address concerns from users who do not want to share\n\ntheir data with a cloud-based model.\n\nDistributing a Model\n\nSo far we’ve talked a lot about models for both training and inference, but\n\nwe’ve had little discussion of how the models themselves are actually\n\nstored to file and loaded for processing. There are several different\n\nframeworks for performing training and inferences (we’ll discuss them in a\n\ncoming section), and each framework has its own file format; for example,\n\nTensorFlow uses the TF2 file format. It’s fine to use a framework-specific\n\nformat if you don’t plan to distribute your model and you don’t expect to\n\never change formats. However, for most users a framework-independent\n\nopen format for models is the best choice.\n\nThe most popular open model format is ONNX (Open Neural Network\n\neXchange), sponsored by Microsoft, Facebook, and Amazon and used by\n\nthousands of open source developers around the world. Models that are\n\nstored in the ONNX format can be used in all of the popular frameworks for\n\ninference and learning and are supported by various model “hubs,” or\n\ngalleries of popular models.\n\nFor many users, especially when you are getting started, using someone\n\nelse’s open source model is the best way to start performing inference,\n\nmuch like cloud native container users start downloading container images\n\nfrom the Docker hub. These model hubs provide easy access to the most\n\npopular open source models. One of the most popular model hubs is called\n\nHugging Face. Hugging Face includes not just the model distribution, but\n\nalso code to make it very easy to download and start using the models\n\nthemselves. Nearly the entire AI ecosystem uses Python as the language of\n\nchoice. Getting started with a model from Hugging Face is as easy as a one-\n\nliner of Python code.\n\nThe models stored in these hubs can be quite large; and thus, while you can\n\ndownload them every time your application starts up, you probably don’t\n\nwant to, as it can add minutes of time to your application startup. Instead,\n\nthe best pattern is to cache the model locally, either within a shared\n\nfilesystem or an in-memory key-value store. Once you cache your model,\n\nall of your instances for inference can load from the same cached model.\n\nWhen you restart your application because of failures or new software\n\nreleases, you don’t have to delay to redownload the same model over and\n\nover again.\n\nThe final consideration with model distribution is safe deployment of new\n\nmodels. Though changes to models typically happen less frequently than\n\nchanges to your code, you will make changes to the model over time. Just\n\nlike safe deployment of your code, it is critical that you perform safe\n\ndeployment of your model. Changes in your model can have significant and\n\nunpredictable impact on the quality and reliability of your application. To\n\nlimit these risks, you should follow the best practice of progressive\n\nexposure and start by using the new model on only a very small subset of\n\nyour users, typically tied to a percentage of users or a specific geography.\n\nAs you gain confidence that the model is working correctly, you can\n\ngradually expand it to more and more users until ultimately all of your\n\napplication is using the new model.\n\nDevelopment with Models\n\nThe complexity of inference means that it requires significant compute\n\ncapacity. Whether you are hosting your own models or using a cloud-based\n\nmodel as a service, this means that every inference is expensive. Unless you\n\nare specifically doing development related to the quality of your inference\n\n(e.g., prompt engineering), it probably doesn’t make sense to pay the costs\n\nof a full model when you are working on the user interface or other parts of\n\nthe service which are not directly related to the AI.\n\nYou should already be using a RESTful web API to perform inference,\n\nwhether with services like Azure machine learning or models you host\n\nyourself. You can use this interface to hide the details of the specific model\n\nbeing used for inference and thus save costs when you don’t need the full\n\nmodel.\n\nIn this approach, you develop a parameter to the API, which indicates\n\nwhich model to use. If you are using a cloud-based API, you may need to\n\ndevelop a wrapper RESTful service that provides this switch. When you\n\ndeploy this compatibility interface, you choose which model to target. In\n\ndevelopment and testing environments, you can create a Kubernetes Service\n\nresource which points to a cheap SLM; whereas in production\n\nenvironments, you can use the full model. There are already several open\n\nsource projects out there which provide generic inference APIs that can\n\nspan a wide variety of different models.\n\nSimilarly, when performing continuous evaluation and automated unit\n\ntesting of your application, it makes sense to deploy smaller, cheaper\n\nmodels, unless you are specifically evaluating quality.\n\nRetrieval-Augmented Generation\n\nUsers interacting with AI systems expect that they are capable of\n\nunderstanding the context of the specific user, as well as the latest\n\ninformation available in the world. Unfortunately, because of the nature of\n\nAI models, the models themselves are fixed at a specific point in time when\n\nthey were trained. Because these models are static and based on generic\n\ninformation, providing responses that are specific to the user or reflect the\n\ncurrent state of the world are challenging. Retrieval-Augmented Generation\n\n(RAG) is a technique developed to handle these challenges.\n\nWith RAG, the user’s prompt is augmented with traditional queries to\n\nprovide the necessary context to correctly deliver an answer to the user’s\n\ninput. Consider, for example, the query:\n\nPrompt: Who is the closest friend to my current location?\n\nIt is clear that a generic LLM has no knowledge of either who the person’s\n\nfriends are, nor their current location. We need RAG to solve this problem.\n\nWhen implementing a system with RAG, in addition to the user’s prompt,\n\nthe system performs two independent queries. The first might be a\n\ntraditional database query across the user’s contact list to determine the\n\ncomplete set of friends. The second query might be to their phone’s location\n\nservices to determine the user’s current location.\n\nThe results of these two traditional queries are combined with the user’s\n\nprompt and then sent to the model. An example RAG prompt might look\n\nsomething like:\n\nPrompt: Given a set of friends with ${address} and a current\n\n${location} , who is the closest friend to the current location?\n\nWe can see how the personalized and up-to-date information provided by\n\nthe RAG retrievals are combined with the user prompt to provide the LLM\n\nwith sufficient information to correctly respond to the user.\n\nRAG is a great technique for experienced developers who may not have lots\n\nof AI experience to develop sophisticated AI applications. By combining\n\ntraditional database queries with natural language prompts from users, very\n\nsuccessful natural language interfaces can be built that feel very personal. A\n\nfurther benefit of the RAG approach is that it respects user privacy. None of\n\nthe user’s personal information is encoded into the LLM, where it might\n\naccidentally be revealed to other users. Instead, the information is only\n\naccessible to the user that has access to the information, and provides it to\n\nthe model in the form of an augmented prompt. Because of its\n\nexplainability and privacy-protecting features, RAG is one of the most\n\npopular techniques for chat-based AI systems today.\n\nTesting and Deployment\n\nAt this point, the idea of testing our services is well understood and\n\naccepted as a required practice if you want to maintain a reliable and well\n\nrunning service. But what does it mean to perform a unit test in the context\n\nof an AI application when making changes to the prompt can have\n\nsignificant impact on the different responses that you get from the model?\n\nThe first important mind-shift that must be made when testing AI\n\napplications is the switch from correctness testing to statistical at-scale\n\nquality testing. When we were taught to write unit tests, we were taught to\n\nevaluate whether the output of the function that we were testing was\n\ncorrect. Indeed, there are whole frameworks dedicated to expressing the\n\nexpectation that Value(A).equals(B) . Such tests do not work for AI\n\napplications.\n\nInstead of looking for exact matches, we are looking for our tests to\n\nevaluate if a response is “good enough.” Once we shift our minds from\n\n“correct” to “good enough,” we realize that evaluating a single input is no\n\nlonger acceptable. As the model changes, the response to any one input may\n\nchange dramatically. Indeed, while a change may make the response to one\n\ntype of input dramatically better, it may make the response to many other\n\ninputs dramatically worse.\n\nThus, to correctly test our AI applications, we have to consider their\n\nperformance across a large selection of prompts and evaluate if, on average,\n\nthe change improves or harms the quality of the responses. If a change has\n\nzero or positive impact on the overall quality of the responses, the change\n\npasses the test. If the quality of the responses is decreased, the test fails.\n\nBut then to actually implement such a test, we need a way to assess the\n\nquality of a response. This task itself can be a challenge. Obviously, it is far\n\ntoo expensive to have humans evaluate the quality of each response—some\n\nautomated solution is required. Fortunately, LLMs themselves can be used\n\nfor some of this evaluation. Just like you used the model to answer the\n\nuser’s input, you can likewise ask the model if the answer it gave is a high-\n\nquality answer. Your prompt looks something like:\n\nPrompt: Given the following input: ${input} is the response:\n\n${response} a high-quality response?\n\nThis approach may seem like giving the inmates control of the asylum, but\n\nit actually works well in many cases.\n\nIn addition to such AI-driven quality testing, before you release a new code\n\nor prompt change, it is critical that you incorporate user feedback signals\n\ninto your safe deployment. While, in other systems, you may look at errors\n\nand latency to determine if a release is safe to continue, when rolling out an\n\nAI application, you must also measure and evaluate if your users continue\n\nto be happy with the responses they are receiving.\n\nAI changes the way we build applications, and thus, also changes the way\n\nthose applications are tested. However, it does not change the criticality of\n\ntesting and evaluating every change you make to ensure it does not impact\n\nthe overall quality of your application.\n\nSummary\n\nIn just a few years, AI has transformed both our imagination of what our\n\napplications could be, as well as our users’ expectations for how they can\n\ninteract with those applications. However, at the end of the day, AI\n\ninference is just another function that is provided by a distributed system\n\nand delivered to our users. This chapter provides an introduction to the core\n\nconcepts and techniques for building AI applications that will enable\n\nanyone familiar with distributed systems to augment their distributed\n\napplications with AI.\n\nOceanofPDF.com\n\nChapter 16. Common Failure Patterns\n\nSo far this book has covered various patterns to help you build your\n\ndistributed system. This chapter is going to be a little different. Instead of\n\nhelping you know what to do, it is intended to help you know what not to\n\ndo. Over numerous years of developing, operating, and debugging systems,\n\ncertain kinds of problems repeat themselves. These patterns are divided into\n\nmistakes that are made in building the systems, as well as common ways in\n\nwhich systems fail. By understanding both what not to do and what to try to\n\nprevent, we can learn from these shared mistakes and prevent them from\n\nrepeating in the future.\n\nThe Thundering Herd\n\nThe thundering herd derives its name from the metaphor of a bison or other\n\nlarge animal on the prairie. Individually they may be manageable, but when\n\nmoving together, charging, they are capable of destroying anything they are\n\ndirected toward. The easiest way to understand the thundering herd is to\n\nimagine yourself interacting with a website that is not behaving properly.\n\nYou attempt to navigate to a particular location, the loading progress bar\n\nspins slowly, not making very much progress, eventually you become\n\nimpatient and you hit the reload button. You may not know it, but you have\n\nbecome the thundering herd.",
      "page_number": 315
    },
    {
      "number": 16,
      "title": "Common Failure Patterns",
      "start_page": 329,
      "end_page": 395,
      "detection_method": "regex_chapter_title",
      "content": "Any particular application has a maximum capacity. Typically we try to size\n\nour applications so that its maximum capacity is greater than any load that it\n\nexperiences, even at its most busy. Unfortunately, sometimes, because of\n\nfailures, lost capacity, or unanticipated demand, the requests sent to a\n\nservice exceed its ability to service those requests. Let’s call the size of this\n\noverload traffic X requests per second. Being overloaded is bad; many, if\n\nnot all, of those X requests per second time out or fail, but what happens\n\nnext is even worse. In the next minute, those same X requests become 2X\n\nrequests. This doubling is the result of any net new traffic (X) as well as all\n\nof those previous requests being retried. The result is even more failures,\n\neven more retries, and even more new requests. What was already a bad\n\nsituation has become irrecoverable. This is especially true when there are\n\nmultiple chained calls and multiple retry algorithms involved. The\n\namplification of one request into many requests can rapidly overwhelm a\n\nsystem.\n\nWhen we build our systems, we build in retry in order to handle when\n\nrequests fail, but if we blindly retry when we see errors, we make the\n\nthundering herd worse and worse. On the client side, the first way to handle\n\nthis is to use exponential backoff. Instead of immediately retrying, the client\n\nwaits a period of time before trying again. Every time the client sees an\n\nerror, it doubles the time it waits.\n\nExponential backoff is good, but it is imperfect; in addition to exponential\n\nbackoff, it is also very useful to add jitter. Jitter is a small amount of\n\nrandomness which spreads load across time. Without jitter, a situation can\n\noccur where everyone fails at the same time, waits the same amount of\n\ntime, and then hammers the service again at the same time. Jitter is\n\nespecially important in situations where programs are talking to other\n\nprograms. Humans are sufficiently random that they add their own jitter, but\n\nunfortunately, machines are not.\n\nThe final and most comprehensive way to solve the thundering herd is to\n\nadd a circuit breaker to your client. The circuit breaker is something that\n\ntrips when error rates exceed some threshold and stops all traffic for a\n\nperiod of time, giving the system time to recover. Finally, to aid in recovery,\n\nsometimes it is also necessary to only gradually let traffic back into the\n\nsystem. Many systems can be stable under high load but must be brought up\n\nto that load level gradually. Dumping 100% of the traffic on a server all at\n\nonce can simply reintroduce the thundering herd.\n\nThe Absence of Errors Is an Error\n\nWe will move from problems of too much traffic to problems caused by too\n\nlittle traffic. When building a system, it is common to build monitoring and\n\nalerts to fire when there are too many errors in the system. Obviously, too\n\nmany errors is a problem that requires a person to look into the system,\n\ndetermine the cause of the errors, and restore the service. But what about\n\ntoo few errors? Though it can seem counter-intuitive, some small number of\n\nerrors is the steady state for most distributed systems. It is generally very\n\nhard and not worth the effort to prevent any errors from ever occurring;\n\nthus, there is a low level of ambient errors that are handled by retry and\n\nother error-correction systems. Consequently, the absence of any errors is\n\nmore likely to indicate a serious problem rather than that everything is\n\nawesome.\n\nTo understand how this can happen, imagine monitoring a system which\n\nadds subtitles to movies that are uploaded to a storage bucket. Whenever a\n\nmovie is uploaded, the system performs speech to text and translation to\n\nadd subtitles and then stores the new movie back into the storage bucket.\n\nImagine what happens when the process that uploads movies accidentally\n\nloses permissions to upload those movies. If you are only monitoring for\n\nconditions where errors are greater than some percentage (say, 10%), then\n\nyou will never alert for this condition. If the uploader cannot upload\n\nmovies, then no movies are being processed, so no errors can occur, so your\n\nerror rate is 0%. But clearly, if your uploader can’t upload movies, your\n\nsystem is broken. Alerting on both too many and too few errors (or too few\n\nrequests generally) will enable you to catch (and fix) such problems in your\n\nsystems.\n\n“Client” and “Expected” Errors\n\nThe next pattern considered is also related to errors in the system, but this\n\ntime related to ways in which we feel OK ignoring errors. Many systems\n\ntake in user input and requests for processing. In such systems, it is possible\n\nfor users (or clients generally) to send requests which are invalid or\n\nunprocessable in some way. When calculating the reliability of a service,\n\nit’s clearly problematic to treat errors caused by bad client requests as true\n\nerrors, since the service isn’t in control of what the clients send. While this\n\nis true in general, in practice such “not my problem” buckets become\n\nconvenient places to ignore true system errors. Imagine, for example,\n\nsomething as simple as an authorization error. Certainly a user not being\n\nauthorized to make a request is a “client” error and doesn’t indicate a\n\nproblem with reliability. But what if suddenly all users are unauthorized?\n\nThat’s probably a problem with our authorization code. But if you treat all\n\nunauthorized errors as “user errors” and you don’t look for anomalies like\n\nall users failing authorization, you probably won’t alert when you break\n\nyour authorization code.\n\nThe same thing can be true of “expected” errors. I mentioned in “The\n\nAbsence of Errors Is an Error” that there’s a certain level of errors that are\n\nexpected in the system, but for synthetic requests that you create, that’s not\n\ntrue. When monitoring a system, it is also important to monitor real client\n\nrequests where some small percentage of errors is expected and also to\n\nwrite synthetic or system-generated requests, which you control entirely.\n\nWith synthetic requests, you control both the client and server side of the\n\ninteractions and, consequently, there should never be any “client” or\n\n“expected” errors for those requests. By combining monitoring of real\n\ntraffic with monitoring of synthetic requests, you can quickly identify\n\nsituations where your code changes are causing errors which would\n\notherwise be ignored as client errors.\n\nVersioning Errors\n\nWhenever you are building software, you are going to need to iterate and\n\nmake changes to that software; but clients require and expect consistency in\n\nhow they interact with your systems. If you need to update every client at\n\nthe exact same time as you update your system, you haven’t actually built a\n\ndecoupled, distributed system. Any system is made up of the external API\n\nthat it exposes to clients and the internal in-memory representation of that\n\nAPI. At first glance, it seems that the best solution is to make these two\n\nrepresentations the same. If they are different, then translation logic needs\n\nto be applied to take the external representation to the internal\n\nrepresentation and vice versa. And of course, when you start out, the\n\nexternal and internal representations are identical.\n\nIn practice, it turns out that separating the internal on-disk or in-memory\n\nrepresentation of your API from the external client-perceived representation\n\nof your API pays immense dividends. For starters, it makes it relatively\n\neasy to have multiple different external client API versions implemented by\n\nthe same internal in-memory version. This means that different clients can\n\ntalk to you using different versions of your APIs, and you don’t need to\n\ntightly synchronize your clients and your application. Additionally, the\n\nseparate internal representation means that it is easier for you to iterate on\n\nyour API implementation; adding, removing, or merging API fields as\n\nnecessary to support your application can be done without the client\n\nnoticing as long as you maintain the translation logic between external and\n\ninternal representations.\n\nThough the benefits of this separation are often seen in supporting multiple\n\nclients, the same can be applied to your internal storage versioning.\n\nUltimately, your application will also need to store the objects in your API\n\nin some data store. This storage also needs to be versioned. If it is tightly\n\ncoupled with a specific version of your code, it can make it very difficult to\n\nroll out changes to your storage layer, or roll back code when there are\n\nbugs.\n\nEven though it may seem overly complicated to begin with, building every\n\napplication with an external (client) version, an internal (in-memory)\n\nversion, and a storage (on-disk or database) version as well as the logic to\n\ntranslate between them pays significant dividends in decoupling and agility\n\nthat will benefit your application in the long run.\n\nThe Myth of Optional Components\n\nDistributed systems are organic systems. We may design them in one way\n\nto start, but over time and in reaction to outages, scale challenges, and\n\nbusiness changes, they morph and adapt to meet new requirements. One of\n\nthe most common changes is the addition of caches in various locations to\n\nimprove performance and reduce load.\n\nTypically, when these caches are added to the system, they are treated as\n\n“optional” components. After all, a cache is just a performance\n\nimprovement. If the system needs to, it can always obtain the data from the\n\nsource of truth. The problem with theoretically optional components in your\n\nsystem is that, over time, they become required parts of your design.\n\nConsider adding a cache to a system. It is an optional performance\n\nimprovement when originally added. However, as the load on the system\n\ngrows over time, its performance becomes more and more dependent on the\n\npresence of the cache until the system can no longer meet its performance\n\ngoals without the cache.\n\nUnfortunately, this increased reliance often goes unnoticed, and the overall\n\nreliability of the caches in the system is not maintained in correspondence\n\nwith its increased criticality to overall system stability.\n\nOne of the most common ways that this manifests is in the difference\n\nbetween a local and a global cache. A local cache, as the name suggests, is a\n\ncache that is maintained locally in the memory of your application, often as\n\na simple hash table. Such caches are very easy to implement. Unfortunately,\n\nas the system grows, the overall size of such local caches has to grow\n\nproportionally, and eventually the overall memory demands of the system\n\nbecome too great to sustain the cache. Furthermore, there is a tendency to\n\nadd many such in-memory caches within a single application because it is\n\nso easy to do. Consequently, the memory usage of such caches is difficult\n\nboth to visualize and manage.\n\nAdditionally, as discussed in Chapter 15 on caching, local caches are very\n\nbad at optimizing memory usage. Consequently, if you ever find yourself\n\nadding a local “optional” cache or any other theoretically “optional”\n\ncomponent to your system, accept that you are actually making a permanent\n\n(and very required) change to your overall distributed system that deserves\n\nthe attention of any other significant change or refactor.\n\nOops, We “Cleaned Up” Everything\n\nIn any real system, it’s an unfortunate truth that errors occur. Bugs and\n\nfailures are bound to happen. Given this, it is often important to consider\n\nhow errors and failures might impact the overall state of the system that we\n\nare building. When doing this, it is important to think about more than just\n\nthe operation of the individual pieces that make up the system, but also to\n\nthink in terms of how all of these pieces interact together to manage the\n\nentire distributed system. Failing to anticipate how these systems might\n\ninteract together in the presence of a failure is a common cause of severe\n\nproblems. One of the most severe is a runaway deletion of a system because\n\nof some sort of unanticipated interaction effect.\n\nDeleting everything in your system seems like such an extreme failure that\n\nit is hard to understand how it might happen by accident via a minor bug.\n\nConsequently, it is useful to examine real-world examples of how such a\n\nthing might happen.\n\nAn example is a photo storage system. Imagine that it has four components:\n\nA database that stores user information\n\nA frontend RESTful API server that implements the user API\n\nA file storage system where all of the photos are stored\n\nA garbage collection system, which cleans up photos for users after their\n\naccounts are deleted\n\nIn normal operation, a user initiates an account deletion through the\n\nwebsite, and the user is removed from the database. Then, for each photo\n\nstored in the filesystem, the garbage collection system looks up the user in\n\nthe database. If the user can’t be found, the photo is deleted. This garbage\n\ncollection system exists to keep the system compliant with data privacy\n\nlaws as well as to ensure photos don’t “leak” and cause infinite growth in\n\nthe filesystem.\n\nNow consider a piece of code written in the frontend RESTful API to look\n\nup a user. It initiates a connection to the database, looks up the user’s\n\ninformation, and then returns it to the caller. But what if the database is\n\nunavailable for some reason? Suppose the person who coded that piece of\n\nlogic chooses to return a 404 indicating that the user couldn’t be found.\n\n“Not Found” is perhaps not the best error to return in this case—500\n\ninternal service error is definitely better; but it is hard to see how this error\n\ncode is important until you think about the garbage collection system. When\n\nthe garbage collection system inquires about any user, they will all appear\n\nto be missing, and the garbage collection system will delete all images.\n\nFortunately, because people anticipated catastrophic problems, the photos\n\nwere restored from backups; but the service was down for many hours\n\nwhile these backups were restored from cold storage. During this period,\n\nuser confidence in something so deeply personal and important as your\n\nfamily photos was shaken, and the service had to work very hard to restore\n\nconfidence.\n\nSo what went wrong? There are secondary basic failure points in this\n\nsystem that caused the catastrophic failure. The first is a human failing.\n\nNamely, it is hard for any human to keep the full context of a distributed\n\nsystem in their head at all times. Especially in a situation where one team\n\nmay have owned the user API system, while a different team, possibly in a\n\ndifferent geographic location, owned the garbage collection system. It is\n\ntempting to focus on this human failure. The developer made a mistake and\n\n“caused” this outage. Unfortunately, human failures are an inherent part of\n\ndistributed systems; and ultimately they cannot be fixed, only prevented.\n\nHumans, regardless of their seniority, will make mistakes. Our systems\n\nmust anticipate and prevent the effects of those mistakes.\n\nThis leads us to the second failing: the process or technical failure in the\n\nsystem, namely, rate limiting and a circuit breaker in the garbage collection\n\nlogic. In the system under normal operation, there is always roughly the\n\nsame number of users deleting their accounts. Let’s say that one percent of\n\nall users delete their accounts in a given day; this will lead to a relatively\n\nconstant rate of image deletes. When this bug occurred, the rate of image\n\ndeletes increased by 100 times. This is clearly unusual, and the system\n\nshould have detected and prevented such a high rate of deletes. The first\n\nsystematic improvement would be the addition of throttling to the deletes.\n\nThe throttling sets an upper bound on deletes per second and would have\n\ndramatically slowed down the rate at which all of the photos could be\n\ndeleted to give humans a better chance to react and stop the process. The\n\nsecond is a “circuit breaker,” much like the circuit breakers in our houses;\n\nwhen this high rate of deletes was hit for an extended period of time, the\n\ngarbage collection system should have stopped itself from performing any\n\ndeletes until a human reset the breaker. Combined, while these technical\n\nchanges couldn’t prevent there being some impact from the human mistake,\n\nthey would dramatically reduce the impact on both the customers of the\n\nservice and the brand of the service itself.\n\nSuch accidentally “we cleaned up everything” scenarios are actually\n\nsurprisingly common, because as developers we tend to focus on the happy\n\npath where everything is working correctly. In that world, the call to the\n\nuser API can never return a wrong answer. Experience in distributed\n\nsystems teaches us that we always need to be thinking about a world where\n\nthings are going right, but also what happens when things go wrong.\n\nChallenges with the Breadth of Inputs\n\nOne of the most disappointing kinds of failures is the failure that slips\n\nthrough all of your testing and goes undetected until a customer notices it.\n\nSuch failures are frustrating to your customers. “Don’t you test your code?”\n\nis a hard question to answer well in such cases. They are equally\n\nembarrassing because often they sit in your codebase for a long time before\n\nsomeone notices and/or complains loud enough to get you to notice.\n\nSometimes these errors are undetected because it takes a very rare\n\nconfluence of events to make them happen, e.g., tricky race conditions that\n\noccur once in a blue moon. Such bugs are actually significantly less likely\n\nto frustrate an end user; they just look like flakiness. We have all been\n\ntrained to “reload and try again,” which generally fixes such bugs.\n\nThe bad ones are the ones which occur for specific users 100% of the time,\n\nbut only for a small number of users. I typically call these problems\n\n“compiler” problems, because they typically occur when there is a very\n\nbroad set of allowed inputs to our system. In such a system, very specific\n\ncombinations of input can cause problems 100% of the time, but only\n\ncertain users present such inputs.\n\nThese are called “compiler” problems because the compiler is one of the\n\nfirst programs where such problems manifest themselves. For any given\n\nprogramming language, the space of possible legal programs is effectively\n\ninfinite. Given enough programmers working on enough programs, sooner\n\nor later, one of them will write a program that is by the book legal, but\n\nexposes a bug in the compiler that causes it to crash. Sometimes these\n\ninputs are the product of random chance and human nature, but more and\n\nmore often these inputs are created by fuzzing programs, special purpose\n\ncode that fire random data at APIs in the hopes of causing crashes to deny\n\nservice or exploit weaknesses in the APIs. An (in)famous version of such\n\nattacks were the SQL injection attacks that were typical of the early web,\n\nwhere bits of random SQL (e.g., '; DROP TABLES' ) were provided as\n\ninputs to places where no developer could reasonably expect SQL\n\nstatements (e.g., a credit card number). Though, by now SQL injection\n\nattacks are largely a thing of the past, accidentally breaking legitimate users\n\nor randomized “fuzzing” attacks are still quite common.\n\nUltimately, this failure represents a failure of coverage in testing. If the\n\nsurface area of a program is completely tested, such failures can’t happen;\n\nbut the space of valid inputs for many APIs is infinite or near infinite. How\n\ncan you ensure coverage of your testing in such a situation? There are two\n\nbasic solutions.\n\nThe first is to record a representative sample of real-world inputs. This\n\nsample can never be exhaustive, but if you record (for example) all user\n\ninputs over the last two months, you can be reasonably sure that you are\n\nunlikely to break a legitimate user. When you are doing this recording,\n\nthough, you need to be careful about both where and when you record it. If\n\nyou have a global service, but only record inputs in North America, you will\n\nlikely miss many bugs in inputs with Asian characters. Similarly, if your\n\ninput varies between night and day, and you only record at a certain time,\n\nyou will miss variation in your inputs due to the time of day. High-quality\n\ncoverage in testing only comes when your test data is representative of all\n\nof your users.\n\nThe second solution is to use fully randomized or “fuzzed” data for your\n\ninputs. These tests can be significantly more comprehensive, but they are\n\nalso much harder to create. For a simple API that perhaps only takes a few\n\nintegers as parameters, it is easy enough to generate random numbers, but\n\nfor more complicated APIs with richer semantics, you may need to write\n\ncustom random input generation logic (for example, something that\n\ngenerates random but valid IP addresses). Lately, innovation in the space of\n\nLLMs and generative AI makes it possible to generate valid but random\n\ninputs for even more classes of data.\n\nThe combination of testing previously seen inputs as well as using fuzzed\n\ndata can help ensure that your new code won’t break existing users and that\n\nit is hardened against possible attacks via random or maliciously crafted\n\ncommand inputs.\n\nProcessing Obsolete Work\n\nWe’ve all been faced with an unresponsive computer system: an email\n\nsystem where searching isn’t working, a retail site which refuses to load the\n\nnext page. This chapter is full of the many different ways in which systems\n\ncan fail to operate correctly. Faced with these failing systems, what do we\n\nall do? We hit the reload button. And thus, an additional way to fail has\n\nbeen created for our services.\n\nIn “The Thundering Herd”, we discussed the thundering herd and how\n\nretries or failovers can rapidly take down an entire system. The problem\n\ndiscussed in this section has somewhat of the opposite flavor, and that is a\n\nsystem struggling to catch back up.\n\nImagine a system that is searching for photos that match a photo of our\n\nfavorite pet. In this system, requests come in, they are processed\n\nasynchronously, and the results are made available for the person making\n\nthe request. But what if, by the time the results come back, the person is no\n\nlonger waiting? What if they have hit reload and resubmitted one, or even\n\ncountless additional requests. With no one waiting for the results, the work\n\ndone to produce the results is wasted, and worse yet, the time taken to\n\nproduce those results has delayed our system responding to actual user\n\nrequests.\n\nTo understand how this happens, imagine that our photo processing system\n\nhas suffered a failure in its object recognition microservice. Processing\n\nrequests grinds to a stop, but the inbound requests keep arriving. A little like\n\na jam in an assembly line, even though no work is being processed, new\n\nwork keeps coming down the conveyor belt. This is obviously problematic;\n\nbut working quickly, we manage to resolve the problem, and work starts\n\nbeing processed again. But what happens now?\n\nDuring the time that the system wasn’t processing any work, a backlog of\n\nwork has accrued at the component which failed. If the system is close to its\n\noptimal load at steady state, as is desirable for maximal efficiency, the\n\nsystem will have a very hard time processing through this backlog. As long\n\nas the backlog exists, new requests are delayed. It’s not uncommon in such\n\nsituations for a short outage, say less than 10 minutes, to cause a multihour\n\nincrease in the latency of request processing, and corresponding\n\ndegradation in customer experience.\n\nSo how can we build our systems to respond better in the face of such\n\nbacklogs and latency increases? There are three approaches which can be\n\nused independently or together to solve the situation. The first is timing out\n\nrequests. Every request that enters the system should have a timeout, the\n\ntime after which the request is considered invalid and failed. When the\n\nrequest times out, any work associated with that request is aborted.\n\nTimeouts can help the system shed work when significantly overloaded.\n\nHowever, it is important to note that timeouts by themselves cannot resolve\n\nthe problem described above. If the latency of each request is high, but\n\nslightly lower than the timeout value, your system will seem very sluggish,\n\nbut it won’t ever shed work to get back to normal.\n\nThe second approach is to use autoscaling based on request latency. We are\n\nused to seeing autoscaling based on CPU or memory usage, but often it\n\nmakes even more sense to autoscale based on user-facing metrics like\n\nprocessing latency. If your system can enable autoscaling, scaling based on\n\nrequest latency can help process the backlog that accumulated when it was\n\ndown. Even simply manually scaling after an outage, while less efficient,\n\ncan be an effective mitigation.\n\nThe final approach is to explicitly prioritize newer requests over older\n\nrequests when your queues grow too long or latency grows too high. This\n\napproach is a form of triage, which sacrifices the experience of some users,\n\nwho in all likelihood already abandoned that request anyway, to ensure that\n\nmore recent requests, likely the same users hitting reload, have a better\n\nexperience.\n\nDesigning reliable distributed systems means not only designing systems\n\nthat prevent failures from occurring, but also systems that recover quickly\n\nwhen failures do occur. Users are far more accepting of short outages when\n\nthe system recovers quickly than prolonged outages where recovery takes a\n\nlong time, even after the problem is “fixed.”\n\nThe “Second System” Problem\n\nSo far we have seen numerous common problems that arise in distributed\n\nsystems. As you have read through this chapter, you may have recognized\n\nseveral different problems that are present in your current system. Seeing\n\nthe possibility of failures, you are eager to improve the system, but how can\n\nyou do it?\n\nSometimes there are only a few changes needed, and it’s obvious how to\n\nmake these minor improvements to the existing system—roll them out and\n\nbe done with it. Unfortunately, more often than not, the problems seem so\n\nlarge and so hard to fix in the existing system that it seems the most\n\nefficient solution is to build a new system, from scratch, that replaces the\n\nexisting system at some date in the future. This approach is attractive for a\n\nnumber of reasons: it’s always easier to build from scratch rather than\n\nrefactor; it is always more exciting to build something new than it is to\n\nrefactor an existing solution; and finally, building the new system alongside\n\nthe old system allows the new system to be built without the pressures of\n\nbeing used in production until it is ready.\n\nThe attraction of all of these perceived benefits makes building the “second\n\nsystem” one of the most common failures in distributed systems; but it is\n\nthis last phrase, “until it is ready,” that turns out to be the curse of the\n\nsecond system. In practice, many second systems never actually become\n\nready. There are many different organizational reasons why this can happen,\n\nbut the most common are that the second system is forever playing catch-up\n\nto the current system and that it never gets the production mileage\n\nnecessary to replace the original system.\n\nThe second system tends to play catch-up precisely because it is not the\n\ncurrent system in production. This means that while it is being built, all of\n\nthe important features, bug fixes, and improvements necessary to run the\n\nbusiness land in the original system. Suppose it takes six months to\n\ncomplete the second system. At the moment when it is “complete,” there\n\nare six more months of features and fixes that it needs to implement until it\n\nmatches the capabilities of the original system. Unless you are willing to\n\nsignificantly over-staff the team building the second system, you are always\n\nplaying catch-up.\n\nThe need for over-staffing the development of the second system leads to\n\nthe other failing, which is the production readiness of the second system. In\n\norder for any system to become production ready, it needs mileage on it. No\n\nmatter how good you think your code is, I guarantee that there are bugs that\n\nwill only be found once it is released into production. But when bugs are\n\nfound in the second system, the tendency is to turn it off—after all, the\n\noriginal system didn’t have those bugs. This dramatically reduces the\n\nexposure and overall mileage that the second system can obtain toward\n\nbeing truly production ready. Just like the feature debt that accumulates as\n\nthe second system is built, the production readiness debt accrues, and it is\n\nvery hard for the second system to catch up.\n\nGiven that there are probably design problems or technical debt in your\n\ndistributed system that you need or want to fix, and the “second system”\n\napproach is often doomed to failure, what can you do? The solution lies in\n\nthe nature of the distributed system itself. Rather than replacing the entire\n\ndistributed system, focusing on improving individual microservices can\n\nmake real progress tractable and significantly reduce the costs of running\n\ntwo systems in production at once. Similarly, it makes sense to invest in\n\nabstractions that separate the core business logic from code that may need\n\nto change as you grow and scale, for example, the storage layer. This\n\nseparation means that you can iterate your core systems like storage\n\nindependently from your business logic and even run two different storage\n\nlayers side by side for evaluation and testing.\n\nMuch like many other aspects of our lives, the urge to throw away the old\n\nand build something new, while tempting, is ultimately far less successful\n\nthan adapting and improving our existing systems to meet the new needs of\n\nour businesses and our users.\n\nSummary\n\nThis chapter is unlike any of the others; instead of discussing how to build,\n\nit discusses how not to build. Hopefully, seeing the common ways that\n\ndistributed systems fail helps you find problems in your existing systems\n\nand also to build more reliable new systems. Learning from the failures of\n\nothers prevents you from making these common mistakes in your systems.\n\nOceanofPDF.com\n\nConclusion: A New Beginning?\n\nEvery company, regardless of its origins, is becoming a digital company.\n\nThis transformation requires the delivery of APIs and services to be\n\nconsumed by mobile applications, devices in the internet of things (IoT), or\n\neven autonomous vehicles and systems. The increasing criticality of these\n\nsystems means that these online systems must be built for redundancy, fault\n\ntolerance, and high availability. At the same time, the requirements of\n\nbusiness necessitate rapid agility to develop and roll out new software,\n\niterate on existing applications, or experiment with new user interfaces and\n\nAPIs. The confluence of these requirements has led to an order of\n\nmagnitude increase in the number of distributed systems that need to be\n\nbuilt.\n\nThe task of building these systems is still far too difficult. The overall cost\n\nof developing, updating, and maintaining such a system is far too high.\n\nLikewise, the set of people with the capabilities and skills to build such\n\napplications is far too small to address the growing need.\n\nHistorically, when these situations presented themselves in software\n\ndevelopment and technology, new abstraction layers and patterns of\n\nsoftware development emerged to make building software faster, easier, and\n\nmore reliable. This first occurred with the development of the first\n\ncompilers and programming languages. Later, the development of object-\n\noriented programming languages and managed code occurred. Likewise, at\n\neach of these moments, these technical developments crystallized the\n\ndistillation of the knowledge and practices of experts into a series of\n\nalgorithms and patterns that could be applied by a much wider group of\n\npractitioners. Technological advancement combined with the establishment\n\nof patterns democratized the process of developing software and expanded\n\nthe set of developers who could build applications on the new platform.\n\nThis in turn led to the development of more applications and application\n\ndiversity, which in turn expanded the market for these developers’ skills.\n\nAgain, we find ourselves at a moment of technological transformation. The\n\nneed for distributed systems far exceeds our ability to deliver them.\n\nFortunately, the development of technology has produced another set of\n\ntools to further expand the pool of developers capable of building these\n\ndistributed systems. The recent development of containers and container\n\norchestration has brought tools that enable rapid, easier development of\n\ndistributed systems. With luck, these tools, when combined with the\n\npatterns and practices described in this book, can enhance and improve the\n\ndistributed systems built by current developers, and more importantly,\n\ndevelop a whole new expanded group of developers capable of building\n\nthese systems.\n\nPatterns like sidecars, ambassadors, sharded services, FaaS, work queues,\n\nand more can form the foundation on which modern distributed systems are\n\nbuilt. Distributed system developers should no longer be building their\n\nsystems from scratch as individuals but rather collaborating together on\n\nreusable, shared implementations of canonical patterns that form the basis\n\nof all of the systems we collectively deploy. This will enable us to meet the\n\ndemands of today’s reliable, scalable APIs and services and empower a new\n\nset of applications and services for the future.\n\nOceanofPDF.com\n\nIndex\n\nA\n\nabsence of errors problem, The Absence of Errors Is an Error\n\nabstraction and encapsulation, microservices, Serving Patterns\n\naccessibility of the data, Aggregating Information\n\nadapter containers, Adapters, Adding a Health Monitor\n\nadapter pattern, Adapters-Summary\n\nversus decorator pattern, The Decorator Pattern: Request or Response\n\nTransformation\n\nhealth monitoring, Adding a Health Monitor-Hands On: Adding Rich\n\nHealth Monitoring for MySQL\n\nlogging, Logging-Hands On: Normalizing Different Logging Formats\n\nwith fluentd\n\nand merger pattern for workflow systems, Merger\n\nmonitoring applications, Monitoring-Hands On: Using Prometheus\n\nfor Monitoring\n\naggregating information, Hands On: Using Prometheus for Monitoring,\n\nMetrics, Metrics, Aggregating Information-Aggregating Information\n\nAI inference and serving, AI Inference and Serving-Summary\n\nAI systems, The Basics of AI Systems-The Basics of AI Systems\n\ndevelopment with models, Development with Models\n\ndistributing a model, Distributing a Model-Distributing a Model\n\nhosting a model, Hosting a Model\n\nRAG, Retrieval-Augmented Generation\n\ntesting and deployment of services, Testing and Deployment-Testing\n\nand Deployment\n\nalerting, monitoring and observability patterns, Monitoring and\n\nObservability Basics, Alerting-Alerting on anomolies\n\nalgorithmic programming, Formalization of Algorithmic Programming\n\nambassador pattern, Ambassadors-Summary\n\nfor experimentation, Using an Ambassador to Do Experimentation or\n\nRequest Splitting-Hands On: Implementing 10% Experiments\n\nmemcache deployment, Hands On: Deploying an Ambassador and\n\nMemcache for a Sharded Cache-Hands On: Deploying an\n\nAmbassador and Memcache for a Sharded Cache\n\nrequest splitting, Using an Ambassador to Do Experimentation or\n\nRequest Splitting-Hands On: Implementing 10% Experiments\n\nservice brokering, Using an Ambassador for Service Brokering\n\nfor sharded cache, Hands On: Deploying an Ambassador and\n\nMemcache for a Sharded Cache-Hands On: Deploying an\n\nAmbassador and Memcache for a Sharded Cache\n\nsharding a service, Using an Ambassador to Shard a Service-Hands\n\nOn: Implementing a Sharded Redis\n\nwork queues, The Source Container Interface-Work Queue API\n\nanomalies, alerting on, Alerting on anomolies\n\nApache Storm, Hands On: Normalizing Different Logging Formats with\n\nfluentd\n\napplication containers\n\nand adapter container, Adapters\n\ndealing with reused, Logging\n\nhealth monitoring, Health Checks, Adding a Health Monitor-Hands\n\nOn: Adding Rich Health Monitoring for MySQL\n\nand sidecar pattern, Single-Node Patterns, Modular Application\n\nContainers-Hands On: Deploying the topz Container\n\napplication programming interfaces (APIs), APIs and RPCs-APIs and\n\nRPCs\n\ndefining container’s API, Define Each Container’s API\n\ndynamic configuration, sidecar pattern, Dynamic Configuration with\n\nSidecars-Dynamic Configuration with Sidecars\n\nfile-based API, The Worker Container Interface\n\nHTTP RESTful API, The Decorator Pattern: Request or Response\n\nTransformation-Hands On: Adding Request Defaulting Prior to\n\nRequest Processing, Work Queue API, The Worker Container\n\nInterface\n\nmetrics API, Prometheus, Hands On: Using Prometheus for\n\nMonitoring\n\nand microservices, Serving Patterns\n\npub/sub API, Publisher/Subscriber Infrastructure\n\nRESTful web API, AI models, Development with Models\n\nB\n\nversioning of, Work Queue API\n\nwork queue systems, Work Queue API-Work Queue API\n\napplication protocol, replicated services, Application-Layer Replicated\n\nServices\n\napplication-layer replicated services, Application-Layer Replicated\n\nServices\n\nThe Art of Computer Programming (Knuth), Formalization of\n\nAlgorithmic Programming\n\nasynchronous versus synchronous APIs, APIs and RPCs\n\nauthentication, FaaS, Hands On: Implementing Two-Factor\n\nAuthentication-Hands On: Implementing Two-Factor Authentication\n\nautoscaler, work queues, Dynamic Scaling of the Workers\n\nautoscaling, obsolete work processing solution, Processing Obsolete\n\nWork\n\navailability, relationship to consistency and partition tolerance, Data\n\nConsistency\n\nbackground processing, FaaS, The Need for Background Processing\n\nbacklog\n\nprocessing obsolete work, Processing Obsolete Work\n\nwork item, Errors, Priority, and Retry\n\nbarrier synchronization, Join (or Barrier Synchronization)\n\nbatch computational patterns\n\nAI inference and serving, AI Inference and Serving-Summary\n\nC\n\ncoordinated batch processing, Coordinated Batch Processing-\n\nSummary\n\nevent-based batch processing, Event-Driven Batch Processing-\n\nSummary\n\nfailure patterns, Common Failure Patterns-Summary\n\nmonitoring and observability patterns, Monitoring and Observability\n\nPatterns-Summary\n\nwork queues, Work Queue Systems-Summary\n\nbreadth of inputs challenge, Challenges with the Breadth of Inputs-\n\nChallenges with the Breadth of Inputs\n\nbrokering a service, ambassador pattern, Using an Ambassador for\n\nService Brokering\n\ncaches and caching\n\nAI models, Distributing a Model\n\nlocal versus global caches, The Myth of Optional Components\n\nmemcache deployment, Hands On: Deploying an Ambassador and\n\nMemcache for a Sharded Cache-Hands On: Deploying an\n\nAmbassador and Memcache for a Sharded Cache\n\nreplicated services, Introducing a Caching Layer-Hands On:\n\nDeploying nginx and SSL Termination\n\nsharded cache, Sharded Caching-Hands On: Deploying an\n\nAmbassador and Memcache for a Sharded Cache, Hot Sharding\n\nSystems\n\ncaching web proxy, Introducing a Caching Layer-Deploying Your Cache\n\nCAP theorem (consistency, availability, partition tolerance), Data\n\nConsistency\n\ncircuit breaker solution, The Thundering Herd, Oops, We “Cleaned Up”\n\nEverything\n\nclient errors problem, “Client” and “Expected” Errors\n\nclient IP, sharding function, Selecting a Key\n\nclient-server architectures, A Brief History of Systems Development\n\nclient-side considerations\n\nambassador pattern, Using an Ambassador to Shard a Service, Hands\n\nOn: Implementing 10% Experiments\n\nand calling APIs, APIs and RPCs\n\nthundering herd problem, The Thundering Herd\n\nCloud Native Computing Foundation (CNCF), Hands On: Deploying\n\netcd\n\ncollection of queues, Publisher/Subscriber Infrastructure\n\ncompare-and-swap operation, The Basics of Leader Election\n\ncompiler problems, Challenges with the Breadth of Inputs\n\nconcurrency, Ownership Election, Implementing Locks\n\nconcurrent data manipulation handling, Handling Concurrent Data\n\nManipulation-Handling Concurrent Data Manipulation\n\nConfigMap, Dynamic Configuration with Sidecars, The Worker\n\nContainer Interface\n\nconfiguration synchronization, sidecar pattern, Dynamic Configuration\n\nwith Sidecars-Dynamic Configuration with Sidecars\n\nconsensus algorithm, The Value of Patterns, Practices, and Components-\n\nShared Components for Easy Reuse, The Basics of Leader Election-\n\nHands On: Deploying etcd\n\nconsistency\n\nof data, Data Consistency-Data Consistency\n\nrelationship to availability and partition tolerance, Data Consistency\n\nconsistent hashing function, Session Tracked Services\n\ncontainer groups, Single-Node Patterns-Single-Node Patterns, The\n\nSidecar Pattern\n\ncontainer images, Shared Components for Easy Reuse, Documenting\n\nYour Containers, Documenting Your Containers, The Worker Container\n\nInterface\n\ncontainer orchestration, Introduction, A Brief History of Systems\n\nDevelopment, Conclusion: A New Beginning?\n\n(see also Kubernetes)\n\ncontainers and containerization, Introduction, A Brief History of Systems\n\nDevelopment, Conclusion: A New Beginning?\n\nadding HTTPS to legacy web service, An Example Sidecar: Adding\n\nHTTPS to a Legacy Service\n\ndefining container’s API, Define Each Container’s API\n\ndocumenting containers, Documenting Your Containers-Documenting\n\nYour Containers\n\nand health checks, Health Checks\n\nimpact on distributed systems, Why I Wrote This Book\n\nleader election, Determining If You Even Need Leader Election-\n\nDetermining If You Even Need Leader Election\n\nmodularity, designing for, Modular Application Containers-Hands\n\nOn: Deploying the topz Container, Ambassadors, Adapters\n\nonline resources, Online Resources\n\nteam ownership goals, Single-Node Patterns\n\nin work queues, A Generic Work Queue System, The Worker\n\nContainer Interface-The Worker Container Interface\n\ncoordinated batch processing, Coordinated Batch Processing-Summary\n\nimage tagging and processing pipeline, Hands On: An Image Tagging\n\nand Processing Pipeline-Hands On: An Image Tagging and Processing\n\nPipeline\n\njoin pattern (barrier synchronization, Join (or Barrier\n\nSynchronization)\n\nreduce pattern, Reduce-Reduce\n\ncopier pattern, work queue, Copier\n\nCoreOS, Hands On: Deploying etcd\n\ncorrelation ID, request tracing, Tracing\n\ncounting, coordinated batch processing, Hands On: Count-Hands On:\n\nCount\n\ncounts, monitoring data, Metrics\n\nD\n\ndata consistency, Data Consistency-Data Consistency\n\ndata replication, Data Consistency\n\ndata synchronization, Relational Integrity\n\ndebugging, and microservices-based systems, Serving Patterns\n\ndecorator pattern, FaaS, The Decorator Pattern: Request or Response\n\nTransformation-Hands On: Adding Request Defaulting Prior to Request\n\nProcessing\n\ndecoupling\n\nevent-based pipelines, Event-Based Pipelines-Hands On:\n\nImplementing a Pipeline for New User Signup\n\nand FaaS, The Benefits of FaaS-The Challenges of FaaS, Handling\n\nEvents\n\nof microservices, Serving Patterns\n\ndeep monitoring, Hands On: Adding Rich Health Monitoring for\n\nMySQL-Hands On: Adding Rich Health Monitoring for MySQL\n\ndelivery semantics, messages, Delivery Semantics\n\ndenial-of-service attacks, Hands On: Deploying nginx and SSL\n\nTermination\n\ndependencies and dependency management, Shared Components for\n\nEasy Reuse\n\nDesign Patterns: Elements of Reusable Object-Oriented Programming\n\n(Gamma et al.), Patterns for Object-Oriented Programming\n\ndeterminism characteristic, sharding function, An Examination of\n\nSharding Functions\n\ndevelopment with AI models, Development with Models\n\ndictionary-server service, Hands On: Creating a Replicated Service in\n\nKubernetes-Hands On: Creating a Replicated Service in Kubernetes,\n\nHands On: Deploying the Caching Layer-Hands On: Deploying the\n\nCaching Layer\n\ndistributed consensus algorithm, The Value of Patterns, Practices, and\n\nComponents-Shared Components for Easy Reuse, The Basics of Leader\n\nElection-Hands On: Deploying etcd\n\ndistributed lock server (etcd) implementations, The Basics of Leader\n\nElection, Hands On: Deploying etcd, Hands On: Implementing Locks in\n\netcd-Hands On: Implementing Locks in etcd, Hands On: Implementing\n\nLeases in etcd\n\ndistributed ownership, Ownership Election\n\n(see also ownership election)\n\ndistributed systems, Introduction-Summary\n\nAPIs, APIs and RPCs-APIs and RPCs\n\nchallenges in meeting demands of, Conclusion: A New Beginning?-\n\nConclusion: A New Beginning?\n\ncontainer orchestration, Introduction, A Brief History of Systems\n\nDevelopment, Orchestration and Kubernetes, Conclusion: A New\n\nBeginning?\n\ncurrent state of, The World of Distributed Systems Today\n\ndata consistency, Data Consistency-Data Consistency\n\ndelivery semantics, Delivery Semantics\n\nhealth checks, Health Checks\n\nhistorical development, A Brief History of Systems Development\n\nidempotency, Idempotency, Errors, Priority, and Retry\n\nlatency, Latency\n\npattern history, A Brief History of Patterns in Software Development-\n\nThe Rise of Open Source Software\n\npercentiles, Percentiles\n\nrelational integrity, Relational Integrity\n\nreliability, A Brief History of Systems Development, Reliability,\n\nRelational Integrity\n\nvalue of patterns, practices, and components, The Value of Patterns,\n\nPractices, and Components-Shared Components for Easy Reuse\n\nDockerfiles, Documenting Your Containers, Hands On: Implementing a\n\nVideo Thumbnailer\n\ndocument search, scatter/gather pattern, Hands On: Distributed\n\nDocument Search-Hands On: Sharded Document Search\n\ndocumenting container, sidecar pattern, Documenting Your Containers-\n\nDocumenting Your Containers\n\ndownsampling, metrics, Aggregating Information\n\ndynamic configuration, sidecar pattern, Dynamic Configuration with\n\nSidecars-Dynamic Configuration with Sidecars\n\nE\n\nembarrassingly parallel problem, scatter/gather pattern, Scatter/Gather\n\nwith Root Distribution\n\nencapsulation and abstraction, microservices, Serving Patterns\n\nENV directive, Documenting Your Containers\n\nenvironment variables, parameterized container, Documenting Your\n\nContainers\n\nerror handling, work queue system, Errors, Priority, and Retry\n\netcd (distributed lock server) implementations, The Basics of Leader\n\nElection, Hands On: Deploying etcd, Hands On: Implementing Locks in\n\netcd-Hands On: Implementing Locks in etcd, Hands On: Implementing\n\nLeases in etcd\n\nevent-based pipelines, Event-Based Pipelines-Hands On: Implementing\n\na Pipeline for New User Signup\n\nevent-driven batch processing (see workflow systems)\n\nevent-driven long-running processing (see function-as-a-service)\n\nevents versus requests, Handling Events\n\neventual data consistency, Data Consistency-Data Consistency\n\n“expected” errors problem, “Client” and “Expected” Errors\n\nexperimentation, ambassador pattern, Using an Ambassador to Do\n\nExperimentation or Request Splitting-Hands On: Implementing 10%\n\nExperiments\n\nexponential backoff, Errors, Priority, and Retry, The Thundering Herd\n\nEXPOSE directive, Documenting Your Containers\n\nF\n\nfailure patterns, Common Failure Patterns-Summary\n\nabsence of errors problem, The Absence of Errors Is an Error\n\nbreadth of inputs challenges, Challenges with the Breadth of Inputs-\n\nChallenges with the Breadth of Inputs\n\nclient errors problem, “Client” and “Expected” Errors\n\n“expected” errors problem, “Client” and “Expected” Errors\n\nobsolete work, processing, Processing Obsolete Work-Processing\n\nObsolete Work\n\noptional components myth, The Myth of Optional Components\n\nrunaway deletion of system error, Oops, We “Cleaned Up”\n\nEverything-Oops, We “Cleaned Up” Everything\n\n“second system” problem, The “Second System” Problem-The\n\n“Second System” Problem\n\nthundering herd, The Thundering Herd-The Thundering Herd\n\nversioning errors, Versioning Errors\n\nffmpeg utility, Hands On: Implementing a Video Thumbnailer\n\nfile-based API, worker container, The Worker Container Interface\n\nfilter pattern, work queue, Filter\n\nfine-tuning process, AI models, The Basics of AI Systems\n\nfluentd logging agent, Hands On: Normalizing Different Logging\n\nFormats with fluentd-Hands On: Normalizing Different Logging\n\nFormats with fluentd\n\nframeworks, AI model, Distributing a Model\n\nG\n\nfunction-as-a-service (FaaS), Functions and Event-Driven Processing-\n\nSummary\n\nbackground processing, need for, The Need for Background\n\nProcessing\n\nbenefits of, The Benefits of FaaS\n\nchallenges of, The Challenges of FaaS\n\ncosts of sustained request-based processing, The Costs of Sustained\n\nRequest-Based Processing\n\ndecorator pattern, The Decorator Pattern: Request or Response\n\nTransformation-Hands On: Adding Request Defaulting Prior to\n\nRequest Processing\n\nevent-based pipelines, Event-Based Pipelines-Hands On:\n\nImplementing a Pipeline for New User Signup\n\nhandling events, Handling Events-Hands On: Implementing Two-\n\nFactor Authentication\n\nholding data in memory, need for, The Need to Hold Data in Memory\n\npatterns for, Patterns for FaaS-Hands On: Implementing a Pipeline for\n\nNew User Signup\n\ntwo-factor authentication, Hands On: Implementing Two-Factor\n\nAuthentication-Hands On: Implementing Two-Factor Authentication\n\n“fuzzed” (randomized) data, testing for breadth of inputs problem,\n\nChallenges with the Breadth of Inputs\n\nGamma, Erich et al., Patterns for Object-Oriented Programming\n\nH\n\ngarbage collection, and runaway deletion problem, Oops, We “Cleaned\n\nUp” Everything-Oops, We “Cleaned Up” Everything\n\nhashing function, Session Tracked Services, An Examination of Sharding\n\nFunctions-Hands On: Building a Consistent HTTP Sharding Proxy\n\nhealth monitoring, Health Checks, Adding a Health Monitor-Hands On:\n\nAdding Rich Health Monitoring for MySQL\n\nHelm package manager, Hands On: Deploying etcd, Hands On:\n\nDeploying Kafka-Hands On: Deploying Kafka\n\nhistograms, Histogram, Metrics, Basic Request Monitoring\n\nhit rate, cache, Deploying Your Cache, The Role of the Cache in System\n\nPerformance\n\nhorizontally scalable systems, Stateless Services\n\nhosting an AI model, Hosting a Model\n\nhot sharding systems, Hot Sharding Systems\n\nHugging Face, Distributing a Model\n\nHypertext Transfer Protocol (HTTP)\n\nadding HTTPS to legacy web service with sidecar, An Example\n\nSidecar: Adding HTTPS to a Legacy Service\n\nrequests\n\ndecorator pattern, The Decorator Pattern: Request or Response\n\nTransformation\n\nand reliability metric, Reliability\n\nI\n\nsharding proxy, Hands On: Building a Consistent HTTP Sharding\n\nProxy\n\nresponse codes, Basic Request Monitoring\n\nRESTful API, The Decorator Pattern: Request or Response\n\nTransformation-Hands On: Adding Request Defaulting Prior to\n\nRequest Processing, Work Queue API, The Worker Container\n\nInterface\n\ntopz interface, Modular Application Containers\n\nidempotency, Idempotency, Errors, Priority, and Retry\n\nimage tagging and processing pipeline, Hands On: An Image Tagging\n\nand Processing Pipeline-Hands On: An Image Tagging and Processing\n\nPipeline\n\nindex\n\nlog, Aggregating Information\n\nscatter/gather pattern, Hands On: Distributed Document Search\n\ninference, AI models, The Basics of AI Systems, Hosting a Model,\n\nDevelopment with Models\n\nintegrity, relational, Relational Integrity\n\ninteractive inference, AI models, Hosting a Model\n\ninterarrival time, work queues, Dynamic Scaling of the Workers\n\nIP-based session tracking, Session Tracked Services\n\nisolation boundary, sharding, Sharded Services\n\nJ\n\nK\n\njitter, using to spread client load, The Thundering Herd\n\nJob objects, The Shared Work Queue Infrastructure, Dynamic Scaling of\n\nthe Workers\n\njoin pattern, Join (or Barrier Synchronization)\n\nKafka deployment, workflow systems, Hands On: Deploying Kafka-\n\nHands On: Deploying Kafka\n\nKEDA (Kubernetes Event-Driven Autoscaling), Dynamic Scaling of the\n\nWorkers\n\nkey, sharding function, Selecting a Key-Selecting a Key\n\nkey-value stores, The Basics of Leader Election, Implementing Locks-\n\nImplementing Locks, Implementing Ownership-Hands On:\n\nImplementing Leases in etcd\n\nKnuth, Donald, Formalization of Algorithmic Programming\n\nKubeless FaaS framework, The Decorator Pattern: Request or Response\n\nTransformation\n\nKubernetes, Orchestration and Kubernetes\n\nConfigMap, Dynamic Configuration with Sidecars, The Worker\n\nContainer Interface\n\ncreating replicated service, Hands On: Creating a Replicated Service\n\nin Kubernetes-Hands On: Creating a Replicated Service in\n\nL\n\nKubernetes\n\netcd deployment with, Hands On: Deploying etcd\n\ngrouping logs across containers, Aggregating Information\n\nKafka deployment as container, Hands On: Deploying Kafka-Hands\n\nOn: Deploying Kafka\n\nKEDA project, Dynamic Scaling of the Workers\n\nKubeless FaaS framework, The Decorator Pattern: Request or\n\nResponse Transformation\n\nmemcache deployment, Hands On: Deploying an Ambassador and\n\nMemcache for a Sharded Cache-Hands On: Deploying an\n\nAmbassador and Memcache for a Sharded Cache\n\npod (container group), Single-Node Patterns, The Sidecar Pattern\n\nRedis service sharded deployment, Hands On: Implementing a\n\nSharded Redis-Hands On: Implementing a Sharded Redis\n\nrunning FaaS on, The Costs of Sustained Request-Based Processing\n\nshared work queue infrastructure, The Shared Work Queue\n\nInfrastructure\n\nshell scripts as health checks with adapter, Adding a Health Monitor\n\nKubernetes Event-Driven Autoscaling (KEDA), Dynamic Scaling of the\n\nWorkers\n\nLABEL directive, Documenting Your Containers\n\nlarge language models (LLMs), The Basics of AI Systems, Hosting a\n\nModel\n\nlatency, Latency\n\nAI models, Hosting a Model\n\nbacklog, processing obsolete work, Processing Obsolete Work\n\ncaching, The Role of the Cache in System Performance\n\njoin pattern for workflow systems, Join (or Barrier Synchronization)\n\nmonitoring server requests, Basic Request Monitoring\n\nand percentiles, Percentiles\n\ntracking in work queues, Dynamic Scaling of the Workers\n\nleader election\n\nconcurrent data manipulation, Handling Concurrent Data\n\nManipulation-Handling Concurrent Data Manipulation\n\nconsensus algorithm tools, The Basics of Leader Election-Hands On:\n\nDeploying etcd\n\ndetermining need for, Determining If You Even Need Leader\n\nElection-Determining If You Even Need Leader Election\n\netcd implementations, The Basics of Leader Election, Hands On:\n\nDeploying etcd, Hands On: Implementing Locks in etcd-Hands On:\n\nImplementing Locks in etcd\n\nlocks, Implementing Locks-Hands On: Implementing Locks in etcd\n\nleaf sharding, scatter/gather pattern, Scatter/Gather with Leaf Sharding-\n\nChoosing the Right Number of Leaves\n\nleases, implementing in etcd, Hands On: Implementing Leases in etcd\n\nlegacy web service, adding HTTPS to, An Example Sidecar: Adding\n\nHTTPS to a Legacy Service\n\nM\n\nleveled logging, Logging\n\nlibrary containers, work queues, A Generic Work Queue System\n\nliveness check, Health Checks\n\nLLMs (large language models), The Basics of AI Systems, Hosting a\n\nModel\n\nload-balanced services (see replicated load-balanced services)\n\nload-balancing node, sharded services, Sharded Services\n\nlocks, distributed, Ownership Election\n\n(see also mutual exclusion locks)\n\nlogging\n\nadapter pattern, Logging-Hands On: Normalizing Different Logging\n\nFormats with fluentd\n\naggregating across processes, Aggregating Information-Aggregating\n\nInformation\n\nmonitoring and observability patterns, Monitoring and Observability\n\nBasics, Logging-Logging\n\nlogging libraries, Logging-Logging\n\nlong-running server applications versus batch processing, Batch\n\nComputational Patterns\n\nMapReduce pattern, Batch Computational Patterns, Coordinated Batch\n\nProcessing, Reduce\n\nmemcache deployment, ambassador pattern, Hands On: Deploying an\n\nAmbassador and Memcache for a Sharded Cache-Hands On: Deploying\n\nan Ambassador and Memcache for a Sharded Cache\n\nmemory\n\ncaching issues with usage, The Myth of Optional Components\n\nneed to holding data in memory (FaaS), The Need to Hold Data in\n\nMemory\n\nmerger pattern, Merger, Join (or Barrier Synchronization)\n\nmessage delivery semantics, Delivery Semantics\n\nmetrics\n\nmonitoring and observability, Monitoring and Observability Basics,\n\nMetrics-Metrics, Alerting on anomolies, Aggregating Information\n\nPrometheus monitoring, Hands On: Using Prometheus for\n\nMonitoring, Basic Request Monitoring\n\nreliability as key in distributed systems, Reliability\n\nmicrocontainers, Define Each Container’s API\n\nmicroservices, Serving Patterns-Serving Patterns\n\ndecoupling of, Serving Patterns\n\nversus event-based pipelines, Event-Based Pipelines\n\nexperimentation as separate service, Hands On: Implementing 10%\n\nExperiments\n\nfocus on improving to catch up with needed changes, The “Second\n\nSystem” Problem\n\ntracing requests through multiple, Tracing\n\nmiss rate, sharding function, Consistent Hashing Functions\n\nmodel, AI, The Basics of AI Systems\n\nmodularity, designing patterns with\n\nadapters, Adapters\n\nambassadors, Ambassadors\n\nsidecars, Modular Application Containers-Documenting Your\n\nContainers\n\nmodulo operator (%), hash function, An Examination of Sharding\n\nFunctions\n\nmonitoring and observability patterns, Monitoring and Observability\n\nPatterns-Summary\n\nadapter pattern, Monitoring-Hands On: Using Prometheus for\n\nMonitoring, Adding a Health Monitor-Hands On: Adding Rich Health\n\nMonitoring for MySQL\n\naggregating information, Aggregating Information-Aggregating\n\nInformation\n\nalerting, Monitoring and Observability Basics, Alerting-Alerting on\n\nanomolies\n\nhealth of application containers, Adding a Health Monitor-Hands On:\n\nAdding Rich Health Monitoring for MySQL\n\nlogging, Monitoring and Observability Basics, Logging-Logging\n\nmetrics, Monitoring and Observability Basics, Metrics-Metrics,\n\nAggregating Information\n\nrequest monitoring, Basic Request Monitoring-Advanced Request\n\nMonitoring\n\ntracing, Tracing\n\nN\n\nmonitoring container, adapter pattern, Monitoring\n\nmonolithic systems versus microservices, Serving Patterns\n\nmultinode serving patterns, Serving Patterns\n\n(see also serving patterns)\n\nmultiworker pattern, The Multiworker Pattern\n\nmutual exclusion locks (mutexes), Ownership Election\n\nconcurrent data manipulation handling, Handling Concurrent Data\n\nManipulation-Handling Concurrent Data Manipulation\n\nimplementing, The Basics of Leader Election, Hands On: Deploying\n\netcd, Hands On: Implementing Leases in etcd\n\nrenewable locks, Implementing Ownership-Hands On: Implementing\n\nLeases in etcd\n\nMySQL database\n\nrich health monitoring, Hands On: Adding Rich Health Monitoring\n\nfor MySQL-Hands On: Adding Rich Health Monitoring for MySQL\n\nservice brokering with ambassador pattern, Using an Ambassador for\n\nService Brokering\n\nnetwork address translation (NAT), Session Tracked Services\n\nNetwork File System (NFS) share, Hands On: Implementing a Video\n\nThumbnailer\n\nneural network models, The Basics of AI Systems\n\nnew-user signup, Hands On: Implementing a Pipeline for New User\n\nSignup-Hands On: Implementing a Pipeline for New User Signup,\n\nO\n\nHands On: Building an Event-Driven Flow for New User Signup-Hands\n\nOn: Building an Event-Driven Flow for New User Signup\n\nnginx application, SSL termination with, SSL Termination, Hands On:\n\nDeploying nginx and SSL Termination-Hands On: Deploying nginx and\n\nSSL Termination\n\nnginx sidecar container, An Example Sidecar: Adding HTTPS to a\n\nLegacy Service\n\nnginx web server, Hands On: Implementing 10% Experiments\n\nNoSQL versus SQL data stores, Relational Integrity\n\nobject-oriented programming, patterns for, Patterns for Object-Oriented\n\nProgramming\n\nobsolete work, processing, Processing Obsolete Work-Processing\n\nObsolete Work\n\nOpen Containers Initiative (OCI), Documenting Your Containers\n\nOpen Neural Network eXchange (ONNX), Distributing a Model\n\nopen source software, The Rise of Open Source Software, Distributing a\n\nModel\n\nOpenTelemetry, Tracing\n\noptional components myth, The Myth of Optional Components\n\noverloading of requests (thundering herd), The Thundering Herd-The\n\nThundering Herd\n\nownership election, Ownership Election-Summary\n\nP\n\nconcurrent data manipulation, Handling Concurrent Data\n\nManipulation-Handling Concurrent Data Manipulation\n\netcd implementations, Hands On: Implementing Leases in etcd\n\nleader election (see leader election)\n\nrenewable locks, Implementing Ownership-Hands On: Implementing\n\nLeases in etcd\n\nPaaS (platform-as-a-service), sidecar pattern, Building a Simple PaaS\n\nwith Sidecars-Building a Simple PaaS with Sidecars\n\nparallel processing\n\nlogging duplications, Logging\n\nscatter/gather pattern, Scatter/Gather, Choosing the Right Number of\n\nLeaves\n\nwork queue dynamic scaling, Dynamic Scaling of the Workers\n\nparameterized sidecar containers, Parameterized Containers\n\npartition tolerance, relationship to consistency and availability, Data\n\nConsistency\n\npatterns, The Rise of Open Source Software\n\nbatch computational patterns (see batch computational patterns)\n\nhistory in software development, A Brief History of Patterns in\n\nSoftware Development-The Rise of Open Source Software\n\nand microservices, Serving Patterns\n\nmonitoring and observability, Monitoring and Observability Patterns-\n\nSummary\n\nserving patterns (see serving patterns)\n\nshared components for reuse, Shared Components for Easy Reuse\n\nas shared language, A Shared Language for Discussing Our Practice\n\nsingle-node patterns (see single-node patterns)\n\nvalue of, The Value of Patterns, Practices, and Components-Shared\n\nComponents for Easy Reuse\n\npercentiles, Percentiles\n\npipelines\n\nevent-based, Event-Based Pipelines-Hands On: Implementing a\n\nPipeline for New User Signup\n\nimage tagging and processing, Hands On: An Image Tagging and\n\nProcessing Pipeline-Hands On: An Image Tagging and Processing\n\nPipeline\n\nplatform-as-a-service (PaaS), sidecar pattern, Building a Simple PaaS\n\nwith Sidecars-Building a Simple PaaS with Sidecars\n\npod (container group), Single-Node Patterns, The Sidecar Pattern\n\npoison requests, isolation boundary to protect from, Sharded Services\n\npoison work solution, Errors, Priority, and Retry\n\npretrained models, AI, Hosting a Model\n\nprioritizing newer requests over older, Processing Obsolete Work\n\nprocess-ID (PID) namespace, sidecar container, Modular Application\n\nContainers\n\nPrometheus monitoring aggregator, Hands On: Using Prometheus for\n\nMonitoring, Metrics, Metrics, Basic Request Monitoring\n\nR\n\nprompts, AI, The Basics of AI Systems\n\npublisher/subscriber API, Publisher/Subscriber Infrastructure\n\npublisher/subscriber infrastructure, Publisher/Subscriber Infrastructure-\n\nHands On: Deploying Kafka\n\nPython, as language for AI models, Distributing a Model\n\nRAG (Retrieval-Augmented Generation), Retrieval-Augmented\n\nGeneration\n\nrate limiting, Rate Limiting and Denial-of-Service Defense\n\nrate of requests, monitoring, Basic Request Monitoring\n\nreadiness probes for load balancing, Readiness Probes for Load\n\nBalancing\n\nRedis, Hands On: Implementing a Sharded Redis, Hands On:\n\nImplementing a Sharded Redis-Hands On: Implementing a Sharded\n\nRedis, Hands On: Using Prometheus for Monitoring-Hands On: Using\n\nPrometheus for Monitoring, Hands On: Normalizing Different Logging\n\nFormats with fluentd\n\nreduce patterns, Batch Computational Patterns, Coordinated Batch\n\nProcessing, Reduce-Reduce, Sum\n\nrelational integrity, Relational Integrity\n\nreliability\n\nof distributed systems, A Brief History of Systems Development,\n\nReliability, Relational Integrity\n\nmicroservices, Serving Patterns\n\nmonitoring and observability as important for, Monitoring and\n\nObservability Patterns\n\nscaling of scatter/gather pattern for, Scaling Scatter/Gather for\n\nReliability and Scale\n\nremote procedure calls (RPCs), APIs and RPCs\n\nrenewable locks, ownership election, Implementing Ownership-Hands\n\nOn: Implementing Leases in etcd\n\nreplicated load-balanced services, Replicated Load-Balanced Services-\n\nHands On: Deploying nginx and SSL Termination\n\napplication protocol, Application-Layer Replicated Services\n\napplication-layer replicated services, Application-Layer Replicated\n\nServices\n\ncaching layer deployment and expansion, Introducing a Caching\n\nLayer-Hands On: Deploying nginx and SSL Termination\n\ncreating, Hands On: Creating a Replicated Service in Kubernetes-\n\nHands On: Creating a Replicated Service in Kubernetes\n\nsession tracked services, Session Tracked Services-Session Tracked\n\nServices\n\nversus sharded services, Sharded Services\n\nstateless services, Stateless Services-Hands On: Creating a Replicated\n\nService in Kubernetes\n\nreplicated sharded cache, Replicated Sharded Caches\n\nrequest-based processing, FaaS, The Costs of Sustained Request-Based\n\nProcessing, Hands On: Adding Request Defaulting Prior to Request\n\nProcessing\n\nrequests\n\ndecorator pattern, The Decorator Pattern: Request or Response\n\nTransformation\n\nversus events, Handling Events\n\nmonitoring, Basic Request Monitoring-Tracing\n\nand reliability metric, Reliability\n\nsharding proxy, Hands On: Building a Consistent HTTP Sharding\n\nProxy\n\nsplitting, Using an Ambassador to Do Experimentation or Request\n\nSplitting-Hands On: Implementing 10% Experiments\n\nresource isolation, Single-Node Patterns\n\nresource version, distributed locks, Implementing Locks\n\nRESTful web API, AI models, Development with Models\n\nRetrieval-Augmented Generation (RAG), Retrieval-Augmented\n\nGeneration\n\nretry queue to handle backlogs, Errors, Priority, and Retry\n\nreusability, Introduction, Shared Components for Easy Reuse, Single-\n\nNode Patterns\n\n(see also single-node patterns)\n\nmultiworker pattern, The Multiworker Pattern\n\nsidecar pattern, Modular Application Containers-Hands On:\n\nDeploying the topz Container, Designing Sidecars for Modularity and\n\nReusability-Documenting Your Containers\n\nS\n\nroot (load-balancing node), sharded services, Sharded Services\n\nroot distribution, scatter/gather pattern, Scatter/Gather with Root\n\nDistribution-Hands On: Distributed Document Search\n\nRPCs (remote procedure calls), APIs and RPCs\n\nrunaway deletion of system error, Oops, We “Cleaned Up” Everything-\n\nOops, We “Cleaned Up” Everything\n\nscaling and scalability\n\nautoscaling for obsolete work processing solution, Processing\n\nObsolete Work\n\nand consistent hashing function, Session Tracked Services\n\nin decoupling of microservices, Serving Patterns\n\ndynamic scaling of workers, Dynamic Scaling of the Workers-\n\nDynamic Scaling of the Workers\n\nwith FaaS, The Costs of Sustained Request-Based Processing\n\nhorizontally scalable systems, Stateless Services\n\nscatter/gather pattern, Scatter/Gather-Summary\n\nsharded services, Sharded Services-Summary\n\nstateless replicated services, Stateless Services-Hands On: Creating a\n\nReplicated Service in Kubernetes\n\nteam scaling, Single-Node Patterns\n\nscatter/gather pattern, Scatter/Gather-Summary\n\ndistributed document search, Hands On: Distributed Document\n\nSearch-Hands On: Distributed Document Search\n\nleaf sharding, Scatter/Gather with Leaf Sharding-Choosing the Right\n\nNumber of Leaves\n\nwith root distribution, Scatter/Gather with Root Distribution-Hands\n\nOn: Distributed Document Search\n\nscaling for reliability, Scaling Scatter/Gather for Reliability and Scale\n\n“second system” problem, The “Second System” Problem-The “Second\n\nSystem” Problem\n\nseparation of concerns, Single-Node Patterns, Ambassadors, Using an\n\nAmbassador to Shard a Service\n\nserverless computing, versus FaaS, Functions and Event-Driven\n\nProcessing\n\nservice broker, Using an Ambassador for Service Brokering\n\nservice brokering, ambassador pattern, Using an Ambassador for Service\n\nBrokering\n\nservice discovery, Using an Ambassador for Service Brokering\n\nservice level agreement (SLA), Stateless Services\n\nservice level objectives (SLOs), API, APIs and RPCs, Basic alerting\n\nserving patterns, Serving Patterns-Serving Patterns\n\nFaaS, Functions and Event-Driven Processing-Summary\n\nownership election, Ownership Election-Summary\n\nreplicated load-balanced services, Replicated Load-Balanced\n\nServices-Hands On: Deploying nginx and SSL Termination\n\nscatter/gather pattern, Scatter/Gather-Summary\n\nsharded services, Sharded Services-Summary\n\nsession tracked services, Session Tracked Services-Session Tracked\n\nServices, Deploying Your Cache\n\nsessions, and events versus requests, Handling Events\n\nshard key, Selecting a Key-Selecting a Key\n\nshard pattern, workflow systems, Hands On: Building an Event-Driven\n\nFlow for New User Signup\n\nshard router service, Hands On: Deploying an Ambassador and\n\nMemcache for a Sharded Cache-Hands On: Deploying an Ambassador\n\nand Memcache for a Sharded Cache\n\nsharded cache, Sharded Caching-Hands On: Deploying an Ambassador\n\nand Memcache for a Sharded Cache, Hot Sharding Systems\n\nsharded services, Sharded Services-Summary\n\nambassador pattern, Using an Ambassador to Shard a Service-Hands\n\nOn: Implementing a Sharded Redis\n\ncaching layer, Sharded Caching-Hands On: Deploying an\n\nAmbassador and Memcache for a Sharded Cache, Hot Sharding\n\nSystems\n\ndocument search, Hands On: Sharded Document Search\n\nhashing functions, An Examination of Sharding Functions-Hands On:\n\nBuilding a Consistent HTTP Sharding Proxy\n\nleaf sharding, scatter/gather pattern, Scatter/Gather with Leaf\n\nSharding-Choosing the Right Number of Leaves\n\nversus replicated services, Sharded Services\n\nreplicated serving, Sharded Replicated Serving\n\nsharder pattern, workflow systems, Sharder-Sharder\n\nsharding ambassador proxy, Using an Ambassador to Shard a Service\n\nsharding functions, Sharder-Sharder\n\nsharding proxy, HTTP requests, Hands On: Building a Consistent HTTP\n\nSharding Proxy\n\nshared infrastructure, work queues, The Shared Work Queue\n\nInfrastructure\n\nsidecar pattern, A Shared Language for Discussing Our Practice, The\n\nSidecar Pattern-Summary\n\nadding HTTPS to a legacy service, An Example Sidecar: Adding\n\nHTTPS to a Legacy Service\n\ncache deployment with, Deploying Your Cache\n\ndesigning for modularity and reusability, Modular Application\n\nContainers-Documenting Your Containers\n\ndocumenting container, Documenting Your Containers-Documenting\n\nYour Containers\n\ndynamic configuration with, Dynamic Configuration with Sidecars-\n\nDynamic Configuration with Sidecars\n\nPaaS, building with sidecar, Building a Simple PaaS with Sidecars-\n\nBuilding a Simple PaaS with Sidecars\n\nsingle-node patterns, Single-Node Patterns-Single-Node Patterns\n\nadapters, Adapters-Summary\n\nambassadors, Ambassadors-Summary\n\nsidecars, The Sidecar Pattern-Summary\n\nsingleton pattern, ownership election, Determining If You Even Need\n\nLeader Election-Determining If You Even Need Leader Election\n\nSLA (service level agreement), Stateless Services\n\nSLOs (service level objectives), API, APIs and RPCs, Basic alerting\n\nslow-query logging, adding to Redis, Hands On: Normalizing Different\n\nLogging Formats with fluentd\n\nsmall language models (SLMs), Hosting a Model\n\nsource container interface, work queues, The Source Container Interface\n\nsplitter pattern, work queue, Splitter\n\nSQL injection attacks, Challenges with the Breadth of Inputs\n\nSQL versus NoSQL data stores, Relational Integrity\n\nSSL termination, caching layer for, SSL Termination-Hands On:\n\nDeploying nginx and SSL Termination\n\nstateful services (see sharded services)\n\nstateless services, Using an Ambassador to Shard a Service, Stateless\n\nServices-Hands On: Creating a Replicated Service in Kubernetes,\n\nSharded Services\n\n(see also replicated load-balanced services)\n\nstatic alerting, Alerting on anomolies\n\nstdout, adapter container’s redirect of logs to, Logging\n\n“straggler” problem, scatter/gather pattern, Choosing the Right Number\n\nof Leaves\n\nstrong data consistency, Data Consistency-Data Consistency\n\nsums, coordinated batch processing, Sum\n\nT\n\nsynchronous versus asynchronous APIs, APIs and RPCs\n\nsynthetic request to test system, “Client” and “Expected” Errors\n\nsystem-generated request to test system, “Client” and “Expected” Errors\n\nsystems development, history of, A Brief History of Systems\n\nDevelopment\n\nteams\n\nownership and containerization goals, Single-Node Patterns\n\nreliable contract for different services in microservices, Serving\n\nPatterns\n\nscaling of, Single-Node Patterns\n\ntee-d service, Using an Ambassador to Do Experimentation or Request\n\nSplitting\n\nTensorFlow, Distributing a Model\n\ntesting and deployment of AI services, Testing and Deployment-Testing\n\nand Deployment\n\ntesting for breadth of inputs problem, Challenges with the Breadth of\n\nInputs-Challenges with the Breadth of Inputs\n\nthree-nines service, Stateless Services\n\nthrottle module (Varnish), Rate Limiting and Denial-of-Service Defense\n\nthrottling, to prevent runaway deletion, Oops, We “Cleaned Up”\n\nEverything\n\nthundering herd, failure pattern, The Thundering Herd-The Thundering\n\nHerd\n\nU\n\ntime series data monitoring, Metrics\n\ntime-to-live (TTL) functionality of key-value store, Implementing\n\nLocks-Implementing Locks, Implementing Ownership-Hands On:\n\nImplementing Leases in etcd\n\ntiming out requests, obsolete work processing solution, Processing\n\nObsolete Work\n\ntopz sidecar container, Modular Application Containers-Hands On:\n\nDeploying the topz Container\n\ntracing, monitoring and observability patterns, Monitoring and\n\nObservability Basics, Tracing\n\ntraffic overload, failure pattern, The Thundering Herd-The Thundering\n\nHerd\n\ntraffic reduction problem, The Absence of Errors Is an Error\n\ntraining data, The Basics of AI Systems\n\nTTL (time-to-live) functionality of key-value store, Implementing\n\nLocks-Implementing Locks, Implementing Ownership-Hands On:\n\nImplementing Leases in etcd\n\ntwemproxy, Ambassadors\n\ntwo-factor authentication, Hands On: Implementing Two-Factor\n\nAuthentication-Hands On: Implementing Two-Factor Authentication\n\nuniformity characteristic, sharding function, An Examination of\n\nSharding Functions\n\nUnix signals\n\nV\n\nW\n\nSIGHUP, Dynamic Configuration with Sidecars\n\nSIGKILL, Dynamic Configuration with Sidecars\n\nuser feedback, AI testing, Testing and Deployment\n\nvalues, monitoring data, Metrics\n\nVarnish (HTTP reverse proxy), Expanding the Caching Layer-SSL\n\nTermination\n\nversioning errors, Versioning Errors\n\nversioning of APIs, Work Queue API\n\nvideo thumbnailer implementation, Hands On: Implementing a Video\n\nThumbnailer-Hands On: Implementing a Video Thumbnailer\n\nwork items, The Source Container Interface, Work Stealing\n\nwork queue systems, Work Queue Systems-Summary\n\nAPI (ambassador), Work Queue API-Work Queue API\n\nbuilding event-driven flow, Hands On: Building an Event-Driven\n\nFlow for New User Signup-Hands On: Building an Event-Driven\n\nFlow for New User Signup\n\ndynamic scaling of workers, Dynamic Scaling of the Workers-\n\nDynamic Scaling of the Workers\n\nmultiworker pattern, The Multiworker Pattern\n\nshared infrastructure, The Shared Work Queue Infrastructure\n\nsource container interface, The Source Container Interface\n\nvideo thumbnailer implementation, Hands On: Implementing a Video\n\nThumbnailer-Hands On: Implementing a Video Thumbnailer\n\nand work stealing technique, Work Stealing\n\nworker container interface, The Worker Container Interface-The\n\nWorker Container Interface\n\nwork stealing, work queues, Work Stealing\n\nworker container interface, The Worker Container Interface-The Worker\n\nContainer Interface\n\nworkflow systems, Event-Driven Batch Processing-Summary\n\nbuilding event-driven flow, Hands On: Building an Event-Driven\n\nFlow for New User Signup-Hands On: Building an Event-Driven\n\nFlow for New User Signup\n\ncopier pattern, Copier\n\nfilter pattern, Filter\n\nmerger pattern, Merger\n\npublisher/subscriber infrastructure, Publisher/Subscriber\n\nInfrastructure-Hands On: Deploying Kafka\n\nsharder pattern, Sharder-Sharder\n\nsplitter pattern, Splitter\n\nwork queue resiliency and performance, Resiliency and Performance\n\nin Work Queues-Errors, Priority, and Retry\n\nOceanofPDF.com\n\nAbout the Author\n\nBrendan Burns is corporate vice president at Microsoft responsible for\n\nAzure management and governance, Azure Arc, Kubernetes on Azure,\n\nLinux on Azure, and PowerShell. Prior to Microsoft, he worked at Google\n\nin the Google Cloud Platform, where he cofounded the Kubernetes project\n\nand helped build APIs like Deployment Manager and Cloud DNS. Before\n\nworking on cloud, he worked on Google’s web-search infrastructure with a\n\nfocus on low-latency indexing. He has a PhD in computer science from the\n\nUniversity of Massachusetts Amherst with a specialty in robotics. He lives\n\nin Seattle with his wife, Robin Sanders, their two children, and a cat, Mrs.\n\nPaws, who rules over their household with an iron paw.\n\nOceanofPDF.com\n\nColophon\n\nThe animal on the cover of Designing Distributed Systems is a Java sparrow\n\n(Padda oryzivora). This bird is loathed in the wild but loved in captivity.\n\nFarmers destroy thousands of wild Javas each year to prevent the flocks\n\nfrom devouring their crops. They also trap the birds for food or sell them in\n\nthe international bird trade. Despite this battle, the species continues to\n\nthrive in Java and Bali in Indonesia, as well as Australia, Mexico, and North\n\nAmerica.\n\nIts plumage is pearly-gray, turning pinkish on the front and white toward\n\nthe tail. It has a black head with white cheeks. Its large bill, legs, and eye\n\ncircles are bright pink. The song of the Java sparrow begins with single\n\nnotes, like a bell, before developing into a continuous trilling and clucking,\n\nmixed with high-pitched and deeper notes.\n\nThe main part of their diet is rice, but they also eat small seeds, grasses,\n\ninsects, and flowering plants. In the wild, these birds will build a nest out of\n\ndried grass, normally under the roofs of buildings or in bushes or treetops.\n\nThe Java will lay a clutch of three or four eggs between February and\n\nAugust, with most eggs laid in April or May.\n\nThe Java sparrow is listed as endangered on the International Union for\n\nConservation of Nature (IUCN) Red List of Threatened Species due to the\n\ncontinued loss of its natural habitat, hunting, and trapping. Many of the\n\nanimals on O’Reilly covers are endangered; all of them are important to the\n\nworld.\n\nThe cover illustration is by Karen Montgomery, based on a black-and-white\n\nengraving from Lydekker’s Royal Natural History. The series design is by\n\nEdie Freedman, Ellie Volckhausen, and Karen Montgomery. The cover\n\nfonts are Gilroy Semibold and Guardian Sans. The text font is Adobe\n\nMinion Pro; the heading font is Adobe Myriad Condensed; and the code\n\nfont is Dalton Maag’s Ubuntu Mono.\n\nOceanofPDF.com",
      "page_number": 329
    }
  ],
  "pages": [
    {
      "page_number": 2,
      "content": "Praise for Designing Distributed Systems\n\nAn essential guide for anyone working with scalable and reliable\n\nsystems, especially in the context of Kubernetes. Burns brings clarity\n\nto complex distributed systems concepts and provides practical design\n\npatterns, making this book invaluable for engineers looking to build\n\nrobust systems in modern, cloud native environments.\n\n—Rajeev Reddy Vishaka, software engineering leader,\n\nCoinbase\n\nDesigning Distributed Systems by Brendan Burns offers an in-depth\n\nexploration of key distributed system concepts, from stateless and\n\nsharded services to event-driven processing and observability. A\n\nmust-read for SREs and engineers looking to harness the full power\n\nof Kubernetes to build resilient, high-performance infrastructures.\n\n—Swapnil Shevate, site reliability engineering\n\nprofessional and advocate",
      "content_length": 849,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 3,
      "content": "A brilliant resource that simplifies the complexity of distributed\n\nsystems. Brendan Burns offers practical patterns and design\n\nparadigms that are indispensable for building modern cloud native\n\napplications.\n\n—Lalithkumar Prakashchand, software engineer, Meta\n\nPlatforms\n\nDesigning Distributed Systems, 2nd ed., remains an excellent book for\n\nintroducing developers to architectural concepts that add both\n\nresilience and greater efficiency to new and legacy systems. The book\n\ndescribes a set of simple patterns that work particularly well with\n\nKubernetes and are a great place to start building better systems in\n\nthe field.\n\n—Anne Currie, CEO, Strategically Green Learning and\n\nDevelopment, and author, Building Green Software\n\nOceanofPDF.com",
      "content_length": 748,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 4,
      "content": "Designing Distributed Systems\n\nSECOND EDITION\n\nPatterns and Paradigms for Scalable, Reliable Systems Using Kubernetes\n\nBrendan Burns\n\nOceanofPDF.com",
      "content_length": 148,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 5,
      "content": "Designing Distributed Systems\n\nby Brendan Burns\n\nCopyright © 2025 Brendan Burns. All rights reserved.\n\nPrinted in the United States of America.\n\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North,\n\nSebastopol, CA 95472.\n\nO’Reilly books may be purchased for educational, business, or sales\n\npromotional use. Online editions are also available for most titles\n\n(http://oreilly.com). For more information, contact our\n\ncorporate/institutional sales department: 800-998-9938 or\n\ncorporate@oreilly.com.\n\nAcquisitions Editor: Louise Corrigan\n\nDevelopment Editor: Jill Leonard\n\nProduction Editor: Elizabeth Faerm\n\nCopyeditor: Dwight Ramsey\n\nProofreader: Emily Wydeven\n\nIndexer: Potomac Indexing, LLC",
      "content_length": 707,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 6,
      "content": "Interior Designer: David Futato\n\nCover Designer: Karen Montgomery\n\nIllustrator: Kate Dullea\n\nDecember 2024: Second Edition\n\nRevision History for the Second Edition\n\n2024-12-04: First Release\n\nSee http://oreilly.com/catalog/errata.csp?isbn=9781098156350 for release\n\ndetails.\n\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc.\n\nDesigning Distributed Systems, the cover image, and related trade dress are\n\ntrademarks of O’Reilly Media, Inc.\n\nThe views expressed in this work are those of the author and do not\n\nrepresent the publisher’s views. While the publisher and the author have\n\nused good faith efforts to ensure that the information and instructions\n\ncontained in this work are accurate, the publisher and the author disclaim all\n\nresponsibility for errors or omissions, including without limitation\n\nresponsibility for damages resulting from the use of or reliance on this\n\nwork. Use of the information and instructions contained in this work is at",
      "content_length": 972,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 7,
      "content": "your own risk. If any code samples or other technology this work contains\n\nor describes is subject to open source licenses or the intellectual property\n\nrights of others, it is your responsibility to ensure that your use thereof\n\ncomplies with such licenses and/or rights.\n\n978-1-098-15635-0\n\n[LSI]\n\nOceanofPDF.com",
      "content_length": 314,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 8,
      "content": "Preface\n\nWho Should Read This Book\n\nAt this point, nearly every developer is a developer or consumer (or both)\n\nof distributed systems. Even relatively simple mobile applications are\n\nbacked with cloud APIs so that their data can be present on whatever device\n\nthe customer happens to be using. Whether you are new to developing\n\ndistributed systems or an expert with scars on your hands to prove it, the\n\npatterns and components described in this book can transform your\n\ndevelopment of distributed systems from art to science. Reusable\n\ncomponents and patterns for distributed systems will enable you to focus on\n\nthe core details of your application. This book will help any developer\n\nbecome better, faster, and more efficient at building distributed systems.\n\nWhy I Wrote This Book\n\nThroughout my career as a developer of a variety of software systems, from\n\nweb search to the cloud, I have built a large number of scalable, reliable\n\ndistributed systems. Each of these systems was, by and large, built from\n\nscratch. In general, this is true of all distributed applications. Despite having\n\nmany of the same concepts and even at times nearly identical logic, the\n\nability to apply patterns or reuse components is often very, very",
      "content_length": 1235,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 9,
      "content": "challenging. This forced me to waste time reimplementing systems, and\n\neach system ended up less polished than it might have otherwise been.\n\nThe recent introduction of containers and container orchestrators\n\nfundamentally changed the landscape of distributed system development.\n\nSuddenly we have an object and interface for expressing core distributed\n\nsystem patterns and building reusable containerized components. I wrote\n\nthis book to bring together all of the practitioners of distributed systems,\n\ngiving us a shared language and common standard library so that we can all\n\nbuild better systems more quickly.\n\nThe World of Distributed Systems Today\n\nOnce upon a time, people wrote programs that ran on one machine and\n\nwere also accessed from that machine. The world has changed. Now, nearly\n\nevery application is a distributed system running on multiple machines and\n\naccessed by multiple users from all over the world. Despite their\n\nprevalence, the design and development of these systems is often a black art\n\npracticed by a select group of wizards. But as with everything in\n\ntechnology, the world of distributed systems is advancing, regularizing, and\n\nabstracting. In this book I capture a collection of repeatable, generic\n\npatterns that can make the development of reliable distributed systems more\n\napproachable and efficient. The adoption of patterns and reusable\n\ncomponents frees developers from reimplementing the same systems over",
      "content_length": 1453,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 10,
      "content": "and over again. This time is then freed to focus on building the core\n\napplication itself.\n\nNavigating This Book\n\nThis book is organized into five parts as follows:\n\nPart I, “Foundational Concepts”\n\nChapters 1 and 2 introduce distributed systems as well as some\n\nfundamental concepts which are essential to understanding the\n\ndistributed system designs described in Part II, “Single-Node\n\nPatterns”.\n\nPart II, “Single-Node Patterns”\n\nChapters 3 through 5 discuss reusable patterns and components that\n\noccur on individual nodes within a distributed system. They cover\n\nthe sidecar, adapter, and ambassador single-node patterns.\n\nPart III, “Serving Patterns”\n\nChapters 6 through 8 cover multinode distributed patterns for long-\n\nrunning serving systems like web applications. Patterns for many\n\ndifferent types of serving systems, including basic replication,\n\nsharding, and work sharing, are discussed. Additionally, Chapters 9",
      "content_length": 927,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 11,
      "content": "and 10 discuss essential distributed concepts like functions, event-\n\ndriven programming, and leader election.\n\nPart IV, “Batch Computational Patterns”\n\nChapters 11 through 13 cover distributed system patterns for large-\n\nscale batch data processing regarding work queues, event-based\n\nprocessing, and coordinated workflows.\n\nPart V, “Universal Concepts”\n\nThe book concludes with several topics that are universal to all\n\ndistributed systems. Chapter 14 covers logging, monitoring, and\n\nalerting for your application; Chapter 15 provides a survey of AI\n\ninfrastructure; and Chapter 16 describes many common failures and\n\ndesign errors that occur over and over again as we build distributed\n\nsystems.\n\nIf you are an experienced distributed systems engineer, you can likely skip\n\nChapters 1 and 2, though you may want to skim them to understand how I\n\nexpect these patterns to be applied and why I think the general notion of\n\ndistributed system patterns is so important.\n\nYou will likely find utility in the single-node patterns, as they are the most\n\ngeneric and most reusable patterns in the book.\n\nDepending on your goals and the systems you are interested in developing,\n\nyou can choose to focus on either large-scale big data patterns or patterns",
      "content_length": 1250,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 12,
      "content": "for long-running servers (or both). The two parts are largely independent\n\nfrom each other and can be read in any order.\n\nLikewise, if you have extensive distributed system experience, you may\n\nfind that some of the early patterns chapters (e.g., Part III on naming,\n\ndiscovery, and load balancing) are redundant with what you already know,\n\nso feel free to skim through to gain the high-level insights—but don’t forget\n\nto look at all of the pretty pictures!\n\nConventions Used in This Book\n\nThe following typographical conventions are used in this book:\n\nItalic\n\nIndicates new terms, URLs, email addresses, filenames, and file\n\nextensions.\n\nConstant width\n\nUsed for program listings, as well as within paragraphs to refer to\n\nprogram elements such as variable or function names, databases, data\n\ntypes, environment variables, statements, and keywords.\n\nConstant width italic",
      "content_length": 875,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 13,
      "content": "Shows text that should be replaced with user-supplied values or by\n\nvalues determined by context.\n\nNOTE\n\nThis element signifies a general note.\n\nOnline Resources\n\nThough this book describes generally applicable distributed system\n\npatterns, it expects that readers are familiar with containers and container\n\norchestration systems. If you don’t have a lot of preexisting knowledge\n\nabout these things, I recommend the following resources:\n\nDocker\n\nKubernetes\n\nDC/OS\n\nUsing Code Examples\n\nSupplemental material (code examples, exercises, etc.) is available for\n\ndownload at https://github.com/brendandburns/designing-distributed-\n\nsystems.",
      "content_length": 638,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 14,
      "content": "If you have a technical question or a problem using the code examples,\n\nplease send email to support@oreilly.com.\n\nThis book is here to help you get your job done. In general, if example code\n\nis offered with this book, you may use it in your programs and\n\ndocumentation. You do not need to contact us for permission unless you’re\n\nreproducing a significant portion of the code. For example, writing a\n\nprogram that uses several chunks of code from this book does not require\n\npermission. Selling or distributing examples from O’Reilly books does\n\nrequire permission. Answering a question by citing this book and quoting\n\nexample code does not require permission. Incorporating a significant\n\namount of example code from this book into your product’s documentation\n\ndoes require permission.\n\nWe appreciate, but generally do not require, attribution. An attribution\n\nusually includes the title, author, publisher, and ISBN. For example:\n\n“Designing Distributed Systems by Brendan Burns (O’Reilly). Copyright\n\n2025 Brendan Burns, 978-1-098-15635-0.”\n\nIf you feel your use of code examples falls outside fair use or the\n\npermission given above, feel free to contact us at permissions@oreilly.com.",
      "content_length": 1193,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 15,
      "content": "O’Reilly Online Learning\n\nNOTE\n\nFor more than 40 years, O’Reilly Media has provided technology and business training, knowledge,\n\nand insight to help companies succeed.\n\nOur unique network of experts and innovators share their knowledge and\n\nexpertise through books, articles, and our online learning platform.\n\nO’Reilly’s online learning platform gives you on-demand access to live\n\ntraining courses, in-depth learning paths, interactive coding environments,\n\nand a vast collection of text and video from O’Reilly and 200+ other\n\npublishers. For more information, visit https://oreilly.com.\n\nHow to Contact Us\n\nPlease address comments and questions concerning this book to the\n\npublisher:\n\nO’Reilly Media, Inc.\n\n1005 Gravenstein Highway North\n\nSebastopol, CA 95472\n\n800-889-8969 (in the United States or Canada)",
      "content_length": 812,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 16,
      "content": "707-827-7019 (international or local)\n\n707-829-0104 (fax)\n\nsupport@oreilly.com\n\nhttps://oreilly.com/about/contact.html\n\nWe have a web page for this book, where we list errata, examples, and any\n\nadditional information. You can access this page at\n\nhttps://oreil.ly/designing-distributed-systems-2e.\n\nFor news and information about our books and courses, visit\n\nhttps://oreilly.com.\n\nFind us on LinkedIn: https://linkedin.com/company/oreilly-media.\n\nWatch us on YouTube: https://youtube.com/oreillymedia.\n\nAcknowledgments\n\nI’d like to thank my wife, Robin, and my children for everything they do to\n\nkeep me happy and sane. To all of the people along the way who took the\n\ntime to help me learn all of these things, many thanks! Also thanks to my\n\nparents for that first SE/30.",
      "content_length": 776,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 17,
      "content": "I would also like to thank the technical reviewers who took time to provide\n\ntheir feedback and make this book better:\n\nDinesh Reddy Chittibala\n\nAnne Currie\n\nChris Devers\n\nWerner Dijkerman\n\nSukanya Moorthy\n\nLalithkumar Prakashchand\n\nWilliam Jamir Silva\n\nRajeev Reddy Vishaka\n\nFinally, I would like to thank the staff at O’Reilly and everyone who\n\nprovided feedback for the first edition of this book. You’ve helped me make\n\na better book, and I’m grateful.\n\nOceanofPDF.com",
      "content_length": 472,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 18,
      "content": "Part I. Foundational Concepts\n\nBefore we get started describing distributed systems, there are motivations\n\nand concepts that form the foundation of both why and how we build\n\ndistributed systems. This section covers these foundational concepts to\n\nprovide a basis for the rest of the book.\n\nDistributed systems don’t exist in a vacuum. The development of such\n\nsystems is based on the evolving role of both computing and online systems\n\nin business and entertainment. In particular, in the development of always-\n\non, mission-critical systems that we rely on every day. Additionally, the\n\ndevelopment of modern distributed systems is based on the history of how\n\nsuch systems have been designed and built in the past. This history of both\n\nhow the systems are built, and often more importantly how they have failed,\n\nhas led us to the current containerized and microservice architectures that\n\nyou find in this book.\n\nBefore the design of distributed systems can be described, it is necessary to\n\nhave a grounding in core concepts for how server systems operate, as well\n\nas fundamental computer science concepts like locking and APIs. It is also\n\nnecessary to have a grounding in basic operations for distributed systems,\n\nsuch as monitoring and logging. Finally, because distributed systems\n\ninvolve numerous interactions across many different systems and many\n\ndifferent requests, it is necessary to have a basic understanding of statistics",
      "content_length": 1444,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 19,
      "content": "and how we can measure the common behavior across the system through\n\nobserving multiple different requests on different machines.\n\nAfter reading these introductory chapters, you should have the foundational\n\ngrounding in the context, history, and concepts necessary to understand how\n\nthe design of these systems is described. This grounding also helps explain\n\nwhy some of the seemingly complex aspects of the design become\n\nnecessary for reliability or scale.\n\nOceanofPDF.com",
      "content_length": 478,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 20,
      "content": "Chapter 1. Introduction\n\nToday’s world of always-on applications and APIs has availability and\n\nreliability requirements that would have been required of only a handful of\n\nmission-critical services around the globe only a few decades ago.\n\nLikewise, the potential for rapid, viral growth of a service means that every\n\napplication has to be built to scale nearly instantly in response to user\n\ndemand. These constraints and requirements mean that almost every\n\napplication that is built—whether it is a consumer mobile app or a backend\n\npayments application—needs to be a distributed system.\n\nBut building distributed systems is challenging. Often, they are one-off\n\nbespoke solutions. In this way, distributed system development bears a\n\nstriking resemblance to the world of software development prior to the\n\ndevelopment of modern object-oriented programming languages.\n\nFortunately, as with the development of object-oriented languages, there\n\nhave been technological advances that have dramatically reduced the\n\nchallenges of building distributed systems. In this case, it is the rising\n\npopularity of containers and container orchestrators. As with the concept of\n\nobjects within object-oriented programming, these containerized building\n\nblocks are the basis for the development of reusable components and\n\npatterns that dramatically simplify and make accessible the practices of\n\nbuilding reliable distributed systems. In the following introduction, we give\n\na brief history of the developments that have led to where we are today.",
      "content_length": 1539,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 21,
      "content": "A Brief History of Systems Development\n\nIn the beginning, there were machines built for specific purposes, such as\n\ncalculating artillery tables or the tides, breaking codes, or other precise,\n\ncomplicated but rote mathematical applications. Eventually these purpose-\n\nbuilt machines evolved into general-purpose programmable machines. And\n\neventually they evolved from running one program at a time to running\n\nmultiple programs on a single machine via time-sharing operating systems,\n\nbut these machines were still disjoint from each other.\n\nGradually, machines came to be networked together, and client-server\n\narchitectures were born so that a relatively low-powered machine on\n\nsomeone’s desk could be used to harness the greater power of a mainframe\n\nin another room or building. While this sort of client-server programming\n\nwas somewhat more complicated than writing a program for a single\n\nmachine, it was still fairly straightforward to understand. The client(s) made\n\nrequests; the server(s) serviced those requests.\n\nIn the early 2000s, the rise of the internet and large-scale data centers\n\nconsisting of thousands of relatively low-cost commodity computers\n\nnetworked together gave rise to the widespread development of distributed\n\nsystems. Unlike client-server architectures, distributed system applications\n\nare made up of multiple different applications running on different\n\nmachines, or many replicas running across different machines, all",
      "content_length": 1459,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 22,
      "content": "communicating together to implement a system like web search or a retail\n\nsales platform.\n\nBecause of their distributed nature, when structured properly, distributed\n\nsystems are inherently more reliable. And when architected correctly, they\n\ncan lead to much more scalable organizational models for the teams of\n\nsoftware engineers that built these systems. Unfortunately, these advantages\n\ncome at a cost. These distributed systems can be significantly more\n\ncomplicated to design, build, and debug correctly. The engineering skills\n\nneeded to build a reliable distributed system are significantly higher than\n\nthose needed to build single-machine applications like mobile or web\n\nfrontends. Regardless, the need for reliable distributed systems only\n\ncontinues to grow. Thus, there is a corresponding need for the tools,\n\npatterns, and practices for building them.\n\nFortunately, technology has also increased the ease with which you can\n\nbuild distributed systems. Containers, container images, and container\n\norchestrators have all become popular in recent years because they are the\n\nfoundation and building blocks for reliable distributed systems. Using\n\ncontainers and container orchestration as a foundation, we can establish a\n\ncollection of patterns and reusable components. These patterns and\n\ncomponents are a toolkit that we can use to build our systems more reliably\n\nand efficiently.",
      "content_length": 1398,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 23,
      "content": "A Brief History of Patterns in Software\n\nDevelopment\n\nThis is not the first time such a transformation has occurred in the software\n\nindustry. For a better context on how patterns, practices, and reusable\n\ncomponents have previously reshaped systems development, it is helpful to\n\nlook at past moments when similar transformations have taken place.\n\nFormalization of Algorithmic Programming\n\nThough people had been programming for more than a decade before its\n\npublication in 1962, Donald Knuth’s collection, The Art of Computer\n\nProgramming (Addison-Wesley Professional), marks an important chapter\n\nin the development of computer science. In particular, the books contain\n\nalgorithms not designed for any specific computer, but rather to educate the\n\nreader on the algorithms themselves. These algorithms then could be\n\nadapted to the specific architecture of the machine being used or the\n\nspecific problem that the reader was solving. This formalization was\n\nimportant because it provided users with a shared toolkit for building their\n\nprograms, but also because it showed that there was a general-purpose\n\nconcept that programmers should learn and then subsequently apply in a\n\nvariety of different contexts. The algorithms themselves, independent of\n\nany specific problem to solve, were worth understanding for their own sake.",
      "content_length": 1334,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 24,
      "content": "Patterns for Object-Oriented Programming\n\nKnuth’s books represent an important landmark in the thinking about\n\ncomputer programming, and algorithms represent an important component\n\nin the development of computer programming. However, as the complexity\n\nof programs grew, and the number of people writing a single program grew\n\nfrom the single digits to the double digits and eventually to the thousands, it\n\nbecame clear that procedural programming languages and algorithms were\n\ninsufficient for the tasks of modern-day programming. These changes in\n\ncomputer programming led to the development of object-oriented\n\nprogramming languages, which elevated data, reusability, and extensibility\n\nto peers of the algorithm in the development of computer programs.\n\nIn response to these changes to computer programming, there were changes\n\nto the patterns and practices for programming as well. Throughout the early\n\nto mid-1990s, there was an explosion of books on patterns for object-\n\noriented programming. The most famous of these is the “gang of four”\n\nbook, Design Patterns: Elements of Reusable Object-Oriented\n\nProgramming by Erich Gamma et al. (Addison-Wesley Professional).\n\nDesign Patterns gave a common language and framework to the task of\n\nprogramming. It described a series of interface-based patterns that could be\n\nreused in a variety of contexts. Because of advances in object-oriented\n\nprogramming and specifically interfaces, these patterns could also be\n\nimplemented as generic reusable libraries. These libraries could be written",
      "content_length": 1546,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 25,
      "content": "once by a community of developers and reused repeatedly, saving time and\n\nimproving reliability.\n\nThe Rise of Open Source Software\n\nThough the concept of developers sharing source code has been around\n\nnearly since the beginning of computing, and formal free software\n\norganizations have been in existence since the mid-1980s, the very late\n\n1990s and the 2000s saw a dramatic increase in the development and\n\ndistribution of open source software. Though open source is only\n\ntangentially related to the development of patterns for distributed systems, it\n\nis important in the sense that it was through the open source communities\n\nthat it became increasingly clear that software development in general and\n\ndistributed systems development in particular are community endeavors. It\n\nis important to note that all of the container technology that forms the\n\nfoundation of the patterns described in this book has been developed and\n\nreleased as open source software. The value of patterns for both describing\n\nand improving the practice of distributed development is especially clear\n\nwhen you look at it from this community perspective.",
      "content_length": 1135,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 26,
      "content": "NOTE\n\nWhat is a pattern for a distributed system? There are plenty of instructions out there that will tell you\n\nhow to install specific distributed systems (such as a NoSQL database). Likewise, there are recipes\n\nfor a specific collection of systems (like a MEAN stack). But when I speak of patterns, I’m referring\n\nto general blueprints for organizing distributed systems, without mandating any specific technology\n\nor application choices. The purpose of a pattern is to provide general advice or structure to guide\n\nyour design. The hope is that such patterns will guide your thinking and also be generally applicable\n\nto a wide variety of applications and environments.\n\nThe Value of Patterns, Practices, and\n\nComponents\n\nBefore spending any of your valuable time reading about a series of\n\npatterns that I claim will improve your development practices, teach you\n\nnew skills, and—let’s face it—change your life, it’s reasonable to ask:\n\n“Why?” What is it about the design patterns and practices that can change\n\nthe way that we design and build software? In this section, I’ll lay out the\n\nreasons I think this is an important topic, and hopefully convince you to\n\nstick with me for the rest of the book.\n\nStanding on the Shoulders of Giants\n\nAs a starting point, the value that patterns for distributed systems offer is\n\nthe opportunity to figuratively stand on the shoulders of giants. It’s rarely\n\nthe case that the problems we solve or the systems we build are truly",
      "content_length": 1475,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 27,
      "content": "unique. Ultimately, the combination of pieces that we put together and the\n\noverall business model that the software enables may be something that the\n\nworld has never seen before. But the way the system is built and the\n\nproblems it encounters as it aspires to be reliable, agile, and scalable are not\n\nnew.\n\nThis, then, is the first value of patterns: they allow us to learn from the\n\nmistakes of others. Perhaps you have never built a distributed system\n\nbefore, or perhaps you have never built this type of distributed system.\n\nRather than hoping that a colleague has some experience in this area or\n\nlearning by making the same mistakes that others have already made, you\n\ncan turn to patterns as your guide. Learning about patterns for distributed\n\nsystem development is the same as learning about any other best practice in\n\ncomputer programming. It accelerates your ability to build software without\n\nrequiring that you have direct experience with the systems, mistakes, and\n\nfirsthand learning that led to the codification of the pattern in the first place.\n\nA Shared Language for Discussing Our Practice\n\nLearning about and accelerating our understanding of distributed systems is\n\nonly the first value of having a shared set of patterns. Patterns have value\n\neven for experienced distributed system developers who already understand\n\nthem well. Patterns provide a shared vocabulary that enables us to\n\nunderstand each other quickly. This understanding forms the basis for\n\nknowledge sharing and further learning.",
      "content_length": 1523,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 28,
      "content": "To better understand this, imagine that we both are using the same object to\n\nbuild our house. I call that object a “Foo” while you call that object a “Bar.”\n\nHow long will we spend arguing about the value of a Foo versus that of a\n\nBar, or trying to explain the differing properties of Foo and Bar until we\n\nfigure out that we’re speaking about the same object? Only once we\n\ndetermine that Foo and Bar are the same can we truly start learning from\n\neach other’s experience.\n\nWithout a common vocabulary, we waste time in arguments of “violent\n\nagreement” or in explaining concepts that others understand but know by\n\nanother name. Consequently, another significant value of patterns is to\n\nprovide a common set of names and definitions so that we don’t waste time\n\nworrying about naming, and instead get right down to discussing the details\n\nand implementation of the core concepts.\n\nI have seen this happen in my short time working on containers. Along the\n\nway, the notion of a sidecar container (described in Chapter 3 of this book)\n\ntook hold within the container community. Because of this, we no longer\n\nhave to spend time defining what it means to be a sidecar and can instead\n\njump immediately to how the concept can be used to solve a particular\n\nproblem. “If we just use a sidecar” … “Yeah, and I know just the container\n\nwe can use for that.” This example leads to the third value of patterns: the\n\nconstruction of reusable components.",
      "content_length": 1448,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 29,
      "content": "Shared Components for Easy Reuse\n\nBeyond enabling people to learn from others and providing a shared\n\nvocabulary for discussing the art of building systems, patterns provide\n\nanother important tool for computer programming: the ability to identify\n\ncommon components that can be implemented once.\n\nIf we had to create all of the code that our programs use ourselves, we\n\nwould never get done. Indeed, we would barely get started. Today, every\n\nsystem ever written stands on the shoulders of thousands if not hundreds of\n\nthousands of years of human effort. Code for operating systems, printer\n\ndrivers, distributed databases, container runtimes, and container\n\norchestrators—indeed, the entirety of applications that we build today are\n\nbuilt with reusable shared libraries and components.\n\nPatterns are the basis for the definition and development of such reusable\n\ncomponents. The formalization of algorithms led to reusable\n\nimplementations of sorting and other canonical algorithms. The\n\nidentification of interface-based patterns gave rise to a collection of generic\n\nobject-oriented libraries implementing those patterns.\n\nIdentifying core patterns for distributed systems enables us to build shared\n\ncommon components. Implementing these patterns as container images\n\nwith HTTP-based interfaces means they can be reused across many\n\ndifferent programming languages. And, of course, building reusable",
      "content_length": 1406,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 30,
      "content": "components improves the quality of each component because the shared\n\ncodebase gets sufficient usage to identify bugs and weaknesses, and\n\nsufficient attention to ensure that they get fixed.\n\nMore recently, a series of software supply chain attacks has made\n\ndependencies and dependency management a critical part of securing our\n\napplications. In the context of a secure software supply chain, these shared\n\ncomponents take on even more importance. Every library or application that\n\nwe use brings in more dependencies—and consequently, more risk. Relying\n\non a single shared implementation of a core idea reduces the total amount of\n\nsoftware that the world needs to depend on, and by focusing attention on a\n\nfew dependencies, significantly improves the chances that they are properly\n\nmaintained and protected from software supply chain attacks.\n\nSummary\n\nDistributed systems are required to implement the level of reliability,\n\nagility, and scale expected of modern computer programs. Distributed\n\nsystem design continues to be more of a black art practiced by wizards than\n\na science applied by laypeople. The identification of common patterns and\n\npractices has regularized and improved the practice of algorithmic\n\ndevelopment and object-oriented programming. It is this book’s goal to do\n\nthe same for distributed systems. Enjoy!",
      "content_length": 1338,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 31,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 32,
      "content": "Chapter 2. Important Distributed System\n\nConcepts\n\nBefore we get too deep into the design of distributed systems, it is valuable\n\nto ground ourselves in some key concepts that underpin the development of\n\nthese distributed systems. If you are already experienced in distributed\n\nsystems or systems design in general, some of these ideas may be familiar\n\nto you, and you may want to skip ahead, but hopefully at the completion of\n\nthis chapter, you will have a core foundation for the patterns and designs\n\npresented throughout the rest of the book.\n\nAPIs and RPCs\n\nApplication programming interfaces, or APIs, form the core of any\n\ndistributed system. You may have seen APIs in the context of programming\n\nlanguage libraries or calls to operating system APIs. In distributed systems\n\nthe idea is similar, but the API calls take place over the network between\n\nremote services. These calls are referred to as “RPCs” or “remote procedure\n\ncalls.” RPCs rely on an underlying network transport, commonly, though\n\nnot exclusively, the HTTP protocol and the JSON object format. Numerous\n\nother more structured protocols have existed over the years, from CORBA\n\nto gRPC as well as database-specific protocols. Though such systems\n\nprovide capabilities beyond HTTP + JSON (also referred to as REST or",
      "content_length": 1292,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 33,
      "content": "RESTful), these continue to be the most popular because of their wide\n\nsupport, simplicity, and ease of integration with web-based frontends.\n\nAPIs can be synchronous or asynchronous. Synchronous APIs return only\n\nafter the request has been fully processed, while asynchronous APIs accept\n\nthe request and return an operation ID for a background process that can be\n\nmonitored via subsequent status requests. Synchronous APIs are the easiest\n\nto implement, but long-running operations are a bad match for HTTP or\n\nother network protocols because of timeouts and other limitations. Thus, if\n\nthe length of your operation is measured in minutes, it is better off being\n\nmodeled as an asynchronous operation.\n\nAPIs are generally not very useful without clients that can call those APIs.\n\nHaving every developer hand-roll HTTP calls to your API is a recipe for\n\nunhappy developers and badly designed systems. In smaller systems, which\n\nmay only be required to support individual languages, it is possible to\n\nhandwrite these client libraries. But larger systems rely on IDLs, or\n\ninterface description languages, such as OpenAPI, which can describe your\n\nAPI’s structure in such a way that clients can be automatically generated for\n\nnearly any programming language.\n\nBeyond the structure of your API, the core functionality of an API is\n\ndescribed in terms of its SLO, its service level objectives, which describe\n\nhow the API performs in terms of latency and reliability. Having formal",
      "content_length": 1483,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 34,
      "content": "SLOs for latency, reliability, and throughput (requests per second) enables\n\nclients to successfully develop distributed systems.\n\nLatency\n\nOne of the most important measures of your system’s performance is the\n\ntime that it takes to do something. This measure is commonly referred to as\n\nlatency. Generally speaking, latency is measured in milliseconds, though\n\ndepending on your system, it could be measured in microseconds (very fast)\n\nor seconds (very slow). When measuring latency, it is important to ensure\n\nthat it is actually representative of end-user experience (see “Percentiles”)\n\nand that it is attributable, meaning you understand where the latency is\n\ncoming from. Without proper attribution, you may understand that your\n\nsystem is slow, but you will have little idea about how to make it better.\n\nReliability\n\nIn addition to latency, reliability is obviously a key metric for any\n\ndistributed system. At its simplest, reliability is just the fraction of\n\nsuccessful requests relative to the total number of requests. But as with all\n\nthings engineering, in the real world, the devil is in the details. Consider\n\nrequests that are based on the HTTP(S) protocol. Using this protocol, you\n\ncould measure the number of requests failed (i.e., requests that didn’t",
      "content_length": 1275,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 35,
      "content": "actually complete the HTTP protocol). However, for any reasonable web\n\nservice, this number rapidly trends to zero. The reason for this is that HTTP\n\nitself has rich error semantics in the form of the HTTP response code. Thus,\n\nnearly every HTTP request succeeds at the protocol level, but often the\n\nresponse code indicates an error. From this, it is clear that our measure of\n\nreliability should not be at the HTTP protocol level, but rather at the HTTP\n\nresponse code level. However, HTTP defines a large number of response\n\ncodes. Which should we treat as errors? It’s fairly obvious that the 50x\n\nseries of codes are true errors. Most of them represent either internal server\n\nerrors (500) or network transmission errors (e.g., 502). Similarly it is fairly\n\nclear that the 20x series codes are successful requests. Unfortunately, in\n\nbetween 20x and 50x is a gray area. Typically we can consider the 30x\n\n(mostly redirects) status codes to be “successful,” though an excessive\n\nnumber of redirects can be an error in some conditions. Most of the\n\nuncertainty lies in the 40x collection of errors. The 40x collection of status\n\ncodes are the land of ambiguity. Is that 404 (not found) an indication that a\n\nclient simply made a mistake? Or is your service somehow misconfigured\n\nto look for files in the wrong location? Similarly, it may seem obvious that\n\n403 (unauthorized) represents a “client error,” but what if your\n\nauthorization service is having an outage?\n\nUltimately, though the examples above are drawn from the HTTP protocol,\n\nthe important takeaway is that measuring reliability is both critical and\n\ncomplicated. In practice, treating 404 (not found) as a client error can mask\n\ntrue reliability problems and prevent you from alerting when there is a",
      "content_length": 1769,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 36,
      "content": "customer-facing outage in your system. At the same time, a spike in 404\n\nerrors can also be from a single misconfigured client, causing noise and\n\nfalse alerts, which will plague your on-call engineers. In this gray area,\n\nproper monitoring will be application dependent. Further AI-based\n\noperations and anomaly detection become critical in differentiating between\n\ntrue problems and false alerts.\n\nPercentiles\n\nThroughout this book there are discussions of percentiles. The 99th\n\npercentile latency, for example. Percentiles are key to understanding\n\ndistributed systems because distributed systems are inherently about serving\n\nlarge numbers of users and large numbers of requests. Of course, in serving\n\nlots of users, we want to understand the operation of our services. By\n\ndefault it is easy to use the average as the way in which we measure user\n\nexperience. It makes sense. We want to understand the experience of the\n\naverage user, so it makes sense to look at the average experience of our\n\nvarious metrics. Unfortunately, averages are often really distorted in\n\nconsidering the experience of a system. There are several reasons for this,\n\nbut the two largest have to do with the calculation of the average as well as\n\nthe ways in which individual metrics (e.g., request latency) come together\n\nin an end-to-end user experience. When you consider the arithmetic\n\naverage, it is heavily skewed by either large or small outliers. Consider\n\nmeasuring latency, a relatively small number of users with very fast latency",
      "content_length": 1525,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 37,
      "content": "will distort the average latency of your system and make you believe that\n\nyour “average” performance is much better than it actually is.\n\nThe other reason that the average is problematic is that it considers\n\nindividual requests separately; however, most user experiences are the\n\ncombination of multiple requests which must succeed. The combining of all\n\nof these experiences means that almost no one actually has an “average”\n\nexperience in terms of latency; instead, the probability that every user has at\n\nleast one exceptionally slow request rapidly approaches 100% as the\n\nnumber of requests in an end-to-end experience increases.\n\nFor both of these reasons, to truly understand the behavior of your system,\n\nit is far more useful to consider the 90th or 99th percentile of your user\n\nexperience. These percentiles represent the place in the distribution of\n\nlatency where 90% or 99% of your users experience lower latency. For\n\nexample, if your 99th percentile latency is one second, then 99% of your\n\nusers have latency less than one second. This gives you a much better sense\n\nfor the operation of your service, but importantly, even the 99th percentile\n\nsuffers from the problems of multiple requests in a single end-to-end\n\nexperience. Taken together, the 99th percentile latency of a single request\n\nrapidly becomes the 90th (or even 50th) percentile of an end-to-end\n\nexperience with enough requests in that experience. Because of this, it is\n\nalways worth understanding exactly which experiences you are measuring.",
      "content_length": 1529,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 38,
      "content": "Idempotency\n\nDesigning distributed systems is, in many ways, the study of, and\n\nanticipation of, how systems can fail. Of course, we don’t want our systems\n\nto fail, and thus, in order to compensate for when the underlying systems do\n\nfail, we introduce ideas that make it easier for us to restore our services. The\n\nfirst of these is the idea of idempotency. Idempotency is the notion that\n\nwhether an action is performed a single time or many times, the result is\n\nidentical. An example of an idempotent operation is the statement X = 5. No\n\nmatter how many times you execute that operation, the result is the same. X\n\nalways is equal to 5. Compare that with the statement X = X + 1. This\n\noperation is not idempotent. Each time it’s executed, the value of X\n\nincrements by one. It is clear that the eventual value of X is dependent on\n\nthe number of times that instruction is executed. If you are familiar with the\n\nideas of infrastructure as code and declarative configuration, idempotency\n\nmaps directly to declarative versus imperative configuration.\n\nIdempotency is an important concept for distributed systems because it\n\nmakes error and failure handling significantly easier. If we know that an\n\noperation is idempotent, in the face of failures, we can simply repeat it until\n\nit succeeds. No matter how many times it takes to get a successful outcome,\n\nwe know that that outcome will always be the same. Idempotent operations\n\nmake designing reliable systems easier.",
      "content_length": 1476,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 39,
      "content": "Delivery Semantics\n\nThough idempotency allows us to ignore the number of times a particular\n\nrequest has been made, sometimes it is necessary to actually control the\n\nnumber of times a particular request has been made in the presence of error\n\nconditions.\n\nGenerally, delivery of a message (or the success of a request) is divided into\n\ntwo categories. The first is “at least once” delivery. As its name suggests, at\n\nleast once ensures that a message or a request will be delivered at least once\n\nto the recipient. The “at least” part is the important part, because with at\n\nleast once delivery there are no guarantees about how many times a\n\nmessage will be received.\n\nIn contrast to “at least once” delivery, the alternative is “at most once”\n\ndelivery, which ensures that a message can never be delivered more than\n\nonce. As with at least once, the important part of this is the “at most”\n\nphrase, because with at most once delivery it is perfectly acceptable for a\n\nmessage to never be delivered. A good example of where you might want at\n\nmost once delivery is the charging of a credit card. Clearly multiple\n\nidentical charges to the same card are not allowable; “at most once”\n\ndelivery assumes that in the presence of failure (e.g., the absence of\n\npayment) someone will try again.",
      "content_length": 1290,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 40,
      "content": "You may notice that “exactly once” delivery is missing from the options\n\ndescribed. This is because, in practice, exactly once delivery is\n\nexceptionally hard to achieve. You are far better designing systems which\n\ncan handle either at most or at least once delivery.\n\nRelational Integrity\n\nThe reason that message delivery semantics become so important is that\n\nthey ultimately affect the data that is the core of our application. If the data\n\nis your bank account, the integrity (e.g., reliability) of that data is clearly a\n\nvery important concern. Ensuring the reliability of a single data entry (e.g.,\n\nthe balance in your bank account) is important, but relational integrity is an\n\nequally important concern for distributed systems. Relational integrity\n\nrefers to the integrity or accuracy of data stored in multiple different rows\n\nor stores within a system. For example, your bank balance might be stored\n\nas a pair of (user-id, balance) , and your address might be stored\n\nin a different store as (user-id, address) . The integrity of that data\n\nrefers to whether there are strong guarantees that for every user-id in\n\nthe balances table, there is a corresponding entry in the addresses table. If\n\nthe code in your application relies on there being consistency between\n\nvarious data stores, then your code relies on strong relational integrity. If\n\ninstead your code can handle cases in which data in multiple stores may not\n\nbe fully in sync, your code does not rely on relational integrity. The reason\n\nthat this becomes important is that maintaining relational integrity requires",
      "content_length": 1592,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 41,
      "content": "carefully synchronizing changes to multiple data stores. This\n\nsynchronization has significant performance and reliability implications. In\n\nparticular, it requires distributed transactions across multiple different data\n\nstores, which requires distributed locks that make your system more\n\ncomplicated and less parallel. Relational integrity can generally be seen as\n\nthe distinction between SQL and NoSQL data stores. Traditional databases\n\nand SQL go to great pains to ensure relational consistency between various\n\ntables, including transactions and rollback semantics. NoSQL data stores\n\nlike Azure’s CosmosDB and the open source Cassandra database favor\n\nmore relaxed relational semantics, where everything is a key/value store\n\nand consistency is a problem for application code. In general, relational\n\nconsistency can be seen as a trade-off between performance and code\n\ncomplexity. If you don’t need performance, the consistency offered by a\n\ntraditional SQL database can make your code much easier, while looser\n\napproaches facilitate increased parallelism and corresponding performance.\n\nData Consistency\n\nBeyond relational consistency, there is another related, important dimension\n\nof consistency to consider, and that is data consistency. In any distributed\n\nsystem, data is replicated to ensure the availability of data in the face of\n\nerrors. This data replication has performance implications, and it is\n\nimportant to decide whether your data requires strong or eventual\n\nconsistency.",
      "content_length": 1501,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 42,
      "content": "Clearly, consistency, the notion that the data written is the same in all\n\nlocations, is a critically important part of data storage. But how quickly\n\nmust that data be the same? That is what we are considering when we talk\n\nabout data consistency. In a strongly consistent system, the data is\n\nguaranteed to be present everywhere before the writing of the data is\n\nconsidered to be “complete.” In an eventually consistent system, the data is\n\nguaranteed to be the same everywhere “eventually,” where eventually can\n\nactually mean a long-ish time.\n\nTo make this a little more concrete, consider a system which replicates its\n\ndata to all of the data centers it maintains in the world. In a strongly\n\nconsistent system, a user requests a write of some data item, and that data is\n\nimmediately written to all of the different data centers. Until all of these\n\ndata centers successfully acknowledge that the write has completed, the\n\nuser request to write is not considered successful. At first blush, this seems\n\nlike the right approach—until the data is everywhere, the user should not be\n\ntold that their request is successful. Unfortunately, the result of this is\n\nsignificantly decreased performance. As the number of data centers grows,\n\nthe number of required writes also grows, up until a point where every\n\nrequest is basically guaranteed to hit a data center which is running slowly\n\nor even failing. Thus, while strong consistency ensures data reliability, it\n\nsignificantly reduces write throughput.\n\nIn the previous example, an eventually consistent system might return as\n\nsoon as a single data center has acknowledged the write. Replication to all",
      "content_length": 1659,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 43,
      "content": "of the data centers is performed by an asynchronous background process,\n\nand the user doesn’t have to wait around for it to complete. The system says\n\n“I got this,” and the user trusts the system to eventually make the data\n\nconsistent. Waiting only for a single write makes the system both more\n\nreliable and more performant. You are significantly less likely to hit a slow\n\nor failing data center with a single write.\n\nUnfortunately, in many use cases eventual consistency gives the impression\n\nto the end user that the system is less reliable. The reason for this is the\n\n“read your own writes” problem. Distributed systems replicate themselves\n\nacross multiple data centers for reliability and performance. Most of the\n\ntime, a single user’s requests will all go to the same data center, but\n\noccasionally, under load, failures, or other specific network conditions,\n\ndifferent requests from the same user will go to different data centers. In an\n\neventually consistent system, this means that after a user writes something\n\nto the system, a subsequent read that is load balanced to a different data\n\ncenter may not see the recent write. This can be quite distressing to a user.\n\nImagine if you ordered something on a retail website, clicked on “view my\n\norders,” and couldn’t see your order in the returned list. It’s highly likely\n\nthat you would never order from that site again.\n\nBecause of these complications, the choice between strong consistency and\n\neventual consistency is never a very clean one. It depends on the use case as\n\nwell as the performance requirements of the system. Be careful when\n\ndeciding which approach to take with your system, though. The decision to",
      "content_length": 1684,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 44,
      "content": "use strong or eventual consistency will permeate your system design and\n\nyour application’s code. Once written, it is very hard to change the storage\n\nmodel that is used by a distributed system, and the implications of the\n\ndecision often take years to show up.\n\nThe trade-offs above between consistency, availability, and partition\n\ntolerance is known as the CAP theorem, and it states that you can build a\n\nsystem with two of the three attributes, e.g., a system that is consistent and\n\navailable but not partition tolerant, but you can’t build a system with all\n\nthree. It’s worth noting that the CAP theorem is not a draconian rule. You\n\nmay not be able to build a system that guarantees all three, but it is possible\n\nto build a system where each are reasonable for your application. For some\n\napplications, 99% availability is sufficient, and thus, you can use that trade-\n\noff to ensure the consistency and partition tolerance you need as well.\n\nEffectively, the CAP theorem says that whenever you are optimizing one of\n\nthe system characteristics, you are inherently trading off against other goals.\n\nOrchestration and Kubernetes\n\nDistributed systems defined today are not deployed in a vacuum or using\n\nonly code written by the system’s developers. Instead, the ubiquity of\n\ndistributed systems today has meant that shared infrastructure has been built\n\nto support the deployment and operation of distributed systems. This\n\ninfrastructure is commonly referred to as an orchestrator, because it",
      "content_length": 1502,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 45,
      "content": "orchestrates the operation of the distributed system without implementing\n\nthe details of the systems logic. The most common orchestrator is the open\n\nsource Kubernetes system for orchestrating deployments of containerized\n\napplications. The orchestrator is responsible for taking the desired state of\n\nthe application (e.g., I want three replicas of my frontend and five replicas\n\nof my backend) and ensuring that the real state of the system matches the\n\ndesired state that has been declared.\n\nHealth Checks\n\nTo achieve successful orchestration, the orchestration system needs to\n\nunderstand the health of the pieces that make up the distributed system.\n\nHealth is used to understand when an application is ready to be added to a\n\nload balancer, when an application is unhealthy and needs to be restarted, or\n\nwhen a rollout should be halted because the health of the application is\n\nimpacted. While there are some basic aspects of health (e.g., is the program\n\nrunning) that the orchestrator can observe from outside the system, true\n\nunderstanding of your application requires that you implement health\n\nchecks that are specific to your application. When providing health\n\ninformation to the orchestrator, there are two different notions of\n\napplication health. The first is the simplest to understand, in that it simply\n\nrepresents whether or not your application is alive. Applications which are\n\nno longer responding to a liveness check should be terminated. And if a\n\nrollout is causing a significant degradation in the number of alive",
      "content_length": 1543,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 46,
      "content": "applications, that rollout should be halted. The second notion of health is\n\nreadiness. Readiness indicates that an application is ready to be used. At\n\nfirst blush, readiness and liveness would seem to be the same thing, but in\n\nmany circumstances an application can be alive but not ready. The easiest\n\nexample to understand is when an application has just started up and is\n\ninitializing its state, perhaps by downloading and caching large files over\n\nthe network. Such an application is alive, it is doing useful work and should\n\nnot be terminated, but it is not ready until it has successfully downloaded\n\nand made ready to serve all of the files in its cache. By properly\n\nimplementing both liveness and readiness checks, you can ensure that your\n\ndistributed systems interact well with the orchestrator on which they run.\n\nSummary\n\nThis chapter has described a number of key concepts that are foundational\n\nto the design and understanding of distributed systems. These concepts\n\nform the basis for both understanding the requirements and describing the\n\ncharacteristics of the systems that we build. Before going further into the\n\nbook, make sure that they all feel intuitive and familiar to you.\n\nOceanofPDF.com",
      "content_length": 1219,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 47,
      "content": "Part II. Single-Node Patterns\n\nThis book concerns itself with distributed systems, which are applications\n\nmade up of many different components running on many different\n\nmachines. However, the first section of this book is devoted to patterns that\n\nexist on a single node. The motivation for this is straightforward.\n\nContainers are the foundational building block for the patterns in this book,\n\nbut in the end, groups of containers colocated on a single machine make up\n\nthe atomic elements of distributed system patterns.\n\nThough it is clear as to why you might want to break your distributed\n\napplication into a collection of different containers running on different\n\nmachines, it is perhaps somewhat less clear as to why you might also want\n\nto break up the components running on a single machine into different\n\ncontainers. To understand the motivation for these groups of containers, it is\n\nworth considering the goals behind containerization. In general, the goal of\n\na container is to establish boundaries around specific resources (e.g., this\n\napplication needs two cores and 8 GB of memory). Likewise, the boundary\n\ndelineates team ownership (e.g., this team owns this image). Finally, the\n\nboundary is intended to provide separation of concerns (e.g., this image\n\ndoes this one thing).\n\nAll of these reasons provide motivation for splitting up an application on a\n\nsingle machine into a group of containers. Consider resource isolation first.",
      "content_length": 1456,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 48,
      "content": "Your application may be made up of two components: one is a user-facing\n\napplication server and the other is a background configuration file loader.\n\nClearly, end-user-facing request latency is the highest priority, so the user-\n\nfacing application needs to have sufficient resources to ensure that it is\n\nhighly responsive. On the other hand, the background configuration loader\n\nis mostly a best-effort service; if it is delayed slightly during times of high\n\nuser-request volume, the system will be OK. Likewise, the background\n\nconfiguration loader should not impact the quality of service that end users\n\nreceive. For all of these reasons, you want to separate the user-facing\n\nservice and the background shard loader into different containers. This\n\nallows you to attach different resource requirements and priorities to the\n\ntwo different containers and, for example, ensure that the background\n\nloader opportunistically steals cycles from the user-facing service whenever\n\nit is lightly loaded and the cycles are free. Likewise, separate resource\n\nrequirements for the two containers ensure that the background loader will\n\nbe terminated before the user-facing service if there is a resource contention\n\nissue caused by a memory leak or other overcommitment of memory\n\nresources.\n\nIn addition to this resource isolation, there are other reasons to split your\n\nsingle-node application into multiple containers. Consider the task of\n\nscaling a team. There is good reason to believe that the ideal team size is six\n\nto eight people. In order to structure teams in this manner and yet still build\n\nsignificant systems, we need to have small, focused pieces for each team to",
      "content_length": 1677,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 49,
      "content": "own. Additionally, often some of the components, if factored properly, are\n\nreusable modules that can be used by many teams.\n\nConsider, for example, the task of keeping a local filesystem synchronized\n\nwith a Git source code repository. If you build this Git sync tool as a\n\nseparate container, you can reuse it with PHP, HTML, JavaScript, Python,\n\nand numerous other web-serving environments. If you instead factor each\n\nenvironment as a single container where, for example, the Python runtime\n\nand the Git synchronization are inextricably bound, then this sort of\n\nmodular reuse (and the corresponding small team that owns that reusable\n\nmodule) is impossible.\n\nFinally, even if your application is small and all of your containers are\n\nowned by a single team, the separation of concerns ensures that your\n\napplication is well understood and can easily be tested, updated, and\n\ndeployed. Small, focused applications are easier to understand and have\n\nfewer couplings to other systems. This means, for example, that you can\n\ndeploy the Git synchronization container without having to also redeploy\n\nyour application server. This leads to rollouts with fewer dependencies and\n\nsmaller scope. That, in turn, leads to more reliable rollouts (and rollbacks),\n\nwhich leads to greater agility and flexibility when deploying your\n\napplication.\n\nI hope that all of these examples have motivated you to think about\n\nbreaking up your applications, even those on a single node, into multiple",
      "content_length": 1481,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 50,
      "content": "1\n\ncontainers. The following chapters describe some patterns that can help\n\nguide you as you build modular groups of containers. In contrast to later\n\npatterns we’ll discuss, all of the patterns in this section assume tight\n\ndependencies between all containers. In particular, they assume that all of\n\nthe containers in the pattern can be reliably coscheduled onto a single\n\nmachine.\n\nThey also assume that all of the containers in the pattern can optionally\n\nshare volumes or parts of their filesystems as well as other key container\n\nresources like network namespaces and shared memory. This tight grouping\n\n1\n\nis called a pod in Kubernetes, but the concept is generally applicable to\n\ndifferent container orchestrators, though some support it more natively than\n\nothers.\n\nKubernetes is an open source system for automating deployment, scaling, and management of\n\ncontainerized applications. Check out my book, Kubernetes: Up and Running (O’Reilly).\n\nOceanofPDF.com",
      "content_length": 967,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 51,
      "content": "Chapter 3. The Sidecar Pattern\n\nThe first single-node pattern is the sidecar pattern. The sidecar pattern is a\n\nsingle-node pattern made up of two containers. The first is the application\n\ncontainer. It contains the core logic for the application. Without this\n\ncontainer, the application would not exist. In addition to the application\n\ncontainer, there is a sidecar container. The role of the sidecar is to augment\n\nand improve the application container, often without the application\n\ncontainer’s knowledge. In its simplest form, a sidecar container can be used\n\nto add functionality to a container that might otherwise be difficult to\n\nimprove. Sidecar containers are coscheduled onto the same machine via an\n\natomic container group, such as the pod API object in Kubernetes. In\n\naddition to being scheduled on the same machine, the application container\n\nand sidecar container share a number of resources, including parts of the\n\nfilesystem, hostname, and network, and many other namespaces. A generic\n\nimage of this sidecar pattern is shown in Figure 3-1.",
      "content_length": 1061,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 52,
      "content": "Figure 3-1. The generic sidecar pattern\n\nAn Example Sidecar: Adding HTTPS to\n\na Legacy Service\n\nConsider, for example, a legacy web service. Years ago, when it was built,\n\ninternal network security was not as high a priority for the company, and\n\nthus, the application only services requests over unencrypted HTTP, not\n\nHTTPS. Due to recent security incidents, the company has mandated the\n\nuse of HTTPS for all company websites. To compound the misery of the\n\nteam sent to update this particular web service, the source code for this\n\napplication was built with an old version of the company’s build system,\n\nwhich no longer functions.\n\nContainerizing this HTTP application is simple enough: the binary can run\n\nin a container with a version of an old Linux distribution on top of a more",
      "content_length": 788,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 53,
      "content": "modern kernel being run by the team’s container orchestrator. However, the\n\ntask of adding HTTPS to this application is significantly more challenging.\n\nThe team is trying to decide between resurrecting the old build system\n\nversus porting the application’s source code to the new build system, when\n\none of the team members suggests that they use the sidecar pattern to\n\nresolve the situation more easily.\n\nThe application of the sidecar pattern to this situation is straightforward.\n\nThe legacy web service can be configured to serve exclusively on localhost\n\n(127.0.0.1), which means that only services that share the local network\n\nwith the server will be able to access the service. Normally, this wouldn’t be\n\na practical choice because it would mean that no one could access the web\n\nservice. However, using the sidecar pattern, in addition to the legacy\n\ncontainer, we will add an nginx sidecar container. This nginx\n\ncontainer lives in the same network namespace as the legacy web\n\napplication, so it can access the service that is running on localhost.\n\nAt the same time, this nginx service can terminate HTTPS traffic on the\n\nexternal IP address of the pod and proxy that traffic to the legacy web\n\napplication (see Figure 3-2). Since this unencrypted traffic is only sent via\n\nthe local loopback adapter inside the container group, the network security\n\nteam is satisfied that the data is safe. Likewise, by using the sidecar pattern,\n\nthe team has modernized a legacy application without having to figure out\n\nhow to rebuild a new application to serve HTTPS. A similar form of this\n\npattern can also be used to add automatic certificate rotation, or even",
      "content_length": 1667,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 54,
      "content": "authentication and authorization to legacy web applications that may not be\n\neasy to modify.\n\nFigure 3-2. The HTTPS sidecar\n\nDynamic Configuration with Sidecars\n\nSimply proxying traffic into an existing application is not the only use for a\n\nsidecar. Another common example is configuration synchronization. Many\n\nolder applications use a configuration file for parameterizing the\n\napplication; this may be a raw text file or something more structured like\n\nTOML, XML, JSON, or YAML. Many preexisting applications were\n\nwritten to assume that this file was present on the filesystem and read their\n\nconfiguration from there. However, in a cloud native environment it is often\n\nquite useful to use an API for updating configuration stored elsewhere. This\n\nallows you to do a dynamic push of configuration information via an API\n\ninstead of manually logging in to every server and updating the\n\nconfiguration file using imperative commands. The desire for such an API\n\nis driven both by ease of use as well as the ability to add automation like\n\nrollback, which makes configuring (and reconfiguring) safer and easier.\n\nKubernetes supports ConfigMap resources for exactly this purpose.",
      "content_length": 1182,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 55,
      "content": "Similar to the case of HTTPS, new applications can be written with the\n\nexpectation that configuration is a dynamic property that should be obtained\n\nusing a cloud API, but adapting and updating an existing application can be\n\nsignificantly more challenging. Fortunately, the sidecar pattern again can be\n\nused to provide new functionality that augments a legacy application\n\nwithout changing the existing application. For the sidecar pattern shown in\n\nFigure 3-3, there again are two containers: the container that is the serving\n\napplication and the container that is the configuration manager. The two\n\ncontainers are grouped together into a pod, where they share a directory\n\nbetween themselves. This shared directory is where the configuration file is\n\nmaintained.",
      "content_length": 769,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 56,
      "content": "Figure 3-3. A sidecar example of managing a dynamic configuration\n\nWhen the legacy application starts, it loads its configuration from the\n\nfilesystem, as expected. This configuration is placed into the filesystem\n\nusing a ConfigMap volume which places the current contents of a\n\nConfigMap resource at a particular location in the filesystem. When the\n\nconfiguration manager starts, it examines the configuration API and looks",
      "content_length": 426,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 57,
      "content": "for differences between the local filesystem and the configuration stored in\n\nthe API. If there are differences, the configuration manager downloads the\n\nnew configuration to the local filesystem and signals to the legacy\n\napplication that it should reconfigure itself with this new configuration. The\n\nactual mechanism for this notification varies by application. Some\n\napplications actually watch the configuration file for changes, while others\n\nrespond to a SIGHUP signal. In extreme cases, the configuration manager\n\nmay send a SIGKILL signal to abort the legacy application. Once aborted,\n\nthe container orchestration system will restart the legacy application, at\n\nwhich point it will load its new configuration. As with adding HTTPS to an\n\nexisting application, this pattern illustrates how the sidecar pattern can help\n\nadapt pre-existing applications to more cloud native scenarios.\n\nModular Application Containers\n\nAt this point, you might be forgiven if you thought that the sole reason for\n\nthe sidecar pattern to exist was to adapt legacy applications where you no\n\nlonger wanted to make modifications to the original source code. While this\n\nis a common use case for the pattern, there are many other motivations for\n\ndesigning things using sidecars. One of the other main advantages of using\n\nthe sidecar pattern is modularity and reuse of the components used as\n\nsidecars. In deploying any real-world, reliable application, there is\n\nfunctionality that you need for debugging or other management of the",
      "content_length": 1519,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 58,
      "content": "application, such as giving a readout of all of the different processes using\n\nresources in the container, similar to the top command-line tool.\n\nOne approach to providing this introspection is to require that each\n\ndeveloper implement an HTTP /topz interface that provides a readout of\n\nresource usage. To make this easier, you might implement this webhook as\n\na language-specific plug-in that the developer could simply link into their\n\napplication. But even if done this way, the developer would be forced to\n\nchoose to link it in, and your organization would be forced to implement the\n\ninterface for every language it wants to support. Unless done with extreme\n\nrigor, this approach is bound to lead to variations among languages as well\n\nas a lack of support for the functionality when using new languages.\n\nInstead, this topz functionality can be deployed as a sidecar container\n\nthat shares the process-ID (PID) namespace with the application container.\n\nThis topz container can introspect all running processes and provide a\n\nconsistent user interface. Moreover, you can use the orchestration system to\n\nautomatically add this container to all applications deployed via the\n\norchestration system to ensure that there is a consistent set of tools available\n\nfor all applications running in your infrastructure.\n\nOf course, as with any technical choice, there are trade-offs between this\n\nmodular container-based pattern and rolling your own code into your\n\napplication. The library-based approach is always going to be somewhat\n\nless tailored to the specifics of your application. This means that it may be\n\nless efficient in terms of performance, or that the API may require some",
      "content_length": 1688,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 59,
      "content": "adaptation to fit into your environment. I would compare these trade-offs to\n\nthe difference between buying off-the-rack clothing versus bespoke fashion.\n\nThe bespoke fashion will always fit you better, but it will take longer to\n\narrive and cost more to acquire. As with clothes, for most of us it makes\n\nsense to buy the more general-purpose solution when it comes to coding. Of\n\ncourse, if your application demands extremes in terms of performance, you\n\ncan always choose the handwritten solution.\n\nHands On: Deploying the topz Container\n\nTo see the topz sidecar in action, you first need to deploy some other\n\ncontainer to act as the application container. Choose an existing application\n\nthat you are running and deploy it using Docker:\n\n$ docker run -d <my-app-image>\n\n<container-hash-value>\n\nAfter you run that image, you will receive the identifier for that specific\n\ncontainer. It will look something like: cccf82b85000… If you don’t\n\nhave it, you can always look it up using the docker ps command,\n\nwhich will show all currently running containers. Assuming you have\n\nstashed that value in an environment variable named APP_ID , you can\n\nthen run the topz container in the same PID namespace using:",
      "content_length": 1208,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 60,
      "content": "$ docker run --pid=container:${APP_ID} \\\n\np 8080:8080 \\\n\nbrendanburns/topz:db0fa58 \\\n\n/server --addr=0.0.0.0:8080\n\nThis will launch the topz sidecar in the same PID namespace as the\n\napplication container. Note that you may need to change the port that the\n\nsidecar uses for serving if your application container is also running on port\n\n8080 . Once the sidecar is running, you can visit\n\nhttp://localhost:8080/topz to get a complete readout of the processes that\n\nare running in the application container and their resource usage.\n\nYou can mix this sidecar with any other existing container to easily get a\n\nview into how the container is using its resources via a web interface.\n\nBuilding a Simple PaaS with Sidecars\n\nThe sidecar pattern can be used for more than adaptation and monitoring. It\n\ncan also be used as a means to implement the complete logic for your\n\napplication in a simplified, modular manner. As an example, imagine\n\nbuilding a simple platform as a service (PaaS) built around the git\n\nworkflow. Once you deploy this PaaS, simply pushing new code up to a Git\n\nrepository results in that code being deployed to the running servers. We’ll",
      "content_length": 1155,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 61,
      "content": "see how the sidecar pattern makes building this PaaS remarkably\n\nstraightforward.\n\nAs previously stated, in the sidecar pattern there are two containers: the\n\nmain application container and the sidecar. In our simple PaaS application,\n\nthe main container is a Node.js server that implements a web server. The\n\nNode.js server is instrumented so that it automatically reloads the server\n\nwhen new files are updated. This is accomplished with the nodemon tool.\n\nThe sidecar container shares a filesystem with the main application\n\ncontainer and runs a simple loop that synchronizes the filesystem with an\n\nexisting Git repository:\n\n#!/bin/bash\n\nwhile true; do\n\ngit pull\n\nsleep 10\n\ndone\n\nObviously, this script could be more complex, pulling from a specific\n\nbranch instead of simply from HEAD. It is left purposefully simple to\n\nimprove the readability of this example.\n\nThe Node.js application and Git synchronization sidecar are coscheduled\n\nand deployed together to implement our simple PaaS (Figure 3-4). Once",
      "content_length": 1010,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 62,
      "content": "deployed, every time new code is pushed to a Git repository, the code is\n\nautomatically updated by the sidecar and reloaded by the server.\n\nFigure 3-4. A simple sidecar-based PaaS",
      "content_length": 179,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 63,
      "content": "Designing Sidecars for Modularity and\n\nReusability\n\nIn all of the examples of sidecars that we have detailed throughout this\n\nchapter, one of the most important themes is that every one was a modular,\n\nreusable artifact. To be successful, the sidecar should be reusable across a\n\nwide variety of applications and deployments. By achieving modular reuse,\n\nsidecars can dramatically speed up the building of your application.\n\nHowever, this modularity and reusability, just like achieving modularity in\n\nhigh-quality software development, requires focus and discipline. In\n\nparticular, you need to focus on developing three areas:\n\nParameterizing your containers\n\nCreating the API surface of your container\n\nDocumenting the operation of your container\n\nOnce you follow these common approaches, you will develop sidecars that\n\ncan be re-used across a wide variety of applications.\n\nParameterized Containers\n\nParameterizing your containers is the most important thing you can do to\n\nmake your containers modular and reusable regardless of whether they are",
      "content_length": 1051,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 64,
      "content": "sidecars or not, though sidecars and other add-on containers are especially\n\nimportant to parameterize.\n\nWhat do I mean when I say “parameterize”? Consider your container as a\n\nfunction in your program. How many parameters does it have? Each\n\nparameter represents an input that can customize a generic container to a\n\nspecific situation. Consider, for example, the SSL add-on sidecar deployed\n\npreviously. To be generally useful, it likely needs at least two parameters:\n\nthe first is the name of the certificate being used to provide SSL, and the\n\nother is the port of the “legacy” application server running on localhost.\n\nWithout these parameters, it is hard to imagine this sidecar container being\n\nusable for a broad array of applications. Similar parameters exist for all of\n\nthe other sidecars described in this chapter.\n\nNow that we know the parameters we want to expose, how do we actually\n\nexpose them to users, and how do we consume them inside the container.\n\nThere are two ways in which such parameters can be passed to your\n\ncontainer: through environment variables or the command line. Though\n\neither is feasible, I have a general preference for passing parameters via\n\nenvironment variables. An example of passing such parameters to a sidecar\n\ncontainer is:\n\ndocker run -e=PORT=<port> -d <image>",
      "content_length": 1311,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 65,
      "content": "Of course, delivering values into the container is only part of the battle. The\n\nother part is actually using these variables inside the container. Typically, to\n\ndo that, a simple shell script is used that loads the environment variables\n\nsupplied with the sidecar container and either adjusts the configuration files\n\nor parameterizes the underlying application.\n\nFor example, you might pass in the certificate path and port as environment\n\nvariables:\n\ndocker run -e=PROXY_PORT=8080 -e=CERTIFICATE_PATH\n\nIn your container, you would use those variables to configure an\n\nnginx.conf file that points the web server to the correct file and proxy\n\nlocation.\n\nDefine Each Container’s API\n\nGiven that you are parameterizing your containers, it is obvious that your\n\ncontainers are defining a “function” that is called whenever the container is\n\nexecuted. This function is clearly a part of the API that is defined by your\n\ncontainer, but there are other parts to this API as well, including calls that\n\nthe container will make to other services as well as traditional HTTP or\n\nother APIs that the container provides.",
      "content_length": 1112,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 66,
      "content": "As you think about defining modular, reusable containers, it is important to\n\nrealize that all aspects of how your container interacts with its world are\n\npart of the API defined by that reusable container. As in the world of\n\nmicroservices, these microcontainers rely on APIs to ensure that there is a\n\nclean separation between the main application container and the sidecar.\n\nAdditionally, the API exists to ensure that all consumers of the sidecar will\n\ncontinue to work correctly as subsequent versions are released. Likewise,\n\nhaving a clean API for a sidecar enables the sidecar developer to move\n\nmore quickly since they have a clear definition (and hopefully unit tests) for\n\nthe services they provide as a part of the sidecar.\n\nTo see a concrete example of why this API surface area is important,\n\nconsider the configuration management sidecar we discussed earlier. A\n\nuseful configuration for this sidecar might be UPDATE_FREQUENCY ,\n\nwhich indicates how often the configuration should be synchronized with\n\nthe filesystem. It is clear that if, at some later time, the name of the\n\nparameter is changed to UPDATE_PERIOD , this change would be a\n\nviolation of the sidecar’s API and clearly would break it for some users.\n\nWhile that example is obvious, even more subtle changes can break the\n\nsidecar API. Imagine, for example, that UPDATE_FREQUENCY originally\n\ntook a number in seconds. Over time and with feedback from users, the\n\nsidecar developer determined that specifying seconds for large time values\n\n(e.g., minutes) was annoying users and changed the parameter to accept\n\nstrings (10 m, 5 s, etc.). Because old parameter values (e.g., 10, for 10",
      "content_length": 1663,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 67,
      "content": "seconds) won’t parse in this new scheme, this is a breaking API change.\n\nSuppose still that the developer anticipated this but made values without\n\nunits parse to milliseconds where they had previously parsed to seconds.\n\nEven this change, despite not leading to an error, is a breaking API change\n\nfor the sidecar since it will lead to significantly more frequent configuration\n\nchecks and correspondingly more load on the cloud configuration server.\n\nI hope this discussion has shown you that for true modularity, you need to\n\nbe very conscious of the API that your sidecar provides, and that “breaking”\n\nchanges to that API may not always be as obvious as changing the name of\n\na parameter.\n\nDocumenting Your Containers\n\nBy now, you’ve seen how you can parameterize your sidecar containers to\n\nmake them modular and reuseable. You’ve learned about the importance of\n\nmaintaining a stable API to ensure that you don’t break sidecars for your\n\nusers. But there’s one final step to building modular, reusable containers:\n\nensuring that people can use them in the first place.\n\nAs with software libraries, the key to building something truly useful is\n\nexplaining how to use it. There is little good in building a flexible, reliable\n\nmodular container if no one can figure out how to use it. Sadly, there are\n\nfew formal tools available for documenting container images, but there are\n\nsome best practices you can use to accomplish this.",
      "content_length": 1436,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 68,
      "content": "For every container image, the most obvious place to look for\n\ndocumentation is the Dockerfile from which the container was built.\n\nThere are some parts of the Dockerfile that already document how the\n\ncontainer works. One example of this is the EXPOSE directive, which\n\nindicates the ports that the image listens on. Even though EXPOSE is not\n\nnecessary, it is a good practice to include it in your Dockerfile and\n\nalso to add a comment that explains what exactly is listening on that port.\n\nFor example:\n\n...\n\n# Main web server runs on port 8080\n\nEXPOSE 8080\n\n...\n\nAdditionally, if you use environment variables to parameterize your\n\ncontainer, you can use the ENV directive to set default values for those\n\nparameters as well as document their usage:\n\n...\n\n# The PROXY_PORT parameter indicates the port on # traffic to.\n\nENV PROXY_PORT 8000\n\n...",
      "content_length": 848,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 69,
      "content": "Finally, you should always use the LABEL directive to add metadata for\n\nyour image; for example, the maintainer’s email address, web page, and\n\nversion of the image:\n\n...\n\nLABEL \"org.opencontainers.image.vendor\"=\"name@com\n\nLABEL \"org.opencontainers.image.url\"=\"http://imag LABEL \"org.opencontainers.image.version\"=\"1.0.3\"\n\n...\n\nThe names for these labels are drawn from the schema established by the\n\nOpen Containers Initiative. The Open Containers Initiative (OCI) is the\n\nspecification body which maintains the definition of what it means to be a\n\ncontainer image. A part of that specification is the definition of a shared set\n\nof annotations/labels. By using a common taxonomy of image labels,\n\nmultiple different tools can rely on the same meta information in order to\n\nvisualize, monitor, and correctly use an application. By adopting shared\n\nterms, you can use the set of tools developed in the community without\n\nmodifying your image. Of course, you can also add whatever additional\n\nlabels make sense in the context of your image.",
      "content_length": 1039,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 70,
      "content": "Summary\n\nOver the course of this chapter, we’ve introduced the sidecar pattern for\n\ncombining containers on a single machine. In the sidecar pattern, a sidecar\n\ncontainer augments and extends an application container to add\n\nfunctionality. Sidecars can be used to update existing legacy applications\n\nwhen changing the application is too costly. Likewise, they can be used to\n\ncreate modular utility containers that standardize implementations of\n\ncommon functionality. These utility containers can be reused in a large\n\nnumber of applications, increasing consistency and reducing the cost of\n\ndeveloping each application. Subsequent chapters introduce other single-\n\nnode patterns that demonstrate other uses for modular, reusable containers.\n\nOceanofPDF.com",
      "content_length": 759,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 71,
      "content": "Chapter 4. Ambassadors\n\nChapter 3 introduced the sidecar pattern, where one container augments a\n\npre-existing container to add functionality. This chapter introduces the\n\nambassador pattern, where an ambassador container brokers interactions\n\nbetween the application container and the rest of the world. As with other\n\nsingle-node patterns, the two containers are tightly linked in a symbiotic\n\npairing that is scheduled to a single machine. A canonical diagram of this\n\npattern is shown in Figure 4-1.\n\nFigure 4-1. Generic ambassador pattern\n\nThe value of the ambassador pattern is twofold. First, as with the other\n\nsingle-node patterns, there is inherent value in building modular, reusable\n\ncontainers. The separation of concerns makes the containers easier to build\n\nand maintain. Likewise, the ambassador container can be reused with a\n\nnumber of different application containers. This reuse speeds up application\n\ndevelopment because the container’s code can be reused in a number of",
      "content_length": 991,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 72,
      "content": "places. Additionally, the implementation is both more consistent and of a\n\nhigher quality because it is built once and used in many different contexts.\n\nThe rest of this chapter provides a number of examples of using the\n\nambassador pattern to implement a series of real-world applications.\n\nUsing an Ambassador to Shard a Service\n\nSometimes the data that you want to store in a storage layer becomes too\n\nbig for a single machine to handle. In such situations, you need to shard\n\nyour storage layer. Sharding splits up the layer into multiple disjoint pieces,\n\neach hosted by a separate machine. This chapter focuses on a single-node\n\npattern for adapting an existing service to talk to a sharded service that\n\nexists somewhere in the world. It does not discuss how the sharded service\n\ncame to exist. Sharding and a multinode sharded service design pattern are\n\ndiscussed in great detail in Chapter 7. A diagram of a sharded service is\n\nshown in Figure 4-2.",
      "content_length": 959,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 73,
      "content": "Figure 4-2. A generic sharded service\n\nWhen deploying a sharded service, one question that arises is how to\n\nintegrate it with the frontend or middleware code that stores data. Clearly\n\nthere needs to be logic that routes a particular request to a particular shard,\n\nbut often it is difficult to retrofit such a sharded client into existing source\n\ncode that expects to connect to a single storage backend. Additionally,\n\nsharded services make it difficult to share configuration between\n\ndevelopment environments (where there is often only a single storage\n\nshard) and production environments (where there are often many storage\n\nshards).\n\nOne approach is to build all of the sharding logic into the sharded service\n\nitself. In this approach, the sharded service also has a stateless load balancer\n\nthat directs traffic to the appropriate shard. Effectively, this load balancer is",
      "content_length": 881,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 74,
      "content": "a distributed ambassador as a service. This makes a client-side ambassador\n\nunnecessary at the expense of a more complicated deployment for the\n\nsharded service. The alternative is to integrate a single-node ambassador on\n\nthe client side to route traffic to the appropriate shard. This makes\n\ndeploying the client somewhat more complicated but simplifies the\n\ndeployment of the sharded service. As is always the case with trade-offs, it\n\nis up to the particulars of your specific application to determine which\n\napproach makes the most sense. Some factors to consider include where\n\nteam lines fall in your architecture, as well as where you are writing code\n\nversus simply deploying off-the-shelf software. Ultimately, either choice is\n\nvalid. The section “Hands On: Implementing a Sharded Redis” describes\n\nhow to use the single-node ambassador pattern for client-side sharding.\n\nWhen adapting an existing application to a sharded backend, you can\n\nintroduce an ambassador container that contains all of the logic needed to\n\nroute requests to the appropriate storage shard. Thus, your frontend or\n\nmiddleware application only connects to what appears to be a single storage\n\nbackend running on localhost. However, this server is in fact actually a\n\nsharding ambassador proxy, which receives all of the requests from your\n\napplication code, sends a request to the appropriate storage shard, and then\n\nreturns the result to your application. This use of an ambassador is\n\ndiagrammed in Figure 4-3.\n\nThe net result of applying the ambassador pattern to sharded services is a\n\nseparation of concerns between the application container, which simply",
      "content_length": 1646,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 75,
      "content": "knows it needs to talk to a storage service and discovers that service on\n\nlocalhost, and the sharding ambassador proxy, which only contains the code\n\nnecessary to perform appropriate sharding. As with all good single-node\n\npatterns, this ambassador can be reused between many different\n\napplications. Or, as we’ll see in the following hands-on example, an off-the\n\nshelf open source implementation can be used for the ambassador, speeding\n\nup the development of the overall distributed system.\n\nHands On: Implementing a Sharded Redis\n\nRedis is a fast key-value store that can be used as a cache or for more\n\npersistent storage. In this example, we’ll be using it as a cache. We’ll begin\n\nby deploying a sharded Redis service to a Kubernetes cluster. We’ll use the\n\nStatefulSet API object to deploy it, since it will give us unique DNS\n\nnames for each shard that we can use when configuring the proxy.\n\nThe StatefulSet for Redis looks like this:\n\napiVersion: apps/v1beta1 kind: StatefulSet\n\nmetadata:\n\nname: sharded-redis\n\nspec: serviceName: \"redis\"\n\nreplicas: 3\n\ntemplate:",
      "content_length": 1073,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 76,
      "content": "metadata: labels:\n\napp: redis\n\nspec: terminationGracePeriodSeconds: 10\n\ncontainers:\n\nname: redis\n\nimage: redis ports:\n\ncontainerPort: 6379\n\nname: redis\n\nSave this to a file named redis-shards.yaml, and you can deploy this with\n\nkubectl create -f redis-shards.yaml . This will create three\n\ncontainers running redis. You can see these by running kubectl get\n\npods ; you should see sharded-redis-[0,1,2] .\n\nOf course, just running the replicas isn’t sufficient; we also need names by\n\nwhich we can refer to the replicas. In this case, we’ll use a Kubernetes\n\nService , which will create DNS names for the replicas we have created.\n\nThe Service looks like this:\n\napiVersion: v1\n\nkind: Service metadata:\n\nname: redis",
      "content_length": 712,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 77,
      "content": "labels: app: redis\n\nspec:\n\nports: - port: 6379\n\nname: redis\n\nclusterIP: None\n\nselector: app: redis\n\nSave this to a file named redis-service.yaml and deploy with kubectl\n\ncreate -f redis-service.yaml . You should now have DNS\n\nentries for sharded-redis-0.redis , sharded-redis-\n\n1.redis , etc. We can use these names to configure twemproxy .\n\ntwemproxy is a lightweight, highly performant proxy for memcached\n\nand Redis , which was originally developed by Twitter and is open source\n\nand available on GitHub. We can configure twemproxy to point to the\n\nreplicas we created by using the following configuration:\n\nredis: listen: 127.0.0.1:6379\n\nhash: fnv1a_64\n\ndistribution: ketama\n\nauto_eject_hosts: true\n\nredis: true\n\ntimeout: 400",
      "content_length": 729,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 78,
      "content": "server_retry_timeout: 2000\n\nserver_failure_limit: 1\n\nservers:\n\nsharded-redis-0.redis:6379:1\n\nsharded-redis-1.redis:6379:1\n\nsharded-redis-2.redis:6379:1\n\nIn this config, you can see that we are serving the Redis protocol on\n\nlocalhost:6379 so that the application container can access the\n\nambassador. We will deploy this into our ambassador pod using a\n\nKubernetes ConfigMap object that we can create with:\n\nkubectl create configmap twem-config --from-file=\n\nFinally, all of the preparations are done and we can deploy our ambassador\n\nexample. We define a pod that looks like:\n\napiVersion: v1 kind: Pod\n\nmetadata: name: ambassador-example\n\nspec: containers: # This is where the application container wou # - name: nginx",
      "content_length": 719,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 79,
      "content": "# image: nginx\n\n# This is the ambassador container - name: twemproxy\n\nimage: ganomede/twemproxy\n\ncommand:\n\n\"nutcracker\"\n\n\"-c\" - \"/etc/config/nutcracker.yaml\"\n\n\"-v\"\n\n\"7\"\n\n\"-s\"\n\n\"6222\"\n\nvolumeMounts:\n\nname: config-volume\n\nmountPath: /etc/config\n\nvolumes:\n\nname: config-volume\n\nconfigMap:\n\nname: twem-config\n\nThis pod defines the ambassador; then the specific user’s application\n\ncontainer can be injected to complete the pod.\n\nUsing an Ambassador for Service",
      "content_length": 456,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 80,
      "content": "Brokering\n\nWhen trying to render an application portable across multiple environments\n\n(e.g., public cloud, physical data center, or private cloud), one of the\n\nprimary challenges is service discovery and configuration. To understand\n\nwhat this means, imagine a frontend that relies on a MySQL database to\n\nstore its data. In the public cloud, this MySQL service might be provided as\n\nsoftware as a service (SaaS), whereas in a private cloud it might be\n\nnecessary to dynamically spin up a new virtual machine or container\n\nrunning MySQL.\n\nConsequently, building a portable application requires that the application\n\nknow how to introspect its environment and find the appropriate MySQL\n\nservice to connect to. This process is called service discovery, and the\n\nsystem that performs this discovery and linking is commonly called a\n\nservice broker. As with previous examples, the ambassador pattern enables\n\na system to separate the logic of the application container from the logic of\n\nthe service broker ambassador. The application simply always connects to\n\nan instance of the service (e.g., MySQL) running on localhost. It is the\n\nresponsibility of the service broker ambassador to introspect its\n\nenvironment and broker the appropriate connection. This process is shown\n\nin Figure 4-3.",
      "content_length": 1289,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 81,
      "content": "Figure 4-3. A service broker ambassador creating a MySQL service\n\nUsing an Ambassador to Do\n\nExperimentation or Request Splitting\n\nA final example application of the ambassador pattern is to perform\n\nexperimentation or other forms of request splitting. In many production\n\nsystems, it is advantageous to be able to perform request splitting, where\n\nsome fraction of all requests are not serviced by the main production\n\nservice but rather are redirected to a different implementation of the service.\n\nMost often, this is used to perform experiments with new beta versions of\n\nthe service to determine if the new version of the software is reliable or\n\ncomparable in performance to the currently deployed version.",
      "content_length": 712,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 82,
      "content": "Additionally, request splitting is sometimes used to tee or split traffic\n\nsuch that all traffic goes to both the production system as well as a newer,\n\nundeployed version. The responses from the production system are returned\n\nto the user, while the responses from the tee-d service are ignored. Most\n\noften, this form of request splitting is used to simulate production load on\n\nthe new version of the service without risking impact to existing production\n\nusers.\n\nGiven the previous examples, it is straightforward to see how a request-\n\nsplitting ambassador can interact with an application container to\n\nimplement request splitting. As before, the application container simply\n\nconnects to the service on localhost, while the ambassador container\n\nreceives the requests, proxies responses to both the production and\n\nexperimental systems, and then returns the production responses back as if\n\nit had performed the work itself.\n\nThis separation of concerns keeps the code in each container slim and\n\nfocused, and the modular factoring of the application ensures that the\n\nrequest-splitting ambassador can be reused for a variety of different\n\napplications and settings.\n\nHands On: Implementing 10% Experiments\n\nTo implement our request-splitting experiment, we’re going to use the\n\nnginx web server. nginx is a powerful, richly featured open source",
      "content_length": 1352,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 83,
      "content": "server. To configure nginx as the ambassador, we’ll use the following\n\nconfiguration (note that this is for HTTP but it could easily be adapted for\n\nHTTPS as well):\n\nworker_processes 5;\n\nerror_log error.log;\n\npid nginx.pid;\n\nworker_rlimit_nofile 8192;\n\nevents {\n\nworker_connections 1024;\n\n}\n\nhttp {\n\nupstream backend { ip_hash;\n\nserver web weight=9;\n\nserver experiment;\n\n}\n\nserver {\n\nlisten localhost:80;\n\nlocation / {\n\nproxy_pass http://backend;\n\n}\n\n}\n\n}",
      "content_length": 455,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 84,
      "content": "NOTE\n\nAs with the previous discussion of sharded services, it’s also possible to deploy the experiment\n\nframework as a separate microservice in front of your application instead of integrating it as a part of\n\nyour client pods. Of course, by doing this you are introducing another service that needs to be\n\nmaintained, scaled, monitored, etc. If experimentation is likely to be a longstanding component in\n\nyour architecture, this might be worthwhile. If it is used more occasionally, then a client-side\n\nambassador might make more sense.\n\nYou’ll note that I’m using IP hashing in this configuration. This is important\n\nbecause it ensures that the user doesn’t flip-flop back and forth between the\n\nexperiment and the main site. This assures that every user has a consistent\n\nexperience with the application.\n\nThe weight parameter is used to send 90% of the traffic to the main\n\nexisting application, while 10% of the traffic is redirected to the\n\nexperiment.\n\nAs with other examples, we’ll deploy this configuration as a ConfigMap\n\nobject in Kubernetes:\n\nkubectl create configmap experiment-config --from\n\nOf course, this assumes that you have both a web and experiment\n\nservice defined. If you don’t, you need to create them now before you try to\n\ncreate the ambassador container, since nginx doesn’t like to start if it",
      "content_length": 1322,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 85,
      "content": "can’t find the services it is proxying to. Here are some example service\n\nconfigs:\n\n# This is the 'experiment' service\n\napiVersion: v1\n\nkind: Service\n\nmetadata: name: experiment\n\nlabels:\n\napp: experiment\n\nspec:\n\nports:\n\nport: 80\n\nname: web\n\nselector:\n\n# Change this selector to match your applicat\n\napp: experiment\n\n---\n\n# This is the 'prod' service\n\napiVersion: v1 kind: Service\n\nmetadata: name: web\n\nlabels: app: web spec:\n\nports:\n\nport: 80",
      "content_length": 442,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 86,
      "content": "name: web\n\nselector: # Change this selector to match your applicat\n\napp: web\n\nAnd then we will deploy nginx itself as the ambassador container within\n\na pod:\n\napiVersion: v1\n\nkind: Pod\n\nmetadata:\n\nname: experiment-example\n\nspec:\n\ncontainers:\n\n# This is where the application container wou\n\n# - name: some-name\n\n# image: some-image\n\n# This is the ambassador container\n\nname: nginx\n\nimage: nginx volumeMounts:\n\nname: config-volume mountPath: /etc/nginx volumes:\n\nname: config-volume",
      "content_length": 480,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 87,
      "content": "configMap:\n\nname: experiment-config\n\nYou can add a second (or third, or fourth) container to the pod to take\n\nadvantage of the ambassador.\n\nSummary\n\nAmbassadors are a key way to simplify life for application developers.\n\nThey can encapsulate complex logic that is necessary for scale or reliability,\n\nsuch as sharding, and provide a simplified interface which makes such a\n\ncomplex system easy to use. For platform engineers, ambassadors can be an\n\nimportant tool in constructing a powerful platform that is easy to use.\n\nOceanofPDF.com",
      "content_length": 536,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 88,
      "content": "Chapter 5. Adapters\n\nIn the preceding chapters, we saw how the sidecar pattern can extend and\n\naugment existing application containers. We also saw how ambassadors can\n\nalter and broker how an application container communicates with the\n\nexternal world. This chapter describes the final single-node pattern: the\n\nadapter pattern. In the adapter pattern, the adapter container is used to\n\nmodify the interface of the application container so that it conforms to\n\nsome predefined interface that is expected of all applications. For example,\n\nan adapter might ensure that an application implements a consistent\n\nmonitoring interface. Or it might ensure that log files are always written to\n\nstdout or any number of other conventions.\n\nReal-world application development is a heterogeneous, hybrid exercise.\n\nSome parts of your application might be written from scratch by your team,\n\nsome supplied by vendors, and some might consist entirely of off-the-shelf\n\nopen source or proprietary software that you consume as precompiled\n\nbinary. The net effect of this heterogeneity is that any real-world application\n\nyou deploy will have been written in a variety of languages, with a variety\n\nof conventions for logging, monitoring, and other common services.\n\nYet, to effectively monitor and operate your application, you need common\n\ninterfaces. When each application provides metrics using a different format\n\nand interface, it is very difficult to collect all of those metrics in a single",
      "content_length": 1483,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 89,
      "content": "place for visualization and alerting. This is where the adapter pattern is\n\nrelevant. Like other single-node patterns, the adapter pattern is made up of\n\nmodular containers. Different application containers can present many\n\ndifferent monitoring interfaces while the adapter container adapts this\n\nheterogeneity to present a consistent interface. This enables you to deploy a\n\nsingle tool that expects this single interface. Figure 5-1 illustrates this\n\ngeneral pattern.\n\nFigure 5-1. The generic adapter pattern\n\nThe remainder of this chapter gives several different applications of the\n\nadapter pattern.\n\nMonitoring\n\nWhen monitoring your software, you want a single solution that can\n\nautomatically discover and monitor any application that is deployed into\n\nyour environment. To make this feasible, every application has to\n\nimplement the same monitoring interface. There are numerous examples of\n\nstandardized monitoring interfaces, such as syslog , Event Tracing for",
      "content_length": 970,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 90,
      "content": "Windows (ETW), JMX for Java applications, and many, many other\n\nprotocols and interfaces. However, each of these is unique in both protocol\n\nfor communication as well as the style of communication (push versus\n\npull).\n\nSadly, applications in your distributed system are likely to span the gamut\n\nfrom code that you have written yourself to off-the-shelf open source\n\ncomponents. As a result, you will find yourself with a wide range of\n\ndifferent monitoring interfaces that you need to integrate into a single well-\n\nunderstood system.\n\nFortunately, most monitoring solutions understand that they need to be\n\nwidely applicable, and thus they have implemented a variety of plug-ins\n\nthat can adapt one monitoring format to a common interface. Given this set\n\nof tools, how can we deploy and manage our applications in an agile and\n\nstable manner? Fortunately, the adapter pattern can provide us with the\n\nanswers. Applying the adapter pattern to monitoring, we see that the\n\napplication container is simply the application that we want to monitor. The\n\nadapter container contains the tools for transforming the monitoring\n\ninterface exposed by the application container into the interface expected by\n\nthe general-purpose monitoring system.\n\nDecoupling the system in this fashion makes for a more comprehensible,\n\nmaintainable system. Rolling out new versions of the application doesn’t\n\nrequire a rollout of the monitoring adapter. Additionally, the monitoring",
      "content_length": 1460,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 91,
      "content": "container can be reused with multiple different application containers. The\n\nmonitoring container may even have been supplied by the monitoring\n\nsystem maintainers independent of the application developers. Finally,\n\ndeploying the monitoring adapter as a separate container ensures that each\n\ncontainer gets its own dedicated resources in terms of both CPU and\n\nmemory. This ensures that a misbehaving monitoring adapter cannot cause\n\nproblems with a user-facing service.\n\nHands On: Using Prometheus for Monitoring\n\nAs an example, consider monitoring your containers via the Prometheus\n\nopen source project. Prometheus is a monitoring aggregator, which collects\n\nmetrics and aggregates them into a single time-series database. On top of\n\nthis database, Prometheus provides visualization and query language for\n\nintrospecting the collected metrics. To collect metrics from a variety of\n\ndifferent systems, Prometheus expects every container to expose a specific\n\nmetrics API. This enables Prometheus to monitor a wide variety of\n\ndifferent programs through a single interface.\n\nHowever, many popular programs, such as the Redis key-value store, do not\n\nexport metrics in a format that is compatible with Prometheus.\n\nConsequently, the adapter pattern is quite useful for taking an existing\n\nservice like Redis and adapting it to the Prometheus metrics-collection\n\ninterface.",
      "content_length": 1373,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 92,
      "content": "Consider a simple Kubernetes pod definition for a Redis server:\n\napiVersion: v1\n\nkind: Pod\n\nmetadata: name: adapter-example\n\nnamespace: default\n\nspec:\n\ncontainers: - image: redis\n\nname: redis\n\nAt this point, this container is not capable of being monitored by\n\nPrometheus because it does not export the right interface. However, if we\n\nsimply add an adapter container (in this case, an open source Prometheus\n\nexporter), we can modify this pod to export the correct interface and thus\n\nadapt it to fit Prometheus’s expectations:\n\napiVersion: v1\n\nkind: Pod\n\nmetadata: name: adapter-example\n\nnamespace: default spec: containers: - image: redis",
      "content_length": 641,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 93,
      "content": "name: redis\n\n# Provide an adapter that implements the Prometh - image: oliver006/redis_exporter\n\nname: adapter\n\nThis example illustrates not only the value of the adapter pattern for\n\nensuring a consistent interface, but also the value of container patterns in\n\ngeneral for modular container reuse. In this case, the example shown\n\ncombines an existing Redis container with an existing Prometheus adapter.\n\nThe net effect is a monitorable Redis server, with little work on our part to\n\ndeploy it. In the absence of the adapter pattern, the same deployment would\n\nhave required significantly more custom work and would have resulted in a\n\nmuch less operable solution, since any updates to either Redis or the adapter\n\nwould have required work to apply the update.\n\nLogging\n\nMuch like monitoring, there is a wide variety of heterogeneity in how\n\nsystems log data to an output stream. Systems might divide their logs into\n\ndifferent levels (such as debug, info, warning, and error) with each level\n\ngoing into a different file. Some might simply log to stdout and\n\nstderr . This is especially problematic in the world of containerized\n\napplications, where there is a general expectation that your containers will",
      "content_length": 1209,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 94,
      "content": "log to stdout , because that is what is available via commands like\n\ndocker logs or kubectl logs .\n\nAdding further complexity, the information logged generally has structured\n\ninformation (e.g., the date/time of the log), but this information varies\n\nwidely between different logging libraries (e.g., Java’s built-in logging\n\nversus the glog package for Go).\n\nOf course, when you are storing and querying the logs for your distributed\n\nsystem, you don’t really care about these differences in logging format. You\n\nwant to ensure that despite different structures for the data, every log ends\n\nup with the appropriate timestamp.\n\nFortunately, as with monitoring, the adapter pattern can help provide a\n\nmodular, re-usable design for both of these situations. While the application\n\ncontainer may log to a file, the adapter container can redirect that file to\n\nstdout . Different application containers can log information in different\n\nformats, but the adapter container can transform that data into a single\n\nstructured representation that can be consumed by your log aggregator.\n\nAgain, the adapter is taking a heterogeneous world of applications and\n\ncreating a homogenous world of common interfaces.",
      "content_length": 1202,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 95,
      "content": "NOTE\n\nOne question that often comes up when considering adapter patterns is: why not simply modify the\n\napplication container itself? If you are the developer responsible for the application container, then\n\nthis might actually be a good solution. Adapting your code or your container to implement a\n\nconsistent interface can work well. However, in many cases we are reusing a container produced by\n\nanother party. In such cases, deriving a slightly modified image that we have to maintain (patch,\n\nrebase, etc.) is significantly more expensive than developing an adapter container that can run\n\nalongside the other party’s image. Additionally, decoupling the adapter into its own container allows\n\nfor the possibility of sharing and reuse, which isn’t possible when you modify the application\n\ncontainer.\n\nHands On: Normalizing Different Logging\n\nFormats with fluentd\n\nOne common task for an adapter is to normalize log metrics into a standard\n\nset of events. Many different applications have different output formats, but\n\nyou can use a standard logging tool deployed as an adapter to normalize\n\nthem all to a consistent format. In this example, we will use the fluentd\n\nmonitoring agent as well as some community-supported plug-ins to obtain\n\nlogs from a variety of different sources.\n\nfluentd is one of the more popular open source logging agents\n\navailable. One of its major features is a rich set of community-supported\n\nplug-ins that enable a great deal of flexibility in monitoring a variety of\n\napplications.",
      "content_length": 1517,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 96,
      "content": "The first application that we will monitor is Redis. Redis is a popular key-\n\nvalue store; one of the commands it offers is the SLOWLOG command.\n\nThis command lists recent queries that exceeded a particular time interval.\n\nSuch information is quite useful in debugging your application’s\n\nperformance. Unfortunately, SLOWLOG is only available as a command on\n\nthe Redis server, which means that it is difficult to use retrospectively if a\n\nproblem happens when someone isn’t available to debug the server. To fix\n\nthis limitation, we can use fluentd and the adapter pattern to add slow-\n\nquery logging to Redis.\n\nTo do this, we use the adapter pattern with a redis container as the main\n\napplication container, and the fluentd container as our adapter\n\ncontainer. In this case, we will also use the fluent-plugin-redis-\n\nslowlog fluentd plug-in to listen to the slow queries. We can\n\nconfigure this plug-in by using the following snippet:\n\n<source>\n\ntype redis_slowlog\n\nhost localhost\n\nport 6379 tag redis.slowlog\n\n</source>\n\nBecause we are using an adapter and the containers both share a network\n\nnamespace, configuring the logging simply uses localhost and the",
      "content_length": 1163,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 97,
      "content": "default Redis port (6379). Given this application of the adapter pattern,\n\nlogging will always be available whenever we want to debug slow Redis\n\nqueries.\n\nA similar exercise can be done to monitor logs from the Apache Storm\n\nsystem. Again, Storm provides data via a RESTful API, which is useful but\n\nhas limitations if we are not currently monitoring the system when a\n\nproblem occurs. Like Redis, we can use a fluentd adapter to transform\n\nthe Storm process into a time series of queryable logs. To do this, we deploy\n\na fluentd adapter with the fluent-plugin-storm plug-in\n\nenabled. We can configure this plug-in with a fluentd config pointed at\n\nlocalhost (because again, we are running as a container group with a shared\n\nlocalhost); the config for the plug-in looks like:\n\n<source>\n\ntype storm tag storm\n\nurl http://localhost:8080\n\nwindow 600\n\nsys 0\n\n</source>\n\nThis adapter builds a bridge between Storm and the time series of logs.",
      "content_length": 939,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 98,
      "content": "Adding a Health Monitor\n\nOne last example of applying the adapter pattern is derived from monitoring\n\nthe health of an application container. Consider the task of monitoring the\n\nhealth of an off-the-shelf database container. In this case, the container for\n\nthe database is supplied by the database project, and we would rather not\n\nmodify that container simply to add health checks. Of course, a container\n\norchestrator will allow us to add simple health checks to ensure that the\n\nprocess is running and that it is listening on a particular port, but what if we\n\nwant to add richer health checks that actually run queries against the\n\ndatabase?\n\nContainer orchestration systems like Kubernetes enable us to use shell\n\nscripts as health checks as well. Given this capability, we can write a rich\n\nshell script that runs a number of different diagnostic queries against the\n\ndatabase to determine its health. But where can we store such a script and\n\nhow can we version it?\n\nThe answer to these problems should be easy to guess by now: we can use\n\nan adapter container. The database runs in the application container and\n\nshares a network interface with the adapter container. The adapter container\n\nis a simple container that only contains the shell script for determining the\n\nhealth of the database. This script can then be set up as the health check for\n\nthe database container and can perform whatever rich health checks our",
      "content_length": 1430,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 99,
      "content": "application requires. If these checks ever fail, the database will be\n\nautomatically restarted.\n\nHands On: Adding Rich Health Monitoring for MySQL\n\nSuppose, then, that you want to add deep monitoring on a MySQL database\n\nwhere you actually run a query that is representative of your workload. In\n\nthis case, one option would be to update the MySQL container to contain a\n\nhealth check that is specific to your application. However, this is generally\n\nan unattractive idea because it requires that you both modify some existing\n\nMySQL base image as well as update that image as new MySQL images are\n\nreleased.\n\nUsing the adapter pattern is a much more attractive approach to adding\n\nhealth checks to your database container. Instead of modifying the existing\n\nMySQL container, you can add an additional adapter container to the\n\npreexisting MySQL container, which runs the appropriate query to test the\n\ndatabase health. Given that this adapter container implements the expected\n\nHTTP health check, it is simply a case of defining the MySQL database\n\nprocess’s health check in terms of the interface exposed by this database\n\nadapter.\n\nThe source code for this adapter is relatively straightforward and looks like\n\nthis in Go (though clearly other language implementations are possible as",
      "content_length": 1287,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 100,
      "content": "well):\n\npackage main\n\nimport (\n\n\"database/sql\" \"flag\"\n\n\"fmt\"\n\n\"net/http\"\n\n_ \"github.com/go-sql-driver/mysql\"\n\n)\n\nvar (\n\nuser = flag.String(\"user\", \"\", \"The dat\n\npasswd = flag.String(\"password\", \"\", \"The\n\ndb = flag.String(\"database\", \"\", \"The\n\nquery = flag.String(\"query\", \"\", \"The te\n\naddr = flag.String(\"address\", \"localhos\n\n\"The address to liste\n\n)\n\n// Basic usage: // db-check --query=\"SELECT * from my-cool-tabl // --user=bdburns \\\n\n// --passwd=\"you wish\"\n\n//\n\nfunc main() {",
      "content_length": 478,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 101,
      "content": "flag.Parse()\n\ndb, err := sql.Open(\"localhost\", fmt.Sprintf(\"%s:%s@/%\n\nif err != nil {\n\nfmt.Printf(\"Error opening databas\n\n}\n\n// Simple web handler that runs the query\n\nhttp.HandleFunc(\"\", func(res http.Respons\n\n_, err := db.Exec(*query)\n\nif err != nil {\n\nres.WriteHeader(http.Stat\n\nres.Write([]byte(err.Erro\n\nreturn\n\n}\n\nres.WriteHeader(http.StatusOK)\n\nres.Write([]byte(\"OK\"))\n\nreturn\n\n})\n\n// Startup the server\n\nhttp.ListenAndServe(*addr, nil)\n\n}\n\nWe can then build this into a container image and pull it into a pod that\n\nlooks like:",
      "content_length": 534,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 102,
      "content": "apiVersion: v1\n\nkind: Pod\n\nmetadata: name: adapter-example-health\n\nnamespace: default\n\nspec:\n\ncontainers:\n\nimage: mysql\n\nname: mysql\n\nimage: brendanburns/mysql-adapter\n\nname: adapter\n\nThat way, the mysql container is unchanged, but the desired feedback\n\nabout the health of the MySQL server can still be obtained from the adapter\n\ncontainer.\n\nWhen looking at this application of the adapter pattern, it may seem like\n\napplying the pattern is superfluous. Clearly we could have built our own\n\ncustom image that knew how to health check the mysql instance itself.\n\nWhile this is true, this method ignores the strong benefits that derive from\n\nmodularity. If every developer implements their own specific container with\n\nhealth checking built in, there are no opportunities for reuse or sharing.\n\nIn contrast, if we use patterns like the adapter to develop modular solutions\n\ncomprised of multiple containers, the work is inherently decoupled and",
      "content_length": 943,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 103,
      "content": "more easily shared. An adapter that is developed to health check mysql is\n\na module that can be shared and reused by a variety of people. Further,\n\npeople can apply the adapter pattern using this shared health-checking\n\ncontainer, without having deep knowledge of how to health check a\n\nmysql database. Thus the modularity and adapter pattern serve not to just\n\nfacilitate sharing, but also to empower people to take advantage of the\n\nknowledge of others.\n\nSometimes design patterns aren’t just for the developers who apply them,\n\nbut lead to the development of communities that can collaborate and share\n\nsolutions between members of the community as well as the broader\n\ndeveloper ecosystem.\n\nSummary\n\nSometimes the world isn’t perfectly aligned. The components that we need\n\nto use to build our systems can have different interfaces or different\n\nprotocols for interactions like logging, monitoring, and RPCs. Adapters can\n\nbe a critical tool to add consistency to our systems, enabling them to share a\n\nconsistent protocol and way of connecting. Using an adapter is often\n\nsignificantly easier than trying to change the upstream project.\n\nOceanofPDF.com",
      "content_length": 1157,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 104,
      "content": "Part III. Serving Patterns\n\nChapter 5 described patterns for grouping collections of containers that are\n\nscheduled on the same machine. These groups are tightly coupled,\n\nsymbiotic systems. They depend on local, shared resources like disk,\n\nnetwork interface, or inter-process communications. Such collections of\n\ncontainers are important patterns, but they are also building blocks for\n\nlarger systems. Reliability, scalability, and separation of concerns dictate\n\nthat real-world systems are built out of many different components, spread\n\nacross multiple machines. In contrast to single-node patterns, the multinode\n\ndistributed patterns are more loosely coupled. While the patterns dictate\n\npatterns of communication between the components, this communication is\n\nbased on network calls. Furthermore, many calls are issued in parallel, and\n\nsystems coordinate via loose synchronization rather than tight constraints.\n\nRecently, the term microservices has become a common term for describing\n\nmultinode distributed software architectures. Microservices describe a\n\nsystem built out of many different components running in different\n\nprocesses and communicating over defined APIs. Microservices stand in\n\ncontrast to monolithic systems, which tend to place all of the functionality\n\nfor a service within a single, tightly coordinated application. These two\n\ndifferent architectural approaches are shown in Figures III-1 and III-2.",
      "content_length": 1433,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 105,
      "content": "Figure III-1. A monolithic service with all functions in a single container\n\nFigure III-2. A microservice architecture with each function broken out as a separate microservice\n\nThere are numerous benefits to the microservices approach—most of them\n\nare centered around reliability and agility. Microservices break down an\n\napplication into small pieces, each focused on providing a single service.\n\nThis reduced scope enables each service to be built and maintained by a\n\nsingle “two pizza” team. Reduced team size also reduces the overhead\n\nassociated with keeping a team focused and moving in one direction.",
      "content_length": 609,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 106,
      "content": "Additionally, the introduction of formal APIs in between different\n\nmicroservices decouples the teams from one another and provides a reliable\n\ncontract between the different services. This formal contract reduces the\n\nneed for tight synchronization among the teams because the team providing\n\nthe API understands the surface area that it needs to keep stable, and the\n\nteam consuming the API can rely on a stable service without worrying\n\nabout its details. This decoupling enables teams to independently manage\n\ntheir code and release schedules, which in turn improves each team’s ability\n\nto iterate and improve their code.\n\nThe design of a service architecture is much like the design of a class\n\nhierarchy—its goals are to introduce abstraction, encapsulation, and\n\nmodularity. It is important to note that in each of these categories the\n\n“right” design has as much to do with the humans involved in building the\n\ndesign as it does with the core application being developed.\n\nAbstraction and encapsulation can often be seen as foils of each other.\n\nAbstraction enables teams to not worry about the details of how a complex\n\npiece of functionality is implemented and focus instead on what they can do\n\nwith that capability. The programming required to recognize objects in an\n\nimage using machine learning is quite complicated, but the abstract service\n\nfindObjectsInImage(image): objects is quite easy to use.\n\nDistributed systems are quite complex, and trying to keep all of the details\n\nin our heads rapidly leads to information overload. Abstraction enables",
      "content_length": 1566,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 107,
      "content": "teams to focus on higher-level capabilities, not lower-level implementation\n\ndetails.\n\nEncapsulation is the same but in reverse; encapsulation allows teams to\n\ndeliver a service while hiding the implementation details from the user.\n\nHiding these details is critical because it enables the implementation team\n\nto make changes without breaking their users. This flexibility makes teams\n\nmore agile because they are decoupled from each other. Taken together,\n\nencapsulation and abstraction enable us to build optimally sized teams for\n\nexecution and delivery. Big enough to take on hard problems with a sense\n\nof purpose, but small enough to move quickly without the communication\n\nbaggage that arises in larger teams.\n\nFinally, the decoupling of microservices enables better scaling. Because\n\neach component has been broken out into its own service, it can be scaled\n\nindependently. It is rare for each service within a larger application to grow\n\nat the same rate, or have the same way of scaling. Some systems are\n\nstateless and can simply scale horizontally, whereas other systems maintain\n\nstate and require sharding or other approaches to scale. By separating each\n\nservice out, each service can use the approach to scaling that suits it best.\n\nThis is not possible when all services are part of a single monolith.\n\nBut of course there are downsides to the microservices approach to system\n\ndesign as well. As the system has become more loosely coupled, debugging\n\nthe system when failures occur is significantly more difficult. You can no",
      "content_length": 1544,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 108,
      "content": "longer simply load a single application into a debugger and determine what\n\nwent wrong. Any errors are the by-products of a large number of systems\n\noften running on different machines. This environment is quite challenging\n\nto reproduce in a debugger. As a corollary, microservices-based systems are\n\nalso difficult to design and architect. A microservices-based system uses\n\nmultiple methods of communicating between services; different patterns\n\n(e.g., synchronous, asynchronous, message-passing, etc.); and multiple\n\ndifferent patterns of coordination and control among the services.\n\nAdditionally, there can be a tendency within teams to take a microservice\n\npattern to an extreme and end up with far too many microservices that are\n\nfar too small. This over-segmentation not only adds complexity, but it also\n\ncan add significant inefficiency in terms of both network overhead (since\n\ncommunication between microservices occurs on the network) and also\n\ncosts (since each instance of a microservice has a fixed amount of overhead\n\nin terms of CPU and memory). As a general rule of thumb, if you have\n\nmore microservices than you have engineers on your team, you either have\n\na very small team or you are overdoing the microservice approach.\n\nThese challenges are the motivation for distributed patterns. If a\n\nmicroservices architecture is made up of well-known patterns, then it is\n\neasier to design because many of the design practices are specified by the\n\npatterns. Additionally, patterns make the systems easier to debug because\n\nthey enable developers to apply lessons learned across a number of different\n\nsystems that use the same patterns.",
      "content_length": 1654,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 109,
      "content": "With that in mind, this section introduces a number of multinode patterns\n\nfor building distributed systems. These patterns are not mutually exclusive.\n\nAny real-world system will be built from a collection of these patterns\n\nworking together to produce a single higher-level application.\n\nOceanofPDF.com",
      "content_length": 304,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 110,
      "content": "Chapter 6. Replicated Load-Balanced\n\nServices\n\nThe simplest distributed pattern, and one that most are familiar with, is a\n\nreplicated load-balanced service. In such a service, every server is identical\n\nto every other server, and all are capable of supporting traffic from any\n\nclient. The pattern consists of a scalable number of servers with a load\n\nbalancer in front of them. The load balancer is typically either completely\n\nround-robin or uses some form of session stickiness. The chapter will give a\n\nconcrete example of how to deploy such a service in Kubernetes.\n\nStateless Services\n\nStateless services are ones that don’t require saved state to operate correctly.\n\nIn the simplest stateless applications, even individual requests may be\n\nrouted to separate instances of the service (see Figure 6-1). Examples of\n\nstateless services include things like static content servers and complex\n\nmiddleware systems that receive and aggregate responses from numerous\n\ndifferent backend systems.",
      "content_length": 995,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 111,
      "content": "Figure 6-1. Basic replicated stateless service\n\nStateless systems are replicated to provide redundancy and scale. No matter\n\nhow small your service is, you need at least two replicas to provide a\n\nservice with a “highly available” service level agreement (SLA). To\n\nunderstand why this is true, consider trying to deliver a three-nines (99.9%\n\navailability). In a three-nines service, you get 1.4 minutes of downtime per\n\nday (24 × 60 × 0.001). Assuming that you have a service that never crashes,\n\nthat still means you need to be able to do a software upgrade in less than 1.4\n\nminutes in order to hit your SLA with a single instance. And that’s\n\nassuming that you do daily software rollouts. If your team is really\n\nembracing continuous delivery and you’re pushing a new version of",
      "content_length": 783,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 112,
      "content": "software every hour, you need to be able to do a software rollout in 3.6\n\nseconds to achieve your 99.9% uptime SLA with a single instance. Any\n\nlonger than that, and you will have more than 0.01% downtime from those\n\n3.6 seconds. And all of this is assuming that your code never fails due to\n\nbugs, which is unrealistic in any real-world codebase.\n\nOf course, instead of all of that work, you could just have two replicas of\n\nyour service with a load balancer in front of them. That way, while you are\n\ndoing a rollout, or in the—unlikely, I’m sure—event that your software\n\ncrashes, your users will be served by the other replica of the service and\n\nnever know anything was going on.\n\nAs services grow larger, they are also replicated to support additional users.\n\nHorizontally scalable systems handle more and more users by adding more\n\nreplicas; see Figure 6-2. They achieve this with the load-balanced replicated\n\nserving pattern.\n\nFigure 6-2. Horizontal scaling of a replicated stateless application",
      "content_length": 1004,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 113,
      "content": "Readiness Probes for Load Balancing\n\nOf course, simply replicating your service and adding a load balancer is\n\nonly part of a complete pattern for stateless replicated serving. When\n\ndesigning a replicated service, it is equally important to build and deploy a\n\nreadiness probe to inform the load balancer. We have discussed how health\n\nprobes can be used by a container orchestration system to determine when\n\nan application needs to be restarted. In contrast, a readiness probe\n\ndetermines when an application is ready to serve user requests. The reason\n\nfor the differentiation is that many applications require some time to\n\nbecome initialized before they are ready to serve. They may need to connect\n\nto databases, load plugins, or download serving files from the network. In\n\nall of these cases, the containers are alive, but they are not ready. When\n\nbuilding an application for a replicated service pattern, be sure to include a\n\nspecial URL that implements this readiness check.\n\nHands On: Creating a Replicated Service in\n\nKubernetes\n\nThe instructions below give a concrete example of how to deploy a stateless\n\nreplicated service behind a load balancer. These directions use the\n\nKubernetes container orchestrator, but the pattern can be implemented on\n\ntop of a number of different container orchestrators.",
      "content_length": 1318,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 114,
      "content": "To begin with, we will create a small Node.js application that serves\n\ndefinitions of words from the dictionary.\n\nTo try this service out, you can run it using a container image:\n\ndocker run -p 8080:8080 brendanburns/dictionary-s\n\nThis runs a simple dictionary server on your local machine. For example,\n\nyou can visit http://localhost:8080/dog to see the definition for dog.\n\nIf you look at the logs for the container, you’ll see that it starts serving\n\nimmediately but only reports readiness after the dictionary (which is\n\napproximately 8 MB) has been downloaded over the network.\n\nTo deploy this in Kubernetes, you create a Deployment :\n\napiVersion: extensions/v1beta1\n\nkind: Deployment metadata:\n\nname: dictionary-server\n\nspec: replicas: 3\n\ntemplate:\n\nmetadata: labels:\n\napp: dictionary-server",
      "content_length": 798,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 115,
      "content": "spec: containers:\n\nname: server\n\nimage: brendanburns/dictionary-server ports:\n\ncontainerPort: 8080\n\nreadinessProbe:\n\nhttpGet: path: /ready\n\nport: 8080\n\ninitialDelaySeconds: 5 periodSeconds: 5\n\nYou can create this replicated stateless service with:\n\nkubectl create -f dictionary-deploy.yaml\n\nNow that you have a number of replicas, you need a load balancer to bring\n\nrequests to your replicas. The load balancer serves to distribute the load as\n\nwell as to provide an abstraction to separate the replicated service from the\n\nconsumers of the service. The load balancer also provides a resolvable\n\nname that is independent of any of the specific replicas.\n\nWith Kubernetes, you can create this load balancer with a Service\n\nobject:",
      "content_length": 729,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 116,
      "content": "kind: Service apiVersion: v1\n\nmetadata:\n\nname: dictionary-server-service spec:\n\nselector:\n\napp: dictionary-server ports:\n\nprotocol: TCP\n\nport: 8080\n\ntargetPort: 8080\n\nOnce you have the configuration file, you can create the dictionary service\n\nwith:\n\nkubectl create -f dictionary-service.yaml\n\nSession Tracked Services\n\nThe previous examples of the stateless replicated pattern routed requests\n\nfrom all users to all replicas of a service. While this ensures an even\n\ndistribution of load and fault tolerance, it is not always the preferred\n\nsolution. Often there are reasons for wanting to ensure that a particular\n\nuser’s requests always end up on the same machine. Sometimes this is",
      "content_length": 685,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 117,
      "content": "because you are caching that user’s data in memory, so landing on the same\n\nmachine ensures a higher cache hit rate. Sometimes it is because the\n\ninteraction is long-running in nature, so some amount of state is maintained\n\nbetween requests. Regardless of the reason, an adaption of the stateless\n\nreplicated service pattern is to use session tracked services, which ensure\n\nthat all requests for a single user map to the same replica, as illustrated in\n\nFigure 6-3.\n\nFigure 6-3. A session tracked service where all requests for a specific user are routed to a single instance\n\nGenerally speaking, this session tracking is performed by hashing the\n\nsource and destination IP addresses and using that key to identify the server",
      "content_length": 726,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 118,
      "content": "that should service the requests. So long as the source and destination IP\n\naddresses remain constant, all requests are sent to the same replica.\n\nNOTE\n\nIP-based session tracking works within a cluster (internal IPs) but generally doesn’t work well with\n\nexternal IP addresses because of network address translation (NAT). For external session tracking,\n\napplication-level tracking (e.g., via cookies) is preferred.\n\nOften, session tracking is accomplished via a consistent hashing function.\n\nThe benefit of a consistent hashing function becomes evident when the\n\nservice is scaled up or down. Obviously, when the number of replicas\n\nchanges, the mapping of a particular user to a replica may change.\n\nConsistent hashing functions minimize the number of users that actually\n\nchange which replica they are mapped to, reducing the impact of scaling on\n\nyour application.\n\nApplication-Layer Replicated Services\n\nIn all of the preceding examples, the replication and load balancing takes\n\nplace in the network layer of the service. The load balancing is independent\n\nof the actual protocol that is being spoken over the network, beyond\n\nTCP/IP. However, many applications use HTTP as the protocol for speaking\n\nwith each other, and knowledge of the application protocol that is being",
      "content_length": 1279,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 119,
      "content": "spoken enables further refinements to the replicated stateless serving pattern\n\nfor additional functionality.\n\nIntroducing a Caching Layer\n\nSometimes the code in your stateless service is still expensive despite being\n\nstateless. It might make queries to a database to service requests or do a\n\nsignificant amount of rendering or data mixing to service the request. In\n\nsuch a world, a caching layer can make a great deal of sense. A cache exists\n\nbetween your stateless application and the end-user request. The simplest\n\nform of caching for web applications is a caching web proxy. The caching\n\nproxy is simply an HTTP server that maintains the correct responses to user\n\nrequests in memory. If two users request the same web page or API call,\n\nonly one request will go to your backend; the other will be serviced out of\n\nmemory in the cache. This is illustrated in Figure 6-4.\n\nFigure 6-4. The operation of a cache server\n\nFor our purposes, we will use Varnish, an open source web cache.",
      "content_length": 990,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 120,
      "content": "Deploying Your Cache\n\nThe simplest way to deploy the web cache is alongside each instance of\n\nyour web server using the sidecar pattern (see Figure 6-5).\n\nFigure 6-5. Adding the web cache server as a sidecar\n\nThough this approach is simple, it has some disadvantages, namely that you\n\nwill have to scale your cache at the same scale as your web servers. This is\n\noften not the approach you want. For your cache, you want as few replicas\n\nas possible with lots of resources for each replica (e.g., rather than 10\n\nreplicas with 1 GB of RAM each, you’d want two replicas with 5 GB of\n\nRAM each). To understand why this is preferable, consider that every page\n\nwill be stored in every replica. With 10 replicas, you will store every page\n\n10 times, reducing the overall set of pages that you can keep in memory in\n\nthe cache. This causes a reduction in the hit rate, the fraction of the time\n\nthat a request can be served out of cache, which in turn decreases the utility\n\nof the cache. Though you do want a few large caches, you might also want\n\nlots of small replicas of your web servers. Many languages (e.g., Node.js)",
      "content_length": 1118,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 121,
      "content": "can really only utilize a single core, and thus you want many replicas to be\n\nable to take advantage of multiple cores, even on the same machine.\n\nTherefore, it makes the most sense to configure your caching layer as a\n\nsecond stateless replicated serving tier above your web-serving tier, as\n\nillustrated in Figure 6-6.\n\nFigure 6-6. Adding the caching layer to our replicated service",
      "content_length": 384,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 122,
      "content": "NOTE\n\nUnless you are careful, caching can break session tracking. The reason for this is that if you use\n\ndefault IP address affinity and load balancing, all requests will be sent from the IP addresses of the\n\ncache, not the end user of your service. If you’ve followed the advice previously given and deployed\n\na few large caches, your IP-address-based affinity may in fact mean that some replicas of your web\n\nlayer see no traffic. Instead, you need to use something like a cookie or HTTP header for session\n\ntracking.\n\nHands On: Deploying the Caching Layer\n\nThe dictionary-server service we built earlier distributes traffic to\n\nthe dictionary server and is discoverable as the DNS name dictionary-\n\nserver . This pattern is illustrated in Figure 6-7.\n\nWe can begin building this with the following Varnish cache configuration:\n\nvcl 4.0;\n\nbackend default {\n\n.host = \"dictionary-server-service\";\n\n.port = \"8080\"; }\n\nCreate a ConfigMap object to hold this configuration:\n\nkubectl create configmap varnish-config --from-fi",
      "content_length": 1022,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 123,
      "content": "Figure 6-7. Adding a caching layer to the dictionary server",
      "content_length": 59,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 124,
      "content": "Now we can deploy the replicated Varnish cache, which will load this\n\nconfiguration:\n\napiVersion: extensions/v1beta1\n\nkind: Deployment\n\nmetadata:\n\nname: varnish-cache spec:\n\nreplicas: 2\n\ntemplate:\n\nmetadata:\n\nlabels:\n\napp: varnish-cache\n\nspec:\n\ncontainers:\n\nname: cache\n\nresources:\n\nrequests:\n\n# We'll use two gigabytes for each va\n\nmemory: 2Gi image: brendanburns/varnish\n\ncommand: - varnishd\n\n-F - -f - /etc/varnish-config/default.vcl\n\n-a\n\n0.0.0.0:8080",
      "content_length": 454,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 125,
      "content": "-s\n\n# This memory allocation should match the - malloc,2G\n\nports:\n\ncontainerPort: 8080\n\nvolumeMounts:\n\nname: varnish mountPath: /etc/varnish-config volumes:\n\nname: varnish\n\nconfigMap:\n\nname: varnish-config\n\nYou can deploy the replicated Varnish servers with:\n\nkubectl create -f varnish-deploy.yaml\n\nAnd then finally deploy a load balancer for this Varnish cache:\n\nkind: Service\n\napiVersion: v1\n\nmetadata:\n\nname: varnish-service\n\nspec:\n\nselector:\n\napp: varnish-cache",
      "content_length": 465,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 126,
      "content": "ports:\n\nprotocol: TCP\n\nport: 80\n\ntargetPort: 8080\n\nwhich you can create with:\n\nkubectl create -f varnish-service.yaml\n\nExpanding the Caching Layer\n\nNow that we have inserted a caching layer into our stateless replicated\n\nservice, let’s look at what this layer can provide beyond standard caching.\n\nHTTP reverse proxies like Varnish are generally pluggable and can provide\n\na number of advanced features that are useful beyond caching.\n\nRate Limiting and Denial-of-Service Defense\n\nFew of us build sites with the expectation that we will encounter a denial-\n\nof-service attack. But as more and more of us build APIs, a denial of service\n\ncan come simply from a developer misconfiguring a client or a site-\n\nreliability engineer accidentally running a load test against a production\n\ninstallation. Thus, it makes sense to add general denial-of-service defense\n\nvia rate limiting to the caching layer. Most HTTP reverse proxies like",
      "content_length": 929,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 127,
      "content": "Varnish have capabilities along this line. In particular, Varnish has a\n\nthrottle module that can be configured to provide throttling based on\n\nIP address and request path, as well as whether or not a user is logged in.\n\nIf you are deploying an API, it is generally a best practice to have a\n\nrelatively small rate limit for anonymous access and then force users to log\n\nin to obtain a higher rate limit. Requiring a login provides auditing to\n\ndetermine who is responsible for the unexpected load, and also offers a\n\nbarrier to would-be attackers who need to obtain multiple identities to\n\nlaunch a successful attack.\n\nWhen a user hits the rate limit, the server will return the 429 error code\n\nindicating that too many requests have been issued. However, many users\n\nwant to understand how many requests they have left before hitting that\n\nlimit. To that end, you will likely also want to populate an HTTP header\n\nwith the remaining-calls information. Though there isn’t a standard header\n\nfor returning this data, many APIs return some variation of X-\n\nRateLimit-Remaining .",
      "content_length": 1077,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 128,
      "content": "NOTE\n\nWhile Varnish can provide some basic protection against DDoS attacks, in the real world attackers\n\nare often sophisticated organizations with large amounts of resources to bring to bear in ever-\n\nevolving attacks against organizations. If you are planning on deploying large-scale or mission-\n\ncritical infrastructure, you will probably also want to add cloud-based denial of service protection\n\nfrom a public cloud provider to your application. Cloud-based DDoS systems can handle\n\nsignificantly more load and also are constantly evolving to meet the changing landscape of threat\n\nactors.\n\nSSL Termination\n\nIn addition to performing caching for performance, one of the other\n\ncommon tasks performed by the edge layer is SSL termination. Even if you\n\nplan on using SSL for communication between layers in your cluster, you\n\nshould still use different certificates for the edge and your internal services.\n\nIndeed, each individual internal service should use its own certificate to\n\nensure that each layer can be rolled out independently. Unfortunately, the\n\nVarnish web cache can’t be used for SSL termination, but fortunately, the\n\nnginx application can. Thus, we want to add a third layer to our stateless\n\napplication pattern, which will be a replicated layer of nginx servers that\n\nwill handle SSL termination for HTTPS traffic and forward traffic on to our\n\nVarnish cache. HTTP traffic continues to travel to the Varnish web cache,\n\nand Varnish forwards traffic on to our web application, as shown in\n\nFigure 6-8.",
      "content_length": 1524,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 129,
      "content": "Figure 6-8. Complete replicated stateless serving example\n\nHands On: Deploying nginx and SSL Termination\n\nThe following instructions describe how to add a replicated SSL\n\nterminating nginx to the replicated service and cache that we previously\n\ndeployed.",
      "content_length": 254,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 130,
      "content": "NOTE\n\nThese instructions assume that you have a certificate. If you need to obtain a certificate, the easiest\n\nway to do that is via the tools at Let’s Encrypt. Alternatively, you can use the openssl tool to\n\ncreate them. The following instructions assume that you’ve named them server.crt (public\n\ncertificate) and server.key (private key on the server). Such self-signed certificates will cause\n\nsecurity alerts in modern web browsers and should never be used for production.\n\nThe first step is to upload your certificate as a secret to Kubernetes:\n\nkubectl create secret tls ssl --cert=server.crt -\n\nOnce you have uploaded your certificate as a secret, you need to create an\n\nnginx configuration to serve SSL:\n\nevents {\n\nworker_connections 1024;\n\n}\n\nhttp { server { listen 443 ssl; server_name my-domain.com www.my-domain.com; ssl on; ssl_certificate /etc/certs/tls.crt; ssl_certificate_key /etc/certs/tls.key;\n\nlocation / {",
      "content_length": 927,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 131,
      "content": "proxy_pass http://varnish-service:80;\n\nproxy_set_header Host $host; proxy_set_header X-Forwarded-For $proxy_a\n\nproxy_set_header X-Forwarded-Proto $schem\n\nproxy_set_header X-Real-IP $remote_addr;\n\n}\n\n} }\n\nAs with Varnish, you need to transform this into a ConfigMap object:\n\nkubectl create configmap nginx-conf --from-file=n\n\nNow that you have a secret and an nginx configuration, it’s time to\n\ncreate the replicated stateless nginx layer:\n\napiVersion: extensions/v1beta1\n\nkind: Deployment\n\nmetadata: name: nginx-ssl\n\nspec: replicas: 4 template: metadata: labels: app: nginx-ssl",
      "content_length": 577,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 132,
      "content": "spec:\n\ncontainers: - name: nginx\n\nimage: nginx\n\nports:\n\ncontainerPort: 443\n\nvolumeMounts: - name: conf\n\nmountPath: /etc/nginx\n\nname: certs\n\nmountPath: /etc/certs\n\nvolumes:\n\nname: conf\n\nconfigMap:\n\n# This is the ConfigMap for nginx we cr\n\nname: nginx-conf\n\nname: certs\n\nsecret:\n\n# This is the secret we created above\n\nsecretName: ssl\n\nTo create the replicated nginx servers, you use:\n\nkubectl create -f nginx-deploy.yaml\n\nFinally, you can expose this nginx SSL server with a service:",
      "content_length": 482,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 133,
      "content": "kind: Service\n\napiVersion: v1\n\nmetadata:\n\nname: nginx-service spec:\n\nselector:\n\napp: nginx-ssl\n\ntype: LoadBalancer\n\nports:\n\nprotocol: TCP\n\nport: 443\n\ntargetPort: 443\n\nTo create this load-balancing service, run:\n\nkubectl create -f nginx-service.yaml\n\nIf you create this service on a Kubernetes cluster that supports external load\n\nbalancers, this will create an externalized, public service that services traffic\n\non a public IP address.\n\nTo get this IP address, you can run:\n\nkubectl get services\n\nYou should then be able to access the service with your web browser.",
      "content_length": 566,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 134,
      "content": "Summary\n\nThis chapter began with a simple pattern for replicated stateless services.\n\nThen we saw how this pattern grows with two additional replicated load-\n\nbalanced layers to provide caching for performance and SSL termination for\n\nsecure web serving. This complete pattern for stateless replicated serving is\n\nshown in Figure 6-8.\n\nThis complete pattern can be deployed into Kubernetes using three\n\nDeployments and Service load balancers to connect the layers\n\nshown in Figure 6-8. The complete source for these examples can be found\n\nat this book’s GitHub repository.\n\nOceanofPDF.com",
      "content_length": 588,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 135,
      "content": "Chapter 7. Sharded Services\n\nIn Chapter 6, we saw the value of replicating stateless services for\n\nreliability, redundancy, and scaling. This chapter considers sharded services.\n\nWith the replicated services that we introduced in Chapter 6, each replica\n\nwas entirely homogeneous and capable of serving every request. In contrast\n\nto replicated services, with sharded services, each replica, or shard, is only\n\ncapable of serving a subset of all requests. A load-balancing node, or root, is\n\nresponsible for examining each request and distributing each request to the\n\nappropriate shard or shards for processing. The contrast between replicated\n\nand sharded services is represented in Figure 7-1.\n\nFigure 7-1. Replicated service versus sharded service\n\nReplicated services are generally used for building stateless services,\n\nwhereas sharded services are generally used for building stateful services.\n\nThe primary reason for sharding the data is because the size of the state is",
      "content_length": 979,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 136,
      "content": "too large to be served by a single machine. Sharding enables you to scale a\n\nservice in response to the size of the state that needs to be served.\n\nSharding is not exclusively for stateful services, though. Sometimes\n\nsharding can also be useful for stateless services for the purpose of\n\nisolation. Failures in distributed systems can sometimes come from the\n\ninput requests to the system (“poison requests”) in a replicated system—if\n\nthe poison requests come in a large enough volume, they can take down all\n\nof the replicas. In such systems, sharding can form an isolation boundary\n\nthat protects the majority of the requests from the poison ones. Though the\n\npoison requests may take out the entire shard, that will only be a fraction of\n\nthe requests that the system processes.\n\nSharded Caching\n\nTo completely illustrate the design of a sharded system, this section\n\nprovides a deep dive into the design of a sharded caching system. A sharded\n\ncache is a cache that sits between the user requests and the actual frontend\n\nimplementation. A high-level diagram of the system is shown in Figure 7-2.",
      "content_length": 1102,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 137,
      "content": "Figure 7-2. A sharded cache\n\nIn Chapter 4, we discussed how an ambassador could be used to distribute\n\ndata to a sharded service. This section discusses how to build that service.\n\nWhen designing a sharded cache, there are a number of design aspects to\n\nconsider:\n\nWhy you might need a sharded cache",
      "content_length": 299,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 138,
      "content": "The role of the cache in your architecture\n\nReplicated sharded caches\n\nThe sharding function\n\nWhy You Might Need a Sharded Cache\n\nReminder: the primary reason for sharding any service is to increase the\n\nsize of the data being stored in the service. To understand how this helps a\n\ncaching system, imagine the following system: each cache has 10 GB of\n\nRAM available to store results, and can serve 100 requests per second\n\n(RPS). Suppose then that our service has a total of 200 GB possible results\n\nthat could be returned, and an expected 1,000 RPS. Clearly, we need 10\n\nreplicas of the cache in order to satisfy 1,000 RPS (10 replicas × 100\n\nrequests per second per replica). The simplest way to deploy this service\n\nwould be as a replicated service, as described in Chapter 6. But deployed\n\nthis way, the distributed cache can only hold a maximum of 5% (10 GB/200\n\nGB) of the total data set that we are serving. This is because each cache\n\nreplica is independent, and thus each cache replica stores roughly the exact\n\nsame data in the cache. This is great for redundancy, but pretty terrible for\n\nmaximizing memory utilization. If instead, we deploy a 10-way sharded\n\ncache, we can still serve the appropriate number of RPS (10 × 100 is still\n\n1,000), but because each cache serves a completely unique set of data, we\n\nare able to store 50% (10 × 10 GB/200 GB) of the total data set. This\n\ntenfold increase in cache storage means that the memory for the cache is\n\nmuch better utilized, since each key exists only in a single cache. The more",
      "content_length": 1544,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 139,
      "content": "data we store in the cache, the faster the overall system will be able to\n\noperate since more requests can be served from the cache.\n\nThe Role of the Cache in System Performance\n\nIn Chapter 6 we discussed how caches can be used to optimize end-user\n\nperformance and latency, but one thing that wasn’t covered was the\n\ncriticality of the cache to your application’s performance, reliability, and\n\nstability.\n\nPut simply, the important question for you to consider is: if the cache were\n\nto fail, what would the impact be for your users and your service?\n\nWhen we discussed the replicated cache, this question was less relevant\n\nbecause the cache itself was horizontally scalable, and failures of specific\n\nreplicas would only lead to transient failures. Likewise, the cache could be\n\nhorizontally scaled in response to increased load without impacting the end\n\nuser.\n\nThis changes when you consider sharded caches. Because a specific user or\n\nrequest is always mapped to the same shard, if that shard fails, that user or\n\nrequest will always miss the cache until the shard is restored. Given the\n\nnature of a cache as transient data, this miss is not inherently a problem, and\n\nyour system must know how to recalculate the data. However, this",
      "content_length": 1241,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 140,
      "content": "recalculation is inherently slower than using the cache directly, and thus it\n\nhas performance implications for your end users.\n\nThe performance of your cache is defined in terms of its hit rate. The hit\n\nrate is the percentage of the time that your cache contains the data for a user\n\nrequest. Ultimately, the hit rate determines the overall capacity of your\n\ndistributed system and affects the overall capacity and performance of your\n\nsystem.\n\nImagine, if you will, that you have a request-serving layer that can handle\n\n1,000 RPS. After 1,000 RPS, the system starts to return HTTP 500 errors\n\nto users. If you place a cache with a 50% hit rate in front of this request-\n\nserving layer, adding this cache increases your maximum RPS from 1,000\n\nRPS to 2,000 RPS. To understand why this is true, you can see that of the\n\n2,000 inbound requests, 1,000 (50%) can be serviced by the cache, leaving\n\n1,000 requests to be serviced by your serving layer. In this instance, the\n\ncache is fairly critical to your service, because if the cache fails, then the\n\nserving layer will be overloaded and half of all your user requests will fail.\n\nGiven this, it likely makes sense to rate your service at a maximum of 1,500\n\nRPS rather than the full 2,000 RPS. If you do this, then you can sustain a\n\nfailure of half of your cache replicas and still keep your service stable.\n\nBut the performance of your system isn’t just defined in terms of the\n\nnumber of requests that it can process. Your system’s end-user performance\n\nis defined in terms of the latency of requests as well. A result from a cache",
      "content_length": 1587,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 141,
      "content": "is generally significantly faster than calculating that result from scratch.\n\nConsequently, a cache can improve the speed of requests as well as the total\n\nnumber of requests processed. To see why this is true, imagine that your\n\nsystem can serve a request from a user in 100 milliseconds. You add a cache\n\nwith a 25% hit rate that can return a result in 10 milliseconds. Thus, the\n\naverage latency for a request in your system is now 77.5 milliseconds.\n\nUnlike maximum requests per second, the cache simply makes your\n\nrequests faster, so there is somewhat less need to worry about the fact that\n\nrequests will slow down if the cache fails or is being upgraded. However, in\n\nsome cases, the performance impact can cause too many user requests to\n\npile up in request queues and ultimately time out. It’s always recommended\n\nthat you load test your system both with and without caches to understand\n\nthe impact of the cache on the overall performance of your system.\n\nFinally, it isn’t just failures that you need to think about. If you need to\n\nupgrade or redeploy a sharded cache, you cannot just deploy a new replica\n\nand assume it will immediately take the existing load. A newly created\n\ncache shard doesn’t have any data stored in memory. Responses are cached\n\nas they are received in response to user requests. A newly deployed cache\n\nneeds to be “warmed up” or filled with responses before it can provide\n\nmaximal benefit. Deploying a new version of a sharded cache will generally\n\nresult in temporarily losing some capacity. Another, more advanced option\n\nis to replicate your shards.",
      "content_length": 1592,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 142,
      "content": "Replicated Sharded Caches\n\nSometimes your system is so dependent on a cache for latency or load that\n\nit is not acceptable to lose an entire cache shard if there is a failure or you\n\nare doing a rollout. Alternatively, you may have so much load on a\n\nparticular cache shard that you need to scale it to handle the load. For these\n\nreasons, you may choose to deploy a sharded replicated service. A sharded\n\nreplicated service combines the replicated service pattern described in\n\nChapter 6 with the sharded pattern described in previous sections. In a\n\nnutshell, rather than having a single server implement each shard in the\n\ncache, a replicated service is used to implement each cache shard.\n\nThis design is obviously more complicated to implement and deploy, but it\n\nhas several advantages over a simple sharded service. Most importantly, by\n\nreplacing a single server with a replicated service, each cache shard is\n\nresilient to failures and is always present during failures. Rather than\n\ndesigning your system to be tolerant to performance degradation resulting\n\nfrom cache shard failures, you can rely on the performance improvements\n\nthat the cache provides. Assuming that you are willing to over-provision\n\nshard capacity, this means that it is safe for you to do a cache rollout during\n\npeak traffic, rather than waiting for a quiet period for your service.\n\nAdditionally, because each replicated cache shard is an independent\n\nreplicated service, you can scale each cache shard in response to its load;\n\nthis sort of “hot sharding” is discussed at the end of this chapter.",
      "content_length": 1582,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 143,
      "content": "Hands On: Deploying an Ambassador and Memcache for a Sharded Cache\n\nIn Chapter 4 we saw how to deploy a sharded Redis service. Deploying a\n\nsharded memcache is similar.\n\nFirst, we will deploy memcache as a Kubernetes StatefulSet :\n\napiVersion: apps/v1\n\nkind: StatefulSet\n\nmetadata: name: sharded-memcache\n\nspec:\n\nserviceName: \"memcache\"\n\nreplicas: 3\n\ntemplate:\n\nmetadata: labels:\n\napp: memcache\n\nspec:\n\nterminationGracePeriodSeconds: 10\n\ncontainers:\n\nname: memcache\n\nimage: memcached ports:",
      "content_length": 490,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 144,
      "content": "containerPort: 11211\n\nname: memcache\n\nSave this to a file named memcached-shards.yaml, and you can deploy this\n\nwith kubectl create -f memcached-shards.yaml . This will\n\ncreate three containers running memcached.\n\nAs with the sharded Redis example, we also need to create a Kubernetes\n\nService that will create DNS names for the replicas we have created.\n\nThe service looks like this:\n\napiVersion: v1 kind: Service\n\nmetadata:\n\nname: memcache\n\nlabels:\n\napp: memcache\n\nspec:\n\nports: - port: 11211\n\nname: memcache\n\nclusterIP: None\n\nselector:\n\napp: memcache",
      "content_length": 553,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 145,
      "content": "Save this to a file named memcached-service.yaml and deploy it with\n\nkubectl create -f memcached-service.yaml . You should\n\nnow have DNS entries for memcache-0.memcache , memcache-\n\n1.memcache , etc. As with Redis, we can use these names to configure\n\ntwemproxy :\n\nmemcache:\n\nlisten: 127.0.0.1:11211\n\nhash: fnv1a_64\n\ndistribution: ketama\n\nauto_eject_hosts: true\n\ntimeout: 400 server_retry_timeout: 2000\n\nserver_failure_limit: 1\n\nservers:\n\nmemcache-0.memcache:11211:1\n\nmemcache-1.memcache:11211:1\n\nmemcache-2.memcache:11211:1\n\nIn this config, you can see that we are serving the memcache protocol on\n\nlocalhost:11211 so that the application container can access the\n\nambassador. We will deploy this into our ambassador pod using a\n\nKubernetes ConfigMap object that we can create with: kubectl\n\ncreate configmap --from-file=nutcracker.yaml twem-\n\nconfig .",
      "content_length": 853,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 146,
      "content": "Finally, all of the preparations are done, and we can deploy our ambassador\n\nexample. We define a pod that looks like this:\n\napiVersion: v1\n\nkind: Pod\n\nmetadata:\n\nname: sharded-memcache-ambassador spec:\n\ncontainers:\n\n# This is where the application container wou\n\n# - name: nginx\n\n# image: nginx\n\n# This is the ambassador container\n\nname: twemproxy\n\nimage: ganomede/twemproxy\n\ncommand:\n\nnutcracker\n\n-c\n\n/etc/config/nutcracker.yaml\n\n-v - 7\n\n-s - 6222\n\nvolumeMounts: - name: config-volume mountPath: /etc/config\n\nvolumes:\n\nname: config-volume",
      "content_length": 540,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 147,
      "content": "configMap:\n\nname: twem-config\n\nYou can save this to a file named memcached-ambassador-pod.yaml, and\n\nthen deploy it with:\n\nkubectl create -f memcached-ambassador-pod.yaml\n\nOf course, we don’t have to use the ambassador pattern if we don’t want to.\n\nAn alternative is to deploy a replicated shard router service. There are\n\ntrade-offs between using an ambassador versus using a shard routing\n\nservice. The value of the service is a reduction of complexity. You don’t\n\nhave to deploy the ambassador with every pod that wants to access the\n\nsharded memcache service; it can be accessed via a named and load-\n\nbalanced service. The downside of a shared service is twofold. First,\n\nbecause it is a shared service, you will have to scale it larger as demand\n\nload increases. Second, using the shared service introduces an extra\n\nnetwork hop that will add some latency to requests and contribute network\n\nbandwidth to the overall distributed system.\n\nTo deploy a shared routing service, you need to change the twemproxy\n\nconfiguration slightly so that it listens on all interfaces, not just localhost:",
      "content_length": 1094,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 148,
      "content": "memcache:\n\nlisten: 0.0.0.0:11211\n\nhash: fnv1a_64 distribution: ketama\n\nauto_eject_hosts: true\n\ntimeout: 400\n\nserver_retry_timeout: 2000\n\nserver_failure_limit: 1\n\nservers:\n\nmemcache-0.memcache:11211:1\n\nmemcache-1.memcache:11211:1\n\nmemcache-2.memcache:11211:1\n\nYou can save this to a file named shared-nutcracker.yaml, and then create a\n\ncorresponding ConfigMap using kubectl :\n\nkubectl create configmap --from-file=shared-nutcr\n\nThen you can turn up the replicated shard routing service as a\n\nDeployment :\n\napiVersion: extensions/v1\n\nkind: Deployment\n\nmetadata:\n\nname: shared-twemproxy",
      "content_length": 584,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 149,
      "content": "spec:\n\nreplicas: 3\n\ntemplate:\n\nmetadata: labels:\n\napp: shared-twemproxy\n\nspec:\n\ncontainers:\n\nname: twemproxy\n\nimage: ganomede/twemproxy\n\ncommand:\n\nnutcracker\n\n-c\n\n/etc/config/shared-nutcracker.yaml\n\n-v\n\n7\n\n-s - 6222\n\nvolumeMounts:\n\nname: config-volume\n\nmountPath: /etc/config\n\nvolumes:\n\nname: config-volume\n\nconfigMap:\n\nname: shared-twem-config",
      "content_length": 344,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 150,
      "content": "If you save this to shared-twemproxy-deploy.yaml, you can create the\n\nreplicated shard router using kubectl :\n\nkubectl create -f shared-twemproxy-deploy.yaml\n\nTo complete the shard router, we have to declare a load balancer to process\n\nrequests:\n\nkind: Service\n\napiVersion: v1\n\nmetadata:\n\nname: shard-router-service\n\nspec:\n\nselector:\n\napp: shared-twemproxy\n\nports:\n\nprotocol: TCP\n\nport: 11211\n\ntargetPort: 11211\n\nThis load balancer can be created using kubectl create -f\n\nshard-router-service.yaml .",
      "content_length": 499,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 151,
      "content": "An Examination of Sharding Functions\n\nSo far we’ve discussed the design and deployment of both simple sharded\n\nand replicated sharded caches, but we haven’t spent very much time\n\nconsidering how traffic is routed to different shards. Consider a sharded\n\nservice where you have 10 independent shards. Given some specific user\n\nrequest Req, how do you determine which shard S in the range from zero to\n\nnine should be used for the request? This mapping is the responsibility of\n\nthe sharding function. A sharding function is very similar to a hashing\n\nfunction, which you may have encountered when learning about hashtable\n\ndata structures. Indeed, a bucket-based hashtable could be considered an\n\nexample of a sharded service. Given both Req and Shard, then the role of\n\nthe sharding function is to relate them together, specifically:\n\nShard = ShardingFunction(Req)\n\nCommonly, the sharding function is defined using a hashing function and\n\nthe modulo (%) operator. Hashing functions are functions that transform an\n\narbitrary object into an integer hash. The hash function has two important\n\ncharacteristics for our sharding:\n\nDeterminism\n\nThe output should always be the same for a unique input.",
      "content_length": 1195,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 152,
      "content": "Uniformity\n\nThe distribution of outputs across the output space should be equal.\n\nFor our sharded service, determinism and uniformity are the most important\n\ncharacteristics. Determinism is important because it ensures that a particular\n\nrequest R always goes to the same shard in the service. Uniformity is\n\nimportant because it ensures that load is evenly spread between the different\n\nshards.\n\nFortunately for us, modern programming languages include a wide variety\n\nof high-quality hash functions. However, the outputs of these hash functions\n\nare often significantly larger than the number of shards in a sharded service.\n\nConsequently, we use the modulo operator (%) to reduce a hash function to\n\nthe appropriate range. Returning to our sharded service with 10 shards, we\n\ncan see that we can define our sharding function as:\n\nShard = hash(Req) % 10\n\nIf the output of the hash function has the appropriate properties in terms of\n\ndeterminism and uniformity, those properties will be preserved by the\n\nmodulo operator.",
      "content_length": 1023,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 153,
      "content": "Selecting a Key\n\nGiven this sharding function, it might be tempting to simply use the hashing\n\nfunction that is built into the programming language, hash the entire object,\n\nand call it a day. The result of this, however, will not be a very good\n\nsharding function.\n\nTo understand this, consider a simple HTTP request that contains three\n\nthings:\n\nThe time of the request\n\nThe source IP address from the client\n\nThe HTTP request path (e.g., /some/page.html)\n\nIf we use a simple object-based hashing function, shard ( request ),\n\nthen it is clear that {12:00, 1.2.3.4, /some/file.html} has a\n\ndifferent shard value than {12:01, 5.6.7.8,\n\n/some/file.html} . The output of the sharding function is different\n\nbecause the client’s IP address and the time of the request are different\n\nbetween the two requests. But of course, in most cases, the IP address of the\n\nclient and the time of the request don’t impact the response to the HTTP\n\nrequest. Consequently, instead of hashing the entire request object, a much\n\nbetter sharding function would be shard ( request.path ). When we\n\nuse request.path as the shard key, then we map both requests to the",
      "content_length": 1145,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 154,
      "content": "same shard, and thus the response to one request can be served out of the\n\ncache to service the other.\n\nOf course, sometimes client IP is important to the response that is returned\n\nfrom the frontend. For example, client IP may be used to look up the\n\ngeographic region that the user is located in, and different content (e.g.,\n\ndifferent languages) may be returned to different IP addresses. In such\n\ncases, the previous sharding function shard ( request.path ) will\n\nactually result in errors, since a cache request from a French IP address may\n\nbe served a result page from the cache in English. In such cases, the cache\n\nfunction is too general, as it groups together requests that do not have\n\nidentical responses.\n\nGiven this problem, it would be tempting then to define our sharding\n\nfunction as shard ( request.ip, request.path ), but this\n\nsharding function has problems as well. It will cause two different French\n\nIP addresses to map to different shards, thus resulting in inefficient\n\nsharding. This shard function is too specific, as it fails to group together\n\nrequests that are identical. A better sharding function for this situation\n\nwould be:\n\nshard ( country ( request.ip ), request.path )\n\nThis first determines the country from the IP address, and then uses that\n\ncountry as part of the key for the sharding function. Thus, multiple requests",
      "content_length": 1362,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 155,
      "content": "from France will be routed to one shard, while requests from the United\n\nStates will be routed to a different shard.\n\nDetermining the appropriate key for your sharding function is vital to the\n\noptimal design of a sharded system. Determining the correct shard key\n\nrequires an understanding of the requests that you expect to see.\n\nConsistent Hashing Functions\n\nSetting up the initial shards for a new service is relatively straightforward:\n\nyou set up the appropriate shards and the roots to perform the sharding, and\n\nyou are off to the races. However, what happens when you need to change\n\nthe number of shards in your sharded service? Such “re-sharding” is often a\n\ncomplicated process.\n\nTo understand why this is true, consider the sharded cache previously\n\nexamined. Certainly, scaling the cache from 10 to 11 replicas is\n\nstraightforward to do with a container orchestrator, but consider the effect\n\nof changing the scaling function from hash(Req) % 10 to hash(Req) % 11.\n\nWhen you deploy this new scaling function, a large number of requests are\n\ngoing to be mapped to a different shard than the one they were previously\n\nmapped to. In a sharded cache, this is going to temporarily cause a dramatic\n\nincrease in your miss rate until the cache is repopulated with responses for\n\nthe new requests that have been mapped to that cache shard by the new",
      "content_length": 1355,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 156,
      "content": "sharding function. In the worst case, rolling out a new sharding function for\n\nyour sharded cache will be equivalent to a complete cache failure.\n\nTo resolve these kinds of problems, many sharding functions use consistent\n\nhashing functions. Consistent hashing functions are special hash functions\n\nthat are guaranteed to only remap # keys / # shards, when being resized to #\n\nshards. For example, if we use a consistent hashing function for our sharded\n\ncache, moving from 10 to 11 shards will only result in remapping < 10% (K\n\n/ 11) keys. This is dramatically better than losing the entire sharded service.\n\nHands On: Building a Consistent HTTP Sharding Proxy\n\nTo shard HTTP requests, the first question to answer is what to use as the\n\nkey for the sharding function. Though there are several options, a good\n\ngeneral-purpose key is the request path as well as the fragment and query\n\nparameters (i.e., everything that makes the request unique). Note that this\n\ndoes not include cookies from the user or the language/location (e.g.,\n\nEN_US). If your service provides extensive customization to users or their\n\nlocation, you will need to include them in the hash key as well.\n\nWe can use the versatile nginx HTTP server for our sharding proxy:\n\nworker_processes 5; error_log error.log;",
      "content_length": 1287,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 157,
      "content": "pid nginx.pid;\n\nworker_rlimit_nofile 8192;\n\nevents {\n\nworker_connections 1024;\n\n}\n\nhttp {\n\n# define a named 'backend' that we can use in\n\n# below.\n\nupstream backend {\n\n# Has the full URI of the request and use\n\nhash $request_uri consistent\n\nserver web-shard-1.web;\n\nserver web-shard-2.web;\n\nserver web-shard-3.web;\n\n}\n\nserver {\n\nlisten localhost:80;\n\nlocation / {\n\nproxy_pass http://backend; }\n\n}\n\n}",
      "content_length": 399,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 158,
      "content": "Note that we chose to use the full request URI as the key for the hash and\n\nuse the key word consistent to indicate that we want to use a\n\nconsistent hashing function.\n\nSharded Replicated Serving\n\nMost of the examples in this chapter so far have described sharding in terms\n\nof cache serving. But, of course, caches are not the only kinds of services\n\nthat can benefit from sharding. Sharding is useful when considering any\n\nsort of service where there is more data than can fit on a single machine. In\n\ncontrast to previous examples, the key and sharding function are not a part\n\nof the HTTP request, but rather some context for the user.\n\nFor example, consider implementing a large-scale multiplayer game. Such a\n\ngame world is likely to be far too large to fit on a single machine. However,\n\nplayers who are distant from each other in this virtual world are unlikely to\n\ninteract. Consequently, the world of the game can be sharded across many\n\ndifferent machines. The sharding function is keyed off of the player’s\n\nlocation so that all players in a particular location land on the same set of\n\nservers.",
      "content_length": 1107,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 159,
      "content": "Hot Sharding Systems\n\nIdeally the load on a sharded cache will be perfectly even, but in many\n\ncases this isn’t true, and “hot shards” appear because organic load patterns\n\ndrive more traffic to one particular shard.\n\nAs an example of this, consider a sharded cache for a user’s photos; when a\n\nparticular photo goes viral and suddenly receives a disproportionate amount\n\nof traffic, the cache shard containing that photo will become “hot.” When\n\nthis happens, with a replicated sharded cache, you can scale the cache shard\n\nto respond to the increased load. Indeed, if you set up autoscaling for each\n\ncache shard, you can dynamically grow and shrink each replicated shard as\n\nthe organic traffic to your service shifts around. An illustration of this\n\nprocess is shown in Figure 7-3. Initially the sharded service receives equal\n\ntraffic to all three shards. Then the traffic shifts so that Shard A is receiving\n\nfour times as much traffic as Shard B and Shard C. The hot sharding system\n\nmoves Shard B to the same machine as Shard C, and replicates Shard A to a\n\nsecond machine. Traffic is now, once again, equally shared among replicas.",
      "content_length": 1140,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 160,
      "content": "Figure 7-3. An example of a hot sharded system: initially the shards are evenly distributed, but when extra traffic comes to shard A, it is replicated to two machines, and shards B and C are combined on a",
      "content_length": 204,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 161,
      "content": "single machine\n\nSummary\n\nThis chapter introduced the concept of sharding, which directs particular\n\nrequests to particular groups of machines instead of load-balancing all\n\ntraffic to all replicas of a service. An example was given of how sharding\n\ncan significantly improve the efficiency and performance of a caching\n\nsystem attached to a serving API. Sharding is not just useful for caching,\n\nhowever. Systems like large-scale open world game servers and databases\n\ncan scale beyond the size limits of a single machine by sharding their data\n\nacross multiple different shard servers. Sharding also helps with isolation,\n\nlimiting the blast radius of poison requests to a subset of all servers. Setting\n\nup a sharded system requires a careful understanding and design of both the\n\nshard key and the key hashing function. Identifying the right way to\n\npartition your data can often make the difference between a scalable system\n\nand one which runs into bottlenecks as it scales. Understanding the details\n\nof sharding is critical to building larger and more complex distributed\n\nsystems.\n\nOceanofPDF.com",
      "content_length": 1104,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 162,
      "content": "Chapter 8. Scatter/Gather\n\nSo far we’ve examined systems that replicate for scalability in terms of the\n\nnumber of requests processed per second (the stateless replicated pattern),\n\nas well as scalability for the size of the data (the sharded data pattern). In\n\nthis chapter we introduce the scatter/gather pattern, which uses replication\n\nfor scalability in terms of time. Specifically, the scatter/gather pattern\n\nallows you to achieve parallelism in servicing requests, enabling you to\n\nservice them significantly faster than you could if you had to service them\n\nsequentially.\n\nLike replicated and sharded systems, the scatter/gather pattern is a tree\n\npattern with a root that distributes requests and leaves that process those\n\nrequests. However, in contrast to replicated and sharded systems,\n\nscatter/gather requests are simultaneously farmed out to all of the replicas in\n\nthe system. Each replica does a small amount of processing and then returns\n\na fraction of the result to the root. The root server then combines the various\n\npartial results together to form a single complete response to the request and\n\nthen sends this request back out to the client. The scatter/gather pattern is\n\nillustrated in Figure 8-1.",
      "content_length": 1225,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 163,
      "content": "Figure 8-1. A scatter/gather pattern\n\nScatter/gather is quite useful when you have a large amount of mostly\n\nindependent processing that is needed to handle a particular request. With\n\nsuch problems, it is relatively easy to spread the computation required\n\nacross multiple different independent processors.\n\nWhereas the previous examples of caching sharded the data to maximize\n\nthe efficiency of memory usage, the scatter/gather algorithm shards the\n\ncomputation necessary to service the request, reducing the latency via",
      "content_length": 523,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 164,
      "content": "parallel processing. Though sometimes, as we will see, the data is sharded\n\nas well.\n\nScatter/Gather with Root Distribution\n\nThe simplest form of scatter/gather is one in which each leaf is entirely\n\nhomogenous but the work is distributed to a number of different leaves in\n\norder to improve the performance of the request. This pattern is equivalent\n\nto solving an “embarrassingly parallel” problem. The problem can be\n\nbroken up into many different pieces, and each piece can be put back\n\ntogether with all of the other pieces to form a complete answer.\n\nTo understand this in more concrete terms, imagine that you need to service\n\na user request R, and it takes one minute for a single core to produce the\n\nanswer A to this request. If we program a multithreaded application, we can\n\nparallelize this request on a single machine by using multiple cores. Given\n\nthis approach and a 30-core processor (yes, typically it would be a 32-core\n\nprocessor, but 30 makes the math cleaner), we can reduce the time that it\n\ntakes to process a single request down to 2 seconds (60 seconds of\n\ncomputation split across 30 threads for computation is equal to 2 seconds).\n\nBut even 2 seconds is pretty slow to service a user’s web request.\n\nAdditionally, truly achieving a completely parallel speed-up on a single\n\nprocess is going to be tricky, as things like memory, network, or disk\n\nbandwidth start to become the bottleneck. Instead of (or in addition to)",
      "content_length": 1447,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 165,
      "content": "parallelizing an application across cores on a single machine, we can use\n\nthe scatter/gather pattern to parallelize requests across multiple processes on\n\nmany different machines. In this way, we can improve our overall latency\n\nrequests, since we are no longer bound by the number of cores we can get\n\non a single machine, as well as ensure that the bottleneck in our process\n\ncontinues to be CPU, since the memory, network, and disk bandwidth are\n\nall spread across a number of different machines. Additionally, because\n\nevery machine in the scatter/gather tree is capable of handling every\n\nrequest, the root of the tree can dynamically dispatch load to different nodes\n\nat different times depending on their responsiveness. If, for some reason, a\n\nparticular leaf node is responding more slowly than other machines (e.g., it\n\nhas a noisy neighbor process that is interfering with resources), then the\n\nroot can dynamically redistribute load to assure a fast response.\n\nHands On: Distributed Document Search\n\nTo see an example of scatter/gather in action, consider the task of searching\n\nacross a large database of documents for all documents that contain the\n\nwords “cat” and “dog.” One way to perform this search would be to open\n\nup all of the documents, read through the entire set, searching for the words\n\nin each document, and then return to the user the set of documents that\n\ncontain both words.\n\nAs you might imagine, this is quite a slow process because it requires\n\nopening and reading through a large number of files for each request. To",
      "content_length": 1554,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 166,
      "content": "make request processing faster, you can build an index. Just like in the back\n\nof a reference book, the index is effectively a hashtable, where the keys are\n\nindividual words (e.g., “cat”) and the values are a list of documents (or\n\npages) containing that word.\n\nNow, instead of searching through every document, finding the documents\n\nthat match any one word is as easy as doing a lookup in this hashtable.\n\nHowever, we have lost one important ability. Remember that we were\n\nlooking for all documents that contained “cat” and “dog.” Since the index\n\nonly has single words, not conjunctions of words, we still need to find the\n\ndocuments that contain both words. Luckily, this is just an intersection of\n\nthe sets of documents returned for each word.\n\nWe could implement this algorithm on a single machine. Step one would be\n\nto use the index to look up the list of documents containing cat. Step two\n\nwould be to use the index to look up the list of documents containing dog.\n\nAnd the final step would be to find the intersection, the documents which\n\nare present in both lists. Looking at these steps, it’s clear that the lookup for\n\ncat and the lookup for dog are independent; they can easily be performed in\n\nparallel and can even be performed on different machines. Calculating the\n\nintersection, however, requires both lists of documents, and thus is\n\ndependent on the data from the first two steps.\n\nGiven this understanding, we can implement this document search as an\n\nexample of the scatter/gather pattern. When a request comes in to the",
      "content_length": 1548,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 167,
      "content": "document search root, it parses the request and farms out to two leaf\n\nmachines (one for the word “cat” and one for the word “dog”). Each of\n\nthese machines returns a list of documents that match one of the words, and\n\nthe root node returns the list of documents containing both “cat” and “dog.”\n\nA diagram of this process is shown in Figure 8-2: the leaf returns {doc1,\n\ndoc2, doc4} for “cat” and {doc1, doc3, doc4} for “dog,” so\n\nthe root finds the intersection and returns {doc1, doc4} .\n\nFigure 8-2. Example of a term-sharded scatter/gather system",
      "content_length": 551,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 168,
      "content": "Scatter/Gather with Leaf Sharding\n\nWhile applying the replicated data scatter/gather pattern allows you to\n\nreduce the processing time required for handling user requests, it doesn’t\n\nallow you to scale beyond an amount of data that can be held in the memory\n\nor disk of a single machine. Much like the replicated serving pattern that\n\nwas previously described, it is simple to build a replicated scatter/gather\n\nsystem. But at a certain data size, it is necessary to introduce sharding in\n\norder to build a system that can hold more data than can be stored on a\n\nsingle machine.\n\nPreviously, when sharding was introduced to scale replicated systems, the\n\nsharding was done at a per-request level. Some part of the request was used\n\nto determine where the request was sent. That replica then handled all of the\n\nprocessing for the request, and the response was handed back to the user.\n\nInstead, with scatter/gather sharding, the request is sent to all of the leaf\n\nnodes (or shards) in the system. Each leaf node processes the request using\n\nthe data that it has loaded in its shard. This partial response is then returned\n\nto the root node that requested data, and that root node merges all of the\n\nresponses together to form a comprehensive response for the user.\n\nAs a concrete example of this sort of architecture, consider implementing\n\nsearch across a very large document set (all patents in the world, for\n\nexample); in such a case, the data is too large to fit in the memory of a",
      "content_length": 1488,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 169,
      "content": "single machine, so instead the data is sharded across multiple replicas. For\n\nexample, patents 0–100,000 might be on the first machine, 100,001–\n\n200,000 on the next machine, and so forth. (Note that this is not actually a\n\ngood sharding scheme since it will continually force us to add new shards\n\nas new patents are registered. In practice, we’d likely use the patent number\n\nmodulo the total number of shards.)\n\nWhen a user submits a request to find a particular word (e.g., “rockets”) in\n\nall of the patents in the index, that request is sent to each shard, which\n\nsearches through its patent shard for patents that match the word in the\n\nquery. Any matches that are found are returned to the root node in response\n\nto the shard request. The root node then collates all of these responses\n\ntogether into a single response that contains all the patents that match the\n\nparticular word. The operation of this search index is illustrated in Figure 8-\n\n3.\n\nHands On: Sharded Document Search\n\nThe previous example scattered the different term requests across the\n\ncluster, but this only works if all of the documents are present on all of the\n\nmachines in the scatter/gather tree. If there is not enough room for all of the\n\ndocuments in all of the leaves in the tree, then sharding must be used to put\n\ndifferent sets of documents onto different leaves.",
      "content_length": 1353,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 170,
      "content": "This means that when a user makes a request for all documents that match\n\nthe words “cat” and “dog,” the request is actually sent out to every leaf in\n\nthe scatter/gather system. Each leaf node returns the set of documents that it\n\nknows about that matches “cat” and “dog.” Previously, the root node was\n\nresponsible for performing the intersection of the two sets of documents\n\nreturned for two different words. In the sharded case, the root node is\n\nresponsible for generating the union of all of the documents returned by all\n\nof the different shards and returning this complete set of documents back up\n\nto the user.\n\nIn Figure 8-3, the first leaf serves documents 1 through 10 and returns\n\n{doc1, doc5} . The second leaf serves documents 11 through 20 and\n\nreturns {doc15} . The third leaf serves documents 21 through 30 and\n\nreturns {doc22, doc28} . The root combines all of these responses\n\ntogether into a single response and returns {doc1, doc5, doc15,\n\ndoc22, doc28} .",
      "content_length": 978,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 171,
      "content": "Figure 8-3. Conjunctive query executing in a scatter/gather search system\n\nChoosing the Right Number of Leaves\n\nIt might seem that in the scatter/gather pattern, replicating out to a very\n\nlarge number of leaves would always be a good idea. You parallelize your\n\ncomputation and consequently reduce the clock time required to process\n\nany particular request. However, increased parallelization comes at a cost,\n\nand thus choosing the right number of leaf nodes in the scatter/gather\n\npattern is critical to designing a performant distributed system.\n\nTo understand how this can happen, it’s worth considering two things. The\n\nfirst is that processing any particular request has a certain amount of\n\noverhead. This includes the time spent parsing a request, sending requests\n\nacross the network, and performing encryption. In general, the overhead",
      "content_length": 846,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 172,
      "content": "due to system request handling is constant and significantly less than the\n\ntime spent in user code processing the request. Consequently, this overhead\n\ncan generally be ignored when assessing the performance of the\n\nscatter/gather pattern. However, it is important to understand that you pay\n\nthe cost of this overhead on every leaf node in the scatter/gather system.\n\nEach leaf node you add to increase parallelism increases the total overhead\n\ncost. Additionally, increasing the parallelism reduces the time spent\n\nperforming “useful” computation on each leaf node, until eventually the\n\ntime spent on the computational overhead is significantly greater than the\n\ntime spent performing “useful” computation. When this happens, you can\n\nno longer improve performance by adding additional leaf nodes. This\n\nmeans that the gains of parallelization are asymptotic. You cannot\n\ncontinually add nodes and expect to improve performance.\n\nIn addition to the fact that adding more leaf nodes may not actually speed\n\nup processing, scatter/gather systems also suffer from the “straggler”\n\nproblem, which you may have encountered in MapReduce big-data systems\n\nlike Hadoop. To understand how this works, it is important to remember\n\nthat in a scatter/gather system, the root node waits for requests from all of\n\nthe leaf nodes to return before sending a response back to the end user.\n\nSince data from every leaf node is required, the overall time it takes to\n\nprocess a user request is defined by the slowest leaf node that sends a\n\nresponse. To understand the impact of this, imagine that we have a service\n\nthat has a 99th percentile latency of 2 seconds. This means that on average\n\none request out of every 100 has a latency of 2 seconds, or put another way,",
      "content_length": 1755,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 173,
      "content": "there is a 1% chance that a request will take 2 seconds. This may be totally\n\nacceptable at first glance: a single user out of 100 has a slow request.\n\nHowever, consider how this actually works in a scatter/gather system. Since\n\nthe time of the user request is defined by the slowest response, we need to\n\nconsider not a single request but all requests scattered out to the various leaf\n\nnodes.\n\nLet’s see what happens when we scatter out to five leaf nodes. In this\n\nsituation, there is a 5% chance that one of these five scatter requests has a\n\nlatency of 2 seconds (0.99 × 0.99 × 0.99 × 0.99 × 0.99 = 0.95). This means\n\nthat our 99th percentile latency for individual requests becomes a 95th\n\npercentile latency for our complete scatter/gather system. And it only gets\n\nworse from there: if we scatter out to 100 leaves, then we are more or less\n\nguaranteeing that our overall latency for all requests will be 2 seconds.\n\nTogether, these complications of scatter/gather systems lead us to some\n\nconclusions:\n\nIncreased parallelism doesn’t always speed things up because of\n\noverhead on each node.\n\nIncreased parallelism doesn’t always speed things up because of the\n\nstraggler problem.\n\nThe performance of the 99th percentile is more important than in other\n\nsystems because each user request actually becomes numerous requests\n\nto the service.",
      "content_length": 1347,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 174,
      "content": "The same straggler problem applies to availability. If you issue a request to\n\n100 leaf nodes, and the probability that any leaf node failing is 1 in 100,\n\nyou are again practically guaranteed to fail every single user request.\n\nScaling Scatter/Gather for Reliability and\n\nScale\n\nOf course, just as with a sharded system, having a single replica of a\n\nsharded scatter/gather system is likely not the desirable design choice. A\n\nsingle replica means that if it fails, all scatter/gather requests will fail for the\n\nduration that the shard is unavailable because all requests are required to be\n\nprocessed by all leaf nodes in the scatter/gather pattern. Likewise, upgrades\n\nwill take out a percentage of your shards, so an upgrade while under user-\n\nfacing load is no longer possible. Finally, the computational scale of your\n\nsystem will be limited by the load that any single node is capable of\n\nachieving. Ultimately, this limits your scale, and as we have seen in\n\nprevious sections, you cannot simply increase the number of shards in order\n\nto improve the computational power of a scatter/gather pattern.\n\nGiven these challenges of reliability and scale, the correct approach is to\n\nreplicate each of the individual shards so that instead of a single instance at\n\neach leaf node, there is a replicated service that implements each leaf shard.\n\nThis replicated sharded scatter/gather pattern is shown in Figure 8-4.",
      "content_length": 1418,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 175,
      "content": "Figure 8-4. A sharded replicated scatter/gather system\n\nBuilt this way, each leaf request from the root is actually load balanced\n\nacross all healthy replicas of the shard. This means that if there are any\n\nfailures, they won’t result in a user-visible outage for your system.\n\nLikewise, you can safely perform an upgrade under load, since each\n\nreplicated shard can be upgraded one replica at a time. Indeed, you can\n\nperform the upgrade across multiple shards simultaneously, depending on\n\nhow quickly you want to perform the upgrade.",
      "content_length": 536,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 176,
      "content": "Summary\n\nThis chapter describes the scatter/gather pattern, a distributed system for\n\nimproving computation speed and reducing the latency of handling any\n\nparticular request. Whereas previous serving patterns focused on scaling for\n\nadditional load or scaling for additional data, scatter/gather scales to\n\nincrease responsiveness, to reduce the user-perceived time spent processing\n\na request. We also saw how compute sharding with scatter/gather can be\n\ncombined with data sharding for large indexes, as well as adding replication\n\nto improve reliability and redundancy. Any large-scale distributed system is\n\noften a combination of multiple different serving patterns.\n\nOceanofPDF.com",
      "content_length": 688,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 177,
      "content": "Chapter 9. Functions and Event-Driven\n\nProcessing\n\nSo far, we have examined design for systems with long-running\n\ncomputation. The servers that handle user requests are always up and\n\nrunning. This pattern is the right one for many applications that are under\n\nheavy load, keep a large amount of data in memory, or require some sort of\n\nbackground processing. However, there is a class of applications that might\n\nonly need to temporarily come into existence to handle a single request, or\n\nsimply need to respond to a specific event. This style of request or event-\n\ndriven application design has flourished recently as large-scale public cloud\n\nproviders have developed function-as-a-service (FaaS) products. More\n\nrecently, FaaS implementations have also emerged running on top of cluster\n\norchestrators in private cloud or physical environments. This chapter\n\ndescribes emerging architectures for this new style of computing. In many\n\ncases, FaaS is a component in a broader architecture rather than a complete\n\nsolution.",
      "content_length": 1025,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 178,
      "content": "NOTE\n\nOftentimes, FaaS is referred to as serverless computing. And while this is true (you don’t see the\n\nservers in FaaS), it’s worth differentiating between event-driven FaaS and the broader notion of\n\nserverless computing. Indeed, serverless computing can apply to a wide variety of computing\n\nservices; for example, a multitenant container orchestrator (container-as-a-service) is serverless but\n\nnot event-driven. Conversely, an open source FaaS running on a cluster of physical machines that\n\nyou own and administer is event-driven but not serverless. Understanding this distinction enables you\n\nto determine when event-driven, serverless, or both is the right choice for your application.\n\nDetermining When FaaS Makes Sense\n\nAs with many tools for developing a distributed system, it can be tempting\n\nto see a particular solution like event-driven processing as a universal\n\nhammer. However, the truth is that it is best suited to a particular set of\n\nproblems. Within a particular context it is a powerful tool, but stretching it\n\nto fit all applications or systems will lead to overly complicated, brittle\n\ndesigns. Especially since FaaS is a relatively new computing tool, before\n\ndiscussing specific design patterns, it is worth discussing the benefits,\n\nlimitations, and optimal situations for employing event-driven computing.\n\nThe Benefits of FaaS\n\nThe benefits of FaaS are primarily for the developer. It dramatically\n\nsimplifies the distance from code to running service. Because there is no\n\nartifact to create or push beyond the source code itself, FaaS makes it",
      "content_length": 1580,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 179,
      "content": "simple to go from code on a laptop or web browser to running code in the\n\ncloud.\n\nLikewise, the code that is deployed is managed and scaled automatically. As\n\nmore traffic is loaded onto the service, more instances of the function are\n\ncreated to handle that increase in traffic. If a function fails due to\n\napplication or machine failures, it is automatically restarted on some other\n\nmachine.\n\nFinally, much like containers, functions are an even more granular building\n\nblock for designing distributed systems. Functions are stateless, and thus\n\nany system you build on top of functions is inherently more modular and\n\ndecoupled than a similar system built into a single binary. But, of course,\n\nthis is also the challenge of developing systems in FaaS. The decoupling is\n\nboth a strength and a weakness. The following section describes some of\n\nthe challenges that come from developing systems using FaaS.\n\nThe Challenges of FaaS\n\nAs described in “The Benefits of FaaS”, developing systems using FaaS\n\nforces you to strongly decouple each piece of your service. Each function is\n\nentirely independent. The only communication is across the network, and\n\neach function instance cannot have local memory, requiring all states to be\n\nstored in a storage service. This forced decoupling can improve the agility",
      "content_length": 1309,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 180,
      "content": "and speed with which you can develop services, but it can also significantly\n\ncomplicate the operations of the same service.\n\nIn particular, it is often quite difficult to obtain a comprehensive view of\n\nyour service, determine how the various functions integrate with one\n\nanother, and understand when things go wrong, and why they go wrong.\n\nAdditionally, the request-based and serverless nature of functions means\n\nthat certain problems are quite difficult to detect. As an example, consider\n\nthe following functions:\n\nfunctionA() which calls functionB()\n\nfunctionB() which calls functionC()\n\nfunctionC() which calls back to functionA()\n\nNow consider what happens when a request comes into any of these\n\nfunctions: it kicks off an infinite loop that only terminates when the original\n\nrequest times out (and possibly not even then) or when you run out of\n\nmoney to pay for requests in the system. Obviously, the above example is\n\nquite contrived, but it is actually quite difficult to detect in your code. Since\n\neach function is radically decoupled from the other functions, there is no\n\nreal representation of the dependencies or interactions between different\n\nfunctions. These problems are not unsolvable, and I expect that as FaaSs\n\nmature, more analysis and debugging tools will provide a richer experience\n\nto understand how and why an application comprised of FaaS is performing\n\nthe way that it does.",
      "content_length": 1412,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 181,
      "content": "For now, when adopting FaaS, you must be vigilant to adopt rigorous\n\nmonitoring and alerting for how your system is behaving so that you can\n\ndetect situations and correct them before they become significant problems.\n\nOf course, the complexity introduced by monitoring flies somewhat in the\n\nface of the simplicity of deploying to FaaS, which is friction that your\n\ndevelopers must overcome.\n\nThe Need for Background Processing\n\nFaaS is inherently an event-based application model. Functions are\n\nexecuted in response to discrete events that occur and trigger the execution\n\nof the functions. Additionally, because of the serverless nature of the\n\nimplementation of these services, the runtime of any particular function\n\ninstance is generally time bounded. This means that FaaS is usually a poor\n\nfit for situations that require processing. Examples of such background\n\nprocessing might be transcoding a video, compressing log files, or other\n\nsorts of low-priority, long-running computations. In many cases, it is\n\npossible to set up a scheduled trigger that synthetically generates events in\n\nyour functions on a particular schedule. Though this is a good fit for\n\nresponding to temporal events (e.g., firing a text-message alarm to wake\n\nsomeone up), it is still not sufficient infrastructure for generic background\n\nprocessing. To achieve that, you need to launch your code in an\n\nenvironment that supports long-running processes. And this generally",
      "content_length": 1455,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 182,
      "content": "means switching to a pay-per-consumption rather than pay-per-request\n\nmodel for the parts of your application that do background processing.\n\nThe Need to Hold Data in Memory\n\nIn addition to the operational challenges, there are some architectural\n\nlimitations that make FaaS ill-suited for some types of applications. The\n\nfirst of these limitations is the need to have a significant amount of data\n\nloaded into memory in order to process user requests. There are a variety of\n\nservices (e.g., serving a search index of documents) that require a great deal\n\nof data to be loaded in memory in order to service user requests. Even with\n\na relatively fast storage layer, loading such data can take significantly\n\nlonger than the desired time to service a user request. Because with FaaS,\n\nthe function itself may be dynamically spun up in response to a user request\n\nwhile the user is waiting, the need to load a lot of detail may significantly\n\nimpact the latency that the user perceives while interacting with your\n\nservice. Of course, once your FaaS has been created, it may handle a large\n\nnumber of requests, so this loading cost can be amortized across a large\n\nnumber of requests. But if you have a sufficient number of requests to keep\n\na function active, then it’s likely you are overpaying for the requests you are\n\nprocessing.",
      "content_length": 1334,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 183,
      "content": "The Costs of Sustained Request-Based Processing\n\nThe cost model of public cloud FaaS is based on per-request pricing. This\n\napproach is great if you only have a few requests per minute or hour. In\n\nsuch a situation, you are idle most of the time, and given a pay-per-request\n\nmodel, you are only paying for the time when your service is actively\n\nserving requests. In contrast, if you service requests via a long-running\n\nservice, either in a container or a virtual machine, then you are always\n\npaying for processor cycles that are largely sitting around waiting for a user\n\nrequest.\n\nHowever, as a service grows, the number of requests that you are servicing\n\ngrows to the point where you can keep a processor continuously active\n\nservicing user requests. At this point, the economics of a pay-per-request\n\nmodel start to become bad, and only get worse because the cost of cloud\n\nvirtual machines generally decreases as you add more cores (and also via\n\ncommitted resources like reservations or sustained use discounts), whereas\n\nthe cost per-request largely grows linearly with the number of requests.\n\nConsequently, as your service grows and evolves, it’s highly likely that your\n\nuse of FaaS will evolve as well. One ideal way to scale FaaS is to run an\n\nopen source FaaS that runs on a container orchestrator like Kubernetes.\n\nThat way, you can still take advantage of the developer benefits of FaaS,\n\nwhile taking advantage of the pricing models of virtual machines.",
      "content_length": 1473,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 184,
      "content": "Patterns for FaaS\n\nIn addition to understanding the trade-offs in deploying event-driven or\n\nFaaS architectures as part of your distributed system, understanding the best\n\nways to deploy FaaS is critical to the design of a successful system. This\n\nsection describes some canonical patterns for incorporating FaaS.\n\nThe Decorator Pattern: Request or Response Transformation\n\nFaaS is ideal for deploying simple functions that can take an input,\n\ntransform it into an output, and then pass it on to a different service. This\n\ngeneral pattern can be used to augment or decorate HTTP requests to or\n\nfrom a different service. A basic illustration of this pattern is shown in\n\nFigure 9-1.",
      "content_length": 682,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 185,
      "content": "Figure 9-1. The decorator pattern applied to HTTP APIs\n\nInterestingly, there are several analogies to this pattern in programming\n\nlanguages. In particular, the decorator pattern from Python is a close\n\nanalogue for the services that a request or response decorator can perform.\n\nBecause decoration transformations are generally stateless, and also\n\nbecause they are often added after the fact to existing code as the service\n\nevolves, they are ideal services to implement via FaaS. Additionally, the\n\nlightness of FaaS means that you can experiment with a variety of different",
      "content_length": 577,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 186,
      "content": "decorators before finally adopting one and pulling it more completely into\n\nthe implementation of the service.\n\nA great example of the value of the decorator pattern is adding defaults to\n\nthe input to an HTTP RESTful API. In many cases in the API, there are\n\nfields whose values should have sane defaults if they are left empty. For\n\nexample, you may want a field to default to true , but it’s difficult to\n\naccomplish this in classical JSON, because the default value for a field is\n\nnull , which is generally understood to be false . To resolve this, we\n\ncan add defaulting logic either in the front of the API server or within the\n\napplication code itself (e.g., if (field == null) field =\n\ntrue ). However, both of these solutions are somewhat unappealing since\n\nthe defaulting mechanism is fairly conceptually independent from the\n\nhandling of the request. Instead, we can use the FaaS decorator pattern to\n\ntransform the request in between the user and the service implementation.\n\nGiven the previous discussion of the adapter pattern in the single-node\n\nsection, you may be wondering why we don’t simply package this\n\ndefaulting as an adapter container. And this is a totally reasonable approach,\n\nbut it does mean that we are going to couple the scale of the defaulting\n\nservice with the API service itself. The defaulting is actually a lightweight\n\noperation, and we are likely to need far fewer instances of it than the service\n\nitself to handle the load.",
      "content_length": 1466,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 187,
      "content": "NOTE\n\nFor the examples in this chapter, we are going to use the kubeless FaaS framework. Kubeless is\n\ndeployed on top of the Kubernetes container orchestration service. Assuming that you have\n\nprovisioned a Kubernetes cluster, you can install Kubeless from its releases page. Once you have the\n\nkubeless binary installed, you can install it into your cluster with the following commands:\n\nkubeless install .\n\nKubeless installs itself as a native Kubernetes third-party API. This means that once it is installed,\n\nyou can use the native kubectl command-line tool. For example, you can see deployed functions\n\nusing kubectl get functions . Currently, you should have no functions deployed.\n\nHands On: Adding Request Defaulting Prior to\n\nRequest Processing\n\nTo demonstrate the utility of the decorator pattern for FaaS, consider the\n\ntask of adding default values to a RESTful function call if the values are\n\nmissing. This is quite straightforward to do using FaaS. We’ll write the\n\ndefaulting function using the Python programming language:\n\n# Simple handler function for adding default valu def handler(context):\n\n# Get the input value obj = context.json\n\n# If the 'name' field is not present, set it ra if obj.get(\"name\", None) is None: obj[\"name\"] = random_name() # If the 'color' field is not present, set it t",
      "content_length": 1313,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 188,
      "content": "if obj.get(\"color\", None) is None:\n\nobj[\"color\"] = \"blue\" # Call the actual API, potentially with the new\n\n# values, and return the result\n\nreturn call_my_api(obj)\n\nSave this function in a file named defaults.py. You obviously will want to\n\nupdate the call_my_api code so that it points to the actual API you\n\nwant to call. Once you have finished writing the code, this defaulting\n\nfunction can be installed as a kubeless function using:\n\nkubeless function deploy add-defaults \\ --runtime python27 \\\n\n--handler defaults.handler \\\n\n--from-file defaults.py \\\n\n--trigger-http\n\nIf you want to test the handling of this function, you can also use the\n\nkubeless tool:\n\nkubeless function call add-defaults --data '{\"nam\n\nThe decorator pattern shows just how easy it is to adapt and extend existing\n\nAPIs with additional features like validation or defaulting.",
      "content_length": 852,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 189,
      "content": "Handling Events\n\nWhile most systems are request-driven, handling a steady stream of user\n\nand API requests, many other systems are more event-driven in nature. The\n\ndifferentiation, in my mind at least, between a request and an event has to\n\ndo with the notion of session. Requests are part of a larger series of\n\ninteractions or sessions; generally each user request is part of a larger\n\ninteraction with a complete web application or API. Events, as I see them,\n\ninstead tend to be single-instance and asynchronous in nature. Events are\n\nimportant and need to be properly handled, but they are fired off from a\n\nmain interaction and responded to some time later. Examples of events\n\ninclude a user signing up for a new service (which might trigger a welcome\n\nemail), someone uploading a file to a shared folder (which might send\n\nnotifications to everyone who has access to the folder), or even a machine\n\nbeing about to reboot (which might notify an operator or automated system\n\nto take appropriate action).\n\nBecause these events tend to be largely independent and stateless in nature,\n\nand because the rate of events can be highly variable, they are ideal\n\ncandidates for event-driven and FaaS architectures. In this role, they are\n\noften deployed alongside a production application server as augmentation to\n\nthe main user experience, or to handle some sort of reactive, background\n\nprocessing. Additionally, because new events are often dynamically added\n\nto the service, the lightweight nature of deploying functions is a good match\n\nfor defining new event handlers. Likewise, because each event is",
      "content_length": 1606,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 190,
      "content": "conceptually independent, the forced decoupling of a functions-based\n\nsystem actually helps reduce the conceptual complexity by enabling a\n\ndeveloper to focus on the steps required to handle just a single type of\n\nevent.\n\nA concrete example of integrating an event-based component to an existing\n\nservice is implementing two-factor authentication. In this case, the event is\n\nthe user logging into a service. The service can generate an event for this\n\naction, fire it into a function-based handler that takes the code and the user’s\n\ncontact information, and sends the two-factor code via text message.\n\nHands On: Implementing Two-Factor Authentication\n\nTwo-factor authentication requires that the user both have something that\n\nthey know (e.g., a password) as well as something that they possess (e.g., a\n\nphone) to be able to log in to the system. Two-factor authentication is\n\nsignificantly more secure than passwords alone since it requires two\n\ndifferent security compromises (a thief learning your password and a thief\n\nstealing your phone) to enable a true security problem.\n\nWhen considering how to implement two-factor authentication, one of the\n\nchallenges is how to handle the request to generate a random code and\n\nregister it with the login service as well as send the text message. It is\n\npossible to add this code to the main login web server. But it is complicated",
      "content_length": 1381,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 191,
      "content": "and monolithic, and forces the act of sending a text message, which can\n\nhave some latency, to be inline with the code that renders the login web\n\npage. This latency produces a substandard user experience.\n\nA better option is to register a FaaS to asynchronously generate the random\n\nnumber, register it with the login service, and send the number to the user’s\n\nphone. In this way, the login server can simply fire an asynchronous web-\n\nhook request to a FaaS, and that FaaS can handle the somewhat slow and\n\nasynchronous task of registering the two-factor code and sending the text\n\nmessage.\n\nTo see how this works in practice, consider the following code:\n\ndef two_factor(context):\n\n# Generate a random six-digit code\n\ncode = random.randint(100000, 999999)\n\n# Register the code with the login service\n\nuser = context.json[\"user\"]\n\nregister_code_with_login_service(user, code)\n\n# Use the twillio library to send texts account = \"my-account-sid\" token = \"my-token\"\n\nclient = twilio.rest.Client(account, token)\n\nuser_number = context.json[\"phoneNumber\"]",
      "content_length": 1053,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 192,
      "content": "msg = \"Hello {} your authentication code is: {}\n\nmessage = client.api.account.messages.create(to fr\n\nbo\n\nreturn {\"status\": \"ok\"}\n\nWe can then register this FaaS with kubeless :\n\nkubeless function deploy add-two-factor \\\n\n--runtime python27 \\\n\n--handler two_factor.two_factor \\\n\n--from-file two_factor.py \\\n\n--trigger-http\n\nThis function can then be made asynchronously from client-side JavaScript\n\nwhenever the user successfully provides their password. The web UX can\n\nthen immediately display a page to enter the code, and the user (once they\n\nreceive the code as a text message) can supply it to the service, where the\n\ncode has already been registered via our FaaS.\n\nAgain, developing a simple, asynchronous, event-based service that is\n\ntriggered whenever a user logs in is made dramatically simpler using FaaS.",
      "content_length": 816,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 193,
      "content": "Event-Based Pipelines\n\nThere are some applications that are inherently easier to think about in\n\nterms of a pipeline of decoupled events. These event pipelines often\n\nresemble the flowcharts of old. They can be represented as a directed graph\n\nof connected event syncs. In the event pipeline pattern, each node is a\n\ndifferent function or webhook, and the edges linking the graph together are\n\nHTTP or other network calls to the function/webhook. In general, there is\n\nno shared state between the different pieces of the pipeline, but there may\n\nbe a context or other reference point that can be used to look up information\n\nin shared storage.\n\nSo, what is the difference between this type of pipeline and a\n\n“microservices” architecture? There are two central differences. The first is\n\nthe main difference between functions in general and long-running services,\n\nwhich is that an event-based pipeline is by its very nature event-driven.\n\nConversely, a microservices architecture features a collection of long-\n\nrunning services. Additionally, event-driven pipelines may be highly\n\nasynchronous and diverse in the things that they connect together. For\n\nexample, while it is difficult to see how a human approving a ticket in a\n\nticketing system like Jira could be integrated into a microservices\n\napplication, it’s quite easy to see how that event could be incorporated into\n\nan event-driven pipeline.",
      "content_length": 1403,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 194,
      "content": "As an example of this, imagine a pipeline in which the original event is\n\ncode being submitted into a source control system. This event then triggers\n\na build. The build may take multiple minutes to complete, and when it does,\n\nit fires an event to a build analysis function. This function takes different\n\nactions if the build is successful or fails. If the build succeeds, a ticket is\n\ncreated for a human to approve it to be pushed to production. Once the\n\nticket is closed, the act of closing is an event that triggers the actual push to\n\nproduction. If the build fails, a bug is filed on the failure, and the event\n\npipeline terminates.\n\nHands On: Implementing a Pipeline for New User Signup\n\nConsider the task of implementing a new user signup flow. When a new\n\nuser account is created, there are certain things that are always done, such\n\nas sending a welcome email. And there are some things that are optionally\n\ndone, such as registering a user to receive product updates (sometimes\n\nknown as “spam”) via their email.\n\nOne approach to implementing this logic would be to place everything into\n\na single monolithic user-creation server. However, this factoring means that\n\na single team owns the entirety of the user-creation service, and that the\n\nentire experience is deployed as a single service. Both of these mean that it",
      "content_length": 1334,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 195,
      "content": "is more difficult to perform experiments or make changes to the user\n\nexperience.\n\nConsider, instead, implementing the user login experience as an event\n\npipeline with a series of FaaS. In this factoring, the user-creation function is\n\nactually unaware of the details of what happens on user login. Instead, the\n\nmain user-creation service simply has two lists:\n\nA list of required actions (e.g., sending a welcome mail)\n\nA list of optional actions (e.g., subscribing the user to a mailing list)\n\nEach of these actions is also implemented as a FaaS, and the list of actions\n\nis actually just a list of webhooks. Consequently, the main user creation\n\nfunction looks like this:\n\ndef create_user(context):\n\n# For required event handlers, call them univer\n\nfor key, value in required.items():\n\ncall_function(value.webhook, context.json)\n\n# For optional event handlers, check and call t # conditionally for key, value in optional.items(): if context.json.get(key, None) is not None: call_function(value.webhook, context.json)",
      "content_length": 1020,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 196,
      "content": "Now we can also use FaaS to implement each of these handlers:\n\ndef email_user(context):\n\n# Get the user name\n\nuser = context.json['username']\n\nmsg = 'Hello {} thanks for joining my awesome s\n\nsend_email(msg, contex.json['email])\n\ndef subscribe_user(context):\n\n# Get the user name\n\nemail = context.json['email']\n\nsubscribe_user(email)\n\nFactored in this way, each FaaS is simple, containing only a few lines of\n\ncode and focused on implementing one specific piece of functionality. This\n\nmicroservices-based approach is simple to write but might lead to\n\ncomplexity if we actually had to deploy and manage three different\n\nmicroservices. This is where FaaS can shine, since it makes it trivially easy\n\nto host these small code snippets. Additionally, by visualizing our user-\n\ncreation flow as an event-driven pipeline, it is also straightforward to have a\n\nhigh-level understanding of what exactly happens on user login, simply by",
      "content_length": 929,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 197,
      "content": "following the flow of the context through the various functions in the\n\npipeline.\n\nSummary\n\nFunctions as a service, or FaaS, is a powerful tool for easily hosting simple,\n\nscalable applications. As with any tool, the key to using it for maximal\n\nvalue lies in understanding what it is best at. The simplicity of functions as\n\na service comes with restrictions that may limit ways in which your\n\napplication can develop or libraries that your application can use. However,\n\nFaaS can also radically reduce your time to market. As you gain experience\n\nwith distributed systems, you will see that the ideal design is often a\n\ncombination of different pieces. Combining FaaS with more flexible\n\ninfrastructure or IaaS primitives, like the core of the Kubernetes API, often\n\nallows you to gain the best of both possible worlds and blend power and\n\nflexibility with simplicity and automation for the optimal system design.\n\nOceanofPDF.com",
      "content_length": 931,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 198,
      "content": "Chapter 10. Ownership Election\n\nThe previous patterns that we have seen have been about distributing\n\nrequests in order to scale requests per second, the state being served, or the\n\ntime to process a request. This final chapter on multinode serving patterns is\n\nabout how you scale assignment. In many different systems, there is a\n\nnotion of ownership where a specific process owns a specific task. We have\n\npreviously seen this in the context of sharded and hot-sharded systems\n\nwhere specific instances owned specific sections of the sharded key space.\n\nIn the context of a single process on a single server, you have likely already\n\nlearned how to manage concurrency using primitives like locks or mutexes\n\nthat are present in modern programming languages. A lock establishes\n\nownership within the context of a single application running on a single\n\nmachine because it uses storage in the memory of that machine and\n\nprimitives in the processor and operating system to establish exclusive\n\naccess. Even in the context of a single machine, concurrency can be\n\nchallenging to implement, and mistakes in concurrency are at the root of\n\nsome of the most challenging bugs you may encounter.\n\nUnfortunately, restricting ownership to a single application limits\n\nscalability, since the task can’t be replicated, and reliability, since if the task\n\nfails, it is unavailable for a period of time. If you want to build a large-scale\n\nreliable system that requires ownership, you need to build distributed locks.",
      "content_length": 1506,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 199,
      "content": "Distributed locks introduce additional complexity since you cannot rely on\n\nshared memory or processors between different containers in your\n\ndistributed system.\n\nA general diagram of distributed ownership is shown in Figure 10-1. In the\n\ndiagram, there are three replicas that could be the owner or leader. Initially,\n\nthe first replica is the leader. Then that replica fails, and replica number\n\nthree then becomes the leader. Finally, replica number one recovers and\n\nreturns to the group, but replica three remains as the leader/owner.",
      "content_length": 539,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 200,
      "content": "Figure 10-1. A leader election protocol in operation: initially the first leader is selected, but when it fails, the third leader takes over",
      "content_length": 140,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 201,
      "content": "Often, establishing distributed ownership is both the most complicated and\n\nmost important part of designing a reliable distributed system.\n\nDetermining If You Even Need Leader\n\nElection\n\nThe simplest form of ownership is to just have a single replica of the\n\nservice. Since there is only one instance running at a time, that instance\n\nimplicitly owns everything without any need for election. This has\n\nadvantages of simplifying your application and deployment, but it has\n\ndisadvantages in terms of downtime and reliability. However, for many\n\napplications, the simplicity of this singleton pattern may be worth the\n\nreliability trade-off. Let’s look at this further.\n\nAssuming that you run your singleton in a container orchestration system\n\nlike Kubernetes, you have the following guarantees:\n\nIf the container crashes, it will automatically be restarted.\n\nIf the container hangs, and you implement a health check, it will\n\nautomatically be restarted.\n\nIf the machine fails, the container will be moved to a different machine.\n\nBecause of these guarantees, a singleton of a service running in a container\n\norchestrator has pretty good uptime. To take the definition of “pretty good”",
      "content_length": 1186,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 202,
      "content": "a little further, let’s examine what happens in each of these failure modes. If\n\nthe container process fails or the container hangs, your application will be\n\nrestarted in a few seconds. Assuming your container crashes once a day, this\n\nis roughly three to four nines of uptime (2 seconds of downtime / day ~=\n\n99.99% uptime). If your container crashes less often, it’s even better than\n\nthat. If your machine fails, it takes a while for Kubernetes to decide that the\n\nmachine has failed and move your workload over to a different machine;\n\nlet’s assume that takes around 5 minutes. Given that, if every machine in\n\nyour cluster fails every day, then your service will have two nines of\n\nuptime. And honestly, if every machine in your cluster fails every day, then\n\nyou have way worse problems than the uptime of your leader-elected\n\nservice.\n\nIt’s worth considering, of course, that there are more reasons for downtime\n\nthan just failures. When you are rolling out new software, it takes time to\n\ndownload and start the new version. With a singleton, you cannot have both\n\nold and new versions running at the same time, so you will need to take\n\ndown the old version for the duration of the upgrade, which may be several\n\nminutes if your image is large. Consequently, if you deploy daily and it\n\ntakes two minutes to upgrade your software, you will only be able to run a\n\ntwo nines service, and if you deploy hourly, it won’t even be a single nine\n\nservice. Of course, there are ways that you can speed up your deployment\n\nby pre-pulling the new image onto the machine before you run the update.\n\nThis can reduce the time it takes to deploy a new version to a few seconds,",
      "content_length": 1673,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 203,
      "content": "but the trade-off is added complexity, which was what we were trying to\n\navoid in the first place.\n\nRegardless, there are many applications (e.g., background asynchronous\n\nprocessing) where such an SLA is an acceptable trade-off for application\n\nsimplicity. One of the key components of designing a distributed system is\n\ndeciding when the “distributed” part is actually unnecessarily complex. But\n\nthere are certainly situations where high availability (four+ nines) is a\n\ncritical component of the application, and in such systems, you need to run\n\nmultiple replicas of the service, where only one replica is the designated\n\nowner. The design of these types of systems is described in the sections that\n\nfollow.\n\nThe significant downside of just using a singleton is that for the period that\n\nthe leader is down, your entire application will be unable to process\n\nrequests. While you may be able to achieve a reasonable uptime measured\n\nacross a day, for the seconds or minutes that you might be down, your\n\nservice is completely down. For a system which is interactive, like a game,\n\nretail, or banking system, being completely down even for a short period of\n\ntime will likely lead to significant customer dissatisfaction.",
      "content_length": 1226,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 204,
      "content": "The Basics of Leader Election\n\nImagine that there is a service Foo with three replicas: Foo-1, Foo-2, and\n\nFoo-3. There is also some object Bar that must only be “owned” by one of\n\nthe replicas (e.g., Foo-1) at a time. Often this replica is called the leader,\n\nhence the term leader election used to describe the process of how this\n\nleader is selected as well as how a new leader is selected if that leader fails.\n\nThere are two ways to implement this leader election. This first is to\n\nimplement a distributed consensus algorithm like Paxos or RAFT, but the\n\ncomplexity of these algorithms make them beyond the scope of this book\n\nand not worthwhile to implement. Implementing one of these algorithms is\n\nakin to implementing locks on top of assembly code compare-and-swap\n\ninstructions. It’s an interesting exercise for an undergraduate computer\n\nscience course, but it is not something that is generally worth doing in\n\npractice.\n\nFortunately, there are a large number of distributed key-value stores that\n\nhave implemented such consensus algorithms for you. At a general level,\n\nthese systems provide a replicated, reliable data store and the primitives\n\nnecessary to build more complicated locking and election abstractions on\n\ntop. Examples of these distributed stores include etcd, ZooKeeper, and\n\nConsul. The basic primitives that these systems provide is the ability to\n\nperform a compare-and-swap operation for a particular key. Compare-and-",
      "content_length": 1452,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 205,
      "content": "swap atomically writes a new value if the existing value matches the\n\nexpected value. If the value doesn’t match, it returns false . If the value\n\ndoesn’t exist and currentValue is not null, it returns an error. If you\n\nhaven’t seen compare-and-swap before, it is basically an atomic operation\n\nthat looks like this:\n\nlock := sync.Mutex{}\n\nstore := map[string]string{}\n\nfunc compareAndSwap(key, nextValue, currentValue\n\nlock.Lock()\n\ndefer lock.Unlock()\n\n_, containsKey := store[key]\n\nif !containsKey {\n\nif len(currentValue) == 0 {\n\nstore[key] = nextValue\n\nreturn true, nil\n\n}\n\nreturn false, fmt.Errorf(\n\n\"Expected value %s for key %s, but found em }\n\nif store[key] == currentValue {\n\nstore[key] = nextValue return true, nil\n\n}",
      "content_length": 726,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 206,
      "content": "return false, nil\n\n}\n\nIn addition to compare-and-swap, the key-value stores allow you to set a\n\ntime-to-live (TTL) for a key. Once the TTL expires, the key is set back to\n\nempty.\n\nPut together, these functions are sufficient to implement a variety of\n\ndistributed synchronization primitives.\n\nHands On: Deploying etcd\n\netcd is a distributed lock server originally developed by CoreOS and\n\nnow donated to the Cloud Native Computing Foundation (CNCF) and\n\nmaintained by the community. It is robust and proven in production at high\n\nscale, and is used by a variety of projects, including Kubernetes.\n\nThe easiest way to run etcd is to run it locally. The project makes\n\navailable binaries for Linux, MacOS X, and Windows. You can download\n\nthose binaries from GitHub; version 3.5.16 was the latest at the time of\n\nwriting, but you may want to check for later versions.\n\nOnce you have downloaded the archive for the release, you can unpack it,\n\nand you will find three executable binaries, etcd, etcdctl, and etcdutl, as\n\nwell as some documentation. For the purposes of this exercise, you can just",
      "content_length": 1093,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 207,
      "content": "run etcd locally by running the command etcd ; this will start etcd\n\nrunning on localhost (127.0.0.1).\n\nObviously, running etcd locally is not a very robust or permanent\n\nsolution. If you would rather, you can run etcd inside a Kubernetes\n\ncluster using the open source pacakge manager Helm. If you don’t already\n\nhave Helm installed, you can do so from the Helm website.\n\nOnce you have the helm tool installed in your environment, you can\n\ninstall the etcd operator using helm , as follows:\n\n# Initialize helm\n\nhelm init\n\n# Install etcd using helm\n\nhelm install my-release oci://registry-1.docker.i\n\nOnce the etcd chart is installed, you can use kubectl get pods to\n\nsee the replicas in your etcd cluster.\n\nImplementing Locks\n\nThe simplest form of synchronization is the mutual exclusion lock (a.k.a.\n\nmutex). Anyone who has done concurrent programming on a single\n\nmachine is familiar with locks, and the same concept can be applied to",
      "content_length": 937,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 208,
      "content": "distributed replicas. Instead of local memory and assembly instructions,\n\nthese distributed locks can be implemented in terms of the distributed key-\n\nvalue stores described previously.\n\nAs with locks in memory, the first step is to acquire the lock:\n\nfunc (Lock l) simpleLock() boolean {\n\n// compare and swap \"1\" for \"0\"\n\nlocked, _ = compareAndSwap(l.lockName, \"1\", \"0\"\n\nreturn locked\n\n}\n\nBut of course, it’s possible that the lock doesn’t already exist, because we\n\nare the first to claim it, so we need to handle that case, too:\n\nfunc (Lock l) simpleLock() boolean {\n\n// compare and swap \"1\" for \"0\"\n\nlocked, error = compareAndSwap(l.lockName, \"1\",\n\n// lock doesn't exist, try to write \"1\" with a\n\n// non-existent if error != nil { locked, _ = compareAndSwap(l.lockName, \"1\", n } return locked }",
      "content_length": 798,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 209,
      "content": "Traditional lock implementations block until the lock is acquired, so we\n\nactually want something like this:\n\nfunc (Lock l) lock() {\n\nwhile (!l.simpleLock()) {\n\nsleep(2)\n\n} }\n\nThis implementation, though simple, has the problem that you will always\n\nwait at least a second after the lock is released before you acquire the lock.\n\nFortunately, many key-value stores let you watch for changes instead of\n\npolling, so you can implement:\n\nfunc (Lock l) lock() {\n\nwhile (!l.simpleLock()) {\n\nwaitForChanges(l.lockName)\n\n}\n\n}\n\nGiven this locking function, we can also implement unlock:\n\nfunc (Lock l) unlock() {\n\ncompareAndSwap(l.lockName, \"0\", \"1\")\n\n}",
      "content_length": 645,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 210,
      "content": "You might now think that we are done, but remember that we are building\n\nthis for a distributed system. A process could fail in the middle of holding\n\nthe lock, and at that point there is no one left to release it. In such a\n\nsituation, our system will become stuck. To resolve this, we take advantage\n\nof the TTL functionality of the key-value store. We change our\n\nsimpleLock function so that it always writes with a TTL, so if we don’t\n\nunlock within a given time, the lock will automatically unlock:\n\nfunc (Lock l) simpleLock() boolean {\n\n// compare and swap \"1\" for \"0\"\n\nlocked, error = compareAndSwap(l.lockName, \"1\",\n\n// lock doesn't exist, try to write \"1\" with a\n\n// non-existent\n\nif error != nil {\n\nlocked, _ = compareAndSwap(l.lockName, \"1\", n\n\n}\n\nreturn locked\n\n}\n\nNOTE\n\nWhen using distributed locks, it is critical to ensure that any processing you do doesn’t last longer\n\nthan the TTL of the lock. One good practice is to set a watchdog timer when you acquire the lock.\n\nThe watchdog contains an assertion that will crash your program if the TTL of the lock expires\n\nbefore you have called unlock . If you don’t do this, you may perform computation when you have\n\nlost the lock.",
      "content_length": 1192,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 211,
      "content": "By adding TTL to our locks, we have actually introduced a bug into our\n\nunlock function. Consider the following scenario:\n\n1. Process-1 obtains the lock with TTL t.\n\n2. Process-1 runs really slowly for some reason, for longer than t.\n\n3. The lock expires.\n\n4. Process-2 acquires the lock, since Process-1 has lost it due to TTL.\n\n5. Process-1 finishes and calls unlock.\n\n6. Process-3 acquires the lock.\n\nAt this point, Process-1 believes that it has unlocked the lock that it held at\n\nthe beginning; it doesn’t understand that it has actually lost the lock due to\n\nTTL, and in fact unlocked the lock held by Process-2. Then Process-3\n\ncomes along and also grabs the lock. Now both Process-2 and Process-3\n\nbelieve they own the lock, which means that your system likely has\n\nunexpected failures or data corruption.\n\nFortunately, the key-value store provides a resource version for every write\n\nthat is performed. Whenever a new write is performed, the resource version\n\nchanges. We can use this resource version as a precondition for any\n\nsubsequent write to ensure ordering. Our lock function stores the resource\n\nversion and uses it in compareAndSwap to ensure that not only is the\n\nvalue as expected, but the resource version is the same as when the lock\n\noperation occurred. This changes our simple Lock function to look like\n\nthis:",
      "content_length": 1335,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 212,
      "content": "func (Lock l) simpleLock() boolean {\n\n// compare and swap \"1\" for \"0\"\n\nlocked, l.version, error = compareAndSwap(l.loc\n\n// lock doesn't exist, try to write \"1\" with a // non-existent\n\nif error != null {\n\nlocked, l.version, _ = compareAndSwap(l.lockN\n\n} return locked\n\n}\n\nAnd the unlock function then looks like this:\n\nfunc (Lock l) unlock() {\n\ncompareAndSwap(l.lockName, \"0\", \"1\", l.version)\n\n}\n\nThis ensures that the lock is only unlocked if the TTL has not expired. The\n\nlock cannot be unlocked accidentally if someone else has acquired it after\n\nthe TTL expired. In such a case, the new owner of the lock has to have\n\nwritten a value into the storage system which results in a new resource\n\nversion, and our attempt to unlock using the old resource version will fail.",
      "content_length": 770,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 213,
      "content": "Hands On: Implementing Locks in etcd\n\nTo implement locks in etcd , you can use a key as the name of the lock\n\nand pre-condition writes to ensure that only one lock holder is allowed at a\n\ntime. For simplicity, we’ll use the etcdctl command line to lock and\n\nunlock the lock. In reality, of course, you would want to use a programming\n\nlanguage; there are etcd clients for most popular programming\n\nlanguages.\n\nLet’s start by creating a lock named my-lock :\n\nkubectl exec my-etcd-cluster-0000 -- sh -c \\\n\n\"ETCD_API=3 etcdctl --endpoints=${ETCD_ENDPOINT\n\nThis creates a key in etcd named my-lock and sets the initial value to\n\nunlocked .\n\nNow let’s suppose that Alice and Bob both want to take ownership of my-\n\nlock . Alice and Bob both try to write their name to the lock, using a\n\nprecondition that the value of the lock is unlocked .\n\nAlice first runs:\n\nkubectl exec my-etcd-cluster-0000 -- sh -c \\ \"ETCD_API=3 etcdctl --endpoints=${ETCD_ENDPOINT",
      "content_length": 948,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 214,
      "content": "set --swap-with-value unlocked my-lock alic\n\nAnd obtains the lock. Now Bob attempts to obtain the lock:\n\nkubectl exec my-etcd-cluster-0000 -- sh -c \\\n\n\"ETCD_API=3 etcdctl --endpoints=${ETCD_ENDPOINT\n\nset --swap-with-value unlocked my-lock bob\"\n\nError: 101: Compare failed ([unlocked != alice])\n\nYou can see that Bob’s attempt to claim the lock has failed, since Alice\n\ncurrently owns the lock.\n\nTo unlock the lock, Alice writes unlocked with a precondition value of\n\nalice :\n\nkubectl exec my-etcd-cluster-0000 -- sh -c \\\n\n\"ETCD_API=3 etcdctl --endpoints=${ETCD_ENDPOINT\n\nset --swap-with-value alice my-lock unlocke\n\nImplementing Ownership\n\nWhile locks are great for establishing temporary ownership of some critical\n\ncomponent, sometimes you want to take ownership for the duration of the\n\ntime that the component is running. For example, in a highly available",
      "content_length": 860,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 215,
      "content": "deployment of Kubernetes, there are multiple replicas of the scheduler but\n\nonly one replica is actively making scheduling decisions. Further, once it\n\nbecomes the active scheduler, it remains the active scheduler until that\n\nprocess fails for some reason.\n\nObviously, one way to do this would be to extend the TTL for the lock to a\n\nvery long period (say a week or longer), but this has the significant\n\ndownside that if the current lock owner fails, a new lock owner wouldn’t be\n\nchosen until the TTL expired a week later.\n\nInstead, we need to create a renewable lock, which can be periodically\n\nrenewed by the owner so that the lock can be retained for an arbitrary\n\nperiod of time.\n\nWe can extend the existing Lock that we defined previously to create a\n\nrenewable lock, which enables the lock holder to renew the lock:\n\nfunc (Lock l) renew() boolean {\n\nlocked, _ = compareAndSwap(l.lockName, \"1\", \"1\"\n\nreturn locked }\n\nOf course, you probably want to do this repeatedly in a separate thread so\n\nthat you hold onto the lock indefinitely. Notice that the lock is renewed",
      "content_length": 1073,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 216,
      "content": "every ttl/2 seconds; that way there is significantly less risk that the lock\n\nwill accidentally expire due to timing subtleties:\n\nfor {\n\nif !l.renew() {\n\nhandleLockLost()\n\n}\n\nsleep(ttl/2)\n\n}\n\nOf course, you need to implement the handleLockLost() function so\n\nthat it terminates all activity that required the lock in the first place. In a\n\ncontainer orchestration system, the easiest way to do this may simply be to\n\nterminate the application and let the orchestrator restart it. This is safe,\n\nbecause some other replica has grabbed the lock in the interim, and when\n\nthe restarted application comes back online it will become a secondary\n\nlistener waiting for the lock to become free.\n\nHands On: Implementing Leases in etcd\n\nTo see how we implement leases using etcd , we will return to our earlier\n\nlocking example and add the --ttl=<seconds> flag to our lock create\n\nand update calls. The ttl flag defines a time after which the lock that we\n\ncreate is deleted. Because the lock disappears after the ttl expires,\n\ninstead of creating with the value of unlocked, we will assume that the",
      "content_length": 1089,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 217,
      "content": "absence of the lock means that it is unlocked. To do this, we use the mk\n\ncommand instead of the set command. etcdctl mk only succeeds if\n\nthe key does not currently exist.\n\nThus, to lock a leased lock, Alice executes:\n\nkubectl exec my-etcd-cluster-0000 -- \\\n\nsh -c \"ETCD_API=3 etcdctl --endpoints=${ETCD_\n\n--ttl=10 mk my-lock alice\"\n\nThis creates a leased lock with a duration of 10 seconds.\n\nFor Alice to continue to hold the lock, she needs to execute:\n\nkubectl exec my-etcd-cluster-0000 -- \\\n\nsh -c \"ETCD_API=3 etcdctl --endpoints=${ETCD_\n\nset --ttl=10 --swap-with-value alice my-l\n\nIt may seem odd that Alice is continually rewriting her own name into the\n\nlock, but this is the way the lock lease is extended beyond the 10-second\n\nTTL.\n\nIf, for some reason, the TTL expires, then the lock update will fail, and\n\nAlice will go back to creating the lock using the etcd mk command, or\n\nBob may also use the mk command to obtain the lock for himself. Bob",
      "content_length": 956,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 218,
      "content": "will likewise need to set and update the lock every 10 seconds to maintain\n\nownership.\n\nHandling Concurrent Data Manipulation\n\nEven with all of the locking mechanisms we have described, it is still\n\npossible for two replicas to simultaneously believe they hold the lock for a\n\nvery brief period of time. To understand how this can happen, imagine that\n\nthe original lock holder becomes so overwhelmed that its processor stops\n\nrunning for minutes at a time. This can happen on extremely overscheduled\n\nmachines. In such a case, the lock will time out and some other replica will\n\nown the lock. Now the processor frees up the replica that was the original\n\nlock holder. Obviously, the handleLockLost() function will quickly\n\nbe called, but there will be a brief period where the replica still believes it\n\nholds the lock. Although such an event is fairly unlikely, systems need to be\n\nbuilt to be robust to such occurrences. The first step to take is to double-\n\ncheck that the lock is still held, using a function like this:\n\nfunc (Lock l) isLocked() boolean { return l.locked && l.lockTime + 0.75 * l.ttl >\n\n}\n\nIf this function executes prior to any code that needs to be protected by a\n\nlock, then the probability of two leaders being active is significantly",
      "content_length": 1260,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 219,
      "content": "reduced, but—it is important to note—it is not completely eliminated. The\n\nlock timeout could always occur between the time that the lock was\n\nchecked and the guarded code was executed. To protect against these\n\nscenarios, the system that is being called from the replica needs to validate\n\nthat the replica sending a request is actually still the leader. To do this, the\n\nhostname of the replica holding the lock is stored in the key-value store in\n\naddition to the state of the lock. That way, others can double-check that a\n\nreplica asserting that it is the leader is in fact the leader.\n\nThis system diagram is shown in Figure 10-2. In the image, shard2 is\n\nthe owner of the lock, and when a request is sent to the worker, the worker\n\ndouble-checks with the lock server and validates that shard2 is actually\n\nthe current owner.\n\nFigure 10-2. A worker double-checking to validate that the requester who sent a message is actually the current owner of the shard",
      "content_length": 963,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 220,
      "content": "In the second case, shard2 has lost ownership of the lock, but it has not\n\nyet realized this, so it continues to send requests to the worker node. This\n\ntime, when the worker node receives a request from shard2 , it double-\n\nchecks with the lock service and realizes that shard2 is no longer the\n\nlock owner, and thus the requests are rejected.\n\nTo add one final further complicating wrinkle, it’s always possible that\n\nownership could be obtained, lost, and then re-obtained by the system,\n\nwhich could actually cause a request to succeed when it should actually be\n\nrejected. To understand how this is possible, consider the following\n\nsequence of events:\n\n1. shard1 obtains ownership to become leader.\n\n2. shard1 sends a request R1 as leader at time T1 .\n\n3. The network hiccups and delivery of R1 is delayed.\n\n4. shard1 fails TTL because of the network and loses lock to shard2 .\n\n5. shard2 becomes leader and sends a request R2 at time T2 .\n\n6. Request R2 is received and processed.\n\n7. shard2 crashes and loses ownership back to shard1 .\n\n8. Request R1 finally arrives, and shard1 is the current leader, so it is\n\naccepted, but this is bad because R2 has already been processed.\n\nSuch sequences of events seem byzantine, but in reality, in any large system\n\nthey occur with disturbing frequency. Fortunately, this is similar to the case\n\ndescribed previously, which we resolved with the resource version in",
      "content_length": 1412,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 221,
      "content": "etcd . We can do the same thing here. In addition to storing the name of\n\nthe current owner in etcd , we also send the resource version along with\n\neach request. So in the previous example, R1 becomes (R1,\n\nVersion1) . Now when the request is received, the double-check\n\nvalidates both the current owner and the resource version of the request. If\n\neither match fails, the request is rejected. This patches up this example.\n\nUltimately, in designing a distributed ownership system, you have to choose\n\nbetween at-most once processing, which is possible, but requires you to\n\npossibly lose availability for a period of time. Or you can have at least once\n\nprocessing, which can provide higher availability, but will occasionally\n\nresult in double processing of some request. Depending on the type of\n\nsystem you are building, duplicate processing may simply waste some\n\nresources (e.g., compute time on a GPU); in which case, if it is infrequent, it\n\nis likely acceptable. However, some systems may involve actual\n\ntransactions (e.g., transferring money between accounts) where you would\n\nrather lose availability but preserve consistency. Understanding these trade-\n\noffs is a critical part of distributed system design.\n\nSummary\n\nIn this chapter we have explored one of the most complicated parts of\n\ndistributed system design, distributed ownership and locks. While parallel\n\nconcurrent processing is a key part of building reliable, scalable systems,",
      "content_length": 1453,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 222,
      "content": "the concurrency also introduces challenges in systems where ownership of\n\nspecific work is important. Distributed ownership or locks enable pieces of\n\nyour distributed system to maintain exclusive access to a particular part of\n\nthe data store or compute operations. Implementing locks involves\n\ninteracting with consensus-based distributed storage, like etcd or\n\nZooKeeper, with very specific coding patterns. In many languages, code for\n\ndistributed locks have already been implemented. Since writing this code\n\nyourself is a challenge, if there’s already a library for your programming\n\nlanguage, it’s a best practice to rely on pre-existing production-grade code.\n\nFinally, some simple systems don’t need locks and can get away with using\n\nSingletons and in-process concurrency. Such systems trade availability for\n\nsimplicity.\n\nOceanofPDF.com",
      "content_length": 847,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 223,
      "content": "Part IV. Batch Computational Patterns\n\nThe preceding chapters described patterns for reliable, long-running server\n\napplications. This section describes patterns for batch processing. In\n\ncontrast to long-running applications, batch processes are expected to only\n\nrun for a short period of time. Examples of a batch process include\n\ngenerating aggregation of user telemetry data, analyzing sales data for daily\n\nor weekly reporting, or transcoding video files. Batch processes are\n\ngenerally characterized by the need to process large amounts of data\n\nquickly using parallelism to speed up the processing. The most famous\n\npattern for distributed batch processing is the MapReduce pattern, which\n\nhas become an entire industry in itself. However, there are several other\n\npatterns that are useful for batch processing, which are described in the\n\nfollowing chapters.\n\nOceanofPDF.com",
      "content_length": 883,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 224,
      "content": "Chapter 11. Work Queue Systems\n\nThe simplest form of batch processing is a work queue. In a work queue\n\nsystem, there is a batch of work to be performed. Each piece of work is\n\nwholly independent of the other and can be processed without any\n\ninteractions. Generally, the goals of the work queue system are to ensure\n\nthat each piece of work is processed within a certain amount of time.\n\nWorkers are scaled up or scaled down to ensure that the work can be\n\nhandled. An illustration of a generic work queue is shown in Figure 11-1.\n\nFigure 11-1. A generic work queue\n\nA Generic Work Queue System\n\nThe work queue is an ideal way to demonstrate the power of distributed\n\nsystem patterns. Most of the logic in the work queue is wholly independent\n\nof the actual work being done, and in many cases the delivery of the work",
      "content_length": 818,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 225,
      "content": "can be performed in an independent manner as well. To illustrate this point,\n\nconsider the work queue illustrated in Figure 11-1. If we take a look at it\n\nagain and identify functionality that can be provided by a shared set of\n\nlibrary containers, it becomes apparent that most of the implementation of\n\na containerized work queue can be shared across a wide variety of users, as\n\nshown in Figure 11-2.\n\nFigure 11-2. The same work queue as shown in Figure 11-1, but this time using reusable containers; the reusable system containers are shown in white, while the user-supplied container is shown in the darker shaded boxes\n\nBuilding a reusable container-based work queue requires the definition of\n\ninterfaces between the generic library containers and the user-defined\n\napplication logic. In the containerized work queue, there are two interfaces:",
      "content_length": 850,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 226,
      "content": "the source container interface, which provides a stream of work items that\n\nneed processing, and the worker container interface, which knows how to\n\nactually process a work item.\n\nThe Source Container Interface\n\nTo operate, every work queue needs a collection of work items that need\n\nprocessing. There are many different sources of work items for the work\n\nqueue, depending on the specific application of the work queue. However,\n\nonce the set of items has been obtained, the actual operation of the work\n\nqueue is quite generic. Consequently, we can separate the application-\n\nspecific queue source logic from the generic queue processing logic. Given\n\nthe previously defined patterns of container groups, this can be seen as an\n\nexample of the ambassador pattern defined previously. The generic work\n\nqueue container is the primary application container, and the application-\n\nspecific source container is the ambassador that proxies the generic work\n\nqueue’s requests out to the concrete definition of the work queue out in the\n\nreal world. This container group is illustrated in Figure 11-3.",
      "content_length": 1096,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 227,
      "content": "Figure 11-3. The work queue container group\n\nInterestingly, while the ambassador container is clearly application-specific,\n\nthere is also a variety of generic implementations of the work queue source\n\nAPI. For example, a source might be a list of pictures stored in a cloud\n\nstorage API, a collection of files stored on network storage, or a queue in a\n\npub/sub system like Kafka or Redis. In these cases, though the user chooses\n\nthe particular work queue ambassador that fits their scenario, they should be\n\nreusing a single generic “library” implementation of the container itself.\n\nThis minimizes work and maximizes code reuse.\n\nWork Queue API\n\nGiven this coordination between the generic work-queue manager and the\n\napplication-specific ambassador, we need a formal definition of the\n\ninterface between the two containers. Though there are a variety of different\n\nprotocols, an HTTP RESTful API is both the easiest to implement as well as",
      "content_length": 944,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 228,
      "content": "the de facto standard for such an interface. The coordinating work queue\n\nexpects the ambassador to implement the following URLs:\n\nGET http://localhost/api/v1/items\n\nGET http://localhost/api/v1/items/<item-name>\n\nNOTE\n\nYou might wonder why we include a v1 in the API definition. Will there ever be a v2 of this\n\ninterface? It may not seem logical, but it costs very little to version your API when you initially\n\ndefine it. Refactoring versioning onto an API without it, on the other hand, is very expensive.\n\nConsequently, it is a best practice to always add versions to your APIs even if you’re not sure they\n\nwill ever change. Better safe than sorry.\n\nThe /items/ URL returns a complete list of all items:\n\n{\n\nkind: ItemList,\n\napiVersion: v1,\n\nitems: [ \"item-1\",\n\n\"item-2\",\n\n…. ]\n\n}\n\nThe /items/<item-name> URL provides the details for a specific item:",
      "content_length": 855,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 229,
      "content": "{ kind: Item,\n\napiVersion: v1,\n\ndata: { \"some\": \"json\",\n\n\"object\": \"here\",\n\n} }\n\nImportantly, you will notice that this API doesn’t have any affordances for\n\nrecording that a work item has been processed. We could have designed a\n\nmore complicated API and then pushed more implementation into the\n\nambassador container, but remember, the goal of this effort is to place as\n\nmuch of the generic implementation inside of the generic work queue\n\nmanager as possible. To that end, the work queue manager itself is\n\nresponsible for tracking which items have been processed and which items\n\nremain to be processed.\n\nThe item details are obtained from this API, and the item.data field is\n\npassed along to the worker interface for processing.\n\nThe Worker Container Interface\n\nOnce a particular work item has been obtained by the work queue manager,\n\nit needs to be processed by a worker. This is the second container interface",
      "content_length": 919,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 230,
      "content": "in our generic work queue. This container and interface are slightly\n\ndifferent than the previous work queue source interface for a few reasons.\n\nThe first is that it is a one-off API: a single call is made to begin the work,\n\nand no other API calls are made throughout the life of the worker container.\n\nSecondly, the worker container is not inside a container group with the\n\nwork queue manager. Instead, it is launched via a container orchestration\n\nAPI and scheduled to its own container group. This means that the work\n\nqueue manager has to make a remote call to the worker container in order to\n\nstart work. It also means that we may need to be more careful about security\n\nto prevent a malicious user in our cluster from injecting extra work into the\n\nsystem.\n\nWith the work queue source API, we used a simple HTTP-based API for\n\nsending items back to the work queue manager. This was because we\n\nneeded to make repeated calls to the API, and security wasn’t a concern\n\nsince everything was running on localhost. With the worker container, we\n\nonly need to make a single call, and we want to ensure that other users in\n\nthe system can’t accidentally or maliciously add work to our workers.\n\nConsequently, for the worker container, we will use a file-based API.\n\nNamely, when the worker container is created, it will receive an\n\nenvironment variable named WORK_ITEM_FILE ; this will point to a file\n\nin the container’s local filesystem, where the data field from a work item\n\nhas been written to a file. Concretely, as you will see below, this API can be\n\nimplemented via a Kubernetes ConfigMap object that can be mounted\n\ninto a container group as a file, as illustrated in Figure 11-4.",
      "content_length": 1693,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 231,
      "content": "Figure 11-4. The work queue worker API\n\nThis file-based API pattern is also easier for the container to implement.\n\nOften a work queue worker is simply a shell script across a few command-\n\nline tools. In that context, spinning up a web server to manage the work to\n\nperform is an unnecessary complexity. As was true with the work queue\n\nsource implementation, most of the worker containers will be special-\n\npurpose container images built for specific work queue applications, but\n\nthere are also generic workers that can be applied to multiple different work\n\nqueue applications.\n\nConsider the example of a work queue worker that downloads a file from\n\ncloud storage and runs a shell script with that file as input, and then copies\n\nits output back up to cloud storage. Such a container can be mostly generic\n\nbut then have the specific script to execute supplied to it as a runtime\n\nparameter. In this way, most of the work of file handling can be shared by\n\nmultiple users/work queues and only the specifics of file processing need to\n\nbe supplied by the end user.",
      "content_length": 1068,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 232,
      "content": "The Shared Work Queue Infrastructure\n\nGiven implementations of the two container interfaces described previously,\n\nwhat is left to implement our reusable work queue implementation? The\n\nbasic algorithm for the work queue is fairly straightforward:\n\n1. Load the available work by calling into source container interface.\n\n2. Consult with work queue state to determine which work items have been\n\nprocessed or are being processed currently.\n\n3. For these items, spawn jobs that use the worker container interface to\n\nprocess the work item.\n\n4. When one of these worker containers finishes successfully, record that\n\nthe work item has been completed.\n\nWhile this algorithm is simple to express in words, it is somewhat more\n\ncomplicated to implement in reality. Fortunately for us, the Kubernetes\n\ncontainer orchestrator contains a number of features that make it\n\nsignificantly easier to implement. Namely, Kubernetes contains a Job\n\nobject that allows for the reliable execution of the work queue. The Job\n\ncan be configured to either run the worker container once or to run it until it\n\ncompletes successfully. If the worker container is set to run to completion,\n\nthen even if a machine in the cluster fails, the job will eventually be run to\n\nsuccess. This dramatically simplifies the task of building a work queue\n\nbecause the orchestrator takes on responsibility for the reliable operation of\n\neach work item.",
      "content_length": 1413,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 233,
      "content": "Additionally, Kubernetes has annotations for each Job object that enable\n\nus to mark each job with the work item it is processing. This enables us to\n\nunderstand which items are being processed as well as those that have\n\ncompleted in either failure or success.\n\nPut together, this means that we can implement a work queue on top of the\n\nKubernetes orchestration layer without using any storage of our own.\n\nThus, the expanded operation of our work queue container looks like this:\n\nRepeat forever\n\nGet the list of work items from the work sour\n\nGet the list of all jobs that have been creat\n\nqueue.\n\nDifference these lists to find the set of wor\n\nbeen processed.\n\nFor these unprocessed items, create new Job o\n\nappropriate worker container.\n\nHere is a simple Python script that implements this work queue:\n\nimport requests import json\n\nfrom kubernetes import client, config import time",
      "content_length": 886,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 234,
      "content": "namespace = \"default\"\n\ndef make_container(item, obj):\n\ncontainer = client.V1Container()\n\ncontainer.image = \"my/worker-image\"\n\ncontainer.name = \"worker\" return container\n\ndef make_job(item):\n\nresponse = requests.get(\"http://localhost:800\n\nobj = json.loads(response.text)\n\njob = client.V1Job()\n\njob.metadata = client.V1ObjectMeta()\n\njob.metadata.name = item\n\njob.spec = client.V1JobSpec()\n\njob.spec.template = client.V1PodTemplate()\n\njob.spec.template.spec = client.V1PodTemplate\n\njob.spec.template.spec.restart_policy = \"Neve\n\njob.spec.template.spec.containers = [\n\nmake_container(item, obj)\n\n]\n\nreturn job\n\ndef update_queue(batch):\n\nresponse = requests.get(\"http://localhost:800\n\nobj = json.loads(response.text)\n\nitems = obj['items']",
      "content_length": 733,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 235,
      "content": "ret = batch.list_namespaced_job(namespace, wa\n\nfor item in items:\n\nfound = False\n\nfor i in ret.items: if i.metadata.name == item:\n\nfound = True\n\nif not found:\n\n# This function creates the job objec\n\n# brevity\n\njob = make_job(item)\n\nbatch.create_namespaced_job(namespace\n\nconfig.load_kube_config()\n\nbatch = client.BatchV1Api()\n\nwhile True:\n\nupdate_queue(batch)\n\ntime.sleep(10)",
      "content_length": 375,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 236,
      "content": "Hands On: Implementing a Video\n\nThumbnailer\n\nTo provide a concrete example of how we might use a work queue, consider\n\nthe task of generating thumbnails for videos. These thumbnails help users\n\ndetermine which videos they want to watch.\n\nTo implement this video thumbnailer, we need two different user containers.\n\nThe first is the work item source container. The simplest way for this to\n\nwork is for the work items to appear on a shared disk, such as a Network\n\nFile System (NFS) share. The work item source simply lists the files in this\n\ndirectory and returns them to the caller. Here’s a simple node program\n\nthat does this:\n\nconst http = require('http');\n\nconst fs = require('fs');\n\nconst port = 8080;\n\nconst path = process.env.MEDIA_PATH;\n\nconst requestHandler = (request, response) => {\n\nconsole.log(request.url); fs.readdir(path + '/*.mp4', (err, items)\n\nvar msg = {\n\n'kind': 'ItemList',",
      "content_length": 896,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 237,
      "content": "'apiVersion': 'v1',\n\n'items': []\n\n};\n\nif (!items) {\n\nreturn msg;\n\n}\n\nfor (var i = 0; i < items.length; msg.items.push(items[i]);\n\n}\n\nresponse.end(JSON.stringify(msg))\n\n});\n\n}\n\nconst server = http.createServer(requestHandler);\n\nserver.listen(port, (err) => {\n\nif (err) {\n\nreturn console.log('Error startin\n\n}\n\nconsole.log(`server is active on ${port}`\n\n});\n\nThis source of defines the queue of movies to thumbnail. We use the\n\nffmpeg utility to actually perform the thumbnailing work.\n\nYou can create a container that uses the following as its command line:",
      "content_length": 556,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 238,
      "content": "ffmpeg -i ${INPUT_FILE} -frames:v 100 thumb.png\n\nThis command will take one frame every 100 frames (that’s the -\n\nframes:v 100 flag) and output it as a PNG file (e.g., thumb1.png ,\n\nthumb2.png , etc.).\n\nYou can create the ffmpeg container using a base image of your choice\n\nand installing the ffmpeg executable in that container. There is also an\n\nopen source project with Dockerfiles for many popular operating systems\n\non GitHub. You can use those scripts as a starting point for your container\n\nimage.\n\nBy defining a simple source container as well as an even simpler worker\n\ncontainer, we can clearly see the power and utility of a generic container-\n\nbased queuing system. It dramatically reduces the time/distance between an\n\nidea for implementing a work queue and the corresponding concrete\n\nimplementation.\n\nDynamic Scaling of the Workers\n\nThe previously described work queue is great for processing work items as\n\nquickly as they arrive in the work queue, but this can lead to bursty resource\n\nloads being placed onto a container orchestrator cluster. This is good if you\n\nhave a lot of different workloads that will burst at different times and thus",
      "content_length": 1159,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 239,
      "content": "keep your infrastructure evenly utilized. But if you don’t have a sufficient\n\nnumber of different workloads, this feast or famine approach to scaling your\n\nwork queue might require that you over-provision resources to support the\n\nbursts that will lie idle (and cost too much money) while you don’t have\n\nwork to perform.\n\nTo address this problem, you can limit the overall number of Job objects\n\nthat your work queue is willing to create. This will naturally serve to limit\n\nthe number of work items you process in parallel and consequentially limit\n\nthe maximum amount of resources that you use at a particular time.\n\nHowever, doing this will increase the time to completion (latency) for each\n\nwork item being completed when under heavy load. If the load is bursty,\n\nthen this is probably OK because you can use the slack times to catch up\n\nwith the backlog that developed during a burst of usage. However, if your\n\nsteady-state usage is too high, your work queue may never be able to catch\n\nup, and the time to completion will simply get longer and longer.\n\nWhen your work queue is faced with this situation, you need to have it\n\ndynamically adjust itself to increase the parallelism that it is willing to\n\ncreate (and correspondingly the resources it is willing to use) so that it can\n\nkeep up with the incoming work. Fortunately, there are mathematical\n\nformulas that we can use to determine when we need to dynamically scale\n\nup our work queue.",
      "content_length": 1451,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 240,
      "content": "Consider a work queue where a new work item arrives an average of once\n\nevery minute, and each work item takes an average of 30 seconds to\n\ncomplete. Such a system is capable of keeping up with all of the work it\n\nreceives. Even if a large batch of work arrives all at once and creates a\n\nbacklog, on average the work queue processes two work items for every\n\none work item that arrives, and thus it will be able to gradually work\n\nthrough its backlog.\n\nIf, instead, a new work item arrives on average once every minute and it\n\ntakes an average of one minute to process each work item, then the system\n\nis perfectly balanced, but it does not handle variance well. It can catch up\n\nwith bursts—but it will take a while, and it has no slack or capacity to\n\nabsorb a sustained increase in the rate at which new work items arrive. This\n\nis probably not an ideal way to run, as some safety margin for growth and\n\nother sustained increases in work (or unexpected slowdowns in processing)\n\nis needed to preserve a stable system.\n\nFinally, consider a system in which a work item arrives every minute and\n\neach item takes two minutes to process. In such a system, we are always\n\nlosing ground. The queue of work will grow without bound, and the latency\n\nof any one item in the queue will grow toward infinity (and our users will\n\nbecome very frustrated).\n\nThus, we can keep track of both of these metrics for our work queue, and\n\nthe average time between work items over an extended period of time (#",
      "content_length": 1491,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 241,
      "content": "work items / 24 hours) will give us the interarrival time for new work. We\n\ncan also keep track of the average latency, the time to process any one item\n\nonce we start working on it, not counting any time spent waiting in the\n\nqueue. To have a stable work queue, we need to adjust the number of\n\nresources so that latency is less than the interarrival time of new items. If\n\nwe are processing work items in parallel, we also divide the processing time\n\nfor a work item by the parallelism. For example, if each item takes one\n\nminute to process but we process four items in parallel, the effective time to\n\nprocess one item is 15 seconds, and thus we can sustain an interarrival\n\nperiod of 16 or more seconds.\n\nThis approach makes it fairly straightforward to build an autoscaler to\n\ndynamically size up our work queue. Sizing down the work queue is\n\nsomewhat trickier, but you can use the same math as well as a heuristic for\n\nthe amount of spare capacity for the safety margin you want to maintain.\n\nFor example, you can reduce the parallelism until the processing time for an\n\nitem is 90% of the interarrival time for new items. While in some cases it is\n\nmore efficient to build your own scaler, many times it is better to use an\n\nopen source solution. That is where the Kubernetes Event-Driven\n\nAutoscaling (KEDA) project is useful. KEDA provides numerous different\n\nevent-based scalers, which can be used to scale your application. KEDA\n\nsupports event sources ranging from files on disk to events in pub/sub\n\nsystems and many more.",
      "content_length": 1537,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 242,
      "content": "The Multiworker Pattern\n\nOne of the themes of this book has been the use of containers for\n\nencapsulation and reuse of code. The same holds true for the work queue\n\npatterns described in this chapter. In addition to the patterns for reusing\n\ncontainers for driving the work queue itself, you can also reuse multiple\n\ndifferent containers to compose a worker implementation. Suppose, for\n\nexample, that you have three different types of work that you want to\n\nperform on a particular work queue item. For example, you might want to\n\ndetect faces in an image, tag those faces with identities, and then blur the\n\nfaces in the image. You could write a single worker to perform this\n\ncomplete set of tasks, but this would be a bespoke solution that would not\n\nbe reusable the next time you want to identify something else, such as cars,\n\nyet still provide the same blurring.\n\nTo achieve this kind of code reuse, the multiworker pattern is something of\n\na specialization of the adapter pattern described in previous chapters. In this\n\ncase, the multiworker pattern transforms a collection of different worker\n\ncontainers into a single unified container that implements the worker\n\ninterface, yet delegates the actual work to a collection of different reusable\n\ncontainers. This process is illustrated in Figure 11-5.",
      "content_length": 1310,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 243,
      "content": "Figure 11-5. The multiworker aggregator pattern as a group of containers\n\nBecause of this code reuse, the composition of multiple different worker\n\ncontainers means an increase in the reuse of code and a reduction in effort\n\nfor people designing batch-oriented distributed systems.",
      "content_length": 281,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 244,
      "content": "Summary\n\nThis chapter begins the discussion of batch systems by introducing\n\nworkflows. A workflow is a sequence of discrete actions which, when put\n\ntogether in a sequence, achieve some goal like the processing of an order or\n\nthe machine-learning analysis of a video. This chapter showed how\n\ncontainers can be used to build modular workflows with sidecar containers\n\nproviding workflow orchestration while application containers supply the\n\nspecific processing necessary for your workflow. Workflow is a key part of\n\nmost distributed batch systems. Its generic pattern is easy to adapt and\n\nextend to meet the specific needs of your workload.\n\nOceanofPDF.com",
      "content_length": 661,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 245,
      "content": "Chapter 12. Event-Driven Batch\n\nProcessing\n\nIn Chapter 11, we saw a generic framework for work queue processing, as\n\nwell as a number of example applications of simple work queue processing.\n\nWork queues are great for enabling individual transformations of one input\n\nto one output. However, there are a number of batch applications where you\n\nwant to perform more than a single action, or you may need to generate\n\nmultiple different outputs from a single data input. In these cases, you start\n\nto link work queues together so that the output of one work queue becomes\n\nthe input to one or more other work queues, and so on. This forms a series\n\nof processing steps that respond to events, with the events being the\n\ncompletion of the preceding step in the work queue that came before it.\n\nThese sort of event-driven processing systems are often called workflow\n\nsystems, since there is a flow of work through a directed, acyclic graph that\n\ndescribes the various stages and their coordination. A basic illustration of\n\nsuch a system is shown in Figure 12-1.\n\nThe most straightforward application of this type of system simply chains\n\nthe output of one queue to the input of the next queue. But as systems\n\nbecome more complicated, there are a series of different patterns that\n\nemerge for linking a series of work queues together. Understanding and\n\ndesigning in terms of these patterns is important for comprehending how",
      "content_length": 1423,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 246,
      "content": "the system is working. The operation of an event-driven batch processor is\n\nsimilar to event-driven FaaS. Consequently, without an overall blueprint for\n\nhow the different event queues relate to each other, it can be hard to fully\n\nunderstand how the system is operating. Furthermore, though the initial\n\ndesign of a work queue can be straightforward, correctly handling errors\n\nand other unexpected conditions rapidly makes a simple work queue much\n\nmore complicated.",
      "content_length": 468,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 247,
      "content": "Figure 12-1. This workflow combines copying work into multiple queues (Stage 2a, 2b), parallel processing of those queues, and combining the result back into a single queue (Stage 3)",
      "content_length": 182,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 248,
      "content": "Patterns of Event-Driven Processing\n\nBeyond the simple work queue described in Chapter 11, there are a number\n\nof patterns for linking work queues together. The simplest pattern—one\n\nwhere the output of a single queue becomes the input to a second queue—is\n\nstraightforward enough that we won’t cover it here. We will describe\n\npatterns that involve the coordination of multiple different queues or the\n\nmodification of the output of one or more work queues.\n\nCopier\n\nThe first pattern for coordinating work queues is a copier. The job of a\n\ncopier is to take a single stream of work items and duplicate it out into two\n\nor more identical streams. This pattern is useful when there are multiple\n\ndifferent pieces of work to be done on the same work item. An example of\n\nthis might be rendering a video. When rendering a video, there are a variety\n\nof different formats that are useful, depending on where the video is\n\nintended to be shown. There might be a 4K high-resolution format for\n\nplaying off of a hard drive, a 1080-pixel rendering for digital streaming, a\n\nlow-resolution format for streaming to mobile users on slow networks, and\n\nan animated GIF thumbnail for displaying in a movie-picking user\n\ninterface. All of these work items can be modeled as separate work queues\n\nfor each render, but the input to each work item is identical. An illustration\n\nof the copier pattern applied to transcoding is shown in Figure 12-2.",
      "content_length": 1432,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 249,
      "content": "Figure 12-2. The copier batch pattern for transcoding\n\nThe copy pattern can also be used to trade additional computational power\n\nfor increased reliability or performance. Imagine that you have a worker\n\nprocessing work that usually takes one second to run, but occasionally takes\n\none minute (60x slowdown). If you process every work item twice and take\n\nwhichever result completes first, you are much less likely to be impacted by\n\nthis slowdown. Similarly, if your worker crashes 1% of the time, you can go\n\nfrom a two-nines (99% reliable) system to a four-nines (99.99% reliable)\n\nsimply by processing every item twice. Obviously, in both cases, doing the",
      "content_length": 659,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 250,
      "content": "engineering work to improve the system is a better long-term solution. But\n\nsometimes as a temporary workaround, or in situations where the\n\nengineering work isn’t possible, it’s useful to be able to just throw resources\n\nat the problem.\n\nFilter\n\nThe second pattern for event-driven batch processing is a filter. The role of\n\na filter is to reduce a stream of work items to a smaller stream of work\n\nitems by filtering out work items that don’t meet particular criteria. As an\n\nexample of this, consider setting up a batch workflow that handles new\n\nusers signing up for a service. Some set of those users will have ticked the\n\ncheckbox that indicates that they wish to be contacted via email for\n\npromotions and other information. In such a workflow, you can filter the set\n\nof newly signed-up users to only be those who have explicitly opted into\n\nbeing contacted.\n\nIdeally you would compose a filter work queue source as an ambassador\n\nthat wraps up an existing work queue source. The original source container\n\nprovides the complete list of items to be worked on, and the filter container\n\nthen adjusts that list based on the filter criteria and only returns those\n\nfiltered results to the work queue infrastructure. An illustration of this use of\n\nthe adapter pattern is shown in Figure 12-3.",
      "content_length": 1297,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 251,
      "content": "Figure 12-3. An example of a filter pattern that removes all odd-numbered work items",
      "content_length": 84,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 252,
      "content": "Splitter\n\nSometimes you don’t want to just filter things out by dropping them on the\n\nfloor, but rather you have two different kinds of input present in your set of\n\nwork items, and you want to divide them into two separate work queues\n\nwithout dropping any of them. For this task, you want to use a splitter. The\n\nrole of a splitter is to evaluate some criteria—just like a filter—but instead\n\nof eliminating input, the splitter sends different inputs to different queues\n\nbased on that criteria.\n\nAn example of an application of the splitter pattern is processing online\n\norders where people can receive shipping notifications either by email or\n\ntext message. Given a work queue of items that have been shipped, the\n\nsplitter divides it into two different queues: one that is responsible for\n\nsending emails and another devoted to sending text messages. A splitter can\n\nalso be a copier if it sends the same output to multiple queues, such as when\n\na user selects both text messages and email notifications in the previous\n\nexample. It is interesting to note that a splitter can actually also be\n\nimplemented by a copier and two different filters. But the splitter pattern is\n\na more compact representation that captures the job of the splitter more\n\nsuccinctly. This is a little like the fact that an XOR (exclusive or) logic gate\n\ncan actually be implemented with OR/NOT/AND gates, but it is convenient\n\nto think of it as an XOR. An example of using the splitter pattern to send\n\nshipping notifications to users is shown in Figure 12-4.",
      "content_length": 1541,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 253,
      "content": "Figure 12-4. An example of the batch splitter pattern splitting shipping notifications into two different queues\n\nSharder\n\nA slightly more generic form of splitter is a sharder. Much like the sharded\n\nserver that we saw in earlier chapters, the role of a sharder in a workflow is\n\nto divide up a single queue into an evenly divided collection of work items\n\nbased upon some sort of sharding function. There are several different\n\nreasons why you might consider sharding your workflow. One of the first is\n\nfor reliability. If you shard your work queue, then the failure of a single",
      "content_length": 581,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 254,
      "content": "workflow due to a bad update, infrastructure failure, or other problem only\n\naffects a fraction of your service.\n\nFor example, imagine that you push a bad update to your worker container,\n\nwhich causes your workers to crash and your queue to stop processing work\n\nitems. If you only have a single work queue that is processing items, then\n\nyou will have a complete outage for your service with all users affected. If,\n\ninstead, you have sharded your work queue into four different shards, you\n\nhave the opportunity to do a staged rollout of your new worker container.\n\nAssuming you catch the failure in the first phase of the staged rollout,\n\nsharding your queue into four different shards means that only one-quarter\n\nof your users would be affected.\n\nAn additional reason to shard your work queue is to more evenly distribute\n\nwork across different resources. If you don’t really care which region or\n\ndata center is used to process a particular set of work items, you can use a\n\nsharder to evenly spread work across multiple data centers to even out\n\nutilization of all data centers/regions. As with updates, spreading your work\n\nqueue across multiple failure regions also has the benefit of providing\n\nreliability against data center or region failures. An illustration of a sharded\n\nqueue when everything is working correctly is shown in Figure 12-5.",
      "content_length": 1355,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 255,
      "content": "Figure 12-5. An example of the sharding pattern in a healthy operation\n\nWhen the number of healthy shards is reduced due to failures, the sharding\n\nalgorithm dynamically adjusts to send work to the remaining healthy work\n\nqueues, even if only a single queue remains.\n\nThis is illustrated in Figure 12-6.",
      "content_length": 303,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 256,
      "content": "Figure 12-6. When one work queue is unhealthy, the remaining work spills over to a different queue\n\nMerger\n\nThe last pattern for event-driven or workflow batch systems is a merger. A\n\nmerger is the opposite of a copier; the job of a merger is to take two\n\ndifferent work queues and turn them into a single work queue. Suppose, for\n\nexample, that you have a large number of different source repositories all",
      "content_length": 406,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 257,
      "content": "adding new commits at the same time. You want to take each of these\n\ncommits and perform a build-and-test for it. It is not scalable to create a\n\nseparate build infrastructure for each source repository. We can model each\n\nof the different source repositories as a separate work queue source that\n\nprovides a set of commits. We can transform all of these different work\n\nqueue inputs into a single merged set of inputs using a merger adapter. This\n\nmerged stream of commits is then the single source to the build system that\n\nperforms the actual build. The merger is another great example of the\n\nadapter pattern, though in this case, the adapter is actually adapting multiple\n\nrunning source containers into a single merged source.\n\nThis multiadapter pattern is shown in Figure 12-7.",
      "content_length": 784,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 258,
      "content": "Figure 12-7. Using multiple levels of containers to adapt multiple independent work queues into a single shared queue\n\nHands On: Building an Event-Driven",
      "content_length": 153,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 259,
      "content": "Flow for New User Signup\n\nA concrete example of a workflow helps show how these patterns can be\n\nput together to form a complete operating system. The problem this\n\nexample will consider is a new user signup flow.\n\nImagine that our user acquisition funnel has two stages. The first is user\n\nverification. After a new user signs up, the user then has to receive an email\n\nnotification to validate their email. Once the user validates their email, they\n\nare sent a confirmation email. Then they are optionally registered for email,\n\ntext message, both, or neither for notifications.\n\nThe first step in the event-driven workflow is the generation of the\n\nverification email. To achieve this reliably, we will use the shard pattern to\n\nshard users across multiple different geographic failure zones. This ensures\n\nthat we will continue to process new user signups, even in the presence of\n\npartial failures. Each work queue shard sends a verification email to the end\n\nuser. At this point, this substage of the workflow is complete. This first\n\nstage of the flow is illustrated in Figure 12-8.",
      "content_length": 1089,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 260,
      "content": "Figure 12-8. The first stage of the workflow for user signup\n\nThe workflow begins again when we receive a verification email from the\n\nend user. These emails become new events in a separate (but clearly related)\n\nworkflow that sends welcome emails and sets up notifications. The first\n\nstage of this workflow is an example of the copier pattern, where the user is\n\ncopied into two work queues. The first work queue is responsible for\n\nsending the welcome email, and the second work queue is responsible for\n\nsetting up user notifications.\n\nOnce the work items have been duplicated between the queues, the email-\n\nsending queue simply takes care of sending an email message, and the\n\nworkflow exits. But remember that because of the use of the copier pattern,\n\nthere is still an additional copy of the event active in our workflow. This",
      "content_length": 835,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 261,
      "content": "copier triggers an additional work queue to handle notification settings.\n\nThis work queue feeds into an example of the filter pattern, which splits the\n\nwork queue into separate email and text message notification queues. These\n\nspecific queues register the user for email, text, or both notifications.\n\nThe remainder of this workflow is shown in Figure 12-9.\n\nFigure 12-9. The user notification and welcome email work queue",
      "content_length": 425,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 262,
      "content": "Publisher/Subscriber Infrastructure\n\nWe have seen a variety of abstract patterns for linking together different\n\nevent-driven batch processing patterns. But when it comes time to actually\n\nbuild such a system, we need to figure out how to manage the stream of\n\ndata that passes through the event-driven workflow. The simplest thing to\n\ndo would be to simply write each element in the work queue to a particular\n\ndirectory on a local filesystem, and then have each stage monitor that\n\ndirectory for input. But of course doing this with a local filesystem limits\n\nour workflow to operating on a single node. We can introduce a network\n\nfilesystem to distribute files to multiple nodes, but this introduces\n\nincreasing complexity both in our code and in the deployment of the batch\n\nworkflow.\n\nInstead, a popular approach to building a workflow like this is to use a\n\npublisher/subscriber (pub/sub) API or service. A pub/sub API allows a user\n\nto define a collection of queues (sometimes called topics). One or more\n\npublishers publishes messages to these queues. Likewise, one or more\n\nsubscribers is listening to these queues for new messages. When a message\n\nis published, it is reliably stored by the queue and subsequently delivered to\n\nsubscribers in a reliable manner.\n\nAt this point, most public clouds feature a pub/sub API such as Azure’s\n\nEventGrid or Amazon’s Simple Queue Service. Additionally, the open",
      "content_length": 1413,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 263,
      "content": "source Kafka project provides a very popular pub/sub implementation that\n\nyou can run on your own hardware as well as on cloud virtual machines.\n\nFor the remainder of this overview of pub/sub APIs, we’ll use Kafka for our\n\nexamples, but they are relatively simple to port to alternate pub/sub APIs.\n\nHands On: Deploying Kafka\n\nThere are obviously many ways to deploy Kafka, and one of the easiest\n\nways is to run it as a container using a Kubernetes cluster and the Helm\n\npackage manager.\n\nHelm is a package manager for Kubernetes that makes it easy to deploy and\n\nmanage prepackaged off-the-shelf applications like Kafka. If you don’t\n\nalready have the helm command-line tool installed, you can install it from\n\nthe Helm website.\n\nOnce the helm tool is on your machine, you need to initialize it.\n\nInitializing Helm deploys a cluster-side component named tiller to\n\nyour cluster and installs some templates to your local filesystem:\n\nhelm init\n\nNow that Helm is initialized, you can install Kafka using this command:",
      "content_length": 1017,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 264,
      "content": "helm repo add incubator http://storage.googleapis\n\nhelm install --name kafka-service incubator/kafka\n\nNOTE\n\nHelm templates have different levels of production hardening and support. stable templates are\n\nthe most strictly vetted and supported, whereas incubator templates like Kafka are more\n\nexperimental and have less production mileage. Regardless, incubator templates are useful for quick\n\nproof of concepts as well as a place to start from when implementing a production deployment of a\n\nKubernetes-based service.\n\nOnce you have Kafka up and running, you can create a topic to publish to.\n\nGenerally in batch processing, you’re going to use a topic to represent the\n\noutput of one module in your workflow. This output is likely to be the input\n\nfor another module in the workflow.\n\nFor example, if you are using the Sharder pattern described previously,\n\nyou would have a topic for each of the output shards. If you called your\n\noutput Photos and you chose to have three shards, then you would have\n\nthree topics: Photos-1 , Photos-2 , and Photos-3 . Your sharder\n\nmodule would output messages to the appropriate topic after applying the\n\nsharding function.\n\nHere’s how you create a topic. First, create a container in the cluster so that\n\nyou can access Kafka:",
      "content_length": 1266,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 265,
      "content": "for x in 0 1 2; do\n\nkubectl run kafka --image=solsson/kafka:0.11.0\n\n./bin/kafka-topics.sh --create --zookeeper ka\n\n--replication-factor 3 --partitions 10 --to done\n\nNote that there are two interesting parameters in addition to the topic name\n\nand the ZooKeeper service. They are --replication-factor and -\n\npartitions . The replication factor is how many different machines\n\nmessages in the topic will be replicated to. This is the redundancy that is\n\navailable in case things crash. A value of 3 or 5 is recommended. The\n\nsecond parameter is the number of partitions for the topic. The number of\n\npartitions represents the maximum distribution of the topic onto multiple\n\nmachines for purposes of load balancing. In this case, since there are 10\n\npartitions, there can be at most 10 different replicas of the topic for load\n\nbalancing.\n\nNow that we have created a topic, we can send messages to that topic:\n\nkubectl run kafka-producer --image=solsson/kafka ./bin/kafka-console-producer.sh --broker-list\n\n--topic photos-1",
      "content_length": 1021,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 266,
      "content": "Once that command is up and connected, you should see the Kafka prompt,\n\nand you can then send messages to the topic(s). To receive messages, you\n\ncan run:\n\nkubectl run kafka-consumer --image=solsson/kafka\n\n./bin/kafka-console-consumer.sh --bootstrap-s\n\n--topic photos-1 \\\n\n--from-beginning\n\nOf course, running these command lines only gives you a taste of how to\n\ncommunicate via Kafka messages. To build a real-world event-driven batch\n\nprocessing system, you would likely use a proper programming language\n\nand Kafka SDK to access the service. But on the other hand, never\n\nunderestimate the power of a good bash script!\n\nThis section has shown how installing Kafka into your Kubernetes cluster\n\ncan dramatically simplify the task of building a work queue-based system.\n\nResiliency and Performance in Work\n\nQueues\n\nSo far in this chapter, we have designed workflows to optimize the design\n\nof the work queue to match the problem being addressed by the work",
      "content_length": 959,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 267,
      "content": "queue. This is an important part of engineering. The clearer the design of\n\nthe system matches the task, the easier it is to understand, update, and\n\nextend. However, an additional critical part of engineering is anticipating\n\nand designing systems for both the performance and reliability requirements\n\nof the users.\n\nSo far, in all of the designs we have assumed that all of the work in the\n\nsystem is roughly identical and each piece of work is equally reliable.\n\nUnfortunately, in the real world, neither one of these assumptions is\n\ntypically true. Unless your system is very, very unusual, it contains a mix of\n\ndifferent work. Different videos, different emails, different builds—work\n\nqueues are constantly dealing with similar but different individual pieces of\n\nwork.\n\nWork Stealing\n\nThe net result of this is that some pieces of work take longer than other\n\npieces of work. Sometimes this is inherent (that movie is just larger), but\n\noften it is because of design decisions in our code which are better or worse\n\nfits for a specific work item. We may have optimized our code for lots of\n\nfiles and few directories, and the work item might be the opposite: deep\n\ndirectory trees with relatively few files. Oftentimes the latency of individual\n\nwork items might not matter; if they generally land on different queues via\n\nsomething like sharding, the overall average times generally work out. But\n\nunfortunately, sometimes this average performance doesn’t work out, and",
      "content_length": 1479,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 268,
      "content": "all of the slow items end up on one queue. When this happens, the inputs to\n\nthe queue pile up and latency increases, much like a slow checkout lane in a\n\ngrocery store. Just like in the grocery store, the best option is to move from\n\nthe slow lane to a lane with a shorter line. This algorithm is called “work\n\nstealing.”\n\nThe basic algorithm for work stealing says that for any worker, as long as\n\nthat worker has work on their queue, the work on the queue is performed as\n\nyou would expect. When there is no work on a worker’s queue, instead of\n\nsleeping until more work arrives, the worker takes work off of the back of\n\nthe longest queue of any other worker. In this way, the maximum length of\n\nany work queue is kept to a minimum since workers with shorter work\n\nqueues remove work from workers with longer queues. Work is taken from\n\nthe back of the queue because that removal can be done safely without\n\nworrying about whether the work may be performed by multiple workers.\n\nErrors, Priority, and Retry\n\nThe other problems that can occur in a work queue system come when there\n\nare errors processing the work itself. The first problem that can happen is if\n\nyour worker crashes in the middle of processing the work. In the worst case,\n\nthe work that was being removed from the queue is lost. Fortunately, most\n\nqueue systems have semantics where the message being processed is\n\ninitially removed from the queue for processing. This initial removal hides\n\nthe message from other workers, but it doesn’t actually remove it from the",
      "content_length": 1537,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 269,
      "content": "queue. When the work has been processed, a second “completed” message\n\nis sent to the queue, which permanently removes the work from the queue.\n\nTypically a message queue has a timeout after which, if the work has not\n\nbeen “completed,” the work item is returned to the head of the queue for\n\nother workers to pick it up.\n\nUnfortunately, recovering from this error results in an additional\n\ncomplication. What if the first worker is merely running slowly, not\n\ncrashed? Now we have two workers processing the same work item. The\n\neasiest way to deal with this situation is to make each work item\n\nidempotent. That way the additional processing is wasted computation, but\n\nit does not cause any additional errors. If it isn’t possible to make your work\n\nidempotent, there are various distributed locking methods discussed in\n\nprevious chapters that can be used. The important takeaway is that when\n\nwriting a worker, you must assume that multiple workers could be operating\n\non the same work item.\n\nAssuming idempotent work, we’re now safe from workers crashing, right?\n\nSadly, it’s not that easy. Imagine if the crashing worker is not due to random\n\nbugs in the worker’s code, but something about the work item itself.\n\nSuppose there is a “poison” work item which crashes a worker 100% of the\n\ntime. In such a situation, where one worker is recovering from the crash, the\n\npoison work is picked up by another worker, which itself crashes, causing\n\nanother worker to pick up the poison and so on and so forth until all of your\n\nworkers are crashing. Even if you are lucky and your workers recover",
      "content_length": 1595,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 270,
      "content": "quickly, poison work can cause you to lose a significant percentage of your\n\nworker capacity and slow down your work queue significantly.\n\nMuch as with error handling, the first solution to poison work is to keep\n\ntrack of the number of times that a particular piece of work has been\n\nremoved from the message queue. We can then use exponential backoff to\n\nincrease the timeout for the poison message larger and larger until we\n\nremove it from processing entirely. Generally, exponential backoff will\n\nsolve the problem of poison work.\n\nUnfortunately, requeuing work for processing introduces additional\n\nproblems. To understand why, consider the case where workers are crashing\n\nfor reasons unrelated to the work. Perhaps a downstream dependency is\n\nreturning bad data and causing the crash. In this situation, the work queue\n\neffectively stalls, and more and more work piles up in the queue. Since no\n\nworkers can make progress, even work stealing described previously won’t\n\nhelp. Suppose the situation resolves itself and the workers can start making\n\nprogress, like a traffic jam long after an accident; all of the work that has\n\npiled up in the queue means that new requests are still experiencing\n\nsignificant latency. This is often made especially bad in user-facing\n\nsituations where the old work items are often no longer relevant, but the\n\nnew work has value if it can be completed in time.\n\nTo address this problem of a backlog, it is often useful to actually have two\n\nqueues of work items. The first queue is used for all work items which have",
      "content_length": 1557,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 271,
      "content": "never had an attempt made at processing. The second queue is the “retry”\n\nqueue, which is used for any message which has had at least one\n\n(unsuccessful) attempt at processing. Using this dual queue approach, in the\n\nabove situation, there will be a lot of work piled up in the retry queue\n\nwaiting for processing, but new work is no longer standing at the back of\n\nthe line of retries. It has an equal chance of being processed along with the\n\nretry work. This two-queue approach is an example of a general priority\n\nqueue approach, where different work queues have different priority and\n\nwork is taken off the higher priority queue until it is empty, and then the\n\nnext highest priority queue, etc. This can ensure that performance goes to\n\nwhere it is needed most to achieve latency goals.\n\nSummary\n\nThis chapter has introduced the idea of a workflow. Workflows combine\n\nmultiple work queues together into an orchestration of work to achieve\n\nsome more complex objective. Multiple patterns for building workflows,\n\nfrom copying, to filtering, sharding, and merging were introduced. Finally,\n\nthe important considerations of both performance and reliability in the face\n\nof slow work and worker crashes was examined. Using these patterns and\n\nerror handling approaches, any complex work can be broken into an easily\n\nreasoned-about and constructed workflow for processing data reliably and\n\nefficiently.",
      "content_length": 1406,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 272,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 273,
      "content": "Chapter 13. Coordinated Batch\n\nProcessing\n\nChapter 12 described a number of patterns for splitting and chaining queues\n\ntogether to achieve more complex batch processing. Duplicating and\n\nproducing multiple different outputs is often an important part of batch\n\nprocessing, but sometimes it is equally important to pull multiple outputs\n\nback together in order to generate some sort of aggregate output. A generic\n\nillustration of such a pattern is shown in Figure 13-1.",
      "content_length": 470,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 274,
      "content": "Figure 13-1. A generic parallel work distribution and result aggregation batch system",
      "content_length": 85,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 275,
      "content": "Probably the most canonical example of this aggregation is the reduce part\n\nof the MapReduce pattern. It’s easy to see that the map step is an example\n\nof sharding a work queue, and the reduce step is an example of coordinated\n\nprocessing that eventually reduces a large number of outputs down to a\n\nsingle aggregate response. However, there are a number of different\n\naggregate patterns for batch processing, and this chapter discusses a number\n\nof them in addition to real-world applications.\n\nJoin (or Barrier Synchronization)\n\nIn previous chapters, we saw patterns for breaking up work and distributing\n\nit in parallel on multiple nodes. In particular, we saw how a sharded work\n\nqueue could distribute work in parallel to a number of different work queue\n\nshards. However, sometimes when processing a workflow, it is necessary to\n\nhave the complete set of work available to you before you move on to the\n\nnext stage of the workflow.\n\nOne option for doing this was shown in Chapter 12, which was to merge\n\nmultiple queues together. However, merge simply blends the output of two\n\nwork queues into a single work queue for additional processing. While the\n\nmerge pattern is sufficient in some cases, it does not ensure that a complete\n\ndata set is present prior to the beginning of processing. This means that\n\nthere can be no guarantees about the completeness of the processing being",
      "content_length": 1386,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 276,
      "content": "performed, as well as no opportunity to compute aggregate statistics for all\n\nof the elements that have been processed.\n\nTo give a concrete example of the necessity of the join pattern, consider the\n\ncreation of a digitally animated film. Each frame of the film is an\n\nindependent work item; it can be rendered in parallel without reference to\n\nany of the other frames in the movie. However, consider transforming each\n\nframe of the movie into one second of video. You clearly need to have all\n\nthirty frames that make up that one second present before they are\n\ntransformed into video.\n\nWe need a stronger, coordinated primitive for batch data processing, and\n\nthat primitive is the join pattern. Join is similar to joining a thread. The\n\nbasic idea is that all of the work is happening in parallel, but work items\n\naren’t released out of the join until all of the work items that are processed\n\nin parallel are completed. This is also generally known as barrier\n\nsynchronization in concurrent programming. An illustration of the join\n\npattern for a coordinated batch is shown in Figure 13-2.\n\nCoordination through join ensures that no data is missing before some sort\n\nof aggregation phase is performed (e.g., finding the sum of some value in a\n\nset). The value of the join is that it ensures that all of the data in the set is\n\npresent. The downside of the join pattern is that it requires that all data be\n\nprocessed by a previous stage before subsequent computation can begin.\n\nThis reduces the parallelism that is possible in the batch workflow, and thus",
      "content_length": 1560,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 277,
      "content": "increases the overall latency of running the workflow, as discussed in\n\nChapter 12. This latency increase can be compensated for by trading\n\ncompute for time and duplicating the processing of all of the work items to\n\nreduce the impact of tail latency.",
      "content_length": 252,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 278,
      "content": "Figure 13-2. The join pattern for batch processing",
      "content_length": 50,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 279,
      "content": "Reduce\n\nIf sharding a work queue is an example of the map phase of the canonical\n\nMapReduce algorithm, then what remains is the reduce phase. Reduce is an\n\nexample of a coordinated batch processing pattern because it can happen\n\nregardless of how the input is split up, and it is used similar to join; that is,\n\nto group together the parallel output of a number of different batch\n\noperations on different pieces of data.\n\nHowever, in contrast to the join pattern described previously, the goal of\n\nreduce is not to wait until all data has been processed, but rather to\n\noptimistically merge together all of the parallel data items into a single\n\ncomprehensive representation of the full set.\n\nWith the reduce pattern, each step in the reduce merges several different\n\noutputs into a single output. This stage is called “reduce” because it reduces\n\nthe total number of outputs. Additionally, it reduces the data from a\n\ncomplete data item to simply the representative data necessary for\n\nproducing the answer to a specific batch computation. Because the reduce\n\nphase operates on a range of input and produces a similar output, the reduce\n\nphase can be repeated as many or as few times as necessary in order to\n\nsuccessfully reduce the output down to a single output for the entire data\n\nset.",
      "content_length": 1292,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 280,
      "content": "This is a fortunate contrast to the join pattern, because unlike join, it means\n\nthat reduce can be started in parallel while there is still processing going on\n\nas part of the map/shard phase. Of course, in order to produce a complete\n\noutput, all of the data must be processed eventually, but the ability to begin\n\nearly means that the batch computation executes more quickly overall.\n\nHands On: Count\n\nTo understand how the reduce pattern works, consider the task of counting\n\nthe number of instances of a particular word in a book. We can first use\n\nsharding to divide up the work of counting words into a number of different\n\nwork queues. As an example, we could create 10 different sharded work\n\nqueues with 10 different people responsible for counting words in each\n\nqueue. We can shard the book among these 10 work queues by looking at\n\nthe page number. All pages that end in the number 1 will go to the first\n\nqueue, all pages that end in the number 2 will go to the second, and so forth.\n\nOnce all of the people have finished processing their pages, they write\n\ndown their results on a piece of paper. For example, they might write:\n\na: 50\n\nthe: 17 cat: 2\n\nairplane: 1\n\n...",
      "content_length": 1183,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 281,
      "content": "This can be output to the reduce phase. Remember that the reduce pattern\n\nreduces by combining two or more outputs into a single output.\n\nGiven a second output:\n\na: 30\n\nthe: 25\n\ndog: 4\n\nairplane: 2\n\n...\n\nThe reduction proceeds by summing up all of the counts for the various\n\nwords, in this example producing:\n\na: 80\n\nthe 42\n\ndog: 4\n\ncat: 2\n\nairplane: 3\n\n...\n\nIt’s clear to see that this reduction phase can be repeated on the output of\n\nprevious reduce phases until there is only a single reduced output left. This\n\nis valuable since this means that reductions can be performed in parallel.",
      "content_length": 591,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 282,
      "content": "Ultimately, in this example you can see that the output of the reduction will\n\nbe a single output with the count of all of the various words that are present\n\nin the book. Of course, the number of times reduce is run has an impact on\n\nthe overall performance of the batch processing. Too many reduce\n\noperations cause significant wasted computation, while too few begin to\n\nresemble the synchronization of the join pattern.\n\nSum\n\nA similar but slightly different form of reduction is the summation of a\n\ncollection of different values. This is like counting, but rather than simply\n\ncounting one for every value, you actually add together a value that is\n\npresent in the original output data.\n\nSuppose, for example, you want to sum the total population of the United\n\nStates. Assume that you will do this by measuring the population in every\n\ntown and then summing them all together.\n\nA first step might be to shard the work into work queues of towns, sharded\n\nby state. This is a great first sharding, but it’s clear that even when\n\ndistributed in parallel, it would take a single person a long time to count the\n\nnumber of people in every town. Consequently, we perform a second\n\nsharding to another set of work queues, this time by county.",
      "content_length": 1242,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 283,
      "content": "At this point, we have parallelized first to the level of states, then to the\n\nlevel of counties, and then each work queue in each county produces a\n\nstream of outputs of (town, population) tuples.\n\nNow that we are producing output, the reduce pattern can kick in.\n\nIn this case, the reduce doesn’t even really need to be aware of the two-\n\nlevel sharding that we performed. It is sufficient for the reduce to simply\n\ngrab two or more output items, such as (Seattle, 4,000,000) and\n\n(Northampton, 25,000) , and sum them together to produce a new\n\noutput (Seattle-Northampton, 4,025,000) . It’s clear to see\n\nthat, like counting, this reduction can be performed an arbitrary number of\n\ntimes with the same code running at each interval, and at the end, there will\n\nonly be a single output containing the complete population of the United\n\nStates. Importantly, again, nearly all of the computation required is\n\nhappening in parallel.\n\nHistogram\n\nAs a final example of the reduce pattern, consider that while we are\n\ncounting the population of the United States via parallel sharding/mapping\n\nand reducing, we also want to build a model of the average American\n\nfamily. To do this, we want to develop a histogram of family size; that is, a\n\nmodel that estimates the total number of families with zero to 10 children.",
      "content_length": 1313,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 284,
      "content": "We will perform our multilevel sharding exactly as before (indeed, we can\n\nlikely use the same workers).\n\nHowever, this time, the output of the data collection phase is a histogram\n\nper town:\n\n0: 15% 1: 25%\n\n2: 45%\n\n3: 10%\n\n4: 5%\n\nFrom the previous examples, we can see that if we apply the reduce pattern,\n\nwe should be able to combine all of these histograms to develop a\n\ncomprehensive picture of the United States. At first blush, it may seem\n\nquite difficult to understand how to merge these histograms, but when\n\ncombined with the population data from the summation example, we can\n\nsee that if we multiply each histogram by its relative population, then we\n\ncan obtain the total population for each item being merged. If we then\n\ndivide this new total by the sum of the merged populations, it is clear that\n\nwe can merge and update multiple different histograms into a single output.\n\nGiven this, we can apply the reduce pattern as many times as necessary\n\nuntil a single output is produced.",
      "content_length": 998,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 285,
      "content": "Hands On: An Image Tagging and Processing Pipeline\n\nTo see how coordinated batch processing can be used to accomplish a larger\n\nbatch task, consider the job of tagging and processing a set of images. Let\n\nus assume that we have a large collection of images of highways at rush\n\nhour, and we want to count both the numbers of cars, trucks, and\n\nmotorcycles, as well as distribution of the colors of each of the vehicles. Let\n\nus also suppose that there is a preliminary step to blur the license plates of\n\nall of the vehicles to preserve anonymity.\n\nThe images are delivered to us as a series of HTTPS URLs where each\n\nURL points to a raw image. The first stage in the pipeline is to find and blur\n\nthe license plates. To simplify each task in the work queue, we will have\n\none worker that detects a license plate, and a second worker that blurs that\n\nlocation in the image. We will combine these two different worker\n\ncontainers into a single container group using the multiworker pattern\n\ndescribed in Chapter 12.\n\nThis separation of concerns may seem unnecessary, but it facilitates\n\ncontainer image reuse. Imagine if the API to our container is an image and a\n\nbox to blur. The same container that blurs license plates can be reused to\n\nblur people’s faces in a different pipeline.",
      "content_length": 1284,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 286,
      "content": "Additionally, to ensure reliability and to maximize parallel processing, we\n\nwill shard the images across multiple worker queues. This complete\n\nworkflow for sharded image blurring is shown in Figure 13-3.",
      "content_length": 205,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 287,
      "content": "Figure 13-3. The sharded work queue and the multiple blurring shards",
      "content_length": 68,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 288,
      "content": "Once each image has been successfully blurred, we will upload it to a\n\ndifferent location, and we will then delete the originals. However, we don’t\n\nwant to delete the original until all of the images have been successfully\n\nblurred in case there is some sort of catastrophic failure and we need to\n\nrerun this entire pipeline. Thus, to wait for all of the blurring to complete,\n\nwe use the join pattern to merge the output of all of the sharded blurring\n\nwork queues into a single queue that will only release its items after all of\n\nthe shards have completed the work.\n\nNow we are ready to delete the original images as well as begin work on\n\nvehicle model and color detection. Again, we want to maximize the\n\nthroughput of this pipeline, so we will use the copier pattern from\n\nChapter 12 to duplicate the work queue items to two different queues:\n\nA work queue that deletes the original images\n\nA work queue that identifies the type of vehicle (car, truck, motorcycle)\n\nand the color of the vehicle\n\nFigure 13-4 shows these stages of the processing pipeline.",
      "content_length": 1062,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 289,
      "content": "Figure 13-4. The output join, copier, deletion, and image recognition parts of the pipeline",
      "content_length": 91,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 290,
      "content": "Finally, we need to design the queue that identifies vehicles and colors and\n\naggregates these statistics into a final count. To do this, we first again apply\n\nthe shard pattern to distribute the work out to a number of queues. Each of\n\nthese queues has two different workers: one that identifies the location and\n\ntype of each vehicle and one that identifies the color of a region.\n\nWe will again join these together using the multiworker pattern described in\n\nChapter 12. As before, the separation of code into different containers\n\nenables us to reuse the color detection container for multiple tasks beyond\n\nidentifying the color of the vehicles.\n\nThe output of this work queue is a JSON tuple that looks like this:\n\n{\n\n\"vehicles\": {\n\n\"car\": 12,\n\n\"truck\": 7,\n\n\"motorcycle\": 4\n\n}, \"colors\": {\n\n\"white\": 8,\n\n\"black\": 3,\n\n\"blue\": 6,\n\n\"red\": 6\n\n}\n\n}",
      "content_length": 849,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 291,
      "content": "This data represents the information found in a single image. To aggregate\n\nall of this data together, we will use the reduce pattern described previously\n\nand made famous by MapReduce to sum everything together just as we did\n\nin the count example above. At the end, this reduce pipeline stage produces\n\nthe final count of images and colors found in the complete set of images.\n\nSummary\n\nThis chapter describes coordinated batch processing, which is used to\n\nsynchronize workflows for preconditions or to combine output from\n\nprevious workflows into a merged output. The join pattern is used as a\n\nbarrier synchronizer to ensure that all of the necessary output is ready\n\nbefore proceeding to the next stage. Reduce combines output from multiple\n\nshards into an output that can be reduced further until the final output is\n\nready. Sum and histogram are specific types of reduce operations that are\n\nuseful for obtaining specific statistical results from your data. Taken\n\ntogether, these patterns for coordination in batch processing can ensure that\n\nyou can build the workflow that is necessary for any batch operation or\n\ncalculation you need to perform.\n\nOceanofPDF.com",
      "content_length": 1173,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 292,
      "content": "Part V. Universal Concepts\n\nSo far we have focused on patterns for building your applications, all the\n\nway from single nodes to complex distributed systems. In addition to these\n\npatterns, there are parts of our systems that are universal to all applications\n\nthat you build. The following chapters in this section cover some of these\n\nuniversal concepts. Chapter 14 discusses observability for your application\n\nvia logging, monitoring and alerting. Observability is critical for you to\n\nunderstand if your application is operating correctly. Chapter 15 covers AI,\n\nwhich is revolutionizing the way that we think of applications and user\n\ninterfaces. Finally, Chapter 16 ends this section by including common\n\nmistakes and errors that occur with unfortunate regularity in distributed\n\nsystems that people build. No matter your application, all of these chapters\n\nwill enable you to make it more intelligent and more reliable.\n\nOceanofPDF.com",
      "content_length": 943,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 293,
      "content": "Chapter 14. Monitoring and\n\nObservability Patterns\n\nOne of the core differences between client applications and distributed\n\nsystems is that generally distributed systems implement services. Services\n\nare always on, always available for users around the world in all time zones\n\nand ways of working. Because of the 24/7 nature of these systems,\n\nmonitoring and observability become critical to building reliable systems.\n\nTo deliver reliability, you must notice a problem before the customer\n\nnotices a problem; and to solve any problems you find, you need to be able\n\nto understand how your system is operating. This chapter focuses on best\n\npractices for such monitoring and observability.\n\nMonitoring and Observability Basics\n\nBefore we get into the details of implementing monitoring and\n\nobservability, it is useful to ground ourselves in the core set of concepts that\n\nmake up any monitoring and observability solution.\n\nIn any system, there are four key concepts which make up our solutions:\n\nLogging\n\nMetrics\n\nAlerting",
      "content_length": 1026,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 294,
      "content": "Tracing\n\nWe’ll step through each of these in a little more detail.\n\nIt’s highly likely that anyone who has built even the smallest system has\n\nimplemented logging, even if they don’t realize that they have. The simplest\n\nversion of logging is the humble printf statement. Of course, there are\n\nmany more sophisticated ways to do logging, but ultimately they all serve\n\nthe same purpose as that print statement. Namely, they show us that a\n\nparticular place in our code has executed, and they record data associated\n\nwith that place in the code. Logging records data that helps us understand\n\nspecific executions of our code.\n\nThe contrast to logging are metrics or monitoring. While logging is\n\nassociated with a specific execution of our source code, a metric is\n\ngenerally related to aggregate data across multiple requests. Examples of\n\nmetrics could be the number of times this particular function has been\n\ncalled in the last minute, or the average length of time spent in that function\n\nacross all of these calls. While logging provides us a perspective to\n\nunderstand a specific execution of our service, metrics provide us a global\n\nperspective of how our service is executing in general.\n\nThe purpose of logging and metrics are to help us understand our systems\n\nwhen we are looking at them, or when we know that there is a problem. If\n\nwe spent all of our time staring at graphs of metrics and logging interfaces,",
      "content_length": 1423,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 295,
      "content": "that might be all we need; but of course, in real life we spend much of our\n\ntime in other environments. Consequently, we need alerting to detect\n\nproblems and notify us that we need to interrupt our lives and go look at the\n\nlogging or metrics. Effectively, alerts are rules that are applied to either logs\n\nor metrics which trigger an event. Generally, this event triggers a\n\nnotification which is intended to draw our attention to a problem (or\n\npotential problem) in our systems. Crafting high-quality alerts is a key part\n\nof building a reliable system.\n\nThe final component in our system relates to the distributed part of our\n\ndistributed system. One of the hardest problems in building out a distributed\n\nsystem is understanding how all of the pieces work together, or even harder,\n\nunderstanding why they may fail in certain circumstances. From a naive\n\npoint of view, every individual request to a microservice is unique and\n\nindividual. But the truth is that from the broader perspective of the\n\napplication, there is often a larger request, which is being serviced by a\n\nchain of calls through all of the microservices. Tracing is the act of\n\nrebuilding this context so that instead of seeing individual, unique requests\n\nat each microservice, we can understand that all of these individual pieces\n\nare part of a broader user request. This global perspective, in turn, enables\n\nus to understand (for example) why a particular user’s request is running\n\nslowly or failing to work at all. Tracing takes all of the pieces of a\n\ndistributed system and reconstructs the user’s perspective.",
      "content_length": 1596,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 296,
      "content": "In the coming sections, we’ll dive deeper into each of these components and\n\nsee how you can use them to build a reliable system.\n\nLogging\n\nOnce you figure out how to log information, one of the first things that you\n\nnotice in a distributed system is just how much information gets logged.\n\nThe volume of logs, or “log spam” can often be a significant problem in\n\nterms of understanding a problem, or even just the raw cost of storing and\n\nsearching all of those logs.\n\nThe truth is that the value of logging is very contextual. When you are\n\ndebugging a specific problem in a specific component, then verbose,\n\ndetailed logging can be critical to identifying the problem. On the other\n\nhand, during standard execution, where everything is working correctly,\n\nmost of the logging is just wasted noise, except those logs which trigger\n\nalerts because of system errors.\n\nConsequently, only the simplest distributed systems use logging like basic\n\nprinting. Nearly every system uses some sort of logging library. These\n\nlogging libraries provide important capabilities that make it much easier to\n\ndeal with lots and lots of logs. The most basic capability provided by such\n\nlibraries is basic timestamping. Oftentimes we are trying to understand what\n\nhappened and in what sequence; and without a shared notion of time in\n\nevery log line, it is difficult to know (for example) the amount of time that",
      "content_length": 1399,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 297,
      "content": "passed between different logs. In addition to timestamp, most logging\n\nlibraries enable the addition of other types of context to the logs. In any\n\nmultithreaded system, many requests may be processed in parallel, and thus\n\nmany different identical logs may be issued. To differentiate between these\n\nlogs and understand how they relate to a specific request, context like the\n\nthread identifier, user ID, or request ID help differentiate between otherwise\n\nidentical logs on the same server and filter to the context of a specific\n\nrequest. The final, and often most valuable, service provided by logging\n\nlibraries is leveled logging. Leveled logs allow you to express the type of\n\nlogs being emitted. The specific number of different levels often vary by\n\nlibrary, but in general they include:\n\nDebug\n\nVerbose logs, generally only useful when doing detailed debugging.\n\nInfo\n\nOften useful but not exceptional messages tied to the operation of the\n\nservice.\n\nWarning\n\nSomething that is concerning, but not necessarily fatal to the entire\n\napplication.\n\nError",
      "content_length": 1060,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 298,
      "content": "Something very bad has happened, and someone should take a look\n\nas soon as possible.\n\nFatal\n\nSomething unrecoverable has happened, and the application will\n\nterminate after writing this log.\n\nLeveled logging enables you to filter only to certain types of messages, and\n\nit allows you to leave detailed logging in your code, for when it is needed,\n\nwithout filling up your log streams with many, many spammy log messages.\n\nGenerally, your log level for an application is set to Info or Warning\n\ndepending on your logging style. Debug messages are reserved for\n\nconditional activation to debug specific problems.\n\nIn addition to the request context, a valuable context for the logging library\n\nis the location of the log call itself. Typically, logging systems understand\n\nwhich class or component contains the log. This enables conditionally\n\nlogging where you dynamically activate detailed debug logging for specific\n\nclasses or other components on running servers. Conditional logging\n\nenables you to respond to a reported incident or a problem seen by a\n\nspecific user without requiring the storage or latency of detailed logging for\n\nevery request. Some care should be used when performing conditional\n\nlogging, as it can significantly increase the latency of end-user requests.\n\nNothing is worse than causing a worse incident or outage in the process of\n\ninvestigating an outage.",
      "content_length": 1384,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 299,
      "content": "Of course, to make logging useful, you must have a clear sense of what the\n\npurpose of logging is in the first place. Ultimately, logging provides insight\n\ninto the operation of your service or application. One common joke is that\n\nthe way to know what to log is to remember what you were missing when\n\nyou initially debugged the problem. That is, logging is often a retrospective\n\nexercise “I wish I had logged… .” It goes without saying that the most\n\nuseful things to log are exceptional or erroneous situations. Every error or\n\nexception should probably be logged unless you are completely confident in\n\nthe error-handling code (and sometimes even then). Beyond errors, often\n\nthe most interesting things to log are the things that are weird. For example,\n\nsome requests are just slow. You can stare at metrics (see “Metrics”)\n\nshowing that your 99th-percentile requests take a ridiculous amount of time,\n\nbut without logs indicating which parts of request handling are\n\nexceptionally slow, it is hard to take action. On the other hand, just logging\n\nthe play by play (“function foo was called”) is rarely going to be that useful,\n\nand it is definitely going to spam your logs. When logging, try to imagine\n\nthe hardest problem you might need to debug in that particular part of the\n\ncode, and write down in the log whatever you would need to successfully\n\nfigure it out.\n\nMetrics\n\nIn contrast to logging, with its focus on individual executions, monitoring\n\nmetrics are designed to collect statistics over time which characterize the",
      "content_length": 1538,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 300,
      "content": "general execution of your services. The focus of this metrics section will be\n\nthe Prometheus monitoring application, which has become the de facto\n\nindustry standard. Other monitoring solutions offer similar capabilities.\n\nMonitoring data is generally recorded over time; from this view you can see\n\nboth the instantaneous state of your application and also how that state is\n\nchanging over time. When monitoring your application there are three basic\n\ntypes of data that are interesting to record:\n\nHistograms\n\nCounts\n\nValues\n\nHistograms represent a distribution of values over some interval.\n\nHistograms enable you to understand both the average and the extreme\n\nexperiences of your users. For example, a histogram of latency can show\n\nyou the best 10% average and worst 10% latency that your users\n\nexperienced. Such information is useful to give you an “at-a-glance” sense\n\nof how your application is performing.\n\nCounts are monotonically increasing values that keep track of some value\n\nthat can only increase. One of the most common examples of a count is the\n\ntotal number of requests served by your service. This number can clearly\n\nonly increase over time, as there are more and more requests. Counts are\n\nalso often transformed into rates, which are the first-order derivative of the",
      "content_length": 1294,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 301,
      "content": "count and represent the rate of change of that count over time. Given a\n\ncount that tracks the total number of requests, the rate could give you the\n\nnumber of requests per second. Counts are inherently integral (whole\n\nnumbers) in nature.\n\nFinally, there are values; these are numbers which can take on any value\n\nand can both increase and decrease over time. Common examples of values\n\ninclude CPU and memory usage for a service. Clearly, the amount of CPU\n\nand memory use can vary over time and increase or decrease based on the\n\noperation of the service. Values are most useful when viewed over time\n\nbecause they can indicate the overall behavior of the system. Steadily\n\nincreasing memory usage can indicate a memory leak. Periodically spikey\n\nCPU can indicate some background process which is unnecessarily\n\nexpensive.\n\nIt should be clear at this point that the main value of monitoring data is the\n\nperspective that it gives over time. Indeed, most monitoring is referred to as\n\na time series representing exactly these values over time. Dedicated\n\ndatabases for this time series data have been developed to efficiently store\n\nand query this data. The Prometheus system comes with a simple time-\n\nseries database, but most people will integrate Prometheus with a dedicated\n\ntime series database. Often, a good option is to integrate Prometheus\n\nmonitoring with a cloud-based monitoring as a service. At this point, most\n\ncloud-based monitoring services, for example Azure Monitor, support the\n\ningestion of Prometheus metrics.",
      "content_length": 1534,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 302,
      "content": "When it comes to ingesting metrics, Prometheus is a pull-based or scraping\n\nmetrics system. Periodically the Prometheus application sends requests out\n\nto your application for the current state of all of the metrics. These metrics\n\nare stored into the time-series database for later query and recording. Pull-\n\nbased monitoring works well for applications that are always running, but\n\nhow can you record metrics for batch and other transient jobs that may not\n\nbe around long enough to be scraped? For such jobs, you need to implement\n\npush-based metrics instead. Fortunately, Prometheus has a sister project, the\n\nPrometheus push gateway, which implements exactly this pattern. When\n\nyou run the push gateway, it runs a dedicated server that Prometheus can\n\nscrape. Your applications can send or “push” metrics into this gateway so\n\nthat they are recorded even if your application is no longer running.\n\nCombining pull-based monitoring for your long-running applications and\n\npush-based metrics for transient jobs allows you to have a complete\n\nperspective on the operation of your application.\n\nBasic Request Monitoring\n\nOne of the first types of monitoring that most people add is for the number\n\nof requests served by your application. This is a basic statistic that\n\ndemonstrates how many people (or other services) are using your service\n\nover time. The number of requests is an example of a count metric. The\n\nnumber of requests cannot decrease since there is no such thing as a\n\nnegative request.",
      "content_length": 1505,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 303,
      "content": "However, the total number of requests is actually not that interesting, what\n\nis more interesting is the rate of change of the total number of requests.\n\nRate of change is a secondary metric defined as the difference between the\n\nnumber of requests at time t and the number of requests at time t + 1. To\n\nmake this concrete, the rate of requests per minute is the total number of\n\nrequests right now, minus the total number of requests one minute ago. To\n\nturn this into a graph, the rate is calculated for each previous minute\n\nmoving backward and producing a rate at one-minute intervals. Of course,\n\nin this example, one minute is arbitrary; you can do equivalent calculations\n\nfor whatever time interval that you want, from a millisecond to a day (or\n\nmore).\n\nThinking about calculating the rate of requests for a millisecond\n\ndemonstrates one of the challenges with rate metrics. If your server serves\n\nfewer than 1,000 requests per second, calculating the rate of requests at\n\nmillisecond granularity will lead to a very spikey metric, which oscillates\n\nbetween zero and one (and possibly more if there is a brief surge of\n\nrequests). Rates are variable and highly dependent on external factors.\n\nConsequently, it is often more useful to use the average of the rate of\n\nrequests per time interval, rather than just the raw value. Averaging the rate\n\nover time smooths the peaks and valleys and gives you a more realistic\n\npicture of how your service is being accessed.\n\nThe total number of requests is a good metric to start with, but to truly\n\nunderstand the operation of your system, it is useful to look at more details",
      "content_length": 1628,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 304,
      "content": "associated with each request. One of the most common features of a request\n\nis the HTTP response code which it returned to the users. If you are not\n\nalready familiar with HTTP response codes, they are a part of the HTTP\n\nprotocol that indicates how the server responded to the request. Some of the\n\nmost common are 200 (OK), 404 (Not found), and 500 (Internal Server\n\nError), but there are many more response codes for different situations.\n\nWith Prometheus metrics, values can have labels which add detail to the\n\nmetric. Using this capability, we can view the response code as a label\n\nassociated with the request count. Using this label, we can query for the\n\ntotal number of requests which were served successfully ( code==200 ) or\n\nthe total number of requests which resulted in errors ( code==500 ); we\n\ncan also calculate the rate of each of these types of requests using the\n\nmethods described previously. Subdividing the total request count by the\n\nresponse code gives us a much richer view into the operation of our\n\napplication. We can also group error codes together and view how many\n\nrequests were errors (codes starting with 5xx) and how many requests were\n\nsuccessful (codes starting with 2xx).\n\nIn addition to the response code for a server request, the next most\n\nimportant characteristic of the request is the latency of the request, or how\n\nmuch time it took to process. It might be tempting to think that latency\n\ncould be a label on the request count as well, but this doesn’t work. Labels\n\nneed to be a limited set of discrete values, while latency is a value that can\n\ntake on many different arbitrary values. Request latency is a separate\n\nmetric, and it is generally modeled as a histogram. Using the histogram",
      "content_length": 1737,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 305,
      "content": "metric for request latency, you can understand the average experience of the\n\nuser as well as the experience of users who have the slowest requests. As\n\nwith request count, the request latency can also be labeled using the\n\nresponse code. That way, not only can you see the average experience of\n\nany request, but you can also calculate the average latency of an error, of a\n\nnot found request, or any other HTTP response code.\n\nAdvanced Request Monitoring\n\nThough total request count, the rate of requests, and latency are the most\n\ncommon and most generally useful metrics for monitoring requests to your\n\nserver or service, there are other more specialized metrics, which can be\n\nquite useful to have a deep understanding of how your service is\n\nperforming. A good example of this might be the request and response sizes\n\nin bytes for your service. Request size is useful because you can use it to\n\ncorrelate to slow requests (larger requests may take longer to parse and\n\nprocess) and also to look at it over time to detect patterns like increasing\n\nrequests or response sizes over time. Like latency, request and response size\n\nare examples of histogram metrics.\n\nSometimes useful metrics are recorded by adding detail to how you\n\ncalculate and report metrics in your application. For example, it is\n\nsometimes useful to understand the amount of time a request has spent in a\n\nqueue, versus how much time was spent processing a request. This sort of\n\ninformation is especially useful when you are debugging why your service",
      "content_length": 1528,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 306,
      "content": "is slow in processing requests. Simply seeing that latency is high will show\n\nyou that there is a problem and that users are likely unhappy, but having\n\nmetrics for queue time and processing time will help you understand\n\nwhether the problem is that you need to scale out (queue time is too long) or\n\nif you need to optimize your code (processing time is too long). To get these\n\nmetrics, you actually need to add additional metrics to your code. In this\n\ncase, you would add two new histogram metrics, one measuring the time\n\nfrom when a request is received to when processing starts, and one\n\nmeasuring the time from when processing starts to when the request is\n\ncompleted. This system now has three histogram metrics measuring latency:\n\ntotal latency, queue latency, and processing latency. Metrics are cheap to\n\nadd, and the additional insight that they provide is often invaluable in\n\ndebugging your service. Of course, as with anything, it is very possible to\n\noverdo things, and if you are monitoring the latency of each line of code (or\n\neven each function), you’re probably taking things a little too far.\n\nAnother advanced metric for requests might be to break them down by\n\ngeography or customer. This monitoring can give you business insights that\n\ncan help you build better products, or possibly identify abusive requests\n\ncoming from specific users or locations. If you are willing to only look at\n\ncoarse-grained geography (e.g., country), then it is possible to view\n\ngeography as another label that you can apply to the request count metric;\n\nbut more often, this sort of data needs to be obtained by querying across\n\nlogs instead of looking at metrics directly. For example, in any large system\n\nthere are far too many customers to use a customer ID as a label for the",
      "content_length": 1787,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 307,
      "content": "request count metric. Instead, the system logs the customer ID whenever a\n\nrequest is processed. This example demonstrates the interplay between\n\nlogging and monitoring. Monitoring and metrics can provide aggregate data\n\nthat is quick to query. For detailed information, logging and log search is an\n\nessential technique that provides slower, but more rich, access to data. In\n\nthis case, because the business analysis can likely stand extra latency in\n\nprocessing, obtaining customer usage information from log search can meet\n\nthe business requirements.\n\nAlerting\n\nSo far we’ve discussed all of the different ways to record information from\n\nyour application’s logs and metrics. This data can be critical for\n\nunderstanding why your system isn’t behaving correctly during an outage or\n\nother incident. But how can you figure out something is happening in the\n\nfirst place?\n\nObviously, the first way we find out that there is a problem in our system is\n\nwhen a customer or user complains. But waiting for such customer reported\n\nincidents, or CRIs, is a bad experience for both the user and the person\n\nreceiving the alert. Nothing hurts customer confidence like being told that\n\nthey were the first one to notice a problem. At this point with our systems,\n\nevery user expects that we will notice (and fix) problems in our services\n\nbefore the user notices that anything is amiss.",
      "content_length": 1381,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 308,
      "content": "Thus, the best way to find out about a problem is via alerting. Alerting is\n\nthe practice of establishing conditions under which the team responsible for\n\na service is notified, typically through a pager alert, that there is a problem\n\nwith the system.\n\nBasic alerting\n\nThe most basic form of alerting is based on metric queries and static\n\nthresholds. For example, you might choose to alert if the latency at the 90th\n\npercentile in your system is greater than half a second. You might also\n\nchose to alert if the number of HTTP 500 errors reported by your system is\n\ngreater than 1% of all requests. Given these rules, it’s easy to see that if\n\neither occurred it would be a good idea to let the engineers know; but how\n\ndo you go about determining the right alerts for your system?\n\nThe most basic way to think about writing alerts is that they define the\n\nservice level objective (SLO) for your application. If you don’t alert, you\n\nwon’t notice and so your alert thresholds define the difference between\n\nnominal and exceptional operation. Every application is different, but it is\n\ngood to write down what the goals are for a “normal” user experience of\n\nyour service and right alerts that fire if those goals aren’t being achieved.\n\nAnother consideration is the experience of your on-call engineers. Firing\n\nalerts constantly is a recipe for burnout and people quitting. Similarly, firing\n\nalerts that are false alarms (“crying wolf”) will cause your on-call engineers\n\nto stop responding. Therefore, you need to balance the goals of your service",
      "content_length": 1553,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 309,
      "content": "with the needs of your on-call engineers to determine the right alerts. The\n\ndevelopment of alerts is an ongoing activity throughout the lifespan of your\n\nservice. As you add more capabilities and, thus, more monitoring, you\n\nshould also be adding more alerting; as you get more experience with the\n\nsystem, you can tune the alerts to be tighter or looser depending on the\n\nneeds of your users or your on-call engineers.\n\nAlerting on anomolies\n\nBasic alerting is great for many use cases, but sometimes it is insufficient. A\n\ngreat example of this is an error that causes a 100% outage for a small (1%)\n\nsubset of your users. From a static alerting perspective, your system is\n\nfunctioning within bounds, but for those poor users in that subset your\n\nservice is completely down. Static alerting assumes that all metrics are\n\nuniform across all users and service calls. However, this isn’t always the\n\ncase. Anomoly detection is a form of AI that can identify anmolies: for\n\nexample, customer A is 100% down while customer B is working perfectly.\n\nThough alerting on anomolies is an advanced topic, it is often the only way\n\nto find a balance between alerting when there is a problem and firing lots of\n\nfalse alarms. As your system gets larger and more complex, anomoly\n\ndetection alerting will become a critical requirement.\n\nMonitoring and logging provides visibility into your application. Alerting\n\ntells you when you should be paying attention. Importantly, not every\n\nmetric requires an alert; some are useful only for debugging and",
      "content_length": 1538,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 310,
      "content": "visualization. However, the performance of your application will be defined\n\nby the quality of your alerts. Invest in alerting with the same energy you\n\nbring to designing customer-facing features.\n\nTracing\n\nAs we have seen in the preceding chapters, most modern applications\n\ninvolve requests that span multiple machines and processes. From the\n\ntechniques above, you can learn how to monitor all of these components\n\nindividually. But unfortunately, this service-by-service view shows you all\n\nof the different requests flowing through your system at the same time, but\n\nit doesn’t do a good job showing you the path of a single request through all\n\nthe microservices in your system. Request tracing correlates all of the\n\nservice metrics together and gives you that end-to-end perspective.\n\nThe easiest way to achieve tracing is to create a unique correlation ID for\n\nevery request as it enters your system. Effectively, this correlation ID is a\n\nnumber that can be used to group together logs, metrics, and other\n\ninformation across your application. The correlation ID can be anything you\n\nwant as long as it uniquely identifies each request. The simplest correlation\n\nID would be a large random number; a better one would be a hash function\n\napplied to unique characteristics of the original request like its RESTful\n\npath and source IP address.",
      "content_length": 1351,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 311,
      "content": "Once you have a correlation ID, whenever you emit a log or a metric, you\n\nemit the correlation ID. For example, in a logging system, in addition to the\n\ntimestamp and the log level, you would also log the request correlation ID.\n\nTo enable you to log that ID within all components in your system, you\n\nneed to pass it along with the API calls you make throughout your system.\n\nAssuming that you are making API requests using the HTTP protocol, you\n\ncan embed your correlation ID in an application-specific header, for\n\nexample, x-my-apps-correlation-id , and modify your code to\n\nread from that header and populate a request context object with that ID.\n\nOnce you have correlation IDs populated throughout your logging and\n\nmetrics, you can group that information back together to visualize the path\n\nof a single request through your entire system.\n\nIf this sounds like a lot of work to implement and get right, you’re right, it\n\nis. Fortunately, the OpenTelemetry project has taken care of much of the\n\nheavy lifting for you. OpenTelemetry has SDKs for all popular languages\n\nthat make it easy to integrate request tracing into your application. It has a\n\nvendor-agnostic philosophy that allows it to integrate with numerous\n\nbackends for metrics and logging. Additionally, numerous visualization and\n\ndebugging tools have also integrated with OpenTelemetry to provide rich\n\nintrospection capabilities. In nearly all cases, integrating with a community-\n\ndriven open source project like OpenTelemetry for request tracing is a\n\nbetter idea than building your own.",
      "content_length": 1563,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 312,
      "content": "Aggregating Information\n\nOnce you start monitoring, you will discover that monitoring can generate a\n\ntremendous amount of information. Efficiently maintaining necessary\n\ninformation as well as being able to quickly sift through it to find the right\n\ninformation for your current issue becomes an important challenge.\n\nAggregation enables you to group information together so that it is easier to\n\nunderstand or more efficient to store.\n\nThe first form of aggregation that most people begin with is querying logs\n\nacross multiple processes. In “Tracing” we discussed how OpenTelemetry\n\nprovides a request-oriented view of your data, but that’s not the only\n\naggregation you might want to perform. If you have a component that has\n\nbeen replicated for scale, you may want to search for instances of a specific\n\nerror or failure across all of the replicas.\n\nFortunately, there are numerous tools that make it easier to group logs\n\nacross multiple containers in Kubernetes. One of the easiest to use is a tool\n\nlike ktail , which extends the traditional kubectl log interface to\n\ncombine logs across multiple pods. More sophisticated tools ingest logs into\n\na search index like Elasticsearch, which makes it possible to run arbitrary\n\nqueries over your logs, just like you would with web search.\n\nSetting and maintaining a log index is complicated and error-prone, but\n\nfortunately, at this point all public clouds and many startups offer log",
      "content_length": 1439,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 313,
      "content": "ingestion and search as a service. Similar services are also available for\n\ntime series metrics. Whether you are building in the public cloud or your\n\nown infrastructure, using a service for something as critical as your logs is\n\nthe right choice more often than not.\n\nIn addition to storing your logs and metrics in systems designed for efficient\n\nstorage and querying, sometimes it is necessary to reduce the information\n\nbeing stored. This can be referred to as downsampling or aggregation. The\n\nsimplest form of downsampling for metrics is to just store the metric less\n\noften. For example, if you are recording the metric every second, after a\n\nweek you might reduce that to once per minute. Even this simple change\n\nreduces the space necessary by 60 times. Of course, simply dropping the\n\nsample rate can cause you to lose information. Another option would be to\n\nuse the average value over those sixty seconds instead of an arbitrary\n\ninstantaneous value. This simple kind of downsampling can be used for\n\nlogs. The simplest approach is to keep only a certain class of logs, such as\n\nerrors, while dropping less important information. You can also just keep a\n\nfraction of the logs, though this can be very lossy in terms of information.\n\nFinally, with more effort, you can develop application-specific aggregations\n\nthat maintain the information that is relevant to your application while\n\ncompressing the data used to store it.\n\nThough downsampling is often a good solution, if you want to reduce costs\n\nwhile maintaining full data fidelity, perhaps for compliance or security, you\n\ncan instead change the accessibility of the data. Uploading logs to cloud-",
      "content_length": 1666,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 314,
      "content": "based “cold” storage can significantly reduce the costs of storing the data,\n\nwhile making it slower and harder to query information from those logs.\n\nThis can often be a good solution when data needs to be retained for long\n\nperiods of time, but is infrequently used. Some logging as a service\n\nsolutions will implement this sort of data tiering for you automatically.\n\nSummary\n\nMonitoring is a critical part of any distributed system. Though it doesn’t\n\ncontribute to making the system operate correctly, when the system is not\n\noperating correctly, it is the key component which enables us to understand\n\nwhat is failing and how to correct it. Learning how to correctly monitor and\n\nobserve your systems is critical to ensuring that you can rapidly restore\n\nreliable operation in the face of a problem or outage.\n\nOceanofPDF.com",
      "content_length": 831,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 315,
      "content": "Chapter 15. AI Inference and Serving\n\nIn the last few years, AI has become a key part of many different types of\n\napplications. Though the basics of neural networks and machine learning\n\nhave been around for decades, in the last decade advances in deep learning\n\nand large language models (LLMs) have created a phase shift in the quality\n\nof models and the applications that are possible for AI. More crucially,\n\nthese systems have captured the imagination of application developers all\n\nover the world who see limitless ways to apply LLMs to their particular\n\nbusiness domains.\n\nAI and machine learning is a complex topic that can take years to master,\n\nbut fortunately, with the assistance of libraries and pre-built models, it takes\n\nsignificantly less time to begin to incorporate intelligence into your\n\napplication. This chapter does not attempt to make you an AI expert, but it\n\ncan serve as an introduction to the concepts and approaches for using AI in\n\nyour system.\n\nThe Basics of AI Systems\n\nBefore we get started on the details of using AI in your system, it is useful\n\nto get a grounding in the core concepts that make up AI application. The\n\nplace that most people start is with a model. A model is a collection of\n\nnumeric weights that encode the knowledge in a neural network. In modern",
      "content_length": 1302,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 316,
      "content": "LLMs, there are trillions of these weights. As a rough definition, you can\n\nthink of the model as a function that takes a collection of inputs and\n\ntransforms them into some output.\n\nUnlike a traditional function in a programming language, however, a neural\n\nnetwork model is learned via training. Training is the act of taking large\n\namounts of training data and training the model from this training data. The\n\ntraining data contains pairs of both inputs and outputs. When training\n\nhappens, the input is fed into the existing model, and its current output is\n\nobserved. The current output is compared to the expected output in the\n\ntraining data. The error between the expected output and the observed\n\noutput is fed back through the model in a single training iteration. This\n\nprocess is repeated over and over again until the model correctly outputs the\n\nexpected training outputs. This training process is computationally\n\nexpensive and uses dedicated hardware.\n\nBecause of the cost and complexity of training these LLMs, more recently\n\npeople have used fine-tuning to refine foundational models like ChatGPT.\n\nFine-tuning is the process of taking a general-purpose model and refining it\n\nto better match your application. Fine-tuning generally uses smaller data\n\nsets and less computation so it is more efficient from a cost perspective.\n\nOnce you have a model trained, it can be put to use within your application.\n\nWhile training evaluated the model with known inputs and expected\n\noutputs, inference is the process of evaluating the model on user inputs",
      "content_length": 1563,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 317,
      "content": "where the output is unknown. Inference is typically implemented as a\n\nRESTful API, which you can call with the user’s input.\n\nMany cloud providers provide inference as a service. Such solutions can\n\nmake it significantly easier to get started incorporating AI into your\n\napplication and often allow access to models that may not be available for\n\nyour own usage. However, some applications have data privacy or security\n\nrequirements that mean they need to host inference on their own servers.\n\nWe’ll discuss how this is done in later sections.\n\nFor LLMs, the final part of inference is the prompt itself. When you chat\n\nwith an LLM, it may seem as if your query text is sent directly to the\n\nmodel, but in most cases it is wrapped as part of a larger text that is called\n\nthe prompt. The prompt provides context and instructions to the model that\n\nare not included in your specific query. For example, a prompt might add\n\nyour location or other personalization information that is known about you\n\nto your query. The structure of the prompt is critical in getting good\n\nresponses from an LLM. The act of creating a good prompt is known as\n\nprompt engineering. In addition to the examples above, prompt engineering\n\ncan be used to add additional data from traditional data sources and to\n\nensure that the responses from the model adhere to your application’s\n\nrequirements for accuracy and stability.\n\n“The Basics of AI Systems” described the components of an AI application.\n\nThe following sections will get into more details about how the application",
      "content_length": 1552,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 318,
      "content": "can be built.\n\nHosting a Model\n\nBuilding a model is a large, complex task that could fill an entire book (or\n\nmore) and is thus outside of the scope for this chapter. Even if you are\n\ninterested in doing your own training or fine-tuning, you are encouraged to\n\nbegin by using pretrained models so that you can get a sense for how well\n\nAI works with your application. Not every application is a great fit for the\n\ncurrent state of AI models, and thus, before investing the time, energy, and\n\nmoney in training, it makes sense to evaluate your overall approach.\n\nWhen you are looking to host a model, the first thing to consider is the\n\ncomputational requirements of the model and your application. If your\n\napplication is going to be used interactively by a user, the latency of the\n\nresponses is critical to the quality of the application. Generally, users begin\n\nto notice latency at 250 milliseconds and start turning away and doing other\n\ntasks when latency reaches more than a few seconds. Thus, in interactive\n\ninference, performance is critical.\n\nTypically, LLMs require high-performance dedicated GPUs to achieve the\n\nrequired latency. Such GPUs are expensive to purchase for your own\n\nhardware or rent from the cloud; further, given the intense interest in AI, the\n\nchips themselves have been hard to obtain from time to time. LLMs",
      "content_length": 1340,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 319,
      "content": "typically cannot run on the hardware profile of a mobile device such as a\n\nphone or tablet. If your application is intended for a mobile use, you will\n\nneed to run the inference in the cloud and access it from the mobile device\n\nover the network.\n\nThe challenge of the cost of inference has led to the development so-called\n\n“small language models.” These models, like Microsoft’s Phi family of\n\nmodels, use much more focused training data to achieve reasonable\n\nperformance with many fewer weights (billions versus trillions). Though\n\ntheir scores on AI benchmarks cannot compete with larger models like\n\nChatGPT, the reduced size means that both the cost and latency of inference\n\ncan be dramatically reduced. For many applications, the trade-off is worth\n\nit, and small language models (SLMs) can deliver acceptable quality at a\n\nmuch lower cost. Furthermore, since SLMs can run on a mobile device,\n\nthey also can help address concerns from users who do not want to share\n\ntheir data with a cloud-based model.\n\nDistributing a Model\n\nSo far we’ve talked a lot about models for both training and inference, but\n\nwe’ve had little discussion of how the models themselves are actually\n\nstored to file and loaded for processing. There are several different\n\nframeworks for performing training and inferences (we’ll discuss them in a\n\ncoming section), and each framework has its own file format; for example,",
      "content_length": 1404,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 320,
      "content": "TensorFlow uses the TF2 file format. It’s fine to use a framework-specific\n\nformat if you don’t plan to distribute your model and you don’t expect to\n\never change formats. However, for most users a framework-independent\n\nopen format for models is the best choice.\n\nThe most popular open model format is ONNX (Open Neural Network\n\neXchange), sponsored by Microsoft, Facebook, and Amazon and used by\n\nthousands of open source developers around the world. Models that are\n\nstored in the ONNX format can be used in all of the popular frameworks for\n\ninference and learning and are supported by various model “hubs,” or\n\ngalleries of popular models.\n\nFor many users, especially when you are getting started, using someone\n\nelse’s open source model is the best way to start performing inference,\n\nmuch like cloud native container users start downloading container images\n\nfrom the Docker hub. These model hubs provide easy access to the most\n\npopular open source models. One of the most popular model hubs is called\n\nHugging Face. Hugging Face includes not just the model distribution, but\n\nalso code to make it very easy to download and start using the models\n\nthemselves. Nearly the entire AI ecosystem uses Python as the language of\n\nchoice. Getting started with a model from Hugging Face is as easy as a one-\n\nliner of Python code.\n\nThe models stored in these hubs can be quite large; and thus, while you can\n\ndownload them every time your application starts up, you probably don’t",
      "content_length": 1479,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 321,
      "content": "want to, as it can add minutes of time to your application startup. Instead,\n\nthe best pattern is to cache the model locally, either within a shared\n\nfilesystem or an in-memory key-value store. Once you cache your model,\n\nall of your instances for inference can load from the same cached model.\n\nWhen you restart your application because of failures or new software\n\nreleases, you don’t have to delay to redownload the same model over and\n\nover again.\n\nThe final consideration with model distribution is safe deployment of new\n\nmodels. Though changes to models typically happen less frequently than\n\nchanges to your code, you will make changes to the model over time. Just\n\nlike safe deployment of your code, it is critical that you perform safe\n\ndeployment of your model. Changes in your model can have significant and\n\nunpredictable impact on the quality and reliability of your application. To\n\nlimit these risks, you should follow the best practice of progressive\n\nexposure and start by using the new model on only a very small subset of\n\nyour users, typically tied to a percentage of users or a specific geography.\n\nAs you gain confidence that the model is working correctly, you can\n\ngradually expand it to more and more users until ultimately all of your\n\napplication is using the new model.",
      "content_length": 1298,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 322,
      "content": "Development with Models\n\nThe complexity of inference means that it requires significant compute\n\ncapacity. Whether you are hosting your own models or using a cloud-based\n\nmodel as a service, this means that every inference is expensive. Unless you\n\nare specifically doing development related to the quality of your inference\n\n(e.g., prompt engineering), it probably doesn’t make sense to pay the costs\n\nof a full model when you are working on the user interface or other parts of\n\nthe service which are not directly related to the AI.\n\nYou should already be using a RESTful web API to perform inference,\n\nwhether with services like Azure machine learning or models you host\n\nyourself. You can use this interface to hide the details of the specific model\n\nbeing used for inference and thus save costs when you don’t need the full\n\nmodel.\n\nIn this approach, you develop a parameter to the API, which indicates\n\nwhich model to use. If you are using a cloud-based API, you may need to\n\ndevelop a wrapper RESTful service that provides this switch. When you\n\ndeploy this compatibility interface, you choose which model to target. In\n\ndevelopment and testing environments, you can create a Kubernetes Service\n\nresource which points to a cheap SLM; whereas in production\n\nenvironments, you can use the full model. There are already several open",
      "content_length": 1336,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 323,
      "content": "source projects out there which provide generic inference APIs that can\n\nspan a wide variety of different models.\n\nSimilarly, when performing continuous evaluation and automated unit\n\ntesting of your application, it makes sense to deploy smaller, cheaper\n\nmodels, unless you are specifically evaluating quality.\n\nRetrieval-Augmented Generation\n\nUsers interacting with AI systems expect that they are capable of\n\nunderstanding the context of the specific user, as well as the latest\n\ninformation available in the world. Unfortunately, because of the nature of\n\nAI models, the models themselves are fixed at a specific point in time when\n\nthey were trained. Because these models are static and based on generic\n\ninformation, providing responses that are specific to the user or reflect the\n\ncurrent state of the world are challenging. Retrieval-Augmented Generation\n\n(RAG) is a technique developed to handle these challenges.\n\nWith RAG, the user’s prompt is augmented with traditional queries to\n\nprovide the necessary context to correctly deliver an answer to the user’s\n\ninput. Consider, for example, the query:\n\nPrompt: Who is the closest friend to my current location?",
      "content_length": 1170,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 324,
      "content": "It is clear that a generic LLM has no knowledge of either who the person’s\n\nfriends are, nor their current location. We need RAG to solve this problem.\n\nWhen implementing a system with RAG, in addition to the user’s prompt,\n\nthe system performs two independent queries. The first might be a\n\ntraditional database query across the user’s contact list to determine the\n\ncomplete set of friends. The second query might be to their phone’s location\n\nservices to determine the user’s current location.\n\nThe results of these two traditional queries are combined with the user’s\n\nprompt and then sent to the model. An example RAG prompt might look\n\nsomething like:\n\nPrompt: Given a set of friends with ${address} and a current\n\n${location} , who is the closest friend to the current location?\n\nWe can see how the personalized and up-to-date information provided by\n\nthe RAG retrievals are combined with the user prompt to provide the LLM\n\nwith sufficient information to correctly respond to the user.\n\nRAG is a great technique for experienced developers who may not have lots\n\nof AI experience to develop sophisticated AI applications. By combining\n\ntraditional database queries with natural language prompts from users, very\n\nsuccessful natural language interfaces can be built that feel very personal. A\n\nfurther benefit of the RAG approach is that it respects user privacy. None of\n\nthe user’s personal information is encoded into the LLM, where it might",
      "content_length": 1450,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 325,
      "content": "accidentally be revealed to other users. Instead, the information is only\n\naccessible to the user that has access to the information, and provides it to\n\nthe model in the form of an augmented prompt. Because of its\n\nexplainability and privacy-protecting features, RAG is one of the most\n\npopular techniques for chat-based AI systems today.\n\nTesting and Deployment\n\nAt this point, the idea of testing our services is well understood and\n\naccepted as a required practice if you want to maintain a reliable and well\n\nrunning service. But what does it mean to perform a unit test in the context\n\nof an AI application when making changes to the prompt can have\n\nsignificant impact on the different responses that you get from the model?\n\nThe first important mind-shift that must be made when testing AI\n\napplications is the switch from correctness testing to statistical at-scale\n\nquality testing. When we were taught to write unit tests, we were taught to\n\nevaluate whether the output of the function that we were testing was\n\ncorrect. Indeed, there are whole frameworks dedicated to expressing the\n\nexpectation that Value(A).equals(B) . Such tests do not work for AI\n\napplications.\n\nInstead of looking for exact matches, we are looking for our tests to\n\nevaluate if a response is “good enough.” Once we shift our minds from",
      "content_length": 1320,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 326,
      "content": "“correct” to “good enough,” we realize that evaluating a single input is no\n\nlonger acceptable. As the model changes, the response to any one input may\n\nchange dramatically. Indeed, while a change may make the response to one\n\ntype of input dramatically better, it may make the response to many other\n\ninputs dramatically worse.\n\nThus, to correctly test our AI applications, we have to consider their\n\nperformance across a large selection of prompts and evaluate if, on average,\n\nthe change improves or harms the quality of the responses. If a change has\n\nzero or positive impact on the overall quality of the responses, the change\n\npasses the test. If the quality of the responses is decreased, the test fails.\n\nBut then to actually implement such a test, we need a way to assess the\n\nquality of a response. This task itself can be a challenge. Obviously, it is far\n\ntoo expensive to have humans evaluate the quality of each response—some\n\nautomated solution is required. Fortunately, LLMs themselves can be used\n\nfor some of this evaluation. Just like you used the model to answer the\n\nuser’s input, you can likewise ask the model if the answer it gave is a high-\n\nquality answer. Your prompt looks something like:\n\nPrompt: Given the following input: ${input} is the response:\n\n${response} a high-quality response?\n\nThis approach may seem like giving the inmates control of the asylum, but\n\nit actually works well in many cases.",
      "content_length": 1430,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 327,
      "content": "In addition to such AI-driven quality testing, before you release a new code\n\nor prompt change, it is critical that you incorporate user feedback signals\n\ninto your safe deployment. While, in other systems, you may look at errors\n\nand latency to determine if a release is safe to continue, when rolling out an\n\nAI application, you must also measure and evaluate if your users continue\n\nto be happy with the responses they are receiving.\n\nAI changes the way we build applications, and thus, also changes the way\n\nthose applications are tested. However, it does not change the criticality of\n\ntesting and evaluating every change you make to ensure it does not impact\n\nthe overall quality of your application.\n\nSummary\n\nIn just a few years, AI has transformed both our imagination of what our\n\napplications could be, as well as our users’ expectations for how they can\n\ninteract with those applications. However, at the end of the day, AI\n\ninference is just another function that is provided by a distributed system\n\nand delivered to our users. This chapter provides an introduction to the core\n\nconcepts and techniques for building AI applications that will enable\n\nanyone familiar with distributed systems to augment their distributed\n\napplications with AI.",
      "content_length": 1256,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 328,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 329,
      "content": "Chapter 16. Common Failure Patterns\n\nSo far this book has covered various patterns to help you build your\n\ndistributed system. This chapter is going to be a little different. Instead of\n\nhelping you know what to do, it is intended to help you know what not to\n\ndo. Over numerous years of developing, operating, and debugging systems,\n\ncertain kinds of problems repeat themselves. These patterns are divided into\n\nmistakes that are made in building the systems, as well as common ways in\n\nwhich systems fail. By understanding both what not to do and what to try to\n\nprevent, we can learn from these shared mistakes and prevent them from\n\nrepeating in the future.\n\nThe Thundering Herd\n\nThe thundering herd derives its name from the metaphor of a bison or other\n\nlarge animal on the prairie. Individually they may be manageable, but when\n\nmoving together, charging, they are capable of destroying anything they are\n\ndirected toward. The easiest way to understand the thundering herd is to\n\nimagine yourself interacting with a website that is not behaving properly.\n\nYou attempt to navigate to a particular location, the loading progress bar\n\nspins slowly, not making very much progress, eventually you become\n\nimpatient and you hit the reload button. You may not know it, but you have\n\nbecome the thundering herd.",
      "content_length": 1310,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 330,
      "content": "Any particular application has a maximum capacity. Typically we try to size\n\nour applications so that its maximum capacity is greater than any load that it\n\nexperiences, even at its most busy. Unfortunately, sometimes, because of\n\nfailures, lost capacity, or unanticipated demand, the requests sent to a\n\nservice exceed its ability to service those requests. Let’s call the size of this\n\noverload traffic X requests per second. Being overloaded is bad; many, if\n\nnot all, of those X requests per second time out or fail, but what happens\n\nnext is even worse. In the next minute, those same X requests become 2X\n\nrequests. This doubling is the result of any net new traffic (X) as well as all\n\nof those previous requests being retried. The result is even more failures,\n\neven more retries, and even more new requests. What was already a bad\n\nsituation has become irrecoverable. This is especially true when there are\n\nmultiple chained calls and multiple retry algorithms involved. The\n\namplification of one request into many requests can rapidly overwhelm a\n\nsystem.\n\nWhen we build our systems, we build in retry in order to handle when\n\nrequests fail, but if we blindly retry when we see errors, we make the\n\nthundering herd worse and worse. On the client side, the first way to handle\n\nthis is to use exponential backoff. Instead of immediately retrying, the client\n\nwaits a period of time before trying again. Every time the client sees an\n\nerror, it doubles the time it waits.\n\nExponential backoff is good, but it is imperfect; in addition to exponential\n\nbackoff, it is also very useful to add jitter. Jitter is a small amount of",
      "content_length": 1633,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 331,
      "content": "randomness which spreads load across time. Without jitter, a situation can\n\noccur where everyone fails at the same time, waits the same amount of\n\ntime, and then hammers the service again at the same time. Jitter is\n\nespecially important in situations where programs are talking to other\n\nprograms. Humans are sufficiently random that they add their own jitter, but\n\nunfortunately, machines are not.\n\nThe final and most comprehensive way to solve the thundering herd is to\n\nadd a circuit breaker to your client. The circuit breaker is something that\n\ntrips when error rates exceed some threshold and stops all traffic for a\n\nperiod of time, giving the system time to recover. Finally, to aid in recovery,\n\nsometimes it is also necessary to only gradually let traffic back into the\n\nsystem. Many systems can be stable under high load but must be brought up\n\nto that load level gradually. Dumping 100% of the traffic on a server all at\n\nonce can simply reintroduce the thundering herd.\n\nThe Absence of Errors Is an Error\n\nWe will move from problems of too much traffic to problems caused by too\n\nlittle traffic. When building a system, it is common to build monitoring and\n\nalerts to fire when there are too many errors in the system. Obviously, too\n\nmany errors is a problem that requires a person to look into the system,\n\ndetermine the cause of the errors, and restore the service. But what about\n\ntoo few errors? Though it can seem counter-intuitive, some small number of",
      "content_length": 1473,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 332,
      "content": "errors is the steady state for most distributed systems. It is generally very\n\nhard and not worth the effort to prevent any errors from ever occurring;\n\nthus, there is a low level of ambient errors that are handled by retry and\n\nother error-correction systems. Consequently, the absence of any errors is\n\nmore likely to indicate a serious problem rather than that everything is\n\nawesome.\n\nTo understand how this can happen, imagine monitoring a system which\n\nadds subtitles to movies that are uploaded to a storage bucket. Whenever a\n\nmovie is uploaded, the system performs speech to text and translation to\n\nadd subtitles and then stores the new movie back into the storage bucket.\n\nImagine what happens when the process that uploads movies accidentally\n\nloses permissions to upload those movies. If you are only monitoring for\n\nconditions where errors are greater than some percentage (say, 10%), then\n\nyou will never alert for this condition. If the uploader cannot upload\n\nmovies, then no movies are being processed, so no errors can occur, so your\n\nerror rate is 0%. But clearly, if your uploader can’t upload movies, your\n\nsystem is broken. Alerting on both too many and too few errors (or too few\n\nrequests generally) will enable you to catch (and fix) such problems in your\n\nsystems.",
      "content_length": 1291,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 333,
      "content": "“Client” and “Expected” Errors\n\nThe next pattern considered is also related to errors in the system, but this\n\ntime related to ways in which we feel OK ignoring errors. Many systems\n\ntake in user input and requests for processing. In such systems, it is possible\n\nfor users (or clients generally) to send requests which are invalid or\n\nunprocessable in some way. When calculating the reliability of a service,\n\nit’s clearly problematic to treat errors caused by bad client requests as true\n\nerrors, since the service isn’t in control of what the clients send. While this\n\nis true in general, in practice such “not my problem” buckets become\n\nconvenient places to ignore true system errors. Imagine, for example,\n\nsomething as simple as an authorization error. Certainly a user not being\n\nauthorized to make a request is a “client” error and doesn’t indicate a\n\nproblem with reliability. But what if suddenly all users are unauthorized?\n\nThat’s probably a problem with our authorization code. But if you treat all\n\nunauthorized errors as “user errors” and you don’t look for anomalies like\n\nall users failing authorization, you probably won’t alert when you break\n\nyour authorization code.\n\nThe same thing can be true of “expected” errors. I mentioned in “The\n\nAbsence of Errors Is an Error” that there’s a certain level of errors that are\n\nexpected in the system, but for synthetic requests that you create, that’s not\n\ntrue. When monitoring a system, it is also important to monitor real client\n\nrequests where some small percentage of errors is expected and also to",
      "content_length": 1567,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 334,
      "content": "write synthetic or system-generated requests, which you control entirely.\n\nWith synthetic requests, you control both the client and server side of the\n\ninteractions and, consequently, there should never be any “client” or\n\n“expected” errors for those requests. By combining monitoring of real\n\ntraffic with monitoring of synthetic requests, you can quickly identify\n\nsituations where your code changes are causing errors which would\n\notherwise be ignored as client errors.\n\nVersioning Errors\n\nWhenever you are building software, you are going to need to iterate and\n\nmake changes to that software; but clients require and expect consistency in\n\nhow they interact with your systems. If you need to update every client at\n\nthe exact same time as you update your system, you haven’t actually built a\n\ndecoupled, distributed system. Any system is made up of the external API\n\nthat it exposes to clients and the internal in-memory representation of that\n\nAPI. At first glance, it seems that the best solution is to make these two\n\nrepresentations the same. If they are different, then translation logic needs\n\nto be applied to take the external representation to the internal\n\nrepresentation and vice versa. And of course, when you start out, the\n\nexternal and internal representations are identical.\n\nIn practice, it turns out that separating the internal on-disk or in-memory\n\nrepresentation of your API from the external client-perceived representation",
      "content_length": 1450,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 335,
      "content": "of your API pays immense dividends. For starters, it makes it relatively\n\neasy to have multiple different external client API versions implemented by\n\nthe same internal in-memory version. This means that different clients can\n\ntalk to you using different versions of your APIs, and you don’t need to\n\ntightly synchronize your clients and your application. Additionally, the\n\nseparate internal representation means that it is easier for you to iterate on\n\nyour API implementation; adding, removing, or merging API fields as\n\nnecessary to support your application can be done without the client\n\nnoticing as long as you maintain the translation logic between external and\n\ninternal representations.\n\nThough the benefits of this separation are often seen in supporting multiple\n\nclients, the same can be applied to your internal storage versioning.\n\nUltimately, your application will also need to store the objects in your API\n\nin some data store. This storage also needs to be versioned. If it is tightly\n\ncoupled with a specific version of your code, it can make it very difficult to\n\nroll out changes to your storage layer, or roll back code when there are\n\nbugs.\n\nEven though it may seem overly complicated to begin with, building every\n\napplication with an external (client) version, an internal (in-memory)\n\nversion, and a storage (on-disk or database) version as well as the logic to\n\ntranslate between them pays significant dividends in decoupling and agility\n\nthat will benefit your application in the long run.",
      "content_length": 1517,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 336,
      "content": "The Myth of Optional Components\n\nDistributed systems are organic systems. We may design them in one way\n\nto start, but over time and in reaction to outages, scale challenges, and\n\nbusiness changes, they morph and adapt to meet new requirements. One of\n\nthe most common changes is the addition of caches in various locations to\n\nimprove performance and reduce load.\n\nTypically, when these caches are added to the system, they are treated as\n\n“optional” components. After all, a cache is just a performance\n\nimprovement. If the system needs to, it can always obtain the data from the\n\nsource of truth. The problem with theoretically optional components in your\n\nsystem is that, over time, they become required parts of your design.\n\nConsider adding a cache to a system. It is an optional performance\n\nimprovement when originally added. However, as the load on the system\n\ngrows over time, its performance becomes more and more dependent on the\n\npresence of the cache until the system can no longer meet its performance\n\ngoals without the cache.\n\nUnfortunately, this increased reliance often goes unnoticed, and the overall\n\nreliability of the caches in the system is not maintained in correspondence\n\nwith its increased criticality to overall system stability.\n\nOne of the most common ways that this manifests is in the difference\n\nbetween a local and a global cache. A local cache, as the name suggests, is a",
      "content_length": 1407,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 337,
      "content": "cache that is maintained locally in the memory of your application, often as\n\na simple hash table. Such caches are very easy to implement. Unfortunately,\n\nas the system grows, the overall size of such local caches has to grow\n\nproportionally, and eventually the overall memory demands of the system\n\nbecome too great to sustain the cache. Furthermore, there is a tendency to\n\nadd many such in-memory caches within a single application because it is\n\nso easy to do. Consequently, the memory usage of such caches is difficult\n\nboth to visualize and manage.\n\nAdditionally, as discussed in Chapter 15 on caching, local caches are very\n\nbad at optimizing memory usage. Consequently, if you ever find yourself\n\nadding a local “optional” cache or any other theoretically “optional”\n\ncomponent to your system, accept that you are actually making a permanent\n\n(and very required) change to your overall distributed system that deserves\n\nthe attention of any other significant change or refactor.\n\nOops, We “Cleaned Up” Everything\n\nIn any real system, it’s an unfortunate truth that errors occur. Bugs and\n\nfailures are bound to happen. Given this, it is often important to consider\n\nhow errors and failures might impact the overall state of the system that we\n\nare building. When doing this, it is important to think about more than just\n\nthe operation of the individual pieces that make up the system, but also to\n\nthink in terms of how all of these pieces interact together to manage the",
      "content_length": 1480,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 338,
      "content": "entire distributed system. Failing to anticipate how these systems might\n\ninteract together in the presence of a failure is a common cause of severe\n\nproblems. One of the most severe is a runaway deletion of a system because\n\nof some sort of unanticipated interaction effect.\n\nDeleting everything in your system seems like such an extreme failure that\n\nit is hard to understand how it might happen by accident via a minor bug.\n\nConsequently, it is useful to examine real-world examples of how such a\n\nthing might happen.\n\nAn example is a photo storage system. Imagine that it has four components:\n\nA database that stores user information\n\nA frontend RESTful API server that implements the user API\n\nA file storage system where all of the photos are stored\n\nA garbage collection system, which cleans up photos for users after their\n\naccounts are deleted\n\nIn normal operation, a user initiates an account deletion through the\n\nwebsite, and the user is removed from the database. Then, for each photo\n\nstored in the filesystem, the garbage collection system looks up the user in\n\nthe database. If the user can’t be found, the photo is deleted. This garbage\n\ncollection system exists to keep the system compliant with data privacy\n\nlaws as well as to ensure photos don’t “leak” and cause infinite growth in\n\nthe filesystem.",
      "content_length": 1319,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 339,
      "content": "Now consider a piece of code written in the frontend RESTful API to look\n\nup a user. It initiates a connection to the database, looks up the user’s\n\ninformation, and then returns it to the caller. But what if the database is\n\nunavailable for some reason? Suppose the person who coded that piece of\n\nlogic chooses to return a 404 indicating that the user couldn’t be found.\n\n“Not Found” is perhaps not the best error to return in this case—500\n\ninternal service error is definitely better; but it is hard to see how this error\n\ncode is important until you think about the garbage collection system. When\n\nthe garbage collection system inquires about any user, they will all appear\n\nto be missing, and the garbage collection system will delete all images.\n\nFortunately, because people anticipated catastrophic problems, the photos\n\nwere restored from backups; but the service was down for many hours\n\nwhile these backups were restored from cold storage. During this period,\n\nuser confidence in something so deeply personal and important as your\n\nfamily photos was shaken, and the service had to work very hard to restore\n\nconfidence.\n\nSo what went wrong? There are secondary basic failure points in this\n\nsystem that caused the catastrophic failure. The first is a human failing.\n\nNamely, it is hard for any human to keep the full context of a distributed\n\nsystem in their head at all times. Especially in a situation where one team\n\nmay have owned the user API system, while a different team, possibly in a\n\ndifferent geographic location, owned the garbage collection system. It is\n\ntempting to focus on this human failure. The developer made a mistake and\n\n“caused” this outage. Unfortunately, human failures are an inherent part of",
      "content_length": 1732,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 340,
      "content": "distributed systems; and ultimately they cannot be fixed, only prevented.\n\nHumans, regardless of their seniority, will make mistakes. Our systems\n\nmust anticipate and prevent the effects of those mistakes.\n\nThis leads us to the second failing: the process or technical failure in the\n\nsystem, namely, rate limiting and a circuit breaker in the garbage collection\n\nlogic. In the system under normal operation, there is always roughly the\n\nsame number of users deleting their accounts. Let’s say that one percent of\n\nall users delete their accounts in a given day; this will lead to a relatively\n\nconstant rate of image deletes. When this bug occurred, the rate of image\n\ndeletes increased by 100 times. This is clearly unusual, and the system\n\nshould have detected and prevented such a high rate of deletes. The first\n\nsystematic improvement would be the addition of throttling to the deletes.\n\nThe throttling sets an upper bound on deletes per second and would have\n\ndramatically slowed down the rate at which all of the photos could be\n\ndeleted to give humans a better chance to react and stop the process. The\n\nsecond is a “circuit breaker,” much like the circuit breakers in our houses;\n\nwhen this high rate of deletes was hit for an extended period of time, the\n\ngarbage collection system should have stopped itself from performing any\n\ndeletes until a human reset the breaker. Combined, while these technical\n\nchanges couldn’t prevent there being some impact from the human mistake,\n\nthey would dramatically reduce the impact on both the customers of the\n\nservice and the brand of the service itself.",
      "content_length": 1605,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 341,
      "content": "Such accidentally “we cleaned up everything” scenarios are actually\n\nsurprisingly common, because as developers we tend to focus on the happy\n\npath where everything is working correctly. In that world, the call to the\n\nuser API can never return a wrong answer. Experience in distributed\n\nsystems teaches us that we always need to be thinking about a world where\n\nthings are going right, but also what happens when things go wrong.\n\nChallenges with the Breadth of Inputs\n\nOne of the most disappointing kinds of failures is the failure that slips\n\nthrough all of your testing and goes undetected until a customer notices it.\n\nSuch failures are frustrating to your customers. “Don’t you test your code?”\n\nis a hard question to answer well in such cases. They are equally\n\nembarrassing because often they sit in your codebase for a long time before\n\nsomeone notices and/or complains loud enough to get you to notice.\n\nSometimes these errors are undetected because it takes a very rare\n\nconfluence of events to make them happen, e.g., tricky race conditions that\n\noccur once in a blue moon. Such bugs are actually significantly less likely\n\nto frustrate an end user; they just look like flakiness. We have all been\n\ntrained to “reload and try again,” which generally fixes such bugs.\n\nThe bad ones are the ones which occur for specific users 100% of the time,\n\nbut only for a small number of users. I typically call these problems",
      "content_length": 1425,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 342,
      "content": "“compiler” problems, because they typically occur when there is a very\n\nbroad set of allowed inputs to our system. In such a system, very specific\n\ncombinations of input can cause problems 100% of the time, but only\n\ncertain users present such inputs.\n\nThese are called “compiler” problems because the compiler is one of the\n\nfirst programs where such problems manifest themselves. For any given\n\nprogramming language, the space of possible legal programs is effectively\n\ninfinite. Given enough programmers working on enough programs, sooner\n\nor later, one of them will write a program that is by the book legal, but\n\nexposes a bug in the compiler that causes it to crash. Sometimes these\n\ninputs are the product of random chance and human nature, but more and\n\nmore often these inputs are created by fuzzing programs, special purpose\n\ncode that fire random data at APIs in the hopes of causing crashes to deny\n\nservice or exploit weaknesses in the APIs. An (in)famous version of such\n\nattacks were the SQL injection attacks that were typical of the early web,\n\nwhere bits of random SQL (e.g., '; DROP TABLES' ) were provided as\n\ninputs to places where no developer could reasonably expect SQL\n\nstatements (e.g., a credit card number). Though, by now SQL injection\n\nattacks are largely a thing of the past, accidentally breaking legitimate users\n\nor randomized “fuzzing” attacks are still quite common.\n\nUltimately, this failure represents a failure of coverage in testing. If the\n\nsurface area of a program is completely tested, such failures can’t happen;\n\nbut the space of valid inputs for many APIs is infinite or near infinite. How",
      "content_length": 1636,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 343,
      "content": "can you ensure coverage of your testing in such a situation? There are two\n\nbasic solutions.\n\nThe first is to record a representative sample of real-world inputs. This\n\nsample can never be exhaustive, but if you record (for example) all user\n\ninputs over the last two months, you can be reasonably sure that you are\n\nunlikely to break a legitimate user. When you are doing this recording,\n\nthough, you need to be careful about both where and when you record it. If\n\nyou have a global service, but only record inputs in North America, you will\n\nlikely miss many bugs in inputs with Asian characters. Similarly, if your\n\ninput varies between night and day, and you only record at a certain time,\n\nyou will miss variation in your inputs due to the time of day. High-quality\n\ncoverage in testing only comes when your test data is representative of all\n\nof your users.\n\nThe second solution is to use fully randomized or “fuzzed” data for your\n\ninputs. These tests can be significantly more comprehensive, but they are\n\nalso much harder to create. For a simple API that perhaps only takes a few\n\nintegers as parameters, it is easy enough to generate random numbers, but\n\nfor more complicated APIs with richer semantics, you may need to write\n\ncustom random input generation logic (for example, something that\n\ngenerates random but valid IP addresses). Lately, innovation in the space of\n\nLLMs and generative AI makes it possible to generate valid but random\n\ninputs for even more classes of data.",
      "content_length": 1490,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 344,
      "content": "The combination of testing previously seen inputs as well as using fuzzed\n\ndata can help ensure that your new code won’t break existing users and that\n\nit is hardened against possible attacks via random or maliciously crafted\n\ncommand inputs.\n\nProcessing Obsolete Work\n\nWe’ve all been faced with an unresponsive computer system: an email\n\nsystem where searching isn’t working, a retail site which refuses to load the\n\nnext page. This chapter is full of the many different ways in which systems\n\ncan fail to operate correctly. Faced with these failing systems, what do we\n\nall do? We hit the reload button. And thus, an additional way to fail has\n\nbeen created for our services.\n\nIn “The Thundering Herd”, we discussed the thundering herd and how\n\nretries or failovers can rapidly take down an entire system. The problem\n\ndiscussed in this section has somewhat of the opposite flavor, and that is a\n\nsystem struggling to catch back up.\n\nImagine a system that is searching for photos that match a photo of our\n\nfavorite pet. In this system, requests come in, they are processed\n\nasynchronously, and the results are made available for the person making\n\nthe request. But what if, by the time the results come back, the person is no\n\nlonger waiting? What if they have hit reload and resubmitted one, or even",
      "content_length": 1303,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 345,
      "content": "countless additional requests. With no one waiting for the results, the work\n\ndone to produce the results is wasted, and worse yet, the time taken to\n\nproduce those results has delayed our system responding to actual user\n\nrequests.\n\nTo understand how this happens, imagine that our photo processing system\n\nhas suffered a failure in its object recognition microservice. Processing\n\nrequests grinds to a stop, but the inbound requests keep arriving. A little like\n\na jam in an assembly line, even though no work is being processed, new\n\nwork keeps coming down the conveyor belt. This is obviously problematic;\n\nbut working quickly, we manage to resolve the problem, and work starts\n\nbeing processed again. But what happens now?\n\nDuring the time that the system wasn’t processing any work, a backlog of\n\nwork has accrued at the component which failed. If the system is close to its\n\noptimal load at steady state, as is desirable for maximal efficiency, the\n\nsystem will have a very hard time processing through this backlog. As long\n\nas the backlog exists, new requests are delayed. It’s not uncommon in such\n\nsituations for a short outage, say less than 10 minutes, to cause a multihour\n\nincrease in the latency of request processing, and corresponding\n\ndegradation in customer experience.\n\nSo how can we build our systems to respond better in the face of such\n\nbacklogs and latency increases? There are three approaches which can be\n\nused independently or together to solve the situation. The first is timing out",
      "content_length": 1513,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 346,
      "content": "requests. Every request that enters the system should have a timeout, the\n\ntime after which the request is considered invalid and failed. When the\n\nrequest times out, any work associated with that request is aborted.\n\nTimeouts can help the system shed work when significantly overloaded.\n\nHowever, it is important to note that timeouts by themselves cannot resolve\n\nthe problem described above. If the latency of each request is high, but\n\nslightly lower than the timeout value, your system will seem very sluggish,\n\nbut it won’t ever shed work to get back to normal.\n\nThe second approach is to use autoscaling based on request latency. We are\n\nused to seeing autoscaling based on CPU or memory usage, but often it\n\nmakes even more sense to autoscale based on user-facing metrics like\n\nprocessing latency. If your system can enable autoscaling, scaling based on\n\nrequest latency can help process the backlog that accumulated when it was\n\ndown. Even simply manually scaling after an outage, while less efficient,\n\ncan be an effective mitigation.\n\nThe final approach is to explicitly prioritize newer requests over older\n\nrequests when your queues grow too long or latency grows too high. This\n\napproach is a form of triage, which sacrifices the experience of some users,\n\nwho in all likelihood already abandoned that request anyway, to ensure that\n\nmore recent requests, likely the same users hitting reload, have a better\n\nexperience.",
      "content_length": 1434,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 347,
      "content": "Designing reliable distributed systems means not only designing systems\n\nthat prevent failures from occurring, but also systems that recover quickly\n\nwhen failures do occur. Users are far more accepting of short outages when\n\nthe system recovers quickly than prolonged outages where recovery takes a\n\nlong time, even after the problem is “fixed.”\n\nThe “Second System” Problem\n\nSo far we have seen numerous common problems that arise in distributed\n\nsystems. As you have read through this chapter, you may have recognized\n\nseveral different problems that are present in your current system. Seeing\n\nthe possibility of failures, you are eager to improve the system, but how can\n\nyou do it?\n\nSometimes there are only a few changes needed, and it’s obvious how to\n\nmake these minor improvements to the existing system—roll them out and\n\nbe done with it. Unfortunately, more often than not, the problems seem so\n\nlarge and so hard to fix in the existing system that it seems the most\n\nefficient solution is to build a new system, from scratch, that replaces the\n\nexisting system at some date in the future. This approach is attractive for a\n\nnumber of reasons: it’s always easier to build from scratch rather than\n\nrefactor; it is always more exciting to build something new than it is to\n\nrefactor an existing solution; and finally, building the new system alongside",
      "content_length": 1362,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 348,
      "content": "the old system allows the new system to be built without the pressures of\n\nbeing used in production until it is ready.\n\nThe attraction of all of these perceived benefits makes building the “second\n\nsystem” one of the most common failures in distributed systems; but it is\n\nthis last phrase, “until it is ready,” that turns out to be the curse of the\n\nsecond system. In practice, many second systems never actually become\n\nready. There are many different organizational reasons why this can happen,\n\nbut the most common are that the second system is forever playing catch-up\n\nto the current system and that it never gets the production mileage\n\nnecessary to replace the original system.\n\nThe second system tends to play catch-up precisely because it is not the\n\ncurrent system in production. This means that while it is being built, all of\n\nthe important features, bug fixes, and improvements necessary to run the\n\nbusiness land in the original system. Suppose it takes six months to\n\ncomplete the second system. At the moment when it is “complete,” there\n\nare six more months of features and fixes that it needs to implement until it\n\nmatches the capabilities of the original system. Unless you are willing to\n\nsignificantly over-staff the team building the second system, you are always\n\nplaying catch-up.\n\nThe need for over-staffing the development of the second system leads to\n\nthe other failing, which is the production readiness of the second system. In\n\norder for any system to become production ready, it needs mileage on it. No",
      "content_length": 1536,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 349,
      "content": "matter how good you think your code is, I guarantee that there are bugs that\n\nwill only be found once it is released into production. But when bugs are\n\nfound in the second system, the tendency is to turn it off—after all, the\n\noriginal system didn’t have those bugs. This dramatically reduces the\n\nexposure and overall mileage that the second system can obtain toward\n\nbeing truly production ready. Just like the feature debt that accumulates as\n\nthe second system is built, the production readiness debt accrues, and it is\n\nvery hard for the second system to catch up.\n\nGiven that there are probably design problems or technical debt in your\n\ndistributed system that you need or want to fix, and the “second system”\n\napproach is often doomed to failure, what can you do? The solution lies in\n\nthe nature of the distributed system itself. Rather than replacing the entire\n\ndistributed system, focusing on improving individual microservices can\n\nmake real progress tractable and significantly reduce the costs of running\n\ntwo systems in production at once. Similarly, it makes sense to invest in\n\nabstractions that separate the core business logic from code that may need\n\nto change as you grow and scale, for example, the storage layer. This\n\nseparation means that you can iterate your core systems like storage\n\nindependently from your business logic and even run two different storage\n\nlayers side by side for evaluation and testing.\n\nMuch like many other aspects of our lives, the urge to throw away the old\n\nand build something new, while tempting, is ultimately far less successful",
      "content_length": 1587,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 350,
      "content": "than adapting and improving our existing systems to meet the new needs of\n\nour businesses and our users.\n\nSummary\n\nThis chapter is unlike any of the others; instead of discussing how to build,\n\nit discusses how not to build. Hopefully, seeing the common ways that\n\ndistributed systems fail helps you find problems in your existing systems\n\nand also to build more reliable new systems. Learning from the failures of\n\nothers prevents you from making these common mistakes in your systems.\n\nOceanofPDF.com",
      "content_length": 502,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 351,
      "content": "Conclusion: A New Beginning?\n\nEvery company, regardless of its origins, is becoming a digital company.\n\nThis transformation requires the delivery of APIs and services to be\n\nconsumed by mobile applications, devices in the internet of things (IoT), or\n\neven autonomous vehicles and systems. The increasing criticality of these\n\nsystems means that these online systems must be built for redundancy, fault\n\ntolerance, and high availability. At the same time, the requirements of\n\nbusiness necessitate rapid agility to develop and roll out new software,\n\niterate on existing applications, or experiment with new user interfaces and\n\nAPIs. The confluence of these requirements has led to an order of\n\nmagnitude increase in the number of distributed systems that need to be\n\nbuilt.\n\nThe task of building these systems is still far too difficult. The overall cost\n\nof developing, updating, and maintaining such a system is far too high.\n\nLikewise, the set of people with the capabilities and skills to build such\n\napplications is far too small to address the growing need.\n\nHistorically, when these situations presented themselves in software\n\ndevelopment and technology, new abstraction layers and patterns of\n\nsoftware development emerged to make building software faster, easier, and\n\nmore reliable. This first occurred with the development of the first\n\ncompilers and programming languages. Later, the development of object-",
      "content_length": 1421,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 352,
      "content": "oriented programming languages and managed code occurred. Likewise, at\n\neach of these moments, these technical developments crystallized the\n\ndistillation of the knowledge and practices of experts into a series of\n\nalgorithms and patterns that could be applied by a much wider group of\n\npractitioners. Technological advancement combined with the establishment\n\nof patterns democratized the process of developing software and expanded\n\nthe set of developers who could build applications on the new platform.\n\nThis in turn led to the development of more applications and application\n\ndiversity, which in turn expanded the market for these developers’ skills.\n\nAgain, we find ourselves at a moment of technological transformation. The\n\nneed for distributed systems far exceeds our ability to deliver them.\n\nFortunately, the development of technology has produced another set of\n\ntools to further expand the pool of developers capable of building these\n\ndistributed systems. The recent development of containers and container\n\norchestration has brought tools that enable rapid, easier development of\n\ndistributed systems. With luck, these tools, when combined with the\n\npatterns and practices described in this book, can enhance and improve the\n\ndistributed systems built by current developers, and more importantly,\n\ndevelop a whole new expanded group of developers capable of building\n\nthese systems.\n\nPatterns like sidecars, ambassadors, sharded services, FaaS, work queues,\n\nand more can form the foundation on which modern distributed systems are\n\nbuilt. Distributed system developers should no longer be building their",
      "content_length": 1620,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 353,
      "content": "systems from scratch as individuals but rather collaborating together on\n\nreusable, shared implementations of canonical patterns that form the basis\n\nof all of the systems we collectively deploy. This will enable us to meet the\n\ndemands of today’s reliable, scalable APIs and services and empower a new\n\nset of applications and services for the future.\n\nOceanofPDF.com",
      "content_length": 368,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 354,
      "content": "Index\n\nA\n\nabsence of errors problem, The Absence of Errors Is an Error\n\nabstraction and encapsulation, microservices, Serving Patterns\n\naccessibility of the data, Aggregating Information\n\nadapter containers, Adapters, Adding a Health Monitor\n\nadapter pattern, Adapters-Summary\n\nversus decorator pattern, The Decorator Pattern: Request or Response\n\nTransformation\n\nhealth monitoring, Adding a Health Monitor-Hands On: Adding Rich\n\nHealth Monitoring for MySQL\n\nlogging, Logging-Hands On: Normalizing Different Logging Formats\n\nwith fluentd\n\nand merger pattern for workflow systems, Merger\n\nmonitoring applications, Monitoring-Hands On: Using Prometheus\n\nfor Monitoring\n\naggregating information, Hands On: Using Prometheus for Monitoring,\n\nMetrics, Metrics, Aggregating Information-Aggregating Information\n\nAI inference and serving, AI Inference and Serving-Summary\n\nAI systems, The Basics of AI Systems-The Basics of AI Systems\n\ndevelopment with models, Development with Models\n\ndistributing a model, Distributing a Model-Distributing a Model",
      "content_length": 1040,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 355,
      "content": "hosting a model, Hosting a Model\n\nRAG, Retrieval-Augmented Generation\n\ntesting and deployment of services, Testing and Deployment-Testing\n\nand Deployment\n\nalerting, monitoring and observability patterns, Monitoring and\n\nObservability Basics, Alerting-Alerting on anomolies\n\nalgorithmic programming, Formalization of Algorithmic Programming\n\nambassador pattern, Ambassadors-Summary\n\nfor experimentation, Using an Ambassador to Do Experimentation or\n\nRequest Splitting-Hands On: Implementing 10% Experiments\n\nmemcache deployment, Hands On: Deploying an Ambassador and\n\nMemcache for a Sharded Cache-Hands On: Deploying an\n\nAmbassador and Memcache for a Sharded Cache\n\nrequest splitting, Using an Ambassador to Do Experimentation or\n\nRequest Splitting-Hands On: Implementing 10% Experiments\n\nservice brokering, Using an Ambassador for Service Brokering\n\nfor sharded cache, Hands On: Deploying an Ambassador and\n\nMemcache for a Sharded Cache-Hands On: Deploying an\n\nAmbassador and Memcache for a Sharded Cache\n\nsharding a service, Using an Ambassador to Shard a Service-Hands\n\nOn: Implementing a Sharded Redis\n\nwork queues, The Source Container Interface-Work Queue API\n\nanomalies, alerting on, Alerting on anomolies",
      "content_length": 1211,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 356,
      "content": "Apache Storm, Hands On: Normalizing Different Logging Formats with\n\nfluentd\n\napplication containers\n\nand adapter container, Adapters\n\ndealing with reused, Logging\n\nhealth monitoring, Health Checks, Adding a Health Monitor-Hands\n\nOn: Adding Rich Health Monitoring for MySQL\n\nand sidecar pattern, Single-Node Patterns, Modular Application\n\nContainers-Hands On: Deploying the topz Container\n\napplication programming interfaces (APIs), APIs and RPCs-APIs and\n\nRPCs\n\ndefining container’s API, Define Each Container’s API\n\ndynamic configuration, sidecar pattern, Dynamic Configuration with\n\nSidecars-Dynamic Configuration with Sidecars\n\nfile-based API, The Worker Container Interface\n\nHTTP RESTful API, The Decorator Pattern: Request or Response\n\nTransformation-Hands On: Adding Request Defaulting Prior to\n\nRequest Processing, Work Queue API, The Worker Container\n\nInterface\n\nmetrics API, Prometheus, Hands On: Using Prometheus for\n\nMonitoring\n\nand microservices, Serving Patterns\n\npub/sub API, Publisher/Subscriber Infrastructure\n\nRESTful web API, AI models, Development with Models",
      "content_length": 1078,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 357,
      "content": "B\n\nversioning of, Work Queue API\n\nwork queue systems, Work Queue API-Work Queue API\n\napplication protocol, replicated services, Application-Layer Replicated\n\nServices\n\napplication-layer replicated services, Application-Layer Replicated\n\nServices\n\nThe Art of Computer Programming (Knuth), Formalization of\n\nAlgorithmic Programming\n\nasynchronous versus synchronous APIs, APIs and RPCs\n\nauthentication, FaaS, Hands On: Implementing Two-Factor\n\nAuthentication-Hands On: Implementing Two-Factor Authentication\n\nautoscaler, work queues, Dynamic Scaling of the Workers\n\nautoscaling, obsolete work processing solution, Processing Obsolete\n\nWork\n\navailability, relationship to consistency and partition tolerance, Data\n\nConsistency\n\nbackground processing, FaaS, The Need for Background Processing\n\nbacklog\n\nprocessing obsolete work, Processing Obsolete Work\n\nwork item, Errors, Priority, and Retry\n\nbarrier synchronization, Join (or Barrier Synchronization)\n\nbatch computational patterns\n\nAI inference and serving, AI Inference and Serving-Summary",
      "content_length": 1038,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 358,
      "content": "C\n\ncoordinated batch processing, Coordinated Batch Processing-\n\nSummary\n\nevent-based batch processing, Event-Driven Batch Processing-\n\nSummary\n\nfailure patterns, Common Failure Patterns-Summary\n\nmonitoring and observability patterns, Monitoring and Observability\n\nPatterns-Summary\n\nwork queues, Work Queue Systems-Summary\n\nbreadth of inputs challenge, Challenges with the Breadth of Inputs-\n\nChallenges with the Breadth of Inputs\n\nbrokering a service, ambassador pattern, Using an Ambassador for\n\nService Brokering\n\ncaches and caching\n\nAI models, Distributing a Model\n\nlocal versus global caches, The Myth of Optional Components\n\nmemcache deployment, Hands On: Deploying an Ambassador and\n\nMemcache for a Sharded Cache-Hands On: Deploying an\n\nAmbassador and Memcache for a Sharded Cache\n\nreplicated services, Introducing a Caching Layer-Hands On:\n\nDeploying nginx and SSL Termination\n\nsharded cache, Sharded Caching-Hands On: Deploying an\n\nAmbassador and Memcache for a Sharded Cache, Hot Sharding\n\nSystems",
      "content_length": 1006,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 359,
      "content": "caching web proxy, Introducing a Caching Layer-Deploying Your Cache\n\nCAP theorem (consistency, availability, partition tolerance), Data\n\nConsistency\n\ncircuit breaker solution, The Thundering Herd, Oops, We “Cleaned Up”\n\nEverything\n\nclient errors problem, “Client” and “Expected” Errors\n\nclient IP, sharding function, Selecting a Key\n\nclient-server architectures, A Brief History of Systems Development\n\nclient-side considerations\n\nambassador pattern, Using an Ambassador to Shard a Service, Hands\n\nOn: Implementing 10% Experiments\n\nand calling APIs, APIs and RPCs\n\nthundering herd problem, The Thundering Herd\n\nCloud Native Computing Foundation (CNCF), Hands On: Deploying\n\netcd\n\ncollection of queues, Publisher/Subscriber Infrastructure\n\ncompare-and-swap operation, The Basics of Leader Election\n\ncompiler problems, Challenges with the Breadth of Inputs\n\nconcurrency, Ownership Election, Implementing Locks\n\nconcurrent data manipulation handling, Handling Concurrent Data\n\nManipulation-Handling Concurrent Data Manipulation\n\nConfigMap, Dynamic Configuration with Sidecars, The Worker\n\nContainer Interface",
      "content_length": 1105,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 360,
      "content": "configuration synchronization, sidecar pattern, Dynamic Configuration\n\nwith Sidecars-Dynamic Configuration with Sidecars\n\nconsensus algorithm, The Value of Patterns, Practices, and Components-\n\nShared Components for Easy Reuse, The Basics of Leader Election-\n\nHands On: Deploying etcd\n\nconsistency\n\nof data, Data Consistency-Data Consistency\n\nrelationship to availability and partition tolerance, Data Consistency\n\nconsistent hashing function, Session Tracked Services\n\ncontainer groups, Single-Node Patterns-Single-Node Patterns, The\n\nSidecar Pattern\n\ncontainer images, Shared Components for Easy Reuse, Documenting\n\nYour Containers, Documenting Your Containers, The Worker Container\n\nInterface\n\ncontainer orchestration, Introduction, A Brief History of Systems\n\nDevelopment, Conclusion: A New Beginning?\n\n(see also Kubernetes)\n\ncontainers and containerization, Introduction, A Brief History of Systems\n\nDevelopment, Conclusion: A New Beginning?\n\nadding HTTPS to legacy web service, An Example Sidecar: Adding\n\nHTTPS to a Legacy Service\n\ndefining container’s API, Define Each Container’s API\n\ndocumenting containers, Documenting Your Containers-Documenting\n\nYour Containers",
      "content_length": 1174,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 361,
      "content": "and health checks, Health Checks\n\nimpact on distributed systems, Why I Wrote This Book\n\nleader election, Determining If You Even Need Leader Election-\n\nDetermining If You Even Need Leader Election\n\nmodularity, designing for, Modular Application Containers-Hands\n\nOn: Deploying the topz Container, Ambassadors, Adapters\n\nonline resources, Online Resources\n\nteam ownership goals, Single-Node Patterns\n\nin work queues, A Generic Work Queue System, The Worker\n\nContainer Interface-The Worker Container Interface\n\ncoordinated batch processing, Coordinated Batch Processing-Summary\n\nimage tagging and processing pipeline, Hands On: An Image Tagging\n\nand Processing Pipeline-Hands On: An Image Tagging and Processing\n\nPipeline\n\njoin pattern (barrier synchronization, Join (or Barrier\n\nSynchronization)\n\nreduce pattern, Reduce-Reduce\n\ncopier pattern, work queue, Copier\n\nCoreOS, Hands On: Deploying etcd\n\ncorrelation ID, request tracing, Tracing\n\ncounting, coordinated batch processing, Hands On: Count-Hands On:\n\nCount\n\ncounts, monitoring data, Metrics",
      "content_length": 1045,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 362,
      "content": "D\n\ndata consistency, Data Consistency-Data Consistency\n\ndata replication, Data Consistency\n\ndata synchronization, Relational Integrity\n\ndebugging, and microservices-based systems, Serving Patterns\n\ndecorator pattern, FaaS, The Decorator Pattern: Request or Response\n\nTransformation-Hands On: Adding Request Defaulting Prior to Request\n\nProcessing\n\ndecoupling\n\nevent-based pipelines, Event-Based Pipelines-Hands On:\n\nImplementing a Pipeline for New User Signup\n\nand FaaS, The Benefits of FaaS-The Challenges of FaaS, Handling\n\nEvents\n\nof microservices, Serving Patterns\n\ndeep monitoring, Hands On: Adding Rich Health Monitoring for\n\nMySQL-Hands On: Adding Rich Health Monitoring for MySQL\n\ndelivery semantics, messages, Delivery Semantics\n\ndenial-of-service attacks, Hands On: Deploying nginx and SSL\n\nTermination\n\ndependencies and dependency management, Shared Components for\n\nEasy Reuse\n\nDesign Patterns: Elements of Reusable Object-Oriented Programming\n\n(Gamma et al.), Patterns for Object-Oriented Programming",
      "content_length": 1012,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 363,
      "content": "determinism characteristic, sharding function, An Examination of\n\nSharding Functions\n\ndevelopment with AI models, Development with Models\n\ndictionary-server service, Hands On: Creating a Replicated Service in\n\nKubernetes-Hands On: Creating a Replicated Service in Kubernetes,\n\nHands On: Deploying the Caching Layer-Hands On: Deploying the\n\nCaching Layer\n\ndistributed consensus algorithm, The Value of Patterns, Practices, and\n\nComponents-Shared Components for Easy Reuse, The Basics of Leader\n\nElection-Hands On: Deploying etcd\n\ndistributed lock server (etcd) implementations, The Basics of Leader\n\nElection, Hands On: Deploying etcd, Hands On: Implementing Locks in\n\netcd-Hands On: Implementing Locks in etcd, Hands On: Implementing\n\nLeases in etcd\n\ndistributed ownership, Ownership Election\n\n(see also ownership election)\n\ndistributed systems, Introduction-Summary\n\nAPIs, APIs and RPCs-APIs and RPCs\n\nchallenges in meeting demands of, Conclusion: A New Beginning?-\n\nConclusion: A New Beginning?\n\ncontainer orchestration, Introduction, A Brief History of Systems\n\nDevelopment, Orchestration and Kubernetes, Conclusion: A New\n\nBeginning?\n\ncurrent state of, The World of Distributed Systems Today",
      "content_length": 1195,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 364,
      "content": "data consistency, Data Consistency-Data Consistency\n\ndelivery semantics, Delivery Semantics\n\nhealth checks, Health Checks\n\nhistorical development, A Brief History of Systems Development\n\nidempotency, Idempotency, Errors, Priority, and Retry\n\nlatency, Latency\n\npattern history, A Brief History of Patterns in Software Development-\n\nThe Rise of Open Source Software\n\npercentiles, Percentiles\n\nrelational integrity, Relational Integrity\n\nreliability, A Brief History of Systems Development, Reliability,\n\nRelational Integrity\n\nvalue of patterns, practices, and components, The Value of Patterns,\n\nPractices, and Components-Shared Components for Easy Reuse\n\nDockerfiles, Documenting Your Containers, Hands On: Implementing a\n\nVideo Thumbnailer\n\ndocument search, scatter/gather pattern, Hands On: Distributed\n\nDocument Search-Hands On: Sharded Document Search\n\ndocumenting container, sidecar pattern, Documenting Your Containers-\n\nDocumenting Your Containers\n\ndownsampling, metrics, Aggregating Information\n\ndynamic configuration, sidecar pattern, Dynamic Configuration with\n\nSidecars-Dynamic Configuration with Sidecars",
      "content_length": 1115,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 365,
      "content": "E\n\nembarrassingly parallel problem, scatter/gather pattern, Scatter/Gather\n\nwith Root Distribution\n\nencapsulation and abstraction, microservices, Serving Patterns\n\nENV directive, Documenting Your Containers\n\nenvironment variables, parameterized container, Documenting Your\n\nContainers\n\nerror handling, work queue system, Errors, Priority, and Retry\n\netcd (distributed lock server) implementations, The Basics of Leader\n\nElection, Hands On: Deploying etcd, Hands On: Implementing Locks in\n\netcd-Hands On: Implementing Locks in etcd, Hands On: Implementing\n\nLeases in etcd\n\nevent-based pipelines, Event-Based Pipelines-Hands On: Implementing\n\na Pipeline for New User Signup\n\nevent-driven batch processing (see workflow systems)\n\nevent-driven long-running processing (see function-as-a-service)\n\nevents versus requests, Handling Events\n\neventual data consistency, Data Consistency-Data Consistency\n\n“expected” errors problem, “Client” and “Expected” Errors\n\nexperimentation, ambassador pattern, Using an Ambassador to Do\n\nExperimentation or Request Splitting-Hands On: Implementing 10%\n\nExperiments\n\nexponential backoff, Errors, Priority, and Retry, The Thundering Herd\n\nEXPOSE directive, Documenting Your Containers",
      "content_length": 1213,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 366,
      "content": "F\n\nfailure patterns, Common Failure Patterns-Summary\n\nabsence of errors problem, The Absence of Errors Is an Error\n\nbreadth of inputs challenges, Challenges with the Breadth of Inputs-\n\nChallenges with the Breadth of Inputs\n\nclient errors problem, “Client” and “Expected” Errors\n\n“expected” errors problem, “Client” and “Expected” Errors\n\nobsolete work, processing, Processing Obsolete Work-Processing\n\nObsolete Work\n\noptional components myth, The Myth of Optional Components\n\nrunaway deletion of system error, Oops, We “Cleaned Up”\n\nEverything-Oops, We “Cleaned Up” Everything\n\n“second system” problem, The “Second System” Problem-The\n\n“Second System” Problem\n\nthundering herd, The Thundering Herd-The Thundering Herd\n\nversioning errors, Versioning Errors\n\nffmpeg utility, Hands On: Implementing a Video Thumbnailer\n\nfile-based API, worker container, The Worker Container Interface\n\nfilter pattern, work queue, Filter\n\nfine-tuning process, AI models, The Basics of AI Systems\n\nfluentd logging agent, Hands On: Normalizing Different Logging\n\nFormats with fluentd-Hands On: Normalizing Different Logging\n\nFormats with fluentd\n\nframeworks, AI model, Distributing a Model",
      "content_length": 1168,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 367,
      "content": "G\n\nfunction-as-a-service (FaaS), Functions and Event-Driven Processing-\n\nSummary\n\nbackground processing, need for, The Need for Background\n\nProcessing\n\nbenefits of, The Benefits of FaaS\n\nchallenges of, The Challenges of FaaS\n\ncosts of sustained request-based processing, The Costs of Sustained\n\nRequest-Based Processing\n\ndecorator pattern, The Decorator Pattern: Request or Response\n\nTransformation-Hands On: Adding Request Defaulting Prior to\n\nRequest Processing\n\nevent-based pipelines, Event-Based Pipelines-Hands On:\n\nImplementing a Pipeline for New User Signup\n\nhandling events, Handling Events-Hands On: Implementing Two-\n\nFactor Authentication\n\nholding data in memory, need for, The Need to Hold Data in Memory\n\npatterns for, Patterns for FaaS-Hands On: Implementing a Pipeline for\n\nNew User Signup\n\ntwo-factor authentication, Hands On: Implementing Two-Factor\n\nAuthentication-Hands On: Implementing Two-Factor Authentication\n\n“fuzzed” (randomized) data, testing for breadth of inputs problem,\n\nChallenges with the Breadth of Inputs\n\nGamma, Erich et al., Patterns for Object-Oriented Programming",
      "content_length": 1101,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 368,
      "content": "H\n\ngarbage collection, and runaway deletion problem, Oops, We “Cleaned\n\nUp” Everything-Oops, We “Cleaned Up” Everything\n\nhashing function, Session Tracked Services, An Examination of Sharding\n\nFunctions-Hands On: Building a Consistent HTTP Sharding Proxy\n\nhealth monitoring, Health Checks, Adding a Health Monitor-Hands On:\n\nAdding Rich Health Monitoring for MySQL\n\nHelm package manager, Hands On: Deploying etcd, Hands On:\n\nDeploying Kafka-Hands On: Deploying Kafka\n\nhistograms, Histogram, Metrics, Basic Request Monitoring\n\nhit rate, cache, Deploying Your Cache, The Role of the Cache in System\n\nPerformance\n\nhorizontally scalable systems, Stateless Services\n\nhosting an AI model, Hosting a Model\n\nhot sharding systems, Hot Sharding Systems\n\nHugging Face, Distributing a Model\n\nHypertext Transfer Protocol (HTTP)\n\nadding HTTPS to legacy web service with sidecar, An Example\n\nSidecar: Adding HTTPS to a Legacy Service\n\nrequests\n\ndecorator pattern, The Decorator Pattern: Request or Response\n\nTransformation\n\nand reliability metric, Reliability",
      "content_length": 1044,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 369,
      "content": "I\n\nsharding proxy, Hands On: Building a Consistent HTTP Sharding\n\nProxy\n\nresponse codes, Basic Request Monitoring\n\nRESTful API, The Decorator Pattern: Request or Response\n\nTransformation-Hands On: Adding Request Defaulting Prior to\n\nRequest Processing, Work Queue API, The Worker Container\n\nInterface\n\ntopz interface, Modular Application Containers\n\nidempotency, Idempotency, Errors, Priority, and Retry\n\nimage tagging and processing pipeline, Hands On: An Image Tagging\n\nand Processing Pipeline-Hands On: An Image Tagging and Processing\n\nPipeline\n\nindex\n\nlog, Aggregating Information\n\nscatter/gather pattern, Hands On: Distributed Document Search\n\ninference, AI models, The Basics of AI Systems, Hosting a Model,\n\nDevelopment with Models\n\nintegrity, relational, Relational Integrity\n\ninteractive inference, AI models, Hosting a Model\n\ninterarrival time, work queues, Dynamic Scaling of the Workers\n\nIP-based session tracking, Session Tracked Services\n\nisolation boundary, sharding, Sharded Services",
      "content_length": 999,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 370,
      "content": "J\n\nK\n\njitter, using to spread client load, The Thundering Herd\n\nJob objects, The Shared Work Queue Infrastructure, Dynamic Scaling of\n\nthe Workers\n\njoin pattern, Join (or Barrier Synchronization)\n\nKafka deployment, workflow systems, Hands On: Deploying Kafka-\n\nHands On: Deploying Kafka\n\nKEDA (Kubernetes Event-Driven Autoscaling), Dynamic Scaling of the\n\nWorkers\n\nkey, sharding function, Selecting a Key-Selecting a Key\n\nkey-value stores, The Basics of Leader Election, Implementing Locks-\n\nImplementing Locks, Implementing Ownership-Hands On:\n\nImplementing Leases in etcd\n\nKnuth, Donald, Formalization of Algorithmic Programming\n\nKubeless FaaS framework, The Decorator Pattern: Request or Response\n\nTransformation\n\nKubernetes, Orchestration and Kubernetes\n\nConfigMap, Dynamic Configuration with Sidecars, The Worker\n\nContainer Interface\n\ncreating replicated service, Hands On: Creating a Replicated Service\n\nin Kubernetes-Hands On: Creating a Replicated Service in",
      "content_length": 966,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 371,
      "content": "L\n\nKubernetes\n\netcd deployment with, Hands On: Deploying etcd\n\ngrouping logs across containers, Aggregating Information\n\nKafka deployment as container, Hands On: Deploying Kafka-Hands\n\nOn: Deploying Kafka\n\nKEDA project, Dynamic Scaling of the Workers\n\nKubeless FaaS framework, The Decorator Pattern: Request or\n\nResponse Transformation\n\nmemcache deployment, Hands On: Deploying an Ambassador and\n\nMemcache for a Sharded Cache-Hands On: Deploying an\n\nAmbassador and Memcache for a Sharded Cache\n\npod (container group), Single-Node Patterns, The Sidecar Pattern\n\nRedis service sharded deployment, Hands On: Implementing a\n\nSharded Redis-Hands On: Implementing a Sharded Redis\n\nrunning FaaS on, The Costs of Sustained Request-Based Processing\n\nshared work queue infrastructure, The Shared Work Queue\n\nInfrastructure\n\nshell scripts as health checks with adapter, Adding a Health Monitor\n\nKubernetes Event-Driven Autoscaling (KEDA), Dynamic Scaling of the\n\nWorkers\n\nLABEL directive, Documenting Your Containers\n\nlarge language models (LLMs), The Basics of AI Systems, Hosting a\n\nModel",
      "content_length": 1079,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 372,
      "content": "latency, Latency\n\nAI models, Hosting a Model\n\nbacklog, processing obsolete work, Processing Obsolete Work\n\ncaching, The Role of the Cache in System Performance\n\njoin pattern for workflow systems, Join (or Barrier Synchronization)\n\nmonitoring server requests, Basic Request Monitoring\n\nand percentiles, Percentiles\n\ntracking in work queues, Dynamic Scaling of the Workers\n\nleader election\n\nconcurrent data manipulation, Handling Concurrent Data\n\nManipulation-Handling Concurrent Data Manipulation\n\nconsensus algorithm tools, The Basics of Leader Election-Hands On:\n\nDeploying etcd\n\ndetermining need for, Determining If You Even Need Leader\n\nElection-Determining If You Even Need Leader Election\n\netcd implementations, The Basics of Leader Election, Hands On:\n\nDeploying etcd, Hands On: Implementing Locks in etcd-Hands On:\n\nImplementing Locks in etcd\n\nlocks, Implementing Locks-Hands On: Implementing Locks in etcd\n\nleaf sharding, scatter/gather pattern, Scatter/Gather with Leaf Sharding-\n\nChoosing the Right Number of Leaves\n\nleases, implementing in etcd, Hands On: Implementing Leases in etcd\n\nlegacy web service, adding HTTPS to, An Example Sidecar: Adding\n\nHTTPS to a Legacy Service",
      "content_length": 1186,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 373,
      "content": "M\n\nleveled logging, Logging\n\nlibrary containers, work queues, A Generic Work Queue System\n\nliveness check, Health Checks\n\nLLMs (large language models), The Basics of AI Systems, Hosting a\n\nModel\n\nload-balanced services (see replicated load-balanced services)\n\nload-balancing node, sharded services, Sharded Services\n\nlocks, distributed, Ownership Election\n\n(see also mutual exclusion locks)\n\nlogging\n\nadapter pattern, Logging-Hands On: Normalizing Different Logging\n\nFormats with fluentd\n\naggregating across processes, Aggregating Information-Aggregating\n\nInformation\n\nmonitoring and observability patterns, Monitoring and Observability\n\nBasics, Logging-Logging\n\nlogging libraries, Logging-Logging\n\nlong-running server applications versus batch processing, Batch\n\nComputational Patterns\n\nMapReduce pattern, Batch Computational Patterns, Coordinated Batch\n\nProcessing, Reduce\n\nmemcache deployment, ambassador pattern, Hands On: Deploying an\n\nAmbassador and Memcache for a Sharded Cache-Hands On: Deploying",
      "content_length": 1004,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 374,
      "content": "an Ambassador and Memcache for a Sharded Cache\n\nmemory\n\ncaching issues with usage, The Myth of Optional Components\n\nneed to holding data in memory (FaaS), The Need to Hold Data in\n\nMemory\n\nmerger pattern, Merger, Join (or Barrier Synchronization)\n\nmessage delivery semantics, Delivery Semantics\n\nmetrics\n\nmonitoring and observability, Monitoring and Observability Basics,\n\nMetrics-Metrics, Alerting on anomolies, Aggregating Information\n\nPrometheus monitoring, Hands On: Using Prometheus for\n\nMonitoring, Basic Request Monitoring\n\nreliability as key in distributed systems, Reliability\n\nmicrocontainers, Define Each Container’s API\n\nmicroservices, Serving Patterns-Serving Patterns\n\ndecoupling of, Serving Patterns\n\nversus event-based pipelines, Event-Based Pipelines\n\nexperimentation as separate service, Hands On: Implementing 10%\n\nExperiments\n\nfocus on improving to catch up with needed changes, The “Second\n\nSystem” Problem\n\ntracing requests through multiple, Tracing\n\nmiss rate, sharding function, Consistent Hashing Functions\n\nmodel, AI, The Basics of AI Systems",
      "content_length": 1068,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 375,
      "content": "modularity, designing patterns with\n\nadapters, Adapters\n\nambassadors, Ambassadors\n\nsidecars, Modular Application Containers-Documenting Your\n\nContainers\n\nmodulo operator (%), hash function, An Examination of Sharding\n\nFunctions\n\nmonitoring and observability patterns, Monitoring and Observability\n\nPatterns-Summary\n\nadapter pattern, Monitoring-Hands On: Using Prometheus for\n\nMonitoring, Adding a Health Monitor-Hands On: Adding Rich Health\n\nMonitoring for MySQL\n\naggregating information, Aggregating Information-Aggregating\n\nInformation\n\nalerting, Monitoring and Observability Basics, Alerting-Alerting on\n\nanomolies\n\nhealth of application containers, Adding a Health Monitor-Hands On:\n\nAdding Rich Health Monitoring for MySQL\n\nlogging, Monitoring and Observability Basics, Logging-Logging\n\nmetrics, Monitoring and Observability Basics, Metrics-Metrics,\n\nAggregating Information\n\nrequest monitoring, Basic Request Monitoring-Advanced Request\n\nMonitoring\n\ntracing, Tracing",
      "content_length": 972,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 376,
      "content": "N\n\nmonitoring container, adapter pattern, Monitoring\n\nmonolithic systems versus microservices, Serving Patterns\n\nmultinode serving patterns, Serving Patterns\n\n(see also serving patterns)\n\nmultiworker pattern, The Multiworker Pattern\n\nmutual exclusion locks (mutexes), Ownership Election\n\nconcurrent data manipulation handling, Handling Concurrent Data\n\nManipulation-Handling Concurrent Data Manipulation\n\nimplementing, The Basics of Leader Election, Hands On: Deploying\n\netcd, Hands On: Implementing Leases in etcd\n\nrenewable locks, Implementing Ownership-Hands On: Implementing\n\nLeases in etcd\n\nMySQL database\n\nrich health monitoring, Hands On: Adding Rich Health Monitoring\n\nfor MySQL-Hands On: Adding Rich Health Monitoring for MySQL\n\nservice brokering with ambassador pattern, Using an Ambassador for\n\nService Brokering\n\nnetwork address translation (NAT), Session Tracked Services\n\nNetwork File System (NFS) share, Hands On: Implementing a Video\n\nThumbnailer\n\nneural network models, The Basics of AI Systems\n\nnew-user signup, Hands On: Implementing a Pipeline for New User\n\nSignup-Hands On: Implementing a Pipeline for New User Signup,",
      "content_length": 1139,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 377,
      "content": "O\n\nHands On: Building an Event-Driven Flow for New User Signup-Hands\n\nOn: Building an Event-Driven Flow for New User Signup\n\nnginx application, SSL termination with, SSL Termination, Hands On:\n\nDeploying nginx and SSL Termination-Hands On: Deploying nginx and\n\nSSL Termination\n\nnginx sidecar container, An Example Sidecar: Adding HTTPS to a\n\nLegacy Service\n\nnginx web server, Hands On: Implementing 10% Experiments\n\nNoSQL versus SQL data stores, Relational Integrity\n\nobject-oriented programming, patterns for, Patterns for Object-Oriented\n\nProgramming\n\nobsolete work, processing, Processing Obsolete Work-Processing\n\nObsolete Work\n\nOpen Containers Initiative (OCI), Documenting Your Containers\n\nOpen Neural Network eXchange (ONNX), Distributing a Model\n\nopen source software, The Rise of Open Source Software, Distributing a\n\nModel\n\nOpenTelemetry, Tracing\n\noptional components myth, The Myth of Optional Components\n\noverloading of requests (thundering herd), The Thundering Herd-The\n\nThundering Herd\n\nownership election, Ownership Election-Summary",
      "content_length": 1048,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 378,
      "content": "P\n\nconcurrent data manipulation, Handling Concurrent Data\n\nManipulation-Handling Concurrent Data Manipulation\n\netcd implementations, Hands On: Implementing Leases in etcd\n\nleader election (see leader election)\n\nrenewable locks, Implementing Ownership-Hands On: Implementing\n\nLeases in etcd\n\nPaaS (platform-as-a-service), sidecar pattern, Building a Simple PaaS\n\nwith Sidecars-Building a Simple PaaS with Sidecars\n\nparallel processing\n\nlogging duplications, Logging\n\nscatter/gather pattern, Scatter/Gather, Choosing the Right Number of\n\nLeaves\n\nwork queue dynamic scaling, Dynamic Scaling of the Workers\n\nparameterized sidecar containers, Parameterized Containers\n\npartition tolerance, relationship to consistency and availability, Data\n\nConsistency\n\npatterns, The Rise of Open Source Software\n\nbatch computational patterns (see batch computational patterns)\n\nhistory in software development, A Brief History of Patterns in\n\nSoftware Development-The Rise of Open Source Software\n\nand microservices, Serving Patterns\n\nmonitoring and observability, Monitoring and Observability Patterns-\n\nSummary",
      "content_length": 1093,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 379,
      "content": "serving patterns (see serving patterns)\n\nshared components for reuse, Shared Components for Easy Reuse\n\nas shared language, A Shared Language for Discussing Our Practice\n\nsingle-node patterns (see single-node patterns)\n\nvalue of, The Value of Patterns, Practices, and Components-Shared\n\nComponents for Easy Reuse\n\npercentiles, Percentiles\n\npipelines\n\nevent-based, Event-Based Pipelines-Hands On: Implementing a\n\nPipeline for New User Signup\n\nimage tagging and processing, Hands On: An Image Tagging and\n\nProcessing Pipeline-Hands On: An Image Tagging and Processing\n\nPipeline\n\nplatform-as-a-service (PaaS), sidecar pattern, Building a Simple PaaS\n\nwith Sidecars-Building a Simple PaaS with Sidecars\n\npod (container group), Single-Node Patterns, The Sidecar Pattern\n\npoison requests, isolation boundary to protect from, Sharded Services\n\npoison work solution, Errors, Priority, and Retry\n\npretrained models, AI, Hosting a Model\n\nprioritizing newer requests over older, Processing Obsolete Work\n\nprocess-ID (PID) namespace, sidecar container, Modular Application\n\nContainers\n\nPrometheus monitoring aggregator, Hands On: Using Prometheus for\n\nMonitoring, Metrics, Metrics, Basic Request Monitoring",
      "content_length": 1194,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 380,
      "content": "R\n\nprompts, AI, The Basics of AI Systems\n\npublisher/subscriber API, Publisher/Subscriber Infrastructure\n\npublisher/subscriber infrastructure, Publisher/Subscriber Infrastructure-\n\nHands On: Deploying Kafka\n\nPython, as language for AI models, Distributing a Model\n\nRAG (Retrieval-Augmented Generation), Retrieval-Augmented\n\nGeneration\n\nrate limiting, Rate Limiting and Denial-of-Service Defense\n\nrate of requests, monitoring, Basic Request Monitoring\n\nreadiness probes for load balancing, Readiness Probes for Load\n\nBalancing\n\nRedis, Hands On: Implementing a Sharded Redis, Hands On:\n\nImplementing a Sharded Redis-Hands On: Implementing a Sharded\n\nRedis, Hands On: Using Prometheus for Monitoring-Hands On: Using\n\nPrometheus for Monitoring, Hands On: Normalizing Different Logging\n\nFormats with fluentd\n\nreduce patterns, Batch Computational Patterns, Coordinated Batch\n\nProcessing, Reduce-Reduce, Sum\n\nrelational integrity, Relational Integrity\n\nreliability\n\nof distributed systems, A Brief History of Systems Development,\n\nReliability, Relational Integrity\n\nmicroservices, Serving Patterns",
      "content_length": 1089,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 381,
      "content": "monitoring and observability as important for, Monitoring and\n\nObservability Patterns\n\nscaling of scatter/gather pattern for, Scaling Scatter/Gather for\n\nReliability and Scale\n\nremote procedure calls (RPCs), APIs and RPCs\n\nrenewable locks, ownership election, Implementing Ownership-Hands\n\nOn: Implementing Leases in etcd\n\nreplicated load-balanced services, Replicated Load-Balanced Services-\n\nHands On: Deploying nginx and SSL Termination\n\napplication protocol, Application-Layer Replicated Services\n\napplication-layer replicated services, Application-Layer Replicated\n\nServices\n\ncaching layer deployment and expansion, Introducing a Caching\n\nLayer-Hands On: Deploying nginx and SSL Termination\n\ncreating, Hands On: Creating a Replicated Service in Kubernetes-\n\nHands On: Creating a Replicated Service in Kubernetes\n\nsession tracked services, Session Tracked Services-Session Tracked\n\nServices\n\nversus sharded services, Sharded Services\n\nstateless services, Stateless Services-Hands On: Creating a Replicated\n\nService in Kubernetes\n\nreplicated sharded cache, Replicated Sharded Caches\n\nrequest-based processing, FaaS, The Costs of Sustained Request-Based\n\nProcessing, Hands On: Adding Request Defaulting Prior to Request",
      "content_length": 1221,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 382,
      "content": "Processing\n\nrequests\n\ndecorator pattern, The Decorator Pattern: Request or Response\n\nTransformation\n\nversus events, Handling Events\n\nmonitoring, Basic Request Monitoring-Tracing\n\nand reliability metric, Reliability\n\nsharding proxy, Hands On: Building a Consistent HTTP Sharding\n\nProxy\n\nsplitting, Using an Ambassador to Do Experimentation or Request\n\nSplitting-Hands On: Implementing 10% Experiments\n\nresource isolation, Single-Node Patterns\n\nresource version, distributed locks, Implementing Locks\n\nRESTful web API, AI models, Development with Models\n\nRetrieval-Augmented Generation (RAG), Retrieval-Augmented\n\nGeneration\n\nretry queue to handle backlogs, Errors, Priority, and Retry\n\nreusability, Introduction, Shared Components for Easy Reuse, Single-\n\nNode Patterns\n\n(see also single-node patterns)\n\nmultiworker pattern, The Multiworker Pattern\n\nsidecar pattern, Modular Application Containers-Hands On:\n\nDeploying the topz Container, Designing Sidecars for Modularity and\n\nReusability-Documenting Your Containers",
      "content_length": 1016,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 383,
      "content": "S\n\nroot (load-balancing node), sharded services, Sharded Services\n\nroot distribution, scatter/gather pattern, Scatter/Gather with Root\n\nDistribution-Hands On: Distributed Document Search\n\nRPCs (remote procedure calls), APIs and RPCs\n\nrunaway deletion of system error, Oops, We “Cleaned Up” Everything-\n\nOops, We “Cleaned Up” Everything\n\nscaling and scalability\n\nautoscaling for obsolete work processing solution, Processing\n\nObsolete Work\n\nand consistent hashing function, Session Tracked Services\n\nin decoupling of microservices, Serving Patterns\n\ndynamic scaling of workers, Dynamic Scaling of the Workers-\n\nDynamic Scaling of the Workers\n\nwith FaaS, The Costs of Sustained Request-Based Processing\n\nhorizontally scalable systems, Stateless Services\n\nscatter/gather pattern, Scatter/Gather-Summary\n\nsharded services, Sharded Services-Summary\n\nstateless replicated services, Stateless Services-Hands On: Creating a\n\nReplicated Service in Kubernetes\n\nteam scaling, Single-Node Patterns\n\nscatter/gather pattern, Scatter/Gather-Summary\n\ndistributed document search, Hands On: Distributed Document\n\nSearch-Hands On: Distributed Document Search",
      "content_length": 1140,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 384,
      "content": "leaf sharding, Scatter/Gather with Leaf Sharding-Choosing the Right\n\nNumber of Leaves\n\nwith root distribution, Scatter/Gather with Root Distribution-Hands\n\nOn: Distributed Document Search\n\nscaling for reliability, Scaling Scatter/Gather for Reliability and Scale\n\n“second system” problem, The “Second System” Problem-The “Second\n\nSystem” Problem\n\nseparation of concerns, Single-Node Patterns, Ambassadors, Using an\n\nAmbassador to Shard a Service\n\nserverless computing, versus FaaS, Functions and Event-Driven\n\nProcessing\n\nservice broker, Using an Ambassador for Service Brokering\n\nservice brokering, ambassador pattern, Using an Ambassador for Service\n\nBrokering\n\nservice discovery, Using an Ambassador for Service Brokering\n\nservice level agreement (SLA), Stateless Services\n\nservice level objectives (SLOs), API, APIs and RPCs, Basic alerting\n\nserving patterns, Serving Patterns-Serving Patterns\n\nFaaS, Functions and Event-Driven Processing-Summary\n\nownership election, Ownership Election-Summary\n\nreplicated load-balanced services, Replicated Load-Balanced\n\nServices-Hands On: Deploying nginx and SSL Termination\n\nscatter/gather pattern, Scatter/Gather-Summary\n\nsharded services, Sharded Services-Summary",
      "content_length": 1207,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 385,
      "content": "session tracked services, Session Tracked Services-Session Tracked\n\nServices, Deploying Your Cache\n\nsessions, and events versus requests, Handling Events\n\nshard key, Selecting a Key-Selecting a Key\n\nshard pattern, workflow systems, Hands On: Building an Event-Driven\n\nFlow for New User Signup\n\nshard router service, Hands On: Deploying an Ambassador and\n\nMemcache for a Sharded Cache-Hands On: Deploying an Ambassador\n\nand Memcache for a Sharded Cache\n\nsharded cache, Sharded Caching-Hands On: Deploying an Ambassador\n\nand Memcache for a Sharded Cache, Hot Sharding Systems\n\nsharded services, Sharded Services-Summary\n\nambassador pattern, Using an Ambassador to Shard a Service-Hands\n\nOn: Implementing a Sharded Redis\n\ncaching layer, Sharded Caching-Hands On: Deploying an\n\nAmbassador and Memcache for a Sharded Cache, Hot Sharding\n\nSystems\n\ndocument search, Hands On: Sharded Document Search\n\nhashing functions, An Examination of Sharding Functions-Hands On:\n\nBuilding a Consistent HTTP Sharding Proxy\n\nleaf sharding, scatter/gather pattern, Scatter/Gather with Leaf\n\nSharding-Choosing the Right Number of Leaves\n\nversus replicated services, Sharded Services\n\nreplicated serving, Sharded Replicated Serving",
      "content_length": 1207,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 386,
      "content": "sharder pattern, workflow systems, Sharder-Sharder\n\nsharding ambassador proxy, Using an Ambassador to Shard a Service\n\nsharding functions, Sharder-Sharder\n\nsharding proxy, HTTP requests, Hands On: Building a Consistent HTTP\n\nSharding Proxy\n\nshared infrastructure, work queues, The Shared Work Queue\n\nInfrastructure\n\nsidecar pattern, A Shared Language for Discussing Our Practice, The\n\nSidecar Pattern-Summary\n\nadding HTTPS to a legacy service, An Example Sidecar: Adding\n\nHTTPS to a Legacy Service\n\ncache deployment with, Deploying Your Cache\n\ndesigning for modularity and reusability, Modular Application\n\nContainers-Documenting Your Containers\n\ndocumenting container, Documenting Your Containers-Documenting\n\nYour Containers\n\ndynamic configuration with, Dynamic Configuration with Sidecars-\n\nDynamic Configuration with Sidecars\n\nPaaS, building with sidecar, Building a Simple PaaS with Sidecars-\n\nBuilding a Simple PaaS with Sidecars\n\nsingle-node patterns, Single-Node Patterns-Single-Node Patterns\n\nadapters, Adapters-Summary\n\nambassadors, Ambassadors-Summary\n\nsidecars, The Sidecar Pattern-Summary",
      "content_length": 1101,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 387,
      "content": "singleton pattern, ownership election, Determining If You Even Need\n\nLeader Election-Determining If You Even Need Leader Election\n\nSLA (service level agreement), Stateless Services\n\nSLOs (service level objectives), API, APIs and RPCs, Basic alerting\n\nslow-query logging, adding to Redis, Hands On: Normalizing Different\n\nLogging Formats with fluentd\n\nsmall language models (SLMs), Hosting a Model\n\nsource container interface, work queues, The Source Container Interface\n\nsplitter pattern, work queue, Splitter\n\nSQL injection attacks, Challenges with the Breadth of Inputs\n\nSQL versus NoSQL data stores, Relational Integrity\n\nSSL termination, caching layer for, SSL Termination-Hands On:\n\nDeploying nginx and SSL Termination\n\nstateful services (see sharded services)\n\nstateless services, Using an Ambassador to Shard a Service, Stateless\n\nServices-Hands On: Creating a Replicated Service in Kubernetes,\n\nSharded Services\n\n(see also replicated load-balanced services)\n\nstatic alerting, Alerting on anomolies\n\nstdout, adapter container’s redirect of logs to, Logging\n\n“straggler” problem, scatter/gather pattern, Choosing the Right Number\n\nof Leaves\n\nstrong data consistency, Data Consistency-Data Consistency\n\nsums, coordinated batch processing, Sum",
      "content_length": 1247,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 388,
      "content": "T\n\nsynchronous versus asynchronous APIs, APIs and RPCs\n\nsynthetic request to test system, “Client” and “Expected” Errors\n\nsystem-generated request to test system, “Client” and “Expected” Errors\n\nsystems development, history of, A Brief History of Systems\n\nDevelopment\n\nteams\n\nownership and containerization goals, Single-Node Patterns\n\nreliable contract for different services in microservices, Serving\n\nPatterns\n\nscaling of, Single-Node Patterns\n\ntee-d service, Using an Ambassador to Do Experimentation or Request\n\nSplitting\n\nTensorFlow, Distributing a Model\n\ntesting and deployment of AI services, Testing and Deployment-Testing\n\nand Deployment\n\ntesting for breadth of inputs problem, Challenges with the Breadth of\n\nInputs-Challenges with the Breadth of Inputs\n\nthree-nines service, Stateless Services\n\nthrottle module (Varnish), Rate Limiting and Denial-of-Service Defense\n\nthrottling, to prevent runaway deletion, Oops, We “Cleaned Up”\n\nEverything\n\nthundering herd, failure pattern, The Thundering Herd-The Thundering\n\nHerd",
      "content_length": 1029,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 389,
      "content": "U\n\ntime series data monitoring, Metrics\n\ntime-to-live (TTL) functionality of key-value store, Implementing\n\nLocks-Implementing Locks, Implementing Ownership-Hands On:\n\nImplementing Leases in etcd\n\ntiming out requests, obsolete work processing solution, Processing\n\nObsolete Work\n\ntopz sidecar container, Modular Application Containers-Hands On:\n\nDeploying the topz Container\n\ntracing, monitoring and observability patterns, Monitoring and\n\nObservability Basics, Tracing\n\ntraffic overload, failure pattern, The Thundering Herd-The Thundering\n\nHerd\n\ntraffic reduction problem, The Absence of Errors Is an Error\n\ntraining data, The Basics of AI Systems\n\nTTL (time-to-live) functionality of key-value store, Implementing\n\nLocks-Implementing Locks, Implementing Ownership-Hands On:\n\nImplementing Leases in etcd\n\ntwemproxy, Ambassadors\n\ntwo-factor authentication, Hands On: Implementing Two-Factor\n\nAuthentication-Hands On: Implementing Two-Factor Authentication\n\nuniformity characteristic, sharding function, An Examination of\n\nSharding Functions\n\nUnix signals",
      "content_length": 1055,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 390,
      "content": "V\n\nW\n\nSIGHUP, Dynamic Configuration with Sidecars\n\nSIGKILL, Dynamic Configuration with Sidecars\n\nuser feedback, AI testing, Testing and Deployment\n\nvalues, monitoring data, Metrics\n\nVarnish (HTTP reverse proxy), Expanding the Caching Layer-SSL\n\nTermination\n\nversioning errors, Versioning Errors\n\nversioning of APIs, Work Queue API\n\nvideo thumbnailer implementation, Hands On: Implementing a Video\n\nThumbnailer-Hands On: Implementing a Video Thumbnailer\n\nwork items, The Source Container Interface, Work Stealing\n\nwork queue systems, Work Queue Systems-Summary\n\nAPI (ambassador), Work Queue API-Work Queue API\n\nbuilding event-driven flow, Hands On: Building an Event-Driven\n\nFlow for New User Signup-Hands On: Building an Event-Driven\n\nFlow for New User Signup\n\ndynamic scaling of workers, Dynamic Scaling of the Workers-\n\nDynamic Scaling of the Workers\n\nmultiworker pattern, The Multiworker Pattern\n\nshared infrastructure, The Shared Work Queue Infrastructure",
      "content_length": 959,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 391,
      "content": "source container interface, The Source Container Interface\n\nvideo thumbnailer implementation, Hands On: Implementing a Video\n\nThumbnailer-Hands On: Implementing a Video Thumbnailer\n\nand work stealing technique, Work Stealing\n\nworker container interface, The Worker Container Interface-The\n\nWorker Container Interface\n\nwork stealing, work queues, Work Stealing\n\nworker container interface, The Worker Container Interface-The Worker\n\nContainer Interface\n\nworkflow systems, Event-Driven Batch Processing-Summary\n\nbuilding event-driven flow, Hands On: Building an Event-Driven\n\nFlow for New User Signup-Hands On: Building an Event-Driven\n\nFlow for New User Signup\n\ncopier pattern, Copier\n\nfilter pattern, Filter\n\nmerger pattern, Merger\n\npublisher/subscriber infrastructure, Publisher/Subscriber\n\nInfrastructure-Hands On: Deploying Kafka\n\nsharder pattern, Sharder-Sharder\n\nsplitter pattern, Splitter\n\nwork queue resiliency and performance, Resiliency and Performance\n\nin Work Queues-Errors, Priority, and Retry",
      "content_length": 1005,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 392,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 393,
      "content": "About the Author\n\nBrendan Burns is corporate vice president at Microsoft responsible for\n\nAzure management and governance, Azure Arc, Kubernetes on Azure,\n\nLinux on Azure, and PowerShell. Prior to Microsoft, he worked at Google\n\nin the Google Cloud Platform, where he cofounded the Kubernetes project\n\nand helped build APIs like Deployment Manager and Cloud DNS. Before\n\nworking on cloud, he worked on Google’s web-search infrastructure with a\n\nfocus on low-latency indexing. He has a PhD in computer science from the\n\nUniversity of Massachusetts Amherst with a specialty in robotics. He lives\n\nin Seattle with his wife, Robin Sanders, their two children, and a cat, Mrs.\n\nPaws, who rules over their household with an iron paw.\n\nOceanofPDF.com",
      "content_length": 743,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 394,
      "content": "Colophon\n\nThe animal on the cover of Designing Distributed Systems is a Java sparrow\n\n(Padda oryzivora). This bird is loathed in the wild but loved in captivity.\n\nFarmers destroy thousands of wild Javas each year to prevent the flocks\n\nfrom devouring their crops. They also trap the birds for food or sell them in\n\nthe international bird trade. Despite this battle, the species continues to\n\nthrive in Java and Bali in Indonesia, as well as Australia, Mexico, and North\n\nAmerica.\n\nIts plumage is pearly-gray, turning pinkish on the front and white toward\n\nthe tail. It has a black head with white cheeks. Its large bill, legs, and eye\n\ncircles are bright pink. The song of the Java sparrow begins with single\n\nnotes, like a bell, before developing into a continuous trilling and clucking,\n\nmixed with high-pitched and deeper notes.\n\nThe main part of their diet is rice, but they also eat small seeds, grasses,\n\ninsects, and flowering plants. In the wild, these birds will build a nest out of\n\ndried grass, normally under the roofs of buildings or in bushes or treetops.\n\nThe Java will lay a clutch of three or four eggs between February and\n\nAugust, with most eggs laid in April or May.\n\nThe Java sparrow is listed as endangered on the International Union for\n\nConservation of Nature (IUCN) Red List of Threatened Species due to the\n\ncontinued loss of its natural habitat, hunting, and trapping. Many of the",
      "content_length": 1407,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 395,
      "content": "animals on O’Reilly covers are endangered; all of them are important to the\n\nworld.\n\nThe cover illustration is by Karen Montgomery, based on a black-and-white\n\nengraving from Lydekker’s Royal Natural History. The series design is by\n\nEdie Freedman, Ellie Volckhausen, and Karen Montgomery. The cover\n\nfonts are Gilroy Semibold and Guardian Sans. The text font is Adobe\n\nMinion Pro; the heading font is Adobe Myriad Condensed; and the code\n\nfont is Dalton Maag’s Ubuntu Mono.\n\nOceanofPDF.com",
      "content_length": 490,
      "extraction_method": "Unstructured"
    }
  ]
}