{
  "metadata": {
    "title": "Feature Engineering for Machine Learning - Alice Zheng",
    "author": "Alice Zheng and Amanda Casari",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 217,
    "conversion_date": "2025-12-19T17:28:41.556729",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "Feature Engineering for Machine Learning - Alice Zheng.pdf",
    "extraction_method": "Unstructured"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Segment 1 (pages 1-10)",
      "start_page": 1,
      "end_page": 10,
      "detection_method": "topic_boundary",
      "content": "Feature Engineering for Machine Learning\n\nPRINCIPLES AND TECHNIQUES FOR DATA SCIENTISTS\n\nAlice Zheng & Amanda Casari\n\nFeature Engineering for Machine Learning Principles and Techniques for Data Scientists\n\nAlice Zheng and Amanda Casari\n\nBeijing Beijing\n\nBoston Boston\n\nFarnham Sebastopol Farnham Sebastopol\n\nTokyo Tokyo\n\nFeature Engineering for Machine Learning by Alice Zheng and Amanda Casari\n\nCopyright © 2018 Alice Zheng, Amanda Casari. All rights reserved.\n\nPrinted in the United States of America.\n\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\n\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are also available for most titles (http://oreilly.com/safari). For more information, contact our corporate/insti‐ tutional sales department: 800-998-9938 or corporate@oreilly.com.\n\nEditors: Rachel Roumeliotis and Jeff Bleiel Production Editor: Kristen Brown Copyeditor: Rachel Head Proofreader: Sonia Saruba\n\nIndexer: Ellen Troutman Interior Designer: David Futato Cover Designer: Karen Montgomery Illustrator: Rebecca Demarest\n\nApril 2018:\n\nFirst Edition\n\nRevision History for the First Edition 2018-03-23: First Release\n\nSee http://oreilly.com/catalog/errata.csp?isbn=9781491953242 for release details.\n\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Feature Engineering for Machine Learning, the cover image, and related trade dress are trademarks of O’Reilly Media, Inc.\n\nWhile the publisher and the authors have used good faith efforts to ensure that the information and instructions contained in this work are accurate, the publisher and the authors disclaim all responsibility for errors or omissions, including without limitation responsibility for damages resulting from the use of or reliance on this work. Use of the information and instructions contained in this work is at your own risk. If any code samples or other technology this work contains or describes is subject to open source licenses or the intellectual property rights of others, it is your responsibility to ensure that your use thereof complies with such licenses and/or rights.\n\n978-1-491-95324-2\n\n[LSI]\n\nTable of Contents\n\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . vii\n\n1. The Machine Learning Pipeline. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 Data 1 Tasks 1 Models 2 Features 3 Model Evaluation 3\n\n2. Fancy Tricks with Simple Numbers. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 Scalars, Vectors, and Spaces 6 Dealing with Counts 8 Binarization 9 Quantization or Binning 10 Log Transformation 15 Log Transform in Action 19 Power Transforms: Generalization of the Log Transform 23 Feature Scaling or Normalization 29 Min-Max Scaling 30 Standardization (Variance Scaling) 31 ℓ2 Normalization 32 Interaction Features 35 Feature Selection 38 Summary 39 Bibliography 39\n\n3. Text Data: Flattening, Filtering, and Chunking. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 Bag-of-X: Turning Natural Text into Flat Vectors 42\n\niii\n\nBag-of-Words 42 Bag-of-n-Grams 45 Filtering for Cleaner Features 47 Stopwords 48 Frequency-Based Filtering 48 Stemming 51 Atoms of Meaning: From Words to n-Grams to Phrases 52 Parsing and Tokenization 52 Collocation Extraction for Phrase Detection 52 Summary 59 Bibliography 60\n\n4. The Effects of Feature Scaling: From Bag-of-Words to Tf-Idf. . . . . . . . . . . . . . . . . . . . . . 61 Tf-Idf : A Simple Twist on Bag-of-Words 61 Putting It to the Test 63 Creating a Classification Dataset 64 Scaling Bag-of-Words with Tf-Idf Transformation 65 Classification with Logistic Regression 66 Tuning Logistic Regression with Regularization 68 Deep Dive: What Is Happening? 72 Summary 75 Bibliography 76\n\n5. Categorical Variables: Counting Eggs in the Age of Robotic Chickens. . . . . . . . . . . . . . . 77 Encoding Categorical Variables 78 One-Hot Encoding 78 Dummy Coding 79 Effect Coding 82 Pros and Cons of Categorical Variable Encodings 83 Dealing with Large Categorical Variables 83 Feature Hashing 84 Bin Counting 87 Summary 94 Bibliography 96\n\n6. Dimensionality Reduction: Squashing the Data Pancake with PCA. . . . . . . . . . . . . . . . . 99 Intuition 99 Derivation 101 Linear Projection 102 Variance and Empirical Variance 103 Principal Components: First Formulation 104 Principal Components: Matrix-Vector Formulation 104\n\niv\n\n|\n\nTable of Contents\n\nGeneral Solution of the Principal Components 105 Transforming Features 105 Implementing PCA 106 PCA in Action 106 Whitening and ZCA 108 Considerations and Limitations of PCA 109 Use Cases 111 Summary 112 Bibliography 113\n\n7. Nonlinear Featurization via K-Means Model Stacking. . . . . . . . . . . . . . . . . . . . . . . . . . . 115 k-Means Clustering 117 Clustering as Surface Tiling 119 k-Means Featurization for Classification 122 Alternative Dense Featurization 127 Pros, Cons, and Gotchas 128 Summary 130 Bibliography 131\n\n8. Automating the Featurizer: Image Feature Extraction and Deep Learning. . . . . . . . . 133 The Simplest Image Features (and Why They Don’t Work) 134 Manual Feature Extraction: SIFT and HOG 135 Image Gradients 135 Gradient Orientation Histograms 139 SIFT Architecture 143 Learning Image Features with Deep Neural Networks 144 Fully Connected Layers 144 Convolutional Layers 146 Rectified Linear Unit (ReLU) Transformation 150 Response Normalization Layers 151 Pooling Layers 153 Structure of AlexNet 153 Summary 157 Bibliography 157\n\n9. Back to the Feature: Building an Academic Paper Recommender. . . . . . . . . . . . . . . . . 159 Item-Based Collaborative Filtering 159 First Pass: Data Import, Cleaning, and Feature Parsing 161 Academic Paper Recommender: Naive Approach 161 Second Pass: More Engineering and a Smarter Model 167 Academic Paper Recommender: Take 2 167 Third Pass: More Features = More Information 173\n\nTable of Contents\n\n|\n\nv\n\nAcademic Paper Recommender: Take 3 174 Summary 176 Bibliography 177\n\nA. Linear Modeling and Linear Algebra Basics. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179\n\nIndex. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193\n\nvi\n\n|\n\nTable of Contents\n\nPreface\n\nIntroduction Machine learning fits mathematical models to data in order to derive insights or make predictions. These models take features as input. A feature is a numeric repre‐ sentation of an aspect of raw data. Features sit between data and models in the machine learning pipeline. Feature engineering is the act of extracting features from raw data and transforming them into formats that are suitable for the machine learn‐ ing model. It is a crucial step in the machine learning pipeline, because the right fea‐ tures can ease the difficulty of modeling, and therefore enable the pipeline to output results of higher quality. Practitioners agree that the vast majority of time in building a machine learning pipeline is spent on feature engineering and data cleaning. Yet, despite its importance, the topic is rarely discussed on its own. Perhaps this is because the right features can only be defined in the context of both the model and the data; since data and models are so diverse, it’s difficult to generalize the practice of feature engineering across projects.\n\nNevertheless, feature engineering is not just an ad hoc practice. There are deeper principles at work, and they are best illustrated in situ. Each chapter of this book addresses one data problem: how to represent text data or image data, how to reduce the dimensionality of autogenerated features, when and how to normalize, etc. Think of this as a collection of interconnected short stories, as opposed to a single long novel. Each chapter provides a vignette into the vast array of existing feature engi‐ neering techniques. Together, they illustrate the overarching principles.\n\nMastering a subject is not just about knowing the definitions and being able to derive the formulas. It is not enough to know how the mechanism works and what it can do —one must also understand why it is designed that way, how it relates to other tech‐ niques, and what the pros and cons of each approach are. Mastery is about knowing precisely how something is done, having an intuition for the underlying principles, and integrating it into one’s existing web of knowledge. One does not become a mas‐ ter of something by simply reading a book, though a good book can open new doors.\n\nPreface\n\n|\n\nvii\n\nIt has to involve practice—putting the ideas to use, which is an iterative process. With every iteration, we know the ideas better and become increasingly more adept and creative at applying them. The goal of this book is to facilitate the application of its ideas.\n\nThis book tries to teach the reason first, and the mathematics second. Instead of only discussing how something is done, we try to teach why. Our goal is to provide the intuition behind the ideas, so that the reader may understand how and when to apply them. There are tons of descriptions and pictures for folks who learn in differ‐ ent ways. Mathematical formulas are presented in order to make the intuition pre‐ cise, and also to bridge this book with other existing offerings.\n\nCode examples in this book are given in Python, using a variety of free and open source packages. The NumPy library provides numeric vector and matrix operations. Pandas provides the DataFrame that is the building block of data science in Python. Scikit-learn is a general-purpose machine learning package with extensive coverage of models and feature transformers. Matplotlib and the styling library Sea‐ born provide plotting and visualization support. You can find these examples as Jupyter notebooks in our GitHub repo.\n\nThe first few chapters start out slow in order to provide a bridge for folks who are just getting started with data science and machine learning. Chapter 1 introduces the fun‐ damental concepts in the machine learning pipeline (data, models, features, etc.). In Chapter 2, we explore basic feature engineering for numeric data: filtering, binning, scaling, log transforms and power transforms, and interaction features. Chapter 3 dives into feature engineering for natural text, exploring techniques like bag-of- words, n-grams, and phrase detection. Chapter 4 examines tf-idf (term frequency– inverse document frequency) as an example of feature scaling and discusses why it works. The pace starts to pick up around Chapter 5, where we talk about efficient encoding techniques for categorical variables, including feature hashing and bin counting. By the time we get to principal component analysis (PCA) in Chapter 6, we are deep in the land of machine learning. Chapter 7 looks at k-means as a featuriza‐ tion technique, which illustrates the useful concept of model stacking. Chapter 8 is all about images, which are much more challenging in terms of feature extraction than text data. We look at two manual feature extraction techniques, SIFT and HOG, before concluding with an explanation of deep learning as the latest feature extrac‐ tion technique for images. We finish up in Chapter 9 by showing a few different tech‐ niques in an end-to-end example, creating a recommender for a dataset of academic papers.\n\nviii\n\n| Preface\n\nIn Living Color\n\nThe illustrations in this book are best viewed in color. Really, you should print out the color versions of the Swiss roll in Chapter 7 and paste them into your book. Your aesthetic sense will thank us.\n\nFeature engineering is a vast topic, and more methods are being invented every day, particularly in the area of automatic feature learning. In order to limit the book to a manageable size, we’ve had to make some cuts. This book does not discuss Fourier analysis for audio data, though it is a beautiful subject that is closely related to eigen analysis in linear algebra (which we touch upon in Chapters 4 and 6). We also skip a discussion of random features, which are intimately related to Fourier analysis. We provide an introduction to feature learning via deep learning for image data, but do not go into depth on the numerous deep learning models under active development. Also out of scope are advanced research ideas like random projections, complex text featurization models such as word2vec and Brown clustering, and latent space mod‐ els like Latent Dirichlet allocation and matrix factorization. If those words mean nothing to you, then you are in luck. If the frontiers of feature learning are where your interest lies, then this is probably not the book for you.\n\nThe book assumes knowledge of basic machine learning concepts, such as what a model is and what a vector is, though a refresher is provided so we’re all on the same page. Experience with linear algebra, probability distributions, and optimization are helpful, but not necessary.\n\nConventions Used in This Book The following typographical conventions are used in this book:\n\nItalic\n\nIndicates new terms, URLs, email addresses, filenames, and file extensions.\n\nConstant width\n\nUsed for program listings, as well as within paragraphs to refer to program ele‐ ments such as variable or function names, databases, data types, environment variables, statements, and keywords.\n\nConstant width bold\n\nShows commands or other text that should be typed literally by the user.\n\nConstant width italic\n\nShows text that should be replaced with user-supplied values or by values deter‐ mined by context.\n\nPreface\n\n|\n\nix",
      "page_number": 1
    },
    {
      "number": 2,
      "title": "Segment 2 (pages 11-19)",
      "start_page": 11,
      "end_page": 19,
      "detection_method": "topic_boundary",
      "content": "The book also contains numerous linear algebra equations. We use the following conventions with regard to notation: scalars are shown in lowercase italic (e.g., a), vectors in lowercase bold (e.g., v), and matrices in uppercase bold and italic (e.g., U).\n\nThis element signifies a tip or suggestion.\n\nThis element signifies a general note.\n\nThis element indicates a warning or caution.\n\nUsing Code Examples Supplemental material (code examples, exercises, etc.) is available for download at https://github.com/alicezheng/feature-engineering-book.\n\nThis book is here to help you get your job done. In general, if example code is offered with this book, you may use it in your programs and documentation. You do not need to contact us for permission unless you’re reproducing a significant portion of the code. For example, writing a program that uses several chunks of code from this book does not require permission. Selling or distributing a CD-ROM of examples from O’Reilly books does require permission. Answering a question by citing this book and quoting example code does not require permission. Incorporating a signifi‐ cant amount of example code from this book into your product’s documentation does require permission.\n\nWe appreciate, but do not require, attribution. An attribution usually includes the title, author, publisher, and ISBN. For example: “Feature Engineering for Machine Learning by Alice Zheng and Amanda Casari (O’Reilly). Copyright 2018 Alice Zheng and Amanda Casari, 978-1-491-95324-2.”\n\nIf you feel your use of code examples falls outside fair use or the permission given above, feel free to contact us at permissions@oreilly.com.\n\nx\n\n| Preface\n\nO’Reilly Safari\n\nSafari (formerly Safari Books Online) is a membership-based training and reference platform for enterprise, government, educators, and individuals.\n\nMembers have access to thousands of books, training videos, Learning Paths, interac‐ tive tutorials, and curated playlists from over 250 publishers, including O’Reilly Media, Harvard Business Review, Prentice Hall Professional, Addison-Wesley Pro‐ fessional, Microsoft Press, Sams, Que, Peachpit Press, Adobe, Focal Press, Cisco Press, John Wiley & Sons, Syngress, Morgan Kaufmann, IBM Redbooks, Packt, Adobe Press, FT Press, Apress, Manning, New Riders, McGraw-Hill, Jones & Bartlett, and Course Technology, among others.\n\nFor more information, please visit http://oreilly.com/safari.\n\nHow to Contact Us Please address comments and questions concerning this book to the publisher:\n\nO’Reilly Media, Inc. 1005 Gravenstein Highway North Sebastopol, CA 95472 800-998-9938 (in the United States or Canada) 707-829-0515 (international or local) 707-829-0104 (fax)\n\nWe have a web page for this book, where we list errata, examples, and any additional information. You can access this page at http://bit.ly/featureEngineering_for_ML.\n\nTo comment or ask technical questions about this book, send email to bookques‐ tions@oreilly.com.\n\nFor more information about our books, courses, conferences, and news, see our web‐ site at http://www.oreilly.com.\n\nFind us on Facebook: http://facebook.com/oreilly\n\nFollow us on Twitter: http://twitter.com/oreillymedia\n\nWatch us on YouTube: http://www.youtube.com/oreillymedia\n\nPreface\n\n|\n\nxi\n\nAcknowledgments First and foremost, we want to thank our editors, Shannon Cutt and Jeff Bleiel, for shepherding two first-time authors through the (unknown to us) long marathon of book publishing. Without your many check-ins, this book would not have seen the light of day. Thank you also to Ben Lorica, O’Reilly Mastermind, whose encourage‐ ment and affirmation turned this from a crazy idea into an actual product. Thank you to Kristen Brown and the O’Reilly production team for their superb attention to detail and extreme patience in waiting for our responses.\n\nIf it takes a village to raise a child, it takes a parliament of data scientists to publish a book. We greatly appreciate every hashtag suggestion, notes on room for improve‐ ment and calls for clarification. Andreas Müller, Sethu Raman, and Antoine Atallah took precious time out of their busy days to provide technical reviews. Antoine not only did so at lightning speed, but also made available his beefy machines for use on experiments. Ted Dunning’s statistical fluency and mastery of applied machine learn‐ ing are legendary. He is also incredibly generous with his time and his ideas, and he literally gave us the method and the example described in the k-means chapter. Owen Zhang revealed his cache of Kaggle nuggets on using response rate features, which were added to machine learning folklore on bin-counting collected by Misha Bilenko. Thank you also to Alex Ott, Francisco Martin, and David Garrison for additional feedback.\n\nSpecial Thanks from Alice I would like to thank the GraphLab/Dato/Turi family for their generous support in the first phase of this project. The idea germinated from interactions with our users. In the process of building a brand new machine learning platform for data scientists, we discovered that the world needs a more systematic understanding of feature engi‐ neering. Thank you to Carlos Guestrin for granting me leave from busy startup life to focus on writing.\n\nThank you to Amanda, who started out as technical reviewer and later pitched in to help bring this book to life. You are the best finisher! Now that this book is done, we’ll have to find another project, if only to keep doing our editing sessions over tea and coffee and sandwiches and takeout food.\n\nSpecial thanks to my friend and healer, Daisy Thompson, for her unwavering support throughout all phases of this project. Without your help, I would have taken much longer to take the plunge, and would have resented the marathon. You brought light and relief to this project, as you do with all your work.\n\nxii\n\n| Preface\n\nSpecial Thanks from Amanda As this is a book and not a lifetime achievement award, I will attempt to scope my thanks to the project at hand.\n\nMany thanks to Alice for bringing me in as a technical editor and then coauthor. I continue to learn so much from you, including how to write better math jokes and explain complex concepts clearly.\n\nLast in order only, special thanks to my husband, Matthew, for mastering the nearly impossible role of grounding me, encouraging me towards my next goal, and never allowing a concept to be hand-waved away. You are the best partner and my favorite partner in crime. To the biggest and littlest sunshines, you inspire me to make you proud.\n\nPreface\n\n|\n\nxiii\n\nCHAPTER 1 The Machine Learning Pipeline\n\nBefore diving into feature engineering, let’s take a moment to take a look at the over‐ all machine learning pipeline. This will help us get situated in the larger picture of the application. To that end, we’ll begin with a little musing on the basic concepts like data and models.\n\nData What we call data are observations of real-world phenomena. For instance, stock market data might involve observations of daily stock prices, announcements of earnings by individual companies, and even opinion articles from pundits. Personal biometric data can include measurements of our minute-by-minute heart rate, blood sugar level, blood pressure, etc. Customer intelligence data includes observations such as “Alice bought two books on Sunday,” “Bob browsed these pages on the web‐ site,” and “Charlie clicked on the special offer link from last week.” We can come up with endless examples of data across different domains.\n\nEach piece of data provides a small window into a limited aspect of reality. The col‐ lection of all of these observations gives us a picture of the whole. But the picture is messy because it is composed of a thousand little pieces, and there’s always measure‐ ment noise and missing pieces.\n\nTasks Why do we collect data? There are questions that data can help us answer—questions like “Which stocks I should invest in?” or “How can I live a healthier lifestyle?” or “How can I understand my customers’ changing tastes, so that my business can serve them better?”\n\n1\n\nThe path from data to answers is full of false starts and dead ends (see Figure 1-1). What starts out as a promising approach may not pan out. What was originally just a hunch may end up leading to the best solution. Workflows with data are frequently multistage, iterative processes. For instance, stock prices are observed at the exchange, aggregated by an intermediary like Thomson Reuters, stored in a database, bought by a company, converted into a Hive store on a Hadoop cluster, pulled out of the store by a script, subsampled, massaged, and cleaned by another script, dumped to a file, and converted to a format that you can try out in your favorite modeling library in R, Python, or Scala. The predictions are then dumped back out to a CSV file and parsed by an evaluator, and the model is iterated multiple times, rewritten in C++ or Java by your production team, and run on all of the data before the final pre‐ dictions are pumped out to another database.\n\nFigure 1-1. The garden of bifurcating paths between data and answers\n\nHowever, if we disregard the mess of tools and systems for a moment, we might see that the process involves two mathematical entities that are the bread and butter of machine learning: models and features.\n\nModels Trying to understand the world through data is like trying to piece together reality using a noisy, incomplete jigsaw puzzle with a bunch of extra pieces. This is where mathematical modeling—in particular statistical modeling—comes in. The language of statistics contains concepts for many frequent characteristics of data, such as wrong, redundant, or missing. Wrong data is the result of a mistake in measurement. Redundant data contains multiple aspects that convey exactly the same information. For instance, the day of week may be present as a categorical variable with values of “Monday,” “Tuesday,” ... “Sunday,” and again included as an integer value between 0\n\n2\n\n|\n\nChapter 1: The Machine Learning Pipeline\n\nand 6. If this day-of-week information is not present for some data points, then you’ve got missing data on your hands.\n\nA mathematical model of data describes the relationships between different aspects of the data. For instance, a model that predicts stock prices might be a formula that maps a company’s earning history, past stock prices, and industry to the predicted stock price. A model that recommends music might measure the similarity between users (based on their listening habits), and recommend the same artists to users who have listened to a lot of the same songs.\n\nMathematical formulas relate numeric quantities to each other. But raw data is often not numeric. (The action “Alice bought The Lord of the Rings trilogy on Wednesday” is not numeric, and neither is the review that she subsequently writes about the book.) There must be a piece that connects the two together. This is where features come in.\n\nFeatures A feature is a numeric representation of raw data. There are many ways to turn raw data into numeric measurements, which is why features can end up looking like a lot of things. Naturally, features must derive from the type of data that is available. Per‐ haps less obvious is the fact that they are also tied to the model; some models are more appropriate for some types of features, and vice versa. The right features are rel‐ evant to the task at hand and should be easy for the model to ingest. Feature engineer‐ ing is the process of formulating the most appropriate features given the data, the model, and the task.\n\nThe number of features is also important. If there are not enough informative fea‐ tures, then the model will be unable to perform the ultimate task. If there are too many features, or if most of them are irrelevant, then the model will be more expen‐ sive and tricky to train. Something might go awry in the training process that impacts the model’s performance.\n\nModel Evaluation Features and models sit between raw data and the desired insights (see Figure 1-2). In a machine learning workflow, we pick not only the model, but also the features. This is a double-jointed lever, and the choice of one affects the other. Good features make the subsequent modeling step easy and the resulting model more capable of complet‐ ing the desired task. Bad features may require a much more complicated model to achieve the same level of performance. In the rest of this book, we will cover different kinds of features and discuss their pros and cons for different types of data and mod‐ els. Without further ado, let’s get started!\n\nFeatures\n\n|\n\n3\n\nFigure 1-2. The place of feature engineering in the machine learning workflow\n\n4\n\n|\n\nChapter 1: The Machine Learning Pipeline\n\nCHAPTER 2 Fancy Tricks with Simple Numbers\n\nBefore diving into complex data types such as text and image, let’s start with the sim‐ plest: numeric data. This may come from a variety of sources: geolocation of a place or a person, the price of a purchase, measurements from a sensor, traffic counts, etc. Numeric data is already in a format that’s easily ingestible by mathematical models. This doesn’t mean that feature engineering is no longer necessary, though. Good fea‐ tures should not only represent salient aspects of the data, but also conform to the assumptions of the model. Hence, transformations are often necessary. Numeric fea‐ ture engineering techniques are fundamental. They can be applied whenever raw data is converted into numeric features.\n\nThe first sanity check for numeric data is whether the magnitude matters. Do we just need to know whether it’s positive or negative? Or perhaps we only need to know the magnitude at a very coarse granularity? This sanity check is particularly important for automatically accrued numbers such as counts—the number of daily visits to a website, the number of reviews garnered by a restaurant, etc.\n\nNext, consider the scale of the features. What are the largest and the smallest values? Do they span several orders of magnitude? Models that are smooth functions of input features are sensitive to the scale of the input. For example, 3x + 1 is a simple linear function of the input x, and the scale of its output depends directly on the scale of the input. Other examples include k-means clustering, nearest neighbors methods, radial basis function (RBF) kernels, and anything that uses the Euclidean distance. For these models and modeling components, it is often a good idea to normalize the features so that the output stays on an expected scale.\n\nLogical functions, on the other hand, are not sensitive to input feature scale. Their output is binary no matter what the inputs are. For instance, the logical AND takes any two variables and outputs 1 if and only if both of the inputs are true. Another example of a logical function is the step function (e.g., is input x greater than 5?).\n\n5",
      "page_number": 11
    },
    {
      "number": 3,
      "title": "Segment 3 (pages 20-27)",
      "start_page": 20,
      "end_page": 27,
      "detection_method": "topic_boundary",
      "content": "Decision tree models consist of step functions of input features. Hence, models based on space-partitioning trees (decision trees, gradient boosted machines, random for‐ ests) are not sensitive to scale. The only exception is if the scale of the input grows over time, which is the case if the feature is an accumulated count of some sort— eventually it will grow outside of the range that the tree was trained on. If this might be the case, then it might be necessary to rescale the inputs periodically. Another sol‐ ution is the bin-counting method discussed in Chapter 5.\n\nIt’s also important to consider the distribution of numeric features. Distribution summarizes the probability of taking on a particular value. The distribution of input features matters to some models more than others. For instance, the training process of a linear regression model assumes that prediction errors are distributed like a Gaussian. This is usually fine, except when the prediction target spreads out over sev‐ eral orders of magnitude. In this case, the Gaussian error assumption likely no longer holds. One way to deal with this is to transform the output target in order to tame the magnitude of the growth. (Strictly speaking this would be target engineering, not fea‐ ture engineering.) Log transforms, which are a type of power transform, take the dis‐ tribution of the variable closer to Gaussian.\n\nIn addition to features tailoring to the assumptions of the model or training process, multiple features can be composed together into more complex features. The hope is that complex features can more succinctly capture important information in raw data. Making the input features more “eloquent” allows the model itself to be simpler, easier to train and evaluate, and to make better predictions. Taken to an extreme, complex features may themselves be the output of statistical models. This is a concept known as model stacking, which we discuss in much more detail in Chapters 7 and 8. In this chapter, we give the simplest example of complex features: interaction features.\n\nInteraction features are simple to formulate, but the combination of features results in many more being input into the model. In order to reduce the computational expense, it is usually necessary to prune the input features using automatic feature selection.\n\nWe’ll begin with the basic concepts of scalars, vectors, and spaces, followed by discus‐ sions of scale, distribution, interaction features, and feature selection.\n\nScalars, Vectors, and Spaces Before we go any further, we need to define some basic concepts that underlie the rest of this book. A single numeric feature is also known as a scalar. An ordered list of scalars is known as a vector. Vectors sit within a vector space. In the vast majority of machine learning applications, the input to a model is usually represented as a\n\n6\n\n|\n\nChapter 2: Fancy Tricks with Simple Numbers\n\nnumeric vector. The rest of this book will discuss best-practice strategies for convert‐ ing raw data into a vector of numbers.\n\nA vector can be visualized as a point in space. (Sometimes people draw a line or arrow from the origin to that point. In this book, we will mostly use just the point.) For instance, suppose we have a two-dimensional vector v = [1, –1]. The vector con‐ tains two numbers: in the first direction, d1, the vector has a value of 1, and in the second direction, d2, it has a value of –1. We can plot v in a 2D plot, as shown in Figure 2-1.\n\nFigure 2-1. A single vector\n\nIn the world of data, an abstract vector and its feature dimensions take on actual meaning. For instance, a vector can represent a person’s preference for songs. Each song is a feature, where a value of 1 is equivalent to a thumbs-up, and –1 a thumbs- down. Suppose the vector v represents the preferences of a listener, Bob. Bob likes “Blowin’ in the Wind” by Bob Dylan and “Poker Face” by Lady Gaga. Other people might have different preferences. Collectively, a collection of data can be visualized in feature space as a point cloud.\n\nScalars, Vectors, and Spaces\n\n|\n\n7\n\nConversely, a song can be represented by the individual preferences of a group of people. Suppose there are only two listeners, Alice and Bob. Alice likes “Poker Face,” “Blowin’ in the Wind,” and “Hallelujah” by Leonard Cohen, but hates Katy Perry’s “Roar” and Radiohead’s “Creep.” Bob likes “Roar,” “Hallelujah,” and “Blowin’ in the Wind,” but hates “Poker Face” and “Creep.” Each song is a point in the space of lis‐ teners. Just like we can visualize data in feature space, we can visualize features in data space. Figure 2-2 shows this example.\n\nFigure 2-2. Illustration of feature space versus data space\n\nDealing with Counts In the age of Big Data, counts can quickly accumulate without bound. A user might put a song or a movie on infinite playback or use a script to repeatedly check for the availability of tickets for a popular show, which will cause the play count or website visit count to quickly rise. When data can be produced at high volume and velocity, it’s very likely to contain a few extreme values. It is a good idea to check the scale and determine whether to keep the data as raw numbers, convert them into binary values to indicate presence, or bin them into coarser granularity. To illustrate these ideas, let’s look at a few examples.\n\n8\n\n|\n\nChapter 2: Fancy Tricks with Simple Numbers\n\nBinarization The Echo Nest Taste Profile subset, the official user data collection for the Million Song Dataset, contains the full music listening histories of one million users on Echo Nest. Here are some relevant statistics about the dataset:\n\nStatistics on the Echo Nest Taste Profile Dataset\n\nThere are more than 48 million triplets of user ID, song ID, and listen count.\n\nThe full dataset contains 1,019,318 unique users and 384,546 unique songs.\n\nSuppose our task is to build a recommender to recommend songs to users. One com‐ ponent of the recommender might predict how much a user will enjoy a particular song. Since the data contains actual listen counts, should that be the target of the pre‐ diction? This would be the right thing to do if a large listen count means the user really likes the song and a low listen count means they’re not interested in it. How‐ ever, the data shows that while 99% of the listen counts are 24 or lower, there are also some listen counts in the thousands, with the maximum being 9,667. (As Figure 2-3 shows, the histogram peaks in the bin closest to 0. But more than 10,000 triplets have greater counts, with a few in the thousands.) These values are anomalously large; if we were to try to predict the actual listen counts, the model would be pulled off course by these large values.\n\nFigure 2-3. Histogram of listen counts in the Taste Profile subset of the Million Song Dataset—note that the y-axis is on a log scale\n\nDealing with Counts\n\n|\n\n9\n\nIn the Million Song Dataset, the raw listen count is not a robust measure of user taste. (In statistical lingo, robustness means that the method works under a wide variety of conditions.) Users have different listening habits. Some people might put their favor‐ ite songs on infinite loop, while others might savor them only on special occasions. We can’t necessarily say that someone who listens to a song 20 times must like it twice as much as someone else who listens to it 10 times.\n\nA more robust representation of user preference is to binarize the count and clip all counts greater than 1 to 1, as illustrated in Example 2-1. In other words, if the user listened to a song at least once, then we count it as the user liking the song. This way, the model will not need to spend cycles on predicting the minute differences between the raw counts. The binary target is a simple and robust measure of user preference.\n\nExample 2-1. Binarizing listen counts in the Million Song Dataset\n\n>>> import pandas as pd >>> listen_count = pd.read_csv('millionsong/train_triplets.txt.zip', ... header=None, delimiter='\\t') # The table contains user-song-count triplets. Only nonzero counts are # included. Hence, to binarize the count, we just need to set the entire # count column to 1. >>> listen_count[2] = 1\n\nThis is an example where we engineer the target variable of the model. Strictly speak‐ ing, the target is not a feature because it’s not the input. But on occasion we do need to modify the target in order to solve the right problem.\n\nQuantization or Binning For this exercise, we take data from round 6 of the Yelp dataset challenge and create a much smaller classification dataset. The Yelp dataset contains user reviews of busi‐ nesses from 10 cities across North America and Europe. Each business is labeled with zero or more categories.\n\nStatistics on the Yelp Reviews Dataset (Round 6)\n\nThere are 782 business categories.\n\nThe full dataset contains 1,569,264 (≈1.6M) reviews and 61,184 (61K) businesses.\n\n“Restaurants” (990,627 reviews) and “Nightlife” (210,028 reviews) are the most popular categories, review count–wise.\n\nNo business is categorized as both a restaurant and a nightlife venue. So, there is no overlap between the two groups of reviews.\n\n10\n\n|\n\nChapter 2: Fancy Tricks with Simple Numbers\n\nEach business has a review count. Suppose our task is to use collaborative filtering to predict the rating a user might give to a business. The review count might be a useful input feature because there is usually a strong correlation between popularity and good ratings. Now the question is, should we use the raw review count or process it further? Figure 2-4, produced by Example 2-2, shows the histogram of all business review counts. We see the same pattern as in the listen counts in the previous exam‐ ple: most of the counts are small, but some businesses have reviews in the thousands.\n\nExample 2-2. Visualizing business review counts in the Yelp dataset\n\n>>> import pandas as pd >>> import json\n\n# Load the data about businesses >>> biz_file = open('yelp_academic_dataset_business.json') >>> biz_df = pd.DataFrame([json.loads(x) for x in biz_file.readlines()]) >>> biz_file.close()\n\n>>> import matplotlib.pyplot as plt >>> import seaborn as sns\n\n# Plot the histogram of the review counts >>> sns.set_style('whitegrid') >>> fig, ax = plt.subplots() >>> biz_df['review_count'].hist(ax=ax, bins=100) >>> ax.set_yscale('log') >>> ax.tick_params(labelsize=14) >>> ax.set_xlabel('Review Count', fontsize=14) >>> ax.set_ylabel('Occurrence', fontsize=14)\n\nRaw counts that span several orders of magnitude are problematic for many models. In a linear model, the same linear coefficient would have to work for all possible val‐ ues of the count. Large counts could also wreak havoc in unsupervised learning methods such as k-means clustering, which uses Euclidean distance as a similarity function to measure the similarity between data points. A large count in one element of the data vector would outweigh the similarity in all other elements, which could throw off the entire similarity measurement.\n\nOne solution is to contain the scale by quantizing the count. In other words, we group the counts into bins, and get rid of the actual count values. Quantization maps a continuous number to a discrete one. We can think of the discretized numbers as an ordered sequence of bins that represent a measure of intensity.\n\nDealing with Counts\n\n|\n\n11\n\nFigure 2-4. Histogram of business review counts in the Yelp reviews dataset—the y-axis is on a log scale\n\nIn order to quantize data, we have to decide how wide each bin should be. The solu‐ tions fall into two categories: fixed-width or adaptive. We will give an example of each type.\n\nFixed-width binning\n\nWith fixed-width binning, each bin contains a specific numeric range. The ranges can be custom designed or automatically segmented, and they can be linearly scaled or exponentially scaled. For example, we can group people into age ranges by decade: 0–9 years old in bin 1, 10–19 years in bin 2, etc. To map from the count to the bin, we simply divide by the width of the bin and take the integer part.\n\nIt’s also common to see custom-designed age ranges that better correspond to stages of life, such as:\n\n0–12 years old\n\n12–17 years old\n\n18–24 years old\n\n25–34 years old\n\n35–44 years old\n\n45–54 years old\n\n55–64 years old\n\n12\n\n|\n\nChapter 2: Fancy Tricks with Simple Numbers\n\n65–74 years old\n\n75 years or older\n\nWhen the numbers span multiple magnitudes, it may be better to group by powers of 10 (or powers of any constant): 0–9, 10–99, 100–999, 1000–9999, etc. The bin widths grow exponentially, going from O(10), to O(100), O(1000), and beyond. To map from the count to the bin, we take the log of the count. Exponential-width binning is very much related to the log transform, which we discuss in “Log Transformation” on page 15. Example 2-3 illustrates several of these binning methods.\n\nExample 2-3. Quantizing counts with fixed-width bins\n\n>>> import numpy as np\n\n# Generate 20 random integers uniformly between 0 and 99 >>> small_counts = np.random.randint(0, 100, 20) >>> small_counts array([30, 64, 49, 26, 69, 23, 56, 7, 69, 67, 87, 14, 67, 33, 88, 77, 75, 47, 44, 93]) # Map to evenly spaced bins 0-9 by division >>> np.floor_divide(small_counts, 10) array([3, 6, 4, 2, 6, 2, 5, 0, 6, 6, 8, 1, 6, 3, 8, 7, 7, 4, 4, 9], dtype=int32)\n\n# An array of counts that span several magnitudes >>> large_counts = [296, 8286, 64011, 80, 3, 725, 867, 2215, 7689, 11495, 91897, ... 44, 28, 7971, 926, 122, 22222] # Map to exponential-width bins via the log function >>> np.floor(np.log10(large_counts)) array([ 2., 3., 4., 1., 0., 2., 2., 3., 3., 4., 4., 1., 1., 3., 2., 2., 4.])\n\nQuantile binning\n\nFixed-width binning is easy to compute. But if there are large gaps in the counts, then there will be many empty bins with no data. This problem can be solved by adaptively positioning the bins based on the distribution of the data. This can be done using the quantiles of the distribution.\n\nQuantiles are values that divide the data into equal portions. For example, the median divides the data in halves; half the data points are smaller and half larger than the median. The quartiles divide the data into quarters, the deciles into tenths, etc. Example 2-4 demonstrates how to compute the deciles of the Yelp business review counts, and Figure 2-5 overlays the deciles on the histogram. This gives a much clearer picture of the skew toward smaller counts.\n\nDealing with Counts\n\n|\n\n13",
      "page_number": 20
    },
    {
      "number": 4,
      "title": "Segment 4 (pages 28-38)",
      "start_page": 28,
      "end_page": 38,
      "detection_method": "topic_boundary",
      "content": "Example 2-4. Computing deciles of Yelp business review counts\n\n>>> deciles = biz_df['review_count'].quantile([.1, .2, .3, .4, .5, .6, .7, .8, .9]) >>> deciles 0.1 3.0 0.2 4.0 0.3 5.0 0.4 6.0 0.5 8.0 0.6 12.0 0.7 17.0 0.8 28.0 0.9 58.0 Name: review_count, dtype: float64\n\n# Visualize the deciles on the histogram >>> sns.set_style('whitegrid') >>> fig, ax = plt.subplots() >>> biz_df['review_count'].hist(ax=ax, bins=100) >>> for pos in deciles: ... handle = plt.axvline(pos, color='r') >>> ax.legend([handle], ['deciles'], fontsize=14) >>> ax.set_yscale('log') >>> ax.set_xscale('log') >>> ax.tick_params(labelsize=14) >>> ax.set_xlabel('Review Count', fontsize=14) >>> ax.set_ylabel('Occurrence', fontsize=14)\n\nFigure 2-5. Deciles of the review counts in the Yelp reviews dataset—both the x- and y- axes are on a log scale\n\n14\n\n|\n\nChapter 2: Fancy Tricks with Simple Numbers\n\nTo compute the quantiles and map data into quantile bins, we can use the Pandas library, as shown in Example 2-5. pandas.DataFrame.quantile and pandas.Ser ies.quantile compute the quantiles. pandas.qcut maps data into a desired number of quantiles.\n\nExample 2-5. Binning counts by quantiles\n\n# Continue example 2-3 with large_counts >>> import pandas as pd\n\n# Map the counts to quartiles >>> pd.qcut(large_counts, 4, labels=False) array([1, 2, 3, 0, 0, 1, 1, 2, 2, 3, 3, 0, 0, 2, 1, 0, 3], dtype=int64)\n\n# Compute the quantiles themselves >>> large_counts_series = pd.Series(large_counts) >>> large_counts_series.quantile([0.25, 0.5, 0.75]) 0.25 122.0 0.50 926.0 0.75 8286.0 dtype: float64\n\nLog Transformation In the previous section, we briefly introduced the notion of taking the logarithm of the count to map the data to exponential-width bins. Let’s take a closer look at that now.\n\nThe log function is the inverse of the exponential function. It is defined such that loga(ax) = x, where a is a positive constant, and x can be any positive number. Since a0 = 1, we have loga(1) = 0. This means that the log function maps the small range of numbers between (0, 1) to the entire range of negative numbers (–∞, 0). The function log10(x) maps the range of [1, 10] to [0, 1], [10, 100] to [1, 2], and so on. In other words, the log function compresses the range of large numbers and expands the range of small numbers. The larger x is, the slower log(x) increments.\n\nThis is easier to digest by looking at a plot of the log function (see Figure 2-6). Note how the horizontal x values from 100 to 1,000 get compressed into just 2.0 to 3.0 in the vertical y range, while the tiny horizontal portion of x values less than 100 are mapped to the rest of the vertical range.\n\nLog Transformation\n\n|\n\n15\n\nFigure 2-6. The log function compresses the high numeric range and expands the low range\n\nThe log transform is a powerful tool for dealing with positive numbers with a heavy- tailed distribution. (A heavy-tailed distribution places more probability mass in the tail range than a Gaussian distribution.) It compresses the long tail in the high end of the distribution into a shorter tail, and expands the low end into a longer head. Figure 2-7 compares the histograms of Yelp business review counts before and after log transformation (see Example 2-6). The y-axes are now both on a normal (linear) scale. The increased bin spacing in the bottom plot between the range of (0.5, 1] is due to there being only 10 possible integer counts between 1 and 10. Notice that the original review counts are very concentrated in the low count region, with outliers stretching out above 4,000. After log transformation, the histogram is less concentra‐ ted in the low end and more spread out over the x-axis.\n\nExample 2-6. Visualizing the distribution of review counts before and after log transform\n\n>>> fig, (ax1, ax2) = plt.subplots(2,1) >>> biz_df['review_count'].hist(ax=ax1, bins=100) >>> ax1.tick_params(labelsize=14) >>> ax1.set_xlabel('review_count', fontsize=14) >>> ax1.set_ylabel('Occurrence', fontsize=14)\n\n>>> biz_df['log_review_count'].hist(ax=ax2, bins=100) >>> ax2.tick_params(labelsize=14) >>> ax2.set_xlabel('log10(review_count))', fontsize=14) >>> ax2.set_ylabel('Occurrence', fontsize=14)\n\n16\n\n|\n\nChapter 2: Fancy Tricks with Simple Numbers\n\nFigure 2-7. Comparison of Yelp business review counts before (top) and after (bottom) log transformation\n\nAs another example, let’s consider the Online News Popularity dataset from the UC Irvine Machine Learning Repository (Fernandes et al., 2015).\n\nStatistics on the Online News Popularity Dataset\n\nThe dataset includes 60 features of a set of 39,797 news articles published by Mashable over a period of 2 years.\n\nOur goal is to use these features to predict the popularity of the articles in terms of the number of shares on social media. In this example, we’ll focus on only one feature —the number of words in the article. Figure 2-8 shows the histograms of the feature before and after log transformation (see Example 2-7). Notice that the distribution looks much more Gaussian after log transformation, with the exception of the burst of number of articles of length zero (no content).\n\nLog Transformation\n\n|\n\n17\n\nExample 2-7. Visualizing the distribution of news article popularity with and without log transformation\n\n>>> fig, (ax1, ax2) = plt.subplots(2,1) >>> df['n_tokens_content'].hist(ax=ax1, bins=100) >>> ax1.tick_params(labelsize=14) >>> ax1.set_xlabel('Number of Words in Article', fontsize=14) >>> ax1.set_ylabel('Number of Articles', fontsize=14)\n\n>>> df['log_n_tokens_content'].hist(ax=ax2, bins=100) >>> ax2.tick_params(labelsize=14) >>> ax2.set_xlabel('Log of Number of Words', fontsize=14) >>> ax2.set_ylabel('Number of Articles', fontsize=14)\n\nFigure 2-8. Comparison of word counts in Mashable news articles before (top) and after (bottom) log transformation\n\n18\n\n|\n\nChapter 2: Fancy Tricks with Simple Numbers\n\nLog Transform in Action Let’s see how the log transform performs for supervised learning. We’ll use both of the previous datasets here. For the Yelp reviews dataset, we’ll use the number of reviews to predict the average rating of a business (see Example 2-8). For the Masha‐ ble news articles, we’ll use the number of words in an article to predict its popularity. Since the outputs are continuous numbers, we’ll use simple linear regression as the model. We use scikit-learn to perform 10-fold cross validation of linear regression on the feature with and without log transformation. The models are evaluated by the R- squared score, which measures how well a trained regression model predicts new data. Good models have high R-squared scores. A perfect model gets the maximum score of 1. The score can be negative, and a bad model can get an arbitrarily low neg‐ ative score. Using cross validation, we obtain not only an estimate of the score but also a variance, which helps us gauge whether the differences between the two models are meaningful.\n\nExample 2-8. Using log transformed Yelp review counts to predict average business rating\n\n>>> import pandas as pd >>> import numpy as np >>> import json >>> from sklearn import linear_model >>> from sklearn.model_selection import cross_val_score\n\n# Using the previously loaded Yelp reviews DataFrame, # compute the log transform of the Yelp review count. # Note that we add 1 to the raw count to prevent the logarithm from # exploding into negative infinity in case the count is zero. >>> biz_df['log_review_count'] = np.log10(biz_df['review_count'] + 1)\n\n# Train linear regression models to predict the average star rating of a business, # using the review_count feature with and without log transformation. # Compare the 10-fold cross validation score of the two models. >>> m_orig = linear_model.LinearRegression() >>> scores_orig = cross_val_score(m_orig, biz_df[['review_count']], ... biz_df['stars'], cv=10) >>> m_log = linear_model.LinearRegression() >>> scores_log = cross_val_score(m_log, biz_df[['log_review_count']], ... biz_df['stars'], cv=10) >>> print(\"R-squared score without log transform: %0.5f (+/- %0.5f)\" ... % (scores_orig.mean(), scores_orig.std() * 2)) >>> print(\"R-squared score with log transform: %0.5f (+/- %0.5f)\" ... % (scores_log.mean(), scores_log.std() * 2)) R-squared score without log transform: -0.03683 (+/- 0.07280) R-squared score with log transform: -0.03694 (+/- 0.07650)\n\nLog Transformation\n\n|\n\n19\n\nJudging by the output of the experiment, the two simple models (with and without log transform) are equally bad at predicting the target, with the log transformed fea‐ ture performing slightly worse. How disappointing! It’s not surprising that neither of them are very good, given that they both use just one feature, but one would have hoped that the log transformed feature might have performed better.\n\nNow let’s look at how the log transform does on the Online News Popularity dataset (Example 2-9).\n\nExample 2-9. Using log transformed word counts in the Online News Popularity dataset to predict article popularity\n\n# Download the Online News Popularity dataset from UCI, then use # Pandas to load the file into a DataFrame. >>> df = pd.read_csv('OnlineNewsPopularity.csv', delimiter=', ')\n\n# Take the log transform of the 'n_tokens_content' feature, which # represents the number of words (tokens) in a news article. >>> df['log_n_tokens_content'] = np.log10(df['n_tokens_content'] + 1)\n\n# Train two linear regression models to predict the number of shares # of an article, one using the original feature and the other the # log transformed version. >>> m_orig = linear_model.LinearRegression() >>> scores_orig = cross_val_score(m_orig, df[['n_tokens_content']], ... df['shares'], cv=10) >>> m_log = linear_model.LinearRegression() >>> scores_log = cross_val_score(m_log, df[['log_n_tokens_content']], ... df['shares'], cv=10) >>> print(\"R-squared score without log transform: %0.5f (+/- %0.5f)\" ... % (scores_orig.mean(), scores_orig.std() * 2)) >>> print(\"R-squared score with log transform: %0.5f (+/- %0.5f)\" ... % (scores_log.mean(), scores_log.std() * 2)) R-squared score without log transform: -0.00242 (+/- 0.00509) R-squared score with log transform: -0.00114 (+/- 0.00418)\n\nThe confidence intervals still overlap, but the model with the log transformed feature is doing better than the one without. Why is the log transform so much more suc‐ cessful on this dataset? We can get a clue by looking at the scatter plots (Example 2-10) of the input feature and target values. As can be seen in the bottom panel of Figure 2-9, the log transform reshaped the x-axis, pulling the articles with large outliers in the target value (>200,000 shares) further out toward the righthand side of the axis. This gives the linear model more “breathing room” on the low end of the input feature space. Without the log transform (top panel), the model is under more pressure to fit very different target values under very small changes in the input.\n\n20\n\n|\n\nChapter 2: Fancy Tricks with Simple Numbers\n\nExample 2-10. Visualizing the correlation between input and output in the news popularity prediction problem\n\n>>> fig2, (ax1, ax2) = plt.subplots(2,1) >>> ax1.scatter(df['n_tokens_content'], df['shares']) >>> ax1.tick_params(labelsize=14) >>> ax1.set_xlabel('Number of Words in Article', fontsize=14) >>> ax1.set_ylabel('Number of Shares', fontsize=14)\n\n>>> ax2.scatter(df['log_n_tokens_content'], df['shares']) >>> ax2.tick_params(labelsize=14) >>> ax2.set_xlabel('Log of the Number of Words in Article', fontsize=14) >>> ax2.set_ylabel('Number of Shares', fontsize=14)\n\nFigure 2-9. Scatter plots of number of words (input) versus number of shares (target) in the Online News Popularity dataset—the top plot visualizes the original feature, and the bottom plot shows the scatter plot after log transformation\n\nLog Transformation\n\n|\n\n21\n\nCompare this with the same scatter plot applied to the Yelp reviews dataset (Example 2-11). Figure 2-10 looks very different from Figure 2-9. The average star rating is discretized in increments of half-stars ranging from 1 to 5. High review counts (roughly >2,500 reviews) do correlate with higher average star ratings, but the relationship is far from linear. There is no clear way to draw a line to predict the aver‐ age star rating based on either input. Essentially, the plot shows that review count and its logarithm are both bad linear predictors of average star rating.\n\nExample 2-11. Visualizing the correlation between input and output in Yelp business review prediction\n\n>>> fig, (ax1, ax2) = plt.subplots(2,1) >>> ax1.scatter(biz_df['review_count'], biz_df['stars']) >>> ax1.tick_params(labelsize=14) >>> ax1.set_xlabel('Review Count', fontsize=14) >>> ax1.set_ylabel('Average Star Rating', fontsize=14)\n\n>>> ax2.scatter(biz_df['log_review_count'], biz_df['stars']) >>> ax2.tick_params(labelsize=14) >>> ax2.set_xlabel('Log of Review Count', fontsize=14) >>> ax2.set_ylabel('Average Star Rating', fontsize=14)\n\nThe Importance of Data Visualization\n\nThe comparison of the effect of the log transform on two different datasets illustrates the importance of visualizing the data. Here, we intentionally kept the input and target variables simple so that we can easily visualize the relationship between them. Plots like those in Figure 2-10 immediately reveal that the chosen model (linear) cannot possibly represent the relationship between the chosen input and target. On the other hand, one could convincingly model the distribution of review count given the average star rating. When building models, it is a good idea to visually inspect the rela‐ tionships between input and output, and between different input features.\n\n22\n\n|\n\nChapter 2: Fancy Tricks with Simple Numbers\n\nFigure 2-10. Scatter plots of review counts (input) versus average star rating (target) in the Yelp reviews dataset—the top panel plots the original review count, and the bottom panel plots the review count after log transformation\n\nPower Transforms: Generalization of the Log Transform The log transform is a specific example of a family of transformations known as power transforms. In statistical terms, these are variance-stabilizing transformations. To understand why variance stabilization is good, consider the Poisson distribution. This is a heavy-tailed distribution with a variance that is equal to its mean: hence, the larger its center of mass, the larger its variance, and the heavier the tail. Power trans‐ forms change the distribution of the variable so that the variance is no longer depen‐ dent on the mean. For example, suppose a random variable X has the Poisson distribution. If we transform X by taking its square root, the variance of X˜ = X is roughly constant, instead of being equal to the mean.\n\nLog Transformation\n\n|\n\n23\n\nFigure 2-11 illustrates λ, which represents the mean of the distribution. As λ increa‐ ses, not only does the mode of the distribution shift to the right, but the mass spreads out and the variance becomes larger.\n\nFigure 2-11. A rough illustration of the Poisson distribution, an example distribution where the variance increases along with the mean\n\nA simple generalization of both the square root transform and the log transform is known as the Box-Cox transform:\n\nx˜ ={ x λ – 1\n\nλ\n\nln(x)\n\nif λ ≠ 0,\n\nif λ = 0.\n\nFigure 2-12 shows the Box-Cox transform for λ = 0 (the log transform), λ = 0.25, λ = 0.5 (a scaled and shifted version of the square root transform), λ = 0.75, and λ = 1.5. Setting λ to be less than 1 compresses the higher values, and setting λ higher than 1 has the opposite effect.\n\n24\n\n|\n\nChapter 2: Fancy Tricks with Simple Numbers",
      "page_number": 28
    },
    {
      "number": 5,
      "title": "Segment 5 (pages 39-46)",
      "start_page": 39,
      "end_page": 46,
      "detection_method": "topic_boundary",
      "content": "Figure 2-12. Box-Cox transforms for different values of λ\n\nThe Box-Cox formulation only works when the data is positive. For nonpositive data, one could shift the values by adding a fixed constant. When applying the Box-Cox transformation or a more general power transform, we have to determine a value for the parameter λ. This may be done via maximum likelihood (finding the λ that maxi‐ mizes the Gaussian likelihood of the resulting transformed signal) or Bayesian meth‐ ods. A full treatment of the usage of Box-Cox and general power transforms is outside the scope of this book. Interested readers may find more information on power transforms in Econometric Methods by Johnston and DiNardo (1997). Fortu‐ nately, SciPy’s stats package contains an implementation of the Box-Cox transfor‐ mation that includes finding the optimal transform parameter. Example 2-12 demonstrates its use on the Yelp reviews dataset.\n\nExample 2-12. Box-Cox transformation of Yelp business review counts\n\n>>> from scipy import stats\n\n# Continuing from the previous example, assume biz_df contains # the Yelp business reviews data. # The Box-Cox transform assumes that input data is positive. # Check the min to make sure. >>> biz_df['review_count'].min() 3\n\n# Setting input parameter lmbda to 0 gives us the log transform (without # constant offset)\n\nLog Transformation\n\n|\n\n25\n\n>>> rc_log = stats.boxcox(biz_df['review_count'], lmbda=0) # By default, the scipy implementation of Box-Cox transform finds the lambda # parameter that will make the output the closest to a normal distribution >>> rc_bc, bc_params = stats.boxcox(biz_df['review_count']) >>> bc_params -0.4106510862321085\n\nFigure 2-13 provides a visual comparison of the distributions of the original and transformed counts (see Example 2-13).\n\nExample 2-13. Visualizing the histograms of original, log transformed, and Box-Cox transformed counts\n\n>>> fig, (ax1, ax2, ax3) = plt.subplots(3,1) # original review count histogram >>> biz_df['review_count'].hist(ax=ax1, bins=100) >>> ax1.set_yscale('log') >>> ax1.tick_params(labelsize=14) >>> ax1.set_title('Review Counts Histogram', fontsize=14) >>> ax1.set_xlabel('') >>> ax1.set_ylabel('Occurrence', fontsize=14)\n\n# review count after log transform >>> biz_df['rc_log'].hist(ax=ax2, bins=100) >>> ax2.set_yscale('log') >>> ax2.tick_params(labelsize=14) >>> ax2.set_title('Log Transformed Counts Histogram', fontsize=14) >>> ax2.set_xlabel('') >>> ax2.set_ylabel('Occurrence', fontsize=14)\n\n# review count after optimal Box-Cox transform >>> biz_df['rc_bc'].hist(ax=ax3, bins=100) >>> ax3.set_yscale('log') >>> ax3.tick_params(labelsize=14) >>> ax3.set_title('Box-Cox Transformed Counts Histogram', fontsize=14) >>> ax3.set_xlabel('') >>> ax3.set_ylabel('Occurrence', fontsize=14)\n\n26\n\n|\n\nChapter 2: Fancy Tricks with Simple Numbers\n\nFigure 2-13. Box-Cox transformation of Yelp business review counts (bottom), com‐ pared to original (top) and log transformed (middle) histograms\n\nLog Transformation\n\n|\n\n27\n\nA probability plot, or probplot, is an easy way to visually compare an empirical distri‐ bution of data against a theoretical distribution. This is essentially a scatter plot of observed versus theoretical quantiles. Figure 2-14 shows the probplots of original and transformed Yelp review counts data against the normal distribution (see Example 2-14). Since the observed data is strictly positive and the Gaussian can be negative, the quantiles could never match up on the negative end. Thus, our focus is on the positive side. On this front, the original counts are obviously much more heavy-tailed than a normal distribution. (The ordered values go up to 4,000, whereas the theoretical quantiles only stretch to 4.) Both the plain log transform and the opti‐ mal Box-Cox transform bring the positive tail closer to normal. The optimal Box-Cox transform deflates the tail more than the log transform, as is evident from the fact that the tail flattens out under the red diagonal equivalence line.\n\nExample 2-14. Probability plots of original and transformed counts against the normal distribution\n\n>>> fig2, (ax1, ax2, ax3) = plt.subplots(3,1) >>> prob1 = stats.probplot(biz_df['review_count'], dist=stats.norm, plot=ax1) >>> ax1.set_xlabel('') >>> ax1.set_title('Probplot against normal distribution') >>> prob2 = stats.probplot(biz_df['rc_log'], dist=stats.norm, plot=ax2) >>> ax2.set_xlabel('') >>> ax2.set_title('Probplot after log transform') >>> prob3 = stats.probplot(biz_df['rc_bc'], dist=stats.norm, plot=ax3) >>> ax3.set_xlabel('Theoretical quantiles') >>> ax3.set_title('Probplot after Box-Cox transform')\n\n28\n\n|\n\nChapter 2: Fancy Tricks with Simple Numbers\n\nFigure 2-14. Comparing the distribution of raw and transformed review counts against the normal distribution\n\nFeature Scaling or Normalization Some features, such as latitude or longitude, are bounded in value. Other numeric features, such as counts, may increase without bound. Models that are smooth func‐ tions of the input, such as linear regression, logistic regression, or anything that involves a matrix, are affected by the scale of the input. Tree-based models, on the\n\nFeature Scaling or Normalization\n\n|\n\n29\n\nother hand, couldn’t care less. If your model is sensitive to the scale of input features, feature scaling could help. As the name suggests, feature scaling changes the scale of the feature. Sometimes people also call it feature normalization. Feature scaling is usually done individually to each feature. Next, we will discuss several types of com‐ mon scaling operations, each resulting in a different distribution of feature values.\n\nMin-Max Scaling Let x be an individual feature value (i.e., a value of the feature in some data point), and min(x) and max(x), respectively, be the minimum and maximum values of this feature over the entire dataset. Min-max scaling squeezes (or stretches) all feature values to be within the range of [0, 1]. Figure 2-15 demonstrates this concept. The formula for min-max scaling is:\n\nx˜ =\n\nx – min(x) max(x) – min(x)\n\nFigure 2-15. Illustration of min-max scaling\n\n30\n\n|\n\nChapter 2: Fancy Tricks with Simple Numbers\n\nStandardization (Variance Scaling) Feature standardization is defined as:\n\nx˜ =\n\nx – mean(x) sqrt(var(x))\n\nIt subtracts off the mean of the feature (over all data points) and divides by the var‐ iance. Hence, it can also be called variance scaling. The resulting scaled feature has a mean of 0 and a variance of 1. If the original feature has a Gaussian distribution, then the scaled feature does too. Figure 2-16 is an illustration of standardization.\n\nFigure 2-16. Illustration of feature standardization\n\nFeature Scaling or Normalization\n\n|\n\n31\n\nDon’t “Center” Sparse Data!\n\nUse caution when performing min-max scaling and standardiza‐ tion on sparse features. Both subtract a quantity from the original feature value. For min-max scaling, the shift is the minimum over all values of the current feature; for standardization, it is the mean. If the shift is not zero, then these two transforms can turn a sparse feature vector where most values are zero into a dense one. This in turn could create a huge computational burden for the classifier, depending on how it is implemented (not to mention that it would be horrendous if the representation now included every word that didn’t appear in a document!). Bag-of-words is a sparse representa‐ tion, and most classification libraries optimize for sparse inputs.\n\nℓ2 Normalization This technique normalizes (divides) the original feature value by what’s known as the ℓ2 norm, also known as the Euclidean norm. It’s defined as follows:\n\nx˜ =\n\nx ∥ x ∥ 2\n\nThe ℓ2 norm measures the length of the vector in coordinate space. The definition can be derived from the well-known Pythagorean theorem that gives us the length of the hypotenuse of a right triangle given the lengths of the sides:\n\n∥ x ∥ 2 = x1\n\n2 + x2\n\n2 2 + … + xm\n\nThe ℓ2 norm sums the squares of the values of the features across data points, then takes the square root. After ℓ2 normalization, the feature column has norm 1. This is also sometimes called ℓ2 scaling. (Loosely speaking, scaling means multiplying by a constant, whereas normalization could involve a number of operations.) Figure 2-17 illustrates ℓ2 normalization.\n\n32\n\n|\n\nChapter 2: Fancy Tricks with Simple Numbers",
      "page_number": 39
    },
    {
      "number": 6,
      "title": "Segment 6 (pages 47-56)",
      "start_page": 47,
      "end_page": 56,
      "detection_method": "topic_boundary",
      "content": "Figure 2-17. Illustration of ℓ2 feature normalization\n\nData Space Versus Feature Space\n\nNote that the illustration in Figure 2-17 is in data space, not feature space. One can also do ℓ2 normalization for the data point instead of the feature, which will result in data vectors with unit norm (norm of 1). See the discussion in “Bag-of-Words” on page 42 about the complementary nature of data vectors and feature vectors.\n\nNo matter the scaling method, feature scaling always divides the feature by a constant (known as the normalization constant). Therefore, it does not change the shape of the single-feature distribution. We’ll illustrate this with the online news article token counts (see Example 2-15).\n\nExample 2-15. Feature scaling example\n\n>>> import pandas as pd >>> import sklearn.preprocessing as preproc\n\n# Load the Online News Popularity dataset >>> df = pd.read_csv('OnlineNewsPopularity.csv', delimiter=', ')\n\n# Look at the original data - the number of words in an article >>> df['n_tokens_content'].as_matrix() array([ 219., 255., 211., ..., 442., 682., 157.])\n\n# Min-max scaling >>> df['minmax'] = preproc.minmax_scale(df[['n_tokens_content']])\n\nFeature Scaling or Normalization\n\n|\n\n33\n\n>>> df['minmax'].as_matrix() array([ 0.02584376, 0.03009205, 0.02489969, ..., 0.05215955, 0.08048147, 0.01852726])\n\n# Standardization - note that by definition, some outputs will be negative >>> df['standardized'] = preproc.StandardScaler().fit_transform(df[['n_tokens_content']]) >>> df['standardized'].as_matrix() array([-0.69521045, -0.61879381, -0.71219192, ..., -0.2218518 , 0.28759248, -0.82681689])\n\n# L2-normalization >>> df['l2_normalized'] = preproc.normalize(df[['n_tokens_content']], axis=0) >>> df['l2_normalized'].as_matrix() array([ 0.00152439, 0.00177498, 0.00146871, ..., 0.00307663, 0.0047472 , 0.00109283])\n\nWe can also visualize the distribution of data with different feature scaling methods (Figure 2-18). As Example 2-16 shows, unlike the log transform, feature scaling doesn’t change the shape of the distribution; only the scale of the data changes.\n\nExample 2-16. Plotting the histograms of original and scaled data\n\n>>> fig, (ax1, ax2, ax3, ax4) = plt.subplots(4,1) >>> fig.tight_layout() >>> df['n_tokens_content'].hist(ax=ax1, bins=100) >>> ax1.tick_params(labelsize=14) >>> ax1.set_xlabel('Article word count', fontsize=14) >>> ax1.set_ylabel('Number of articles', fontsize=14)\n\n>>> df['minmax'].hist(ax=ax2, bins=100) >>> ax2.tick_params(labelsize=14) >>> ax2.set_xlabel('Min-max scaled word count', fontsize=14) >>> ax2.set_ylabel('Number of articles', fontsize=14)\n\n>>> df['standardized'].hist(ax=ax3, bins=100) >>> ax3.tick_params(labelsize=14) >>> ax3.set_xlabel('Standardized word count', fontsize=14) >>> ax3.set_ylabel('Number of articles', fontsize=14)\n\n>>> df['l2_normalized'].hist(ax=ax4, bins=100) >>> ax4.tick_params(labelsize=14) >>> ax4.set_xlabel('L2-normalized word count', fontsize=14) >>> ax4.set_ylabel('Number of articles', fontsize=14)\n\n34\n\n|\n\nChapter 2: Fancy Tricks with Simple Numbers\n\nFigure 2-18. Original and scaled news article word counts—note that only the scale of the x-axis changes; the shape of the distribution stays the same with feature scaling\n\nFeature scaling is useful in situations where a set of input features differs wildly in scale. For instance, the number of daily visitors to a popular ecommerce site might be a hundred thousand, while the actual number of sales might be in the thousands. If both of those features are thrown into a model, then the model will need to balance its scale while figuring out what to do. Drastically varying scale in input features can lead to numeric stability issues for the model training algorithm. In those situations, it’s a good idea to standardize the features. Chapter 4 goes into detail about feature scaling in the context of handling natural text, including usage examples.\n\nInteraction Features A simple pairwise interaction feature is the product of two features. The analogy is the logical AND. It expresses the outcome in terms of pairs of conditions: “the pur‐ chase is coming from zip code 98121” AND “the user’s age is between 18 and 35.” Decision tree–based models get this for free, but generalized linear models often find interaction features very helpful.\n\nA simple linear model uses a linear combination of the individual input features x1, x2, ... xn to predict the outcome y:\n\nInteraction Features\n\n|\n\n35\n\ny = w1x1 + w2x2 + ... + wnxn\n\nAn easy way to extend the linear model is to include combinations of pairs of input features, like so:\n\ny = w1x1 + w2x2 + ... + wnxn + w1,1x1x1 + w1,2x1x2 + w1,3x1x3 + ...\n\nThis allows us to capture interactions between features, and hence these pairs are called interaction features. If x1 and x2 are binary, then their product x1x2 is the logical function x1 AND x2. Suppose the problem is to predict a customer’s preference based on their profile information. In our example, instead of making predictions based solely on the age or location of the user, interaction features allow the model to make predictions based on the user being of a certain age AND at a particular location.\n\nIn Example 2-17, we use pairwise interaction features from the UCI Online News Popularity dataset to predict the number of shares for each news article. As the results show, interaction features result in some lift in accuracy above singleton fea‐ tures. Both perform better than Example 2-9, which used as a single predictor the number of words in the body of the article (with or without a log transform).\n\nExample 2-17. Example of interaction features in prediction\n\n>>> from sklearn import linear_model >>> from sklearn.model_selection import train_test_split >>> import sklearn.preprocessing as preproc\n\n# Assume df is a Pandas DataFrame containing the UCI Online News Popularity dataset >>> df.columns Index(['url', 'timedelta', 'n_tokens_title', 'n_tokens_content', 'n_unique_tokens', 'n_non_stop_words', 'n_non_stop_unique_tokens', 'num_hrefs', 'num_self_hrefs', 'num_imgs', 'num_videos', 'average_token_length', 'num_keywords', 'data_channel_is_lifestyle', 'data_channel_is_entertainment', 'data_channel_is_bus', 'data_channel_is_socmed', 'data_channel_is_tech', 'data_channel_is_world', 'kw_min_min', 'kw_max_min', 'kw_avg_min', 'kw_min_max', 'kw_max_max', 'kw_avg_max', 'kw_min_avg', 'kw_max_avg', 'kw_avg_avg', 'self_reference_min_shares', 'self_reference_max_shares', 'self_reference_avg_sharess', 'weekday_is_monday', 'weekday_is_tuesday', 'weekday_is_wednesday', 'weekday_is_thursday', 'weekday_is_friday', 'weekday_is_saturday', 'weekday_is_sunday', 'is_weekend', 'LDA_00', 'LDA_01', 'LDA_02', 'LDA_03', 'LDA_04', 'global_subjectivity', 'global_sentiment_polarity', 'global_rate_positive_words', 'global_rate_negative_words', 'rate_positive_words', 'rate_negative_words', 'avg_positive_polarity', 'min_positive_polarity', 'max_positive_polarity', 'avg_negative_polarity', 'min_negative_polarity', 'max_negative_polarity', 'title_subjectivity', 'title_sentiment_polarity', 'abs_title_subjectivity',\n\n36\n\n|\n\nChapter 2: Fancy Tricks with Simple Numbers\n\n'abs_title_sentiment_polarity', 'shares'], dtype='object')\n\n# Select the content-based features as singleton features in the model, # skipping over the derived features >>> features = ['n_tokens_title', 'n_tokens_content', ... 'n_unique_tokens', 'n_non_stop_words', 'n_non_stop_unique_tokens', ... 'num_hrefs', 'num_self_hrefs', 'num_imgs', 'num_videos', ... 'average_token_length', 'num_keywords', 'data_channel_is_lifestyle', ... 'data_channel_is_entertainment', 'data_channel_is_bus', ... 'data_channel_is_socmed', 'data_channel_is_tech', ... 'data_channel_is_world']\n\n>>> X = df[features] >>> y = df[['shares']]\n\n# Create pairwise interaction features, skipping the constant bias term >>> X2 = preproc.PolynomialFeatures(include_bias=False).fit_transform(X) >>> X2.shape (39644, 170)\n\n# Create train/test sets for both feature sets >>> X1_train, X1_test, X2_train, X2_test, y_train, y_test = \\ ... train_test_split(X, X2, y, test_size=0.3, random_state=123)\n\n>>> def evaluate_feature(X_train, X_test, y_train, y_test): ... \"\"\"Fit a linear regression model on the training set and ... score on the test set\"\"\" ... model = linear_model.LinearRegression().fit(X_train, y_train) ... r_score = model.score(X_test, y_test) ... return (model, r_score)\n\n# Train models and compare score on the two feature sets >>> (m1, r1) = evaluate_feature(X1_train, X1_test, y_train, y_test) >>> (m2, r2) = evaluate_feature(X2_train, X2_test, y_train, y_test) >>> print(\"R-squared score with singleton features: %0.5f\" % r1) >>> print(\"R-squared score with pairwise features: %0.10f\" % r2) R-squared score with singleton features: 0.00924 R-squared score with pairwise features: 0.0113276523\n\nInteraction features are very simple to formulate, but they are expensive to use. The training and scoring time of a linear model with pairwise interaction features would go from O(n) to O(n2), where n is the number of singleton features.\n\nThere are a few ways around the computational expense of higher-order interaction features. One could perform feature selection on top of all of the interaction features. Alternatively, one could more carefully craft a smaller number of complex features.\n\nBoth strategies have their advantages and disadvantages. Feature selection employs computational means to select the best features for a problem. (This technique is not\n\nInteraction Features\n\n|\n\n37\n\nlimited to interaction features.) However, some feature selection techniques still require training multiple models with a large number of features.\n\nHandcrafted complex features can be expressive enough that only a small number of them are needed, which reduces the training time of the model—but the features themselves may be expensive to compute, which increases the computational cost of the model scoring stage. Good examples of handcrafted (or machine-learned) com‐ plex features may be found in Chapter 8. Let’s now look at some feature selection techniques.\n\nFeature Selection Feature selection techniques prune away nonuseful features in order to reduce the complexity of the resulting model. The end goal is a parsimonious model that is quicker to compute, with little or no degradation in predictive accuracy. In order to arrive at such a model, some feature selection techniques require training more than one candidate model. In other words, feature selection is not about reducing training time—in fact, some techniques increase overall training time—but about reducing model scoring time.\n\nRoughly speaking, feature selection techniques fall into three classes:\n\nFiltering\n\nFiltering techniques preprocess features to remove ones that are unlikely to be useful for the model. For example, one could compute the correlation or mutual information between each feature and the response variable, and filter out the features that fall below a threshold. Chapter 3 discusses examples of these techni‐ ques for text features. Filtering techniques are much cheaper than the wrapper techniques described next, but they do not take into account the model being employed. Hence, they may not be able to select the right features for the model. It is best to do prefiltering conservatively, so as not to inadvertently eliminate useful features before they even make it to the model training step.\n\nWrapper methods\n\nThese techniques are expensive, but they allow you to try out subsets of features, which means you won’t accidentally prune away features that are uninformative by themselves but useful when taken in combination. The wrapper method treats the model as a black box that provides a quality score of a proposed subset for features. There is a separate method that iteratively refines the subset.\n\nEmbedded methods\n\nThese methods perform feature selection as part of the model training process. For example, a decision tree inherently performs feature selection because it selects one feature on which to split the tree at each training step. Another exam‐ ple is the ℓ1 regularizer, which can be added to the training objective of any linear\n\n38\n\n|\n\nChapter 2: Fancy Tricks with Simple Numbers\n\nmodel. The ℓ1 regularizer encourages models that use a few features as opposed to a lot of features, so it’s also known as a sparsity constraint on the model. Embedded methods incorporate feature selection as part of the model training process. They are not as powerful as wrapper methods, but they are nowhere near as expensive. Compared to filtering, embedded methods select features that are specific to the model. In this sense, embedded methods strike a balance between computational expense and quality of results.\n\nA full treatment of feature selection is outside the scope of this book. Interested read‐ ers may refer to the survey paper by Guyon and Elisseeff (2003).\n\nSummary This chapter discussed a number of common numeric feature engineering techni‐ ques, such as quantization, scaling (a.k.a. normalization), log transforms (a type of power transform), and interaction features, and gave a brief summary of feature selection techniques, necessary for handling large quantities of interaction features. In statistical machine learning, all data eventually boils down to numeric features. Therefore, all roads lead to some kind of numeric feature engineering technique at the end. Keep these tools handy for the end game of feature engineering!\n\nBibliography Bertin-Mahieux, Thierry, Daniel P.W. Ellis, Brian Whitman, and Paul Lamere. “The Million Song Dataset.” Proceedings of the 12th International Society for Music Infor‐ mation Retrieval Conference (2011): 591–596.\n\nFernandes, K., P. Vinagre, and P. Cortez. “A Proactive Intelligent Decision Support System for Predicting the Popularity of Online News.” Proceedings of the 17th Portu‐ guese Conference on Artificial Intelligence (2015): 535–546.\n\nGuyon, Isabell, and André Elisseeff. “An Introduction to Variable and Feature Selec‐ tion.” Journal of Machine Learning Research Special Issue on Variable and Feature Selection 3 (2003): 1157–1182.\n\nJohnston, Jack, and John DiNardo. Econometric Methods. 4th ed. New York: McGraw Hill, 1997.\n\nSummary\n\n|\n\n39\n\nCHAPTER 3 Text Data: Flattening, Filtering, and Chunking\n\nWhat would you do if you were designing an algorithm to analyze the following para‐ graph of text?\n\nEmma knocked on the door. No answer. She knocked again and waited. There was a large maple tree next to the house. Emma looked up the tree and saw a giant raven perched at the treetop. Under the afternoon sun, the raven gleamed magnificently. Its beak was hard and pointed, its claws sharp and strong. It looked regal and imposing. It reigned the tree it stood on. The raven was looking straight at Emma with its beady black eyes. Emma felt slightly intimidated. She took a step back from the door and ten‐ tatively said, “Hello?”\n\nThe paragraph contains a lot of information. We know that it involves someone named Emma and a raven. There is a house and a tree, and Emma is trying to get into the house but sees the raven instead. The raven is magnificent and has noticed Emma, who is a little scared but is making an attempt at communication.\n\nSo, which parts of this trove of information are salient features that we should extract? To start with, it seems like a good idea to extract the names of the main char‐ acters, Emma and the raven. Next, it might also be good to note the setting of a house, a door, and a tree. And what about the descriptions of the raven? What about Emma’s actions—knocking on the door, taking a step back, and saying hello?\n\nThis chapter introduces the basics of feature engineering for text. We start out with bag-of-words, which is the simplest representation based on word count statistics. A very much related transformation is tf-idf, which is essentially a feature scaling tech‐ nique. It is pulled out into its own chapter (the next one) for a full discussion. The current chapter first talks about text extraction features, then delves into how to filter and clean those features.\n\n41\n\nBag-of-X: Turning Natural Text into Flat Vectors Whether constructing machine learning models or engineering features, it’s nice when the result is simple and interpretable. Simple things are easy to try, and inter‐ pretable features and models are easier to debug than complex ones. Simple and interpretable features do not always lead to the most accurate model, but it’s a good idea to start simple and only add complexity when absolutely necessary.\n\nFor text data, we can start with a list of word count statistics called a bag-of-words. A list of word counts makes no special effort to find the interesting entities, such as Emma or the raven. But those two words are repeatedly mentioned in our sample paragraph, and they have a higher count than a random word like “hello.” For simple tasks such as classifying a document, word count statistics often suffice. This techni‐ que can also be used in information retrieval, where the goal is to retrieve the set of documents that are relevant to an input text query. Both tasks are well served by word-level features because the presence or absence of certain words is a great indica‐ tor of the topic content of the document.\n\nBag-of-Words In bag-of-words (BoW) featurization, a text document is converted into a vector of counts. (A vector is just a collection of n numbers.) The vector contains an entry for every possible word in the vocabulary. If the word—say, “aardvark”—appears three times in the document, then the feature vector has a count of 3 in the position corre‐ sponding to that word. If a word in the vocabulary doesn’t appear in the document, then it gets a count of 0. For example, the text “it is a puppy and it is extremely cute” has the BoW representation shown in Figure 3-1.\n\nFigure 3-1. Turning raw text into a bag-of-words representation\n\n42\n\n|\n\nChapter 3: Text Data: Flattening, Filtering, and Chunking\n\nBag-of-words converts a text document into a flat vector. It is “flat” because it doesn’t contain any of the original textual structures. The original text is a sequence of words. But a bag-of-words has no sequence; it just remembers how many times each word appears in the text. Thus, as Figure 3-2 demonstrates, the ordering of words in the vector is not important, as long as it is consistent for all documents in the dataset. Neither does bag-of-words represent any concept of word hierarchy. For example, the concept of “animal” includes “dog,” “cat,” “raven,” etc. But in a bag-of-words rep‐ resentation, these words are all equal elements of the vector.\n\nFigure 3-2. Two equivalent BoW vectors\n\nWhat is important here is the geometry of data in feature space. In a bag-of-words vector, each word becomes a dimension of the vector. If there are n words in the vocabulary, then a document becomes a point1 in n-dimensional space. It is difficult to visualize the geometry of anything beyond two or three dimensions, so we will have to use our imagination. Figure 3-3 shows what our example sentence looks like in the two-dimensional feature space corresponding to the words “puppy” and “cute.”\n\n1 Sometimes people use the term “document vector.” The vector extends from the origin and ends at the speci‐\n\nfied point. For our purposes, “vector” and “point” are the same thing.\n\nBag-of-X: Turning Natural Text into Flat Vectors\n\n|\n\n43",
      "page_number": 47
    },
    {
      "number": 7,
      "title": "Segment 7 (pages 57-65)",
      "start_page": 57,
      "end_page": 65,
      "detection_method": "topic_boundary",
      "content": "Figure 3-3. Illustration of a sample text document in a 2D feature space\n\nFigure 3-4 shows three sentences in a 3D space corresponding to the words “puppy,” “extremely,” and “cute.”\n\nFigure 3-4. Three sentences in 3D feature space\n\nThese figures both depict data vectors in feature space. The axes denote individual words, which are features in the bag-of-words representation, and the points in space denote data points (text documents). Sometimes it is also informative to look at fea‐ ture vectors in data space. A feature vector contains the value of the feature in each data point. The axes denote individual data points, and the points denote feature vec‐ tors. Figure 3-5 shows an example. With bag-of-words featurization for text docu‐ ments, a feature is a word, and a feature vector contains the counts of this word in each document. In this way, a word is represented as a “bag-of-documents.” As we shall see in Chapter 4, these bag-of-documents vectors come from the matrix trans‐ pose of the bag-of-words vectors.\n\n44\n\n|\n\nChapter 3: Text Data: Flattening, Filtering, and Chunking\n\nFigure 3-5. Word vectors in document space\n\nBag-of-words is not perfect. Breaking down a sentence into single words can destroy the semantic meaning. For instance, “not bad” semantically means “decent” or even “good” (especially if you’re British). But “not” and “bad” constitute a floating nega‐ tion plus a negative sentiment. “toy dog” and “dog toy” could be very different things (unless it’s a dog toy of a toy dog), and the meaning is lost with the singleton words “toy” and “dog.” It’s easy to come up with many such examples. Bag-of-n-Grams, which we discuss next, alleviates some of the issue but is not a fundamental fix. It’s good to keep in mind that bag-of-words is a simple and useful heuristic, but it is far from a correct semantic understanding of text.\n\nBag-of-n-Grams Bag-of-n-Grams, or bag-of-n-grams, is a natural extension of bag-of-words. An n-gram is a sequence of n tokens. A word is essentially a 1-gram, also known as a unigram. After tokenization, the counting mechanism can collate individual tokens into word counts, or count overlapping sequences as n-grams. For example, the sen‐ tence “Emma knocked on the door” generates the n-grams “Emma knocked,” “knocked on,” “on the,” and “the door.”\n\nn-grams retain more of the original sequence structure of the text, and therefore the bag-of-n-grams representation can be more informative. However, this comes at a cost. Theoretically, with k unique words, there could be k2 unique 2-grams (also called bigrams). In practice, there are not nearly so many, because not every word can follow every other word. Nevertheless, there are usually a lot more distinct n-grams (n > 1) than words. This means that bag-of-n-grams is a much bigger and sparser fea‐ ture space. It also means that n-grams are more expensive to compute, store, and model. The larger n is, the richer the information, and the greater the cost.\n\nBag-of-X: Turning Natural Text into Flat Vectors\n\n|\n\n45\n\nTo illustrate how the number of n-grams grows with increasing n (see Figure 3-6), let’s compute n-grams on the Yelp reviews dataset. In Example 3-1, we compute the n-grams of the first 10,000 reviews using Pandas and the CountVectorizer trans‐ former in scikit-learn.\n\nExample 3-1. Computing n-grams\n\n>>> import pandas >>> import json >>> from sklearn.feature_extraction.text import CountVectorizer\n\n# Load the first 10,000 reviews >>> f = open('data/yelp/v6/yelp_academic_dataset_review.json') >>> js = [] >>> for i in range(10000): ... js.append(json.loads(f.readline())) >>> f.close() >>> review_df = pd.DataFrame(js)\n\n# Create feature transformers for unigrams, bigrams, and trigrams. # The default ignores single-character words, which is useful in practice because # it trims uninformative words, but we explicitly include them in this example for # illustration purposes. >>> bow_converter = CountVectorizer(token_pattern='(?u)\\\\b\\\\w+\\\\b') >>> bigram_converter = CountVectorizer(ngram_range=(2,2), ... token_pattern='(?u)\\\\b\\\\w+\\\\b') >>> trigram_converter = CountVectorizer(ngram_range=(3,3), ... token_pattern='(?u)\\\\b\\\\w+\\\\b')\n\n# Fit the transformers and look at vocabulary size >>> bow_converter.fit(review_df['text']) >>> words = bow_converter.get_feature_names() >>> bigram_converter.fit(review_df['text']) >>> bigrams = bigram_converter.get_feature_names() >>> trigram_converter.fit(review_df['text']) >>> trigrams = trigram_converter.get_feature_names() >>> print (len(words), len(bigrams), len(trigrams)) 26047 346301 847545\n\n# Sneak a peek at the n-grams themselves >>> words[:10] ['0', '00', '000', '0002', '00am', '00ish', '00pm', '01', '01am', '02']\n\n>>> bigrams[-10:] ['zucchinis at', 'zucchinis took', 'zucchinis we', 'zuma over', 'zuppa di', 'zuppa toscana', 'zuppe di',\n\n46\n\n|\n\nChapter 3: Text Data: Flattening, Filtering, and Chunking\n\n'zurich and', 'zz top', 'à la']\n\n>>> trigrams[:10] ['0 10 definitely', '0 2 also', '0 25 per', '0 3 miles', '0 30 a', '0 30 everything', '0 30 lb', '0 35 tip', '0 5 curry', '0 5 pork']\n\nFigure 3-6. Number of unique n-grams in the first 10,000 reviews of the Yelp dataset\n\nFiltering for Cleaner Features With words, how do we cleanly separate the signal from the noise? Through filtering, techniques that use raw tokenization and counting to generate lists of simple words or n-grams become more usable. Phrase detection, which we will discuss next, can be seen as a particular bigram filter. Here are a few more ways to perform filtering.\n\nFiltering for Cleaner Features\n\n|\n\n47\n\nStopwords Classification and retrieval do not usually require an in-depth understanding of the text. For instance, in the sentence “Emma knocked on the door,” the words “on” and “the” don’t change the fact that this sentence is about a person and a door. For coarse-grained tasks such as classification, the pronouns, articles, and prepositions may not add much value. The case may be very different in sentiment analysis, which requires a fine-grained understanding of semantics.\n\nThe popular Python NLP package NLTK contains a linguist-defined stopword list for many languages. (You will need to install NLTK and run nltk.download() to get all the goodies.) Various stopword lists can also be found on the web. For instance, here are some sample words from the English stopword list:\n\na, about, above, am, an, been, didn't, couldn't, i'd, i'll, itself, let's, myself, our, they, through, when's, whom, ...\n\nNote that the list contains apostrophes, and the words are uncapitalized. In order to use it as is, the tokenization process must not eat up apostrophes, and the words need to be converted to lowercase.\n\nFrequency-Based Filtering Stopword lists are a way of weeding out common words that make for vacuous fea‐ tures. There are other, more statistical ways of getting at the concept of “common words.” In collocation extraction, we see methods that depend on manual definitions, and those that use statistics. The same idea applies to word filtering. We can use fre‐ quency statistics here as well.\n\nFrequent words\n\nFrequency statistics are great for filtering out corpus-specific common words as well as general-purpose stopwords. For instance, the phrase “New York Times” and each of the individual words in it appear frequently in the New York Times Annotated Corpus dataset. Similarly, the word “house” appears often in the phrase “House of Commons” in the Hansard corpus of Canadian parliament debates, a dataset that is popularly used for statistical machine translation because it contains both an English and a French version of all documents. These words are meaningful in general, but not within those particular corpora. A typical stopword list will catch the general stopwords, but not corpus-specific ones.\n\nLooking at the most frequent words can reveal parsing problems and highlight nor‐ mally useful words that happen to appear too many times in the corpus. For example, Table 3-1 lists the 40 most frequent words in the Yelp reviews dataset. Here, fre‐ quency is based on the number of documents (reviews) they appear in, not their count within a document. As we can see, the list includes many stopwords. It also\n\n48\n\n|\n\nChapter 3: Text Data: Flattening, Filtering, and Chunking\n\ncontains some surprises. “s” and “t” are on the list because we used the apostrophe as a tokenization delimiter, and words such as “Mary’s” or “didn’t” got parsed as “Mary s” and “didn t.” Furthermore, the words “good,” “food,” and “great” each appear in around a third of the reviews, but we might want to keep them around because they are very useful for tasks such as sentiment analysis or business categorization.\n\nTable 3-1. Most frequent words in the Yelp reviews dataset\n\nRank Word Document frequency Rank Word Document frequency 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\n\nthe and a i to it of for is in was this but my that with on they you have\n\n1416058 1381324 1263126 1230214 1196238 1027835 1025638 993430 988547 961518 929703 844824 822313 786595 777045 775044 735419 720994 701015 692749\n\n21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40\n\nt not s had so place good at are food be we great were there here all if very out\n\n684049 649824 626764 620284 608061 601918 598393 596317 585548 562332 543588 537133 520634 516685 510897 481542 478490 475175 460796 460452\n\nIn practice, it helps to combine frequency-based filtering with a stopword list. There is also the tricky question of where to place the cutoff. Unfortunately there is no uni‐ versal answer. Most of the time the cutoff needs to be determined manually, and may need to be reexamined when the dataset changes.\n\nRare words\n\nDepending on the task, one might also need to filter out rare words. These might be truly obscure words, or misspellings of common words. To a statistical model, a word that appears in only one or two documents is more like noise than useful informa‐ tion. For example, suppose the task is to categorize businesses based on their Yelp reviews, and a single review contains the word “gobbledygook.” How would one tell, based on this one word, whether the business is a restaurant, a beauty salon, or a bar?\n\nFiltering for Cleaner Features\n\n|\n\n49\n\nEven if we knew that the business in this case happened to be a bar, it would probably be a mistake to classify as such for other reviews that contain the word “gobbledy‐ gook.”\n\nNot only are rare words unreliable as predictors, they also generate computational overhead. The set of 1.6 million Yelp reviews contains 357,481 unique words (toke‐ nized by space and punctuation characters), 189,915 of which appear in only one review, and 41,162 in two reviews. Over 60% of the vocabulary occurs rarely. This is a so-called heavy-tailed distribution, and it is very common in real-world data. The training time of many statistical machine learning models scales linearly with the number of features, and some models are quadratic or worse. Rare words incur a large computation and storage cost for not much additional gain.\n\nRare words can be easily identified and trimmed based on word count statistics. Alternatively, their counts can be aggregated into a special garbage bin, which can serve as an additional feature. Figure 3-7 demonstrates this representation on a short document that contains a bunch of usual words and two rare words, “gobbledygook” and “zylophant.” The usual words retain their own counts, which can be further fil‐ tered by stopword lists or other frequency-based methods. The rare words lose their identity and get grouped into a garbage bin feature.\n\nFigure 3-7. Bag-of-words feature vector with a garbage bin\n\nSince one won’t know which words are rare until the whole corpus has been counted, the garbage bin feature will need to be collected as a post-processing step.\n\nSince this book is about feature engineering, our focus is on features. But the concept of rarity also applies to data points. If a text document is very short, then it likely con‐ tains no useful information and should not be used when training a model. One must use caution when applying this rule, however. The Wikipedia dump contains many pages that are incomplete stubs, which are probably safe to filter out. Tweets, on the other hand, are inherently short, and require other featurization and modeling tricks.\n\n50\n\n|\n\nChapter 3: Text Data: Flattening, Filtering, and Chunking\n\nStemming One problem with simple parsing is that different variations of the same word get counted as separate words. For instance, “flower” and “flowers” are technically differ‐ ent tokens, and so are “swimmer,” “swimming,” and “swim,” even though they are very close in meaning. It would be nice if all of these different variations got mapped to the same word.\n\nStemming is an NLP task that tries to chop each word down to its basic linguistic word stem form. There are different approaches. Some are based on linguistic rules, others on observed statistics. A subclass of algorithms incorporate part-of-speech tag‐ ging and linguistic rules in a process known as lemmatization.\n\nMost stemming tools focus on the English language, though efforts are ongoing for other languages. The Porter stemmer is the most widely used free stemming tool for the English language. The original program is written in ANSI C, but many other packages have since wrapped it to provide access to other languages.\n\nHere is an example of running the Porter stemmer through the NLTK Python pack‐ age. As you can see, it handles a large number of cases, but it’s not perfect. The word “goes” is mapped to “goe,” while “go” is mapped to itself:\n\n>>> import nltk >>> stemmer = nltk.stem.porter.PorterStemmer() >>> stemmer.stem('flowers') u'flower' >>> stemmer.stem('zeroes') u'zero' >>> stemmer.stem('stemmer') u'stem' >>> stemmer.stem('sixties') u'sixti' >>> stemmer.stem('sixty') u'sixty' >>> stemmer.stem('goes') u'goe' >>> stemmer.stem('go') u'go'\n\nStemming does have a computation cost. Whether the end benefit outweighs the cost is application-dependent. It is also worth noting that stemming could hurt more than it helps. The words “new” and “news” have very different meanings, but both would be stemmed to “new.” Similar examples abound. For this reason, stemming is not always used.\n\nFiltering for Cleaner Features\n\n|\n\n51\n\nAtoms of Meaning: From Words to n-Grams to Phrases The concept of bag-of-words is straightforward. But how does a computer know what a word is? A text document is represented digitally as a string, which is basically a sequence of characters. One might also run into semi-structured text in the form of JSON blobs or HTML pages. But even with the added tags and structure, the basic unit is still a string. How does one turn a string into a sequence of words? This involves the tasks of parsing and tokenization, which we discuss next.\n\nParsing and Tokenization Parsing is necessary when the string contains more than plain text. For instance, if the raw data is a web page, an email, or a log of some sort, then it contains additional structure. One needs to decide how to handle the markup, the headers and footers, or the uninteresting sections of the log. If the document is a web page, then the parser needs to handle URLs. If it is an email, then fields like From, To, and Subject may require special handling—otherwise these headers will end up as normal words in the final count, which may not be useful.\n\nAfter light parsing, the plain-text portion of the document can go through tokeniza‐ tion. This turns the string—a sequence of characters—into a sequence of tokens. Each token can then be counted as a word. The tokenizer needs to know what charac‐ ters indicate that one token has ended and another is beginning. Space characters are usually good separators, as are punctuation characters. If the text contains tweets, then hash marks (#) should not be used as separators (also known as delimiters).\n\nSometimes, the analysis needs to operate on sentences instead of entire documents. For instance, n-grams, a generalization of the concept of a word, should not extend beyond sentence boundaries. More complex text featurization methods like word2vec also work with sentences or paragraphs. In these cases, one needs to first parse the document into sentences, then further tokenize each sentence into words.\n\nString Objects: More Than Meets the Eye\n\nString objects come in various encodings, like ASCII or Unicode. Plain English text can be encoded in ASCII. Most other languages require Unicode. If the document contains non-ASCII characters, then make sure that the tokenizer can handle that particular encod‐ ing. Otherwise, the results will be incorrect.\n\nCollocation Extraction for Phrase Detection A sequence of tokens immediately yields the list of words and n-grams. Semantically speaking, however, we are more used to understanding phrases, not n-grams. In computational natural language processing (NLP), the concept of a useful phrase is\n\n52\n\n|\n\nChapter 3: Text Data: Flattening, Filtering, and Chunking",
      "page_number": 57
    },
    {
      "number": 8,
      "title": "Segment 8 (pages 66-75)",
      "start_page": 66,
      "end_page": 75,
      "detection_method": "topic_boundary",
      "content": "called a collocation. In the words of Manning and Schütze (1999: 151), “A collocation is an expression consisting of two or more words that correspond to some conven‐ tional way of saying things.”\n\nCollocations are more meaningful than the sum of their parts. For instance, “strong tea” has a different meaning beyond “great physical strength” and “tea”; therefore, it is considered a collocation. The phrase “cute puppy,” on the other hand, means exactly the sum of its parts: “cute” and “puppy.” Thus, it is not considered a colloca‐ tion.\n\nCollocations do not have to be consecutive sequences. For example, the sentence “Emma knocked on the door” is considered to contain the collocation “knock door.” Hence, not every collocation is an n-gram. Conversely, not every n-gram is deemed a meaningful collocation.\n\nBecause collocations are more than the sum of their parts, their meaning cannot be adequately captured by individual word counts. Bag-of-words falls short as a repre‐ sentation. Bag-of-n-grams is also problematic because it captures too many meaning‐ less sequences (consider “this is” in the bag-of-n-grams example) and not enough of the meaningful ones (i.e., knock door).\n\nCollocations are useful as features. But how does one discover and extract them from text? One way is to predefine them. If we tried really hard, we could probably find comprehensive lists of idioms in various languages, and we could look through the text for any matches. It would be very expensive, but it would work. If the corpus is very domain specific and contains esoteric lingo, then this might be the preferred method. But the list would require a lot of manual curation, and it would need to be constantly updated for evolving corpora. For example, it probably wouldn’t be very realistic for analyzing tweets, or for blogs and articles.\n\nSince the advent of statistical NLP in the last two decades, people have opted more and more for statistical methods for finding phrases. Instead of establishing a fixed list of phrases and idiomatic sayings, statistical collocation extraction methods rely on the ever-evolving data to reveal the popular sayings of the day.\n\nFrequency-based methods\n\nA simple hack is to look at the most frequently occurring n-grams. The problem with this approach is that the most frequently occurring ones may not be the most useful ones. Table 3-2 shows the most popular bigrams (n = 2) in the entire Yelp reviews dataset. As we can see, the top 10 most frequently occurring bigrams by document count are very generic terms that don’t contain much meaning.\n\nAtoms of Meaning: From Words to n-Grams to Phrases\n\n|\n\n53\n\nTable 3-2. Most frequently occurring 2-grams in the Yelp reviews dataset\n\nBigram of the and the in the it was this place it s and i on the i was for the\n\nDocument count 450,849 426,346 397,821 396,713 344,800 341,090 332,415 325,044 285,012 276,946\n\nHypothesis testing for collocation extraction\n\nRaw popularity count is too crude of a measure. We have to find more clever statis‐ tics to be able to pick out meaningful phrases easily. The key idea is to ask whether two words appear together more often than they would by chance. The statistical machinery for answering this question is called a hypothesis test.\n\nHypothesis testing is a way to boil noisy data down to “yes” or “no” answers. It involves modeling the data as samples drawn from random distributions. The ran‐ domness means that one can never be 100% sure about the answer; there’s always the chance of an outlier. So, the answers are attached to a probability.\n\nFor example, the outcome of a hypothesis test might be “these two datasets come from the same distribution with 95% probability.” For a gentle introduction to hypothesis testing, see the Khan Academy’s tutorial on Hypothesis Testing and p- Values.\n\nIn the context of collocation extraction, many hypothesis tests have been proposed over the years. One of the most successful methods is based on the likelihood ratio test (Dunning, 1993). For a given pair of words, the method tests two hypotheses on the observed dataset. Hypothesis 1 (the null hypothesis) says that word 1 appears independently from word 2. Another way of saying this is that seeing word 1 has no bearing on whether we also see word 2. Hypothesis 2 (the alternate hypothesis) says that seeing word 1 changes the likelihood of seeing word 2. We take the alternate hypothesis to imply that the two words form a common phrase. Hence, the likelihood ratio test for phrase detection (a.k.a. collocation extraction) asks the following ques‐ tion: are the observed word occurrences in a given text corpus more likely to have been generated from a model where the two words occur independently from one another, or a model where the probabilities of the two words are entangled?\n\n54\n\n|\n\nChapter 3: Text Data: Flattening, Filtering, and Chunking\n\nThat is a mouthful. Let’s math it up a little. (Math is great at expressing things very precisely and concisely, but it does require a completely different parser than natural language.)\n\nWe can express the null hypothesis Hnull (independent) as P(w2 | w1) = P(w2 | not w1), and the alternate hypothesis Halternate (not independent) as P(w2 | w1) ≠ P(w2 | not w1).\n\nThe final statistic is the log of the ratio between the two:\n\nlog λ = log\n\nL (Data; Hnull) L (Data; Halternate)\n\n.\n\nThe likelihood function L(Data; H) represents the probability of seeing the word fre‐ quencies in the dataset under the independent or the not independent model for the word pair. In order to compute this probability, we have to make another assumption about how the data is generated. The simplest data generation model is the binomial model, where for each word in the dataset, we toss a coin, and we insert our special word if the coin comes up heads, and some other word otherwise. Under this strat‐ egy, the count of the number of occurrences of the special word follows a binomial distribution. The binomial distribution is completely determined by the total number of words, the number of occurrences of the word of interest, and the heads probabil‐ ity.\n\nThe algorithm for detecting common phrases through likelihood ratio test analysis proceeds as follows:\n\n1. Compute occurrence probabilities for all singleton words: P(w). 2. Compute conditional pairwise word occurrence probabilities for all unique bigrams: P(w2 | w1).\n\n3. Compute the likelihood ratio log λ for all unique bigrams. 4. Sort the bigrams based on their likelihood ratio. 5. Take the bigrams with the smallest likelihood ratio values as features.\n\nGetting a Grip on the Likelihood Ratio Test\n\nThe key is that what the test compares is not the probability parameters themselves, but rather the probability of seeing the observed data under those parameters (and an assumed data gen‐ eration model). Likelihood is one of the key principles of statistical learning, but it is definitely a brain-twister the first few times you see it. Once you work out the logic, it becomes intuitive.\n\nAtoms of Meaning: From Words to n-Grams to Phrases\n\n|\n\n55\n\nThere is another statistical approach that’s based on pointwise mutual information, but it is very sensitive to rare words, which are always present in real-world text cor‐ pora. Hence, it is not commonly used and we will not be demonstrating it here.\n\nNote that all of the statistical methods for collocation extraction, whether using raw frequency, hypothesis testing, or pointwise mutual information, operate by filtering a list of candidate phrases. The easiest and cheapest way to generate such a list is by counting n-grams. It’s possible to generate nonconsecutive sequences, but they are expensive to compute. In practice, even for consecutive n-grams, people rarely go beyond bigrams or trigrams because there are too many of them, even after filter‐ ing. To generate longer phrases, there are other methods such as chunking or com‐ bining with part-of-speech (PoS) tagging.\n\nChunking and part-of-speech tagging\n\nChunking is a bit more sophisticated than finding n-grams, in that it forms sequences of tokens based on parts of speech, using rule-based models.\n\nFor example, we might be most interested in finding all of the noun phrases in a problem where the entity (in this case the subject of a text) is the most interesting to us. In order to find this, we tokenize each word with a part of speech and then exam‐ ine the token’s neighborhood to look for part-of-speech groupings, or “chunks.” The models that map words to parts of speech are generally language specific. Several open source Python libraries, such as NLTK, spaCy, and TextBlob, have multiple lan‐ guage models available.\n\nTo illustrate how several libraries in Python make chunking using PoS tagging fairly straightforward, let’s use the Yelp reviews dataset again. In Example 3-2, we evaluate the parts of speech to find the noun phrases using both spaCy and TextBlob.\n\nExample 3-2. PoS tagging and chunking\n\n>>> import pandas as pd >>> import json\n\n# Load the first 10 reviews >>> f = open('data/yelp/v6/yelp_academic_dataset_review.json') >>> js = [] >>> for i in range(10): ... js.append(json.loads(f.readline())) >>> f.close() >>> review_df = pd.DataFrame(js)\n\n# First we'll walk through spaCy's functions >>> import spacy # preload the language model >>> nlp = spacy.load('en')\n\n56\n\n|\n\nChapter 3: Text Data: Flattening, Filtering, and Chunking\n\n# We can create a Pandas Series of spaCy nlp variables >>> doc_df = review_df['text'].apply(nlp)\n\n# spaCy gives us fine-grained parts of speech using (.pos_) # and coarse-grained parts of speech using (.tag_) >>> for doc in doc_df[4]: ... print([doc.text, doc.pos_, doc.tag_])\n\nGot VERB VBP a DET DT letter NOUN NN in ADP IN the DET DT mail NOUN NN last ADJ JJ week NOUN NN that ADJ WDT said VERB VBD Dr. PROPN NNP Goldberg PROPN NNP is VERB VBZ moving VERB VBG to ADP IN Arizona PROPN NNP to PART TO take VERB VB a DET DT new ADJ JJ position NOUN NN there ADV RB in ADP IN June PROPN NNP . PUNCT . SPACE SP He PRON PRP will VERB MD be VERB VB missed VERB VBN very ADV RB much ADV RB . PUNCT .\n\nSPACE SP I PRON PRP think VERB VBP finding VERB VBG a DET DT new ADJ JJ doctor NOUN NN in ADP IN NYC PROPN NNP\n\nAtoms of Meaning: From Words to n-Grams to Phrases\n\n|\n\n57\n\nthat ADP IN you PRON PRP actually ADV RB like INTJ UH might VERB MD almost ADV RB be VERB VB as ADV RB awful ADJ JJ as ADP IN trying VERB VBG to PART TO find VERB VB a DET DT date NOUN NN ! PUNCT .\n\n# spaCy also does some basic noun chunking for us >>> print([chunk for chunk in doc_df[4].noun_chunks]) [a letter, the mail, Dr. Goldberg, Arizona, a new position, June, He, I, a new doctor, NYC, you, a date]\n\n##### # We can do the same feature transformations using Textblob from textblob import TextBlob\n\n# The default tagger in TextBlob uses the PatternTagger, which is OK for our example. # You can also specify the NLTK tagger, which works better for incomplete sentences. >>> blob_df = review_df['text'].apply(TextBlob)\n\n>>> blob_df[4].tags [('Got', 'NNP'), ('a', 'DT'), ('letter', 'NN'), ('in', 'IN'), ('the', 'DT'), ('mail', 'NN'), ('last', 'JJ'), ('week', 'NN'), ('that', 'WDT'), ('said', 'VBD'), ('Dr.', 'NNP'), ('Goldberg', 'NNP'), ('is', 'VBZ'), ('moving', 'VBG'), ('to', 'TO'), ('Arizona', 'NNP'), ('to', 'TO'), ('take', 'VB'), ('a', 'DT'), ('new', 'JJ'), ('position', 'NN'),\n\n58\n\n|\n\nChapter 3: Text Data: Flattening, Filtering, and Chunking\n\n('there', 'RB'), ('in', 'IN'), ('June', 'NNP'), ('He', 'PRP'), ('will', 'MD'), ('be', 'VB'), ('missed', 'VBN'), ('very', 'RB'), ('much', 'JJ'), ('I', 'PRP'), ('think', 'VBP'), ('finding', 'VBG'), ('a', 'DT'), ('new', 'JJ'), ('doctor', 'NN'), ('in', 'IN'), ('NYC', 'NNP'), ('that', 'IN'), ('you', 'PRP'), ('actually', 'RB'), ('like', 'IN'), ('might', 'MD'), ('almost', 'RB'), ('be', 'VB'), ('as', 'RB'), ('awful', 'JJ'), ('as', 'IN'), ('trying', 'VBG'), ('to', 'TO'), ('find', 'VB'), ('a', 'DT'), ('date', 'NN')]\n\n>>> print([np for np in blob_df[4].noun_phrases]) ['got', 'goldberg', 'arizona', 'new position', 'june', 'new doctor', 'nyc']\n\nYou can see that the noun phrases found by each library are a little bit different. spaCy includes common words in the English language like “a” and “the,” while TextBlob removes these. This reflects a difference in the rules engines that drive what each library considers to be a noun phrase. You can also write your part-of-speech relationships to define the chunks you are seeking. See Bird et al. (2009) to really dive deep into chunking with Python from scratch.\n\nSummary The bag-of-words representation is simple to understand, easy to compute, and use‐ ful for classification and search tasks. But sometimes single words are too simplistic to encapsulate some information in the text. To fix this problem, people look to\n\nSummary\n\n|\n\n59\n\nlonger sequences. Bag-of-n-grams is a natural generalization of bag-of-words. The concept is still easy to understand, and it’s just as easy to compute as bag-of-words.\n\nBag-of-n-grams generates a lot more distinct n-grams. It increases the feature storage cost, as well as the computation cost of the model training and prediction stages. The number of data points remains the same, but the dimension of the feature space is now much larger. Hence, the data is much more sparse. The higher n is, the higher the storage and computation cost, and the sparser the data. For these reasons, longer n-grams do not always lead to improvements in model accuracy (or any other perfor‐ mance measure). People usually stop at n = 2 or 3. Longer n-grams are rarely used.\n\nOne way to combat the increase in sparsity and cost is to filter the n-grams and retain only the most meaningful phrases. This is the goal of collocation extraction. In theory, collocations (or phrases) could form nonconsecutive token sequences in the text. In practice, however, looking for nonconsecutive phrases has a much higher computation cost for not much gain. So, collocation extraction usually starts with a candidate list of bigrams and utilizes statistical methods to filter them.\n\nAll of these methods turn a sequence of text tokens into a disconnected set of counts. Sets have much less structure than sequences; they lead to flat feature vectors.\n\nIn this chapter, we dipped our toes into the water with simple text featurization tech‐ niques. These techniques turn a piece of natural language text—full of rich semantic structure—into a simple flat vector. We discussed a number of common filtering techniques to clean up the vector entries. We also introduced n-grams and colloca‐ tion extraction as methods that add a little more structure into the flat vectors. The next chapter goes into a lot more detail about another common text featurization trick called tf-idf. Subsequent chapters will discuss more methods for adding struc‐ ture back into a flat vector.\n\nBibliography Bird, Steven, Ewan Klein, and Edward Loper. Natural Language Processing with Python. Sebastopol, CA: O’Reilly Media, 2009.\n\nDunning, Ted. “Accurate Methods for the Statistics of Surprise and Coincidence.” ACM Journal of Computational Linguistics, special issue on using large corpora 19:1 (1993): 61–74.\n\nKhan Academy. “Hypothesis Testing and p-Values.” Retrieved from https:// www.khanacademy.org/math/probability/statistics-inferential/hypothesis-testing/v/ hypothesis-testing-and-p-values.\n\nManning, Christopher D. and Hinrich Schütze. Foundations of Statistical Natural Language Processing. Cambridge, MA: MIT Press, 1999.\n\n60\n\n|\n\nChapter 3: Text Data: Flattening, Filtering, and Chunking\n\nCHAPTER 4 The Effects of Feature Scaling: From Bag-of-Words to Tf-Idf\n\nA bag-of-words representation is simple to generate but far from perfect. If we count all words equally, then some words end up being emphasized more than we need. Recall our example of Emma and the raven from Chapter 3. We’d like a document representation that emphasizes the two main characters. The words “Emma” and “raven” both appear three times, but “the” appears a whopping eight times, “and” appears five times, and “it” and “was” both appear four times. The main characters do not stand out by simple frequency count alone. This is problematic.\n\nIt would also be nice to pick out words such as “magnificently,” “gleamed,” “intimi‐ dated,” “tentatively,” and “reigned,” because they help to set the overall tone of the paragraph. They indicate sentiment, which can be very valuable information to a data scientist. So, ideally, we’d like a representation that highlights meaningful words.\n\nTf-Idf : A Simple Twist on Bag-of-Words Tf-idf is a simple twist on the bag-of-words approach. It stands for term frequency– inverse document frequency. Instead of looking at the raw counts of each word in each document in a dataset, tf-idf looks at a normalized count where each word count is divided by the number of documents this word appears in. That is:\n\nbow(w, d) = # times word w appears in document d\n\ntf-idf(w, d) = bow(w, d) * N / (# documents in which word w appears)\n\nN is the total number of documents in the dataset. The fraction N / (# documents ...) is what’s known as the inverse document frequency. If a word appears in many\n\n61\n\ndocuments, then its inverse document frequency is close to 1. If a word appears in just a few documents, then the inverse document frequency is much higher.\n\nAlternatively, we can take a log transform instead using the raw inverse document frequency. Logarithm turns 1 into 0, and makes large numbers (those much greater than 1) smaller. (More on this later.)\n\nIf we define tf-idf as:\n\ntf-idf(w, d) = bow(w, d) * log (N / # documents in which word w appears)\n\nthen a word that appears in every single document will be effectively zeroed out, and a word that appears in very few documents will have an even larger count than before.\n\nLet’s look at some pictures to understand what it’s all about. Figure 4-1 shows a sim‐ ple example that contains four sentences: “it is a puppy,” “it is a cat,” “it is a kitten,” and “that is a dog and this is a pen.” We plot these sentences in the feature space of three words: “puppy,” “cat,” and “is.”\n\nFigure 4-1. Four sentences about dogs and cats\n\nNow let’s look at the same four sentences in tf-idf representation using the log trans‐ form for the inverse document frequency. Figure 4-2 shows the documents in feature space. Notice that the word “is” is effectively eliminated as a feature since it appears in all sentences in this dataset. Also, because they each appear in only one sentence out of the total four, the words “puppy” and “cat” are now counted higher than before (log(4) = 1.38... > 1). Thus, tf-idf makes rare words more prominent and\n\n62\n\n|\n\nChapter 4: The Effects of Feature Scaling: From Bag-of-Words to Tf-Idf",
      "page_number": 66
    },
    {
      "number": 9,
      "title": "Segment 9 (pages 76-83)",
      "start_page": 76,
      "end_page": 83,
      "detection_method": "topic_boundary",
      "content": "effectively ignores common words. It is closely related to the frequency-based filter‐ ing methods in Chapter 3, but much more mathematically elegant than placing hard cutoff thresholds.\n\nIntuition Behind Tf-Idf\n\nTf-idf makes rare words more prominent and effectively ignores common words.\n\nFigure 4-2. Tf-idf representation of the sentences in Figure 4-1\n\nPutting It to the Test Tf-idf transforms word count features through multiplication with a constant. Hence, it is an example of feature scaling, a concept introduced in Chapter 2. How well does feature scaling work in practice? Let’s compare the performance of scaled and unscaled features in a simple text classification task. Time for some code!\n\nIn Example 4-1, we revisit the Yelp reviews dataset. Round 6 of the Yelp dataset chal‐ lenge contains close to 1.6 million reviews of businesses in six US cities.\n\nExample 4-1. Loading and cleaning the Yelp reviews dataset in Python\n\n>>> import json >>> import pandas as pd\n\n# Load Yelp business data >>> biz_f = open('yelp_academic_dataset_business.json') >>> biz_df = pd.DataFrame([json.loads(x) for x in biz_f.readlines()]) >>> biz_f.close()\n\n# Load Yelp reviews data\n\nPutting It to the Test\n\n|\n\n63\n\n>>> review_file = open('yelp_academic_dataset_review.json') >>> review_df = pd.DataFrame([json.loads(x) for x in review_file.readlines()]) >>> review_file.close()\n\n# Pull out only Nightlife and Restaurants businesses >>> two_biz = biz_df[biz_df.apply(lambda x: 'Nightlife' in x['categories'] or ... 'Restaurants' in x['categories'], ... axis=1)]\n\n# Join with the reviews to get all reviews on the two types of business >>> twobiz_reviews = two_biz.merge(review_df, on='business_id', how='inner')\n\n# Trim away the features we won't use >>> twobiz_reviews = twobiz_reviews[['business_id', ... 'name', ... 'stars_y', ... 'text', ... 'categories']]\n\n# Create the target column--True for Nightlife businesses, and False otherwise >>> two_biz_reviews['target'] = \\ ... twobiz_reviews.apply(lambda x: 'Nightlife' in x['categories'], ... axis=1)\n\nCreating a Classification Dataset Let’s see whether we can use the reviews to categorize a business as either a restaurant or a nightlife venue. To save on training time, we can take a subset of the reviews. In this case, there is a large difference in review count between the two categories. This is called a class-imbalanced dataset. Imbalanced datasets are problematic for model‐ ing because the model will expend most of its effort fitting to the larger class. Since we have plenty of data in both classes, a good way to resolve the problem is to down‐ sample the larger class (restaurants) to be roughly the same size as the smaller class (nightlife). Here is an example workflow:\n\n1. Take a random sample of 10% of nightlife reviews and 2.1% of restaurant reviews (percentages chosen so the number of examples in each class is roughly equal).\n\n2. Create a 70/30 train-test split of this dataset. In this example, the training set ends up with 29,264 reviews, and the test set with 12,542 reviews.\n\n3. The training data contains 46,924 unique words; this is the number of features in the bag-of-words representation.\n\nExample 4-2 shows how we do this.\n\n64\n\n|\n\nChapter 4: The Effects of Feature Scaling: From Bag-of-Words to Tf-Idf\n\nExample 4-2. Creating a balanced classification dataset\n\n# Create a class-balanced subsample to play with >>> nightlife = \\ ... twobiz_reviews[twobiz_reviews.apply(lambda x: 'Nightlife' in x['categories'], ... axis=1)] >>> restaurants = \\ ... twobiz_reviews[twobiz_reviews.apply(lambda x: 'Restaurants' in x['categories'], ... axis=1)] >>> nightlife_subset = nightlife.sample(frac=0.1, random_state=123) >>> restaurant_subset = restaurants.sample(frac=0.021, random_state=123) >>> combined = pd.concat([nightlife_subset, restaurant_subset])\n\n# Split into training and test datasets >>> training_data, test_data = modsel.train_test_split(combined, ... train_size=0.7, ... random_state=123) >>> training_data.shape (29264, 5) >>> test_data.shape (12542, 5)\n\nScaling Bag-of-Words with Tf-Idf Transformation The goal of this experiment is to compare the effectiveness of bag-of-words, tf-idf, and ℓ2 normalization for linear classification. Note that doing tf-idf then ℓ2 normal‐ ization is the same as doing ℓ2 normalization alone. So, we only need to test three sets of features: bag-of-words, tf-idf, and word-wise ℓ2 normalization on top of bag-of- words.\n\nIn Example 4-3, we use scikit-learn’s CountVectorizer to convert the review text into a bag-of-words. All text featurization methods implicitly depend on a tokenizer, which is the module that converts a text string into a list of tokens (words). In this example, scikit-learn’s default tokenizing pattern looks for sequences of two or more alphanumeric characters. Punctuation marks are treated as token separators.\n\nExample 4-3. Transform features\n\n# Represent the review text as a bag-of-words >>> bow_transform = text.CountVectorizer() >>> X_tr_bow = bow_transform.fit_transform(training_data['text']) >>> X_te_bow = bow_transform.transform(test_data['text']) >>> len(bow_transform.vocabulary_) 46924\n\n>>> y_tr = training_data['target'] >>> y_te = test_data['target']\n\n# Create the tf-idf representation using the bag-of-words matrix\n\nPutting It to the Test\n\n|\n\n65\n\n>>> tfidf_trfm = text.TfidfTransformer(norm=None) >>> X_tr_tfidf = tfidf_trfm.fit_transform(X_tr_bow) >>> X_te_tfidf = tfidf_trfm.transform(X_te_bow)\n\n# Just for kicks, l2-normalize the bag-of-words representation >>> X_tr_l2 = preproc.normalize(X_tr_bow, axis=0) >>> X_te_l2 = preproc.normalize(X_te_bow, axis=0)\n\nFeature Scaling on the Test Set\n\nA subtle point about feature scaling is that it requires knowing fea‐ ture statistics that we most likely do not know in practice, such as the mean, variance, document frequency, ℓ2 norm, etc. In order to compute the tf-idf representation, we have to compute the inverse document frequencies based on the training data and use these sta‐ tistics to scale both the training and test data. In scikit-learn, fitting the feature transformer on the training data amounts to collecting the relevant statistics. The fitted transformer can then be applied to the test data.\n\nWhen we use training statistics to scale test data, the result will look a little fuzzy. Min-max scaling on the test set no longer neatly maps to 0 and 1. ℓ2 norms, mean, and variance statistics will all look a little off. This is less problematic than missing data. For instance, the test set may contain words that are not present in the training data, and we would have no document frequency to use for the new words. The com‐ mon solution is to simply drop the new words in the test set. This may seem irre‐ sponsible, but the model—trained on the training set—would not know what to do with these words anyway. A slightly less hacky option would be to explicitly learn a “garbage” word and map all low-frequency words to it, even within the training set, as discussed in “Rare words” on page 49.\n\nClassification with Logistic Regression Logistic regression is a simple, linear classifier. Due to its simplicity, it’s often a good first classifier to try. It takes a weighted combination of the input features, and passes it through a sigmoid function, which smoothly maps any real number to a number between 0 and 1. The function transforms a real number input, x, into a number between 0 and 1. It has one set of parameters, w, which represents the slope of the increase around the midpoint, 0.5. The intercept term b denotes the input value where the function output crosses the midpoint. A logistic classifier would predict the positive class if the sigmoid output is greater than 0.5, and the negative class otherwise. By varying w and b, one can control where that change in decision occurs, and how fast the decision should respond to changing input values around that point.\n\n66\n\n|\n\nChapter 4: The Effects of Feature Scaling: From Bag-of-Words to Tf-Idf\n\nFigure 4-3 illustrates the sigmoid function.\n\nFigure 4-3. Illustration of a sigmoid function\n\nNow let’s build some simple logistic regression classifiers on our various feature sets and see how they do (Example 4-4).\n\nExample 4-4. Training logistic regression classifiers with default parameters\n\n>>> def simple_logistic_classify(X_tr, y_tr, X_test, y_test, description): ... ### Helper function to train a logistic classifier and score on test data ... m = LogisticRegression().fit(X_tr, y_tr) ... s = m.score(X_test, y_test) ... print ('Test score with', description, 'features:', s) ... return m\n\n>>> m1 = simple_logistic_classify(X_tr_bow, y_tr, X_te_bow, y_te, 'bow') >>> m2 = simple_logistic_classify(X_tr_l2, y_tr, X_te_l2, y_te, 'l2-normalized') >>> m3 = simple_logistic_classify(X_tr_tfidf, y_tr, X_te_tfidf, y_te, 'tf-idf') Test score with bow features: 0.775873066497 Test score with l2-normalized features: 0.763514590974 Test score with tf-idf features: 0.743182905438\n\nParadoxically, the results show that the most accurate classifier is the one using BoW features. This was unexpected. As it turns out, the reason is that the classifiers are not well “tuned,” which is a common pitfall when comparing classifiers.\n\nPutting It to the Test\n\n|\n\n67\n\nTuning Logistic Regression with Regularization Logistic regression has a few bells and whistles. When the number of features is greater than the number of data points, the problem of finding the best model is said to be underdetermined. One way to fix this problem is by placing additional con‐ straints on the training process. This is known as regularization, and its technical details are discussed here.\n\nMost implementations of logistic regression allow for regularization. In order to use this functionality, one must specify a regularization parameter. Regularization parameters are hyperparameters that are not learned automatically in the model training process. Rather, they must be tuned on the problem at hand and given to the training algorithm. This process is known as hyperparameter tuning. (For details on how to evaluate machine learning models, see, e.g., Zheng [2015].) One basic method for tuning hyperparameters is called grid search: you specify a grid of hyperparameter values and the tuner programmatically searches for the best hyperparameter setting in the grid. After finding the best hyperparameter setting, you train a model on the entire training set using that setting, and use its performance on the test set as the final evaluation of this class of models.\n\nImportant: Tune Hyperparameters When Comparing Models\n\nIt’s essential to tune hyperparameters when comparing models or features. The default settings of a software package will always return a model. But unless the software performs automatic tuning under the hood, it is likely to return a suboptimal model based on suboptimal hyperparameter settings. The sensitivity of classifier performance to hyperparameter settings depends on the model and the distribution of training data. Logistic regression is relatively robust (or insensitive) to hyperparameter settings. Even so, it is necessary to find and use the right range of hyperparameters. Otherwise, the advantages of one model versus another may be solely due to tuning parameters, and will not reflect the actual behavior of the model or features.\n\nEven the best autotuning packages still require specifying the upper and lower limits of search, and finding those limits can take a few manual tries.\n\nIn the following example, we manually set the search grid of the logistic regulariza‐ tion parameter to {1e-5, 0.001, 0.1, 1, 10, 100}. The upper and lower bounds took a couple of tries to narrow down. The optimal hyperparameter settings for each feature set are given in Table 4-1.\n\n68\n\n|\n\nChapter 4: The Effects of Feature Scaling: From Bag-of-Words to Tf-Idf\n\nTable 4-1. Best hyperparameter settings for logistic regression on a sample of Yelp reviews of nightlife venues and restaurants\n\nℓ2 regularization 0.1 BoW ℓ2-normalized 10 Tf-idf\n\n0.001\n\nWe also want to test whether the difference in accuracy between tf-idf and BoW is due to noise. To this end, we use k-fold cross validation to simulate having multiple statistically independent datasets. It divides the dataset into k folds. The cross valida‐ tion process iterates through the folds, using all but one fold for training, and validat‐ ing the results on the fold that is held out.\n\nEstimating Variance via Resampling Modern statistical methods assume that the underlying data comes from a random distribution. The performance measurements of models derived from data are also subject to random noise. In this situation, it is always a good idea to take the meas‐ urement not just once, but multiple times, based on datasets of comparable statistics. This gives us a confidence interval for the measurement.\n\nk-fold cross validation is one such strategy. Resampling is another technique that generates multiple small samples from the same underlying dataset. See Zheng (2015) for more details on resampling.\n\nThe GridSearchCV function in scikit-learn runs a grid search with cross validation (see Example 4-5). Figure 4-4 shows a box-and-whiskers plot of the distribution of accuracy measurements for models trained on each of the feature sets. The middle line in the box marks the median accuracy, the box itself marks the region between the first and third quartiles, and the whiskers extend to the rest of the distribution.\n\nExample 4-5. Tuning logistic regression hyperparameters with grid search\n\n>>> import sklearn.model_selection as modsel\n\n# Specify a search grid, then do a 5-fold grid search for each of the feature sets >>> param_grid_ = {'C': [1e-5, 1e-3, 1e-1, 1e0, 1e1, 1e2]}\n\n# Tune classifier for bag-of-words representation >>> bow_search = modsel.GridSearchCV(LogisticRegression(), cv=5, ... param_grid=param_grid_) >>> bow_search.fit(X_tr_bow, y_tr)\n\n# Tune classifier for L2-normalized word vector\n\nPutting It to the Test\n\n|\n\n69\n\n>>> l2_search = modsel.GridSearchCV(LogisticRegression(), cv=5, ... param_grid=param_grid_) >>> l2_search.fit(X_tr_l2, y_tr)\n\n# Tune classifier for tf-idf >>> tfidf_search = modsel.GridSearchCV(LogisticRegression(), cv=5, ... param_grid=param_grid_) >>> tfidf_search.fit(X_tr_tfidf, y_tr)\n\n# Let's check out one of the grid search outputs to see how it went >>> bow_search.cv_results_ {'mean_fit_time': array([ 0.43648252, 0.94630651, 5.64090128, 15.31248307, 31.47010217, 42.44257565]), 'mean_score_time': array([ 0.00080056, 0.00392466, 0.00864897, 0 .00784755, 0.01192751, 0.0072515 ]), 'mean_test_score': array([ 0.57897075, 0.7518111 , 0.78283898, 0.77381766, 0.75515992, 0.73937261]), 'mean_train_score': array([ 0.5792185 , 0.76731652, 0.87697341, 0.94629064, 0.98357195, 0.99441294]), 'param_C': masked_array(data = [1e-05 0.001 0.1 1.0 10.0 100.0], mask = [False False False False False False], fill_value = ?), 'params': ({'C': 1e-05}, {'C': 0.001}, {'C': 0.1}, {'C': 1.0}, {'C': 10.0}, {'C': 100.0}), 'rank_test_score': array([6, 4, 1, 2, 3, 5]), 'split0_test_score': array([ 0.58028698, 0.75025624, 0.7799795 , 0.7726341 , 0.75247694, 0.74086095]), 'split0_train_score': array([ 0.57923964, 0.76860316, 0.87560871, 0.94434003, 0.9819308 , 0.99470312]), 'split1_test_score': array([ 0.5786776 , 0.74628396, 0.77669571, 0.76627371, 0 .74867589, 0.73176149]), 'split1_train_score': array([ 0.57917218, 0.7684849 , 0.87945837, 0.94822946, 0.98504976, 0.99538678]), 'split2_test_score': array([ 0.57816504, 0.75533914, 0.78472578, 0.76832394, 0.74799248, 0.7356911 ]), 'split2_train_score': array([ 0.57977019, 0.76613558, 0.87689548, 0.94566657, 0.98368288, 0.99397719]), 'split3_test_score': array([ 0.57894737, 0.75051265, 0.78332194, 0.77682843, 0.75768968, 0.73855092]), 'split3_train_score': array([ 0.57914745, 0.76678626, 0.87634546, 0.94558346, 0.98385443, 0.99474628]), 'split4_test_score': array([ 0.57877649, 0.75666439, 0.78947368, 0.78503076, 0.76896787, 0.75 ]), 'split4_train_score': array([ 0.57876303, 0.7665727 , 0.87655903, 0.94763369, 0.98334188, 0.99325132]), 'std_fit_time': array([ 0.03874582, 0.02297261, 1.18862097, 1.83901079, 4.21516797, 2.93444269]), 'std_score_time': array([ 0.00160112, 0.00605009, 0.00623053, 0.00698687,\n\n70\n\n|\n\nChapter 4: The Effects of Feature Scaling: From Bag-of-Words to Tf-Idf",
      "page_number": 76
    },
    {
      "number": 10,
      "title": "Segment 10 (pages 84-91)",
      "start_page": 84,
      "end_page": 91,
      "detection_method": "topic_boundary",
      "content": "0.00713112, 0.00570195]), 'std_test_score': array([ 0.00070799, 0.00375907, 0.00432957, 0.00668246, 0.00612049]), 'std_train_score': array([ 0.00032232, 0.00102466, 0.00131222, 0.00143229, 0.00100223, 0.00073252])}\n\n# Plot the cross validation results in a box-and-whiskers plot to # visualize and compare classifier performance >>> search_results = pd.DataFrame.from_dict({ ... 'bow': bow_search.cv_results_['mean_test_score'], ... 'tfidf': tfidf_search.cv_results_['mean_test_score'], ... 'l2': l2_search.cv_results_['mean_test_score'] ... })\n\n# Our usual matplotlib incantations. Seaborn is used here to make # the plot pretty. >>> import matplotlib.pyplot as plt >>> import seaborn as sns >>> sns.set_style(\"whitegrid\")\n\n>>> ax = sns.boxplot(data=search_results, width=0.4) >>> ax.set_ylabel('Accuracy', size=14) >>> ax.tick_params(labelsize=14)\n\nFigure 4-4. Distribution of classifier accuracy under each feature set and regularization setting—the accuracy is measured as the average accuracy from 5-fold cross validation\n\nTable 4-2 shows the average cross validation classifier accuracy for each hyperpara‐ meter setting. The asterisk in each column denotes the highest achieved accuracy for that feature set.\n\nTable 4-2. Average cross validation classifier accuracy scores\n\nRegularization parameter BoW 0.00001 0.001 0.1 1\n\n0.578971 0.751811 0.782839 * 0.773818\n\nℓ2-normalized Tf-idf 0.575724 0.575724 0.589120 0.734247\n\n0.721638 0.788648 * 0.763566 0.741150\n\nPutting It to the Test\n\n|\n\n71\n\nRegularization parameter BoW 10 100\n\n0.755160 0.739373\n\nℓ2-normalized Tf-idf 0.776756 * 0.761106\n\n0.721467 0.712309\n\nThe result for ℓ2 normalized features looks alarmingly bad in Figure 4-4. But don’t be fooled. The low accuracy numbers are due to very bad regularization parameter set‐ tings—concrete proof that suboptimal hyperparameters can lead to very wrong con‐ clusions. If we train a model using the best hyperparameter setting for each feature set, as in Example 4-6, the accuracy scores of the different feature sets are very close.\n\nExample 4-6. Final training and testing step to compare the different feature sets\n\n# Train a final model on the entire training set, using the best hyperparameter # settings found previously. Measure accuracy on the test set. >>> m1 = simple_logistic_classify(X_tr_bow, y_tr, X_te_bow, y_te, 'bow', ... _C=bow_search.best_params_['C']) >>> m2 = simple_logistic_classify(X_tr_l2, y_tr, X_te_l2, y_te, 'l2-normalized', ... _C=l2_search.best_params_['C']) >>> m3 = simple_logistic_classify(X_tr_tfidf, y_tr, X_te_tfidf, y_te, 'tf-idf', ... _C=tfidf_search.best_params_['C']) Test score with bow features: 0.78360708021 Test score with l2-normalized features: 0.780178599904 Test score with tf-idf features: 0.788470738319\n\nProper tuning improved the accuracy of all the feature sets, and all three now yield similar classification accuracy under regularized logistic regression. The accuracy score for the tf-idf model is slightly higher, but the difference is likely not statistically significant. These results are completely mystifying. If feature scaling doesn’t work better than vanilla bag-of-words, then why do it at all? Why all the hoopla if tf-idf doesn’t do anything? We’ll explore the answers to those questions in the next section.\n\nDeep Dive: What Is Happening? In order to understand the “why” behind the results, we have to look at how the fea‐ tures are being used by the model. For linear models like logistic regression, this hap‐ pens through an intermediary object called the data matrix.\n\nThe data matrix contains data points represented as fixed-length flat vectors. With bag-of-words vectors, the data matrix is also known as the document-term matrix. Figure 3-1 shows a bag-of-words vector in vector form, and Figure 4-1 illustrates four bag-of-words vectors in feature space. To form a document-term matrix, simply take the document vectors, lay them out flat, and stack them on top of one another. The columns represent all possible words in the vocabulary (see Figure 4-5). Since most documents contain only a small subset of all possible words, most of the entries in this matrix are zeros; it is a sparse matrix.\n\n72\n\n|\n\nChapter 4: The Effects of Feature Scaling: From Bag-of-Words to Tf-Idf\n\nFigure 4-5. An example document-term matrix of five documents and seven words\n\nFeature scaling methods are essentially column operations on the data matrix. In par‐ ticular, tf-idf and ℓ2 normalization both multiply the entire column (an n-gram fea‐ ture, for example) by a constant.\n\nTf-Idf = Column Scaling\n\nTf-idf and ℓ2 normalization are both column operations on the data matrix.\n\nAs discussed in Appendix A, training a linear classifier boils down to finding the best linear combination of features, which are column vectors of the data matrix. The sol‐ ution space is characterized by the column space and the null space of the data matrix. The quality of the trained linear classifier directly depends upon the null space and the column space of the data matrix. A large column space means that there is little linear dependency between the features, which is generally good. The null space contains “novel” data points that cannot be formulated as linear combina‐ tions of existing data; a large null space could be problematic. (A perusal of Appen‐ dix A is highly recommended for readers who would appreciate a review on concepts such as the linear decision surface, eigen decomposition, and the fundamental sub‐ spaces of a matrix.)\n\nHow do column scaling operations affect the column space and null space of the data matrix? The answer is “Not very much.” But there is a small chance that tf-idf and ℓ2 normalization could be different. We’ll look at why now.\n\nThe null space of the data matrix can be large for a couple of reasons. First, many datasets contain data points that are very similar to one another. This means the effective row space is small compared to the number of data points in the dataset. Second, the number of features can be much larger than the number of data points. Bag-of-words is particularly good at creating giant feature spaces. In our Yelp exam‐ ple, there are 47K features in 29K reviews in the training set. Moreover, the number of distinct words usually grows with the number of documents in the dataset, so\n\nDeep Dive: What Is Happening?\n\n|\n\n73\n\nadding more documents would not necessarily decrease the feature-to-data ratio or reduce the null space.\n\nWith bag-of-words, the column space is relatively small compared to the number of features. There could be words that appear roughly the same number of times in the same documents. This would lead to the corresponding column vectors being nearly linearly dependent, which leads to the column space being not as full rank as it could be (see Appendix A for the definition of full rank). This is called a rank deficiency. (Much like how animals can be deficient in vitamins and minerals, matrices can be deficient in rank, and the output space will not be as fluffy as it should.)\n\nRank-deficient row space and column space lead to the model being overly provi‐ sioned for the problem. The linear model outfits a weight parameter for each feature in the dataset. If the row and column spaces were full rank,1 then the model would allow us to generate any target vector in the output space. When they are rank defi‐ cient, the model has more degrees of freedom than it needs. This makes it harder to pin down a solution.\n\nCan feature scaling solve the rank deficiency problem of the data matrix? Let’s take a look.\n\nThe column space is defined as the linear combination of all column vectors (bold‐ face indicates a vector): a1v1 + a2v2 + ... + anvn. Feature scaling replaces a column vec‐ tor with a constant multiple, say ˜ 1 = c1. But we can still generate the original linear combination by just replacing a1 with a˜ 1 = a1/c. It appears that feature scaling does not change the rank of the column space. Similarly, feature scaling does not affect the rank of the null space, because one can counteract the scaled feature column by reverse scaling the corresponding entry in the weight vector.\n\nHowever, as usual, there is one catch. If the scalar is 0, then there is no way to recover the original linear combination; v1 is gone. If that vector is linearly independent from all the other columns, then we’ve effectively shrunk the column space and enlarged the null space.\n\nIf that vector is not correlated with the target output, then this is effectively pruning away noisy signals, which is a good thing. This turns out to be the key difference between tf-idf and ℓ2 normalization. ℓ2 normalization would never compute a norm of zero, unless the vector contains all zeros. If the vector is close to zero, then its norm is also close to zero. Dividing by the small norm would accentuate the vector and make it longer.\n\n1 Strictly speaking, the row space and column space for a rectangular matrix cannot both be full rank. The max‐\n\nimum rank for both subspaces is the smaller of m (the number of rows) and n (the number of columns).\n\n74\n\n|\n\nChapter 4: The Effects of Feature Scaling: From Bag-of-Words to Tf-Idf\n\nTf-idf, on the other hand, can generate scaling factors that are close to zero, as shown in Figure 4-2. This happens when the word is present in a large number of docu‐ ments in the training set. Such a word is likely not strongly correlated with the target vector. Pruning it away allows the solver to focus on the other directions in the col‐ umn space and find better solutions (although the improvement in accuracy will probably not be huge, because there are typically few noisy directions that are pruna‐ ble in this way).\n\nWhere feature scaling—both ℓ2 and tf-idf—does have a telling effect is on the conver‐ gence speed of the solver. This is a sign that the data matrix now has a much smaller condition number (the ratio between the largest and smallest singular values—see Appendix A for a full discussion of these terms). In fact, ℓ2 normalization makes the condition number nearly 1. But it’s not the case that the better the condition number, the better the solution. During this experiment, ℓ2 normalization converged much faster than either BoW or tf-idf. But it is also more sensitive to overfitting: it requires much more regularization and is more sensitive to the number of iterations during optimization.\n\nSummary In this chapter, we used tf-idf as an entry point into a detailed analysis of how feature transformations can affect the model (or not). Tf-idf is an example of feature scaling, so we contrasted its performance with that of another feature scaling method—ℓ2 normalization.\n\nThe results were not as one might have expected. Tf-idf and ℓ2 normalization do not improve the final classifier’s accuracy above plain bag-of-words. After acquiring some statistical modeling and linear algebra chops, we realize why: neither of them changes the column space of the data matrix.\n\nOne small difference between the two is that tf-idf can “stretch” the word count as well as “compress” it. In other words, it makes some counts bigger, and others close to zero. Therefore, tf-idf could altogether eliminate uninformative words.\n\nAlong the way, we also discovered another effect of feature scaling: it improves the condition number of the data matrix, making linear models much faster to train. Both ℓ2 normalization and tf-idf have this effect.\n\nTo summarize, the lesson is: the right feature scaling can be helpful for classification. The right scaling accentuates the informative words and downweights the common words. It can also improve the condition number of the data matrix. The right scaling is not necessarily uniform column scaling.\n\nThis story is a wonderful illustration of the difficulty of analyzing the effects of fea‐ ture engineering in the general case. Changing the features affects the training\n\nSummary\n\n|\n\n75\n\nprocess and the models that ensue. Linear models are the simplest models to under‐ stand, yet it still takes very careful experimentation methodology and a lot of deep mathematical knowledge to tease apart the theoretical and practical impacts. This would be mostly impossible with more complicated models or feature transforma‐ tions.\n\nBibliography Zheng, Alice. Evaluating Machine Learning Models. Sebastopol, CA: O’Reilly Media, 2015.\n\n76\n\n|\n\nChapter 4: The Effects of Feature Scaling: From Bag-of-Words to Tf-Idf\n\nCHAPTER 5 Categorical Variables: Counting Eggs in the Age of Robotic Chickens\n\nA categorical variable, as the name suggests, is used to represent categories or labels. For instance, a categorical variable could represent major cities in the world, the four seasons in a year, or the industry (oil, travel, technology) of a company. The number of category values is always finite in a real-world dataset. The values may be repre‐ sented numerically. However, unlike other numeric variables, the values of a catego‐ rical variable cannot be ordered with respect to one another. (Oil is neither greater than nor less than travel as an industry type.) They are called nonordinal.\n\nA simple question can serve as litmus test for whether something should be a catego‐ rical variable: “Does it matter how different two values are, or only that they are dif‐ ferent?” A stock price of $500 is five times higher than a price of $100. So, stock price should be represented by a continuous numeric variable. The industry of the com‐ pany (oil, travel, tech, etc.), on the other hand, should probably be categorical.\n\nLarge categorical variables are particularly common in transactional records. For instance, many web services track users using an ID, which is a categorical variable with hundreds to hundreds of millions of values, depending on the number of unique users of the service. The IP address of an internet transaction is another example of a large categorical variable. They are categorical variables because, even though user IDs and IP addresses are numeric, their magnitude is usually not relevant to the task at hand. For instance, the IP address might be relevant when doing fraud detection on individual transactions—some IP addresses or subnets may generate more frau‐ dulent transactions than others. But a subnet of 164.203.x.x is not inherently more fraudulent than 164.202.x.x; the numeric value of the subnet does not matter.\n\nThe vocabulary of a document corpus can be interpreted as a large categorical vari‐ able, with the categories being unique words. It can be computationally expensive to\n\n77\n\nrepresent so many distinct categories. If a category (e.g., word) appears multiple times in a data point (document), then we can represent it as a count, and represent all of the categories through their count statistics. This is called bin counting. We start this discussion with common representations of categorical variables, and eventually meander our way to a discussion of bin counting for large categorical variables, which are very common in modern datasets.\n\nEncoding Categorical Variables The categories of a categorical variable are usually not numeric.1 For example, eye color can be “black,” “blue,” “brown,” etc. Thus, an encoding method is needed to turn these nonnumeric categories into numbers. It is tempting to simply assign an integer, say from 1 to k, to each of k possible categories—but the resulting values would be orderable against each other, which should not be permissible for cate‐ gories. So, let’s look at some alternatives.\n\nOne-Hot Encoding A better method is to use a group of bits. Each bit represents a possible category. If the variable cannot belong to multiple categories at once, then only one bit in the group can be “on.” This is called one-hot encoding, and it is implemented in scikit- learn as sklearn.preprocessing.OneHotEncoder. Each of the bits is a feature. Thus, a categorical variable with k possible categories is encoded as a feature vector of length k. Table 5-1 shows an example.\n\nTable 5-1. One-hot encoding of a category of three cities\n\nSan Francisco\n\ne1 1\n\ne2 0\n\ne3 0\n\nNew York\n\n0\n\n1\n\n0\n\nSeattle\n\n0\n\n0\n\n1\n\nOne-hot encoding is very simple to understand, but it uses one more bit than is strictly necessary. If we see that k–1 of the bits are 0, then the last bit must be 1 because the variable must take on one of the k values. Mathematically, one can write this constraint as “the sum of all bits must be equal to 1”:\n\ne1 + e2 + ... + ek = 1\n\n1 In standard statistics literature, the technical term for the categories is levels. A categorical variable with two\n\ndistinct categories has two levels. But there are a number of other things in statistics that are also called levels, so we do not use that terminology here; instead we use the more colloquial and unambiguous term “cate‐ gories.”\n\n78\n\n|\n\nChapter 5: Categorical Variables: Counting Eggs in the Age of Robotic Chickens",
      "page_number": 84
    },
    {
      "number": 11,
      "title": "Segment 11 (pages 92-99)",
      "start_page": 92,
      "end_page": 99,
      "detection_method": "topic_boundary",
      "content": "Thus, we have a linear dependency on our hands. Linear dependent features, as we discovered in Chapter 4, are slightly annoying because they mean that the trained lin‐ ear models will not be unique. Different linear combinations of the features can make the same predictions, so we would need to jump through extra hoops to understand the effect of a feature on the prediction.\n\nDummy Coding The problem with one-hot encoding is that it allows for k degrees of freedom, while the variable itself needs only k–1. Dummy coding2 removes the extra degree of free‐ dom by using only k–1 features in the representation (see Table 5-2). One feature is thrown under the bus and represented by the vector of all zeros. This is known as the reference category. Dummy coding and one-hot encoding are both implemented in Pandas as pandas.get_dummies.\n\nTable 5-2. Dummy coding of a category of three cities\n\nSan Francisco\n\ne1 1\n\ne2 0\n\nNew York\n\n0\n\n1\n\nSeattle\n\n0\n\n0\n\nThe outcome of modeling with dummy coding is more interpretable than with one- hot encoding. This is easy to see in a simple linear regression problem. Suppose we have some data about apartment rental prices in three cities: San Francisco, New York, and Seattle (see Table 5-3).\n\nTable 5-3. Toy dataset of apartment prices in three cities\n\n0\n\nCity SF\n\nRent 3999\n\n1\n\nSF\n\n4000\n\n2\n\nSF\n\n4001\n\n3\n\nNYC\n\n3499\n\n4\n\nNYC\n\n3500\n\n5\n\nNYC\n\n3501\n\n6\n\nSeattle\n\n2499\n\n2 Curious readers might wonder why one is called coding and the other encoding. This is largely convention.\n\nMy guess is that one-hot encoding first became popular in electrical engineering, where information is enco‐ ded and decoded all the time. Dummy coding and effect coding, on the other hand, were invented in the sta‐ tistics community. Somehow the “en” didn’t make its way over the academic divide.\n\nEncoding Categorical Variables\n\n|\n\n79\n\n7\n\nCity Seattle\n\nRent 2500\n\n8\n\nSeattle\n\n2501\n\nWe can train a linear regressor to predict rental price based solely on the identity of the city (see Example 5-1).\n\nThe linear regression model can be written as:\n\ny = w1x1 + ... + wnxn\n\nIt is customary to fit an extra constant term called the intercept, so that y can be a nonzero value when the x’s are zeros:\n\ny = w1x1 + ... + wnxn + b\n\nExample 5-1. Linear regression on a categorical variable using one-hot and dummy codes\n\n>>> import pandas >>> from sklearn import linear_model\n\n# Define a toy dataset of apartment rental prices in # New York, San Francisco, and Seattle >>> df = pd.DataFrame({ ... 'City': ['SF', 'SF', 'SF', 'NYC', 'NYC', 'NYC', ... 'Seattle', 'Seattle', 'Seattle'], ... 'Rent': [3999, 4000, 4001, 3499, 3500, 3501, 2499, 2500, 2501] ... }) >>> df['Rent'].mean() 3333.3333333333335\n\n# Convert the categorical variables in the DataFrame to one-hot encoding # and fit a linear regression model >>> one_hot_df = pd.get_dummies(df, prefix=['city']) >>> one_hot_df Rent city_NYC city_SF city_Seattle 0 3999 0.0 1.0 0.0 1 4000 0.0 1.0 0.0 2 4001 0.0 1.0 0.0 3 3499 1.0 0.0 0.0 4 3500 1.0 0.0 0.0 5 3501 1.0 0.0 0.0 6 2499 0.0 0.0 1.0 7 2500 0.0 0.0 1.0 8 2501 0.0 0.0 1.0\n\n>>> model = linear_regression.LinearRegression()\n\n80\n\n|\n\nChapter 5: Categorical Variables: Counting Eggs in the Age of Robotic Chickens\n\n>>> model.fit(one_hot_df[['city_NYC', 'city_SF', 'city_Seattle']], ... one_hot_df['Rent']) >>> model.coef_ array([ 166.66666667, 666.66666667, -833.33333333]) >>> model.intercept_ 3333.3333333333335\n\n# Train a linear regression model on dummy code # Specify the 'drop_first' flag to get dummy coding >>> dummy_df = pd.get_dummies(df, prefix=['city'], drop_first=True) >>> dummy_df Rent city_SF city_Seattle 0 3999 1.0 0.0 1 4000 1.0 0.0 2 4001 1.0 0.0 3 3499 0.0 0.0 4 3500 0.0 0.0 5 3501 0.0 0.0 6 2499 0.0 1.0 7 2500 0.0 1.0 8 2501 0.0 1.0\n\n>>> model.fit(dummy_df[['city_SF', 'city_Seattle']], dummy_df['Rent']) >>> model.coef_ array([ 500., -1000.]) >>> model.intercept_ 3500.0\n\nWith one-hot encoding, the intercept term represents the global mean of the target variable, Rent, and each of the linear coefficients represents how much that city’s average rent differs from the global mean.\n\nWith dummy coding, the bias coefficient represents the mean value of the response variable y for the reference category, which in the example is the city NYC. The coef‐ ficient for the ith feature is equal to the difference between the mean response value for the ith category and the mean of the reference category.\n\nYou can see pretty clearly in Table 5-4 how these methods produce very different coefficients for linear models.\n\nTable 5-4. Linear regression learned coefficients\n\nx1\n\nOne-hot encoding 166.67 Dummy coding\n\n0\n\nx3 x2 666.67 –833.33\n\n500\n\n–1000\n\nb 3333.33\n\n3500\n\nEncoding Categorical Variables\n\n|\n\n81\n\nEffect Coding Yet another variant of categorical variable encoding is effect coding. Effect coding is very similar to dummy coding, with the difference that the reference category is now represented by the vector of all –1’s.\n\nTable 5-5. Effect coding of a categorical variable representing three cities\n\ne1 San Francisco 1 0 New York\n\ne2 0\n\n1\n\nSeattle\n\n–1 –1\n\nEffect coding is very similar to dummy coding, but results in linear regression models that are even simpler to interpret. Example 5-2 demonstrates what happens with effect coding as input. The intercept term represents the global mean of the target variable, and the individual coefficients indicate how much the means of the individ‐ ual categories differ from the global mean. (This is called the main effect of the cate‐ gory or level, hence the name “effect coding.”) One-hot encoding actually came up with the same intercept and coefficients, but in that case there are linear coefficients for each city. In effect coding, no single feature represents the reference category, so the effect of the reference category needs to be separately computed as the negative sum of the coefficients of all other categories. (See “FAQ: What is effect coding?” on the UCLA IDRE website for more details.)\n\nExample 5-2. Linear regression with effect coding\n\n>>> effect_df = dummy_df.copy() >>> effect_df.ix[3:5, ['city_SF', 'city_Seattle']] = -1.0 >>> effect_df Rent city_SF city_Seattle 0 3999 1.0 0.0 1 4000 1.0 0.0 2 4001 1.0 0.0 3 3499 -1.0 -1.0 4 3500 -1.0 -1.0 5 3501 -1.0 -1.0 6 2499 0.0 1.0 7 2500 0.0 1.0 8 2501 0.0 1.0\n\n>>> model.fit(effect_df[['city_SF', 'city_Seattle']], effect_df['Rent']) >>> model.coef_ array([ 666.66666667, -833.33333333]) >>> model.intercept_ 3333.3333333333335\n\n82\n\n|\n\nChapter 5: Categorical Variables: Counting Eggs in the Age of Robotic Chickens\n\nPros and Cons of Categorical Variable Encodings One-hot, dummy, and effect coding are very similar to one another. They each have pros and cons. One-hot encoding is redundant, which allows for multiple valid mod‐ els for the same problem. The nonuniqueness is sometimes problematic for interpre‐ tation, but the advantage is that each feature clearly corresponds to a category. Moreover, missing data can be encoded as the all-zeros vector, and the output should be the overall mean of the target variable.\n\nDummy coding and effect coding are not redundant. They give rise to unique and interpretable models. The downside of dummy coding is that it cannot easily handle missing data, since the all-zeros vector is already mapped to the reference category. It also encodes the effect of each category relative to the reference category, which may look strange.\n\nEffect coding avoids this problem by using a different code for the reference category, but the vector of all –1’s is a dense vector, which is expensive for both storage and computation. For this reason, popular ML software packages such as Pandas and scikit-learn have opted for dummy coding or one-hot encoding instead of effect coding.\n\nAll three encoding techniques break down when the number of categories becomes very large. Different strategies are needed to handle extremely large categorical variables.\n\nDealing with Large Categorical Variables Automated data collection on the internet can generate large categorical variables. This is common in applications such as targeted advertising and fraud detection.\n\nIn targeted advertising, the task is to match a user with a set of ads. Features include the user ID, the website domain for the ad, the search query, the current page, and all possible pairwise conjunctions of those features. (The query is a text string that can be chopped up and turned into the usual text features. However, queries are generally short and are often composed of phrases, so the best course of action in this case is usually to keep them intact, or pass them through a hash function to make storage and comparisons easier. We will discuss hashing in more detail later.) Each of these is a very large categorical variable. The challenge is to find a good feature representa‐ tion that is memory efficient, yet produces accurate models that are fast to train.\n\nExisting solutions can be categorized (haha) thus:\n\n1. Do nothing fancy with the encoding. Use a simple model that is cheap to train. Feed one-hot encoding into a linear model (logistic regression or linear support vector machine) on lots of machines.\n\nDealing with Large Categorical Variables\n\n|\n\n83\n\n2. Compress the features. There are two choices:\n\na. Feature hashing, popular with linear models b. Bin counting, popular with linear models as well as trees\n\nUsing the vanilla one-hot encoding is a valid option. For Microsoft’s search advertis‐ ing engine, Graepel et al. (2010) report using such binary-valued features in a Baye‐ sian probit regression model that can be trained online using simple updates. Meanwhile, other groups argue for the compression approach. Researchers from Yahoo! swear by feature hashing (Weinberger et al., 2009), though McMahan et al. (2013) experimented with feature hashing on Google’s advertising engine and did not find significant improvements. Yet other folks at Microsoft are taken with the idea of bin counting (Bilenko, 2015).\n\nAs we shall see, all of these ideas have pros and cons. We will first describe the solu‐ tions themselves, then discuss their trade-offs.\n\nFeature Hashing A hash function is a deterministic function that maps a potentially unbounded inte‐ ger to a finite integer range [1, m]. Since the input domain is potentially larger than the output range, multiple numbers may get mapped to the same output. This is called a collision. A uniform hash function ensures that roughly the same number of numbers are mapped into each of the m bins.\n\nVisually, we can think of a hash function as a machine that intakes numbered balls (keys) and routes them to one of m bins. Balls with the same number will always get routed to the same bin (see Figure 5-1). This maintains the feature space while reduc‐ ing the storage and processing time during machine learning training and evaluation cycles.\n\nHash functions can be constructed for any object that can be represented numerically (which is true for any data that can be stored on a computer): numbers, strings, com‐ plex structures, etc.\n\n84\n\n|\n\nChapter 5: Categorical Variables: Counting Eggs in the Age of Robotic Chickens\n\nFigure 5-1. Hash functions map keys to bins\n\nWhen there are very many features, storing the feature vector could take up a lot of space. Feature hashing compresses the original feature vector into an m-dimensional vector by applying a hash function to the feature ID, as shown in Example 5-3. For instance, if the original features were words in a document, then the hashed version would have a fixed vocabulary size of m, no matter how many unique words there are in the input.\n\nExample 5-3. Feature hashing for word features\n\n>>> def hash_features(word_list, m): ... output = [0] * m ... for word in word_list: ... index = hash_fcn(word) % m ... output[index] += 1 ... return output\n\nAnother variation of feature hashing adds a sign component, so that counts are either added to or subtracted from the hashed bin (see Example 5-4). Statistically speaking, this ensures that the inner products between hashed features are equal in expectation to those of the original features.\n\nExample 5-4. Signed feature hashing\n\n>>> def hash_features(word_list, m): ... output = [0] * m\n\nDealing with Large Categorical Variables\n\n|\n\n85\n\n... for word in word_list: ... index = hash_fcn(word) % m ... sign_bit = sign_hash(word) % 2 ... if (sign_bit == 0): ... output[index] -= 1 ... else: ... output[index] += 1 ... return output\n\nThe value of the inner product after hashing is within O( 1 m) of the original inner product, so the size of the hash table m can be selected based on acceptable errors. In practice, picking the right m could take some trial and error.\n\nFeature hashing can be used for models that involve the inner product of feature vec‐ tors and coefficients, such as linear models and kernel methods. It has been demon‐ strated to be successful in the task of spam filtering (Weinberger et al., 2009). In the case of targeted advertising, McMahan et al. (2013) report not being able to get the prediction errors down to an acceptable level unless m is on the order of billions, which does not constitute enough saving in space.\n\nOne downside to feature hashing is that the hashed features, being aggregates of orig‐ inal features, are no longer interpretable.\n\nIn Example 5-5, we use the Yelp reviews dataset to demonstrate storage and inter‐ pretability trade-offs using scikit-learn’s FeatureHasher.\n\nExample 5-5. Feature hashing (a.k.a. “the hashing trick”)\n\n>>> import pandas as pd >>> import json\n\n# Load the first 10,000 reviews >>> f = open('yelp_academic_dataset_review.json') >>> js = [] >>> for i in range(10000): ... js.append(json.loads(f.readline())) >>> f.close() >>> review_df = pd.DataFrame(js)\n\n# Define m as equal to the unique number of business_ids >>> m = len(review_df.business_id.unique()) >>> m 528\n\n>>> from sklearn.feature_extraction import FeatureHasher >>> h = FeatureHasher(n_features=m, input_type='string') >>> f = h.transform(review_df['business_id'])\n\n# How does this affect feature interpretability?\n\n86\n\n|\n\nChapter 5: Categorical Variables: Counting Eggs in the Age of Robotic Chickens",
      "page_number": 92
    },
    {
      "number": 12,
      "title": "Segment 12 (pages 100-107)",
      "start_page": 100,
      "end_page": 107,
      "detection_method": "topic_boundary",
      "content": ">>> review_df['business_id'].unique().tolist()[0:5] ['vcNAWiLM4dR7D2nwwJ7nCA', 'UsFtqoBl7naz8AVUBZMjQQ', 'cE27W9VPgO88Qxe4ol6y_g', 'HZdLhv6COCleJMo7nPl-RA', 'mVHrayjG3uZ_RLHkLj-AMg']\n\n>>> f.toarray() array([[ 0., 0., 0., ..., 0., 0., 0.], [ 0., 0., 0., ..., 0., 0., 0.], [ 0., 0., 0., ..., 0., 0., 0.], ..., [ 0., 0., 0., ..., 0., 0., 0.], [ 0., 0., 0., ..., 0., 0., 0.], [ 0., 0., 0., ..., 0., 0., 0.]])\n\n# Not great. BUT, let's see the storage size of our features. >>> from sys import getsizeof >>> print('Our pandas Series, in bytes: ', getsizeof(review_df['business_id'])) >>> print('Our hashed numpy array, in bytes: ', getsizeof(f)) Our pandas Series, in bytes: 790104 Our hashed numpy array, in bytes: 56\n\nWe can clearly see how using feature hashing will benefit us computationally, sacri‐ ficing immediate user interpretability. This is an easy trade-off to accept when pro‐ gressing from data exploration and visualization into a machine learning pipeline for large datasets.\n\nBin Counting Bin counting is one of the perennial rediscoveries in machine learning. It has been reinvented and used in a variety of applications, from ad click-through rate predic‐ tion to hardware branch prediction (Yeh and Patt, 1991; Lee et al., 1998; Chen et al., 2009; Li et al., 2010). Yet because it is a feature engineering technique and not a mod‐ eling or optimization method, there is no research paper on the topic. The most detailed description of the technique can be found in Misha Bilenko’s (2015) blog post “Big Learning Made Easy—with Counts!” and the associated slides.\n\nThe idea of bin counting is deviously simple: rather than using the value of the cate‐ gorical variable as the feature, instead use the conditional probability of the target under that value. In other words, instead of encoding the identity of the categorical value, we compute the association statistics between that value and the target that we wish to predict. For those familiar with naive Bayes classifiers, this statistic should ring a bell, because it is the conditional probability of the class under the assumption that all features are independent. It is best illustrated with an example (see Table 5-6).\n\nDealing with Large Categorical Variables\n\n|\n\n87\n\nTable 5-6. Example of bin-counting features (reproduced from “Big Learning Made Easy— with Counts!” with permission)\n\nUser\n\nNumber of clicks\n\nAlice 5\n\nBob\n\n20\n\n… 2 Joe\n\nNumber of nonclicks 120\n\n230\n\n3\n\nProbability of click 0.0400\n\n0.0800\n\n0.400\n\nQueryHash, AdDomain 0x598fd4fe, foo.com 0x50fa3cc0, bar.org … 0x437a45e1, qux.net\n\nNumber of clicks 5,000\n\n100\n\n6\n\nNumber of nonclicks 30,000\n\n900\n\n18\n\nProbability of click 0.167\n\n0.100\n\n… 0.250\n\nBin counting assumes that historical data is available for computing the statistics. Table 5-6 contains aggregated historical counts for each possible value of the catego‐ rical variables. Based on the number of times the user “Alice” has clicked on any ad and the number of times she has not clicked, we can calculate the probability of her clicking on any ad. Similarly, we can compute the probability of a click for any query–ad domain combination. At training time, every time we see “Alice,” we can use her probability of click as the input feature to the model. The same goes for QueryHash–AdDomain pairs like “0x437a45e1, qux.net.”\n\nSuppose there were 10,000 users. One-hot encoding would generate a sparse vector of length 10,000, with a single 1 in the column that corresponds to the value of the cur‐ rent data point. Bin counting would encode all 10,000 binary columns as a single fea‐ ture with a real value between 0 and 1.\n\nWe can include other features in addition to the historical click-through probability: the raw counts themselves (number of clicks and nonclicks), the log-odds ratio, or any other derivatives of probability. Our example here is for predicting ad click- through rates, but the technique readily applies to general binary classification. It can also be readily extended to multiclass classification using the usual techniques to extend binary classifiers to multiclass; i.e., via one-against-many odds ratios or other multiclass label encodings.\n\nOdds Ratio and Log Odds Ratio for Bin Counting The odds ratio is usually defined between two binary variables. It looks at their strength of association by asking the question, “How much more likely is it for Y to be true when X is true?” For instance, we might ask, “How much more likely is Alice to click on an ad than the general population?” Here, X is the binary variable “Alice is the current user,” and Y is the variable “click on ad or not.” The computation uses what’s called the two-way contingency table (basically, four numbers that correspond to the four possible combinations of X and Y), as seen in Table 5-7.\n\n88\n\n|\n\nChapter 5: Categorical Variables: Counting Eggs in the Age of Robotic Chickens\n\nTable 5-7. Contingency table for ad click and user\n\nClick 5\n\nAlice Not Alice 995 Total\n\n1,000\n\nNonclick 120\n\n18,880\n\n19,000\n\nTotal 125\n\n19,875\n\n20,000\n\nGiven an input variable X and a target variable Y, the odds ratio is defined as:\n\nodds ratio =\n\nP(Y = 1 | X = 1)/ P(Y = 0 | X = 1) P(Y = 1 | X = 0)/ P(Y = 0 | X = 0)\n\nIn our example, this translates as the ratio between “how much more likely is it that Alice clicks on an ad rather than does not click” and “how much more likely is it that other people click rather than not click.” The number, in this case, is:\n\nodds ratio (user, ad click) =\n\n(5/125)/(120/125) (995/19,875)/(18,880/19,875)\n\n= 0.7906\n\nMore simply, we can just look at the numerator, which examines how much more likely it is that a single user (Alice) clicks on an ad versus not clicking. This is suitable for large categorical variables with many values, not just two:\n\nodds ratio (Alice, ad click) =\n\n5/125 120/125\n\n= 0.04166\n\nProbability ratios can easily become very small or very large. (For instance, there will be users who almost never click on ads, and perhaps users who click on ads much more frequently than not.) The log transform again comes to our rescue. Another useful property of the logarithm is that it turns a division into a subtraction:\n\nlog-odds ratio (Alice, ad click) = log( 5\n\n125) – log( 120\n\n125) = – 3.178\n\nIn short, bin counting converts a categorical variable into statistics about the value. It turns a large, sparse, binary representation of the categorical variable, such as that produced by one-hot encoding, into a very small, dense, real-valued numeric repre‐ sentation (Figure 5-2).\n\nDealing with Large Categorical Variables\n\n|\n\n89\n\nFigure 5-2. An illustration of one-hot encoding versus bin-counting statistics for catego‐ rical variables\n\nIn terms of implementation, bin counting requires storing a map between each cate‐ gory and its associated counts. (The rest of the statistics can be derived on the fly from the raw counts.) Hence it requires O(k) space, where k is the number of unique values of the categorical variable.\n\nTo illustrate bin counting in practice, we’ll use data from a Kaggle competition hos‐ ted by Avazu. Here are some relevant statistics about the dataset:\n\nThere are 24 variables, including click, a binary click/no click counter, and device_id, which tracks which device an ad was displayed on.\n\nThe full dataset contains 40,428,967 observations, with 2,686,408 unique devices.\n\nThe aim of the Avazu competition was to predict click-through rate using ad data, but we will use the dataset to demonstrate how bin counting can greatly reduce the feature space for large amounts of streaming data (see Example 5-6).\n\nExample 5-6. Bin-counting example\n\n>>> import pandas as pd\n\n# train_subset data is first 10K rows of 6+GB set >>> df = pd.read_csv('data/train_subset.csv')\n\n# How many unique features should we have after? >>> len(df['device_id'].unique()) 7201\n\n# For each category, we want to calculate: # Theta = [counts, p(click), p(no click), p(click)/p(no click)]\n\n>>> def click_counting(x, bin_column): ... clicks = pd.Series(x[x['click'] > 0][bin_column].value_counts(),\n\n90\n\n|\n\nChapter 5: Categorical Variables: Counting Eggs in the Age of Robotic Chickens\n\n... name='clicks') ... no_clicks = pd.Series(x[x['click'] < 1][bin_column].value_counts(), ... name='no_clicks')\n\n... counts = pd.DataFrame([clicks,no_clicks]).T.fillna('0') ... counts['total_clicks'] = counts['clicks'].astype('int64') + ... counts['no_clicks'].astype('int64') ... return counts\n\n>>> def bin_counting(counts): ... counts['N+'] = counts['clicks'] ... .astype('int64') ... .divide(counts['total_clicks'].astype('int64')) ... counts['N-'] = counts['no_clicks'] ... .astype('int64') ... .divide(counts['total_clicks'].astype('int64')) ... counts['log_N+'] = counts['N+'].divide(counts['N-']) ... # If we wanted to only return bin-counting properties, ... # we would filter here ... bin_counts = counts.filter(items= ['N+', 'N-', 'log_N+']) ... return counts, bin_counts\n\n# Bin counts example: device_id >>> bin_column = 'device_id' >>> device_clicks = click_counting(df.filter(items=[bin_column, 'click']), ... bin_column) >>> device_all, device_bin_counts = bin_counting(device_clicks)\n\n# Check to make sure we have all the devices >>> len(device_bin_counts) 7201\n\n>>> device_all.sort_values(by = 'total_clicks', ascending=False).head(4)\n\nclicks no_clicks total N+ N- log_N+ a99f214a 15729 71206 86935 0.180928 0.819072 0.220894 c357dbff 33 134 167 0.197605 0.802395 0.246269 31da1bd0 0 62 62 0.000000 1.000000 0.000000 936e92fb 5 54 59 0.084746 0.915254 0.092593\n\nWhat about rare categories?\n\nJust like rare words, rare categories require special treatment. Think about a user who logs in once a year: there will be very little data to reliably estimate that user’s click- through rate for ads. Moreover, rare categories waste space in the counts table.\n\nOne way to deal with this is through back-off, a simple technique that accumulates the counts of all rare categories in a special bin (see Figure 5-3). If the count is greater than a certain threshold, then the category gets its own count statistics. Otherwise, we use the statistics from the back-off bin. This essentially reverts the statistics for a sin‐ gle rare category to the statistics computed on all rare categories. When using the\n\nDealing with Large Categorical Variables\n\n|\n\n91\n\nback-off method, it helps to also add a binary indicator for whether or not the statis‐ tics come from the back-off bin.\n\nFigure 5-3. If a rare category gains counts, it can move above the threshold for the back-off bin, using its own count statistics for modeling\n\nThere is another way to deal with this problem, called the count-min sketch (Cor‐ mode and Muthukrishnan, 2005). In this method, all the categories, rare or frequent alike, are mapped through multiple hash functions with an output range, m, much smaller than the number of categories, k. When retrieving a statistic, recompute all the hashes of the category and return the smallest statistic. Having multiple hash functions mitigates the probability of collision within a single hash function. The scheme works because the number of hash functions times m, the size of the hash table, can be made smaller than k, the number of categories, and still retain low over‐ all collision probability.\n\nFigure 5-4 illustrates. Each item i is mapped to one cell in each row of the array of counts. When an update of ct to item it arrives, ct is added to each of these cells, hashed using functions h1…hd.\n\nFigure 5-4. The count-min sketch\n\n92\n\n|\n\nChapter 5: Categorical Variables: Counting Eggs in the Age of Robotic Chickens\n\nGuarding against data leakage\n\nSince bin counting relies on historical data to generate the necessary statistics, it requires waiting through a data collection period, incurring a slight delay in the learning pipeline. Also, when the data distribution changes, the counts need to be updated. The faster the data changes, the more frequently the counts need to be recomputed. This is particularly important for applications like targeted advertising, where user preferences and popular queries change very quickly, and lack of adapta‐ tion to the current distribution could mean huge losses for the advertising platform.\n\nOne might ask, why not use the same dataset to compute the relevant statistics and train the model? The idea seems innocent enough. The big problem here is that the statistics involve the target variable, which is what the model tries to predict. Using the output to compute the input features leads to a pernicious problem known as leakage. In short, leakage means that information is revealed to the model that gives it an unrealistic advantage to make better predictions. This could happen when test data is leaked into the training set, or when data from the future is leaked to the past. Any time that a model is given information that it shouldn’t have access to when it is making predictions in real time in production, there is leakage. Kaggle’s wiki gives more examples of leakage and why it is bad for machine learning applications.\n\nIf the bin-counting procedure used the current data point’s label to compute part of the input statistic, that would constitute direct leakage. One way to prevent that is by instituting strict separation between count collection (for computing bin-count sta‐ tistics) and training, as illustrated in Figure 5-5—i.e., use an earlier batch of data points for counting, use the current data points for training (mapping categorical variables to historical statistics we just collected), and use future data points for test‐ ing. This fixes the problem of leakage, but introduces the aforementioned delay (the input statistics and therefore the model will trail behind current data).\n\nFigure 5-5. Using time windows can prevent data leakage during bin counting\n\nIt turns out that there is another solution, based on differential privacy. A statistic is approximately leakage-proof if its distribution stays roughly the same with or without any one data point. In practice, adding a small random noise with distribution Lap‐ lace(0,1) is sufficient to cover up any potential leakage from a single data point. This\n\nDealing with Large Categorical Variables\n\n|\n\n93\n\nidea can be combined with leaving-one-out counting to formulate statistics on cur‐ rent data (Zhang, 2015).\n\nCounts without bounds\n\nIf the statistics are updated continuously given more and more historical data, the raw counts will grow without bounds. This could be a problem for the model. A trained model “knows” the input data up to the observed scale. A trained decision tree might say, “When x is greater than 3, predict 1.” A trained linear model might say, “Multiply x by 0.7 and see if the result is greater than the global average.” These might be the correct decisions when x lies between 0 and 5. But what happens beyond that? No one knows.\n\nWhen the input counts increase, the model will need to be retrained to adapt to the current scale. If the counts accumulate rather slowly, then the effective scale won’t change too fast, and the model will not need to be retrained too frequently. But when counts increment very quickly, frequent retraining will be a nuisance.\n\nFor this reason, it is often better to use normalized counts that are guaranteed to be bounded in a known interval. For instance, the estimated click-through probability is bounded between [0, 1]. Another method is to take the log transform, which imposes a strict bound, but the rate of increase will be very slow when the count is very large.\n\nNeither method will guard against shifting input distributions (e.g., last year’s Barbie dolls are now out of style and people will no longer click on those ads). The model will need to be retrained to accommodate these more fundamental changes in input data distribution, or the whole pipeline will need to move to an online learning set‐ ting where the model is continuously adapting to the input.\n\nSummary Each of the approaches detailed in this chapter has its pros and cons. Here is a run‐ down of the trade-offs.\n\nPlain one-hot encoding\n\nO(n) using the sparse vector format, where n is the number of data points\n\nSpace requirement Computation requirement O(nk) under a linear model, where k is the number of categories Pros\n\nEasiest to implement • Potentially most accurate • Feasible for online learning\n\n94\n\n|\n\nChapter 5: Categorical Variables: Counting Eggs in the Age of Robotic Chickens",
      "page_number": 100
    },
    {
      "number": 13,
      "title": "Segment 13 (pages 108-117)",
      "start_page": 108,
      "end_page": 117,
      "detection_method": "topic_boundary",
      "content": "Plain one-hot encoding\n\nCons\n\nComputationally inefficient • Does not adapt to growing categories • Not feasible for anything other than linear models • Requires large-scale distributed optimization with truly large datasets\n\nFeature hashing\n\nO(n) using the sparse matrix format, where n is the number of data points Space requirement Computation requirement O(nm) under a linear or kernel model, where m is the number of hash bins Pros\n\nEasy to implement • Makes model training cheaper • Easily adaptable to new categories • Easily handles rare categories • Feasible for online learning\n\nCons\n\nOnly suitable for linear or kernelized models • Hashed features not interpretable • Mixed reports of accuracy\n\nBin-counting\n\nSpace requirement\n\nO(n+k) for small, dense representation of each data point, plus the count statistics that must be kept for each category\n\nComputation requirement O(n) for linear models; also usable for nonlinear models such as trees Pros\n\nSmallest computational burden at training time • Enables tree-based models • Relatively easy to adapt to new categories • Handles rare categories with back-off or count-min sketch • Interpretable\n\nCons\n\nRequires historical data • Delayed updates required, not completely suitable for online learning • Higher potential for leakage\n\nAs we can see, none of the methods are perfect. Which one to use depends on the desired model. Linear models are cheaper to train and therefore can handle noncom‐ pressed representations such as one-hot encoding. Tree-based models, on the other hand, need to do repeated searches over all features for the right split, and are thus limited to small representations such as bin counting. Feature hashing sits in between those two extremes, but with mixed reports on the resulting accuracy.\n\nSummary\n\n|\n\n95\n\nBibliography Agarwal, Alekh, Oliveier Chapelle, Miroslav Dudík, and John Langford. “A Reliable Effective Terascale Linear Learning System.” Journal of Machine Learning Research 15 (2015): 1111−1133.\n\nBilenko, Misha. “Big Learning Made Easy—with Counts!” Cortana Intelligence and Machine Learning Blog, February 17, 2015. https://blogs.technet.microsoft.com/machi‐ nelearning/2015/02/17/big-learning-made-easy-with-counts/.\n\nChen, Ye, Dmitry Pavlov, and John F. Canny. “Large-Scale Behavioral Targeting.” Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Dis‐ covery and Data Mining (2009): 209–218.\n\nCormode, Graham, and S. Muthukrishnan. “An Improved Data Stream Summary: The Count-Min Sketch and Its Applications.” Algorithms 55 (2005): 29–38.\n\nGraepel, Thore, Joaquin Quiñonero Candela, Thomas Borchert, and Ralf Herbrich. “Web-Scale Bayesian Click-Through Rate Prediction for Sponsored Search Advertis‐ ing in Microsoft’s Bing Search Engine.” Proceedings of the 27th International Confer‐ ence on Machine Learning (2010): 13–20.\n\nHe, Xinran, Junfeng Pan, Ou Jin, Tianbing Xu, Bo Liu, Tao Xu, Yanxin Shi, Antoine Atallah, Ralf Herbrich, Stuart Bowers, and Joaquin Quiñonero Candela. “Practical Lessons from Predicting Clicks on Ads at Facebook.” Proceedings of the 8th Interna‐ tional Workshop on Data Mining for Online Advertising (2014): 1–9.\n\nLee, Wenke, Salvatore J. Stolfo, and Kui W. Mok. 1998. “Mining Audit Data to Build Intrusion Detection Models.” Proceedings of the 4th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (1998): 66–72.\n\nLi, Wei, Xuerui Wang, Ruofei Zhang, Ying Cui, Jianchang Mao, and Rong Jin. “Exploitation and Exploration in a Performance Based Contextual Advertising Sys‐ tem.” Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (2010): 27–36.\n\nMcMahan, H. Brendan, Gary Holt, D. Sculley, Michael Young, Dietmar Ebner, Julian Grady, Lan Nie, Todd Phillips, Eugene Davydov, Daniel Golovin, Sharat Chikkerur, Dan Liu, Martin Wattenberg, Arnar Mar Hrafnkelsson, Tom Boulos, and Jeremy Kubica. “Ad Click Prediction: A View from the Trenches.” Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (2013): 1222–1230.\n\nWeinberger, Kilian, Anirban Dasgupta, Josh Attenberg, John Langford, and Alex Smola. 2009. “Feature Hashing for Large Scale Multitask Learning.” Proceedings of the 26th International Conference on Machine Learning (2009): 1113–1120.\n\n96\n\n|\n\nChapter 5: Categorical Variables: Counting Eggs in the Age of Robotic Chickens\n\nYeh, Tse-Yu, and Yale N. Patt. “Two-Level Adaptive Training Branch Prediction.” Proceedings of the 24th Annual International Symposium on Microarchitecture (1991): 51–61.\n\nZhang, Owen. 2015. “Tips for data science competitions.” SlideShare presentation. Retrieved from http://bit.ly/2DjuhBD.\n\nBibliography\n\n|\n\n97\n\nCHAPTER 6 Dimensionality Reduction: Squashing the Data Pancake with PCA\n\nWith automatic data collection and feature generation techniques, one can quickly obtain a large number of features. But not all of them are useful. In Chapters 3 and 4, we discussed frequency-based filtering and feature scaling as ways of pruning away uninformative features. Now we will take a close look at the topic of feature dimen‐ sionality reduction using principal component analysis (PCA).\n\nThis chapter marks an entry into model-based feature engineering techniques. Prior to this point, most of the techniques can be defined without referencing the data. For instance, frequency-based filtering might say, “Get rid of all counts that are smaller than n,” a procedure that can be carried out without further input from the data itself.\n\nModel-based techniques, on the other hand, require information from the data. For example, PCA is defined around the principal axes of the data. In previous chapters, there was always a clear-cut line between data, features, and models. From this point forward, the difference gets increasingly blurry. This is exactly where the excitement lies in current research on feature learning.\n\nIntuition Dimensionality reduction is about getting rid of “uninformative information” while retaining the crucial bits. There are many ways to define “uninformative.” PCA focu‐ ses on the notion of linear dependency. In “The Anatomy of a Matrix” on page 182, we describe the column space of a data matrix as the span of all feature vectors. If the column space is small compared to the total number of features, then most of the fea‐ tures are linear combinations of a few key features. Linearly dependent features are a\n\n99\n\nwaste of space and computation power because the information could have been encoded in much fewer features. To avoid this situation, principal component analy‐ sis tries to reduce such “fluff” by squashing the data into a much lower-dimensional linear subspace.\n\nPicture the set of data points in feature space. Each data point is a dot, and the whole set of data points forms a blob. In Figure 6-1(a), the data points spread out evenly across both feature dimensions, and the blob fills the space. In this example, the col‐ umn space has full rank. However, if some of those features are linear combinations of others, then the blob won’t look so plump; it will look more like Figure 6-1(b), a flat blob where feature 1 is a duplicate (or a scalar multiple) of feature 2. In this case, we say that the intrinsic dimensionality of the blob is 1, even though it lies in a two- dimensional feature space.\n\nIn practice, things are rarely exactly equal to one another. It is more likely that we see features that are very close to being equal, but not quite. In such a case, the data blob might look something like Figure 6-1(c). It’s an emaciated blob. If we wanted to reduce the number of features to pass to the model, then we could replace feature 1 and feature 2 with a new feature, maybe called feature 1.5, which lies on the diagonal line between the original two features. The original dataset could then be adequately represented by one number—the position along the direction of feature 1.5—instead of two numbers, f1 and f2.\n\nFigure 6-1. Data blobs in feature space: (a) full-rank data blob, (b) low-dimensional data blob, and (c) approximately low-dimensional data blob\n\n100\n\n|\n\nChapter 6: Dimensionality Reduction: Squashing the Data Pancake with PCA\n\nThe key idea here is to replace redundant features with a few new features that ade‐ quately summarize information contained in the original feature space. It’s easy to tell what the new feature should be when there are only two features. It’s much harder when the original feature space has hundreds or thousands of dimensions. We need a way to mathematically describe the new features we are looking for. Then we can use optimization techniques to find them.\n\nOne way to mathematically define “adequately summarize information” is to say that the new data blob should retain as much of the original volume as possible. We are squashing the data blob into a flat pancake, but we want the pancake to be as big as possible in the right directions. This means we need a way to measure volume.\n\nVolume has to do with distance. But the notion of distance in a blob of data points is somewhat fuzzy. One could measure the maximum distance between any two pairs of points, but that turns out to be a very difficult function to mathematically optimize. An alternative is to measure the average distance between pairs of points, or equiva‐ lently, the average distance between each point and its mean, which is the variance. This turns out to be much easier to optimize. (Life is hard. Statisticians have learned to take convenient shortcuts.) Mathematically, this translates into maximizing the variance of the data points in the new feature space.\n\nTips for Navigating Linear Algebra Formulas\n\nTo stay oriented in the world of linear algebra, keep track of which quantities are scalars, which are vectors, and which way the vectors are oriented—vertically or horizontally. Know the dimensions of your matrices, because they often tell you whether the vectors of interest are in the rows or columns. Draw the matrices and vectors as rectangles on a page and make sure the shapes match. Just as one can get far in algebra by noting the units of measurement (dis‐ tance is in miles, speed is in miles per hour), in linear algebra all one needs are the dimensions.\n\nDerivation As before, let X denote the n × d data matrix, where n is the number of data points and d the number of features. Let x be a column vector containing a single data point. (So x is the transpose of one of the rows in X.) Let v denote one of the new feature vectors, or principal components, that we are trying to find.\n\nDerivation\n\n|\n\n101\n\nSingular Value Decomposition (SVD) of a Matrix Any rectangular matrix can be decomposed into three matrices of particular shapes and characteristics:\n\nX = UΣVT\n\nHere, U and V are orthogonal matrices (i.e., UTU = I and VTV = I). Σ is a diagonal matrix containing the singular values of X, which can be positive, zero, or negative. Suppose X has n rows and d columns and n ≥ d. Then U has shape n × d, and Σ and V have shape d × d. (See “Singular Value Decomposition (SVD)” on page 185 for a full review of SVD and eigen decomposition of a matrix.)\n\nLinear Projection Let’s break down the derivation of PCA step by step. Figure 6-2 illustrates the whole process.\n\nFigure 6-2. Illustration of PCA: (a) original data in feature space; (b) centered data; (c) projecting a data vector x onto another vector v; (d) direction of maximum variance of the projected coordinates (equal to the principal eigenvector of XTX)\n\nPCA uses linear projection to transform data into the new feature space. Figure 6-2(c) illustrates what a linear projection looks like. When we project x onto v, the length of the projection is proportional to the inner product between the two,\n\n102\n\n|\n\nChapter 6: Dimensionality Reduction: Squashing the Data Pancake with PCA\n\nnormalized by the norm of v (its inner product with itself). Later on, we will con‐ strain v to have unit norm. So, the only relevant part is the numerator—let’s call it v (see Equation 6-1).\n\nEquation 6-1. Projection coordinate\n\nz = xTv\n\nNote that z is a scalar, whereas x and v are column vectors. Since there are a bunch of data points, we can formulate the vector z of all of their projection coordinates on the new feature v (Equation 6-2). Here, X is the familiar data matrix where each row is a data point. The resulting z is a column vector.\n\nEquation 6-2. Vector of projection coordinates\n\nz = Xv\n\nVariance and Empirical Variance The next step is to compute the variance of the projections. Variance is defined as the expectation of the squared distance to the mean (Equation 6-3).\n\nEquation 6-3. Variance of a random variable Z\n\nVar(Z) = E[Z – E(Z)]2\n\nThere is one tiny problem: our formulation of the problem says nothing about the mean, E(Z); it is a free variable. One solution is to remove it from the equation by subtracting the mean from every data point. The resulting dataset has mean zero, which means that the variance is simply the expectation of Z2. Geometrically, sub‐ tracting the mean has the effect of centering the data. (See Figure 6-2(a-b).)\n\nA closely related quantity is the covariance between two random variables Z1 and Z2 (Equation 6-4). Think of this as the extension of the idea of variance (of a single ran‐ dom variable) to two random variables.\n\nEquation 6-4. Covariance between two random variables Z1 and Z2\n\nCov(Z1, Z2) = E[(Z1 – E(Z1)(Z2 – E(Z2)]\n\nWhen the random variables have mean zero, their covariance coincides with their linear correlation, E[Z1Z2]. We will discuss this concept more later on.\n\nDerivation\n\n|\n\n103\n\nStatistical quantities like variance and expectation are defined on a data distribution. In practice, we don’t have the true distribution, but only a bunch of observed data points, z1, ..., zn. This is called an empirical distribution, and it gives us an empirical estimate of the variance (Equation 6-5).\n\nEquation 6-5. Empirical variance of Z based on observations z\n\nVaremp(Z) =\n\nn\n\n1 n – 1 ∑\n\ni=1\n\nzi\n\n2\n\nPrincipal Components: First Formulation Combined with the definition of zi in Equation 6-1, we have the formulation for max‐ imizing the variance of the projected data given in Equation 6-6. (We drop the denominator n–1 from the definition of empirical variance, because it is a global con‐ stant and does not affect where the maximizing value occurs.)\n\nEquation 6-6. Objective function of principal components\n\nn\n\nmaxw∑\n\n(xi\n\nTw)2, where wTw = 1\n\ni=1\n\nThe constraint here forces the inner product of w with itself to be 1, which is equiva‐ lent to saying that the vector must have unit length. This is because we only care about the direction and not the magnitude of w. The magnitude of w is an unneces‐ sary degree of freedom, so we get rid of it by setting it to an arbitrary value.\n\nPrincipal Components: Matrix-Vector Formulation Next comes the tricky step. The sum of squares term in Equation 6-6 is rather cum‐ bersome. It’d be much cleaner in a matrix-vector format. Can we do it? The answer is yes. The key lies in the sum-of-squares identity: the sum of a bunch of squared terms is equal to the squared norm of a vector whose elements are those terms, which is equivalent to the vector’s inner product with itself. With this identity in hand, we can rewrite Equation 6-6 in matrix-vector notation, as shown in Equation 6-7.\n\nEquation 6-7. Objective function for principal components, matrix-vector formulation\n\nmaxw wTw, where wTw = 1\n\nThis formulation of PCA presents the target more clearly: we look for an input direc‐ tion that maximizes the norm of the output. Does this sound familiar? The answer lies in the singular value decomposition (SVD) of X. The optimal w, as it turns out, is\n\n104\n\n|\n\nChapter 6: Dimensionality Reduction: Squashing the Data Pancake with PCA\n\nthe principal left singular vector of X, which is also the principal eigenvector of XTX. The projected data is called a principal component of the original data.\n\nGeneral Solution of the Principal Components This process can be repeated. Once we find the first principal component, we can rerun Equation 6-7 with the added constraint that the new vector be orthogonal to the previously found vectors (see Equation 6-8).\n\nEquation 6-8. Objective function for k+1st principal components\n\nmaxw wTw, where wTw = 1 and wTw1 = ... = wTwk = 0\n\nThe solution is the k+1st left singular vectors of X, ordered by descending singular values. Thus, the first k principal components correspond to the first k left singular vectors of X.\n\nTransforming Features Once the principal components are found, we can transform the features using linear projection. Let X = UΣVT be the SVD of X, and Vk the matrix whose columns contain the first k left singular vectors. X has dimensions n × d, where d is the number of original features, and Vk has dimensions d × k. Instead of a single projection vector as in Equation 6-2, we can simultaneously project onto multiple vectors in a projection matrix (Equation 6-9).\n\nEquation 6-9. PCA projection matrix\n\nW = Vk\n\nThe matrix of projected coordinates is easy to compute, and can be further simplified using the fact that the singular vectors are orthogonal to each other (see Equation 6-10).\n\nEquation 6-10. Simple PCA transform\n\nZ = XW = XVk = UΣVTVk = UkΣk\n\nThe projected values are simply the first k right singular vectors scaled by the first k singular values. Thus, the entire PCA solution, components and projections alike, can be conveniently obtained through the SVD of X.\n\nDerivation\n\n|\n\n105",
      "page_number": 108
    },
    {
      "number": 14,
      "title": "Segment 14 (pages 118-125)",
      "start_page": 118,
      "end_page": 125,
      "detection_method": "topic_boundary",
      "content": "Implementing PCA Many derivations of PCA involve first centering the data, then taking the eigen decomposition of the covariance matrix. But the easiest way to implement PCA is by taking the singular value decomposition of the centered data matrix.\n\nPCA Implementation Steps\n\n1. Center the data matrix:\n\nC = X – 1μT\n\nwhere 1 is a column vector containing all 1s, and μ is a column vector containing the average of the rows of X.\n\n2. Compute the SVD:\n\nC = UΣVT\n\n3. Find the principal components. The first k principal components are the first k columns of V; i.e., the right singular vectors corresponding to the k largest singu‐ lar values.\n\n4. Transform the data. The transformed data is simply the first k columns of U. (If whitening is desired, then scale the vectors by the inverse singular values. This requires that the selected singular values are nonzero. See “Whitening and ZCA” on page 108.)\n\nPCA in Action Let’s get a better sense for how PCA works by applying it to some image data. The MNIST dataset contains images of handwritten digits from 0 to 9. The original images are 28 × 28 pixels. A lower-resolution subset of the images is distributed with scikit-learn, where each image is downsampled into 8 × 8 pixels. The original data in scikit-learn has 64 dimensions. In Example 6-1, we apply PCA and visualize the data‐ set using the first three principal components.\n\nExample 6-1. Principal component analysis of the scikit-learn digits dataset (a subset of the MNIST dataset)\n\n>>> from sklearn import datasets >>> from sklearn.decomposition import PCA\n\n106\n\n|\n\nChapter 6: Dimensionality Reduction: Squashing the Data Pancake with PCA\n\n# Load the data >>> digits_data = datasets.load_digits() >>> n = len(digits_data.images)\n\n# Each image is represented as an 8-by-8 array. # Flatten this array as input to PCA. >>> image_data = digits_data.images.reshape((n, -1)) >>> image_data.shape (1797, 64)\n\n# Groundtruth label of the number appearing in each image >>> labels = digits_data.target >>> labels array([0, 1, 2, ..., 8, 9, 8])\n\n# Fit a PCA transformer to the dataset. # The number of components is automatically chosen to account for # at least 80% of the total variance. >>> pca_transformer = PCA(n_components=0.8) >>> pca_images = pca_transformer.fit_transform(image_data) >>> pca_transformer.explained_variance_ratio_ array([ 0.14890594, 0.13618771, 0.11794594, 0.08409979, 0.05782415, 0.0491691 , 0.04315987, 0.03661373, 0.03353248, 0.03078806, 0.02372341, 0.02272697, 0.01821863]) >>> pca_transformer.explained_variance_ratio_[:3].sum() 0.40303958587675121\n\n# Visualize the results >>> import matplotlib.pyplot as plt >>> from mpl_toolkits.mplot3d import Axes3D >>> %matplotlib notebook >>> fig = plt.figure() >>> ax = fig.add_subplot(111, projection='3d') >>> for i in range(100): ... ax.scatter(pca_images[i,0], pca_images[i,1], pca_images[i,2], ... marker=r'${}$'.format(labels[i]), s=64)\n\n>>> ax.set_xlabel('Principal component 1') >>> ax.set_ylabel('Principal component 2') >>> ax.set_zlabel('Principal component 3')\n\nThe first 100 projected images are shown in a 3D plot in Figure 6-3. The markers cor‐ respond to the labels. The first three principal components account for roughly 40% of the total variance in the dataset. This is by no means perfect, but it allows for a handy low-dimensional visualization. We see that PCA groups similar numbers close to each other. The numbers 0 and 6 lie in the same region, as do 1 and 7, and 3 and 9. The space is roughly divided between 0, 4, and 6 on one side, and the rest of the num‐ bers on the other.\n\nPCA in Action\n\n|\n\n107\n\nFigure 6-3. PCA projections of subset of MNIST data—markers correspond to image labels\n\nSince there is a fair amount of overlap between numbers, it would be difficult to tell them apart using a linear classifier in the projected space. Hence, if the task is to clas‐ sify the handwritten digits and the chosen model is a linear classifier, then the first three principal components are not sufficient as features. Nevertheless, it is interest‐ ing to see how much of a 64-dimensional dataset can be captured in just 3 dimen‐ sions.\n\nWhitening and ZCA Due to the orthogonality constraint in the objective function, PCA transformation produces a nice side effect: the transformed features are no longer correlated. In other words, the inner products between pairs of feature vectors are zero. It’s easy to prove this using the orthogonality property of the singular vectors:\n\nZTZ = ΣkUk\n\nTUkΣk = Σk\n\n2\n\nThe result is a diagonal matrix containing squares of the singular values representing the correlation of each feature vector with itself, also known as its ℓ2 norm.\n\nSometimes, it is useful to also normalize the scale of the features to 1. In signal pro‐ cessing terms, this is known as whitening. It results in a set of features that have unit correlation with themselves and zero correlation with each other. Mathematically,\n\n108\n\n|\n\nChapter 6: Dimensionality Reduction: Squashing the Data Pancake with PCA\n\nwhitening can done by multiplying the PCA transformation with the inverse singular values (see Equation 6-11).\n\nEquation 6-11. PCA + whitening\n\n1 Wwhite = VkΣk\n\nZwhite = XVkΣk\n\n1 = UΣVTVkΣk\n\n1 = Uk\n\nWhitening is independent from dimensionality reduction; one can perform one without the other. For example, zero-phase component analysis (ZCA) (Bell and Sej‐ nowski, 1996) is a whitening transformation that is closely related to PCA, but that does not reduce the number of features. ZCA whitening uses the full set of principal components V without reduction, and includes an extra multiplication back onto VT (Equation 6-12).\n\nEquation 6-12. ZCA whitening\n\nWZCA = VΣ-1VT\n\nZzca = XVΣ-1VT = UΣVTVΣ-1 = U\n\nSimple PCA projection (Equation 6-10) produces coordinates in the new feature space, where the principal components serve as the basis. These coordinates repre‐ sent only the length of the projected vector, not the direction. Multiplication with the principal components gives us the length and the orientation. Another valid interpre‐ tation is that the extra multiplication rotates the coordinates back into the original feature space. (V is an orthogonal matrix, and orthogonal matrices rotate their input without stretching or compression.) So, ZCA produces whitened data that is as close (in Euclidean distance) to the original data as possible.\n\nConsiderations and Limitations of PCA When using PCA for dimensionality reduction, one must address the question of how many principal components (k) to use. Like all hyperparameters, this number can be tuned based on the quality of the resulting model. But there are also heuristics that do not involve expensive computational methods.\n\nOne possibility is to pick k to account for a desired proportion of total variance. (This option is available in the scikit-learn package PCA.) The variance of the projection onto the kth component is:\n\n║Xvk║2 =║uk σk║2 = σk\n\n2\n\nConsiderations and Limitations of PCA\n\n|\n\n109\n\nwhich is the square of the kth-largest singular value of X. The ordered list of singular values of a matrix is called its spectrum. Thus, to determine how many components to use, one can perform a simple spectral analysis of the data matrix and pick the threshold that retains enough variance.\n\nSelecting k Based on Accounted Variance\n\nTo retain enough components to cover 80% of the total variance in the data, pick k such that\n\nk\n\n∑\n\nσi\n\n2\n\ni=1 d\n\n≥ 0.8.\n\n∑\n\nσi\n\n2\n\ni=1\n\nAnother method for picking k involves the intrinsic dimensionality of a dataset. This is a hazier concept, but can also be determined from the spectrum. Basically, if the spectrum contains a few large singular values and a number of tiny ones, then one can probably just harvest the largest singular values and discard the rest. Sometimes the rest of the spectrum is not tiny, but there’s a large gap between the head and the tail values. That would also be a reasonable cutoff. This method is requires visual inspection of the spectrum and hence cannot be performed as part of an automated pipeline.\n\nOne key criticism of PCA is that the transformation is fairly complex, and the results are therefore hard to interpret. The principal components and the projected vectors are real-valued and could be positive or negative. The principal components are essentially linear combinations of the (centered) rows, and the projection values are linear combinations of the columns. In a stock returns application, for instance, each factor is a linear combination of time slices of stock returns. What does that mean? It is hard to express a human-understandable reason for the learned factors. Therefore, it is hard for analysts to trust the results. If you can’t explain why you should be putting billions of other people’s money into particular stocks, you probably won’t choose to use that model.\n\nPCA is computationally expensive. It relies on SVD, which is an expensive procedure. To compute the full SVD of a matrix takes O(nd2 + d3) operations (Golub and Van Loan, 2012), assuming n ≥ d—i.e., there are more data points than features. Even if we only want k principal components, computing the truncated SVD (the k largest singular values and vectors) still takes O((n+d)2 k) = O(n2k) operations. This is pro‐ hibitive when there are a large number of data points or features.\n\nIt is difficult to perform PCA in a streaming fashion, in batch updates, or from a sam‐ ple of the full data. Streaming computation of the SVD, updating the SVD, and\n\n110\n\n|\n\nChapter 6: Dimensionality Reduction: Squashing the Data Pancake with PCA\n\ncomputing the SVD from a subsample are all difficult research problems. Algorithms exist, but at the cost of reduced accuracy. One implication is that one should expect lower representational accuracy when projecting test data onto principal components found in the training set. As the distribution of the data changes, one would have to recompute the principal components in the current dataset.\n\nLastly, it is best not to apply PCA to raw counts (word counts, music play counts, movie viewing counts, etc.). The reason for this is that such counts often contain large outliers. (The probability is pretty high that there is a fan out there who watched The Lord of the Rings 314,582 times, which dwarfs the rest of the counts.) As we know, PCA looks for linear correlations within the features. Correlation and variance statistics are very sensitive to large outliers; a single large number could change the statistics a lot. So, it is a good idea to first trim the data of large values (“Frequency- Based Filtering” on page 48), or apply a scaling transform like tf-idf (Chapter 4) or the log transform (“Log Transformation” on page 15).\n\nUse Cases PCA reduces feature space dimensionality by looking for linear correlation patterns between features. Since it involves the SVD, PCA is expensive to compute for more than a few thousand features. But for small numbers of real-valued features, it is very much worth trying.\n\nPCA transformation discards information from the data. Thus, the downstream model may be cheaper to train, but less accurate. On the MNIST dataset, some have observed that using reduced-dimensionality data from PCA results in less accurate classification models. In these cases, there is both an upside and a downside to using PCA.\n\nOne of the coolest applications of PCA is in anomaly detection of time series. Lakhina et al. (2004) used PCA to detect and diagnose anomalies in internet traffic. They focused on volume anomalies, i.e., when there is a surge or a dip in the amount of traffic going from one network region to another. These sudden changes may be indicative of a misconfigured network or coordinated denial-of-service attacks. Either way, knowing when and where such changes occur is valuable to internet operators.\n\nSince there is so much total traffic over the internet, isolated surges in small regions are hard to detect. A relatively small set of backbone links handle much of the traffic. Their key insight is that volume anomalies affect multiple links at the same time (because network packets need to hop through multiple nodes to reach their destina‐ tion). Treat each of the links as a feature, and the amount of traffic at each time step as the measurement. A data point is a time slice of traffic measurements across all links on the network. The principal components of this matrix indicate the overall\n\nUse Cases\n\n|\n\n111\n\ntraffic trends on the network. The rest of the components represent the residual sig‐ nal, which contains the anomalies.\n\nPCA is also often used in financial modeling. In those use cases, it works as a type of factor analysis, a term that describes a family of statistical methods that aim to describe observed variability in data using a small number of unobserved factors. In factor analysis applications, the goal is to find the explanatory components, not the transformed data.\n\nFinancial quantities like stock returns are often correlated with each other. Stocks may move up and down at the same time (positive correlation), or move in opposite directions (negative correlation). In order to balance volatility and reduce risk, an investment portfolio needs a diverse set of stocks that are not correlated with each other. (Don’t put all your eggs in one basket if that basket is going to sink.) Finding strong correlation patterns is helpful for deciding on an investment strategy.\n\nStock correlation patterns can be industry-wide. For example, tech stocks may go up and down together, while airline stocks tend to go down when oil prices are high. But industry may not be the best way to explain the outcome. Analysts also look for unex‐ pected correlations in observed statistics. In particular, the statistical factor model (Connor, 1995) runs PCA on the matrix of time series of individual stock returns to find commonly covarying stocks. In this use case, the end goal is the principal com‐ ponents themselves, not the transformed data.\n\nZCA is useful as a preprocessing step when learning from images. In natural images, adjacent pixels often have similar colors. ZCA whitening can remove this correlation, which allows subsequent modeling efforts to focus on more interesting image struc‐ tures. Krizhevsky’s (2009) thesis on “Learning Multiple Layers of Features from Images” contains nice examples that illustrate the effect of ZCA whitening on natural images.\n\nMany deep learning models use PCA or ZCA as a preprocessing step, though it is not always necessary. In “Factored 3-Way Restricted Boltzmann Machines for Modeling Natural Images”, Ranzato et al. (2010) remark, “Whitening is not necessary but speeds up the convergence of the algorithm.” In “An Analysis of Single-Layer Net‐ works in Unsupervised Feature Learning”, Coates et al. (2011) find that ZCA whiten‐ ing is helpful for some models, but not all. (Note the models in this paper are unsupervised feature learning models, so ZCA is used as a feature engineering method for other feature engineering methods. Stacking and chaining of methods is common in machine learning pipelines.)\n\nSummary This concludes the discussion of PCA. The two main things to remember about PCA are its mechanism (linear projection) and objective (to maximize the variance of\n\n112\n\n|\n\nChapter 6: Dimensionality Reduction: Squashing the Data Pancake with PCA\n\nprojected data). The solution involves the eigen decomposition of the covariance matrix, which is closely related to the SVD of the data matrix. One can also remem‐ ber PCA with the mental picture of squashing the data into a pancake that is as fluffy as possible.\n\nPCA is an example of model-driven feature engineering. (One should immediately suspect that a model is lurking in the background whenever an objective function enters the scene.) The modeling assumption here is that variance adequately repre‐ sents the information contained in the data. Equivalently, the model looks for linear correlations between features. This is used in several applications to reduce the corre‐ lation or find common factors in the input.\n\nPCA is a well-known dimensionality reduction method. But it has its limitations, such as high computational cost and uninterpretable outcome. It is useful as a pre‐ processing step, especially when there are linear correlations between features.\n\nWhen seen as a method for eliminating linear correlation, PCA is related to the con‐ cept of whitening. Its cousin, ZCA, whitens the data in an interpretable way, but does not reduce dimensionality.\n\nBibliography Bell, Anthony J. and Terrence J. Sejnowski. “Edges Are the ‘Independent Compo‐ nents’ of Natural Scenes.” Advances in Neural Information Processing Systems 9 (1996): 831–837.\n\nCoates, Adam, Andrew Y. Ng, and Honglak Lee. “An Analysis of Single-Layer Net‐ works in Unsupervised Feature Learning.” Proceedings of the 14th International con‐ ference on Artificial Intelligence and Statistics (2011): 215–223.\n\nConnor, Gregory. “The Three Types of Factor Models: A Comparison of Their Explanatory Power.” Financial Analysts Journal 51:3 (1995) 42–46.\n\nGolub, Gene H., and Charles F. Van Loan. Matrix Computations. 4th ed. Baltimore, MD: Johns Hopkins University Press, 2012.\n\nKrizhevsky, Alex. “Learning Multiple Layers of Features from Tiny Images.” MSc thesis, University of Toronto, 2009.\n\nLakhina, Anukool, Mark Crovella, and Christophe Diot. “Diagnosing Network-wide Traffic Anomalies.” Proceedings of the 2004 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications (2004): 219–230.\n\nRanzato, Marc’Aurelio, Alex Krizhevsky, and Geoffrey E. Hinton. “Factored 3-Way Restricted Boltzmann Machines for Modeling Natural Images.” Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (2010): 621–628.\n\nBibliography\n\n|\n\n113",
      "page_number": 118
    },
    {
      "number": 15,
      "title": "Segment 15 (pages 126-135)",
      "start_page": 126,
      "end_page": 135,
      "detection_method": "topic_boundary",
      "content": "CHAPTER 7 Nonlinear Featurization via K-Means Model Stacking\n\nPCA is very useful when the data lies in a linear subspace like a flat pancake. But what if the data forms a more complicated shape?1 A flat plane (linear subspace) can be generalized to a manifold (nonlinear subspace), which can be thought of as a surface that gets stretched and rolled in various ways.2\n\nIf a linear subspace is a flat sheet of paper, then a rolled up sheet of paper is a simple example of a nonlinear manifold. Informally, this is called a Swiss roll (see Figure 7-1). Once rolled, a 2D plane occupies 3D space. Yet it is essentially still a 2D object. In other words, it has low intrinsic dimensionality, a concept we’ve already touched upon in “Intuition” on page 99. If we could somehow unroll the Swiss roll, we’d recover the 2D plane. This is the goal of nonlinear dimensionality reduction, which assumes that the manifold is simpler than the full dimension it occupies and attempts to unfold it.\n\n1 This chapter is inspired by a conversation with Ted Dunning, active Apache contributor and noted author.\n\nThe stacking example came directly from Ted, and he provided many helpful comments in the course of writ‐ ing. If one could have coauthors for individual chapters, Ted would be a coauthor for this one.\n\n2 We use the words “surface” and “manifold” interchangeably in this chapter. The analogy works well for two- dimensional manifolds embedded in a three-dimensional space, but it breaks down beyond three dimensions. A high-dimensional manifold does not conform to our usual notion of a “surface.” Some of the more out‐ landish manifolds have holes, and some loop back onto themselves in a way that would never happen in the real physical world (e.g., M.C. Escher’s endless waterfall). Most data models assume nice manifolds, not the crazy ones.\n\n115\n\nFigure 7-1. The Swiss roll, a nonlinear manifold\n\nThe key observation is that even when a big manifold looks complicated, the local neighborhood around each point can often be well approximated with a patch of flat surface. In other words, the patches to encode global structure using local structure.3 Nonlinear dimensionality reduction is also called nonlinear embedding or manifold learning. Nonlinear embeddings are useful for aggressively compressing high- dimensional data into low-dimensional data. They are often used for visualization in two or three dimensions.\n\nThe goal of feature engineering, however, isn’t so much to make the feature dimen‐ sions as low as possible, but to arrive at the right features for the task. In this chapter, the right features are those that represent the spatial characteristics of the data.\n\nClustering algorithms are usually not presented as techniques for local structure learning. But they in fact enable just that. Points that are close to each other (where “closeness” can be defined by a chosen metric) belong to the same cluster. Given a clustering, a data point can be represented by its cluster membership vector. If the number of clusters is smaller than the original number of features, then the new rep‐ resentation will have fewer dimensions than the original; the original data is com‐ pressed into a lower dimension. We will unpack this idea in this chapter.\n\nCompared to nonlinear embedding techniques, clustering may produce more fea‐ tures. But if the end goal is feature engineering instead of visualization, this is not a problem.\n\n3 This is a tried-and-true idea in mathematics. For instance, the derivative of a function measures the speed of change at each point. Globally, the function may do all sorts of weird things. But locally, it can be approxima‐ ted by a linear function of the derivative. If we know the derivative at each point, then calculus allows us to more or less recover the entire original function.\n\n116\n\n|\n\nChapter 7: Nonlinear Featurization via K-Means Model Stacking\n\nWe will illustrate the idea of local structure learning with a common clustering algo‐ rithm called k-means. It is simple to understand and implement. Instead of nonlinear manifold reduction, it is more apt to say that k-means performs nonlinear manifold feature extraction. Used correctly, it can be a powerful tool in our feature engineering repertoire.\n\nk-Means Clustering k-means is a clustering algorithm. Clustering algorithms group data depending on how they are laid out in space. They are unsupervised in that they do not require any sort of label—it’s the algorithm’s job to infer cluster labels based solely on the geome‐ try of the data itself.\n\nA clustering algorithm depends on a metric—a measurement of closeness between data points. The most popular metric is the Euclidean distance or Euclidean metric. It comes from Euclidean geometry and measures the straight-line distance between two points. It should feel very normal to us because this is the distance we see in everyday physical reality.\n\nThe Euclidean distance between two vectors x and y is the ℓ2 norm of x – y. (See “ℓ2 Normalization” on page 32 for more on the ℓ2 norm.) In math speak, it is usually written as ‖x – y‖2 or just ‖x – y‖.\n\nk-means establishes a hard clustering, meaning that each data point is assigned to one and only one cluster. The algorithm learns to position the cluster centers such that the total sum of the Euclidean distance between each data point and its cluster center is minimized. For those who like to read math instead of words, here is the objective function:\n\nminC1,…, Ck, μ1,…, μk∑\n\nk\n\ni=1\n\n∑ x∈Ci\n\n∥ x – μi ∥ 2\n\nEach cluster Ci contains a subset of data points. The center of cluster i is equal to the average of all the data points in the cluster:\n\nμi = ∑ x∈Ci\n\nx/ni\n\nwhere ni denotes the number of data points in cluster i.\n\nk-Means Clustering\n\n|\n\n117\n\nFigure 7-2 shows k-means at work on two different, randomly generated datasets. The data in (a) is generated from random Gaussian distributions with the same var‐ iance but different means. The data in (c) is generated uniformly at random. These toy problems are very simple to solve, and k-means does a good job. (The results could be sensitive to the number of clusters, which must be given to the algorithm.)\n\nFigure 7-2. k-means examples demonstrating how the clustering algorithm partitions space\n\nThe code for this example is found in Example 7-1.\n\nExample 7-1. Code to generate k-means examples\n\n>>> import numpy as np >>> from sklearn.cluster import KMeans >>> from sklearn.datasets import make_blobs\n\n>>> import matplotlib.pyplot as plt\n\n>>> n_data = 1000 >>> seed = 1 >>> n_clusters = 4\n\n# Generate random Gaussian blobs and run k-means >>> blobs, blob_labels = make_blobs(n_samples=n_data, n_features=2, ... centers=n_centers, random_state=seed) >>> clusters_blob = KMeans(n_clusters=n_centers, random_state=seed).fit_predict(blobs)\n\n# Generate data uniformly at random and run k-means >>> uniform = np.random.rand(n_data, 2) >>> clusters_uniform = KMeans(n_clusters=n_clusters,\n\n118\n\n|\n\nChapter 7: Nonlinear Featurization via K-Means Model Stacking\n\n... random_state=seed).fit_predict(uniform)\n\n# Matplotlib incantations for visualizing results >>> figure = plt.figure() >>> plt.subplot(221) >>> plt.scatter(blobs[:, 0], blobs[:, 1], c=blob_labels, cmap='gist_rainbow') >>> plt.title(\"(a) Four randomly generated blobs\", fontsize=14) >>> plt.axis('off')\n\n>>> plt.subplot(222) >>> plt.scatter(blobs[:, 0], blobs[:, 1], c=clusters_blob, cmap='gist_rainbow') >>> plt.title(\"(b) Clusters found via K-means\", fontsize=14) >>> plt.axis('off')\n\n>>> plt.subplot(223) >>> plt.scatter(uniform[:, 0], uniform[:, 1]) >>> plt.title(\"(c) 1000 randomly generated points\", fontsize=14) >>> plt.axis('off')\n\n>>> plt.subplot(224) >>> plt.scatter(uniform[:, 0], uniform[:, 1], c=clusters_uniform, cmap='gist_rainbow') >>> plt.title(\"(d) Clusters found via K-means\", fontsize=14) >>> plt.axis('off')\n\nClustering as Surface Tiling Common applications of clustering assume that there are natural clusters to be found; i.e., there are regions of dense data scattered in an otherwise empty space. In these situations, there is a notion of the correct number of clusters, and people have invented clustering indices that measure the quality of data groupings in order to select for k.\n\nHowever, when data is spread out fairly uniformly like in Figure 7-2(c), there is no longer a correct number of clusters. In this case, the role of a clustering algorithm is vector quantization, i.e., partitioning the data into a finite number of chunks. The number of clusters can be selected based on acceptable approximation error when using quantized vectors instead of the original ones.\n\nVisually, this usage of k-means can be thought of as covering the data surface with patches, like in Figure 7-3. This is indeed what we get if we run k-means on a Swiss roll dataset.\n\nClustering as Surface Tiling\n\n|\n\n119\n\nFigure 7-3. Conceptual local patches on the Swiss roll from a clustering algorithm\n\nExample 7-2 uses scikit-learn to generate a noisy dataset on the Swiss roll, cluster it with k-means, and visualize the clustering results using Matplotlib. The data points are colored according to their cluster IDs.\n\nExample 7-2. k-means on the Swiss roll\n\n>>> from mpl_toolkits.mplot3d import Axes3D >>> from sklearn import manifold, datasets\n\n# Generate a noisy Swiss roll dataset >>> X, color = datasets.samples_generator.make_swiss_roll(n_samples=1500)\n\n# Approximate the data with 100 k-means clusters >>> clusters_swiss_roll = KMeans(n_clusters=100, random_state=1).fit_predict(X)\n\n# Plot the dataset with k-means cluster IDs as the color >>> fig2 = plt.figure() >>> ax = fig2.add_subplot(111, projection='3d') >>> ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=clusters_swiss_roll, cmap='Spectral')\n\nIn this example, we generated 1,500 points at random on the Swiss roll surface, and asked k-means to approximate it with 100 clusters. We pulled the number 100 out of\n\n120\n\n|\n\nChapter 7: Nonlinear Featurization via K-Means Model Stacking\n\na hat because it seems like a fairly large number to cover a fairly small space. The result (Figure 7-4) looks nice; the clusters are indeed very local and different sections of the manifold are mapped to different clusters. Great! Are we done?\n\nFigure 7-4. Approximating a Swiss roll dataset using k-means with 100 clusters\n\nThe problem is that if we pick a k that is too small, then the results won’t be so nice from a manifold learning perspective. Figure 7-5 shows the output of k-means on the Swiss roll with 10 clusters. We can clearly see data from very different sections of the manifold being mapped to the same clusters (e.g., the yellow, purple, green, and magenta clusters—see, we told you the illustrations are best viewed in color!).\n\nFigure 7-5. k-means on the Swiss roll with 10 clusters\n\nIf the data is distributed uniformly throughout the space, then picking the right k boils down to a sphere-packing problem. In d dimensions, one could fit roughly 1/rd spheres of radius r. Each k-means cluster is a sphere, and the radius is the maximum error of representing points in that sphere with the centroid. So, if we are willing to tolerate a maximum approximation error of r per data point, then the number of clusters is O(1/rd), where d is the dimension of the original feature space of the data.\n\nClustering as Surface Tiling\n\n|\n\n121\n\nUniform distribution is the worst-case scenario for k-means. If data density is not uniform, then we will be able to represent more data with fewer clusters. In general, it is difficult to tell how data is distributed in high-dimensional space. One can be con‐ servative and pick a larger k, but it can’t be too large, because k will become the num‐ ber of features for the next modeling step.\n\nk-Means Featurization for Classification When using k-means as a featurization procedure, a data point can be represented by its cluster membership (a sparse one-hot encoding of the cluster membership catego‐ rical variable; see “One-Hot Encoding” on page 78), which we now illustrate.\n\nIf a target variable is also available, then we have the choice of giving that information as a hint to the clustering procedure. One way to incorporate target information is to simply include the target variable as an additional input feature to the k-means algo‐ rithm. Since the objective is to minimize the total Euclidean distance over all input dimensions, the clustering procedure will attempt to balance similarity in the target value as well as in the original feature space. The target values can be scaled to get more or less attention from the clustering algorithm. Larger differences in the target will produce clusters that pay more attention to the classification boundary.\n\nk-Means Featurization\n\nClustering algorithms analyze the spatial distribution of data. Therefore, k-means featurization creates a compressed spatial index of the data which can be fed into the model in the next stage. This is an example of model stacking.\n\nExample 7-3 shows a simple k-means featurizer. It is defined as a class object that can be fitted to training data and transform any new data.\n\nExample 7-3. k-means featurizer\n\n>>> import numpy as np >>> from sklearn.cluster import KMeans\n\n>>> class KMeansFeaturizer: ... \"\"\"Transforms numeric data into k-means cluster memberships. ... ... This transformer runs k-means on the input data and converts each data point ... into the ID of the closest cluster. If a target variable is present, it is ... scaled and included as input to k-means in order to derive clusters that ... obey the classification boundary as well as group similar points together. ... \"\"\" ... ... def __init__(self, k=100, target_scale=5.0, random_state=None):\n\n122\n\n|\n\nChapter 7: Nonlinear Featurization via K-Means Model Stacking\n\n... self.k = k ... self.target_scale = target_scale ... self.random_state = random_state ... ... def fit(self, X, y=None): ... \"\"\"Runs k-means on the input data and finds centroids. ... \"\"\" ... if y is None: ... # No target variable, just do plain k-means ... km_model = KMeans(n_clusters=self.k, ... n_init=20, ... random_state=self.random_state) ... km_model.fit(X) ... ... self.km_model_ = km_model ... self.cluster_centers_ = km_model.cluster_centers_ ... return self ... ... # There is target information. Apply appropriate scaling and include ... # it in the input data to k-means. ... data_with_target = np.hstack((X, y[:,np.newaxis]*self.target_scale)) ... ... # Build a pre-training k-means model on data and target ... km_model_pretrain = KMeans(n_clusters=self.k, ... n_init=20, ... random_state=self.random_state) ... km_model_pretrain.fit(data_with_target) ... ... # Run k-means a second time to get the clusters in the original space ... # without target info. Initialize using centroids found in pre-training. ... # Go through a single iteration of cluster assignment and centroid ... # recomputation. ... km_model = KMeans(n_clusters=self.k, ... init=km_model_pretrain.cluster_centers_[:,:2], ... n_init=1, ... max_iter=1) ... km_model.fit(X) ... ... self.km_model = km_model ... self.cluster_centers_ = km_model.cluster_centers_ ... return self ... ... def transform(self, X, y=None): ... \"\"\"Outputs the closest cluster ID for each input data point. ... \"\"\" ... clusters = self.km_model.predict(X) ... return clusters[:,np.newaxis] ... ... def fit_transform(self, X, y=None): ... self.fit(X, y) ... return self.transform(X, y)\n\nk-Means Featurization for Classification\n\n|\n\n123\n\nTo illustrate the difference between using and not using target information when clustering in Example 7-4, we apply the featurizer to a synthetic dataset generated using scikit-learn’s make_moons function and plot the Voronoi diagram of the cluster boundaries.\n\nExample 7-4. k-means featurization with and without target hints\n\n>>> from scipy.spatial import Voronoi, voronoi_plot_2d >>> from sklearn.datasets import make_moons\n\n>>> training_data, training_labels = make_moons(n_samples=2000, noise=0.2) >>> kmf_hint = KMeansFeaturizer(k=100, target_scale=10).fit(training_data, ... training_labels) >>> kmf_no_hint = KMeansFeaturizer(k=100, target_scale=0).fit(training_data, ... training_labels)\n\n>>> def kmeans_voronoi_plot(X, y, cluster_centers, ax): ... \"\"\"Plots the Voronoi diagram of the k-means clusters overlaid with the data\"\"\" ... ax.scatter(X[:, 0], X[:, 1], c=y, cmap='Set1', alpha=0.2) ... vor = Voronoi(cluster_centers) ... voronoi_plot_2d(vor, ax=ax, show_vertices=False, alpha=0.5)\n\nFigure 7-6 shows a comparison of the results. The two moons of the dataset are col‐ ored according to their class labels. The bottom panel shows the clusters trained without target information. Notice that a number of clusters span the empty space between the two classes. The top panel shows that when the clustering algorithm is given target information, the cluster boundaries align much better along class boundaries.\n\nLet’s test the effectiveness of k-means features for classification. Example 7-5 applies logistic regression on the input data augmented with k-means cluster features. It compares the results against the support vector machine with radial basis function kernel (RBF SVM), k-nearest neighbors (kNN), random forest (RF), and gradient boosting tree (GBT) classifiers. RF and GBT are popular nonlinear classifiers with state-of-the-art performance. RBF SVM is a reasonable nonlinear classifier for Eucli‐ dean space. kNN classifies data according to the average of its k nearest neighbors.\n\n124\n\n|\n\nChapter 7: Nonlinear Featurization via K-Means Model Stacking",
      "page_number": 126
    },
    {
      "number": 16,
      "title": "Segment 16 (pages 136-143)",
      "start_page": 136,
      "end_page": 143,
      "detection_method": "topic_boundary",
      "content": "Figure 7-6. k-means clusters with (top panel) and without (bottom panel) using target class information\n\nThe default input data to the classifiers consists of the 2D coordinates of each data point. Logistic regression is also given the cluster membership features (labeled “LR with k-means” in Figure 7-7). As a baseline, we also try logistic regression on just the 2D coordinates (labeled “LR”).\n\nExample 7-5. Classification with k-means cluster features\n\n>>> from sklearn.linear_model import LogisticRegression >>> from sklearn.svm import SVC >>> from sklearn.neighbors import KNeighborsClassifier >>> from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n\n### Generate some test data from the same distribution as training data >>> test_data, test_labels = make_moons(n_samples=2000, noise=0.3)\n\n### Use the k-means featurizer to generate cluster features >>> training_cluster_features = kmf_hint.transform(training_data) >>> test_cluster_features = kmf_hint.transform(test_data)\n\nk-Means Featurization for Classification\n\n|\n\n125\n\n### Form new input features with cluster features >>> training_with_cluster = scipy.sparse.hstack((training_data, ... training_cluster_features)) >>> test_with_cluster = scipy.sparse.hstack((test_data, test_cluster_features))\n\n### Build the classifiers >>> lr_cluster = LogisticRegression(random_state=seed).fit(training_with_cluster, ... training_labels) >>> classifier_names = ['LR', ... 'kNN', ... 'RBF SVM', ... 'Random Forest', ... 'Boosted Trees'] >>> classifiers = [LogisticRegression(random_state=seed), ... KNeighborsClassifier(5), ... SVC(gamma=2, C=1), ... RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1), ... GradientBoostingClassifier(n_estimators=10, learning_rate=1.0, ... max_depth=5)] >>> for model in classifiers: ... model.fit(training_data, training_labels)\n\n### Helper function to evaluate classifier performance using ROC >>> def test_roc(model, data, labels): ... if hasattr(model, \"decision_function\"): ... predictions = model.decision_function(data) ... else: ... predictions = model.predict_proba(data)[:,1] ... fpr, tpr, _ = sklearn.metrics.roc_curve(labels, predictions) ... return fpr, tpr\n\n### Plot results >>> import matplotlib.pyplot as plt >>> plt.figure() >>> fpr_cluster, tpr_cluster = test_roc(lr_cluster, test_with_cluster, test_labels) >>> plt.plot(fpr_cluster, tpr_cluster, 'r-', label='LR with k-means')\n\n>>> for i, model in enumerate(classifiers): ... fpr, tpr = test_roc(model, test_data, test_labels) ... plt.plot(fpr, tpr, label=classifier_names[i])\n\n>>> plt.plot([0, 1], [0, 1], 'k--') >>> plt.legend()\n\nFigure 7-7 shows the receiver operating characteristic (ROC) curves of each of the classifiers when evaluated on the test set. A ROC curve shows the trade-off between true positives and false positives as we vary the classification decision boundary. (See Zheng [2015] for more details.) A good classifier should quickly reach a high true positive rate and a low false positive rate, so curves that rise sharply toward the upper-left corner are good.\n\n126\n\n|\n\nChapter 7: Nonlinear Featurization via K-Means Model Stacking\n\nFigure 7-7. ROCs of k-means + logistic regression versus nonlinear classifiers and plain logistic regression on the synthetic two-moons dataset\n\nOur plot shows that logistic regression performs much better with cluster features than without. In fact, with cluster features, the linear classifier performs just as well as nonlinear classifiers. One minor caveat is that in this toy example, we did not tune the hyperparameters for any of the models. There may be performance differences once the models are fully tuned, but at least this shows that it is possible for LR with k-means to be on a par with nonlinear classifiers. This is a nice result because linear classifiers are much cheaper to train than nonlinear classifiers. Lower computation cost allows us to try more models with different features in the same period of time, which increases the chance of ending up with a much better model.\n\nAlternative Dense Featurization Instead of one-hot cluster membership, a data point can also be represented by a dense vector of its inverse distance to each cluster center. This retains more informa‐ tion than simple binary cluster assignment, but the representation is now dense. There is a trade-off here. One-hot cluster membership results in a very lightweight, sparse representation, but one might need a larger k to represent data of complex shapes. Inverse distance representation is dense, which could be more expensive for the modeling step, but one might be able to get away with a smaller k.\n\nA compromise between sparse and dense is to retain inverse distances for only p of the closest clusters. But now p is an extra hyperparameter to tune. (Can you under‐ stand why feature engineering requires so much fiddling?) There is no free lunch.\n\nk-Means Featurization for Classification\n\n|\n\n127\n\nPros, Cons, and Gotchas Using k-means to turn spatial data into features is an example of model stacking, where the input to one model is the output of another. Another example of stacking is to use the output of a decision tree–type model (random forest or gradient boost‐ ing tree) as input to a linear classifier. Stacking has become an increasingly popular technique in recent years. Nonlinear classifiers are expensive to train and maintain. The key intuition with stacking is to push the nonlinearities into the features and use a very simple, usually linear model as the last layer. The featurizer can be trained off‐ line, which means that one can use expensive models that require more computation power or memory but generate useful features. The simple model at the top level can be quickly adapted to the changing distributions of online data. This is a great trade- off between accuracy and speed, and this strategy is often used in applications like targeted advertising that require fast adaptation to changing data distributions.\n\nKey Intuition for Model Stacking\n\nUse sophisticated base layers (often with expensive models) to gen‐ erate good (often nonlinear) features, combined with a simple and fast top-layer model. This often strikes the right balance between model accuracy and speed.\n\nCompared to using a nonlinear classifier, k-means stacked with logistic regression is cheaper to train and store. Table 7-1 is a chart detailing the training and prediction complexity in both computation and memory for a number of machine learning models. n denotes the number of data points, d the number of (original) features.\n\nTable 7-1. Complexity of ML models\n\nModel\n\nk-means training\n\nk-means predict\n\nTime O(nkd)a O(kd)\n\nSpace O(kd)\n\nO(kd)\n\nLR + cluster features training\n\nO(n(d+k))\n\nO(d+k)\n\nLR + cluster features predict\n\nRBF SVM training\n\nRBF SVM predict\n\nGBT training\n\nGBT predict\n\nkNN training\n\nO(d+k) O(n2d) O(sd) O(nd2mt) O(2mt) O(1)\n\nO(d+k) O(n2) O(sd) O(nd + 2mt) O(2mt) O(nd)\n\nO(nd + k log n)\n\nO(nd)\n\nkNN predict a Streaming k-means can be done in time O(nd (log k + log log n)), which is much faster than O(nkd) for large k.\n\n128\n\n|\n\nChapter 7: Nonlinear Featurization via K-Means Model Stacking\n\nFor k-means, the training time is O(nkd) because each iteration involves computing the d-dimensional distance between every data point and every centroid (k). We opti‐ mistically assume that the number of iterations is not a function of n, though this may not be true in all cases. Prediction requires computing the distance between the new data point and each of the k centroids, which is O(kd). The storage space requirement is O(kd), for the coordinates of the k centroids.\n\nLogistic regression training and prediction are linear in both the number of data points and feature dimensions. RBF SVM training is expensive because it involves computing the kernel matrix for every pair of input data. RBF SVM prediction is less expensive than training; it is linear in the number of support vectors s and the feature dimension d. GBT training and prediction are linear in data size and the size of the model (t trees, each with at most 2m leaves, where m is the maximum depth of the tree). A naive implementation of kNN requires no training time at all because the training data itself is essentially the model. The cost is paid at prediction time, where the input must be evaluated against each of the original training points and partially sorted to retrieve the k closest neighbors.\n\nOverall, k-means + LR is the only combination that is linear (with respect to the size of training data, O(nd), and model size, O(kd)) at both training and prediction time. The complexity is most similar to that of GBT, which has costs that are linear in the number of data points, the feature dimension, and the size of the model (O(2mt)). It is hard to say whether k-means + LR or GBT will result in a smaller model—it depends on the spatial characteristics of the data.\n\nPotential for Data Leakage\n\nThose who remember our caution regarding data leakage (see “Guarding against data leakage” on page 93) might ask whether including the target variable in the k-means featurization step would cause such a problem. The answer is “yes,” but not as much in the case of bin counting. If we use the same dataset for learning the clusters and building the classification model, then information about the target will have leaked into the input variables. As a result, accuracy evaluations on the training data will probably be overly optimistic, but the bias will go away when evaluating on a hold-out validation set or test set. Furthermore, the leakage will not be as bad as in the case of bin-counting statistics (see “Bin Count‐ ing” on page 87), because the lossy compression of the clustering algorithm will have abstracted away some of that information. To be extra careful about preventing leakage, hold out a separate data‐ set for deriving the clusters, just like in the case of bin counting.\n\nk-means featurization is useful for real-valued, bounded numeric features that form clumps of dense regions in space. The clumps can be of any shape, because we can\n\nPros, Cons, and Gotchas\n\n|\n\n129\n\njust increase the number of clusters to approximate them. (Unlike in the classic clus‐ tering setup, we are not concerned with discovering the “true” number of clusters; we only need to cover them.)\n\nk-means cannot handle feature spaces where the Euclidean distance does not make sense—i.e., weirdly distributed numeric variables or categorical variables. If the fea‐ ture set contains those variables, then there are several ways to handle them:\n\n1. Apply k-means featurization only on the real-valued, bounded numeric features. 2. Define a custom metric to handle multiple data types and use the k-medoids algorithms. (k-medoids is analogous to k-means but allows for arbitrary distance metrics.)\n\n3. Convert categorical variables to binning statistics (see “Bin Counting” on page 87), then featurize them using k-means.\n\nCombined with techniques for handling categorical variables and time series, k- means featurization can be adapted to handle the kind of rich data that often appears in customer marketing and sales analytics. The resulting clusters can be thought of as user segments, which are very useful features for the next modeling step.\n\nSummary This chapter illustrated the concept of model stacking using a somewhat unconven‐ tional approach: combining supervised k-means with a simple linear classifier. k- means is usually used as an unsupervised modeling method to find dense clusters of data points in feature space. Here, however, k-means is optionally given the class labels as input. This helps k-means to find clusters that better align with the bound‐ ary between classes.\n\nDeep learning, which we will discuss in the next chapter, takes model stacking to a whole new level by layering neural networks on top of one another. Two recent win‐ ners of the ImageNet Large Scale Visual Recognition Challenge involved 13 and 22 layers of neural networks. They take advantage of the availability of lots of unlabeled training images and look for combinations of pixels that yield good image fea‐ tures. The technique in this chapter separately trains the k-means featurizer from the linear classifier. But it’s possible to jointly optimize the featurizer and the classier. As we shall see, deep learning training takes the latter route.\n\n130\n\n|\n\nChapter 7: Nonlinear Featurization via K-Means Model Stacking\n\nBibliography Dunning, Ted. The man is a walking encyclopedia of data science. He is a frequent speaker at industry events, and likes beer and nice people. Buy him a beer and talk to him. You won’t regret it.\n\nZheng, Alice. Evaluating Machine Learning Models. Sebastopol, CA: O’Reilly Media, 2015.\n\nBibliography\n\n|\n\n131\n\nCHAPTER 8 Automating the Featurizer: Image Feature Extraction and Deep Learning\n\nSight and sound are innate sensory inputs for humans. Our brains are hardwired to rapidly evolve our abilities to process visual and auditory signals, with some systems developing to respond to stimulus even before birth (Eliot, 2000). Language skills, on the other hand, are learned. They take months to develop and years to master. Many people take the development of their vision and hearing for granted, but all of us have had to intentionally train our brains to understand and use language.\n\nInterestingly, the situation is the reverse for machine learning. We have made much more headway with text analysis applications than image or audio. Take the problem of search, for example. People have enjoyed years of relative success in information retrieval and text search, whereas image and audio search are still being perfected (though the breakthrough in deep learning models in the last five years may finally herald the long-awaited revolution in image and speech analysis).\n\nThe difficulty of progress is directly related to the difficulty of extracting meaningful features from the respective types of data. Machine learning models require semanti‐ cally meaningful features to make semantically meaningful predictions. In text analy‐ sis, particularly for languages such as English where a basic unit of semantic meaning (a word) is easily extractable, progress can be made very fast. Images and audio, on the other hand, are recorded as digital pixels or waveforms. A single “atom” in an image is a pixel. In audio data, it is a single measurement of waveform intensity. These contain much less semantic information than an atom—a word—of text data. Therefore, the job of feature extraction and engineering is much more challenging on image and audio than on text.\n\nIn the last 20 years, computer vision research has focused on manually defined pipe‐ lines for extracting good image features. For a while, image feature extractors such as\n\n133",
      "page_number": 136
    },
    {
      "number": 17,
      "title": "Segment 17 (pages 144-154)",
      "start_page": 144,
      "end_page": 154,
      "detection_method": "topic_boundary",
      "content": "SIFT and HOG (described in the following sections) were the standard. Recent devel‐ opments in deep learning research have extended the reach of traditional machine learning models by incorporating automatic feature extraction in the base layers. They essentially replace manually defined feature image extractors with manually defined models that automatically learn and extract features. The manual work is still there, just abstracted further into the belly of the modeling beast.\n\nIn this chapter, we will start with the most popular image feature extractors and then dive into the most complicated modeling machinery covered in this book: deep learning for feature learning.\n\nThe Simplest Image Features (and Why They Don’t Work) What are the right features to extract from an image? The answer of course depends on what we are trying to do with those features. Let’s say our task is image retrieval: we are given a picture and asked to find similar pictures from a database of images. We need to decide how to represent each image, and how to measure the differences between them. Can we just look at the percentage of different colors in an image? Figure 8-1 shows two pictures having roughly the same color profile but very differ‐ ent meanings; one looks like white cloud in a blue sky, and the other is the flag of Greece. So, color information is probably not enough to characterize an image.\n\nFigure 8-1. Blue and white pictures—same color profile, very different meanings\n\nAnother simple idea is to measure the pixel value differences between images. First, resize the images to have the same width and height. Each image is represented by a matrix of pixel values. The matrix can be stacked into one long vector, either by row or by column. The color of each pixel (e.g., the RGB encoding of the color) is now a feature of the image. Finally, measure the Euclidean distance between the long pixel vectors. This would definitely allow us to tell apart the Greek flag and the white clouds, but it is too stringent as a similarity measure. A cloud could take on a thou‐ sand different shapes and still be a cloud. It could be shifted to the side of the image, or half of it might lie in shadow. All of these transformations would increase the\n\n134\n\n|\n\nChapter 8: Automating the Featurizer: Image Feature Extraction and Deep Learning\n\nEuclidean distance, but they shouldn’t change the fact that the picture is still of a cloud.\n\nThe problem is that individual pixels do not carry enough semantic information about the image. Therefore, they are bad atomic units for analysis.\n\nManual Feature Extraction: SIFT and HOG In 1999, computer vision researchers figured out a better way to represent images using statistics of image patches: the Scale Invariant Feature Transform (SIFT) [Lowe, 1999].\n\nSIFT was originally developed for the task of object recognition, which involves not only correctly tagging the image as containing an object, but pinpointing its location in the image. The process involves analyzing the image at a pyramid of possible scales, detecting interest points that could indicate the presence of the object, extract‐ ing features (commonly called image descriptors in computer vision) about the inter‐ est points, and determining the pose of the object.\n\nOver the years, the usage of SIFT expanded to extract features not only for interest points but across the entire image. The SIFT feature extraction procedure is very sim‐ ilar to another technique, called the Histogram of Oriented Gradients (HOG) [Dalal and Triggs, 2005]. Both of them essentially compute histograms of gradient orienta‐ tions. We now describe this process in detail.\n\nImage Gradients To do better than raw pixel values, we have to somehow “organize” the pixels into more informative units. Differences between neighboring pixels are often very useful. Pixel values usually differ at the boundary of objects, when there is a shadow, within a pattern, or on a textured surface. The difference in value between neighboring pixels is called an image gradient.\n\nThe simplest way to compute the image gradient is to separately calculate the differ‐ ences along the horizontal (x) and vertical (y) axes of the image, then compose them into a 2D vector. This involves two 1D difference operations that can be handily rep‐ resented by a vector mask or filter. The mask [1, 0, –1] takes the difference between the left neighbor and the right neighbor or the up-neighbor and the down-neighbor, depending on which direction we apply the mask. There are 2D gradient filters as well, but for the purpose of this example, the 1D filter suffices.\n\nTo apply a filter to an image, we perform a convolution. It involves flipping the filter and taking the inner product with a small patch of the image, then moving to the next patch. Convolutions are very common in signal processing. We’ll use ∗ to denote the operation:\n\nManual Feature Extraction: SIFT and HOG\n\n|\n\n135\n\n[a b c] ∗ [1 2 3] = c*1 + b*2 + a*3\n\nThe x and y gradients at pixel (i,j) are:\n\ngx(i,j) = [1 0 –1] ∗ [I(i – 1,j) I(i,j) I(i + 1,j)] = –1 * I(i – 1,j) + 1 * I(i + 1,j)\n\ngy(i,j) = [1 0 –1] ∗ [I(i,j – 1) I(i,j) I(i,j + 1)] = –1 * I(i,j – 1) + 1 * I(i,j + 1)\n\nTogether, they form the gradient:\n\n∇I(i, j) =\n\ngx(i, j) gy(i, j)\n\nA vector can be completely described by its direction and magnitude. The magnitude of the gradient is equal to the Euclidean norm of the gradient ( gx 2), which indi‐ cates how much the pixel values change around the pixel. The direction or orienta‐ tion of the gradient depends on the relative size of the change in the horizontal and vertical directions; it can be computed as θ = arctan( gy gx). Figure 8-2 illustrates these mathematical concepts.\n\n2 + gy\n\nFigure 8-3 illustrates examples of the simple image gradient that is composed of the vertical and horizontal gradients. Each example is an image of nine pixels. Each pixel is labeled with a grayscale value. (Smaller numbers correspond to a darker color.) The gradient for the center pixel is shown below each image. The image on the left con‐ tains horizontal stripes, where the color only changes vertically. Therefore, the hori‐ zontal gradient is zero and the gradient is nonzero vertically. The center image contains vertical stripes; therefore, the horizontal gradient is zero. The image on the right contains diagonal stripes and the gradient is also diagonal.\n\n136\n\n|\n\nChapter 8: Automating the Featurizer: Image Feature Extraction and Deep Learning\n\nFigure 8-2. Illustration of the definition of an image gradient\n\nFigure 8-3. Simple examples of the image gradient\n\nThe definition works on synthetic toy examples. But would it work well on a real image? In Example 8-1, we examine this using a picture of a cat from scikit-image, shown in Figure 8-4 with its horizontal and vertical gradients. Since the gradients are computed at every pixel location of the original image, we end up with two new matrices, each of which can be visualized as an image.\n\nManual Feature Extraction: SIFT and HOG\n\n|\n\n137\n\nExample 8-1. Calculating simple image gradients using Python\n\n>>> import matplotlib.pyplot as plt >>> import numpy as np >>> from skimage import data, color\n\n### Load the example image and turn it into grayscale >>> image = color.rgb2gray(data.chelsea())\n\n### Compute the horizontal gradient using the centered 1D filter. ### This is equivalent to replacing each non-border pixel with the ### difference between its right and left neighbors. The leftmost ### and rightmost edges have a gradient of 0. >>> gx = np.empty(image.shape, dtype=np.double) >>> gx[:, 0] = 0 >>> gx[:, -1] = 0 >>> gx[:, 1:-1] = image[:, :-2] - image[:, 2:]\n\n### Same deal for the vertical gradient >>> gy = np.empty(image.shape, dtype=np.double) >>> gy[0, :] = 0 >>> gy[-1, :] = 0 >>> gy[1:-1, :] = image[:-2, :] - image[2:, :]\n\n### Matplotlib incantations >>> fig, (ax1, ax2, ax3) = plt.subplots(3, 1, ... figsize=(5, 9), ... sharex=True, ... sharey=True)\n\n>>> ax1.axis('off') >>> ax1.imshow(image, cmap=plt.cm.gray) >>> ax1.set_title('Original image') >>> ax1.set_adjustable('box-forced')\n\n>>> ax2.axis('off') >>> ax2.imshow(gx, cmap=plt.cm.gray) >>> ax2.set_title('Horizontal gradients') >>> ax2.set_adjustable('box-forced')\n\n>>> ax3.axis('off') >>> ax3.imshow(gy, cmap=plt.cm.gray) >>> ax3.set_title('Vertical gradients') >>> ax3.set_adjustable('box-forced')\n\n138\n\n|\n\nChapter 8: Automating the Featurizer: Image Feature Extraction and Deep Learning\n\nFigure 8-4. Gradients of an image of a cat\n\nNote that the horizontal gradient picks out strong vertical patterns such as the inner edges of the cat’s eyes, while the vertical gradient picks out strong horizontal patterns such as the whiskers and the upper and lower lids of the eyes. This might seem a little paradoxical at first, but it makes sense once we think about it a bit more. The hori‐ zontal (x) gradient identifies changes in the horizontal direction. A strong vertical pattern spans multiple y pixels at roughly the same x position. Hence, vertical pat‐ terns result in horizontal differences in pixel values. This is what our eyes detect as well.\n\nGradient Orientation Histograms Individual image gradients can pick out minute differences in an image neighbor‐ hood. But our eyes see bigger patterns than that. For instance, we see an entire cat’s whisker, not just a small section. The human vision system identifies contiguous pat‐ terns in a region, so we still have more work to do to summarize the image gradients in a neighborhood.\n\nHow exactly might we summarize vectors? A statistician would answer, “Look at the distribution!” SIFT and HOG both take this path. In particular, they compute (nor‐ malized) histograms of the gradient vectors as image features. A histogram divides data into bins and counts how many data points are in each bin; this is an (unnor‐ malized) empirical distribution. Normalization ensures that the counts sum to 1. The mathematical language is that it has unit ℓ1 norm.\n\nManual Feature Extraction: SIFT and HOG\n\n|\n\n139\n\nAn image gradient is a vector, and vectors can be represented by two components: the orientation and magnitude. So, we still need to decide how to design the histo‐ gram to take both components into account. SIFT and HOG settled on a scheme where the image gradients are binned by their orientation angle θ, weighted by the magnitude of each gradient. Here is the procedure:\n\n1. Divide 0°–360° into equal-sized bins.\n\n2. For each pixel in the neighborhood, add a weight w to the bin corresponding to its orientation θ. w is a function of the magnitude of the gradient and other rele‐ vant information. For instance, that information might be the inverse distance of the pixel to the center of the image patch. The idea is that the weight should be large if the gradient is large, and pixels near the center of the image neighbor‐ hood matter more than pixels that are farther away.\n\n3. Normalize the histogram.\n\nFigure 8-5 provides an illustration of a gradient orientation histogram of 8 bins com‐ posed from an image neighborhood of 4 × 4 pixels.\n\nFigure 8-5. Illustration of a gradient orientation histogram of 8 bins based on gradients from a 4 × 4 square cell of pixels\n\nThere are, of course, a number of knobs to tweak in the basic gradient orientation histogram algorithm, as well as some optional bells and whistles. As usual, the right settings are probably highly dependent on the particular images one wants to analyze.\n\nLet’s examine next some of the decisions to make and the effects these can have on your model.\n\n140\n\n|\n\nChapter 8: Automating the Featurizer: Image Feature Extraction and Deep Learning\n\nHow many bins should there be? Should they span from 0°–360° (signed gradients) or 0°–180° (unsigned gradients)?\n\nHaving more bins leads to finer-grained quantization of gradient orientation, and thus retains more information about the original gradients. But having too many bins is unnecessary and could lead to overfitting to the training data. For example, recognizing a cat in an image probably does not depend on the cat’s whisker being oriented exactly at 3°.\n\nThere is also the question of whether the bins should span from 0°–360°, which would retain the sign of the gradient along the y-axis, or from 0°–180°, which would not retain the sign of the vertical gradient. The authors of the original HOG paper (Dalal and Triggs, 2005) experimentally determined that 9 bins spanning from 0°– 180° is best, whereas the SIFT paper (Lowe, 2004) recommended 8 bins spanning from 0°–360°.\n\nWhat weight functions should be used?\n\nThe HOG paper compares various gradient magnitude weighting schemes: the mag‐ nitude itself, its square or square root, binarized, or clipped at the high or low ends. The plain magnitude, without adornments, performed the best in the authors’ experi‐ ments.\n\nSIFT also uses the plain magnitude of the gradient. Additionally, it wants to avoid sudden changes in the feature descriptor resulting from small changes in the position of the image window, so it downweights gradients that come from the edges of the neighborhood using a Gaussian distance function measured from the window center.\n\n∥2/2σ 2\n\n∥p-p0\n\n1\n\n, where p is the In other words, the gradient magnitude is multiplied by location of the pixel that generated the gradient, p0 is the location of the center of the image neighborhood, and σ, the width of the Gaussian, is set to one-half the radius of the neighborhood.\n\n2πσ 2 e\n\nSIFT also wants to avoid large changes in the orientation histogram resulting from small changes in the orientation of individual image gradients. So, it uses an interpo‐ lation trick that spreads the weight from a single gradient into adjacent orientation bins. In particular, the root bin (the bin that the gradient is assigned to) gets a vote of 1 times the weighted magnitude. Each of the adjacent bins get a vote of 1 – d, where d is the difference in histogram bin unit from the root bin.\n\nOverall, the vote from a single image gradient for SIFT is:\n\nw(∇p,b) = wbσ ∥ ∇ p ∥\n\nwhere ∇p is the gradient of pixel p in bin b, wb is the interpolation weight of b, and σ is the Gaussian distance to the center from p.\n\nManual Feature Extraction: SIFT and HOG\n\n|\n\n141\n\nHow are neighborhoods defined? How should they cover the image?\n\nHOG and SIFT both settled on a two-level representation of image neighborhoods: first adjacent pixels are organized into cells, and neighboring cells are then organized into blocks. An orientation histogram is computed for each cell, and the cell histo‐ gram vectors are concatenated to form the final feature descriptor for the whole block.\n\nSIFT uses cells of 16 × 16 pixels, organized into 8 orientation bins, then grouped by blocks of 4 × 4 cells, making for 4 × 4 × 8 = 128 features for the image neighborhood.\n\nThe HOG paper experimented with rectangular and circular shapes for the cells and blocks. Rectangular cells are called R-HOG blocks. The best R-HOG setting was found to be 8 × 8 pixels with 9 orientation bins each, grouped into blocks of 2 × 2 cells. Circular cells are called C-HOG blocks, with variants determined by the radius of the central cell, whether or not the cells are radially divided, the width of the outer cells, etc.\n\nNo matter how the neighborhoods are organized, they typically overlap to form the feature vector for the whole image. In other words, cells and blocks shift across the image horizontally and vertically, a few pixels at a time, to cover the entire image.\n\nThe main ingredients of neighborhood architecture are multilevel organization and overlapping windows that shift across the image. The same ingredients are utilized in the design of deep learning networks.\n\nWhat kind of normalization should be done?\n\nNormalization evens out the feature descriptors so that they have comparable magni‐ tude. It is synonymous with scaling, which we discussed in Chapter 4. We found that feature scaling on text features (in the form of tf-idf) did not have a large effect on classification accuracy. The story is quite different for image features, which can be quite sensitive to changes in lighting and contrast that appear in natural images. For instance, consider images of an apple under a strong spotlight versus a soft diffused light coming through a window. The image gradients would have very different mag‐ nitudes, even though the object is the same. For this reason, image featurization in computer vision usually starts with global color normalization to remove illumina‐ tion and contrast variance. For SIFT and HOG, it turns out that such preprocessing is unnecessary so long as we normalize the features.\n\nSIFT follows a normalize–threshold–normalize scheme. First, the block feature vec‐ tor is normalized to unit length (ℓ2 normalization). Then, the features are clipped to a maximum value in order to get rid of extreme lighting effects such as color saturation from the camera. Finally, the clipped features are again normalized to unit length.\n\nThe HOG paper experimented with different normalization schemes involving ℓ2 and ℓ1 norms, including the normalize–threshold–normalize scheme used in the SIFT\n\n142\n\n|\n\nChapter 8: Automating the Featurizer: Image Feature Extraction and Deep Learning\n\npaper. The authors found pure ℓ1 normalization to be slightly less reliable than the other methods (which performed comparably).\n\nSIFT Architecture The SIFT pipeline requires quite a number of steps. HOG is slightly simpler but fol‐ lows many of the same basic steps, such as creating a gradient histogram and normal‐ ization. Figure 8-6 illustrates the SIFT architecture. Starting from a region of interest in the original image, we first divide the region into a grid. Each grid cell is then fur‐ ther divided into subgrids. Each subgrid element contains a number of pixels, and each pixel produces a gradient. Each subgrid element produces a weighted gradient estimate, where the weights are chosen so that gradients outside of the subgrid ele‐ ment can contribute. These gradient estimates are then aggregated into an orienta‐ tion histogram for the subgrid, where gradients can have weighted votes as described previously. The orientation histograms for each subgrid are then concatenated to form a long gradient orientation histogram for the entire grid. (If the grid is divided into 2 × 2 subgrids, then there will be 4 gradient orientation histograms to concate‐ nate into 1.) This is the feature vector for the grid, which then goes through a nor‐ malize–threshold–normalize process. First, the vector is normalized to have unit norm. Then, individual values are clipped to a maximum threshold. Finally, the thresholded vector is normalized again. This is the final SIFT feature descriptor for the image patch.\n\nFigure 8-6. SIFT architecture—steps to produce a feature vector for a region of interest in the original image\n\nManual Feature Extraction: SIFT and HOG\n\n|\n\n143\n\nLearning Image Features with Deep Neural Networks SIFT and HOG went a long way toward defining good image features. However, the latest gains in computer vision have come from a very different direction: deep neural network models. The breakthrough happened at the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012, where a group of researchers from the University of Toronto nearly halved the error rate of the previous year’s winner. They branded their method “deep learning” to emphasize that, unlike previous architec‐ ture neural network models, the latest generation contains many layers of neural net‐ works and transformations stacked on top of each other. The winning model of ILSVRC 2012—subsequently dubbed AlexNet, after the name of the lead author—has 13 layers (Krizhevsky et al., 2012). The winner of ILSVRC 2014, GoogLeNet, has 22 layers (Szegedy et al., 2014).\n\nOn the surface, the mechanism of stacked neural networks appears very different from the image gradient histograms of SIFT and HOG. But a visualization of AlexNet shows that the first few layers are essentially computing edge gradients and other simple patterns, much like SIFT and HOG. Subsequent layers combine local patterns into more global patterns. The end result is a feature extractor that is much more powerful than what came before.\n\nThe infrastructure of stacked layers of neural networks (or any other classification model) is not new. But training such complex models requires a lot of data and a lot of computing power, which was not available until recently. The ImageNet dataset contains a labeled set of 1.2 million images from 1,000 classes. Modern GPUs have sped up matrix-vector computations, which lie at the inner core of many machine learning models (including neural networks). The success of deep learning methods rests upon the availability of lots of data and lots of GPU hours.\n\nDeep learning architectures can be composed of several types of layers. AlexNet, for instance, contains fully connected, convolutional response normalization, and max- pooling layers. We’ll now look at each of these in turn.\n\nFully Connected Layers At the core of all neural networks are linear functions of the input. Logistic regres‐ sion, which we encountered in Chapter 4, is an example of a neural network. A fully connected neural network is simply a set of linear functions of all of the input fea‐ tures. Recall that a linear function can be written as an inner product between the input feature vector and a weight vector, plus a possible constant term. A collection of linear functions can be represented as a matrix-vector product, where the weight vector becomes a weight matrix (W).\n\n144\n\n|\n\nChapter 8: Automating the Featurizer: Image Feature Extraction and Deep Learning",
      "page_number": 144
    },
    {
      "number": 18,
      "title": "Segment 18 (pages 155-164)",
      "start_page": 155,
      "end_page": 164,
      "detection_method": "topic_boundary",
      "content": "The mathematical definition of a fully connected layer is:\n\nz = Wx + b\n\nwhere each row of W is a weight vector that maps the entire input vector x into a single output in z. b is a vector of scalars representing the constant offset (or bias) for each neuron.\n\nThe fully connected layer is so named because every input can be used in every out‐ put. Mathematically, this means that there are no restrictions on the values in the matrix W. (As we will soon see, a convolutional layer makes use of only a small sub‐ set of inputs for each output.) Pictorially, a fully connected neural net can be repre‐ sented by a complete bipartite graph where every node in the input is connected to every node in the output (see Figure 8-7).\n\nFigure 8-7. A fully connected neural network, represented as a graph\n\nFully connected layers contain the maximum possible number of parameters (#input × #output)—hence, they are considered expensive. Such dense connection allows the network to detect global patterns that could involve all inputs. The last two layers of AlexNet are fully connected for this reason. The outputs are still independent from each other, conditioned on the inputs.\n\nLearning Image Features with Deep Neural Networks\n\n|\n\n145\n\nConvolutional Layers In contrast to fully connected layers, a convolutional layer uses only a subset of inputs for each output. The transformation “moves” across the input, producing out‐ puts using a few features at a time. For simplicity, one can use the same weights for different sets of input, instead of learning new weights for each set of input.\n\nMathematically, the convolution operator takes two functions as input and produces one function as output. It flips one of the input functions, moves it across the other function, and outputs the total area under the multiplied curves at each point:\n\n(f * g)(t) =∫–∞\n\n∞\n\nf (τ)g(t – τ)dτ =∫–∞\n\n∞\n\ng(τ)f (t – τ)dτ\n\nThe way to compute total area under a curve is to take its integral. The operator is symmetric in the inputs, meaning that it does not matter whether we flip the first input or the second; the output is the same.\n\nWe’ve already seen an example of simple convolution, when we looked at image gra‐ dients (“Image Gradients” on page 135). But the mathematical definition of convolu‐ tion may still appear to be somewhat convoluted. There is reason to its madness. It’s easiest to explain the intuition behind convolution using an example from signal processing.\n\nImagine that we have a little black box. To see what the black box does, we pass a single unit of stimulus through it. We record whatever the output looks like on a little sheet of paper. We wait until there is no more response to the original stimulus. The resulting function over time is the response function; let’s call it g(t).\n\nImagine now that we have some crazy wild signal f(t), which we proceed to feed through the black box. At time t = 0, f(0) interacts with the black box and produces f(0) multiplied by g(0). At time t = 1, f(1) enters the black box and gets multiplied by g(0). At the same time, the black box continues to respond to the previous signal f(0), which is now multiplied by g(1). So, the total output at time t = 1 is (f(0) * g(1)) + (f(1) * g(0)). At time t = 2, the situation gets even more complicated, with f(2) enter‐ ing the picture, and f(0) and f(1) continuing to generate their responses. The total output at time t = 2 is (f(0) * g(2)) + (f(1) * g(1)) + (f(2) * g(0)). In this way, the response function effectively gets flipped in time, with τ = 0 always interacting with whatever is currently entering the black box, and the tail of the response function interacting with whatever came before.\n\nFigure 8-8 illustrates the quantities at play at each time step (note that we’ve made time discrete for convenience of description—in reality, time is continuous, so the summation is really an integral). When computing the value of the convolution at a particular time step, you multiply the overlapping signals together and sum them.\n\n146\n\n|\n\nChapter 8: Automating the Featurizer: Image Feature Extraction and Deep Learning\n\nFigure 8-8. Convolution of two discrete signals, f and g\n\nThis black box is called a linear system because it doesn’t do anything more crazy than scalar multiplication and summation. The convolution operator cleanly cap‐ tures the effect of a linear system.\n\nIntuition Behind Convolution\n\nThe convolution operator captures the effect of a linear system, which multiplies the incoming signal with its response function, summing over current responses to all past input.\n\nIn our example, g(t) is used to denote the response function, and f(t) the input. But since convolution is symmetric, it doesn’t really matter which is the response and which the input. The output is simply a combination of both. g(t) is also known as a filter.1\n\nImages are two-dimensional signals, so we need a 2D filter. A 2D convolutional filter extends the 1D case by taking the integral over two variables:\n\n(f * g) i, j = ∑\n\nm\n\nn\n\n∑\n\nf u, v g i – u, j – v\n\nu=0\n\nv=0\n\n1 Technically, a filter is a transformation that eliminates certain parts of the Fourier spectrum. But it is increas‐\n\ningly common to use “filter” as a generic term.\n\nLearning Image Features with Deep Neural Networks\n\n|\n\n147\n\nSince digital images have discrete pixels, the convolution integrals become discrete sums. Furthermore, since the number of pixels is finite, the filter function only needs a finite number of elements. In image processing, a 2D convolutional filter is also known as a kernel or a mask.\n\nWhen applying a convolutional filter to an image, one does not necessarily define a giant filter that covers the entire image. Rather, one formulates a small filter covering just a few pixels by a few pixels and applies the same filter across the image, shifting over the horizontal and vertical pixel directions (see Figure 8-9).\n\nFigure 8-9. Structure of a 1D convolutional neural net\n\nBecause the same filter is used across the image, one only needs to define a small set of parameters. The trade-off is that the filter can absorb information only within a small pixel neighborhood at a time. In other words, a convolutional neural net identi‐ fies local patterns instead of global ones.\n\nConvolutional Filter Example In this example, we apply a Gaussian filter to an image. The Gaussian function forms a smooth and symmetric mound around zero. The filter produces a weighted average of nearby function values. When applied to an image, it has the effect of blurring nearby pixel values. The 2D Gaussian filter is defined by:\n\n148\n\n|\n\nChapter 8: Automating the Featurizer: Image Feature Extraction and Deep Learning\n\nG(x, y) =\n\n1 2πσ\n\ne\n\n–\n\nx\n\n2\n\n+y 2\n\n2σ\n\n2\n\n,\n\nwhere σ is the standard deviation of the Gaussian function, which controls the width of the “mound.”\n\nIn Example 8-2, we’ll first create a 2D Gaussian filter, then convolve it with our favor‐ ite cat image to produce a blurred cat (see Figure 8-10). Note that this is not the most accurate way to compute a Gaussian filter, but it is the easiest to understand. A better implementation would take the weighted average value at each discrete point rather than the simple point estimate.\n\nExample 8-2. Applying a simple Gaussian filter on an image\n\n>>> import numpy as np\n\n# First create X,Y meshgrids of size 5x5 on which we compute the Gaussian >>> ind = [-1., -0.5, 0., 0.5, 1.] >>> X,Y = np.meshgrid(ind, ind) >>> X array([[-1. , -0.5, 0. , 0.5, 1. ], [-1. , -0.5, 0. , 0.5, 1. ], [-1. , -0.5, 0. , 0.5, 1. ], [-1. , -0.5, 0. , 0.5, 1. ], [-1. , -0.5, 0. , 0.5, 1. ]])\n\n# G is a simple, unnormalized Gaussian kernel where the value at (0,0) is 1.0 >>> G = np.exp(-(np.multiply(X,X) + np.multiply(Y,Y))/2) >>> G array([[ 0.36787944, 0.53526143, 0.60653066, 0.53526143, 0.36787944], [ 0.53526143, 0.77880078, 0.8824969 , 0.77880078, 0.53526143], [ 0.60653066, 0.8824969 , 1. , 0.8824969 , 0.60653066], [ 0.53526143, 0.77880078, 0.8824969 , 0.77880078, 0.53526143], [ 0.36787944, 0.53526143, 0.60653066, 0.53526143, 0.36787944]])\n\n>>> from skimage import data, color >>> cat = color.rgb2gray(data.chelsea())\n\n>>> from scipy import signal >>> blurred_cat = signal.convolve2d(cat, G, mode='valid')\n\n>>> import matplotlib.pyplot as plt >>> fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,4), ... sharex=True, sharey=True)\n\n>>> ax1.axis('off') >>> ax1.imshow(cat, cmap=plt.cm.gray) >>> ax1.set_title('Input image') >>> ax1.set_adjustable('box-forced')\n\n>>> ax2.axis('off')\n\nLearning Image Features with Deep Neural Networks\n\n|\n\n149\n\n>>> ax2.imshow(blurred_cat, cmap=plt.cm.gray) >>> ax2.set_title('After convolving with a Gaussian filter') >>> ax2.set_adjustable('box-forced')\n\nFigure 8-10. An image of a cat, before and after applying a 2D Gaussian filter\n\nThe convolutional layers in AlexNet are three-dimensional. In other words they operate on voxels (values in the array representing the 3D space of the image) from the previous layer. The first convolutional neural net takes raw RGB images and learns convolution filters for a local image neighborhood across all three color chan‐ nels. Subsequent layers take as input voxels across space and kernel dimensions. See Figure 8-14 for more details.\n\nRectified Linear Unit (ReLU) Transformation The output of a neural net is often passed through another nonlinear transformation, also known as an activation function. Common choices are the tanh function (a smooth nonlinear function bounded between –1 and 1), the sigmoid function (a smooth nonlinear function bounded between 0 and 1, introduced in “Classification with Logistic Regression” on page 66), or what’s known as a rectified linear unit. A ReLU is a simple variation of a linear function where the negative part is zeroed out. In other words, it trims away the negative values, but leaves the positive part unboun‐ ded. The range of ReLU extends from 0 to ∞.\n\nCommon Activation Functions A ReLU is a linear function with the negative part zeroed out:\n\nReLU(x) = max(0, x)\n\nThe tanh function is a trigonometric function that smoothly increases from –1 to 1:\n\n150\n\n|\n\nChapter 8: Automating the Featurizer: Image Feature Extraction and Deep Learning\n\ntanh(x) =\n\nsinh(x) cosh(x)\n\n=\n\ne x – e –x e x + e –x\n\nThe sigmoid function increases smoothly from 0 to 1:\n\nsigmoid(x) =\n\n1 1 + e –x\n\nThe three functions are illustrated in Figure 8-11.\n\nFigure 8-11. Illustration of three common activation functions: ReLU, tanh, and sigmoid\n\nThe ReLU transformation has no effect on nonnegative functions such as the raw image or the Gaussian filter. However, a trained neural net, whether fully connected or convolutional, will likely output negative values. AlexNet uses ReLU instead of other transformations, citing faster convergence during training (Krizhevsky et al., 2012). It applies ReLU to every convolutional and fully connected layer.\n\nResponse Normalization Layers After the discussions in Chapter 4 and earlier in this chapter, normalization should by now be a familiar concept. Normalization divides an individual output by a func‐ tion of the collective total response. Hence, another way of understanding normaliza‐ tion is that it creates competition amongst neighbors because the strength of each output is now measured relative to its neighbors (see Figure 8-12). AlexNet normal‐ izes the output at each location across different kernels.\n\nLearning Image Features with Deep Neural Networks\n\n|\n\n151\n\nFigure 8-12. Structure of response normalization over convolution kernel outputs from the previous layer—the normalization constants are computed based on a neighbor‐ hood from the previous layer\n\nLocal Response Normalization Breeds Competition Among Neighboring Kernels As the name suggests, local response normalization divides a value by a combination of its neighbors. Here is the formula:\n\nyk = xk /(c + α∑ℓ∈neighborhood of k xℓ\n\n2)β\n\nHere, xk is the output of the kth kernel, and yk is the normalized response relative to other kernels in the neighborhood. The normalization is performed separately for each output location. That is, for each output location (i,j), we normalize across the nearby convolution kernel outputs. Note that this isn’t the same as normalizing over the image neighborhood or output locations. The size of the kernel neighborhood, c, α, and β are all hyperparameters that are tuned via a validation set of images.\n\n152\n\n|\n\nChapter 8: Automating the Featurizer: Image Feature Extraction and Deep Learning\n\nPooling Layers A pooling layer combines multiple inputs into a single output. As the convolutional filter moves across an image, it generates an output for every neighborhood under its lens. Pooling forces a local image neighborhood to produce one value instead of many. This reduces the number of outputs in the intermediate layers of the deep learning network, which effectively reduces the probability of overfitting the network to training data.\n\nThere are multiple ways to pool inputs: averaging, summing (or computing a gener‐ alized norm), or taking the maximum value. Pooling moves across the image or intermediate output layers. AlexNet uses overlapping max pooling, moving across the image in strides of two pixels (or outputs) and pooling across three neighbors.\n\nFigure 8-13. Max pooling outputs the maximum number of nonoverlapping rectangles per subregion using nonlinear downsampling\n\nStructure of AlexNet All together, AlexNet involves five convolution layers, two response normalization layers, three max pooling layers, and two fully connected layers. Combined with the final classification output layer, there are a total of 13 neural network layers in the model, forming 8 layer groups. See Figure 8-14 for details.\n\nLearning Image Features with Deep Neural Networks\n\n|\n\n153\n\nFigure 8-14. Architecture diagram of AlexNet—the different shades of gray (or magenta and blue, if you’re viewing the illustrations in color) denote layers that reside on GPU 1 and GPU 2\n\nThe input image is first scaled to 256 × 256 pixels. The input is actually random crops of size 224 × 224, with 3 color channels. The first two convolution layers are each fol‐ lowed by a response normalization layer and a max pooling layer, and the last convo‐ lution layer is followed by max pooling. The original paper splits training data and computation across two GPUs. Communications between layers are mostly limited to within the same GPU. The exceptions are between layer groups 2 and 3, and after layer group 5. At those boundary points, the next layer takes as input a voxel of ker‐ nels from the previous layer across both GPUs. ReLU transformation follows every intermediate layer.\n\nFigure 8-15 shows a detailed view of convolution+response normalization+max pooling. Note that the normalization constant is computed across kernels, whereas pooling happens across image regions. Also, pooling reduces the dimension of the layer.\n\n154\n\n|\n\nChapter 8: Automating the Featurizer: Image Feature Extraction and Deep Learning",
      "page_number": 155
    },
    {
      "number": 19,
      "title": "Segment 19 (pages 165-172)",
      "start_page": 165,
      "end_page": 172,
      "detection_method": "topic_boundary",
      "content": "Figure 8-15. Detailed view of convolution+response normalization+max pooling\n\nNote that AlexNet’s architecture is reminiscent of the gradient histogram–normal‐ (see ize–threshold–normalize architecture of SIFT/HOG Figure 8-6), but with many more layers. (Hence the “deep” in “deep learning.”) Unlike in SIFT/HOG, however, the convolution kernels and full connection weights are learned from data, not predefined. Also, the normalization steps in SIFT are per‐ formed across the feature vector over the entire image region, whereas the response normalization layer in AlexNet normalizes across the convolution kernels.\n\nfeature extractors\n\nAt a high level, the model starts by extracting patterns out of local image neighbor‐ hoods. Each subsequent layer builds upon the output of the previous layers, effec‐ tively covering successively larger areas of the original image. Hence, even though the first five convolution layers all have fairly small kernel widths, the later layers are able to formulate more global patterns. The fully connected layers at the end are the most global.\n\nAlthough the gist of patterns is conceptually clear, it is a hard problem to visualize the actual patterns each layer picks out. Figures 8-16 and 8-17 show visualizations of the first two layers of convolution kernels learned by the model. The first layer con‐ sists of detectors of grayscale edges and textures at different orientations, and color blobs and textures. The second layer appears to contain detectors of various smooth patterns.\n\nLearning Image Features with Deep Neural Networks\n\n|\n\n155\n\nFigure 8-16. Visualization of the first layer of convolution kernels in a trained AlexNet: the first half of the kernels are learned on GPU 1 and appear to detect grayscale edges and textures at different orientations; the second half, trained on a second GPU, focus on color blobs and patterns\n\nFigure 8-17. Visualization of the second layer of convolution kernels of a trained AlexNet\n\nDespite huge advances in the area, image featurization is still more of an art than a science. Ten years ago, people handcrafted feature extraction steps using a combina‐ tion of image gradients, edge detection, orientation, spatial cues, smoothing, and nor‐ malization. Nowadays, deep learning architects build models that encapsulate much\n\n156\n\n|\n\nChapter 8: Automating the Featurizer: Image Feature Extraction and Deep Learning\n\nthe same ideas, but the parameters are automatically learned from training images. The magic voodoo is still there, just hidden one abstraction deeper in the model!\n\nSummary Nearing the end, we can build on the intuition gained to better understand why the most straightforward and simple image features will not always be the most useful for performing tasks such as image classification. Instead of representing each pixel as an atomic unit, it is more important to consider the relationships pixels have with other pixels near them. We can adapt techniques developed for other tasks, such as SIFT and HOG, to better extract features across entire images by analyzing gradients in neighborhoods.\n\nThe next leap forward in recent years applies deep neural networks to computer vision to push feature extraction of images even further. The important thing to remember here is that deep learning stacks many layers of neural networks and trans‐ formations on top of each other. Some of these layers, when examined individually, begin to tease out similar features that can be identified as building blocks for human vision: defining lines, gradients, color maps.\n\nBibliography “CS231n: Convolutional Neural Networks for Visual Recognition.” Retrieved from http://cs231n.github.io/convolutional-networks/.\n\nDalal, Navneet, and Bill Triggs. “Histograms of Oriented Gradients for Human Detection.” Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (2005): 886–893.\n\nEliot, Lise. What’s Going On in There? How the Brain and Mind Develop in the First Five Years of Life. New York: Bantam Books, 2000.\n\nKrizhevsky, Alex, Ilya Sutskever, and Geoffrey Hinton. “ImageNet Classification with Deep Convolutional Neural Networks.” Advances in Neural Information Processing Systems 25 (2012): 1097–1105.\n\nLowe, David G. “Object Recognition from Local Scale-Invariant Features.” Proceed‐ ings of the International Conference on Computer Vision (1999): 1150–1157.\n\nLowe, David G. “Distinctive Image Features from Scale-Invariant Keypoints.” Inter‐ national Journal of Computer Vision 60:2 (2004): 91–110.\n\nMalisiewicz, Tomasz. “From Feature Descriptors to Deep Learning: 20 Years of Com‐ puter Vision.” Tombone’s Computer Vision Blog, January 20, 2015. http:// www.computervisionblog.com/2015/01/from-feature-descriptors-to-deep.html.\n\nSummary\n\n|\n\n157\n\nSzegedy, Christian, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. “Going Deeper with Convolutions.” Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition (2015): 1–9.\n\nZeiler, Matthew D., and Rob Fergus. “Visualizing and Understanding Convolutional Networks,” Proceedings of the 13th European Conference on Computer Vision (2014): 818–833.\n\n158\n\n|\n\nChapter 8: Automating the Featurizer: Image Feature Extraction and Deep Learning\n\nCHAPTER 9 Back to the Feature: Building an Academic Paper Recommender\n\n“In mathematics you don’t understand things. You just get used to them.”\n\n—John von Neumann\n\nWhen the path from data to results was first introduced in Figure 1-1, it may not have been clear how there would ever be a way forward. Throughout this book, we have focused on introducing basic principles of feature engineering using toy models and clean, simple datasets. These examples were intended to be illustrative and enlightening.\n\nMachine learning examples generally show the best-case scenario and results. This masks the path we have described thus far in the book. Now that the foundation is set, we are leaving the world of simple, toy data and diving into the process of feature engineering with a real-world, structured dataset. As we move through each step, we will be examining the raw data forming each feature, what the transformed feature becomes, and what trade-offs we make along the way.\n\nTo be clear, our goal for this example is not to build the best model for this dataset. Rather, it is to demonstrate the practical application of a handful of our techniques, as well as how to more deeply examine and understand whether each technique is providing value to the model one is building.\n\nItem-Based Collaborative Filtering Our task will be to build a recommender for academic papers using a subsample of the Microsoft Academic Graph dataset. This should come in extremely handy for all\n\n159\n\nof you who are searching for citations but have not yet discovered Google Scholar. Here are some relevant statistics about the dataset:\n\nMicrosoft Academic Graph Dataset\n\nIt contains 166,192,182 unique papers, available via Open Academic Graph. It is intended to be used for research purposes only.\n\nThe total size of the dataset is 104 GB. • Each observation has 18 variables to identify each paper, including the paper’s title, abstract, authors, keywords, and fields of study.\n\nThe dataset is designed to be easy to store and access in a database. It is not tidy for machine learning models out of the box, but requires some initial wrangling. Some teachers like to spare you this step, boosting your ego by getting directly to the mod‐ els and results. None of that here. We are starting together from the very beginning.\n\nOur initial approach will be to wrangle a few variables into the right shape to push through an item-based collaborative filter. We will see if reasonably similar papers can be found in a timely and efficient manner.\n\nThe Origins of Item-Based Collaborative Filtering\n\nThis approach was first developed at Amazon as an improvement to user-based algorithms for recommending products. Sarawar et al. (2001) walk through the challenges and benefits of switching the perspective in recommenders from the user to the item.\n\nItem-based collaborative filtering provides recommendations based on the similarity between items. This works in two stages: first finding the similarity scores between items, then ranking all scores to find the top-N similar item recommendations.\n\nBuilding an Item-Based Recommender\n\nAn item-based recommender performs three tasks:\n\n1. Generalize information about a “thing” or item.\n\n2. Score all other items to find ones “like” this one.\n\n3. Return ranked scores + items.\n\n160\n\n|\n\nChapter 9: Back to the Feature: Building an Academic Paper Recommender\n\nFirst Pass: Data Import, Cleaning, and Feature Parsing Like all good science experiments, we will start off with a hypothesis. In this case, we assume that papers published at about the same time and in similar fields of study will be the most useful to users. We will take a naive approach of parsing out these fields from a subsample of the overall dataset. After generating simple sparse arrays, we’ll run the entire item array through an item-based collaborative filter to see if we get good results.\n\nThe item-based collaborative filter depends on a similarity score to compare items. In this case, the cosine similarity provides a reasonable comparison between two non- zero vectors. The following example actually uses the cosine distance, which is the complement of the cosine similarity in the positive space, or:\n\nDC(A,B) = 1 – SC(A,B)\n\nwhere DC is the cosine distance and SC is the cosine similarity.\n\nAcademic Paper Recommender: Naive Approach The first step in our journey is to import and examine the dataset. In Example 9-1, we scope our experiment by limiting the fields available after the initial import. These fields are still rich in possibility, as shown in Figure 9-1.\n\nExample 9-1. Import + filter data\n\n>>> import pandas as pd\n\n>>> model_df = pd.read_json('data/mag_papers_0/mag_subset20K.txt', lines=True) >>> model_df.shape (20000, 19) >>> model_df.columns Index(['abstract', 'authors', 'doc_type', 'doi', 'fos', 'id', 'issue', 'keywords', 'lang', 'n_citation', 'page_end', 'page_start', 'publisher', 'references', 'title', 'url', 'venue', 'volume', 'year'], dtype='object')\n\n# filter out non-English articles and focus on a few variables >>> model_df = model_df[model_df.lang == 'en'] ... .drop_duplicates(subset='title', keep='first') ... .drop(['doc_type', 'doi', 'id', 'issue', 'lang', 'n_citation', ... 'page_end', 'page_start', 'publisher', 'references', ... 'url', 'venue', 'volume'], ... axis=1) >>> model_df.shape (10399, 6)\n\nFirst Pass: Data Import, Cleaning, and Feature Parsing\n\n|\n\n161\n\nFigure 9-1. First two rows of the Microsoft Academic Graph dataset\n\nTable 9-1 summarizes best how further wrangling is needed to get the raw data into a better shape for a model. Lists and dictionaries are good for data storage, but are not tidy or well suited for machine learning without some unpacking (Wickham, 2014).\n\nTable 9-1. Data schema for model_df\n\nField name Description abstract authors fos keywords title year\n\npaper abstract author names and affiliations fields of study keywords paper title published year\n\nField type string list of dict, keys = name, org list of strings list of strings string int\n\n# NaN 4393 1 1733 4294 0 0\n\nWe focus first on two fields in Example 9-2, transforming them from lists and inte‐ gers into a feature array, as shown in Figure 9-2.\n\nExample 9-2. Collaborative filtering stage 1: Build item feature matrix\n\n>>> unique_fos = sorted(list({feature ... for paper_row in model_df.fos.fillna('0') ... for feature in paper_row }))\n\n>>> unique_year = sorted(model_df['year'].astype('str').unique()) >>> def feature_array(x, var, unique_array): ... row_dict = {} ... for i in x.index: ... var_dict = {} ... for j in range(len(unique_array)): ... if type(x[i]) is list: ... if unique_array[j] in x[i]: ... var_dict.update({var + '_' + unique_array[j]: 1}) ... else: ... var_dict.update({var + '_' + unique_array[j]: 0}) ... else: ... if unique_array[j] == str(x[i]): ... var_dict.update({var + '_' + unique_array[j]: 1}) ... else: ... var_dict.update({var + '_' + unique_array[j]: 0}) ... row_dict.update({i : var_dict}) ... feature_df = pd.DataFrame.from_dict(row_dict, dtype='str').T ... return feature_df\n\n162\n\n|\n\nChapter 9: Back to the Feature: Building an Academic Paper Recommender",
      "page_number": 165
    },
    {
      "number": 20,
      "title": "Segment 20 (pages 173-180)",
      "start_page": 173,
      "end_page": 180,
      "detection_method": "topic_boundary",
      "content": ">>> year_features = feature_array(model_df['year'], unique_year) >>> fos_features = feature_array(model_df['fos'], unique_fos)\n\n>>> first_features = fos_features.join(year_features).T\n\n>>> from sys import getsizeof >>> print('Size of first feature array: ', getsizeof(first_features)) Size of first feature array: 2583077234\n\nFigure 9-2. Head of first_features—observations’ (papers') indices from the original data set are columns, features are rows\n\nWe have now successfully turned a relatively small dataset, ~10K rows of raw data, into 2.5 GB of features. But this path is too sluggish for quick, iterative exploration. We need methods that will be faster and result in features that will consume less computational resources and experimentation time.\n\nFor now, though, let’s see how our current features perform at giving us a good rec‐ ommendation in the next stage (Example 9-3). We’ll define a “good” recommenda‐ tion as a paper that looks similar to the input.\n\nExample 9-3. Collaborative filtering stage 2: Search for similar items\n\n>>> from scipy.spatial.distance import cosine\n\n>>> def item_collab_filter(features_df): ... item_similarities = pd.DataFrame(index = features_df.columns, ... columns = features_df.columns) ... for i in features_df.columns: ... for j in features_df.columns: ... item_similarities.loc[i][j] = 1 - cosine(features_df[i], ... features_df[j]) ... return item_similarities\n\n>>> first_items = item_collab_filter(first_features.loc[:, 0:1000])\n\nWhy does it take so long for us to calculate the item similarities using only two fea‐ tures? We are taking the dot product of a 10,399 × 1,000 matrix using a nested for loop. The time per loop increases as we increase the number of observations we add\n\nFirst Pass: Data Import, Cleaning, and Feature Parsing\n\n|\n\n163\n\nto the model. Remember, this is a subset of the total available dataset, filtered for English-only papers. As we move closer to a “good” result, we’ll need to go back and test on the larger set for our best results.\n\nHow can we make this faster? Since we only need one result at a time, we can change our function so that we only calculate one item at a time, specifying the number of top results we want. We’ll do this later, as we continue to move through our experi‐ ment. For now, it is useful to see the full feature space to get an understanding of the impact of iterative work on brute-forcing our way through a real-world dataset.\n\nWe need to get a better idea of how these features will translate to us getting a good recommendation. Do we have enough observations to move forward? Let’s plot a heatmap (Example 9-4) to see if we have any papers that are similar to each other. Figure 9-3 shows the result.\n\nExample 9-4. Heatmap of paper recommendations\n\n>>> import matplotlib.pyplot as plt >>> import seaborn as sns >>> import numpy as np >>> %matplotlib inline >>> sns.set() >>> ax = sns.heatmap(first_items.fillna(0), ... vmin=0, vmax=1, ... cmap=\"YlGnBu\", ... xticklabels=250, yticklabels=250) >>> ax.tick_params(labelsize=12)\n\nDarker pixels signal items that are similar to one another. The dark diagonal line shows that the cosine similarity is correctly indicating that each paper is most similar to itself. However, because there are a lot of NaNs for one of our features, the line is broken along the diagonal. We can see that while most of the items are not similar to one another—i.e., our dataset is fairly diverse—there are some other high-scoring candidates. These may or may not be good recommendations qualitatively, but at least we can see that our methods are not so mad.\n\n164\n\n|\n\nChapter 9: Back to the Feature: Building an Academic Paper Recommender\n\nFigure 9-3. Heatmap of similar papers based on two raw features: year and fields of study\n\nExample 9-5 shows how to translate these item similarities into a recommendation. The good news is that we have a wide variety of features still available, with lots of room for improvement.\n\nExample 9-5. Item-based collaborative filtering recommendations\n\n>>> def paper_recommender(paper_ix, items_df): ... print('Based on the paper: \\nindex = ', paper_ix) ... print(model_df.iloc[paper_ix]) ... top_results = items_df.loc[paper_ix].sort_values(ascending=False).head(4) ... print('\\nTop three results: ') ... order = 1 ... for i in top_results.index.tolist()[-3:]: ... print(order,'. Paper index = ', i) ... print('Similarity score: ', top_results[i]) ... print(model_df.iloc[i], '\\n') ... if order < 5: order += 1\n\n>>> paper_recommender(2, first_items)\n\nBased on the paper: index = 2\n\nFirst Pass: Data Import, Cleaning, and Feature Parsing\n\n|\n\n165\n\nabstract NaN authors [{'name': 'Jovana P. Lekovich', 'org': 'Weill ... fos NaN keywords NaN title Should endometriosis be an indication for intr... year 2015 Name: 2, dtype: object\n\nTop three results: 1 . Paper index = 2 Similarity score: 1.0 abstract NaN authors [{'name': 'Jovana P. Lekovich', 'org': 'Weill ... fos NaN keywords NaN title Should endometriosis be an indication for intr... year 2015 Name: 2, dtype: object\n\n2 . Paper index = 292 Similarity score: 1.0 abstract NaN authors [{'name': 'John C. Newton'}, {'name': 'Beers M... fos [Wide area multilateration, Maneuvering speed,... keywords NaN title Automatic speed control for aircraft year 1955 Name: 561, dtype: object\n\n3 . Paper index = 593 Similarity score: 1.0 abstract This paper demonstrates that on‐site greywater... authors [{'name': 'Eran Friedler', 'org': 'Division of... fos [Public opinion, Environmental Engineering, Wa... keywords [economic analysis, tratamiento desperdicios, ... title The water saving potential and the socio-econo... year 2008 Name: 1152, dtype: object\n\nYikes. The good news is that the most similar paper returned is the one we are look‐ ing for. The bad news is that the next two papers don’t seem to be very close to our initial search, even for the features we have chosen.\n\n“Yes, yes,” you may say, “but this is the era of Big Data! That will solve our problems! Can’t we just push more data through for better results?” Potentially. But even Big Data cannot compensate for poor data and engineering choices.\n\n166\n\n|\n\nChapter 9: Back to the Feature: Building an Academic Paper Recommender\n\nFigure 9-4. Machine learning (https://xkcd.com/1838/)\n\nOur current brute-force methods are too slow for smart, iterative engineering. Let’s try some of our new feature engineering tricks to see if we can speed up the computa‐ tion time and find better features and a better way to search for results.\n\nSecond Pass: More Engineering and a Smarter Model The initial approach of creating a large, sparse array and shoving it through a filter can be improved in many ways. The next steps will focus specifically on applying bet‐ ter techniques to the two initial features and altering the item-based collaborative fil‐ ter method for faster iteration.\n\nFirst, it is time to try out some of those great feature engineering tricks for the two variables in our hypothesis. Looking deeper into the features already developed, we can choose techniques that will address each type of variable and convert it to a “bet‐ ter” feature for our recommendation system.\n\nAcademic Paper Recommender: Take 2 Let’s focus on the year first. In “Quantization or Binning” on page 10, we reviewed how using raw counts for features can be problematic for methods using similarity metrics. Example 9-6 (and Figure 9-5) will examine how we can transform 'year' to better fit the model we have selected.\n\nSecond Pass: More Engineering and a Smarter Model\n\n|\n\n167\n\nExample 9-6. Fixed-width binning + dummy coding (part 1)\n\n>>> print(\"Year spread: \", model_df['year'].min(),\" - \", model_df['year'].max()) >>> print(\"Quantile spread:\\n\", model_df['year'].quantile([0.25, 0.5, 0.75])) Year spread: 1831 - 2017 Quantile spread: 0.25 1990.0 0.50 2005.0 0.75 2012.0 Name: year, dtype: float64\n\n# plot years to see the distribution >>> fig, ax = plt.subplots() >>> model_df['year'].hist(ax=ax, ... bins= model_df['year'].max() - model_df['year'].min()) >>> ax.tick_params(labelsize=12) >>> ax.set_xlabel('Year Count', fontsize=12) >>> ax.set_ylabel('Occurrence', fontsize=12)\n\nWe can see from the skewed distribution (Figure 9-5) that this is an excellent candi‐ date for binning.\n\nFigure 9-5. Raw year distribution for 10K+ academic papers in dataset\n\n168\n\n|\n\nChapter 9: Back to the Feature: Building an Academic Paper Recommender\n\nThe bins will be based on ranges within the variable, rather than the unique number of features. To further reduce the feature space, we will dummy-code the resultant bins (see Example 9-7). Pandas can do both using built-in functions. These methods will make our results easy to interpret, so we can do a quick check of the transformed features before moving on (see Figure 9-6).\n\nExample 9-7. Fixed-width binning + dummy coding (part 2)\n\n# binning here (by 10 years) reduces the year feature space from 156 to 19 >>> bins = int(round((model_df['year'].max() - model_df['year'].min()) / 10))\n\n>>> temp_df = pd.DataFrame(index = model_df.index) >>> temp_df['yearBinned'] = pd.cut(model_df['year'].tolist(), bins, precision = 0) >>> X_yrs = pd.get_dummies(temp_df['yearBinned']) >>> X_yrs.columns.categories IntervalIndex([(1831.0, 1841.0], (1841.0, 1851.0], (1851.0, 1860.0], (1860.0, 1870.0], (1870.0, 1880.0] ... (1968.0, 1978.0], (1978.0, 1988.0], (1988.0, 1997.0], (1997.0, 2007.0], (2007.0, 2017.0]] closed='right', dtype='interval[float64]')\n\n# plot the new distribution >>> fig, ax = plt.subplots() >>> X_yrs.sum().plot.bar(ax = ax) >>> ax.tick_params(labelsize=8) >>> ax.set_xlabel('Binned Years', fontsize=12) >>> ax.set_ylabel('Counts', fontsize=12)\n\nWe have preserved the underlying distribution of the original variable through bin‐ ning by decades. If we desired to use a method that would benefit from a different distribution, we could alter our binning choices to change how this variable presents itself to the model. Since we are using cosine similarity, this is fine. Let’s move on to the next feature we originally included in our model.\n\nThe fields-of-study feature space contributed significantly to the original model’s size and processing time.\n\nSecond Pass: More Engineering and a Smarter Model\n\n|\n\n169\n\nFigure 9-6. Distribution of new binned X_yrs feature\n\nLet’s examine the work we have already done. By parsing out the list of strings, we created a “bag-of-phrases” in the first pass. Since we already have a useful sparse array, we can focus on using a more efficient data type. Example 9-8 illustrates how converting from a Pandas DataFrame to a NumPy sparse array affects computation time.\n\nExample 9-8. Converting bag-of-phrases pd.Series to NumPy sparse array\n\n>>> X_fos = fos_features.values\n\n# We can see how this will make a difference in the future by looking # at the size of each >>> print('Our pandas Series, in bytes: ', getsizeof(fos_features)) >>> print('Our hashed numpy array, in bytes: ', getsizeof(X_fos))\n\n170\n\n|\n\nChapter 9: Back to the Feature: Building an Academic Paper Recommender",
      "page_number": 173
    },
    {
      "number": 21,
      "title": "Segment 21 (pages 181-189)",
      "start_page": 181,
      "end_page": 189,
      "detection_method": "topic_boundary",
      "content": "Our pandas Series, in bytes: 2530632380 Our hashed numpy array, in bytes: 112\n\nMuch better! Putting it back together, we’ll pipe our features together (Example 9-9) and rerun our recommender (Example 9-10) to see if we have improved results, tak‐ ing advantage of scikit-learn’s cosine similarity function. We will also reduce the computational time by only focusing on one item at a time.\n\nExample 9-9. Collaborative filtering stages 1 + 2: Build item feature matrix, search for similar items\n\n>>> second_features = np.append(X_fos, X_yrs, axis = 1) >>> print(\"The power of feature engineering saves us, in bytes: \", ... getsizeof(first_features) - getsizeof(second_features)) The power of feature engineering saves us, in bytes: 168066769\n\n>>> from sklearn.metrics.pairwise import cosine_similarity\n\n>>> def piped_collab_filter(features_matrix, index, top_n): ... item_similarities = \\ ... 1 - cosine_similarity(features_matrix[index:index+1], ... features_matrix).flatten() ... related_indices = \\ ... [i for i in item_similarities.argsort()[::-1] if i != index] ... return [(index, item_similarities[index]) ... for index in related_indices ... ][0:top_n]\n\nExample 9-10. Item-based collaborative filtering recommendations: Take 2\n\n>>> def paper_recommender(items_df, paper_ix, top_n): ... if paper_ix in model_df.index: ... print('Based on the paper:') ... print('Paper index = ', model_df.loc[paper_ix].name) ... print('Title :', model_df.loc[paper_ix]['title']) ... print('FOS :', model_df.loc[paper_ix]['fos']) ... print('Year :', model_df.loc[paper_ix]['year']) ... print('Abstract :', model_df.loc[paper_ix]['abstract']) ... print('Authors :', model_df.loc[paper_ix]['authors'], '\\n') ... # define the location index for the DataFrame index requested ... array_ix = model_df.index.get_loc(paper_ix) ... top_results = piped_collab_filter(items_df, array_ix, top_n) ... print('\\nTop',top_n,'results: ')\n\n... order = 1 ... for i in range(len(top_results)): ... print(order,'. Paper index = ', ... model_df.iloc[top_results[i][0]].name) ... print('Similarity score: ', top_results[i][1]) ... print('Title :', model_df.iloc[top_results[i][0]]['title'])\n\nSecond Pass: More Engineering and a Smarter Model\n\n|\n\n171\n\n... print('FOS :', model_df.iloc[top_results[i][0]]['fos']) ... print('Year :', model_df.iloc[top_results[i][0]]['year']) ... print('Abstract :', model_df.iloc[top_results[i][0]]['abstract']) ... print('Authors :', model_df.iloc[top_results[i][0]]['authors'], ... '\\n') ... if order < top_n: order += 1 ... else: ... print('Whoops! Choose another paper. Try something from here: \\n', ... model_df.index[100:200])\n\n>>> paper_recommender(second_features, 2, 3) Based on the paper: Paper index = 2 Title : Should endometriosis be an indication for intracytoplasmic sperm inject ... FOS : nan Year : 2015 Abstract : nan Authors : [{'name': 'Jovana P. Lekovich', 'org': 'Weill Cornell Medical College, ...\n\nTop 3 results: 1 . Paper index = 10055 Similarity score: 1.0 Title : [Diagnosis of cerebral tumors; comparative studies on arteriography, ... FOS : ['Radiology', 'Pathology', 'Surgery'] Year : 1953 Abstract : nan Authors : [{'name': 'Antoine'}, {'name': 'Lepoire'}, {'name': 'Schoumacker'}]\n\n2 . Paper index = 11771 Similarity score: 1.0 Title : A Study of Special Functions in the Theory of Eclipsing Binary Systems FOS : ['Contact binary'] Year : 1981 Abstract : nan Authors : [{'name': 'Filaretti Zafiropoulos', 'org': 'University of Manchester'}]\n\n3 . Paper index = 11773 Similarity score: 1.0 Title : Studies of powder flow using a recording powder flowmeter and measure ... FOS : nan Year : 1985 Abstract : This paper describes the utility of the dynamic measurement of the ... Authors : [{'name': 'Ramachandra P. Hegde', 'org': 'Department of Pharmacy, ...\n\nTo be honest, I don’t think our feature selection is working out too well. There is a lot of missing data in these fields. Let’s keep going to see if we can choose richer features with more information.\n\n172\n\n|\n\nChapter 9: Back to the Feature: Building an Academic Paper Recommender\n\nFinding Your Place Converting between Pandas DataFrames and NumPy matrices can make indices tricky—we have the same size index, but the index assignments are not the same. Pandas assists with this using .iloc, .loc, and .get_loc, as we show in Example 9-11:\n\n.loc returns the index based on the original Pandas DataFrame, allowing us to reference specific papers.\n\n.iloc uses the integer location, which is the same index as our NumPy array. • .get_loc helps us find the integer location when we know the DataFrame index.\n\nExample 9-11. Maintaining index assignment during conversions\n\n>>> model_df.loc[21] abstract A microprocessor includes hardware registers t... authors [{'name': 'Mark John Ebersole'}] fos [Embedded system, Parallel computing, Computer... keywords NaN title Microprocessor that enables ARM ISA program to... year 2013 Name: 21, dtype: object\n\n>>> model_df.iloc[21] abstract NaN authors [{'name': 'Nicola M. Heller'}, {'name': 'Steph... fos [Biology, Medicine, Post-transcriptional regul... keywords [glucocorticoids, post transcriptional regulat... title Post-transcriptional regulation of eotaxin by ... year 2002 Name: 30, dtype: object\n\n>>> model_df.index.get_loc(30) 21\n\nThird Pass: More Features = More Information Our experiment thus far is not supporting the original hypothesis that year and fields- of-study would be sufficient to recommend a similar paper. At this point, we have a few options:\n\nUpload more of the original dataset to see if we get better results. • Spend more time exploring the data to examine if we have a sufficiently dense set to provide good recommendations.\n\nIterate on the current model by adding more features.\n\nThird Pass: More Features = More Information\n\n|\n\n173\n\nThe first option makes the assumption that the problem is in our sampling of the data. This might be the case, but is similar to Figure 9-4’s analogy of stirring the data pile for better results.\n\nThe second option would give a better idea of the underlying raw data. This should be continually revisited based on how your decisions for features and model selection change during the exploration process. The initial subsample chosen here reflects this step. Since we have more variables available in the dataset, we will not go back here yet.\n\nThis leaves the third option, moving forward on our current model by adding more features. Providing more information about each item can improve the similarity scores and result in better recommendations.\n\nBased on our initial exploration, the next steps will focus on the fields with the most information, abstract and authors.\n\nAcademic Paper Recommender: Take 3 Looking back at Chapter 4, we can see that abstract is a good candidate for tf-idf to filter through the noise and find the salient associative words. We do this in Example 9-12.\n\nExample 9-12. Stopwords + tf-idf\n\n# need to fill in NaN for sklearn use in future >>> filled_df = model_df.fillna('None')\n\n>>> from sklearn.feature_extraction.text import TfidfVectorizer\n\n>>> vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, ... stop_words='english') >>> X_abstract = vectorizer.fit_transform(filled_df['abstract']) >>> third_features = np.append(second_features, X_abstract.toarray(), axis = 1)\n\nWe can reduce the computational load of the messy and uneven authors by wran‐ gling into a dictionary and then running it through a one-hot encoder, as shown in Example 9-13.\n\nExample 9-13. One-hot encoding using scikit-learn’s DictVectorizer\n\n>>> authors_list = []\n\n>>> for row in filled_df.authors.itertuples(): ... # create a dictionary from each Series index ... if type(row.authors) is str: ... y = {'None': row.Index} ... if type(row.authors) is list:\n\n174\n\n|\n\nChapter 9: Back to the Feature: Building an Academic Paper Recommender\n\n... # add these keys + values to our running dictionary ... y = dict.fromkeys(row.authors[0].values(), row.Index) ... authors_list.append(y)\n\n>>> authors_list[0:5] [{'None': 0}, {'Ahmed M. Alluwaimi': 1}, {'Jovana P. Lekovich': 2, 'Weill Cornell Medical College, New York, NY': 2}, {'George C. Sponsler': 5}, {'M. T. Richards': 7}]\n\n>>> from sklearn.feature_extraction import DictVectorizer >>> v = DictVectorizer(sparse=False) >>> D = authors_list >>> X_authors = v.fit_transform(D) >>> fourth_features = np.append(third_features, X_authors, axis = 1)\n\nTime to check in with the recommender to see how these new features are working out. Example 9-14 shows the results.\n\nExample 9-14. Item-based collaborative filtering recommendations: Take 3\n\n>>> paper_recommender(fourth_features, 2, 3)\n\nBased on the paper: Paper index = 2 Title : Should endometriosis be an indication for intracytoplasmic sperm inject ... FOS : nan Year : 2015 Abstract : nan Authors : [{'name': 'Jovana P. Lekovich', 'org': 'Weill Cornell Medical College, ...\n\nTop 3 results: 1 . Paper index = 10055 Similarity score: 1.0 Title : [Diagnosis of cerebral tumors; comparative studies on arteriography, ... FOS : ['Radiology', 'Pathology', 'Surgery'] Year : 1953 Abstract : nan Authors : [{'name': 'Antoine'}, {'name': 'Lepoire'}, {'name': 'Schoumacker'}]\n\n2 . Paper index = 5601 Similarity score: 1.0 Title : 633 Survival after coronary revascularization, with and without mitral ... FOS : ['Cardiology'] Year : 2005 Abstract : nan Authors : [{'name': 'J.B. Le Polain De Waroux'}, {'name': 'Anne-Catherine ...\n\n3 . Paper index = 12256\n\nThird Pass: More Features = More Information\n\n|\n\n175\n\nSimilarity score: 1.0 Title : Nucleotide Sequence and Analysis of an Insertion Sequence from Bacillus ... FOS : ['Biology', 'Molecular biology', 'Insertion sequence', 'Nucleic acid ... Year : 1994 Abstract : A 5.8-kb DNA fragment encoding the cryIC gene from Bacillus thur... Authors : [{'name': 'Geoffrey P. Smith'}, {'name': 'David J. Ellar'}, {'name': ...\n\nEven accounting for missing data in certain fields, the top three results from the last round of feature engineering are directing us to other papers in the medical field.\n\nThe range of papers represented in this dataset is broad; for example, a random sam‐ ple of papers exposed fields of study such as “Coupling constant,” “Evapotranspira‐ tion,” “Hash function,” “IVMS,” “Meditation,” “Pareto analysis,” “Second-generation wavelet transform,” “Slip,” and “Spiral galaxy.” Given that there are 7,604 unique fields of study listed for 10K+ papers, these last results seem to be moving in the right direction. We can be confident that our work is progressing toward a useful model.\n\nContinued iteration on more text variables, such as the finding the noun phrases of the paper titles or stemming the keywords, could bring us even closer to a “best” recommendation.\n\nIt should be noted here that this definition of “best” is the Holy Grail of all recom‐ menders and search engines alike. We are searching for what a user will find most helpful, which may or may not be directly represented by the data. Feature engineer‐ ing allows us to abstract salient features into representations such that algorithms can expose both the explicit and implicit information contained therein.\n\nSummary As you can see, building models for machine learning is easy. Building good models for useful results takes time and work. We hiked through the messy processes here of examining a collection of possible variables and experimenting with different feature engineering methods to achieve better results. We define “better” here not just in terms of good outcomes from our training and testing, but also reducing the size of the model and the time it takes us to iterate over different experiments.\n\nWe started this book by talking about how mastery of a subject comes from deeply learning the principles at work, in order to gain intuition to effectively put your knowledge to work. We hope that our work has given you the necessary tools to become more efficient and effective, as well as enriched your mathematical and com‐ putational understanding of how feature engineering is an essential skill to develop useful machine learning models.\n\n176\n\n|\n\nChapter 9: Back to the Feature: Building an Academic Paper Recommender\n\nBibliography Sarwar, Badrul, George Karypis, Joseph Konstan, and John Riedl. “Item-Based Col‐ laborative Filtering Recommendation Algorithms.” Proceedings of the 10th Interna‐ tional Conference on the World Wide Web (2001) 285–295.\n\nSinha, Arnab, Zhihong Shen, Yang Song, Hao Ma, Darrin Eide, Bo-June (Paul) Hsu, and Kuansan Wang. “An Overview of Microsoft Academic Service (MAS) and Appli‐ cations.” Proceedings of the 24th International Conference on the World Wide Web (2015): 243–246.\n\nTang, Jie, Jing Zhang, Limin Yao, Juanzi Li, Li Zhang, and Zhong Su. “ArnetMiner: Extraction and Mining of Academic Social Networks.” Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (2008): 990–998.\n\nWickham, Hadley. “Tidy Data.” The Journal of Statistical Software 59 (2014).\n\nBibliography\n\n|\n\n177\n\nAPPENDIX A Linear Modeling and Linear Algebra Basics\n\nOverview of Linear Classification When we have a labeled dataset, the feature space is strewn with data points from dif‐ ferent classes. It is the job of the classifier to separate the data points from different classes. It can do so by producing an output that is very different for data points from one class versus another. For instance, when there are only two classes, then a good classifier should produce large outputs for one class, and small ones for another. The points right on the cusp of being in one class versus another form a decision surface (Figure A-1).\n\nFigure A-1. Simple binary classification finds a surface that separates two classes of data points\n\nMany functions can be made into classifiers. It’s a good idea to look for the simplest function that cleanly separates the classes, for a few reasons. First of all, it’s easier to find the best simple separator than the best complex separator. Also, simple functions\n\nLinear Modeling and Linear Algebra Basics\n\n|\n\n179\n\noften generalize better to new data, because it’s harder to tailor them too specifically to the training data (a concept known as overfitting). A simple model might make mistakes—like in Figure A-1, where some points are on the wrong side of the divide—but we’re willing to sacrifice some training accuracy in order to have a sim‐ pler decision surface that can achieve better test accuracy. The principle of minimiz‐ ing complexity and maximizing usefulness is called “Occam’s razor,” and is widely applicable in science and engineering.\n\nThe simplest function is a line. A linear function of one input variable is a familiar sight (Figure A-2).\n\nFigure A-2. A linear function of one input variable\n\nA linear function with two input variables can be visualized as either a flat plane in 3D or a contour plot in 2D (shown in Figure A-3). Like a topological geographic map, each line of the contour plot represents points in the input space that have the same output.\n\nIt’s harder to visualize higher-dimensional linear functions, which are called hyper‐ planes. But it’s easy enough to write down the algebraic formula. A multidimensional linear function has a set of inputs x1, x2, ..., xn and a set of weight parameters w0, w1, ..., wn:\n\nfw(x1, x2, ..., xn) = w0 + w1 * x1 + w2 * x2 + ... + wn * xn\n\nIt can be written more succinctly using vector notation:\n\nfw(x) = xTw\n\n180\n\n| Appendix A: Linear Modeling and Linear Algebra Basics",
      "page_number": 181
    },
    {
      "number": 22,
      "title": "Segment 22 (pages 190-198)",
      "start_page": 190,
      "end_page": 198,
      "detection_method": "topic_boundary",
      "content": "Figure A-3. Contour plot of a linear function in 2D\n\nWe follow the usual convention for mathematical notations, which uses boldface to indicate a vector and non-boldface to indicate a scalar. The vector x is padded with an extra 1 at the beginning, as a placeholder for the intercept term w0. If all input fea‐ tures are 0, then the output of the function is w0. So, w0 is also known as the bias or intercept term.\n\nTraining a linear classifier is equivalent to picking out the best separating hyperplane between the classes. This translates into finding the best vector w that is oriented exactly right in space. Since each data point has a target label y, we could find a w that tries to directly emulate the target label:1\n\nxTw = y\n\nSince there is usually more than one data point, we want a w that simultaneously makes all of the predictions close to the target labels:\n\nAw = y\n\nHere, A is known as the data matrix (also known as the design matrix in statistics). It contains the data in a particular form: each row is a data point and each column a\n\n1 Strictly speaking, the formula given here is for linear regression, not linear classification. The difference is that regression allows for real-valued target variables, whereas classification targets are usually integers that repre‐ sent different classes. A regressor can be turned into a classifier via a nonlinear transform. For instance, the logistic regression classifier passes the linear transform of the input through a logistic function. Such models are called generalized linear models and have linear functions at their core. Even though this example is about classification, we use the formula for linear regression as a teaching tool, because it is much easier to analyze. The intuitions readily map to generalized linear classifiers.\n\nLinear Modeling and Linear Algebra Basics\n\n|\n\n181\n\nfeature. (Sometimes people also look at its transpose, where features are the rows and data points the columns.)\n\nThe Anatomy of a Matrix In order to solve the preceding equation, we need some basic knowledge of linear algebra. For a systematic introduction to the subject, we highly recommend Strang (2006).\n\nThe equation states that when a certain matrix multiplies a certain vector, there is a certain outcome. A matrix is also called a linear operator, a name that makes it more apparent that a matrix is a little machine. This machine takes a vector as input and spits out another vector using a combination of several key operations: rotating a vec‐ tor’s direction, adding or subtracting dimensions, and stretching or compressing its length. This combination can be quite powerful for manipulating shapes in the input space.\n\nFor example, as Figure A-4 shows, a 3 × 2 matrix can transform a square area in 2D into a diamond-shaped area in 3D. It does so by rotating and stretching each vector in the input space into a new vector in the output space.\n\nFigure A-4. A 2D to 3D matrix transformation\n\n182\n\n| Appendix A: Linear Modeling and Linear Algebra Basics\n\nFrom Vectors to Subspaces In order to understand a linear operator, we have to look at how it morphs the input into output. Luckily, we don’t have to analyze one input vector at a time. Vectors can be organized into subspaces, and linear operators manipulate vector subspaces.\n\nA subspace is a set of vectors that satisfies two criteria. First, if it contains a vector, then it contains the line that passes through the origin and that point. Second, if it contains two points, then it contains all the linear combinations of those two vectors. Linear combination is a combination of two types of operations: multiplying a vector with a scalar, and adding two vectors together.\n\nOne important property of a subspace is its rank or dimensionality, which is a meas‐ ure of the degrees of freedom in this space. A line has rank 1, a 2D plane has rank 2, and so on. If you can imagine a multidimensional bird in our multidimensional space, then the rank of the subspace tells us in how many “independent” directions the bird could fly. “Independence” here means “linear independence”: two vectors are linearly independent if one isn’t a constant multiple of another (i.e., they are not pointing in exactly the same or opposite directions).\n\nA subspace can be defined as the span of a set of basis vectors. (Span is a technical term that describes the set of all linear combinations of a set of vectors.) The span of a set of vectors is invariant under linear combinations (because it’s defined that way). So, if we have one set of basis vectors, then we can multiply the vectors by any non‐ zero constants or add the vectors to get another basis.\n\nIt would be nice to have a more unique and identifiable basis to describe a subspace. An orthonormal basis contains vectors that have unit length and are orthogonal to each other. Orthogonality is another technical term. (At least 50% of all math and sci‐ ence is made up of technical terms. If you don’t believe me, do a bag-of-words count on this book.) Two vectors are orthogonal to each other if their inner product is zero. For all intents and purposes, we can think of orthogonal vectors as being at 90 degrees to each other. (This is true in Euclidean space, which closely resembles our physical 3D reality.) Normalizing these vectors to have unit length turns them into a uniform set of measuring sticks.\n\nAll in all, a subspace is like a tent, and the orthogonal basis vectors are the number of poles at right angles that are required to prop up the tent. The rank is equal to the total number of orthogonal basis vectors. Figure A-5 illustrates some these concepts.\n\nLinear Modeling and Linear Algebra Basics\n\n|\n\n183\n\nFigure A-5. Illustrations of four useful linear algebra concepts: inner product, linear combination, basis vectors, and orthogonal basis vectors\n\nUseful Linear Algebra Definitions\n\nFor those who think in math, here is some math to make our descriptions precise:\n\nScalar\n\nA number c, in contrast to a vector.\n\nVector\n\nx = (x1, x2, ..., xn)\n\nLinear combination\n\nax + by = (ax1 + by1, ax2 + by2, ..., axn + byn)\n\nSpan of a set of vectors v1, ..., vk\n\nThe set of vectors u = a1v1 + ... + akvk for any a1, ..., ak.\n\nLinear independence\n\nx and y are independent if x ≠ cy for any scalar constant c.\n\nInner product:\n\n⟨x, y⟩ = x1y1 + x2y2 + ... + xnyn\n\nOrthogonal vectors\n\nTwo vectors x and y are orthogonal if ⟨x, y⟩ = 0.\n\n184\n\n| Appendix A: Linear Modeling and Linear Algebra Basics\n\nSubspace\n\nA subset of vectors within a larger containing vector space, satisfying these three criteria:\n\n1. It contains the zero vector. 2. If it contains a vector v, then it contains all vectors cv, where c is a scalar. 3. If it contains two vectors u and v, then it contains the vector u + v.\n\nBasis\n\nA set of vectors that span a subspace.\n\nOrthogonal basis\n\nA basis {v1, v2, ..., vd} where ⟨vi, vj⟩ = 0 for all i, j.\n\nRank of subspace\n\nThe minimum number of linearly independent basis vectors that span the sub‐ space.\n\nSingular Value Decomposition (SVD) A matrix performs a linear transformation on the input vector. Linear transforma‐ tions are very simple and constrained. It follows that a matrix can’t manipulate a sub‐ space willy-nilly. One of the most fascinating theorems of linear algebra proves that every square matrix, no matter what numbers it contains, must map a certain set of vectors back to themselves with some scaling. In the general case of a rectangular matrix, it maps a set of input vectors into a corresponding set of output vectors, and its transpose maps those outputs back to the original inputs. The technical terminol‐ ogy is that square matrices have eigenvectors with eigenvalues, and rectangular matrices have left and right singular vectors with singular values.\n\nEigenvector and Singular Vector Let A be an n × n matrix. If there is a vector v and a scalar λ such that Av = λv, then v is an eigenvector and λ an eigenvalue of A.\n\nLet A be a rectangular matrix. If there are vectors u and v and a scalar σ such that Av = σu and ATu = σv, then u and v are called left and right singular vectors and σ is a singular value of A.\n\nAlgebraically, the SVD of a matrix looks like this:\n\nA = UΣVT\n\nLinear Modeling and Linear Algebra Basics\n\n|\n\n185\n\nwhere the columns of the matrices U and V form orthonormal bases of the input and output space, respectively. Σ is a diagonal matrix containing the singular values.\n\nGeometrically, a matrix performs the following sequence of transformations:\n\n1. Map the input vector onto the right singular basis vector. 2. Scale each coordinate by the corresponding singular values. 3. Multiply this score with each of the left singular vectors. 4. Sum up the results.\n\nFigure A-6 provides an illustration. The operations go from right to left for a matrix- vector multiplication. The rightmost machine rotates and potentially projects the input into a lower-dimensional space. In this illustration, the input cube becomes a flat square, and is also rotated. The next machine squeezes the square in one direction and stretches it in another; the square becomes a rectangle. The last, leftmost machine rotates the rectangle again, and projects it back out into a possibly higher- dimensional space—but it remains a flat rectangle instead of some higher- dimensional object.\n\nFigure A-6. A matrix decomposed into three little machines: rotate, scale, rotate\n\nWhen A is a real matrix (i.e., all of the elements are real-valued), all of the singular values and singular vectors are real-valued. A singular value can be positive, negative, or zero. The ordered set of singular values of a matrix is called its spectrum, and it reveals a lot about the matrix. The gap between the singular values affects how stable the solutions are, and the ratio between the maximum and minimum absolute singu‐ lar values (the condition number) affects how quickly an iterative solver can find the\n\n186\n\n| Appendix A: Linear Modeling and Linear Algebra Basics\n\nsolution. Both of these properties have notable impacts on the quality of the solution one can find.\n\nThe Four Fundamental Subspaces of the Data Matrix Another useful way to dissect a matrix is via the four fundamental subspaces: column space, row space, null space, and left null space. These four subspaces completely characterize the solutions to linear systems involving A or AT (hence the moniker).\n\nFor the data matrix (where the rows are data points and columns are features), the four fundamental subspaces can be understood in relation to the data and features. Let’s look at them in more detail.\n\nColumn space\n\nMathematical definition:\n\nThe set of output vectors s where s = Aw as we vary the weight vector w.\n\nMathematical interpretation:\n\nAll possible linear combinations of columns.\n\nData interpretation:\n\nAll outcomes that are linearly predictable based on observed features. The vec‐ tor w contains the weight of each feature.\n\nBasis:\n\nThe left singular vectors corresponding to nonzero singular values (a subset of the columns of U).\n\nRow space\n\nMathematical definition:\n\nThe set of output vectors r where r = uTA as we vary the weight vector u.\n\nMathematical interpretation:\n\nAll possible linear combinations of rows.\n\nData interpretation:\n\nA vector in the row space is something that can be represented as a linear combi‐ nation of existing data points. Hence, this can be interpreted as the space of “non-novel” data. The vector u contains the weight of each data point in the lin‐ ear combination.\n\nBasis:\n\nThe right singular vectors corresponding to nonzero singular values (a subset of the columns of V).\n\nLinear Modeling and Linear Algebra Basics\n\n|\n\n187\n\nNull space\n\nMathematical definition:\n\nThe set of input vectors w where Aw = 0.\n\nMathematical interpretation:\n\nVectors that are orthogonal to all rows of A. The null space gets squashed to zero by the matrix. This is the “fluff” that adds volume to the solution space of Aw = y.\n\nData interpretation:\n\n“Novel” data points that cannot be represented as any linear combination of existing data points.\n\nBasis:\n\nThe right singular vectors corresponding to the zero singular values (the rest of the columns of V).\n\nLeft null space\n\nMathematical definition:\n\nThe set of input vectors u where uTA = 0.\n\nMathematical interpretation:\n\nVectors that are orthogonal to all columns of A. The left null space is orthogonal to the column space.\n\nData interpretation:\n\n“Novel feature vectors\" that are not representable by linear combinations of existing features.\n\nBasis:\n\nThe left singular vectors corresponding to the zero singular values (the rest of the columns of U).\n\nColumn space and row space contain what is already representable based on observed data and features. Those vectors that lie in the column space are non-novel features. Those vectors that lie in the row space are non-novel data points.\n\nFor the purposes of modeling and prediction, non-novelty is good. A full column space means that the feature set contains enough information to model any target vector we wish. A full row space means that the different data points contain enough variation to cover all possible corners of the feature space. It’s the novel data points and features—respectively contained in the null space and the left null space—that we have to worry about.\n\nIn the application of building linear models of data, the null space can also be viewed as the subspace of “novel” data points. Novelty is not a good thing in this context.\n\n188\n\n| Appendix A: Linear Modeling and Linear Algebra Basics\n\nNovel data points indicate phantom data that is not linearly representable by the training set. Similarly, the left null space contains novel features that are not repre‐ sentable as linear combinations of existing features.\n\nThe null space is orthogonal to the row space. It’s easy to see why. The definition of null space states that w has an inner product of 0 with every row vector in A. There‐ fore, w is orthogonal to the space spanned by these row vectors, i.e., the row space. Similarly, the left null space is orthogonal to the column space.\n\nSolving a Linear System Let’s tie all this math back to the problem at hand: training a linear classifier, which is intimately connected to the task of solving a linear system. We look closely at how a matrix operates because we have to reverse engineer it. In order to train a linear model, we have to find the input weight vector w that maps to the observed output targets y in the system Aw = y, where A is the data matrix.2\n\nLet’s try to crank the machine of the linear operator in reverse. If we had the SVD decomposition of A, then we could map y onto the left singular vectors (columns of U), reverse the scaling factors (multiply by the inverse of the nonzero singular val‐ ues), and finally map them back to the right singular vectors (columns of V). Ta-da! Simple, right?\n\nThis is in fact the process of computing the pseudo-inverse of A. It makes use of a key property of an orthonormal basis: the transpose is the inverse. This is why SVD is so powerful. (In practice, real linear system solvers do not use the SVD, because it’s rather expensive to compute. There are other, much cheaper ways to decompose a matrix, such as QR or LU or Cholesky decompositions.)\n\nHowever, we skipped one tiny little detail in our haste. What happens if the singular value is zero? We can’t take the inverse of 0 because 1/0 = ∞. This is why it’s called the pseudo-inverse. (The real inverse isn’t even defined for rectangular matri‐ ces. Only square matrices have them, as long as all of the eigenvalues are nonzero.) A singular value of zero squashes whatever input was given; there’s no way to retrace its steps and come up with the original input.\n\n2 Actually, it’s a little more complicated than that. y may not be in the column space of A, so there may not be a solution to this equation. Instead of giving up, statistical machine learning looks for an approximate solution. It defines a loss function that quantifies the quality of a solution. If the solution is exact, then the loss is 0. Small errors, small loss; big errors, big loss, and so on. The training process then looks for the best parameters that minimize this loss function. In ordinary linear regression, the loss function is called the squared residual loss, which essentially maps y to the closest point in the column space of A. Logistic regression minimizes the log loss. In both cases, and linear models in general, the linear system Aw=y often lies at the core. Hence, our analysis here is very much relevant.\n\nLinear Modeling and Linear Algebra Basics\n\n|\n\n189",
      "page_number": 190
    },
    {
      "number": 23,
      "title": "Segment 23 (pages 199-206)",
      "start_page": 199,
      "end_page": 206,
      "detection_method": "topic_boundary",
      "content": "Okay, going backward we get stuck on this one little detail. Let’s take what we’ve got and go forward again to see if we can unjam the machine. Suppose we came up with an answer to Aw = y. Let’s call it wparticular, because it’s particularly suited for y. Sup‐ pose that there are also a bunch of input vectors that A squashes to zero. Let’s take one of them and call it wsad-trumpet, because wah wah. Then, what do you think happens when we add wparticular to wsad-trumpet?\n\nA(wparticular + wsad-trumpet) = y\n\nAmazing! So this is a solution too. In fact, any input that gets squashed to zero could be added to a particular solution and give us another solution. The general solution looks like this:\n\nwgeneral = wparticular + whomogeneous\n\nwparticular is an exact solution to the equation Aw = y. There may or may not be such a solution. If there isn’t, then the system can only be approximately solved. If there is, then y belongs to what’s known as the column space of A. The column space is the set of vectors that A can map to, by taking linear combinations of its columns.\n\nwhomogeneous is a solution to the equation Aw = 0. (The grown-up name for wsad-trumpet is whomogeneous.) This should now look familiar. The set of all whomogeneous vectors forms the null space of A. This is the span of the right singular vectors with singular value 0.\n\nThe name “null space” sounds like the destination of woe for an existential crisis. If the null space contains any vectors other than the all-zero vector, then there are infinitely many solutions to the equation Aw = y. Having too many solutions to choose from is not in itself a bad thing. Sometimes any solution will do. But if there are many possible answers, then there are many sets of features that are useful for the classification task. It becomes difficult to understand which ones are truly important.\n\nOne way to fix the problem of a large null space is to regulate the model by adding additional constraints:\n\nAw = y,\n\nwhere w is such that wTw = c.\n\nThis form of regularization constrains the weight vector to have a certain norm, c. The strength of this regularization is controlled by a regularization parameter, which must be tuned, as is done in our experiments.\n\nIn general, feature selection methods deal with selecting the most useful features to reduce computation burden, decrease the amount of confusion for the model, and\n\n190\n\n| Appendix A: Linear Modeling and Linear Algebra Basics\n\nmake the learned model more unique. This is the focus of “Feature Selection” on page 38.\n\nAnother problem is the “unevenness” of the spectrum of the data matrix. When we train a linear classifier, we care not only that there is a general solution to the linear system, but also that we can find it easily. Typically, the training process employs a solver that works by calculating a gradient of the loss function and walking downhill in small steps. When some singular values are very large and others very close to zero, the solver needs to carefully step around the longer singular vectors (those that correspond to large singular values) and spend a lot of time digging around in the shorter singular vectors to find the true answer. This “unevenness” in the spectrum is measured by the condition number of the matrix, which is basically the ratio between the largest and the smallest absolute value of the singular values.\n\nTo summarize, in order for there to be a good linear model that is relatively unique, and in order for it to be easy to find, we wish for the following:\n\n1. The label vector can be well approximated by a linear combination of a subset of features (column vectors). Better yet, the set of features should be linearly inde‐ pendent.\n\n2. In order for the null space to be small, the row space must be large. (This is due to the fact that the two subspaces are orthogonal.) The more linearly independ‐ ent the set of data points (row vectors), the smaller the null space.\n\n3. In order for the solution to be easy to find, the condition number of the data matrix—the ratio between the maximum and minimum singular values—should be small.\n\nBibliography Strang, Gilbert. Linear Algebra and Its Applications. 4th ed. Boston, MA: Cengage Learning, 2006.\n\nLinear Modeling and Linear Algebra Basics\n\n|\n\n191\n\nSymbols ℓ² normalization, 32, 65, 66, 108, 117\n\ncross validation classifier accuracy, 72\n\nA academic paper recommender (see recommen‐\n\nder for academic papers)\n\nactivation functions, 150 AlexNet, 144\n\nconvolutional layers, 150 fully connected layers, 145 pooling layers, 153 ReLU transformation, 151 response normalization layers, 151 structure of, 153-157\n\nanomaly detection of time series, use of PCA,\n\n111\n\napproximately leakage-proof statistics, 93 ASCII, 52 audio data, 133\n\nB back-off bin, 91 bag-of-n-grams, 45 bag-of-words (BoW) featurization, 42\n\nscaling with tf-idf transformation, 65\n\nbasis vectors, 183, 185 bias, 181 “Big Learning Made Easy—With Counts!” blog\n\npost, 87 bigrams, 45 Bilenko, Misha, 87 bin counting, 78, 87-94\n\ncounts without bounds, 94\n\nIndex\n\nexample of, 87 example, using data from Avazu Kaggle\n\ncompetition, 90\n\nguarding against data leakage, 93 odds ratio and log odds ratio for, 88 one-hot encoding vs., 89 rare categories, 91 trade-offs, 95\n\nbinarization (of counts), 9 binning\n\nfixed-width, 12 quantile, 13\n\nbinomial distribution, 55 Box-Cox transforms, 24\n\nC C-HOG blocks, 142 categorical variables, 77-97\n\nencodings, 78-83\n\ndummy coding, 79-82 effect coding, 82 one-hot encoding, 78 pros and cons of, 83 large, dealing with, 83-94 bin counting, 87-94 feature hashing, 84-87\n\nchunking, 56 class-imbalanced dataset, 64 classification\n\nusing k-means featurization, 122-127 with logistic regression, 66-67 classification dataset, creating, 64 clustering algorithms, 116 (see also k-means)\n\nIndex\n\n|\n\n193\n\ncollisions, 84 collocations, 52-59\n\nextracting using chunking and part-of-\n\nspeech tagging, 56\n\nextracting using frequency-based methods,\n\n53\n\nextracting using hypothesis testing, 54 n-grams vs., 53 column space, 187 complex features, 6 handcrafted, 37\n\nconvolutional layers in neural networks,\n\n146-150 convolutional filter, 147 convolutional filter example, 148-150 intuition behind convolutions, 147\n\nconvolutions, 135 count-min sketch, 92 counts, 8-15\n\nbinarization, 9 quantization or binning, 10-15\n\nvisualizing business review counts in\n\nYelp dataset, 11\n\nwithout bounds, 94\n\nCountVectorizer transformer, 46, 65 covariance between random variables (in PCA),\n\n103\n\nD data\n\nabout, 1 answering questions with, 1 importing in recommender for academic\n\npapers (example), 161\n\ndata leakage\n\nguarding against in bin counting, 93 in k-means feature classification, 129\n\ndata matrix, 72-75, 181 data space, 8\n\nfeature vectors in, 44 vs. feature space, 33\n\ndata visualization, importance of, 22 decision surface, 179 decision tree models, 35, 38 input feature scale and, 6 use in model stacking, 128\n\ndeep learning\n\nlearning image features with deep neural\n\nnetworks, 144-157\n\n194\n\n|\n\nIndex\n\nfully connected layers, 144\n\nuse of PCA or ZCA in preprocessing, 112\n\ndelimiters, 52 dense featurization with k-means, 127 dimensionality of subspaces, 183 dimensionality reduction, 99\n\n(see also PCA) nonlinear, 115\n\ndistance, 101 distribution, 6 document frequency, 66 document-term matrix, 72 dummy coding, 79-82\n\npros and cons of, 83\n\nE effect coding, 82\n\npros and cons of, 83\n\neigen decomposition of a matrix, 104 eigenvectors, 185 embedded methods, 38 empirical variance, 104 encodings\n\ncategorical variables, 78-83 dummy coding, 79-82 effect coding, 82 one-hot encoding, 78 pros and cons of, 83\n\nstring objects, 52 Euclidean distance, 117 Euclidean norm, 32\n\nF factor analysis, 112 feature engineering, 5\n\n(see also numeric data; text data) defined, 3 in machine learning workflow, 3 numeric data, 5 feature extraction, 134\n\n(see also image feature extraction) automatic, in deep learning, 134 nonlinear manifold feature extraction by k-\n\nmeans, 117\n\nfeature hashing, 84-87\n\nfor word features, 85 signed, 85 storage and interpretability tradeoffs, 86 trade-offs, 95\n\nfeature normalization, 30\n\n(see also feature scaling) feature scaling, 29-35, 61-76 min-max scaling, 30 standardization (variance scaling), 31 testing scaled vs. unscaled features, 63-72 classification with logistic regression,\n\n66-67\n\ncreating classification dataset, 64 scaling bag-of-words with tf-idf trans‐\n\nform, 65\n\ntuning logistic regression with regulari‐\n\nzation, 68-72\n\nunderstanding the results, 72-75\n\ntf-idf, 61-63 use case for, 35 ℓ² normalization, 32 feature selection, 38-39\n\nembedded methods, 38 filtering techniques, 38 focus of, 190 with interaction features, 37 wrapper methods, 38\n\nfeature space, 7\n\ndata space vs., 33 text sentences in 3D feature space, 44\n\nFeatureHasher, scikit-learn, 86 features\n\nabout, 3 multiple, composed into more complex fea‐\n\ntures, 6\n\nfiltering, 38, 159\n\n(see also item-based collaborative filtering) for cleaner features, using text data, 47-51\n\nfrequency-based filtering, 48 stemming, 51 stopwords, 48\n\ntechniques for collocation extraction, 56\n\nfilters\n\n2D convolutional filter, 147-150 applying to an image, 135\n\nfinancial modeling, use of PCA in, 112 fixed-width binning, 12\n\nin recommender for academic papers\n\n(example), 167 part two, 169\n\nfrequency-based filtering (text data), 48 frequent words, filtering from text, 48 fully connected layers (in neural networks), 144\n\nG Gaussian distribution, 6 Gaussian filter, applying to an image, 148-150 gradient boosting tree (GBT) classifiers, 124,\n\n129\n\ngradient orientation histograms, 139 bins, considerations for, 141 image neighborhoods, 142 normalization, 142 weight functions for gradient magnitude,\n\n141\n\ngrid search, 68\n\ntuning logistic regression hyperparameters\n\nwith, 69\n\nGridSearchCV function, scikit-learn, 69\n\nH hard clustering, 117 hash functions, 84\n\n(see also feature hashing)\n\nheatmap of paper recommendations, 164 heavy-tailed distribution, 16\n\nin text data, 50\n\nHOG (Histogram of Oriented Gradients),\n\n135-144 gradient orientation histograms, 139-143\n\nbins, considerations for, 141 image neighborhoods, 142 normalization, 142 weight functions, 141 image gradients, 135-139 horizontal image gradients, 139 hyperparameter tuning, 68\n\nfor logistic regression with grid search, 69 in comparing models, importance of, 68 in k-means feature classification, 127 using grid search, 68\n\nhyperparameters, 68 hyperplanes, 180 hypothesis testing, using for collocation extrac‐\n\ntion, 54\n\nI image descriptors, 135 image feature extraction, 133-157\n\nlearning image features with deep neural\n\nnetworks, 144-157 convolutional layers, 146-150\n\nIndex\n\n|\n\n195\n\nfully connected layers, 144 pooling layers, 153 ReLU transformation, 150-151 response normalization layers, 151 structure of AlexNet, 153-157\n\nmanual feature extraction with SIFT and\n\nHOG, 135-144\n\nsimplest image features (why they don't\n\nwork), 134 image gradients, 135-139 image neighborhoods, 142 ImageNet Large Scale Visual Recognition Chal‐\n\nlenge (ILSVRC), 144\n\nindices, maintaining assignments during cover‐\n\nsions, 173\n\ninner product, 183, 184 interaction features, 6, 35-38\n\ncomputational expense of, 37 example of use in prediction, 36\n\nintercept, 80 intercept term, 181 intrinsic dimensionality, 100 inverse document frequency, 61 item-based collaborative filtering, 159\n\nbuilding an item-based recommender, 160 filter for academic paper recommender, 161 for academic paper recommender\n\nstage 1, building item feature matrix, 162 stage 2, searching for similar items, 163\n\norigins of, 160 recommendations, 165, 171, 175\n\nK k-means, 117-130\n\nclustering as surface tiling, 119-122 clustering with, 117 featurization for classification, 122-130\n\nk-nearest neighbors (kNN), 124, 129 kernel, 148\n\nL left null space, 188 likelihood ratio test analysis, 54\n\nalgorithm for detecting common phrases\n\nwith, 55 linear algebra\n\nanatomy of a matrix, 182 from vectors to subspaces, 183\n\nuseful concepts, 183\n\n196\n\n|\n\nIndex\n\nfundamental subspaces of the data matrix,\n\n187-189\n\nsingular value decomposition (SVD),\n\n185-187\n\ntips for navigating formulas, 101 useful definitions, 184\n\nlinear classification\n\noverview, 179-182 solving a linear system, 189-191\n\nlinear combination, 184 linear correlation, 103 linear dependent features, 79 linear independence, 184 linear operators, 182 linear projection (in PCA), 102\n\nusing to transform features, 105\n\nlinear regression\n\nlearned coefficients, 81 on categorical variable, using one-hot and\n\ndummy codes, 80 with effect coding, 82 log transforms, 6, 15-29\n\ngeneralization of, in power transforms,\n\n23-29\n\nusing log transformed data to make predic‐\n\ntions, 19-23\n\nusing with inverse document frequency, 62\n\nlog-odds ratio for bin counting, 89 logical functions, 5 logistic regression\n\nclassification with, 66-67\n\ntuning logistic regression with regulari‐\n\nzation, 68-72\n\nlooking at model's use of features, 72-75 with k-means cluster features, 125, 128\n\nM machine learning\n\nprogress in text analysis vs. audio and\n\nimages, 133\n\nworkflow, 3\n\nmagnitude of numeric data, 5 manifold (nonlinear subspace), 115 manifold learning, 116 mask, 148 mathematical formulas, 3 mathematical modeling, 2 matrices\n\nanatomy of, 182\n\ndecomposition of, methods for, 189\n\nmatrix-vector formulation, principal compo‐\n\nnents, 104\n\nmean, 66 metric (k-means), 117 Microsoft Academic Graph dataset, 160 min-max scaling, 30\n\ncaution when performing on sparse fea‐\n\ntures, 32 missing data, 3 model evaluation, 3 model stacking, 6 about, 128 k-means featurization, 122-127 key intuition for, 128\n\nmodels\n\nabout, 2 based on space-partitioning trees, 6 comparing, using hyperparameter tuning\n\nin, 68\n\ndata scheme for academic paper recommen‐\n\nder model, 162\n\nevaluating for use with categorical variable\n\nfeatures, 95\n\ngood linear model that is relatively unique,\n\n191\n\nmodel-driven feature engineering, PCA as\n\nexample of, 113\n\nN n-grams, 45, 52\n\ncollocations vs., 53 computing, 46\n\nnatural language processing (NLP), 52 neighborhoods (image), 142 neural networks (deep), learning image features\n\nwith, 144-157 convolutional layers, 146-150 fully connected layers, 144 pooling layers, 153 ReLU transformation, 150-151 response normalization layers, 151 structure of AlexNet, 153-157 NLP (natural language processing), 52 NLTK Python package, 51 nonlinear dimensionality reduction, 115 nonlinear embedding, 116 nonlinear featurization, 115\n\nnonlinear manifold feature extraction (k-\n\nmeans), 117\n\nnonordinal values, 77 normalization, 5 feature, 30\n\n(see also feature scaling)\n\nof gradient orientation histograms, 142 response normalization layers in neural net‐\n\nworks, 151\n\nnormalization constant, 33 null space, 188, 190 numeric data, 5-39 counts, 8-15\n\nbinarization, 9 quantization or binning, 10-15 feature scaling or normalization, 29-35\n\nmin-max scaling, 30 standardization (variance scaling), 31 ℓ² normalization, 32 feature selection, 38-39 interaction features, 35-38 log transformation, 15-29\n\ngeneralization of, in power transforms,\n\n23-29\n\nusing log transformed data to make pre‐\n\ndictions, 19-23\n\nscalars, vectors, and spaces, 6-8\n\nNumPy sparse array, converting Pandas Data‐\n\nFrame to, 170\n\nO odds ratio for bin counting, 88 one-hot encoding, 78, 88\n\nof cluster membership categorical variable,\n\n122 dense featurization vs., 127\n\npros and cons of, 83 trade-offs, 94 using scikit-learn DictVectorizer, 174 vs. bin counting, 89\n\northogonal basis, 185 orthogonal vectors, 184 orthogonality, 183 orthonormal basis, 183\n\nP Pandas\n\ncomputing quantiles and mapping data into\n\nquantile bins, 15\n\nIndex\n\n|\n\n197\n\nconverting Pandas DataFrame to NumPy\n\nsparse array, 170 maintaining index assignments, 173\n\ndummy coding and one-hot encoding\n\nimplementation, 79\n\nusing to compute n-grams, 46\n\nparsing, 52 part-of-speech (PoS) tagging, 56 PCA (principal component analysis), 99-113 considerations and limitations, 109-111 derivation, 101-106\n\nimplementing PCA, 106 linear projection, 102 principal components, first formulation,\n\n104\n\nprincipal components, general solution\n\nof, 105\n\nprincipal components, matrix-vector\n\nformulation, 104\n\ntransforming features using linear pro‐\n\njection, 105\n\nvariance and empirical variance, 103\n\nuse cases, 111 using on scikit-learn digits dataset, 106-108 whitening and ZCA, 108\n\nphrase detection, 47\n\ncollocation extraction for, 52-59 frequency-based methods, 53 using chunking and part-of-speech tag‐\n\nging, 56\n\nusing hypothesis testing, 54\n\nPoisson distribution, 23 pooling layers (in neural networks), 153 Porter stemmer, 51 power transforms, 6, 23-29 principal component analysis (see PCA) probability plots (probplots), 28 Pythagorean theorem, 32 Python\n\nconverting Pandas DataFrame to NumPy\n\nsparse array, 170 maintaining index assignments, 173 libraries for chunking and part-of-speech\n\n(PoS) tagging, 56\n\nusing to calculate image gradients, 137\n\nQ quantiles, 13 quantization (or binning), 10-15\n\n198\n\n|\n\nIndex\n\nfixed-width binning, 12 quantile binning, 13 quantizing a count, 11\n\nR R-HOG blocks, 142 radial basis function support vector machine\n\n(RBF SVM), 124, 129 random forest classifiers, 124 rank or dimensionality (subspaces), 183, 185 rare categories, 91 rare words, filtering from text, 49 raw counts, PCA and, 111 receiver operating characteristic (ROC) curves,\n\n126\n\nrecommender for academic papers (example),\n\n159-177 first pass, data import, cleaning, and feature\n\nparsing, 161-167 naive approach, 161-167\n\nsecond pass, more engineering and smarter\n\nmodel, 167-173 fixed-width binning and dummy coding\n\n(part 2), 169\n\nthird pass, more features and more infor‐\n\nmation, 173-176 rectified linear unit, 150 rectified linear unit (ReLU) transformation,\n\n150-151\n\nredundant data, 2 reference category, 79\n\neffect, calculating, 82\n\nregularization constraints, adding to a model,\n\n190\n\nregularization, tuning logistic regression with,\n\n68-72 resampling, 69 response normalization layers (in neural net‐\n\nworks), 151 local response normalization, 152\n\nrobustness, 10 row space, 187\n\nS scalars, 6, 184 scale, 5 scikit-learn\n\nCountVectorizer transformer, 46, 65 digits dataset, 106",
      "page_number": 199
    },
    {
      "number": 24,
      "title": "Segment 24 (pages 207-215)",
      "start_page": 207,
      "end_page": 215,
      "detection_method": "topic_boundary",
      "content": "FeatureHasher, 86 GridSearchCV function, 69 k-means clustering on Swiss roll, 120 one-hot encoding, 78 one-hot encoding using DictVectorizer, 174 PCA package, 109 SciPy, stats package, 25 sentences, analysis of, 52 separators, 52 SIFT (Scale Invariant Feature Transform),\n\n135-144 architecture, 143 gradient orientation histograms, 139-143\n\nbins, considerations for, 141 image neighborhoods, 142 normalization, 142 weight functions, 141 image gradients, 135-139\n\nsigmoid function, 66, 150 signed feature hashing, 85 singular value decomposition (SVD) of a\n\nmatrix, 101, 104, 185-187, 189 computational expense of, 110\n\nsingular vectors, 185 space characters, 52 spaces, 7\n\ndata space vs. feature space, 33\n\nspaCy library, 56 span of a set of vectors, 184 sparse data\n\ncaution when performing min-max scaling\n\nor standardization on, 32\n\nexample document-term matrix, 72\n\nspectrum (of a matrix), 110, 186\n\nunevenness in, 191\n\nstandardization, 31\n\ncaution when performing on sparse fea‐\n\ntures, 32\n\nstatistical factor model, 112 statistical modeling, 2 stats package, 25 stemming, 51 stocks, correlation patterns in, 112 stopwords, 48 string objects, 52 subspaces, 185\n\nfundamental, of the data matrix, 187-189 organizing vectors into, 183\n\nSwiss roll, 115\n\nk-means clustering on, 119-122\n\nT tanh function, 150 target engineering, 6, 10 target hints, k-means featurization with/\n\nwithout, 124 text data, 41-60\n\natoms of meaning, 52-59\n\nparsing and tokenization, 52\n\nbag-of-x, turning text into flat vectors,\n\n42-47 bag-of-n-grams, 45 bag-of-words (BoW) featurization, 42\n\nfiltering for cleaner features, 47-51 frequency-based filtering, 48 stemming, 51\n\ntext analysis in machine learning, 133\n\nTextBlob library, 56 tf-idf (term frequency-inverse document fre‐\n\nquency), 61-63 scaling bag-of-words with tf-idf transform,\n\n65\n\nusing in recommender for academic papers\n\n(example), 174\n\ntokenization, 45, 52\n\nscikit-learn's tokenizing pattern, 65\n\ntraining\n\nfitting feature transformer on training data,\n\n66\n\nof logistic regression classifiers with default\n\nparameters, 67 tree-based models, 6\n\n(see also decision tree models) input feature scale and, 29\n\nU Unicode, 52 uniform hash functions, 84 unigrams, 45\n\nV variance, 66\n\nand empirical variance in PCA, 103 estimating via resampling, 69 in a blob of data points, 101\n\nvariance scaling, 31 variance-stabilizing transformations, 23\n\nIndex\n\n|\n\n199\n\n(see also log transforms; power transforms)\n\nvector quantization, 119 vector spaces, 7 vectors, 6, 184\n\nEuclidean distance between, 117 from vectors to subspaces, 183 turning text into flat vectors with bag-of-\n\nwords (BoW), 43 vertical image gradients, 139 volume, 101\n\nW weighting schemes, gradient magnitude, 141\n\n200\n\n|\n\nIndex\n\nwhitening, 108 word features, feature hashing for, 85 wrapper methods, 38 wrong data, 2\n\nZ ZCA (zero-phase component analysis), 109\n\nuse cases, 112\n\nAbout the Authors\n\nAlice Zheng is a technical leader in applied machine learning, spanning algorithm and platform development. Currently, she is a research science manager in Amazon Advertising. Previously, she worked on toolkit development and user education at GraphLab/Dato/Turi, and was a machine learning researcher at Microsoft Research. She holds a PhD in electrical engineering and computer science, and BA degrees in computer science and mathematics, all from UC Berkeley.\n\nAmanda Casari is a leader and engineer who explores the next horizons of technol‐ ogy and how to best demonstrate the impacts these will bring. She is currently a senior product manager and data scientist in Concur Labs and cofounder of the Con‐ cur Labs AI Research team at SAP Concur. She has worked in a breadth of cross- functional roles and engineering disciplines for the last 16 years, including data science, machine learning, complex systems, and robotics. Amanda holds a BS in control systems engineering from the United States Naval Academy and an MS in electrical engineering from the University of Vermont.\n\nColophon\n\nThe animal on the cover of Feature Engineering for Machine Learning is a pharaoh eagle-owl (Bubo ascalaphus). This bird of prey is found in Northern Africa and the Arabian peninsula in rocky, arid habitat. It is among the smaller eagle-owls at 18–20 inches long, though the Bubo genus contains some of the largest owl species. Most eagle-owls (as well as their American cousins, horned owls) have distinctive ear tufts.\n\nThe pharaoh eagle-owl is nocturnal and hunts with a perching method. From a high vantage point, it waits for small mammals, snakes, lizards, birds, and even insects to come into range, before swiftly swooping toward its prey. It is well equipped for this with keen farsightedness and hearing, feathers optimized for silent flight, and sharp talons. Owls can also turn their heads in a range of about 270 degrees, allowing them to look behind themselves without making much movement.\n\nThis species has mottled brown, black, and white plumage, and distinctive orange- yellow eyes. Pharaoh owls are known to mate for life. Nesting sites are created in shallow scrapes among rocks, within crevices, or (occasionally) manmade structures like wells. In Egypt, the owls have been seen nesting on the pyramids.\n\nMany of the animals on O’Reilly covers are endangered; all of them are important to the world. To learn more about how you can help, go to animals.oreilly.com.\n\nThe cover image is from Elements of Ornithology. The cover fonts are URW Type‐ writer and Guardian Sans. The text font is Adobe Minion Pro; the heading font is Adobe Myriad Condensed; and the code font is Dalton Maag’s Ubuntu Mono.",
      "page_number": 207
    },
    {
      "number": 25,
      "title": "Segment 25 (pages 216-217)",
      "start_page": 216,
      "end_page": 217,
      "detection_method": "topic_boundary",
      "content": "",
      "page_number": 216
    }
  ],
  "pages": [
    {
      "page_number": 1,
      "content": "Feature Engineering for Machine Learning\n\nPRINCIPLES AND TECHNIQUES FOR DATA SCIENTISTS\n\nAlice Zheng & Amanda Casari",
      "content_length": 116,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 3,
      "content": "Feature Engineering for Machine Learning Principles and Techniques for Data Scientists\n\nAlice Zheng and Amanda Casari\n\nBeijing Beijing\n\nBoston Boston\n\nFarnham Sebastopol Farnham Sebastopol\n\nTokyo Tokyo",
      "content_length": 201,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 4,
      "content": "Feature Engineering for Machine Learning by Alice Zheng and Amanda Casari\n\nCopyright © 2018 Alice Zheng, Amanda Casari. All rights reserved.\n\nPrinted in the United States of America.\n\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\n\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are also available for most titles (http://oreilly.com/safari). For more information, contact our corporate/insti‐ tutional sales department: 800-998-9938 or corporate@oreilly.com.\n\nEditors: Rachel Roumeliotis and Jeff Bleiel Production Editor: Kristen Brown Copyeditor: Rachel Head Proofreader: Sonia Saruba\n\nIndexer: Ellen Troutman Interior Designer: David Futato Cover Designer: Karen Montgomery Illustrator: Rebecca Demarest\n\nApril 2018:\n\nFirst Edition\n\nRevision History for the First Edition 2018-03-23: First Release\n\nSee http://oreilly.com/catalog/errata.csp?isbn=9781491953242 for release details.\n\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Feature Engineering for Machine Learning, the cover image, and related trade dress are trademarks of O’Reilly Media, Inc.\n\nWhile the publisher and the authors have used good faith efforts to ensure that the information and instructions contained in this work are accurate, the publisher and the authors disclaim all responsibility for errors or omissions, including without limitation responsibility for damages resulting from the use of or reliance on this work. Use of the information and instructions contained in this work is at your own risk. If any code samples or other technology this work contains or describes is subject to open source licenses or the intellectual property rights of others, it is your responsibility to ensure that your use thereof complies with such licenses and/or rights.\n\n978-1-491-95324-2\n\n[LSI]",
      "content_length": 1878,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 5,
      "content": "Table of Contents\n\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . vii\n\n1. The Machine Learning Pipeline. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 Data 1 Tasks 1 Models 2 Features 3 Model Evaluation 3\n\n2. Fancy Tricks with Simple Numbers. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 Scalars, Vectors, and Spaces 6 Dealing with Counts 8 Binarization 9 Quantization or Binning 10 Log Transformation 15 Log Transform in Action 19 Power Transforms: Generalization of the Log Transform 23 Feature Scaling or Normalization 29 Min-Max Scaling 30 Standardization (Variance Scaling) 31 ℓ2 Normalization 32 Interaction Features 35 Feature Selection 38 Summary 39 Bibliography 39\n\n3. Text Data: Flattening, Filtering, and Chunking. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 Bag-of-X: Turning Natural Text into Flat Vectors 42\n\niii",
      "content_length": 1048,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 6,
      "content": "Bag-of-Words 42 Bag-of-n-Grams 45 Filtering for Cleaner Features 47 Stopwords 48 Frequency-Based Filtering 48 Stemming 51 Atoms of Meaning: From Words to n-Grams to Phrases 52 Parsing and Tokenization 52 Collocation Extraction for Phrase Detection 52 Summary 59 Bibliography 60\n\n4. The Effects of Feature Scaling: From Bag-of-Words to Tf-Idf. . . . . . . . . . . . . . . . . . . . . . 61 Tf-Idf : A Simple Twist on Bag-of-Words 61 Putting It to the Test 63 Creating a Classification Dataset 64 Scaling Bag-of-Words with Tf-Idf Transformation 65 Classification with Logistic Regression 66 Tuning Logistic Regression with Regularization 68 Deep Dive: What Is Happening? 72 Summary 75 Bibliography 76\n\n5. Categorical Variables: Counting Eggs in the Age of Robotic Chickens. . . . . . . . . . . . . . . 77 Encoding Categorical Variables 78 One-Hot Encoding 78 Dummy Coding 79 Effect Coding 82 Pros and Cons of Categorical Variable Encodings 83 Dealing with Large Categorical Variables 83 Feature Hashing 84 Bin Counting 87 Summary 94 Bibliography 96\n\n6. Dimensionality Reduction: Squashing the Data Pancake with PCA. . . . . . . . . . . . . . . . . 99 Intuition 99 Derivation 101 Linear Projection 102 Variance and Empirical Variance 103 Principal Components: First Formulation 104 Principal Components: Matrix-Vector Formulation 104\n\niv\n\n|\n\nTable of Contents",
      "content_length": 1355,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 7,
      "content": "General Solution of the Principal Components 105 Transforming Features 105 Implementing PCA 106 PCA in Action 106 Whitening and ZCA 108 Considerations and Limitations of PCA 109 Use Cases 111 Summary 112 Bibliography 113\n\n7. Nonlinear Featurization via K-Means Model Stacking. . . . . . . . . . . . . . . . . . . . . . . . . . . 115 k-Means Clustering 117 Clustering as Surface Tiling 119 k-Means Featurization for Classification 122 Alternative Dense Featurization 127 Pros, Cons, and Gotchas 128 Summary 130 Bibliography 131\n\n8. Automating the Featurizer: Image Feature Extraction and Deep Learning. . . . . . . . . 133 The Simplest Image Features (and Why They Don’t Work) 134 Manual Feature Extraction: SIFT and HOG 135 Image Gradients 135 Gradient Orientation Histograms 139 SIFT Architecture 143 Learning Image Features with Deep Neural Networks 144 Fully Connected Layers 144 Convolutional Layers 146 Rectified Linear Unit (ReLU) Transformation 150 Response Normalization Layers 151 Pooling Layers 153 Structure of AlexNet 153 Summary 157 Bibliography 157\n\n9. Back to the Feature: Building an Academic Paper Recommender. . . . . . . . . . . . . . . . . 159 Item-Based Collaborative Filtering 159 First Pass: Data Import, Cleaning, and Feature Parsing 161 Academic Paper Recommender: Naive Approach 161 Second Pass: More Engineering and a Smarter Model 167 Academic Paper Recommender: Take 2 167 Third Pass: More Features = More Information 173\n\nTable of Contents\n\n|\n\nv",
      "content_length": 1475,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 8,
      "content": "Academic Paper Recommender: Take 3 174 Summary 176 Bibliography 177\n\nA. Linear Modeling and Linear Algebra Basics. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179\n\nIndex. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193\n\nvi\n\n|\n\nTable of Contents",
      "content_length": 366,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 9,
      "content": "Preface\n\nIntroduction Machine learning fits mathematical models to data in order to derive insights or make predictions. These models take features as input. A feature is a numeric repre‐ sentation of an aspect of raw data. Features sit between data and models in the machine learning pipeline. Feature engineering is the act of extracting features from raw data and transforming them into formats that are suitable for the machine learn‐ ing model. It is a crucial step in the machine learning pipeline, because the right fea‐ tures can ease the difficulty of modeling, and therefore enable the pipeline to output results of higher quality. Practitioners agree that the vast majority of time in building a machine learning pipeline is spent on feature engineering and data cleaning. Yet, despite its importance, the topic is rarely discussed on its own. Perhaps this is because the right features can only be defined in the context of both the model and the data; since data and models are so diverse, it’s difficult to generalize the practice of feature engineering across projects.\n\nNevertheless, feature engineering is not just an ad hoc practice. There are deeper principles at work, and they are best illustrated in situ. Each chapter of this book addresses one data problem: how to represent text data or image data, how to reduce the dimensionality of autogenerated features, when and how to normalize, etc. Think of this as a collection of interconnected short stories, as opposed to a single long novel. Each chapter provides a vignette into the vast array of existing feature engi‐ neering techniques. Together, they illustrate the overarching principles.\n\nMastering a subject is not just about knowing the definitions and being able to derive the formulas. It is not enough to know how the mechanism works and what it can do —one must also understand why it is designed that way, how it relates to other tech‐ niques, and what the pros and cons of each approach are. Mastery is about knowing precisely how something is done, having an intuition for the underlying principles, and integrating it into one’s existing web of knowledge. One does not become a mas‐ ter of something by simply reading a book, though a good book can open new doors.\n\nPreface\n\n|\n\nvii",
      "content_length": 2270,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 10,
      "content": "It has to involve practice—putting the ideas to use, which is an iterative process. With every iteration, we know the ideas better and become increasingly more adept and creative at applying them. The goal of this book is to facilitate the application of its ideas.\n\nThis book tries to teach the reason first, and the mathematics second. Instead of only discussing how something is done, we try to teach why. Our goal is to provide the intuition behind the ideas, so that the reader may understand how and when to apply them. There are tons of descriptions and pictures for folks who learn in differ‐ ent ways. Mathematical formulas are presented in order to make the intuition pre‐ cise, and also to bridge this book with other existing offerings.\n\nCode examples in this book are given in Python, using a variety of free and open source packages. The NumPy library provides numeric vector and matrix operations. Pandas provides the DataFrame that is the building block of data science in Python. Scikit-learn is a general-purpose machine learning package with extensive coverage of models and feature transformers. Matplotlib and the styling library Sea‐ born provide plotting and visualization support. You can find these examples as Jupyter notebooks in our GitHub repo.\n\nThe first few chapters start out slow in order to provide a bridge for folks who are just getting started with data science and machine learning. Chapter 1 introduces the fun‐ damental concepts in the machine learning pipeline (data, models, features, etc.). In Chapter 2, we explore basic feature engineering for numeric data: filtering, binning, scaling, log transforms and power transforms, and interaction features. Chapter 3 dives into feature engineering for natural text, exploring techniques like bag-of- words, n-grams, and phrase detection. Chapter 4 examines tf-idf (term frequency– inverse document frequency) as an example of feature scaling and discusses why it works. The pace starts to pick up around Chapter 5, where we talk about efficient encoding techniques for categorical variables, including feature hashing and bin counting. By the time we get to principal component analysis (PCA) in Chapter 6, we are deep in the land of machine learning. Chapter 7 looks at k-means as a featuriza‐ tion technique, which illustrates the useful concept of model stacking. Chapter 8 is all about images, which are much more challenging in terms of feature extraction than text data. We look at two manual feature extraction techniques, SIFT and HOG, before concluding with an explanation of deep learning as the latest feature extrac‐ tion technique for images. We finish up in Chapter 9 by showing a few different tech‐ niques in an end-to-end example, creating a recommender for a dataset of academic papers.\n\nviii\n\n| Preface",
      "content_length": 2809,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 11,
      "content": "In Living Color\n\nThe illustrations in this book are best viewed in color. Really, you should print out the color versions of the Swiss roll in Chapter 7 and paste them into your book. Your aesthetic sense will thank us.\n\nFeature engineering is a vast topic, and more methods are being invented every day, particularly in the area of automatic feature learning. In order to limit the book to a manageable size, we’ve had to make some cuts. This book does not discuss Fourier analysis for audio data, though it is a beautiful subject that is closely related to eigen analysis in linear algebra (which we touch upon in Chapters 4 and 6). We also skip a discussion of random features, which are intimately related to Fourier analysis. We provide an introduction to feature learning via deep learning for image data, but do not go into depth on the numerous deep learning models under active development. Also out of scope are advanced research ideas like random projections, complex text featurization models such as word2vec and Brown clustering, and latent space mod‐ els like Latent Dirichlet allocation and matrix factorization. If those words mean nothing to you, then you are in luck. If the frontiers of feature learning are where your interest lies, then this is probably not the book for you.\n\nThe book assumes knowledge of basic machine learning concepts, such as what a model is and what a vector is, though a refresher is provided so we’re all on the same page. Experience with linear algebra, probability distributions, and optimization are helpful, but not necessary.\n\nConventions Used in This Book The following typographical conventions are used in this book:\n\nItalic\n\nIndicates new terms, URLs, email addresses, filenames, and file extensions.\n\nConstant width\n\nUsed for program listings, as well as within paragraphs to refer to program ele‐ ments such as variable or function names, databases, data types, environment variables, statements, and keywords.\n\nConstant width bold\n\nShows commands or other text that should be typed literally by the user.\n\nConstant width italic\n\nShows text that should be replaced with user-supplied values or by values deter‐ mined by context.\n\nPreface\n\n|\n\nix",
      "content_length": 2202,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 12,
      "content": "The book also contains numerous linear algebra equations. We use the following conventions with regard to notation: scalars are shown in lowercase italic (e.g., a), vectors in lowercase bold (e.g., v), and matrices in uppercase bold and italic (e.g., U).\n\nThis element signifies a tip or suggestion.\n\nThis element signifies a general note.\n\nThis element indicates a warning or caution.\n\nUsing Code Examples Supplemental material (code examples, exercises, etc.) is available for download at https://github.com/alicezheng/feature-engineering-book.\n\nThis book is here to help you get your job done. In general, if example code is offered with this book, you may use it in your programs and documentation. You do not need to contact us for permission unless you’re reproducing a significant portion of the code. For example, writing a program that uses several chunks of code from this book does not require permission. Selling or distributing a CD-ROM of examples from O’Reilly books does require permission. Answering a question by citing this book and quoting example code does not require permission. Incorporating a signifi‐ cant amount of example code from this book into your product’s documentation does require permission.\n\nWe appreciate, but do not require, attribution. An attribution usually includes the title, author, publisher, and ISBN. For example: “Feature Engineering for Machine Learning by Alice Zheng and Amanda Casari (O’Reilly). Copyright 2018 Alice Zheng and Amanda Casari, 978-1-491-95324-2.”\n\nIf you feel your use of code examples falls outside fair use or the permission given above, feel free to contact us at permissions@oreilly.com.\n\nx\n\n| Preface",
      "content_length": 1674,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 13,
      "content": "O’Reilly Safari\n\nSafari (formerly Safari Books Online) is a membership-based training and reference platform for enterprise, government, educators, and individuals.\n\nMembers have access to thousands of books, training videos, Learning Paths, interac‐ tive tutorials, and curated playlists from over 250 publishers, including O’Reilly Media, Harvard Business Review, Prentice Hall Professional, Addison-Wesley Pro‐ fessional, Microsoft Press, Sams, Que, Peachpit Press, Adobe, Focal Press, Cisco Press, John Wiley & Sons, Syngress, Morgan Kaufmann, IBM Redbooks, Packt, Adobe Press, FT Press, Apress, Manning, New Riders, McGraw-Hill, Jones & Bartlett, and Course Technology, among others.\n\nFor more information, please visit http://oreilly.com/safari.\n\nHow to Contact Us Please address comments and questions concerning this book to the publisher:\n\nO’Reilly Media, Inc. 1005 Gravenstein Highway North Sebastopol, CA 95472 800-998-9938 (in the United States or Canada) 707-829-0515 (international or local) 707-829-0104 (fax)\n\nWe have a web page for this book, where we list errata, examples, and any additional information. You can access this page at http://bit.ly/featureEngineering_for_ML.\n\nTo comment or ask technical questions about this book, send email to bookques‐ tions@oreilly.com.\n\nFor more information about our books, courses, conferences, and news, see our web‐ site at http://www.oreilly.com.\n\nFind us on Facebook: http://facebook.com/oreilly\n\nFollow us on Twitter: http://twitter.com/oreillymedia\n\nWatch us on YouTube: http://www.youtube.com/oreillymedia\n\nPreface\n\n|\n\nxi",
      "content_length": 1586,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 14,
      "content": "Acknowledgments First and foremost, we want to thank our editors, Shannon Cutt and Jeff Bleiel, for shepherding two first-time authors through the (unknown to us) long marathon of book publishing. Without your many check-ins, this book would not have seen the light of day. Thank you also to Ben Lorica, O’Reilly Mastermind, whose encourage‐ ment and affirmation turned this from a crazy idea into an actual product. Thank you to Kristen Brown and the O’Reilly production team for their superb attention to detail and extreme patience in waiting for our responses.\n\nIf it takes a village to raise a child, it takes a parliament of data scientists to publish a book. We greatly appreciate every hashtag suggestion, notes on room for improve‐ ment and calls for clarification. Andreas Müller, Sethu Raman, and Antoine Atallah took precious time out of their busy days to provide technical reviews. Antoine not only did so at lightning speed, but also made available his beefy machines for use on experiments. Ted Dunning’s statistical fluency and mastery of applied machine learn‐ ing are legendary. He is also incredibly generous with his time and his ideas, and he literally gave us the method and the example described in the k-means chapter. Owen Zhang revealed his cache of Kaggle nuggets on using response rate features, which were added to machine learning folklore on bin-counting collected by Misha Bilenko. Thank you also to Alex Ott, Francisco Martin, and David Garrison for additional feedback.\n\nSpecial Thanks from Alice I would like to thank the GraphLab/Dato/Turi family for their generous support in the first phase of this project. The idea germinated from interactions with our users. In the process of building a brand new machine learning platform for data scientists, we discovered that the world needs a more systematic understanding of feature engi‐ neering. Thank you to Carlos Guestrin for granting me leave from busy startup life to focus on writing.\n\nThank you to Amanda, who started out as technical reviewer and later pitched in to help bring this book to life. You are the best finisher! Now that this book is done, we’ll have to find another project, if only to keep doing our editing sessions over tea and coffee and sandwiches and takeout food.\n\nSpecial thanks to my friend and healer, Daisy Thompson, for her unwavering support throughout all phases of this project. Without your help, I would have taken much longer to take the plunge, and would have resented the marathon. You brought light and relief to this project, as you do with all your work.\n\nxii\n\n| Preface",
      "content_length": 2598,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 15,
      "content": "Special Thanks from Amanda As this is a book and not a lifetime achievement award, I will attempt to scope my thanks to the project at hand.\n\nMany thanks to Alice for bringing me in as a technical editor and then coauthor. I continue to learn so much from you, including how to write better math jokes and explain complex concepts clearly.\n\nLast in order only, special thanks to my husband, Matthew, for mastering the nearly impossible role of grounding me, encouraging me towards my next goal, and never allowing a concept to be hand-waved away. You are the best partner and my favorite partner in crime. To the biggest and littlest sunshines, you inspire me to make you proud.\n\nPreface\n\n|\n\nxiii",
      "content_length": 696,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 17,
      "content": "CHAPTER 1 The Machine Learning Pipeline\n\nBefore diving into feature engineering, let’s take a moment to take a look at the over‐ all machine learning pipeline. This will help us get situated in the larger picture of the application. To that end, we’ll begin with a little musing on the basic concepts like data and models.\n\nData What we call data are observations of real-world phenomena. For instance, stock market data might involve observations of daily stock prices, announcements of earnings by individual companies, and even opinion articles from pundits. Personal biometric data can include measurements of our minute-by-minute heart rate, blood sugar level, blood pressure, etc. Customer intelligence data includes observations such as “Alice bought two books on Sunday,” “Bob browsed these pages on the web‐ site,” and “Charlie clicked on the special offer link from last week.” We can come up with endless examples of data across different domains.\n\nEach piece of data provides a small window into a limited aspect of reality. The col‐ lection of all of these observations gives us a picture of the whole. But the picture is messy because it is composed of a thousand little pieces, and there’s always measure‐ ment noise and missing pieces.\n\nTasks Why do we collect data? There are questions that data can help us answer—questions like “Which stocks I should invest in?” or “How can I live a healthier lifestyle?” or “How can I understand my customers’ changing tastes, so that my business can serve them better?”\n\n1",
      "content_length": 1527,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 18,
      "content": "The path from data to answers is full of false starts and dead ends (see Figure 1-1). What starts out as a promising approach may not pan out. What was originally just a hunch may end up leading to the best solution. Workflows with data are frequently multistage, iterative processes. For instance, stock prices are observed at the exchange, aggregated by an intermediary like Thomson Reuters, stored in a database, bought by a company, converted into a Hive store on a Hadoop cluster, pulled out of the store by a script, subsampled, massaged, and cleaned by another script, dumped to a file, and converted to a format that you can try out in your favorite modeling library in R, Python, or Scala. The predictions are then dumped back out to a CSV file and parsed by an evaluator, and the model is iterated multiple times, rewritten in C++ or Java by your production team, and run on all of the data before the final pre‐ dictions are pumped out to another database.\n\nFigure 1-1. The garden of bifurcating paths between data and answers\n\nHowever, if we disregard the mess of tools and systems for a moment, we might see that the process involves two mathematical entities that are the bread and butter of machine learning: models and features.\n\nModels Trying to understand the world through data is like trying to piece together reality using a noisy, incomplete jigsaw puzzle with a bunch of extra pieces. This is where mathematical modeling—in particular statistical modeling—comes in. The language of statistics contains concepts for many frequent characteristics of data, such as wrong, redundant, or missing. Wrong data is the result of a mistake in measurement. Redundant data contains multiple aspects that convey exactly the same information. For instance, the day of week may be present as a categorical variable with values of “Monday,” “Tuesday,” ... “Sunday,” and again included as an integer value between 0\n\n2\n\n|\n\nChapter 1: The Machine Learning Pipeline",
      "content_length": 1969,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 19,
      "content": "and 6. If this day-of-week information is not present for some data points, then you’ve got missing data on your hands.\n\nA mathematical model of data describes the relationships between different aspects of the data. For instance, a model that predicts stock prices might be a formula that maps a company’s earning history, past stock prices, and industry to the predicted stock price. A model that recommends music might measure the similarity between users (based on their listening habits), and recommend the same artists to users who have listened to a lot of the same songs.\n\nMathematical formulas relate numeric quantities to each other. But raw data is often not numeric. (The action “Alice bought The Lord of the Rings trilogy on Wednesday” is not numeric, and neither is the review that she subsequently writes about the book.) There must be a piece that connects the two together. This is where features come in.\n\nFeatures A feature is a numeric representation of raw data. There are many ways to turn raw data into numeric measurements, which is why features can end up looking like a lot of things. Naturally, features must derive from the type of data that is available. Per‐ haps less obvious is the fact that they are also tied to the model; some models are more appropriate for some types of features, and vice versa. The right features are rel‐ evant to the task at hand and should be easy for the model to ingest. Feature engineer‐ ing is the process of formulating the most appropriate features given the data, the model, and the task.\n\nThe number of features is also important. If there are not enough informative fea‐ tures, then the model will be unable to perform the ultimate task. If there are too many features, or if most of them are irrelevant, then the model will be more expen‐ sive and tricky to train. Something might go awry in the training process that impacts the model’s performance.\n\nModel Evaluation Features and models sit between raw data and the desired insights (see Figure 1-2). In a machine learning workflow, we pick not only the model, but also the features. This is a double-jointed lever, and the choice of one affects the other. Good features make the subsequent modeling step easy and the resulting model more capable of complet‐ ing the desired task. Bad features may require a much more complicated model to achieve the same level of performance. In the rest of this book, we will cover different kinds of features and discuss their pros and cons for different types of data and mod‐ els. Without further ado, let’s get started!\n\nFeatures\n\n|\n\n3",
      "content_length": 2596,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 20,
      "content": "Figure 1-2. The place of feature engineering in the machine learning workflow\n\n4\n\n|\n\nChapter 1: The Machine Learning Pipeline",
      "content_length": 125,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 21,
      "content": "CHAPTER 2 Fancy Tricks with Simple Numbers\n\nBefore diving into complex data types such as text and image, let’s start with the sim‐ plest: numeric data. This may come from a variety of sources: geolocation of a place or a person, the price of a purchase, measurements from a sensor, traffic counts, etc. Numeric data is already in a format that’s easily ingestible by mathematical models. This doesn’t mean that feature engineering is no longer necessary, though. Good fea‐ tures should not only represent salient aspects of the data, but also conform to the assumptions of the model. Hence, transformations are often necessary. Numeric fea‐ ture engineering techniques are fundamental. They can be applied whenever raw data is converted into numeric features.\n\nThe first sanity check for numeric data is whether the magnitude matters. Do we just need to know whether it’s positive or negative? Or perhaps we only need to know the magnitude at a very coarse granularity? This sanity check is particularly important for automatically accrued numbers such as counts—the number of daily visits to a website, the number of reviews garnered by a restaurant, etc.\n\nNext, consider the scale of the features. What are the largest and the smallest values? Do they span several orders of magnitude? Models that are smooth functions of input features are sensitive to the scale of the input. For example, 3x + 1 is a simple linear function of the input x, and the scale of its output depends directly on the scale of the input. Other examples include k-means clustering, nearest neighbors methods, radial basis function (RBF) kernels, and anything that uses the Euclidean distance. For these models and modeling components, it is often a good idea to normalize the features so that the output stays on an expected scale.\n\nLogical functions, on the other hand, are not sensitive to input feature scale. Their output is binary no matter what the inputs are. For instance, the logical AND takes any two variables and outputs 1 if and only if both of the inputs are true. Another example of a logical function is the step function (e.g., is input x greater than 5?).\n\n5",
      "content_length": 2154,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 22,
      "content": "Decision tree models consist of step functions of input features. Hence, models based on space-partitioning trees (decision trees, gradient boosted machines, random for‐ ests) are not sensitive to scale. The only exception is if the scale of the input grows over time, which is the case if the feature is an accumulated count of some sort— eventually it will grow outside of the range that the tree was trained on. If this might be the case, then it might be necessary to rescale the inputs periodically. Another sol‐ ution is the bin-counting method discussed in Chapter 5.\n\nIt’s also important to consider the distribution of numeric features. Distribution summarizes the probability of taking on a particular value. The distribution of input features matters to some models more than others. For instance, the training process of a linear regression model assumes that prediction errors are distributed like a Gaussian. This is usually fine, except when the prediction target spreads out over sev‐ eral orders of magnitude. In this case, the Gaussian error assumption likely no longer holds. One way to deal with this is to transform the output target in order to tame the magnitude of the growth. (Strictly speaking this would be target engineering, not fea‐ ture engineering.) Log transforms, which are a type of power transform, take the dis‐ tribution of the variable closer to Gaussian.\n\nIn addition to features tailoring to the assumptions of the model or training process, multiple features can be composed together into more complex features. The hope is that complex features can more succinctly capture important information in raw data. Making the input features more “eloquent” allows the model itself to be simpler, easier to train and evaluate, and to make better predictions. Taken to an extreme, complex features may themselves be the output of statistical models. This is a concept known as model stacking, which we discuss in much more detail in Chapters 7 and 8. In this chapter, we give the simplest example of complex features: interaction features.\n\nInteraction features are simple to formulate, but the combination of features results in many more being input into the model. In order to reduce the computational expense, it is usually necessary to prune the input features using automatic feature selection.\n\nWe’ll begin with the basic concepts of scalars, vectors, and spaces, followed by discus‐ sions of scale, distribution, interaction features, and feature selection.\n\nScalars, Vectors, and Spaces Before we go any further, we need to define some basic concepts that underlie the rest of this book. A single numeric feature is also known as a scalar. An ordered list of scalars is known as a vector. Vectors sit within a vector space. In the vast majority of machine learning applications, the input to a model is usually represented as a\n\n6\n\n|\n\nChapter 2: Fancy Tricks with Simple Numbers",
      "content_length": 2921,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 23,
      "content": "numeric vector. The rest of this book will discuss best-practice strategies for convert‐ ing raw data into a vector of numbers.\n\nA vector can be visualized as a point in space. (Sometimes people draw a line or arrow from the origin to that point. In this book, we will mostly use just the point.) For instance, suppose we have a two-dimensional vector v = [1, –1]. The vector con‐ tains two numbers: in the first direction, d1, the vector has a value of 1, and in the second direction, d2, it has a value of –1. We can plot v in a 2D plot, as shown in Figure 2-1.\n\nFigure 2-1. A single vector\n\nIn the world of data, an abstract vector and its feature dimensions take on actual meaning. For instance, a vector can represent a person’s preference for songs. Each song is a feature, where a value of 1 is equivalent to a thumbs-up, and –1 a thumbs- down. Suppose the vector v represents the preferences of a listener, Bob. Bob likes “Blowin’ in the Wind” by Bob Dylan and “Poker Face” by Lady Gaga. Other people might have different preferences. Collectively, a collection of data can be visualized in feature space as a point cloud.\n\nScalars, Vectors, and Spaces\n\n|\n\n7",
      "content_length": 1166,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 24,
      "content": "Conversely, a song can be represented by the individual preferences of a group of people. Suppose there are only two listeners, Alice and Bob. Alice likes “Poker Face,” “Blowin’ in the Wind,” and “Hallelujah” by Leonard Cohen, but hates Katy Perry’s “Roar” and Radiohead’s “Creep.” Bob likes “Roar,” “Hallelujah,” and “Blowin’ in the Wind,” but hates “Poker Face” and “Creep.” Each song is a point in the space of lis‐ teners. Just like we can visualize data in feature space, we can visualize features in data space. Figure 2-2 shows this example.\n\nFigure 2-2. Illustration of feature space versus data space\n\nDealing with Counts In the age of Big Data, counts can quickly accumulate without bound. A user might put a song or a movie on infinite playback or use a script to repeatedly check for the availability of tickets for a popular show, which will cause the play count or website visit count to quickly rise. When data can be produced at high volume and velocity, it’s very likely to contain a few extreme values. It is a good idea to check the scale and determine whether to keep the data as raw numbers, convert them into binary values to indicate presence, or bin them into coarser granularity. To illustrate these ideas, let’s look at a few examples.\n\n8\n\n|\n\nChapter 2: Fancy Tricks with Simple Numbers",
      "content_length": 1312,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 25,
      "content": "Binarization The Echo Nest Taste Profile subset, the official user data collection for the Million Song Dataset, contains the full music listening histories of one million users on Echo Nest. Here are some relevant statistics about the dataset:\n\nStatistics on the Echo Nest Taste Profile Dataset\n\nThere are more than 48 million triplets of user ID, song ID, and listen count.\n\nThe full dataset contains 1,019,318 unique users and 384,546 unique songs.\n\nSuppose our task is to build a recommender to recommend songs to users. One com‐ ponent of the recommender might predict how much a user will enjoy a particular song. Since the data contains actual listen counts, should that be the target of the pre‐ diction? This would be the right thing to do if a large listen count means the user really likes the song and a low listen count means they’re not interested in it. How‐ ever, the data shows that while 99% of the listen counts are 24 or lower, there are also some listen counts in the thousands, with the maximum being 9,667. (As Figure 2-3 shows, the histogram peaks in the bin closest to 0. But more than 10,000 triplets have greater counts, with a few in the thousands.) These values are anomalously large; if we were to try to predict the actual listen counts, the model would be pulled off course by these large values.\n\nFigure 2-3. Histogram of listen counts in the Taste Profile subset of the Million Song Dataset—note that the y-axis is on a log scale\n\nDealing with Counts\n\n|\n\n9",
      "content_length": 1490,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 26,
      "content": "In the Million Song Dataset, the raw listen count is not a robust measure of user taste. (In statistical lingo, robustness means that the method works under a wide variety of conditions.) Users have different listening habits. Some people might put their favor‐ ite songs on infinite loop, while others might savor them only on special occasions. We can’t necessarily say that someone who listens to a song 20 times must like it twice as much as someone else who listens to it 10 times.\n\nA more robust representation of user preference is to binarize the count and clip all counts greater than 1 to 1, as illustrated in Example 2-1. In other words, if the user listened to a song at least once, then we count it as the user liking the song. This way, the model will not need to spend cycles on predicting the minute differences between the raw counts. The binary target is a simple and robust measure of user preference.\n\nExample 2-1. Binarizing listen counts in the Million Song Dataset\n\n>>> import pandas as pd >>> listen_count = pd.read_csv('millionsong/train_triplets.txt.zip', ... header=None, delimiter='\\t') # The table contains user-song-count triplets. Only nonzero counts are # included. Hence, to binarize the count, we just need to set the entire # count column to 1. >>> listen_count[2] = 1\n\nThis is an example where we engineer the target variable of the model. Strictly speak‐ ing, the target is not a feature because it’s not the input. But on occasion we do need to modify the target in order to solve the right problem.\n\nQuantization or Binning For this exercise, we take data from round 6 of the Yelp dataset challenge and create a much smaller classification dataset. The Yelp dataset contains user reviews of busi‐ nesses from 10 cities across North America and Europe. Each business is labeled with zero or more categories.\n\nStatistics on the Yelp Reviews Dataset (Round 6)\n\nThere are 782 business categories.\n\nThe full dataset contains 1,569,264 (≈1.6M) reviews and 61,184 (61K) businesses.\n\n“Restaurants” (990,627 reviews) and “Nightlife” (210,028 reviews) are the most popular categories, review count–wise.\n\nNo business is categorized as both a restaurant and a nightlife venue. So, there is no overlap between the two groups of reviews.\n\n10\n\n|\n\nChapter 2: Fancy Tricks with Simple Numbers",
      "content_length": 2315,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 27,
      "content": "Each business has a review count. Suppose our task is to use collaborative filtering to predict the rating a user might give to a business. The review count might be a useful input feature because there is usually a strong correlation between popularity and good ratings. Now the question is, should we use the raw review count or process it further? Figure 2-4, produced by Example 2-2, shows the histogram of all business review counts. We see the same pattern as in the listen counts in the previous exam‐ ple: most of the counts are small, but some businesses have reviews in the thousands.\n\nExample 2-2. Visualizing business review counts in the Yelp dataset\n\n>>> import pandas as pd >>> import json\n\n# Load the data about businesses >>> biz_file = open('yelp_academic_dataset_business.json') >>> biz_df = pd.DataFrame([json.loads(x) for x in biz_file.readlines()]) >>> biz_file.close()\n\n>>> import matplotlib.pyplot as plt >>> import seaborn as sns\n\n# Plot the histogram of the review counts >>> sns.set_style('whitegrid') >>> fig, ax = plt.subplots() >>> biz_df['review_count'].hist(ax=ax, bins=100) >>> ax.set_yscale('log') >>> ax.tick_params(labelsize=14) >>> ax.set_xlabel('Review Count', fontsize=14) >>> ax.set_ylabel('Occurrence', fontsize=14)\n\nRaw counts that span several orders of magnitude are problematic for many models. In a linear model, the same linear coefficient would have to work for all possible val‐ ues of the count. Large counts could also wreak havoc in unsupervised learning methods such as k-means clustering, which uses Euclidean distance as a similarity function to measure the similarity between data points. A large count in one element of the data vector would outweigh the similarity in all other elements, which could throw off the entire similarity measurement.\n\nOne solution is to contain the scale by quantizing the count. In other words, we group the counts into bins, and get rid of the actual count values. Quantization maps a continuous number to a discrete one. We can think of the discretized numbers as an ordered sequence of bins that represent a measure of intensity.\n\nDealing with Counts\n\n|\n\n11",
      "content_length": 2147,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 28,
      "content": "Figure 2-4. Histogram of business review counts in the Yelp reviews dataset—the y-axis is on a log scale\n\nIn order to quantize data, we have to decide how wide each bin should be. The solu‐ tions fall into two categories: fixed-width or adaptive. We will give an example of each type.\n\nFixed-width binning\n\nWith fixed-width binning, each bin contains a specific numeric range. The ranges can be custom designed or automatically segmented, and they can be linearly scaled or exponentially scaled. For example, we can group people into age ranges by decade: 0–9 years old in bin 1, 10–19 years in bin 2, etc. To map from the count to the bin, we simply divide by the width of the bin and take the integer part.\n\nIt’s also common to see custom-designed age ranges that better correspond to stages of life, such as:\n\n0–12 years old\n\n12–17 years old\n\n18–24 years old\n\n25–34 years old\n\n35–44 years old\n\n45–54 years old\n\n55–64 years old\n\n12\n\n|\n\nChapter 2: Fancy Tricks with Simple Numbers",
      "content_length": 981,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 29,
      "content": "65–74 years old\n\n75 years or older\n\nWhen the numbers span multiple magnitudes, it may be better to group by powers of 10 (or powers of any constant): 0–9, 10–99, 100–999, 1000–9999, etc. The bin widths grow exponentially, going from O(10), to O(100), O(1000), and beyond. To map from the count to the bin, we take the log of the count. Exponential-width binning is very much related to the log transform, which we discuss in “Log Transformation” on page 15. Example 2-3 illustrates several of these binning methods.\n\nExample 2-3. Quantizing counts with fixed-width bins\n\n>>> import numpy as np\n\n# Generate 20 random integers uniformly between 0 and 99 >>> small_counts = np.random.randint(0, 100, 20) >>> small_counts array([30, 64, 49, 26, 69, 23, 56, 7, 69, 67, 87, 14, 67, 33, 88, 77, 75, 47, 44, 93]) # Map to evenly spaced bins 0-9 by division >>> np.floor_divide(small_counts, 10) array([3, 6, 4, 2, 6, 2, 5, 0, 6, 6, 8, 1, 6, 3, 8, 7, 7, 4, 4, 9], dtype=int32)\n\n# An array of counts that span several magnitudes >>> large_counts = [296, 8286, 64011, 80, 3, 725, 867, 2215, 7689, 11495, 91897, ... 44, 28, 7971, 926, 122, 22222] # Map to exponential-width bins via the log function >>> np.floor(np.log10(large_counts)) array([ 2., 3., 4., 1., 0., 2., 2., 3., 3., 4., 4., 1., 1., 3., 2., 2., 4.])\n\nQuantile binning\n\nFixed-width binning is easy to compute. But if there are large gaps in the counts, then there will be many empty bins with no data. This problem can be solved by adaptively positioning the bins based on the distribution of the data. This can be done using the quantiles of the distribution.\n\nQuantiles are values that divide the data into equal portions. For example, the median divides the data in halves; half the data points are smaller and half larger than the median. The quartiles divide the data into quarters, the deciles into tenths, etc. Example 2-4 demonstrates how to compute the deciles of the Yelp business review counts, and Figure 2-5 overlays the deciles on the histogram. This gives a much clearer picture of the skew toward smaller counts.\n\nDealing with Counts\n\n|\n\n13",
      "content_length": 2107,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 30,
      "content": "Example 2-4. Computing deciles of Yelp business review counts\n\n>>> deciles = biz_df['review_count'].quantile([.1, .2, .3, .4, .5, .6, .7, .8, .9]) >>> deciles 0.1 3.0 0.2 4.0 0.3 5.0 0.4 6.0 0.5 8.0 0.6 12.0 0.7 17.0 0.8 28.0 0.9 58.0 Name: review_count, dtype: float64\n\n# Visualize the deciles on the histogram >>> sns.set_style('whitegrid') >>> fig, ax = plt.subplots() >>> biz_df['review_count'].hist(ax=ax, bins=100) >>> for pos in deciles: ... handle = plt.axvline(pos, color='r') >>> ax.legend([handle], ['deciles'], fontsize=14) >>> ax.set_yscale('log') >>> ax.set_xscale('log') >>> ax.tick_params(labelsize=14) >>> ax.set_xlabel('Review Count', fontsize=14) >>> ax.set_ylabel('Occurrence', fontsize=14)\n\nFigure 2-5. Deciles of the review counts in the Yelp reviews dataset—both the x- and y- axes are on a log scale\n\n14\n\n|\n\nChapter 2: Fancy Tricks with Simple Numbers",
      "content_length": 875,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 31,
      "content": "To compute the quantiles and map data into quantile bins, we can use the Pandas library, as shown in Example 2-5. pandas.DataFrame.quantile and pandas.Ser ies.quantile compute the quantiles. pandas.qcut maps data into a desired number of quantiles.\n\nExample 2-5. Binning counts by quantiles\n\n# Continue example 2-3 with large_counts >>> import pandas as pd\n\n# Map the counts to quartiles >>> pd.qcut(large_counts, 4, labels=False) array([1, 2, 3, 0, 0, 1, 1, 2, 2, 3, 3, 0, 0, 2, 1, 0, 3], dtype=int64)\n\n# Compute the quantiles themselves >>> large_counts_series = pd.Series(large_counts) >>> large_counts_series.quantile([0.25, 0.5, 0.75]) 0.25 122.0 0.50 926.0 0.75 8286.0 dtype: float64\n\nLog Transformation In the previous section, we briefly introduced the notion of taking the logarithm of the count to map the data to exponential-width bins. Let’s take a closer look at that now.\n\nThe log function is the inverse of the exponential function. It is defined such that loga(ax) = x, where a is a positive constant, and x can be any positive number. Since a0 = 1, we have loga(1) = 0. This means that the log function maps the small range of numbers between (0, 1) to the entire range of negative numbers (–∞, 0). The function log10(x) maps the range of [1, 10] to [0, 1], [10, 100] to [1, 2], and so on. In other words, the log function compresses the range of large numbers and expands the range of small numbers. The larger x is, the slower log(x) increments.\n\nThis is easier to digest by looking at a plot of the log function (see Figure 2-6). Note how the horizontal x values from 100 to 1,000 get compressed into just 2.0 to 3.0 in the vertical y range, while the tiny horizontal portion of x values less than 100 are mapped to the rest of the vertical range.\n\nLog Transformation\n\n|\n\n15",
      "content_length": 1794,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 32,
      "content": "Figure 2-6. The log function compresses the high numeric range and expands the low range\n\nThe log transform is a powerful tool for dealing with positive numbers with a heavy- tailed distribution. (A heavy-tailed distribution places more probability mass in the tail range than a Gaussian distribution.) It compresses the long tail in the high end of the distribution into a shorter tail, and expands the low end into a longer head. Figure 2-7 compares the histograms of Yelp business review counts before and after log transformation (see Example 2-6). The y-axes are now both on a normal (linear) scale. The increased bin spacing in the bottom plot between the range of (0.5, 1] is due to there being only 10 possible integer counts between 1 and 10. Notice that the original review counts are very concentrated in the low count region, with outliers stretching out above 4,000. After log transformation, the histogram is less concentra‐ ted in the low end and more spread out over the x-axis.\n\nExample 2-6. Visualizing the distribution of review counts before and after log transform\n\n>>> fig, (ax1, ax2) = plt.subplots(2,1) >>> biz_df['review_count'].hist(ax=ax1, bins=100) >>> ax1.tick_params(labelsize=14) >>> ax1.set_xlabel('review_count', fontsize=14) >>> ax1.set_ylabel('Occurrence', fontsize=14)\n\n>>> biz_df['log_review_count'].hist(ax=ax2, bins=100) >>> ax2.tick_params(labelsize=14) >>> ax2.set_xlabel('log10(review_count))', fontsize=14) >>> ax2.set_ylabel('Occurrence', fontsize=14)\n\n16\n\n|\n\nChapter 2: Fancy Tricks with Simple Numbers",
      "content_length": 1547,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 33,
      "content": "Figure 2-7. Comparison of Yelp business review counts before (top) and after (bottom) log transformation\n\nAs another example, let’s consider the Online News Popularity dataset from the UC Irvine Machine Learning Repository (Fernandes et al., 2015).\n\nStatistics on the Online News Popularity Dataset\n\nThe dataset includes 60 features of a set of 39,797 news articles published by Mashable over a period of 2 years.\n\nOur goal is to use these features to predict the popularity of the articles in terms of the number of shares on social media. In this example, we’ll focus on only one feature —the number of words in the article. Figure 2-8 shows the histograms of the feature before and after log transformation (see Example 2-7). Notice that the distribution looks much more Gaussian after log transformation, with the exception of the burst of number of articles of length zero (no content).\n\nLog Transformation\n\n|\n\n17",
      "content_length": 918,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 34,
      "content": "Example 2-7. Visualizing the distribution of news article popularity with and without log transformation\n\n>>> fig, (ax1, ax2) = plt.subplots(2,1) >>> df['n_tokens_content'].hist(ax=ax1, bins=100) >>> ax1.tick_params(labelsize=14) >>> ax1.set_xlabel('Number of Words in Article', fontsize=14) >>> ax1.set_ylabel('Number of Articles', fontsize=14)\n\n>>> df['log_n_tokens_content'].hist(ax=ax2, bins=100) >>> ax2.tick_params(labelsize=14) >>> ax2.set_xlabel('Log of Number of Words', fontsize=14) >>> ax2.set_ylabel('Number of Articles', fontsize=14)\n\nFigure 2-8. Comparison of word counts in Mashable news articles before (top) and after (bottom) log transformation\n\n18\n\n|\n\nChapter 2: Fancy Tricks with Simple Numbers",
      "content_length": 714,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 35,
      "content": "Log Transform in Action Let’s see how the log transform performs for supervised learning. We’ll use both of the previous datasets here. For the Yelp reviews dataset, we’ll use the number of reviews to predict the average rating of a business (see Example 2-8). For the Masha‐ ble news articles, we’ll use the number of words in an article to predict its popularity. Since the outputs are continuous numbers, we’ll use simple linear regression as the model. We use scikit-learn to perform 10-fold cross validation of linear regression on the feature with and without log transformation. The models are evaluated by the R- squared score, which measures how well a trained regression model predicts new data. Good models have high R-squared scores. A perfect model gets the maximum score of 1. The score can be negative, and a bad model can get an arbitrarily low neg‐ ative score. Using cross validation, we obtain not only an estimate of the score but also a variance, which helps us gauge whether the differences between the two models are meaningful.\n\nExample 2-8. Using log transformed Yelp review counts to predict average business rating\n\n>>> import pandas as pd >>> import numpy as np >>> import json >>> from sklearn import linear_model >>> from sklearn.model_selection import cross_val_score\n\n# Using the previously loaded Yelp reviews DataFrame, # compute the log transform of the Yelp review count. # Note that we add 1 to the raw count to prevent the logarithm from # exploding into negative infinity in case the count is zero. >>> biz_df['log_review_count'] = np.log10(biz_df['review_count'] + 1)\n\n# Train linear regression models to predict the average star rating of a business, # using the review_count feature with and without log transformation. # Compare the 10-fold cross validation score of the two models. >>> m_orig = linear_model.LinearRegression() >>> scores_orig = cross_val_score(m_orig, biz_df[['review_count']], ... biz_df['stars'], cv=10) >>> m_log = linear_model.LinearRegression() >>> scores_log = cross_val_score(m_log, biz_df[['log_review_count']], ... biz_df['stars'], cv=10) >>> print(\"R-squared score without log transform: %0.5f (+/- %0.5f)\" ... % (scores_orig.mean(), scores_orig.std() * 2)) >>> print(\"R-squared score with log transform: %0.5f (+/- %0.5f)\" ... % (scores_log.mean(), scores_log.std() * 2)) R-squared score without log transform: -0.03683 (+/- 0.07280) R-squared score with log transform: -0.03694 (+/- 0.07650)\n\nLog Transformation\n\n|\n\n19",
      "content_length": 2491,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 36,
      "content": "Judging by the output of the experiment, the two simple models (with and without log transform) are equally bad at predicting the target, with the log transformed fea‐ ture performing slightly worse. How disappointing! It’s not surprising that neither of them are very good, given that they both use just one feature, but one would have hoped that the log transformed feature might have performed better.\n\nNow let’s look at how the log transform does on the Online News Popularity dataset (Example 2-9).\n\nExample 2-9. Using log transformed word counts in the Online News Popularity dataset to predict article popularity\n\n# Download the Online News Popularity dataset from UCI, then use # Pandas to load the file into a DataFrame. >>> df = pd.read_csv('OnlineNewsPopularity.csv', delimiter=', ')\n\n# Take the log transform of the 'n_tokens_content' feature, which # represents the number of words (tokens) in a news article. >>> df['log_n_tokens_content'] = np.log10(df['n_tokens_content'] + 1)\n\n# Train two linear regression models to predict the number of shares # of an article, one using the original feature and the other the # log transformed version. >>> m_orig = linear_model.LinearRegression() >>> scores_orig = cross_val_score(m_orig, df[['n_tokens_content']], ... df['shares'], cv=10) >>> m_log = linear_model.LinearRegression() >>> scores_log = cross_val_score(m_log, df[['log_n_tokens_content']], ... df['shares'], cv=10) >>> print(\"R-squared score without log transform: %0.5f (+/- %0.5f)\" ... % (scores_orig.mean(), scores_orig.std() * 2)) >>> print(\"R-squared score with log transform: %0.5f (+/- %0.5f)\" ... % (scores_log.mean(), scores_log.std() * 2)) R-squared score without log transform: -0.00242 (+/- 0.00509) R-squared score with log transform: -0.00114 (+/- 0.00418)\n\nThe confidence intervals still overlap, but the model with the log transformed feature is doing better than the one without. Why is the log transform so much more suc‐ cessful on this dataset? We can get a clue by looking at the scatter plots (Example 2-10) of the input feature and target values. As can be seen in the bottom panel of Figure 2-9, the log transform reshaped the x-axis, pulling the articles with large outliers in the target value (>200,000 shares) further out toward the righthand side of the axis. This gives the linear model more “breathing room” on the low end of the input feature space. Without the log transform (top panel), the model is under more pressure to fit very different target values under very small changes in the input.\n\n20\n\n|\n\nChapter 2: Fancy Tricks with Simple Numbers",
      "content_length": 2598,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 37,
      "content": "Example 2-10. Visualizing the correlation between input and output in the news popularity prediction problem\n\n>>> fig2, (ax1, ax2) = plt.subplots(2,1) >>> ax1.scatter(df['n_tokens_content'], df['shares']) >>> ax1.tick_params(labelsize=14) >>> ax1.set_xlabel('Number of Words in Article', fontsize=14) >>> ax1.set_ylabel('Number of Shares', fontsize=14)\n\n>>> ax2.scatter(df['log_n_tokens_content'], df['shares']) >>> ax2.tick_params(labelsize=14) >>> ax2.set_xlabel('Log of the Number of Words in Article', fontsize=14) >>> ax2.set_ylabel('Number of Shares', fontsize=14)\n\nFigure 2-9. Scatter plots of number of words (input) versus number of shares (target) in the Online News Popularity dataset—the top plot visualizes the original feature, and the bottom plot shows the scatter plot after log transformation\n\nLog Transformation\n\n|\n\n21",
      "content_length": 836,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 38,
      "content": "Compare this with the same scatter plot applied to the Yelp reviews dataset (Example 2-11). Figure 2-10 looks very different from Figure 2-9. The average star rating is discretized in increments of half-stars ranging from 1 to 5. High review counts (roughly >2,500 reviews) do correlate with higher average star ratings, but the relationship is far from linear. There is no clear way to draw a line to predict the aver‐ age star rating based on either input. Essentially, the plot shows that review count and its logarithm are both bad linear predictors of average star rating.\n\nExample 2-11. Visualizing the correlation between input and output in Yelp business review prediction\n\n>>> fig, (ax1, ax2) = plt.subplots(2,1) >>> ax1.scatter(biz_df['review_count'], biz_df['stars']) >>> ax1.tick_params(labelsize=14) >>> ax1.set_xlabel('Review Count', fontsize=14) >>> ax1.set_ylabel('Average Star Rating', fontsize=14)\n\n>>> ax2.scatter(biz_df['log_review_count'], biz_df['stars']) >>> ax2.tick_params(labelsize=14) >>> ax2.set_xlabel('Log of Review Count', fontsize=14) >>> ax2.set_ylabel('Average Star Rating', fontsize=14)\n\nThe Importance of Data Visualization\n\nThe comparison of the effect of the log transform on two different datasets illustrates the importance of visualizing the data. Here, we intentionally kept the input and target variables simple so that we can easily visualize the relationship between them. Plots like those in Figure 2-10 immediately reveal that the chosen model (linear) cannot possibly represent the relationship between the chosen input and target. On the other hand, one could convincingly model the distribution of review count given the average star rating. When building models, it is a good idea to visually inspect the rela‐ tionships between input and output, and between different input features.\n\n22\n\n|\n\nChapter 2: Fancy Tricks with Simple Numbers",
      "content_length": 1887,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 39,
      "content": "Figure 2-10. Scatter plots of review counts (input) versus average star rating (target) in the Yelp reviews dataset—the top panel plots the original review count, and the bottom panel plots the review count after log transformation\n\nPower Transforms: Generalization of the Log Transform The log transform is a specific example of a family of transformations known as power transforms. In statistical terms, these are variance-stabilizing transformations. To understand why variance stabilization is good, consider the Poisson distribution. This is a heavy-tailed distribution with a variance that is equal to its mean: hence, the larger its center of mass, the larger its variance, and the heavier the tail. Power trans‐ forms change the distribution of the variable so that the variance is no longer depen‐ dent on the mean. For example, suppose a random variable X has the Poisson distribution. If we transform X by taking its square root, the variance of X˜ = X is roughly constant, instead of being equal to the mean.\n\nLog Transformation\n\n|\n\n23",
      "content_length": 1048,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 40,
      "content": "Figure 2-11 illustrates λ, which represents the mean of the distribution. As λ increa‐ ses, not only does the mode of the distribution shift to the right, but the mass spreads out and the variance becomes larger.\n\nFigure 2-11. A rough illustration of the Poisson distribution, an example distribution where the variance increases along with the mean\n\nA simple generalization of both the square root transform and the log transform is known as the Box-Cox transform:\n\nx˜ ={ x λ – 1\n\nλ\n\nln(x)\n\nif λ ≠ 0,\n\nif λ = 0.\n\nFigure 2-12 shows the Box-Cox transform for λ = 0 (the log transform), λ = 0.25, λ = 0.5 (a scaled and shifted version of the square root transform), λ = 0.75, and λ = 1.5. Setting λ to be less than 1 compresses the higher values, and setting λ higher than 1 has the opposite effect.\n\n24\n\n|\n\nChapter 2: Fancy Tricks with Simple Numbers",
      "content_length": 849,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 41,
      "content": "Figure 2-12. Box-Cox transforms for different values of λ\n\nThe Box-Cox formulation only works when the data is positive. For nonpositive data, one could shift the values by adding a fixed constant. When applying the Box-Cox transformation or a more general power transform, we have to determine a value for the parameter λ. This may be done via maximum likelihood (finding the λ that maxi‐ mizes the Gaussian likelihood of the resulting transformed signal) or Bayesian meth‐ ods. A full treatment of the usage of Box-Cox and general power transforms is outside the scope of this book. Interested readers may find more information on power transforms in Econometric Methods by Johnston and DiNardo (1997). Fortu‐ nately, SciPy’s stats package contains an implementation of the Box-Cox transfor‐ mation that includes finding the optimal transform parameter. Example 2-12 demonstrates its use on the Yelp reviews dataset.\n\nExample 2-12. Box-Cox transformation of Yelp business review counts\n\n>>> from scipy import stats\n\n# Continuing from the previous example, assume biz_df contains # the Yelp business reviews data. # The Box-Cox transform assumes that input data is positive. # Check the min to make sure. >>> biz_df['review_count'].min() 3\n\n# Setting input parameter lmbda to 0 gives us the log transform (without # constant offset)\n\nLog Transformation\n\n|\n\n25",
      "content_length": 1360,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 42,
      "content": ">>> rc_log = stats.boxcox(biz_df['review_count'], lmbda=0) # By default, the scipy implementation of Box-Cox transform finds the lambda # parameter that will make the output the closest to a normal distribution >>> rc_bc, bc_params = stats.boxcox(biz_df['review_count']) >>> bc_params -0.4106510862321085\n\nFigure 2-13 provides a visual comparison of the distributions of the original and transformed counts (see Example 2-13).\n\nExample 2-13. Visualizing the histograms of original, log transformed, and Box-Cox transformed counts\n\n>>> fig, (ax1, ax2, ax3) = plt.subplots(3,1) # original review count histogram >>> biz_df['review_count'].hist(ax=ax1, bins=100) >>> ax1.set_yscale('log') >>> ax1.tick_params(labelsize=14) >>> ax1.set_title('Review Counts Histogram', fontsize=14) >>> ax1.set_xlabel('') >>> ax1.set_ylabel('Occurrence', fontsize=14)\n\n# review count after log transform >>> biz_df['rc_log'].hist(ax=ax2, bins=100) >>> ax2.set_yscale('log') >>> ax2.tick_params(labelsize=14) >>> ax2.set_title('Log Transformed Counts Histogram', fontsize=14) >>> ax2.set_xlabel('') >>> ax2.set_ylabel('Occurrence', fontsize=14)\n\n# review count after optimal Box-Cox transform >>> biz_df['rc_bc'].hist(ax=ax3, bins=100) >>> ax3.set_yscale('log') >>> ax3.tick_params(labelsize=14) >>> ax3.set_title('Box-Cox Transformed Counts Histogram', fontsize=14) >>> ax3.set_xlabel('') >>> ax3.set_ylabel('Occurrence', fontsize=14)\n\n26\n\n|\n\nChapter 2: Fancy Tricks with Simple Numbers",
      "content_length": 1465,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 43,
      "content": "Figure 2-13. Box-Cox transformation of Yelp business review counts (bottom), com‐ pared to original (top) and log transformed (middle) histograms\n\nLog Transformation\n\n|\n\n27",
      "content_length": 172,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 44,
      "content": "A probability plot, or probplot, is an easy way to visually compare an empirical distri‐ bution of data against a theoretical distribution. This is essentially a scatter plot of observed versus theoretical quantiles. Figure 2-14 shows the probplots of original and transformed Yelp review counts data against the normal distribution (see Example 2-14). Since the observed data is strictly positive and the Gaussian can be negative, the quantiles could never match up on the negative end. Thus, our focus is on the positive side. On this front, the original counts are obviously much more heavy-tailed than a normal distribution. (The ordered values go up to 4,000, whereas the theoretical quantiles only stretch to 4.) Both the plain log transform and the opti‐ mal Box-Cox transform bring the positive tail closer to normal. The optimal Box-Cox transform deflates the tail more than the log transform, as is evident from the fact that the tail flattens out under the red diagonal equivalence line.\n\nExample 2-14. Probability plots of original and transformed counts against the normal distribution\n\n>>> fig2, (ax1, ax2, ax3) = plt.subplots(3,1) >>> prob1 = stats.probplot(biz_df['review_count'], dist=stats.norm, plot=ax1) >>> ax1.set_xlabel('') >>> ax1.set_title('Probplot against normal distribution') >>> prob2 = stats.probplot(biz_df['rc_log'], dist=stats.norm, plot=ax2) >>> ax2.set_xlabel('') >>> ax2.set_title('Probplot after log transform') >>> prob3 = stats.probplot(biz_df['rc_bc'], dist=stats.norm, plot=ax3) >>> ax3.set_xlabel('Theoretical quantiles') >>> ax3.set_title('Probplot after Box-Cox transform')\n\n28\n\n|\n\nChapter 2: Fancy Tricks with Simple Numbers",
      "content_length": 1670,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 45,
      "content": "Figure 2-14. Comparing the distribution of raw and transformed review counts against the normal distribution\n\nFeature Scaling or Normalization Some features, such as latitude or longitude, are bounded in value. Other numeric features, such as counts, may increase without bound. Models that are smooth func‐ tions of the input, such as linear regression, logistic regression, or anything that involves a matrix, are affected by the scale of the input. Tree-based models, on the\n\nFeature Scaling or Normalization\n\n|\n\n29",
      "content_length": 518,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 46,
      "content": "other hand, couldn’t care less. If your model is sensitive to the scale of input features, feature scaling could help. As the name suggests, feature scaling changes the scale of the feature. Sometimes people also call it feature normalization. Feature scaling is usually done individually to each feature. Next, we will discuss several types of com‐ mon scaling operations, each resulting in a different distribution of feature values.\n\nMin-Max Scaling Let x be an individual feature value (i.e., a value of the feature in some data point), and min(x) and max(x), respectively, be the minimum and maximum values of this feature over the entire dataset. Min-max scaling squeezes (or stretches) all feature values to be within the range of [0, 1]. Figure 2-15 demonstrates this concept. The formula for min-max scaling is:\n\nx˜ =\n\nx – min(x) max(x) – min(x)\n\nFigure 2-15. Illustration of min-max scaling\n\n30\n\n|\n\nChapter 2: Fancy Tricks with Simple Numbers",
      "content_length": 952,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 47,
      "content": "Standardization (Variance Scaling) Feature standardization is defined as:\n\nx˜ =\n\nx – mean(x) sqrt(var(x))\n\nIt subtracts off the mean of the feature (over all data points) and divides by the var‐ iance. Hence, it can also be called variance scaling. The resulting scaled feature has a mean of 0 and a variance of 1. If the original feature has a Gaussian distribution, then the scaled feature does too. Figure 2-16 is an illustration of standardization.\n\nFigure 2-16. Illustration of feature standardization\n\nFeature Scaling or Normalization\n\n|\n\n31",
      "content_length": 547,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 48,
      "content": "Don’t “Center” Sparse Data!\n\nUse caution when performing min-max scaling and standardiza‐ tion on sparse features. Both subtract a quantity from the original feature value. For min-max scaling, the shift is the minimum over all values of the current feature; for standardization, it is the mean. If the shift is not zero, then these two transforms can turn a sparse feature vector where most values are zero into a dense one. This in turn could create a huge computational burden for the classifier, depending on how it is implemented (not to mention that it would be horrendous if the representation now included every word that didn’t appear in a document!). Bag-of-words is a sparse representa‐ tion, and most classification libraries optimize for sparse inputs.\n\nℓ2 Normalization This technique normalizes (divides) the original feature value by what’s known as the ℓ2 norm, also known as the Euclidean norm. It’s defined as follows:\n\nx˜ =\n\nx ∥ x ∥ 2\n\nThe ℓ2 norm measures the length of the vector in coordinate space. The definition can be derived from the well-known Pythagorean theorem that gives us the length of the hypotenuse of a right triangle given the lengths of the sides:\n\n∥ x ∥ 2 = x1\n\n2 + x2\n\n2 2 + … + xm\n\nThe ℓ2 norm sums the squares of the values of the features across data points, then takes the square root. After ℓ2 normalization, the feature column has norm 1. This is also sometimes called ℓ2 scaling. (Loosely speaking, scaling means multiplying by a constant, whereas normalization could involve a number of operations.) Figure 2-17 illustrates ℓ2 normalization.\n\n32\n\n|\n\nChapter 2: Fancy Tricks with Simple Numbers",
      "content_length": 1643,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 49,
      "content": "Figure 2-17. Illustration of ℓ2 feature normalization\n\nData Space Versus Feature Space\n\nNote that the illustration in Figure 2-17 is in data space, not feature space. One can also do ℓ2 normalization for the data point instead of the feature, which will result in data vectors with unit norm (norm of 1). See the discussion in “Bag-of-Words” on page 42 about the complementary nature of data vectors and feature vectors.\n\nNo matter the scaling method, feature scaling always divides the feature by a constant (known as the normalization constant). Therefore, it does not change the shape of the single-feature distribution. We’ll illustrate this with the online news article token counts (see Example 2-15).\n\nExample 2-15. Feature scaling example\n\n>>> import pandas as pd >>> import sklearn.preprocessing as preproc\n\n# Load the Online News Popularity dataset >>> df = pd.read_csv('OnlineNewsPopularity.csv', delimiter=', ')\n\n# Look at the original data - the number of words in an article >>> df['n_tokens_content'].as_matrix() array([ 219., 255., 211., ..., 442., 682., 157.])\n\n# Min-max scaling >>> df['minmax'] = preproc.minmax_scale(df[['n_tokens_content']])\n\nFeature Scaling or Normalization\n\n|\n\n33",
      "content_length": 1203,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 50,
      "content": ">>> df['minmax'].as_matrix() array([ 0.02584376, 0.03009205, 0.02489969, ..., 0.05215955, 0.08048147, 0.01852726])\n\n# Standardization - note that by definition, some outputs will be negative >>> df['standardized'] = preproc.StandardScaler().fit_transform(df[['n_tokens_content']]) >>> df['standardized'].as_matrix() array([-0.69521045, -0.61879381, -0.71219192, ..., -0.2218518 , 0.28759248, -0.82681689])\n\n# L2-normalization >>> df['l2_normalized'] = preproc.normalize(df[['n_tokens_content']], axis=0) >>> df['l2_normalized'].as_matrix() array([ 0.00152439, 0.00177498, 0.00146871, ..., 0.00307663, 0.0047472 , 0.00109283])\n\nWe can also visualize the distribution of data with different feature scaling methods (Figure 2-18). As Example 2-16 shows, unlike the log transform, feature scaling doesn’t change the shape of the distribution; only the scale of the data changes.\n\nExample 2-16. Plotting the histograms of original and scaled data\n\n>>> fig, (ax1, ax2, ax3, ax4) = plt.subplots(4,1) >>> fig.tight_layout() >>> df['n_tokens_content'].hist(ax=ax1, bins=100) >>> ax1.tick_params(labelsize=14) >>> ax1.set_xlabel('Article word count', fontsize=14) >>> ax1.set_ylabel('Number of articles', fontsize=14)\n\n>>> df['minmax'].hist(ax=ax2, bins=100) >>> ax2.tick_params(labelsize=14) >>> ax2.set_xlabel('Min-max scaled word count', fontsize=14) >>> ax2.set_ylabel('Number of articles', fontsize=14)\n\n>>> df['standardized'].hist(ax=ax3, bins=100) >>> ax3.tick_params(labelsize=14) >>> ax3.set_xlabel('Standardized word count', fontsize=14) >>> ax3.set_ylabel('Number of articles', fontsize=14)\n\n>>> df['l2_normalized'].hist(ax=ax4, bins=100) >>> ax4.tick_params(labelsize=14) >>> ax4.set_xlabel('L2-normalized word count', fontsize=14) >>> ax4.set_ylabel('Number of articles', fontsize=14)\n\n34\n\n|\n\nChapter 2: Fancy Tricks with Simple Numbers",
      "content_length": 1839,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 51,
      "content": "Figure 2-18. Original and scaled news article word counts—note that only the scale of the x-axis changes; the shape of the distribution stays the same with feature scaling\n\nFeature scaling is useful in situations where a set of input features differs wildly in scale. For instance, the number of daily visitors to a popular ecommerce site might be a hundred thousand, while the actual number of sales might be in the thousands. If both of those features are thrown into a model, then the model will need to balance its scale while figuring out what to do. Drastically varying scale in input features can lead to numeric stability issues for the model training algorithm. In those situations, it’s a good idea to standardize the features. Chapter 4 goes into detail about feature scaling in the context of handling natural text, including usage examples.\n\nInteraction Features A simple pairwise interaction feature is the product of two features. The analogy is the logical AND. It expresses the outcome in terms of pairs of conditions: “the pur‐ chase is coming from zip code 98121” AND “the user’s age is between 18 and 35.” Decision tree–based models get this for free, but generalized linear models often find interaction features very helpful.\n\nA simple linear model uses a linear combination of the individual input features x1, x2, ... xn to predict the outcome y:\n\nInteraction Features\n\n|\n\n35",
      "content_length": 1399,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 52,
      "content": "y = w1x1 + w2x2 + ... + wnxn\n\nAn easy way to extend the linear model is to include combinations of pairs of input features, like so:\n\ny = w1x1 + w2x2 + ... + wnxn + w1,1x1x1 + w1,2x1x2 + w1,3x1x3 + ...\n\nThis allows us to capture interactions between features, and hence these pairs are called interaction features. If x1 and x2 are binary, then their product x1x2 is the logical function x1 AND x2. Suppose the problem is to predict a customer’s preference based on their profile information. In our example, instead of making predictions based solely on the age or location of the user, interaction features allow the model to make predictions based on the user being of a certain age AND at a particular location.\n\nIn Example 2-17, we use pairwise interaction features from the UCI Online News Popularity dataset to predict the number of shares for each news article. As the results show, interaction features result in some lift in accuracy above singleton fea‐ tures. Both perform better than Example 2-9, which used as a single predictor the number of words in the body of the article (with or without a log transform).\n\nExample 2-17. Example of interaction features in prediction\n\n>>> from sklearn import linear_model >>> from sklearn.model_selection import train_test_split >>> import sklearn.preprocessing as preproc\n\n# Assume df is a Pandas DataFrame containing the UCI Online News Popularity dataset >>> df.columns Index(['url', 'timedelta', 'n_tokens_title', 'n_tokens_content', 'n_unique_tokens', 'n_non_stop_words', 'n_non_stop_unique_tokens', 'num_hrefs', 'num_self_hrefs', 'num_imgs', 'num_videos', 'average_token_length', 'num_keywords', 'data_channel_is_lifestyle', 'data_channel_is_entertainment', 'data_channel_is_bus', 'data_channel_is_socmed', 'data_channel_is_tech', 'data_channel_is_world', 'kw_min_min', 'kw_max_min', 'kw_avg_min', 'kw_min_max', 'kw_max_max', 'kw_avg_max', 'kw_min_avg', 'kw_max_avg', 'kw_avg_avg', 'self_reference_min_shares', 'self_reference_max_shares', 'self_reference_avg_sharess', 'weekday_is_monday', 'weekday_is_tuesday', 'weekday_is_wednesday', 'weekday_is_thursday', 'weekday_is_friday', 'weekday_is_saturday', 'weekday_is_sunday', 'is_weekend', 'LDA_00', 'LDA_01', 'LDA_02', 'LDA_03', 'LDA_04', 'global_subjectivity', 'global_sentiment_polarity', 'global_rate_positive_words', 'global_rate_negative_words', 'rate_positive_words', 'rate_negative_words', 'avg_positive_polarity', 'min_positive_polarity', 'max_positive_polarity', 'avg_negative_polarity', 'min_negative_polarity', 'max_negative_polarity', 'title_subjectivity', 'title_sentiment_polarity', 'abs_title_subjectivity',\n\n36\n\n|\n\nChapter 2: Fancy Tricks with Simple Numbers",
      "content_length": 2682,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 53,
      "content": "'abs_title_sentiment_polarity', 'shares'], dtype='object')\n\n# Select the content-based features as singleton features in the model, # skipping over the derived features >>> features = ['n_tokens_title', 'n_tokens_content', ... 'n_unique_tokens', 'n_non_stop_words', 'n_non_stop_unique_tokens', ... 'num_hrefs', 'num_self_hrefs', 'num_imgs', 'num_videos', ... 'average_token_length', 'num_keywords', 'data_channel_is_lifestyle', ... 'data_channel_is_entertainment', 'data_channel_is_bus', ... 'data_channel_is_socmed', 'data_channel_is_tech', ... 'data_channel_is_world']\n\n>>> X = df[features] >>> y = df[['shares']]\n\n# Create pairwise interaction features, skipping the constant bias term >>> X2 = preproc.PolynomialFeatures(include_bias=False).fit_transform(X) >>> X2.shape (39644, 170)\n\n# Create train/test sets for both feature sets >>> X1_train, X1_test, X2_train, X2_test, y_train, y_test = \\ ... train_test_split(X, X2, y, test_size=0.3, random_state=123)\n\n>>> def evaluate_feature(X_train, X_test, y_train, y_test): ... \"\"\"Fit a linear regression model on the training set and ... score on the test set\"\"\" ... model = linear_model.LinearRegression().fit(X_train, y_train) ... r_score = model.score(X_test, y_test) ... return (model, r_score)\n\n# Train models and compare score on the two feature sets >>> (m1, r1) = evaluate_feature(X1_train, X1_test, y_train, y_test) >>> (m2, r2) = evaluate_feature(X2_train, X2_test, y_train, y_test) >>> print(\"R-squared score with singleton features: %0.5f\" % r1) >>> print(\"R-squared score with pairwise features: %0.10f\" % r2) R-squared score with singleton features: 0.00924 R-squared score with pairwise features: 0.0113276523\n\nInteraction features are very simple to formulate, but they are expensive to use. The training and scoring time of a linear model with pairwise interaction features would go from O(n) to O(n2), where n is the number of singleton features.\n\nThere are a few ways around the computational expense of higher-order interaction features. One could perform feature selection on top of all of the interaction features. Alternatively, one could more carefully craft a smaller number of complex features.\n\nBoth strategies have their advantages and disadvantages. Feature selection employs computational means to select the best features for a problem. (This technique is not\n\nInteraction Features\n\n|\n\n37",
      "content_length": 2369,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 54,
      "content": "limited to interaction features.) However, some feature selection techniques still require training multiple models with a large number of features.\n\nHandcrafted complex features can be expressive enough that only a small number of them are needed, which reduces the training time of the model—but the features themselves may be expensive to compute, which increases the computational cost of the model scoring stage. Good examples of handcrafted (or machine-learned) com‐ plex features may be found in Chapter 8. Let’s now look at some feature selection techniques.\n\nFeature Selection Feature selection techniques prune away nonuseful features in order to reduce the complexity of the resulting model. The end goal is a parsimonious model that is quicker to compute, with little or no degradation in predictive accuracy. In order to arrive at such a model, some feature selection techniques require training more than one candidate model. In other words, feature selection is not about reducing training time—in fact, some techniques increase overall training time—but about reducing model scoring time.\n\nRoughly speaking, feature selection techniques fall into three classes:\n\nFiltering\n\nFiltering techniques preprocess features to remove ones that are unlikely to be useful for the model. For example, one could compute the correlation or mutual information between each feature and the response variable, and filter out the features that fall below a threshold. Chapter 3 discusses examples of these techni‐ ques for text features. Filtering techniques are much cheaper than the wrapper techniques described next, but they do not take into account the model being employed. Hence, they may not be able to select the right features for the model. It is best to do prefiltering conservatively, so as not to inadvertently eliminate useful features before they even make it to the model training step.\n\nWrapper methods\n\nThese techniques are expensive, but they allow you to try out subsets of features, which means you won’t accidentally prune away features that are uninformative by themselves but useful when taken in combination. The wrapper method treats the model as a black box that provides a quality score of a proposed subset for features. There is a separate method that iteratively refines the subset.\n\nEmbedded methods\n\nThese methods perform feature selection as part of the model training process. For example, a decision tree inherently performs feature selection because it selects one feature on which to split the tree at each training step. Another exam‐ ple is the ℓ1 regularizer, which can be added to the training objective of any linear\n\n38\n\n|\n\nChapter 2: Fancy Tricks with Simple Numbers",
      "content_length": 2710,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 55,
      "content": "model. The ℓ1 regularizer encourages models that use a few features as opposed to a lot of features, so it’s also known as a sparsity constraint on the model. Embedded methods incorporate feature selection as part of the model training process. They are not as powerful as wrapper methods, but they are nowhere near as expensive. Compared to filtering, embedded methods select features that are specific to the model. In this sense, embedded methods strike a balance between computational expense and quality of results.\n\nA full treatment of feature selection is outside the scope of this book. Interested read‐ ers may refer to the survey paper by Guyon and Elisseeff (2003).\n\nSummary This chapter discussed a number of common numeric feature engineering techni‐ ques, such as quantization, scaling (a.k.a. normalization), log transforms (a type of power transform), and interaction features, and gave a brief summary of feature selection techniques, necessary for handling large quantities of interaction features. In statistical machine learning, all data eventually boils down to numeric features. Therefore, all roads lead to some kind of numeric feature engineering technique at the end. Keep these tools handy for the end game of feature engineering!\n\nBibliography Bertin-Mahieux, Thierry, Daniel P.W. Ellis, Brian Whitman, and Paul Lamere. “The Million Song Dataset.” Proceedings of the 12th International Society for Music Infor‐ mation Retrieval Conference (2011): 591–596.\n\nFernandes, K., P. Vinagre, and P. Cortez. “A Proactive Intelligent Decision Support System for Predicting the Popularity of Online News.” Proceedings of the 17th Portu‐ guese Conference on Artificial Intelligence (2015): 535–546.\n\nGuyon, Isabell, and André Elisseeff. “An Introduction to Variable and Feature Selec‐ tion.” Journal of Machine Learning Research Special Issue on Variable and Feature Selection 3 (2003): 1157–1182.\n\nJohnston, Jack, and John DiNardo. Econometric Methods. 4th ed. New York: McGraw Hill, 1997.\n\nSummary\n\n|\n\n39",
      "content_length": 2022,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 57,
      "content": "CHAPTER 3 Text Data: Flattening, Filtering, and Chunking\n\nWhat would you do if you were designing an algorithm to analyze the following para‐ graph of text?\n\nEmma knocked on the door. No answer. She knocked again and waited. There was a large maple tree next to the house. Emma looked up the tree and saw a giant raven perched at the treetop. Under the afternoon sun, the raven gleamed magnificently. Its beak was hard and pointed, its claws sharp and strong. It looked regal and imposing. It reigned the tree it stood on. The raven was looking straight at Emma with its beady black eyes. Emma felt slightly intimidated. She took a step back from the door and ten‐ tatively said, “Hello?”\n\nThe paragraph contains a lot of information. We know that it involves someone named Emma and a raven. There is a house and a tree, and Emma is trying to get into the house but sees the raven instead. The raven is magnificent and has noticed Emma, who is a little scared but is making an attempt at communication.\n\nSo, which parts of this trove of information are salient features that we should extract? To start with, it seems like a good idea to extract the names of the main char‐ acters, Emma and the raven. Next, it might also be good to note the setting of a house, a door, and a tree. And what about the descriptions of the raven? What about Emma’s actions—knocking on the door, taking a step back, and saying hello?\n\nThis chapter introduces the basics of feature engineering for text. We start out with bag-of-words, which is the simplest representation based on word count statistics. A very much related transformation is tf-idf, which is essentially a feature scaling tech‐ nique. It is pulled out into its own chapter (the next one) for a full discussion. The current chapter first talks about text extraction features, then delves into how to filter and clean those features.\n\n41",
      "content_length": 1882,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 58,
      "content": "Bag-of-X: Turning Natural Text into Flat Vectors Whether constructing machine learning models or engineering features, it’s nice when the result is simple and interpretable. Simple things are easy to try, and inter‐ pretable features and models are easier to debug than complex ones. Simple and interpretable features do not always lead to the most accurate model, but it’s a good idea to start simple and only add complexity when absolutely necessary.\n\nFor text data, we can start with a list of word count statistics called a bag-of-words. A list of word counts makes no special effort to find the interesting entities, such as Emma or the raven. But those two words are repeatedly mentioned in our sample paragraph, and they have a higher count than a random word like “hello.” For simple tasks such as classifying a document, word count statistics often suffice. This techni‐ que can also be used in information retrieval, where the goal is to retrieve the set of documents that are relevant to an input text query. Both tasks are well served by word-level features because the presence or absence of certain words is a great indica‐ tor of the topic content of the document.\n\nBag-of-Words In bag-of-words (BoW) featurization, a text document is converted into a vector of counts. (A vector is just a collection of n numbers.) The vector contains an entry for every possible word in the vocabulary. If the word—say, “aardvark”—appears three times in the document, then the feature vector has a count of 3 in the position corre‐ sponding to that word. If a word in the vocabulary doesn’t appear in the document, then it gets a count of 0. For example, the text “it is a puppy and it is extremely cute” has the BoW representation shown in Figure 3-1.\n\nFigure 3-1. Turning raw text into a bag-of-words representation\n\n42\n\n|\n\nChapter 3: Text Data: Flattening, Filtering, and Chunking",
      "content_length": 1883,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 59,
      "content": "Bag-of-words converts a text document into a flat vector. It is “flat” because it doesn’t contain any of the original textual structures. The original text is a sequence of words. But a bag-of-words has no sequence; it just remembers how many times each word appears in the text. Thus, as Figure 3-2 demonstrates, the ordering of words in the vector is not important, as long as it is consistent for all documents in the dataset. Neither does bag-of-words represent any concept of word hierarchy. For example, the concept of “animal” includes “dog,” “cat,” “raven,” etc. But in a bag-of-words rep‐ resentation, these words are all equal elements of the vector.\n\nFigure 3-2. Two equivalent BoW vectors\n\nWhat is important here is the geometry of data in feature space. In a bag-of-words vector, each word becomes a dimension of the vector. If there are n words in the vocabulary, then a document becomes a point1 in n-dimensional space. It is difficult to visualize the geometry of anything beyond two or three dimensions, so we will have to use our imagination. Figure 3-3 shows what our example sentence looks like in the two-dimensional feature space corresponding to the words “puppy” and “cute.”\n\n1 Sometimes people use the term “document vector.” The vector extends from the origin and ends at the speci‐\n\nfied point. For our purposes, “vector” and “point” are the same thing.\n\nBag-of-X: Turning Natural Text into Flat Vectors\n\n|\n\n43",
      "content_length": 1437,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 60,
      "content": "Figure 3-3. Illustration of a sample text document in a 2D feature space\n\nFigure 3-4 shows three sentences in a 3D space corresponding to the words “puppy,” “extremely,” and “cute.”\n\nFigure 3-4. Three sentences in 3D feature space\n\nThese figures both depict data vectors in feature space. The axes denote individual words, which are features in the bag-of-words representation, and the points in space denote data points (text documents). Sometimes it is also informative to look at fea‐ ture vectors in data space. A feature vector contains the value of the feature in each data point. The axes denote individual data points, and the points denote feature vec‐ tors. Figure 3-5 shows an example. With bag-of-words featurization for text docu‐ ments, a feature is a word, and a feature vector contains the counts of this word in each document. In this way, a word is represented as a “bag-of-documents.” As we shall see in Chapter 4, these bag-of-documents vectors come from the matrix trans‐ pose of the bag-of-words vectors.\n\n44\n\n|\n\nChapter 3: Text Data: Flattening, Filtering, and Chunking",
      "content_length": 1092,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 61,
      "content": "Figure 3-5. Word vectors in document space\n\nBag-of-words is not perfect. Breaking down a sentence into single words can destroy the semantic meaning. For instance, “not bad” semantically means “decent” or even “good” (especially if you’re British). But “not” and “bad” constitute a floating nega‐ tion plus a negative sentiment. “toy dog” and “dog toy” could be very different things (unless it’s a dog toy of a toy dog), and the meaning is lost with the singleton words “toy” and “dog.” It’s easy to come up with many such examples. Bag-of-n-Grams, which we discuss next, alleviates some of the issue but is not a fundamental fix. It’s good to keep in mind that bag-of-words is a simple and useful heuristic, but it is far from a correct semantic understanding of text.\n\nBag-of-n-Grams Bag-of-n-Grams, or bag-of-n-grams, is a natural extension of bag-of-words. An n-gram is a sequence of n tokens. A word is essentially a 1-gram, also known as a unigram. After tokenization, the counting mechanism can collate individual tokens into word counts, or count overlapping sequences as n-grams. For example, the sen‐ tence “Emma knocked on the door” generates the n-grams “Emma knocked,” “knocked on,” “on the,” and “the door.”\n\nn-grams retain more of the original sequence structure of the text, and therefore the bag-of-n-grams representation can be more informative. However, this comes at a cost. Theoretically, with k unique words, there could be k2 unique 2-grams (also called bigrams). In practice, there are not nearly so many, because not every word can follow every other word. Nevertheless, there are usually a lot more distinct n-grams (n > 1) than words. This means that bag-of-n-grams is a much bigger and sparser fea‐ ture space. It also means that n-grams are more expensive to compute, store, and model. The larger n is, the richer the information, and the greater the cost.\n\nBag-of-X: Turning Natural Text into Flat Vectors\n\n|\n\n45",
      "content_length": 1943,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 62,
      "content": "To illustrate how the number of n-grams grows with increasing n (see Figure 3-6), let’s compute n-grams on the Yelp reviews dataset. In Example 3-1, we compute the n-grams of the first 10,000 reviews using Pandas and the CountVectorizer trans‐ former in scikit-learn.\n\nExample 3-1. Computing n-grams\n\n>>> import pandas >>> import json >>> from sklearn.feature_extraction.text import CountVectorizer\n\n# Load the first 10,000 reviews >>> f = open('data/yelp/v6/yelp_academic_dataset_review.json') >>> js = [] >>> for i in range(10000): ... js.append(json.loads(f.readline())) >>> f.close() >>> review_df = pd.DataFrame(js)\n\n# Create feature transformers for unigrams, bigrams, and trigrams. # The default ignores single-character words, which is useful in practice because # it trims uninformative words, but we explicitly include them in this example for # illustration purposes. >>> bow_converter = CountVectorizer(token_pattern='(?u)\\\\b\\\\w+\\\\b') >>> bigram_converter = CountVectorizer(ngram_range=(2,2), ... token_pattern='(?u)\\\\b\\\\w+\\\\b') >>> trigram_converter = CountVectorizer(ngram_range=(3,3), ... token_pattern='(?u)\\\\b\\\\w+\\\\b')\n\n# Fit the transformers and look at vocabulary size >>> bow_converter.fit(review_df['text']) >>> words = bow_converter.get_feature_names() >>> bigram_converter.fit(review_df['text']) >>> bigrams = bigram_converter.get_feature_names() >>> trigram_converter.fit(review_df['text']) >>> trigrams = trigram_converter.get_feature_names() >>> print (len(words), len(bigrams), len(trigrams)) 26047 346301 847545\n\n# Sneak a peek at the n-grams themselves >>> words[:10] ['0', '00', '000', '0002', '00am', '00ish', '00pm', '01', '01am', '02']\n\n>>> bigrams[-10:] ['zucchinis at', 'zucchinis took', 'zucchinis we', 'zuma over', 'zuppa di', 'zuppa toscana', 'zuppe di',\n\n46\n\n|\n\nChapter 3: Text Data: Flattening, Filtering, and Chunking",
      "content_length": 1858,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 63,
      "content": "'zurich and', 'zz top', 'à la']\n\n>>> trigrams[:10] ['0 10 definitely', '0 2 also', '0 25 per', '0 3 miles', '0 30 a', '0 30 everything', '0 30 lb', '0 35 tip', '0 5 curry', '0 5 pork']\n\nFigure 3-6. Number of unique n-grams in the first 10,000 reviews of the Yelp dataset\n\nFiltering for Cleaner Features With words, how do we cleanly separate the signal from the noise? Through filtering, techniques that use raw tokenization and counting to generate lists of simple words or n-grams become more usable. Phrase detection, which we will discuss next, can be seen as a particular bigram filter. Here are a few more ways to perform filtering.\n\nFiltering for Cleaner Features\n\n|\n\n47",
      "content_length": 677,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 64,
      "content": "Stopwords Classification and retrieval do not usually require an in-depth understanding of the text. For instance, in the sentence “Emma knocked on the door,” the words “on” and “the” don’t change the fact that this sentence is about a person and a door. For coarse-grained tasks such as classification, the pronouns, articles, and prepositions may not add much value. The case may be very different in sentiment analysis, which requires a fine-grained understanding of semantics.\n\nThe popular Python NLP package NLTK contains a linguist-defined stopword list for many languages. (You will need to install NLTK and run nltk.download() to get all the goodies.) Various stopword lists can also be found on the web. For instance, here are some sample words from the English stopword list:\n\na, about, above, am, an, been, didn't, couldn't, i'd, i'll, itself, let's, myself, our, they, through, when's, whom, ...\n\nNote that the list contains apostrophes, and the words are uncapitalized. In order to use it as is, the tokenization process must not eat up apostrophes, and the words need to be converted to lowercase.\n\nFrequency-Based Filtering Stopword lists are a way of weeding out common words that make for vacuous fea‐ tures. There are other, more statistical ways of getting at the concept of “common words.” In collocation extraction, we see methods that depend on manual definitions, and those that use statistics. The same idea applies to word filtering. We can use fre‐ quency statistics here as well.\n\nFrequent words\n\nFrequency statistics are great for filtering out corpus-specific common words as well as general-purpose stopwords. For instance, the phrase “New York Times” and each of the individual words in it appear frequently in the New York Times Annotated Corpus dataset. Similarly, the word “house” appears often in the phrase “House of Commons” in the Hansard corpus of Canadian parliament debates, a dataset that is popularly used for statistical machine translation because it contains both an English and a French version of all documents. These words are meaningful in general, but not within those particular corpora. A typical stopword list will catch the general stopwords, but not corpus-specific ones.\n\nLooking at the most frequent words can reveal parsing problems and highlight nor‐ mally useful words that happen to appear too many times in the corpus. For example, Table 3-1 lists the 40 most frequent words in the Yelp reviews dataset. Here, fre‐ quency is based on the number of documents (reviews) they appear in, not their count within a document. As we can see, the list includes many stopwords. It also\n\n48\n\n|\n\nChapter 3: Text Data: Flattening, Filtering, and Chunking",
      "content_length": 2704,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 65,
      "content": "contains some surprises. “s” and “t” are on the list because we used the apostrophe as a tokenization delimiter, and words such as “Mary’s” or “didn’t” got parsed as “Mary s” and “didn t.” Furthermore, the words “good,” “food,” and “great” each appear in around a third of the reviews, but we might want to keep them around because they are very useful for tasks such as sentiment analysis or business categorization.\n\nTable 3-1. Most frequent words in the Yelp reviews dataset\n\nRank Word Document frequency Rank Word Document frequency 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\n\nthe and a i to it of for is in was this but my that with on they you have\n\n1416058 1381324 1263126 1230214 1196238 1027835 1025638 993430 988547 961518 929703 844824 822313 786595 777045 775044 735419 720994 701015 692749\n\n21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40\n\nt not s had so place good at are food be we great were there here all if very out\n\n684049 649824 626764 620284 608061 601918 598393 596317 585548 562332 543588 537133 520634 516685 510897 481542 478490 475175 460796 460452\n\nIn practice, it helps to combine frequency-based filtering with a stopword list. There is also the tricky question of where to place the cutoff. Unfortunately there is no uni‐ versal answer. Most of the time the cutoff needs to be determined manually, and may need to be reexamined when the dataset changes.\n\nRare words\n\nDepending on the task, one might also need to filter out rare words. These might be truly obscure words, or misspellings of common words. To a statistical model, a word that appears in only one or two documents is more like noise than useful informa‐ tion. For example, suppose the task is to categorize businesses based on their Yelp reviews, and a single review contains the word “gobbledygook.” How would one tell, based on this one word, whether the business is a restaurant, a beauty salon, or a bar?\n\nFiltering for Cleaner Features\n\n|\n\n49",
      "content_length": 1962,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 66,
      "content": "Even if we knew that the business in this case happened to be a bar, it would probably be a mistake to classify as such for other reviews that contain the word “gobbledy‐ gook.”\n\nNot only are rare words unreliable as predictors, they also generate computational overhead. The set of 1.6 million Yelp reviews contains 357,481 unique words (toke‐ nized by space and punctuation characters), 189,915 of which appear in only one review, and 41,162 in two reviews. Over 60% of the vocabulary occurs rarely. This is a so-called heavy-tailed distribution, and it is very common in real-world data. The training time of many statistical machine learning models scales linearly with the number of features, and some models are quadratic or worse. Rare words incur a large computation and storage cost for not much additional gain.\n\nRare words can be easily identified and trimmed based on word count statistics. Alternatively, their counts can be aggregated into a special garbage bin, which can serve as an additional feature. Figure 3-7 demonstrates this representation on a short document that contains a bunch of usual words and two rare words, “gobbledygook” and “zylophant.” The usual words retain their own counts, which can be further fil‐ tered by stopword lists or other frequency-based methods. The rare words lose their identity and get grouped into a garbage bin feature.\n\nFigure 3-7. Bag-of-words feature vector with a garbage bin\n\nSince one won’t know which words are rare until the whole corpus has been counted, the garbage bin feature will need to be collected as a post-processing step.\n\nSince this book is about feature engineering, our focus is on features. But the concept of rarity also applies to data points. If a text document is very short, then it likely con‐ tains no useful information and should not be used when training a model. One must use caution when applying this rule, however. The Wikipedia dump contains many pages that are incomplete stubs, which are probably safe to filter out. Tweets, on the other hand, are inherently short, and require other featurization and modeling tricks.\n\n50\n\n|\n\nChapter 3: Text Data: Flattening, Filtering, and Chunking",
      "content_length": 2180,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 67,
      "content": "Stemming One problem with simple parsing is that different variations of the same word get counted as separate words. For instance, “flower” and “flowers” are technically differ‐ ent tokens, and so are “swimmer,” “swimming,” and “swim,” even though they are very close in meaning. It would be nice if all of these different variations got mapped to the same word.\n\nStemming is an NLP task that tries to chop each word down to its basic linguistic word stem form. There are different approaches. Some are based on linguistic rules, others on observed statistics. A subclass of algorithms incorporate part-of-speech tag‐ ging and linguistic rules in a process known as lemmatization.\n\nMost stemming tools focus on the English language, though efforts are ongoing for other languages. The Porter stemmer is the most widely used free stemming tool for the English language. The original program is written in ANSI C, but many other packages have since wrapped it to provide access to other languages.\n\nHere is an example of running the Porter stemmer through the NLTK Python pack‐ age. As you can see, it handles a large number of cases, but it’s not perfect. The word “goes” is mapped to “goe,” while “go” is mapped to itself:\n\n>>> import nltk >>> stemmer = nltk.stem.porter.PorterStemmer() >>> stemmer.stem('flowers') u'flower' >>> stemmer.stem('zeroes') u'zero' >>> stemmer.stem('stemmer') u'stem' >>> stemmer.stem('sixties') u'sixti' >>> stemmer.stem('sixty') u'sixty' >>> stemmer.stem('goes') u'goe' >>> stemmer.stem('go') u'go'\n\nStemming does have a computation cost. Whether the end benefit outweighs the cost is application-dependent. It is also worth noting that stemming could hurt more than it helps. The words “new” and “news” have very different meanings, but both would be stemmed to “new.” Similar examples abound. For this reason, stemming is not always used.\n\nFiltering for Cleaner Features\n\n|\n\n51",
      "content_length": 1910,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 68,
      "content": "Atoms of Meaning: From Words to n-Grams to Phrases The concept of bag-of-words is straightforward. But how does a computer know what a word is? A text document is represented digitally as a string, which is basically a sequence of characters. One might also run into semi-structured text in the form of JSON blobs or HTML pages. But even with the added tags and structure, the basic unit is still a string. How does one turn a string into a sequence of words? This involves the tasks of parsing and tokenization, which we discuss next.\n\nParsing and Tokenization Parsing is necessary when the string contains more than plain text. For instance, if the raw data is a web page, an email, or a log of some sort, then it contains additional structure. One needs to decide how to handle the markup, the headers and footers, or the uninteresting sections of the log. If the document is a web page, then the parser needs to handle URLs. If it is an email, then fields like From, To, and Subject may require special handling—otherwise these headers will end up as normal words in the final count, which may not be useful.\n\nAfter light parsing, the plain-text portion of the document can go through tokeniza‐ tion. This turns the string—a sequence of characters—into a sequence of tokens. Each token can then be counted as a word. The tokenizer needs to know what charac‐ ters indicate that one token has ended and another is beginning. Space characters are usually good separators, as are punctuation characters. If the text contains tweets, then hash marks (#) should not be used as separators (also known as delimiters).\n\nSometimes, the analysis needs to operate on sentences instead of entire documents. For instance, n-grams, a generalization of the concept of a word, should not extend beyond sentence boundaries. More complex text featurization methods like word2vec also work with sentences or paragraphs. In these cases, one needs to first parse the document into sentences, then further tokenize each sentence into words.\n\nString Objects: More Than Meets the Eye\n\nString objects come in various encodings, like ASCII or Unicode. Plain English text can be encoded in ASCII. Most other languages require Unicode. If the document contains non-ASCII characters, then make sure that the tokenizer can handle that particular encod‐ ing. Otherwise, the results will be incorrect.\n\nCollocation Extraction for Phrase Detection A sequence of tokens immediately yields the list of words and n-grams. Semantically speaking, however, we are more used to understanding phrases, not n-grams. In computational natural language processing (NLP), the concept of a useful phrase is\n\n52\n\n|\n\nChapter 3: Text Data: Flattening, Filtering, and Chunking",
      "content_length": 2728,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 69,
      "content": "called a collocation. In the words of Manning and Schütze (1999: 151), “A collocation is an expression consisting of two or more words that correspond to some conven‐ tional way of saying things.”\n\nCollocations are more meaningful than the sum of their parts. For instance, “strong tea” has a different meaning beyond “great physical strength” and “tea”; therefore, it is considered a collocation. The phrase “cute puppy,” on the other hand, means exactly the sum of its parts: “cute” and “puppy.” Thus, it is not considered a colloca‐ tion.\n\nCollocations do not have to be consecutive sequences. For example, the sentence “Emma knocked on the door” is considered to contain the collocation “knock door.” Hence, not every collocation is an n-gram. Conversely, not every n-gram is deemed a meaningful collocation.\n\nBecause collocations are more than the sum of their parts, their meaning cannot be adequately captured by individual word counts. Bag-of-words falls short as a repre‐ sentation. Bag-of-n-grams is also problematic because it captures too many meaning‐ less sequences (consider “this is” in the bag-of-n-grams example) and not enough of the meaningful ones (i.e., knock door).\n\nCollocations are useful as features. But how does one discover and extract them from text? One way is to predefine them. If we tried really hard, we could probably find comprehensive lists of idioms in various languages, and we could look through the text for any matches. It would be very expensive, but it would work. If the corpus is very domain specific and contains esoteric lingo, then this might be the preferred method. But the list would require a lot of manual curation, and it would need to be constantly updated for evolving corpora. For example, it probably wouldn’t be very realistic for analyzing tweets, or for blogs and articles.\n\nSince the advent of statistical NLP in the last two decades, people have opted more and more for statistical methods for finding phrases. Instead of establishing a fixed list of phrases and idiomatic sayings, statistical collocation extraction methods rely on the ever-evolving data to reveal the popular sayings of the day.\n\nFrequency-based methods\n\nA simple hack is to look at the most frequently occurring n-grams. The problem with this approach is that the most frequently occurring ones may not be the most useful ones. Table 3-2 shows the most popular bigrams (n = 2) in the entire Yelp reviews dataset. As we can see, the top 10 most frequently occurring bigrams by document count are very generic terms that don’t contain much meaning.\n\nAtoms of Meaning: From Words to n-Grams to Phrases\n\n|\n\n53",
      "content_length": 2640,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 70,
      "content": "Table 3-2. Most frequently occurring 2-grams in the Yelp reviews dataset\n\nBigram of the and the in the it was this place it s and i on the i was for the\n\nDocument count 450,849 426,346 397,821 396,713 344,800 341,090 332,415 325,044 285,012 276,946\n\nHypothesis testing for collocation extraction\n\nRaw popularity count is too crude of a measure. We have to find more clever statis‐ tics to be able to pick out meaningful phrases easily. The key idea is to ask whether two words appear together more often than they would by chance. The statistical machinery for answering this question is called a hypothesis test.\n\nHypothesis testing is a way to boil noisy data down to “yes” or “no” answers. It involves modeling the data as samples drawn from random distributions. The ran‐ domness means that one can never be 100% sure about the answer; there’s always the chance of an outlier. So, the answers are attached to a probability.\n\nFor example, the outcome of a hypothesis test might be “these two datasets come from the same distribution with 95% probability.” For a gentle introduction to hypothesis testing, see the Khan Academy’s tutorial on Hypothesis Testing and p- Values.\n\nIn the context of collocation extraction, many hypothesis tests have been proposed over the years. One of the most successful methods is based on the likelihood ratio test (Dunning, 1993). For a given pair of words, the method tests two hypotheses on the observed dataset. Hypothesis 1 (the null hypothesis) says that word 1 appears independently from word 2. Another way of saying this is that seeing word 1 has no bearing on whether we also see word 2. Hypothesis 2 (the alternate hypothesis) says that seeing word 1 changes the likelihood of seeing word 2. We take the alternate hypothesis to imply that the two words form a common phrase. Hence, the likelihood ratio test for phrase detection (a.k.a. collocation extraction) asks the following ques‐ tion: are the observed word occurrences in a given text corpus more likely to have been generated from a model where the two words occur independently from one another, or a model where the probabilities of the two words are entangled?\n\n54\n\n|\n\nChapter 3: Text Data: Flattening, Filtering, and Chunking",
      "content_length": 2233,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 71,
      "content": "That is a mouthful. Let’s math it up a little. (Math is great at expressing things very precisely and concisely, but it does require a completely different parser than natural language.)\n\nWe can express the null hypothesis Hnull (independent) as P(w2 | w1) = P(w2 | not w1), and the alternate hypothesis Halternate (not independent) as P(w2 | w1) ≠ P(w2 | not w1).\n\nThe final statistic is the log of the ratio between the two:\n\nlog λ = log\n\nL (Data; Hnull) L (Data; Halternate)\n\n.\n\nThe likelihood function L(Data; H) represents the probability of seeing the word fre‐ quencies in the dataset under the independent or the not independent model for the word pair. In order to compute this probability, we have to make another assumption about how the data is generated. The simplest data generation model is the binomial model, where for each word in the dataset, we toss a coin, and we insert our special word if the coin comes up heads, and some other word otherwise. Under this strat‐ egy, the count of the number of occurrences of the special word follows a binomial distribution. The binomial distribution is completely determined by the total number of words, the number of occurrences of the word of interest, and the heads probabil‐ ity.\n\nThe algorithm for detecting common phrases through likelihood ratio test analysis proceeds as follows:\n\n1. Compute occurrence probabilities for all singleton words: P(w). 2. Compute conditional pairwise word occurrence probabilities for all unique bigrams: P(w2 | w1).\n\n3. Compute the likelihood ratio log λ for all unique bigrams. 4. Sort the bigrams based on their likelihood ratio. 5. Take the bigrams with the smallest likelihood ratio values as features.\n\nGetting a Grip on the Likelihood Ratio Test\n\nThe key is that what the test compares is not the probability parameters themselves, but rather the probability of seeing the observed data under those parameters (and an assumed data gen‐ eration model). Likelihood is one of the key principles of statistical learning, but it is definitely a brain-twister the first few times you see it. Once you work out the logic, it becomes intuitive.\n\nAtoms of Meaning: From Words to n-Grams to Phrases\n\n|\n\n55",
      "content_length": 2199,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 72,
      "content": "There is another statistical approach that’s based on pointwise mutual information, but it is very sensitive to rare words, which are always present in real-world text cor‐ pora. Hence, it is not commonly used and we will not be demonstrating it here.\n\nNote that all of the statistical methods for collocation extraction, whether using raw frequency, hypothesis testing, or pointwise mutual information, operate by filtering a list of candidate phrases. The easiest and cheapest way to generate such a list is by counting n-grams. It’s possible to generate nonconsecutive sequences, but they are expensive to compute. In practice, even for consecutive n-grams, people rarely go beyond bigrams or trigrams because there are too many of them, even after filter‐ ing. To generate longer phrases, there are other methods such as chunking or com‐ bining with part-of-speech (PoS) tagging.\n\nChunking and part-of-speech tagging\n\nChunking is a bit more sophisticated than finding n-grams, in that it forms sequences of tokens based on parts of speech, using rule-based models.\n\nFor example, we might be most interested in finding all of the noun phrases in a problem where the entity (in this case the subject of a text) is the most interesting to us. In order to find this, we tokenize each word with a part of speech and then exam‐ ine the token’s neighborhood to look for part-of-speech groupings, or “chunks.” The models that map words to parts of speech are generally language specific. Several open source Python libraries, such as NLTK, spaCy, and TextBlob, have multiple lan‐ guage models available.\n\nTo illustrate how several libraries in Python make chunking using PoS tagging fairly straightforward, let’s use the Yelp reviews dataset again. In Example 3-2, we evaluate the parts of speech to find the noun phrases using both spaCy and TextBlob.\n\nExample 3-2. PoS tagging and chunking\n\n>>> import pandas as pd >>> import json\n\n# Load the first 10 reviews >>> f = open('data/yelp/v6/yelp_academic_dataset_review.json') >>> js = [] >>> for i in range(10): ... js.append(json.loads(f.readline())) >>> f.close() >>> review_df = pd.DataFrame(js)\n\n# First we'll walk through spaCy's functions >>> import spacy # preload the language model >>> nlp = spacy.load('en')\n\n56\n\n|\n\nChapter 3: Text Data: Flattening, Filtering, and Chunking",
      "content_length": 2328,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 73,
      "content": "# We can create a Pandas Series of spaCy nlp variables >>> doc_df = review_df['text'].apply(nlp)\n\n# spaCy gives us fine-grained parts of speech using (.pos_) # and coarse-grained parts of speech using (.tag_) >>> for doc in doc_df[4]: ... print([doc.text, doc.pos_, doc.tag_])\n\nGot VERB VBP a DET DT letter NOUN NN in ADP IN the DET DT mail NOUN NN last ADJ JJ week NOUN NN that ADJ WDT said VERB VBD Dr. PROPN NNP Goldberg PROPN NNP is VERB VBZ moving VERB VBG to ADP IN Arizona PROPN NNP to PART TO take VERB VB a DET DT new ADJ JJ position NOUN NN there ADV RB in ADP IN June PROPN NNP . PUNCT . SPACE SP He PRON PRP will VERB MD be VERB VB missed VERB VBN very ADV RB much ADV RB . PUNCT .\n\nSPACE SP I PRON PRP think VERB VBP finding VERB VBG a DET DT new ADJ JJ doctor NOUN NN in ADP IN NYC PROPN NNP\n\nAtoms of Meaning: From Words to n-Grams to Phrases\n\n|\n\n57",
      "content_length": 864,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 74,
      "content": "that ADP IN you PRON PRP actually ADV RB like INTJ UH might VERB MD almost ADV RB be VERB VB as ADV RB awful ADJ JJ as ADP IN trying VERB VBG to PART TO find VERB VB a DET DT date NOUN NN ! PUNCT .\n\n# spaCy also does some basic noun chunking for us >>> print([chunk for chunk in doc_df[4].noun_chunks]) [a letter, the mail, Dr. Goldberg, Arizona, a new position, June, He, I, a new doctor, NYC, you, a date]\n\n##### # We can do the same feature transformations using Textblob from textblob import TextBlob\n\n# The default tagger in TextBlob uses the PatternTagger, which is OK for our example. # You can also specify the NLTK tagger, which works better for incomplete sentences. >>> blob_df = review_df['text'].apply(TextBlob)\n\n>>> blob_df[4].tags [('Got', 'NNP'), ('a', 'DT'), ('letter', 'NN'), ('in', 'IN'), ('the', 'DT'), ('mail', 'NN'), ('last', 'JJ'), ('week', 'NN'), ('that', 'WDT'), ('said', 'VBD'), ('Dr.', 'NNP'), ('Goldberg', 'NNP'), ('is', 'VBZ'), ('moving', 'VBG'), ('to', 'TO'), ('Arizona', 'NNP'), ('to', 'TO'), ('take', 'VB'), ('a', 'DT'), ('new', 'JJ'), ('position', 'NN'),\n\n58\n\n|\n\nChapter 3: Text Data: Flattening, Filtering, and Chunking",
      "content_length": 1153,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 75,
      "content": "('there', 'RB'), ('in', 'IN'), ('June', 'NNP'), ('He', 'PRP'), ('will', 'MD'), ('be', 'VB'), ('missed', 'VBN'), ('very', 'RB'), ('much', 'JJ'), ('I', 'PRP'), ('think', 'VBP'), ('finding', 'VBG'), ('a', 'DT'), ('new', 'JJ'), ('doctor', 'NN'), ('in', 'IN'), ('NYC', 'NNP'), ('that', 'IN'), ('you', 'PRP'), ('actually', 'RB'), ('like', 'IN'), ('might', 'MD'), ('almost', 'RB'), ('be', 'VB'), ('as', 'RB'), ('awful', 'JJ'), ('as', 'IN'), ('trying', 'VBG'), ('to', 'TO'), ('find', 'VB'), ('a', 'DT'), ('date', 'NN')]\n\n>>> print([np for np in blob_df[4].noun_phrases]) ['got', 'goldberg', 'arizona', 'new position', 'june', 'new doctor', 'nyc']\n\nYou can see that the noun phrases found by each library are a little bit different. spaCy includes common words in the English language like “a” and “the,” while TextBlob removes these. This reflects a difference in the rules engines that drive what each library considers to be a noun phrase. You can also write your part-of-speech relationships to define the chunks you are seeking. See Bird et al. (2009) to really dive deep into chunking with Python from scratch.\n\nSummary The bag-of-words representation is simple to understand, easy to compute, and use‐ ful for classification and search tasks. But sometimes single words are too simplistic to encapsulate some information in the text. To fix this problem, people look to\n\nSummary\n\n|\n\n59",
      "content_length": 1383,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 76,
      "content": "longer sequences. Bag-of-n-grams is a natural generalization of bag-of-words. The concept is still easy to understand, and it’s just as easy to compute as bag-of-words.\n\nBag-of-n-grams generates a lot more distinct n-grams. It increases the feature storage cost, as well as the computation cost of the model training and prediction stages. The number of data points remains the same, but the dimension of the feature space is now much larger. Hence, the data is much more sparse. The higher n is, the higher the storage and computation cost, and the sparser the data. For these reasons, longer n-grams do not always lead to improvements in model accuracy (or any other perfor‐ mance measure). People usually stop at n = 2 or 3. Longer n-grams are rarely used.\n\nOne way to combat the increase in sparsity and cost is to filter the n-grams and retain only the most meaningful phrases. This is the goal of collocation extraction. In theory, collocations (or phrases) could form nonconsecutive token sequences in the text. In practice, however, looking for nonconsecutive phrases has a much higher computation cost for not much gain. So, collocation extraction usually starts with a candidate list of bigrams and utilizes statistical methods to filter them.\n\nAll of these methods turn a sequence of text tokens into a disconnected set of counts. Sets have much less structure than sequences; they lead to flat feature vectors.\n\nIn this chapter, we dipped our toes into the water with simple text featurization tech‐ niques. These techniques turn a piece of natural language text—full of rich semantic structure—into a simple flat vector. We discussed a number of common filtering techniques to clean up the vector entries. We also introduced n-grams and colloca‐ tion extraction as methods that add a little more structure into the flat vectors. The next chapter goes into a lot more detail about another common text featurization trick called tf-idf. Subsequent chapters will discuss more methods for adding struc‐ ture back into a flat vector.\n\nBibliography Bird, Steven, Ewan Klein, and Edward Loper. Natural Language Processing with Python. Sebastopol, CA: O’Reilly Media, 2009.\n\nDunning, Ted. “Accurate Methods for the Statistics of Surprise and Coincidence.” ACM Journal of Computational Linguistics, special issue on using large corpora 19:1 (1993): 61–74.\n\nKhan Academy. “Hypothesis Testing and p-Values.” Retrieved from https:// www.khanacademy.org/math/probability/statistics-inferential/hypothesis-testing/v/ hypothesis-testing-and-p-values.\n\nManning, Christopher D. and Hinrich Schütze. Foundations of Statistical Natural Language Processing. Cambridge, MA: MIT Press, 1999.\n\n60\n\n|\n\nChapter 3: Text Data: Flattening, Filtering, and Chunking",
      "content_length": 2748,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 77,
      "content": "CHAPTER 4 The Effects of Feature Scaling: From Bag-of-Words to Tf-Idf\n\nA bag-of-words representation is simple to generate but far from perfect. If we count all words equally, then some words end up being emphasized more than we need. Recall our example of Emma and the raven from Chapter 3. We’d like a document representation that emphasizes the two main characters. The words “Emma” and “raven” both appear three times, but “the” appears a whopping eight times, “and” appears five times, and “it” and “was” both appear four times. The main characters do not stand out by simple frequency count alone. This is problematic.\n\nIt would also be nice to pick out words such as “magnificently,” “gleamed,” “intimi‐ dated,” “tentatively,” and “reigned,” because they help to set the overall tone of the paragraph. They indicate sentiment, which can be very valuable information to a data scientist. So, ideally, we’d like a representation that highlights meaningful words.\n\nTf-Idf : A Simple Twist on Bag-of-Words Tf-idf is a simple twist on the bag-of-words approach. It stands for term frequency– inverse document frequency. Instead of looking at the raw counts of each word in each document in a dataset, tf-idf looks at a normalized count where each word count is divided by the number of documents this word appears in. That is:\n\nbow(w, d) = # times word w appears in document d\n\ntf-idf(w, d) = bow(w, d) * N / (# documents in which word w appears)\n\nN is the total number of documents in the dataset. The fraction N / (# documents ...) is what’s known as the inverse document frequency. If a word appears in many\n\n61",
      "content_length": 1616,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 78,
      "content": "documents, then its inverse document frequency is close to 1. If a word appears in just a few documents, then the inverse document frequency is much higher.\n\nAlternatively, we can take a log transform instead using the raw inverse document frequency. Logarithm turns 1 into 0, and makes large numbers (those much greater than 1) smaller. (More on this later.)\n\nIf we define tf-idf as:\n\ntf-idf(w, d) = bow(w, d) * log (N / # documents in which word w appears)\n\nthen a word that appears in every single document will be effectively zeroed out, and a word that appears in very few documents will have an even larger count than before.\n\nLet’s look at some pictures to understand what it’s all about. Figure 4-1 shows a sim‐ ple example that contains four sentences: “it is a puppy,” “it is a cat,” “it is a kitten,” and “that is a dog and this is a pen.” We plot these sentences in the feature space of three words: “puppy,” “cat,” and “is.”\n\nFigure 4-1. Four sentences about dogs and cats\n\nNow let’s look at the same four sentences in tf-idf representation using the log trans‐ form for the inverse document frequency. Figure 4-2 shows the documents in feature space. Notice that the word “is” is effectively eliminated as a feature since it appears in all sentences in this dataset. Also, because they each appear in only one sentence out of the total four, the words “puppy” and “cat” are now counted higher than before (log(4) = 1.38... > 1). Thus, tf-idf makes rare words more prominent and\n\n62\n\n|\n\nChapter 4: The Effects of Feature Scaling: From Bag-of-Words to Tf-Idf",
      "content_length": 1570,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 79,
      "content": "effectively ignores common words. It is closely related to the frequency-based filter‐ ing methods in Chapter 3, but much more mathematically elegant than placing hard cutoff thresholds.\n\nIntuition Behind Tf-Idf\n\nTf-idf makes rare words more prominent and effectively ignores common words.\n\nFigure 4-2. Tf-idf representation of the sentences in Figure 4-1\n\nPutting It to the Test Tf-idf transforms word count features through multiplication with a constant. Hence, it is an example of feature scaling, a concept introduced in Chapter 2. How well does feature scaling work in practice? Let’s compare the performance of scaled and unscaled features in a simple text classification task. Time for some code!\n\nIn Example 4-1, we revisit the Yelp reviews dataset. Round 6 of the Yelp dataset chal‐ lenge contains close to 1.6 million reviews of businesses in six US cities.\n\nExample 4-1. Loading and cleaning the Yelp reviews dataset in Python\n\n>>> import json >>> import pandas as pd\n\n# Load Yelp business data >>> biz_f = open('yelp_academic_dataset_business.json') >>> biz_df = pd.DataFrame([json.loads(x) for x in biz_f.readlines()]) >>> biz_f.close()\n\n# Load Yelp reviews data\n\nPutting It to the Test\n\n|\n\n63",
      "content_length": 1207,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 80,
      "content": ">>> review_file = open('yelp_academic_dataset_review.json') >>> review_df = pd.DataFrame([json.loads(x) for x in review_file.readlines()]) >>> review_file.close()\n\n# Pull out only Nightlife and Restaurants businesses >>> two_biz = biz_df[biz_df.apply(lambda x: 'Nightlife' in x['categories'] or ... 'Restaurants' in x['categories'], ... axis=1)]\n\n# Join with the reviews to get all reviews on the two types of business >>> twobiz_reviews = two_biz.merge(review_df, on='business_id', how='inner')\n\n# Trim away the features we won't use >>> twobiz_reviews = twobiz_reviews[['business_id', ... 'name', ... 'stars_y', ... 'text', ... 'categories']]\n\n# Create the target column--True for Nightlife businesses, and False otherwise >>> two_biz_reviews['target'] = \\ ... twobiz_reviews.apply(lambda x: 'Nightlife' in x['categories'], ... axis=1)\n\nCreating a Classification Dataset Let’s see whether we can use the reviews to categorize a business as either a restaurant or a nightlife venue. To save on training time, we can take a subset of the reviews. In this case, there is a large difference in review count between the two categories. This is called a class-imbalanced dataset. Imbalanced datasets are problematic for model‐ ing because the model will expend most of its effort fitting to the larger class. Since we have plenty of data in both classes, a good way to resolve the problem is to down‐ sample the larger class (restaurants) to be roughly the same size as the smaller class (nightlife). Here is an example workflow:\n\n1. Take a random sample of 10% of nightlife reviews and 2.1% of restaurant reviews (percentages chosen so the number of examples in each class is roughly equal).\n\n2. Create a 70/30 train-test split of this dataset. In this example, the training set ends up with 29,264 reviews, and the test set with 12,542 reviews.\n\n3. The training data contains 46,924 unique words; this is the number of features in the bag-of-words representation.\n\nExample 4-2 shows how we do this.\n\n64\n\n|\n\nChapter 4: The Effects of Feature Scaling: From Bag-of-Words to Tf-Idf",
      "content_length": 2075,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 81,
      "content": "Example 4-2. Creating a balanced classification dataset\n\n# Create a class-balanced subsample to play with >>> nightlife = \\ ... twobiz_reviews[twobiz_reviews.apply(lambda x: 'Nightlife' in x['categories'], ... axis=1)] >>> restaurants = \\ ... twobiz_reviews[twobiz_reviews.apply(lambda x: 'Restaurants' in x['categories'], ... axis=1)] >>> nightlife_subset = nightlife.sample(frac=0.1, random_state=123) >>> restaurant_subset = restaurants.sample(frac=0.021, random_state=123) >>> combined = pd.concat([nightlife_subset, restaurant_subset])\n\n# Split into training and test datasets >>> training_data, test_data = modsel.train_test_split(combined, ... train_size=0.7, ... random_state=123) >>> training_data.shape (29264, 5) >>> test_data.shape (12542, 5)\n\nScaling Bag-of-Words with Tf-Idf Transformation The goal of this experiment is to compare the effectiveness of bag-of-words, tf-idf, and ℓ2 normalization for linear classification. Note that doing tf-idf then ℓ2 normal‐ ization is the same as doing ℓ2 normalization alone. So, we only need to test three sets of features: bag-of-words, tf-idf, and word-wise ℓ2 normalization on top of bag-of- words.\n\nIn Example 4-3, we use scikit-learn’s CountVectorizer to convert the review text into a bag-of-words. All text featurization methods implicitly depend on a tokenizer, which is the module that converts a text string into a list of tokens (words). In this example, scikit-learn’s default tokenizing pattern looks for sequences of two or more alphanumeric characters. Punctuation marks are treated as token separators.\n\nExample 4-3. Transform features\n\n# Represent the review text as a bag-of-words >>> bow_transform = text.CountVectorizer() >>> X_tr_bow = bow_transform.fit_transform(training_data['text']) >>> X_te_bow = bow_transform.transform(test_data['text']) >>> len(bow_transform.vocabulary_) 46924\n\n>>> y_tr = training_data['target'] >>> y_te = test_data['target']\n\n# Create the tf-idf representation using the bag-of-words matrix\n\nPutting It to the Test\n\n|\n\n65",
      "content_length": 2024,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 82,
      "content": ">>> tfidf_trfm = text.TfidfTransformer(norm=None) >>> X_tr_tfidf = tfidf_trfm.fit_transform(X_tr_bow) >>> X_te_tfidf = tfidf_trfm.transform(X_te_bow)\n\n# Just for kicks, l2-normalize the bag-of-words representation >>> X_tr_l2 = preproc.normalize(X_tr_bow, axis=0) >>> X_te_l2 = preproc.normalize(X_te_bow, axis=0)\n\nFeature Scaling on the Test Set\n\nA subtle point about feature scaling is that it requires knowing fea‐ ture statistics that we most likely do not know in practice, such as the mean, variance, document frequency, ℓ2 norm, etc. In order to compute the tf-idf representation, we have to compute the inverse document frequencies based on the training data and use these sta‐ tistics to scale both the training and test data. In scikit-learn, fitting the feature transformer on the training data amounts to collecting the relevant statistics. The fitted transformer can then be applied to the test data.\n\nWhen we use training statistics to scale test data, the result will look a little fuzzy. Min-max scaling on the test set no longer neatly maps to 0 and 1. ℓ2 norms, mean, and variance statistics will all look a little off. This is less problematic than missing data. For instance, the test set may contain words that are not present in the training data, and we would have no document frequency to use for the new words. The com‐ mon solution is to simply drop the new words in the test set. This may seem irre‐ sponsible, but the model—trained on the training set—would not know what to do with these words anyway. A slightly less hacky option would be to explicitly learn a “garbage” word and map all low-frequency words to it, even within the training set, as discussed in “Rare words” on page 49.\n\nClassification with Logistic Regression Logistic regression is a simple, linear classifier. Due to its simplicity, it’s often a good first classifier to try. It takes a weighted combination of the input features, and passes it through a sigmoid function, which smoothly maps any real number to a number between 0 and 1. The function transforms a real number input, x, into a number between 0 and 1. It has one set of parameters, w, which represents the slope of the increase around the midpoint, 0.5. The intercept term b denotes the input value where the function output crosses the midpoint. A logistic classifier would predict the positive class if the sigmoid output is greater than 0.5, and the negative class otherwise. By varying w and b, one can control where that change in decision occurs, and how fast the decision should respond to changing input values around that point.\n\n66\n\n|\n\nChapter 4: The Effects of Feature Scaling: From Bag-of-Words to Tf-Idf",
      "content_length": 2680,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 83,
      "content": "Figure 4-3 illustrates the sigmoid function.\n\nFigure 4-3. Illustration of a sigmoid function\n\nNow let’s build some simple logistic regression classifiers on our various feature sets and see how they do (Example 4-4).\n\nExample 4-4. Training logistic regression classifiers with default parameters\n\n>>> def simple_logistic_classify(X_tr, y_tr, X_test, y_test, description): ... ### Helper function to train a logistic classifier and score on test data ... m = LogisticRegression().fit(X_tr, y_tr) ... s = m.score(X_test, y_test) ... print ('Test score with', description, 'features:', s) ... return m\n\n>>> m1 = simple_logistic_classify(X_tr_bow, y_tr, X_te_bow, y_te, 'bow') >>> m2 = simple_logistic_classify(X_tr_l2, y_tr, X_te_l2, y_te, 'l2-normalized') >>> m3 = simple_logistic_classify(X_tr_tfidf, y_tr, X_te_tfidf, y_te, 'tf-idf') Test score with bow features: 0.775873066497 Test score with l2-normalized features: 0.763514590974 Test score with tf-idf features: 0.743182905438\n\nParadoxically, the results show that the most accurate classifier is the one using BoW features. This was unexpected. As it turns out, the reason is that the classifiers are not well “tuned,” which is a common pitfall when comparing classifiers.\n\nPutting It to the Test\n\n|\n\n67",
      "content_length": 1259,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 84,
      "content": "Tuning Logistic Regression with Regularization Logistic regression has a few bells and whistles. When the number of features is greater than the number of data points, the problem of finding the best model is said to be underdetermined. One way to fix this problem is by placing additional con‐ straints on the training process. This is known as regularization, and its technical details are discussed here.\n\nMost implementations of logistic regression allow for regularization. In order to use this functionality, one must specify a regularization parameter. Regularization parameters are hyperparameters that are not learned automatically in the model training process. Rather, they must be tuned on the problem at hand and given to the training algorithm. This process is known as hyperparameter tuning. (For details on how to evaluate machine learning models, see, e.g., Zheng [2015].) One basic method for tuning hyperparameters is called grid search: you specify a grid of hyperparameter values and the tuner programmatically searches for the best hyperparameter setting in the grid. After finding the best hyperparameter setting, you train a model on the entire training set using that setting, and use its performance on the test set as the final evaluation of this class of models.\n\nImportant: Tune Hyperparameters When Comparing Models\n\nIt’s essential to tune hyperparameters when comparing models or features. The default settings of a software package will always return a model. But unless the software performs automatic tuning under the hood, it is likely to return a suboptimal model based on suboptimal hyperparameter settings. The sensitivity of classifier performance to hyperparameter settings depends on the model and the distribution of training data. Logistic regression is relatively robust (or insensitive) to hyperparameter settings. Even so, it is necessary to find and use the right range of hyperparameters. Otherwise, the advantages of one model versus another may be solely due to tuning parameters, and will not reflect the actual behavior of the model or features.\n\nEven the best autotuning packages still require specifying the upper and lower limits of search, and finding those limits can take a few manual tries.\n\nIn the following example, we manually set the search grid of the logistic regulariza‐ tion parameter to {1e-5, 0.001, 0.1, 1, 10, 100}. The upper and lower bounds took a couple of tries to narrow down. The optimal hyperparameter settings for each feature set are given in Table 4-1.\n\n68\n\n|\n\nChapter 4: The Effects of Feature Scaling: From Bag-of-Words to Tf-Idf",
      "content_length": 2612,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 85,
      "content": "Table 4-1. Best hyperparameter settings for logistic regression on a sample of Yelp reviews of nightlife venues and restaurants\n\nℓ2 regularization 0.1 BoW ℓ2-normalized 10 Tf-idf\n\n0.001\n\nWe also want to test whether the difference in accuracy between tf-idf and BoW is due to noise. To this end, we use k-fold cross validation to simulate having multiple statistically independent datasets. It divides the dataset into k folds. The cross valida‐ tion process iterates through the folds, using all but one fold for training, and validat‐ ing the results on the fold that is held out.\n\nEstimating Variance via Resampling Modern statistical methods assume that the underlying data comes from a random distribution. The performance measurements of models derived from data are also subject to random noise. In this situation, it is always a good idea to take the meas‐ urement not just once, but multiple times, based on datasets of comparable statistics. This gives us a confidence interval for the measurement.\n\nk-fold cross validation is one such strategy. Resampling is another technique that generates multiple small samples from the same underlying dataset. See Zheng (2015) for more details on resampling.\n\nThe GridSearchCV function in scikit-learn runs a grid search with cross validation (see Example 4-5). Figure 4-4 shows a box-and-whiskers plot of the distribution of accuracy measurements for models trained on each of the feature sets. The middle line in the box marks the median accuracy, the box itself marks the region between the first and third quartiles, and the whiskers extend to the rest of the distribution.\n\nExample 4-5. Tuning logistic regression hyperparameters with grid search\n\n>>> import sklearn.model_selection as modsel\n\n# Specify a search grid, then do a 5-fold grid search for each of the feature sets >>> param_grid_ = {'C': [1e-5, 1e-3, 1e-1, 1e0, 1e1, 1e2]}\n\n# Tune classifier for bag-of-words representation >>> bow_search = modsel.GridSearchCV(LogisticRegression(), cv=5, ... param_grid=param_grid_) >>> bow_search.fit(X_tr_bow, y_tr)\n\n# Tune classifier for L2-normalized word vector\n\nPutting It to the Test\n\n|\n\n69",
      "content_length": 2149,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 86,
      "content": ">>> l2_search = modsel.GridSearchCV(LogisticRegression(), cv=5, ... param_grid=param_grid_) >>> l2_search.fit(X_tr_l2, y_tr)\n\n# Tune classifier for tf-idf >>> tfidf_search = modsel.GridSearchCV(LogisticRegression(), cv=5, ... param_grid=param_grid_) >>> tfidf_search.fit(X_tr_tfidf, y_tr)\n\n# Let's check out one of the grid search outputs to see how it went >>> bow_search.cv_results_ {'mean_fit_time': array([ 0.43648252, 0.94630651, 5.64090128, 15.31248307, 31.47010217, 42.44257565]), 'mean_score_time': array([ 0.00080056, 0.00392466, 0.00864897, 0 .00784755, 0.01192751, 0.0072515 ]), 'mean_test_score': array([ 0.57897075, 0.7518111 , 0.78283898, 0.77381766, 0.75515992, 0.73937261]), 'mean_train_score': array([ 0.5792185 , 0.76731652, 0.87697341, 0.94629064, 0.98357195, 0.99441294]), 'param_C': masked_array(data = [1e-05 0.001 0.1 1.0 10.0 100.0], mask = [False False False False False False], fill_value = ?), 'params': ({'C': 1e-05}, {'C': 0.001}, {'C': 0.1}, {'C': 1.0}, {'C': 10.0}, {'C': 100.0}), 'rank_test_score': array([6, 4, 1, 2, 3, 5]), 'split0_test_score': array([ 0.58028698, 0.75025624, 0.7799795 , 0.7726341 , 0.75247694, 0.74086095]), 'split0_train_score': array([ 0.57923964, 0.76860316, 0.87560871, 0.94434003, 0.9819308 , 0.99470312]), 'split1_test_score': array([ 0.5786776 , 0.74628396, 0.77669571, 0.76627371, 0 .74867589, 0.73176149]), 'split1_train_score': array([ 0.57917218, 0.7684849 , 0.87945837, 0.94822946, 0.98504976, 0.99538678]), 'split2_test_score': array([ 0.57816504, 0.75533914, 0.78472578, 0.76832394, 0.74799248, 0.7356911 ]), 'split2_train_score': array([ 0.57977019, 0.76613558, 0.87689548, 0.94566657, 0.98368288, 0.99397719]), 'split3_test_score': array([ 0.57894737, 0.75051265, 0.78332194, 0.77682843, 0.75768968, 0.73855092]), 'split3_train_score': array([ 0.57914745, 0.76678626, 0.87634546, 0.94558346, 0.98385443, 0.99474628]), 'split4_test_score': array([ 0.57877649, 0.75666439, 0.78947368, 0.78503076, 0.76896787, 0.75 ]), 'split4_train_score': array([ 0.57876303, 0.7665727 , 0.87655903, 0.94763369, 0.98334188, 0.99325132]), 'std_fit_time': array([ 0.03874582, 0.02297261, 1.18862097, 1.83901079, 4.21516797, 2.93444269]), 'std_score_time': array([ 0.00160112, 0.00605009, 0.00623053, 0.00698687,\n\n70\n\n|\n\nChapter 4: The Effects of Feature Scaling: From Bag-of-Words to Tf-Idf",
      "content_length": 2339,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 87,
      "content": "0.00713112, 0.00570195]), 'std_test_score': array([ 0.00070799, 0.00375907, 0.00432957, 0.00668246, 0.00612049]), 'std_train_score': array([ 0.00032232, 0.00102466, 0.00131222, 0.00143229, 0.00100223, 0.00073252])}\n\n# Plot the cross validation results in a box-and-whiskers plot to # visualize and compare classifier performance >>> search_results = pd.DataFrame.from_dict({ ... 'bow': bow_search.cv_results_['mean_test_score'], ... 'tfidf': tfidf_search.cv_results_['mean_test_score'], ... 'l2': l2_search.cv_results_['mean_test_score'] ... })\n\n# Our usual matplotlib incantations. Seaborn is used here to make # the plot pretty. >>> import matplotlib.pyplot as plt >>> import seaborn as sns >>> sns.set_style(\"whitegrid\")\n\n>>> ax = sns.boxplot(data=search_results, width=0.4) >>> ax.set_ylabel('Accuracy', size=14) >>> ax.tick_params(labelsize=14)\n\nFigure 4-4. Distribution of classifier accuracy under each feature set and regularization setting—the accuracy is measured as the average accuracy from 5-fold cross validation\n\nTable 4-2 shows the average cross validation classifier accuracy for each hyperpara‐ meter setting. The asterisk in each column denotes the highest achieved accuracy for that feature set.\n\nTable 4-2. Average cross validation classifier accuracy scores\n\nRegularization parameter BoW 0.00001 0.001 0.1 1\n\n0.578971 0.751811 0.782839 * 0.773818\n\nℓ2-normalized Tf-idf 0.575724 0.575724 0.589120 0.734247\n\n0.721638 0.788648 * 0.763566 0.741150\n\nPutting It to the Test\n\n|\n\n71",
      "content_length": 1496,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 88,
      "content": "Regularization parameter BoW 10 100\n\n0.755160 0.739373\n\nℓ2-normalized Tf-idf 0.776756 * 0.761106\n\n0.721467 0.712309\n\nThe result for ℓ2 normalized features looks alarmingly bad in Figure 4-4. But don’t be fooled. The low accuracy numbers are due to very bad regularization parameter set‐ tings—concrete proof that suboptimal hyperparameters can lead to very wrong con‐ clusions. If we train a model using the best hyperparameter setting for each feature set, as in Example 4-6, the accuracy scores of the different feature sets are very close.\n\nExample 4-6. Final training and testing step to compare the different feature sets\n\n# Train a final model on the entire training set, using the best hyperparameter # settings found previously. Measure accuracy on the test set. >>> m1 = simple_logistic_classify(X_tr_bow, y_tr, X_te_bow, y_te, 'bow', ... _C=bow_search.best_params_['C']) >>> m2 = simple_logistic_classify(X_tr_l2, y_tr, X_te_l2, y_te, 'l2-normalized', ... _C=l2_search.best_params_['C']) >>> m3 = simple_logistic_classify(X_tr_tfidf, y_tr, X_te_tfidf, y_te, 'tf-idf', ... _C=tfidf_search.best_params_['C']) Test score with bow features: 0.78360708021 Test score with l2-normalized features: 0.780178599904 Test score with tf-idf features: 0.788470738319\n\nProper tuning improved the accuracy of all the feature sets, and all three now yield similar classification accuracy under regularized logistic regression. The accuracy score for the tf-idf model is slightly higher, but the difference is likely not statistically significant. These results are completely mystifying. If feature scaling doesn’t work better than vanilla bag-of-words, then why do it at all? Why all the hoopla if tf-idf doesn’t do anything? We’ll explore the answers to those questions in the next section.\n\nDeep Dive: What Is Happening? In order to understand the “why” behind the results, we have to look at how the fea‐ tures are being used by the model. For linear models like logistic regression, this hap‐ pens through an intermediary object called the data matrix.\n\nThe data matrix contains data points represented as fixed-length flat vectors. With bag-of-words vectors, the data matrix is also known as the document-term matrix. Figure 3-1 shows a bag-of-words vector in vector form, and Figure 4-1 illustrates four bag-of-words vectors in feature space. To form a document-term matrix, simply take the document vectors, lay them out flat, and stack them on top of one another. The columns represent all possible words in the vocabulary (see Figure 4-5). Since most documents contain only a small subset of all possible words, most of the entries in this matrix are zeros; it is a sparse matrix.\n\n72\n\n|\n\nChapter 4: The Effects of Feature Scaling: From Bag-of-Words to Tf-Idf",
      "content_length": 2763,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 89,
      "content": "Figure 4-5. An example document-term matrix of five documents and seven words\n\nFeature scaling methods are essentially column operations on the data matrix. In par‐ ticular, tf-idf and ℓ2 normalization both multiply the entire column (an n-gram fea‐ ture, for example) by a constant.\n\nTf-Idf = Column Scaling\n\nTf-idf and ℓ2 normalization are both column operations on the data matrix.\n\nAs discussed in Appendix A, training a linear classifier boils down to finding the best linear combination of features, which are column vectors of the data matrix. The sol‐ ution space is characterized by the column space and the null space of the data matrix. The quality of the trained linear classifier directly depends upon the null space and the column space of the data matrix. A large column space means that there is little linear dependency between the features, which is generally good. The null space contains “novel” data points that cannot be formulated as linear combina‐ tions of existing data; a large null space could be problematic. (A perusal of Appen‐ dix A is highly recommended for readers who would appreciate a review on concepts such as the linear decision surface, eigen decomposition, and the fundamental sub‐ spaces of a matrix.)\n\nHow do column scaling operations affect the column space and null space of the data matrix? The answer is “Not very much.” But there is a small chance that tf-idf and ℓ2 normalization could be different. We’ll look at why now.\n\nThe null space of the data matrix can be large for a couple of reasons. First, many datasets contain data points that are very similar to one another. This means the effective row space is small compared to the number of data points in the dataset. Second, the number of features can be much larger than the number of data points. Bag-of-words is particularly good at creating giant feature spaces. In our Yelp exam‐ ple, there are 47K features in 29K reviews in the training set. Moreover, the number of distinct words usually grows with the number of documents in the dataset, so\n\nDeep Dive: What Is Happening?\n\n|\n\n73",
      "content_length": 2093,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 90,
      "content": "adding more documents would not necessarily decrease the feature-to-data ratio or reduce the null space.\n\nWith bag-of-words, the column space is relatively small compared to the number of features. There could be words that appear roughly the same number of times in the same documents. This would lead to the corresponding column vectors being nearly linearly dependent, which leads to the column space being not as full rank as it could be (see Appendix A for the definition of full rank). This is called a rank deficiency. (Much like how animals can be deficient in vitamins and minerals, matrices can be deficient in rank, and the output space will not be as fluffy as it should.)\n\nRank-deficient row space and column space lead to the model being overly provi‐ sioned for the problem. The linear model outfits a weight parameter for each feature in the dataset. If the row and column spaces were full rank,1 then the model would allow us to generate any target vector in the output space. When they are rank defi‐ cient, the model has more degrees of freedom than it needs. This makes it harder to pin down a solution.\n\nCan feature scaling solve the rank deficiency problem of the data matrix? Let’s take a look.\n\nThe column space is defined as the linear combination of all column vectors (bold‐ face indicates a vector): a1v1 + a2v2 + ... + anvn. Feature scaling replaces a column vec‐ tor with a constant multiple, say ˜ 1 = c1. But we can still generate the original linear combination by just replacing a1 with a˜ 1 = a1/c. It appears that feature scaling does not change the rank of the column space. Similarly, feature scaling does not affect the rank of the null space, because one can counteract the scaled feature column by reverse scaling the corresponding entry in the weight vector.\n\nHowever, as usual, there is one catch. If the scalar is 0, then there is no way to recover the original linear combination; v1 is gone. If that vector is linearly independent from all the other columns, then we’ve effectively shrunk the column space and enlarged the null space.\n\nIf that vector is not correlated with the target output, then this is effectively pruning away noisy signals, which is a good thing. This turns out to be the key difference between tf-idf and ℓ2 normalization. ℓ2 normalization would never compute a norm of zero, unless the vector contains all zeros. If the vector is close to zero, then its norm is also close to zero. Dividing by the small norm would accentuate the vector and make it longer.\n\n1 Strictly speaking, the row space and column space for a rectangular matrix cannot both be full rank. The max‐\n\nimum rank for both subspaces is the smaller of m (the number of rows) and n (the number of columns).\n\n74\n\n|\n\nChapter 4: The Effects of Feature Scaling: From Bag-of-Words to Tf-Idf",
      "content_length": 2820,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 91,
      "content": "Tf-idf, on the other hand, can generate scaling factors that are close to zero, as shown in Figure 4-2. This happens when the word is present in a large number of docu‐ ments in the training set. Such a word is likely not strongly correlated with the target vector. Pruning it away allows the solver to focus on the other directions in the col‐ umn space and find better solutions (although the improvement in accuracy will probably not be huge, because there are typically few noisy directions that are pruna‐ ble in this way).\n\nWhere feature scaling—both ℓ2 and tf-idf—does have a telling effect is on the conver‐ gence speed of the solver. This is a sign that the data matrix now has a much smaller condition number (the ratio between the largest and smallest singular values—see Appendix A for a full discussion of these terms). In fact, ℓ2 normalization makes the condition number nearly 1. But it’s not the case that the better the condition number, the better the solution. During this experiment, ℓ2 normalization converged much faster than either BoW or tf-idf. But it is also more sensitive to overfitting: it requires much more regularization and is more sensitive to the number of iterations during optimization.\n\nSummary In this chapter, we used tf-idf as an entry point into a detailed analysis of how feature transformations can affect the model (or not). Tf-idf is an example of feature scaling, so we contrasted its performance with that of another feature scaling method—ℓ2 normalization.\n\nThe results were not as one might have expected. Tf-idf and ℓ2 normalization do not improve the final classifier’s accuracy above plain bag-of-words. After acquiring some statistical modeling and linear algebra chops, we realize why: neither of them changes the column space of the data matrix.\n\nOne small difference between the two is that tf-idf can “stretch” the word count as well as “compress” it. In other words, it makes some counts bigger, and others close to zero. Therefore, tf-idf could altogether eliminate uninformative words.\n\nAlong the way, we also discovered another effect of feature scaling: it improves the condition number of the data matrix, making linear models much faster to train. Both ℓ2 normalization and tf-idf have this effect.\n\nTo summarize, the lesson is: the right feature scaling can be helpful for classification. The right scaling accentuates the informative words and downweights the common words. It can also improve the condition number of the data matrix. The right scaling is not necessarily uniform column scaling.\n\nThis story is a wonderful illustration of the difficulty of analyzing the effects of fea‐ ture engineering in the general case. Changing the features affects the training\n\nSummary\n\n|\n\n75",
      "content_length": 2751,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 92,
      "content": "process and the models that ensue. Linear models are the simplest models to under‐ stand, yet it still takes very careful experimentation methodology and a lot of deep mathematical knowledge to tease apart the theoretical and practical impacts. This would be mostly impossible with more complicated models or feature transforma‐ tions.\n\nBibliography Zheng, Alice. Evaluating Machine Learning Models. Sebastopol, CA: O’Reilly Media, 2015.\n\n76\n\n|\n\nChapter 4: The Effects of Feature Scaling: From Bag-of-Words to Tf-Idf",
      "content_length": 516,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 93,
      "content": "CHAPTER 5 Categorical Variables: Counting Eggs in the Age of Robotic Chickens\n\nA categorical variable, as the name suggests, is used to represent categories or labels. For instance, a categorical variable could represent major cities in the world, the four seasons in a year, or the industry (oil, travel, technology) of a company. The number of category values is always finite in a real-world dataset. The values may be repre‐ sented numerically. However, unlike other numeric variables, the values of a catego‐ rical variable cannot be ordered with respect to one another. (Oil is neither greater than nor less than travel as an industry type.) They are called nonordinal.\n\nA simple question can serve as litmus test for whether something should be a catego‐ rical variable: “Does it matter how different two values are, or only that they are dif‐ ferent?” A stock price of $500 is five times higher than a price of $100. So, stock price should be represented by a continuous numeric variable. The industry of the com‐ pany (oil, travel, tech, etc.), on the other hand, should probably be categorical.\n\nLarge categorical variables are particularly common in transactional records. For instance, many web services track users using an ID, which is a categorical variable with hundreds to hundreds of millions of values, depending on the number of unique users of the service. The IP address of an internet transaction is another example of a large categorical variable. They are categorical variables because, even though user IDs and IP addresses are numeric, their magnitude is usually not relevant to the task at hand. For instance, the IP address might be relevant when doing fraud detection on individual transactions—some IP addresses or subnets may generate more frau‐ dulent transactions than others. But a subnet of 164.203.x.x is not inherently more fraudulent than 164.202.x.x; the numeric value of the subnet does not matter.\n\nThe vocabulary of a document corpus can be interpreted as a large categorical vari‐ able, with the categories being unique words. It can be computationally expensive to\n\n77",
      "content_length": 2113,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 94,
      "content": "represent so many distinct categories. If a category (e.g., word) appears multiple times in a data point (document), then we can represent it as a count, and represent all of the categories through their count statistics. This is called bin counting. We start this discussion with common representations of categorical variables, and eventually meander our way to a discussion of bin counting for large categorical variables, which are very common in modern datasets.\n\nEncoding Categorical Variables The categories of a categorical variable are usually not numeric.1 For example, eye color can be “black,” “blue,” “brown,” etc. Thus, an encoding method is needed to turn these nonnumeric categories into numbers. It is tempting to simply assign an integer, say from 1 to k, to each of k possible categories—but the resulting values would be orderable against each other, which should not be permissible for cate‐ gories. So, let’s look at some alternatives.\n\nOne-Hot Encoding A better method is to use a group of bits. Each bit represents a possible category. If the variable cannot belong to multiple categories at once, then only one bit in the group can be “on.” This is called one-hot encoding, and it is implemented in scikit- learn as sklearn.preprocessing.OneHotEncoder. Each of the bits is a feature. Thus, a categorical variable with k possible categories is encoded as a feature vector of length k. Table 5-1 shows an example.\n\nTable 5-1. One-hot encoding of a category of three cities\n\nSan Francisco\n\ne1 1\n\ne2 0\n\ne3 0\n\nNew York\n\n0\n\n1\n\n0\n\nSeattle\n\n0\n\n0\n\n1\n\nOne-hot encoding is very simple to understand, but it uses one more bit than is strictly necessary. If we see that k–1 of the bits are 0, then the last bit must be 1 because the variable must take on one of the k values. Mathematically, one can write this constraint as “the sum of all bits must be equal to 1”:\n\ne1 + e2 + ... + ek = 1\n\n1 In standard statistics literature, the technical term for the categories is levels. A categorical variable with two\n\ndistinct categories has two levels. But there are a number of other things in statistics that are also called levels, so we do not use that terminology here; instead we use the more colloquial and unambiguous term “cate‐ gories.”\n\n78\n\n|\n\nChapter 5: Categorical Variables: Counting Eggs in the Age of Robotic Chickens",
      "content_length": 2339,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 95,
      "content": "Thus, we have a linear dependency on our hands. Linear dependent features, as we discovered in Chapter 4, are slightly annoying because they mean that the trained lin‐ ear models will not be unique. Different linear combinations of the features can make the same predictions, so we would need to jump through extra hoops to understand the effect of a feature on the prediction.\n\nDummy Coding The problem with one-hot encoding is that it allows for k degrees of freedom, while the variable itself needs only k–1. Dummy coding2 removes the extra degree of free‐ dom by using only k–1 features in the representation (see Table 5-2). One feature is thrown under the bus and represented by the vector of all zeros. This is known as the reference category. Dummy coding and one-hot encoding are both implemented in Pandas as pandas.get_dummies.\n\nTable 5-2. Dummy coding of a category of three cities\n\nSan Francisco\n\ne1 1\n\ne2 0\n\nNew York\n\n0\n\n1\n\nSeattle\n\n0\n\n0\n\nThe outcome of modeling with dummy coding is more interpretable than with one- hot encoding. This is easy to see in a simple linear regression problem. Suppose we have some data about apartment rental prices in three cities: San Francisco, New York, and Seattle (see Table 5-3).\n\nTable 5-3. Toy dataset of apartment prices in three cities\n\n0\n\nCity SF\n\nRent 3999\n\n1\n\nSF\n\n4000\n\n2\n\nSF\n\n4001\n\n3\n\nNYC\n\n3499\n\n4\n\nNYC\n\n3500\n\n5\n\nNYC\n\n3501\n\n6\n\nSeattle\n\n2499\n\n2 Curious readers might wonder why one is called coding and the other encoding. This is largely convention.\n\nMy guess is that one-hot encoding first became popular in electrical engineering, where information is enco‐ ded and decoded all the time. Dummy coding and effect coding, on the other hand, were invented in the sta‐ tistics community. Somehow the “en” didn’t make its way over the academic divide.\n\nEncoding Categorical Variables\n\n|\n\n79",
      "content_length": 1847,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 96,
      "content": "7\n\nCity Seattle\n\nRent 2500\n\n8\n\nSeattle\n\n2501\n\nWe can train a linear regressor to predict rental price based solely on the identity of the city (see Example 5-1).\n\nThe linear regression model can be written as:\n\ny = w1x1 + ... + wnxn\n\nIt is customary to fit an extra constant term called the intercept, so that y can be a nonzero value when the x’s are zeros:\n\ny = w1x1 + ... + wnxn + b\n\nExample 5-1. Linear regression on a categorical variable using one-hot and dummy codes\n\n>>> import pandas >>> from sklearn import linear_model\n\n# Define a toy dataset of apartment rental prices in # New York, San Francisco, and Seattle >>> df = pd.DataFrame({ ... 'City': ['SF', 'SF', 'SF', 'NYC', 'NYC', 'NYC', ... 'Seattle', 'Seattle', 'Seattle'], ... 'Rent': [3999, 4000, 4001, 3499, 3500, 3501, 2499, 2500, 2501] ... }) >>> df['Rent'].mean() 3333.3333333333335\n\n# Convert the categorical variables in the DataFrame to one-hot encoding # and fit a linear regression model >>> one_hot_df = pd.get_dummies(df, prefix=['city']) >>> one_hot_df Rent city_NYC city_SF city_Seattle 0 3999 0.0 1.0 0.0 1 4000 0.0 1.0 0.0 2 4001 0.0 1.0 0.0 3 3499 1.0 0.0 0.0 4 3500 1.0 0.0 0.0 5 3501 1.0 0.0 0.0 6 2499 0.0 0.0 1.0 7 2500 0.0 0.0 1.0 8 2501 0.0 0.0 1.0\n\n>>> model = linear_regression.LinearRegression()\n\n80\n\n|\n\nChapter 5: Categorical Variables: Counting Eggs in the Age of Robotic Chickens",
      "content_length": 1372,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 97,
      "content": ">>> model.fit(one_hot_df[['city_NYC', 'city_SF', 'city_Seattle']], ... one_hot_df['Rent']) >>> model.coef_ array([ 166.66666667, 666.66666667, -833.33333333]) >>> model.intercept_ 3333.3333333333335\n\n# Train a linear regression model on dummy code # Specify the 'drop_first' flag to get dummy coding >>> dummy_df = pd.get_dummies(df, prefix=['city'], drop_first=True) >>> dummy_df Rent city_SF city_Seattle 0 3999 1.0 0.0 1 4000 1.0 0.0 2 4001 1.0 0.0 3 3499 0.0 0.0 4 3500 0.0 0.0 5 3501 0.0 0.0 6 2499 0.0 1.0 7 2500 0.0 1.0 8 2501 0.0 1.0\n\n>>> model.fit(dummy_df[['city_SF', 'city_Seattle']], dummy_df['Rent']) >>> model.coef_ array([ 500., -1000.]) >>> model.intercept_ 3500.0\n\nWith one-hot encoding, the intercept term represents the global mean of the target variable, Rent, and each of the linear coefficients represents how much that city’s average rent differs from the global mean.\n\nWith dummy coding, the bias coefficient represents the mean value of the response variable y for the reference category, which in the example is the city NYC. The coef‐ ficient for the ith feature is equal to the difference between the mean response value for the ith category and the mean of the reference category.\n\nYou can see pretty clearly in Table 5-4 how these methods produce very different coefficients for linear models.\n\nTable 5-4. Linear regression learned coefficients\n\nx1\n\nOne-hot encoding 166.67 Dummy coding\n\n0\n\nx3 x2 666.67 –833.33\n\n500\n\n–1000\n\nb 3333.33\n\n3500\n\nEncoding Categorical Variables\n\n|\n\n81",
      "content_length": 1509,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 98,
      "content": "Effect Coding Yet another variant of categorical variable encoding is effect coding. Effect coding is very similar to dummy coding, with the difference that the reference category is now represented by the vector of all –1’s.\n\nTable 5-5. Effect coding of a categorical variable representing three cities\n\ne1 San Francisco 1 0 New York\n\ne2 0\n\n1\n\nSeattle\n\n–1 –1\n\nEffect coding is very similar to dummy coding, but results in linear regression models that are even simpler to interpret. Example 5-2 demonstrates what happens with effect coding as input. The intercept term represents the global mean of the target variable, and the individual coefficients indicate how much the means of the individ‐ ual categories differ from the global mean. (This is called the main effect of the cate‐ gory or level, hence the name “effect coding.”) One-hot encoding actually came up with the same intercept and coefficients, but in that case there are linear coefficients for each city. In effect coding, no single feature represents the reference category, so the effect of the reference category needs to be separately computed as the negative sum of the coefficients of all other categories. (See “FAQ: What is effect coding?” on the UCLA IDRE website for more details.)\n\nExample 5-2. Linear regression with effect coding\n\n>>> effect_df = dummy_df.copy() >>> effect_df.ix[3:5, ['city_SF', 'city_Seattle']] = -1.0 >>> effect_df Rent city_SF city_Seattle 0 3999 1.0 0.0 1 4000 1.0 0.0 2 4001 1.0 0.0 3 3499 -1.0 -1.0 4 3500 -1.0 -1.0 5 3501 -1.0 -1.0 6 2499 0.0 1.0 7 2500 0.0 1.0 8 2501 0.0 1.0\n\n>>> model.fit(effect_df[['city_SF', 'city_Seattle']], effect_df['Rent']) >>> model.coef_ array([ 666.66666667, -833.33333333]) >>> model.intercept_ 3333.3333333333335\n\n82\n\n|\n\nChapter 5: Categorical Variables: Counting Eggs in the Age of Robotic Chickens",
      "content_length": 1836,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 99,
      "content": "Pros and Cons of Categorical Variable Encodings One-hot, dummy, and effect coding are very similar to one another. They each have pros and cons. One-hot encoding is redundant, which allows for multiple valid mod‐ els for the same problem. The nonuniqueness is sometimes problematic for interpre‐ tation, but the advantage is that each feature clearly corresponds to a category. Moreover, missing data can be encoded as the all-zeros vector, and the output should be the overall mean of the target variable.\n\nDummy coding and effect coding are not redundant. They give rise to unique and interpretable models. The downside of dummy coding is that it cannot easily handle missing data, since the all-zeros vector is already mapped to the reference category. It also encodes the effect of each category relative to the reference category, which may look strange.\n\nEffect coding avoids this problem by using a different code for the reference category, but the vector of all –1’s is a dense vector, which is expensive for both storage and computation. For this reason, popular ML software packages such as Pandas and scikit-learn have opted for dummy coding or one-hot encoding instead of effect coding.\n\nAll three encoding techniques break down when the number of categories becomes very large. Different strategies are needed to handle extremely large categorical variables.\n\nDealing with Large Categorical Variables Automated data collection on the internet can generate large categorical variables. This is common in applications such as targeted advertising and fraud detection.\n\nIn targeted advertising, the task is to match a user with a set of ads. Features include the user ID, the website domain for the ad, the search query, the current page, and all possible pairwise conjunctions of those features. (The query is a text string that can be chopped up and turned into the usual text features. However, queries are generally short and are often composed of phrases, so the best course of action in this case is usually to keep them intact, or pass them through a hash function to make storage and comparisons easier. We will discuss hashing in more detail later.) Each of these is a very large categorical variable. The challenge is to find a good feature representa‐ tion that is memory efficient, yet produces accurate models that are fast to train.\n\nExisting solutions can be categorized (haha) thus:\n\n1. Do nothing fancy with the encoding. Use a simple model that is cheap to train. Feed one-hot encoding into a linear model (logistic regression or linear support vector machine) on lots of machines.\n\nDealing with Large Categorical Variables\n\n|\n\n83",
      "content_length": 2659,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 100,
      "content": "2. Compress the features. There are two choices:\n\na. Feature hashing, popular with linear models b. Bin counting, popular with linear models as well as trees\n\nUsing the vanilla one-hot encoding is a valid option. For Microsoft’s search advertis‐ ing engine, Graepel et al. (2010) report using such binary-valued features in a Baye‐ sian probit regression model that can be trained online using simple updates. Meanwhile, other groups argue for the compression approach. Researchers from Yahoo! swear by feature hashing (Weinberger et al., 2009), though McMahan et al. (2013) experimented with feature hashing on Google’s advertising engine and did not find significant improvements. Yet other folks at Microsoft are taken with the idea of bin counting (Bilenko, 2015).\n\nAs we shall see, all of these ideas have pros and cons. We will first describe the solu‐ tions themselves, then discuss their trade-offs.\n\nFeature Hashing A hash function is a deterministic function that maps a potentially unbounded inte‐ ger to a finite integer range [1, m]. Since the input domain is potentially larger than the output range, multiple numbers may get mapped to the same output. This is called a collision. A uniform hash function ensures that roughly the same number of numbers are mapped into each of the m bins.\n\nVisually, we can think of a hash function as a machine that intakes numbered balls (keys) and routes them to one of m bins. Balls with the same number will always get routed to the same bin (see Figure 5-1). This maintains the feature space while reduc‐ ing the storage and processing time during machine learning training and evaluation cycles.\n\nHash functions can be constructed for any object that can be represented numerically (which is true for any data that can be stored on a computer): numbers, strings, com‐ plex structures, etc.\n\n84\n\n|\n\nChapter 5: Categorical Variables: Counting Eggs in the Age of Robotic Chickens",
      "content_length": 1930,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 101,
      "content": "Figure 5-1. Hash functions map keys to bins\n\nWhen there are very many features, storing the feature vector could take up a lot of space. Feature hashing compresses the original feature vector into an m-dimensional vector by applying a hash function to the feature ID, as shown in Example 5-3. For instance, if the original features were words in a document, then the hashed version would have a fixed vocabulary size of m, no matter how many unique words there are in the input.\n\nExample 5-3. Feature hashing for word features\n\n>>> def hash_features(word_list, m): ... output = [0] * m ... for word in word_list: ... index = hash_fcn(word) % m ... output[index] += 1 ... return output\n\nAnother variation of feature hashing adds a sign component, so that counts are either added to or subtracted from the hashed bin (see Example 5-4). Statistically speaking, this ensures that the inner products between hashed features are equal in expectation to those of the original features.\n\nExample 5-4. Signed feature hashing\n\n>>> def hash_features(word_list, m): ... output = [0] * m\n\nDealing with Large Categorical Variables\n\n|\n\n85",
      "content_length": 1123,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 102,
      "content": "... for word in word_list: ... index = hash_fcn(word) % m ... sign_bit = sign_hash(word) % 2 ... if (sign_bit == 0): ... output[index] -= 1 ... else: ... output[index] += 1 ... return output\n\nThe value of the inner product after hashing is within O( 1 m) of the original inner product, so the size of the hash table m can be selected based on acceptable errors. In practice, picking the right m could take some trial and error.\n\nFeature hashing can be used for models that involve the inner product of feature vec‐ tors and coefficients, such as linear models and kernel methods. It has been demon‐ strated to be successful in the task of spam filtering (Weinberger et al., 2009). In the case of targeted advertising, McMahan et al. (2013) report not being able to get the prediction errors down to an acceptable level unless m is on the order of billions, which does not constitute enough saving in space.\n\nOne downside to feature hashing is that the hashed features, being aggregates of orig‐ inal features, are no longer interpretable.\n\nIn Example 5-5, we use the Yelp reviews dataset to demonstrate storage and inter‐ pretability trade-offs using scikit-learn’s FeatureHasher.\n\nExample 5-5. Feature hashing (a.k.a. “the hashing trick”)\n\n>>> import pandas as pd >>> import json\n\n# Load the first 10,000 reviews >>> f = open('yelp_academic_dataset_review.json') >>> js = [] >>> for i in range(10000): ... js.append(json.loads(f.readline())) >>> f.close() >>> review_df = pd.DataFrame(js)\n\n# Define m as equal to the unique number of business_ids >>> m = len(review_df.business_id.unique()) >>> m 528\n\n>>> from sklearn.feature_extraction import FeatureHasher >>> h = FeatureHasher(n_features=m, input_type='string') >>> f = h.transform(review_df['business_id'])\n\n# How does this affect feature interpretability?\n\n86\n\n|\n\nChapter 5: Categorical Variables: Counting Eggs in the Age of Robotic Chickens",
      "content_length": 1899,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 103,
      "content": ">>> review_df['business_id'].unique().tolist()[0:5] ['vcNAWiLM4dR7D2nwwJ7nCA', 'UsFtqoBl7naz8AVUBZMjQQ', 'cE27W9VPgO88Qxe4ol6y_g', 'HZdLhv6COCleJMo7nPl-RA', 'mVHrayjG3uZ_RLHkLj-AMg']\n\n>>> f.toarray() array([[ 0., 0., 0., ..., 0., 0., 0.], [ 0., 0., 0., ..., 0., 0., 0.], [ 0., 0., 0., ..., 0., 0., 0.], ..., [ 0., 0., 0., ..., 0., 0., 0.], [ 0., 0., 0., ..., 0., 0., 0.], [ 0., 0., 0., ..., 0., 0., 0.]])\n\n# Not great. BUT, let's see the storage size of our features. >>> from sys import getsizeof >>> print('Our pandas Series, in bytes: ', getsizeof(review_df['business_id'])) >>> print('Our hashed numpy array, in bytes: ', getsizeof(f)) Our pandas Series, in bytes: 790104 Our hashed numpy array, in bytes: 56\n\nWe can clearly see how using feature hashing will benefit us computationally, sacri‐ ficing immediate user interpretability. This is an easy trade-off to accept when pro‐ gressing from data exploration and visualization into a machine learning pipeline for large datasets.\n\nBin Counting Bin counting is one of the perennial rediscoveries in machine learning. It has been reinvented and used in a variety of applications, from ad click-through rate predic‐ tion to hardware branch prediction (Yeh and Patt, 1991; Lee et al., 1998; Chen et al., 2009; Li et al., 2010). Yet because it is a feature engineering technique and not a mod‐ eling or optimization method, there is no research paper on the topic. The most detailed description of the technique can be found in Misha Bilenko’s (2015) blog post “Big Learning Made Easy—with Counts!” and the associated slides.\n\nThe idea of bin counting is deviously simple: rather than using the value of the cate‐ gorical variable as the feature, instead use the conditional probability of the target under that value. In other words, instead of encoding the identity of the categorical value, we compute the association statistics between that value and the target that we wish to predict. For those familiar with naive Bayes classifiers, this statistic should ring a bell, because it is the conditional probability of the class under the assumption that all features are independent. It is best illustrated with an example (see Table 5-6).\n\nDealing with Large Categorical Variables\n\n|\n\n87",
      "content_length": 2242,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 104,
      "content": "Table 5-6. Example of bin-counting features (reproduced from “Big Learning Made Easy— with Counts!” with permission)\n\nUser\n\nNumber of clicks\n\nAlice 5\n\nBob\n\n20\n\n… 2 Joe\n\nNumber of nonclicks 120\n\n230\n\n3\n\nProbability of click 0.0400\n\n0.0800\n\n0.400\n\nQueryHash, AdDomain 0x598fd4fe, foo.com 0x50fa3cc0, bar.org … 0x437a45e1, qux.net\n\nNumber of clicks 5,000\n\n100\n\n6\n\nNumber of nonclicks 30,000\n\n900\n\n18\n\nProbability of click 0.167\n\n0.100\n\n… 0.250\n\nBin counting assumes that historical data is available for computing the statistics. Table 5-6 contains aggregated historical counts for each possible value of the catego‐ rical variables. Based on the number of times the user “Alice” has clicked on any ad and the number of times she has not clicked, we can calculate the probability of her clicking on any ad. Similarly, we can compute the probability of a click for any query–ad domain combination. At training time, every time we see “Alice,” we can use her probability of click as the input feature to the model. The same goes for QueryHash–AdDomain pairs like “0x437a45e1, qux.net.”\n\nSuppose there were 10,000 users. One-hot encoding would generate a sparse vector of length 10,000, with a single 1 in the column that corresponds to the value of the cur‐ rent data point. Bin counting would encode all 10,000 binary columns as a single fea‐ ture with a real value between 0 and 1.\n\nWe can include other features in addition to the historical click-through probability: the raw counts themselves (number of clicks and nonclicks), the log-odds ratio, or any other derivatives of probability. Our example here is for predicting ad click- through rates, but the technique readily applies to general binary classification. It can also be readily extended to multiclass classification using the usual techniques to extend binary classifiers to multiclass; i.e., via one-against-many odds ratios or other multiclass label encodings.\n\nOdds Ratio and Log Odds Ratio for Bin Counting The odds ratio is usually defined between two binary variables. It looks at their strength of association by asking the question, “How much more likely is it for Y to be true when X is true?” For instance, we might ask, “How much more likely is Alice to click on an ad than the general population?” Here, X is the binary variable “Alice is the current user,” and Y is the variable “click on ad or not.” The computation uses what’s called the two-way contingency table (basically, four numbers that correspond to the four possible combinations of X and Y), as seen in Table 5-7.\n\n88\n\n|\n\nChapter 5: Categorical Variables: Counting Eggs in the Age of Robotic Chickens",
      "content_length": 2636,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 105,
      "content": "Table 5-7. Contingency table for ad click and user\n\nClick 5\n\nAlice Not Alice 995 Total\n\n1,000\n\nNonclick 120\n\n18,880\n\n19,000\n\nTotal 125\n\n19,875\n\n20,000\n\nGiven an input variable X and a target variable Y, the odds ratio is defined as:\n\nodds ratio =\n\nP(Y = 1 | X = 1)/ P(Y = 0 | X = 1) P(Y = 1 | X = 0)/ P(Y = 0 | X = 0)\n\nIn our example, this translates as the ratio between “how much more likely is it that Alice clicks on an ad rather than does not click” and “how much more likely is it that other people click rather than not click.” The number, in this case, is:\n\nodds ratio (user, ad click) =\n\n(5/125)/(120/125) (995/19,875)/(18,880/19,875)\n\n= 0.7906\n\nMore simply, we can just look at the numerator, which examines how much more likely it is that a single user (Alice) clicks on an ad versus not clicking. This is suitable for large categorical variables with many values, not just two:\n\nodds ratio (Alice, ad click) =\n\n5/125 120/125\n\n= 0.04166\n\nProbability ratios can easily become very small or very large. (For instance, there will be users who almost never click on ads, and perhaps users who click on ads much more frequently than not.) The log transform again comes to our rescue. Another useful property of the logarithm is that it turns a division into a subtraction:\n\nlog-odds ratio (Alice, ad click) = log( 5\n\n125) – log( 120\n\n125) = – 3.178\n\nIn short, bin counting converts a categorical variable into statistics about the value. It turns a large, sparse, binary representation of the categorical variable, such as that produced by one-hot encoding, into a very small, dense, real-valued numeric repre‐ sentation (Figure 5-2).\n\nDealing with Large Categorical Variables\n\n|\n\n89",
      "content_length": 1689,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 106,
      "content": "Figure 5-2. An illustration of one-hot encoding versus bin-counting statistics for catego‐ rical variables\n\nIn terms of implementation, bin counting requires storing a map between each cate‐ gory and its associated counts. (The rest of the statistics can be derived on the fly from the raw counts.) Hence it requires O(k) space, where k is the number of unique values of the categorical variable.\n\nTo illustrate bin counting in practice, we’ll use data from a Kaggle competition hos‐ ted by Avazu. Here are some relevant statistics about the dataset:\n\nThere are 24 variables, including click, a binary click/no click counter, and device_id, which tracks which device an ad was displayed on.\n\nThe full dataset contains 40,428,967 observations, with 2,686,408 unique devices.\n\nThe aim of the Avazu competition was to predict click-through rate using ad data, but we will use the dataset to demonstrate how bin counting can greatly reduce the feature space for large amounts of streaming data (see Example 5-6).\n\nExample 5-6. Bin-counting example\n\n>>> import pandas as pd\n\n# train_subset data is first 10K rows of 6+GB set >>> df = pd.read_csv('data/train_subset.csv')\n\n# How many unique features should we have after? >>> len(df['device_id'].unique()) 7201\n\n# For each category, we want to calculate: # Theta = [counts, p(click), p(no click), p(click)/p(no click)]\n\n>>> def click_counting(x, bin_column): ... clicks = pd.Series(x[x['click'] > 0][bin_column].value_counts(),\n\n90\n\n|\n\nChapter 5: Categorical Variables: Counting Eggs in the Age of Robotic Chickens",
      "content_length": 1558,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 107,
      "content": "... name='clicks') ... no_clicks = pd.Series(x[x['click'] < 1][bin_column].value_counts(), ... name='no_clicks')\n\n... counts = pd.DataFrame([clicks,no_clicks]).T.fillna('0') ... counts['total_clicks'] = counts['clicks'].astype('int64') + ... counts['no_clicks'].astype('int64') ... return counts\n\n>>> def bin_counting(counts): ... counts['N+'] = counts['clicks'] ... .astype('int64') ... .divide(counts['total_clicks'].astype('int64')) ... counts['N-'] = counts['no_clicks'] ... .astype('int64') ... .divide(counts['total_clicks'].astype('int64')) ... counts['log_N+'] = counts['N+'].divide(counts['N-']) ... # If we wanted to only return bin-counting properties, ... # we would filter here ... bin_counts = counts.filter(items= ['N+', 'N-', 'log_N+']) ... return counts, bin_counts\n\n# Bin counts example: device_id >>> bin_column = 'device_id' >>> device_clicks = click_counting(df.filter(items=[bin_column, 'click']), ... bin_column) >>> device_all, device_bin_counts = bin_counting(device_clicks)\n\n# Check to make sure we have all the devices >>> len(device_bin_counts) 7201\n\n>>> device_all.sort_values(by = 'total_clicks', ascending=False).head(4)\n\nclicks no_clicks total N+ N- log_N+ a99f214a 15729 71206 86935 0.180928 0.819072 0.220894 c357dbff 33 134 167 0.197605 0.802395 0.246269 31da1bd0 0 62 62 0.000000 1.000000 0.000000 936e92fb 5 54 59 0.084746 0.915254 0.092593\n\nWhat about rare categories?\n\nJust like rare words, rare categories require special treatment. Think about a user who logs in once a year: there will be very little data to reliably estimate that user’s click- through rate for ads. Moreover, rare categories waste space in the counts table.\n\nOne way to deal with this is through back-off, a simple technique that accumulates the counts of all rare categories in a special bin (see Figure 5-3). If the count is greater than a certain threshold, then the category gets its own count statistics. Otherwise, we use the statistics from the back-off bin. This essentially reverts the statistics for a sin‐ gle rare category to the statistics computed on all rare categories. When using the\n\nDealing with Large Categorical Variables\n\n|\n\n91",
      "content_length": 2160,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 108,
      "content": "back-off method, it helps to also add a binary indicator for whether or not the statis‐ tics come from the back-off bin.\n\nFigure 5-3. If a rare category gains counts, it can move above the threshold for the back-off bin, using its own count statistics for modeling\n\nThere is another way to deal with this problem, called the count-min sketch (Cor‐ mode and Muthukrishnan, 2005). In this method, all the categories, rare or frequent alike, are mapped through multiple hash functions with an output range, m, much smaller than the number of categories, k. When retrieving a statistic, recompute all the hashes of the category and return the smallest statistic. Having multiple hash functions mitigates the probability of collision within a single hash function. The scheme works because the number of hash functions times m, the size of the hash table, can be made smaller than k, the number of categories, and still retain low over‐ all collision probability.\n\nFigure 5-4 illustrates. Each item i is mapped to one cell in each row of the array of counts. When an update of ct to item it arrives, ct is added to each of these cells, hashed using functions h1…hd.\n\nFigure 5-4. The count-min sketch\n\n92\n\n|\n\nChapter 5: Categorical Variables: Counting Eggs in the Age of Robotic Chickens",
      "content_length": 1281,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 109,
      "content": "Guarding against data leakage\n\nSince bin counting relies on historical data to generate the necessary statistics, it requires waiting through a data collection period, incurring a slight delay in the learning pipeline. Also, when the data distribution changes, the counts need to be updated. The faster the data changes, the more frequently the counts need to be recomputed. This is particularly important for applications like targeted advertising, where user preferences and popular queries change very quickly, and lack of adapta‐ tion to the current distribution could mean huge losses for the advertising platform.\n\nOne might ask, why not use the same dataset to compute the relevant statistics and train the model? The idea seems innocent enough. The big problem here is that the statistics involve the target variable, which is what the model tries to predict. Using the output to compute the input features leads to a pernicious problem known as leakage. In short, leakage means that information is revealed to the model that gives it an unrealistic advantage to make better predictions. This could happen when test data is leaked into the training set, or when data from the future is leaked to the past. Any time that a model is given information that it shouldn’t have access to when it is making predictions in real time in production, there is leakage. Kaggle’s wiki gives more examples of leakage and why it is bad for machine learning applications.\n\nIf the bin-counting procedure used the current data point’s label to compute part of the input statistic, that would constitute direct leakage. One way to prevent that is by instituting strict separation between count collection (for computing bin-count sta‐ tistics) and training, as illustrated in Figure 5-5—i.e., use an earlier batch of data points for counting, use the current data points for training (mapping categorical variables to historical statistics we just collected), and use future data points for test‐ ing. This fixes the problem of leakage, but introduces the aforementioned delay (the input statistics and therefore the model will trail behind current data).\n\nFigure 5-5. Using time windows can prevent data leakage during bin counting\n\nIt turns out that there is another solution, based on differential privacy. A statistic is approximately leakage-proof if its distribution stays roughly the same with or without any one data point. In practice, adding a small random noise with distribution Lap‐ lace(0,1) is sufficient to cover up any potential leakage from a single data point. This\n\nDealing with Large Categorical Variables\n\n|\n\n93",
      "content_length": 2622,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 110,
      "content": "idea can be combined with leaving-one-out counting to formulate statistics on cur‐ rent data (Zhang, 2015).\n\nCounts without bounds\n\nIf the statistics are updated continuously given more and more historical data, the raw counts will grow without bounds. This could be a problem for the model. A trained model “knows” the input data up to the observed scale. A trained decision tree might say, “When x is greater than 3, predict 1.” A trained linear model might say, “Multiply x by 0.7 and see if the result is greater than the global average.” These might be the correct decisions when x lies between 0 and 5. But what happens beyond that? No one knows.\n\nWhen the input counts increase, the model will need to be retrained to adapt to the current scale. If the counts accumulate rather slowly, then the effective scale won’t change too fast, and the model will not need to be retrained too frequently. But when counts increment very quickly, frequent retraining will be a nuisance.\n\nFor this reason, it is often better to use normalized counts that are guaranteed to be bounded in a known interval. For instance, the estimated click-through probability is bounded between [0, 1]. Another method is to take the log transform, which imposes a strict bound, but the rate of increase will be very slow when the count is very large.\n\nNeither method will guard against shifting input distributions (e.g., last year’s Barbie dolls are now out of style and people will no longer click on those ads). The model will need to be retrained to accommodate these more fundamental changes in input data distribution, or the whole pipeline will need to move to an online learning set‐ ting where the model is continuously adapting to the input.\n\nSummary Each of the approaches detailed in this chapter has its pros and cons. Here is a run‐ down of the trade-offs.\n\nPlain one-hot encoding\n\nO(n) using the sparse vector format, where n is the number of data points\n\nSpace requirement Computation requirement O(nk) under a linear model, where k is the number of categories Pros\n\nEasiest to implement • Potentially most accurate • Feasible for online learning\n\n94\n\n|\n\nChapter 5: Categorical Variables: Counting Eggs in the Age of Robotic Chickens",
      "content_length": 2225,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 111,
      "content": "Plain one-hot encoding\n\nCons\n\nComputationally inefficient • Does not adapt to growing categories • Not feasible for anything other than linear models • Requires large-scale distributed optimization with truly large datasets\n\nFeature hashing\n\nO(n) using the sparse matrix format, where n is the number of data points Space requirement Computation requirement O(nm) under a linear or kernel model, where m is the number of hash bins Pros\n\nEasy to implement • Makes model training cheaper • Easily adaptable to new categories • Easily handles rare categories • Feasible for online learning\n\nCons\n\nOnly suitable for linear or kernelized models • Hashed features not interpretable • Mixed reports of accuracy\n\nBin-counting\n\nSpace requirement\n\nO(n+k) for small, dense representation of each data point, plus the count statistics that must be kept for each category\n\nComputation requirement O(n) for linear models; also usable for nonlinear models such as trees Pros\n\nSmallest computational burden at training time • Enables tree-based models • Relatively easy to adapt to new categories • Handles rare categories with back-off or count-min sketch • Interpretable\n\nCons\n\nRequires historical data • Delayed updates required, not completely suitable for online learning • Higher potential for leakage\n\nAs we can see, none of the methods are perfect. Which one to use depends on the desired model. Linear models are cheaper to train and therefore can handle noncom‐ pressed representations such as one-hot encoding. Tree-based models, on the other hand, need to do repeated searches over all features for the right split, and are thus limited to small representations such as bin counting. Feature hashing sits in between those two extremes, but with mixed reports on the resulting accuracy.\n\nSummary\n\n|\n\n95",
      "content_length": 1797,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 112,
      "content": "Bibliography Agarwal, Alekh, Oliveier Chapelle, Miroslav Dudík, and John Langford. “A Reliable Effective Terascale Linear Learning System.” Journal of Machine Learning Research 15 (2015): 1111−1133.\n\nBilenko, Misha. “Big Learning Made Easy—with Counts!” Cortana Intelligence and Machine Learning Blog, February 17, 2015. https://blogs.technet.microsoft.com/machi‐ nelearning/2015/02/17/big-learning-made-easy-with-counts/.\n\nChen, Ye, Dmitry Pavlov, and John F. Canny. “Large-Scale Behavioral Targeting.” Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Dis‐ covery and Data Mining (2009): 209–218.\n\nCormode, Graham, and S. Muthukrishnan. “An Improved Data Stream Summary: The Count-Min Sketch and Its Applications.” Algorithms 55 (2005): 29–38.\n\nGraepel, Thore, Joaquin Quiñonero Candela, Thomas Borchert, and Ralf Herbrich. “Web-Scale Bayesian Click-Through Rate Prediction for Sponsored Search Advertis‐ ing in Microsoft’s Bing Search Engine.” Proceedings of the 27th International Confer‐ ence on Machine Learning (2010): 13–20.\n\nHe, Xinran, Junfeng Pan, Ou Jin, Tianbing Xu, Bo Liu, Tao Xu, Yanxin Shi, Antoine Atallah, Ralf Herbrich, Stuart Bowers, and Joaquin Quiñonero Candela. “Practical Lessons from Predicting Clicks on Ads at Facebook.” Proceedings of the 8th Interna‐ tional Workshop on Data Mining for Online Advertising (2014): 1–9.\n\nLee, Wenke, Salvatore J. Stolfo, and Kui W. Mok. 1998. “Mining Audit Data to Build Intrusion Detection Models.” Proceedings of the 4th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (1998): 66–72.\n\nLi, Wei, Xuerui Wang, Ruofei Zhang, Ying Cui, Jianchang Mao, and Rong Jin. “Exploitation and Exploration in a Performance Based Contextual Advertising Sys‐ tem.” Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (2010): 27–36.\n\nMcMahan, H. Brendan, Gary Holt, D. Sculley, Michael Young, Dietmar Ebner, Julian Grady, Lan Nie, Todd Phillips, Eugene Davydov, Daniel Golovin, Sharat Chikkerur, Dan Liu, Martin Wattenberg, Arnar Mar Hrafnkelsson, Tom Boulos, and Jeremy Kubica. “Ad Click Prediction: A View from the Trenches.” Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (2013): 1222–1230.\n\nWeinberger, Kilian, Anirban Dasgupta, Josh Attenberg, John Langford, and Alex Smola. 2009. “Feature Hashing for Large Scale Multitask Learning.” Proceedings of the 26th International Conference on Machine Learning (2009): 1113–1120.\n\n96\n\n|\n\nChapter 5: Categorical Variables: Counting Eggs in the Age of Robotic Chickens",
      "content_length": 2607,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 113,
      "content": "Yeh, Tse-Yu, and Yale N. Patt. “Two-Level Adaptive Training Branch Prediction.” Proceedings of the 24th Annual International Symposium on Microarchitecture (1991): 51–61.\n\nZhang, Owen. 2015. “Tips for data science competitions.” SlideShare presentation. Retrieved from http://bit.ly/2DjuhBD.\n\nBibliography\n\n|\n\n97",
      "content_length": 312,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 115,
      "content": "CHAPTER 6 Dimensionality Reduction: Squashing the Data Pancake with PCA\n\nWith automatic data collection and feature generation techniques, one can quickly obtain a large number of features. But not all of them are useful. In Chapters 3 and 4, we discussed frequency-based filtering and feature scaling as ways of pruning away uninformative features. Now we will take a close look at the topic of feature dimen‐ sionality reduction using principal component analysis (PCA).\n\nThis chapter marks an entry into model-based feature engineering techniques. Prior to this point, most of the techniques can be defined without referencing the data. For instance, frequency-based filtering might say, “Get rid of all counts that are smaller than n,” a procedure that can be carried out without further input from the data itself.\n\nModel-based techniques, on the other hand, require information from the data. For example, PCA is defined around the principal axes of the data. In previous chapters, there was always a clear-cut line between data, features, and models. From this point forward, the difference gets increasingly blurry. This is exactly where the excitement lies in current research on feature learning.\n\nIntuition Dimensionality reduction is about getting rid of “uninformative information” while retaining the crucial bits. There are many ways to define “uninformative.” PCA focu‐ ses on the notion of linear dependency. In “The Anatomy of a Matrix” on page 182, we describe the column space of a data matrix as the span of all feature vectors. If the column space is small compared to the total number of features, then most of the fea‐ tures are linear combinations of a few key features. Linearly dependent features are a\n\n99",
      "content_length": 1733,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 116,
      "content": "waste of space and computation power because the information could have been encoded in much fewer features. To avoid this situation, principal component analy‐ sis tries to reduce such “fluff” by squashing the data into a much lower-dimensional linear subspace.\n\nPicture the set of data points in feature space. Each data point is a dot, and the whole set of data points forms a blob. In Figure 6-1(a), the data points spread out evenly across both feature dimensions, and the blob fills the space. In this example, the col‐ umn space has full rank. However, if some of those features are linear combinations of others, then the blob won’t look so plump; it will look more like Figure 6-1(b), a flat blob where feature 1 is a duplicate (or a scalar multiple) of feature 2. In this case, we say that the intrinsic dimensionality of the blob is 1, even though it lies in a two- dimensional feature space.\n\nIn practice, things are rarely exactly equal to one another. It is more likely that we see features that are very close to being equal, but not quite. In such a case, the data blob might look something like Figure 6-1(c). It’s an emaciated blob. If we wanted to reduce the number of features to pass to the model, then we could replace feature 1 and feature 2 with a new feature, maybe called feature 1.5, which lies on the diagonal line between the original two features. The original dataset could then be adequately represented by one number—the position along the direction of feature 1.5—instead of two numbers, f1 and f2.\n\nFigure 6-1. Data blobs in feature space: (a) full-rank data blob, (b) low-dimensional data blob, and (c) approximately low-dimensional data blob\n\n100\n\n|\n\nChapter 6: Dimensionality Reduction: Squashing the Data Pancake with PCA",
      "content_length": 1760,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 117,
      "content": "The key idea here is to replace redundant features with a few new features that ade‐ quately summarize information contained in the original feature space. It’s easy to tell what the new feature should be when there are only two features. It’s much harder when the original feature space has hundreds or thousands of dimensions. We need a way to mathematically describe the new features we are looking for. Then we can use optimization techniques to find them.\n\nOne way to mathematically define “adequately summarize information” is to say that the new data blob should retain as much of the original volume as possible. We are squashing the data blob into a flat pancake, but we want the pancake to be as big as possible in the right directions. This means we need a way to measure volume.\n\nVolume has to do with distance. But the notion of distance in a blob of data points is somewhat fuzzy. One could measure the maximum distance between any two pairs of points, but that turns out to be a very difficult function to mathematically optimize. An alternative is to measure the average distance between pairs of points, or equiva‐ lently, the average distance between each point and its mean, which is the variance. This turns out to be much easier to optimize. (Life is hard. Statisticians have learned to take convenient shortcuts.) Mathematically, this translates into maximizing the variance of the data points in the new feature space.\n\nTips for Navigating Linear Algebra Formulas\n\nTo stay oriented in the world of linear algebra, keep track of which quantities are scalars, which are vectors, and which way the vectors are oriented—vertically or horizontally. Know the dimensions of your matrices, because they often tell you whether the vectors of interest are in the rows or columns. Draw the matrices and vectors as rectangles on a page and make sure the shapes match. Just as one can get far in algebra by noting the units of measurement (dis‐ tance is in miles, speed is in miles per hour), in linear algebra all one needs are the dimensions.\n\nDerivation As before, let X denote the n × d data matrix, where n is the number of data points and d the number of features. Let x be a column vector containing a single data point. (So x is the transpose of one of the rows in X.) Let v denote one of the new feature vectors, or principal components, that we are trying to find.\n\nDerivation\n\n|\n\n101",
      "content_length": 2404,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 118,
      "content": "Singular Value Decomposition (SVD) of a Matrix Any rectangular matrix can be decomposed into three matrices of particular shapes and characteristics:\n\nX = UΣVT\n\nHere, U and V are orthogonal matrices (i.e., UTU = I and VTV = I). Σ is a diagonal matrix containing the singular values of X, which can be positive, zero, or negative. Suppose X has n rows and d columns and n ≥ d. Then U has shape n × d, and Σ and V have shape d × d. (See “Singular Value Decomposition (SVD)” on page 185 for a full review of SVD and eigen decomposition of a matrix.)\n\nLinear Projection Let’s break down the derivation of PCA step by step. Figure 6-2 illustrates the whole process.\n\nFigure 6-2. Illustration of PCA: (a) original data in feature space; (b) centered data; (c) projecting a data vector x onto another vector v; (d) direction of maximum variance of the projected coordinates (equal to the principal eigenvector of XTX)\n\nPCA uses linear projection to transform data into the new feature space. Figure 6-2(c) illustrates what a linear projection looks like. When we project x onto v, the length of the projection is proportional to the inner product between the two,\n\n102\n\n|\n\nChapter 6: Dimensionality Reduction: Squashing the Data Pancake with PCA",
      "content_length": 1238,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 119,
      "content": "normalized by the norm of v (its inner product with itself). Later on, we will con‐ strain v to have unit norm. So, the only relevant part is the numerator—let’s call it v (see Equation 6-1).\n\nEquation 6-1. Projection coordinate\n\nz = xTv\n\nNote that z is a scalar, whereas x and v are column vectors. Since there are a bunch of data points, we can formulate the vector z of all of their projection coordinates on the new feature v (Equation 6-2). Here, X is the familiar data matrix where each row is a data point. The resulting z is a column vector.\n\nEquation 6-2. Vector of projection coordinates\n\nz = Xv\n\nVariance and Empirical Variance The next step is to compute the variance of the projections. Variance is defined as the expectation of the squared distance to the mean (Equation 6-3).\n\nEquation 6-3. Variance of a random variable Z\n\nVar(Z) = E[Z – E(Z)]2\n\nThere is one tiny problem: our formulation of the problem says nothing about the mean, E(Z); it is a free variable. One solution is to remove it from the equation by subtracting the mean from every data point. The resulting dataset has mean zero, which means that the variance is simply the expectation of Z2. Geometrically, sub‐ tracting the mean has the effect of centering the data. (See Figure 6-2(a-b).)\n\nA closely related quantity is the covariance between two random variables Z1 and Z2 (Equation 6-4). Think of this as the extension of the idea of variance (of a single ran‐ dom variable) to two random variables.\n\nEquation 6-4. Covariance between two random variables Z1 and Z2\n\nCov(Z1, Z2) = E[(Z1 – E(Z1)(Z2 – E(Z2)]\n\nWhen the random variables have mean zero, their covariance coincides with their linear correlation, E[Z1Z2]. We will discuss this concept more later on.\n\nDerivation\n\n|\n\n103",
      "content_length": 1763,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 120,
      "content": "Statistical quantities like variance and expectation are defined on a data distribution. In practice, we don’t have the true distribution, but only a bunch of observed data points, z1, ..., zn. This is called an empirical distribution, and it gives us an empirical estimate of the variance (Equation 6-5).\n\nEquation 6-5. Empirical variance of Z based on observations z\n\nVaremp(Z) =\n\nn\n\n1 n – 1 ∑\n\ni=1\n\nzi\n\n2\n\nPrincipal Components: First Formulation Combined with the definition of zi in Equation 6-1, we have the formulation for max‐ imizing the variance of the projected data given in Equation 6-6. (We drop the denominator n–1 from the definition of empirical variance, because it is a global con‐ stant and does not affect where the maximizing value occurs.)\n\nEquation 6-6. Objective function of principal components\n\nn\n\nmaxw∑\n\n(xi\n\nTw)2, where wTw = 1\n\ni=1\n\nThe constraint here forces the inner product of w with itself to be 1, which is equiva‐ lent to saying that the vector must have unit length. This is because we only care about the direction and not the magnitude of w. The magnitude of w is an unneces‐ sary degree of freedom, so we get rid of it by setting it to an arbitrary value.\n\nPrincipal Components: Matrix-Vector Formulation Next comes the tricky step. The sum of squares term in Equation 6-6 is rather cum‐ bersome. It’d be much cleaner in a matrix-vector format. Can we do it? The answer is yes. The key lies in the sum-of-squares identity: the sum of a bunch of squared terms is equal to the squared norm of a vector whose elements are those terms, which is equivalent to the vector’s inner product with itself. With this identity in hand, we can rewrite Equation 6-6 in matrix-vector notation, as shown in Equation 6-7.\n\nEquation 6-7. Objective function for principal components, matrix-vector formulation\n\nmaxw wTw, where wTw = 1\n\nThis formulation of PCA presents the target more clearly: we look for an input direc‐ tion that maximizes the norm of the output. Does this sound familiar? The answer lies in the singular value decomposition (SVD) of X. The optimal w, as it turns out, is\n\n104\n\n|\n\nChapter 6: Dimensionality Reduction: Squashing the Data Pancake with PCA",
      "content_length": 2192,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 121,
      "content": "the principal left singular vector of X, which is also the principal eigenvector of XTX. The projected data is called a principal component of the original data.\n\nGeneral Solution of the Principal Components This process can be repeated. Once we find the first principal component, we can rerun Equation 6-7 with the added constraint that the new vector be orthogonal to the previously found vectors (see Equation 6-8).\n\nEquation 6-8. Objective function for k+1st principal components\n\nmaxw wTw, where wTw = 1 and wTw1 = ... = wTwk = 0\n\nThe solution is the k+1st left singular vectors of X, ordered by descending singular values. Thus, the first k principal components correspond to the first k left singular vectors of X.\n\nTransforming Features Once the principal components are found, we can transform the features using linear projection. Let X = UΣVT be the SVD of X, and Vk the matrix whose columns contain the first k left singular vectors. X has dimensions n × d, where d is the number of original features, and Vk has dimensions d × k. Instead of a single projection vector as in Equation 6-2, we can simultaneously project onto multiple vectors in a projection matrix (Equation 6-9).\n\nEquation 6-9. PCA projection matrix\n\nW = Vk\n\nThe matrix of projected coordinates is easy to compute, and can be further simplified using the fact that the singular vectors are orthogonal to each other (see Equation 6-10).\n\nEquation 6-10. Simple PCA transform\n\nZ = XW = XVk = UΣVTVk = UkΣk\n\nThe projected values are simply the first k right singular vectors scaled by the first k singular values. Thus, the entire PCA solution, components and projections alike, can be conveniently obtained through the SVD of X.\n\nDerivation\n\n|\n\n105",
      "content_length": 1725,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 122,
      "content": "Implementing PCA Many derivations of PCA involve first centering the data, then taking the eigen decomposition of the covariance matrix. But the easiest way to implement PCA is by taking the singular value decomposition of the centered data matrix.\n\nPCA Implementation Steps\n\n1. Center the data matrix:\n\nC = X – 1μT\n\nwhere 1 is a column vector containing all 1s, and μ is a column vector containing the average of the rows of X.\n\n2. Compute the SVD:\n\nC = UΣVT\n\n3. Find the principal components. The first k principal components are the first k columns of V; i.e., the right singular vectors corresponding to the k largest singu‐ lar values.\n\n4. Transform the data. The transformed data is simply the first k columns of U. (If whitening is desired, then scale the vectors by the inverse singular values. This requires that the selected singular values are nonzero. See “Whitening and ZCA” on page 108.)\n\nPCA in Action Let’s get a better sense for how PCA works by applying it to some image data. The MNIST dataset contains images of handwritten digits from 0 to 9. The original images are 28 × 28 pixels. A lower-resolution subset of the images is distributed with scikit-learn, where each image is downsampled into 8 × 8 pixels. The original data in scikit-learn has 64 dimensions. In Example 6-1, we apply PCA and visualize the data‐ set using the first three principal components.\n\nExample 6-1. Principal component analysis of the scikit-learn digits dataset (a subset of the MNIST dataset)\n\n>>> from sklearn import datasets >>> from sklearn.decomposition import PCA\n\n106\n\n|\n\nChapter 6: Dimensionality Reduction: Squashing the Data Pancake with PCA",
      "content_length": 1650,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 123,
      "content": "# Load the data >>> digits_data = datasets.load_digits() >>> n = len(digits_data.images)\n\n# Each image is represented as an 8-by-8 array. # Flatten this array as input to PCA. >>> image_data = digits_data.images.reshape((n, -1)) >>> image_data.shape (1797, 64)\n\n# Groundtruth label of the number appearing in each image >>> labels = digits_data.target >>> labels array([0, 1, 2, ..., 8, 9, 8])\n\n# Fit a PCA transformer to the dataset. # The number of components is automatically chosen to account for # at least 80% of the total variance. >>> pca_transformer = PCA(n_components=0.8) >>> pca_images = pca_transformer.fit_transform(image_data) >>> pca_transformer.explained_variance_ratio_ array([ 0.14890594, 0.13618771, 0.11794594, 0.08409979, 0.05782415, 0.0491691 , 0.04315987, 0.03661373, 0.03353248, 0.03078806, 0.02372341, 0.02272697, 0.01821863]) >>> pca_transformer.explained_variance_ratio_[:3].sum() 0.40303958587675121\n\n# Visualize the results >>> import matplotlib.pyplot as plt >>> from mpl_toolkits.mplot3d import Axes3D >>> %matplotlib notebook >>> fig = plt.figure() >>> ax = fig.add_subplot(111, projection='3d') >>> for i in range(100): ... ax.scatter(pca_images[i,0], pca_images[i,1], pca_images[i,2], ... marker=r'${}$'.format(labels[i]), s=64)\n\n>>> ax.set_xlabel('Principal component 1') >>> ax.set_ylabel('Principal component 2') >>> ax.set_zlabel('Principal component 3')\n\nThe first 100 projected images are shown in a 3D plot in Figure 6-3. The markers cor‐ respond to the labels. The first three principal components account for roughly 40% of the total variance in the dataset. This is by no means perfect, but it allows for a handy low-dimensional visualization. We see that PCA groups similar numbers close to each other. The numbers 0 and 6 lie in the same region, as do 1 and 7, and 3 and 9. The space is roughly divided between 0, 4, and 6 on one side, and the rest of the num‐ bers on the other.\n\nPCA in Action\n\n|\n\n107",
      "content_length": 1949,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 124,
      "content": "Figure 6-3. PCA projections of subset of MNIST data—markers correspond to image labels\n\nSince there is a fair amount of overlap between numbers, it would be difficult to tell them apart using a linear classifier in the projected space. Hence, if the task is to clas‐ sify the handwritten digits and the chosen model is a linear classifier, then the first three principal components are not sufficient as features. Nevertheless, it is interest‐ ing to see how much of a 64-dimensional dataset can be captured in just 3 dimen‐ sions.\n\nWhitening and ZCA Due to the orthogonality constraint in the objective function, PCA transformation produces a nice side effect: the transformed features are no longer correlated. In other words, the inner products between pairs of feature vectors are zero. It’s easy to prove this using the orthogonality property of the singular vectors:\n\nZTZ = ΣkUk\n\nTUkΣk = Σk\n\n2\n\nThe result is a diagonal matrix containing squares of the singular values representing the correlation of each feature vector with itself, also known as its ℓ2 norm.\n\nSometimes, it is useful to also normalize the scale of the features to 1. In signal pro‐ cessing terms, this is known as whitening. It results in a set of features that have unit correlation with themselves and zero correlation with each other. Mathematically,\n\n108\n\n|\n\nChapter 6: Dimensionality Reduction: Squashing the Data Pancake with PCA",
      "content_length": 1410,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 125,
      "content": "whitening can done by multiplying the PCA transformation with the inverse singular values (see Equation 6-11).\n\nEquation 6-11. PCA + whitening\n\n1 Wwhite = VkΣk\n\nZwhite = XVkΣk\n\n1 = UΣVTVkΣk\n\n1 = Uk\n\nWhitening is independent from dimensionality reduction; one can perform one without the other. For example, zero-phase component analysis (ZCA) (Bell and Sej‐ nowski, 1996) is a whitening transformation that is closely related to PCA, but that does not reduce the number of features. ZCA whitening uses the full set of principal components V without reduction, and includes an extra multiplication back onto VT (Equation 6-12).\n\nEquation 6-12. ZCA whitening\n\nWZCA = VΣ-1VT\n\nZzca = XVΣ-1VT = UΣVTVΣ-1 = U\n\nSimple PCA projection (Equation 6-10) produces coordinates in the new feature space, where the principal components serve as the basis. These coordinates repre‐ sent only the length of the projected vector, not the direction. Multiplication with the principal components gives us the length and the orientation. Another valid interpre‐ tation is that the extra multiplication rotates the coordinates back into the original feature space. (V is an orthogonal matrix, and orthogonal matrices rotate their input without stretching or compression.) So, ZCA produces whitened data that is as close (in Euclidean distance) to the original data as possible.\n\nConsiderations and Limitations of PCA When using PCA for dimensionality reduction, one must address the question of how many principal components (k) to use. Like all hyperparameters, this number can be tuned based on the quality of the resulting model. But there are also heuristics that do not involve expensive computational methods.\n\nOne possibility is to pick k to account for a desired proportion of total variance. (This option is available in the scikit-learn package PCA.) The variance of the projection onto the kth component is:\n\n║Xvk║2 =║uk σk║2 = σk\n\n2\n\nConsiderations and Limitations of PCA\n\n|\n\n109",
      "content_length": 1968,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 126,
      "content": "which is the square of the kth-largest singular value of X. The ordered list of singular values of a matrix is called its spectrum. Thus, to determine how many components to use, one can perform a simple spectral analysis of the data matrix and pick the threshold that retains enough variance.\n\nSelecting k Based on Accounted Variance\n\nTo retain enough components to cover 80% of the total variance in the data, pick k such that\n\nk\n\n∑\n\nσi\n\n2\n\ni=1 d\n\n≥ 0.8.\n\n∑\n\nσi\n\n2\n\ni=1\n\nAnother method for picking k involves the intrinsic dimensionality of a dataset. This is a hazier concept, but can also be determined from the spectrum. Basically, if the spectrum contains a few large singular values and a number of tiny ones, then one can probably just harvest the largest singular values and discard the rest. Sometimes the rest of the spectrum is not tiny, but there’s a large gap between the head and the tail values. That would also be a reasonable cutoff. This method is requires visual inspection of the spectrum and hence cannot be performed as part of an automated pipeline.\n\nOne key criticism of PCA is that the transformation is fairly complex, and the results are therefore hard to interpret. The principal components and the projected vectors are real-valued and could be positive or negative. The principal components are essentially linear combinations of the (centered) rows, and the projection values are linear combinations of the columns. In a stock returns application, for instance, each factor is a linear combination of time slices of stock returns. What does that mean? It is hard to express a human-understandable reason for the learned factors. Therefore, it is hard for analysts to trust the results. If you can’t explain why you should be putting billions of other people’s money into particular stocks, you probably won’t choose to use that model.\n\nPCA is computationally expensive. It relies on SVD, which is an expensive procedure. To compute the full SVD of a matrix takes O(nd2 + d3) operations (Golub and Van Loan, 2012), assuming n ≥ d—i.e., there are more data points than features. Even if we only want k principal components, computing the truncated SVD (the k largest singular values and vectors) still takes O((n+d)2 k) = O(n2k) operations. This is pro‐ hibitive when there are a large number of data points or features.\n\nIt is difficult to perform PCA in a streaming fashion, in batch updates, or from a sam‐ ple of the full data. Streaming computation of the SVD, updating the SVD, and\n\n110\n\n|\n\nChapter 6: Dimensionality Reduction: Squashing the Data Pancake with PCA",
      "content_length": 2599,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 127,
      "content": "computing the SVD from a subsample are all difficult research problems. Algorithms exist, but at the cost of reduced accuracy. One implication is that one should expect lower representational accuracy when projecting test data onto principal components found in the training set. As the distribution of the data changes, one would have to recompute the principal components in the current dataset.\n\nLastly, it is best not to apply PCA to raw counts (word counts, music play counts, movie viewing counts, etc.). The reason for this is that such counts often contain large outliers. (The probability is pretty high that there is a fan out there who watched The Lord of the Rings 314,582 times, which dwarfs the rest of the counts.) As we know, PCA looks for linear correlations within the features. Correlation and variance statistics are very sensitive to large outliers; a single large number could change the statistics a lot. So, it is a good idea to first trim the data of large values (“Frequency- Based Filtering” on page 48), or apply a scaling transform like tf-idf (Chapter 4) or the log transform (“Log Transformation” on page 15).\n\nUse Cases PCA reduces feature space dimensionality by looking for linear correlation patterns between features. Since it involves the SVD, PCA is expensive to compute for more than a few thousand features. But for small numbers of real-valued features, it is very much worth trying.\n\nPCA transformation discards information from the data. Thus, the downstream model may be cheaper to train, but less accurate. On the MNIST dataset, some have observed that using reduced-dimensionality data from PCA results in less accurate classification models. In these cases, there is both an upside and a downside to using PCA.\n\nOne of the coolest applications of PCA is in anomaly detection of time series. Lakhina et al. (2004) used PCA to detect and diagnose anomalies in internet traffic. They focused on volume anomalies, i.e., when there is a surge or a dip in the amount of traffic going from one network region to another. These sudden changes may be indicative of a misconfigured network or coordinated denial-of-service attacks. Either way, knowing when and where such changes occur is valuable to internet operators.\n\nSince there is so much total traffic over the internet, isolated surges in small regions are hard to detect. A relatively small set of backbone links handle much of the traffic. Their key insight is that volume anomalies affect multiple links at the same time (because network packets need to hop through multiple nodes to reach their destina‐ tion). Treat each of the links as a feature, and the amount of traffic at each time step as the measurement. A data point is a time slice of traffic measurements across all links on the network. The principal components of this matrix indicate the overall\n\nUse Cases\n\n|\n\n111",
      "content_length": 2877,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 128,
      "content": "traffic trends on the network. The rest of the components represent the residual sig‐ nal, which contains the anomalies.\n\nPCA is also often used in financial modeling. In those use cases, it works as a type of factor analysis, a term that describes a family of statistical methods that aim to describe observed variability in data using a small number of unobserved factors. In factor analysis applications, the goal is to find the explanatory components, not the transformed data.\n\nFinancial quantities like stock returns are often correlated with each other. Stocks may move up and down at the same time (positive correlation), or move in opposite directions (negative correlation). In order to balance volatility and reduce risk, an investment portfolio needs a diverse set of stocks that are not correlated with each other. (Don’t put all your eggs in one basket if that basket is going to sink.) Finding strong correlation patterns is helpful for deciding on an investment strategy.\n\nStock correlation patterns can be industry-wide. For example, tech stocks may go up and down together, while airline stocks tend to go down when oil prices are high. But industry may not be the best way to explain the outcome. Analysts also look for unex‐ pected correlations in observed statistics. In particular, the statistical factor model (Connor, 1995) runs PCA on the matrix of time series of individual stock returns to find commonly covarying stocks. In this use case, the end goal is the principal com‐ ponents themselves, not the transformed data.\n\nZCA is useful as a preprocessing step when learning from images. In natural images, adjacent pixels often have similar colors. ZCA whitening can remove this correlation, which allows subsequent modeling efforts to focus on more interesting image struc‐ tures. Krizhevsky’s (2009) thesis on “Learning Multiple Layers of Features from Images” contains nice examples that illustrate the effect of ZCA whitening on natural images.\n\nMany deep learning models use PCA or ZCA as a preprocessing step, though it is not always necessary. In “Factored 3-Way Restricted Boltzmann Machines for Modeling Natural Images”, Ranzato et al. (2010) remark, “Whitening is not necessary but speeds up the convergence of the algorithm.” In “An Analysis of Single-Layer Net‐ works in Unsupervised Feature Learning”, Coates et al. (2011) find that ZCA whiten‐ ing is helpful for some models, but not all. (Note the models in this paper are unsupervised feature learning models, so ZCA is used as a feature engineering method for other feature engineering methods. Stacking and chaining of methods is common in machine learning pipelines.)\n\nSummary This concludes the discussion of PCA. The two main things to remember about PCA are its mechanism (linear projection) and objective (to maximize the variance of\n\n112\n\n|\n\nChapter 6: Dimensionality Reduction: Squashing the Data Pancake with PCA",
      "content_length": 2915,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 129,
      "content": "projected data). The solution involves the eigen decomposition of the covariance matrix, which is closely related to the SVD of the data matrix. One can also remem‐ ber PCA with the mental picture of squashing the data into a pancake that is as fluffy as possible.\n\nPCA is an example of model-driven feature engineering. (One should immediately suspect that a model is lurking in the background whenever an objective function enters the scene.) The modeling assumption here is that variance adequately repre‐ sents the information contained in the data. Equivalently, the model looks for linear correlations between features. This is used in several applications to reduce the corre‐ lation or find common factors in the input.\n\nPCA is a well-known dimensionality reduction method. But it has its limitations, such as high computational cost and uninterpretable outcome. It is useful as a pre‐ processing step, especially when there are linear correlations between features.\n\nWhen seen as a method for eliminating linear correlation, PCA is related to the con‐ cept of whitening. Its cousin, ZCA, whitens the data in an interpretable way, but does not reduce dimensionality.\n\nBibliography Bell, Anthony J. and Terrence J. Sejnowski. “Edges Are the ‘Independent Compo‐ nents’ of Natural Scenes.” Advances in Neural Information Processing Systems 9 (1996): 831–837.\n\nCoates, Adam, Andrew Y. Ng, and Honglak Lee. “An Analysis of Single-Layer Net‐ works in Unsupervised Feature Learning.” Proceedings of the 14th International con‐ ference on Artificial Intelligence and Statistics (2011): 215–223.\n\nConnor, Gregory. “The Three Types of Factor Models: A Comparison of Their Explanatory Power.” Financial Analysts Journal 51:3 (1995) 42–46.\n\nGolub, Gene H., and Charles F. Van Loan. Matrix Computations. 4th ed. Baltimore, MD: Johns Hopkins University Press, 2012.\n\nKrizhevsky, Alex. “Learning Multiple Layers of Features from Tiny Images.” MSc thesis, University of Toronto, 2009.\n\nLakhina, Anukool, Mark Crovella, and Christophe Diot. “Diagnosing Network-wide Traffic Anomalies.” Proceedings of the 2004 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications (2004): 219–230.\n\nRanzato, Marc’Aurelio, Alex Krizhevsky, and Geoffrey E. Hinton. “Factored 3-Way Restricted Boltzmann Machines for Modeling Natural Images.” Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (2010): 621–628.\n\nBibliography\n\n|\n\n113",
      "content_length": 2487,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 131,
      "content": "CHAPTER 7 Nonlinear Featurization via K-Means Model Stacking\n\nPCA is very useful when the data lies in a linear subspace like a flat pancake. But what if the data forms a more complicated shape?1 A flat plane (linear subspace) can be generalized to a manifold (nonlinear subspace), which can be thought of as a surface that gets stretched and rolled in various ways.2\n\nIf a linear subspace is a flat sheet of paper, then a rolled up sheet of paper is a simple example of a nonlinear manifold. Informally, this is called a Swiss roll (see Figure 7-1). Once rolled, a 2D plane occupies 3D space. Yet it is essentially still a 2D object. In other words, it has low intrinsic dimensionality, a concept we’ve already touched upon in “Intuition” on page 99. If we could somehow unroll the Swiss roll, we’d recover the 2D plane. This is the goal of nonlinear dimensionality reduction, which assumes that the manifold is simpler than the full dimension it occupies and attempts to unfold it.\n\n1 This chapter is inspired by a conversation with Ted Dunning, active Apache contributor and noted author.\n\nThe stacking example came directly from Ted, and he provided many helpful comments in the course of writ‐ ing. If one could have coauthors for individual chapters, Ted would be a coauthor for this one.\n\n2 We use the words “surface” and “manifold” interchangeably in this chapter. The analogy works well for two- dimensional manifolds embedded in a three-dimensional space, but it breaks down beyond three dimensions. A high-dimensional manifold does not conform to our usual notion of a “surface.” Some of the more out‐ landish manifolds have holes, and some loop back onto themselves in a way that would never happen in the real physical world (e.g., M.C. Escher’s endless waterfall). Most data models assume nice manifolds, not the crazy ones.\n\n115",
      "content_length": 1843,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 132,
      "content": "Figure 7-1. The Swiss roll, a nonlinear manifold\n\nThe key observation is that even when a big manifold looks complicated, the local neighborhood around each point can often be well approximated with a patch of flat surface. In other words, the patches to encode global structure using local structure.3 Nonlinear dimensionality reduction is also called nonlinear embedding or manifold learning. Nonlinear embeddings are useful for aggressively compressing high- dimensional data into low-dimensional data. They are often used for visualization in two or three dimensions.\n\nThe goal of feature engineering, however, isn’t so much to make the feature dimen‐ sions as low as possible, but to arrive at the right features for the task. In this chapter, the right features are those that represent the spatial characteristics of the data.\n\nClustering algorithms are usually not presented as techniques for local structure learning. But they in fact enable just that. Points that are close to each other (where “closeness” can be defined by a chosen metric) belong to the same cluster. Given a clustering, a data point can be represented by its cluster membership vector. If the number of clusters is smaller than the original number of features, then the new rep‐ resentation will have fewer dimensions than the original; the original data is com‐ pressed into a lower dimension. We will unpack this idea in this chapter.\n\nCompared to nonlinear embedding techniques, clustering may produce more fea‐ tures. But if the end goal is feature engineering instead of visualization, this is not a problem.\n\n3 This is a tried-and-true idea in mathematics. For instance, the derivative of a function measures the speed of change at each point. Globally, the function may do all sorts of weird things. But locally, it can be approxima‐ ted by a linear function of the derivative. If we know the derivative at each point, then calculus allows us to more or less recover the entire original function.\n\n116\n\n|\n\nChapter 7: Nonlinear Featurization via K-Means Model Stacking",
      "content_length": 2054,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 133,
      "content": "We will illustrate the idea of local structure learning with a common clustering algo‐ rithm called k-means. It is simple to understand and implement. Instead of nonlinear manifold reduction, it is more apt to say that k-means performs nonlinear manifold feature extraction. Used correctly, it can be a powerful tool in our feature engineering repertoire.\n\nk-Means Clustering k-means is a clustering algorithm. Clustering algorithms group data depending on how they are laid out in space. They are unsupervised in that they do not require any sort of label—it’s the algorithm’s job to infer cluster labels based solely on the geome‐ try of the data itself.\n\nA clustering algorithm depends on a metric—a measurement of closeness between data points. The most popular metric is the Euclidean distance or Euclidean metric. It comes from Euclidean geometry and measures the straight-line distance between two points. It should feel very normal to us because this is the distance we see in everyday physical reality.\n\nThe Euclidean distance between two vectors x and y is the ℓ2 norm of x – y. (See “ℓ2 Normalization” on page 32 for more on the ℓ2 norm.) In math speak, it is usually written as ‖x – y‖2 or just ‖x – y‖.\n\nk-means establishes a hard clustering, meaning that each data point is assigned to one and only one cluster. The algorithm learns to position the cluster centers such that the total sum of the Euclidean distance between each data point and its cluster center is minimized. For those who like to read math instead of words, here is the objective function:\n\nminC1,…, Ck, μ1,…, μk∑\n\nk\n\ni=1\n\n∑ x∈Ci\n\n∥ x – μi ∥ 2\n\nEach cluster Ci contains a subset of data points. The center of cluster i is equal to the average of all the data points in the cluster:\n\nμi = ∑ x∈Ci\n\nx/ni\n\nwhere ni denotes the number of data points in cluster i.\n\nk-Means Clustering\n\n|\n\n117",
      "content_length": 1868,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 134,
      "content": "Figure 7-2 shows k-means at work on two different, randomly generated datasets. The data in (a) is generated from random Gaussian distributions with the same var‐ iance but different means. The data in (c) is generated uniformly at random. These toy problems are very simple to solve, and k-means does a good job. (The results could be sensitive to the number of clusters, which must be given to the algorithm.)\n\nFigure 7-2. k-means examples demonstrating how the clustering algorithm partitions space\n\nThe code for this example is found in Example 7-1.\n\nExample 7-1. Code to generate k-means examples\n\n>>> import numpy as np >>> from sklearn.cluster import KMeans >>> from sklearn.datasets import make_blobs\n\n>>> import matplotlib.pyplot as plt\n\n>>> n_data = 1000 >>> seed = 1 >>> n_clusters = 4\n\n# Generate random Gaussian blobs and run k-means >>> blobs, blob_labels = make_blobs(n_samples=n_data, n_features=2, ... centers=n_centers, random_state=seed) >>> clusters_blob = KMeans(n_clusters=n_centers, random_state=seed).fit_predict(blobs)\n\n# Generate data uniformly at random and run k-means >>> uniform = np.random.rand(n_data, 2) >>> clusters_uniform = KMeans(n_clusters=n_clusters,\n\n118\n\n|\n\nChapter 7: Nonlinear Featurization via K-Means Model Stacking",
      "content_length": 1260,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 135,
      "content": "... random_state=seed).fit_predict(uniform)\n\n# Matplotlib incantations for visualizing results >>> figure = plt.figure() >>> plt.subplot(221) >>> plt.scatter(blobs[:, 0], blobs[:, 1], c=blob_labels, cmap='gist_rainbow') >>> plt.title(\"(a) Four randomly generated blobs\", fontsize=14) >>> plt.axis('off')\n\n>>> plt.subplot(222) >>> plt.scatter(blobs[:, 0], blobs[:, 1], c=clusters_blob, cmap='gist_rainbow') >>> plt.title(\"(b) Clusters found via K-means\", fontsize=14) >>> plt.axis('off')\n\n>>> plt.subplot(223) >>> plt.scatter(uniform[:, 0], uniform[:, 1]) >>> plt.title(\"(c) 1000 randomly generated points\", fontsize=14) >>> plt.axis('off')\n\n>>> plt.subplot(224) >>> plt.scatter(uniform[:, 0], uniform[:, 1], c=clusters_uniform, cmap='gist_rainbow') >>> plt.title(\"(d) Clusters found via K-means\", fontsize=14) >>> plt.axis('off')\n\nClustering as Surface Tiling Common applications of clustering assume that there are natural clusters to be found; i.e., there are regions of dense data scattered in an otherwise empty space. In these situations, there is a notion of the correct number of clusters, and people have invented clustering indices that measure the quality of data groupings in order to select for k.\n\nHowever, when data is spread out fairly uniformly like in Figure 7-2(c), there is no longer a correct number of clusters. In this case, the role of a clustering algorithm is vector quantization, i.e., partitioning the data into a finite number of chunks. The number of clusters can be selected based on acceptable approximation error when using quantized vectors instead of the original ones.\n\nVisually, this usage of k-means can be thought of as covering the data surface with patches, like in Figure 7-3. This is indeed what we get if we run k-means on a Swiss roll dataset.\n\nClustering as Surface Tiling\n\n|\n\n119",
      "content_length": 1825,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 136,
      "content": "Figure 7-3. Conceptual local patches on the Swiss roll from a clustering algorithm\n\nExample 7-2 uses scikit-learn to generate a noisy dataset on the Swiss roll, cluster it with k-means, and visualize the clustering results using Matplotlib. The data points are colored according to their cluster IDs.\n\nExample 7-2. k-means on the Swiss roll\n\n>>> from mpl_toolkits.mplot3d import Axes3D >>> from sklearn import manifold, datasets\n\n# Generate a noisy Swiss roll dataset >>> X, color = datasets.samples_generator.make_swiss_roll(n_samples=1500)\n\n# Approximate the data with 100 k-means clusters >>> clusters_swiss_roll = KMeans(n_clusters=100, random_state=1).fit_predict(X)\n\n# Plot the dataset with k-means cluster IDs as the color >>> fig2 = plt.figure() >>> ax = fig2.add_subplot(111, projection='3d') >>> ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=clusters_swiss_roll, cmap='Spectral')\n\nIn this example, we generated 1,500 points at random on the Swiss roll surface, and asked k-means to approximate it with 100 clusters. We pulled the number 100 out of\n\n120\n\n|\n\nChapter 7: Nonlinear Featurization via K-Means Model Stacking",
      "content_length": 1122,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 137,
      "content": "a hat because it seems like a fairly large number to cover a fairly small space. The result (Figure 7-4) looks nice; the clusters are indeed very local and different sections of the manifold are mapped to different clusters. Great! Are we done?\n\nFigure 7-4. Approximating a Swiss roll dataset using k-means with 100 clusters\n\nThe problem is that if we pick a k that is too small, then the results won’t be so nice from a manifold learning perspective. Figure 7-5 shows the output of k-means on the Swiss roll with 10 clusters. We can clearly see data from very different sections of the manifold being mapped to the same clusters (e.g., the yellow, purple, green, and magenta clusters—see, we told you the illustrations are best viewed in color!).\n\nFigure 7-5. k-means on the Swiss roll with 10 clusters\n\nIf the data is distributed uniformly throughout the space, then picking the right k boils down to a sphere-packing problem. In d dimensions, one could fit roughly 1/rd spheres of radius r. Each k-means cluster is a sphere, and the radius is the maximum error of representing points in that sphere with the centroid. So, if we are willing to tolerate a maximum approximation error of r per data point, then the number of clusters is O(1/rd), where d is the dimension of the original feature space of the data.\n\nClustering as Surface Tiling\n\n|\n\n121",
      "content_length": 1351,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 138,
      "content": "Uniform distribution is the worst-case scenario for k-means. If data density is not uniform, then we will be able to represent more data with fewer clusters. In general, it is difficult to tell how data is distributed in high-dimensional space. One can be con‐ servative and pick a larger k, but it can’t be too large, because k will become the num‐ ber of features for the next modeling step.\n\nk-Means Featurization for Classification When using k-means as a featurization procedure, a data point can be represented by its cluster membership (a sparse one-hot encoding of the cluster membership catego‐ rical variable; see “One-Hot Encoding” on page 78), which we now illustrate.\n\nIf a target variable is also available, then we have the choice of giving that information as a hint to the clustering procedure. One way to incorporate target information is to simply include the target variable as an additional input feature to the k-means algo‐ rithm. Since the objective is to minimize the total Euclidean distance over all input dimensions, the clustering procedure will attempt to balance similarity in the target value as well as in the original feature space. The target values can be scaled to get more or less attention from the clustering algorithm. Larger differences in the target will produce clusters that pay more attention to the classification boundary.\n\nk-Means Featurization\n\nClustering algorithms analyze the spatial distribution of data. Therefore, k-means featurization creates a compressed spatial index of the data which can be fed into the model in the next stage. This is an example of model stacking.\n\nExample 7-3 shows a simple k-means featurizer. It is defined as a class object that can be fitted to training data and transform any new data.\n\nExample 7-3. k-means featurizer\n\n>>> import numpy as np >>> from sklearn.cluster import KMeans\n\n>>> class KMeansFeaturizer: ... \"\"\"Transforms numeric data into k-means cluster memberships. ... ... This transformer runs k-means on the input data and converts each data point ... into the ID of the closest cluster. If a target variable is present, it is ... scaled and included as input to k-means in order to derive clusters that ... obey the classification boundary as well as group similar points together. ... \"\"\" ... ... def __init__(self, k=100, target_scale=5.0, random_state=None):\n\n122\n\n|\n\nChapter 7: Nonlinear Featurization via K-Means Model Stacking",
      "content_length": 2432,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 139,
      "content": "... self.k = k ... self.target_scale = target_scale ... self.random_state = random_state ... ... def fit(self, X, y=None): ... \"\"\"Runs k-means on the input data and finds centroids. ... \"\"\" ... if y is None: ... # No target variable, just do plain k-means ... km_model = KMeans(n_clusters=self.k, ... n_init=20, ... random_state=self.random_state) ... km_model.fit(X) ... ... self.km_model_ = km_model ... self.cluster_centers_ = km_model.cluster_centers_ ... return self ... ... # There is target information. Apply appropriate scaling and include ... # it in the input data to k-means. ... data_with_target = np.hstack((X, y[:,np.newaxis]*self.target_scale)) ... ... # Build a pre-training k-means model on data and target ... km_model_pretrain = KMeans(n_clusters=self.k, ... n_init=20, ... random_state=self.random_state) ... km_model_pretrain.fit(data_with_target) ... ... # Run k-means a second time to get the clusters in the original space ... # without target info. Initialize using centroids found in pre-training. ... # Go through a single iteration of cluster assignment and centroid ... # recomputation. ... km_model = KMeans(n_clusters=self.k, ... init=km_model_pretrain.cluster_centers_[:,:2], ... n_init=1, ... max_iter=1) ... km_model.fit(X) ... ... self.km_model = km_model ... self.cluster_centers_ = km_model.cluster_centers_ ... return self ... ... def transform(self, X, y=None): ... \"\"\"Outputs the closest cluster ID for each input data point. ... \"\"\" ... clusters = self.km_model.predict(X) ... return clusters[:,np.newaxis] ... ... def fit_transform(self, X, y=None): ... self.fit(X, y) ... return self.transform(X, y)\n\nk-Means Featurization for Classification\n\n|\n\n123",
      "content_length": 1693,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 140,
      "content": "To illustrate the difference between using and not using target information when clustering in Example 7-4, we apply the featurizer to a synthetic dataset generated using scikit-learn’s make_moons function and plot the Voronoi diagram of the cluster boundaries.\n\nExample 7-4. k-means featurization with and without target hints\n\n>>> from scipy.spatial import Voronoi, voronoi_plot_2d >>> from sklearn.datasets import make_moons\n\n>>> training_data, training_labels = make_moons(n_samples=2000, noise=0.2) >>> kmf_hint = KMeansFeaturizer(k=100, target_scale=10).fit(training_data, ... training_labels) >>> kmf_no_hint = KMeansFeaturizer(k=100, target_scale=0).fit(training_data, ... training_labels)\n\n>>> def kmeans_voronoi_plot(X, y, cluster_centers, ax): ... \"\"\"Plots the Voronoi diagram of the k-means clusters overlaid with the data\"\"\" ... ax.scatter(X[:, 0], X[:, 1], c=y, cmap='Set1', alpha=0.2) ... vor = Voronoi(cluster_centers) ... voronoi_plot_2d(vor, ax=ax, show_vertices=False, alpha=0.5)\n\nFigure 7-6 shows a comparison of the results. The two moons of the dataset are col‐ ored according to their class labels. The bottom panel shows the clusters trained without target information. Notice that a number of clusters span the empty space between the two classes. The top panel shows that when the clustering algorithm is given target information, the cluster boundaries align much better along class boundaries.\n\nLet’s test the effectiveness of k-means features for classification. Example 7-5 applies logistic regression on the input data augmented with k-means cluster features. It compares the results against the support vector machine with radial basis function kernel (RBF SVM), k-nearest neighbors (kNN), random forest (RF), and gradient boosting tree (GBT) classifiers. RF and GBT are popular nonlinear classifiers with state-of-the-art performance. RBF SVM is a reasonable nonlinear classifier for Eucli‐ dean space. kNN classifies data according to the average of its k nearest neighbors.\n\n124\n\n|\n\nChapter 7: Nonlinear Featurization via K-Means Model Stacking",
      "content_length": 2079,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 141,
      "content": "Figure 7-6. k-means clusters with (top panel) and without (bottom panel) using target class information\n\nThe default input data to the classifiers consists of the 2D coordinates of each data point. Logistic regression is also given the cluster membership features (labeled “LR with k-means” in Figure 7-7). As a baseline, we also try logistic regression on just the 2D coordinates (labeled “LR”).\n\nExample 7-5. Classification with k-means cluster features\n\n>>> from sklearn.linear_model import LogisticRegression >>> from sklearn.svm import SVC >>> from sklearn.neighbors import KNeighborsClassifier >>> from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n\n### Generate some test data from the same distribution as training data >>> test_data, test_labels = make_moons(n_samples=2000, noise=0.3)\n\n### Use the k-means featurizer to generate cluster features >>> training_cluster_features = kmf_hint.transform(training_data) >>> test_cluster_features = kmf_hint.transform(test_data)\n\nk-Means Featurization for Classification\n\n|\n\n125",
      "content_length": 1058,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 142,
      "content": "### Form new input features with cluster features >>> training_with_cluster = scipy.sparse.hstack((training_data, ... training_cluster_features)) >>> test_with_cluster = scipy.sparse.hstack((test_data, test_cluster_features))\n\n### Build the classifiers >>> lr_cluster = LogisticRegression(random_state=seed).fit(training_with_cluster, ... training_labels) >>> classifier_names = ['LR', ... 'kNN', ... 'RBF SVM', ... 'Random Forest', ... 'Boosted Trees'] >>> classifiers = [LogisticRegression(random_state=seed), ... KNeighborsClassifier(5), ... SVC(gamma=2, C=1), ... RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1), ... GradientBoostingClassifier(n_estimators=10, learning_rate=1.0, ... max_depth=5)] >>> for model in classifiers: ... model.fit(training_data, training_labels)\n\n### Helper function to evaluate classifier performance using ROC >>> def test_roc(model, data, labels): ... if hasattr(model, \"decision_function\"): ... predictions = model.decision_function(data) ... else: ... predictions = model.predict_proba(data)[:,1] ... fpr, tpr, _ = sklearn.metrics.roc_curve(labels, predictions) ... return fpr, tpr\n\n### Plot results >>> import matplotlib.pyplot as plt >>> plt.figure() >>> fpr_cluster, tpr_cluster = test_roc(lr_cluster, test_with_cluster, test_labels) >>> plt.plot(fpr_cluster, tpr_cluster, 'r-', label='LR with k-means')\n\n>>> for i, model in enumerate(classifiers): ... fpr, tpr = test_roc(model, test_data, test_labels) ... plt.plot(fpr, tpr, label=classifier_names[i])\n\n>>> plt.plot([0, 1], [0, 1], 'k--') >>> plt.legend()\n\nFigure 7-7 shows the receiver operating characteristic (ROC) curves of each of the classifiers when evaluated on the test set. A ROC curve shows the trade-off between true positives and false positives as we vary the classification decision boundary. (See Zheng [2015] for more details.) A good classifier should quickly reach a high true positive rate and a low false positive rate, so curves that rise sharply toward the upper-left corner are good.\n\n126\n\n|\n\nChapter 7: Nonlinear Featurization via K-Means Model Stacking",
      "content_length": 2091,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 143,
      "content": "Figure 7-7. ROCs of k-means + logistic regression versus nonlinear classifiers and plain logistic regression on the synthetic two-moons dataset\n\nOur plot shows that logistic regression performs much better with cluster features than without. In fact, with cluster features, the linear classifier performs just as well as nonlinear classifiers. One minor caveat is that in this toy example, we did not tune the hyperparameters for any of the models. There may be performance differences once the models are fully tuned, but at least this shows that it is possible for LR with k-means to be on a par with nonlinear classifiers. This is a nice result because linear classifiers are much cheaper to train than nonlinear classifiers. Lower computation cost allows us to try more models with different features in the same period of time, which increases the chance of ending up with a much better model.\n\nAlternative Dense Featurization Instead of one-hot cluster membership, a data point can also be represented by a dense vector of its inverse distance to each cluster center. This retains more informa‐ tion than simple binary cluster assignment, but the representation is now dense. There is a trade-off here. One-hot cluster membership results in a very lightweight, sparse representation, but one might need a larger k to represent data of complex shapes. Inverse distance representation is dense, which could be more expensive for the modeling step, but one might be able to get away with a smaller k.\n\nA compromise between sparse and dense is to retain inverse distances for only p of the closest clusters. But now p is an extra hyperparameter to tune. (Can you under‐ stand why feature engineering requires so much fiddling?) There is no free lunch.\n\nk-Means Featurization for Classification\n\n|\n\n127",
      "content_length": 1803,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 144,
      "content": "Pros, Cons, and Gotchas Using k-means to turn spatial data into features is an example of model stacking, where the input to one model is the output of another. Another example of stacking is to use the output of a decision tree–type model (random forest or gradient boost‐ ing tree) as input to a linear classifier. Stacking has become an increasingly popular technique in recent years. Nonlinear classifiers are expensive to train and maintain. The key intuition with stacking is to push the nonlinearities into the features and use a very simple, usually linear model as the last layer. The featurizer can be trained off‐ line, which means that one can use expensive models that require more computation power or memory but generate useful features. The simple model at the top level can be quickly adapted to the changing distributions of online data. This is a great trade- off between accuracy and speed, and this strategy is often used in applications like targeted advertising that require fast adaptation to changing data distributions.\n\nKey Intuition for Model Stacking\n\nUse sophisticated base layers (often with expensive models) to gen‐ erate good (often nonlinear) features, combined with a simple and fast top-layer model. This often strikes the right balance between model accuracy and speed.\n\nCompared to using a nonlinear classifier, k-means stacked with logistic regression is cheaper to train and store. Table 7-1 is a chart detailing the training and prediction complexity in both computation and memory for a number of machine learning models. n denotes the number of data points, d the number of (original) features.\n\nTable 7-1. Complexity of ML models\n\nModel\n\nk-means training\n\nk-means predict\n\nTime O(nkd)a O(kd)\n\nSpace O(kd)\n\nO(kd)\n\nLR + cluster features training\n\nO(n(d+k))\n\nO(d+k)\n\nLR + cluster features predict\n\nRBF SVM training\n\nRBF SVM predict\n\nGBT training\n\nGBT predict\n\nkNN training\n\nO(d+k) O(n2d) O(sd) O(nd2mt) O(2mt) O(1)\n\nO(d+k) O(n2) O(sd) O(nd + 2mt) O(2mt) O(nd)\n\nO(nd + k log n)\n\nO(nd)\n\nkNN predict a Streaming k-means can be done in time O(nd (log k + log log n)), which is much faster than O(nkd) for large k.\n\n128\n\n|\n\nChapter 7: Nonlinear Featurization via K-Means Model Stacking",
      "content_length": 2222,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 145,
      "content": "For k-means, the training time is O(nkd) because each iteration involves computing the d-dimensional distance between every data point and every centroid (k). We opti‐ mistically assume that the number of iterations is not a function of n, though this may not be true in all cases. Prediction requires computing the distance between the new data point and each of the k centroids, which is O(kd). The storage space requirement is O(kd), for the coordinates of the k centroids.\n\nLogistic regression training and prediction are linear in both the number of data points and feature dimensions. RBF SVM training is expensive because it involves computing the kernel matrix for every pair of input data. RBF SVM prediction is less expensive than training; it is linear in the number of support vectors s and the feature dimension d. GBT training and prediction are linear in data size and the size of the model (t trees, each with at most 2m leaves, where m is the maximum depth of the tree). A naive implementation of kNN requires no training time at all because the training data itself is essentially the model. The cost is paid at prediction time, where the input must be evaluated against each of the original training points and partially sorted to retrieve the k closest neighbors.\n\nOverall, k-means + LR is the only combination that is linear (with respect to the size of training data, O(nd), and model size, O(kd)) at both training and prediction time. The complexity is most similar to that of GBT, which has costs that are linear in the number of data points, the feature dimension, and the size of the model (O(2mt)). It is hard to say whether k-means + LR or GBT will result in a smaller model—it depends on the spatial characteristics of the data.\n\nPotential for Data Leakage\n\nThose who remember our caution regarding data leakage (see “Guarding against data leakage” on page 93) might ask whether including the target variable in the k-means featurization step would cause such a problem. The answer is “yes,” but not as much in the case of bin counting. If we use the same dataset for learning the clusters and building the classification model, then information about the target will have leaked into the input variables. As a result, accuracy evaluations on the training data will probably be overly optimistic, but the bias will go away when evaluating on a hold-out validation set or test set. Furthermore, the leakage will not be as bad as in the case of bin-counting statistics (see “Bin Count‐ ing” on page 87), because the lossy compression of the clustering algorithm will have abstracted away some of that information. To be extra careful about preventing leakage, hold out a separate data‐ set for deriving the clusters, just like in the case of bin counting.\n\nk-means featurization is useful for real-valued, bounded numeric features that form clumps of dense regions in space. The clumps can be of any shape, because we can\n\nPros, Cons, and Gotchas\n\n|\n\n129",
      "content_length": 2981,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 146,
      "content": "just increase the number of clusters to approximate them. (Unlike in the classic clus‐ tering setup, we are not concerned with discovering the “true” number of clusters; we only need to cover them.)\n\nk-means cannot handle feature spaces where the Euclidean distance does not make sense—i.e., weirdly distributed numeric variables or categorical variables. If the fea‐ ture set contains those variables, then there are several ways to handle them:\n\n1. Apply k-means featurization only on the real-valued, bounded numeric features. 2. Define a custom metric to handle multiple data types and use the k-medoids algorithms. (k-medoids is analogous to k-means but allows for arbitrary distance metrics.)\n\n3. Convert categorical variables to binning statistics (see “Bin Counting” on page 87), then featurize them using k-means.\n\nCombined with techniques for handling categorical variables and time series, k- means featurization can be adapted to handle the kind of rich data that often appears in customer marketing and sales analytics. The resulting clusters can be thought of as user segments, which are very useful features for the next modeling step.\n\nSummary This chapter illustrated the concept of model stacking using a somewhat unconven‐ tional approach: combining supervised k-means with a simple linear classifier. k- means is usually used as an unsupervised modeling method to find dense clusters of data points in feature space. Here, however, k-means is optionally given the class labels as input. This helps k-means to find clusters that better align with the bound‐ ary between classes.\n\nDeep learning, which we will discuss in the next chapter, takes model stacking to a whole new level by layering neural networks on top of one another. Two recent win‐ ners of the ImageNet Large Scale Visual Recognition Challenge involved 13 and 22 layers of neural networks. They take advantage of the availability of lots of unlabeled training images and look for combinations of pixels that yield good image fea‐ tures. The technique in this chapter separately trains the k-means featurizer from the linear classifier. But it’s possible to jointly optimize the featurizer and the classier. As we shall see, deep learning training takes the latter route.\n\n130\n\n|\n\nChapter 7: Nonlinear Featurization via K-Means Model Stacking",
      "content_length": 2325,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 147,
      "content": "Bibliography Dunning, Ted. The man is a walking encyclopedia of data science. He is a frequent speaker at industry events, and likes beer and nice people. Buy him a beer and talk to him. You won’t regret it.\n\nZheng, Alice. Evaluating Machine Learning Models. Sebastopol, CA: O’Reilly Media, 2015.\n\nBibliography\n\n|\n\n131",
      "content_length": 318,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 149,
      "content": "CHAPTER 8 Automating the Featurizer: Image Feature Extraction and Deep Learning\n\nSight and sound are innate sensory inputs for humans. Our brains are hardwired to rapidly evolve our abilities to process visual and auditory signals, with some systems developing to respond to stimulus even before birth (Eliot, 2000). Language skills, on the other hand, are learned. They take months to develop and years to master. Many people take the development of their vision and hearing for granted, but all of us have had to intentionally train our brains to understand and use language.\n\nInterestingly, the situation is the reverse for machine learning. We have made much more headway with text analysis applications than image or audio. Take the problem of search, for example. People have enjoyed years of relative success in information retrieval and text search, whereas image and audio search are still being perfected (though the breakthrough in deep learning models in the last five years may finally herald the long-awaited revolution in image and speech analysis).\n\nThe difficulty of progress is directly related to the difficulty of extracting meaningful features from the respective types of data. Machine learning models require semanti‐ cally meaningful features to make semantically meaningful predictions. In text analy‐ sis, particularly for languages such as English where a basic unit of semantic meaning (a word) is easily extractable, progress can be made very fast. Images and audio, on the other hand, are recorded as digital pixels or waveforms. A single “atom” in an image is a pixel. In audio data, it is a single measurement of waveform intensity. These contain much less semantic information than an atom—a word—of text data. Therefore, the job of feature extraction and engineering is much more challenging on image and audio than on text.\n\nIn the last 20 years, computer vision research has focused on manually defined pipe‐ lines for extracting good image features. For a while, image feature extractors such as\n\n133",
      "content_length": 2037,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 150,
      "content": "SIFT and HOG (described in the following sections) were the standard. Recent devel‐ opments in deep learning research have extended the reach of traditional machine learning models by incorporating automatic feature extraction in the base layers. They essentially replace manually defined feature image extractors with manually defined models that automatically learn and extract features. The manual work is still there, just abstracted further into the belly of the modeling beast.\n\nIn this chapter, we will start with the most popular image feature extractors and then dive into the most complicated modeling machinery covered in this book: deep learning for feature learning.\n\nThe Simplest Image Features (and Why They Don’t Work) What are the right features to extract from an image? The answer of course depends on what we are trying to do with those features. Let’s say our task is image retrieval: we are given a picture and asked to find similar pictures from a database of images. We need to decide how to represent each image, and how to measure the differences between them. Can we just look at the percentage of different colors in an image? Figure 8-1 shows two pictures having roughly the same color profile but very differ‐ ent meanings; one looks like white cloud in a blue sky, and the other is the flag of Greece. So, color information is probably not enough to characterize an image.\n\nFigure 8-1. Blue and white pictures—same color profile, very different meanings\n\nAnother simple idea is to measure the pixel value differences between images. First, resize the images to have the same width and height. Each image is represented by a matrix of pixel values. The matrix can be stacked into one long vector, either by row or by column. The color of each pixel (e.g., the RGB encoding of the color) is now a feature of the image. Finally, measure the Euclidean distance between the long pixel vectors. This would definitely allow us to tell apart the Greek flag and the white clouds, but it is too stringent as a similarity measure. A cloud could take on a thou‐ sand different shapes and still be a cloud. It could be shifted to the side of the image, or half of it might lie in shadow. All of these transformations would increase the\n\n134\n\n|\n\nChapter 8: Automating the Featurizer: Image Feature Extraction and Deep Learning",
      "content_length": 2343,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 151,
      "content": "Euclidean distance, but they shouldn’t change the fact that the picture is still of a cloud.\n\nThe problem is that individual pixels do not carry enough semantic information about the image. Therefore, they are bad atomic units for analysis.\n\nManual Feature Extraction: SIFT and HOG In 1999, computer vision researchers figured out a better way to represent images using statistics of image patches: the Scale Invariant Feature Transform (SIFT) [Lowe, 1999].\n\nSIFT was originally developed for the task of object recognition, which involves not only correctly tagging the image as containing an object, but pinpointing its location in the image. The process involves analyzing the image at a pyramid of possible scales, detecting interest points that could indicate the presence of the object, extract‐ ing features (commonly called image descriptors in computer vision) about the inter‐ est points, and determining the pose of the object.\n\nOver the years, the usage of SIFT expanded to extract features not only for interest points but across the entire image. The SIFT feature extraction procedure is very sim‐ ilar to another technique, called the Histogram of Oriented Gradients (HOG) [Dalal and Triggs, 2005]. Both of them essentially compute histograms of gradient orienta‐ tions. We now describe this process in detail.\n\nImage Gradients To do better than raw pixel values, we have to somehow “organize” the pixels into more informative units. Differences between neighboring pixels are often very useful. Pixel values usually differ at the boundary of objects, when there is a shadow, within a pattern, or on a textured surface. The difference in value between neighboring pixels is called an image gradient.\n\nThe simplest way to compute the image gradient is to separately calculate the differ‐ ences along the horizontal (x) and vertical (y) axes of the image, then compose them into a 2D vector. This involves two 1D difference operations that can be handily rep‐ resented by a vector mask or filter. The mask [1, 0, –1] takes the difference between the left neighbor and the right neighbor or the up-neighbor and the down-neighbor, depending on which direction we apply the mask. There are 2D gradient filters as well, but for the purpose of this example, the 1D filter suffices.\n\nTo apply a filter to an image, we perform a convolution. It involves flipping the filter and taking the inner product with a small patch of the image, then moving to the next patch. Convolutions are very common in signal processing. We’ll use ∗ to denote the operation:\n\nManual Feature Extraction: SIFT and HOG\n\n|\n\n135",
      "content_length": 2609,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 152,
      "content": "[a b c] ∗ [1 2 3] = c*1 + b*2 + a*3\n\nThe x and y gradients at pixel (i,j) are:\n\ngx(i,j) = [1 0 –1] ∗ [I(i – 1,j) I(i,j) I(i + 1,j)] = –1 * I(i – 1,j) + 1 * I(i + 1,j)\n\ngy(i,j) = [1 0 –1] ∗ [I(i,j – 1) I(i,j) I(i,j + 1)] = –1 * I(i,j – 1) + 1 * I(i,j + 1)\n\nTogether, they form the gradient:\n\n∇I(i, j) =\n\ngx(i, j) gy(i, j)\n\nA vector can be completely described by its direction and magnitude. The magnitude of the gradient is equal to the Euclidean norm of the gradient ( gx 2), which indi‐ cates how much the pixel values change around the pixel. The direction or orienta‐ tion of the gradient depends on the relative size of the change in the horizontal and vertical directions; it can be computed as θ = arctan( gy gx). Figure 8-2 illustrates these mathematical concepts.\n\n2 + gy\n\nFigure 8-3 illustrates examples of the simple image gradient that is composed of the vertical and horizontal gradients. Each example is an image of nine pixels. Each pixel is labeled with a grayscale value. (Smaller numbers correspond to a darker color.) The gradient for the center pixel is shown below each image. The image on the left con‐ tains horizontal stripes, where the color only changes vertically. Therefore, the hori‐ zontal gradient is zero and the gradient is nonzero vertically. The center image contains vertical stripes; therefore, the horizontal gradient is zero. The image on the right contains diagonal stripes and the gradient is also diagonal.\n\n136\n\n|\n\nChapter 8: Automating the Featurizer: Image Feature Extraction and Deep Learning",
      "content_length": 1538,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 153,
      "content": "Figure 8-2. Illustration of the definition of an image gradient\n\nFigure 8-3. Simple examples of the image gradient\n\nThe definition works on synthetic toy examples. But would it work well on a real image? In Example 8-1, we examine this using a picture of a cat from scikit-image, shown in Figure 8-4 with its horizontal and vertical gradients. Since the gradients are computed at every pixel location of the original image, we end up with two new matrices, each of which can be visualized as an image.\n\nManual Feature Extraction: SIFT and HOG\n\n|\n\n137",
      "content_length": 550,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 154,
      "content": "Example 8-1. Calculating simple image gradients using Python\n\n>>> import matplotlib.pyplot as plt >>> import numpy as np >>> from skimage import data, color\n\n### Load the example image and turn it into grayscale >>> image = color.rgb2gray(data.chelsea())\n\n### Compute the horizontal gradient using the centered 1D filter. ### This is equivalent to replacing each non-border pixel with the ### difference between its right and left neighbors. The leftmost ### and rightmost edges have a gradient of 0. >>> gx = np.empty(image.shape, dtype=np.double) >>> gx[:, 0] = 0 >>> gx[:, -1] = 0 >>> gx[:, 1:-1] = image[:, :-2] - image[:, 2:]\n\n### Same deal for the vertical gradient >>> gy = np.empty(image.shape, dtype=np.double) >>> gy[0, :] = 0 >>> gy[-1, :] = 0 >>> gy[1:-1, :] = image[:-2, :] - image[2:, :]\n\n### Matplotlib incantations >>> fig, (ax1, ax2, ax3) = plt.subplots(3, 1, ... figsize=(5, 9), ... sharex=True, ... sharey=True)\n\n>>> ax1.axis('off') >>> ax1.imshow(image, cmap=plt.cm.gray) >>> ax1.set_title('Original image') >>> ax1.set_adjustable('box-forced')\n\n>>> ax2.axis('off') >>> ax2.imshow(gx, cmap=plt.cm.gray) >>> ax2.set_title('Horizontal gradients') >>> ax2.set_adjustable('box-forced')\n\n>>> ax3.axis('off') >>> ax3.imshow(gy, cmap=plt.cm.gray) >>> ax3.set_title('Vertical gradients') >>> ax3.set_adjustable('box-forced')\n\n138\n\n|\n\nChapter 8: Automating the Featurizer: Image Feature Extraction and Deep Learning",
      "content_length": 1426,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 155,
      "content": "Figure 8-4. Gradients of an image of a cat\n\nNote that the horizontal gradient picks out strong vertical patterns such as the inner edges of the cat’s eyes, while the vertical gradient picks out strong horizontal patterns such as the whiskers and the upper and lower lids of the eyes. This might seem a little paradoxical at first, but it makes sense once we think about it a bit more. The hori‐ zontal (x) gradient identifies changes in the horizontal direction. A strong vertical pattern spans multiple y pixels at roughly the same x position. Hence, vertical pat‐ terns result in horizontal differences in pixel values. This is what our eyes detect as well.\n\nGradient Orientation Histograms Individual image gradients can pick out minute differences in an image neighbor‐ hood. But our eyes see bigger patterns than that. For instance, we see an entire cat’s whisker, not just a small section. The human vision system identifies contiguous pat‐ terns in a region, so we still have more work to do to summarize the image gradients in a neighborhood.\n\nHow exactly might we summarize vectors? A statistician would answer, “Look at the distribution!” SIFT and HOG both take this path. In particular, they compute (nor‐ malized) histograms of the gradient vectors as image features. A histogram divides data into bins and counts how many data points are in each bin; this is an (unnor‐ malized) empirical distribution. Normalization ensures that the counts sum to 1. The mathematical language is that it has unit ℓ1 norm.\n\nManual Feature Extraction: SIFT and HOG\n\n|\n\n139",
      "content_length": 1567,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 156,
      "content": "An image gradient is a vector, and vectors can be represented by two components: the orientation and magnitude. So, we still need to decide how to design the histo‐ gram to take both components into account. SIFT and HOG settled on a scheme where the image gradients are binned by their orientation angle θ, weighted by the magnitude of each gradient. Here is the procedure:\n\n1. Divide 0°–360° into equal-sized bins.\n\n2. For each pixel in the neighborhood, add a weight w to the bin corresponding to its orientation θ. w is a function of the magnitude of the gradient and other rele‐ vant information. For instance, that information might be the inverse distance of the pixel to the center of the image patch. The idea is that the weight should be large if the gradient is large, and pixels near the center of the image neighbor‐ hood matter more than pixels that are farther away.\n\n3. Normalize the histogram.\n\nFigure 8-5 provides an illustration of a gradient orientation histogram of 8 bins com‐ posed from an image neighborhood of 4 × 4 pixels.\n\nFigure 8-5. Illustration of a gradient orientation histogram of 8 bins based on gradients from a 4 × 4 square cell of pixels\n\nThere are, of course, a number of knobs to tweak in the basic gradient orientation histogram algorithm, as well as some optional bells and whistles. As usual, the right settings are probably highly dependent on the particular images one wants to analyze.\n\nLet’s examine next some of the decisions to make and the effects these can have on your model.\n\n140\n\n|\n\nChapter 8: Automating the Featurizer: Image Feature Extraction and Deep Learning",
      "content_length": 1616,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 157,
      "content": "How many bins should there be? Should they span from 0°–360° (signed gradients) or 0°–180° (unsigned gradients)?\n\nHaving more bins leads to finer-grained quantization of gradient orientation, and thus retains more information about the original gradients. But having too many bins is unnecessary and could lead to overfitting to the training data. For example, recognizing a cat in an image probably does not depend on the cat’s whisker being oriented exactly at 3°.\n\nThere is also the question of whether the bins should span from 0°–360°, which would retain the sign of the gradient along the y-axis, or from 0°–180°, which would not retain the sign of the vertical gradient. The authors of the original HOG paper (Dalal and Triggs, 2005) experimentally determined that 9 bins spanning from 0°– 180° is best, whereas the SIFT paper (Lowe, 2004) recommended 8 bins spanning from 0°–360°.\n\nWhat weight functions should be used?\n\nThe HOG paper compares various gradient magnitude weighting schemes: the mag‐ nitude itself, its square or square root, binarized, or clipped at the high or low ends. The plain magnitude, without adornments, performed the best in the authors’ experi‐ ments.\n\nSIFT also uses the plain magnitude of the gradient. Additionally, it wants to avoid sudden changes in the feature descriptor resulting from small changes in the position of the image window, so it downweights gradients that come from the edges of the neighborhood using a Gaussian distance function measured from the window center.\n\n∥2/2σ 2\n\n∥p-p0\n\n1\n\n, where p is the In other words, the gradient magnitude is multiplied by location of the pixel that generated the gradient, p0 is the location of the center of the image neighborhood, and σ, the width of the Gaussian, is set to one-half the radius of the neighborhood.\n\n2πσ 2 e\n\nSIFT also wants to avoid large changes in the orientation histogram resulting from small changes in the orientation of individual image gradients. So, it uses an interpo‐ lation trick that spreads the weight from a single gradient into adjacent orientation bins. In particular, the root bin (the bin that the gradient is assigned to) gets a vote of 1 times the weighted magnitude. Each of the adjacent bins get a vote of 1 – d, where d is the difference in histogram bin unit from the root bin.\n\nOverall, the vote from a single image gradient for SIFT is:\n\nw(∇p,b) = wbσ ∥ ∇ p ∥\n\nwhere ∇p is the gradient of pixel p in bin b, wb is the interpolation weight of b, and σ is the Gaussian distance to the center from p.\n\nManual Feature Extraction: SIFT and HOG\n\n|\n\n141",
      "content_length": 2583,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 158,
      "content": "How are neighborhoods defined? How should they cover the image?\n\nHOG and SIFT both settled on a two-level representation of image neighborhoods: first adjacent pixels are organized into cells, and neighboring cells are then organized into blocks. An orientation histogram is computed for each cell, and the cell histo‐ gram vectors are concatenated to form the final feature descriptor for the whole block.\n\nSIFT uses cells of 16 × 16 pixels, organized into 8 orientation bins, then grouped by blocks of 4 × 4 cells, making for 4 × 4 × 8 = 128 features for the image neighborhood.\n\nThe HOG paper experimented with rectangular and circular shapes for the cells and blocks. Rectangular cells are called R-HOG blocks. The best R-HOG setting was found to be 8 × 8 pixels with 9 orientation bins each, grouped into blocks of 2 × 2 cells. Circular cells are called C-HOG blocks, with variants determined by the radius of the central cell, whether or not the cells are radially divided, the width of the outer cells, etc.\n\nNo matter how the neighborhoods are organized, they typically overlap to form the feature vector for the whole image. In other words, cells and blocks shift across the image horizontally and vertically, a few pixels at a time, to cover the entire image.\n\nThe main ingredients of neighborhood architecture are multilevel organization and overlapping windows that shift across the image. The same ingredients are utilized in the design of deep learning networks.\n\nWhat kind of normalization should be done?\n\nNormalization evens out the feature descriptors so that they have comparable magni‐ tude. It is synonymous with scaling, which we discussed in Chapter 4. We found that feature scaling on text features (in the form of tf-idf) did not have a large effect on classification accuracy. The story is quite different for image features, which can be quite sensitive to changes in lighting and contrast that appear in natural images. For instance, consider images of an apple under a strong spotlight versus a soft diffused light coming through a window. The image gradients would have very different mag‐ nitudes, even though the object is the same. For this reason, image featurization in computer vision usually starts with global color normalization to remove illumina‐ tion and contrast variance. For SIFT and HOG, it turns out that such preprocessing is unnecessary so long as we normalize the features.\n\nSIFT follows a normalize–threshold–normalize scheme. First, the block feature vec‐ tor is normalized to unit length (ℓ2 normalization). Then, the features are clipped to a maximum value in order to get rid of extreme lighting effects such as color saturation from the camera. Finally, the clipped features are again normalized to unit length.\n\nThe HOG paper experimented with different normalization schemes involving ℓ2 and ℓ1 norms, including the normalize–threshold–normalize scheme used in the SIFT\n\n142\n\n|\n\nChapter 8: Automating the Featurizer: Image Feature Extraction and Deep Learning",
      "content_length": 3017,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 159,
      "content": "paper. The authors found pure ℓ1 normalization to be slightly less reliable than the other methods (which performed comparably).\n\nSIFT Architecture The SIFT pipeline requires quite a number of steps. HOG is slightly simpler but fol‐ lows many of the same basic steps, such as creating a gradient histogram and normal‐ ization. Figure 8-6 illustrates the SIFT architecture. Starting from a region of interest in the original image, we first divide the region into a grid. Each grid cell is then fur‐ ther divided into subgrids. Each subgrid element contains a number of pixels, and each pixel produces a gradient. Each subgrid element produces a weighted gradient estimate, where the weights are chosen so that gradients outside of the subgrid ele‐ ment can contribute. These gradient estimates are then aggregated into an orienta‐ tion histogram for the subgrid, where gradients can have weighted votes as described previously. The orientation histograms for each subgrid are then concatenated to form a long gradient orientation histogram for the entire grid. (If the grid is divided into 2 × 2 subgrids, then there will be 4 gradient orientation histograms to concate‐ nate into 1.) This is the feature vector for the grid, which then goes through a nor‐ malize–threshold–normalize process. First, the vector is normalized to have unit norm. Then, individual values are clipped to a maximum threshold. Finally, the thresholded vector is normalized again. This is the final SIFT feature descriptor for the image patch.\n\nFigure 8-6. SIFT architecture—steps to produce a feature vector for a region of interest in the original image\n\nManual Feature Extraction: SIFT and HOG\n\n|\n\n143",
      "content_length": 1680,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 160,
      "content": "Learning Image Features with Deep Neural Networks SIFT and HOG went a long way toward defining good image features. However, the latest gains in computer vision have come from a very different direction: deep neural network models. The breakthrough happened at the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012, where a group of researchers from the University of Toronto nearly halved the error rate of the previous year’s winner. They branded their method “deep learning” to emphasize that, unlike previous architec‐ ture neural network models, the latest generation contains many layers of neural net‐ works and transformations stacked on top of each other. The winning model of ILSVRC 2012—subsequently dubbed AlexNet, after the name of the lead author—has 13 layers (Krizhevsky et al., 2012). The winner of ILSVRC 2014, GoogLeNet, has 22 layers (Szegedy et al., 2014).\n\nOn the surface, the mechanism of stacked neural networks appears very different from the image gradient histograms of SIFT and HOG. But a visualization of AlexNet shows that the first few layers are essentially computing edge gradients and other simple patterns, much like SIFT and HOG. Subsequent layers combine local patterns into more global patterns. The end result is a feature extractor that is much more powerful than what came before.\n\nThe infrastructure of stacked layers of neural networks (or any other classification model) is not new. But training such complex models requires a lot of data and a lot of computing power, which was not available until recently. The ImageNet dataset contains a labeled set of 1.2 million images from 1,000 classes. Modern GPUs have sped up matrix-vector computations, which lie at the inner core of many machine learning models (including neural networks). The success of deep learning methods rests upon the availability of lots of data and lots of GPU hours.\n\nDeep learning architectures can be composed of several types of layers. AlexNet, for instance, contains fully connected, convolutional response normalization, and max- pooling layers. We’ll now look at each of these in turn.\n\nFully Connected Layers At the core of all neural networks are linear functions of the input. Logistic regres‐ sion, which we encountered in Chapter 4, is an example of a neural network. A fully connected neural network is simply a set of linear functions of all of the input fea‐ tures. Recall that a linear function can be written as an inner product between the input feature vector and a weight vector, plus a possible constant term. A collection of linear functions can be represented as a matrix-vector product, where the weight vector becomes a weight matrix (W).\n\n144\n\n|\n\nChapter 8: Automating the Featurizer: Image Feature Extraction and Deep Learning",
      "content_length": 2788,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 161,
      "content": "The mathematical definition of a fully connected layer is:\n\nz = Wx + b\n\nwhere each row of W is a weight vector that maps the entire input vector x into a single output in z. b is a vector of scalars representing the constant offset (or bias) for each neuron.\n\nThe fully connected layer is so named because every input can be used in every out‐ put. Mathematically, this means that there are no restrictions on the values in the matrix W. (As we will soon see, a convolutional layer makes use of only a small sub‐ set of inputs for each output.) Pictorially, a fully connected neural net can be repre‐ sented by a complete bipartite graph where every node in the input is connected to every node in the output (see Figure 8-7).\n\nFigure 8-7. A fully connected neural network, represented as a graph\n\nFully connected layers contain the maximum possible number of parameters (#input × #output)—hence, they are considered expensive. Such dense connection allows the network to detect global patterns that could involve all inputs. The last two layers of AlexNet are fully connected for this reason. The outputs are still independent from each other, conditioned on the inputs.\n\nLearning Image Features with Deep Neural Networks\n\n|\n\n145",
      "content_length": 1230,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 162,
      "content": "Convolutional Layers In contrast to fully connected layers, a convolutional layer uses only a subset of inputs for each output. The transformation “moves” across the input, producing out‐ puts using a few features at a time. For simplicity, one can use the same weights for different sets of input, instead of learning new weights for each set of input.\n\nMathematically, the convolution operator takes two functions as input and produces one function as output. It flips one of the input functions, moves it across the other function, and outputs the total area under the multiplied curves at each point:\n\n(f * g)(t) =∫–∞\n\n∞\n\nf (τ)g(t – τ)dτ =∫–∞\n\n∞\n\ng(τ)f (t – τ)dτ\n\nThe way to compute total area under a curve is to take its integral. The operator is symmetric in the inputs, meaning that it does not matter whether we flip the first input or the second; the output is the same.\n\nWe’ve already seen an example of simple convolution, when we looked at image gra‐ dients (“Image Gradients” on page 135). But the mathematical definition of convolu‐ tion may still appear to be somewhat convoluted. There is reason to its madness. It’s easiest to explain the intuition behind convolution using an example from signal processing.\n\nImagine that we have a little black box. To see what the black box does, we pass a single unit of stimulus through it. We record whatever the output looks like on a little sheet of paper. We wait until there is no more response to the original stimulus. The resulting function over time is the response function; let’s call it g(t).\n\nImagine now that we have some crazy wild signal f(t), which we proceed to feed through the black box. At time t = 0, f(0) interacts with the black box and produces f(0) multiplied by g(0). At time t = 1, f(1) enters the black box and gets multiplied by g(0). At the same time, the black box continues to respond to the previous signal f(0), which is now multiplied by g(1). So, the total output at time t = 1 is (f(0) * g(1)) + (f(1) * g(0)). At time t = 2, the situation gets even more complicated, with f(2) enter‐ ing the picture, and f(0) and f(1) continuing to generate their responses. The total output at time t = 2 is (f(0) * g(2)) + (f(1) * g(1)) + (f(2) * g(0)). In this way, the response function effectively gets flipped in time, with τ = 0 always interacting with whatever is currently entering the black box, and the tail of the response function interacting with whatever came before.\n\nFigure 8-8 illustrates the quantities at play at each time step (note that we’ve made time discrete for convenience of description—in reality, time is continuous, so the summation is really an integral). When computing the value of the convolution at a particular time step, you multiply the overlapping signals together and sum them.\n\n146\n\n|\n\nChapter 8: Automating the Featurizer: Image Feature Extraction and Deep Learning",
      "content_length": 2887,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 163,
      "content": "Figure 8-8. Convolution of two discrete signals, f and g\n\nThis black box is called a linear system because it doesn’t do anything more crazy than scalar multiplication and summation. The convolution operator cleanly cap‐ tures the effect of a linear system.\n\nIntuition Behind Convolution\n\nThe convolution operator captures the effect of a linear system, which multiplies the incoming signal with its response function, summing over current responses to all past input.\n\nIn our example, g(t) is used to denote the response function, and f(t) the input. But since convolution is symmetric, it doesn’t really matter which is the response and which the input. The output is simply a combination of both. g(t) is also known as a filter.1\n\nImages are two-dimensional signals, so we need a 2D filter. A 2D convolutional filter extends the 1D case by taking the integral over two variables:\n\n(f * g) i, j = ∑\n\nm\n\nn\n\n∑\n\nf u, v g i – u, j – v\n\nu=0\n\nv=0\n\n1 Technically, a filter is a transformation that eliminates certain parts of the Fourier spectrum. But it is increas‐\n\ningly common to use “filter” as a generic term.\n\nLearning Image Features with Deep Neural Networks\n\n|\n\n147",
      "content_length": 1169,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 164,
      "content": "Since digital images have discrete pixels, the convolution integrals become discrete sums. Furthermore, since the number of pixels is finite, the filter function only needs a finite number of elements. In image processing, a 2D convolutional filter is also known as a kernel or a mask.\n\nWhen applying a convolutional filter to an image, one does not necessarily define a giant filter that covers the entire image. Rather, one formulates a small filter covering just a few pixels by a few pixels and applies the same filter across the image, shifting over the horizontal and vertical pixel directions (see Figure 8-9).\n\nFigure 8-9. Structure of a 1D convolutional neural net\n\nBecause the same filter is used across the image, one only needs to define a small set of parameters. The trade-off is that the filter can absorb information only within a small pixel neighborhood at a time. In other words, a convolutional neural net identi‐ fies local patterns instead of global ones.\n\nConvolutional Filter Example In this example, we apply a Gaussian filter to an image. The Gaussian function forms a smooth and symmetric mound around zero. The filter produces a weighted average of nearby function values. When applied to an image, it has the effect of blurring nearby pixel values. The 2D Gaussian filter is defined by:\n\n148\n\n|\n\nChapter 8: Automating the Featurizer: Image Feature Extraction and Deep Learning",
      "content_length": 1405,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 165,
      "content": "G(x, y) =\n\n1 2πσ\n\ne\n\n–\n\nx\n\n2\n\n+y 2\n\n2σ\n\n2\n\n,\n\nwhere σ is the standard deviation of the Gaussian function, which controls the width of the “mound.”\n\nIn Example 8-2, we’ll first create a 2D Gaussian filter, then convolve it with our favor‐ ite cat image to produce a blurred cat (see Figure 8-10). Note that this is not the most accurate way to compute a Gaussian filter, but it is the easiest to understand. A better implementation would take the weighted average value at each discrete point rather than the simple point estimate.\n\nExample 8-2. Applying a simple Gaussian filter on an image\n\n>>> import numpy as np\n\n# First create X,Y meshgrids of size 5x5 on which we compute the Gaussian >>> ind = [-1., -0.5, 0., 0.5, 1.] >>> X,Y = np.meshgrid(ind, ind) >>> X array([[-1. , -0.5, 0. , 0.5, 1. ], [-1. , -0.5, 0. , 0.5, 1. ], [-1. , -0.5, 0. , 0.5, 1. ], [-1. , -0.5, 0. , 0.5, 1. ], [-1. , -0.5, 0. , 0.5, 1. ]])\n\n# G is a simple, unnormalized Gaussian kernel where the value at (0,0) is 1.0 >>> G = np.exp(-(np.multiply(X,X) + np.multiply(Y,Y))/2) >>> G array([[ 0.36787944, 0.53526143, 0.60653066, 0.53526143, 0.36787944], [ 0.53526143, 0.77880078, 0.8824969 , 0.77880078, 0.53526143], [ 0.60653066, 0.8824969 , 1. , 0.8824969 , 0.60653066], [ 0.53526143, 0.77880078, 0.8824969 , 0.77880078, 0.53526143], [ 0.36787944, 0.53526143, 0.60653066, 0.53526143, 0.36787944]])\n\n>>> from skimage import data, color >>> cat = color.rgb2gray(data.chelsea())\n\n>>> from scipy import signal >>> blurred_cat = signal.convolve2d(cat, G, mode='valid')\n\n>>> import matplotlib.pyplot as plt >>> fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,4), ... sharex=True, sharey=True)\n\n>>> ax1.axis('off') >>> ax1.imshow(cat, cmap=plt.cm.gray) >>> ax1.set_title('Input image') >>> ax1.set_adjustable('box-forced')\n\n>>> ax2.axis('off')\n\nLearning Image Features with Deep Neural Networks\n\n|\n\n149",
      "content_length": 1872,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 166,
      "content": ">>> ax2.imshow(blurred_cat, cmap=plt.cm.gray) >>> ax2.set_title('After convolving with a Gaussian filter') >>> ax2.set_adjustable('box-forced')\n\nFigure 8-10. An image of a cat, before and after applying a 2D Gaussian filter\n\nThe convolutional layers in AlexNet are three-dimensional. In other words they operate on voxels (values in the array representing the 3D space of the image) from the previous layer. The first convolutional neural net takes raw RGB images and learns convolution filters for a local image neighborhood across all three color chan‐ nels. Subsequent layers take as input voxels across space and kernel dimensions. See Figure 8-14 for more details.\n\nRectified Linear Unit (ReLU) Transformation The output of a neural net is often passed through another nonlinear transformation, also known as an activation function. Common choices are the tanh function (a smooth nonlinear function bounded between –1 and 1), the sigmoid function (a smooth nonlinear function bounded between 0 and 1, introduced in “Classification with Logistic Regression” on page 66), or what’s known as a rectified linear unit. A ReLU is a simple variation of a linear function where the negative part is zeroed out. In other words, it trims away the negative values, but leaves the positive part unboun‐ ded. The range of ReLU extends from 0 to ∞.\n\nCommon Activation Functions A ReLU is a linear function with the negative part zeroed out:\n\nReLU(x) = max(0, x)\n\nThe tanh function is a trigonometric function that smoothly increases from –1 to 1:\n\n150\n\n|\n\nChapter 8: Automating the Featurizer: Image Feature Extraction and Deep Learning",
      "content_length": 1627,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 167,
      "content": "tanh(x) =\n\nsinh(x) cosh(x)\n\n=\n\ne x – e –x e x + e –x\n\nThe sigmoid function increases smoothly from 0 to 1:\n\nsigmoid(x) =\n\n1 1 + e –x\n\nThe three functions are illustrated in Figure 8-11.\n\nFigure 8-11. Illustration of three common activation functions: ReLU, tanh, and sigmoid\n\nThe ReLU transformation has no effect on nonnegative functions such as the raw image or the Gaussian filter. However, a trained neural net, whether fully connected or convolutional, will likely output negative values. AlexNet uses ReLU instead of other transformations, citing faster convergence during training (Krizhevsky et al., 2012). It applies ReLU to every convolutional and fully connected layer.\n\nResponse Normalization Layers After the discussions in Chapter 4 and earlier in this chapter, normalization should by now be a familiar concept. Normalization divides an individual output by a func‐ tion of the collective total response. Hence, another way of understanding normaliza‐ tion is that it creates competition amongst neighbors because the strength of each output is now measured relative to its neighbors (see Figure 8-12). AlexNet normal‐ izes the output at each location across different kernels.\n\nLearning Image Features with Deep Neural Networks\n\n|\n\n151",
      "content_length": 1251,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 168,
      "content": "Figure 8-12. Structure of response normalization over convolution kernel outputs from the previous layer—the normalization constants are computed based on a neighbor‐ hood from the previous layer\n\nLocal Response Normalization Breeds Competition Among Neighboring Kernels As the name suggests, local response normalization divides a value by a combination of its neighbors. Here is the formula:\n\nyk = xk /(c + α∑ℓ∈neighborhood of k xℓ\n\n2)β\n\nHere, xk is the output of the kth kernel, and yk is the normalized response relative to other kernels in the neighborhood. The normalization is performed separately for each output location. That is, for each output location (i,j), we normalize across the nearby convolution kernel outputs. Note that this isn’t the same as normalizing over the image neighborhood or output locations. The size of the kernel neighborhood, c, α, and β are all hyperparameters that are tuned via a validation set of images.\n\n152\n\n|\n\nChapter 8: Automating the Featurizer: Image Feature Extraction and Deep Learning",
      "content_length": 1034,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 169,
      "content": "Pooling Layers A pooling layer combines multiple inputs into a single output. As the convolutional filter moves across an image, it generates an output for every neighborhood under its lens. Pooling forces a local image neighborhood to produce one value instead of many. This reduces the number of outputs in the intermediate layers of the deep learning network, which effectively reduces the probability of overfitting the network to training data.\n\nThere are multiple ways to pool inputs: averaging, summing (or computing a gener‐ alized norm), or taking the maximum value. Pooling moves across the image or intermediate output layers. AlexNet uses overlapping max pooling, moving across the image in strides of two pixels (or outputs) and pooling across three neighbors.\n\nFigure 8-13. Max pooling outputs the maximum number of nonoverlapping rectangles per subregion using nonlinear downsampling\n\nStructure of AlexNet All together, AlexNet involves five convolution layers, two response normalization layers, three max pooling layers, and two fully connected layers. Combined with the final classification output layer, there are a total of 13 neural network layers in the model, forming 8 layer groups. See Figure 8-14 for details.\n\nLearning Image Features with Deep Neural Networks\n\n|\n\n153",
      "content_length": 1294,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 170,
      "content": "Figure 8-14. Architecture diagram of AlexNet—the different shades of gray (or magenta and blue, if you’re viewing the illustrations in color) denote layers that reside on GPU 1 and GPU 2\n\nThe input image is first scaled to 256 × 256 pixels. The input is actually random crops of size 224 × 224, with 3 color channels. The first two convolution layers are each fol‐ lowed by a response normalization layer and a max pooling layer, and the last convo‐ lution layer is followed by max pooling. The original paper splits training data and computation across two GPUs. Communications between layers are mostly limited to within the same GPU. The exceptions are between layer groups 2 and 3, and after layer group 5. At those boundary points, the next layer takes as input a voxel of ker‐ nels from the previous layer across both GPUs. ReLU transformation follows every intermediate layer.\n\nFigure 8-15 shows a detailed view of convolution+response normalization+max pooling. Note that the normalization constant is computed across kernels, whereas pooling happens across image regions. Also, pooling reduces the dimension of the layer.\n\n154\n\n|\n\nChapter 8: Automating the Featurizer: Image Feature Extraction and Deep Learning",
      "content_length": 1220,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 171,
      "content": "Figure 8-15. Detailed view of convolution+response normalization+max pooling\n\nNote that AlexNet’s architecture is reminiscent of the gradient histogram–normal‐ (see ize–threshold–normalize architecture of SIFT/HOG Figure 8-6), but with many more layers. (Hence the “deep” in “deep learning.”) Unlike in SIFT/HOG, however, the convolution kernels and full connection weights are learned from data, not predefined. Also, the normalization steps in SIFT are per‐ formed across the feature vector over the entire image region, whereas the response normalization layer in AlexNet normalizes across the convolution kernels.\n\nfeature extractors\n\nAt a high level, the model starts by extracting patterns out of local image neighbor‐ hoods. Each subsequent layer builds upon the output of the previous layers, effec‐ tively covering successively larger areas of the original image. Hence, even though the first five convolution layers all have fairly small kernel widths, the later layers are able to formulate more global patterns. The fully connected layers at the end are the most global.\n\nAlthough the gist of patterns is conceptually clear, it is a hard problem to visualize the actual patterns each layer picks out. Figures 8-16 and 8-17 show visualizations of the first two layers of convolution kernels learned by the model. The first layer con‐ sists of detectors of grayscale edges and textures at different orientations, and color blobs and textures. The second layer appears to contain detectors of various smooth patterns.\n\nLearning Image Features with Deep Neural Networks\n\n|\n\n155",
      "content_length": 1585,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 172,
      "content": "Figure 8-16. Visualization of the first layer of convolution kernels in a trained AlexNet: the first half of the kernels are learned on GPU 1 and appear to detect grayscale edges and textures at different orientations; the second half, trained on a second GPU, focus on color blobs and patterns\n\nFigure 8-17. Visualization of the second layer of convolution kernels of a trained AlexNet\n\nDespite huge advances in the area, image featurization is still more of an art than a science. Ten years ago, people handcrafted feature extraction steps using a combina‐ tion of image gradients, edge detection, orientation, spatial cues, smoothing, and nor‐ malization. Nowadays, deep learning architects build models that encapsulate much\n\n156\n\n|\n\nChapter 8: Automating the Featurizer: Image Feature Extraction and Deep Learning",
      "content_length": 818,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 173,
      "content": "the same ideas, but the parameters are automatically learned from training images. The magic voodoo is still there, just hidden one abstraction deeper in the model!\n\nSummary Nearing the end, we can build on the intuition gained to better understand why the most straightforward and simple image features will not always be the most useful for performing tasks such as image classification. Instead of representing each pixel as an atomic unit, it is more important to consider the relationships pixels have with other pixels near them. We can adapt techniques developed for other tasks, such as SIFT and HOG, to better extract features across entire images by analyzing gradients in neighborhoods.\n\nThe next leap forward in recent years applies deep neural networks to computer vision to push feature extraction of images even further. The important thing to remember here is that deep learning stacks many layers of neural networks and trans‐ formations on top of each other. Some of these layers, when examined individually, begin to tease out similar features that can be identified as building blocks for human vision: defining lines, gradients, color maps.\n\nBibliography “CS231n: Convolutional Neural Networks for Visual Recognition.” Retrieved from http://cs231n.github.io/convolutional-networks/.\n\nDalal, Navneet, and Bill Triggs. “Histograms of Oriented Gradients for Human Detection.” Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (2005): 886–893.\n\nEliot, Lise. What’s Going On in There? How the Brain and Mind Develop in the First Five Years of Life. New York: Bantam Books, 2000.\n\nKrizhevsky, Alex, Ilya Sutskever, and Geoffrey Hinton. “ImageNet Classification with Deep Convolutional Neural Networks.” Advances in Neural Information Processing Systems 25 (2012): 1097–1105.\n\nLowe, David G. “Object Recognition from Local Scale-Invariant Features.” Proceed‐ ings of the International Conference on Computer Vision (1999): 1150–1157.\n\nLowe, David G. “Distinctive Image Features from Scale-Invariant Keypoints.” Inter‐ national Journal of Computer Vision 60:2 (2004): 91–110.\n\nMalisiewicz, Tomasz. “From Feature Descriptors to Deep Learning: 20 Years of Com‐ puter Vision.” Tombone’s Computer Vision Blog, January 20, 2015. http:// www.computervisionblog.com/2015/01/from-feature-descriptors-to-deep.html.\n\nSummary\n\n|\n\n157",
      "content_length": 2385,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 174,
      "content": "Szegedy, Christian, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. “Going Deeper with Convolutions.” Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition (2015): 1–9.\n\nZeiler, Matthew D., and Rob Fergus. “Visualizing and Understanding Convolutional Networks,” Proceedings of the 13th European Conference on Computer Vision (2014): 818–833.\n\n158\n\n|\n\nChapter 8: Automating the Featurizer: Image Feature Extraction and Deep Learning",
      "content_length": 540,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 175,
      "content": "CHAPTER 9 Back to the Feature: Building an Academic Paper Recommender\n\n“In mathematics you don’t understand things. You just get used to them.”\n\n—John von Neumann\n\nWhen the path from data to results was first introduced in Figure 1-1, it may not have been clear how there would ever be a way forward. Throughout this book, we have focused on introducing basic principles of feature engineering using toy models and clean, simple datasets. These examples were intended to be illustrative and enlightening.\n\nMachine learning examples generally show the best-case scenario and results. This masks the path we have described thus far in the book. Now that the foundation is set, we are leaving the world of simple, toy data and diving into the process of feature engineering with a real-world, structured dataset. As we move through each step, we will be examining the raw data forming each feature, what the transformed feature becomes, and what trade-offs we make along the way.\n\nTo be clear, our goal for this example is not to build the best model for this dataset. Rather, it is to demonstrate the practical application of a handful of our techniques, as well as how to more deeply examine and understand whether each technique is providing value to the model one is building.\n\nItem-Based Collaborative Filtering Our task will be to build a recommender for academic papers using a subsample of the Microsoft Academic Graph dataset. This should come in extremely handy for all\n\n159",
      "content_length": 1481,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 176,
      "content": "of you who are searching for citations but have not yet discovered Google Scholar. Here are some relevant statistics about the dataset:\n\nMicrosoft Academic Graph Dataset\n\nIt contains 166,192,182 unique papers, available via Open Academic Graph. It is intended to be used for research purposes only.\n\nThe total size of the dataset is 104 GB. • Each observation has 18 variables to identify each paper, including the paper’s title, abstract, authors, keywords, and fields of study.\n\nThe dataset is designed to be easy to store and access in a database. It is not tidy for machine learning models out of the box, but requires some initial wrangling. Some teachers like to spare you this step, boosting your ego by getting directly to the mod‐ els and results. None of that here. We are starting together from the very beginning.\n\nOur initial approach will be to wrangle a few variables into the right shape to push through an item-based collaborative filter. We will see if reasonably similar papers can be found in a timely and efficient manner.\n\nThe Origins of Item-Based Collaborative Filtering\n\nThis approach was first developed at Amazon as an improvement to user-based algorithms for recommending products. Sarawar et al. (2001) walk through the challenges and benefits of switching the perspective in recommenders from the user to the item.\n\nItem-based collaborative filtering provides recommendations based on the similarity between items. This works in two stages: first finding the similarity scores between items, then ranking all scores to find the top-N similar item recommendations.\n\nBuilding an Item-Based Recommender\n\nAn item-based recommender performs three tasks:\n\n1. Generalize information about a “thing” or item.\n\n2. Score all other items to find ones “like” this one.\n\n3. Return ranked scores + items.\n\n160\n\n|\n\nChapter 9: Back to the Feature: Building an Academic Paper Recommender",
      "content_length": 1900,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 177,
      "content": "First Pass: Data Import, Cleaning, and Feature Parsing Like all good science experiments, we will start off with a hypothesis. In this case, we assume that papers published at about the same time and in similar fields of study will be the most useful to users. We will take a naive approach of parsing out these fields from a subsample of the overall dataset. After generating simple sparse arrays, we’ll run the entire item array through an item-based collaborative filter to see if we get good results.\n\nThe item-based collaborative filter depends on a similarity score to compare items. In this case, the cosine similarity provides a reasonable comparison between two non- zero vectors. The following example actually uses the cosine distance, which is the complement of the cosine similarity in the positive space, or:\n\nDC(A,B) = 1 – SC(A,B)\n\nwhere DC is the cosine distance and SC is the cosine similarity.\n\nAcademic Paper Recommender: Naive Approach The first step in our journey is to import and examine the dataset. In Example 9-1, we scope our experiment by limiting the fields available after the initial import. These fields are still rich in possibility, as shown in Figure 9-1.\n\nExample 9-1. Import + filter data\n\n>>> import pandas as pd\n\n>>> model_df = pd.read_json('data/mag_papers_0/mag_subset20K.txt', lines=True) >>> model_df.shape (20000, 19) >>> model_df.columns Index(['abstract', 'authors', 'doc_type', 'doi', 'fos', 'id', 'issue', 'keywords', 'lang', 'n_citation', 'page_end', 'page_start', 'publisher', 'references', 'title', 'url', 'venue', 'volume', 'year'], dtype='object')\n\n# filter out non-English articles and focus on a few variables >>> model_df = model_df[model_df.lang == 'en'] ... .drop_duplicates(subset='title', keep='first') ... .drop(['doc_type', 'doi', 'id', 'issue', 'lang', 'n_citation', ... 'page_end', 'page_start', 'publisher', 'references', ... 'url', 'venue', 'volume'], ... axis=1) >>> model_df.shape (10399, 6)\n\nFirst Pass: Data Import, Cleaning, and Feature Parsing\n\n|\n\n161",
      "content_length": 2023,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 178,
      "content": "Figure 9-1. First two rows of the Microsoft Academic Graph dataset\n\nTable 9-1 summarizes best how further wrangling is needed to get the raw data into a better shape for a model. Lists and dictionaries are good for data storage, but are not tidy or well suited for machine learning without some unpacking (Wickham, 2014).\n\nTable 9-1. Data schema for model_df\n\nField name Description abstract authors fos keywords title year\n\npaper abstract author names and affiliations fields of study keywords paper title published year\n\nField type string list of dict, keys = name, org list of strings list of strings string int\n\n# NaN 4393 1 1733 4294 0 0\n\nWe focus first on two fields in Example 9-2, transforming them from lists and inte‐ gers into a feature array, as shown in Figure 9-2.\n\nExample 9-2. Collaborative filtering stage 1: Build item feature matrix\n\n>>> unique_fos = sorted(list({feature ... for paper_row in model_df.fos.fillna('0') ... for feature in paper_row }))\n\n>>> unique_year = sorted(model_df['year'].astype('str').unique()) >>> def feature_array(x, var, unique_array): ... row_dict = {} ... for i in x.index: ... var_dict = {} ... for j in range(len(unique_array)): ... if type(x[i]) is list: ... if unique_array[j] in x[i]: ... var_dict.update({var + '_' + unique_array[j]: 1}) ... else: ... var_dict.update({var + '_' + unique_array[j]: 0}) ... else: ... if unique_array[j] == str(x[i]): ... var_dict.update({var + '_' + unique_array[j]: 1}) ... else: ... var_dict.update({var + '_' + unique_array[j]: 0}) ... row_dict.update({i : var_dict}) ... feature_df = pd.DataFrame.from_dict(row_dict, dtype='str').T ... return feature_df\n\n162\n\n|\n\nChapter 9: Back to the Feature: Building an Academic Paper Recommender",
      "content_length": 1723,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 179,
      "content": ">>> year_features = feature_array(model_df['year'], unique_year) >>> fos_features = feature_array(model_df['fos'], unique_fos)\n\n>>> first_features = fos_features.join(year_features).T\n\n>>> from sys import getsizeof >>> print('Size of first feature array: ', getsizeof(first_features)) Size of first feature array: 2583077234\n\nFigure 9-2. Head of first_features—observations’ (papers') indices from the original data set are columns, features are rows\n\nWe have now successfully turned a relatively small dataset, ~10K rows of raw data, into 2.5 GB of features. But this path is too sluggish for quick, iterative exploration. We need methods that will be faster and result in features that will consume less computational resources and experimentation time.\n\nFor now, though, let’s see how our current features perform at giving us a good rec‐ ommendation in the next stage (Example 9-3). We’ll define a “good” recommenda‐ tion as a paper that looks similar to the input.\n\nExample 9-3. Collaborative filtering stage 2: Search for similar items\n\n>>> from scipy.spatial.distance import cosine\n\n>>> def item_collab_filter(features_df): ... item_similarities = pd.DataFrame(index = features_df.columns, ... columns = features_df.columns) ... for i in features_df.columns: ... for j in features_df.columns: ... item_similarities.loc[i][j] = 1 - cosine(features_df[i], ... features_df[j]) ... return item_similarities\n\n>>> first_items = item_collab_filter(first_features.loc[:, 0:1000])\n\nWhy does it take so long for us to calculate the item similarities using only two fea‐ tures? We are taking the dot product of a 10,399 × 1,000 matrix using a nested for loop. The time per loop increases as we increase the number of observations we add\n\nFirst Pass: Data Import, Cleaning, and Feature Parsing\n\n|\n\n163",
      "content_length": 1796,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 180,
      "content": "to the model. Remember, this is a subset of the total available dataset, filtered for English-only papers. As we move closer to a “good” result, we’ll need to go back and test on the larger set for our best results.\n\nHow can we make this faster? Since we only need one result at a time, we can change our function so that we only calculate one item at a time, specifying the number of top results we want. We’ll do this later, as we continue to move through our experi‐ ment. For now, it is useful to see the full feature space to get an understanding of the impact of iterative work on brute-forcing our way through a real-world dataset.\n\nWe need to get a better idea of how these features will translate to us getting a good recommendation. Do we have enough observations to move forward? Let’s plot a heatmap (Example 9-4) to see if we have any papers that are similar to each other. Figure 9-3 shows the result.\n\nExample 9-4. Heatmap of paper recommendations\n\n>>> import matplotlib.pyplot as plt >>> import seaborn as sns >>> import numpy as np >>> %matplotlib inline >>> sns.set() >>> ax = sns.heatmap(first_items.fillna(0), ... vmin=0, vmax=1, ... cmap=\"YlGnBu\", ... xticklabels=250, yticklabels=250) >>> ax.tick_params(labelsize=12)\n\nDarker pixels signal items that are similar to one another. The dark diagonal line shows that the cosine similarity is correctly indicating that each paper is most similar to itself. However, because there are a lot of NaNs for one of our features, the line is broken along the diagonal. We can see that while most of the items are not similar to one another—i.e., our dataset is fairly diverse—there are some other high-scoring candidates. These may or may not be good recommendations qualitatively, but at least we can see that our methods are not so mad.\n\n164\n\n|\n\nChapter 9: Back to the Feature: Building an Academic Paper Recommender",
      "content_length": 1878,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 181,
      "content": "Figure 9-3. Heatmap of similar papers based on two raw features: year and fields of study\n\nExample 9-5 shows how to translate these item similarities into a recommendation. The good news is that we have a wide variety of features still available, with lots of room for improvement.\n\nExample 9-5. Item-based collaborative filtering recommendations\n\n>>> def paper_recommender(paper_ix, items_df): ... print('Based on the paper: \\nindex = ', paper_ix) ... print(model_df.iloc[paper_ix]) ... top_results = items_df.loc[paper_ix].sort_values(ascending=False).head(4) ... print('\\nTop three results: ') ... order = 1 ... for i in top_results.index.tolist()[-3:]: ... print(order,'. Paper index = ', i) ... print('Similarity score: ', top_results[i]) ... print(model_df.iloc[i], '\\n') ... if order < 5: order += 1\n\n>>> paper_recommender(2, first_items)\n\nBased on the paper: index = 2\n\nFirst Pass: Data Import, Cleaning, and Feature Parsing\n\n|\n\n165",
      "content_length": 940,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 182,
      "content": "abstract NaN authors [{'name': 'Jovana P. Lekovich', 'org': 'Weill ... fos NaN keywords NaN title Should endometriosis be an indication for intr... year 2015 Name: 2, dtype: object\n\nTop three results: 1 . Paper index = 2 Similarity score: 1.0 abstract NaN authors [{'name': 'Jovana P. Lekovich', 'org': 'Weill ... fos NaN keywords NaN title Should endometriosis be an indication for intr... year 2015 Name: 2, dtype: object\n\n2 . Paper index = 292 Similarity score: 1.0 abstract NaN authors [{'name': 'John C. Newton'}, {'name': 'Beers M... fos [Wide area multilateration, Maneuvering speed,... keywords NaN title Automatic speed control for aircraft year 1955 Name: 561, dtype: object\n\n3 . Paper index = 593 Similarity score: 1.0 abstract This paper demonstrates that on‐site greywater... authors [{'name': 'Eran Friedler', 'org': 'Division of... fos [Public opinion, Environmental Engineering, Wa... keywords [economic analysis, tratamiento desperdicios, ... title The water saving potential and the socio-econo... year 2008 Name: 1152, dtype: object\n\nYikes. The good news is that the most similar paper returned is the one we are look‐ ing for. The bad news is that the next two papers don’t seem to be very close to our initial search, even for the features we have chosen.\n\n“Yes, yes,” you may say, “but this is the era of Big Data! That will solve our problems! Can’t we just push more data through for better results?” Potentially. But even Big Data cannot compensate for poor data and engineering choices.\n\n166\n\n|\n\nChapter 9: Back to the Feature: Building an Academic Paper Recommender",
      "content_length": 1592,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 183,
      "content": "Figure 9-4. Machine learning (https://xkcd.com/1838/)\n\nOur current brute-force methods are too slow for smart, iterative engineering. Let’s try some of our new feature engineering tricks to see if we can speed up the computa‐ tion time and find better features and a better way to search for results.\n\nSecond Pass: More Engineering and a Smarter Model The initial approach of creating a large, sparse array and shoving it through a filter can be improved in many ways. The next steps will focus specifically on applying bet‐ ter techniques to the two initial features and altering the item-based collaborative fil‐ ter method for faster iteration.\n\nFirst, it is time to try out some of those great feature engineering tricks for the two variables in our hypothesis. Looking deeper into the features already developed, we can choose techniques that will address each type of variable and convert it to a “bet‐ ter” feature for our recommendation system.\n\nAcademic Paper Recommender: Take 2 Let’s focus on the year first. In “Quantization or Binning” on page 10, we reviewed how using raw counts for features can be problematic for methods using similarity metrics. Example 9-6 (and Figure 9-5) will examine how we can transform 'year' to better fit the model we have selected.\n\nSecond Pass: More Engineering and a Smarter Model\n\n|\n\n167",
      "content_length": 1334,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 184,
      "content": "Example 9-6. Fixed-width binning + dummy coding (part 1)\n\n>>> print(\"Year spread: \", model_df['year'].min(),\" - \", model_df['year'].max()) >>> print(\"Quantile spread:\\n\", model_df['year'].quantile([0.25, 0.5, 0.75])) Year spread: 1831 - 2017 Quantile spread: 0.25 1990.0 0.50 2005.0 0.75 2012.0 Name: year, dtype: float64\n\n# plot years to see the distribution >>> fig, ax = plt.subplots() >>> model_df['year'].hist(ax=ax, ... bins= model_df['year'].max() - model_df['year'].min()) >>> ax.tick_params(labelsize=12) >>> ax.set_xlabel('Year Count', fontsize=12) >>> ax.set_ylabel('Occurrence', fontsize=12)\n\nWe can see from the skewed distribution (Figure 9-5) that this is an excellent candi‐ date for binning.\n\nFigure 9-5. Raw year distribution for 10K+ academic papers in dataset\n\n168\n\n|\n\nChapter 9: Back to the Feature: Building an Academic Paper Recommender",
      "content_length": 859,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 185,
      "content": "The bins will be based on ranges within the variable, rather than the unique number of features. To further reduce the feature space, we will dummy-code the resultant bins (see Example 9-7). Pandas can do both using built-in functions. These methods will make our results easy to interpret, so we can do a quick check of the transformed features before moving on (see Figure 9-6).\n\nExample 9-7. Fixed-width binning + dummy coding (part 2)\n\n# binning here (by 10 years) reduces the year feature space from 156 to 19 >>> bins = int(round((model_df['year'].max() - model_df['year'].min()) / 10))\n\n>>> temp_df = pd.DataFrame(index = model_df.index) >>> temp_df['yearBinned'] = pd.cut(model_df['year'].tolist(), bins, precision = 0) >>> X_yrs = pd.get_dummies(temp_df['yearBinned']) >>> X_yrs.columns.categories IntervalIndex([(1831.0, 1841.0], (1841.0, 1851.0], (1851.0, 1860.0], (1860.0, 1870.0], (1870.0, 1880.0] ... (1968.0, 1978.0], (1978.0, 1988.0], (1988.0, 1997.0], (1997.0, 2007.0], (2007.0, 2017.0]] closed='right', dtype='interval[float64]')\n\n# plot the new distribution >>> fig, ax = plt.subplots() >>> X_yrs.sum().plot.bar(ax = ax) >>> ax.tick_params(labelsize=8) >>> ax.set_xlabel('Binned Years', fontsize=12) >>> ax.set_ylabel('Counts', fontsize=12)\n\nWe have preserved the underlying distribution of the original variable through bin‐ ning by decades. If we desired to use a method that would benefit from a different distribution, we could alter our binning choices to change how this variable presents itself to the model. Since we are using cosine similarity, this is fine. Let’s move on to the next feature we originally included in our model.\n\nThe fields-of-study feature space contributed significantly to the original model’s size and processing time.\n\nSecond Pass: More Engineering and a Smarter Model\n\n|\n\n169",
      "content_length": 1827,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 186,
      "content": "Figure 9-6. Distribution of new binned X_yrs feature\n\nLet’s examine the work we have already done. By parsing out the list of strings, we created a “bag-of-phrases” in the first pass. Since we already have a useful sparse array, we can focus on using a more efficient data type. Example 9-8 illustrates how converting from a Pandas DataFrame to a NumPy sparse array affects computation time.\n\nExample 9-8. Converting bag-of-phrases pd.Series to NumPy sparse array\n\n>>> X_fos = fos_features.values\n\n# We can see how this will make a difference in the future by looking # at the size of each >>> print('Our pandas Series, in bytes: ', getsizeof(fos_features)) >>> print('Our hashed numpy array, in bytes: ', getsizeof(X_fos))\n\n170\n\n|\n\nChapter 9: Back to the Feature: Building an Academic Paper Recommender",
      "content_length": 803,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 187,
      "content": "Our pandas Series, in bytes: 2530632380 Our hashed numpy array, in bytes: 112\n\nMuch better! Putting it back together, we’ll pipe our features together (Example 9-9) and rerun our recommender (Example 9-10) to see if we have improved results, tak‐ ing advantage of scikit-learn’s cosine similarity function. We will also reduce the computational time by only focusing on one item at a time.\n\nExample 9-9. Collaborative filtering stages 1 + 2: Build item feature matrix, search for similar items\n\n>>> second_features = np.append(X_fos, X_yrs, axis = 1) >>> print(\"The power of feature engineering saves us, in bytes: \", ... getsizeof(first_features) - getsizeof(second_features)) The power of feature engineering saves us, in bytes: 168066769\n\n>>> from sklearn.metrics.pairwise import cosine_similarity\n\n>>> def piped_collab_filter(features_matrix, index, top_n): ... item_similarities = \\ ... 1 - cosine_similarity(features_matrix[index:index+1], ... features_matrix).flatten() ... related_indices = \\ ... [i for i in item_similarities.argsort()[::-1] if i != index] ... return [(index, item_similarities[index]) ... for index in related_indices ... ][0:top_n]\n\nExample 9-10. Item-based collaborative filtering recommendations: Take 2\n\n>>> def paper_recommender(items_df, paper_ix, top_n): ... if paper_ix in model_df.index: ... print('Based on the paper:') ... print('Paper index = ', model_df.loc[paper_ix].name) ... print('Title :', model_df.loc[paper_ix]['title']) ... print('FOS :', model_df.loc[paper_ix]['fos']) ... print('Year :', model_df.loc[paper_ix]['year']) ... print('Abstract :', model_df.loc[paper_ix]['abstract']) ... print('Authors :', model_df.loc[paper_ix]['authors'], '\\n') ... # define the location index for the DataFrame index requested ... array_ix = model_df.index.get_loc(paper_ix) ... top_results = piped_collab_filter(items_df, array_ix, top_n) ... print('\\nTop',top_n,'results: ')\n\n... order = 1 ... for i in range(len(top_results)): ... print(order,'. Paper index = ', ... model_df.iloc[top_results[i][0]].name) ... print('Similarity score: ', top_results[i][1]) ... print('Title :', model_df.iloc[top_results[i][0]]['title'])\n\nSecond Pass: More Engineering and a Smarter Model\n\n|\n\n171",
      "content_length": 2215,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 188,
      "content": "... print('FOS :', model_df.iloc[top_results[i][0]]['fos']) ... print('Year :', model_df.iloc[top_results[i][0]]['year']) ... print('Abstract :', model_df.iloc[top_results[i][0]]['abstract']) ... print('Authors :', model_df.iloc[top_results[i][0]]['authors'], ... '\\n') ... if order < top_n: order += 1 ... else: ... print('Whoops! Choose another paper. Try something from here: \\n', ... model_df.index[100:200])\n\n>>> paper_recommender(second_features, 2, 3) Based on the paper: Paper index = 2 Title : Should endometriosis be an indication for intracytoplasmic sperm inject ... FOS : nan Year : 2015 Abstract : nan Authors : [{'name': 'Jovana P. Lekovich', 'org': 'Weill Cornell Medical College, ...\n\nTop 3 results: 1 . Paper index = 10055 Similarity score: 1.0 Title : [Diagnosis of cerebral tumors; comparative studies on arteriography, ... FOS : ['Radiology', 'Pathology', 'Surgery'] Year : 1953 Abstract : nan Authors : [{'name': 'Antoine'}, {'name': 'Lepoire'}, {'name': 'Schoumacker'}]\n\n2 . Paper index = 11771 Similarity score: 1.0 Title : A Study of Special Functions in the Theory of Eclipsing Binary Systems FOS : ['Contact binary'] Year : 1981 Abstract : nan Authors : [{'name': 'Filaretti Zafiropoulos', 'org': 'University of Manchester'}]\n\n3 . Paper index = 11773 Similarity score: 1.0 Title : Studies of powder flow using a recording powder flowmeter and measure ... FOS : nan Year : 1985 Abstract : This paper describes the utility of the dynamic measurement of the ... Authors : [{'name': 'Ramachandra P. Hegde', 'org': 'Department of Pharmacy, ...\n\nTo be honest, I don’t think our feature selection is working out too well. There is a lot of missing data in these fields. Let’s keep going to see if we can choose richer features with more information.\n\n172\n\n|\n\nChapter 9: Back to the Feature: Building an Academic Paper Recommender",
      "content_length": 1849,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 189,
      "content": "Finding Your Place Converting between Pandas DataFrames and NumPy matrices can make indices tricky—we have the same size index, but the index assignments are not the same. Pandas assists with this using .iloc, .loc, and .get_loc, as we show in Example 9-11:\n\n.loc returns the index based on the original Pandas DataFrame, allowing us to reference specific papers.\n\n.iloc uses the integer location, which is the same index as our NumPy array. • .get_loc helps us find the integer location when we know the DataFrame index.\n\nExample 9-11. Maintaining index assignment during conversions\n\n>>> model_df.loc[21] abstract A microprocessor includes hardware registers t... authors [{'name': 'Mark John Ebersole'}] fos [Embedded system, Parallel computing, Computer... keywords NaN title Microprocessor that enables ARM ISA program to... year 2013 Name: 21, dtype: object\n\n>>> model_df.iloc[21] abstract NaN authors [{'name': 'Nicola M. Heller'}, {'name': 'Steph... fos [Biology, Medicine, Post-transcriptional regul... keywords [glucocorticoids, post transcriptional regulat... title Post-transcriptional regulation of eotaxin by ... year 2002 Name: 30, dtype: object\n\n>>> model_df.index.get_loc(30) 21\n\nThird Pass: More Features = More Information Our experiment thus far is not supporting the original hypothesis that year and fields- of-study would be sufficient to recommend a similar paper. At this point, we have a few options:\n\nUpload more of the original dataset to see if we get better results. • Spend more time exploring the data to examine if we have a sufficiently dense set to provide good recommendations.\n\nIterate on the current model by adding more features.\n\nThird Pass: More Features = More Information\n\n|\n\n173",
      "content_length": 1722,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 190,
      "content": "The first option makes the assumption that the problem is in our sampling of the data. This might be the case, but is similar to Figure 9-4’s analogy of stirring the data pile for better results.\n\nThe second option would give a better idea of the underlying raw data. This should be continually revisited based on how your decisions for features and model selection change during the exploration process. The initial subsample chosen here reflects this step. Since we have more variables available in the dataset, we will not go back here yet.\n\nThis leaves the third option, moving forward on our current model by adding more features. Providing more information about each item can improve the similarity scores and result in better recommendations.\n\nBased on our initial exploration, the next steps will focus on the fields with the most information, abstract and authors.\n\nAcademic Paper Recommender: Take 3 Looking back at Chapter 4, we can see that abstract is a good candidate for tf-idf to filter through the noise and find the salient associative words. We do this in Example 9-12.\n\nExample 9-12. Stopwords + tf-idf\n\n# need to fill in NaN for sklearn use in future >>> filled_df = model_df.fillna('None')\n\n>>> from sklearn.feature_extraction.text import TfidfVectorizer\n\n>>> vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, ... stop_words='english') >>> X_abstract = vectorizer.fit_transform(filled_df['abstract']) >>> third_features = np.append(second_features, X_abstract.toarray(), axis = 1)\n\nWe can reduce the computational load of the messy and uneven authors by wran‐ gling into a dictionary and then running it through a one-hot encoder, as shown in Example 9-13.\n\nExample 9-13. One-hot encoding using scikit-learn’s DictVectorizer\n\n>>> authors_list = []\n\n>>> for row in filled_df.authors.itertuples(): ... # create a dictionary from each Series index ... if type(row.authors) is str: ... y = {'None': row.Index} ... if type(row.authors) is list:\n\n174\n\n|\n\nChapter 9: Back to the Feature: Building an Academic Paper Recommender",
      "content_length": 2052,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 191,
      "content": "... # add these keys + values to our running dictionary ... y = dict.fromkeys(row.authors[0].values(), row.Index) ... authors_list.append(y)\n\n>>> authors_list[0:5] [{'None': 0}, {'Ahmed M. Alluwaimi': 1}, {'Jovana P. Lekovich': 2, 'Weill Cornell Medical College, New York, NY': 2}, {'George C. Sponsler': 5}, {'M. T. Richards': 7}]\n\n>>> from sklearn.feature_extraction import DictVectorizer >>> v = DictVectorizer(sparse=False) >>> D = authors_list >>> X_authors = v.fit_transform(D) >>> fourth_features = np.append(third_features, X_authors, axis = 1)\n\nTime to check in with the recommender to see how these new features are working out. Example 9-14 shows the results.\n\nExample 9-14. Item-based collaborative filtering recommendations: Take 3\n\n>>> paper_recommender(fourth_features, 2, 3)\n\nBased on the paper: Paper index = 2 Title : Should endometriosis be an indication for intracytoplasmic sperm inject ... FOS : nan Year : 2015 Abstract : nan Authors : [{'name': 'Jovana P. Lekovich', 'org': 'Weill Cornell Medical College, ...\n\nTop 3 results: 1 . Paper index = 10055 Similarity score: 1.0 Title : [Diagnosis of cerebral tumors; comparative studies on arteriography, ... FOS : ['Radiology', 'Pathology', 'Surgery'] Year : 1953 Abstract : nan Authors : [{'name': 'Antoine'}, {'name': 'Lepoire'}, {'name': 'Schoumacker'}]\n\n2 . Paper index = 5601 Similarity score: 1.0 Title : 633 Survival after coronary revascularization, with and without mitral ... FOS : ['Cardiology'] Year : 2005 Abstract : nan Authors : [{'name': 'J.B. Le Polain De Waroux'}, {'name': 'Anne-Catherine ...\n\n3 . Paper index = 12256\n\nThird Pass: More Features = More Information\n\n|\n\n175",
      "content_length": 1659,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 192,
      "content": "Similarity score: 1.0 Title : Nucleotide Sequence and Analysis of an Insertion Sequence from Bacillus ... FOS : ['Biology', 'Molecular biology', 'Insertion sequence', 'Nucleic acid ... Year : 1994 Abstract : A 5.8-kb DNA fragment encoding the cryIC gene from Bacillus thur... Authors : [{'name': 'Geoffrey P. Smith'}, {'name': 'David J. Ellar'}, {'name': ...\n\nEven accounting for missing data in certain fields, the top three results from the last round of feature engineering are directing us to other papers in the medical field.\n\nThe range of papers represented in this dataset is broad; for example, a random sam‐ ple of papers exposed fields of study such as “Coupling constant,” “Evapotranspira‐ tion,” “Hash function,” “IVMS,” “Meditation,” “Pareto analysis,” “Second-generation wavelet transform,” “Slip,” and “Spiral galaxy.” Given that there are 7,604 unique fields of study listed for 10K+ papers, these last results seem to be moving in the right direction. We can be confident that our work is progressing toward a useful model.\n\nContinued iteration on more text variables, such as the finding the noun phrases of the paper titles or stemming the keywords, could bring us even closer to a “best” recommendation.\n\nIt should be noted here that this definition of “best” is the Holy Grail of all recom‐ menders and search engines alike. We are searching for what a user will find most helpful, which may or may not be directly represented by the data. Feature engineer‐ ing allows us to abstract salient features into representations such that algorithms can expose both the explicit and implicit information contained therein.\n\nSummary As you can see, building models for machine learning is easy. Building good models for useful results takes time and work. We hiked through the messy processes here of examining a collection of possible variables and experimenting with different feature engineering methods to achieve better results. We define “better” here not just in terms of good outcomes from our training and testing, but also reducing the size of the model and the time it takes us to iterate over different experiments.\n\nWe started this book by talking about how mastery of a subject comes from deeply learning the principles at work, in order to gain intuition to effectively put your knowledge to work. We hope that our work has given you the necessary tools to become more efficient and effective, as well as enriched your mathematical and com‐ putational understanding of how feature engineering is an essential skill to develop useful machine learning models.\n\n176\n\n|\n\nChapter 9: Back to the Feature: Building an Academic Paper Recommender",
      "content_length": 2666,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 193,
      "content": "Bibliography Sarwar, Badrul, George Karypis, Joseph Konstan, and John Riedl. “Item-Based Col‐ laborative Filtering Recommendation Algorithms.” Proceedings of the 10th Interna‐ tional Conference on the World Wide Web (2001) 285–295.\n\nSinha, Arnab, Zhihong Shen, Yang Song, Hao Ma, Darrin Eide, Bo-June (Paul) Hsu, and Kuansan Wang. “An Overview of Microsoft Academic Service (MAS) and Appli‐ cations.” Proceedings of the 24th International Conference on the World Wide Web (2015): 243–246.\n\nTang, Jie, Jing Zhang, Limin Yao, Juanzi Li, Li Zhang, and Zhong Su. “ArnetMiner: Extraction and Mining of Academic Social Networks.” Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (2008): 990–998.\n\nWickham, Hadley. “Tidy Data.” The Journal of Statistical Software 59 (2014).\n\nBibliography\n\n|\n\n177",
      "content_length": 839,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 195,
      "content": "APPENDIX A Linear Modeling and Linear Algebra Basics\n\nOverview of Linear Classification When we have a labeled dataset, the feature space is strewn with data points from dif‐ ferent classes. It is the job of the classifier to separate the data points from different classes. It can do so by producing an output that is very different for data points from one class versus another. For instance, when there are only two classes, then a good classifier should produce large outputs for one class, and small ones for another. The points right on the cusp of being in one class versus another form a decision surface (Figure A-1).\n\nFigure A-1. Simple binary classification finds a surface that separates two classes of data points\n\nMany functions can be made into classifiers. It’s a good idea to look for the simplest function that cleanly separates the classes, for a few reasons. First of all, it’s easier to find the best simple separator than the best complex separator. Also, simple functions\n\nLinear Modeling and Linear Algebra Basics\n\n|\n\n179",
      "content_length": 1045,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 196,
      "content": "often generalize better to new data, because it’s harder to tailor them too specifically to the training data (a concept known as overfitting). A simple model might make mistakes—like in Figure A-1, where some points are on the wrong side of the divide—but we’re willing to sacrifice some training accuracy in order to have a sim‐ pler decision surface that can achieve better test accuracy. The principle of minimiz‐ ing complexity and maximizing usefulness is called “Occam’s razor,” and is widely applicable in science and engineering.\n\nThe simplest function is a line. A linear function of one input variable is a familiar sight (Figure A-2).\n\nFigure A-2. A linear function of one input variable\n\nA linear function with two input variables can be visualized as either a flat plane in 3D or a contour plot in 2D (shown in Figure A-3). Like a topological geographic map, each line of the contour plot represents points in the input space that have the same output.\n\nIt’s harder to visualize higher-dimensional linear functions, which are called hyper‐ planes. But it’s easy enough to write down the algebraic formula. A multidimensional linear function has a set of inputs x1, x2, ..., xn and a set of weight parameters w0, w1, ..., wn:\n\nfw(x1, x2, ..., xn) = w0 + w1 * x1 + w2 * x2 + ... + wn * xn\n\nIt can be written more succinctly using vector notation:\n\nfw(x) = xTw\n\n180\n\n| Appendix A: Linear Modeling and Linear Algebra Basics",
      "content_length": 1433,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 197,
      "content": "Figure A-3. Contour plot of a linear function in 2D\n\nWe follow the usual convention for mathematical notations, which uses boldface to indicate a vector and non-boldface to indicate a scalar. The vector x is padded with an extra 1 at the beginning, as a placeholder for the intercept term w0. If all input fea‐ tures are 0, then the output of the function is w0. So, w0 is also known as the bias or intercept term.\n\nTraining a linear classifier is equivalent to picking out the best separating hyperplane between the classes. This translates into finding the best vector w that is oriented exactly right in space. Since each data point has a target label y, we could find a w that tries to directly emulate the target label:1\n\nxTw = y\n\nSince there is usually more than one data point, we want a w that simultaneously makes all of the predictions close to the target labels:\n\nAw = y\n\nHere, A is known as the data matrix (also known as the design matrix in statistics). It contains the data in a particular form: each row is a data point and each column a\n\n1 Strictly speaking, the formula given here is for linear regression, not linear classification. The difference is that regression allows for real-valued target variables, whereas classification targets are usually integers that repre‐ sent different classes. A regressor can be turned into a classifier via a nonlinear transform. For instance, the logistic regression classifier passes the linear transform of the input through a logistic function. Such models are called generalized linear models and have linear functions at their core. Even though this example is about classification, we use the formula for linear regression as a teaching tool, because it is much easier to analyze. The intuitions readily map to generalized linear classifiers.\n\nLinear Modeling and Linear Algebra Basics\n\n|\n\n181",
      "content_length": 1856,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 198,
      "content": "feature. (Sometimes people also look at its transpose, where features are the rows and data points the columns.)\n\nThe Anatomy of a Matrix In order to solve the preceding equation, we need some basic knowledge of linear algebra. For a systematic introduction to the subject, we highly recommend Strang (2006).\n\nThe equation states that when a certain matrix multiplies a certain vector, there is a certain outcome. A matrix is also called a linear operator, a name that makes it more apparent that a matrix is a little machine. This machine takes a vector as input and spits out another vector using a combination of several key operations: rotating a vec‐ tor’s direction, adding or subtracting dimensions, and stretching or compressing its length. This combination can be quite powerful for manipulating shapes in the input space.\n\nFor example, as Figure A-4 shows, a 3 × 2 matrix can transform a square area in 2D into a diamond-shaped area in 3D. It does so by rotating and stretching each vector in the input space into a new vector in the output space.\n\nFigure A-4. A 2D to 3D matrix transformation\n\n182\n\n| Appendix A: Linear Modeling and Linear Algebra Basics",
      "content_length": 1165,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 199,
      "content": "From Vectors to Subspaces In order to understand a linear operator, we have to look at how it morphs the input into output. Luckily, we don’t have to analyze one input vector at a time. Vectors can be organized into subspaces, and linear operators manipulate vector subspaces.\n\nA subspace is a set of vectors that satisfies two criteria. First, if it contains a vector, then it contains the line that passes through the origin and that point. Second, if it contains two points, then it contains all the linear combinations of those two vectors. Linear combination is a combination of two types of operations: multiplying a vector with a scalar, and adding two vectors together.\n\nOne important property of a subspace is its rank or dimensionality, which is a meas‐ ure of the degrees of freedom in this space. A line has rank 1, a 2D plane has rank 2, and so on. If you can imagine a multidimensional bird in our multidimensional space, then the rank of the subspace tells us in how many “independent” directions the bird could fly. “Independence” here means “linear independence”: two vectors are linearly independent if one isn’t a constant multiple of another (i.e., they are not pointing in exactly the same or opposite directions).\n\nA subspace can be defined as the span of a set of basis vectors. (Span is a technical term that describes the set of all linear combinations of a set of vectors.) The span of a set of vectors is invariant under linear combinations (because it’s defined that way). So, if we have one set of basis vectors, then we can multiply the vectors by any non‐ zero constants or add the vectors to get another basis.\n\nIt would be nice to have a more unique and identifiable basis to describe a subspace. An orthonormal basis contains vectors that have unit length and are orthogonal to each other. Orthogonality is another technical term. (At least 50% of all math and sci‐ ence is made up of technical terms. If you don’t believe me, do a bag-of-words count on this book.) Two vectors are orthogonal to each other if their inner product is zero. For all intents and purposes, we can think of orthogonal vectors as being at 90 degrees to each other. (This is true in Euclidean space, which closely resembles our physical 3D reality.) Normalizing these vectors to have unit length turns them into a uniform set of measuring sticks.\n\nAll in all, a subspace is like a tent, and the orthogonal basis vectors are the number of poles at right angles that are required to prop up the tent. The rank is equal to the total number of orthogonal basis vectors. Figure A-5 illustrates some these concepts.\n\nLinear Modeling and Linear Algebra Basics\n\n|\n\n183",
      "content_length": 2670,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 200,
      "content": "Figure A-5. Illustrations of four useful linear algebra concepts: inner product, linear combination, basis vectors, and orthogonal basis vectors\n\nUseful Linear Algebra Definitions\n\nFor those who think in math, here is some math to make our descriptions precise:\n\nScalar\n\nA number c, in contrast to a vector.\n\nVector\n\nx = (x1, x2, ..., xn)\n\nLinear combination\n\nax + by = (ax1 + by1, ax2 + by2, ..., axn + byn)\n\nSpan of a set of vectors v1, ..., vk\n\nThe set of vectors u = a1v1 + ... + akvk for any a1, ..., ak.\n\nLinear independence\n\nx and y are independent if x ≠ cy for any scalar constant c.\n\nInner product:\n\n⟨x, y⟩ = x1y1 + x2y2 + ... + xnyn\n\nOrthogonal vectors\n\nTwo vectors x and y are orthogonal if ⟨x, y⟩ = 0.\n\n184\n\n| Appendix A: Linear Modeling and Linear Algebra Basics",
      "content_length": 776,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 201,
      "content": "Subspace\n\nA subset of vectors within a larger containing vector space, satisfying these three criteria:\n\n1. It contains the zero vector. 2. If it contains a vector v, then it contains all vectors cv, where c is a scalar. 3. If it contains two vectors u and v, then it contains the vector u + v.\n\nBasis\n\nA set of vectors that span a subspace.\n\nOrthogonal basis\n\nA basis {v1, v2, ..., vd} where ⟨vi, vj⟩ = 0 for all i, j.\n\nRank of subspace\n\nThe minimum number of linearly independent basis vectors that span the sub‐ space.\n\nSingular Value Decomposition (SVD) A matrix performs a linear transformation on the input vector. Linear transforma‐ tions are very simple and constrained. It follows that a matrix can’t manipulate a sub‐ space willy-nilly. One of the most fascinating theorems of linear algebra proves that every square matrix, no matter what numbers it contains, must map a certain set of vectors back to themselves with some scaling. In the general case of a rectangular matrix, it maps a set of input vectors into a corresponding set of output vectors, and its transpose maps those outputs back to the original inputs. The technical terminol‐ ogy is that square matrices have eigenvectors with eigenvalues, and rectangular matrices have left and right singular vectors with singular values.\n\nEigenvector and Singular Vector Let A be an n × n matrix. If there is a vector v and a scalar λ such that Av = λv, then v is an eigenvector and λ an eigenvalue of A.\n\nLet A be a rectangular matrix. If there are vectors u and v and a scalar σ such that Av = σu and ATu = σv, then u and v are called left and right singular vectors and σ is a singular value of A.\n\nAlgebraically, the SVD of a matrix looks like this:\n\nA = UΣVT\n\nLinear Modeling and Linear Algebra Basics\n\n|\n\n185",
      "content_length": 1777,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 202,
      "content": "where the columns of the matrices U and V form orthonormal bases of the input and output space, respectively. Σ is a diagonal matrix containing the singular values.\n\nGeometrically, a matrix performs the following sequence of transformations:\n\n1. Map the input vector onto the right singular basis vector. 2. Scale each coordinate by the corresponding singular values. 3. Multiply this score with each of the left singular vectors. 4. Sum up the results.\n\nFigure A-6 provides an illustration. The operations go from right to left for a matrix- vector multiplication. The rightmost machine rotates and potentially projects the input into a lower-dimensional space. In this illustration, the input cube becomes a flat square, and is also rotated. The next machine squeezes the square in one direction and stretches it in another; the square becomes a rectangle. The last, leftmost machine rotates the rectangle again, and projects it back out into a possibly higher- dimensional space—but it remains a flat rectangle instead of some higher- dimensional object.\n\nFigure A-6. A matrix decomposed into three little machines: rotate, scale, rotate\n\nWhen A is a real matrix (i.e., all of the elements are real-valued), all of the singular values and singular vectors are real-valued. A singular value can be positive, negative, or zero. The ordered set of singular values of a matrix is called its spectrum, and it reveals a lot about the matrix. The gap between the singular values affects how stable the solutions are, and the ratio between the maximum and minimum absolute singu‐ lar values (the condition number) affects how quickly an iterative solver can find the\n\n186\n\n| Appendix A: Linear Modeling and Linear Algebra Basics",
      "content_length": 1723,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 203,
      "content": "solution. Both of these properties have notable impacts on the quality of the solution one can find.\n\nThe Four Fundamental Subspaces of the Data Matrix Another useful way to dissect a matrix is via the four fundamental subspaces: column space, row space, null space, and left null space. These four subspaces completely characterize the solutions to linear systems involving A or AT (hence the moniker).\n\nFor the data matrix (where the rows are data points and columns are features), the four fundamental subspaces can be understood in relation to the data and features. Let’s look at them in more detail.\n\nColumn space\n\nMathematical definition:\n\nThe set of output vectors s where s = Aw as we vary the weight vector w.\n\nMathematical interpretation:\n\nAll possible linear combinations of columns.\n\nData interpretation:\n\nAll outcomes that are linearly predictable based on observed features. The vec‐ tor w contains the weight of each feature.\n\nBasis:\n\nThe left singular vectors corresponding to nonzero singular values (a subset of the columns of U).\n\nRow space\n\nMathematical definition:\n\nThe set of output vectors r where r = uTA as we vary the weight vector u.\n\nMathematical interpretation:\n\nAll possible linear combinations of rows.\n\nData interpretation:\n\nA vector in the row space is something that can be represented as a linear combi‐ nation of existing data points. Hence, this can be interpreted as the space of “non-novel” data. The vector u contains the weight of each data point in the lin‐ ear combination.\n\nBasis:\n\nThe right singular vectors corresponding to nonzero singular values (a subset of the columns of V).\n\nLinear Modeling and Linear Algebra Basics\n\n|\n\n187",
      "content_length": 1677,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 204,
      "content": "Null space\n\nMathematical definition:\n\nThe set of input vectors w where Aw = 0.\n\nMathematical interpretation:\n\nVectors that are orthogonal to all rows of A. The null space gets squashed to zero by the matrix. This is the “fluff” that adds volume to the solution space of Aw = y.\n\nData interpretation:\n\n“Novel” data points that cannot be represented as any linear combination of existing data points.\n\nBasis:\n\nThe right singular vectors corresponding to the zero singular values (the rest of the columns of V).\n\nLeft null space\n\nMathematical definition:\n\nThe set of input vectors u where uTA = 0.\n\nMathematical interpretation:\n\nVectors that are orthogonal to all columns of A. The left null space is orthogonal to the column space.\n\nData interpretation:\n\n“Novel feature vectors\" that are not representable by linear combinations of existing features.\n\nBasis:\n\nThe left singular vectors corresponding to the zero singular values (the rest of the columns of U).\n\nColumn space and row space contain what is already representable based on observed data and features. Those vectors that lie in the column space are non-novel features. Those vectors that lie in the row space are non-novel data points.\n\nFor the purposes of modeling and prediction, non-novelty is good. A full column space means that the feature set contains enough information to model any target vector we wish. A full row space means that the different data points contain enough variation to cover all possible corners of the feature space. It’s the novel data points and features—respectively contained in the null space and the left null space—that we have to worry about.\n\nIn the application of building linear models of data, the null space can also be viewed as the subspace of “novel” data points. Novelty is not a good thing in this context.\n\n188\n\n| Appendix A: Linear Modeling and Linear Algebra Basics",
      "content_length": 1873,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 205,
      "content": "Novel data points indicate phantom data that is not linearly representable by the training set. Similarly, the left null space contains novel features that are not repre‐ sentable as linear combinations of existing features.\n\nThe null space is orthogonal to the row space. It’s easy to see why. The definition of null space states that w has an inner product of 0 with every row vector in A. There‐ fore, w is orthogonal to the space spanned by these row vectors, i.e., the row space. Similarly, the left null space is orthogonal to the column space.\n\nSolving a Linear System Let’s tie all this math back to the problem at hand: training a linear classifier, which is intimately connected to the task of solving a linear system. We look closely at how a matrix operates because we have to reverse engineer it. In order to train a linear model, we have to find the input weight vector w that maps to the observed output targets y in the system Aw = y, where A is the data matrix.2\n\nLet’s try to crank the machine of the linear operator in reverse. If we had the SVD decomposition of A, then we could map y onto the left singular vectors (columns of U), reverse the scaling factors (multiply by the inverse of the nonzero singular val‐ ues), and finally map them back to the right singular vectors (columns of V). Ta-da! Simple, right?\n\nThis is in fact the process of computing the pseudo-inverse of A. It makes use of a key property of an orthonormal basis: the transpose is the inverse. This is why SVD is so powerful. (In practice, real linear system solvers do not use the SVD, because it’s rather expensive to compute. There are other, much cheaper ways to decompose a matrix, such as QR or LU or Cholesky decompositions.)\n\nHowever, we skipped one tiny little detail in our haste. What happens if the singular value is zero? We can’t take the inverse of 0 because 1/0 = ∞. This is why it’s called the pseudo-inverse. (The real inverse isn’t even defined for rectangular matri‐ ces. Only square matrices have them, as long as all of the eigenvalues are nonzero.) A singular value of zero squashes whatever input was given; there’s no way to retrace its steps and come up with the original input.\n\n2 Actually, it’s a little more complicated than that. y may not be in the column space of A, so there may not be a solution to this equation. Instead of giving up, statistical machine learning looks for an approximate solution. It defines a loss function that quantifies the quality of a solution. If the solution is exact, then the loss is 0. Small errors, small loss; big errors, big loss, and so on. The training process then looks for the best parameters that minimize this loss function. In ordinary linear regression, the loss function is called the squared residual loss, which essentially maps y to the closest point in the column space of A. Logistic regression minimizes the log loss. In both cases, and linear models in general, the linear system Aw=y often lies at the core. Hence, our analysis here is very much relevant.\n\nLinear Modeling and Linear Algebra Basics\n\n|\n\n189",
      "content_length": 3084,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 206,
      "content": "Okay, going backward we get stuck on this one little detail. Let’s take what we’ve got and go forward again to see if we can unjam the machine. Suppose we came up with an answer to Aw = y. Let’s call it wparticular, because it’s particularly suited for y. Sup‐ pose that there are also a bunch of input vectors that A squashes to zero. Let’s take one of them and call it wsad-trumpet, because wah wah. Then, what do you think happens when we add wparticular to wsad-trumpet?\n\nA(wparticular + wsad-trumpet) = y\n\nAmazing! So this is a solution too. In fact, any input that gets squashed to zero could be added to a particular solution and give us another solution. The general solution looks like this:\n\nwgeneral = wparticular + whomogeneous\n\nwparticular is an exact solution to the equation Aw = y. There may or may not be such a solution. If there isn’t, then the system can only be approximately solved. If there is, then y belongs to what’s known as the column space of A. The column space is the set of vectors that A can map to, by taking linear combinations of its columns.\n\nwhomogeneous is a solution to the equation Aw = 0. (The grown-up name for wsad-trumpet is whomogeneous.) This should now look familiar. The set of all whomogeneous vectors forms the null space of A. This is the span of the right singular vectors with singular value 0.\n\nThe name “null space” sounds like the destination of woe for an existential crisis. If the null space contains any vectors other than the all-zero vector, then there are infinitely many solutions to the equation Aw = y. Having too many solutions to choose from is not in itself a bad thing. Sometimes any solution will do. But if there are many possible answers, then there are many sets of features that are useful for the classification task. It becomes difficult to understand which ones are truly important.\n\nOne way to fix the problem of a large null space is to regulate the model by adding additional constraints:\n\nAw = y,\n\nwhere w is such that wTw = c.\n\nThis form of regularization constrains the weight vector to have a certain norm, c. The strength of this regularization is controlled by a regularization parameter, which must be tuned, as is done in our experiments.\n\nIn general, feature selection methods deal with selecting the most useful features to reduce computation burden, decrease the amount of confusion for the model, and\n\n190\n\n| Appendix A: Linear Modeling and Linear Algebra Basics",
      "content_length": 2456,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 207,
      "content": "make the learned model more unique. This is the focus of “Feature Selection” on page 38.\n\nAnother problem is the “unevenness” of the spectrum of the data matrix. When we train a linear classifier, we care not only that there is a general solution to the linear system, but also that we can find it easily. Typically, the training process employs a solver that works by calculating a gradient of the loss function and walking downhill in small steps. When some singular values are very large and others very close to zero, the solver needs to carefully step around the longer singular vectors (those that correspond to large singular values) and spend a lot of time digging around in the shorter singular vectors to find the true answer. This “unevenness” in the spectrum is measured by the condition number of the matrix, which is basically the ratio between the largest and the smallest absolute value of the singular values.\n\nTo summarize, in order for there to be a good linear model that is relatively unique, and in order for it to be easy to find, we wish for the following:\n\n1. The label vector can be well approximated by a linear combination of a subset of features (column vectors). Better yet, the set of features should be linearly inde‐ pendent.\n\n2. In order for the null space to be small, the row space must be large. (This is due to the fact that the two subspaces are orthogonal.) The more linearly independ‐ ent the set of data points (row vectors), the smaller the null space.\n\n3. In order for the solution to be easy to find, the condition number of the data matrix—the ratio between the maximum and minimum singular values—should be small.\n\nBibliography Strang, Gilbert. Linear Algebra and Its Applications. 4th ed. Boston, MA: Cengage Learning, 2006.\n\nLinear Modeling and Linear Algebra Basics\n\n|\n\n191",
      "content_length": 1823,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 209,
      "content": "Symbols ℓ² normalization, 32, 65, 66, 108, 117\n\ncross validation classifier accuracy, 72\n\nA academic paper recommender (see recommen‐\n\nder for academic papers)\n\nactivation functions, 150 AlexNet, 144\n\nconvolutional layers, 150 fully connected layers, 145 pooling layers, 153 ReLU transformation, 151 response normalization layers, 151 structure of, 153-157\n\nanomaly detection of time series, use of PCA,\n\n111\n\napproximately leakage-proof statistics, 93 ASCII, 52 audio data, 133\n\nB back-off bin, 91 bag-of-n-grams, 45 bag-of-words (BoW) featurization, 42\n\nscaling with tf-idf transformation, 65\n\nbasis vectors, 183, 185 bias, 181 “Big Learning Made Easy—With Counts!” blog\n\npost, 87 bigrams, 45 Bilenko, Misha, 87 bin counting, 78, 87-94\n\ncounts without bounds, 94\n\nIndex\n\nexample of, 87 example, using data from Avazu Kaggle\n\ncompetition, 90\n\nguarding against data leakage, 93 odds ratio and log odds ratio for, 88 one-hot encoding vs., 89 rare categories, 91 trade-offs, 95\n\nbinarization (of counts), 9 binning\n\nfixed-width, 12 quantile, 13\n\nbinomial distribution, 55 Box-Cox transforms, 24\n\nC C-HOG blocks, 142 categorical variables, 77-97\n\nencodings, 78-83\n\ndummy coding, 79-82 effect coding, 82 one-hot encoding, 78 pros and cons of, 83 large, dealing with, 83-94 bin counting, 87-94 feature hashing, 84-87\n\nchunking, 56 class-imbalanced dataset, 64 classification\n\nusing k-means featurization, 122-127 with logistic regression, 66-67 classification dataset, creating, 64 clustering algorithms, 116 (see also k-means)\n\nIndex\n\n|\n\n193",
      "content_length": 1537,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 210,
      "content": "collisions, 84 collocations, 52-59\n\nextracting using chunking and part-of-\n\nspeech tagging, 56\n\nextracting using frequency-based methods,\n\n53\n\nextracting using hypothesis testing, 54 n-grams vs., 53 column space, 187 complex features, 6 handcrafted, 37\n\nconvolutional layers in neural networks,\n\n146-150 convolutional filter, 147 convolutional filter example, 148-150 intuition behind convolutions, 147\n\nconvolutions, 135 count-min sketch, 92 counts, 8-15\n\nbinarization, 9 quantization or binning, 10-15\n\nvisualizing business review counts in\n\nYelp dataset, 11\n\nwithout bounds, 94\n\nCountVectorizer transformer, 46, 65 covariance between random variables (in PCA),\n\n103\n\nD data\n\nabout, 1 answering questions with, 1 importing in recommender for academic\n\npapers (example), 161\n\ndata leakage\n\nguarding against in bin counting, 93 in k-means feature classification, 129\n\ndata matrix, 72-75, 181 data space, 8\n\nfeature vectors in, 44 vs. feature space, 33\n\ndata visualization, importance of, 22 decision surface, 179 decision tree models, 35, 38 input feature scale and, 6 use in model stacking, 128\n\ndeep learning\n\nlearning image features with deep neural\n\nnetworks, 144-157\n\n194\n\n|\n\nIndex\n\nfully connected layers, 144\n\nuse of PCA or ZCA in preprocessing, 112\n\ndelimiters, 52 dense featurization with k-means, 127 dimensionality of subspaces, 183 dimensionality reduction, 99\n\n(see also PCA) nonlinear, 115\n\ndistance, 101 distribution, 6 document frequency, 66 document-term matrix, 72 dummy coding, 79-82\n\npros and cons of, 83\n\nE effect coding, 82\n\npros and cons of, 83\n\neigen decomposition of a matrix, 104 eigenvectors, 185 embedded methods, 38 empirical variance, 104 encodings\n\ncategorical variables, 78-83 dummy coding, 79-82 effect coding, 82 one-hot encoding, 78 pros and cons of, 83\n\nstring objects, 52 Euclidean distance, 117 Euclidean norm, 32\n\nF factor analysis, 112 feature engineering, 5\n\n(see also numeric data; text data) defined, 3 in machine learning workflow, 3 numeric data, 5 feature extraction, 134\n\n(see also image feature extraction) automatic, in deep learning, 134 nonlinear manifold feature extraction by k-\n\nmeans, 117\n\nfeature hashing, 84-87\n\nfor word features, 85 signed, 85 storage and interpretability tradeoffs, 86 trade-offs, 95",
      "content_length": 2259,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 211,
      "content": "feature normalization, 30\n\n(see also feature scaling) feature scaling, 29-35, 61-76 min-max scaling, 30 standardization (variance scaling), 31 testing scaled vs. unscaled features, 63-72 classification with logistic regression,\n\n66-67\n\ncreating classification dataset, 64 scaling bag-of-words with tf-idf trans‐\n\nform, 65\n\ntuning logistic regression with regulari‐\n\nzation, 68-72\n\nunderstanding the results, 72-75\n\ntf-idf, 61-63 use case for, 35 ℓ² normalization, 32 feature selection, 38-39\n\nembedded methods, 38 filtering techniques, 38 focus of, 190 with interaction features, 37 wrapper methods, 38\n\nfeature space, 7\n\ndata space vs., 33 text sentences in 3D feature space, 44\n\nFeatureHasher, scikit-learn, 86 features\n\nabout, 3 multiple, composed into more complex fea‐\n\ntures, 6\n\nfiltering, 38, 159\n\n(see also item-based collaborative filtering) for cleaner features, using text data, 47-51\n\nfrequency-based filtering, 48 stemming, 51 stopwords, 48\n\ntechniques for collocation extraction, 56\n\nfilters\n\n2D convolutional filter, 147-150 applying to an image, 135\n\nfinancial modeling, use of PCA in, 112 fixed-width binning, 12\n\nin recommender for academic papers\n\n(example), 167 part two, 169\n\nfrequency-based filtering (text data), 48 frequent words, filtering from text, 48 fully connected layers (in neural networks), 144\n\nG Gaussian distribution, 6 Gaussian filter, applying to an image, 148-150 gradient boosting tree (GBT) classifiers, 124,\n\n129\n\ngradient orientation histograms, 139 bins, considerations for, 141 image neighborhoods, 142 normalization, 142 weight functions for gradient magnitude,\n\n141\n\ngrid search, 68\n\ntuning logistic regression hyperparameters\n\nwith, 69\n\nGridSearchCV function, scikit-learn, 69\n\nH hard clustering, 117 hash functions, 84\n\n(see also feature hashing)\n\nheatmap of paper recommendations, 164 heavy-tailed distribution, 16\n\nin text data, 50\n\nHOG (Histogram of Oriented Gradients),\n\n135-144 gradient orientation histograms, 139-143\n\nbins, considerations for, 141 image neighborhoods, 142 normalization, 142 weight functions, 141 image gradients, 135-139 horizontal image gradients, 139 hyperparameter tuning, 68\n\nfor logistic regression with grid search, 69 in comparing models, importance of, 68 in k-means feature classification, 127 using grid search, 68\n\nhyperparameters, 68 hyperplanes, 180 hypothesis testing, using for collocation extrac‐\n\ntion, 54\n\nI image descriptors, 135 image feature extraction, 133-157\n\nlearning image features with deep neural\n\nnetworks, 144-157 convolutional layers, 146-150\n\nIndex\n\n|\n\n195",
      "content_length": 2562,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 212,
      "content": "fully connected layers, 144 pooling layers, 153 ReLU transformation, 150-151 response normalization layers, 151 structure of AlexNet, 153-157\n\nmanual feature extraction with SIFT and\n\nHOG, 135-144\n\nsimplest image features (why they don't\n\nwork), 134 image gradients, 135-139 image neighborhoods, 142 ImageNet Large Scale Visual Recognition Chal‐\n\nlenge (ILSVRC), 144\n\nindices, maintaining assignments during cover‐\n\nsions, 173\n\ninner product, 183, 184 interaction features, 6, 35-38\n\ncomputational expense of, 37 example of use in prediction, 36\n\nintercept, 80 intercept term, 181 intrinsic dimensionality, 100 inverse document frequency, 61 item-based collaborative filtering, 159\n\nbuilding an item-based recommender, 160 filter for academic paper recommender, 161 for academic paper recommender\n\nstage 1, building item feature matrix, 162 stage 2, searching for similar items, 163\n\norigins of, 160 recommendations, 165, 171, 175\n\nK k-means, 117-130\n\nclustering as surface tiling, 119-122 clustering with, 117 featurization for classification, 122-130\n\nk-nearest neighbors (kNN), 124, 129 kernel, 148\n\nL left null space, 188 likelihood ratio test analysis, 54\n\nalgorithm for detecting common phrases\n\nwith, 55 linear algebra\n\nanatomy of a matrix, 182 from vectors to subspaces, 183\n\nuseful concepts, 183\n\n196\n\n|\n\nIndex\n\nfundamental subspaces of the data matrix,\n\n187-189\n\nsingular value decomposition (SVD),\n\n185-187\n\ntips for navigating formulas, 101 useful definitions, 184\n\nlinear classification\n\noverview, 179-182 solving a linear system, 189-191\n\nlinear combination, 184 linear correlation, 103 linear dependent features, 79 linear independence, 184 linear operators, 182 linear projection (in PCA), 102\n\nusing to transform features, 105\n\nlinear regression\n\nlearned coefficients, 81 on categorical variable, using one-hot and\n\ndummy codes, 80 with effect coding, 82 log transforms, 6, 15-29\n\ngeneralization of, in power transforms,\n\n23-29\n\nusing log transformed data to make predic‐\n\ntions, 19-23\n\nusing with inverse document frequency, 62\n\nlog-odds ratio for bin counting, 89 logical functions, 5 logistic regression\n\nclassification with, 66-67\n\ntuning logistic regression with regulari‐\n\nzation, 68-72\n\nlooking at model's use of features, 72-75 with k-means cluster features, 125, 128\n\nM machine learning\n\nprogress in text analysis vs. audio and\n\nimages, 133\n\nworkflow, 3\n\nmagnitude of numeric data, 5 manifold (nonlinear subspace), 115 manifold learning, 116 mask, 148 mathematical formulas, 3 mathematical modeling, 2 matrices\n\nanatomy of, 182",
      "content_length": 2553,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 213,
      "content": "decomposition of, methods for, 189\n\nmatrix-vector formulation, principal compo‐\n\nnents, 104\n\nmean, 66 metric (k-means), 117 Microsoft Academic Graph dataset, 160 min-max scaling, 30\n\ncaution when performing on sparse fea‐\n\ntures, 32 missing data, 3 model evaluation, 3 model stacking, 6 about, 128 k-means featurization, 122-127 key intuition for, 128\n\nmodels\n\nabout, 2 based on space-partitioning trees, 6 comparing, using hyperparameter tuning\n\nin, 68\n\ndata scheme for academic paper recommen‐\n\nder model, 162\n\nevaluating for use with categorical variable\n\nfeatures, 95\n\ngood linear model that is relatively unique,\n\n191\n\nmodel-driven feature engineering, PCA as\n\nexample of, 113\n\nN n-grams, 45, 52\n\ncollocations vs., 53 computing, 46\n\nnatural language processing (NLP), 52 neighborhoods (image), 142 neural networks (deep), learning image features\n\nwith, 144-157 convolutional layers, 146-150 fully connected layers, 144 pooling layers, 153 ReLU transformation, 150-151 response normalization layers, 151 structure of AlexNet, 153-157 NLP (natural language processing), 52 NLTK Python package, 51 nonlinear dimensionality reduction, 115 nonlinear embedding, 116 nonlinear featurization, 115\n\nnonlinear manifold feature extraction (k-\n\nmeans), 117\n\nnonordinal values, 77 normalization, 5 feature, 30\n\n(see also feature scaling)\n\nof gradient orientation histograms, 142 response normalization layers in neural net‐\n\nworks, 151\n\nnormalization constant, 33 null space, 188, 190 numeric data, 5-39 counts, 8-15\n\nbinarization, 9 quantization or binning, 10-15 feature scaling or normalization, 29-35\n\nmin-max scaling, 30 standardization (variance scaling), 31 ℓ² normalization, 32 feature selection, 38-39 interaction features, 35-38 log transformation, 15-29\n\ngeneralization of, in power transforms,\n\n23-29\n\nusing log transformed data to make pre‐\n\ndictions, 19-23\n\nscalars, vectors, and spaces, 6-8\n\nNumPy sparse array, converting Pandas Data‐\n\nFrame to, 170\n\nO odds ratio for bin counting, 88 one-hot encoding, 78, 88\n\nof cluster membership categorical variable,\n\n122 dense featurization vs., 127\n\npros and cons of, 83 trade-offs, 94 using scikit-learn DictVectorizer, 174 vs. bin counting, 89\n\northogonal basis, 185 orthogonal vectors, 184 orthogonality, 183 orthonormal basis, 183\n\nP Pandas\n\ncomputing quantiles and mapping data into\n\nquantile bins, 15\n\nIndex\n\n|\n\n197",
      "content_length": 2369,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 214,
      "content": "converting Pandas DataFrame to NumPy\n\nsparse array, 170 maintaining index assignments, 173\n\ndummy coding and one-hot encoding\n\nimplementation, 79\n\nusing to compute n-grams, 46\n\nparsing, 52 part-of-speech (PoS) tagging, 56 PCA (principal component analysis), 99-113 considerations and limitations, 109-111 derivation, 101-106\n\nimplementing PCA, 106 linear projection, 102 principal components, first formulation,\n\n104\n\nprincipal components, general solution\n\nof, 105\n\nprincipal components, matrix-vector\n\nformulation, 104\n\ntransforming features using linear pro‐\n\njection, 105\n\nvariance and empirical variance, 103\n\nuse cases, 111 using on scikit-learn digits dataset, 106-108 whitening and ZCA, 108\n\nphrase detection, 47\n\ncollocation extraction for, 52-59 frequency-based methods, 53 using chunking and part-of-speech tag‐\n\nging, 56\n\nusing hypothesis testing, 54\n\nPoisson distribution, 23 pooling layers (in neural networks), 153 Porter stemmer, 51 power transforms, 6, 23-29 principal component analysis (see PCA) probability plots (probplots), 28 Pythagorean theorem, 32 Python\n\nconverting Pandas DataFrame to NumPy\n\nsparse array, 170 maintaining index assignments, 173 libraries for chunking and part-of-speech\n\n(PoS) tagging, 56\n\nusing to calculate image gradients, 137\n\nQ quantiles, 13 quantization (or binning), 10-15\n\n198\n\n|\n\nIndex\n\nfixed-width binning, 12 quantile binning, 13 quantizing a count, 11\n\nR R-HOG blocks, 142 radial basis function support vector machine\n\n(RBF SVM), 124, 129 random forest classifiers, 124 rank or dimensionality (subspaces), 183, 185 rare categories, 91 rare words, filtering from text, 49 raw counts, PCA and, 111 receiver operating characteristic (ROC) curves,\n\n126\n\nrecommender for academic papers (example),\n\n159-177 first pass, data import, cleaning, and feature\n\nparsing, 161-167 naive approach, 161-167\n\nsecond pass, more engineering and smarter\n\nmodel, 167-173 fixed-width binning and dummy coding\n\n(part 2), 169\n\nthird pass, more features and more infor‐\n\nmation, 173-176 rectified linear unit, 150 rectified linear unit (ReLU) transformation,\n\n150-151\n\nredundant data, 2 reference category, 79\n\neffect, calculating, 82\n\nregularization constraints, adding to a model,\n\n190\n\nregularization, tuning logistic regression with,\n\n68-72 resampling, 69 response normalization layers (in neural net‐\n\nworks), 151 local response normalization, 152\n\nrobustness, 10 row space, 187\n\nS scalars, 6, 184 scale, 5 scikit-learn\n\nCountVectorizer transformer, 46, 65 digits dataset, 106",
      "content_length": 2512,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 215,
      "content": "FeatureHasher, 86 GridSearchCV function, 69 k-means clustering on Swiss roll, 120 one-hot encoding, 78 one-hot encoding using DictVectorizer, 174 PCA package, 109 SciPy, stats package, 25 sentences, analysis of, 52 separators, 52 SIFT (Scale Invariant Feature Transform),\n\n135-144 architecture, 143 gradient orientation histograms, 139-143\n\nbins, considerations for, 141 image neighborhoods, 142 normalization, 142 weight functions, 141 image gradients, 135-139\n\nsigmoid function, 66, 150 signed feature hashing, 85 singular value decomposition (SVD) of a\n\nmatrix, 101, 104, 185-187, 189 computational expense of, 110\n\nsingular vectors, 185 space characters, 52 spaces, 7\n\ndata space vs. feature space, 33\n\nspaCy library, 56 span of a set of vectors, 184 sparse data\n\ncaution when performing min-max scaling\n\nor standardization on, 32\n\nexample document-term matrix, 72\n\nspectrum (of a matrix), 110, 186\n\nunevenness in, 191\n\nstandardization, 31\n\ncaution when performing on sparse fea‐\n\ntures, 32\n\nstatistical factor model, 112 statistical modeling, 2 stats package, 25 stemming, 51 stocks, correlation patterns in, 112 stopwords, 48 string objects, 52 subspaces, 185\n\nfundamental, of the data matrix, 187-189 organizing vectors into, 183\n\nSwiss roll, 115\n\nk-means clustering on, 119-122\n\nT tanh function, 150 target engineering, 6, 10 target hints, k-means featurization with/\n\nwithout, 124 text data, 41-60\n\natoms of meaning, 52-59\n\nparsing and tokenization, 52\n\nbag-of-x, turning text into flat vectors,\n\n42-47 bag-of-n-grams, 45 bag-of-words (BoW) featurization, 42\n\nfiltering for cleaner features, 47-51 frequency-based filtering, 48 stemming, 51\n\ntext analysis in machine learning, 133\n\nTextBlob library, 56 tf-idf (term frequency-inverse document fre‐\n\nquency), 61-63 scaling bag-of-words with tf-idf transform,\n\n65\n\nusing in recommender for academic papers\n\n(example), 174\n\ntokenization, 45, 52\n\nscikit-learn's tokenizing pattern, 65\n\ntraining\n\nfitting feature transformer on training data,\n\n66\n\nof logistic regression classifiers with default\n\nparameters, 67 tree-based models, 6\n\n(see also decision tree models) input feature scale and, 29\n\nU Unicode, 52 uniform hash functions, 84 unigrams, 45\n\nV variance, 66\n\nand empirical variance in PCA, 103 estimating via resampling, 69 in a blob of data points, 101\n\nvariance scaling, 31 variance-stabilizing transformations, 23\n\nIndex\n\n|\n\n199",
      "content_length": 2392,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 216,
      "content": "(see also log transforms; power transforms)\n\nvector quantization, 119 vector spaces, 7 vectors, 6, 184\n\nEuclidean distance between, 117 from vectors to subspaces, 183 turning text into flat vectors with bag-of-\n\nwords (BoW), 43 vertical image gradients, 139 volume, 101\n\nW weighting schemes, gradient magnitude, 141\n\n200\n\n|\n\nIndex\n\nwhitening, 108 word features, feature hashing for, 85 wrapper methods, 38 wrong data, 2\n\nZ ZCA (zero-phase component analysis), 109\n\nuse cases, 112",
      "content_length": 479,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 217,
      "content": "About the Authors\n\nAlice Zheng is a technical leader in applied machine learning, spanning algorithm and platform development. Currently, she is a research science manager in Amazon Advertising. Previously, she worked on toolkit development and user education at GraphLab/Dato/Turi, and was a machine learning researcher at Microsoft Research. She holds a PhD in electrical engineering and computer science, and BA degrees in computer science and mathematics, all from UC Berkeley.\n\nAmanda Casari is a leader and engineer who explores the next horizons of technol‐ ogy and how to best demonstrate the impacts these will bring. She is currently a senior product manager and data scientist in Concur Labs and cofounder of the Con‐ cur Labs AI Research team at SAP Concur. She has worked in a breadth of cross- functional roles and engineering disciplines for the last 16 years, including data science, machine learning, complex systems, and robotics. Amanda holds a BS in control systems engineering from the United States Naval Academy and an MS in electrical engineering from the University of Vermont.\n\nColophon\n\nThe animal on the cover of Feature Engineering for Machine Learning is a pharaoh eagle-owl (Bubo ascalaphus). This bird of prey is found in Northern Africa and the Arabian peninsula in rocky, arid habitat. It is among the smaller eagle-owls at 18–20 inches long, though the Bubo genus contains some of the largest owl species. Most eagle-owls (as well as their American cousins, horned owls) have distinctive ear tufts.\n\nThe pharaoh eagle-owl is nocturnal and hunts with a perching method. From a high vantage point, it waits for small mammals, snakes, lizards, birds, and even insects to come into range, before swiftly swooping toward its prey. It is well equipped for this with keen farsightedness and hearing, feathers optimized for silent flight, and sharp talons. Owls can also turn their heads in a range of about 270 degrees, allowing them to look behind themselves without making much movement.\n\nThis species has mottled brown, black, and white plumage, and distinctive orange- yellow eyes. Pharaoh owls are known to mate for life. Nesting sites are created in shallow scrapes among rocks, within crevices, or (occasionally) manmade structures like wells. In Egypt, the owls have been seen nesting on the pyramids.\n\nMany of the animals on O’Reilly covers are endangered; all of them are important to the world. To learn more about how you can help, go to animals.oreilly.com.\n\nThe cover image is from Elements of Ornithology. The cover fonts are URW Type‐ writer and Guardian Sans. The text font is Adobe Minion Pro; the heading font is Adobe Myriad Condensed; and the code font is Dalton Maag’s Ubuntu Mono.",
      "content_length": 2731,
      "extraction_method": "Unstructured"
    }
  ]
}