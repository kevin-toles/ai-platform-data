{
  "metadata": {
    "title": "Distributed Services with Go - Travis Jeffery",
    "author": "Travis Jeffery;",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 245,
    "conversion_date": "2025-12-19T17:26:58.070636",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "Distributed Services with Go - Travis Jeffery.pdf",
    "extraction_method": "Unstructured"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Let’s Go • 4",
      "start_page": 17,
      "end_page": 25,
      "detection_method": "regex_chapter_title",
      "content": "Chapter 1. Let’s Go • 6\n\nBuild a Commit Log Prototype\n\nWe’ll explore commit logs in depth in Chapter 3, Write a Log Package, on page 23, when we build a persisted commit log library. For now, all you need to know about commit logs is that they’re a data structure for an append-only sequence of records, ordered by time, and you can build a simple commit log with a slice.\n\nCreate an internal/server directory tree in the root of your project and put the following code under the server directory in a file called log.go:\n\nLetsGo/internal/server/log.go package server\n\nimport (\n\n\"fmt\" \"sync\"\n\n)\n\ntype Log struct { mu records []Record\n\nsync.Mutex\n\n}\n\nfunc NewLog() *Log {\n\nreturn &Log{}\n\n}\n\nfunc (c *Log) Append(record Record) (uint64, error) {\n\nc.mu.Lock() defer c.mu.Unlock() record.Offset = uint64(len(c.records)) c.records = append(c.records, record) return record.Offset, nil\n\n}\n\nfunc (c *Log) Read(offset uint64) (Record, error) {\n\nc.mu.Lock() defer c.mu.Unlock() if offset >= uint64(len(c.records)) {\n\nreturn Record{}, ErrOffsetNotFound\n\n} return c.records[offset], nil\n\n}\n\ntype Record struct {\n\nValue []byte `json:\"value\"` Offset uint64 `json:\"offset\"`\n\n}\n\nvar ErrOffsetNotFound = fmt.Errorf(\"offset not found\")\n\nreport erratum • discuss\n\nBuild a JSON over HTTP Server • 7\n\nTo append a record to the log, you just append to the slice. Each time we read a record given an index, we use that index to look up the record in the slice. If the offset given by the client doesn’t exist, we return an error saying that the offset doesn’t exist. All really simple stuff, as it should be since we’re using this log as a prototype and want to keep moving.\n\nIgnore Chapter Namespaces in the File Paths\n\nYou may have noticed that code snippet’s file path said LetsGo/inter- nal/server/log.go instead of internal/server/log.go and that subsequent code snippets have similar per-chapter directory namespaces. These namespaces were needed to structure the code for the book build. When writing your code, pretend that these namespaces don’t exist. So for the previous example, the internal directory would go at the root of your project.\n\nBuild a JSON over HTTP Server\n\nNow we’ll write our JSON/HTTP web server. A Go web server comprises one function—a net/http HandlerFunc(ResponseWriter, *Request)—for each of your API’s endpoints. Our API has two endpoints: Produce for writing to the log and Consume for reading from the log. When building a JSON/HTTP Go server, each handler consists of three steps:\n\n1. Unmarshal the request’s JSON body into a struct. 2. Run that endpoint’s logic with the request to obtain a result. 3. Marshal and write that result to the response. If your handlers become much more complicated than this, then you should move the code out, move request and response handling into HTTP middle- ware, and move business logic further down the stack.\n\nLet’s start by adding a function for users to create our HTTP server. Inside your server directory, create a file called http.go that contains the following code:\n\nLetsGo/internal/server/http.go package server\n\nimport (\n\n\"encoding/json\" \"net/http\"\n\n\"github.com/gorilla/mux\"\n\n)\n\nfunc NewHTTPServer(addr string) *http.Server {\n\nhttpsrv := newHTTPServer() r := mux.NewRouter()\n\nreport erratum • discuss\n\nChapter 1. Let’s Go • 8\n\nr.HandleFunc(\"/\", httpsrv.handleProduce).Methods(\"POST\") r.HandleFunc(\"/\", httpsrv.handleConsume).Methods(\"GET\") return &http.Server{ Addr: Handler: r,\n\naddr,\n\n}\n\n}\n\nNewHTTPServer(addrstring) takes in an address for the server to run on and returns an *http.Server. We create our server and use the popular gorilla/mux library to write nice, RESTful routes that match incoming requests to their respective handlers. An HTTP POST request to / matches the produce handler and appends the record to the log, and an HTTP GET request to / matches the consume handler and reads the record from the log. We wrap our server with a *net/http.Server so the user just needs to call ListenAndServe() to listen for and handle incoming requests.\n\nNext, we’ll define our server and the request and response structs by adding this snippet below NewHTTPServer():\n\nLetsGo/internal/server/http.go type httpServer struct { Log *Log\n\n}\n\nfunc newHTTPServer() *httpServer { return &httpServer{\n\nLog: NewLog(),\n\n}\n\n}\n\ntype ProduceRequest struct {\n\nRecord Record `json:\"record\"`\n\n}\n\ntype ProduceResponse struct {\n\nOffset uint64 `json:\"offset\"`\n\n}\n\ntype ConsumeRequest struct {\n\nOffset uint64 `json:\"offset\"`\n\n}\n\ntype ConsumeResponse struct {\n\nRecord Record `json:\"record\"`\n\n}\n\nWe now have a server referencing a log for the server to defer to in its handlers. A produce request contains the record that the caller of our API wants appended to the log, and a produce response tells the caller what offset the log stored the records under. A consume request specifies which records the\n\nreport erratum • discuss\n\nBuild a JSON over HTTP Server • 9\n\ncaller of our API wants to read and the consume response to send back those records to the caller. Not bad for just 28 lines of code, huh?\n\nNext, we need to implement the server’s handlers. Add the following code below your types from the previous code snippet:\n\nLetsGo/internal/server/http.go func (s *httpServer) handleProduce(w http.ResponseWriter, r *http.Request) {\n\nvar req ProduceRequest err := json.NewDecoder(r.Body).Decode(&req) if err != nil {\n\nhttp.Error(w, err.Error(), http.StatusBadRequest) return\n\n} off, err := s.Log.Append(req.Record) if err != nil {\n\nhttp.Error(w, err.Error(), http.StatusInternalServerError) return\n\n} res := ProduceResponse{Offset: off} err = json.NewEncoder(w).Encode(res) if err != nil {\n\nhttp.Error(w, err.Error(), http.StatusInternalServerError) return\n\n}\n\n}\n\nThe produce handler implements the three steps we discussed before: unmarshaling the request into a struct, using that struct to produce to the log and getting the offset that the log stored the record under, and marshaling and writing the result to the response. Our consume handler looks almost identical. Add the following snippet below your produce handler:\n\nLetsGo/internal/server/http.go func (s *httpServer) handleConsume(w http.ResponseWriter, r *http.Request) {\n\nvar req ConsumeRequest err := json.NewDecoder(r.Body).Decode(&req) if err != nil {\n\nhttp.Error(w, err.Error(), http.StatusBadRequest) return\n\n} record, err := s.Log.Read(req.Offset) if err == ErrOffsetNotFound {\n\nhttp.Error(w, err.Error(), http.StatusNotFound) return\n\n} if err != nil {\n\nhttp.Error(w, err.Error(), http.StatusInternalServerError) return\n\n}\n\nreport erratum • discuss\n\nChapter 1. Let’s Go • 10\n\nres := ConsumeResponse{Record: record} err = json.NewEncoder(w).Encode(res) if err != nil {\n\nhttp.Error(w, err.Error(), http.StatusInternalServerError) return\n\n}\n\n}\n\nThe consume handler is like the produce handler but calls Read(offset uint64) to get the record stored in the log. This handler contains more error checking so we can provide an accurate status code to the client if the server can’t handle the request, like if the client requested a record that doesn’t exist.\n\nThat’s all the code needed for our server. Now let’s write some code to turn your server library into a program we can execute.\n\nRun Your Server\n\nThe last code you need to write is a main package with a main() function to start your server. In the root directory of your project, create a cmd/server directory tree, and in the server directory create a file named main.go with this code:\n\nLetsGo/cmd/server/main.go package main\n\nimport (\n\n\"log\"\n\n\"github.com/travisjeffery/proglog/internal/server\"\n\n)\n\nfunc main() {\n\nsrv := server.NewHTTPServer(\":8080\") log.Fatal(srv.ListenAndServe())\n\n}\n\nOur main() function just needs to create and start the server, passing in the address to listen on (localhost:8080) and telling the server to listen for and handle requests by calling ListenAndServe(). Wrapping our server with the *net/http.Server in NewHTTPServer() saved us from writing a bunch of code here—and anywhere else we’d create an HTTP server.\n\nIt’s time to test our slick new service.\n\nreport erratum • discuss\n\nTest Your API • 11\n\nTest Your API\n\nYou now have a functioning JSON/HTTP commit log service you can run and test by hitting the endpoints with curl. Run the following snippet to start the server:\n\n$ go run main.go\n\nOpen another tab in your terminal and run the following commands to add some records to your log:\n\n$ curl -X POST localhost:8080 -d \\\n\n'{\"record\": {\"value\": \"TGV0J3MgR28gIzEK\"}}'\n\n$ curl -X POST localhost:8080 -d \\\n\n'{\"record\": {\"value\": \"TGV0J3MgR28gIzIK\"}}'\n\n$ curl -X POST localhost:8080 -d \\\n\n'{\"record\": {\"value\": \"TGV0J3MgR28gIzMK\"}}'\n\nGo’s encoding/json package encodes []byte as a base64-encoding string. The record’s value is a []byte, so that’s why our requests have the base64 encoded forms of Let’s Go #1–3. You can read the records back by running the following commands and verifying that you get the associated records back from the server:\n\n$ curl -X GET localhost:8080 -d '{\"offset\": 0}' $ curl -X GET localhost:8080 -d '{\"offset\": 1}' $ curl -X GET localhost:8080 -d '{\"offset\": 2}'\n\nCongratulations—you have built a simple JSON/HTTP service and confirmed it works!\n\nWhat You Learned\n\nIn this chapter, we built a simple JSON/HTTP commit log service that accepts and responds with JSON and stores the records in those requests to an in- memory log. Next, we’ll use protocol buffers to manage our API types, generate custom code, and prepare to write a service with gRPC—an open source, high- performance remote procedure call framework that’s great for building dis- tributed services.\n\nreport erratum • discuss\n\nCHAPTER 2\n\nStructure Data with Protocol Buffers\n\nWhen building distributed services, you’re communicating between the services over a network. To send data (such as your structs) over a network, you need to encode the data in a format to transmit, and lots of programmers choose JSON. When you’re building public APIs or you’re creating a project where you don’t control the clients, JSON makes sense because it’s accessible—both for humans to read and computers to parse. But when you’re building private APIs or building projects where you do control the clients, you can make use of a mechanism for structuring and transmitting data that—compared to JSON—makes you more productive and helps you create services that are faster, have more features, and have fewer bugs.\n\nSo what is this mechanism? Protocol buffers (also known as protobuf), which is Google’s language and platform-neutral extensible mechanism for structur- ing and serializing data. The advantages of using protobuf are that it:\n\nGuarantees type-safety; • Prevents schema-violations; • Enables fast serialization; and • Offers backward compatibility.\n\nProtobuf lets you define how you want your data structured, compile your protobuf into code in potentially many languages, and then read and write your structured data to and from different data streams. Protocol buffers are good for communicating between two systems (such as microservices), which is why Google used protobuf when building gRPC to develop a high-perfor- mance remote procedure call (RPC) framework.\n\nIf you haven’t worked with protobuf before, you may have some of the same concerns I had—that protobuf seems like a lot of extra work. I promise you that, after working with it in this chapter and the rest of the book, you’ll see\n\nreport erratum • discuss\n\nChapter 2. Structure Data with Protocol Buffers • 14\n\nthat it’s really not so bad. It offers many advantages over JSON, and it’ll end up saving you a lot of work.\n\nHere’s a quick example that shows what protocol buffers look like and how they work. Imagine you work at Twitter and one of the object types you work with are Tweets. Tweets, at the very least, comprise the author’s message. If you defined this in protobuf, it would look like this:\n\nStructureDataWithProtobuf/example.proto syntax = \"proto3\";\n\npackage twitter;\n\nmessage Tweet {\n\nstring message = 1;\n\n}\n\nYou’d then compile this protobuf into code in the language of your choice. For example, the protobuf compiler would take this protobuf and generate the following Go code:\n\nStructureDataWithProtobuf/example.pb.go // Code generated by protoc-gen-go. DO NOT EDIT. // source: example.proto\n\npackage twitter\n\ntype Tweet struct {\n\nMessage string `protobuf:\"bytes,1,opt,name=message,proto3\"\n\njson:\"message,omitempty\"`\n\n// Note: Protobuf generates internal fields and methods // I haven't included for brevity.\n\n}\n\nBut why not just write that Go code yourself? Why use protobuf instead? I’m glad you asked.\n\nWhy Use Protocol Buffers?\n\nProtobuf offers all kinds of useful features:\n\nConsistent schemas\n\nWith protobuf, you encode your semantics once and use them across your services to ensure a consistent data model throughout your whole system. My colleagues and I built the infrastructures at my last two companies on microservices, and we had a repo called “structs” that housed our protobuf and their compiled code, which all our services depended on. By doing this, we ensured that we didn’t send multiple, inconsistent schemas to prod. Thanks to Go’s type checking, we could update our structs dependency, run the tests that touched our data models, and the\n\nreport erratum • discuss\n\nWhy Use Protocol Buffers? • 15\n\ncompiler and tests would tell us whether our code was consistent with our schema.\n\nVersioning for free\n\nOne of Google’s motivations for creating protobuf was to eliminate the need for version checks and prevent ugly code like this:\n\nStructureDataWithProtobuf/example.go if (version == 3) { ... } else if (version > 4) {\n\nif (version == 5) {\n\n...\n\n} ...\n\n}\n\nThink of a protobuf message like a Go struct because when you compile a message it turns into a struct. With protobuf, you number your fields on your messages to ensure you maintain backward compatibility as you roll out new features and changes to your protobuf. So it’s easy to add new fields, and intermediate servers that need not use the data can simply parse it and pass through it without needing to know about all the fields. Likewise with removing fields: you can ensure that deprecated fields are no longer used by marking them as reserved; the compiler will then complain if anyone tries to use to the deprecated fields.\n\nLess boilerplate\n\nThe protobuf libraries handle encoding and decoding for you, which means you don’t have to handwrite that code yourself.\n\nExtensibility\n\nThe protobuf compiler supports extensions that can compile your protobuf into code using your own compilation logic. For example, you might want several structs to have a common method. With protobuf, you can write a plugin to generate that method automatically.\n\nLanguage agnosticism\n\nProtobuf is implemented in many languages: since Protobuf version 3.0, there’s support for Go, C++, Java, JavaScript, Python, Ruby, C#, Objective C, and PHP, and third-party support for other languages. And you don’t have to do any extra work to communicate between services written in different languages. This is great for companies with various teams that want to use different languages, or when your team wants to migrate to another language.\n\nreport erratum • discuss",
      "page_number": 17
    },
    {
      "number": 2,
      "title": "Structure Data with Protocol Buffers • 14",
      "start_page": 26,
      "end_page": 34,
      "detection_method": "regex_chapter_title",
      "content": "Chapter 2. Structure Data with Protocol Buffers • 16\n\nPerformance\n\nProtobuf is highly performant, and has smaller payloads and serializes up to six times faster than JSON.1\n\ngRPC uses protocol buffers to define APIs and serialize messages; we’ll use gRPC to build our client and server.\n\nHopefully I’ve done a decent job of convincing you that protobuf is cool. But the theory alone is boring! Let’s get you set up to create your own protobuf and use it to build stuff.\n\nInstall the Protocol Buffer Compiler\n\nThe first thing we need to do to get you compiling protobuf is—you guessed it—install the compiler. Go to the Protobuf release page on GitHub2 and download the relevant release for your computer. If you’re on a Mac, for instance, you’d download protoc-3.9.0-osx-x86_64.zip. You can download and install in your terminal like so:\n\n$ wget https://github.com/protocolbuffers/protobuf/\\ releases/download/v3.9.0/protoc-3.9.0-osx-x86_64.zip $ unzip protoc-3.9.0-osx-x86_64.zip -d /usr/local/protobuf\n\nHere’s what the layout and files in the extracted protobuf directory look like:\n\n❯ tree /usr/local/protobuf /usr/local/protobuf ├── bin │ ├── include │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ └── readme.txt\n\n└── protoc\n\n└── google\n\n└── protobuf\n\n├── any.proto ├── api.proto ├── compiler └── plugin.proto │ ├── descriptor.proto ├── duration.proto ├── empty.proto ├── field_mask.proto ├── source_context.proto ├── struct.proto ├── timestamp.proto ├── type.proto └── wrappers.proto\n\n1. 2.\n\nhttps://auth0.com/blog/beating-json-performance-with-protobuf\n\nhttps://github.com/protocolbuffers/protobuf/releases\n\nreport erratum • discuss\n\nDefine Your Domain Types as Protocol Buffers • 17\n\nAs you can see, a protobuf installation consists of two directories. The bin directory contains the compiler binary named protoc, and the include directories contains a bunch of protobuf files that are like protobuf’s standard library. A mistake I’ve seen many people make when setting up their systems to work with protobuf is that they install the compiler binary without the include protobuf files. But without those files you can’t compile successfully, so just extract the whole release using the commands I just showed you and you’ll be just dandy.\n\nNow that you’ve got the compiler binary installed, make sure your shell can find and run it. Add the binary to your PATH env var using your shell’s con- figuration file. If you’re using ZSH for instance, run something like the follow- ing to update your configuration:\n\n$ echo 'export PATH=\"$PATH:/usr/local/protobuf/bin\"' >> ~/.zshenv\n\nAt this point the protobuf compiler is installed on your machine. To test the installation, run protoc --version. If you don’t see any errors, you’re ready to handle the rest of this chapter. If you do see errors, don’t worry: few installa- tion problems are unique. Google will show you the way.\n\nWith the compiler installed, you’re ready to write and compile some protobuf. Let’s get to it!\n\nDefine Your Domain Types as Protocol Buffers\n\nIn the previous chapter, we defined our Record type in Go as this struct:\n\nLetsGo/internal/server/log.go type Record struct {\n\nValue []byte `json:\"value\"` Offset uint64 `json:\"offset\"`\n\n}\n\nTo turn that into a protobuf message we need to convert the Go code into protobuf syntax.\n\nThe convention for Go projects is to put your protobuf in an api directory. So run mkdir -p api/v1 to create your directories, then create a file called log.proto in the v1 directory and put this code in it:\n\nStructureDataWithProtobuf/api/v1/log.proto syntax = \"proto3\";\n\npackage log.v1;\n\noption go_package = \"github.com/travisjeffery/api/log_v1\";\n\nreport erratum • discuss\n\nChapter 2. Structure Data with Protocol Buffers • 18\n\nmessage Record {\n\nbytes value = 1; uint64 offset = 2;\n\n}\n\nIn this protobuf code, we specify that we’re using proto3 syntax—the latest version of protobuf syntax. Then we specify a package name for two reasons: because this protobuf package name is used as the package name in the generated Go code and because it prevents name clashes between protocol message types that have the same name.\n\nThese protobuf messages are equivalent to the Go structs shown earlier. You’ll notice the two syntaxes are very similar: in Go you have struct, and with protobuf you have a message—both with a list of fields. In Go you put the name of the field on the left followed by its type, and with protobuf you put the name of the field on right followed by its name (with an additional field ID).\n\nFollowing the package declarations in the protobuf code, we define our Record type. Protocol buffer programmers use the repeated keyword to define a slice of some type, so repeated Record records means the records field is a []Record in Go.\n\nI mentioned earlier that one handy feature of protobuf is the ability to version fields. Each field has a type, name, and unique field number. These field numbers identify your fields in the marshaled binary format, and you shouldn’t change them once your messages are in use in your projects. Consider fields immutable: you can stop using old fields and add new fields, but you can’t modify existing fields. You want to change fields like this when you make small, iterative changes—like when you add or remove features or data from a message.\n\nBesides field versions, you’ll also want to group your messages by a major version. The major version gives you control over your protobuf when you overhaul projects to rearchitect your infrastructure or run multiple message versions at the same time for a migration period. Bumping major versions should be a rare occurrence because for most changes, field versioning is sufficient. I’ve only had to bump the major version of my protobuf twice, and if you look at Google’s API definitions3 protobuf, they’ve only bumped their major version a couple times. So changing major versions is uncommon, but it’s nice to have the ability when you need it.\n\n3.\n\nhttps://github.com/googleapis/googleapis\n\nreport erratum • discuss\n\nCompile Protocol Buffers • 19\n\nAt the beginning of this section, I had you put the log.proto file into an api/v1 directory. The v1 represents these protobufs’ major version. If you were to continue building this project and decided to break API compatibility, you would create a v2 directory to package the new messages together and com- municate to your users you’ve made incompatible API changes.\n\nNow that we’ve created the protocol buffer messages, let’s compile your pro- tobuf into Go code.\n\nCompile Protocol Buffers\n\nTo compile protobuf into the code of some programming language, you need the runtime for that language. The compiler itself doesn’t know how to compile protobuf into every language—it needs a language-specific runtime to do so.\n\nGo has two runtimes to compile protobuf into Go code. The Go team and the protobuf team at Google developed the original runtime.4 Then a team of folks who wanted more features forked the original runtime and developed it into gogoprotobuf, with more code-generation features and faster marshaling and unmarshaling. Projects like Etcd, Mesos, Kubernetes, Docker, CockroachDB, and NATS as well as companies like Dropbox and Sendgrid used gogoprotobuf. I used gogoprotobuf for my projects to integrate with Kubernetes’ protocol buffers and for gogoprotobuf’s features.\n\nIn March 2020, the Go team released a major revision of the Go API (APIv2)5 for protocol buffers with improved performance6 and a reflection API that enables adding features like those provided by gogoprotobuf. Projects7 that used gogoprotobuf have begun switching to APIv28 because of APIv2’s improved performance, its new reflection API, its incompatibility with gogoprotobuf, and the gogoprotobuf project needing new ownership.9 I recommend using APIv2, too.\n\nTo compile our protobuf into Go, we need to install the protobuf runtime by running the following command:\n\n$ go get google.golang.org/protobuf/...@v1.25.0\n\n4. 5. 6. 7. 8. 9.\n\nhttps://github.com/golang/protobuf\n\nhttps://github.com/alexshtin/proto-bench/blob/master/README.md\n\nhttps://github.com/istio/istio/pull/24956\n\nhttps://github.com/istio/api/pull/1607\n\nhttps://github.com/envoyproxy/go-control-plane/pull/226\n\nhttps://github.com/gogo/protobuf/issues/691\n\nreport erratum • discuss\n\nChapter 2. Structure Data with Protocol Buffers • 20\n\nYou can now compile your protobuf by running the following command at the root of your project:\n\n$ protoc api/v1/*.proto \\\n\n--go_out=. \\ --go_opt=paths=source_relative \\ --proto_path=.\n\nLook in the api/v1 directory and you’ll see a new file called log.pb.go. Open it up to see the Go code that the compiler generated from your protobuf code. Your protobuf message has been turned into a Go struct, along with some methods on the struct for marshaling to the protobuf binary wire format, and getters for your fields.\n\nSince you’ll compile your protobuf every time you change them, it’s worth adding a Makefile file with a compile target that you can quickly run again and again. We’ll include a test target for testing our code too. So create a Makefile file at the root of your repo with the following code:\n\nStructureDataWithProtobuf/Makefile compile:\n\nprotoc api/v1/*.proto \\\n\n--go_out=. \\ --go_opt=paths=source_relative \\ --proto_path=.\n\ntest:\n\ngo test -race ./...\n\nThat’s all there is to compiling your protobuf code into Go code. Now let’s talk about how to work with the generated code and extend the compiler to gener- ate your own code.\n\nWork with the Generated Code\n\nAlthough the generated code in log.pb.go is a lot longer than your handwritten code in log.go (because of the extra code needed to marshal to the protobuf binary wire format), you’ll use the code as if you’d handwritten it. For example, you’ll create instances using the & operator (or new keyword) and access fields using a dot.\n\nThe compiler generates various methods on the struct, but the only methods you’ll use directly are the getters. Use the struct’s fields when you can, but you’ll find the getters useful when you have multiple messages with the same getter(s) and you want to abstract those method(s) into an interface. For example, imagine you’re building a retail site like Amazon and have different types of stuff you sell—books, games, and so on—each with a field for the\n\nreport erratum • discuss\n\nWhat You Learned • 21\n\nitem’s price, and you want to find the total of the items in the user’s cart. You’d make a Pricer interface and a Total function that takes in a slice of Pricer interfaces and returns their total cost. Here’s what the code would look like:\n\ntype Book struct {\n\nPrice uint64\n\n}\n\nfunc(b *Book) GetPrice() uint64 { // ... }\n\ntype Game struct {\n\nPrice uint64\n\n}\n\nfunc(b *Game) GetPrice() uint64 { // ... }\n\ntype Pricer interface {\n\nGetPrice() uint64\n\n}\n\nfunc Total(items []Pricer) uint64 { // ... }\n\nNow imagine that you want to write a script to change the price of all your inventory—books, games, and so on. You could do this with reflection, but reflection should be your last resort since, as the Go proverb goes, reflection is never clear.10 If we just had setters, we could use an interface like the fol- lowing to set the price on the different kinds of items in your inventory:\n\ntype PriceAdjuster interface { SetPrice(price uint64)\n\n}\n\nWhen the compiled code isn’t quite what you need, you can extend the com- piler’s output with plugins. Though we don’t need to write a plugin for this project, I’ve written some plugins that were incredibly useful to the projects I worked on; it’s worth learning to write your own so you can recognize when a plugin will save you a ton of manual labor.\n\nWhat You Learned\n\nIn this chapter, we covered the protobuf fundamentals we’ll use throughout our project. These concepts will be vital throughout our project, especially as we build our gRPC client and server. Now let’s create the next vital piece of our project: a commit log library.\n\n10. https://bit.ly/2HcYojl\n\nreport erratum • discuss\n\nCHAPTER 3\n\nWrite a Log Package\n\nIn this book we’re building a distributed service to learn how to create dis- tributed services with Go (shocker). But how does building a log in this chapter help us achieve that goal? I believe the log is the most important tool in your toolkit when building distributed services. Logs—which are sometimes also called write-ahead logs, transaction logs, or commit logs—are at the heart of storage engines, message queues, version control, and replication and consensus algorithms. As you build distributed services, you’ll face problems that you can solve with logs. By building a log yourself, you’ll learn how to:\n\nSolve problems using logs and discover how they can make hard problems\n\neasier.\n\nChange existing log-based systems to fit your needs and build your own\n\nlog-based systems.\n\nWrite and read data efficiently when building storage engines.\n\nProtect against data loss caused by system failures.\n\nEncode data to persist it to a disk or to build your own wire protocols and\n\nsend the data between applications.\n\nAnd who knows—maybe you’ll be the one who builds the next big distributed log service.\n\nThe Log Is a Powerful Tool\n\nFolks who develop storage engines of filesystems and databases use logs to improve the data integrity of their systems. The ext filesystems, for example, log changes to a journal instead of directly changing the disk’s data file. Once the filesystem has safely written the changes to the journal, it then applies those changes to the data files. Logging to the journal is simple and fast, so\n\nreport erratum • discuss\n\nChapter 3. Write a Log Package • 24\n\nthere’s little chance of losing data. Even if your computer crashed before ext had finished updating the disk files, then on the next boot, the filesystem would process the data in the journal to complete its updates. Database developers, like PostgreSQL, use the same technique to make their systems durable: they record changes to a log, called a write-ahead log (WAL), and later process the WAL to apply the changes to their database’s data files.\n\nDatabase developers use the WAL for replication, too. Instead of writing the logs to a disk, they write the logs over the network to its replicas. The replicas apply the changes to their own data copies, and eventually they all end up at the same state. Raft, a consensus algorithm, uses the same idea to get distributed services to agree on a cluster-wide state. Each node in a Raft cluster runs a state machine with a log as its input. The leader of the Raft cluster appends changes to its followers’ logs. Since the state machines use the logs as input and because the logs have the same records in the same order, all the services end up with the same state.\n\nWeb front-end developers use logs to help manage state in their applications. In Redux,1 a popular JavaScript library commonly used with React, you log changes as plain objects and handle those changes with pure functions that apply the updates to your application’s state.\n\nAll these examples use logs to store, share, and process ordered data. This is really cool because the same tool helps replicate databases, coordinate distributed services, and manage state in front-end applications. You can solve a lot of problems, especially in distributed services, by breaking down the changes in your system until they’re single, atomic operations that you can store, share, and process with a log.\n\nDatabases often provide a way to restore their state to some time in the past, often referred to as point-in-time recovery. You take a snapshot of your database from the past and then replay the logs from the write-ahead log until it’s at the point in time you want. You don’t need the snapshot if you have every single log since the beginning to replay, but for databases with long histories and a lot of changes, keeping every log isn’t feasible. Redux uses the same idea to undo/redo actions: it logs the application’s state after each action and undoing an action just requires Redux to move the state shown in the UI to the previously logged state. Distributed version control systems like Git work similarly; your commit log history is a literal commit log.\n\n1.\n\nhttps://redux.js.org\n\nreport erratum • discuss\n\nHow Logs Work • 25\n\nAs you can see, a complete log not only holds the latest state, but all states that have existed, which allows you to build some cool features that you’d find complicated to build otherwise. Logs are simple—and that’s why they’re good.\n\nHow Logs Work\n\nA log is an append-only sequence of records. You append records to the end of the log, and you typically read top to bottom, oldest to newest—similar to running tail -f on a file. You can log any data. People have historically used the term logs to refer to lines of text meant for humans to read, but that’s changed as more people use log systems where their “logs” are binary- encoded messages meant for other programs to read. When I talk about logs and records in this book, I’m not talking about any particular type of data. When you append a record to a log, the log assigns the record a unique and sequential offset number that acts like the ID for that record. A log is like a table that always orders the records by time and indexes each record by its offset and time created.\n\nConcrete implementations of logs have to deal with us not having disks with infinite space, which means we can’t append to the same file forever. So we split the log into a list of segments. When the log grows too big, we free up disk space by deleting old segments whose data we’ve already processed or archived. This cleaning up of old segments can run in a background process while our service can still produce to the active (newest) segment and consume from other segments with no, or at least fewer, conflicts where goroutines access the same data.\n\nThere’s always one special segment among the list of segments, and that’s the active segment. We call it the active segment because it’s the only segment we actively write to. When we’ve filled the active segment, we create a new segment and make it the active segment.\n\nEach segment comprises a store file and an index file. The segment’s store file is where we store the record data; we continually append records to this file. The segment’s index file is where we index each record in the store file. The index file speeds up reads because it maps record offsets to their position in the store file. Reading a record given its offset is a two-step process: first you get the entry from the index file for the record, which tells you the position of the record in the store file, and then you read the record at that position in the store file. Since the index file requires only two small fields—the offset and stored position of the record—the index file is much smaller than the store file that stores all your record data. Index files are small enough that\n\nreport erratum • discuss",
      "page_number": 26
    },
    {
      "number": 3,
      "title": "Write a Log Package • 24",
      "start_page": 35,
      "end_page": 64,
      "detection_method": "regex_chapter_title",
      "content": "Chapter 3. Write a Log Package • 26\n\nwe can memory-map2 them and make operations on the file as fast as operat- ing on in-memory data.\n\nNow that you know how logs work, it’s time to build our own. Let’s get cracking to code it up.\n\nBuild a Log\n\nWe will build our log from the bottom up, starting with the store and index files, then the segment, and finally the log. That way we can write and run tests as we build each piece. Since the word log can refer to at least three different things—a record, the file that stores records, and the abstract data type that ties segments together—to make things less confusing, throughout this chapter, I will consistently use the following terms to mean these things:\n\nRecord—the data stored in our log. • Store—the file we store records in. • Index—the file we store index entries in. • Segment—the abstraction that ties a store and an index together. • Log—the abstraction that ties all the segments together.\n\nCode the Store\n\nTo get started, create a directory at internal/log for our log package, then create a file called store.go in that directory that contains the following code:\n\nWriteALogPackage/internal/log/store.go package log\n\nimport (\n\n\"bufio\" \"encoding/binary\" \"os\" \"sync\"\n\n)\n\nvar (\n\nenc = binary.BigEndian\n\n)\n\nconst (\n\nlenWidth = 8\n\n)\n\ntype store struct {\n\nos.File\n\n2.\n\nhttps://en.wikipedia.org/wiki/Memory-mapped_file\n\nreport erratum • discuss\n\nBuild a Log • 27\n\nmu buf *bufio.Writer size uint64\n\nsync.Mutex\n\n}\n\nfunc newStore(f *os.File) (*store, error) { fi, err := os.Stat(f.Name()) if err != nil {\n\nreturn nil, err\n\n} size := uint64(fi.Size()) return &store{\n\nFile: f, size: size, buf: bufio.NewWriter(f),\n\n}, nil\n\n}\n\nThe store struct is a simple wrapper around a file with two APIs to append and read bytes to and from the file. The newStore(*os.File) function creates a store for the given file. The function calls os.Stat(name string) to get the file’s current size, in case we’re re-creating the store from a file that has existing data, which would happen if, for example, our service had restarted.\n\nWe refer to the enc variable and lenWidth constant repeatedly in the store, so we place them up top where they’re easy to find. enc defines the encoding that we persist record sizes and index entries in and lenWidth defines the number of bytes used to store the record’s length.\n\nNext, write the following Append() method below newStore():\n\nWriteALogPackage/internal/log/store.go func (s *store) Append(p []byte) (n uint64, pos uint64, err error) {\n\ns.mu.Lock() defer s.mu.Unlock() pos = s.size if err := binary.Write(s.buf, enc, uint64(len(p))); err != nil {\n\nreturn 0, 0, err\n\n} w, err := s.buf.Write(p) if err != nil {\n\nreturn 0, 0, err\n\n} w += lenWidth s.size += uint64(w) return uint64(w), pos, nil\n\n}\n\nAppend([]byte) persists the given bytes to the store. We write the length of the record so that, when we read the record, we know how many bytes to read.\n\nreport erratum • discuss\n\nChapter 3. Write a Log Package • 28\n\nWe write to the buffered writer instead of directly to the file to reduce the number of system calls and improve performance. If a user wrote a lot of small records, this would help a lot. Then we return the number of bytes written, which similar Go APIs conventionally do, and the position where the store holds the record in its file. The segment will use this position when it creates an associated index entry for this record.\n\nBelow Append(), add the following Read() method:\n\nWriteALogPackage/internal/log/store.go func (s *store) Read(pos uint64) ([]byte, error) {\n\ns.mu.Lock() defer s.mu.Unlock() if err := s.buf.Flush(); err != nil {\n\nreturn nil, err\n\n} size := make([]byte, lenWidth) if _, err := s.File.ReadAt(size, int64(pos)); err != nil {\n\nreturn nil, err\n\n} b := make([]byte, enc.Uint64(size)) if _, err := s.File.ReadAt(b, int64(pos+lenWidth)); err != nil {\n\nreturn nil, err\n\n} return b, nil\n\n}\n\nRead(pos uint64) returns the record stored at the given position. First it flushes the writer buffer, in case we’re about to try to read a record that the buffer hasn’t flushed to disk yet. We find out how many bytes we have to read to get the whole record, and then we fetch and return the record. The compiler allocates byte slices that don’t escape the functions they’re declared in on the stack. A value escapes when it lives beyond the lifetime of the function call—if you return the value, for example.\n\nPut this ReadAt() method under Read():\n\nWriteALogPackage/internal/log/store.go func (s *store) ReadAt(p []byte, off int64) (int, error) {\n\ns.mu.Lock() defer s.mu.Unlock() if err := s.buf.Flush(); err != nil {\n\nreturn 0, err\n\n} return s.File.ReadAt(p, off)\n\n}\n\nReadAt(p []byte, off int64) reads len(p) bytes into p beginning at the off offset in the store’s file. It implements io.ReaderAt on the store type.\n\nreport erratum • discuss\n\nBuild a Log • 29\n\nLast, add this Close() method after ReadAt():\n\nWriteALogPackage/internal/log/store.go func (s *store) Close() error {\n\ns.mu.Lock() defer s.mu.Unlock() err := s.buf.Flush() if err != nil {\n\nreturn err\n\n} return s.File.Close()\n\n}\n\nClose() persists any buffered data before closing the file.\n\nLet’s test that our store works. Create a store_test.go file in the log directory with the following code:\n\nWriteALogPackage/internal/log/store_test.go package log\n\nimport (\n\n\"io/ioutil\" \"os\" \"testing\"\n\n\"github.com/stretchr/testify/require\"\n\n)\n\nvar (\n\nwrite = []byte(\"hello world\") width = uint64(len(write)) + lenWidth\n\n)\n\nfunc TestStoreAppendRead(t *testing.T) {\n\nf, err := ioutil.TempFile(\"\", \"store_append_read_test\") require.NoError(t, err) defer os.Remove(f.Name())\n\ns, err := newStore(f) require.NoError(t, err)\n\ntestAppend(t, s) testRead(t, s) testReadAt(t, s)\n\ns, err = newStore(f) require.NoError(t, err) testRead(t, s)\n\n}\n\nIn this test, we create a store with a temporary file and call two test helpers to test appending and reading from the store. Then we create the store again\n\nreport erratum • discuss\n\nChapter 3. Write a Log Package • 30\n\nand test reading from it again to verify that our service will recover its state after a restart.\n\nAfter the TestStoreAppendRead() function, add these test helpers:\n\nWriteALogPackage/internal/log/store_test.go func testAppend(t *testing.T, s *store) {\n\nt.Helper() for i := uint64(1); i < 4; i++ {\n\nn, pos, err := s.Append(write) require.NoError(t, err) require.Equal(t, pos+n, width*i)\n\n}\n\n}\n\nfunc testRead(t *testing.T, s *store) {\n\nt.Helper() var pos uint64 for i := uint64(1); i < 4; i++ { read, err := s.Read(pos) require.NoError(t, err) require.Equal(t, write, read) pos += width\n\n}\n\n}\n\nfunc testReadAt(t *testing.T, s *store) {\n\nt.Helper() for i, off := uint64(1), int64(0); i < 4; i++ {\n\nb := make([]byte, lenWidth) n, err := s.ReadAt(b, off) require.NoError(t, err) require.Equal(t, lenWidth, n) off += int64(n)\n\nsize := enc.Uint64(b) b = make([]byte, size) n, err = s.ReadAt(b, off) require.NoError(t, err) require.Equal(t, write, b) require.Equal(t, int(size), n) off += int64(n)\n\n}\n\n}\n\nBelow testReadAt(), add this snippet to test the Close() method:\n\nWriteALogPackage/internal/log/store_test.go func TestStoreClose(t *testing.T) {\n\nf, err := ioutil.TempFile(\"\", \"store_close_test\") require.NoError(t, err) defer os.Remove(f.Name()) s, err := newStore(f)\n\nreport erratum • discuss\n\nBuild a Log • 31\n\nrequire.NoError(t, err) _, _, err = s.Append(write) require.NoError(t, err)\n\nf, beforeSize, err := openFile(f.Name()) require.NoError(t, err)\n\nerr = s.Close() require.NoError(t, err)\n\n_, afterSize, err := openFile(f.Name()) require.NoError(t, err) require.True(t, afterSize > beforeSize)\n\n}\n\nfunc openFile(name string) (file *os.File, size int64, err error)\n\n{\n\nf, err := os.OpenFile( name, os.O_RDWR|os.O_CREATE|os.O_APPEND, 0644,\n\n) if err != nil {\n\nreturn nil, 0, err\n\n} fi, err := f.Stat() if err != nil {\n\nreturn nil, 0, err\n\n} return f, fi.Size(), nil\n\n}\n\nAssuming these tests pass, you know that your log can append and read persisted records.\n\nWrite the Index\n\nNext let’s code the index. Create an index.go file inside internal/log that contains the following code:\n\nWriteALogPackage/internal/log/index.go package log\n\nimport (\n\n\"io\" \"os\"\n\n\"github.com/tysontate/gommap\"\n\n)\n\nvar (\n\noffWidth uint64 = 4 posWidth uint64 = 8 entWidth\n\n= offWidth + posWidth\n\n)\n\nreport erratum • discuss\n\nChapter 3. Write a Log Package • 32\n\ntype index struct {\n\nfile *os.File mmap gommap.MMap size uint64\n\n}\n\nWe use the *Width constants throughout the index, so like with the store’s variables and constants, we put the constants at the top of the file to make them easy to find. The *Width constants define the number of bytes that make up each index entry.\n\nOur index entries contain two fields: the record’s offset and its position in the store file. We store offsets as uint32s and positions as uint64s, so they take up 4 and 8 bytes of space, respectively. We use the entWidth to jump straight to the position of an entry given its offset since the position in the file is offset * entWidth.\n\nindex defines our index file, which comprises a persisted file and a memory- mapped file. The size tells us the size of the index and where to write the next entry appended to the index.\n\nNow add the following newIndex() function below the index:\n\nWriteALogPackage/internal/log/index.go func newIndex(f *os.File, c Config) (*index, error) {\n\nidx := &index{\n\nfile: f,\n\n} fi, err := os.Stat(f.Name()) if err != nil {\n\nreturn nil, err\n\n} idx.size = uint64(fi.Size()) if err = os.Truncate(\n\nf.Name(), int64(c.Segment.MaxIndexBytes),\n\n); err != nil {\n\nreturn nil, err\n\n} if idx.mmap, err = gommap.Map( idx.file.Fd(), gommap.PROT_READ|gommap.PROT_WRITE, gommap.MAP_SHARED,\n\n); err != nil {\n\nreturn nil, err\n\n} return idx, nil\n\n}\n\nreport erratum • discuss\n\nBuild a Log • 33\n\nnewIndex(*os.File) creates an index for the given file. We create the index and save the current size of the file so we can track the amount of data in the index file as we add index entries. We grow the file to the max index size before memory-mapping the file and then return the created index to the caller.\n\nNext, add the following Close() method below newIndex():\n\nWriteALogPackage/internal/log/index.go func (i *index) Close() error {\n\nif err := i.mmap.Sync(gommap.MS_SYNC); err != nil {\n\nreturn err\n\n} if err := i.file.Sync(); err != nil {\n\nreturn err\n\n} if err := i.file.Truncate(int64(i.size)); err != nil {\n\nreturn err\n\n} return i.file.Close()\n\n}\n\nClose() makes sure the memory-mapped file has synced its data to the persisted file and that the persisted file has flushed its contents to stable storage. Then it truncates the persisted file to the amount of data that’s actually in it and closes the file.\n\nNow that we’ve seen the code for both opening and closing an index, we can discuss what this growing and truncating the file business is all about.\n\nWhen we start our service, the service needs to know the offset to set on the next record appended to the log. The service learns the next record’s offset by looking at the last entry of the index, a simple process of reading the last 12 bytes of the file. However, we mess up this process when we grow the files so we can memory-map them. (The reason we resize them now is that, once they’re memory-mapped, we can’t resize them, so it’s now or never.) We grow the files by appending empty space at the end of them, so the last entry is no longer at the end of the file—instead, there’s some unknown amount of space between this entry and the file’s end. This space prevents the service from restarting properly. That’s why we shut down the service by truncating the index files to remove the empty space and put the last entry at the end of the file once again. This graceful shutdown returns the service to a state where it can restart properly and efficiently.\n\nreport erratum • discuss\n\nChapter 3. Write a Log Package • 34\n\nHandling Ungraceful Shutdowns\n\nA graceful shutdown occurs when a service finishes its ongoing tasks, performs its processes to ensure there’s no data loss, and prepares for a restart. If your service crashes or its hardware fails, you’ll experience an ungraceful shutdown. An example of an ungraceful shutdown for the service we’re building would be if it lost power before it finished truncating its index files. You handle ungraceful shutdowns by performing a sanity check when your service restarts to find corrupted data. If you have corrupted data, you can rebuild the data or replicate the data from an uncorrupted source. The log we’re building doesn’t handle ungraceful shut- downs because I wanted to keep the code simple.\n\nAnd now back to our regularly scheduled programming.\n\nAdd the following Read() method below newIndex():\n\nWriteALogPackage/internal/log/index.go func (i *index) Read(in int64) (out uint32, pos uint64, err error) {\n\nif i.size == 0 {\n\nreturn 0, 0, io.EOF\n\n} if in == -1 {\n\nout = uint32((i.size / entWidth) - 1)\n\n} else {\n\nout = uint32(in)\n\n} pos = uint64(out) * entWidth if i.size < pos+entWidth {\n\nreturn 0, 0, io.EOF\n\n} out = enc.Uint32(i.mmap[pos : pos+offWidth]) pos = enc.Uint64(i.mmap[pos+offWidth : pos+entWidth]) return out, pos, nil\n\n}\n\nRead(int64) takes in an offset and returns the associated record’s position in the store. The given offset is relative to the segment’s base offset; 0 is always the offset of the index’s first entry, 1 is the second entry, and so on. We use relative offsets to reduce the size of the indexes by storing offsets as uint32s. If we used absolute offsets, we’d have to store the offsets as uint64s and require four more bytes for each entry. Four bytes doesn’t sound like much, until you multiply it by the number of records people often use distributed logs for, which with a company like LinkedIn is trillions of records every day. Even relatively small companies can make billions of records per day.\n\nreport erratum • discuss\n\nBuild a Log • 35\n\nNow add the following Write() method below Read():\n\nWriteALogPackage/internal/log/index.go func (i *index) Write(off uint32, pos uint64) error {\n\nif uint64(len(i.mmap)) < i.size+entWidth {\n\nreturn io.EOF\n\n} enc.PutUint32(i.mmap[i.size:i.size+offWidth], off) enc.PutUint64(i.mmap[i.size+offWidth:i.size+entWidth], pos) i.size += uint64(entWidth) return nil\n\n}\n\nWrite(off uint32, pos uint32) appends the given offset and position to the index. First, we validate that we have space to write the entry. If there’s space, we then encode the offset and position and write them to the memory-mapped file. Then we increment the position where the next write will go.\n\nAdd this Name() method to return the index’s file path:\n\nWriteALogPackage/internal/log/index.go func (i *index) Name() string {\n\nreturn i.file.Name()\n\n}\n\nLet’s test our index. Create an index_test.go file in internal/log starting with the following code:\n\nWriteALogPackage/internal/log/index_test.go package log\n\nimport (\n\n\"io\" \"io/ioutil\" \"os\" \"testing\"\n\n\"github.com/stretchr/testify/require\"\n\n)\n\nfunc TestIndex(t *testing.T) {\n\nf, err := ioutil.TempFile(os.TempDir(), \"index_test\") require.NoError(t, err) defer os.Remove(f.Name())\n\nc := Config{} c.Segment.MaxIndexBytes = 1024 idx, err := newIndex(f, c) require.NoError(t, err) _, _, err = idx.Read(-1) require.Error(t, err) require.Equal(t, f.Name(), idx.Name())\n\nreport erratum • discuss\n\nChapter 3. Write a Log Package • 36\n\nentries := []struct {\n\nOff uint32 Pos uint64\n\n}{\n\n{Off: 0, Pos: 0}, {Off: 1, Pos: 10},\n\n}\n\nThis code sets up the test. We create an index file and make it big enough to contain our test entries via the Truncate() call. We have to grow the file before we use it because we memory-map the file to a slice of bytes and if we didn’t increase the size of the file before we wrote to it, we’d get an out-of-bounds error.\n\nFinally, add the following code beneath the previous snippet to finish the test:\n\nWriteALogPackage/internal/log/index_test.go\n\nfor _, want := range entries {\n\nerr = idx.Write(want.Off, want.Pos) require.NoError(t, err)\n\n_, pos, err := idx.Read(int64(want.Off)) require.NoError(t, err) require.Equal(t, want.Pos, pos)\n\n}\n\n// index and scanner should error when reading past existing entries _, _, err = idx.Read(int64(len(entries))) require.Equal(t, io.EOF, err) _ = idx.Close()\n\n// index should build its state from the existing file f, _ = os.OpenFile(f.Name(), os.O_RDWR, 0600) idx, err = newIndex(f, c) require.NoError(t, err) off, pos, err := idx.Read(-1) require.NoError(t, err) require.Equal(t, uint32(1), off) require.Equal(t, entries[1].Pos, pos)\n\n}\n\nWe iterate over each entry and write it to the index. We check that we can read the same entry back via the Read() method. Then we verify that the index and scanner error when we try to read beyond the number of entries stored in the index. And we check that the index builds its state from the existing file, for when our service restarts with existing data.\n\nWe need to configure the max size of a segment’s store and index. Let’s add a config struct to centralize the log’s configuration, making it easy to configure\n\nreport erratum • discuss\n\nBuild a Log • 37\n\nthe log and use the configs throughout the code. Create an internal/log/config.go file with the following code:\n\nWriteALogPackage/internal/log/config.go package log\n\ntype Config struct {\n\nSegment struct {\n\nMaxStoreBytes uint64 MaxIndexBytes uint64 InitialOffset uint64\n\n}\n\n}\n\nThat wraps up the code for store and index types, which make up the lowest level of our log. Now let’s code the segment.\n\nCreate the Segment\n\nThe segment wraps the index and store types to coordinate operations across the two. For example, when the log appends a record to the active segment, the segment needs to write the data to its store and add a new entry in the index. Similarly for reads, the segment needs to look up the entry from the index and then fetch the data from the store.\n\nTo get started, create a file called segment.go in internal/log that starts with the following code:\n\nWriteALogPackage/internal/log/segment.go package log\n\nimport (\n\n\"fmt\" \"os\" \"path\"\n\napi \"github.com/travisjeffery/proglog/api/v1\" \"google.golang.org/protobuf/proto\"\n\n)\n\ntype segment struct { *store store index *index baseOffset, nextOffset uint64 Config config\n\n}\n\nOur segment needs to call its store and index files, so we keep pointers to those in the first two fields. We need the next and base offsets to know what offset to append new records under and to calculate the relative offsets for the index entries. And we put the config on the segment so we can compare\n\nreport erratum • discuss\n\nChapter 3. Write a Log Package • 38\n\nthe store file and index sizes to the configured limits, which lets us know when the segment is maxed out.\n\nBelow the previous snippet, add the following newSegment() function:\n\nWriteALogPackage/internal/log/segment.go func newSegment(dir string, baseOffset uint64, c Config) (*segment, error) {\n\ns := &segment{\n\nbaseOffset: baseOffset, config:\n\nc,\n\n} var err error storeFile, err := os.OpenFile(\n\npath.Join(dir, fmt.Sprintf(\"%d%s\", baseOffset, \".store\")), os.O_RDWR|os.O_CREATE|os.O_APPEND, 0644,\n\n) if err != nil {\n\nreturn nil, err\n\n} if s.store, err = newStore(storeFile); err != nil {\n\nreturn nil, err\n\n} indexFile, err := os.OpenFile(\n\npath.Join(dir, fmt.Sprintf(\"%d%s\", baseOffset, \".index\")), os.O_RDWR|os.O_CREATE, 0644,\n\n) if err != nil {\n\nreturn nil, err\n\n} if s.index, err = newIndex(indexFile, c); err != nil {\n\nreturn nil, err\n\n} if off, _, err := s.index.Read(-1); err != nil {\n\ns.nextOffset = baseOffset\n\n} else {\n\ns.nextOffset = baseOffset + uint64(off) + 1\n\n} return s, nil\n\n}\n\nThe log calls newSegment() when it needs to add a new segment, such as when the current active segment hits its max size. We open the store and index files and pass the os.O_CREATE file mode flag as an argument to os.OpenFile() to create the files if they don’t exist yet. When we create the store file, we pass the os.O_APPEND flag to make the operating system append to the file when writing. Then we create our index and store with these files. Finally, we set the segment’s next offset to prepare for the next appended record. If the index\n\nreport erratum • discuss\n\nBuild a Log • 39\n\nis empty, then the next record appended to the segment would be the first record and its offset would be the segment’s base offset. If the index has at least one entry, then that means the offset of the next record written should take the offset at the end of the segment, which we get by adding 1 to the base offset and relative offset. Our segment is ready to write to and read from the log—once we’ve written those methods!\n\nNext, below newSegment() put the following Append() method:\n\nWriteALogPackage/internal/log/segment.go func (s *segment) Append(record *api.Record) (offset uint64, err error) {\n\ncur := s.nextOffset record.Offset = cur p, err := proto.Marshal(record) if err != nil {\n\nreturn 0, err\n\n} _, pos, err := s.store.Append(p) if err != nil {\n\nreturn 0, err\n\n} if err = s.index.Write(\n\n// index offsets are relative to base offset uint32(s.nextOffset-uint64(s.baseOffset)), pos,\n\n); err != nil {\n\nreturn 0, err\n\n} s.nextOffset++ return cur, nil\n\n}\n\nAppend() writes the record to the segment and returns the newly appended record’s offset. The log returns the offset to the API response. The segment appends a record in a two-step process: it appends the data to the store and then adds an index entry. Since index offsets are relative to the base offset, we subtract the segment’s next offset from its base offset (which are both absolute offsets) to get the entry’s relative offset in the segment. We then increment the next offset to prep for a future append call.\n\nNow add the following Read() method below Append():\n\nWriteALogPackage/internal/log/segment.go func (s *segment) Read(off uint64) (*api.Record, error) {\n\n_, pos, err := s.index.Read(int64(off - s.baseOffset)) if err != nil {\n\nreturn nil, err\n\n} p, err := s.store.Read(pos)\n\nreport erratum • discuss\n\nChapter 3. Write a Log Package • 40\n\nif err != nil {\n\nreturn nil, err\n\n} record := &api.Record{} err = proto.Unmarshal(p, record) return record, err\n\n}\n\nRead(off uint64) returns the record for the given offset. Similar to writes, to read a record the segment must first translate the absolute index into a relative offset and get the associated index entry. Once it has the index entry, the segment can go straight to the record’s position in the store and read the proper amount of data.\n\nNext, put the following IsMaxed() method below Read():\n\nWriteALogPackage/internal/log/segment.go func (s *segment) IsMaxed() bool {\n\nreturn s.store.size >= s.config.Segment.MaxStoreBytes ||\n\ns.index.size >= s.config.Segment.MaxIndexBytes\n\n}\n\nIsMaxed() returns whether the segment has reached its max size, either by writing too much to the store or the index. If you wrote a small number of long logs, then you’d hit the segment bytes limit; if you wrote a lot of small logs, then you’d hit the index bytes limit. The log uses this method to know it needs to create a new segment.\n\nWrite this Remove() method below IsMaxed():\n\nWriteALogPackage/internal/log/segment.go func (s *segment) Remove() error {\n\nif err := s.Close(); err != nil {\n\nreturn err\n\n} if err := os.Remove(s.index.Name()); err != nil {\n\nreturn err\n\n} if err := os.Remove(s.store.Name()); err != nil {\n\nreturn err\n\n} return nil\n\n}\n\nRemove() closes the segment and removes the index and store files.\n\nAnd put this Close() method below Remove():\n\nreport erratum • discuss\n\nBuild a Log • 41\n\nWriteALogPackage/internal/log/segment.go func (s *segment) Close() error {\n\nif err := s.index.Close(); err != nil {\n\nreturn err\n\n} if err := s.store.Close(); err != nil {\n\nreturn err\n\n} return nil\n\n}\n\nFinally, add this last function at the end of the file:\n\nWriteALogPackage/internal/log/segment.go func nearestMultiple(j, k uint64) uint64 {\n\nif j >= 0 {\n\nreturn (j / k) * k\n\n} return ((j - k + 1) / k) * k\n\n}\n\nnearestMultiple(j uint64, k uint64) returns the nearest and lesser multiple of k in j, for example nearestMultiple(9, 4) == 8. We take the lesser multiple to make sure we stay under the user’s disk capacity.\n\nThat’s all the segment code, so now let’s test it. Create a segment_test.go file inside internal/log with the following test code:\n\nWriteALogPackage/internal/log/segment_test.go package log\n\nimport (\n\n\"io\" \"io/ioutil\" \"os\" \"testing\"\n\n\"github.com/stretchr/testify/require\" api \"github.com/travisjeffery/proglog/api/v1\"\n\n)\n\nfunc TestSegment(t *testing.T) {\n\ndir, _ := ioutil.TempDir(\"\", \"segment-test\") defer os.RemoveAll(dir)\n\nwant := &api.Record{Value: []byte(\"hello world\")}\n\nc := Config{} c.Segment.MaxStoreBytes = 1024 c.Segment.MaxIndexBytes = entWidth * 3\n\ns, err := newSegment(dir, 16, c) require.NoError(t, err)\n\nreport erratum • discuss\n\nChapter 3. Write a Log Package • 42\n\nrequire.Equal(t, uint64(16), s.nextOffset, s.nextOffset) require.False(t, s.IsMaxed())\n\nfor i := uint64(0); i < 3; i++ {\n\noff, err := s.Append(want) require.NoError(t, err) require.Equal(t, 16+i, off)\n\ngot, err := s.Read(off) require.NoError(t, err) require.Equal(t, want.Value, got.Value)\n\n}\n\n_, err = s.Append(want) require.Equal(t, io.EOF, err)\n\n// maxed index require.True(t, s.IsMaxed())\n\nc.Segment.MaxStoreBytes = uint64(len(want.Value) * 3) c.Segment.MaxIndexBytes = 1024\n\ns, err = newSegment(dir, 16, c) require.NoError(t, err) // maxed store require.True(t, s.IsMaxed())\n\nerr = s.Remove() require.NoError(t, err) s, err = newSegment(dir, 16, c) require.NoError(t, err) require.False(t, s.IsMaxed())\n\n}\n\nWe test that we can append a record to a segment, read back the same record, and eventually hit the configured max size for both the store and index. Calling newSegment() twice with the same base offset and dir also checks that the function loads a segment’s state from the persisted index and log files.\n\nNow that we know that our segment works, we’re ready to create the log.\n\nCode the Log\n\nAll right, one last piece to go and that’s the log, which manages the list of seg- ments. Create a log.go file inside internal/log that starts with the following code:\n\nWriteALogPackage/internal/log/log.go package log\n\nimport (\n\n\"fmt\" \"io\" \"io/ioutil\" \"os\"\n\nreport erratum • discuss\n\nBuild a Log • 43\n\n\"path\" \"sort\" \"strconv\" \"strings\" \"sync\"\n\napi \"github.com/travisjeffery/proglog/api/v1\"\n\n)\n\ntype Log struct {\n\nmu sync.RWMutex\n\nDir string Config Config\n\nactiveSegment *segment segments\n\n[]*segment\n\n}\n\nThe log consists of a list of segments and a pointer to the active segment to append writes to. The directory is where we store the segments.\n\nBelow the Log struct, write the following NewLog() function:\n\nWriteALogPackage/internal/log/log.go func NewLog(dir string, c Config) (*Log, error) { if c.Segment.MaxStoreBytes == 0 {\n\nc.Segment.MaxStoreBytes = 1024\n\n} if c.Segment.MaxIndexBytes == 0 {\n\nc.Segment.MaxIndexBytes = 1024\n\n} l := &Log{\n\nDir: Config: c,\n\ndir,\n\n}\n\nreturn l, l.setup()\n\n}\n\nIn NewLog(dir string, c Config), we first set defaults for the configs the caller didn’t specify, create a log instance, and set up that instance.\n\nNext, add this setup() method below NewLog():\n\nWriteALogPackage/internal/log/log.go func (l *Log) setup() error {\n\nfiles, err := ioutil.ReadDir(l.Dir) if err != nil {\n\nreturn err\n\n} var baseOffsets []uint64 for _, file := range files {\n\noffStr := strings.TrimSuffix(\n\nreport erratum • discuss\n\nChapter 3. Write a Log Package • 44\n\nfile.Name(), path.Ext(file.Name()),\n\n) off, _ := strconv.ParseUint(offStr, 10, 0) baseOffsets = append(baseOffsets, off)\n\n} sort.Slice(baseOffsets, func(i, j int) bool {\n\nreturn baseOffsets[i] < baseOffsets[j]\n\n}) for i := 0; i < len(baseOffsets); i++ {\n\nif err = l.newSegment(baseOffsets[i]); err != nil {\n\nreturn err\n\n} // baseOffset contains dup for index and store so we skip // the dup i++\n\n} if l.segments == nil {\n\nif err = l.newSegment(\n\nl.Config.Segment.InitialOffset,\n\n); err != nil {\n\nreturn err\n\n}\n\n} return nil\n\n}\n\nWhen a log starts, it’s responsible for setting itself up for the segments that already exist on disk or, if the log is new and has no existing segments, for bootstrapping the initial segment. We fetch the list of the segments on disk, parse and sort the base offsets (because we want our slice of segments to be in order from oldest to newest), and then create the segments with the newSegment() helper method, which creates a segment for the base offset you pass in.\n\nNow add the following Append() function below setup():\n\nWriteALogPackage/internal/log/log.go func (l *Log) Append(record *api.Record) (uint64, error) {\n\nl.mu.Lock() defer l.mu.Unlock() off, err := l.activeSegment.Append(record) if err != nil {\n\nreturn 0, err\n\n} if l.activeSegment.IsMaxed() {\n\nerr = l.newSegment(off + 1)\n\n} return off, err\n\n}\n\nreport erratum • discuss\n\nBuild a Log • 45\n\nAppend(*api.Record) appends a record to the log. We append the record to the active segment. Afterward, if the segment is at its max size (per the max size configs), then we make a new active segment. Note that we’re wrapping this func (and subsequent funcs) with a mutex to coordinate access to this section of the code. We use a RWMutex to grant access to reads when there isn’t a write holding the lock. If you felt so inclined, you could optimize this further and make the locks per segment rather than across the whole log. (I haven’t done that here because I want to keep this code simple.)\n\nBelow Append(), add this Read() method:\n\nWriteALogPackage/internal/log/log.go func (l *Log) Read(off uint64) (*api.Record, error) {\n\nl.mu.RLock() defer l.mu.RUnlock() var s *segment for _, segment := range l.segments {\n\nif segment.baseOffset <= off && off < segment.nextOffset {\n\ns = segment break\n\n}\n\n} if s == nil || s.nextOffset <= off {\n\nreturn nil, fmt.Errorf(\"offset out of range: %d\", off)\n\n} return s.Read(off)\n\n}\n\nRead(offset uint64) reads the record stored at the given offset. In Read(offset uint64), we first find the segment that contains the given record. Since the segments are in order from oldest to newest and the segment’s base offset is the smallest offset in the segment, we iterate over the segments until we find the first segment whose base offset is less than or equal to the offset we’re looking for. Once we know the segment that contains the record, we get the index entry from the segment’s index, and we read the data out of the segment’s store file and return the data to the caller.\n\nBelow Read(), add this snippet to define the Close(), Remove(), and Reset() methods:\n\nWriteALogPackage/internal/log/log.go func (l *Log) Close() error {\n\nl.mu.Lock() defer l.mu.Unlock() for _, segment := range l.segments {\n\nif err := segment.Close(); err != nil {\n\nreturn err\n\n}\n\n}\n\nreport erratum • discuss\n\nChapter 3. Write a Log Package • 46\n\nreturn nil\n\n}\n\nfunc (l *Log) Remove() error {\n\nif err := l.Close(); err != nil {\n\nreturn err\n\n} return os.RemoveAll(l.Dir)\n\n}\n\nfunc (l *Log) Reset() error {\n\nif err := l.Remove(); err != nil {\n\nreturn err\n\n} return l.setup()\n\n}\n\nThis snippet implements a few related methods:\n\nClose() iterates over the segments and closes them. • Remove() closes the log and then removes its data. • Reset() removes the log and then creates a new log to replace it.\n\nAfter the previous snippet, add this snippet to implement the LowestOffset() and HighestOffset() methods:\n\nWriteALogPackage/internal/log/log.go func (l *Log) LowestOffset() (uint64, error) {\n\nl.mu.RLock() defer l.mu.RUnlock() return l.segments[0].baseOffset, nil\n\n}\n\nfunc (l *Log) HighestOffset() (uint64, error) {\n\nl.mu.RLock() defer l.mu.RUnlock() off := l.segments[len(l.segments)-1].nextOffset if off == 0 {\n\nreturn 0, nil\n\n} return off - 1, nil\n\n}\n\nThese methods tell us the offset range stored in the log. In Chapter 8, Coordi- nate Your Services with Consensus, on page 141, when we work on supporting a replicated, coordinated cluster, we’ll need this information to know what nodes have the oldest and newest data and what nodes are falling behind and need to replicate.\n\nBelow HighestOffset(), add this Truncate() method:\n\nreport erratum • discuss\n\nBuild a Log • 47\n\nWriteALogPackage/internal/log/log.go func (l *Log) Truncate(lowest uint64) error {\n\nl.mu.Lock() defer l.mu.Unlock() var segments []*segment for _, s := range l.segments {\n\nif s.nextOffset <= lowest+1 {\n\nif err := s.Remove(); err != nil {\n\nreturn err\n\n} continue\n\n} segments = append(segments, s)\n\n} l.segments = segments return nil\n\n}\n\nTruncate(lowest uint64) removes all segments whose highest offset is lower than lowest. Because we don’t have disks with infinite space, we’ll periodically call Truncate() to remove old segments whose data we (hopefully) have processed by then and don’t need anymore.\n\nAfter Truncate(), add this snippet:\n\nWriteALogPackage/internal/log/log.go func (l *Log) Reader() io.Reader {\n\nl.mu.RLock() defer l.mu.RUnlock() readers := make([]io.Reader, len(l.segments)) for i, segment := range l.segments {\n\nreaders[i] = &originReader{segment.store, 0}\n\n} return io.MultiReader(readers...)\n\n}\n\ntype originReader struct {\n\nstore off int64\n\n}\n\nfunc (o *originReader) Read(p []byte) (int, error) {\n\nn, err := o.ReadAt(p, o.off) o.off += int64(n) return n, err\n\n}\n\nReader() returns an io.Reader to read the whole log. We’ll need this capability when we implement coordinate consensus and need to support snapshots and restoring a log. Reader() uses an io.MultiReader() call to concatenate the seg- ments’ stores. The segment stores are wrapped by the originReader type for two\n\nreport erratum • discuss\n\nChapter 3. Write a Log Package • 48\n\nreasons. The first reason is to satisfy the io.Reader interface so we can pass it into the io.MultiReader() call. The second is to ensure that we begin reading from the origin of the store and read its entire file.\n\nWe’ve got one last method to add to our log, and that’s the function to create new segments. Copy the following newSegment() method below Read():\n\nWriteALogPackage/internal/log/log.go func (l *Log) newSegment(off uint64) error {\n\ns, err := newSegment(l.Dir, off, l.Config) if err != nil {\n\nreturn err\n\n} l.segments = append(l.segments, s) l.activeSegment = s return nil\n\n}\n\nnewSegment(off int64) creates a new segment, appends that segment to the log’s slice of segments, and makes the new segment the active segment so that subsequent append calls write to it.\n\nYou know the deal: it’s time to test our log. Create a log_test.go inside internal/log that starts with the following code:\n\nWriteALogPackage/internal/log/log_test.go package log\n\nimport (\n\n\"io/ioutil\" \"os\" \"testing\"\n\n\"github.com/stretchr/testify/require\" api \"github.com/travisjeffery/proglog/api/v1\" \"google.golang.org/protobuf/proto\"\n\n)\n\nfunc TestLog(t *testing.T) {\n\nfor scenario, fn := range map[string]func(\n\nt *testing.T, log *Log,\n\n){\n\n\"append and read a record succeeds\": testAppendRead, \"offset out of range error\": \"init with existing segments\": \"reader\": \"truncate\":\n\ntestOutOfRangeErr, testInitExisting, testReader, testTruncate,\n\n} {\n\nt.Run(scenario, func(t *testing.T) {\n\ndir, err := ioutil.TempDir(\"\", \"store-test\") require.NoError(t, err) defer os.RemoveAll(dir)\n\nreport erratum • discuss\n\nBuild a Log • 49\n\nc := Config{} c.Segment.MaxStoreBytes = 32 log, err := NewLog(dir, c) require.NoError(t, err)\n\nfn(t, log)\n\n})\n\n}\n\n}\n\nTestLog(*testing.T) defines a table of tests to, well, test the log. I used a table to write the log tests so we don’t have to repeat the code that creates a new log for every test case.\n\nNow, let’s define the test cases. Put the following test cases below the TestLog() function:\n\nWriteALogPackage/internal/log/log_test.go func testAppendRead(t *testing.T, log *Log) {\n\nappend := &api.Record{\n\nValue: []byte(\"hello world\"),\n\n} off, err := log.Append(append) require.NoError(t, err) require.Equal(t, uint64(0), off)\n\nread, err := log.Read(off) require.NoError(t, err) require.Equal(t, append.Value, read.Value)\n\n}\n\ntestAppendRead(*testing.T, *log.Log) tests that we can successfully append to and read from the log. When we append a record to the log, the log returns the offset it associated that record with. So, when we ask the log for the record at that offset, we expect to get the same record that we appended.\n\nWriteALogPackage/internal/log/log_test.go func testOutOfRangeErr(t *testing.T, log *Log) {\n\nread, err := log.Read(1) require.Nil(t, read) require.Error(t, err)\n\n}\n\ntestOutOfRangeErr(*testing.T, *log.Log) tests that the log returns an error when we try to read an offset that’s outside of the range of offsets the log has stored.\n\nWriteALogPackage/internal/log/log_test.go func testInitExisting(t *testing.T, o *Log) {\n\nappend := &api.Record{\n\nValue: []byte(\"hello world\"),\n\n}\n\nreport erratum • discuss\n\nChapter 3. Write a Log Package • 50\n\nfor i := 0; i < 3; i++ {\n\n_, err := o.Append(append) require.NoError(t, err)\n\n} require.NoError(t, o.Close())\n\noff, err := o.LowestOffset() require.NoError(t, err) require.Equal(t, uint64(0), off) off, err = o.HighestOffset() require.NoError(t, err) require.Equal(t, uint64(2), off)\n\nn, err := NewLog(o.Dir, o.Config) require.NoError(t, err)\n\noff, err = n.LowestOffset() require.NoError(t, err) require.Equal(t, uint64(0), off) off, err = n.HighestOffset() require.NoError(t, err) require.Equal(t, uint64(2), off)\n\n}\n\ntestInitExisting(*testing.T,*log.Log) tests that when we create a log, the log bootstraps itself from the data stored by prior log instances. We append three records to the original log before closing it. Then we create a new log configured with the same directory as the old log. Finally, we confirm that the new log set itself up from the data stored by the original log.\n\nWriteALogPackage/internal/log/log_test.go func testReader(t *testing.T, log *Log) {\n\nappend := &api.Record{\n\nValue: []byte(\"hello world\"),\n\n} off, err := log.Append(append) require.NoError(t, err) require.Equal(t, uint64(0), off)\n\nreader := log.Reader() b, err := ioutil.ReadAll(reader) require.NoError(t, err)\n\nread := &api.Record{} err = proto.Unmarshal(b[lenWidth:], read) require.NoError(t, err) require.Equal(t, append.Value, read.Value)\n\n}\n\ntestReader(*testing.T, *log.Log) tests that we can read the full, raw log as it’s stored on disk so that we can snapshot and restore the logs in Finite-State Machine, on page 151.\n\nreport erratum • discuss\n\nWhat You Learned • 51\n\nWriteALogPackage/internal/log/log_test.go func testTruncate(t *testing.T, log *Log) {\n\nappend := &api.Record{\n\nValue: []byte(\"hello world\"),\n\n} for i := 0; i < 3; i++ {\n\n_, err := log.Append(append) require.NoError(t, err)\n\n}\n\nerr := log.Truncate(1) require.NoError(t, err)\n\n_, err = log.Read(0) require.Error(t, err)\n\n}\n\ntestTruncate(*testing.T, *log.Log) tests that we can truncate the log and remove old segments that we don’t need any more.\n\nThat wraps up our log code! We just wrote a log that’s not that watered down from the log that drives Kafka, and we didn’t even have to work too hard.\n\nWhat You Learned\n\nYou now know what logs are, why they’re important, and how they’re used in various applications including distributed services. And then you learned how to build one! This log serves as the foundation of our distributed log. Now we can build a service on our library and make the library’s functionality accessible to people on other computers.\n\nreport erratum • discuss\n\nPart II\n\nNetwork\n\nCHAPTER 4\n\nServe Requests with gRPC\n\nWe’ve set up our project and protocol buffers and written our log library. Currently, our library can only be used on a single computer by a single person at a time. Plus that person has to learn our library’s API, run our code, and store the log on their disk—none of which most people will do, which limits our user base. We can solve these problems and appeal to a larger audience by turning our library into a web service. Compared to a program that runs on a single computer, networked services provide three major advantages:\n\nYou can run them across multiple computers for availability and scalability. • They allow multiple people to interact with the same data. • They provide accessible interfaces that are easy for people to use.\n\nSome situations where you’ll want to write services to reap these advantages include providing a public API for your front end to hit, building internal infrastructure tools, and making a service to build your own business on (people rarely pay to use libraries).\n\nIn this chapter, we’ll build on our library and make a service that allows multiple people to interact with the same data and runs across multiple computers. We won’t add support for clusters right now; we’ll do that in Chapter 8, Coordinate Your Services with Consensus, on page 141. The best tool I’ve found for serving requests across distributed services is Google’s gRPC.\n\nWhat Is gRPC?\n\nWhen I was building distributed services in the past, the two common prob- lems that drove me batty were maintaining compatibility and maintaining performance between clients and the server.\n\nreport erratum • discuss\n\nChapter 4. Serve Requests with gRPC • 56\n\nI wanted to ensure that clients and the server were always compatible—that the client was sending requests that the server understood, and vice versa with the server’s responses. When I made breaking changes to the server, I wanted to ensure that old clients continued to work, and I accomplished this by versioning my API.\n\nFor maintaining good performance, your main priorities are optimizing your database queries and optimizing the algorithms you’ve used to implement your business logic. Once you’ve optimized those though, performance will often come down to how fast your service unmarshals requests and marshals responses, and down to reducing the overhead each time clients and the server communicate—like using a single, long-lasting connection rather than a new connection for each request.\n\nSo I was happy when Google released gRPC, an open source, high-performance RPC (remote procedure call) framework. gRPC has been a great help in solving these problems when building distributed systems, and I think you’ll find that it simplifies your work. How does gRPC help you build services?\n\nGoals When Building a Service\n\nHere are the most important goals to aim for when you’re building a networked service—and some info about how gRPC helps you achieve them:\n\nSimplicity\n\nNetworked communication is technical and complex. When building our service, we want to focus on the problem it solves rather than the technical minutiae of request-response serialization, and so on. You want to work with APIs that abstract these details away. However, when you need to work at lower levels of abstraction, then you need those levels to be accessible.\n\nOn the spectrum of low- to high-level frameworks, in terms of the abstrac- tions you’re working with, gRPC is mid-to-high level. It’s above a framework like Express since gRPC decides how to serialize and structure your end- points and provides features like bidirectional streaming, but below a framework like Rails since Rails handles everything from handling requests to storing your data and structuring your application. gRPC is extendable via middleware, and its active community1 has written middleware2 to\n\n1. 2.\n\nhttps://github.com/grpc-ecosystem\n\nhttps://github.com/grpc-ecosystem/go-grpc-middleware\n\nreport erratum • discuss\n\nGoals When Building a Service • 57\n\nsolve a lot of the problems you’ll face when building services—for example, logging, authentication, rate limiting, and tracing.\n\nMaintainability\n\nWriting the first version of a service is a brief period of the total time you’ll spend working on the service. Once your service is live and people depend on it, you must maintain backward compatibility. With request-response type APIs, the simplest way to maintain backward compatibility is to version and run multiple instances of your API.\n\nWith gRPC, you can easily write and run separate versions of your services when you have major API changes, while still taking advantage of proto- buf’s field versioning for small changes. Having all your requests and responses type checked helps prevent accidentally introducing back- ward-incompatible changes as you and your peers build your service.\n\nSecurity\n\nWhen you expose a service on a network, you expose the service to who- ever is on that network—potentially the whole Internet. It’s important that you control who has access to your service and what they can do.\n\ngRPC supports Secure Sockets Layer/Transport Layer Security (SSL/TLS) to encrypt all data exchanged between the client and server and lets you attach credentials to requests so you know which user is making each request. We’ll discuss security in the next chapter.\n\nEase of use\n\nThe whole point of writing a service is to have people use it and solve some problem of theirs. The easier your service is to use, the more popular it will be. You go a long way toward making your service easy to use by telling your users when they’re doing something wrong, such as calling your API with a bad request.\n\nWith gRPC, everything from your service methods to your requests and responses and their bodies are all defined in types. The compiler copies the comments from your protobuf to your code to help users when the type defi- nitions aren’t good enough. Your users will know whether they’re using the API correctly thanks to their code being type checked. Having every- thing—requests, responses, models, and serialization—type checked is a big help to people learning how to use your service. gRPC also lets users look up the API’s details with godoc. Many frameworks don’t offer either of these handy features.\n\nreport erratum • discuss",
      "page_number": 35
    },
    {
      "number": 4,
      "title": "Serve Requests with gRPC • 56",
      "start_page": 65,
      "end_page": 83,
      "detection_method": "regex_chapter_title",
      "content": "Chapter 4. Serve Requests with gRPC • 58\n\nPerformance\n\nYou want your service to be as fast as possible while using as few resources as possible. For example, if you can run your application on an n1-standard-1 (~$35 per month) instance on Google Cloud Platform rather than on an n1-standard-2 (~$71 per month) instance, that cuts your costs in half.\n\ngRPC is built on solid foundations with protobuf and HTTP/2 because protobuf performs very well at serialization and HTTP/2 provides a means for long- lasting connections, which gRPC takes advantage of. So your service runs efficiently and doesn’t cause unnecessarily high server bills.\n\nScalability\n\nScalability can refer to scaling up with load balancing to balance the load across multiple computers and to scaling up the number of people developing a project. gRPC helps make both types of scaling easier.\n\nYou can use different kinds of load balancing with gRPC3 based on your needs, including thick client-side load balancing, proxy load balancing, look-aside load balancing, or service mesh.\n\nFor scaling up the number of people working on your project, gRPC lets you compile your service into clients and servers in the languages that gRPC supports. This allows people to use their own languages to build services that communicate with each other.\n\nWe now know what we want out of building our service, so let’s create a gRPC service that fulfills our goals.\n\nDefine a gRPC Service\n\nA gRPC service is essentially a group of related RPC endpoints—exactly how they’re related is up to you. A common example is a RESTful grouping where the relation is that the endpoints operate on the same resource, but the grouping could be looser than that. In general, it’s just a group of endpoints needed to solve some problem. In our case, the goal is to enable people to write to and read from their log.\n\nCreating a gRPC service involves defining it in protobuf and then compiling your protocol buffers into code comprising the client and server stubs that you then implement. To get started, open log.proto, the file where we defined our Record message, and add the following service definition above those messages:\n\n3.\n\nhttps://grpc.io/blog/grpc-load-balancing\n\nreport erratum • discuss\n\nDefine a gRPC Service • 59\n\nServeRequestsWithgRPC/api/v1/log.proto service Log {\n\nrpc Produce(ProduceRequest) returns (ProduceResponse) {} rpc Consume(ConsumeRequest) returns (ConsumeResponse) {} rpc ConsumeStream(ConsumeRequest) returns (stream ConsumeResponse) {} rpc ProduceStream(stream ProduceRequest) returns (stream ProduceResponse) {}\n\n}\n\nThe service keyword says that this is a service for the compiler to generate, and each RPC line is an endpoint in that service, specifying the type of request and response the endpoint accepts. The requests and responses are messages that the compiler turns into Go structs, like the ones we saw in the previous chapter.\n\nWe have two streaming endpoints:\n\nConsumeStream—a server-side streaming RPC where the client sends a request to the server and gets back a stream to read a sequence of messages.\n\nProduceStream—a bidirectional streaming RPC where both the client and server send a sequence of messages using a read-write stream. The two streams operate independently, so the clients and servers can read and write in whatever order they like. For example, the server could wait to receive all of the client’s requests before sending back its response. You’d order your calls this way if your server needed to process the requests in batches or aggregate a response over multiple requests. Alternatively, the server could send back a response for each request in lockstep. You’d order your calls this way if each request required its own corresponding response.\n\nBelow your service definition, add the following code to define our requests and responses:\n\nServeRequestsWithgRPC/api/v1/log.proto message ProduceRequest {\n\nRecord record = 1;\n\n}\n\nmessage ProduceResponse {\n\nuint64 offset = 1;\n\n}\n\nmessage ConsumeRequest {\n\nuint64 offset = 1;\n\n}\n\nmessage ConsumeResponse {\n\nRecord record = 2;\n\n}\n\nreport erratum • discuss\n\nChapter 4. Serve Requests with gRPC • 60\n\nThe request includes the record to produce to the log, and the response sends back the record’s offset, which is essentially the record’s identifier. Similarly with consuming: the user specifies the offset of the logs they want to consume, and the server responds back with the specified record.\n\nTo generate the client- and server-side code with our Log service definition, we need to tell the protobuf compiler to use the gRPC plugin.\n\nCompile with the gRPC Plugin\n\nThis task takes just a second. Install the gRPC package by running this command:\n\n$ go get google.golang.org/grpc@v1.32.0 $ go get google.golang.org/grpc/cmd/protoc-gen-go-grpc@v1.0.0\n\nThen open up your Makefile and update your compile target to match the fol- lowing to enable the gRPC plugin and compile our gRPC service:\n\nServeRequestsWithgRPC/Makefile compile:\n\nprotoc api/v1/*.proto \\\n\n--go_out=. \\ --go-grpc_out=. \\ --go_opt=paths=source_relative \\ --go-grpc_opt=paths=source_relative \\ --proto_path=.\n\nRun $ make compile, and then open up the log_grpc.pb.go file in the api/v1 directory and check out the generated code. You’ll see a working gRPC log client, and the compiler left the log service API for us to implement.\n\nImplement a gRPC Server\n\nBecause the compiler generated a server stub, the job left for us is to write it. To implement a server, you need to build a struct whose methods match the service definition in your protobuf.\n\nCreate an internal/server directory tree in the root of your project by running mkdir -p internal/server. Internal packages are magical packages in Go that can only be imported by nearby code. For example, you can import code in /a/b/c/internal/d/e/f by code rooted by /a/b/c, but not code rooted by /a/b/g. In this directory, we’ll implement our server in a file called server.go and a package named server. The first order of business is to define our server type and a factory function to create an instance of the server.\n\nHere’s the code we need to add to our server.go file:\n\nreport erratum • discuss\n\nImplement a gRPC Server • 61\n\nServeRequestsWithgRPC/internal/server/server.go package server\n\nimport (\n\n\"context\"\n\napi \"github.com/travisjeffery/proglog/api/v1\" \"google.golang.org/grpc\"\n\n)\n\ntype Config struct {\n\nCommitLog CommitLog\n\n}\n\nvar _ api.LogServer = (*grpcServer)(nil)\n\ntype grpcServer struct {\n\napi.UnimplementedLogServer *Config\n\n}\n\nfunc newgrpcServer(config *Config) (srv *grpcServer, err error) {\n\nsrv = &grpcServer{\n\nConfig: config,\n\n} return srv, nil\n\n}\n\nTo implement the API you saw in log_grpc.pb.go, we need to implement the Con- sume() and Produce() handlers. Our gRPC layer is thin because it defers to our log library, so to implement these methods, you call down to the library and handle any errors. Add the following code below your newgrpcServer function:\n\nServeRequestsWithgRPC/internal/server/server.go func (s *grpcServer) Produce(ctx context.Context, req *api.ProduceRequest) (\n\napi.ProduceResponse, error) { offset, err := s.CommitLog.Append(req.Record) if err != nil { return nil, err\n\n} return &api.ProduceResponse{Offset: offset}, nil\n\n}\n\nfunc (s *grpcServer) Consume(ctx context.Context, req *api.ConsumeRequest) (\n\napi.ConsumeResponse, error) { record, err := s.CommitLog.Read(req.Offset) if err != nil { return nil, err\n\n} return &api.ConsumeResponse{Record: record}, nil\n\n}\n\nreport erratum • discuss\n\nChapter 4. Serve Requests with gRPC • 62\n\nWith this snippet, we’ve implemented the Produce(context.Context,*api.ProduceRequest) and Consume(context.Context, *api.ConsumeRequest) methods on our server. These methods handle the requests made by clients to produce and consume to the server’s log. Now let’s add the streaming APIs. Put the following code below the previous snippet:\n\nServeRequestsWithgRPC/internal/server/server.go func (s *grpcServer) ProduceStream(\n\nstream api.Log_ProduceStreamServer,\n\n) error {\n\nfor {\n\nreq, err := stream.Recv() if err != nil {\n\nreturn err\n\n} res, err := s.Produce(stream.Context(), req) if err != nil {\n\nreturn err\n\n} if err = stream.Send(res); err != nil {\n\nreturn err\n\n}\n\n}\n\n}\n\nfunc (s *grpcServer) ConsumeStream(\n\nreq *api.ConsumeRequest, stream api.Log_ConsumeStreamServer,\n\n) error {\n\nfor {\n\nselect { case <-stream.Context().Done():\n\nreturn nil\n\ndefault:\n\nres, err := s.Consume(stream.Context(), req) switch err.(type) { case nil: case api.ErrOffsetOutOfRange:\n\ncontinue\n\ndefault:\n\nreturn err\n\n} if err = stream.Send(res); err != nil {\n\nreturn err\n\n} req.Offset++\n\n}\n\n}\n\n}\n\nreport erratum • discuss\n\nImplement a gRPC Server • 63\n\nProduceStream(api.Log_ProduceStreamServer) implements a bidirectional streaming RPC so the client can stream data into the server’s log and the server can tell the client whether each request succeeded. ConsumeStream(*api.ConsumeRequest, api.Log_ConsumeStreamServer) implements a server-side streaming RPC so the client can tell the server where in the log to read records, and then the server will stream every record that follows—even records that aren’t in the log yet! When the server reaches the end of the log, the server will wait until someone appends a record to the log and then continue streaming records to the client.\n\nThe code that makes up our gRPC service is short and simple, which is a sign that we have a clean separation between our networking code and log code. However, one reason our service’s code is so short is because we have the most basic error handling ever: we just send the client whatever error our library returned.\n\nIf a client tried to consume a message but the request failed, the developer would want to know why. Could the server not find the message? Did the server fail unexpectedly? The server communicates this info with a status code. Also, end users need to know when the application fails, so the server should send back a human-readable version of the error for the client to show to the user.\n\nLet’s explore how to improve our service’s error handling, shall we?\n\nError Handling in gRPC\n\nYet another nice feature of gRPC is how it handles errors. In the previous code, we return errors just like you’d see in code from the Go standard library. Even though this code is handling calls between people on different computers, you wouldn’t know it—thanks to gRPC, which abstracts away the networking details. By default your errors will only have a string description, but you may want to include more information such as a status code or some other arbitrary data.\n\nGo’s gRPC implementation has an awesome status package4 that you can use to build errors with status codes or whatever other data you want to include in your errors. To create an error with a status code, you create the error with the Error function from the status package and pass the relevant code from the codes package5 that matches the type of error you have. Any status code you attach on the error here must be a code defined in the codes package—they’re meant to be consistent across the languages gRPC supports.\n\n4. 5.\n\nhttps://godoc.org/google.golang.org/grpc/status\n\nhttps://godoc.org/google.golang.org/grpc/codes\n\nreport erratum • discuss\n\nChapter 4. Serve Requests with gRPC • 64\n\nFor example, if you couldn’t find a record for some ID, then you’d use the NotFound code like this:\n\nerr := status.Error(codes.NotFound, \"id was not found\") return nil, err\n\nOn the client side, you’d parse out the code from the error with the FromError function from the status package. Your goal is to have as few non-status errors as possible so you know why the errors happen and can handle them gracefully. The non-status errors that are OK are unforeseen, internal server errors. Here’s how to use the FromError function to parse out a status from a gRPC error:\n\nst, ok := status.FromError(err) if !ok {\n\n// Error was not a status error\n\n} // Use st.Message() and st.Code()\n\nWhen you want more than a status code (say you’re trying to debug an error and want more details like logs or traces), then you can use the status pack- age’s WithDetails function, which allows you to attach any protobuf message you want to the error.\n\nThe errdetailspackage6 provides some protobufs you’ll likely find useful when building your service, including messages to use to handle bad requests, debug info, and localized messages.\n\nLet’s use the LocalizedMessage from the errdetails package to change the previous example to respond with an error message that’s safe to return to the user. In the following code, we first create a new not-found status, then we create the localized message specifying the message and locale used. Next we attach the details to the status, and then finally convert and return the status as a Go error:\n\nst := status.New(codes.NotFound, \"id was not found\") d := &errdetails.LocalizedMessage{\n\nLocale: \"en-US\", Message: fmt.Sprintf(\n\n\"We couldn't find a user with the email address: %s\", id,\n\n),\n\n} var err error st, err = st.WithDetails(d)\n\n6.\n\nhttps://godoc.org/google.golang.org/genproto/googleapis/rpc/errdetails\n\nreport erratum • discuss\n\nImplement a gRPC Server • 65\n\nif err != nil {\n\n// If this errored, it will always error // here, so better panic so we can figure // out why than have this silently passing. panic(fmt.Sprintf(\"Unexpected error attaching metadata: %v\", err))\n\n} return st.Err()\n\nTo extract these details on the client side, you need to convert the error back into a status, pull out the details via its Details method, and then convert the type of the details to match the type of the protobuf you set on the server, which in our case is *errdetails.LocalizedMessage.\n\nThe code to do that looks like this:\n\nst := status.Convert(err) for _, detail := range st.Details() {\n\nswitch t := detail.(type) { case *errdetails.LocalizedMessage:\n\n// send t.Message back to the user\n\n}\n\n}\n\nFocusing back on our service, let’s add a custom error named ErrOffsetOutOfRange that the server will send back to the client when the client tries to consume an offset that’s outside of the log. Create an error.go file inside the api/v1 direc- tory with the following code:\n\nServeRequestsWithgRPC/api/v1/error.go package log_v1\n\nimport (\n\n\"fmt\"\n\n\"google.golang.org/genproto/googleapis/rpc/errdetails\" \"google.golang.org/grpc/status\"\n\n)\n\ntype ErrOffsetOutOfRange struct {\n\nOffset uint64\n\n}\n\nfunc (e ErrOffsetOutOfRange) GRPCStatus() *status.Status {\n\nst := status.New(\n\n404, fmt.Sprintf(\"offset out of range: %d\", e.Offset),\n\n) msg := fmt.Sprintf(\n\n\"The requested offset is outside the log's range: %d\", e.Offset,\n\n)\n\nreport erratum • discuss\n\nChapter 4. Serve Requests with gRPC • 66\n\nd := &errdetails.LocalizedMessage{\n\nLocale: \"en-US\", Message: msg,\n\n} std, err := st.WithDetails(d) if err != nil {\n\nreturn st\n\n} return std\n\n}\n\nfunc (e ErrOffsetOutOfRange) Error() string {\n\nreturn e.GRPCStatus().Err().Error()\n\n}\n\nNext, let’s update your log to use this error. Find this section of the Read(offset uint64) method of your log in internal/log/log.go:\n\nWriteALogPackage/internal/log/log.go if s == nil || s.nextOffset <= off {\n\nreturn nil, fmt.Errorf(\"offset out of range: %d\", off)\n\n}\n\nAnd then change that section to this:\n\nServeRequestsWithgRPC/internal/log/log.go if s == nil || s.nextOffset <= off {\n\nreturn nil, api.ErrOffsetOutOfRange{Offset: off}\n\n}\n\nFinally, we need to update the associated testOutOfRange(*testing.T, *log.Log) test in internal/log/log_test.go to the following code:\n\nServeRequestsWithgRPC/internal/log/log_test.go func testOutOfRangeErr(t *testing.T, log *Log) {\n\nread, err := log.Read(1) require.Nil(t, read) apiErr := err.(api.ErrOffsetOutOfRange) require.Equal(t, uint64(1), apiErr.Offset)\n\n}\n\nWith our custom error, when the client tries to consume an offset that’s out- side of the log, the log returns an error with plenty of useful information: a localized message, a status code, and an error message. Because our error is a struct type, we can type-switch the error returned by the Read(offset uint64) method to know what happened. We already use this feature in our Con- sumeStream(*api.ConsumeRequest,api.Log_ConsumeStreamServer) method to know whether the server has read to the end of the log and just needs to wait until someone produces another record to the client:\n\nreport erratum • discuss\n\nImplement a gRPC Server • 67\n\nServeRequestsWithgRPC/internal/server/server.go func (s *grpcServer) ConsumeStream(\n\nreq *api.ConsumeRequest, stream api.Log_ConsumeStreamServer,\n\n) error {\n\nfor {\n\nselect { case <-stream.Context().Done():\n\nreturn nil\n\ndefault:\n\nres, err := s.Consume(stream.Context(), req) switch err.(type) { case nil: case api.ErrOffsetOutOfRange:\n\ncontinue\n\ndefault:\n\nreturn err\n\n} if err = stream.Send(res); err != nil {\n\nreturn err\n\n} req.Offset++\n\n}\n\n}\n\n}\n\nWe’ve improved our service’s error handling to include status codes and a human-readable, localized error message to help our users know why a failure occurred. Next, let’s define the log field that’s on our service such that we can pass in different log implementations and make the service easier to write tests against.\n\nDependency Inversion with Interfaces\n\nOur server depends on a log abstraction. For example, when running in a production environment—where we need our service to persist our user’s data—the service will depend on our library. But when running in a test environment, where we don’t need to persist our test data, we could use a naive, in-memory log. An in-memory log would also be good for testing because it would make the tests run faster.\n\nAs you can see from these examples, it would be best if our service weren’t tied to a specific log implementation. Instead, we want to pass in a log implementation based on our needs at the time. We can make this possible by having our service depend on a log interface rather than on a concrete type. That way, the service can use any log implementation that satisfies the log interface.\n\nreport erratum • discuss\n\nChapter 4. Serve Requests with gRPC • 68\n\nAdd this code below your grpcServer methods in server.go:\n\nServeRequestsWithgRPC/internal/server/server.go type CommitLog interface {\n\nAppend(*api.Record) (uint64, error) Read(uint64) (*api.Record, error)\n\n}\n\nThat’s all we need to do to allow our service to use any given log implementa- tion that satisfies our CommitLog interface. Easy, huh?\n\nNow, let’s write an exported API that enables our users to instantiate a new service.\n\nRegister Your Server\n\nWe implemented the server writing nothing gRPC-specific yet. There are just three steps left to get our service working with gRPC, and happily we only need to perform two of them: creating a gRPC server and registering our service with it. The final step is giving the server a listener to accept incoming con- nections from, but we’ll require our users to pass their own listener implemen- tation, as they might like to when testing. Once these three steps are complete, the gRPC server will listen on the network, handle requests, call our server, and respond to the client with the result.\n\nAbove your grpcServer struct in server.go, add the following NewGRPCServer() function to provide your users a way to instantiate your service, create a gRPC server, and register your service to that server (this will give the user a server that just needs a listener for it to accept incoming connections):\n\nServeRequestsWithgRPC/internal/server/server.go func NewGRPCServer(config *Config) (*grpc.Server, error) {\n\ngsrv := grpc.NewServer() srv, err := newgrpcServer(config) if err != nil {\n\nreturn nil, err\n\n} api.RegisterLogServer(gsrv, srv) return gsrv, nil\n\n}\n\nWe’re now done writing our service. Let’s create some tests to verify that it works.\n\nTest a gRPC Server and Client\n\nNow that we’ve finished our gRPC server, we need some tests to check that our client and server work like we expect. We’ve already tested the details of our log’s library implementation in the library, so the tests we’re writing here are at a higher level and focus on ensuring that everything’s hooked up\n\nreport erratum • discuss\n\nTest a gRPC Server and Client • 69\n\nproperly between the gRPC and library bits and that our gRPC client and server can communicate.\n\nIn the grpc directory, create a server_test.go file, and add the following code that will set up your test:\n\nServeRequestsWithgRPC/internal/server/server_test.go package server\n\nimport (\n\n\"context\" \"io/ioutil\" \"net\" \"testing\"\n\n\"github.com/stretchr/testify/require\" api \"github.com/travisjeffery/proglog/api/v1\" \"github.com/travisjeffery/proglog/internal/log\" \"google.golang.org/grpc\"\n\n)\n\nfunc TestServer(t *testing.T) {\n\nfor scenario, fn := range map[string]func(\n\nt *testing.T, client api.LogClient, config *Config,\n\n){\n\n\"produce/consume a message to/from the log succeeeds\":\n\ntestProduceConsume, \"produce/consume stream succeeds\":\n\ntestProduceConsumeStream,\n\n\"consume past log boundary fails\":\n\ntestConsumePastBoundary,\n\n} {\n\nt.Run(scenario, func(t *testing.T) {\n\nclient, config, teardown := setupTest(t, nil) defer teardown() fn(t, client, config)\n\n})\n\n}\n\n}\n\nTestServer(*testing.T) defines our list of test cases and then runs a subtest for each case. Add the following setupTest(*testing.T, func(*Config)) function below Test- Server():\n\nServeRequestsWithgRPC/internal/server/server_test.go func setupTest(t *testing.T, fn func(*Config)) (\n\nclient api.LogClient, cfg *Config, teardown func(),\n\n) {\n\nreport erratum • discuss\n\nChapter 4. Serve Requests with gRPC • 70\n\nt.Helper()\n\nl, err := net.Listen(\"tcp\", \":0\") require.NoError(t, err)\n\nclientOptions := []grpc.DialOption{grpc.WithInsecure()} cc, err := grpc.Dial(l.Addr().String(), clientOptions...) require.NoError(t, err)\n\ndir, err := ioutil.TempDir(\"\", \"server-test\") require.NoError(t, err)\n\nclog, err := log.NewLog(dir, log.Config{}) require.NoError(t, err)\n\ncfg = &Config{\n\nCommitLog: clog,\n\n} if fn != nil {\n\nfn(cfg)\n\n} server, err := NewGRPCServer(cfg) require.NoError(t, err)\n\ngo func() {\n\nserver.Serve(l)\n\n}()\n\nclient = api.NewLogClient(cc)\n\nreturn client, cfg, func() { server.Stop() cc.Close() l.Close() clog.Remove()\n\n}\n\n}\n\nsetupTest(*testing.T, func(*Config)) is a helper function to set up each test case. Our test setup begins by creating a listener on the local network address that our server will run on. The 0 port is useful for when we don’t care what port we use since 0 will automatically assign us a free port. We then make an insecure connection to our listener and, with it, a client we’ll use to hit our server with. Next we create our server and start serving requests in a goroutine because the Serve method is a blocking call, and if we didn’t run it in a goroutine our tests further down would never run.\n\nNow we’re ready to write some test cases. Add the following code below setupTest():\n\nServeRequestsWithgRPC/internal/server/server_test.go func testProduceConsume(t *testing.T, client api.LogClient, config *Config) {\n\nctx := context.Background()\n\nreport erratum • discuss\n\nTest a gRPC Server and Client • 71\n\nwant := &api.Record{\n\nValue: []byte(\"hello world\"),\n\n}\n\nproduce, err := client.Produce(\n\nctx, &api.ProduceRequest{\n\nRecord: want,\n\n},\n\n) require.NoError(t, err)\n\nconsume, err := client.Consume(ctx, &api.ConsumeRequest{\n\nOffset: produce.Offset,\n\n}) require.NoError(t, err) require.Equal(t, want.Value, consume.Record.Value) require.Equal(t, want.Offset, consume.Record.Offset)\n\n}\n\ntestProduceConsume(*testing.T,api.LogClient,*Config) tests that producing and consuming works by using our client and server to produce a record to the log, consume it back, and then check that the record we sent is the same one we got back.\n\nAdd the following test case below testProduceConsume():\n\nServeRequestsWithgRPC/internal/server/server_test.go func testConsumePastBoundary( t *testing.T, client api.LogClient, config *Config,\n\n) {\n\nctx := context.Background()\n\nproduce, err := client.Produce(ctx, &api.ProduceRequest{\n\nRecord: &api.Record{\n\nValue: []byte(\"hello world\"),\n\n},\n\n}) require.NoError(t, err)\n\nconsume, err := client.Consume(ctx, &api.ConsumeRequest{\n\nOffset: produce.Offset + 1,\n\n}) if consume != nil {\n\nt.Fatal(\"consume not nil\")\n\n} got := grpc.Code(err) want := grpc.Code(api.ErrOffsetOutOfRange{}.GRPCStatus().Err()) if got != want {\n\nt.Fatalf(\"got err: %v, want: %v\", got, want)\n\n}\n\n}\n\nreport erratum • discuss\n\nChapter 4. Serve Requests with gRPC • 72\n\ntestConsumePastBoundary(*testing.T,api.LogClient, *Config) tests that our server responds with an api.ErrOffsetOutOfRange() error when a client tries to consume beyond the log’s boundaries.\n\nWe have one more test case. Put the following snippet at the bottom of the file:\n\nServeRequestsWithgRPC/internal/server/server_test.go func testProduceConsumeStream( t *testing.T, client api.LogClient, config *Config,\n\n) {\n\nctx := context.Background()\n\nrecords := []*api.Record{{\n\nValue: []byte(\"first message\"), Offset: 0,\n\n}, {\n\nValue: []byte(\"second message\"), Offset: 1,\n\n}}\n\n{\n\nstream, err := client.ProduceStream(ctx) require.NoError(t, err)\n\nfor offset, record := range records {\n\nerr = stream.Send(&api.ProduceRequest{\n\nRecord: record,\n\n}) require.NoError(t, err) res, err := stream.Recv() require.NoError(t, err) if res.Offset != uint64(offset) {\n\nt.Fatalf(\n\n\"got offset: %d, want: %d\", res.Offset, offset,\n\n)\n\n}\n\n}\n\n}\n\n{\n\nstream, err := client.ConsumeStream(\n\nctx, &api.ConsumeRequest{Offset: 0},\n\n) require.NoError(t, err)\n\nfor i, record := range records {\n\nres, err := stream.Recv()\n\nreport erratum • discuss\n\nWhat You Learned • 73\n\nrequire.NoError(t, err) require.Equal(t, res.Record, &api.Record{\n\nValue: record.Value, Offset: uint64(i),\n\n})\n\n}\n\n}\n\n}\n\ntestProduceConsumeStream(*testing.T, api.LogClient, *Config) is the streaming counterpart to testProduceConsume(), testing that we can produce and consume through streams.\n\nRun $ make test to test your code. In the test output, you’ll see your TestServer test passing.\n\nWahoo! You’ve written and tested your first gRPC service.\n\nWhat You Learned\n\nYou now know how to define a gRPC service in protobuf, compile your gRPC protobufs into code, build a gRPC server, and test that everything works end- to-end across your client and server. You can build a gRPC server and client, and you can use your log over the network.\n\nNext we’ll improve the security of our service by encrypting the data sent between the client and server with SSL/TLS, and authenticating requests so we can know who’s making each request and whether they’re allowed to.\n\nreport erratum • discuss\n\nCHAPTER 5\n\nSecure Your Services\n\nWhen you build a project, your goal is to solve a problem. You may get so focused on this goal that you ignore the other factors you should consider, like security. Security is one of those things that’s super important but easy to ignore.\n\nYes, creating a secure solution is more complicated than building a solution without considering security. But if you want to build something that people will actually use, it has to be secure. And it’s far easier to incorporate security from the start than it is to retrofit security into a finished project. So you need to keep security in mind from the very beginning. In this book, for example, we don’t just want to build a tool to stream data—we want to build a tool that streams data securely.\n\nWhen you start your career as a software engineer, security can seem like a thankless job. If you do it right, no one will know you did it at all, and building it can be scary and even boring at times. Over the years, from building several software-as-a-service startups, I’ve changed my tune—I now consider securing my services as important as the problems they solve. Here’s why:\n\nSecurity saves you from being hacked. When you don’t follow security best practices, breaches and leaks follow with amazing regularity and severity, as we’ve seen in the news. Whenever I’m building a service, I think about what it’d be like if the data I’m trying to protect was publicly posted all over the planet. Picturing this gives me the motivation to make sure that sort of thing doesn’t happen to me, and thankfully it hasn’t yet (knock on wood).\n\nSecurity wins deals. In my experience, the most important factor in whether a potential customer bought software I worked on came down to whether the software fulfilled some security requirement.\n\nreport erratum • discuss\n\nChapter 5. Secure Your Services • 76\n\nSecurity is painful to tack on. Taking an insecure service that lacks the basic security features most people need and then trying to tack those features on is a painful, tricky process. In contrast, it’s relatively easy to build those features from the start.\n\nThose high stakes get me fired up about building secure services. So let’s get to it.\n\nSecure Services in Three Steps\n\nSecurity in distributed services can be broken down into three steps:\n\n1. Encrypt data in-flight to protect against man-in-the-middle attacks; 2. Authenticate to identify clients; and 3. Authorize to determine the permissions of the identified clients. Let’s talk about these phases in more detail, explore the security benefits they provide, and write the code to build them into our service.\n\nEncrypt In-Flight Data\n\nEncryption of data in-flight prevents man-in-the-middle attacks (MITM).1 An example of a MITM attack is active eavesdropping, where the attacker makes independent connections with the victims to make them think they’re talking directly with each other when in fact the conversation is controlled by the attacker. This is bad because not only can the attacker learn confidential information, but also the attacker can maliciously change the messages sent between the victims. For example, say Bob was trying to send money to Alice using PayPal, but Mallory changed the account the money was sent to from Alice’s to her own.\n\nCryptography’s Conventional Names\n\nBob, Alice, and Mallory are placeholder names commonly used when discussing cryptography (en.wikipedia.org/wiki/ Alice_and_Bob#Cast_of_characters). Typically Alice and Bob want to exchange a message, and Mallory is a malicious attacker. There’s a whole cast of characters, and they’re named with rhyming mnemonics to their role (for example: Mallory the malicious attacker, Eve the eavesdropper, Craig the password cracker).\n\nThe most widely used technology for preventing MITM attacks and encrypting data in-flight is TLS, the successor to SSL. TLS used to be considered necessary\n\n1.\n\nhttps://en.wikipedia.org/wiki/Man-in-the-middle_attack\n\nreport erratum • discuss\n\nSecure Services in Three Steps • 77\n\nonly for \"serious\" websites like online banks, but these days all sites should use TLS.2 Modern browsers highlight websites that don’t use TLS as unsafe and recommend their users to not even use them.\n\nThe process by which a client and server communicate is kicked off by a TLS handshake. During this handshake, the client and server:\n\n1. Specify which version of TLS they’ll use;\n\n2. Decide which cipher suites (the set of encryption algorithms) they’ll use;\n\n3. Authenticate the identity of the server via the server’s private key and the\n\ncertificate authority’s digital signature; and\n\n4. Generate session keys for symmetric encryption after the handshake is\n\ncomplete.\n\nOnce this handshake process is complete, the client and server can commu- nicate securely.\n\nFortunately we don’t have to worry about implementing these TLS handshake steps, as TLS handles them for us behind the scenes. Our job is to obtain certificates for our client and server to use and to tell gRPC to communicate over TLS using the given the certs.\n\nWe’ll build TLS support into our service to encrypt data in-flight and authenticate the server.\n\nAuthenticate to Identify Clients\n\nOnce you’ve secured the communication between your client and server with TLS, the next step to a secure service is authentication. Authentication is the process of identifying who the client is (TLS has already handled authenticating the server). For example, whenever you post a tweet, Twitter needs to verify that the person trying to post the tweet to your account is really you.\n\nMost web services use TLS for one-way authentication and only authenticate the server. The authentication of the client is left to the application to work out, usually by some combination of username-password credentials and tokens. TLS mutual authentication, also commonly referred to as two-way authentication, in which both the server and the client validate the other’s communication, is more commonly used in machine-to-machine communica- tion—like distributed systems! In this setup, both the server and the client use a certificate to prove their identity.\n\n2.\n\nhttps://doesmysiteneedhttps.com\n\nreport erratum • discuss",
      "page_number": 65
    },
    {
      "number": 5,
      "title": "Secure Your Services • 76",
      "start_page": 84,
      "end_page": 107,
      "detection_method": "regex_chapter_title",
      "content": "Chapter 5. Secure Your Services • 78\n\nBecause mutual TLS authentication is so effective, relatively simple, and well adopted (both in terms of how many people use it and the number of technolo- gies that support it), many companies use it to secure the communications3 between their internal distributed services. Because so many people use mutual TLS authentication, it’s important for new services (like ours) to support it. So we’ll build mutual TLS authentication into our service.\n\nAuthorize to Determine the Permissions of Clients\n\nAuthentication and authorization are so closely related that people often use the word “auth” to refer to both. Authentication and authorization are almost always done at the same time in terms of the request’s life-cycle and place in the server’s code base. In fact, for many web services where resources have a single owner, authentication and authorization are the same process. For example, a Twitter account has one owner, so if a client authenticates as that owner, then Twitter lets them do whatever they want with the account.\n\nDifferentiating between authentication and authorization is necessary when you have a resource with shared access and varying levels of ownership. With our log service for example, Alice might be the owner and have both read and write access to the contents of the log, whereas Bob might be allowed to read the contents but isn’t able to write. In this type of situation, you need authorization with granular access control.\n\nIn our service, we’ll build access control list-based authorization to control whether a client is allowed to read from or write to (or both) the log.\n\nNow that you have a general understanding of the three key aspects of securing a distributed system, let’s implement them in our service.\n\nAuthenticate the Server with TLS\n\nYou’ve now seen how TLS works and why to use it, so we’re ready to build TLS support into our service to encrypt data in-flight and authenticate the server. I’ll also cover how to make obtaining and working with certificates easier to manage.\n\nOperate as Your Own CA with CFSSL\n\nBefore changing our server’s code, let’s get some certs. We could use a third- party certificate authority (CA) to get the certs, but that could cost money (depending on the CA) and is a hassle. For internal services (like ours), there’s\n\n3.\n\nhttps://blog.cloudflare.com/how-to-build-your-own-public-key-infrastructure\n\nreport erratum • discuss\n\nAuthenticate the Server with TLS • 79\n\nno need to go through a third-party authority. Trusted certificates don’t have to come from Comodo or Let’s Encrypt or any other CA—they can come from a CA you operate yourself. It’s free and easy with the right tools.\n\nCloudFlare4 wrote a toolkit called CFSSL for signing, verifying, and bundling TLS certificates. CloudFlare uses CFSSL for their internal services’ TLS cer- tificates, acting as their own certificate authority. CloudFlare open sourced CFSSL so others, including us, can use it. Even major CA vendors like Let’s Encrypt use CFSSL. Big thanks to CloudFlare because CFSSL is a seriously useful toolkit.\n\nCFSSL has two tools we’ll need:\n\ncfssl to sign, verify, and bundle TLS certificates and output the results as JSON. • cfssljson to take that JSON output and split them into separate key, cer- tificate, CSR, and bundle files.\n\nInstall the CloudFlare CLIs by running the following commands:\n\n$ go get github.com/cloudflare/cfssl/cmd/cfssl@v1.4.1 $ go get github.com/cloudflare/cfssl/cmd/cfssljson@v1.4.1\n\nTo initialize our CA and generate certs, we need to pass various config files to the cfssl commands we’ll run. We need separate config files to generate our CA and server certs and we need a config file containing general config info about our CA. So let’s create a directory in our project to contain these config files by running $ mkdir test.\n\nPut the following JSON into a file called ca-csr.json in your test directory:\n\nSecureYourServices/test/ca-csr.json {\n\n\"CN\": \"My Awesome CA\", \"key\": {\n\n\"algo\": \"rsa\", \"size\": 2048\n\n}, \"names\": [ {\n\n\"C\": \"CA\", \"L\": \"ON\", \"ST\": \"Toronto\", \"O\": \"My Awesome Company\", \"OU\": \"CA Services\"\n\n}\n\n]\n\n}\n\n4.\n\nhttps://www.cloudflare.com\n\nreport erratum • discuss\n\nChapter 5. Secure Your Services • 80\n\ncfssl will use this file to configure our CA’s certificate. CN stands for Common Name, so we’re saying our CA is called “My Awesome CA.” key specifies the algorithm and size of key to sign the certificate with; names is a list of various name information that’ll be added to the certificate. Each name object should contain at least one “C,” “L,” “O,” “OU,” or “ST” value (or any combination of these). They stand for:\n\nC—country • L—locality or municipality (such as city) • ST—state or province • O—organization • OU—organizational unit (such as the department responsible for owning the key)\n\nCreate a test/ca-config.json that looks like this to define the CA’s policy:\n\nSecureYourServices/test/ca-config.json {\n\n\"signing\": {\n\n\"profiles\": {\n\n\"server\": {\n\n\"expiry\": \"8760h\", \"usages\": [\n\n\"signing\", \"key encipherment\", \"server auth\"\n\n]\n\n}, \"client\": {\n\n\"expiry\": \"8760h\", \"usages\": [\n\n\"signing\", \"key encipherment\", \"client auth\"\n\n]\n\n}\n\n}\n\n}\n\n}\n\nOur CA needs to know what kind of certificates it will issue. The signing section of this configuration file defines your CA’s signing policy. Our configuration file says that the CA can generate client and server certificates that will expire after a year and the certificates may be used for digital signatures, encrypting keys, and auth.\n\nPut the following JSON into a file called server-csr.json in your test directory:\n\nreport erratum • discuss\n\nAuthenticate the Server with TLS • 81\n\nSecureYourServices/test/server-csr.json {\n\n\"CN\": \"127.0.0.1\", \"hosts\": [\n\n\"localhost\", \"127.0.0.1\"\n\n], \"key\": {\n\n\"algo\": \"rsa\", \"size\": 2048\n\n}, \"names\": [ {\n\n\"C\": \"CA\", \"L\": \"ON\", \"ST\": \"Toronto\", \"O\": \"My Awesome Company\", \"OU\": \"Distributed Services\"\n\n}\n\n]\n\n}\n\ncfssl will use these configs to configure our server’s certificate. The “hosts” field is a list of the domain names that the certificate should be valid for. Since we’re running our service locally, we just need 127.0.0.1 and localhost.\n\nNow let’s update our Makefile to call cfssl and cfssljson to actually generate the certs. Make your project’s Makefile look like this:\n\nSecureYourServices/Makefile CONFIG_PATH=${HOME}/.proglog/\n\n.PHONY: init init:\n\nmkdir -p ${CONFIG_PATH}\n\n.PHONY: gencert gencert:\n\ncfssl gencert \\\n\ninitca test/ca-csr.json | cfssljson -bare ca\n\ncfssl gencert \\\n\nca=ca.pem \\ -ca-key=ca-key.pem \\ -config=test/ca-config.json \\ -profile=server \\ test/server-csr.json | cfssljson -bare server\n\nmv *.pem *.csr ${CONFIG_PATH}\n\n.PHONY: test test:\n\ngo test -race ./...\n\nreport erratum • discuss\n\nChapter 5. Secure Your Services • 82\n\n.PHONY: compile compile:\n\nprotoc api/v1/*.proto \\\n\n--go_out=. \\ --go-grpc_out=. \\ --go_opt=paths=source_relative \\ --go-grpc_opt=paths=source_relative \\ --proto_path=.\n\nIn this updated Makefile, we’ve added a CONFIG_PATH variable to specify where we’d like to put our generated certs and an init target to create that directory. With these configs in a static and known location on the filesystem, it’s easier to look up and use the certs in our code. The gencert target calls cfssl to gen- erate the certificate and private keys for our CA and server using the config files we added earlier.\n\nWe’ll reference these config files frequently in our tests, so let’s make a package containing their file paths as variables to make referencing them easy. Create an internal/config directory with a files.go file containing this code:\n\nSecureYourServices/internal/config/files.go package config\n\nimport (\n\n\"os\" \"path/filepath\"\n\n)\n\nvar (\n\nCAFile ServerCertFile ServerKeyFile\n\n= configFile(\"ca.pem\") = configFile(\"server.pem\") = configFile(\"server-key.pem\")\n\n)\n\nfunc configFile(filename string) string {\n\nif dir := os.Getenv(\"CONFIG_DIR\"); dir != \"\" {\n\nreturn filepath.Join(dir, filename)\n\n} homeDir, err := os.UserHomeDir() if err != nil {\n\npanic(err)\n\n} return filepath.Join(homeDir, \".proglog\", filename)\n\n}\n\nThese variables define the paths to the certs we generated and need to look up and parse for our tests. I would use constants and the const keyword if Go allowed using const with function calls.\n\nreport erratum • discuss\n\nAuthenticate the Server with TLS • 83\n\nWe’ll use the certificate and key files to build *tls.Configs, so let’s add a helper function and struct for that. In the config directory, create a tls.go file beginning with this code:\n\nSecureYourServices/internal/config/tls.go package config\n\nimport (\n\n\"crypto/tls\" \"crypto/x509\" \"fmt\" \"io/ioutil\"\n\n)\n\nfunc SetupTLSConfig(cfg TLSConfig) (*tls.Config, error) {\n\nvar err error tlsConfig := &tls.Config{} if cfg.CertFile != \"\" && cfg.KeyFile != \"\" {\n\ntlsConfig.Certificates = make([]tls.Certificate, 1) tlsConfig.Certificates[0], err = tls.LoadX509KeyPair(\n\ncfg.CertFile, cfg.KeyFile,\n\n) if err != nil {\n\nreturn nil, err\n\n}\n\n} if cfg.CAFile != \"\" {\n\nb, err := ioutil.ReadFile(cfg.CAFile) if err != nil {\n\nreturn nil, err\n\n} ca := x509.NewCertPool() ok := ca.AppendCertsFromPEM([]byte(b)) if !ok {\n\nreturn nil, fmt.Errorf(\n\n\"failed to parse root certificate: %q\", cfg.CAFile,\n\n)\n\n} if cfg.Server {\n\ntlsConfig.ClientCAs = ca tlsConfig.ClientAuth = tls.RequireAndVerifyClientCert\n\n} else {\n\ntlsConfig.RootCAs = ca\n\n} tlsConfig.ServerName = cfg.ServerAddress\n\n} return tlsConfig, nil\n\n}\n\nreport erratum • discuss\n\nChapter 5. Secure Your Services • 84\n\nOur tests use a few different *tls.Config configurations, and SetupTLSConfig() allows us to get each type of *tls.Config with one function call. These are the different configurations:\n\nClient *tls.Config is set up to verify the server’s certificate with the client’s\n\nby setting the *tls.Config’s RootCAs.\n\nClient *tls.Config is set up to verify the server’s certificate and allow the server to verify the client’s certificate by setting its RootCAs and its Certificates.\n\nServer *tls.Config is set up to verify the client’s certificate and allow the client to verify the server’s certificate by setting its ClientCAs, Certificate, and ClientAuth mode set to tls.RequireAndVerifyCert.\n\nBelow SetupTLSConfig(), put this struct:\n\nSecureYourServices/internal/config/tls.go type TLSConfig struct { string CertFile string KeyFile string CAFile ServerAddress string Server\n\nbool\n\n}\n\nTLSConfig defines the parameters that SetupTLSConfig() uses to determine what type of *tls.Config to return.\n\nBack to our tests. Let’s test that the client uses our CA to verify the server’s certificate. If the server’s certificate came from a different authority, the client wouldn’t trust the server and wouldn’t make a connection. In setup_test.go, add these imports:\n\nSecureYourServices/internal/server/server_test.go \"github.com/travisjeffery/proglog/internal/config\" \"google.golang.org/grpc/credentials\"\n\nNow replace the code in your existing setupTest() function with the following code:\n\nSecureYourServices/internal/server/server_test.go t.Helper()\n\nl, err := net.Listen(\"tcp\", \"127.0.0.1:0\") require.NoError(t, err)\n\nclientTLSConfig, err := config.SetupTLSConfig(config.TLSConfig{\n\nCAFile: config.CAFile,\n\n}) require.NoError(t, err)\n\nclientCreds := credentials.NewTLS(clientTLSConfig) cc, err := grpc.Dial(\n\nreport erratum • discuss\n\nAuthenticate the Server with TLS • 85\n\nl.Addr().String(), grpc.WithTransportCredentials(clientCreds),\n\n) require.NoError(t, err)\n\nclient = api.NewLogClient(cc)\n\nIn this code, we configure our client’s TLS credentials to use our CA as the client’s Root CA (the CA it will use to verify the server). Then we tell the client to use those credentials for its connection.\n\nNext we need to hook up our server with its certificate and enable it to handle TLS connections. Add the following code below the previous snippet:\n\nSecureYourServices/internal/server/server_test.go serverTLSConfig, err := config.SetupTLSConfig(config.TLSConfig{\n\nCertFile: KeyFile: CAFile: ServerAddress: l.Addr().String(),\n\nconfig.ServerCertFile, config.ServerKeyFile, config.CAFile,\n\n}) require.NoError(t, err) serverCreds := credentials.NewTLS(serverTLSConfig)\n\ndir, err := ioutil.TempDir(\"\", \"server-test\") require.NoError(t, err)\n\nclog, err := log.NewLog(dir, log.Config{}) require.NoError(t, err)\n\ncfg = &Config{\n\nCommitLog: clog,\n\n} if fn != nil {\n\nfn(cfg)\n\n} server, err := NewGRPCServer(cfg, grpc.Creds(serverCreds)) require.NoError(t, err)\n\ngo func() {\n\nserver.Serve(l)\n\n}()\n\nreturn client, cfg, func() { server.Stop() cc.Close() l.Close()\n\n}\n\nIn this code, we’re parsing the server’s cert and key, which we then use to configure the server’s TLS credentials. We then pass those credentials as a gRPC server option to our NewGRPCServer() function so it can create our gRPC server with that option. gRPC server options are how you enable features in\n\nreport erratum • discuss\n\nChapter 5. Secure Your Services • 86\n\ngRPC servers. We’re setting the credentials for the server connections in this case, but there are plenty of other server options5 to configure connection timeouts, keep alive policies, and so on.\n\nFinally, we need to update the NewGRPCServer() function in server.go to take in the given gRPC server options and create the server with them. Change the NewGRPCServer() function to this:\n\nSecureYourServices/internal/server/server.go func NewGRPCServer(config *Config, opts ...grpc.ServerOption) (\n\ngrpc.Server, error,\n\n) {\n\ngsrv := grpc.NewServer(opts...) srv, err := newgrpcServer(config) if err != nil {\n\nreturn nil, err\n\n} api.RegisterLogServer(gsrv, srv) return gsrv, nil\n\n}\n\nAt this point you can run the tests with $ make test, and our tests should pass as they did before the changes we’ve made in this chapter. The difference is that your server is now authenticated and your connection is encrypted. You can verify this by temporarily changing your test code back to using an insecure client connection with the grpc.WithInsecure() dial option, and then running the tests again. This time the tests will fail because the client and server won’t be able to connect with each other because the server is expecting the client to run over TLS.\n\nYour server is authenticated so you know your client is communicating with your actual server and not some middleman’s. Now we’ll use mutual TLS authentication to verify that the client hitting your server really is your client.\n\nAuthenticate the Client with Mutual TLS Authentication\n\nIn the previous section, we used TLS to encrypt our connections and authenticate the server. Now we’ll go one step further and implement mutual TLS authentication (also known as two-way authentication) so the server will use our CA to verify that the client is authentic.\n\nThe first thing we need is a cert for our client, which we can generate with cfssl and cfssljson just like our CA and server’s certificates. Put the following JSON in a file called client-csr.json in your test directory:\n\n5.\n\nhttps://godoc.org/google.golang.org/grpc#ServerOption\n\nreport erratum • discuss\n\n➤ ➤\n\nAuthenticate the Client with Mutual TLS Authentication • 87\n\nSecureYourServices/test/client-csr.json {\n\n\"CN\": \"client\", \"hosts\": [\"\"], \"key\": {\n\n\"algo\": \"rsa\", \"size\": 2048\n\n}, \"names\": [ {\n\n\"C\": \"CA\", \"L\": \"ON\", \"ST\": \"Toronto\", \"O\": \"My Company\", \"OU\": \"Distributed Services\"\n\n}\n\n]\n\n}\n\nThe CN field is the important config because that’s the client’s identity—their username, in a sense. This is the identity we’ll store their permissions under for authorization. (We’ll do this in the next section.)\n\nNext, update the gencert target in your Makefile, to include the following snippet. Place it right below where you generate the server cert:\n\nSecureYourServices/Makefile cfssl gencert \\\n\nca=ca.pem \\ -ca-key=ca-key.pem \\ -config=test/ca-config.json \\ -profile=client \\ test/client-csr.json | cfssljson -bare client\n\nOnce that is done, generate the cert for your client by running $ make gencert.\n\nAdd configuration file variables for your client certificates in internal/config/files.go:\n\nSecureYourServices/internal/config/files.go var (\n\nCAFile ServerCertFile ServerKeyFile ClientCertFile ClientKeyFile\n\n= configFile(\"ca.pem\") = configFile(\"server.pem\") = configFile(\"server-key.pem\") = configFile(\"client.pem\") = configFile(\"client-key.pem\")\n\n)\n\nNext we need to update the server to verify that the certificate the client has sent the server is signed by our CA. Update your server setup in server_test.go like this:\n\nreport erratum • discuss\n\n➤ ➤\n\n➤\n\nChapter 5. Secure Your Services • 88\n\nSecureYourServices/internal/server/server_test.go clientTLSConfig, err := config.SetupTLSConfig(config.TLSConfig{\n\nCertFile: config.ClientCertFile, KeyFile: config.ClientKeyFile, CAFile: config.CAFile,\n\n}) require.NoError(t, err)\n\nclientCreds := credentials.NewTLS(clientTLSConfig) cc, err := grpc.Dial(\n\nl.Addr().String(), grpc.WithTransportCredentials(clientCreds),\n\n) require.NoError(t, err)\n\nclient = api.NewLogClient(cc)\n\nserverTLSConfig, err := config.SetupTLSConfig(config.TLSConfig{\n\nCertFile: KeyFile: CAFile: ServerAddress: l.Addr().String(), Server: true,\n\nconfig.ServerCertFile, config.ServerKeyFile, config.CAFile,\n\n})\n\nNow run your tests again. They’ll still pass because you’re using a valid cert and your tests expect the client to be authentic. For a fun exercise, try generating a cert from a different CA for your client to use and then watch your tests fail. (Okay, maybe I’m the only one who considers this kind of thing fun.)\n\nYour server and client now have mutual TLS authentication with both sides verifying that your CA vouches for their authenticity, so you know it’s your actual client communicating with your server without a middleman eavesdrop- ping. Hooray for security!\n\nAuthorize with Access Control Lists\n\nAuthentication is usually half of what you need from your auth process. You authenticate to know who’s behind the client so you can then complete the auth process by authorizing whoever is behind the client for whatever action they’ve attempted. As I mentioned earlier, authorization is the process of verifying what someone has access to.\n\nThe simplest way to implement authorization is with an access control list (ACL).6 An ACL is a table of rules where each row says something like “Subject A is permitted to do action B on object C.” For example: Alice is permitted to\n\n6.\n\nhttps://en.wikipedia.org/wiki/Access_control_list\n\nreport erratum • discuss\n\nAuthorize with Access Control Lists • 89\n\nread Distributed Services with Go. In this example, Alice is the subject, to read is the action, and Distributed Services with Go is the object.\n\nOne of the beautiful things about an ACL is that it’s easy to build. Since it’s just a table, something as simple as a map or a CSV file could back the data—in more complex implementations, a key-value store or relational database would store the data. So building an ACL library from scratch isn’t difficult, but there’s a nice library called Casbin7 that supports enforcing authorization based on various control models8—including ACLs. Plus Casbin is well adopted, tested, and extendable. Casbin is a useful tool to have in your toolkit, so let’s learn how to use it and take advantage of it!\n\nFirst, add Casbin as a dependency by running the following command at the root of your project:\n\n$ go get github.com/casbin/casbin@v1.9.1\n\nWe’ll wrap Casbin with our own internal library. If we later use another authoriza- tion tool, we won’t have to change a bunch of code throughout our project, just the code in our library. Create an auth directory inside your internal directory by running:\n\n$ mkdir internal/auth\n\nThen create a file called authorizer.go in that directory with your favorite text editor and add the following code:\n\nSecureYourServices/internal/auth/authorizer.go package auth\n\nimport (\n\n\"fmt\"\n\n\"github.com/casbin/casbin\" \"google.golang.org/grpc/codes\" \"google.golang.org/grpc/status\"\n\n)\n\nfunc New(model, policy string) *Authorizer {\n\nenforcer := casbin.NewEnforcer(model, policy) return &Authorizer{\n\nenforcer: enforcer,\n\n}\n\n}\n\ntype Authorizer struct {\n\nenforcer *casbin.Enforcer\n\n}\n\n7. 8.\n\nhttps://github.com/casbin/casbin\n\nhttps://github.com/casbin/casbin#supported-models\n\nreport erratum • discuss\n\nChapter 5. Secure Your Services • 90\n\nfunc (a *Authorizer) Authorize(subject, object, action string) error {\n\nif !a.enforcer.Enforce(subject, object, action) {\n\nmsg := fmt.Sprintf(\n\n\"%s not permitted to %s to %s\", subject, action, object,\n\n) st := status.New(codes.PermissionDenied, msg) return st.Err()\n\n} return nil\n\n}\n\nIn this code, we define an Authorizer type whose sole method, Authorize, defers to Casbin’s Enforce function. This function returns whether the given subject is permitted to run the given action on the given object based on the model and policy you configure Casbin with. The New function’s model and policy arguments are paths to the files where you’ve defined the model (which will configure Casbin’s authorization mechanism—which for us will be ACL) and the policy (which is a CSV file containing your ACL table).\n\nBecause we’re testing authorization, we need multiple clients with different permissions and hence multiple client certs. Having multiple clients with different permissions lets us check whether the server permits or denies a client’s request based on the rules defined in the ACL. So let’s change the cert generation code in your Makefile to generate multiple client certs. To do that, in the gencert target of your Makefile, replace the client cert section to look like this:\n\nSecureYourServices/Makefile cfssl gencert \\\n\nca=ca.pem \\ -ca-key=ca-key.pem \\ -config=test/ca-config.json \\ -profile=client \\ -cn=\"root\" \\ test/client-csr.json | cfssljson -bare root-client\n\ncfssl gencert \\\n\nca=ca.pem \\ -ca-key=ca-key.pem \\ -config=test/ca-config.json \\ -profile=client \\ -cn=\"nobody\" \\ test/client-csr.json | cfssljson -bare nobody-client\n\nThen run $ make gencert to generate the certs.\n\nreport erratum • discuss\n\nAuthorize with Access Control Lists • 91\n\nNow let’s update our server tests to test for authorization and check that the tests fail (since our server doesn’t have authorization support yet). Later, when we implement authorization in our server, the tests will pass, and we’ll know we’ve successfully implemented authorization in the server.\n\nFirst, let’s update our client setup in our tests to build two clients we can use for testing our authorization setup. Update your client setup code in server_test.go to look like this:\n\nSecureYourServices/internal/server/server_test.go newClient := func(crtPath, keyPath string) (\n\ngrpc.ClientConn, api.LogClient, []grpc.DialOption,\n\n) {\n\ntlsConfig, err := config.SetupTLSConfig(config.TLSConfig{\n\nCertFile: crtPath, KeyFile: keyPath, CAFile: Server:\n\nconfig.CAFile, false,\n\n}) require.NoError(t, err) tlsCreds := credentials.NewTLS(tlsConfig) opts := []grpc.DialOption{grpc.WithTransportCredentials(tlsCreds)} conn, err := grpc.Dial(l.Addr().String(), opts...) require.NoError(t, err) client := api.NewLogClient(conn) return conn, client, opts\n\n}\n\nvar rootConn *grpc.ClientConn rootConn, rootClient, _ = newClient(\n\nconfig.RootClientCertFile, config.RootClientKeyFile,\n\n)\n\nvar nobodyConn *grpc.ClientConn nobodyConn, nobodyClient, _ = newClient(\n\nconfig.NobodyClientCertFile, config.NobodyClientKeyFile,\n\n)\n\nAnd update the teardown function to close the client connections:\n\nSecureYourServices/internal/server/server_test.go return rootClient, nobodyClient, cfg, func() {\n\nserver.Stop() rootConn.Close() nobodyConn.Close() l.Close()\n\n}\n\nreport erratum • discuss\n\n➤ ➤ ➤ ➤\n\n➤ ➤ ➤ ➤ ➤ ➤\n\nChapter 5. Secure Your Services • 92\n\nWe’re creating two clients: a superuser9 client called root who’s permitted to produce and consume, and a nobody10 client who isn’t permitted to do any- thing. Because the code for creating both clients is the same (aside from which cert and key they’re configured with), we’ve refactored the client creation code into a newClient(crtPath, keyPath string) helper function. Our server now takes in an Authorizer instance that the server will defer its authorization logic to. And we pass both our root and nobody clients to the test functions so they can use whatever client they need based on whether they’re testing how the server works with an authorized or unauthorized client. This last change also requires us to make some changes to our existing tests, so let’s fix those.\n\nChange your TestServer() function to the following so your test functions take in the unauthorized client:\n\nSecureYourServices/internal/server/server_test.go func TestServer(t *testing.T) {\n\nfor scenario, fn := range map[string]func(\n\nt *testing.T, rootClient api.LogClient, nobodyClient api.LogClient, config *Config,\n\n){\n\n// ...\n\n} {\n\nt.Run(scenario, func(t *testing.T) {\n\nrootClient,\n\nnobodyClient, config, teardown := setupTest(t, nil)\n\ndefer teardown() fn(t, rootClient, nobodyClient, config)\n\n})\n\n}\n\n}\n\nWe need to update our existing tests to handle the second client, which we do by changing the arguments of your test functions to the following:\n\nt *testing.T, client, _ api.LogClient, cfg *Config\n\nWe also need to add more variables to specify the locations of our nobody client’s cert and key, along with the configuration files for Casbin. So add these variables to your var declaration in internal/config/files.go:\n\n9. 10. https://en.wikipedia.org/wiki/Nobody_(username)\n\nhttps://en.wikipedia.org/wiki/Superuser\n\nreport erratum • discuss\n\n➤ ➤ ➤ ➤ ➤ ➤\n\nAuthorize with Access Control Lists • 93\n\nSecureYourServices/internal/config/files.go var (\n\nCAFile ServerCertFile ServerKeyFile RootClientCertFile RootClientKeyFile NobodyClientCertFile = configFile(\"nobody-client.pem\") NobodyClientKeyFile = configFile(\"nobody-client-key.pem\") ACLModelFile ACLPolicyFile\n\n= configFile(\"ca.pem\") = configFile(\"server.pem\") = configFile(\"server-key.pem\") = configFile(\"root-client.pem\") = configFile(\"root-client-key.pem\")\n\n= configFile(\"model.conf\") = configFile(\"policy.csv\")\n\n)\n\nSince the ACL policy is specific and used throughout our tests, we’ll put our Casbin configuration in the test directory as well. Inside the test directory, create a file called model.conf with the following configuration:\n\nSecureYourServices/test/model.conf # Request definition [request_definition] r = sub, obj, act\n\n# Policy definition [policy_definition] p = sub, obj, act\n\n# Policy effect [policy_effect] e = some(where (p.eft == allow))\n\n# Matchers [matchers] m = r.sub == p.sub && r.obj == p.obj && r.act == p.act\n\nThis configures Casbin to use ACL as its authorization mechanism.\n\nAlongside the model.conf file, add a policy.csv file with this snippet:\n\nSecureYourServices/test/policy.csv p, root, *, produce p, root, *, consume\n\nThis is your ACL table, with two entries saying that the root client has produce and consume permissions on the * object (which we’re using as a wildcard, meaning any object). All other clients, including nobody, will be denied.\n\nNow we need to install the policy and model files into the CONFIG_PATH so our tests can find them. Update your Makefile’s test target to the following:\n\nreport erratum • discuss\n\nChapter 5. Secure Your Services • 94\n\nSecureYourServices/Makefile $(CONFIG_PATH)/model.conf:\n\ncp test/model.conf $(CONFIG_PATH)/model.conf\n\n$(CONFIG_PATH)/policy.csv:\n\ncp test/policy.csv $(CONFIG_PATH)/policy.csv\n\n.PHONY: test test: $(CONFIG_PATH)/policy.csv $(CONFIG_PATH)/model.conf\n\ngo test -race ./...\n\nNow your tests are in a runnable state again, so you can run $ make test to see that they still pass! This is because the existing tests use the root client, which is authorized to produce and consume, and our current tests assume the client is authorized, and so they pass.\n\nLet’s add a test to check that unauthorized clients are denied. In server_test.go, import these packages:\n\nSecureYourServices/internal/server/server_test.go \"google.golang.org/grpc/codes\" \"google.golang.org/grpc/status\"\n\nBelow the testProduceConsumeStream() test we added in the last chapter, add this testUnauthorized() test:\n\nSecureYourServices/internal/server/server_test.go func testUnauthorized(\n\nt *testing.T, _, client api.LogClient, config *Config,\n\n) {\n\nctx := context.Background() produce, err := client.Produce(ctx, &api.ProduceRequest{\n\nRecord: &api.Record{\n\nValue: []byte(\"hello world\"),\n\n},\n\n},\n\n) if produce != nil {\n\nt.Fatalf(\"produce response should be nil\")\n\n} gotCode, wantCode := status.Code(err), codes.PermissionDenied if gotCode != wantCode {\n\nt.Fatalf(\"got code: %d, want: %d\", gotCode, wantCode)\n\n} consume, err := client.Consume(ctx, &api.ConsumeRequest{\n\nOffset: 0,\n\n})\n\nreport erratum • discuss\n\n➤\n\nAuthorize with Access Control Lists • 95\n\nif consume != nil {\n\nt.Fatalf(\"consume response should be nil\")\n\n} gotCode, wantCode = status.Code(err), codes.PermissionDenied if gotCode != wantCode {\n\nt.Fatalf(\"got code: %d, want: %d\", gotCode, wantCode)\n\n}\n\n}\n\nIn this test, we use the nobody client, which isn’t permitted to do anything. We try to use the client to produce and consume, just as we did in the suc- cessful test case. Since our client isn’t authorized, we want our server to deny the client, which we verify by checking the code on the returned error.\n\nUpdate the test table in TestServer(*testing.T) to include our unauthorized test by adding the highlighted line:\n\nSecureYourServices/internal/server/server_test.go \"produce/consume a message to/from the log succeeeds\": testProduceConsume, \"produce/consume stream succeeds\": \"consume past log boundary fails\": \"unauthorized fails\": testUnauthorized,\n\ntestProduceConsumeStream, testConsumePastBoundary,\n\nIf we run our tests with $ make test, they’ll fail because our server is still per- mitting all clients to do anything, since we haven’t hooked up its authorization yet. Let’s add authorization to the server now.\n\nUpdate your Config in server.go, update your imports to the following:\n\nSecureYourServices/internal/server/server.go import (\n\n\"context\"\n\napi \"github.com/travisjeffery/proglog/api/v1\"\n\ngrpc_middleware \"github.com/grpc-ecosystem/go-grpc-middleware\" grpc_auth \"github.com/grpc-ecosystem/go-grpc-middleware/auth\" \"google.golang.org/grpc\" \"google.golang.org/grpc/credentials\" \"google.golang.org/grpc/codes\" \"google.golang.org/grpc/peer\" \"google.golang.org/grpc/status\"\n\n)\n\nNow we’ll add a field for the authorizer and some constants we will use for authorization:\n\nSecureYourServices/internal/server/server.go type Config struct {\n\nCommitLog CommitLog Authorizer Authorizer\n\n}\n\nreport erratum • discuss\n\n➤ ➤ ➤ ➤ ➤ ➤ ➤\n\n➤ ➤ ➤ ➤ ➤ ➤ ➤\n\nChapter 5. Secure Your Services • 96\n\nconst (\n\nobjectWildcard = \"*\" produceAction = \"produce\" consumeAction = \"consume\"\n\n)\n\nThe constants match the values we in our ACL policy table, and we’ll reference them a few times in this file so they make sense being constants. The Config’s Authorizer field is an interface we need to define; put the following snippet below the CommitLog interface:\n\nSecureYourServices/internal/server/server.go type Authorizer interface {\n\nAuthorize(subject, object, action string) error\n\n}\n\nWe depend on an interface for the Authorizer so that we can switch out the authorization implementation—same as the CommitLog in Dependency Inversion with Interfaces, on page 67. Update your Produce() method to this snippet, adding the highlighted lines:\n\nSecureYourServices/internal/server/server.go func (s *grpcServer) Produce(ctx context.Context, req *api.ProduceRequest) (\n\napi.ProduceResponse, error) { if err := s.Authorizer.Authorize( subject(ctx), objectWildcard, produceAction,\n\n); err != nil {\n\nreturn nil, err\n\n} offset, err := s.CommitLog.Append(req.Record) if err != nil {\n\nreturn nil, err\n\n} return &api.ProduceResponse{Offset: offset}, nil\n\n}\n\nMake a similar change to your Consume() method, changing the method to this:\n\nSecureYourServices/internal/server/server.go func (s *grpcServer) Consume(ctx context.Context, req *api.ConsumeRequest) (\n\napi.ConsumeResponse, error) { if err := s.Authorizer.Authorize( subject(ctx), objectWildcard, consumeAction,\n\n); err != nil {\n\nreturn nil, err\n\n}\n\nreport erratum • discuss\n\nAuthorize with Access Control Lists • 97\n\nrecord, err := s.CommitLog.Read(req.Offset) if err != nil {\n\nreturn nil, err\n\n} return &api.ConsumeResponse{Record: record}, nil\n\n}\n\nWe now have the server checking whether the client (identified by the cert’s subject) is authorized to produce and consume, and if not, sending the per- mission denied error back to the client. When producing, if the client is authorized, then the method will continue and append the given record to the log. And when consuming, if the client is authorized, then the method will consume the record from the log. We take the subject out of the client’s cert with two helper functions. Add the following code at the bottom of server.go:\n\nSecureYourServices/internal/server/server.go func authenticate(ctx context.Context) (context.Context, error) {\n\npeer, ok := peer.FromContext(ctx) if !ok {\n\nreturn ctx, status.New(\n\ncodes.Unknown, \"couldn't find peer info\",\n\n).Err()\n\n}\n\nif peer.AuthInfo == nil {\n\nreturn context.WithValue(ctx, subjectContextKey{}, \"\"), nil\n\n}\n\ntlsInfo := peer.AuthInfo.(credentials.TLSInfo) subject := tlsInfo.State.VerifiedChains[0][0].Subject.CommonName ctx = context.WithValue(ctx, subjectContextKey{}, subject)\n\nreturn ctx, nil\n\n}\n\nfunc subject(ctx context.Context) string {\n\nreturn ctx.Value(subjectContextKey{}).(string)\n\n}\n\ntype subjectContextKey struct{}\n\nThe authenticate(context.Context) function is an interceptor that reads the subject out of the client’s cert and writes it to the RPC’s context. With interceptors, you can intercept and modify the execution of each RPC call, allowing you to break the request handling into smaller, reusable chunks. (Other frameworks name the same concept middleware.) The subject(context.Context) function returns the client’s cert’s subject so we can identify a client and check their access.\n\nUpdate your NewGRPCServer(*Config, ...grpc.ServerOption) function to the following code:\n\nreport erratum • discuss\n\nChapter 5. Secure Your Services • 98\n\nSecureYourServices/internal/server/server.go func NewGRPCServer(config *Config, opts ...grpc.ServerOption) (\n\ngrpc.Server, error,\n\n) {\n\nopts = append(opts, grpc.StreamInterceptor(\n\ngrpc_middleware.ChainStreamServer(\n\ngrpc_auth.StreamServerInterceptor(authenticate),\n\n)), grpc.UnaryInterceptor(grpc_middleware.ChainUnaryServer( grpc_auth.UnaryServerInterceptor(authenticate),\n\n))) gsrv := grpc.NewServer(opts...) srv, err := newgrpcServer(config) if err != nil {\n\nreturn nil, err\n\n} api.RegisterLogServer(gsrv, srv) return gsrv, nil\n\n}\n\nWe hook up our authenticate() interceptor to our gRPC server so that our server identifies the subject of each RPC to kick off the authorization process.\n\nNow update your test server’s configuration to pass in an authorizer. In set- up_test.go’s setupTest, import your auth package, and update the server’s configu- ration to the following:\n\nSecureYourServices/internal/server/server_test.go authorizer := auth.New(config.ACLModelFile, config.ACLPolicyFile) cfg = &Config{\n\nCommitLog: clog, Authorizer: authorizer,\n\n}\n\nOur server now authorizes its requests! You can verify that everything works by running the tests again: $ make test. Last time we ran them they failed because our server didn’t deny the nobody client who doesn’t have any per- missions. This time the tests pass since our server will now only authorize users who are permitted based on your ACL!\n\nWhat You Learned\n\nYou’ve learned how to secure services in three parts: by encrypting connections with TLS, through mutual TLS authentication to verify the identities of clients and servers, and by using ACL-based authorization to permit client actions. Next we’ll make our service observable by adding metrics, logs, and traces.\n\nreport erratum • discuss\n\nCHAPTER 6\n\nObserve Your Systems\n\nImagine waking up one day and noticing that the last hole in your belt doesn’t fit. You head to your scale and you see that you’ve gained a significant amount of weight overnight. You go on an emergency diet and fitness regimen. A couple of weeks later, you check the scales and see you gained even more weight somehow. What’s going on?\n\nWhat you need is insight into what’s going on in your body. If our body had built-in observability, we’d have metrics on our body, like hormone levels that we could graph on a dashboard. If we could see a sudden imbalance in our hormone levels, with all things being equal, we could surmise that a hormonal imbalance must be the root cause. But without being able to see what had changed, you’d make many changes in search of the problem, each with their own effects.\n\nWe make our systems observable so we can we can ask questions that will give us insight into the system and debug unexpected problems. The keyword is unexpected—making our system observable means we can fix arbitrary problems that haven’t happened before. In this chapter, we’ll make our service observable so we understand what’s going on within it.\n\nThree Types of Telemetry Data\n\nObservability is a measure of how well we understand our system’s inter- nals—its behavior and state—from its external outputs. We use metrics, structured logs, and traces as the outputs to make our systems observable. While there are three types of telemetry data, each with its own use case we’ll talk about, it’ll often derive from the same events. For example, each time a web service handles a request, it may increment a “requests handled” metric, emit a log for the request, and make a trace.\n\nreport erratum • discuss\n\nChapter 6. Observe Your Systems • 100\n\nMetrics\n\nMetrics measure numeric data over time, such as how many requests failed or how long each request took. Metrics like these help to define service-level indicators (SLI), objectives (SLO), and agreements (SLA). You’ll use metrics to report the health of your system, trigger internal alerts, and graph on dashboards to get an idea of how your system’s doing at a glance.\n\nBecause metrics are numerical data, you can gradually reduce resolution to reduce the storage requirements and time to query. For example, if we ran a book publishing company, we’d have metrics on each book purchase. To ship a customer’s books, we’d need to know the customer’s order, but after we’ve delivered the books and the return policy has passed, we don’t care about the order anymore. When we’re doing accounting or analysis on our business, that’s too much detail. Eventually we’d only need quarterly earnings to do our taxes, calculate year-over-year growth, and know if we can hire more editors and authors to expand our business.\n\nThere are three kinds of metrics:\n\nCounters\n\nCounters track the number of times an event happened, such as the number of requests that failed or the sum of some fact of your system like the number of bytes processed.\n\nYou’ll often take a counter and use it to get a rate: the number of times an event happened in an interval. Who cares about the total requests we’ve received other than to brag about it? What we care about is how many requests we’ve handled in the past second or minute—if that dropped significantly you’d want to check for latency in your system. You’d want to know when your request error rate spikes so you can see what’s wrong and fix it.\n\nHistograms\n\nHistograms show you a distribution of your data. You’ll mainly use his- tograms for measuring the percentiles of your request duration and sizes.\n\nGauges\n\nGauges track the current value of something. You can replace that value entirely. Gauges are useful for saturation-type metrics, like a host’s disk usage percentage or the number of load balancers compared to your cloud provider’s limits.\n\nreport erratum • discuss\n\nThree Types of Telemetry Data • 101\n\nYou could measure just about anything, so what data should you measure? What metrics will provide worthy signals on your system? These are Google’s four golden signals1 to measure:\n\nLatency—the time it takes your service to process requests. If your latency spikes, you often need to scale your system vertically by changing to an instance with more memory, CPUs, or IOPS, or scale your system horizontally by adding more instances to your load balancer.\n\nTraffic—the amount of demand on your service. For a typical web service, this could be requests processed per second. For an online video game or video streaming service, it could be the number of concurrent users. These metrics are good for bragging rights (hopefully), but more important, they can help give you an idea of the scale at which you’re working and when you’ve scaled to the point you need a new design.\n\nErrors—your service’s request failure rate. Internal server errors are par-\n\nticularly important.\n\nSaturation—a measure of your service’s capacity. For example, if your service persists data to disk, at your current ingress rate will you run out of hard drive space soon? If you have an in-memory store, how much memory is your service using compared to the memory available?\n\nWhile most debugging stories begin with metrics—either through an alert or someone noticing abnormalities on the dashboard—you’ll go to your logs and traces to learn more details about the problem. Let’s take a look at those next.\n\nStructured Logs\n\nLogs describe events in your system. You should log any event that gives you useful insight into your service. Logs should help us troubleshoot, audit, and profile so we can learn what went wrong and why, who ran what actions, and how long those actions took. For example, a gRPC service log could log this per RPC call:\n\n{\n\n\"request_id\": \"f47ac10b-58cc-0372-8567-0e02b2c3d479\", \"level\": \"info\", \"ts\": 1600139560.3399575, \"caller\": \"zap/server_interceptors.go:67\", \"msg\": \"finished streaming call with code OK\", \"peer.address\": \"127.0.0.1:54304\",\n\n1.\n\nhttps://landing.google.com/sre/sre-book/chapters/monitoring-distributed-systems/#xref_monitoring_golden-\n\nsignals\n\nreport erratum • discuss",
      "page_number": 84
    },
    {
      "number": 6,
      "title": "Observe Your Systems • 100",
      "start_page": 108,
      "end_page": 119,
      "detection_method": "regex_chapter_title",
      "content": "Chapter 6. Observe Your Systems • 102\n\n\"grpc.start_time\": \"2020-09-14T22:12:40-05:00\", \"system\": \"grpc\", \"span.kind\": \"server\", \"grpc.service\": \"log.v1.Log\", \"grpc.method\": \"ConsumeStream\", \"peer.address\": \"127.0.0.1:54304\", \"grpc.code\": \"OK\", \"grpc.time_ns\": 197740\n\n}\n\nIn this log we see when the caller called the method, the caller’s IP address, the service and method they called, if the call succeeded, and how long the request took. In distributed systems, the request ID is helpful for piecing together a complete picture of a request that’s handled by multiple services.\n\nThis gRPC log is a JSON formatted, structured log. A structured log is a set of name and value ordered pairs encoded in consistent schema and format that’s easily read by programs. Structured logs enable us to separate log capturing, transporting, persisting, and querying. For example, we could capture and transport our logs as protocol buffers and then re-encode them in the Parquet2 format and persist them in your columnar database.\n\nI recommend collecting your structured logs in an event streaming platform like Kafka to enable arbitrary processing and transporting of your logs. For example, you can connect Kafka with a database like BigQuery to query your logs while connecting Kafka with an object store like GCS to maintain histor- ical copies.\n\nAt play is a balance between logging too little and being without the informa- tion needed to debug a problem, or logging too much and being overwhelmed by too much information and missing what’s important. I suggest erring on logging too much, and cut back on the logs that aren’t useful as you learn more. That way you’re less likely to be without information you need to trou- bleshoot or audit a problem.\n\nTraces\n\nTraces capture request lifecycles and let you track requests as they flow through your system. Tracing user interfaces like Jaegar,3 Stackdriver,4 and Lightstep5 give you a visual representation of where requests spend time in your system. In distributed systems, this is especially useful as requests\n\n2. 3. 4. 5.\n\nhttps://parquet.apache.org\n\nhttps://www.jaegertracing.io\n\nhttps://cloud.google.com/products/operations\n\nhttps://lightstep.com\n\nreport erratum • discuss\n\nMake Your Service Observable • 103\n\nexecute over multiple services. The following screenshot shows an example of a trace of Jocko’s request handling in Jaegar.\n\nYou can tag your traces with details to know more about each request. A common example is tagging each trace with a user ID so that if users experi- ence a problem, you can easily find their requests.\n\nTraces comprise one or more spans. Spans can have parent/child relationships or be linked as siblings. Each span represents a part of the request’s execution. How detailed you break up those parts is up to you. Go wide to begin: trace requests across all your services end-to-end, with spans that begin and end at the entry and exit points of your services. Then go deep in each service and trace important method calls.\n\nNow, let’s update our code to make your service observable.\n\nMake Your Service Observable\n\nLet’s make your service observable by adding metrics, structured logs, and traces. When you deploy your services to production, you’ll usually configure your metrics, structured logs, and traces to go to external services like Prometheus,6 Elasticsearch,7 and Jaegar. To keep things simple, we’ll just log our observability pieces to files and see what the data looks like.\n\n6. 7.\n\nhttps://prometheus.io\n\nhttps://www.elastic.co/elasticsearch\n\nreport erratum • discuss\n\n➤ ➤ ➤ ➤ ➤ ➤ ➤ ➤ ➤\n\nChapter 6. Observe Your Systems • 104\n\nOpenTelemetry8 is a Cloud Native Computing Foundation (CNCF) project that provides robust and portable APIs and libraries that we can use for metrics and distributed tracing in our service. (OpenCensus and OpenTracing merged to form OpenTelemetry, which is backward-compatible with existing Open- Census integrations.) OpenTelemetry’s Go gRPC integration supports traces but not metrics, so we’ll use the OpenCensus libraries in our service since OpenCensus’s gRPC integration supports them both. Unfortunately, neither OpenTelemetry nor OpenCensus support logging yet. OpenTelemetry should support logging at some point—a special interest group9 is planning Open- Telemetry’s logging specification. In the meantime, we’ll use Uber’s Zap logging library.10\n\nMost Go networking APIs support middleware, so you can wrap request handling with your own logic. This is where I recommend beginning making your service observable by wrapping all requests with metrics, logs, and traces. That’s why we’re using the OpenCensus and Zap integrations’ inter- ceptors.\n\nRun the following commands within your project to add the OpenCensus and Zap dependencies:\n\n$ go get go.uber.org/zap@v1.10.0 $ go get go.opencensus.io@v0.22.2\n\nThen open internal/server/server.go and update your imports to include the high- lighted imports in this snippet:\n\nObserveYourServices/internal/server/server.go import (\n\n\"context\"\n\ngrpc_middleware \"github.com/grpc-ecosystem/go-grpc-middleware\" grpc_auth \"github.com/grpc-ecosystem/go-grpc-middleware/auth\" api \"github.com/travisjeffery/proglog/api/v1\"\n\n\"time\"\n\ngrpc_zap \"github.com/grpc-ecosystem/go-grpc-middleware/logging/zap\" grpc_ctxtags \"github.com/grpc-ecosystem/go-grpc-middleware/tags\" \"go.opencensus.io/plugin/ocgrpc\" \"go.opencensus.io/stats/view\" \"go.opencensus.io/trace\" \"go.uber.org/zap\" \"go.uber.org/zap/zapcore\"\n\n8. 9. 10. https://github.com/uber-go/zap\n\nhttps://opentelemetry.io\n\nhttps://github.com/open-telemetry/community#logs-working-group\n\nreport erratum • discuss\n\nMake Your Service Observable • 105\n\n\"google.golang.org/grpc\" \"google.golang.org/grpc/codes\" \"google.golang.org/grpc/credentials\" \"google.golang.org/grpc/peer\" \"google.golang.org/grpc/status\"\n\n)\n\nNow update NewGRPCServer() to configure Zap:\n\nObserveYourServices/internal/server/server.go func NewGRPCServer(config *Config, grpcOpts ...grpc.ServerOption) (\n\ngrpc.Server, error,\n\n) {\n\nlogger := zap.L().Named(\"server\") zapOpts := []grpc_zap.Option{\n\ngrpc_zap.WithDurationField(\n\nfunc(duration time.Duration) zapcore.Field {\n\nreturn zap.Int64(\n\n\"grpc.time_ns\", duration.Nanoseconds(),\n\n)\n\n},\n\n),\n\n}\n\nWe specify the logger’s name to differentiate the server logs from other logs in our service. Then we add a “grpc.time_ns” field to our structured logs to log the duration of each request in nanoseconds.\n\nAfter the previous snippet, add the following snippet to configure how Open- Census collects metrics and traces:\n\nObserveYourServices/internal/server/server.go trace.ApplyConfig(trace.Config{DefaultSampler: trace.AlwaysSample()}) err := view.Register(ocgrpc.DefaultServerViews...) if err != nil {\n\nreturn nil, err\n\n}\n\nWe’ve configured OpenCensus to always sample the traces because we’re developing our service and we want all of our requests traced.\n\nIn production you may not want to trace every request because it could affect performance, require too much data, or trace confidential data. If tracing too much is the problem, you can use the probability sampler and sample a percentage of the requests. However, one problem with using the probability sampler is that you may miss important requests. We could try to reconcile these trade-offs by writing our own sampler that always traces important\n\nreport erratum • discuss\n\n➤ ➤\n\n➤ ➤\n\n➤\n\nChapter 6. Observe Your Systems • 106\n\nrequests and samples a percentage of the rest of the requests. The code for that would look like this:\n\nhalfSampler := trace.ProbabilitySampler(0.5) trace.ApplyConfig(trace.Config{\n\nDefaultSampler: func(p trace.SamplingParameters) trace.SamplingDecision {\n\nif strings.Contains(p.Name, \"Produce\"){\n\nreturn trace.SamplingDecision{Sample: true}\n\n} return halfSampler(p)\n\n},\n\n})\n\nThe views specify what stats OpenCensus will collect. The default server views track stats on:\n\nReceived bytes per RPC • Sent bytes per RPC • Latency • Completed RPCs\n\nNow, change the grpcOpts after the previous snippet to include the lines high- lighted here:\n\nObserveYourServices/internal/server/server.go grpcOpts = append(grpcOpts,\n\ngrpc.StreamInterceptor(\n\ngrpc_middleware.ChainStreamServer(\n\ngrpc_ctxtags.StreamServerInterceptor(), grpc_zap.StreamServerInterceptor(logger, zapOpts...), grpc_auth.StreamServerInterceptor(authenticate),\n\n)), grpc.UnaryInterceptor(grpc_middleware.ChainUnaryServer( grpc_ctxtags.UnaryServerInterceptor(), grpc_zap.UnaryServerInterceptor(logger, zapOpts...), grpc_auth.UnaryServerInterceptor(authenticate),\n\n)), grpc.StatsHandler(&ocgrpc.ServerHandler{}),\n\n)\n\nThese lines configure gRPC to apply the Zap interceptors that log the gRPC calls and attach OpenCensus as the server’s stat handler so that OpenCensus can record stats on the server’s request handling.\n\nOkay, now we just have to change our test setup to configure the metrics and traces log files. Open internal/server/server_test.go and add these imports:\n\nObserveYourServices/internal/server/server_test.go \"os\" \"time\" \"flag\"\n\nreport erratum • discuss\n\nMake Your Service Observable • 107\n\n\"go.opencensus.io/examples/exporter\"\n\n\"go.uber.org/zap\"\n\nBelow your imports, add this snippet that defines a debug flag to enable observability output:\n\nObserveYourServices/internal/server/server_test.go // imports...\n\nvar debug = flag.Bool(\"debug\", false, \"Enable observability for debugging.\")\n\nfunc TestMain(m *testing.M) { flag.Parse() if *debug {\n\nlogger, err := zap.NewDevelopment() if err != nil {\n\npanic(err)\n\n} zap.ReplaceGlobals(logger)\n\n} os.Exit(m.Run())\n\n}\n\nWhen a test file implements TestMain(m *testing.M), Go will call TestMain(m) instead of running the tests directly. TestMain() gives us a place for setup that applies to all tests in that file, like enabling our debug output. Flag parsing has to go in TestMain() instead of init(), otherwise Go can’t define the flag and your code will error and exit.\n\nIn the setupTest() function, after the authorizer variable, add this snippet:\n\nObserveYourServices/internal/server/server_test.go var telemetryExporter *exporter.LogExporter if *debug {\n\nmetricsLogFile, err := ioutil.TempFile(\"\", \"metrics-*.log\") require.NoError(t, err) t.Logf(\"metrics log file: %s\", metricsLogFile.Name())\n\ntracesLogFile, err := ioutil.TempFile(\"\", \"traces-*.log\") require.NoError(t, err) t.Logf(\"traces log file: %s\", tracesLogFile.Name())\n\ntelemetryExporter, err = exporter.NewLogExporter(exporter.Options{\n\nMetricsLogFile: TracesLogFile: ReportingInterval: time.Second,\n\nmetricsLogFile.Name(), tracesLogFile.Name(),\n\n}) require.NoError(t, err) err = telemetryExporter.Start() require.NoError(t, err)\n\n}\n\nreport erratum • discuss\n\n➤ ➤ ➤ ➤ ➤\n\nChapter 6. Observe Your Systems • 108\n\nThis snippet sets up and starts the telemetry exporter to write to two files. Each test gets its own separate trace and metrics files so we can see each test’s requests.\n\nAt the bottom of setupTest(), update the teardown function to include these highlighted lines:\n\nObserveYourServices/internal/server/server_test.go return rootClient, nobodyClient, cfg, func() {\n\nserver.Stop() rootConn.Close() nobodyConn.Close() l.Close() if telemetryExporter != nil {\n\ntime.Sleep(1500 * time.Millisecond) telemetryExporter.Stop() telemetryExporter.Close()\n\n}\n\n}\n\nWe sleep for 1.5 seconds to give the telemetry exporter enough time to flush its data to disk. Then we stop and close the exporter.\n\nRun your server tests by navigating into the internal/server directory and execut- ing this command:\n\n$ go test -v -debug=true\n\nIn the test output, find these metrics and traces file logs, and open them to see the exported metrics and trace data:\n\nmetrics log file: /tmp/metrics-{{random string}}.log traces log file: /tmp/traces-{{random string}}.log\n\nFor example, here’s the completed RPC stat showing that the server handled two successful produce calls:\n\nMetric: name: grpc.io/server/completed_rpcs, type: TypeCumulativeInt64, unit: ms\n\nLabels: [\n\n{grpc_server_method}={log.v1.Log/Produce true} {grpc_server_status}={OK true}] Value : value=2\n\nAnd here’s a trace for a produce call:\n\nTraceID: SpanID:\n\n3e3343b74193e6a807cac515e82fb3b3 045493d1be3f7188\n\nSpan: Status: Elapsed: 1ms SpanKind: Server\n\nlog.v1.Log.Produce\n\n[0]\n\nreport erratum • discuss\n\nWhat You Learned • 109\n\nAttributes:\n\nClient=false - FailFast=false\n\nMessageEvents: Received UncompressedByteSize: 15 CompressedByteSize: 0\n\nSent UncompressedByteSize: 0 CompressedByteSize: 5\n\nWe can now observe what’s going on in our service!\n\nWhat You Learned\n\nIn this chapter, you learned about observability and its role in making reliable systems. You’ll find tracing especially useful in distributed systems, as it gives you a complete story of requests that take part over multiple services. You also learned how to make your service observable. Next, we’ll make our server support clustering to the service highly available and scalable.\n\nreport erratum • discuss\n\nPart III\n\nDistribute\n\nCHAPTER 7\n\nServer-to-Server Service Discovery\n\nSo far we’ve built a secure, stand-alone gRPC web service. Now let’s start our journey toward making our stand-alone service into a distributed one by incorporating service discovery so that our service automatically handles when a node is added to or removed from our cluster.\n\nIf you’re not familiar with service discovery, don’t worry—you will be after reading this chapter. Service discovery is one of the coolest things about dis- tributed services: machines automatically discovering other machines! (When Skynet becomes self aware and takes over, we can thank service discovery for playing a part.) Here’s a quick overview of the many benefits of service discovery.\n\nWhy Use Service Discovery?\n\nService discovery is the process of figuring out how to connect to a service. A service discovery solution must keep an up-to-date list (also known as a registry) of services, their locations, and their health. Downstream services then query this registry to discover the location of upstream services and connect to them—for example, a web service discovering and connecting to its database. This way, even if the upstream services change (scale up or down, or get replaced), downstream services can still connect to them.\n\nIn the pre-cloud days, you could set up “service discovery” with manually managed and configured static addresses, which was workable since applica- tions ran on static hardware. Today, service discovery plays a big part in modern cloud applications where nodes change frequently.\n\nInstead of using service discovery, some developers put load balancers in front of their services so that the load balancers provide static IPs. But for server-to-server communication, where you control the servers and you don’t\n\nreport erratum • discuss\n\nChapter 7. Server-to-Server Service Discovery • 114\n\nneed a load balancer to act as a trust boundary1 between clients and servers, use service discovery instead. Load balancers add cost, increase latency, introduce single points of failure, and need updates as services scale up and down. If you manage tens or hundreds of microservices, then not using service discovery means you also have to manage tens or hundreds of load balancers and DNS records. For a distributed service like ours, using a load balancer would force us to depend on a load-balancer service like nginx or the various cloud load balancers like AWS’s ELB or Google Cloud’s Load Balancer. This would increase our operational burden, infrastructure costs, and latency.\n\nIn our system, we have two service-discovery problems to solve:\n\nHow will the servers in our cluster discover each other? • How will the clients discover the servers?\n\nIn this chapter, we’ll work on implementing the discovery for the servers. Then, after we implement consensus in Chapter 8, Coordinate Your Services with Consensus, on page 141, we’ll work on the clients’ discovery in Chapter 9, Discover Servers and Load Balance from the Client, on page 171.\n\nNow that you know what service discovery can do, we’re ready to embed it into our service.\n\nEmbed Service Discovery\n\nWhen you have an application that needs to talk to a service, the tool you use for service discovery needs to perform the following tasks:\n\nManage a registry of services containing info such as their IPs and ports; • Help services find other services using the registry; • Health check service instances and remove them if they’re not well; and • Deregister services when they go offline.\n\nHistorically, people who’ve built distributed services have depended on sepa- rate, stand-alone services for service discovery (such as Consul, ZooKeeper, and Etcd). In this architecture, users of your service run two clusters: one for your service and one for your service discovery. The benefit of using a service-discovery service is that you don’t have to build service discovery yourself. The downside to using such a service, from your users’ standpoint, is that they have to learn, launch, and operate an extra service’s cluster. So using a stand-alone service for discovery removes the burden from your shoulders and puts it on your users’. That means many users won’t use your\n\n1.\n\nhttps://en.wikipedia.org/wiki/Trust_boundary\n\nreport erratum • discuss\n\nEmbed Service Discovery • 115\n\nservice because the burden is too much for them, and users who do take it on won’t recommend your service to others as often or as highly.\n\nSo why did people who built distributed services use stand-alone service- discovery services, and why did their users put up with the extra burden? Because neither had much of a choice. The people building distributed services didn’t have the libraries they needed to embed service discovery into their services, and users didn’t have other options.\n\nFortunately, times have changed. Today, Gophers have Serf—a library that provides decentralized cluster membership, failure detection, and orchestration that you can use to easily embed service discovery into your distributed ser- vices. Hashicorp, the company that created it, uses Serf to power its own service-discovery product, Consul, so you’re in good company.\n\nUsing Serf to embed service discovery into your services means that you don’t have to implement service discovery yourself and your users don’t have to run an extra cluster. It’s a win-win.\n\nWhen to Depend on a Stand-Alone Service-Discovery Solution\n\nYou may encounter cases where depending on a stand-alone ser- vice for service discovery makes sense—for example, if you need to integrate your service discovery with many platforms. You sink a lot of effort into that kind of work, and that’s likely a poor use of your time when you could just use a service like Consul that provides those integrations. In any case, Serf is always a good place to start. Once you’ve developed your service to solve the core problem it’s targeting and your service is stable or close to it, then you will have a good sense of whether you need to depend on a service-discovery service.\n\nHere are some other benefits of building our service with Serf:\n\nIn the early days of building a service, Serf is faster to set up and build\n\nour service against than having to set up a separate service.\n\nIt’s easier to move from Serf to a stand-alone service than to move from\n\na stand-alone service to Serf, so we still have both options open.\n\nOur service will be easier and more flexible to deploy, making our service\n\nmore accessible.\n\nSo for our service, we’ll use Serf to build service discovery.\n\nNow that we’ve seen the benefits of using Serf, let’s quickly discuss how Serf does its thing.\n\nreport erratum • discuss",
      "page_number": 108
    },
    {
      "number": 7,
      "title": "Server-to-Server Service Discovery • 114",
      "start_page": 120,
      "end_page": 146,
      "detection_method": "regex_chapter_title",
      "content": "Chapter 7. Server-to-Server Service Discovery • 116\n\nDiscover Services with Serf\n\nSerf maintains cluster membership by using an efficient, lightweight gossip protocol to communicate between the service’s nodes. Unlike service registry projects like ZooKeeper and Consul, Serf doesn’t have a central-registry architectural style. Instead, each instance of your service in the cluster runs as a Serf node. These nodes exchange messages with each other in the same way a zombie apocalypse might occur: one infected zombie soon spreads to infect everyone else. With Serf, instead of a spreading zombie virus, you’re spreading information about the nodes in your cluster. You listen to Serf for messages about changes in the cluster and then handle them accordingly.\n\nTo implement service discovery with Serf we need to:\n\n1. Create a Serf node on each server.\n\n2. Configure each Serf node with an address to listen on and accept connec-\n\ntions from other Serf nodes.\n\n3. Configure each Serf node with addresses of other Serf nodes and join their\n\ncluster.\n\n4. Handle Serf’s cluster discovery events, such as when a node joins or fails\n\nin the cluster.\n\nLet’s get coding.\n\nSerf is a lightweight tool that you can use for infinite use cases, but its API can be verbose when you have a specific problem to solve. The specific job we want our discovery layer to solve is to tell us when a server joined or left the cluster and what its ID and address are with as little API as possible. So let’s make a discovery package our server will use.\n\nTo get started, install the Serf package by running this command:\n\n$ go get github.com/hashicorp/serf@v0.8.5\n\nThen create an internal/discovery directory and inside it create a membership.go file, beginning with this code:\n\nServerSideServiceDiscovery/internal/discovery/membership.go package discovery\n\nimport (\n\n\"net\"\n\n\"go.uber.org/zap\"\n\n\"github.com/hashicorp/serf/serf\"\n\n)\n\nreport erratum • discuss\n\nDiscover Services with Serf • 117\n\ntype Membership struct {\n\nConfig handler Handler serf events chan serf.Event logger *zap.Logger\n\nserf.Serf\n\n}\n\nfunc New(handler Handler, config Config) (*Membership, error) {\n\nc := &Membership{\n\nConfig: config, handler: handler, logger: zap.L().Named(\"membership\"),\n\n} if err := c.setupSerf(); err != nil {\n\nreturn nil, err\n\n} return c, nil\n\n}\n\nMembership is our type wrapping Serf to provide discovery and cluster member- ship to our service. Users will call New() to create a Membership with the required configuration and event handler.\n\nAdd this code below the New() function to define the configuration type and set up Serf:\n\nLine 1\n\nServerSideServiceDiscovery/internal/discovery/membership.go type Config struct {\n\n\n\n\n\n\n\n5 }- -\n\nNodeName BindAddr Tags StartJoinAddrs []string\n\nstring string map[string]string\n\n\n\nfunc (m *Membership) setupSerf() (err error) {\n\n\n\n10\n\naddr, err := net.ResolveTCPAddr(\"tcp\", m.BindAddr) if err != nil {\n\n\n\nreturn err\n\n\n\n\n\n\n\n15\n\n\n\n\n\n\n\n\n\n20\n\n\n\n} config := serf.DefaultConfig() config.Init() config.MemberlistConfig.BindAddr = addr.IP.String() config.MemberlistConfig.BindPort = addr.Port m.events = make(chan serf.Event) config.EventCh = m.events config.Tags = m.Tags config.NodeName = m.Config.NodeName m.serf, err = serf.Create(config) if err != nil {\n\n\n\n\n\nreturn err\n\n\n\n}\n\nreport erratum • discuss\n\nChapter 7. Server-to-Server Service Discovery • 118\n\n25\n\n\n\ngo m.eventHandler() if m.StartJoinAddrs != nil {\n\n\n\n\n\n_, err = m.serf.Join(m.StartJoinAddrs, true) if err != nil {\n\n\n\nreturn err\n\n30\n\n}\n\n\n\n}-\n\n} return nil\n\nSerf has a lot of configurable parameters, but the five parameters you’ll typi- cally use are:\n\nNodeName—the node name acts as the node’s unique identifier across the Serf cluster. If you don’t set the node name, Serf uses the hostname.\n\nBindAddr and BindPort—Serf listens on this address and port for gossiping.\n\nTags—Serf shares these tags to the other nodes in the cluster and should use these tags for simple data that informs the cluster how to handle this node. For example, Consul shares each node’s RPC address with Serf tags, and once they know each other’s RPC address, they can make RPCs to each other. Consul shares whether the node is a voter or non-voter, which changes the node’s role in the Raft cluster. We’ll talk about this more in the next chapter when we use Raft to build consensus in our cluster. In our code, similar to Consul, we’ll share each node’s user-con- figured RPC address with a Serf tag so the nodes know which addresses to send their RPCs.\n\nEventCh—the event channel is how you’ll receive Serf’s events when a node joins or leaves the cluster. If you want a snapshot of the members at any point in time, you can call Serf’s Members() method.\n\nStartJoinAddrs—when you have an existing cluster and you create a new node that you want to add to that cluster, you need to point your new node to at least one of the nodes now in the cluster. After the new node connects to one of those nodes in the existing cluster, it’ll learn about the rest of the nodes, and vice versa (the existing nodes learn about the new node). The StartJoinAddrs field is how you configure new nodes to join an existing cluster. You set the field to the addresses of nodes in the cluster, and Serf’s gossip protocol takes care of the rest to join your node to the cluster. In a production environment, specify at least three addresses to make your cluster resilient to one or two node failures or a disrupted network.\n\nsetupSerf() creates and configures a Serf instance and starts the eventsHandler() goroutine to handle Serf’s events.\n\nreport erratum • discuss\n\nDiscover Services with Serf • 119\n\nDefine the Handler interface by putting this snippet below setupSerf():\n\nServerSideServiceDiscovery/internal/discovery/membership.go type Handler interface {\n\nJoin(name, addr string) error Leave(name string) error\n\n}\n\nThe Handler represents some component in our service that needs to know when a server joins or leaves the cluster.\n\nIn this chapter we will build a component that replicates the data of servers that join the cluster. In the next chapter, where we will build consensus in our service, Raft needs to know when servers join the cluster to coordi- nate with them.\n\nAdd this snippet below Handler() to define the eventHandler() method:\n\nServerSideServiceDiscovery/internal/discovery/membership.go func (m *Membership) eventHandler() {\n\nfor e := range m.events {\n\nswitch e.EventType() { case serf.EventMemberJoin:\n\nfor _, member := range e.(serf.MemberEvent).Members {\n\nif m.isLocal(member) {\n\ncontinue\n\n} m.handleJoin(member)\n\n}\n\ncase serf.EventMemberLeave, serf.EventMemberFailed:\n\nfor _, member := range e.(serf.MemberEvent).Members {\n\nif m.isLocal(member) { return\n\n} m.handleLeave(member)\n\n}\n\n}\n\n}\n\n}\n\nfunc (m *Membership) handleJoin(member serf.Member) {\n\nif err := m.handler.Join(\n\nmember.Name, member.Tags[\"rpc_addr\"],\n\n); err != nil {\n\nm.logError(err, \"failed to join\", member)\n\n}\n\n}\n\nfunc (m *Membership) handleLeave(member serf.Member) {\n\nif err := m.handler.Leave(\n\nmember.Name,\n\nreport erratum • discuss\n\nChapter 7. Server-to-Server Service Discovery • 120\n\n); err != nil {\n\nm.logError(err, \"failed to leave\", member)\n\n}\n\n}\n\nThe eventHandler() runs in a loop reading events sent by Serf into the events channel, handling each incoming event according to the event’s type. When a node joins or leaves the cluster, Serf sends an event to all nodes, including the node that joined or left the cluster. We check whether the node we got an event for is the local server so the server doesn’t act on itself—we don’t want the server to try and replicate itself, for example.\n\nNotice that Serf may coalesce multiple members updates into one event. For example, say ten nodes join around the same time; in that case, Serf will send you one join event with ten members, so that’s why we iterate over the event’s members.\n\nPut this code below eventHandler() to implement the rest of Membership:\n\nServerSideServiceDiscovery/internal/discovery/membership.go func (m *Membership) isLocal(member serf.Member) bool { return m.serf.LocalMember().Name == member.Name\n\n}\n\nfunc (m *Membership) Members() []serf.Member {\n\nreturn m.serf.Members()\n\n}\n\nfunc (m *Membership) Leave() error { return m.serf.Leave()\n\n}\n\nfunc (m *Membership) logError(err error, msg string, member serf.Member) {\n\nm.logger.Error(\n\nmsg, zap.Error(err), zap.String(\"name\", member.Name), zap.String(\"rpc_addr\", member.Tags[\"rpc_addr\"]),\n\n)\n\n}\n\nThese methods comprise the rest of Membership:\n\nisLocal() returns whether the given Serf member is the local member by\n\nchecking the members’ names.\n\nMembers() returns a point-in-time snapshot of the cluster’s Serf members.\n\nLeave() tells this member to leave the Serf cluster.\n\nlogError() logs the given error and message.\n\nreport erratum • discuss\n\nDiscover Services with Serf • 121\n\nLet’s test our Membership code now. Create a membership_test.go file in the inter- nal/discovery directory, and begin the file with this code:\n\nServerSideServiceDiscovery/internal/discovery/membership_test.go package discovery_test\n\nimport (\n\n\"fmt\" \"testing\" \"time\"\n\n\"github.com/hashicorp/serf/serf\" \"github.com/stretchr/testify/require\" \"github.com/travisjeffery/go-dynaport\" . \"github.com/travisjeffery/proglog/internal/discovery\"\n\n)\n\nfunc TestMembership(t *testing.T) {\n\nm, handler := setupMember(t, nil) m, _ = setupMember(t, m) m, _ = setupMember(t, m)\n\nrequire.Eventually(t, func() bool {\n\nreturn 2 == len(handler.joins) &&\n\n3 == len(m[0].Members()) && 0 == len(handler.leaves)\n\n}, 3*time.Second, 250*time.Millisecond)\n\nrequire.NoError(t, m[2].Leave())\n\nrequire.Eventually(t, func() bool {\n\nreturn 2 == len(handler.joins) &&\n\n3 == len(m[0].Members()) && serf.StatusLeft == m[0].Members()[2].Status && 1 == len(handler.leaves)\n\n}, 3*time.Second, 250*time.Millisecond)\n\nrequire.Equal(t, fmt.Sprintf(\"%d\", 2), <-handler.leaves)\n\n}\n\nOur test sets up a cluster with multiple servers and checks that the Membership returns all the servers that joined the membership and updates after a server leaves the cluster. The handler’s joins and leaves channels tell us how many times each event happened and for what servers. Each member has a status to know how its doing:\n\nAlive—the server is present and healthy. • Leaving—the server is gracefully leaving the cluster. • Left—the server has gracefully left the cluster. • Failed—the server unexpectedly left the cluster.\n\nreport erratum • discuss\n\nChapter 7. Server-to-Server Service Discovery • 122\n\nTestMembership() relies on a helper method to set up a member each time you call it. Define the helper setupMember() by adding the following code below TestMembership():\n\nServerSideServiceDiscovery/internal/discovery/membership_test.go func setupMember(t *testing.T, members []*Membership) (\n\n[]*Membership, *handler,\n\n) {\n\nid := len(members) ports := dynaport.Get(1) addr := fmt.Sprintf(\"%s:%d\", \"127.0.0.1\", ports[0]) tags := map[string]string{\n\n\"rpc_addr\": addr,\n\n} c := Config{\n\nNodeName: fmt.Sprintf(\"%d\", id), BindAddr: addr, tags, Tags:\n\n} h := &handler{} if len(members) == 0 {\n\nh.joins = make(chan map[string]string, 3) h.leaves = make(chan string, 3)\n\n} else {\n\nc.StartJoinAddrs = []string{ members[0].BindAddr,\n\n}\n\n} m, err := New(h, c) require.NoError(t, err) members = append(members, m) return members, h\n\n}\n\nsetupMember() sets up a new member under a free port and with the member’s length as the node name so the names are unique. The member’s length also tells us whether this member is the cluster’s initial member or we have a cluster to join.\n\nDefine the handler mock and finish the test code by putting this snippet below setupMember():\n\nServerSideServiceDiscovery/internal/discovery/membership_test.go type handler struct {\n\njoins chan map[string]string leaves chan string\n\n}\n\nfunc (h *handler) Join(id, addr string) error {\n\nif h.joins != nil {\n\nreport erratum • discuss\n\nRequest Discovered Services and Replicate Logs • 123\n\nh.joins <- map[string]string{\n\n\"id\": \"addr\": addr,\n\nid,\n\n}\n\n} return nil\n\n}\n\nfunc (h *handler) Leave(id string) error {\n\nif h.leaves != nil {\n\nh.leaves <- id\n\n} return nil\n\n}\n\nThe handler mock tracks how many times our Membership calls the handler’s Join() and Leave() methods, and with what IDs and addresses.\n\nRun the Membership’s tests and verify they pass.\n\nNow that we have our discovery and membership package, let’s integrate it with our service and build something we couldn’t before—replication!\n\nRequest Discovered Services and Replicate Logs\n\nLet’s build on our service discovery to add replication in our service so that we store multiple copies of the log data when we have multiple servers in a cluster. Replication makes our service more resilient to failures. For example, if a node’s disk fails and we can’t recover its data, replication can save our butts because it ensures that there’s a copy saved on another disk.\n\nIn the next chapter, we’ll coordinate the servers so our replication will have a defined leader-follower relationship, but for now we simply want the servers to replicate each other when they discover each other and not worry about whether they should, like the scientists from Jurassic Park. Our goal for the rest of this chapter is to build something simple that makes use of our service’s discovery and sets us up for our coordinated replication in the next chapter.\n\nDiscovery alone isn’t useful—so what if a bunch of computers discover each other and they just sit there doing nothing? Discovery is important because the discovery events trigger other processes in our service like replication and consensus. When servers discover other servers, we want to trigger the servers to replicate. We need a component in our service that handles when a server joins (or leaves) the cluster and begins (or ends) replicating from it.\n\nOur replication will be pull-based, with the replication component consum- ing from each discovered server and producing a copy to the local server.\n\nreport erratum • discuss\n\nChapter 7. Server-to-Server Service Discovery • 124\n\nIn pull-based replication, the consumer periodically polls the data source to check if it has new data to consume. In push-based replication, the data source pushes the data to its replicas. (In the next chapter we’ll integrate Raft to our service—and it’s push-based.)\n\nPull-based systems’ flexibility can be great for log and message systems where the consumers and work loads can differ—for example, if you have a client that stream processes its data and runs continuously and you have a client that batch processes its data and runs every twenty-four hours. When repli- cating between servers, we replicate the newest data with as low latency as possible with homogeneous servers, so pull-based and push-based systems behave about the same. But it’ll be easier to write our own pull-based replica- tion that will highlight why we need consensus.\n\nTo add replication to our cluster, we need a replication component that acts as a membership handler handling when a server joins and leaves the cluster. When a server joins the cluster, the component will connect to the server and run a loop that consumes from the discovered server and produces to the local server.\n\nIn the internal/log directory, create a new file named replicator.go to contain our replication code, beginning with this snippet:\n\nServerSideServiceDiscovery/internal/log/replicator.go package log\n\nimport (\n\n\"context\" \"sync\"\n\n\"go.uber.org/zap\" \"google.golang.org/grpc\"\n\napi \"github.com/travisjeffery/proglog/api/v1\"\n\n)\n\ntype Replicator struct {\n\nDialOptions []grpc.DialOption LocalServer api.LogClient\n\nlogger *zap.Logger\n\nmu servers map[string]chan struct{} closed bool close\n\nsync.Mutex\n\nchan struct{}\n\n}\n\nThe replicator connects to other servers with the gRPC client, and we need to configure the client so it can authenticate with the servers. The clientOptions\n\nreport erratum • discuss\n\nRequest Discovered Services and Replicate Logs • 125\n\nfield is how we pass in the options to configure the client. The servers field is a map of server addresses to a channel, which the replicator uses to stop replicating from a server when the server fails or leaves the cluster. The replicator calls the produce function to save a copy of the messages it con- sumes from the other servers.\n\nNext, put the following Join() method below the replicator struct:\n\nServerSideServiceDiscovery/internal/log/replicator.go func (r *Replicator) Join(name, addr string) error {\n\nr.mu.Lock() defer r.mu.Unlock() r.init()\n\nif r.closed {\n\nreturn nil\n\n}\n\nif _, ok := r.servers[name]; ok {\n\n// already replicating so skip return nil\n\n} r.servers[name] = make(chan struct{})\n\ngo r.replicate(addr, r.servers[name])\n\nreturn nil\n\n}\n\nThe Join(name, addr string) method adds the given server address to the list of servers to replicate and kicks off the add goroutine to run the actual replication logic.\n\nNow put the replicate(addr string) method, containing the replication logic, below the previous snippet:\n\nServerSideServiceDiscovery/internal/log/replicator.go func (r *Replicator) replicate(addr string, leave chan struct{}) {\n\ncc, err := grpc.Dial(addr, r.DialOptions...) if err != nil {\n\nr.logError(err, \"failed to dial\", addr) return\n\n} defer cc.Close()\n\nclient := api.NewLogClient(cc)\n\nctx := context.Background() stream, err := client.ConsumeStream(ctx,\n\n&api.ConsumeRequest{\n\nOffset: 0,\n\n},\n\n)\n\nreport erratum • discuss\n\nChapter 7. Server-to-Server Service Discovery • 126\n\nif err != nil {\n\nr.logError(err, \"failed to consume\", addr) return\n\n}\n\nrecords := make(chan *api.Record) go func() {\n\nfor {\n\nrecv, err := stream.Recv() if err != nil {\n\nr.logError(err, \"failed to receive\", addr) return\n\n} records <- recv.Record\n\n}\n\n}()\n\nYou saw most of this code before when we tested our stream consumer and producer. Here we create a client and open up a stream to consume all logs on the server.\n\nAppend the following snippet to finish implementing replicate():\n\nServerSideServiceDiscovery/internal/log/replicator.go\n\nfor {\n\nselect { case <-r.close:\n\nreturn\n\ncase <-leave:\n\nreturn\n\ncase record := <-records:\n\n_, err = r.LocalServer.Produce(ctx, &api.ProduceRequest{\n\nRecord: record,\n\n},\n\n) if err != nil {\n\nr.logError(err, \"failed to produce\", addr) return\n\n}\n\n}\n\n}\n\n}\n\nThe loop consumes the logs from the discovered server in a stream and then produces to the local server to save a copy. We replicate messages from the other server until that server fails or leaves the cluster and the replicator closes the channel for that server, which breaks the loop and ends the replicate() goroutine. The replicator closes the channel when Serf receives an event\n\nreport erratum • discuss\n\nRequest Discovered Services and Replicate Logs • 127\n\nsaying that the other server left the cluster, and then this server calls the Leave() method that we’re about to add.\n\nWrite the Leave(name string) method beneath your replicate() method with the fol- lowing code:\n\nServerSideServiceDiscovery/internal/log/replicator.go func (r *Replicator) Leave(name string) error {\n\nr.mu.Lock() defer r.mu.Unlock() r.init() if _, ok := r.servers[name]; !ok {\n\nreturn nil\n\n} close(r.servers[name]) delete(r.servers, name) return nil\n\n}\n\nThis Leave(name string) method handles the server leaving the cluster by removing the server from the list of servers to replicate and closes the server’s associated channel. Closing the channel signals to the receiver in the replicate() goroutine to stop replicating from that server.\n\nNext, add the following init() helper below your Leave() method:\n\nServerSideServiceDiscovery/internal/log/replicator.go func (r *Replicator) init() {\n\nif r.logger == nil {\n\nr.logger = zap.L().Named(\"replicator\")\n\n} if r.servers == nil {\n\nr.servers = make(map[string]chan struct{})\n\n} if r.close == nil {\n\nr.close = make(chan struct{})\n\n}\n\n}\n\nWe use this init() helper to lazily initialize the server map. You should use lazy initialization to give your structs a useful zero value2 because having a useful zero value reduces the API’s size and complexity while maintaining the same functionality. Without a useful zero value, we’d either have to export a repli- cator constructor function for the user to call or export the servers field on the replicator struct for the user to set—making more API for the user to learn and then requiring them to write more code before they can use our struct.\n\n2.\n\nhttps://dave.cheney.net/2013/01/19/what-is-the-zero-value-and-why-is-it-useful\n\nreport erratum • discuss\n\nChapter 7. Server-to-Server Service Discovery • 128\n\nAppend the following snippet to implement the Close() method:\n\nServerSideServiceDiscovery/internal/log/replicator.go func (r *Replicator) Close() error {\n\nr.mu.Lock() defer r.mu.Unlock() r.init()\n\nif r.closed {\n\nreturn nil\n\n} r.closed = true close(r.close) return nil\n\n}\n\nClose() closes the replicator so it doesn’t replicate new servers that join the cluster and it stops replicating existing servers by causing the replicate() gorou- tines to return.\n\nWe have one last helper to add to handle errors. Add this logError(err error, msg, addr string) method at the bottom of the file:\n\nServerSideServiceDiscovery/internal/log/replicator.go func (r *Replicator) logError(err error, msg, addr string) {\n\nr.logger.Error(\n\nmsg, zap.String(\"addr\", addr), zap.Error(err),\n\n)\n\n}\n\nWith this method, we just log the errors because we have no other use for them and to keep the code short and simple. If your users need access to the errors, a technique you can use to expose these errors is to export an error channel and send the errors into it for your users to receive and handle.\n\nThat’s it for our replicator. In terms of components, we now have our repli- cator, membership, log, and server. Each service instance must set up and connect these components together to work. For simpler, short-running programs, I’ll make a run package that exports a Run() function that’s responsible for running the program. Rob Pike’s Ivy project3 works this way. For more complex, long-running services, I’ll make an agent package that exports an Agent type that manages the different components and processes\n\n3.\n\nhttps://github.com/robpike/ivy\n\nreport erratum • discuss\n\nRequest Discovered Services and Replicate Logs • 129\n\nthat make up the service. Hashicorp’s Consul4 works this way. Let’s write an Agent for our service and then test our log, server, membership, and replicator end-to-end.\n\nCreate an internal/agent directory with a file named agent.go inside that begins with this code:\n\nServerSideServiceDiscovery/internal/agent/agent.go package agent\n\nimport (\n\n\"crypto/tls\" \"fmt\" \"net\" \"sync\"\n\n\"go.uber.org/zap\"\n\n\"google.golang.org/grpc\" \"google.golang.org/grpc/credentials\"\n\napi \"github.com/travisjeffery/proglog/api/v1\" \"github.com/travisjeffery/proglog/internal/auth\" \"github.com/travisjeffery/proglog/internal/discovery\" \"github.com/travisjeffery/proglog/internal/log\" \"github.com/travisjeffery/proglog/internal/server\"\n\n)\n\ntype Agent struct {\n\nConfig\n\nlog server membership *discovery.Membership replicator *log.Replicator\n\nlog.Log *grpc.Server\n\nshutdown shutdowns shutdownLock sync.Mutex\n\nbool chan struct{}\n\n}\n\nAn Agent runs on every service instance, setting up and connecting all the different components. The struct references each component (log, server, membership, replicator) that the Agent manages.\n\nAfter the Agent, add its Config struct:\n\nServerSideServiceDiscovery/internal/agent/agent.go type Config struct {\n\nServerTLSConfig *tls.Config *tls.Config PeerTLSConfig\n\n4.\n\nhttps://github.com/hashicorp/consul\n\nreport erratum • discuss\n\nChapter 7. Server-to-Server Service Discovery • 130\n\nDataDir BindAddr RPCPort NodeName StartJoinAddrs []string ACLModelFile ACLPolicyFile\n\nstring string int string\n\nstring string\n\n}\n\nfunc (c Config) RPCAddr() (string, error) {\n\nhost, _, err := net.SplitHostPort(c.BindAddr) if err != nil {\n\nreturn \"\", err\n\n} return fmt.Sprintf(\"%s:%d\", host, c.RPCPort), nil\n\n}\n\nThe Agent sets up the components so its Config comprises the components’ parameters to pass them through to the components.\n\nBelow Config, place this Agent creator function:\n\nServerSideServiceDiscovery/internal/agent/agent.go func New(config Config) (*Agent, error) {\n\na := &Agent{\n\nConfig: shutdowns: make(chan struct{}),\n\nconfig,\n\n} setup := []func() error{\n\na.setupLogger, a.setupLog, a.setupServer, a.setupMembership,\n\n} for _, fn := range setup {\n\nif err := fn(); err != nil {\n\nreturn nil, err\n\n}\n\n} return a, nil\n\n}\n\nNew(Config) creates an Agent and runs a set of methods to set up and run the agent’s components. After we run New(), we expect to have a running, function- ing service. We’ve seen most of these setup codes before when testing our components, so we’ll cover them quickly.\n\nreport erratum • discuss\n\nRequest Discovered Services and Replicate Logs • 131\n\nFirst, set up the logger with this setupLogger() method. Put setupLogger() under New():\n\nServerSideServiceDiscovery/internal/agent/agent.go func (a *Agent) setupLogger() error {\n\nlogger, err := zap.NewDevelopment() if err != nil {\n\nreturn err\n\n} zap.ReplaceGlobals(logger) return nil\n\n}\n\nThen, we set up the log with this setupLog() method. Put setupLog() under the previous snippet:\n\nServerSideServiceDiscovery/internal/agent/agent.go func (a *Agent) setupLog() error {\n\nvar err error a.log, err = log.NewLog(\n\na.Config.DataDir, log.Config{},\n\n) return err\n\n}\n\nNow we set up the server with setupServer(). Add setupServer() after setupLog():\n\nServerSideServiceDiscovery/internal/agent/agent.go func (a *Agent) setupServer() error {\n\nauthorizer := auth.New(\n\na.Config.ACLModelFile, a.Config.ACLPolicyFile,\n\n) serverConfig := &server.Config{\n\nCommitLog: a.log, Authorizer: authorizer,\n\n} var opts []grpc.ServerOption if a.Config.ServerTLSConfig != nil {\n\ncreds := credentials.NewTLS(a.Config.ServerTLSConfig) opts = append(opts, grpc.Creds(creds))\n\n} var err error a.server, err = server.NewGRPCServer(serverConfig, opts...) if err != nil {\n\nreturn err\n\n} rpcAddr, err := a.RPCAddr() if err != nil {\n\nreturn err\n\n}\n\nreport erratum • discuss\n\nChapter 7. Server-to-Server Service Discovery • 132\n\nln, err := net.Listen(\"tcp\", rpcAddr) if err != nil {\n\nreturn err\n\n} go func() {\n\nif err := a.server.Serve(ln); err != nil {\n\n_ = a.Shutdown()\n\n}\n\n}() return err\n\n}\n\nThen we set up the membership with setupMembership(). Place setupMembership() after setupServer():\n\nServerSideServiceDiscovery/internal/agent/agent.go func (a *Agent) setupMembership() error {\n\nrpcAddr, err := a.Config.RPCAddr() if err != nil {\n\nreturn err\n\n} var opts []grpc.DialOption if a.Config.PeerTLSConfig != nil {\n\nopts = append(opts, grpc.WithTransportCredentials(\n\ncredentials.NewTLS(a.Config.PeerTLSConfig),\n\n), )\n\n} conn, err := grpc.Dial(rpcAddr, opts...) if err != nil {\n\nreturn err\n\n} client := api.NewLogClient(conn) a.replicator = &log.Replicator{\n\nDialOptions: opts, LocalServer: client,\n\n} a.membership, err = discovery.New(a.replicator, discovery.Config{\n\nNodeName: a.Config.NodeName, BindAddr: a.Config.BindAddr, Tags: map[string]string{\n\n\"rpc_addr\": rpcAddr,\n\n}, StartJoinAddrs: a.Config.StartJoinAddrs,\n\n}) return err\n\n}\n\nsetupMembership() sets up a Replicator with the gRPC dial options needed to connect to other servers and a client so the replicator can connect to other servers,\n\nreport erratum • discuss\n\nRequest Discovered Services and Replicate Logs • 133\n\nconsume their data, and produce a copy of the data to the local server. Then we create a Membership passing in the replicator and its handler to notify the replicator when servers join and leave the cluster.\n\nThat’s all of the agent’s setup code. If we call New() now, we’d have a running agent. At some point we’ll want to shut down the agent, so put this Shutdown() method at the bottom of the file:\n\nServerSideServiceDiscovery/internal/agent/agent.go func (a *Agent) Shutdown() error {\n\na.shutdownLock.Lock() defer a.shutdownLock.Unlock() if a.shutdown {\n\nreturn nil\n\n} a.shutdown = true close(a.shutdowns)\n\nshutdown := []func() error{ a.membership.Leave, a.replicator.Close, func() error {\n\na.server.GracefulStop() return nil\n\n}, a.log.Close,\n\n} for _, fn := range shutdown {\n\nif err := fn(); err != nil {\n\nreturn err\n\n}\n\n} return nil\n\n}\n\nThis ensures that the agent will shut down once even if people call Shutdown() multiple times. Then we shut down the agent and its components by:\n\nLeaving the membership so that other servers will see that this server has left the cluster and so that this server doesn’t receive discovery events anymore;\n\nClosing the replicator so it doesn’t continue to replicate;\n\nGracefully stopping the server, which stops the server from accepting new connections and blocks until all the pending RPCs have finished; and\n\nClosing the log.\n\nreport erratum • discuss\n\nChapter 7. Server-to-Server Service Discovery • 134\n\nWe’ve implemented Serf into our service, so we can now run multiple instances of our service that discover and then replicate each other’s data. Let’s write a test to check that our service discovery and replication works and to prevent us from introducing a regression when we build consensus in Chapter 8, Coordinate Your Services with Consensus, on page 141.\n\nTest Discovery and the Service End-to-End\n\nLet’s test that our service discovery and replication works in an end-to-end test. We’ll set up a cluster with three nodes. We’ll produce a record to one server and verify that we can consume the message from the other servers that have (hopefully) replicated for us.\n\nIn internal/agent, create an agent_test.go file, beginning with this snippet:\n\nServerSideServiceDiscovery/internal/agent/agent_test.go package agent_test\n\nimport (\n\n\"context\" \"crypto/tls\" \"fmt\" \"io/ioutil\" \"os\" \"testing\" \"time\"\n\n\"github.com/stretchr/testify/require\" \"github.com/travisjeffery/go-dynaport\" \"google.golang.org/grpc\" \"google.golang.org/grpc/credentials\"\n\napi \"github.com/travisjeffery/proglog/api/v1\" \"github.com/travisjeffery/proglog/internal/agent\" \"github.com/travisjeffery/proglog/internal/config\"\n\n)\n\nWhat can I say? Our end-to-end test has a lot going on and requires a lot of imports to make it happen.\n\nNow we can write the test beginning with this code:\n\nServerSideServiceDiscovery/internal/agent/agent_test.go func TestAgent(t *testing.T) {\n\nserverTLSConfig, err := config.SetupTLSConfig(config.TLSConfig{\n\nCertFile: KeyFile: CAFile: Server: ServerAddress: \"127.0.0.1\",\n\nconfig.ServerCertFile, config.ServerKeyFile, config.CAFile, true,\n\n})\n\nreport erratum • discuss\n\nTest Discovery and the Service End-to-End • 135\n\nrequire.NoError(t, err)\n\npeerTLSConfig, err := config.SetupTLSConfig(config.TLSConfig{\n\nCertFile: KeyFile: CAFile: Server: ServerAddress: \"127.0.0.1\",\n\nconfig.RootClientCertFile, config.RootClientKeyFile, config.CAFile, false,\n\n}) require.NoError(t, err)\n\nThis snippet defines the certificate configurations used in our test to test our security. The serverTLSConfig defines the configuration of the certificate that’s served to clients. And the peerTLSConfig defines the configuration of the certificate that’s served between servers so they can connect with and replicate each other.\n\nNow set up the cluster by placing this code after the previous snippet:\n\nServerSideServiceDiscovery/internal/agent/agent_test.go var agents []*agent.Agent for i := 0; i < 3; i++ {\n\nports := dynaport.Get(2) bindAddr := fmt.Sprintf(\"%s:%d\", \"127.0.0.1\", ports[0]) rpcPort := ports[1]\n\ndataDir, err := ioutil.TempDir(\"\", \"agent-test-log\") require.NoError(t, err)\n\nvar startJoinAddrs []string if i != 0 {\n\nstartJoinAddrs = append(\n\nstartJoinAddrs, agents[0].Config.BindAddr,\n\n)\n\n}\n\nagent, err := agent.New(agent.Config{\n\nNodeName: StartJoinAddrs: startJoinAddrs, BindAddr: RPCPort: DataDir: ACLModelFile: ACLPolicyFile: ServerTLSConfig: serverTLSConfig, PeerTLSConfig:\n\nfmt.Sprintf(\"%d\", i),\n\nbindAddr, rpcPort, dataDir, config.ACLModelFile, config.ACLPolicyFile,\n\npeerTLSConfig,\n\n}) require.NoError(t, err)\n\nagents = append(agents, agent)\n\n} defer func() {\n\nreport erratum • discuss\n\nChapter 7. Server-to-Server Service Discovery • 136\n\nfor _, agent := range agents {\n\nerr := agent.Shutdown() require.NoError(t, err) require.NoError(t,\n\nos.RemoveAll(agent.Config.DataDir),\n\n)\n\n}\n\n}() time.Sleep(3 * time.Second)\n\nThis code sets up a three-node cluster. The second and third nodes join the first node’s cluster.\n\nBecause we now have two addresses to configure in our service (the RPC address and the Serf address), and because we run our tests on a single host, we need two ports. We used the 0 port trick in Test a gRPC Server and Client, on page 68, to get a port automatically assigned to a listener by net.Listen,5 but now we just want the port—with no listener—so we use the dynaport library to allocate the two ports we need: one for our gRPC log connections and one for our Serf service discovery connections.\n\nWe defer a function call that runs after the test to verify that the agents suc- cessfully shut down and to delete the test data. We make the test sleep for a few seconds to give the nodes time to discover each other.\n\nNow that we have a cluster, we can test it works. Put this code after the pre- vious snippet:\n\nServerSideServiceDiscovery/internal/agent/agent_test.go leaderClient := client(t, agents[0], peerTLSConfig) produceResponse, err := leaderClient.Produce(\n\ncontext.Background(), &api.ProduceRequest{\n\nRecord: &api.Record{\n\nValue: []byte(\"foo\"),\n\n},\n\n},\n\n) require.NoError(t, err) consumeResponse, err := leaderClient.Consume(\n\ncontext.Background(), &api.ConsumeRequest{\n\nOffset: produceResponse.Offset,\n\n},\n\n) require.NoError(t, err) require.Equal(t, consumeResponse.Record.Value, []byte(\"foo\"))\n\n5.\n\nhttps://golang.org/pkg/net/#Listen\n\nreport erratum • discuss\n\nTest Discovery and the Service End-to-End • 137\n\nThis code is the same as our testProduceConsume() test case in Test a gRPC Server and Client, on page 68: it checks that we can produce to and consume from a single node. Now we need to check that another node replicated the record. We do that by adding this code to the test, below the previous snippet:\n\nServerSideServiceDiscovery/internal/agent/agent_test.go\n\n// wait until replication has finished time.Sleep(3 * time.Second)\n\nfollowerClient := client(t, agents[1], peerTLSConfig) consumeResponse, err = followerClient.Consume(\n\ncontext.Background(), &api.ConsumeRequest{\n\nOffset: produceResponse.Offset,\n\n},\n\n) require.NoError(t, err) require.Equal(t, consumeResponse.Record.Value, []byte(\"foo\"))\n\n}\n\nBecause our replication works asynchronously across servers, the logs pro- duced to one server won’t be immediately available on the replica servers. This process causes latency between when the message is produced to the first server and when it’s replicated to the second. The stupid, simple6 way to fix this (especially since we’re black-box testing7) is to add a big enough delay in the test for the replicator to have replicated the message, but as small a delay as possible to keep our tests fast. Then we check that we can consume the replicated message.\n\nToo Much Sleep Will Make Your Tests Too Slow\n\nIf we had enough test cases that needed a delay like this, eventu- ally our tests would be slow and annoying to run, in which case we’d want to use a different technique. For example, you could retry your test’s assertion in a loop with a small delay between iterations and timeout after a few seconds. Or you could have your server expose an event channel that included when the server produced a message. Then you’d wait to receive an event on that channel in your test so your test blocked and then continued the instant the second server replicated the message.\n\n6. 7.\n\nhttps://en.wikipedia.org/wiki/KISS_principle\n\nhttps://en.wikipedia.org/wiki/Black-box_testing\n\nreport erratum • discuss\n\nChapter 7. Server-to-Server Service Discovery • 138\n\nLastly, we need to add our client() helper that sets up a client for the service:\n\nServerSideServiceDiscovery/internal/agent/agent_test.go func client(\n\nt *testing.T, agent *agent.Agent, tlsConfig *tls.Config,\n\n) api.LogClient {\n\ntlsCreds := credentials.NewTLS(tlsConfig) opts := []grpc.DialOption{grpc.WithTransportCredentials(tlsCreds)} rpcAddr, err := agent.Config.RPCAddr() require.NoError(t, err) conn, err := grpc.Dial(fmt.Sprintf(\n\n\"%s\", rpcAddr,\n\n), opts...) require.NoError(t, err) client := api.NewLogClient(conn) return client\n\n}\n\nNow, run your tests with $ make test. If all is well, your tests pass and you’ve officially made a distributed service that can replicate data. Congrats!\n\nWhat You Learned\n\nNow when our servers discover other servers, they replicate each other’s data. That’s a problem with our replication implementation: when one server dis- covers another, they replicate each other in a cycle! You can verify it by adding this code at the bottom of your test:\n\nconsumeResponse, err = leaderClient.Consume(\n\ncontext.Background(), &api.ConsumeRequest{\n\nOffset: produceResponse.Offset + 1,\n\n},\n\n) require.Nil(t, consumeResponse) require.Error(t, err) got := grpc.Code(err) want := grpc.Code(api.ErrOffsetOutOfRange{}.GRPCStatus().Err()) require.Equal(t, got, want)\n\nWe only produced one record to our service, and yet we’re able to consume multiple records from the original server because it’s replicated data from another server that replicated its data from the original server. No, Leo, we do not need to go deeper.\n\nreport erratum • discuss\n\nWhat You Learned • 139\n\nI mentioned that in the next chapter we’ll work on coordinating the servers so that they’d have a defined leader-follower relationship so that only the followers would replicate the leader. We also want to control the number of replicas. Typically in a production deployment, three replicas is ideal: you could lose two and still not lose data, and with only three you won’t be storing more data than necessary.\n\nSo let’s work on building consensus with Raft and coordinating the nodes in our cluster.\n\nreport erratum • discuss\n\nCHAPTER 8\n\nCoordinate Your Services with Consensus\n\nDistributed services are like commercial kitchens. Imagine a small restaurant opens up with one stove and one cook. Patrons discover the restaurant and tell their friends, and business is booming. But the kitchen struggles with the mob of customers and sometimes the stove breaks, forcing the restaurant to close for the night and lose business. So the restaurant hires two more cooks and buys two more stoves. The cooks keep up with orders now but they make mistakes: they mix up appetizers and entrees; they mix up tables; they make double of one order while forgetting to make another. They lack coordi- nation. So the kitchen hires a chef to oversee and coordinate the kitchen. When an order comes in, the chef divides the order and assigns the appetizers, entrees, and deserts to the cooks who prepare the food timely and correctly. The patrons love the fast, quality service, and the kitchen becomes world- renowned.\n\nIn this chapter, we look at the chef of distributed services: consensus. Con- sensus algorithms are tools used to get distributed services to agree on shared state even in the face of failures. In Request Discovered Services and Replicate Logs, on page 123, we naively implemented replication in our service, and the servers replicate each other in a cycle, making infinite copies of the same data. We need to put the servers in leader and follower relationships where the followers replicate the leader’s data. We’ll do just that in this chapter using Raft for leader election and replication.\n\nWhat Is Raft and How Does It Work?\n\nRaft is a distributed consensus algorithm designed to be easily understood and implemented. It’s the consensus algorithm behind services like Etcd—the distributed key-value store that backs Kubernetes, Consul, and soon Kafka,\n\nreport erratum • discuss\n\nChapter 8. Coordinate Your Services with Consensus • 142\n\nwhose team is migrating from ZooKeeper to Raft.1 Because Raft is easy to understand and implement, developers have written many quality Raft libraries used in many projects and it’s become the most widely deployed consensus algorithm today.\n\nLet’s talk about Raft’s leader election first and then talk about its replication, and that’ll transition into coding replication in our service.\n\nLeader Election\n\nA Raft cluster has one leader and the rest of the servers are followers. The leader maintains power by sending heartbeat requests to its followers, effec- tively saying: “I’m still here and I’m still the boss.” If the follower times out waiting for a heartbeat request from the leader, then the follower becomes a candidate and begins an election to decide the next leader. The candidate votes for itself and then requests votes from the followers. “The boss is gone! I’m the new boss, right?” If the candidate receives a majority of the votes, it becomes the leader, and it sends heartbeat requests to the followers to establish authority: “Hey y’all, new boss here.”\n\nFollowers can become candidates simultaneously if they time out at the same time waiting for the leader’s heartbeats. They’ll hold their own elections and the elections might not result in a new leader because of vote splitting. So they’ll hold another election. Candidates will hold elections until there’s a winner that becomes the new leader.\n\nEvery Raft server has a term: a monotonically increasing integer that tells other servers how authoritative and current this server is. The servers’ terms act as a logical clock: a way to capture chronological and causal relationships in distributed systems, where real-time clocks are untrustworthy and unim- portant. Each time a candidate begins an election, it increments its term. If the candidate wins the election and becomes the leader, the followers update their terms to match and the terms don’t change until the next election. Servers vote once per term for the first candidate that requests votes, as long as the candidate’s term is greater than the voters’. These conditions help prevent vote splits and ensure the voters elect an up-to-date leader.\n\nDepending on your use case, you might use Raft just for leader election. Imagine you’ve built a job system with a database of jobs to run and a program that queries the database every second to check if there’s a job to run and, if so, runs the job. You want this system to be highly available and resilient\n\n1.\n\nhttps://cwiki.apache.org/confluence/display/KAFKA/KIP-500:+Replace+ZooKeeper+with+a+Self-Managed+Meta-\n\ndata+Quorum\n\nreport erratum • discuss\n\nWhat Is Raft and How Does It Work? • 143\n\nto failures, so you run multiple instances of the job runner. But you don’t want all of the runners running simultaneously and duplicating the work. So you use Raft to elect a leader; only the leader runs the jobs, and if the leader fails, Raft elects a new leader that runs the jobs. Most use cases rely on Raft for both its leader election and replication to get consensus on state.\n\nRaft’s leader election can be useful by itself, but usually the point is to elect a leader that’s responsible for replicating a log to its followers and doing something with the log data. Raft breaks consensus into two parts: leader election and log replication. Let’s talk about how Raft’s replication works.\n\nLog Replication\n\nThe leader accepts client requests, each of which represents some command to run across the cluster. (In a key-value service for example, you’d have a command to assign a key’s value.) For each request, the leader appends the command to its log and then requests its followers to append the command to their logs. After a majority of followers have replicated the command—when the leader considers the command committed—the leader executes the com- mand with a finite-state machine and responds to the client with the result. The leader tracks the highest committed offset and sends this in the requests to its followers. When a follower receives a request, it executes all commands up to the highest committed offset with its finite-state machine. All Raft servers run the same finite-state machine that defines how to handle each command.\n\nReplication saves us from losing data when servers fail. There’s a cost-benefit to replication. Like any insurance, replication costs (in complexity, in network bandwidth, in data storage), but the benefit of having replicated data to handle when a server fails makes it worth paying for the time the servers work. A Raft leader replicates to most of its followers, assuring that we won’t lose data unless a majority of the followers fail.\n\nThe recommended number of servers in a Raft cluster is three and five. A Raft cluster of three servers will tolerate a single server failure while a cluster of five will tolerate two server failures. I recommend odd number cluster sizes because Raft will handle (N–1)/2 failures, where N is the size of your cluster. If you had a cluster with four servers, it would handle losing one server, the same as a cluster with three servers—so you’d pay for an extra server that didn’t increase your fault tolerance. For larger clusters, CockRoachDB wrote a layer on top of Raft called MultiRaft2 that divides the database’s data into\n\n2.\n\nhttps://www.cockroachlabs.com/blog/scaling-raft\n\nreport erratum • discuss",
      "page_number": 120
    },
    {
      "number": 8,
      "title": "Coordinate Your Services with Consensus • 142",
      "start_page": 147,
      "end_page": 175,
      "detection_method": "regex_chapter_title",
      "content": "Chapter 8. Coordinate Your Services with Consensus • 144\n\nranges, each with its own consensus group. To keep our project simple, we’ll have a single Raft cluster.\n\nOur service’s use case is unique because replicating a log is our end goal. Raft’s algorithm replicates a log, and we could defer all log management to Raft’s internals. This would make our service efficient and easy to code, but wouldn’t teach you how to use Raft to build distributed services that aren’t distributed logs.\n\nIn other services, you’ll use Raft as a means to replicate a log of commands and then execute those commands with state machines. If you were building a distributed SQL database, you’d replicate and execute the insert and update SQL commands; if you were building a key-value store, you’d replicate and execute set commands. Because other services you build will replicate a log as a means rather than an end, we’ll build our service the way you would other types of service, by replicating the transformation commands—which in our service are append commands. Technically we’ll replicate two logs: the log containing Raft’s commands and the log that results from the finite-state machines applying those commands. This service may not be as optimized as it could be, but what you’ll learn will be more useful for when you build other services.\n\nImplement Raft in Our Service\n\nWe have a log that can write and read records on one computer. We want a distributed log that’s replicated on multiple computers, so let’s implement Raft in our service to get that.\n\nInstall Raft by running this command:\n\n$ go get github.com/hashicorp/raft@v1.1.1 $ # use etcd's fork of Ben Johnson's Bolt key/value store, $ # which includes fixes for Go 1.14+ $ go mod edit -replace github.com/hashicorp/raft-boltdb=\\ github.com/travisjeffery/raft-boltdb@v1.0.0\n\nIn the internal/log directory, create a distributed.go file, beginning with this snippet:\n\nCoordinateWithConsensus/internal/log/distributed.go package log\n\nimport (\n\n\"bytes\" \"crypto/tls\" \"fmt\" \"io\" \"net\" \"os\"\n\nreport erratum • discuss\n\nImplement Raft in Our Service • 145\n\n\"path/filepath\" \"time\"\n\nraftboltdb \"github.com/hashicorp/raft-boltdb\" \"google.golang.org/protobuf/proto\"\n\n\"github.com/hashicorp/raft\"\n\napi \"github.com/travisjeffery/proglog/api/v1\"\n\n)\n\ntype DistributedLog struct { config Config log raft\n\nLog *raft.Raft\n\n}\n\nfunc NewDistributedLog(dataDir string, config Config) (\n\nDistributedLog, error,\n\n) {\n\nl := &DistributedLog{\n\nconfig: config,\n\n} if err := l.setupLog(dataDir); err != nil {\n\nreturn nil, err\n\n} if err := l.setupRaft(dataDir); err != nil {\n\nreturn nil, err\n\n} return l, nil\n\n}\n\nThis code defines our distributed log type and a function to create the log. The function defers the logic to the setup methods we’ll write shortly. The log package will contain the single-server, non-replicated log we wrote earlier, and the distributed, replicated log built with Raft.\n\nWrite this setupLog() method under NewDistributedLog():\n\nCoordinateWithConsensus/internal/log/distributed.go func (l *DistributedLog) setupLog(dataDir string) error {\n\nlogDir := filepath.Join(dataDir, \"log\") if err := os.MkdirAll(logDir, 0755); err != nil {\n\nreturn err\n\n} var err error l.log, err = NewLog(logDir, l.config) return err\n\n}\n\nsetupLog(dataDir string) creates the log for this server, where this server will store the user’s records.\n\nreport erratum • discuss\n\nChapter 8. Coordinate Your Services with Consensus • 146\n\nSet Up Raft\n\nA Raft instance comprises:\n\nA finite-state machine that applies the commands you give Raft;\n\nA log store where Raft stores those commands;\n\nA stable store where Raft stores the cluster’s configuration—the servers\n\nin the cluster, their addresses, and so on;\n\nA snapshot store where Raft stores compact snapshots of its data; and\n\nA transport that Raft uses to connect with the server’s peers.\n\nWe must set these up to create a Raft instance. Below setupLog(), add this setupRaft() method:\n\nCoordinateWithConsensus/internal/log/distributed.go func (l *DistributedLog) setupRaft(dataDir string) error {\n\nfsm := &fsm{log: l.log}\n\nlogDir := filepath.Join(dataDir, \"raft\", \"log\") if err := os.MkdirAll(logDir, 0755); err != nil {\n\nreturn err\n\n} logConfig := l.config logConfig.Segment.InitialOffset = 1 logStore, err := newLogStore(logDir, logConfig) if err != nil {\n\nreturn err\n\n}\n\nsetupRaft(dataDir string) configures and creates the server’s Raft instance.\n\nWe begin by creating our finite-state machine (FSM) that we’ll implement later in this file.\n\nThen we create Raft’s log store, and we use our own log we wrote in Code the Store, on page 26! We configure our log’s initial offset to 1, as required by Raft. Raft needs a specific log interface satisfied, so we’ll wrap our log to provide those APIs (we’ll write that wrapper shortly):\n\nCoordinateWithConsensus/internal/log/distributed.go stableStore, err := raftboltdb.NewBoltStore(\n\nfilepath.Join(dataDir, \"raft\", \"stable\"),\n\n) if err != nil {\n\nreturn err\n\n}\n\nretain := 1\n\nreport erratum • discuss\n\nImplement Raft in Our Service • 147\n\nsnapshotStore, err := raft.NewFileSnapshotStore( filepath.Join(dataDir, \"raft\"), retain, os.Stderr,\n\n) if err != nil {\n\nreturn err\n\n}\n\nmaxPool := 5 timeout := 10 * time.Second transport := raft.NewNetworkTransport(\n\nl.config.Raft.StreamLayer, maxPool, timeout, os.Stderr,\n\n)\n\nThe stable store is a key-value store where Raft stores important metadata, like the server’s current term or the candidate the server voted for. Bolt3 is an embedded and persisted key-value database for Go we’ve used as our stable store.\n\nThen we set up Raft’s snapshot store. Raft snapshots to recover and restore data efficiently, when necessary, like if your server’s EC2 instance failed and an autoscaling group brought up another instance for the Raft server. Rather than streaming all the data from the Raft leader, the new server would restore from the snapshot (which you could store in S3 or a similar storage service) and then get the latest changes from the leader. This is more efficient and less taxing on the leader. You want to snapshot frequently to minimize the difference between the data in the snapshots and on the leader. The retain variable specifies that we’ll keep one snapshot.\n\nWe create our transport that wraps a stream layer—a low-level stream abstraction (we’ll write our own stream layer implementation in Stream Layer, on page 156):\n\nCoordinateWithConsensus/internal/log/distributed.go config := raft.DefaultConfig() config.LocalID = l.config.Raft.LocalID if l.config.Raft.HeartbeatTimeout != 0 {\n\nconfig.HeartbeatTimeout = l.config.Raft.HeartbeatTimeout\n\n} if l.config.Raft.ElectionTimeout != 0 {\n\nconfig.ElectionTimeout = l.config.Raft.ElectionTimeout\n\n}\n\n3.\n\nhttps://github.com/boltdb/bolt\n\nreport erratum • discuss\n\nChapter 8. Coordinate Your Services with Consensus • 148\n\nif l.config.Raft.LeaderLeaseTimeout != 0 {\n\nconfig.LeaderLeaseTimeout = l.config.Raft.LeaderLeaseTimeout\n\n} if l.config.Raft.CommitTimeout != 0 {\n\nconfig.CommitTimeout = l.config.Raft.CommitTimeout\n\n}\n\nThe config’s LocalID field is the unique ID for this server, and it’s the only config field we must set; the rest are optional, and in normal operation the default config should be fine.\n\nTo make our tests faster, we support overriding a handful of timeout configs to speed up Raft. For example, when we shut down the leader, we want the election to finish within a second, whereas in production you’d need a longer timeout to handle networking latency.\n\nAdd the following code to create the Raft instance and bootstrap the cluster:\n\nCoordinateWithConsensus/internal/log/distributed.go\n\nl.raft, err = raft.NewRaft(\n\nconfig, fsm, logStore, stableStore, snapshotStore, transport,\n\n) if err != nil {\n\nreturn err\n\n} hasState, err := raft.HasExistingState(\n\nlogStore, stableStore, snapshotStore,\n\n) if err != nil {\n\nreturn err\n\n} if l.config.Raft.Bootstrap && !hasState {\n\nconfig := raft.Configuration{\n\nServers: []raft.Server{{\n\nID: Address: transport.LocalAddr(),\n\nconfig.LocalID,\n\n}},\n\n} err = l.raft.BootstrapCluster(config).Error()\n\n} return err\n\n}\n\nreport erratum • discuss\n\nImplement Raft in Our Service • 149\n\nTo support configuring Raft, add these highlighted lines to your log’s Config struct in internal/log/config.go:\n\nCoordinateWithConsensus/internal/log/config.go package log\n\n➤ ➤ )➤\n\nimport (\n\n\"github.com/hashicorp/raft\"\n\ntype Config struct {\n\n➤ ➤ ➤ ➤ ➤\n\nRaft struct {\n\nraft.Config StreamLayer *StreamLayer Bootstrap\n\nbool\n\n} Segment struct {\n\nMaxStoreBytes uint64 MaxIndexBytes uint64 InitialOffset uint64\n\n}\n\n}\n\nGenerally you’ll bootstrap a server configured with itself as the only voter, wait until it becomes the leader, and then tell the leader to add more servers to the cluster. The subsequently added servers don’t bootstrap. That concludes our Raft setup. Let’s continue building our DistributedLog.\n\nLog API\n\nWe’ve written the code to set up a DistributedLog; next we’ll write its public APIs that append records to and read records from the log and wrap Raft. The Dis- tributedLog will have the same API as the Log type to make them interchangeable.\n\nAdd this Append() method below setupRaft():\n\nCoordinateWithConsensus/internal/log/distributed.go func (l *DistributedLog) Append(record *api.Record) (uint64, error) {\n\nres, err := l.apply(\n\nAppendRequestType, &api.ProduceRequest{Record: record},\n\n) if err != nil {\n\nreturn 0, err\n\n} return res.(*api.ProduceResponse).Offset, nil\n\n}\n\nAppend(record*api.Record) appends the record to the log. Unlike in Code the Store, on page 26, where we appended the record directly to this server’s log, we tell\n\nreport erratum • discuss\n\nChapter 8. Coordinate Your Services with Consensus • 150\n\nRaft to apply a command (we’ve reused for the ProduceRequest for the command) that tells the FSM to append the record to the log. Raft runs the process described in Log Replication, on page 143, to replicate the command to a majority of the Raft servers and ultimately append the record to a majority of Raft servers.\n\nPut this apply() method below Apply():\n\nCoordinateWithConsensus/internal/log/distributed.go func (l *DistributedLog) apply(reqType RequestType, req proto.Message) (\n\ninterface{}, error,\n\n) {\n\nvar buf bytes.Buffer _, err := buf.Write([]byte{byte(reqType)}) if err != nil {\n\nreturn nil, err\n\n} b, err := proto.Marshal(req) if err != nil {\n\nreturn nil, err\n\n} _, err = buf.Write(b) if err != nil {\n\nreturn nil, err\n\n} timeout := 10 * time.Second future := l.raft.Apply(buf.Bytes(), timeout) if future.Error() != nil {\n\nreturn nil, future.Error()\n\n} res := future.Response() if err, ok := res.(error); ok { return nil, err\n\n} return res, nil\n\n}\n\napply(reqType RequestType, req proto.Marshaler) wraps Raft’s API to apply requests and return their responses. Even though we have only one request type, the append request type, I’ve written things that easily support multiple request types to show how you would set up your own services when you have different requests. In apply(), we marshal the request type and request into bytes that Raft uses as the record’s data it replicates. The l.raft.Apply(buf.Bytes(), timeout) call has a lot going on behind the scenes, running the steps described in Log Replication, on page 143, to replicate the record and append the record to the leader’s log.\n\nreport erratum • discuss\n\nImplement Raft in Our Service • 151\n\nThe future.Error() API returns an error when something went wrong with Raft’s replication. For example, it took too long for Raft to process the command or the server had to shutdown—the future.Error() API doesn’t return your service’s errors. The future.Response() API returns what your FSM’s Apply() method returned and, opposed to Go’s convention of using Go’s multiple return values to sep- arate errors, you must return a single value for Raft. In our apply() method we check whether the value is an error with a type assertion.\n\nPut this Read() method below apply():\n\nCoordinateWithConsensus/internal/log/distributed.go func (l *DistributedLog) Read(offset uint64) (*api.Record, error) {\n\nreturn l.log.Read(offset)\n\n}\n\nRead(offset uint64) reads the record for the offset from the server’s log. When you’re okay with relaxed consistency, read operations need not go through Raft. When you need strong consistency, where reads must be up-to-date with writes, then you must go through Raft, but then reads are less efficient and take longer.\n\nFinite-State Machine\n\nRaft defers the running of your business logic to the FSM. After the previous snippet, define your fsm type with this code:\n\nCoordinateWithConsensus/internal/log/distributed.go var _ raft.FSM = (*fsm)(nil)\n\ntype fsm struct {\n\nlog *Log\n\n}\n\nThe FSM must access the data it manages. In our service, that’s a log, and the FSM appends records to the log. If you were writing a key-value service, then your FSM would update the store of your data: an int, a map, Postgres —whatever store you’ve used.\n\nYour FSM must implement three methods:\n\nApply(record *raft.Log)—Raft invokes this method after committing a log entry.\n\nSnapshot()—Raft periodically calls this method to snapshot its state. For most services, you’ll be able to build a compacted log—for example, if we were building a key-value store and we had a bunch of commands saying “set foo to bar,” “set foo to baz,” “set foo to qux,” and so on, we would only set the latest command to restore the current state. Because we’re repli- cating a log itself, we need the full log to restore it.\n\nreport erratum • discuss\n\nChapter 8. Coordinate Your Services with Consensus • 152\n\nRestore(io.ReadCloser)—Raft calls this to restore an FSM from a snapshot—for instance, if an EC2 instance failed and a new instance took its place.\n\nPut this code below the fsm type to implement Apply():\n\nCoordinateWithConsensus/internal/log/distributed.go type RequestType uint8\n\nconst (\n\nAppendRequestType RequestType = 0\n\n)\n\nfunc (l *fsm) Apply(record *raft.Log) interface{} {\n\nbuf := record.Data reqType := RequestType(buf[0]) switch reqType { case AppendRequestType:\n\nreturn l.applyAppend(buf[1:])\n\n} return nil\n\n}\n\nfunc (l *fsm) applyAppend(b []byte) interface{} {\n\nvar req api.ProduceRequest err := proto.Unmarshal(b, &req) if err != nil {\n\nreturn err\n\n} offset, err := l.log.Append(req.Record) if err != nil {\n\nreturn err\n\n} return &api.ProduceResponse{Offset: offset}\n\n}\n\nAs I mentioned earlier, even though our service has only one command to replicate, I want to develop things to support multiple commands and show you how to do it for your own projects. So in this snippet, we make our own request type and define our append request type. When we send a request to Raft for it to apply, and when we read the request in the FSM’s Apply() method to apply it, these request types identify the request and tell us how to handle it. In Apply(), we switch on the request type and call the corresponding method containing the logic to run the command. In applyAppend([]byte), we unmarshal the request and then append the record to the local log and return the response for Raft to send back to where we called raft.Apply() in Distributed- Log.Append().\n\nreport erratum • discuss\n\nImplement Raft in Our Service • 153\n\nBelow applyAppend(), put this snippet to support snapshots:\n\nCoordinateWithConsensus/internal/log/distributed.go func (f *fsm) Snapshot() (raft.FSMSnapshot, error) {\n\nr := f.log.Reader() return &snapshot{reader: r}, nil\n\n}\n\nvar _ raft.FSMSnapshot = (*snapshot)(nil)\n\ntype snapshot struct {\n\nreader io.Reader\n\n}\n\nfunc (s *snapshot) Persist(sink raft.SnapshotSink) error { if _, err := io.Copy(sink, s.reader); err != nil {\n\n_ = sink.Cancel() return err\n\n} return sink.Close()\n\n}\n\nfunc (s *snapshot) Release() {}\n\nSnapshot() returns an FSMSnapshot that represents a point-in-time snapshot of the FSM’s state. In our case that state is our FSM’s log, so call Reader() to return an io.Reader that will read all the log’s data.\n\nThese snapshots serve two purposes: they allow Raft to compact its log so it doesn’t store logs whose commands Raft has applied already. And they allow Raft to bootstrap new servers more efficiently than if the leader had to replicate its entire log again and again.\n\nRaft calls Snapshot() according to your configured SnapshotInterval (how often Raft checks if it should snapshot—default is two minutes) and SnapshotThreshold (how many logs since the last snapshot before making a new snapshot—default is 8192).\n\nRaft calls Persist() on the FSMSnapshot we created to write its state to some sink that, depending on the snapshot store you configured Raft with, could be in- memory, a file, an S3 bucket—something to store the bytes in. We’re using the file snapshot store so that when the snapshot completes, we’ll have a file containing all the Raft’s log data. A shared state store such as S3 would put the burden of writing and reading the snapshot on S3 rather than the leader and allow new servers to restore snapshots without streaming from the leader. Raft calls Release() when it’s finished with the snapshot.\n\nreport erratum • discuss\n\nChapter 8. Coordinate Your Services with Consensus • 154\n\nPut this Restore() method below Release():\n\nCoordinateWithConsensus/internal/log/distributed.go func (f *fsm) Restore(r io.ReadCloser) error {\n\nb := make([]byte, lenWidth) var buf bytes.Buffer for i := 0; ; i++ {\n\n_, err := io.ReadFull(r, b) if err == io.EOF {\n\nbreak } else if err != nil {\n\nreturn err\n\n} size := int64(enc.Uint64(b)) if _, err = io.CopyN(&buf, r, size); err != nil {\n\nreturn err\n\n} record := &api.Record{} if err = proto.Unmarshal(buf.Bytes(), record); err != nil {\n\nreturn err\n\n} if i == 0 {\n\nf.log.Config.Segment.InitialOffset = record.Offset if err := f.log.Reset(); err != nil {\n\nreturn err\n\n}\n\n} if _, err = f.log.Append(record); err != nil {\n\nreturn err\n\n} buf.Reset()\n\n} return nil\n\n}\n\nRaft calls Restore() to restore an FSM from a snapshot. For example, if we lost a server and scaled up a new one, we’d want to restore its FSM. The FSM must discard existing state to make sure its state will match the leader’s replicated state.\n\nIn our Restore() implementation, we reset the log and configure its initial offset to the first record’s offset we read from the snapshot so the log’s offsets match. Then we read the records in the snapshot and append them to our new log.\n\nThat’s it for our FSM code.\n\nNext, put this snippet below the FSM to define Raft’s log store:\n\nCoordinateWithConsensus/internal/log/distributed.go var _ raft.LogStore = (*logStore)(nil)\n\nreport erratum • discuss\n\nImplement Raft in Our Service • 155\n\ntype logStore struct {\n\nLog\n\n}\n\nfunc newLogStore(dir string, c Config) (*logStore, error) {\n\nlog, err := NewLog(dir, c) if err != nil {\n\nreturn nil, err\n\n} return &logStore{log}, nil\n\n}\n\nRaft calls your FSM’s Apply() method with *raft.Log’s read from its managed log store. Raft replicates a log and then calls your state machine with the log’s records. We’re using our own log as Raft’s log store, but we need to wrap our log to satisfy the LogStore interface Raft requires. In this snippet, we’ve defined our log store and a function to create it.\n\nBelow newLogStore() add this snippet:\n\nCoordinateWithConsensus/internal/log/distributed.go func (l *logStore) FirstIndex() (uint64, error) {\n\nreturn l.LowestOffset()\n\n}\n\nfunc (l *logStore) LastIndex() (uint64, error) {\n\noff, err := l.HighestOffset() return off, err\n\n}\n\nfunc (l *logStore) GetLog(index uint64, out *raft.Log) error {\n\nin, err := l.Read(index) if err != nil {\n\nreturn err\n\n} out.Data = in.Value out.Index = in.Offset out.Type = raft.LogType(in.Type) out.Term = in.Term return nil\n\n}\n\nRaft uses these APIs to get records and information about the log. We support the functionality on our log already and just needed to wrap our existing methods. What we call offsets, Raft calls indexes.\n\nPut the following snippet below GetLog():\n\nCoordinateWithConsensus/internal/log/distributed.go func (l *logStore) StoreLog(record *raft.Log) error {\n\nreturn l.StoreLogs([]*raft.Log{record})\n\n}\n\nreport erratum • discuss\n\nChapter 8. Coordinate Your Services with Consensus • 156\n\nfunc (l *logStore) StoreLogs(records []*raft.Log) error {\n\nfor _, record := range records {\n\nif _, err := l.Append(&api.Record{ Value: record.Data, Term: record.Term, Type: uint32(record.Type),\n\n}); err != nil {\n\nreturn err\n\n}\n\n} return nil\n\n}\n\nRaft uses these APIs to append records to its log. Again, we just translate the call to our log’s API and our record type. These changes require adding some fields to our Record type.\n\nChange your Record message in api/v1/log.proto to the following:\n\nCoordinateWithConsensus/api/v1/log.proto message Record {\n\nbytes value = 1; uint64 offset = 2; uint64 term = 3; uint32 type = 4;\n\n}\n\nThen compile your protobuf by running $ make compile.\n\nThe last method on the logStore is a method to delete old records. Below StoreLogs(), put this DeleteRange() method:\n\nCoordinateWithConsensus/internal/log/distributed.go func (l *logStore) DeleteRange(min, max uint64) error {\n\nreturn l.Truncate(max)\n\n}\n\nDeleteRange(min, max uint64) removes the records between the offsets—it’s to remove records that are old or stored in a snapshot.\n\nStream Layer\n\nRaft uses a stream layer in the transport to provide a low-level stream abstraction to connect with Raft servers. Our stream layer must satisfy Raft’s StreamLayer interface:\n\ntype StreamLayer interface {\n\nnet.Listener // Dial is used to create a new outgoing connection Dial(address ServerAddress, timeout time.Duration) (net.Conn, error)\n\n}\n\nreport erratum • discuss\n\nImplement Raft in Our Service • 157\n\nAdd this snippet at the bottom of distributed.go to begin your StreamLayer:\n\nCoordinateWithConsensus/internal/log/distributed.go var _ raft.StreamLayer = (*StreamLayer)(nil)\n\ntype StreamLayer struct {\n\nln serverTLSConfig *tls.Config *tls.Config peerTLSConfig\n\nnet.Listener\n\n}\n\nfunc NewStreamLayer(\n\nln net.Listener, serverTLSConfig, peerTLSConfig *tls.Config,\n\n) *StreamLayer {\n\nreturn &StreamLayer{ ln: serverTLSConfig: serverTLSConfig, peerTLSConfig:\n\nln,\n\npeerTLSConfig,\n\n}\n\n}\n\nThis snippet defines the StreamLayer type and checks that it satisfies the raft.Stream- Layer interface. We want to enable encrypted communication between servers with TLS, so we need to take in the TLS configs used to accept incoming connections (the serverTLSConfig) and create outgoing connections (the peerTLSConfig).\n\nBelow NewStreamLayer(), add this Dial() method and RaftRPC constant:\n\nCoordinateWithConsensus/internal/log/distributed.go const RaftRPC = 1\n\nfunc (s *StreamLayer) Dial(\n\naddr raft.ServerAddress, timeout time.Duration,\n\n) (net.Conn, error) {\n\ndialer := &net.Dialer{Timeout: timeout} var conn, err = dialer.Dial(\"tcp\", string(addr)) if err != nil {\n\nreturn nil, err\n\n} // identify to mux this is a raft rpc _, err = conn.Write([]byte{byte(RaftRPC)}) if err != nil {\n\nreturn nil, err\n\n} if s.peerTLSConfig != nil {\n\nconn = tls.Client(conn, s.peerTLSConfig)\n\n} return conn, err\n\n}\n\nreport erratum • discuss\n\nChapter 8. Coordinate Your Services with Consensus • 158\n\nDial(addr raft.ServerAddress, timeout time.Duration) makes outgoing connections to other servers in the Raft cluster. When we connect to a server, we write the RaftRPC byte to identify the connection type so we can multiplex Raft on the same port as our Log gRPC requests. (We’ll take a look at multiplexing shortly.) If we configure the stream layer with a peer TLS config, we make a TLS client-side connection.\n\nThe rest of the methods on the stream layer implement the net.Listener interface. Below Dial() add this snippet:\n\nCoordinateWithConsensus/internal/log/distributed.go func (s *StreamLayer) Accept() (net.Conn, error) {\n\nconn, err := s.ln.Accept() if err != nil {\n\nreturn nil, err\n\n} b := make([]byte, 1) _, err = conn.Read(b) if err != nil {\n\nreturn nil, err\n\n} if bytes.Compare([]byte{byte(RaftRPC)}, b) != 0 {\n\nreturn nil, fmt.Errorf(\"not a raft rpc\")\n\n} if s.serverTLSConfig != nil {\n\nreturn tls.Server(conn, s.serverTLSConfig), nil\n\n} return conn, nil\n\n}\n\nfunc (s *StreamLayer) Close() error {\n\nreturn s.ln.Close()\n\n}\n\nfunc (s *StreamLayer) Addr() net.Addr {\n\nreturn s.ln.Addr()\n\n}\n\nAccept() is the mirror of Dial(). We accept the incoming connection and read the byte that identifies the connection and then create a server-side TLS connec- tion. Close() closes the listener. Addr() returns the listener’s address.\n\nDiscovery Integration\n\nThe next step to implement Raft in our service is to integrate our Serf-driven discovery layer with Raft to make the corresponding change in our Raft cluster when the Serf membership changes. Each time you add a server to the cluster, Serf will publish an event saying a member joined, and our discov- ery.Membership will call its handler’s Join(id, addr string) method. When a server\n\nreport erratum • discuss\n\nImplement Raft in Our Service • 159\n\nleaves the cluster, Serf will publish an event saying a member left, and our discovery.Membership will call its handler’s Leave(id string) method. Our distributed log will act as our Membership’s handler, so we need to implement those Join() and Leave() methods to update Raft.\n\nAdd this snippet below DistributedLog.Read(offset uint64) method:\n\nCoordinateWithConsensus/internal/log/distributed.go func (l *DistributedLog) Join(id, addr string) error {\n\nconfigFuture := l.raft.GetConfiguration() if err := configFuture.Error(); err != nil {\n\nreturn err\n\n} serverID := raft.ServerID(id) serverAddr := raft.ServerAddress(addr) for _, srv := range configFuture.Configuration().Servers {\n\nif srv.ID == serverID || srv.Address == serverAddr {\n\nif srv.ID == serverID && srv.Address == serverAddr {\n\n// server has already joined return nil\n\n} // remove the existing server removeFuture := l.raft.RemoveServer(serverID, 0, 0) if err := removeFuture.Error(); err != nil {\n\nreturn err\n\n}\n\n}\n\n} addFuture := l.raft.AddVoter(serverID, serverAddr, 0, 0) if err := addFuture.Error(); err != nil {\n\nreturn err\n\n} return nil\n\n}\n\nfunc (l *DistributedLog) Leave(id string) error {\n\nremoveFuture := l.raft.RemoveServer(raft.ServerID(id), 0, 0) return removeFuture.Error()\n\n}\n\nJoin(id, addr string) adds the server to the Raft cluster. We add every server as a voter, but Raft supports adding servers as non-voters with the AddNonVoter() API. You’d find non-voter servers useful if you wanted to replicate state to many servers to serve read only eventually consistent state. Each time you add more voter servers, you increase the probability that replications and elections will take longer because the leader has more servers it needs to communicate with to reach a majority.\n\nreport erratum • discuss\n\nChapter 8. Coordinate Your Services with Consensus • 160\n\nLeave(id string) removes the server from the cluster. Removing the leader will trigger a new election.\n\nRaft will error and return ErrNotLeader when you try to change the cluster on non-leader nodes. In our service discovery code we log all handler errors as critical, but if the node is a non-leader, then we should expect these errors and not log them. In internal/discovery/membership.go, import github.com/hashicorp/raft and update your logError() method to this:\n\nCoordinateWithConsensus/internal/discovery/membership.go func (m *Membership) logError(err error, msg string, member serf.Member) {\n\nlog := m.logger.Error if err == raft.ErrNotLeader {\n\nlog = m.logger.Debug\n\n} log(\n\nmsg, zap.Error(err), zap.String(\"name\", member.Name), zap.String(\"rpc_addr\", member.Tags[\"rpc_addr\"]),\n\n)\n\n}\n\nlogError() will log the non-leader errors at the debug level now, and logs like these would be good candidates for removal.\n\nGo back to internal/log/distributed.go and add this WaitForLeader() method below Leave():\n\nCoordinateWithConsensus/internal/log/distributed.go func (l *DistributedLog) WaitForLeader(timeout time.Duration) error {\n\ntimeoutc := time.After(timeout) ticker := time.NewTicker(time.Second) defer ticker.Stop() for {\n\nselect { case <-timeoutc:\n\nreturn fmt.Errorf(\"timed out\")\n\ncase <-ticker.C:\n\nif l := l.raft.Leader(); l != \"\" {\n\nreturn nil\n\n}\n\n}\n\n}\n\n}\n\nWaitForLeader(timeout time.Duration) blocks until the cluster has elected a leader or times out. It’s useful when writing tests because, as we’ve discussed, most operations must run on the leader.\n\nreport erratum • discuss\n\nImplement Raft in Our Service • 161\n\nPut our last method on the DistributedLog under WaitForLeader():\n\nCoordinateWithConsensus/internal/log/distributed.go func (l *DistributedLog) Close() error {\n\nf := l.raft.Shutdown() if err := f.Error(); err != nil {\n\nreturn err\n\n} return l.log.Close()\n\n}\n\nClose() shuts down the Raft instance and closes the local log. And that wraps up the method on our DistributedLog. Now we’ll build out the pieces that the distributed log and Raft depend on, beginning with the FSM.\n\nTest the Distributed Log\n\nNow let’s test our distributed log. In the internal/log directory create a distribut- ed_test.go file, beginning with this code:\n\nCoordinateWithConsensus/internal/log/distributed_test.go package log_test\n\nimport (\n\n\"fmt\" \"io/ioutil\" \"net\" \"os\" \"reflect\" \"testing\" \"time\"\n\n\"github.com/hashicorp/raft\" \"github.com/stretchr/testify/require\" \"github.com/travisjeffery/go-dynaport\" api \"github.com/travisjeffery/proglog/api/v1\" \"github.com/travisjeffery/proglog/internal/log\"\n\n)\n\nfunc TestMultipleNodes(t *testing.T) { var logs []*log.DistributedLog nodeCount := 3 ports := dynaport.Get(nodeCount)\n\nfor i := 0; i < nodeCount; i++ {\n\ndataDir, err := ioutil.TempDir(\"\", \"distributed-log-test\") require.NoError(t, err) defer func(dir string) {\n\n_ = os.RemoveAll(dir)\n\n}(dataDir)\n\nreport erratum • discuss\n\nChapter 8. Coordinate Your Services with Consensus • 162\n\nln, err := net.Listen( \"tcp\", fmt.Sprintf(\"127.0.0.1:%d\", ports[i]),\n\n) require.NoError(t, err)\n\nconfig := log.Config{} config.Raft.StreamLayer = log.NewStreamLayer(ln, nil, nil) config.Raft.LocalID = raft.ServerID(fmt.Sprintf(\"%d\", i)) config.Raft.HeartbeatTimeout = 50 * time.Millisecond config.Raft.ElectionTimeout = 50 * time.Millisecond config.Raft.LeaderLeaseTimeout = 50 * time.Millisecond config.Raft.CommitTimeout = 5 * time.Millisecond\n\nTo begin TestMultipleServers(*testing.T), we set up a three-server cluster. We shorten the default Raft timeout configs so that Raft elects the leader quickly.\n\nBelow the previous code, add this snippet:\n\nCoordinateWithConsensus/internal/log/distributed_test.go\n\nif i == 0 {\n\nconfig.Raft.Bootstrap = true\n\n}\n\nl, err := log.NewDistributedLog(dataDir, config) require.NoError(t, err)\n\nif i != 0 {\n\nerr = logs[0].Join(\n\nfmt.Sprintf(\"%d\", i), ln.Addr().String(),\n\n) require.NoError(t, err)\n\n} else {\n\nerr = l.WaitForLeader(3 * time.Second) require.NoError(t, err)\n\n}\n\nlogs = append(logs, l)\n\n}\n\nThe first server bootstraps the cluster, becomes the leader, and adds the other two servers to the cluster. The leader then must join other servers to its cluster.\n\nBelow the previous snippet, add this code:\n\nCoordinateWithConsensus/internal/log/distributed_test.go records := []*api.Record{\n\n{Value: []byte(\"first\")}, {Value: []byte(\"second\")},\n\n} for _, record := range records {\n\noff, err := logs[0].Append(record) require.NoError(t, err)\n\nreport erratum • discuss\n\nMultiplex to Run Multiple Services on One Port • 163\n\nrequire.Eventually(t, func() bool {\n\nfor j := 0; j < nodeCount; j++ {\n\ngot, err := logs[j].Read(off) if err != nil {\n\nreturn false\n\n} record.Offset = off if !reflect.DeepEqual(got.Value, record.Value) {\n\nreturn false\n\n}\n\n} return true\n\n}, 500*time.Millisecond, 50*time.Millisecond)\n\n}\n\nWe test our replication by appending some records to our leader server and check that Raft replicated the records to its followers. The Raft followers will apply the append message after a short latency, so we use testify’s Eventually() method to give Raft enough time to finish replicating.\n\nNow, finish the test by adding the following snippet:\n\nCoordinateWithConsensus/internal/log/distributed_test.go err := logs[0].Leave(\"1\") require.NoError(t, err)\n\ntime.Sleep(50 * time.Millisecond)\n\noff, err := logs[0].Append(&api.Record{ Value: []byte(\"third\"),\n\n}) require.NoError(t, err)\n\ntime.Sleep(50 * time.Millisecond)\n\nrecord, err := logs[1].Read(off) require.IsType(t, api.ErrOffsetOutOfRange{}, err) require.Nil(t, record)\n\nrecord, err = logs[2].Read(off) require.NoError(t, err) require.Equal(t, []byte(\"third\"), record.Value) require.Equal(t, off, record.Offset)\n\n}\n\nThis code checks that the leader stops replicating to a server that’s left the cluster, while continuing to replicate to the existing servers.\n\nMultiplex to Run Multiple Services on One Port\n\nMultiplexing allows you to serve different services on the same port. This makes your service easier to use: there’s less documentation, less configuration, and\n\nreport erratum • discuss\n\nChapter 8. Coordinate Your Services with Consensus • 164\n\nfewer connections to manage. And you can serve multiple services even when a firewall constrains you to one port. There’s a slight perf hit on each new connection because the multiplexer reads the first bytes to identify the connection, but for long-lived connections that performance hit is negligible. And you must be careful you don’t accidentally expose a service.\n\nMany distributed services that use Raft multiplex Raft with other services, like an RPC service. Running gRPC with mutual TLS makes multiplexing tricky because we want to multiplex the connection after the TLS handshake. Before the handshake, we can’t differentiate the connections; we just know they’re both TLS connections. We need to handshake and see the decrypted packets to know more. After the handshake, we can read the connection’s packets to determine whether the connection is a gRPC or Raft connection. The issue with multiplexing mutual TLS gRPC connections is that gRPC needs information taken during the handshake to authenticate clients later on. So we have to multiplex before the handshake and need to make a way to iden- tify Raft from gRPC connections.\n\nWe identify the Raft connections from the gRPC connections by making the Raft connections write a byte to identify them by. We write the number 1 as the first byte of our Raft connections to separate them from the gRPC connec- tions. If we had other services, we could differentiate them from gRPC by passing a custom dialer to the gRPC client to send the number 2 as the first byte. The TLS standards4 don’t assign multiplexing schemes to the values 0–19, saying that they “require coordination,” like we’ve done. It’s better to handle internal services specially because you control the clients and can make them write whatever you need to identify them.\n\nLet’s update our agent to multiplex its Raft and gRPC connections and create a distributed log.\n\nUpdate your imports in internal/agent/agent.go to the following:\n\nCoordinateWithConsensus/internal/agent/agent.go import (\n\n\"bytes\" \"crypto/tls\" \"fmt\" \"io\" \"net\" \"sync\" \"time\"\n\n4.\n\nhttps://tools.ietf.org/html/rfc7983\n\nreport erratum • discuss\n\n➤\n\nMultiplex to Run Multiple Services on One Port • 165\n\n\"go.uber.org/zap\" \"github.com/hashicorp/raft\" \"github.com/soheilhy/cmux\" \"google.golang.org/grpc\" \"google.golang.org/grpc/credentials\"\n\n\"github.com/travisjeffery/proglog/internal/auth\" \"github.com/travisjeffery/proglog/internal/discovery\" \"github.com/travisjeffery/proglog/internal/log\" \"github.com/travisjeffery/proglog/internal/server\"\n\n)\n\nAnd then update your Agent type to this definition:\n\nCoordinateWithConsensus/internal/agent/agent.go type Agent struct {\n\nConfig Config\n\nmux log server membership *discovery.Membership\n\ncmux.CMux *log.DistributedLog *grpc.Server\n\nshutdown shutdowns shutdownLock sync.Mutex\n\nbool chan struct{}\n\n}\n\nHere we’ve added the mux cmux.CMux field, changed the log to a DistributedLog, and removed the replicator.\n\nAdd this field to your Config struct to enable bootstrapping the Raft cluster:\n\nCoordinateWithConsensus/internal/agent/agent.go Bootstrap bool\n\nIn the New() function, add the highlighted code to set up the mux (short for multiplexer):\n\nCoordinateWithConsensus/internal/agent/agent.go setup := []func() error {\n\na.setupLogger, a.setupMux, a.setupLog, a.setupServer, a.setupMembership,\n\n}\n\nThen put setupMux() after the New() function:\n\nreport erratum • discuss\n\nChapter 8. Coordinate Your Services with Consensus • 166\n\nCoordinateWithConsensus/internal/agent/agent.go func (a *Agent) setupMux() error {\n\nrpcAddr := fmt.Sprintf( \":%d\", a.Config.RPCPort,\n\n) ln, err := net.Listen(\"tcp\", rpcAddr) if err != nil {\n\nreturn err\n\n} a.mux = cmux.New(ln) return nil\n\n}\n\nsetupMux() creates a listener on our RPC address that’ll accept both Raft and gRPC connections and then creates the mux with the listener. The mux will accept connections on that listener and match connections based on your configured rules.\n\nLet’s update setupLog() to configure the rule to match Raft and create the dis- tributed log. Replace your existing setupLog() method and put this snippet in its place:\n\nCoordinateWithConsensus/internal/agent/agent.go func (a *Agent) setupLog() error {\n\nraftLn := a.mux.Match(func(reader io.Reader) bool {\n\nb := make([]byte, 1) if _, err := reader.Read(b); err != nil {\n\nreturn false\n\n} return bytes.Compare(b, []byte{byte(log.RaftRPC)}) == 0\n\n})\n\nIn this snippet, we configure the mux that matches Raft connections. We identify Raft connections by reading one byte and checking that the byte matches the byte we set up our outgoing Raft connections to write in Stream Layer, on page 156:\n\nCoordinateWithConsensus/internal/log/distributed.go // identify to mux this is a raft rpc _, err = conn.Write([]byte{byte(RaftRPC)}) if err != nil {\n\nreturn nil, err\n\n}\n\nIf the mux matches this rule, it will pass the connection to the raftLn listener for Raft to handle the connection. Add the rest of setupLog() after the previous snippet:\n\nreport erratum • discuss\n\n➤ ➤\n\nMultiplex to Run Multiple Services on One Port • 167\n\nCoordinateWithConsensus/internal/agent/agent.go\n\nlogConfig := log.Config{} logConfig.Raft.StreamLayer = log.NewStreamLayer(\n\nraftLn, a.Config.ServerTLSConfig, a.Config.PeerTLSConfig,\n\n) logConfig.Raft.LocalID = raft.ServerID(a.Config.NodeName) logConfig.Raft.Bootstrap = a.Config.Bootstrap var err error a.log, err = log.NewDistributedLog(\n\na.Config.DataDir, logConfig,\n\n) if err != nil {\n\nreturn err\n\n} if a.Config.Bootstrap {\n\nerr = a.log.WaitForLeader(3 * time.Second)\n\n} return err\n\n}\n\nWe configure the distributed log’s Raft to use our multiplexed listener and then configure and create the distributed log.\n\nUpdate your gRPC server to use the mux’s listener by updating setupServer() to the following:\n\nCoordinateWithConsensus/internal/agent/agent.go func (a *Agent) setupServer() error {\n\nauthorizer := auth.New(\n\na.Config.ACLModelFile, a.Config.ACLPolicyFile,\n\n) serverConfig := &server.Config{\n\nCommitLog: a.log, Authorizer: authorizer,\n\n} var opts []grpc.ServerOption if a.Config.ServerTLSConfig != nil {\n\ncreds := credentials.NewTLS(a.Config.ServerTLSConfig) opts = append(opts, grpc.Creds(creds))\n\n} var err error a.server, err = server.NewGRPCServer(serverConfig, opts...) if err != nil {\n\nreturn err\n\n} grpcLn := a.mux.Match(cmux.Any()) go func() {\n\nreport erratum • discuss\n\n➤ ➤ ➤ ➤ ➤\n\nChapter 8. Coordinate Your Services with Consensus • 168\n\nif err := a.server.Serve(grpcLn); err != nil {\n\n_ = a.Shutdown()\n\n}\n\n}() return err\n\n}\n\nBecause we’ve multiplexed two connection types (Raft and gRPC) and we added a matcher for the Raft connections, we know all other connections must be gRPC connections. We use cmux.Any() because it matches any connec- tions. Then we tell our gRPC server to serve on the multiplexed listener.\n\nReplace your setupMembership() method with the following:\n\nCoordinateWithConsensus/internal/agent/agent.go func (a *Agent) setupMembership() error {\n\nrpcAddr, err := a.Config.RPCAddr() if err != nil {\n\nreturn err\n\n} a.membership, err = discovery.New(a.log, discovery.Config{\n\nNodeName: a.Config.NodeName, BindAddr: a.Config.BindAddr, Tags: map[string]string{\n\n\"rpc_addr\": rpcAddr,\n\n}, StartJoinAddrs: a.Config.StartJoinAddrs,\n\n}) return err\n\n}\n\nOur DistributedLog handles coordinated replication, thanks to Raft, so we don’t need the Replicator anymore. Now we need the Membership to tell the DistributedLog when servers join or leave the cluster. Delete the a.replicator.Close line in Shutdown() and delete the internal/log/replicator.go file too. All that’s left is to tell our mux to serve connections. Above the return statement in New(), add this line:\n\nCoordinateWithConsensus/internal/agent/agent.go go a.serve()\n\nAnd then put serve() at the bottom of the file:\n\nCoordinateWithConsensus/internal/agent/agent.go func (a *Agent) serve() error {\n\nif err := a.mux.Serve(); err != nil {\n\n_ = a.Shutdown() return err\n\n} return nil\n\n}\n\nreport erratum • discuss\n\nWhat You Learned • 169\n\nNow let’s update our agent tests for Raft and test our replication and coordi- nation. In What You Learned, on page 138, I showed you a test snippet that’d fail our test because our servers replicated each other in a cycle instead of adhering to a leader-follower relationship. That snippet will pass now!\n\nIn internal/agent/agent_test.go, add the following line to the agent’s config:\n\nCoordinateWithConsensus/internal/agent/agent_test.go Bootstrap: i == 0,\n\nThis line is all we need to bootstrap the Raft cluster.\n\nAt the bottom of the test, add this snippet:\n\nCoordinateWithConsensus/internal/agent/agent_test.go consumeResponse, err = leaderClient.Consume(\n\ncontext.Background(), &api.ConsumeRequest{\n\nOffset: produceResponse.Offset + 1,\n\n},\n\n) require.Nil(t, consumeResponse) require.Error(t, err) got := grpc.Code(err) want := grpc.Code(api.ErrOffsetOutOfRange{}.GRPCStatus().Err()) require.Equal(t, got, want)\n\nNow we check that Raft has replicated the record we produced to the leader by consuming the record from a follower and that the replication stops there—the leader doesn’t replicate from the followers.\n\nRun your tests with $ make test. Your distributed service now uses Raft for consensus and replication!\n\nWhat You Learned\n\nIn this chapter, you learned how to coordinate distributed services with Raft by adding leader election and replication to our service. We also looked at how to multiplex connections and run multiple services on one port. Next, we’ll talk about client-side discovery, so clients can discover and call our servers.\n\nreport erratum • discuss\n\nCHAPTER 9\n\nDiscover Servers and Load Balance from the Client\n\nWe’ve gone through the belly of a whale and built a distributed service with discovery and consensus—a real distributed service! So far we’ve focused on the servers and haven’t changed the clients beyond what gRPC gives us for free. In this chapter, we’ll work on three client features that will improve our service’s availability, scalability, and user experience. We’ll enable our client to automatically:\n\nDiscover servers in the cluster, • Direct append calls to leaders and consume calls to followers, and • Balance consume calls across followers.\n\nAfter we’ve made these improvements, we’ll be ready to deploy!\n\nThree Load-Balancing Strategies\n\nThree strategies can be used for solving the discovery and load balancing problem:\n\nServer proxying—your client sends its requests to a load balancer that knows the servers (either by querying a service registry or by being the service registry) and proxies the requests to your back-end services.\n\nExternal load balancing—your client queries an external load-balancing service that knows the servers and tells the client which server to send the RPC.\n\nClient-side balancing—your client queries a service registry to learn about the servers, picks the server to send its RPC, and sends its RPC directly to the server.\n\nreport erratum • discuss\n\nChapter 9. Discover Servers and Load Balance from the Client • 172\n\nUsing a server proxy is the most commonly used discovery and load-balancing pattern. Most servers don’t trust their clients enough to give them control over how load balancing works because these decisions might affect the ser- vice’s availability (for example, allowing a client to target a single server and call it until it’s unavailable). You can put a proxy between clients and servers to act as a trust boundary. The proxy lets you control how your system ingests requests, as all the networking behind the proxy is in your network, trusted, and under your control. The server proxy knows about the servers it proxies to by maintaining or calling a service registry. People often use AWS’s Elastic Load Balancer (ELB) to load balance external traffic from the internet. The ELB is an example of a service-side discovery router—incoming requests hit the ELB, and the ELB proxies that request to one instance registered with the ELB.\n\nFor complex and very accurate load balancing, you can run an external load balancer. The external load balancer knows all the servers and potentially all the clients, so it has all the data to decide the best server for the client to call. You pay for external load balancers with operational burden. I’ve never needed an external load balancer.\n\nAlternatively, you can use client-side load balancing when you trust the clients. Client-side load balancing reduces latency and increases efficiency because requests go directly to their destination—there are no intermediates. This load balancing pattern is resilient because there isn’t a single point of failure. However, you need to work on your network and security to give clients direct access to your servers.\n\nWe’ll build client-side discovery and load balancing into our service because we control both the client and server and we designed our service for low- latency, high-throughput applications.\n\nLoad Balance on the Client in gRPC\n\nThough the ideas we’ll talk about in this chapter can apply to any client and server, because our service is a gRPC service, we’ll use those terms. gRPC separates server discovery, load balancing, and client requests and response handling—often the only code you’ll write is the latter. In gRPC, resolvers discover servers and pickers load balance by picking what server will handle the current request. gRPC also has balancers that manage subconnections but defer the load balancing to the pickers. gRPC provides an API (base.NewBal- ancerBuilderV2) to create a base balancer, but you probably won’t have to write your own balancer.\n\nreport erratum • discuss\n\nLoad Balance on the Client in gRPC • 173\n\nWhen you call grpc.Dial, gRPC takes the address and passes it on to the resolver, and the resolver discovers the servers. gRPC’s default resolver is the DNS resolver. If the address you give to gRPC has multiple DNS records associated with it, gRPC will balance the requests across each of those records’ servers. You can write your own resolvers and use resolvers written by the community. For example, Kuberesolver1 resolves the servers by fetching the endpoints from Kubernetes’ API.\n\ngRPC uses round-robin load balancing by default. The round-robin algorithm works by sending the first call to the first server, the second call to the second server, and so on. After the last server, it goes back to the first server again. So, we send each server the same number of calls. Round-robin works well when each request requires the same work by the server—stateless services that defer the work to a separate service like a database, for example. You can always begin with round-robin load balancing and optimize later.\n\nThe issue with round-robin load balancing, however, is that it doesn’t consider what you know about each request, client, and server. For example:\n\nIf your server is a replicated distributed service with a single writer and multiple readers, you’ll want to read from replicas so the writer can focus on the writes. This requires knowing whether the request is a read or a write and whether the server is a primary or a replica.\n\nIf your service is a globally distributed service, you’ll want your clients to prioritize networking with local servers, which means you must know the location of the clients and the servers.\n\nIf your system is latency sensitive, you can track metrics on how many in-flight or queued requests a server has or some other combination of latency metrics and have the client request the server with the smallest number.\n\nNow you’ve seen how client-side discovery and load balancing work in gRPC, and when you might want to go beyond round-robin to load balance more effi- ciently, what can you do with this knowledge when building your own services?\n\nThe service we’re building is a single-writer, multiple-reader distributed ser- vice—the leader server is the only server that can append to the log. Currently our clients connect to a single server, so if we want to call a leader and a fol- lower, we have to create multiple clients. And if we want to balance consume calls across the followers, we have to manage it in our client code.\n\n1.\n\nhttps://github.com/sercand/kuberesolver\n\nreport erratum • discuss",
      "page_number": 147
    },
    {
      "number": 9,
      "title": "Discover Servers and Load Balance from the Client • 172",
      "start_page": 176,
      "end_page": 195,
      "detection_method": "regex_chapter_title",
      "content": "Chapter 9. Discover Servers and Load Balance from the Client • 174\n\nWe can solve some problems by writing our own resolver and picker: the resolver discovers the servers and what server is the leader, and the picker manages directing produce calls to the leader and balancing consume calls across the followers. The resolver and picker will make your service easier to use, and we’ll be able to delete some of our test code too. Hopefully that sounds sweet to you—it does to me—so let’s get started.\n\nMake Servers Discoverable\n\nOur resolver will need a way to discover the cluster’s servers. It needs to know each server’s address and whether or not it is the leader. In Implement Raft in Our Service, on page 144, we built Raft into our service, which knows the cluster’s server and what server is the leader. We can expose this information to the resolver with an endpoint on our gRPC service.\n\nUsing an RPC for discovery will be easy because we built Serf and Raft into our service already. Kafka clients discover the cluster’s brokers by requesting a metadata endpoint. Kafka’s metadata endpoint responds with data that’s stored and coordinated with ZooKeeper, though the Kafka developers plan to remove the dependency on ZooKeeper and build Raft into Kafka to coordinate this data, similar to our service. This will be a big change in how this data works in Kafka, specifically with how it manages what servers are in the cluster and how it elects leaders; however, little to nothing will have to change with how the clients discover the servers, thus showing the benefit of using a service endpoint for client-side discovery.\n\nOpen the api/v1/log.proto file and update the Log service to include the GetServers() endpoint like so:\n\nClientSideServiceDiscovery/api/v1/log.proto service Log {\n\nrpc Produce(ProduceRequest) returns (ProduceResponse) {} rpc Consume(ConsumeRequest) returns (ConsumeResponse) {} rpc ConsumeStream(ConsumeRequest) returns (stream ConsumeResponse) {} rpc ProduceStream(stream ProduceRequest) returns (stream ProduceResponse)\n\n{}\n\nrpc GetServers(GetServersRequest) returns (GetServersResponse) {}\n\n}\n\nThis is the endpoint resolvers will call to get the cluster’s servers.\n\nNow, add this snippet to the end of the file to define the endpoint’s request and response:\n\nClientSideServiceDiscovery/api/v1/log.proto message GetServersRequest {}\n\nreport erratum • discuss\n\nLine 1\n\n\n\n\n\n\n\nMake Servers Discoverable • 175\n\nmessage GetServersResponse {\n\nrepeated Server servers = 1;\n\n}\n\nmessage Server { string id = 1; string rpc_addr = 2; bool is_leader = 3;\n\n}\n\nThe endpoint response includes the server addresses clients should connect to and what server is the leader. This information will tell the picker what server to send the server produce calls and what servers to send consume calls.\n\nWe’ll implement the endpoint on our server, but before we do, we need an API on our DistributedLog that exposes Raft’s server data. Open internal/log/distribut- ed.go and put this GetServers() method below DistributedLog.Close:\n\nClientSideServiceDiscovery/internal/log/distributed.go func (l *DistributedLog) GetServers() ([]*api.Server, error) {\n\nfuture := l.raft.GetConfiguration() if err := future.Error(); err != nil {\n\nreturn nil, err\n\n} var servers []*api.Server for _, server := range future.Configuration().Servers { servers = append(servers, &api.Server{\n\nId: RpcAddr: string(server.Address), IsLeader: l.raft.Leader() == server.Address,\n\nstring(server.ID),\n\n})\n\n} return servers, nil\n\n}\n\nRaft’s configuration comprises the servers in the cluster and includes each server’s ID, address, and suffrage—whether the server votes in Raft elections (we don’t need the suffrage in our project). Raft can tell us the address of the cluster’s leader, too. GetServers() converts the data from Raft’s raft.Server type into our *api.Server type for our API to respond with.\n\nLet’s update the DistributedLog tests to check that GetServers() returns the servers in the cluster as we expect. Open internal/log/distributed_test.go and add the new code in this snippet that surrounds the old lines 8 and 9:\n\nClientSideServiceDiscovery/internal/log/distributed_test.go servers, err := logs[0].GetServers() require.NoError(t, err) require.Equal(t, 3, len(servers)) require.True(t, servers[0].IsLeader)\n\nreport erratum • discuss\n\n5\n\n\n\n\n\n\n\n\n\n10\n\n\n\n\n\n\n\n\n\n15\n\n\n\n\n\nChapter 9. Discover Servers and Load Balance from the Client • 176\n\nrequire.False(t, servers[1].IsLeader) require.False(t, servers[2].IsLeader)\n\nerr = logs[0].Leave(\"1\") require.NoError(t, err)\n\ntime.Sleep(50 * time.Millisecond)\n\nservers, err = logs[0].GetServers() require.NoError(t, err) require.Equal(t, 2, len(servers)) require.True(t, servers[0].IsLeader) require.False(t, servers[1].IsLeader)\n\nThe assertions before line 8 test that GetServers() returns all three servers in the cluster and sets the leader server as the leader. After line 9, we expect the cluster to have two servers because these assertions run after we’ve made one server leave the cluster.\n\nThat’s it for the DistributedLog changes and tests. Next we’ll implement the endpoint on the server that calls DistributedLog.GetServers().\n\nOpen internal/server/server.go and update the Config to:\n\nClientSideServiceDiscovery/internal/server/server.go type Config struct {\n\nCommitLog Authorizer Authorizer GetServerer GetServerer\n\nCommitLog\n\n}\n\nAnd put this snippet below the ConsumeStream() method:\n\nClientSideServiceDiscovery/internal/server/server.go func (s *grpcServer) GetServers(\n\nctx context.Context, req *api.GetServersRequest,\n\n) (\n\napi.GetServersResponse, error) { servers, err := s.GetServerer.GetServers() if err != nil { return nil, err\n\n} return &api.GetServersResponse{Servers: servers}, nil\n\n}\n\ntype GetServerer interface {\n\nGetServers() ([]*api.Server, error)\n\n}\n\nThese two snippets enable us to inject different structs that can get servers. We don’t want to add the GetServers() method to our CommitLog interface because\n\nreport erratum • discuss\n\n➤\n\nResolve the Servers • 177\n\na non-distributed log like our Log type doesn’t know about servers. So we made a new interface whose sole method GetServers() matches DistributedLog.Get- Servers. When we update the end-to-end tests in the agent package, we’ll set our DistributedLog on the config as both the CommitLog and the GetServerer—which our new server endpoint wraps with error handling.\n\nIn agent.go, update your setupServer() method to configure the server to get the cluster’s servers from the DistributedLog:\n\nClientSideServiceDiscovery/internal/agent/agent.go serverConfig := &server.Config{\n\nCommitLog: Authorizer: authorizer, GetServerer: a.log,\n\na.log,\n\n}\n\nNow we have a server endpoint that clients can call to get the cluster’s servers. We’re now ready to build our resolver.\n\nResolve the Servers\n\nThe gRPC resolver we’ll write in this section will call the GetServers() endpoint we made and pass its information to gRPC so that the picker knows what servers it can route requests to.\n\nTo start, create a new package for our resolver and picker code by running $ mkdir internal/loadbalance.\n\ngRPC uses the builder pattern for resolvers and pickers, so each has a builder interface and an implementation interface. Because the builder interfaces have one simple method—Build()—we’ll implement both interfaces with one type. Create a file named resolver.go in internal/loadbalance that begins with this code:\n\nClientSideServiceDiscovery/internal/loadbalance/resolver.go package loadbalance\n\nimport (\n\n\"context\" \"fmt\" \"sync\"\n\n\"go.uber.org/zap\" \"google.golang.org/grpc\" \"google.golang.org/grpc/attributes\" \"google.golang.org/grpc/resolver\" \"google.golang.org/grpc/serviceconfig\"\n\napi \"github.com/travisjeffery/proglog/api/v1\"\n\n)\n\nreport erratum • discuss\n\nChapter 9. Discover Servers and Load Balance from the Client • 178\n\ntype Resolver struct {\n\nmu clientConn resolverConn *grpc.ClientConn serviceConfig *serviceconfig.ParseResult *zap.Logger logger\n\nsync.Mutex resolver.ClientConn\n\n}\n\nResolver is the type we’ll implement into gRPC’s resolver.Builder and resolver.Resolver interfaces. The clientConn connection is the user’s client connection and gRPC passes it to the resolver for the resolver to update with the servers it discovers. The resolverConn is the resolver’s own client connection to the server so it can call GetServers() and get the servers.\n\nAdd this snippet below the Resolver type to implement gRPC’s resolver.Builder interface:\n\nClientSideServiceDiscovery/internal/loadbalance/resolver.go var _ resolver.Builder = (*Resolver)(nil)\n\nfunc (r *Resolver) Build(\n\ntarget resolver.Target, cc resolver.ClientConn, opts resolver.BuildOptions,\n\n) (resolver.Resolver, error) {\n\nr.logger = zap.L().Named(\"resolver\") r.clientConn = cc var dialOpts []grpc.DialOption if opts.DialCreds != nil { dialOpts = append(\n\ndialOpts, grpc.WithTransportCredentials(opts.DialCreds),\n\n)\n\n} r.serviceConfig = r.clientConn.ParseServiceConfig(\n\nfmt.Sprintf(`{\"loadBalancingConfig\":[{\"%s\":{}}]}`, Name),\n\n) var err error r.resolverConn, err = grpc.Dial(target.Endpoint, dialOpts...) if err != nil {\n\nreturn nil, err\n\n} r.ResolveNow(resolver.ResolveNowOptions{}) return r, nil\n\n}\n\nconst Name = \"proglog\"\n\nfunc (r *Resolver) Scheme() string {\n\nreturn Name\n\n}\n\nreport erratum • discuss\n\nResolve the Servers • 179\n\nfunc init() {\n\nresolver.Register(&Resolver{})\n\n}\n\nresolver.Builder comprises two methods—Build() and Scheme():\n\nBuild() receives the data needed to build a resolver that can discover the servers (like the target address) and the client connection the resolver will update with the servers it discovers. Build() sets up a client connection to our server so the resolver can call the GetServers() API.\n\nScheme() returns the resolver’s scheme identifier. When you call grpc.Dial, gRPC parses out the scheme from the target address you gave it and tries to find a resolver that matches, defaulting to its DNS resolver. For our resolver, you’ll format the target address like this: proglog://your-service- address.\n\nWe register this resolver with gRPC in init() so gRPC knows about this resolver when it’s looking for resolvers that match the target’s scheme.\n\nPut this snippet below init() to implement gRPC’s resolver.Resolver interface:\n\nClientSideServiceDiscovery/internal/loadbalance/resolver.go var _ resolver.Resolver = (*Resolver)(nil)\n\nfunc (r *Resolver) ResolveNow(resolver.ResolveNowOptions) {\n\nr.mu.Lock() defer r.mu.Unlock() client := api.NewLogClient(r.resolverConn) // get cluster and then set on cc attributes ctx := context.Background() res, err := client.GetServers(ctx, &api.GetServersRequest{}) if err != nil {\n\nr.logger.Error(\n\n\"failed to resolve server\", zap.Error(err),\n\n) return\n\n} var addrs []resolver.Address for _, server := range res.Servers {\n\naddrs = append(addrs, resolver.Address{\n\nAddr: server.RpcAddr, Attributes: attributes.New( \"is_leader\", server.IsLeader,\n\n),\n\n})\n\n}\n\nreport erratum • discuss\n\nChapter 9. Discover Servers and Load Balance from the Client • 180\n\nr.clientConn.UpdateState(resolver.State{\n\nAddresses: ServiceConfig: r.serviceConfig,\n\naddrs,\n\n})\n\n}\n\nfunc (r *Resolver) Close() {\n\nif err := r.resolverConn.Close(); err != nil {\n\nr.logger.Error(\n\n\"failed to close conn\", zap.Error(err),\n\n)\n\n}\n\n}\n\nresolver.Resolver comprises two methods—ResolveNow() and Close(). gRPC calls ResolveNow() to resolve the target, discover the servers, and update the client connection with the servers. How your resolver will discover the servers depends on your resolver and the service you’re working with. For example, a resolver built for Kubernetes could call Kubernetes’ API to get the list of endpoints. We create a gRPC client for our service and call the GetServers() API to get the cluster’s servers.\n\nServices can specify how clients should balance their calls to the service by updating the state with a service config. We update the state with a service config that specifies to use the “proglog” load balancer we’ll write in Route and Balance Requests with Pickers, on page 183.\n\nYou update the state with a slice of resolver.Address to inform the load balancer what servers it can choose from. A resolver.Address has three fields:\n\nAddr (required)—the address of the server to connect to.\n\nAttributes (optional but useful)—a map containing any data that’s useful for the load balancer. We’ll tell the picker what server is the leader and what servers are followers with this field.\n\nServerName (optional and you likely don’t need to set)—the name used as the transport certificate authority for the address, instead of the hostname taken from the Dial target string.\n\nAfter we’ve discovered the servers, we update the client connection by calling UpdateState() with the resolver.Address’s. We set up the addresses with the data in the api.Server’s. gRPC may call ResolveNow() concurrently, so we use a mutex to protect access across goroutines.\n\nClose() closes the resolver. In our resolver, we close the connection to our server created in Build().\n\nreport erratum • discuss\n\nResolve the Servers • 181\n\nThat’s it for our resolver’s code. Let’s test it.\n\nCreate a test file named resolver_test.go in internal/loadbalance that begins with this snippet:\n\nLine 1\n\nClientSideServiceDiscovery/internal/loadbalance/resolver_test.go package loadbalance_test\n\n\n\n\n\nimport (\n\n\n\n5\n\n\"net\" \"testing\"\n\n\n\n\n\n\n\n\n\n10\n\n\n\n\n\n\"github.com/stretchr/testify/require\" \"google.golang.org/grpc\" \"google.golang.org/grpc/attributes\" \"google.golang.org/grpc/credentials\" \"google.golang.org/grpc/resolver\" \"google.golang.org/grpc/serviceconfig\"\n\n\n\n\n\n15\n\n\n\n)- -\n\napi \"github.com/travisjeffery/proglog/api/v1\" \"github.com/travisjeffery/proglog/internal/loadbalance\" \"github.com/travisjeffery/proglog/internal/config\" \"github.com/travisjeffery/proglog/internal/server\"\n\n20\n\nfunc TestResolver(t *testing.T) {\n\n\n\n\n\nl, err := net.Listen(\"tcp\", \"127.0.0.1:0\") require.NoError(t, err)\n\n\n\n\n\ntlsConfig, err := config.SetupTLSConfig(config.TLSConfig{\n\n25\n\n\n\n\n\n\n\n\n\nCertFile: KeyFile: CAFile: Server: ServerAddress: \"127.0.0.1\",\n\nconfig.ServerCertFile, config.ServerKeyFile, config.CAFile, true,\n\n30\n\n\n\n\n\n}) require.NoError(t, err) serverCreds := credentials.NewTLS(tlsConfig)\n\n\n\n\n\nsrv, err := server.NewGRPCServer(&server.Config{\n\n35\n\nGetServerer: &getServers{},\n\n\n\n\n\n}, grpc.Creds(serverCreds)) require.NoError(t, err)\n\n\n\n\n\ngo srv.Serve(l)\n\nThis code sets up a server for our test resolver to try and discover some servers from. We pass in a mock GetServerers on line 35 so we can set what servers the resolver should find.\n\nreport erratum • discuss\n\nChapter 9. Discover Servers and Load Balance from the Client • 182\n\nPut this snippet below the previous snippet to continue writing the test:\n\nClientSideServiceDiscovery/internal/loadbalance/resolver_test.go conn := &clientConn{} tlsConfig, err = config.SetupTLSConfig(config.TLSConfig{\n\nCertFile: KeyFile: CAFile: Server: ServerAddress: \"127.0.0.1\",\n\nconfig.RootClientCertFile, config.RootClientKeyFile, config.CAFile, false,\n\n}) require.NoError(t, err) clientCreds := credentials.NewTLS(tlsConfig) opts := resolver.BuildOptions{\n\nDialCreds: clientCreds,\n\n} r := &loadbalance.Resolver{} _, err = r.Build(\n\nresolver.Target{\n\nEndpoint: l.Addr().String(),\n\n}, conn, opts,\n\n) require.NoError(t, err)\n\nThis code creates and builds the test resolver and configures its target end- point to point to the server we set up in the previous snippet. The resolver will call GetServers() to resolve the servers and update the client connection with the servers’ addresses.\n\nAdd this snippet below the previous snippet to finish writing the test:\n\nClientSideServiceDiscovery/internal/loadbalance/resolver_test.go\n\nwantState := resolver.State{\n\nAddresses: []resolver.Address{{\n\nAddr: Attributes: attributes.New(\"is_leader\", true),\n\n\"localhost:9001\",\n\n}, {\n\nAddr: Attributes: attributes.New(\"is_leader\", false),\n\n\"localhost:9002\",\n\n}},\n\n} require.Equal(t, wantState, conn.state)\n\nconn.state.Addresses = nil r.ResolveNow(resolver.ResolveNowOptions{}) require.Equal(t, wantState, conn.state)\n\n}\n\nreport erratum • discuss\n\nRoute and Balance Requests with Pickers • 183\n\nWe check that the resolver updated the client connection with the servers and data we expected. We wanted the resolver to find two servers and mark the 9001 server as the leader.\n\nOur test depended on some mock types. Add this code at the bottom of the file:\n\nClientSideServiceDiscovery/internal/loadbalance/resolver_test.go type getServers struct{}\n\nfunc (s *getServers) GetServers() ([]*api.Server, error) {\n\nreturn []*api.Server{{\n\nId: RpcAddr: \"localhost:9001\", IsLeader: true,\n\n\"leader\",\n\n}, {\n\nId: RpcAddr: \"localhost:9002\",\n\n\"follower\",\n\n}}, nil\n\n}\n\ntype clientConn struct {\n\nresolver.ClientConn state resolver.State\n\n}\n\nfunc (c *clientConn) UpdateState(state resolver.State) {\n\nc.state = state\n\n}\n\nfunc (c *clientConn) ReportError(err error) {}\n\nfunc (c *clientConn) NewAddress(addrs []resolver.Address) {}\n\nfunc (c *clientConn) NewServiceConfig(config string) {}\n\nfunc (c *clientConn) ParseServiceConfig(\n\nconfig string, ) *serviceconfig.ParseResult {\n\nreturn nil\n\n}\n\ngetServers implements GetServerers, whose job is to return a known server set for the resolver to find. clientConn implements resolver.ClientConn, and its job is to keep a reference to the state the resolver updated it with so that we can verify that the resolver updates the client connection with the correct data.\n\nRun the resolver tests to verify that they pass. And now, we’re on to the picker.\n\nRoute and Balance Requests with Pickers\n\nIn the gRPC architecture, pickers handle the RPC balancing logic. They’re called pickers because they pick a server from the servers discovered by the resolver to handle each RPC. Pickers can route RPCs based on information\n\nreport erratum • discuss\n\nChapter 9. Discover Servers and Load Balance from the Client • 184\n\nabout the RPC, client, and server, so their utility goes beyond balancing to any kind of request-routing logic.\n\nTo implement the picker builder, create a file named picker.go in internal/loadbalance that begins with this code:\n\nClientSideServiceDiscovery/internal/loadbalance/picker.go package loadbalance\n\nimport (\n\n\"strings\" \"sync\" \"sync/atomic\"\n\n\"google.golang.org/grpc/balancer\" \"google.golang.org/grpc/balancer/base\"\n\n)\n\nvar _ base.PickerBuilder = (*Picker)(nil)\n\ntype Picker struct {\n\nmu leader followers []balancer.SubConn current\n\nsync.RWMutex balancer.SubConn\n\nuint64\n\n}\n\nfunc (p *Picker) Build(buildInfo base.PickerBuildInfo) balancer.Picker {\n\np.mu.Lock() defer p.mu.Unlock() var followers []balancer.SubConn for sc, scInfo := range buildInfo.ReadySCs {\n\nisLeader := scInfo.\n\nAddress. Attributes. Value(\"is_leader\").(bool)\n\nif isLeader {\n\np.leader = sc continue\n\n} followers = append(followers, sc)\n\n} p.followers = followers return p\n\n}\n\nPickers use the builder pattern just like resolvers. gRPC passes a map of subconnections with information about those subconnections to Build() to set up the picker—behind the scenes, gRPC connected to the addresses that our resolver discovered. Our picker will route consume RPCs to follower servers and produce RPCs to the leader server. The address attributes help us differ- entiate the servers.\n\nreport erratum • discuss\n\nRoute and Balance Requests with Pickers • 185\n\nTo implement the picker, add this snippet below Build():\n\nClientSideServiceDiscovery/internal/loadbalance/picker.go var _ balancer.Picker = (*Picker)(nil)\n\nfunc (p *Picker) Pick(info balancer.PickInfo) (\n\nbalancer.PickResult, error) { p.mu.RLock() defer p.mu.RUnlock() var result balancer.PickResult if strings.Contains(info.FullMethodName, \"Produce\") ||\n\nlen(p.followers) == 0 { result.SubConn = p.leader\n\n} else if strings.Contains(info.FullMethodName, \"Consume\") {\n\nresult.SubConn = p.nextFollower()\n\n} if result.SubConn == nil {\n\nreturn result, balancer.ErrNoSubConnAvailable\n\n} return result, nil\n\n}\n\nfunc (p *Picker) nextFollower() balancer.SubConn {\n\ncur := atomic.AddUint64(&p.current, uint64(1)) len := uint64(len(p.followers)) idx := int(cur % len) return p.followers[idx]\n\n}\n\nPickers have one method: Pick(balancer.PickInfo). gRPC gives Pick() a balancer.PickInfo containing the RPC’s name and context to help the picker know what subcon- nection to pick. If you have header metadata, you can read it from the context. Pick() returns a balancer.PickResult with the subconnection to handle the call. Optionally, you can set a Done callback on the result that gRPC calls when the RPC completes. The callback tells you the RPC’s error, trailer metadata, and whether there were bytes sent and received to and from the server.\n\nWe look at the RPC’s method name to know whether the call is an append or consume call, and if we should pick a leader subconnection or a follower subconnection. We balance the consume calls across the followers with the round-robin algorithm. Put this snippet at the end of the file to register the picker with gRPC and finish the picker’s code:\n\nClientSideServiceDiscovery/internal/loadbalance/picker.go func init() {\n\nbalancer.Register(\n\nbase.NewBalancerBuilder(Name, &Picker{}, base.Config{}),\n\n)\n\n}\n\nreport erratum • discuss\n\nChapter 9. Discover Servers and Load Balance from the Client • 186\n\nThough pickers handle routing the calls, which we’d traditionally consider handling the balancing, gRPC has a balancer type that takes input from gRPC, manages subconnections, and collects and aggregates connectivity states. gRPC provides a base balancer; you probably don’t need to write your own.\n\nTime to test our picker. Create a test file named picker_test.go in internal/loadbalance that begins with this snippet:\n\nClientSideServiceDiscovery/internal/loadbalance/picker_test.go package loadbalance_test\n\nimport (\n\n\"testing\"\n\n\"google.golang.org/grpc/attributes\" \"google.golang.org/grpc/balancer\" \"google.golang.org/grpc/balancer/base\" \"google.golang.org/grpc/resolver\"\n\n\"github.com/stretchr/testify/require\"\n\n\"github.com/travisjeffery/proglog/internal/loadbalance\"\n\n)\n\nfunc TestPickerNoSubConnAvailable(t *testing.T) {\n\npicker := &loadbalance.Picker{} for _, method := range []string{\n\n\"/log.vX.Log/Produce\", \"/log.vX.Log/Consume\",\n\n} {\n\ninfo := balancer.PickInfo{\n\nFullMethodName: method,\n\n} result, err := picker.Pick(info) require.Equal(t, balancer.ErrNoSubConnAvailable, err) require.Nil(t, result.SubConn)\n\n}\n\n}\n\nTestPickerNoSubConnAvailable() tests that a picker initially returns balancer.ErrNoSub- ConnAvailable before the resolver has discovered servers and updated the picker’s state with available subconnections. balancer.ErrNoSubConnAvailable instructs gRPC to block the client’s RPCs until the picker has an available subconnection to handle them.\n\nNext add this snippet below TestPickerNoSubConnAvailable() to test pickers with subconnections to pick from:\n\nClientSideServiceDiscovery/internal/loadbalance/picker_test.go func TestPickerProducesToLeader(t *testing.T) { picker, subConns := setupTest() info := balancer.PickInfo{\n\nreport erratum • discuss\n\nRoute and Balance Requests with Pickers • 187\n\nFullMethodName: \"/log.vX.Log/Produce\",\n\n} for i := 0; i < 5; i++ {\n\ngotPick, err := picker.Pick(info) require.NoError(t, err) require.Equal(t, subConns[0], gotPick.SubConn)\n\n}\n\n}\n\nfunc TestPickerConsumesFromFollowers(t *testing.T) {\n\npicker, subConns := setupTest() info := balancer.PickInfo{\n\nFullMethodName: \"/log.vX.Log/Consume\",\n\n} for i := 0; i < 5; i++ {\n\npick, err := picker.Pick(info) require.NoError(t, err) require.Equal(t, subConns[i%2+1], pick.SubConn)\n\n}\n\n}\n\nTestPickerProducesToLeader() tests that the picker picks the leader subconnection for append calls. TestPickerConsumesFromFollowers() tests that the picker picks the followers subconnections in a round-robin for consume calls.\n\nPut this final snippet at the end of the file to define the tests’ helpers:\n\nClientSideServiceDiscovery/internal/loadbalance/picker_test.go func setupTest() (*loadbalance.Picker, []*subConn) {\n\nvar subConns []*subConn buildInfo := base.PickerBuildInfo{\n\nReadySCs: make(map[balancer.SubConn]base.SubConnInfo),\n\n} for i := 0; i < 3; i++ { sc := &subConn{} addr := resolver.Address{\n\nAttributes: attributes.New(\"is_leader\", i == 0),\n\n} // 0th sub conn is the leader sc.UpdateAddresses([]resolver.Address{addr}) buildInfo.ReadySCs[sc] = base.SubConnInfo{Address: addr} subConns = append(subConns, sc)\n\n} picker := &loadbalance.Picker{} picker.Build(buildInfo) return picker, subConns\n\n}\n\n// subConn implements balancer.SubConn. type subConn struct {\n\naddrs []resolver.Address\n\n}\n\nreport erratum • discuss\n\n➤ ➤ ➤ ➤ ➤\n\nChapter 9. Discover Servers and Load Balance from the Client • 188\n\nfunc (s *subConn) UpdateAddresses(addrs []resolver.Address) {\n\ns.addrs = addrs\n\n}\n\nfunc (s *subConn) Connect() {}\n\nsetupTest() builds the test picker with some mock subconnections. We create the picker with build information that contains addresses with the same attributes as our resolver sets.\n\nRun the picker’s tests to verify they pass. Now we’re ready to put everything together.\n\nTest Discovery and Balancing End-to-End\n\nWe’re ready to update our agent’s tests to test everything end-to-end: the client configuring the resolver and picker, the resolver discovering the servers, and the picker picking subconnections per RPC.\n\nOpen your agent tests in internal/agent/agent_test.go and add this import:\n\nClientSideServiceDiscovery/internal/agent/agent_test.go \"github.com/travisjeffery/proglog/internal/loadbalance\"\n\nThen update the client() function to use your resolver and picker:\n\nClientSideServiceDiscovery/internal/agent/agent_test.go func client(\n\nt *testing.T, agent *agent.Agent, tlsConfig *tls.Config,\n\n) api.LogClient {\n\ntlsCreds := credentials.NewTLS(tlsConfig) opts := []grpc.DialOption{\n\ngrpc.WithTransportCredentials(tlsCreds),\n\n} rpcAddr, err := agent.Config.RPCAddr() require.NoError(t, err) conn, err := grpc.Dial(fmt.Sprintf(\n\n\"%s:///%s\", loadbalance.Name, rpcAddr,\n\n), opts...) require.NoError(t, err) client := api.NewLogClient(conn) return client\n\n}\n\nThe highlighted lines specify our scheme in the URL so gRPC knows to use our resolver.\n\nreport erratum • discuss\n\nWhat You Learned • 189\n\nRun the agent’s tests by running $ go run ./internal/agent, and you’ll see that the leader client consume call fails now. Why? Before, each client connected to one server. So the leader client connected to the leader. When we produced records, they were immediately available for consuming with the leader client because it consumed from the leader server—we didn’t have to wait for the leader to replicate the record. Now, each client connects to every server and produces to the leader and consumes from the followers, so we must wait for the leader to replicate the record to the followers.\n\nUpdate your test to wait for the servers to replicate the record before consum- ing with the leader client. Move time.Sleep that appears before line 14 to appear before line 4:\n\nLine 1\n\n\n\nClientSideServiceDiscovery/internal/agent/agent_test.go // wait until replication has finished time.Sleep(3 * time.Second)\n\n\n\n\n\nconsumeResponse, err := leaderClient.Consume(\n\n5\n\n\n\ncontext.Background(), &api.ConsumeRequest{\n\n\n\nOffset: produceResponse.Offset,\n\n)-\n\n},\n\n10\n\n\n\nrequire.NoError(t, err) require.Equal(t, consumeResponse.Record.Value, []byte(\"foo\"))\n\n\n\n\n\n\n\nfollowerClient := client(t, agents[1], peerTLSConfig) consumeResponse, err = followerClient.Consume(\n\n15\n\n\n\ncontext.Background(), &api.ConsumeRequest{\n\n\n\nOffset: produceResponse.Offset,\n\n)-\n\n},\n\n20\n\n\n\nrequire.NoError(t, err) require.Equal(t, consumeResponse.Record.Value, []byte(\"foo\"))\n\nRun your tests again with $ make test and watch them pass!\n\nWhat You Learned\n\nNow you know how gRPC resolves services and balances calls across them and how you can build your own resolvers and pickers. You can write your own resolver so that your clients dynamically discover servers. And you saw how pickers are useful for more than just load balancing—you can build your own routing logic with them.\n\nIn the next part of the book, we’ll look at how to deploy our service and make it live.\n\nreport erratum • discuss\n\nPart IV\n\nDeploy\n\nCHAPTER 10\n\nDeploy Applications with Kubernetes Locally\n\nAfter Frodo and Sam had trekked from the Shire to Mount Doom, was their task finished? No—the whole journey would’ve been for nothing if they hadn’t thrown that ring into the fire. Likewise, building a service means something only after you’ve deployed it. Therefore, in this chapter, we’ll deploy a cluster of our service. We’ll:\n\nCreate an agent command-line interface (CLI) so we have an executable\n\nto run our service.\n\nGet set up with Kubernetes and Helm so that we can orchestrate our\n\nservice on both our local machine and later on a cloud platform.\n\nRun a cluster of your service on your machine.\n\nReady? Let’s get started.\n\nWhat Is Kubernetes?\n\nWhile entire books are devoted to answering this question, even they can’t cover everything Kubernetes can do. For our purposes in this book, I will touch upon the information you need to know to have a working knowledge of Kubernetes, enough to continue our journey and deploy and operate our service. Why Kuber- netes? Kubernetes is ubiquitous, it’s available on all cloud platforms, and it’s as close to a standard as we have for deploying distributed services.\n\nKubernetes1 is an open source orchestration system for automating deployment, scaling, and operating services running in containers. You tell Kubernetes what\n\n1.\n\nhttps://kubernetes.io\n\nreport erratum • discuss\n\nChapter 10. Deploy Applications with Kubernetes Locally • 194\n\nto do by using its REST API to create, update, and delete resources that Kubernetes knows how to handle. Kubernetes is a declarative system in that you describe the end-goal state you want and Kubernetes runs the changes to take your system from its current state to your end-goal state.\n\nThe Kubernetes resource that people most commonly see are pods, the smallest deployable unit in Kubernetes. Think of containers as processes and pods as hosts—all containers running in a pod share the same network namespace, the same IP address, and the same interprocess communication (IPC) namespace, and they can share the same volumes. These are logical hosts because a physical host (what Kubernetes calls a node) may run multiple pods. The other resources you’ll work with either configure pods (ConfigMaps, Secrets) or manage a pod set (Deployments, StatefulSets, DaemonSets). You can extend Kubernetes by creating your own custom resources and controllers.\n\nControllers are control loops that watch the state of your resources and make changes where needed. Kubernetes itself is made up of many controllers. For example, the Deployment controller watches your Deployment resources; if you increase the replicas on a Deployment, the controller will schedule more pods.\n\nTo interact with Kubernetes, you’ll need its command-line tool, kubectl, which we’ll look at next.\n\nInstall kubectl\n\nThe Kubernetes command-line tool, kubectl,2 is used to run commands against Kubernetes clusters. You’ll use kubectl to inspect and manage your service’s cluster resources and view logs. Try to use kubectl for one-off operations. For operations you run again and again, like deploying or upgrading a service, you’ll use the Helm package manager or an operator, which we’ll take a look at later in this chapter.\n\nTo install kubectl, run the following:\n\n$ curl -LO \\ https://storage.googleapis.com/kubernetes-release/release/\\ v1.18.0/bin/$(uname)/amd64/kubectl $ chmod +x ./kubectl $ mv ./kubectl /usr/local/bin/kubectl\n\nWe need a Kubernetes cluster and its API for kubectl to call and do anything. In the next section, we’ll use the Kind tool to run a local Kubernetes cluster in Docker.\n\n2.\n\nhttps://kubernetes.io/docs/reference/kubectl/overview\n\nreport erratum • discuss\n\nUse Kind for Local Development and Continuous Integration • 195\n\nUse Kind for Local Development and Continuous Integration\n\nKind3 (an acronym for Kubernetes IN Docker) is a tool developed by the Kubernetes team to run local Kubernetes clusters using Docker containers as nodes. It’s the easiest way to run your own Kubernetes cluster, and it’s great for local development, testing, and continuous integration.\n\nTo install Kind, run the following:\n\n$ curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.8.1/kind-$(uname)-amd64 $ chmod +x ./kind $ mv ./kind /usr/local/bin/kind\n\nTo use Kind, you’ll need to install Docker.4 See Docker’s dedicated install instructions for your operation system.\n\nWith Docker running, you can create a Kind cluster by running:\n\n$ kind create cluster\n\nYou can then verify that Kind created your cluster and configured kubectl to use it by running the following:\n\n$ kubectl cluster-info > Kubernetes master is running at https://127.0.0.1:46023 KubeDNS is running at \\ https://127.0.0.1:46023/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\n\nTo further debug and diagnose cluster problems, use kubectl cluster-info dump.\n\nKind runs one Docker container representing one Kubernetes node in the cluster. By default, Kind runs a single node cluster with everything needed for a functioning Kubernetes cluster. You can see the Node container by running this:\n\n$ docker ps CONTAINER ID IMAGE COMMAND CREATED ... 033de99b1e53 kindest/node:v1.18.2 \"/usr/local/bin/entr…\" 2 minutes...\n\nWe have a running Kubernetes cluster now—let’s run our service on it! To run our service in Kubernetes, we’ll need a Docker image, and our Docker image will need an executable entry point. Let’s write an agent CLI that serves as our service’s executable.\n\n3. 4.\n\nhttps://kind.sigs.k8s.io\n\nhttps://docs.docker.com/install\n\nreport erratum • discuss",
      "page_number": 176
    },
    {
      "number": 10,
      "title": "Deploy Applications with Kubernetes Locally • 194",
      "start_page": 196,
      "end_page": 220,
      "detection_method": "regex_chapter_title",
      "content": "➤ ➤ ➤ ➤ ➤\n\nChapter 10. Deploy Applications with Kubernetes Locally • 196\n\nWrite an Agent Command-Line Interface\n\nOur agent CLI will provide just enough features to use as a Docker image’s entry point and run our service, parse flags, and then configure and run the agent.\n\nI use the Cobra5 library to handle commands and flags because it works well for creating both simple CLIs and complex applications. It’s used in the Go community by projects such as Kubernetes, Docker, Helm, Etcd, Hugo, and more. And Cobra integrates with a library called Viper,6 which is a complete configuration solution for Go applications.\n\nThe first step is to create a cmd/proglog/main.go file, beginning with this code:\n\nDeployLocally/cmd/proglog/main.go package main\n\nimport (\n\n\"log\" \"os\" \"os/signal\" \"path\" \"syscall\"\n\n\"github.com/spf13/cobra\" \"github.com/spf13/viper\" \"github.com/travisjeffery/proglog/internal/agent\" \"github.com/travisjeffery/proglog/internal/config\"\n\n)\n\nfunc main() {\n\ncli := &cli{}\n\ncmd := &cobra.Command{\n\nUse: PreRunE: cli.setupConfig, cli.run, RunE:\n\n\"proglog\",\n\n}\n\nif err := setupFlags(cmd); err != nil {\n\nlog.Fatal(err)\n\n}\n\nif err := cmd.Execute(); err != nil {\n\nlog.Fatal(err)\n\n}\n\n}\n\nThe highlighted code defines our sole command. Our CLI is about as simple as it gets. In more complex applications, this command would act as the root\n\n5. 6.\n\nhttps://github.com/spf13/cobra\n\nhttps://github.com/spf13/viper\n\nreport erratum • discuss\n\nWrite an Agent Command-Line Interface • 197\n\ncommand tying together your subcommands. Cobra calls the RunE function you set on your command when the command runs. Put or call the command’s primary logic in that function. Cobra enables you to run hook functions to run before and after RunE.\n\nCobra provides persistent flags and hooks for applications with many subcom- mands (so we’re not using them in our program)—persistent flags and hooks apply to the current command and all its children. A common use case for a persistent flag is in API-wrapping CLIs. In these CLIs, every subcommand will need a flag for the API’s endpoint address. In this situation, you’d use an --api-addr persistent flag that you declare once on the root command for all the subcommands to inherit.\n\nTo define our cli and cfg types, add the following code:\n\nDeployLocally/cmd/proglog/main.go type cli struct {\n\ncfg cfg\n\n}\n\ntype cfg struct {\n\nagent.Config ServerTLSConfig config.TLSConfig config.TLSConfig PeerTLSConfig\n\n}\n\nI typically create a cli struct in which I can put logic and data that’s common to all the commands. I created a separate cfg struct from the agent.Config struct to handle the field types that we can’t parse without error handling: the *net.TCPAddr and the *tls.Config.\n\nNow, let’s set up our CLI’s flags.\n\nExpose Flags\n\nBelow the previous snippet, add this code to declare our CLI’s flags:\n\nDeployLocally/cmd/proglog/main.go func setupFlags(cmd *cobra.Command) error {\n\nhostname, err := os.Hostname() if err != nil {\n\nlog.Fatal(err)\n\n}\n\ncmd.Flags().String(\"config-file\", \"\", \"Path to config file.\")\n\ndataDir := path.Join(os.TempDir(), \"proglog\") cmd.Flags().String(\"data-dir\",\n\ndataDir, \"Directory to store log and Raft data.\")\n\nreport erratum • discuss\n\nChapter 10. Deploy Applications with Kubernetes Locally • 198\n\ncmd.Flags().String(\"node-name\", hostname, \"Unique server ID.\")\n\ncmd.Flags().String(\"bind-addr\",\n\n\"127.0.0.1:8401\", \"Address to bind Serf on.\")\n\ncmd.Flags().Int(\"rpc-port\",\n\n8400, \"Port for RPC clients (and Raft) connections.\")\n\ncmd.Flags().StringSlice(\"start-join-addrs\",\n\nnil, \"Serf addresses to join.\")\n\ncmd.Flags().Bool(\"bootstrap\", false, \"Bootstrap the cluster.\")\n\ncmd.Flags().String(\"acl-model-file\", \"\", \"Path to ACL model.\") cmd.Flags().String(\"acl-policy-file\", \"\", \"Path to ACL policy.\")\n\ncmd.Flags().String(\"server-tls-cert-file\", \"\", \"Path to server tls cert.\") cmd.Flags().String(\"server-tls-key-file\", \"\", \"Path to server tls key.\") cmd.Flags().String(\"server-tls-ca-file\",\n\n\"\", \"Path to server certificate authority.\")\n\ncmd.Flags().String(\"peer-tls-cert-file\", \"\", \"Path to peer tls cert.\") cmd.Flags().String(\"peer-tls-key-file\", \"\", \"Path to peer tls key.\") cmd.Flags().String(\"peer-tls-ca-file\",\n\n\"\", \"Path to peer certificate authority.\")\n\nreturn viper.BindPFlags(cmd.Flags())\n\n}\n\nThese flags allow people calling your CLI to configure the agent and learn the default configuration.\n\nWith the pflag.FlagSet.{{type}}Var() methods, we can set our configuration’s values directly. However, the problem with setting the configurations directly is that not all types have supporting APIs out of the box. Our BindAddr configuration is an example, which is a *net.TCPAddr that we need to parse from a string. You can define custom flag values7 when you have enough flags of the same type, or just use an intermediate value otherwise.\n\nBut what if we want to configure our service with more than flags, such as with a file? We’ll look at how to read in the configuration from a file, too, for dynamic configurations.\n\nManage Your Configuration\n\nViper provides a centralized config registry system where multiple configuration sources can set the configuration but you can read the result in one place.\n\n7.\n\nhttps://golang.org/pkg/flag/#Value\n\nreport erratum • discuss\n\nWrite an Agent Command-Line Interface • 199\n\nYou could allow users to set the configuration with flags, a file, or by loading dynamic configs from a service like Consul—Viper supports all of these.\n\nWith a configuration file, you can support dynamic config changes to a running service. The service watches the config file for changes and updates accord- ingly. For example, you may run your service at INFO-level logs by default but need DEBUG-level logs when you’re debugging an issue with the running service. A configuration file also enables other processes to set up the config- uration for the service. We’ll see an example of that with our service where we have an init container that sets up the configuration for the service’s container.\n\nI’ve given usable defaults for the configurations we have to set: the data directory, bind address, the RPC port, and the node name. Try to set usable default flag values instead of requiring users to set them.\n\nAfter declaring the flags, the next step is to execute the root command to parse the process’s arguments and search through the command tree to find the correct command to run. We just have the one command, so we’re not making Cobra work hard.\n\nAdd this snippet to set up the config:\n\nDeployLocally/cmd/proglog/main.go func (c *cli) setupConfig(cmd *cobra.Command, args []string) error {\n\nvar err error\n\nconfigFile, err := cmd.Flags().GetString(\"config-file\") if err != nil {\n\nreturn err\n\n} viper.SetConfigFile(configFile)\n\nif err = viper.ReadInConfig(); err != nil {\n\n// it's ok if config file doesn't exist if _, ok := err.(viper.ConfigFileNotFoundError); !ok {\n\nreturn err\n\n}\n\n}\n\nc.cfg.DataDir = viper.GetString(\"data-dir\") c.cfg.NodeName = viper.GetString(\"node-name\") c.cfg.BindAddr = viper.GetString(\"bind-addr\") c.cfg.RPCPort = viper.GetInt(\"rpc-port\") c.cfg.StartJoinAddrs = viper.GetStringSlice(\"start-join-addrs\") c.cfg.Bootstrap = viper.GetBool(\"bootstrap\") c.cfg.ACLModelFile = viper.GetString(\"acl-mode-file\") c.cfg.ACLPolicyFile = viper.GetString(\"acl-policy-file\") c.cfg.ServerTLSConfig.CertFile = viper.GetString(\"server-tls-cert-file\") c.cfg.ServerTLSConfig.KeyFile = viper.GetString(\"server-tls-key-file\")\n\nreport erratum • discuss\n\nChapter 10. Deploy Applications with Kubernetes Locally • 200\n\nc.cfg.ServerTLSConfig.CAFile = viper.GetString(\"server-tls-ca-file\") c.cfg.PeerTLSConfig.CertFile = viper.GetString(\"peer-tls-cert-file\") c.cfg.PeerTLSConfig.KeyFile = viper.GetString(\"peer-tls-key-file\") c.cfg.PeerTLSConfig.CAFile = viper.GetString(\"peer-tls-ca-file\")\n\nif c.cfg.ServerTLSConfig.CertFile != \"\" &&\n\nc.cfg.ServerTLSConfig.KeyFile != \"\" { c.cfg.ServerTLSConfig.Server = true c.cfg.Config.ServerTLSConfig, err = config.SetupTLSConfig(\n\nc.cfg.ServerTLSConfig,\n\n) if err != nil {\n\nreturn err\n\n}\n\n}\n\nif c.cfg.PeerTLSConfig.CertFile != \"\" &&\n\nc.cfg.PeerTLSConfig.KeyFile != \"\" { c.cfg.Config.PeerTLSConfig, err = config.SetupTLSConfig(\n\nc.cfg.PeerTLSConfig,\n\n) if err != nil {\n\nreturn err\n\n}\n\n}\n\nreturn nil\n\n}\n\nsetupConfig(cmd *cobra.Command, args []string) reads the configuration and prepares the agent’s configuration. Cobra calls setupConfig() before running the command’s RunE function.\n\nFinish writing the program by including this run() method:\n\nDeployLocally/cmd/proglog/main.go func (c *cli) run(cmd *cobra.Command, args []string) error {\n\nvar err error agent, err := agent.New(c.cfg.Config) if err != nil {\n\nreturn err\n\n} sigc := make(chan os.Signal, 1) signal.Notify(sigc, syscall.SIGINT, syscall.SIGTERM) <-sigc return agent.Shutdown()\n\n}\n\nrun(cmd *cobra.Command, args []string) runs our executable’s logic by:\n\nCreating the agent; • Handling signals from the operating system; and\n\nreport erratum • discuss\n\nBuild Your Docker Image • 201\n\nShutting down the agent gracefully when the operating system terminates\n\nthe program.\n\nOkay, we have our executable that we can use as our Docker image’s entry point, so let’s write our Dockerfile and build the image.\n\nBuild Your Docker Image\n\nCreate a Dockerfile with this code:\n\nDeployLocally/Dockerfile FROM golang:1.14-alpine AS build WORKDIR /go/src/proglog COPY . . RUN CGO_ENABLED=0 go build -o /go/bin/proglog ./cmd/proglog\n\nFROM scratch COPY --from=build /go/bin/proglog /bin/proglog ENTRYPOINT [\"/bin/proglog\"]\n\nOur Dockerfile uses multistage builds: one stage builds our service and one stage runs it. This makes our Dockerfile easy to read and maintain while keeping our build efficient and the image small.\n\nThe build stage uses the golang:1.14-alpine image because we need the Go compiler, our dependencies, and perhaps various system libraries. These take up disk space, and we don’t need them after we have compiled our binary. In the second stage, we use the scratch empty image—the smallest Docker image. We copy our binary into this image, and this is the image we deploy.\n\nYou must statically compile your binaries for them to run in the scratch image because it doesn’t contain the system libraries needed to run dynamically linked binaries. That’s why we disable Cgo—the compiler links it dynamically. Using the scratch image helps with thinking of the containers as being immutable. Instead of exec’ing into a container and mutating the image by installing tools or changing the filesystem, you run a short-lived container that has the tool you need.\n\nThe next step is to add a target to your Makefile to build the Docker image by adding this snippet to the bottom of the file:\n\nDeployLocally/Makefile TAG ?= 0.0.1\n\nbuild-docker:\n\ndocker build -t github.com/travisjeffery/proglog:$(TAG) .\n\nreport erratum • discuss\n\nChapter 10. Deploy Applications with Kubernetes Locally • 202\n\nThen build the image and load it into your Kind cluster by running:\n\n$ make build-docker $ kind load docker-image github.com/travisjeffery/proglog:0.0.1\n\nNow that we have our Docker image, let’s look at how we can configure and run a cluster of our service in Kubernetes with Helm.\n\nConfigure and Deploy Your Service with Helm\n\nHelm8 is the package manager for Kubernetes that enables you to distribute and install services in Kubernetes. Helm packages are called charts. A chart defines all resources needed to run a service in a Kubernetes cluster—for example, its deployments, services, persistent volume claims, and so on. Charts on Kubernetes are like Debian packages on Debian or Homebrew for- mulas on macOS. As a service developer, you’ll want to build and share a Helm chart for your service to make it easier for people to run your service. (And if you’re dogfooding your own service, you’ll get the same benefit.)\n\nA release is a instance of running a chart. Each time you install a chart into Kubernetes, Helm creates a release. In the Debian package and Homebrew formula examples, releases are like processes.\n\nAnd finally, repositories are where you share charts to and install charts from; they’re like Debian sources and Homebrew taps.\n\nTo install Helm, run this command:\n\n$ curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 \\\n\n| bash\n\nBefore we write our own Helm chart, let’s take Helm for a spin and install an existing chart. Bitnami9 maintains a repository of charts for popular applica- tions. Let’s add a Bitnami repository and install the Nginx chart, which is a web and proxy server:\n\n$ helm repo add bitnami https://charts.bitnami.com/bitnami $ helm install my-nginx bitnami/nginx\n\nWe can see the releases by running $ helm list:\n\n$ helm list NAME my-nginx\n\nNAMESPACE default\n\nREVISION 1\n\nUPDATED 2020...\n\nSTATUS...\n\ndeployed...\n\nLet’s request Nginx to confirm that it’s really running:\n\n8. 9.\n\nhttps://helm.sh\n\nhttps://bitnami.com/kubernetes\n\nreport erratum • discuss\n\nConfigure and Deploy Your Service with Helm • 203\n\n$ POD_NAME=$(kubectl get pod \\\n\n--selector=app.kubernetes.io/name=nginx \\ --template '{{index .items 0 \"metadata\" \"name\" }}')\n\n$ SERVICE_IP=$(kubectl get svc \\\n\n--namespace default my-nginx --template \"{{ .spec.clusterIP }}\")\n\n$ kubectl exec $POD_NAME curl $SERVICE_IP\n\n% Total\n\n% Received % Xferd Average Speed Dload Upload\n\nTime Total\n\nTime Spent\n\nTime Current Left Speed\n\n100 <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> <style>\n\n612 100\n\n612\n\n0\n\n0\n\n597k\n\n0 --:--:-- --:--:-- --:--:-- 597k\n\nbody {\n\nwidth: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif;\n\n} </style> </head> <body> <h1>Welcome to nginx!</h1> <p>If you see this page, the nginx web server is successfully installed and working. Further configuration is required.</p>\n\n<p>For online documentation and support please refer to <a href=\"http://nginx.org/\">nginx.org</a>.<br/> Commercial support is available at <a href=\"http://nginx.com/\">nginx.com</a>.</p>\n\n<p><em>Thank you for using nginx.</em></p> </body> </html>\n\nWe could use the same technique for deploying Nginx in a production envi- ronment, aside from setting some configuration parameters to fit our use case. Helm made it easy to install and configure an Nginx cluster, and we can manage other services the same way.\n\nUninstall the Nginx release by running the following:\n\n$ helm uninstall my-nginx release \"my-nginx\" uninstalled\n\nNow, let’s build our own chart.\n\nBuild Your Own Helm Chart\n\nIn this section, we’ll build a Helm chart for our service and use it to install a cluster in our Kind cluster.\n\nreport erratum • discuss\n\nChapter 10. Deploy Applications with Kubernetes Locally • 204\n\nCreate your Helm chart by running these commands:\n\n$ mkdir deploy && cd deploy $ helm create proglog\n\nHelm created a new chart in a new proglog directory that’s bootstrapped with an example that shows you what a Helm chart looks like—to write your own or to tweak for your own services. The proglog directory contains these directo- ries and files:\n\n. └── proglog\n\n├── charts ├── Chart.yaml ├── templates │ │ │ │ │ │ │ │ └── values.yaml\n\n├── deployment.yaml ├── _helpers.tpl ├── ingress.yaml ├── NOTES.txt ├── serviceaccount.yaml ├── service.yaml └── tests\n\n└── test-connection.yaml\n\n4 directories, 9 files\n\nThe Chart.yaml file describes your chart. You can access the data in this file in your templates. The charts directory may contain subcharts, though I’ve never needed subcharts.\n\nThe values.yaml contains your chart’s default values. Users can override these values when they install or upgrade your chart (for example, the port your service listens on, your service’s resource requirements, log level, and so on).\n\nThe templates directory contains template files that you render with your values to generate valid Kubernetes manifest files. Kubernetes applies the rendered manifest files to install the resources needed for your service. You write your Helm templates using the Go template language.\n\nYou can render the templates locally without applying the resources in your Kubernetes cluster by running $ helm template. This is useful when you’re developing your templates or if you want to apply your changes in a two-step plan-then-apply process because you can see the rendered resources that Kubernetes will apply.\n\nreport erratum • discuss\n\nConfigure and Deploy Your Service with Helm • 205\n\nTo check out the resources Helm would create with the example chart, run this command:\n\n$ helm template proglog\n\nYou’ll see the following:\n\n--- # Source: proglog/templates/serviceaccount.yaml apiVersion: v1 kind: ServiceAccount metadata:\n\nname: RELEASE-NAME-proglog labels:\n\nhelm.sh/chart: proglog-0.1.0 app.kubernetes.io/name: proglog app.kubernetes.io/instance: RELEASE-NAME app.kubernetes.io/version: \"1.16.0\" app.kubernetes.io/managed-by: Helm\n\n--- # Source: proglog/templates/service.yaml «rest»\n\nWe don’t need the example templates, so remove them by running this command:\n\n$ rm proglog/templates/**/*.yaml proglog/templates/NOTES.txt\n\nGenerally, Helm charts include a template file for each resource type. Our service will require two resource types: a StatefulSet and a Service, so we’ll have a statefulset.yaml file and a service.yaml file. Let’s begin with the StatefulSet.\n\nStatefulSets in Kubernetes\n\nYou use StatefulSets to manage stateful applications in Kubernetes, like our service that persists a log. You need a StatefulSet for any service that requires one or more of the following:\n\nStable, unique network identifiers—each node in our service requires\n\nunique node names as identifiers.\n\nStable, persistent storage—our service expects the data its written to\n\npersist across restarts.\n\nOrdered, graceful deployment and scaling—our service needs initial node\n\nto bootstrap the cluster and join subsequent nodes to its cluster.\n\nOrdered, automated rolling updates—we always want our cluster to have a leader, and when we roll the leader we want to give the cluster enough time to elect a new leader before rolling the next node.\n\nreport erratum • discuss\n\nChapter 10. Deploy Applications with Kubernetes Locally • 206\n\nAnd by “stable,” I mean persisted across scheduling changes like restarts and scaling.\n\nIf your service isn’t stateful and doesn’t require these features, then you should use a Deployment instead of a StatefulSet. One example is an API service that persists to a relational database, like Postgres. You’d run the API service with a Deployment because it’s stateless, and you’d run Postgres with a StatefulSet.\n\nCreate a deploy/proglog/templates/statefulset.yaml file with this code:\n\nDeployLocally/deploy/proglog/templates/statefulset.yaml apiVersion: apps/v1 kind: StatefulSet metadata:\n\nname: {{ include \"proglog.fullname\" . }} namespace: {{ .Release.Namespace }} labels: {{ include \"proglog.labels\" . | nindent 4 }}\n\nspec:\n\nselector:\n\nmatchLabels: {{ include \"proglog.selectorLabels\" . | nindent 6 }}\n\nserviceName: {{ include \"proglog.fullname\" . }} replicas: {{ .Values.replicas }} template:\n\nmetadata:\n\nname: {{ include \"proglog.fullname\" . }} labels: {{ include \"proglog.labels\" . | nindent 8 }}\n\nspec:\n\n# initContainers... # containers...\n\nvolumeClaimTemplates: - metadata:\n\nname: datadir\n\nspec:\n\naccessModes: [ \"ReadWriteOnce\" ] resources:\n\nrequests:\n\nstorage: {{ .Values.storage }}\n\nI have omitted the spec’s initContainers and containers fields to make the snippet smaller (we will fill those in next). The only thing of note here is that our StatefulSet has a datadir PersistentVolumeClaim—the claim requests storage for our cluster. Based on our configuration, Kubernetes could fulfill the claim with a local disk, a disk provided by your cloud platform, and so on. Kubernetes takes care of obtaining and binding the storage to your containers.\n\nreport erratum • discuss\n\nConfigure and Deploy Your Service with Helm • 207\n\nNow, replace initContainers... in the previous snippet with this code:\n\nDeployLocally/deploy/proglog/templates/statefulset.yaml initContainers: - name: {{ include \"proglog.fullname\" . }}-config-init\n\nimage: busybox imagePullPolicy: IfNotPresent command:\n\n/bin/sh - -c - |-\n\nID=$(echo $HOSTNAME | rev | cut -d- -f1 | rev) cat > /var/run/proglog/config.yaml <<EOD data-dir: /var/run/proglog/data rpc-port: {{.Values.rpcPort}} # Make sure the following three key-values are on one line each in # your code. I split them across multiple lines to fit them in # for the book. bind-addr: \\\n\n\"$HOSTNAME.proglog.{{.Release.Namespace}}.\\svc.cluster.local:\\\n\n{{.Values.serfPort}}\"\n\nbootstrap: $([ $ID = 0 ] && echo true || echo false) $([ $ID != 0 ] && echo 'start-join-addrs: \\\n\n\"proglog-0.proglog.{{.Release.Namespace}}.svc.cluster.local:\\\n\n{{.Values.serfPort}}\"')\n\nEOD\n\nvolumeMounts: - name: datadir\n\nmountPath: /var/run/proglog\n\nInit containers run to completion before the StatefulSet’s app containers listed in the containers field. Our config init container sets up our service’s configuration file. We configure the first server to bootstrap the Raft cluster. And we configure the subsequent servers to join the cluster. We mount the datadir volume into the container so we can write to the same configuration file our app container will read from later.\n\nReplace containers... in the previous snippet with this:\n\nDeployLocally/deploy/proglog/templates/statefulset.yaml containers: - name: {{ include \"proglog.fullname\" . }}\n\nimage: \"{{ .Values.image.repository }}:{{ .Values.image.tag }}\" ports: - containerPort: {{ .Values.rpcPort }}\n\nname: rpc\n\ncontainerPort: {{ .Values.serfPort }}\n\nname: serf\n\nargs:\n\n--config-file=/var/run/proglog/config.yaml\n\nreport erratum • discuss\n\nChapter 10. Deploy Applications with Kubernetes Locally • 208\n\n# probes... volumeMounts: - name: datadir\n\nmountPath: /var/run/proglog\n\nThese containers define our StatefulSet’s app containers; we need one for our service. We mount the volume to the container for reading the configuration file and persisting the log. We use a flag to tell our service where to find its configuration file.\n\nContainer Probes and gRPC Health Check\n\nKubernetes uses probes to know whether it needs to act on a container to improve your service’s reliability. With a service, usually the probe requests a health check endpoint that responds with the health of the service.\n\nThere are three types of probes:\n\nLiveness probes signal that the container is alive, otherwise Kubernetes will restart the container. Kubernetes calls the liveness probe throughout the container’s lifetime.\n\nReadiness probes check that the container is ready to accept traffic, oth- erwise Kubernetes will remove the pod from the service load balancers. Kubernetes calls the readiness probe throughout the container’s lifetime.\n\nStartup probes signal when the container application has started and Kubernetes can begin probing for liveness and readiness. Distributed services often need to go through service discovery and join in consensus with the cluster before they’re initialized. If we had a liveness probe that failed before our service finished initializing, our service would continually restart. After startup, Kubernetes doesn’t call this probe again.\n\nThese probes should help improve your service’s reliability, but they can cause incidents if they’re not carefully implemented (like the example of the liveness probe that restarts the container before it’s finished initializing). The systems dedicated to improving the reliability of the service can cause more incidents than the service by itself.\n\nYou have three ways of running probes:\n\nMaking an HTTP request against a server; • Opening a TCP socket against a server; and • Running a command in the container (for example, Postgres has a com- mand called pg_isready that connects to a Postgres server).\n\nreport erratum • discuss\n\nConfigure and Deploy Your Service with Helm • 209\n\nThe first two are lightweight because they don’t require any extra binaries in your image. However, a command can be more precise and necessary if you use your own protocol.\n\ngRPC services conventionally use a grpc_health_probe command that expects your server to satisfy the gRPC health checking protocol.10 Our server needs to export a service defined as:\n\nsyntax = \"proto3\";\n\npackage grpc.health.v1;\n\nmessage HealthCheckRequest {\n\nstring service = 1;\n\n}\n\nmessage HealthCheckResponse {\n\nenum ServingStatus {\n\nUNKNOWN = 0; SERVING = 1; NOT_SERVING = 2;\n\n} ServingStatus status = 1;\n\n}\n\nservice Health {\n\nrpc Check(HealthCheckRequest) returns (HealthCheckResponse);\n\nrpc Watch(HealthCheckRequest) returns (stream HealthCheckResponse);\n\n}\n\nLet’s update our server to export the health check service.\n\nOpen internal/server/server.go and add the highlighted imports:\n\nDeployLocally/internal/server/server.go import (\n\n\"context\" \"time\"\n\napi \"github.com/travisjeffery/proglog/api/v1\"\n\ngrpc_middleware \"github.com/grpc-ecosystem/go-grpc-middleware\" grpc_auth \"github.com/grpc-ecosystem/go-grpc-middleware/auth\" grpc_zap \"github.com/grpc-ecosystem/go-grpc-middleware/logging/zap\" grpc_ctxtags \"github.com/grpc-ecosystem/go-grpc-middleware/tags\"\n\n\"go.opencensus.io/plugin/ocgrpc\" \"go.opencensus.io/stats/view\" \"go.opencensus.io/trace\"\n\n10. https://github.com/grpc/grpc/blob/master/doc/health-checking.md\n\nreport erratum • discuss\n\n➤ ➤\n\nChapter 10. Deploy Applications with Kubernetes Locally • 210\n\n\"go.uber.org/zap\" \"go.uber.org/zap/zapcore\"\n\n\"google.golang.org/grpc\" \"google.golang.org/grpc/codes\" \"google.golang.org/grpc/credentials\" \"google.golang.org/grpc/peer\" \"google.golang.org/grpc/status\"\n\n\"google.golang.org/grpc/health\" healthpb \"google.golang.org/grpc/health/grpc_health_v1\"\n\n)\n\nThen, update the NewGRPCServer() function to include the highlighted lines in this snippet:\n\nDeployLocally/internal/server/server.go func NewGRPCServer(config *Config, grpcOpts ...grpc.ServerOption) (\n\ngrpc.Server, error,\n\n) {\n\nlogger := zap.L().Named(\"server\") zapOpts := []grpc_zap.Option{\n\ngrpc_zap.WithDurationField(\n\nfunc(duration time.Duration) zapcore.Field {\n\nreturn zap.Int64(\n\n\"grpc.time_ns\", duration.Nanoseconds(),\n\n)\n\n},\n\n),\n\n}\n\ntrace.ApplyConfig(trace.Config{\n\nDefaultSampler: trace.AlwaysSample(),\n\n}) err := view.Register(ocgrpc.DefaultServerViews...) if err != nil {\n\nreturn nil, err\n\n}\n\ngrpcOpts = append(grpcOpts,\n\ngrpc.StreamInterceptor(\n\ngrpc_middleware.ChainStreamServer(\n\ngrpc_ctxtags.StreamServerInterceptor(), grpc_zap.StreamServerInterceptor(\n\nlogger, zapOpts...,\n\n), grpc_auth.StreamServerInterceptor(\n\nauthenticate,\n\n),\n\n)), grpc.UnaryInterceptor( grpc_middleware.ChainUnaryServer(\n\nreport erratum • discuss\n\n➤ ➤ ➤\n\n➤\n\nConfigure and Deploy Your Service with Helm • 211\n\ngrpc_ctxtags.UnaryServerInterceptor(), grpc_zap.UnaryServerInterceptor(\n\nlogger, zapOpts...,\n\n), grpc_auth.UnaryServerInterceptor(\n\nauthenticate,\n\n),\n\n)),\n\ngrpc.StatsHandler(&ocgrpc.ServerHandler{}),\n\n) gsrv := grpc.NewServer(grpcOpts...)\n\nhsrv := health.NewServer() hsrv.SetServingStatus(\"\", healthpb.HealthCheckResponse_SERVING) healthpb.RegisterHealthServer(gsrv, hsrv)\n\nsrv, err := newgrpcServer(config) if err != nil {\n\nreturn nil, err\n\n} api.RegisterLogServer(gsrv, srv) return gsrv, nil\n\n}\n\nThese lines create a service that supports the health check protocol. We set its serving status as serving so that the probe knows the service is alive and ready to accept connections. Then we register the service with our server so that gRPC can call this service’s endpoints.\n\nReplace probes... in deploy/proglog/templates/statefulset.yaml with this snippet to tell Kubernetes how to probe our service:\n\nDeployLocally/deploy/proglog/templates/statefulset.yaml readinessProbe:\n\nexec:\n\ncommand: [\"/bin/grpc_health_probe\", \"-addr=:{{ .Values.rpcPort }}\"]\n\ninitialDelaySeconds: 10\n\nlivenessProbe:\n\nexec:\n\ncommand: [\"/bin/grpc_health_probe\", \"-addr=:{{ .Values.rpcPort }}\"]\n\ninitialDelaySeconds: 10\n\nThen add these highlighted lines to your Dockerfile to install the grpc_health_probe executable in your image:\n\nDeployLocally/Dockerfile FROM golang:1.14-alpine AS build WORKDIR /go/src/proglog COPY . . RUN CGO_ENABLED=0 go build -o /go/bin/proglog ./cmd/proglog RUN GRPC_HEALTH_PROBE_VERSION=v0.3.2 && \\\n\nreport erratum • discuss\n\n➤ ➤ ➤ ➤\n\n➤\n\nChapter 10. Deploy Applications with Kubernetes Locally • 212\n\nwget -qO/go/bin/grpc_health_probe \\ https://github.com/grpc-ecosystem/grpc-health-probe/releases/download/\\ ${GRPC_HEALTH_PROBE_VERSION}/grpc_health_probe-linux-amd64 && \\ chmod +x /go/bin/grpc_health_probe\n\nFROM scratch COPY --from=build /go/bin/proglog /bin/proglog COPY --from=build /go/bin/grpc_health_probe /bin/grpc_health_probe ENTRYPOINT [\"/bin/proglog\"]\n\nThe last resource we need to define in our Helm chart is the Service.\n\nKubernetes Services\n\nA Service in Kubernetes exposes an application as a network service. You define a Service with policies that specify what Pods the Service applies to and how to access the Pods.\n\nFour types of services specify how the Service exposes the Pods:\n\nClusterIP exposes the Service on a load-balanced cluster-internal IP so the Service is reachable within the Kubernetes cluster only. This is the default Service type.\n\nNodePort exposes the Service on each Node’s IP on a static port—even if the Node doesn’t have a Pod on it, Kubernetes sets up the routing so if you request a Node at the service’s port, it’ll direct the request to the proper place. You can request NodePort services outside the Kubernetes cluster.\n\nLoadBalancer exposes the Service externally using a cloud provider’s load balancer. A LoadBalancer Service automatically creates ClusterIP and NodeIP services behind the scenes and manages the routes to these ser- vices.\n\nExternalName is a special Service that serves as a way to alias a DNS\n\nname.\n\nI don’t recommend using NodePort services (aside from the ones LoadBalancer services create for you). You have to know your nodes’ IPs to use the services, you must secure all your Nodes, and you have to deal with port conflicts. Instead, I recommend using a LoadBalancer or a ClusterIP service if you’re able to run a Pod that can access your internal network.\n\nCreate a deploy/proglog/templates/service.yaml for your service template with the following code:\n\nreport erratum • discuss\n\n➤\n\nAdvertise Raft on the Fully Qualified Domain Name • 213\n\nDeployLocally/deploy/proglog/templates/service.yaml apiVersion: v1 kind: Service metadata:\n\nname: {{ include \"proglog.fullname\" . }} namespace: {{ .Release.Namespace }} labels: {{ include \"proglog.labels\" . | nindent 4 }}\n\nspec:\n\nclusterIP: None publishNotReadyAddresses: true ports:\n\nname: rpc\n\nport: {{ .Values.rpcPort }} targetPort: {{ .Values.rpcPort }}\n\nname: serf-tcp\n\nprotocol: \"TCP\" port: {{ .Values.serfPort }} targetPort: {{ .Values.serfPort }}\n\nname: serf-udp\n\nprotocol: \"UDP\" port: {{ .Values.serfPort }} targetPort: {{ .Values.serfPort }}\n\nselector: {{ include \"proglog.selectorLabels\" . | nindent 4 }}\n\nThis snippet defines our “headless” Service. A headless Service doesn’t load balance to a single IP. You use a headless Service when your distributed service has its own means for service discovery. By defining selectors on our Service, Kubernetes’ endpoint controller changes the DNS configuration to return records that point to the Pods backing the Service. So, each pod will get its own DNS record similar to proglog-{{id}}.proglog.{{namespace}}.svc.cluster.local, and the servers will use these records to discover each other.\n\nAdvertise Raft on the Fully Qualified Domain Name\n\nCurrently, we configure Raft’s address as the transport’s local address, and the server will advertise its address as ::8400. We want to use the fully qualified domain name instead so the node will properly advertise itself to its cluster and to its clients.\n\nIn internal/log/config.go, change your Config to this:\n\nDeployLocally/internal/log/config.go type Config struct {\n\nRaft struct {\n\nraft.Config BindAddr string StreamLayer *StreamLayer Bootstrap\n\nbool\n\n}\n\nreport erratum • discuss\n\n➤\n\nChapter 10. Deploy Applications with Kubernetes Locally • 214\n\nSegment struct {\n\nMaxStoreBytes uint64 MaxIndexBytes uint64 InitialOffset uint64\n\n}\n\n}\n\nChange your DistributedLog’s bootstrap code to use the configured bind address:\n\nDeployLocally/internal/log/distributed.go if l.config.Raft.Bootstrap && !hasState {\n\nconfig := raft.Configuration{\n\nServers: []raft.Server{{\n\nID: Address: raft.ServerAddress(l.config.Raft.BindAddr),\n\nconfig.LocalID,\n\n}},\n\n} err = l.raft.BootstrapCluster(config).Error()\n\n}\n\nAnd in distributed_test.go, update your log configuration to set the address:\n\nDeployLocally/internal/log/distributed_test.go config := log.Config{} config.Raft.StreamLayer = log.NewStreamLayer(ln, nil, nil) config.Raft.LocalID = raft.ServerID(fmt.Sprintf(\"%d\", i)) config.Raft.HeartbeatTimeout = 50 * time.Millisecond config.Raft.ElectionTimeout = 50 * time.Millisecond config.Raft.LeaderLeaseTimeout = 50 * time.Millisecond config.Raft.CommitTimeout = 5 * time.Millisecond config.Raft.BindAddr = ln.Addr().String()\n\nRun your log tests to verify they pass.\n\nFinally, in agent.go, update setupMux() and setupLog() to configure the mux and Raft instance:\n\nDeployLocally/internal/agent/agent.go func (a *Agent) setupMux() error {\n\naddr, err := net.ResolveTCPAddr(\"tcp\", a.Config.BindAddr) if err != nil {\n\nreturn err\n\n} rpcAddr := fmt.Sprintf( \"%s:%d\", addr.IP.String(), a.Config.RPCPort,\n\n) ln, err := net.Listen(\"tcp\", rpcAddr) if err != nil {\n\nreturn err\n\n}\n\nreport erratum • discuss\n\n➤ ➤ ➤ ➤ ➤\n\nAdvertise Raft on the Fully Qualified Domain Name • 215\n\na.mux = cmux.New(ln) return nil\n\n}\n\nfunc (a *Agent) setupLog() error {\n\n// ... logConfig := log.Config{} logConfig.Raft.StreamLayer = log.NewStreamLayer(\n\nraftLn, a.Config.ServerTLSConfig, a.Config.PeerTLSConfig,\n\n) rpcAddr, err := a.Config.RPCAddr() if err != nil {\n\nreturn err\n\n} logConfig.Raft.BindAddr = rpcAddr logConfig.Raft.LocalID = raft.ServerID(a.Config.NodeName) logConfig.Raft.Bootstrap = a.Config.Bootstrap // ...\n\n}\n\nNow we’re ready to deploy the service in our Kubernetes cluster.\n\nInstall Your Helm Chart\n\nWe’ve finished writing our Helm chart and we can install it in our Kind cluster to run a cluster of our service.\n\nYou can see what Helm renders by running:\n\n$ helm template proglog deploy/proglog\n\nYou’ll see that the repository is still set to the default: nginx. Open up deploy/proglog/values.yaml and replace the entire contents to look like this:\n\nDeployLocally/deploy/proglog/values.yaml # Default values for proglog. image:\n\nrepository: github.com/travisjeffery/proglog tag: 0.0.1 pullPolicy: IfNotPresent\n\nserfPort: 8401 rpcPort: 8400 replicas: 3 storage: 1Gi\n\nThe point of the values.yml is to set good defaults and show what parameters users can set if they must.\n\nreport erratum • discuss\n\nChapter 10. Deploy Applications with Kubernetes Locally • 216\n\nNow, install the Chart by running this command:\n\n$ helm install proglog deploy/proglog\n\nWait a few seconds and you’ll see Kubernetes set up three pods. You can list them by running $ kubectl get pods. When all three pods are ready, we can try requesting the API.\n\nWe can tell Kubernetes to forward a pod or a Service’s port to a port on your computer so you can request a service running inside Kubernetes without a load balancer:\n\n$ kubectl port-forward pod/proglog-0 8400 8400\n\nNow we can request our service from a program running outside Kubernetes at :8400.\n\nLet’s write a simple executable to get the list of servers. Create a file named cmd/getservers/main.go that looks like this:\n\nDeployLocally/cmd/getservers/main.go package main\n\nimport (\n\n\"context\" \"flag\" \"fmt\" \"log\"\n\napi \"github.com/travisjeffery/proglog/api/v1\" \"google.golang.org/grpc\"\n\n)\n\nfunc main() {\n\naddr := flag.String(\"addr\", \":8400\", \"service address\") flag.Parse() conn, err := grpc.Dial(*addr, grpc.WithInsecure()) if err != nil {\n\nlog.Fatal(err)\n\n} client := api.NewLogClient(conn) ctx := context.Background() res, err := client.GetServers(ctx, &api.GetServersRequest{}) if err != nil {\n\nlog.Fatal(err)\n\n} fmt.Println(\"servers:\") for _, server := range res.Servers {\n\nfmt.Printf(\"\\t- %v\\n\", server)\n\n}\n\n}\n\nreport erratum • discuss\n\nWhat You Learned • 217\n\nThen, run the command to request our service to get and print the list of servers:\n\n$ go run cmd/getservers/main.go\n\nYou should see the following output:\n\nservers: - id:\"proglog-0\" rpc_addr:\"proglog-0.proglog.default.svc.cluster.local:8400\" - id:\"proglog-1\" rpc_addr:\"proglog-1.proglog.default.svc.cluster.local:8400\" - id:\"proglog-2\" rpc_addr:\"proglog-2.proglog.default.svc.cluster.local:8400\"\n\nThis means all three servers in our cluster have successfully joined the cluster and are coordinating with each other!\n\nWhat You Learned\n\nIn this chapter, you learned the fundamentals of Kubernetes and how to use Kind to set up a Kubernetes cluster that you can run on your machine or on a CI. You also learned how to create a Helm chart and how to install your Helm chart into Kubernetes to run a cluster of your service. You learned quite a lot! In the next chapter, we’ll build on this knowledge and deploy your service on a cloud platform.\n\nreport erratum • discuss\n\nCHAPTER 11\n\nDeploy Applications with Kubernetes to the Cloud\n\nIn the previous chapter, we put the work into making our service deployable, but we only deployed it locally. In this chapter, we’ll deploy our service to the cloud and put it on the Internet. Kubernetes abstracts the resources needed for your applications—containers, networking, volumes, and so on—similar to how Go abstracts the operating system and processor architecture so you can run the same program on each. As such, the changes you need to make to take your local Kubernetes cluster to the cloud can be little to nothing.\n\nThree major cloud platforms dominate the landscape: Google Cloud Platform (GCP),1 Amazon Web Services (AWS),2 and Microsoft Azure.3 All three platforms provide similar feature sets and their own Kubernetes services. With Kuber- netes making up the differences between the platforms, we can deploy to any one, easily move between providers (and bargain with the providers for better prices), or run across them all at the same time. In this chapter, we’ll deploy our service to the Google Cloud Platform.\n\nGCP provides a free tier of products, with limitations, along with $300 credit to spend during your twelve-month free trial. What matters to us for purposes of our work in this book is that the free tier includes one Kubernetes cluster and 5 GB of storage—good enough to deploy our service to the cloud. Though Google won’t charge you for the free trial, you need a credit card to sign up, and during the trial, Google displays a banner showing how many credits and how much time you have left so you know your status. Once your trial\n\n1. 2. 3.\n\nhttps://cloud.google.com\n\nhttps://aws.amazon.com\n\nhttps://azure.microsoft.com/en-us\n\nreport erratum • discuss\n\nChapter 11. Deploy Applications with Kubernetes to the Cloud • 220\n\nis over and/or you decide to purchase the service and use more of the plat- form, Google requires you to enable automatic billing.\n\nCreate a Google Kubernetes Engine Cluster\n\nLet’s start by getting you set up with Google Cloud by creating an account and Google Kubernetes Engine (GKE) cluster and configuring your computer’s Docker and kubectl to work with the cloud services. GKE is GCP’s managed Kubernetes service, enabling you to create a Kubernetes cluster with a single- click. GKE clusters are managed by Google’s Site Reliability Engineers, who ensure that your cluster is available and up-to-date so that you can focus on your applications instead of Kubernetes.\n\nSign Up with Google Cloud\n\nOpen the GCP sign-up form4 and log in to your existing Google account or make a new account. Follow the form instructions, filling in the form with your details until you’ve started your free trial. Then continue to the next step to create a Kubernetes cluster.\n\nCreate a Kubernetes Cluster\n\nNavigate to the Kubernetes Engine service5 and click Create cluster to open the cluster creation form shown in the screenshot that follows. In the form, change the name field from its default cluster-1 to proglog. Keep the location type as its default (Zonal). In the master version section, select the Release channel radio and select the current Regular channel, which is 1.16.11-gke.5 as I write this. Then click the Create button at the bottom of the page. The page will refresh and show a spinner that indicates GCP is provisioning the cluster. You’ll see a green check mark when the cluster is ready, as shown on page 221.\n\nInstall and Authenticate gcloud\n\nGoogle Cloud provides a cloud software development kit (SDK) with various tools and libraries for working with Google’s services. The SDK includes the gcloud CLI, which we need to interact with the Google Cloud APIs and config- ure Docker. Install the latest Cloud SDK by following the installation instructions for your OS from the Google Cloud Developer Tools page.6\n\n4. 5. 6.\n\nhttps://console.cloud.google.com/freetrial/signup/tos?pli=1\n\nhttps://console.cloud.google.com/kubernetes\n\nhttps://cloud.google.com/sdk/docs/downloads-versioned-archives\n\nreport erratum • discuss\n\nCreate a Google Kubernetes Engine Cluster • 221\n\nAfter you’ve installed the gcloud CLI, authenticate the CLI for your account by running this command:\n\n$ gcloud auth login\n\nNow that you’ve authenticated the CLI, you can run gcloud commands against resources in your account. Get your project’s ID, and configure gcloud to use the project by default by running the following:\n\n$ PROJECT_ID=$(gcloud projects list | tail -n 1 | cut -d' ' -f1) $ gcloud config set project $PROJECT_ID\n\nWe’ll refer this PROJECT_ID environment variable several times, so if you make a new terminal session, make sure you set the variable again.\n\nreport erratum • discuss",
      "page_number": 196
    },
    {
      "number": 11,
      "title": "Deploy Applications with Kubernetes to the Cloud • 220",
      "start_page": 221,
      "end_page": 245,
      "detection_method": "regex_chapter_title",
      "content": "Chapter 11. Deploy Applications with Kubernetes to the Cloud • 222\n\nPush Our Service’s Image to Google’s Container Registry\n\nWe need to make our service’s image pullable by our GKE cluster’s nodes by pushing its image to Google’s Container Registry. Run the following to push the image to the registry:\n\n$ gcloud auth configure-docker $ docker tag github.com/travisjeffery/proglog:0.0.1 \\\n\ngcr.io/$PROJECT_ID/proglog:0.0.1\n\n$ docker push gcr.io/$PROJECT_ID/proglog:0.0.1\n\nThe first line configures Docker to use Google’s Container Registry and use gcloud as the credential helper for those registries. You can open your Docker configuration file (at ~/.docker/config.json by default) to see the configuration changes. The second line creates a new tag for the gcr.io registry name. The gcr.io registry hosts images in the United States (though that may change). You’ll also find us.gcr.io, eu.gcr.io, and asia.gcr.io if you need your images in specific regions. The third line pushes the image to the registry.\n\nConfigure kubectl\n\nThe last bit of setup allows kubectl and Helm to call our GKE cluster:\n\n$ gcloud container clusters get-credentials proglog --zone us-central1-c Fetching cluster endpoint and auth data. kubeconfig entry generated for proglog.\n\nThis command updates your kubeconfig file (at ~/.kube/config by default) with the credentials and configuration to point kubectl at your cluster in GKE. Helm uses the kubeconfig file, too.\n\nOkay, we’ve set up our Google Cloud project, created a GKE cluster, and configured our clients to manage the cluster. We could deploy our service as- is to GKE, but Kubernetes won’t make our service available on the Internet with our current deployment setup.\n\nLet’s fix that.\n\nCreate Custom Controllers with Metacontroller\n\nWe could deploy our service with no changes and our service would function the same as it did in our local Kind cluster. But we want to extend our deployment setup to expose our service on the Internet. Because our service load balances client-side, each pod needs its own static IP, so we need a load balancer service for each pod. It’d be nice for Kubernetes to automatically create the load balancers as the pods scale up and delete them as the pods scale down, but Kubernetes doesn’t support this out of the box.\n\nreport erratum • discuss\n\nCreate Custom Controllers with Metacontroller • 223\n\nEnter Metacontroller.\n\nMetacontroller7 is a Kubernetes add-on that makes it easy to write and deploy custom controllers with simple scripts. Metacontroller lets us hook into Kubernetes’ changes so that we can compose with our own changes. Metacon- troller handles all the interactions with Kubernetes’ API, including running a level-triggered reconciliation loop on your behalf. You just receive JSON describing Kubernetes’ observed state and return JSON describing your desired state. You can build features in Kubernetes that would require writing an Operator8 (a popular pattern for extending Kubernetes), with less code and effort than an Operator requires.\n\nInstall Metacontroller\n\nTo install Metacontroller, we need to apply a couple YAML files that define Metacontroller’s APIs and RBAC authorization that enable the APIs to manage the Kubernetes cluster’s resources. You can use two Metacontroller APIs:\n\nCompositeController, which is used to manage child resources based on some parent resource. The Deployment and StatefulSet controllers fit this pattern.\n\nDecoratorController, which is used to add behavior to a resource. This is the controller pattern we need and will build for our service-per-pod feature.\n\nNext, we use Helm to install Metacontroller. From the root of your project, run the following commands to define the Metacontroller Helm chart:\n\n$ cd deploy $ helm create metacontroller $ rm metacontroller/templates/**/*.yaml \\\n\nmetacontroller/templates/NOTES.txt \\ metacontroller/values.yaml\n\n$ MC_URL=https://raw.githubusercontent.com\\ /GoogleCloudPlatform/metacontroller/master/manifests/ $ curl -L $MC_URL/metacontroller-rbac.yaml > \\\n\nmetacontroller/templates/metacontroller-rbac.yaml\n\n$ curl -L $MC_URL/metacontroller.yaml > \\\n\nmetacontroller/templates/metacontroller.yaml\n\nThen install the Metacontroller chart by running these:\n\n$ kubectl create namespace metacontroller $ helm install metacontroller metacontroller\n\n7. 8.\n\nhttps://metacontroller.app\n\nhttps://coreos.com/blog/introducing-operators.html\n\nreport erratum • discuss\n\nChapter 11. Deploy Applications with Kubernetes to the Cloud • 224\n\nNow we can update our proglog chart to support our service-per-pod feature and then deploy our service to the cloud.\n\nAdd Service-per-Pod Load Balancer Hooks\n\nWe’ll create a DecoratorController that adds a load balancer service for each pod in our service’s StatefulSet.\n\nCreate a deploy/proglog/templates/service-per-pod.yaml file with the following code to define our DecoratorController and Metacontroller configuration:\n\nDeployToCloud/deploy/proglog/templates/service-per-pod.yaml {{ if .Values.service.lb }} apiVersion: metacontroller.k8s.io/v1alpha1 kind: DecoratorController metadata:\n\nname: service-per-pod\n\nspec:\n\nresources: - apiVersion: apps/v1\n\nresource: statefulsets annotationSelector: matchExpressions: - {key: service-per-pod-label, operator: Exists} - {key: service-per-pod-ports, operator: Exists}\n\nattachments: - apiVersion: v1\n\nresource: services\n\nhooks:\n\nsync:\n\nwebhook:\n\nurl: \"http://service-per-pod.metacontroller/create-service-per-pod\"\n\nfinalize:\n\nwebhook:\n\nurl: \"http://service-per-pod.metacontroller/delete-service-per-pod\"\n\nOur DecoratorController decorates every StatefulSet with the service-per-pod- label and service-per-pod-ports annotations. The hooks field defines which hooks the controller will call. The sync hook should create and maintain the resources you desire for your StatefulSet. The finalize adds a finalizer to the StatefulSet that prevents Kubernetes from deleting the StatefulSet until the hook has had its chance to run and clean up its resources. Currently Metacontroller supports running webhooks, so we need an internal service and deployment to run the webhooks.\n\nPut this snippet after the previous snippet to define the webhook service and its configuration:\n\nreport erratum • discuss\n\nCreate Custom Controllers with Metacontroller • 225\n\nDeployToCloud/deploy/proglog/templates/service-per-pod.yaml --- apiVersion: v1 kind: ConfigMap metadata:\n\nnamespace: metacontroller name: service-per-pod-hooks\n\ndata: {{ (.Files.Glob \"hooks/*\").AsConfig | indent 2 }} --- apiVersion: apps/v1 kind: Deployment metadata:\n\nname: service-per-pod namespace: metacontroller\n\nspec:\n\nreplicas: 1 selector:\n\nmatchLabels:\n\napp: service-per-pod\n\ntemplate:\n\nmetadata: labels:\n\napp: service-per-pod\n\nspec:\n\ncontainers: - name: hooks\n\nimage: metacontroller/jsonnetd:0.1 imagePullPolicy: Always workingDir: /hooks volumeMounts: - name: hooks\n\nmountPath: /hooks\n\nvolumes: - name: hooks configMap:\n\nname: service-per-pod-hooks\n\n--- apiVersion: v1 kind: Service metadata:\n\nname: service-per-pod namespace: metacontroller\n\nspec:\n\nselector:\n\napp: service-per-pod\n\nports: - port: 80\n\ntargetPort: 8080\n\n{{ end }}\n\nreport erratum • discuss\n\nChapter 11. Deploy Applications with Kubernetes to the Cloud • 226\n\nThis code snippet defines our webhook, Deployment and Service, with a ConfigMap that mounts our hook code files. Our controller calls the http://service- per-pod.metacontroller/create-service-per-pod endpoint when the StatefulSet changes, and calls the http://service-per-pod.metacontroller/delete-service-per-pod endpoint when the StatefulSet is deleted. The paths of the endpoints match the names of our hook filenames.\n\nCreate a hooks directory to put the hook code in:\n\n$ mkdir deploy/proglog/hooks\n\nAdd the hook to create the services by adding this create-service-per-pod.jsonnet file in the hooks directory:\n\nDeployToCloud/deploy/proglog/hooks/create-service-per-pod.jsonnet function(request) {\n\nlocal statefulset = request.object, local labelKey = statefulset.metadata.annotations[\"service-per-pod-label\"], local ports = statefulset.metadata.annotations[\"service-per-pod-ports\"],\n\nattachments: [\n\n{\n\napiVersion: \"v1\", kind: \"Service\", metadata: {\n\nname: statefulset.metadata.name + \"-\" + index, labels: {app: \"service-per-pod\"}\n\n}, spec: {\n\ntype: \"LoadBalancer\", selector: {\n\n[labelKey]: statefulset.metadata.name + \"-\" + index\n\n}, ports: [\n\n{\n\nlocal parts = std.split(portnums, \":\"), port: std.parseInt(parts[0]), targetPort: std.parseInt(parts[1]),\n\n} for portnums in std.split(ports, \",\")\n\n]\n\n}\n\n} for index in std.range(0, statefulset.spec.replicas - 1)\n\n]\n\n}\n\nreport erratum • discuss\n\nCreate Custom Controllers with Metacontroller • 227\n\nWe’ve implemented our hook in Jsonnet,9 a data templating language that simply extends JSON with variables, conditionals, arithmetic, functions, imports, and errors. Kubernetes passes the StatefulSet we’ve decorated into the function. Our implementation iterates over each replica in the StatefulSet and builds a list of service attachments. We can attach arbitrary resources that are only connected to the target resource through owner references, meaning Kubernetes will delete them if the StatefulSet is deleted.\n\nNext, add the hook to delete the service:\n\nDeployToCloud/deploy/proglog/hooks/delete-service-per-pod.jsonnet function(request) { attachments: [], finalized: std.length(request.attachments['Service.v1']) == 0\n\n}\n\nIf the StatefulSet doesn’t match our decorator selector or the StatefulSet is deleted, then we delete any attachments we’ve made. If we observe that all the services are gone, we mark the StatefulSet as finalized so Kubernetes can delete it.\n\nLast, we must update our StatefulSet and set the annotations that signal Kubernetes to decorate this StatefulSet and create a service for each pod. Change the StatefulSet’s metadata defined in statefulset.yaml to include these annotations:\n\nDeployToCloud/deploy/proglog/templates/statefulset.yaml apiVersion: apps/v1 kind: StatefulSet metadata:\n\nname: {{ include \"proglog.fullname\" . }} namespace: {{ .Release.Namespace }} labels: {{ include \"proglog.labels\" . | nindent 4 }} {{ if .Values.service.lb }} annotations:\n\nservice-per-pod-label: \"statefulset.kubernetes.io/pod-name\" service-per-pod-ports: \"{{.Values.rpcPort}}:{{.Values.rpcPort}}\"\n\n{{ end }}\n\nspec:\n\n# ...\n\nAnd that’s all of our Metacontroller changes. Our service should create a load balancer service for each pod now. Let’s deploy our service to our GKE cluster and try it!\n\n9.\n\nhttps://jsonnet.org\n\nreport erratum • discuss\n\nChapter 11. Deploy Applications with Kubernetes to the Cloud • 228\n\nDeploy to the Internet\n\nThis is the moment we’ve been building up to over the course of the book: deploying our distributed service to the cloud. Run the following command:\n\n$ helm install proglog proglog \\\n\n--set image.repository=gcr.io/$PROJECT_ID/proglog \\ --set service.lb=true\n\nThis command installs our proglog chart to our GKE cluster. We’ve set the image repository to configure the StatefulSet to pull the image from the Google Container Registry. And we’ve enabled the service-per-pod controller. You can watch as the services come up by passing the -w flag:\n\n$ kubectl get services -w\n\nWhen all three load balancers are up, we can verify that our client connects to our service running in the cloud and that our service nodes discovered each other:\n\n$ ADDR=$(kubectl get service \\\n\nl app=service-per-pod \\ -o go-template=\\ '{{range .items}}\\\n\n{{(index .status.loadBalancer.ingress 0).ip}}{{\"\\n\"}}\\\n\n{{end}}'\\ | head -n 1)\n\n$ go run cmd/getservers/main.go -addr=$ADDR:8400 servers: - id:\"proglog-0\" rpc_addr:\"proglog-0.proglog.default.svc.cluster.local:8400\" - id:\"proglog-1\" rpc_addr:\"proglog-1.proglog.default.svc.cluster.local:8400\" - id:\"proglog-2\" rpc_addr:\"proglog-2.proglog.default.svc.cluster.local:8400\"\n\nWhat You Learned\n\nCongratulations! You deployed your service to the cloud. Now any person on the Internet can use your service. You set up a Google Cloud account, a project, and a GKE cluster. You also learned how to write a simple controller to extend the behavior of Kubernetes resources with Metacontroller.\n\nWe’ve now reached the end of the book, and you’ve accomplished a lot. You’ve made a distributed service from scratch. You’ve learned distributed computing ideas like service discovery, consensus, and load balancing. You’re ready to make your own distributed services and contribute to existing projects.10\n\nGo leave your mark on this growing field!\n\n10. https://github.com/avelino/awesome-go#distributed-systems\n\nreport erratum • discuss\n\nSYMBOLS & operator, 20 * wildcard, 93 . (dot), 20\n\nDIGITS 1.14-alpine image, 201\n\nA Accept(), 158 access control, 78, 88–98,\n\n223\n\naccess control lists, 88–98 ACL tables, 85, 93 active segment, 25, 37 Addr field, 180 Addr(), 158 addresses\n\ndynamic configuration\n\nwith Viper, 199\n\nlisteners, 158 service discovery with custom pickers, 184 service discovery with\n\ncustom resolvers, 174, 179–180\n\nagents\n\nabout, 129 CLI agent for deploying\n\nwith Kubernetes, 193, 196–202\n\nclient-side load balanc-\n\ning, custom, 188\n\ncreating, 130 multiplexing Raft, 164–\n\n169\n\nreplication, 129–139\n\nAmazon Web Services (AWS)\n\ndeploying to, 219 Elastic Load Balancer\n\n(ELB), 172\n\nAny(), 168 api directory, 17 APIv2 (Go), 19 appending\n\nbuilding index, 31–37 building log code, 44–51 building segment, 37–40 building store, 26–31 in prototype for proglog\n\nproject, 7–11\n\nreplication with consen- sus, 149, 152, 155, 163\n\ntesting index, 35 testing log code, 49–51 testing segments, 41 testing store, 29–31 understanding, 25, 33\n\nApply(), finite-state machines,\n\n151–153, 155\n\nasia.gcr.io registry, 222 Attributes field, 180 authentication about, 76 vs. authorization, 78, 88 defined, 77 gcloud CLI, 221 mutual, 77, 86 with TLS, 76–88\n\nauthorization\n\nabout, 76, 78 with access control, 78,\n\n88–98, 223\n\nACL tables, 85, 93\n\nIndex\n\nvs. authentication, 78, 88 Metacontroller, 223 testing, 91–98\n\nAWS (Amazon Web Services)\n\ndeploying to, 219 Elastic Load Balancer\n\n(ELB), 172\n\nAzure, 219\n\nB backward compatibility\n\ngRPC advantages, 57 with protobufs, 13, 15\n\nbase balancers, 172, 186 base offsets, 37, 39, 44–45 Basecamp, 5 bin directory, 17 BindAddr, 118, 198 BindPort, 118 Bitnami, 202 Bolt, 147 bootstrapping\n\nclusters, 149, 162, 205,\n\n207\n\nlogs, 44, 50 segments, 44 servers, 149, 153, 162,\n\n205, 207 Build(), 177–179, 184 bumping major versions, 18\n\nC C (country), certificate author-\n\nity configuration, 80\n\ncandidates, consensus with\n\nRaft, 142\n\ncapacity, metrics, 101\n\nCasbin, 89, 92 certificates\n\ncertificate authority (CA) with CFSSL, 78–86\n\ngenerating, 82, 87 generating multiple, 90 replication testing, 135 Root CA, 85 server names, 180 with TLS, 77–88\n\nCFSSL, certificate authority\n\n(CA) with, 78–86\n\ncfssl tool, 79–82 cfssljson tool, 79–82 Cgo, disabling, 201 channels\n\nclosing, 126–134 replication, 124–134 service discovery with Serf, 118, 120, 124, 126–128\n\nChart.yaml file, 204 charts\n\nbuilding Helm charts,\n\n203–213 defined, 202 deploying locally with\n\nHelm, 202–217\n\ninstalling Helm charts,\n\n215–217\n\nMetacontroller, 223 Nginx, 202 subcharts, 204\n\nclient-side load balancing custom pickers, 175,\n\n177, 183–189\n\ncustom resolvers, 173–\n\n183\n\ndefined, 171 on gPRC, 172–174 testing, 175, 181–183,\n\n186–189\n\nclients, see also client-side\n\nload balancing\n\nauthentication with TLS,\n\n76–77, 86\n\nauthorization, 78, 88–98 defining gPRC service, 59 generating in gPRC, 60 generating multiple certifi-\n\ncates, 90\n\nhandling in gPRC, 63–68 nobody, 92, 95 replication, 124–134 root, 92, 94 service discovery and,\n\n114, 171\n\nsuperuser, 92 testing gPRC, 68–73\n\nclosing\n\nagents, 133, 136 channels, 126–134 consensus with Raft,\n\n158, 161, 168\n\nindexes, 33 listeners, 158 logs, 45, 133 multiplexing Raft, 168 observability, 108 replication, 126–134 resolvers, 180 segments, 40, 46 store, 29–30\n\ncloud\n\ncustom controllers, 222–\n\n227\n\ndeploying to with Kuber-\n\nnetes, 219–228\n\nCloudFlare, 79 ClusterIP Service, 212 clusters\n\nbootstrapping, 149, 162,\n\n205, 207\n\nchecking local member-\n\nship, 120\n\nconfiguring for agents,\n\n132\n\ncreating Google Kuber- netes Engine (GKE), 220\n\ndeploying locally with Kubernetes, setup, 194–195\n\ndeploying locally, CLI agent for, 196–202 deploying locally, with\n\nHelm, 202–217\n\ndeploying to cloud, cus- tom controllers, 222– 227\n\nmultiplexing Raft, 168 number of servers in a Raft cluster, 143 removing servers from Raft cluster, 160\n\nreplication with consen- sus, implementing, 144–161\n\nreplication with consen- sus, testing, 161–163, 169\n\nreplication with consen- sus, understanding, 141–144 resiliency, 118\n\nIndex • 230\n\nservice discovery and replication, testing, 134–139\n\nservice discovery with\n\nSerf, integrating with consensus, 158–161 service discovery with\n\nSerf, replication, 123– 134\n\nservice discovery with\n\nSerf, setup, 116–123 snapshotting with Serf,\n\n120\n\nstatus, 121 clusters, Kubernetes\n\ndebugging, 195 running in Docker, 195,\n\n202, 215\n\nCN (Common Name), certifi-\n\ncates, 80, 87\n\nCobra, 196 CockRoachDB, 143 code\n\nfor this book, xiii working with generated,\n\n20\n\ncodes package, 63 commit logs, see also config- uring; consensus; load bal- ancing; proglog project; replication; service discov- ery\n\nabout, 6, 23, 25 advantages of, 23–25 building index, 31–37 building segments, 37–42 building store, 26–31 closing, 45, 133 coding log, 42–51 interface, 67 observability, 103–109 prototype for proglog\n\nproject, 6–11\n\nremoving, 45 resetting, 45, 50 restoring, 47, 50 setup, 43, 131 testing log code, 48–51\n\nCommon Name (CN), certifi-\n\ncates, 80, 87\n\ncompilers\n\nperformance, 19 protobuf, 16, 19, 58, 60\n\nCompositeController, 223 configuring\n\nagents, 129\n\nauthorization with Cas-\n\nbin, 92\n\ncertificate authority (CA) with CFSSL, 79–86 certificates for replication\n\ntesting, 135\n\nclient-side load balanc-\n\ning, custom, 175–177, 180\n\ndefaults, 43 deploying locally with\n\nKubernetes, 197–201, 207\n\ndeploying to cloud, 220–\n\n222, 224\n\ndynamic configuration\n\nwith Viper, 196, 198– 201\n\ngcloud CLI, 221 index, 36 Metacontroller, 224 multiplexing Raft, 166–\n\n169\n\nobservability, 105 Raft, 146–149, 153, 175–\n\n177\n\nreplication, 124 segments, 37 service discovery with\n\nSerf, 116–118\n\nstore, 36 TLS for consensus with\n\nRaft, 157\n\nZap, 105\n\nconsensus\n\nabout, 24 adding servers as non-\n\nvoters, 159 API for, 149–151 errors, 151, 160 leader election, 142 performance, 148, 151,\n\n163\n\nwith Raft, implementing,\n\n144–161\n\nwith Raft, integrating\n\nwith discovery, 158– 161\n\nwith Raft, setup, 144–149 with Raft, testing, 161–\n\n163, 169\n\nwith Raft, understanding,\n\n141–144\n\nreading whole log and, 47 testing, 161–163, 169 timeouts, 148, 160, 162,\n\n189\n\nconsistency, Raft and, 151\n\nconst, 82 Consul, 115, 118, 129, 141 consumers, see also gRPC\n\nclient-side load balanc- ing, custom, 176, 187 configuring for authoriza-\n\ntion, 96–98\n\nerror messages, 66 prototype for proglog\n\nproject, 7–11\n\nreplication, 123–139\n\ncontainers\n\nabout, 193 Google Container Reg-\n\nistry, 222\n\nhealth checks with probes, 208–212\n\ninitContainers, 206 StatefulSets and, 206\n\ncontrollers\n\ndeploying locally, 194 deploying to cloud, cus-\n\ntom, 222–227\n\ncounters, 100 country (C), certificate author-\n\nity configuration, 80\n\ncredentials, security with gR-\n\nPC, 57\n\ncryptography conventions, 76\n\nD debugging\n\nflags, 107 Kubernetes clusters, 195 DecoratorController, 223–227 DeleteRange(), 156 deleting\n\nindexes and stores, 40 logs, 45 records with Raft, 156 segments, 25 StatefulSets, 227 test data, 136\n\ndeploying\n\nCLI agent for, 193 with custom controllers,\n\n222–227\n\nwith Kubernetes, CLI agent for, 196–202 with Kubernetes, Helm,\n\n202–217\n\nwith Kubernetes, setup,\n\n194–195\n\nwith Kubernetes, to cloud, 219–228\n\nIndex • 231\n\nDeployments (Kubernetes),\n\n206\n\nDesigning Data-Intensive Ap-\n\nplications, xi\n\nDetails(), 65 dial options, 86, 132 Dial(), 157, 173 directories\n\nconventions, 17, 19 Helm charts, 204 hooks, 226 protobufs, 17 segments, 43\n\ndiscovery, see service discov-\n\nery\n\ndistributed services, see al-\n\nso configuring; consensus; gRPC; load balancing; replication; service discov- ery\n\nadvantages of, 4, 55 goals, 56–58\n\nDocker\n\n1.14-alpine image, 201 adding probes to, 211 building image, 201 deploying to cloud, 220,\n\n222\n\ndeploying with Kuber-\n\nnetes CLI agent, 196– 202\n\nDockerfile, creating, 201 running Kubernetes\n\nclusters in with Kind, 195, 202, 215\n\nscratch (empty) image, 201\n\ndomain types, defining as\n\nprotobufs, 17–19\n\nDone, 185 dot (.), 20 dynamic configuration, 196,\n\n198–201\n\nE Elastic Load Balancer (ELB),\n\n172\n\nElasticsearch, 5, 103 ELB (Elastic Load Balancer),\n\n172 elections\n\nabout, 142 adding servers as non-\n\nvoters, 159\n\nclient-side load balanc-\n\ning, custom, 175\n\nnode tags, 118 timeouts for, 148, 160,\n\n162, 189\n\nenc, 27 encoding, 11, 27 encryption\n\nabout, 76 consensus with Raft,\n\n157, 164–169\n\nwith TLS, 57, 76–86,\n\n157, 164–169\n\nEnforce (Casbin), 90 entWidth, 32 environment variables\n\nPATH, 17 PROJECT_ID, 221\n\nerrdetails package, 64 Error(), 151 errors\n\nclient-side load balanc-\n\ning, custom, 186\n\ncommit log prototype, 7,\n\n10\n\nconsensus, 151, 160 default description, 63 detailed description, 64 handling in gPRC, 63–\n\n68, 72\n\nmetrics, 101 replication, 128, 151 service discovery, 120 status codes, 63–65 testing range errors, 49\n\nEtcd, 5, 141 eu.gcr.io registry, 222 event channels\n\nclosing, 126–134 replication, 124–134 service discovery with\n\nSerf, 118, 120\n\nEventCh, 118 Eventually(), 163 ext files and logs, 23 extensibility and protobufs,\n\n15\n\nexternal load balancing, 171 ExternalName Service, 212\n\nF fields\n\naccessing with . (dot), 20 field versioning, 18, 57 Go syntax, 18 protobuf syntax, 18 file paths, returning index, 35\n\nfiles\n\ncreating, 38 re-creating store from, 27 size, 27\n\nfinalize, 224 finite-state machines calling, 155 creating, 146 defining type, 151 methods, 151 Raft setup, 146–149, 153 replication with consen- sus, 143, 151–154\n\nflags\n\ncustom flag values, 198 deploying with Kuber-\n\nnetes CLI agent, 196– 202\n\nflag parsing, 107 persistent flags with Co-\n\nbra, 197\n\nflushing\n\nclosing indexes, 33 writer buffer, 28\n\nfollowers, in replication with\n\nconsensus, 142\n\nFSM, see finite-state ma-\n\nchines\n\nG gauges, 100 gcloud CLI, 220 GCP (Google Cloud Platform) deploying to, 219–228 sign up, 220 gcr.io registry, 222 gencert, 82, 87, 90 GET, JSON/HTTP servers, 8 getters, 20 GKE (Google Kubernetes En-\n\ngine)\n\ncustom controllers, 222–\n\n227\n\ndeploying to cloud with,\n\n220–228\n\nGo\n\nabout, xi advantages, 3, 5 API version, 19 conventions, 17, 19 converting into protobuf\n\nsyntax, 17\n\nencoding, 11, 27 runtimes, 19 syntax, 18 version, xii\n\nIndex • 232\n\ngodoc, 57 gogoprotobuf compiler, 19 Google, see also gRPC\n\nCloud Developer Tools,\n\n220\n\nContainer Registry, 222 metric recommendations,\n\n101\n\nprotobufs use, 13\n\nGoogle Cloud Platform (GCP) deploying to, 219–228 sign up, 220\n\nGoogle Kubernetes Engine\n\n(GKE)\n\ncustom controllers, 222–\n\n227\n\ndeploying to cloud with,\n\n220–228 gorilla/mux library, 8 goroutines, testing gRPC in,\n\n70 gRPC\n\nabout, 55 advantages, 56–58 client-side load balanc-\n\ning, 172–174\n\nclient-side load balanc- ing, custom pickers, 172, 177, 183–189 client-side load balanc- ing, custom resolvers, 173–183\n\ndial options, 86, 132 error handling, 63–68, 72 health checks with probes, 208–212\n\ninstalling, 60 with interceptors, 97 log interface, 67 multiplexing Raft, 164–\n\n169\n\nprotobufs advantages, 13 resources on, 57–58 server, creating, 68 server, implementing, 60–\n\n63\n\nserver, observability,\n\n103–109\n\nserver, options, 86 server, registering, 68 server, testing, 68–73 service, defining, 58–60 testing, 68–73\n\ngrpc_health_probe, 209\n\nH handlers\n\nprototype for proglog\n\nproject, 7–11\n\nservice discovery integra- tion with consensus, 158–161\n\nservice discovery with\n\nSerf, 119–120 handshake, TLS, 77 Hashicorp, 115, 128 headless Services (Kuber-\n\nnetes), 213\n\nhealth checks, 208–212 heartbeat, leader, 142 Helm\n\nabout, 194, 202 building charts, 203–213 deploying to cloud with,\n\n222–228\n\ndeploying with, 202–217 installing Metacontroller,\n\n223\n\ninstalling charts, 215–\n\n217 HighestOffset(), 46 histograms, 100 hooks\n\nCobra, 197 deploying to cloud with Metacontroller, 224\n\nhosts field, certificate authority\n\nconfiguration, 81\n\nHTTP\n\ngRPC advantages, 58 HTTP/2, 58 JSON/HTTP, role in dis- tributed systems, 4\n\nprobes, 208\n\nI import paths, 5 include directory, 17 indexes\n\nabout, 7, 25 appending records in prototype, 7–11\n\nbuilding, 31–37 building log code, 45 closing, 33 configuring, 36 creating, 32 defined, 26 memory-mapping, 26,\n\n32–33\n\nperformance, 25\n\nremoving, 40 returning file path, 35 size, 32, 34, 36, 40, 42 testing, 35 truncating, 33–34, 36\n\ninit()\n\nreplication, 127 service discovery with custom pickers, 185 service discovery with\n\ncustom resolvers, 179\n\ninitContainers, 206 initialization, lazy, 127 installing\n\nCasbin, 89 CLIs for CFSSL, 79 gcloud CLI, 220 gRPC package, 60 Helm, 202 Helm charts, 215–217 Kind, 195 kubectl, 194 Metacontroller, 223 protobuf compiler, 16, 19 Raft, 144 Serf, 116\n\ninstances, creating, 20 interceptors, 97 interface, log, 67 internal packages, 60 isLocal(), 120 Ivy, 128\n\nJ Jaegar, 102 Jocko, 4 Join, 158 JSON\n\ncertificate authority (CA) with CFSSL, 79–82 JSON/HTTP, role in dis- tributed systems, 4 JSON/HTTP, setup, 5–11 Jsonnet, 227 uses, 13 Jsonnet, 227\n\nK Kafka\n\nconsensus with Raft, 141 Jocko, 4 service discovery, 174 structured logs, 102 key field, certificate authority\n\nconfiguration, 80\n\nIndex • 233\n\nkeys\n\nauthentication with TLS,\n\n77\n\ncertificate authority con-\n\nfiguration, 80\n\ngenerating for certificates,\n\n82\n\nKind, 195, 202, 215 Kleppmann, Martin, xi kubectl, 194, 220, 222 Kuberesolver, 173 Kubernetes\n\nabout, 193 consensus with Raft, 141 deploying locally with CLI\n\nagent, 196–202 deploying locally with\n\nHelm, 202–217\n\ndeploying to cloud, 219–\n\n228\n\ndeploying to cloud, cus- tom controllers, 222– 227\n\nwith gogoprotobuf compil-\n\ner, 19\n\nGoogle Kubernetes En- gine (GKE), creating, 220\n\nhealth checks with probes, 208–212 JSON/HTTP use, 5 Kuberesolver, 173 manifest files, 204 running services inside\n\ncontainers without load balancers, 216 Services, 205, 212 setup, 194–195 StatefulSets, 205–208,\n\n223–227\n\nL L (locality), certificate author-\n\nity configuration, 80\n\nlanguages\n\ngRPC advantages, 58 protobufs advantages, 15 protobufs compiler, 19 status codes, 63\n\nlatency\n\nload balancing and, 172–\n\n173\n\nmetrics, 101 Raft elections, 148, 189 replication, 137 lazy initialization, 127\n\nleaders, election in replication\n\nwith consensus, 142\n\nLeave, 120, 158 lenWidth, 27 Lightstep, 102 Listen, 136 listeners\n\nconsensus with Raft, 158 JSON/HTTP server, building, 7–11\n\nmultiplexing Raft, 166–\n\n169\n\nservice discovery ports,\n\n136\n\ntesting setup, 70 liveness probes, 208 load balancing about, 171 base balancers, 172, 186 client-side, defined, 171 client-side, on gPRC,\n\n172–174\n\ncustom controllers for deploying to cloud, 222–227 external, 171 gRPC advantages, 58 Kubernetes LoadBalancer\n\nService, 212\n\npickers, custom, 175,\n\n177, 183–189\n\nreadiness probes, 208 resolvers, custom, 173–\n\n183\n\nround-robin, 173, 185 scaling and, 114 with server proxies, 171 vs. service discovery, 113 strategies, 171 testing, 175, 181–183,\n\n186–189 types of, 58\n\nLoadBalancer Service, 212 LocalID, 148 locality (L), certificate author-\n\nity configuration, 80\n\nLocalizedMessage, 64 locks, 45 logError(), 120, 160 logs, see also commit logs;\n\nproglog project\n\nadvantages of, 23–25,\n\n101\n\nbootstrapping, 44, 50 compacting, 153 ext files, 23\n\nstructured logs, 99, 101,\n\n103–109\n\nas term, 25–26\n\nLowestOffset(), 46\n\nM maintainability, gRPC advan-\n\ntages, 57\n\nMakefile, creating, 20 man-in-the-middle attacks\n\n(MITM), 76 Members(), 120 membership, see clusters memory\n\nin-memory log interface,\n\n67\n\nmemory-mapping index\n\nfiles, 26, 32–33\n\nmetrics, 101 storing offsets, 34 Metacontroller, 222–227 metadata\n\npickers, 185 Raft, 147\n\nmetrics, 99–101, 103–109 Microsoft Azure, 219 middleware, 97 MITM (man-in-the-middle at-\n\ntacks), 76 model.con file, 93 modules, 5 multiplexing, Raft, 158, 163–\n\n169\n\nMultiRaft, 143 MultiReader(), 47 mutex, wrapping log code in,\n\n45\n\nmutual authentication, 77,\n\n86\n\nN names\n\ncertificates, 80, 87 cryptography conven-\n\ntions, 76\n\nnodes, 118, 122 servers, 180 specifying package names\n\nin protobufs, 18 names field, certificate authori-\n\nty configuration, 80\n\nnearestMultiple, 41 networking, see gRPC new keyword, 20\n\nIndex • 234\n\nnext offsets, 37 Nginx chart, 202 nobody client, 92, 95 NodeName, 118 NodePort Service, 212 nodes, see also replication;\n\nservice discovery\n\ncluster resiliency, 118 dynamic configuration\n\nwith Viper, 199\n\nelections, 118 Kubernetes, 194–195 names, 118, 122 pointing to clusters, 118 tags, 118\n\nO O (organization), certificate\n\nauthority configuration, 80\n\nobservability, 99–109\n\ndefined, 99 importance of, 99 metrics, 99–101, 103–109 output, 108 probability samplers, 105 proglog project, 103–109 structured logs, 99, 101 testing, 106–109 traces, 99, 102–109\n\noffsets\n\nabout, 7 appending records in prototype, 7–11 base, 37, 39, 44–45 building index, 32–33 building log code, 44–45 building segments, 37–40 building store, 28 custom gPRC errors, 65–\n\n68, 72 next, 37 performance, 25 ranges, 46 reading and, 34, 45 relative, 39 replication with consen-\n\nsus, 143 storing, 34 testing errors, 49\n\nOpenCensus, 104–106 OpenTelemetry, 104 OpenTracing, 104 Operators, 223 organization (O), certificate\n\nauthority configuration, 80\n\norganization unit (OU), certifi- cate authority configura- tion, 80 originReader, 47 OU (organizational unit), cer- tificate authority configura- tion, 80\n\noutput, observability, 108\n\nP packages\n\ninternal, 60 specifying package names\n\nin protobufs, 18\n\nParquet, 102 PATH environment variable, 17 peerTLSConfig, 135, 157 performance\n\ncompilers, 19 consensus with Raft,\n\n148, 151, 163\n\ngRPC advantages, 58 multiplexing Raft, 163 protobufs, 13, 16, 19 reading logs, 25 test performance, 137 writing logs, 27 PersistentVolumeClaim, 206 persisting, snapshots, 153 pg_isready, 208 Pick(), 185 pickers\n\nclient-side load balancing on gPRC, 172, 183 client-side load balanc-\n\ning, custom, 175, 177, 183–189\n\nregistering, 185\n\nPike, Rob, 128 plugins, 21 pods\n\nabout, 194 forwarding, 216 listing, 216 service-per-pod load bal-\n\nancers, 222–227 Services and, 212 point-in-time recovery, 24 policy.csv, 93 ports\n\nassigning, 70, 136 consensus with Raft, 158 dynamic configuration\n\nwith Viper, 199\n\nexposing with Kubernetes NodePort Service, 212\n\nforwarding, 216 multiplexing Raft, 163–\n\n169\n\nservice discovery, 118,\n\n136\n\nPOST, JSON/HTTP servers,\n\n8\n\nPostgres, 208 probability samplers, 105 probes\n\nhealth checks with, 208–\n\n212\n\nliveness, 208 readiness, 208 startup, 208\n\nproducers, see also gRPC\n\nconfiguring for authoriza-\n\ntion, 96–98\n\nprototype for proglog\n\nproject, 7–11\n\nreplication, 126–134, 137 proglog project, see also con-\n\nfiguring; testing\n\nauthentication with TLS,\n\n76–88\n\nbuilding, 26–51 building index, 31–37 building segments, 37–42 building store, 26–31 client-side load balanc- ing, pickers, 183–189 client-side load balanc-\n\ning, resolvers, 173–183\n\nclosing logs, 45 coding log, 42–51 deploying with Kuber- netes, CLI agent for, 196–202\n\ndeploying with Kuber-\n\nnetes, Helm, 202–217\n\ndeploying with Kuber-\n\nnetes, setup, 194–195\n\ndeploying with Kuber- netes, to cloud, 219– 228\n\nerror handling with gR-\n\nPC, 63–68\n\nJSON/HTTP setup, 5–11 observability, 103–109 protobufs setup, 16–21 prototype for, 6–11 removing logs, 45 replication, 123–134 replication with consen- sus, API, 149–151\n\nIndex • 235\n\nreplication with consen- sus, implementing, 144–161\n\nreplication with consen- sus, testing, 161–163, 169\n\nresetting logs, 45, 50 running in Kubernetes\n\ncluster, 195\n\nservice discovery and replication, testing, 134–139\n\nservice discovery with\n\nSerf, replication, 123– 134\n\nservice discovery with\n\nSerf, setup, 116–123\n\nservice discovery with\n\nSerf, testing, 121–123, 134–139 setup code, 43\n\nPROJECT_ID environment vari-\n\nable, 221\n\nPrometheus, 103 protobufs\n\nadvantages, 5, 13–16 comments, 57 compiler, 16, 19, 58, 60 defining domain types,\n\n17–19\n\ndefining gPRC service,\n\n58–60\n\ndetailed error description,\n\n64\n\ndirectory structure, 16 performance, 13, 16, 19 proglog project setup, 16–\n\n21\n\nsyntax, 17 versioning with, 15, 18,\n\n57\n\nprotocol buffers, see proto-\n\nbufs\n\npull-based replication,\n\nsee replication\n\nR Raft\n\nabout, 24 adding servers as non-\n\nvoters, 159\n\nadvantages of, 142 configuring, 146–149,\n\n153, 175–177\n\nimplementing, 144–161 integrating with discov-\n\nery, 158–161\n\nleader election, 142\n\nlog API, 149–151 metadata, 147 multiplexing, 158, 163–\n\n169\n\nnode tags, 118 number of servers in a Raft cluster, 143\n\nperformance, 148, 151,\n\n163\n\nremoving servers from a\n\ncluster, 160\n\nrestoring with, 152, 154 separating Raft connec- tions from gRPC, 164, 166\n\nSerf setup and, 118–119 service discovery with\n\ncustom resolvers, 174– 175\n\nsetup, 144–149 snapshotting with, 146–\n\n149, 151–153\n\ntesting, 161–163, 169 understanding, 141–144\n\nrate metrics, 100 readiness probes, 208 reading\n\nabout, 25 building index, 34, 36 building log code, 45, 47–\n\n51\n\nbuilding segment, 39, 42 building store, 28 concatenating stores, 47 consensus and, 47 performance, 25 replication with consen-\n\nsus, 151, 153\n\ntesting log code, 49–51 testing store, 29–31 records, see also appending\n\nabout, 6 defined, 26 deleting with Raft, 156 recovery, point-in-time, 24 redo/undo, 24 Redux, 24 reflection, 19, 21 registries\n\nconfig registry system with Viper, 198–201\n\ndefined, 113 Google Container Reg-\n\nistry, 222\n\nrelative offsets, 39 releases, Helm, 202\n\nreleasing, snapshots, 153 reliability, health checks with\n\nprobes, 208–212\n\nreplication\n\nabout, 24 advantages of, 123, 143 closing, 133 with consensus, API,\n\n149–151\n\nwith consensus, imple- menting, 144–161\n\nwith consensus, integrat-\n\ning with discovery, 158–161\n\nwith consensus, testing,\n\n161–163, 169\n\nwith consensus, under- standing, 141–144\n\nerrors, 128, 151 lazy initialization, 127 number of replicas, 139,\n\n143\n\npull-based, 123–134 push-based, 123 with Serf, 123–134 with Serf, testing, 134–\n\n139\n\nrepositories, deploying with\n\nHelm, 202\n\nrequests, see also gRPC JSON/HTTP server, building, 7–11\n\nobservability, 100–109\n\nresetting, logs, 45, 50 resiliency, clusters, 118 ResolveNow(), 180 resolvers\n\nclient-side load balancing\n\non gPRC, 172\n\nclient-side load balanc- ing, custom, 173–183\n\nclosing, 180 default to DNS resolver,\n\n173\n\nregistering, 179\n\nResponse(), 151 restoring\n\nfinite-state machines,\n\n152, 154 logs, 47, 50 testing, 50\n\nRoot CA, 85 root client, 92, 94 round-robin load balancing,\n\n173, 185\n\nIndex • 236\n\nRPC (remote procedure call),\n\nsee gRPC\n\nruntime compiler, protobufs,\n\n19\n\nRWmutex, wrapping log code\n\nin, 45\n\nS S (state), certificate authority\n\nconfiguration, 80\n\nsaturation, metrics, 101 scaling\n\ndefined, 58 gRPC advantages, 58 load balancers and, 58,\n\n114\n\nmetrics for, 101 service discovery and,\n\n114\n\nStatefulSets and, 205 schema-violations, preventing\n\nwith protobufs, 13–15\n\nScheme(), 179 scratch image, 201 Secure Sockets Layer,\n\nsee SSL\n\nsecurity\n\nauthentication, 76–88,\n\n221\n\nauthorization, 76, 78, 88–\n\n98, 223\n\nencryption, 76–86 gRPC advantages, 57 importance of, 75 man-in-the-middle at-\n\ntacks, 76\n\nname conventions, 76\n\nSegment, 5 segments, see also indexes;\n\nstores\n\nactive, 25, 37 bootstrapping, 44 building, 37–42 building log code, 44–51 building store, 28 closing, 40, 46 configuring, 37 defined, 26 deleting, 25 directory for, 43 removing indexes and\n\nstores, 40\n\nsize, 38, 40, 42, 45 testing, 41 truncating, 46, 51 understanding, 25\n\nSerf\n\nadvantages of, 115 integrating with consen-\n\nsus, 158–161\n\nreplication with, 123–134 setup, 116–123 snapshotting, 118, 120 testing service discovery and replication, 134– 139\n\ntesting setup, 121–123\n\nserialization with protobufs,\n\n13\n\nserver proxies, 171 serverTLSConfig, 135, 157 ServerName field, 180 servers, see also consensus; replication; service discov- ery\n\nadding as non-voters,\n\n159\n\nauthentication with TLS,\n\n78–88\n\nbootstrapping, 149, 153,\n\n162, 205, 207\n\nbuilding JSON/HTTP, 7–\n\n11\n\ncreating gPRC, 68 handling in gPRC, 63–68 implementing gPRC, 60–\n\n63\n\nnames, 180 number of servers in a Raft cluster, 143 observability, 103–109 registering gPRC, 68 removing from Raft clus-\n\nter, 160\n\nrunning JSON/HTTP, 10 server options, gRPC, 86 status of in Serf, 121 terms in Raft, 142 testing gPRC, 68–73\n\nservice discovery\n\nadvantages of, 113 advantages of embedded,\n\n114–115\n\nclient-side load balancing and, 172, 174–183 clients and, 114, 171 defined, 113 errors, 120 integrating with consen-\n\nsus, 158–161\n\nvs. load balancers, 113 replication with, 123–134 with Serf, replication,\n\n123–134\n\nwith Serf, setup, 116–123 with Serf, testing, 134–\n\n139\n\nstand-alone, 114–115 startup probes, 208 task overview, 114 testing, 134–139 testing, custom client- side load balancing, 181–183, 186–189 testing, setup, 121–123\n\nservice keyword, 59 service-per-pod-label, 224 service-per-pod-ports, 224 Services (Kubernetes), 205,\n\n212 shutdown\n\nagents, 133, 136 CLI agent for deploying,\n\n201\n\nindexes and, 33 multiplexing Raft, 168 replication and, 133 test data, 136 ungraceful, 34\n\nsize\n\nfile, 27 index, 32, 34, 36, 40, 42 segments, 38, 40, 42, 45 store, 36, 40, 42 sleep, tests, 136–137 snapshot store, Raft setup,\n\n146–149, 153\n\nsnapshotting\n\ndeleting records, 156 finite-state machines, frequency of, 147 finite-state machines,\n\nimplementation, 151– 153\n\nfinite-state machines, setup, 146–149, 153 reading whole log and,\n\n47, 50\n\nwith Serf, 118, 120 testing, 50\n\nSSL, gRPC advantages, 57,\n\nsee also TLS\n\nstable stores, Raft setup,\n\n146–149\n\nStackdriver, 102 StartJoinAddrs, 118 startup probes, 208 Stat, 27 state, updating with service\n\nconfig, 180\n\nIndex • 237\n\nstate (S), certificate authority\n\nconfiguration, 80\n\nStatefulSets, 205–208, 223–\n\n227\n\nstatus codes, error handling,\n\n63–65\n\nstatus package, 63–65 storage, binding in Kuber-\n\nnetes, 206\n\nstores\n\nbuilding, 26–31 building segments, 37–40 closing, 29–30 concatenating, 47 configuring, 36 defined, 26 offsets, 34 Raft setup, 146–149, 153 re-creating from a file\n\nwith existing data, 27\n\nremoving, 40 size, 36, 40, 42 testing, 29–31 understanding, 25 streams, see also gRPC\n\nclient-side load balanc- ing, custom, 176, 187\n\nconsensus with Raft,\n\n147, 156–158 error messages, 66 replication, 126–134 structured logs, 99, 101, 103–\n\n109\n\nsubcharts, 204 superuser clients, 92 sync, 224\n\nT Tag, 118 tags, Serf nodes, 118 TCP, probes, 208 telemetry exporter, 108 templates, Helm charts, 204 terms, in consensus with\n\nRaft, 142\n\ntesting\n\nauthorization, 91–98 certificate authority con- figuration, 80, 84–86, 88\n\nclient-side load balanc-\n\ning, custom, 175, 181– 183, 186–189\n\ncommit log prototype, 11 consensus, 161–163, 169\n\ndefining test cases, 49 gRPC, 68–73 helpers, 30, 70, 122 index, 35 log code, 48–51 log interface, 67 multiplexing Raft, 169 observability, 106–109 performance of tests, 137 range errors, 49 restoring, 50 segments, 41 service discovery, 134–\n\n139\n\nservice discovery with\n\nSerf, setup, 121–123\n\nservice discovery with\n\nload balancing, 175, 188\n\nsetup, 70, 188 shutting down and delet-\n\ning test data, 136 sleep and, 136–137 store, 29–31\n\ntimeouts, consensus with Raft, 148, 160, 162, 189\n\nTLS\n\nauthentication with, 76–\n\n88\n\ncertificates for replication\n\ntesting, 135\n\nconsensus with Raft,\n\n157, 164–169\n\ngRPC advantages, 57 handshake, 77\n\nmultiplexing Raft, 164–\n\n169\n\nmutual authentication,\n\n77, 86\n\nstandards, 164 version, 77 traces, 102–109 traffic, metrics, 101 transport\n\nRaft implementation,\n\n156–158\n\nRaft setup, 146–149\n\nTransport Layer Security,\n\nsee TLS truncating\n\nindexes, 33–34, 36 segments, 46, 51 trust boundaries, 114, 172 two-way authentication, 77,\n\n86\n\ntype checking\n\ngRPC advantages, 57 with protobufs, 13, 15\n\nU uint32, 34 undo/redo, 24 UpdateState, 180 updating, StatefulSets and,\n\n205\n\nus.gcr.io registry, 222\n\nIndex • 238\n\nV values.yaml file, 204, 215 versioning\n\nabout, 56 bumping major versions,\n\n18\n\nfield versioning, 18, 57 gRPC advantages, 57 with protobufs, 15, 18,\n\n57 versions\n\nGo, xii Go API, 19 TLS, 77\n\nViper, 196, 198–201\n\nW -w flag, 228 webhooks, 224 *Width constants, 32 wildcard, * for, 93 WithInsecure() dial option, 86 write-ahead logs, see commit\n\nlogs writing\n\nbuilding index, 35 building segment, 39 building store, 27 performance, 27 writer buffer, 27–28\n\nZ Zap, 104–106 ZooKeeper, 174\n\nSAVE 30%!Use coupon codeBUYANOTHER2021\n\nThank you!\n\nHow did you enjoy this book? Please let us know. Take a moment and email us at support@pragprog.com with your feedback. Tell us your story and you could win free ebooks. Please use the subject line “Book Feedback.”\n\nReady for your next great Pragmatic Bookshelf book? Come on over to https://pragprog.com and use the coupon code BUYANOTHER2021 to save 30% on your next ebook.\n\nVoid where prohibited, restricted, or otherwise unwelcome. Do not use ebooks near water. If rash persists, see a doctor. Doesn’t apply to The Pragmatic Programmer ebook because it’s older than the Pragmatic Bookshelf itself. Side effects may include increased knowledge and skill, increased marketability, and deep satisfaction. Increase dosage regularly.\n\nAnd thank you for your continued support,\n\nAndy Hunt, Publisher\n\nExplore Software Defined Radio\n\nDo you want to be able to receive satellite images using nothing but your computer, an old TV antenna, and a $20 USB stick? Now you can. At last, the technology exists to turn your computer into a super radio receiv- er, capable of tuning in to FM, shortwave, amateur “ham,” and even satellite frequencies, around the world and above it. Listen to police, fire, and aircraft signals, both in the clear and encoded. And with the book’s advanced antenna design, there’s no limit to the signals you can receive.\n\nWolfram Donat (78 pages) ISBN: 9781680507591. $19.95 https://pragprog.com/book/wdradio\n\nGenetic Algorithms in Elixir\n\nFrom finance to artificial intelligence, genetic algo- rithms are a powerful tool with a wide array of applica- tions. But you don’t need an exotic new language or framework to get started; you can learn about genetic algorithms in a language you’re already familiar with. Join us for an in-depth look at the algorithms, tech- niques, and methods that go into writing a genetic al- gorithm. From introductory problems to real-world applications, you’ll learn the underlying principles of problem solving using genetic algorithms.\n\nSean Moriarity (242 pages) ISBN: 9781680507942. $39.95 https://pragprog.com/book/smgaelixir\n\nDesign and Build Great Web APIs\n\nAPIs are transforming the business world at an increas- ing pace. Gain the essential skills needed to quickly design, build, and deploy quality web APIs that are robust, reliable, and resilient. Go from initial design through prototyping and implementation to deployment of mission-critical APIs for your organization. Test, secure, and deploy your API with confidence and avoid the “release into production” panic. Tackle just about any API challenge with more than a dozen open-source utilities and common programming patterns you can apply right away.\n\nMike Amundsen (330 pages) ISBN: 9781680506808. $45.95 https://pragprog.com/book/maapis\n\nQuantum Computing\n\nYou’ve heard that quantum computing is going to change the world. Now you can check it out for your- self. Learn how quantum computing works, and write programs that run on the IBM Q quantum computer, one of the world’s first functioning quantum computers. Develop your intuition to apply quantum concepts for challenging computational tasks. Write programs to trigger quantum effects and speed up finding the right solution for your problem. Get your hands on the fu- ture of computing today.\n\nNihal Mehta, Ph.D. (580 pages) ISBN: 9781680507201. $45.95 https://pragprog.com/book/nmquantum\n\nA Common-Sense Guide to Data Structures and Algorithms, Second Edition\n\nIf you thought that data structures and algorithms were all just theory, you’re missing out on what they can do for your code. Learn to use Big O Notation to make your code run faster by orders of magnitude. Choose from data structures such as hash tables, trees, and graphs to increase your code’s efficiency exponentially. With simple language and clear dia- grams, this book makes this complex topic accessible, no matter your background. This new edition features practice exercises in every chapter, and new chapters on topics such as dynamic programming and heaps and tries. Get the hands-on info you need to master data structures and algorithms for your day-to-day work.\n\nJay Wengrow (506 pages) ISBN: 9781680507225. $45.95 https://pragprog.com/book/jwdsal2\n\nBuild Location-Based Projects for iOS\n\nCoding is awesome. So is being outside. With location- based iOS apps, you can combine the two for an en- hanced outdoor experience. Use Swift to create your own apps that use GPS data, read sensor data from your iPhone, draw on maps, automate with geofences, and store augmented reality world maps. You’ll have a great time without even noticing that you’re learning. And even better, each of the projects is designed to be extended and eventually submitted to the App Store. Explore, share, and have fun.\n\nDominik Hauser (154 pages) ISBN: 9781680507812. $26.95 https://pragprog.com/book/dhios\n\niOS Unit Testing by Example\n\nFearlessly change the design of your iOS code with solid unit tests. Use Xcode’s built-in test framework XCTest and Swift to get rapid feedback on all your code — including legacy code. Learn the tricks and tech- niques of testing all iOS code, especially view con- trollers (UIViewControllers), which are critical to iOS apps. Learn to isolate and replace dependencies in legacy code written without tests. Practice safe refac- toring that makes these tests possible, and watch all your changes get verified quickly and automatically. Make even the boldest code changes with complete confidence.\n\nJon Reid (300 pages) ISBN: 9781680506815. $47.95 https://pragprog.com/book/jrlegios\n\nBecome an Effective Software Engineering Manager\n\nSoftware startups make global headlines every day. As technology companies succeed and grow, so do their engineering departments. In your career, you’ll may suddenly get the opportunity to lead teams: to become a manager. But this is often uncharted territory. How do you decide whether this career move is right for you? And if you do, what do you need to learn to suc- ceed? Where do you start? How do you know that you’re doing it right? What does “it” even mean? And isn’t management a dirty word? This book will share the secrets you need to know to manage engineers successfully.\n\nJames Stanier (396 pages) ISBN: 9781680507249. $45.95 https://pragprog.com/book/jsengman\n\nThe Pragmatic Bookshelf\n\nThe Pragmatic Bookshelf features books written by professional developers for professional developers. The titles continue the well-known Pragmatic Programmer style and continue to garner awards and rave reviews. As development gets more and more difficult, the Prag- matic Programmers will be there with more titles and products to help you stay on top of your game.\n\nVisit Us Online\n\nThis Book’s Home Page https://pragprog.com/book/tjgo Source code from this book, errata, and other resources. Come give us feedback, too!\n\nKeep Up to Date https://pragprog.com Join our announcement mailing list (low volume) or follow us on twitter @pragprog for new titles, sales, coupons, hot tips, and more.\n\nNew and Noteworthy https://pragprog.com/news Check out the latest pragmatic developments, new titles and other offerings.\n\nBuy the Book\n\nIf you liked this ebook, perhaps you’d like to have a paper copy of the book. Paperbacks are available from your local independent bookstore and wherever fine books are sold.\n\nContact Us\n\nOnline Orders:\n\nhttps://pragprog.com/catalog\n\nCustomer Service:\n\nsupport@pragprog.com\n\nInternational Rights:\n\ntranslations@pragprog.com\n\nAcademic Use:\n\nacademic@pragprog.com\n\nWrite for Us:\n\nhttp://write-for-us.pragprog.com\n\nOr Call:\n\n+1 800-699-7764",
      "page_number": 221
    }
  ],
  "pages": [
    {
      "page_number": 3,
      "content": "Early Praise for Distributed Services with Go\n\nHaving built most of the technologies in this book without the benefit of this book, I can wholeheartedly recommend Distributed Services with Go. Travis delivers years of practical experience distilled into a clear and concise guide that takes the reader step by step from foundational knowledge to production deployment. This book earns my most hearty endorsement.\n\n➤ Brian Ketelsen\n\nPrincipal Developer Advocate, Microsoft; and Organizer, GopherCon\n\nIn this practical, engaging book, Travis Jeffery shines a light on the path to building distributed systems. Read it, learn from it, and get coding!\n\n➤ Jay Kreps\n\nCEO, Confluent, Inc., and Co-Creator of Apache Kafka\n\nTravis Jeffery distills the traditionally academic topic of distributed systems down to a series of practical steps to get you up and running. The book focuses on the real-world concepts used every day by practicing software engineers. It’s a great read for intermediate developers getting into distributed systems or for senior engineers looking to expand their understanding.\n\n➤ Ben Johnson\n\nAuthor of BoltDB\n\nFor any aspiring Gopher, Travis provides a gentle introduction to complex topics in distributed systems and provides a hands-on approach to applying the concepts.\n\n➤ Armon Dadgar\n\nHashiCorp Co-Founder",
      "content_length": 1328,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 4,
      "content": "A must-have for Gophers building systems at scale.\n\n➤ William Rudenmalm\n\nLead Developer, CREANDUM\n\nThis book is a great resource for Go developers looking to build and maintain distributed systems. It pairs an incremental development process with extensive code examples to teach you how to write your own distributed service, understand how it works under the hood, and how to deploy your service so others may start using it.\n\n➤ Nishant Roy Tech Lead",
      "content_length": 452,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 5,
      "content": "Distributed Services with Go Your Guide to Reliable, Scalable, and Maintainable Systems\n\nTravis Jeffery\n\nThe Pragmatic Bookshelf Raleigh, North Carolina",
      "content_length": 152,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 6,
      "content": "Many of the designations used by manufacturers and sellers to distinguish their products are claimed as trademarks. Where those designations appear in this book, and The Pragmatic Programmers, LLC was aware of a trademark claim, the designations have been printed in initial capital letters or in all capitals. The Pragmatic Starter Kit, The Pragmatic Programmer, Pragmatic Programming, Pragmatic Bookshelf, PragProg and the linking g device are trade- marks of The Pragmatic Programmers, LLC.\n\nEvery precaution was taken in the preparation of this book. However, the publisher assumes no responsibility for errors or omissions, or for damages that may result from the use of information (including program listings) contained herein.\n\nFor our complete catalog of hands-on, practical, and Pragmatic content for software devel- opers, please visit https://pragprog.com.\n\nThe team that produced this book includes:\n\nCEO: Dave Rankin COO: Janet Furlow Managing Editor: Tammy Coron Development Editor: Dawn Schanafelt and Katharine Dvorak Copy Editor: L. Sakhi MacMillan Indexing: Potomac Indexing, LLC Layout: Gilson Graphics Founders: Andy Hunt and Dave Thomas\n\nFor sales, volume licensing, and support, please contact support@pragprog.com.\n\nFor international rights, please contact rights@pragprog.com.\n\nCopyright © 2021 The Pragmatic Programmers, LLC.\n\nAll rights reserved. No part of this publication may be reproduced, stored in a retrieval system, or transmitted, in any form, or by any means, electronic, mechanical, photocopying, recording, or otherwise, without the prior consent of the publisher.\n\nISBN-13: 978-1-68050-760-7 Encoded using the finest acid-free high-entropy binary digits. Book version: P1.0—March 2021",
      "content_length": 1724,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 7,
      "content": "Contents\n\nAcknowledgments . Introduction .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\nix xi\n\nPart I — Get Started\n\n1.\n\nLet’s Go . . How JSON over HTTP Services Fits into Distributed Systems Set Up the Project Build a Commit Log Prototype Build a JSON over HTTP Server Run Your Server Test Your API What You Learned\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n3 4 5 6 7 10 11 11\n\n2.\n\nStructure Data with Protocol Buffers . Why Use Protocol Buffers? Install the Protocol Buffer Compiler Define Your Domain Types as Protocol Buffers Compile Protocol Buffers Work with the Generated Code What You Learned\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n13 14 16 17 19 20 21\n\n3. Write a Log Package . . The Log Is a Powerful Tool How Logs Work Build a Log What You Learned\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n23 23 25 26 51",
      "content_length": 787,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 8,
      "content": "4.\n\n5.\n\n6.\n\n7.\n\n8.\n\nPart II — Network\n\nServe Requests with gRPC What Is gRPC? Goals When Building a Service Define a gRPC Service Compile with the gRPC Plugin Implement a gRPC Server Register Your Server Test a gRPC Server and Client What You Learned\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\nSecure Your Services . Secure Services in Three Steps Authenticate the Server with TLS Authenticate the Client with Mutual TLS Authentication Authorize with Access Control Lists What You Learned\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\nObserve Your Systems . Three Types of Telemetry Data Make Your Service Observable What You Learned\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\nPart III — Distribute\n\nServer-to-Server Service Discovery . Why Use Service Discovery? Embed Service Discovery Discover Services with Serf Request Discovered Services and Replicate Logs Test Discovery and the Service End-to-End What You Learned\n\n.\n\n.\n\n.\n\nCoordinate Your Services with Consensus . What Is Raft and How Does It Work? Implement Raft in Our Service Multiplex to Run Multiple Services on One Port What You Learned\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\nContents • vi\n\n.\n\n55 55 56 58 60 60 68 68 73\n\n.\n\n75 76 78 86 88 98\n\n.\n\n99 99 103 109\n\n.\n\n113 113 114 116 123 134 138\n\n.\n\n141 141 144 163 169",
      "content_length": 1202,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 9,
      "content": "9.\n\nDiscover Servers and Load Balance from the Client . Three Load-Balancing Strategies Load Balance on the Client in gRPC Make Servers Discoverable Resolve the Servers Route and Balance Requests with Pickers Test Discovery and Balancing End-to-End What You Learned\n\nPart IV — Deploy\n\n10. Deploy Applications with Kubernetes Locally .\n\nWhat Is Kubernetes? Install kubectl Use Kind for Local Development and Continuous Integration Write an Agent Command-Line Interface Build Your Docker Image Configure and Deploy Your Service with Helm Advertise Raft on the Fully Qualified Domain Name What You Learned\n\n11. Deploy Applications with Kubernetes to the Cloud . .\n\nCreate a Google Kubernetes Engine Cluster Create Custom Controllers with Metacontroller Deploy to the Internet What You Learned\n\nIndex\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\nContents • vii\n\n.\n\n171 171 172 174 177 183 188 189\n\n.\n\n193 193 194 195 196 201 202 213 217\n\n219 220 222 228 228\n\n.\n\n229",
      "content_length": 962,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 10,
      "content": "Acknowledgments\n\nI write this, having finished the book, two and a half years after I began. Writing this book was the hardest thing I’ve done. I’ve built a few startups and several open source projects—this was much harder. I set out to write a good book people would enjoy and find useful. I’m critical of myself and my work and wouldn’t put out anything I didn’t deem worthy. It took me a long time to write because I didn’t want to compromise. I’m happy with this book and proud of myself.\n\nI thank my editors, Dawn Schanafelt and Katharine Dvorak, for their patience and for helping me to improve my writing and motivating me in hard times.\n\nThank you to my publisher, The Pragmatic Bookshelf, for the guidance I received in writing my first book and for all of the work out of view.\n\nI thank my book’s reviewers and beta readers for giving me their impressions of the book and contributing suggestions and errata to help me improve the book. Thank you to Clinton Begin, Armon Dadgar, Ben Johnson, Brian Ketelsen, Jay Kreps, Nishant Roy, William Rudenmalm, and Tyler Treat.\n\nThank you to the free and open source software communities for putting out code to study, change, and run. Special thanks to the people at Hashicorp for open-sourcing their Raft and Serf packages I use in this book and their services like Consul, whose source I studied and learned from a lot. Thank you to the Emacs and Linux contributors—the text editor and operating system I wrote this book with. Thank you to the Go team for creating a simple, stable, useful language.\n\nThank you to my parents, Dave and Tricia Jeffery, for buying my first com- puter and programming books and encouraging me with a strong work ethic.\n\nThank you to my high school English teacher, Graziano Galati, for giving me the right reading at the right time in my life.\n\nreport erratum • discuss",
      "content_length": 1853,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 11,
      "content": "Acknowledgments • x\n\nThank you to J. R. R. Tolkien for authoring The Lord of the Rings. I read it while writing this book, and the rapport with Frodo and Samwise aided me on the journey.\n\nI thank my cat, Callie Jeffery. I adopted her a quarter of the way through writing the book, and her useful contributions to the discussion helped speed up my writing pace.\n\nThank you to Emily Davidson for her love and support and for fueling me with broccoli soup, ginger kombucha, and matcha tea.\n\nThank you, dear reader, for independently furthering your skills and knowl- edge and having the ambition to put your dent in the universe.\n\n—Travis Jeffery\n\nreport erratum • discuss",
      "content_length": 669,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 12,
      "content": "Introduction\n\nGo has become the most popular language for building distributed services, as shown by projects like Docker, Etcd, Vault, CockroachDB, Prometheus, and Kubernetes. Despite the number of prominent projects such as these, however, there’s no resource that teaches you why or how you can extend these projects or build your own.\n\nWhere do you begin if you want to build a distributed service?\n\nWhen I began learning how to build distributed services, I found the existing resources to be of two extremes:\n\nConcrete code—distributed services are large, complex projects, and the prominent ones have had teams working on them for years. The layout of these projects, their technical debt, and their spaghetti code bury the ideas you’re interested in, which means you have to dig them out. At best, learning from code is inefficient. Plus there’s the risk that you may uncover outdated and irrelevant techniques that you’re better off avoiding in your own projects.\n\nAbstract papers and books—papers and books like Designing Data- Intensive Applications by Martin Kleppmann1 describe how the data structures and algorithms behind distributed services work but cover them as discrete ideas, which means you’re left on your own to connect them before you can apply them in a project.\n\nThese two extremes leave a chasm for you to cross. I wanted a resource that held my hand and taught me how to build a distributed service—a resource that explained the big ideas behind distributed services and then showed me how to make something of them.\n\nI wrote this book to be that resource. Read this book, and you’ll be able to build your own distributed services and contribute to existing ones.\n\n1.\n\nhttps://www.oreilly.com/library/view/designing-data-intensive-applications/9781491903063\n\nreport erratum • discuss",
      "content_length": 1812,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 13,
      "content": "Introduction • xii\n\nWho This Book Is For\n\nThis book is for intermediate to advanced developers who want to learn how to build distributed services. I’ve geared the book toward Go programmers, and prior Go experience will help, but you don’t have to be an expert. This book shows you how to build distributed services, and the concepts are the same regardless of what language you use. So if you’re writing distributed services in Go, you can take full advantage of this book; if not, you can apply the ideas I present here in any language.\n\nThis book’s code is compatible with Go 1.13+.\n\nWhat’s in This Book\n\nWe will design, develop, and deploy a distributed service to explore what Go can do. We’ll develop and deploy the service in layers: from the bare essentials of storage handling, to the networking of a client and server, to distributing server instances, deployment, and testing. I divided this book into four parts that parallel those layers. (Don’t worry if you aren’t familiar with the technolo- gies I mention next—I explain them in the relevant chapters.)\n\nPart I — Get Started\n\nWe’ll begin with the basic elements: building our project’s storage layer and defining its data structures.\n\nIn Chapter 1, Let’s Go, on page 3, we’ll kick off our project by building a simple JSON over HTTP commit log service.\n\nIn Chapter 2, Structure Data with Protocol Buffers, on page 13, we’ll set up our protobufs, generate our data structures, and set up automation to quickly generate our code as we make changes.\n\nIn Chapter 3, Write a Log Package, on page 23, we’ll build a commit log library that’ll serve as the heart of our service, storing and looking up data.\n\nPart II — Network\n\nThis part is where we’ll make our service work over a network.\n\nIn Chapter 4, Serve Requests with gRPC, on page 55, we’ll set up gRPC, define our server and client APIs in protobuf, and build our client and server.\n\nreport erratum • discuss",
      "content_length": 1927,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 14,
      "content": "Online Resources • xiii\n\nIn Chapter 5, Secure Your Services, on page 75, we’ll make our connections secure by authenticating our server with SSL/TLS to encrypt data exchanged between client and server and by authenticating requests with access tokens.\n\nIn Chapter 6, Observe Your Systems, on page 99, we’ll make our service observable by adding logs, metrics, and tracing.\n\nPart III — Distribute\n\nIn this part we’ll make our service distributed—highly available, resilient, and scalable.\n\nIn Chapter 7, Server-to-Server Service Discovery, on page 113, we’ll build dis- covery into our service to make server instances aware of each other.\n\nIn Chapter 8, Coordinate Your Services with Consensus, on page 141, we’ll add consensus to coordinate the efforts of our servers and turn them into a cluster.\n\nIn Chapter 9, Discover Servers and Load Balance from the Client, on page 171, we’ll code discovery in our gRPC clients so they discover and connect to the servers with client-side load balancing.\n\nPart IV — Deploy\n\nHere’s where we’ll deploy our service and make it live.\n\nIn Chapter 10, Deploy Applications with Kubernetes Locally, on page 193, we’ll set up Kubernetes locally and run a cluster on your local machine. And we’ll prepare to deploy to the cloud.\n\nIn Chapter 11, Deploy Applications with Kubernetes to the Cloud, on page 219, we’ll create a Kubernetes cluster on Google Cloud’s Kubernetes Engine and deploy our service to the cloud so that people on the Internet can use it.\n\nIf you plan on building the project as you read (which is a great idea), read the parts in order so that your code works. It’s also fine to skip around in the book as well; the ideas we’ll explore in each chapter have value on their own.\n\nOnline Resources\n\nThe code we’ll develop is available on the Pragmatic Bookshelf website: https://pragprog.com/book/tjgo. You’ll also find an errata-submission form there for you to ask questions, report any problems with the text, or make suggestions for future versions of this book.\n\nLet’s get Going!\n\nreport erratum • discuss",
      "content_length": 2057,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 15,
      "content": "Part I\n\nGet Started",
      "content_length": 19,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 16,
      "content": "CHAPTER 1\n\nLet’s Go\n\nThroughout my career I’ve written programs in C, Ruby, Python, JavaScript, Java, Elixir, Erlang, Bash, and more. Each of these languages had a lot of great things going for it but always at least a few things that bugged me a lot. C didn’t have modules, Ruby wasn’t fast enough, JavaScript and its type system made you question your sanity, and so on. This meant that each language had a specific use case, like all the different knives a chef uses. For example, a chef uses a cleaver to cut through big bones. Similarly, I’d use Java when writing big, objective-oriented programs and wanted to make a cup of tea between the time I started the program and it was ready to run. A chef uses a paring knife when making small, delicate cuts, and I’d use Bash when writing small, portable scripts. But I always wished I could find a lan- guage that was useful in almost all situations and didn’t irritate me.\n\nFinally, I came upon Go, a language that can:\n\nCompile and run your programs faster than an interpreted language like Ruby; • Write highly concurrent programs; • Run directly on the underlying hardware; and • Use modern features like packages (while excluding a lot of features I didn’t need, like classes).\n\nGo had more things going for it. So there had to be something that bugged me, right? But no, it was as if the designers of Go had taken all the stuff that bothered me about other languages and stripped them out, leaving the lean, mean programming language that is Go. Go gave me the same feeling that made me first fall in love with programming: that if something was wrong it was my fault, me getting in my way instead of the language burying me under the weight of all its features. If Java is the cleaver and Bash the paring knife, then Go is the katana. Samurai felt that katanas were extensions of\n\nreport erratum • discuss",
      "content_length": 1863,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 17,
      "content": "Chapter 1. Let’s Go • 4\n\nthemselves, things they could spend a lifetime with while pursuing mastery of their craft. That’s the way I feel about Go.\n\nIf you were to pick the software field where Go has had the biggest impact, it would have to be distributed systems. The developers of projects like Docker, Kubernetes, Etcd, and Prometheus all decided to use Go for good reason. Google developed Go and its standard library as an answer to software prob- lems at Google: multicore processors, networked systems, massive computa- tion clusters—in other words, distributed systems, and at large scale in terms of lines of code, programmers, and machines. Because you’re a Go program- mer, you likely use systems like these and want to know how they work, how to debug them, and how to contribute to them, or you want to build similar projects of your own. That’s the case for me: the companies I’ve worked for used Docker and Kubernetes, and I’ve built my own projects like Jocko, an implementation of Kafka (the distributed commit log) in Go.\n\nSo how do you start down the path of knowing how to do all that in Go? Building a distributed service isn’t the easiest or smallest project in the world. If you try to build all the pieces at once, all you’ll end up with is a big, stinking mess of a code base and a fried brain. You build the project piece by piece. A good place to start is a commit log JSON over HTTP service. Even if you’ve never written an HTTP server in Go before, I’ll teach you how to make an accessible application programming interface (API) that clients can call over the network. You’ll learn about commit log APIs and, because we’re working on one project throughout this book, you’ll be set up to write the code we’ll work on in the following chapters.\n\nHow JSON over HTTP Services Fits into Distributed Systems\n\nJSON over HTTP APIs are the most common APIs on the web, and for good reason. They’re simple to build since most languages have JSON support built in. And they’re simple and accessible to use since JSON is human readable and you can call HTTP APIs via the terminal with curl, by visiting the site with your browser, or using any of the plethora of good HTTP clients. If you have an idea for a web service that you want to hack up and have people try as soon as pos- sible, then implementing it with JSON/HTTP is the way to go.\n\nJSON/HTTP isn’t limited to small web services. Most tech companies that provide a web service have at least one JSON/HTTP API acting as the public API of their service either for front-end engineers at their company to use or for engineers outside the company to build their own third-party applications\n\nreport erratum • discuss",
      "content_length": 2692,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 18,
      "content": "Set Up the Project • 5\n\non. For their internal web APIs, the company may take advantage of technolo- gies like protobuf for features that JSON/HTTP doesn’t provide—like type checking and versioning—but their public one will still be JSON/HTTP for accessibility. This is the same architecture I’ve used at my current and previ- ous companies. At Segment we had a JSON/HTTP-based architecture that for years handled billions of API calls a month before we changed our internal services to use protobuf/gRPC to improve efficiency. At Basecamp, all services were JSON/HTTP-based and (as far as I know) still are to this day.\n\nJSON/HTTP is a great choice for the APIs of infrastructure projects. Projects like Elasticsearch (a popular open source, distributed search engine) and Etcd (a popular distributed key-value store used by many projects, including Kubernetes) also use JSON/HTTP for their client-facing APIs, while employing their own binary protocols for communication between nodes to improve perfor- mance. JSON/HTTP is no toy—you can build all kinds of services with it.\n\nGo has great APIs in its standard library for building HTTP servers and working with JSON, making it perfect for building JSON/HTTP web services. I’ve worked on JSON/HTTP services written in Ruby, Node.js, Java, Python, and I’ve found Go to be the most pleasant by far. This is because of the interaction between Go’s declarative tags and the great APIs in the JSON encoding package (encoding/json) in the standard library that save you from the fiddling marshaling code you have to write in other languages. So let’s dive right in.\n\nSet Up the Project\n\nThe first thing we need to do is create a directory for our project’s code. Since we’re using Go 1.13+, we’ll take advantage of modules1 so you don’t have to put your code under your GOPATH. We’ll call our project proglog, so open your terminal to wherever you like to put your code and run the following commands to set up your module:\n\n$ mkdir proglog $ cd proglog $ go mod init github.com/travisjeffery/proglog\n\nReplace travisjeffery with your own GitHub username or with github.com if you use something like Bitbucket, but keep in mind as you’re working through this book that the code examples all have github.com/travisjeffery/proglog as the import path, so if you’re using your own import path, you must change the code examples to use that import path.\n\n1.\n\nhttps://github.com/golang/go/wiki/Modules\n\nreport erratum • discuss",
      "content_length": 2466,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 19,
      "content": "Chapter 1. Let’s Go • 6\n\nBuild a Commit Log Prototype\n\nWe’ll explore commit logs in depth in Chapter 3, Write a Log Package, on page 23, when we build a persisted commit log library. For now, all you need to know about commit logs is that they’re a data structure for an append-only sequence of records, ordered by time, and you can build a simple commit log with a slice.\n\nCreate an internal/server directory tree in the root of your project and put the following code under the server directory in a file called log.go:\n\nLetsGo/internal/server/log.go package server\n\nimport (\n\n\"fmt\" \"sync\"\n\n)\n\ntype Log struct { mu records []Record\n\nsync.Mutex\n\n}\n\nfunc NewLog() *Log {\n\nreturn &Log{}\n\n}\n\nfunc (c *Log) Append(record Record) (uint64, error) {\n\nc.mu.Lock() defer c.mu.Unlock() record.Offset = uint64(len(c.records)) c.records = append(c.records, record) return record.Offset, nil\n\n}\n\nfunc (c *Log) Read(offset uint64) (Record, error) {\n\nc.mu.Lock() defer c.mu.Unlock() if offset >= uint64(len(c.records)) {\n\nreturn Record{}, ErrOffsetNotFound\n\n} return c.records[offset], nil\n\n}\n\ntype Record struct {\n\nValue []byte `json:\"value\"` Offset uint64 `json:\"offset\"`\n\n}\n\nvar ErrOffsetNotFound = fmt.Errorf(\"offset not found\")\n\nreport erratum • discuss",
      "content_length": 1244,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 20,
      "content": "Build a JSON over HTTP Server • 7\n\nTo append a record to the log, you just append to the slice. Each time we read a record given an index, we use that index to look up the record in the slice. If the offset given by the client doesn’t exist, we return an error saying that the offset doesn’t exist. All really simple stuff, as it should be since we’re using this log as a prototype and want to keep moving.\n\nIgnore Chapter Namespaces in the File Paths\n\nYou may have noticed that code snippet’s file path said LetsGo/inter- nal/server/log.go instead of internal/server/log.go and that subsequent code snippets have similar per-chapter directory namespaces. These namespaces were needed to structure the code for the book build. When writing your code, pretend that these namespaces don’t exist. So for the previous example, the internal directory would go at the root of your project.\n\nBuild a JSON over HTTP Server\n\nNow we’ll write our JSON/HTTP web server. A Go web server comprises one function—a net/http HandlerFunc(ResponseWriter, *Request)—for each of your API’s endpoints. Our API has two endpoints: Produce for writing to the log and Consume for reading from the log. When building a JSON/HTTP Go server, each handler consists of three steps:\n\n1. Unmarshal the request’s JSON body into a struct. 2. Run that endpoint’s logic with the request to obtain a result. 3. Marshal and write that result to the response. If your handlers become much more complicated than this, then you should move the code out, move request and response handling into HTTP middle- ware, and move business logic further down the stack.\n\nLet’s start by adding a function for users to create our HTTP server. Inside your server directory, create a file called http.go that contains the following code:\n\nLetsGo/internal/server/http.go package server\n\nimport (\n\n\"encoding/json\" \"net/http\"\n\n\"github.com/gorilla/mux\"\n\n)\n\nfunc NewHTTPServer(addr string) *http.Server {\n\nhttpsrv := newHTTPServer() r := mux.NewRouter()\n\nreport erratum • discuss",
      "content_length": 2019,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 21,
      "content": "Chapter 1. Let’s Go • 8\n\nr.HandleFunc(\"/\", httpsrv.handleProduce).Methods(\"POST\") r.HandleFunc(\"/\", httpsrv.handleConsume).Methods(\"GET\") return &http.Server{ Addr: Handler: r,\n\naddr,\n\n}\n\n}\n\nNewHTTPServer(addrstring) takes in an address for the server to run on and returns an *http.Server. We create our server and use the popular gorilla/mux library to write nice, RESTful routes that match incoming requests to their respective handlers. An HTTP POST request to / matches the produce handler and appends the record to the log, and an HTTP GET request to / matches the consume handler and reads the record from the log. We wrap our server with a *net/http.Server so the user just needs to call ListenAndServe() to listen for and handle incoming requests.\n\nNext, we’ll define our server and the request and response structs by adding this snippet below NewHTTPServer():\n\nLetsGo/internal/server/http.go type httpServer struct { Log *Log\n\n}\n\nfunc newHTTPServer() *httpServer { return &httpServer{\n\nLog: NewLog(),\n\n}\n\n}\n\ntype ProduceRequest struct {\n\nRecord Record `json:\"record\"`\n\n}\n\ntype ProduceResponse struct {\n\nOffset uint64 `json:\"offset\"`\n\n}\n\ntype ConsumeRequest struct {\n\nOffset uint64 `json:\"offset\"`\n\n}\n\ntype ConsumeResponse struct {\n\nRecord Record `json:\"record\"`\n\n}\n\nWe now have a server referencing a log for the server to defer to in its handlers. A produce request contains the record that the caller of our API wants appended to the log, and a produce response tells the caller what offset the log stored the records under. A consume request specifies which records the\n\nreport erratum • discuss",
      "content_length": 1609,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 22,
      "content": "Build a JSON over HTTP Server • 9\n\ncaller of our API wants to read and the consume response to send back those records to the caller. Not bad for just 28 lines of code, huh?\n\nNext, we need to implement the server’s handlers. Add the following code below your types from the previous code snippet:\n\nLetsGo/internal/server/http.go func (s *httpServer) handleProduce(w http.ResponseWriter, r *http.Request) {\n\nvar req ProduceRequest err := json.NewDecoder(r.Body).Decode(&req) if err != nil {\n\nhttp.Error(w, err.Error(), http.StatusBadRequest) return\n\n} off, err := s.Log.Append(req.Record) if err != nil {\n\nhttp.Error(w, err.Error(), http.StatusInternalServerError) return\n\n} res := ProduceResponse{Offset: off} err = json.NewEncoder(w).Encode(res) if err != nil {\n\nhttp.Error(w, err.Error(), http.StatusInternalServerError) return\n\n}\n\n}\n\nThe produce handler implements the three steps we discussed before: unmarshaling the request into a struct, using that struct to produce to the log and getting the offset that the log stored the record under, and marshaling and writing the result to the response. Our consume handler looks almost identical. Add the following snippet below your produce handler:\n\nLetsGo/internal/server/http.go func (s *httpServer) handleConsume(w http.ResponseWriter, r *http.Request) {\n\nvar req ConsumeRequest err := json.NewDecoder(r.Body).Decode(&req) if err != nil {\n\nhttp.Error(w, err.Error(), http.StatusBadRequest) return\n\n} record, err := s.Log.Read(req.Offset) if err == ErrOffsetNotFound {\n\nhttp.Error(w, err.Error(), http.StatusNotFound) return\n\n} if err != nil {\n\nhttp.Error(w, err.Error(), http.StatusInternalServerError) return\n\n}\n\nreport erratum • discuss",
      "content_length": 1691,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 23,
      "content": "Chapter 1. Let’s Go • 10\n\nres := ConsumeResponse{Record: record} err = json.NewEncoder(w).Encode(res) if err != nil {\n\nhttp.Error(w, err.Error(), http.StatusInternalServerError) return\n\n}\n\n}\n\nThe consume handler is like the produce handler but calls Read(offset uint64) to get the record stored in the log. This handler contains more error checking so we can provide an accurate status code to the client if the server can’t handle the request, like if the client requested a record that doesn’t exist.\n\nThat’s all the code needed for our server. Now let’s write some code to turn your server library into a program we can execute.\n\nRun Your Server\n\nThe last code you need to write is a main package with a main() function to start your server. In the root directory of your project, create a cmd/server directory tree, and in the server directory create a file named main.go with this code:\n\nLetsGo/cmd/server/main.go package main\n\nimport (\n\n\"log\"\n\n\"github.com/travisjeffery/proglog/internal/server\"\n\n)\n\nfunc main() {\n\nsrv := server.NewHTTPServer(\":8080\") log.Fatal(srv.ListenAndServe())\n\n}\n\nOur main() function just needs to create and start the server, passing in the address to listen on (localhost:8080) and telling the server to listen for and handle requests by calling ListenAndServe(). Wrapping our server with the *net/http.Server in NewHTTPServer() saved us from writing a bunch of code here—and anywhere else we’d create an HTTP server.\n\nIt’s time to test our slick new service.\n\nreport erratum • discuss",
      "content_length": 1516,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 24,
      "content": "Test Your API • 11\n\nTest Your API\n\nYou now have a functioning JSON/HTTP commit log service you can run and test by hitting the endpoints with curl. Run the following snippet to start the server:\n\n$ go run main.go\n\nOpen another tab in your terminal and run the following commands to add some records to your log:\n\n$ curl -X POST localhost:8080 -d \\\n\n'{\"record\": {\"value\": \"TGV0J3MgR28gIzEK\"}}'\n\n$ curl -X POST localhost:8080 -d \\\n\n'{\"record\": {\"value\": \"TGV0J3MgR28gIzIK\"}}'\n\n$ curl -X POST localhost:8080 -d \\\n\n'{\"record\": {\"value\": \"TGV0J3MgR28gIzMK\"}}'\n\nGo’s encoding/json package encodes []byte as a base64-encoding string. The record’s value is a []byte, so that’s why our requests have the base64 encoded forms of Let’s Go #1–3. You can read the records back by running the following commands and verifying that you get the associated records back from the server:\n\n$ curl -X GET localhost:8080 -d '{\"offset\": 0}' $ curl -X GET localhost:8080 -d '{\"offset\": 1}' $ curl -X GET localhost:8080 -d '{\"offset\": 2}'\n\nCongratulations—you have built a simple JSON/HTTP service and confirmed it works!\n\nWhat You Learned\n\nIn this chapter, we built a simple JSON/HTTP commit log service that accepts and responds with JSON and stores the records in those requests to an in- memory log. Next, we’ll use protocol buffers to manage our API types, generate custom code, and prepare to write a service with gRPC—an open source, high- performance remote procedure call framework that’s great for building dis- tributed services.\n\nreport erratum • discuss",
      "content_length": 1542,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 25,
      "content": "CHAPTER 2\n\nStructure Data with Protocol Buffers\n\nWhen building distributed services, you’re communicating between the services over a network. To send data (such as your structs) over a network, you need to encode the data in a format to transmit, and lots of programmers choose JSON. When you’re building public APIs or you’re creating a project where you don’t control the clients, JSON makes sense because it’s accessible—both for humans to read and computers to parse. But when you’re building private APIs or building projects where you do control the clients, you can make use of a mechanism for structuring and transmitting data that—compared to JSON—makes you more productive and helps you create services that are faster, have more features, and have fewer bugs.\n\nSo what is this mechanism? Protocol buffers (also known as protobuf), which is Google’s language and platform-neutral extensible mechanism for structur- ing and serializing data. The advantages of using protobuf are that it:\n\nGuarantees type-safety; • Prevents schema-violations; • Enables fast serialization; and • Offers backward compatibility.\n\nProtobuf lets you define how you want your data structured, compile your protobuf into code in potentially many languages, and then read and write your structured data to and from different data streams. Protocol buffers are good for communicating between two systems (such as microservices), which is why Google used protobuf when building gRPC to develop a high-perfor- mance remote procedure call (RPC) framework.\n\nIf you haven’t worked with protobuf before, you may have some of the same concerns I had—that protobuf seems like a lot of extra work. I promise you that, after working with it in this chapter and the rest of the book, you’ll see\n\nreport erratum • discuss",
      "content_length": 1794,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 26,
      "content": "Chapter 2. Structure Data with Protocol Buffers • 14\n\nthat it’s really not so bad. It offers many advantages over JSON, and it’ll end up saving you a lot of work.\n\nHere’s a quick example that shows what protocol buffers look like and how they work. Imagine you work at Twitter and one of the object types you work with are Tweets. Tweets, at the very least, comprise the author’s message. If you defined this in protobuf, it would look like this:\n\nStructureDataWithProtobuf/example.proto syntax = \"proto3\";\n\npackage twitter;\n\nmessage Tweet {\n\nstring message = 1;\n\n}\n\nYou’d then compile this protobuf into code in the language of your choice. For example, the protobuf compiler would take this protobuf and generate the following Go code:\n\nStructureDataWithProtobuf/example.pb.go // Code generated by protoc-gen-go. DO NOT EDIT. // source: example.proto\n\npackage twitter\n\ntype Tweet struct {\n\nMessage string `protobuf:\"bytes,1,opt,name=message,proto3\"\n\njson:\"message,omitempty\"`\n\n// Note: Protobuf generates internal fields and methods // I haven't included for brevity.\n\n}\n\nBut why not just write that Go code yourself? Why use protobuf instead? I’m glad you asked.\n\nWhy Use Protocol Buffers?\n\nProtobuf offers all kinds of useful features:\n\nConsistent schemas\n\nWith protobuf, you encode your semantics once and use them across your services to ensure a consistent data model throughout your whole system. My colleagues and I built the infrastructures at my last two companies on microservices, and we had a repo called “structs” that housed our protobuf and their compiled code, which all our services depended on. By doing this, we ensured that we didn’t send multiple, inconsistent schemas to prod. Thanks to Go’s type checking, we could update our structs dependency, run the tests that touched our data models, and the\n\nreport erratum • discuss",
      "content_length": 1848,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 27,
      "content": "Why Use Protocol Buffers? • 15\n\ncompiler and tests would tell us whether our code was consistent with our schema.\n\nVersioning for free\n\nOne of Google’s motivations for creating protobuf was to eliminate the need for version checks and prevent ugly code like this:\n\nStructureDataWithProtobuf/example.go if (version == 3) { ... } else if (version > 4) {\n\nif (version == 5) {\n\n...\n\n} ...\n\n}\n\nThink of a protobuf message like a Go struct because when you compile a message it turns into a struct. With protobuf, you number your fields on your messages to ensure you maintain backward compatibility as you roll out new features and changes to your protobuf. So it’s easy to add new fields, and intermediate servers that need not use the data can simply parse it and pass through it without needing to know about all the fields. Likewise with removing fields: you can ensure that deprecated fields are no longer used by marking them as reserved; the compiler will then complain if anyone tries to use to the deprecated fields.\n\nLess boilerplate\n\nThe protobuf libraries handle encoding and decoding for you, which means you don’t have to handwrite that code yourself.\n\nExtensibility\n\nThe protobuf compiler supports extensions that can compile your protobuf into code using your own compilation logic. For example, you might want several structs to have a common method. With protobuf, you can write a plugin to generate that method automatically.\n\nLanguage agnosticism\n\nProtobuf is implemented in many languages: since Protobuf version 3.0, there’s support for Go, C++, Java, JavaScript, Python, Ruby, C#, Objective C, and PHP, and third-party support for other languages. And you don’t have to do any extra work to communicate between services written in different languages. This is great for companies with various teams that want to use different languages, or when your team wants to migrate to another language.\n\nreport erratum • discuss",
      "content_length": 1936,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 28,
      "content": "Chapter 2. Structure Data with Protocol Buffers • 16\n\nPerformance\n\nProtobuf is highly performant, and has smaller payloads and serializes up to six times faster than JSON.1\n\ngRPC uses protocol buffers to define APIs and serialize messages; we’ll use gRPC to build our client and server.\n\nHopefully I’ve done a decent job of convincing you that protobuf is cool. But the theory alone is boring! Let’s get you set up to create your own protobuf and use it to build stuff.\n\nInstall the Protocol Buffer Compiler\n\nThe first thing we need to do to get you compiling protobuf is—you guessed it—install the compiler. Go to the Protobuf release page on GitHub2 and download the relevant release for your computer. If you’re on a Mac, for instance, you’d download protoc-3.9.0-osx-x86_64.zip. You can download and install in your terminal like so:\n\n$ wget https://github.com/protocolbuffers/protobuf/\\ releases/download/v3.9.0/protoc-3.9.0-osx-x86_64.zip $ unzip protoc-3.9.0-osx-x86_64.zip -d /usr/local/protobuf\n\nHere’s what the layout and files in the extracted protobuf directory look like:\n\n❯ tree /usr/local/protobuf /usr/local/protobuf ├── bin │ ├── include │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ └── readme.txt\n\n└── protoc\n\n└── google\n\n└── protobuf\n\n├── any.proto ├── api.proto ├── compiler └── plugin.proto │ ├── descriptor.proto ├── duration.proto ├── empty.proto ├── field_mask.proto ├── source_context.proto ├── struct.proto ├── timestamp.proto ├── type.proto └── wrappers.proto\n\n1. 2.\n\nhttps://auth0.com/blog/beating-json-performance-with-protobuf\n\nhttps://github.com/protocolbuffers/protobuf/releases\n\nreport erratum • discuss",
      "content_length": 1621,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 29,
      "content": "Define Your Domain Types as Protocol Buffers • 17\n\nAs you can see, a protobuf installation consists of two directories. The bin directory contains the compiler binary named protoc, and the include directories contains a bunch of protobuf files that are like protobuf’s standard library. A mistake I’ve seen many people make when setting up their systems to work with protobuf is that they install the compiler binary without the include protobuf files. But without those files you can’t compile successfully, so just extract the whole release using the commands I just showed you and you’ll be just dandy.\n\nNow that you’ve got the compiler binary installed, make sure your shell can find and run it. Add the binary to your PATH env var using your shell’s con- figuration file. If you’re using ZSH for instance, run something like the follow- ing to update your configuration:\n\n$ echo 'export PATH=\"$PATH:/usr/local/protobuf/bin\"' >> ~/.zshenv\n\nAt this point the protobuf compiler is installed on your machine. To test the installation, run protoc --version. If you don’t see any errors, you’re ready to handle the rest of this chapter. If you do see errors, don’t worry: few installa- tion problems are unique. Google will show you the way.\n\nWith the compiler installed, you’re ready to write and compile some protobuf. Let’s get to it!\n\nDefine Your Domain Types as Protocol Buffers\n\nIn the previous chapter, we defined our Record type in Go as this struct:\n\nLetsGo/internal/server/log.go type Record struct {\n\nValue []byte `json:\"value\"` Offset uint64 `json:\"offset\"`\n\n}\n\nTo turn that into a protobuf message we need to convert the Go code into protobuf syntax.\n\nThe convention for Go projects is to put your protobuf in an api directory. So run mkdir -p api/v1 to create your directories, then create a file called log.proto in the v1 directory and put this code in it:\n\nStructureDataWithProtobuf/api/v1/log.proto syntax = \"proto3\";\n\npackage log.v1;\n\noption go_package = \"github.com/travisjeffery/api/log_v1\";\n\nreport erratum • discuss",
      "content_length": 2037,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 30,
      "content": "Chapter 2. Structure Data with Protocol Buffers • 18\n\nmessage Record {\n\nbytes value = 1; uint64 offset = 2;\n\n}\n\nIn this protobuf code, we specify that we’re using proto3 syntax—the latest version of protobuf syntax. Then we specify a package name for two reasons: because this protobuf package name is used as the package name in the generated Go code and because it prevents name clashes between protocol message types that have the same name.\n\nThese protobuf messages are equivalent to the Go structs shown earlier. You’ll notice the two syntaxes are very similar: in Go you have struct, and with protobuf you have a message—both with a list of fields. In Go you put the name of the field on the left followed by its type, and with protobuf you put the name of the field on right followed by its name (with an additional field ID).\n\nFollowing the package declarations in the protobuf code, we define our Record type. Protocol buffer programmers use the repeated keyword to define a slice of some type, so repeated Record records means the records field is a []Record in Go.\n\nI mentioned earlier that one handy feature of protobuf is the ability to version fields. Each field has a type, name, and unique field number. These field numbers identify your fields in the marshaled binary format, and you shouldn’t change them once your messages are in use in your projects. Consider fields immutable: you can stop using old fields and add new fields, but you can’t modify existing fields. You want to change fields like this when you make small, iterative changes—like when you add or remove features or data from a message.\n\nBesides field versions, you’ll also want to group your messages by a major version. The major version gives you control over your protobuf when you overhaul projects to rearchitect your infrastructure or run multiple message versions at the same time for a migration period. Bumping major versions should be a rare occurrence because for most changes, field versioning is sufficient. I’ve only had to bump the major version of my protobuf twice, and if you look at Google’s API definitions3 protobuf, they’ve only bumped their major version a couple times. So changing major versions is uncommon, but it’s nice to have the ability when you need it.\n\n3.\n\nhttps://github.com/googleapis/googleapis\n\nreport erratum • discuss",
      "content_length": 2343,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 31,
      "content": "Compile Protocol Buffers • 19\n\nAt the beginning of this section, I had you put the log.proto file into an api/v1 directory. The v1 represents these protobufs’ major version. If you were to continue building this project and decided to break API compatibility, you would create a v2 directory to package the new messages together and com- municate to your users you’ve made incompatible API changes.\n\nNow that we’ve created the protocol buffer messages, let’s compile your pro- tobuf into Go code.\n\nCompile Protocol Buffers\n\nTo compile protobuf into the code of some programming language, you need the runtime for that language. The compiler itself doesn’t know how to compile protobuf into every language—it needs a language-specific runtime to do so.\n\nGo has two runtimes to compile protobuf into Go code. The Go team and the protobuf team at Google developed the original runtime.4 Then a team of folks who wanted more features forked the original runtime and developed it into gogoprotobuf, with more code-generation features and faster marshaling and unmarshaling. Projects like Etcd, Mesos, Kubernetes, Docker, CockroachDB, and NATS as well as companies like Dropbox and Sendgrid used gogoprotobuf. I used gogoprotobuf for my projects to integrate with Kubernetes’ protocol buffers and for gogoprotobuf’s features.\n\nIn March 2020, the Go team released a major revision of the Go API (APIv2)5 for protocol buffers with improved performance6 and a reflection API that enables adding features like those provided by gogoprotobuf. Projects7 that used gogoprotobuf have begun switching to APIv28 because of APIv2’s improved performance, its new reflection API, its incompatibility with gogoprotobuf, and the gogoprotobuf project needing new ownership.9 I recommend using APIv2, too.\n\nTo compile our protobuf into Go, we need to install the protobuf runtime by running the following command:\n\n$ go get google.golang.org/protobuf/...@v1.25.0\n\n4. 5. 6. 7. 8. 9.\n\nhttps://github.com/golang/protobuf\n\nhttps://github.com/alexshtin/proto-bench/blob/master/README.md\n\nhttps://github.com/istio/istio/pull/24956\n\nhttps://github.com/istio/api/pull/1607\n\nhttps://github.com/envoyproxy/go-control-plane/pull/226\n\nhttps://github.com/gogo/protobuf/issues/691\n\nreport erratum • discuss",
      "content_length": 2269,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 32,
      "content": "Chapter 2. Structure Data with Protocol Buffers • 20\n\nYou can now compile your protobuf by running the following command at the root of your project:\n\n$ protoc api/v1/*.proto \\\n\n--go_out=. \\ --go_opt=paths=source_relative \\ --proto_path=.\n\nLook in the api/v1 directory and you’ll see a new file called log.pb.go. Open it up to see the Go code that the compiler generated from your protobuf code. Your protobuf message has been turned into a Go struct, along with some methods on the struct for marshaling to the protobuf binary wire format, and getters for your fields.\n\nSince you’ll compile your protobuf every time you change them, it’s worth adding a Makefile file with a compile target that you can quickly run again and again. We’ll include a test target for testing our code too. So create a Makefile file at the root of your repo with the following code:\n\nStructureDataWithProtobuf/Makefile compile:\n\nprotoc api/v1/*.proto \\\n\n--go_out=. \\ --go_opt=paths=source_relative \\ --proto_path=.\n\ntest:\n\ngo test -race ./...\n\nThat’s all there is to compiling your protobuf code into Go code. Now let’s talk about how to work with the generated code and extend the compiler to gener- ate your own code.\n\nWork with the Generated Code\n\nAlthough the generated code in log.pb.go is a lot longer than your handwritten code in log.go (because of the extra code needed to marshal to the protobuf binary wire format), you’ll use the code as if you’d handwritten it. For example, you’ll create instances using the & operator (or new keyword) and access fields using a dot.\n\nThe compiler generates various methods on the struct, but the only methods you’ll use directly are the getters. Use the struct’s fields when you can, but you’ll find the getters useful when you have multiple messages with the same getter(s) and you want to abstract those method(s) into an interface. For example, imagine you’re building a retail site like Amazon and have different types of stuff you sell—books, games, and so on—each with a field for the\n\nreport erratum • discuss",
      "content_length": 2043,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 33,
      "content": "What You Learned • 21\n\nitem’s price, and you want to find the total of the items in the user’s cart. You’d make a Pricer interface and a Total function that takes in a slice of Pricer interfaces and returns their total cost. Here’s what the code would look like:\n\ntype Book struct {\n\nPrice uint64\n\n}\n\nfunc(b *Book) GetPrice() uint64 { // ... }\n\ntype Game struct {\n\nPrice uint64\n\n}\n\nfunc(b *Game) GetPrice() uint64 { // ... }\n\ntype Pricer interface {\n\nGetPrice() uint64\n\n}\n\nfunc Total(items []Pricer) uint64 { // ... }\n\nNow imagine that you want to write a script to change the price of all your inventory—books, games, and so on. You could do this with reflection, but reflection should be your last resort since, as the Go proverb goes, reflection is never clear.10 If we just had setters, we could use an interface like the fol- lowing to set the price on the different kinds of items in your inventory:\n\ntype PriceAdjuster interface { SetPrice(price uint64)\n\n}\n\nWhen the compiled code isn’t quite what you need, you can extend the com- piler’s output with plugins. Though we don’t need to write a plugin for this project, I’ve written some plugins that were incredibly useful to the projects I worked on; it’s worth learning to write your own so you can recognize when a plugin will save you a ton of manual labor.\n\nWhat You Learned\n\nIn this chapter, we covered the protobuf fundamentals we’ll use throughout our project. These concepts will be vital throughout our project, especially as we build our gRPC client and server. Now let’s create the next vital piece of our project: a commit log library.\n\n10. https://bit.ly/2HcYojl\n\nreport erratum • discuss",
      "content_length": 1658,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 34,
      "content": "CHAPTER 3\n\nWrite a Log Package\n\nIn this book we’re building a distributed service to learn how to create dis- tributed services with Go (shocker). But how does building a log in this chapter help us achieve that goal? I believe the log is the most important tool in your toolkit when building distributed services. Logs—which are sometimes also called write-ahead logs, transaction logs, or commit logs—are at the heart of storage engines, message queues, version control, and replication and consensus algorithms. As you build distributed services, you’ll face problems that you can solve with logs. By building a log yourself, you’ll learn how to:\n\nSolve problems using logs and discover how they can make hard problems\n\neasier.\n\nChange existing log-based systems to fit your needs and build your own\n\nlog-based systems.\n\nWrite and read data efficiently when building storage engines.\n\nProtect against data loss caused by system failures.\n\nEncode data to persist it to a disk or to build your own wire protocols and\n\nsend the data between applications.\n\nAnd who knows—maybe you’ll be the one who builds the next big distributed log service.\n\nThe Log Is a Powerful Tool\n\nFolks who develop storage engines of filesystems and databases use logs to improve the data integrity of their systems. The ext filesystems, for example, log changes to a journal instead of directly changing the disk’s data file. Once the filesystem has safely written the changes to the journal, it then applies those changes to the data files. Logging to the journal is simple and fast, so\n\nreport erratum • discuss",
      "content_length": 1589,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 35,
      "content": "Chapter 3. Write a Log Package • 24\n\nthere’s little chance of losing data. Even if your computer crashed before ext had finished updating the disk files, then on the next boot, the filesystem would process the data in the journal to complete its updates. Database developers, like PostgreSQL, use the same technique to make their systems durable: they record changes to a log, called a write-ahead log (WAL), and later process the WAL to apply the changes to their database’s data files.\n\nDatabase developers use the WAL for replication, too. Instead of writing the logs to a disk, they write the logs over the network to its replicas. The replicas apply the changes to their own data copies, and eventually they all end up at the same state. Raft, a consensus algorithm, uses the same idea to get distributed services to agree on a cluster-wide state. Each node in a Raft cluster runs a state machine with a log as its input. The leader of the Raft cluster appends changes to its followers’ logs. Since the state machines use the logs as input and because the logs have the same records in the same order, all the services end up with the same state.\n\nWeb front-end developers use logs to help manage state in their applications. In Redux,1 a popular JavaScript library commonly used with React, you log changes as plain objects and handle those changes with pure functions that apply the updates to your application’s state.\n\nAll these examples use logs to store, share, and process ordered data. This is really cool because the same tool helps replicate databases, coordinate distributed services, and manage state in front-end applications. You can solve a lot of problems, especially in distributed services, by breaking down the changes in your system until they’re single, atomic operations that you can store, share, and process with a log.\n\nDatabases often provide a way to restore their state to some time in the past, often referred to as point-in-time recovery. You take a snapshot of your database from the past and then replay the logs from the write-ahead log until it’s at the point in time you want. You don’t need the snapshot if you have every single log since the beginning to replay, but for databases with long histories and a lot of changes, keeping every log isn’t feasible. Redux uses the same idea to undo/redo actions: it logs the application’s state after each action and undoing an action just requires Redux to move the state shown in the UI to the previously logged state. Distributed version control systems like Git work similarly; your commit log history is a literal commit log.\n\n1.\n\nhttps://redux.js.org\n\nreport erratum • discuss",
      "content_length": 2665,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 36,
      "content": "How Logs Work • 25\n\nAs you can see, a complete log not only holds the latest state, but all states that have existed, which allows you to build some cool features that you’d find complicated to build otherwise. Logs are simple—and that’s why they’re good.\n\nHow Logs Work\n\nA log is an append-only sequence of records. You append records to the end of the log, and you typically read top to bottom, oldest to newest—similar to running tail -f on a file. You can log any data. People have historically used the term logs to refer to lines of text meant for humans to read, but that’s changed as more people use log systems where their “logs” are binary- encoded messages meant for other programs to read. When I talk about logs and records in this book, I’m not talking about any particular type of data. When you append a record to a log, the log assigns the record a unique and sequential offset number that acts like the ID for that record. A log is like a table that always orders the records by time and indexes each record by its offset and time created.\n\nConcrete implementations of logs have to deal with us not having disks with infinite space, which means we can’t append to the same file forever. So we split the log into a list of segments. When the log grows too big, we free up disk space by deleting old segments whose data we’ve already processed or archived. This cleaning up of old segments can run in a background process while our service can still produce to the active (newest) segment and consume from other segments with no, or at least fewer, conflicts where goroutines access the same data.\n\nThere’s always one special segment among the list of segments, and that’s the active segment. We call it the active segment because it’s the only segment we actively write to. When we’ve filled the active segment, we create a new segment and make it the active segment.\n\nEach segment comprises a store file and an index file. The segment’s store file is where we store the record data; we continually append records to this file. The segment’s index file is where we index each record in the store file. The index file speeds up reads because it maps record offsets to their position in the store file. Reading a record given its offset is a two-step process: first you get the entry from the index file for the record, which tells you the position of the record in the store file, and then you read the record at that position in the store file. Since the index file requires only two small fields—the offset and stored position of the record—the index file is much smaller than the store file that stores all your record data. Index files are small enough that\n\nreport erratum • discuss",
      "content_length": 2703,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 37,
      "content": "Chapter 3. Write a Log Package • 26\n\nwe can memory-map2 them and make operations on the file as fast as operat- ing on in-memory data.\n\nNow that you know how logs work, it’s time to build our own. Let’s get cracking to code it up.\n\nBuild a Log\n\nWe will build our log from the bottom up, starting with the store and index files, then the segment, and finally the log. That way we can write and run tests as we build each piece. Since the word log can refer to at least three different things—a record, the file that stores records, and the abstract data type that ties segments together—to make things less confusing, throughout this chapter, I will consistently use the following terms to mean these things:\n\nRecord—the data stored in our log. • Store—the file we store records in. • Index—the file we store index entries in. • Segment—the abstraction that ties a store and an index together. • Log—the abstraction that ties all the segments together.\n\nCode the Store\n\nTo get started, create a directory at internal/log for our log package, then create a file called store.go in that directory that contains the following code:\n\nWriteALogPackage/internal/log/store.go package log\n\nimport (\n\n\"bufio\" \"encoding/binary\" \"os\" \"sync\"\n\n)\n\nvar (\n\nenc = binary.BigEndian\n\n)\n\nconst (\n\nlenWidth = 8\n\n)\n\ntype store struct {\n\nos.File\n\n2.\n\nhttps://en.wikipedia.org/wiki/Memory-mapped_file\n\nreport erratum • discuss",
      "content_length": 1401,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 38,
      "content": "Build a Log • 27\n\nmu buf *bufio.Writer size uint64\n\nsync.Mutex\n\n}\n\nfunc newStore(f *os.File) (*store, error) { fi, err := os.Stat(f.Name()) if err != nil {\n\nreturn nil, err\n\n} size := uint64(fi.Size()) return &store{\n\nFile: f, size: size, buf: bufio.NewWriter(f),\n\n}, nil\n\n}\n\nThe store struct is a simple wrapper around a file with two APIs to append and read bytes to and from the file. The newStore(*os.File) function creates a store for the given file. The function calls os.Stat(name string) to get the file’s current size, in case we’re re-creating the store from a file that has existing data, which would happen if, for example, our service had restarted.\n\nWe refer to the enc variable and lenWidth constant repeatedly in the store, so we place them up top where they’re easy to find. enc defines the encoding that we persist record sizes and index entries in and lenWidth defines the number of bytes used to store the record’s length.\n\nNext, write the following Append() method below newStore():\n\nWriteALogPackage/internal/log/store.go func (s *store) Append(p []byte) (n uint64, pos uint64, err error) {\n\ns.mu.Lock() defer s.mu.Unlock() pos = s.size if err := binary.Write(s.buf, enc, uint64(len(p))); err != nil {\n\nreturn 0, 0, err\n\n} w, err := s.buf.Write(p) if err != nil {\n\nreturn 0, 0, err\n\n} w += lenWidth s.size += uint64(w) return uint64(w), pos, nil\n\n}\n\nAppend([]byte) persists the given bytes to the store. We write the length of the record so that, when we read the record, we know how many bytes to read.\n\nreport erratum • discuss",
      "content_length": 1551,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 39,
      "content": "Chapter 3. Write a Log Package • 28\n\nWe write to the buffered writer instead of directly to the file to reduce the number of system calls and improve performance. If a user wrote a lot of small records, this would help a lot. Then we return the number of bytes written, which similar Go APIs conventionally do, and the position where the store holds the record in its file. The segment will use this position when it creates an associated index entry for this record.\n\nBelow Append(), add the following Read() method:\n\nWriteALogPackage/internal/log/store.go func (s *store) Read(pos uint64) ([]byte, error) {\n\ns.mu.Lock() defer s.mu.Unlock() if err := s.buf.Flush(); err != nil {\n\nreturn nil, err\n\n} size := make([]byte, lenWidth) if _, err := s.File.ReadAt(size, int64(pos)); err != nil {\n\nreturn nil, err\n\n} b := make([]byte, enc.Uint64(size)) if _, err := s.File.ReadAt(b, int64(pos+lenWidth)); err != nil {\n\nreturn nil, err\n\n} return b, nil\n\n}\n\nRead(pos uint64) returns the record stored at the given position. First it flushes the writer buffer, in case we’re about to try to read a record that the buffer hasn’t flushed to disk yet. We find out how many bytes we have to read to get the whole record, and then we fetch and return the record. The compiler allocates byte slices that don’t escape the functions they’re declared in on the stack. A value escapes when it lives beyond the lifetime of the function call—if you return the value, for example.\n\nPut this ReadAt() method under Read():\n\nWriteALogPackage/internal/log/store.go func (s *store) ReadAt(p []byte, off int64) (int, error) {\n\ns.mu.Lock() defer s.mu.Unlock() if err := s.buf.Flush(); err != nil {\n\nreturn 0, err\n\n} return s.File.ReadAt(p, off)\n\n}\n\nReadAt(p []byte, off int64) reads len(p) bytes into p beginning at the off offset in the store’s file. It implements io.ReaderAt on the store type.\n\nreport erratum • discuss",
      "content_length": 1892,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 40,
      "content": "Build a Log • 29\n\nLast, add this Close() method after ReadAt():\n\nWriteALogPackage/internal/log/store.go func (s *store) Close() error {\n\ns.mu.Lock() defer s.mu.Unlock() err := s.buf.Flush() if err != nil {\n\nreturn err\n\n} return s.File.Close()\n\n}\n\nClose() persists any buffered data before closing the file.\n\nLet’s test that our store works. Create a store_test.go file in the log directory with the following code:\n\nWriteALogPackage/internal/log/store_test.go package log\n\nimport (\n\n\"io/ioutil\" \"os\" \"testing\"\n\n\"github.com/stretchr/testify/require\"\n\n)\n\nvar (\n\nwrite = []byte(\"hello world\") width = uint64(len(write)) + lenWidth\n\n)\n\nfunc TestStoreAppendRead(t *testing.T) {\n\nf, err := ioutil.TempFile(\"\", \"store_append_read_test\") require.NoError(t, err) defer os.Remove(f.Name())\n\ns, err := newStore(f) require.NoError(t, err)\n\ntestAppend(t, s) testRead(t, s) testReadAt(t, s)\n\ns, err = newStore(f) require.NoError(t, err) testRead(t, s)\n\n}\n\nIn this test, we create a store with a temporary file and call two test helpers to test appending and reading from the store. Then we create the store again\n\nreport erratum • discuss",
      "content_length": 1124,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 41,
      "content": "Chapter 3. Write a Log Package • 30\n\nand test reading from it again to verify that our service will recover its state after a restart.\n\nAfter the TestStoreAppendRead() function, add these test helpers:\n\nWriteALogPackage/internal/log/store_test.go func testAppend(t *testing.T, s *store) {\n\nt.Helper() for i := uint64(1); i < 4; i++ {\n\nn, pos, err := s.Append(write) require.NoError(t, err) require.Equal(t, pos+n, width*i)\n\n}\n\n}\n\nfunc testRead(t *testing.T, s *store) {\n\nt.Helper() var pos uint64 for i := uint64(1); i < 4; i++ { read, err := s.Read(pos) require.NoError(t, err) require.Equal(t, write, read) pos += width\n\n}\n\n}\n\nfunc testReadAt(t *testing.T, s *store) {\n\nt.Helper() for i, off := uint64(1), int64(0); i < 4; i++ {\n\nb := make([]byte, lenWidth) n, err := s.ReadAt(b, off) require.NoError(t, err) require.Equal(t, lenWidth, n) off += int64(n)\n\nsize := enc.Uint64(b) b = make([]byte, size) n, err = s.ReadAt(b, off) require.NoError(t, err) require.Equal(t, write, b) require.Equal(t, int(size), n) off += int64(n)\n\n}\n\n}\n\nBelow testReadAt(), add this snippet to test the Close() method:\n\nWriteALogPackage/internal/log/store_test.go func TestStoreClose(t *testing.T) {\n\nf, err := ioutil.TempFile(\"\", \"store_close_test\") require.NoError(t, err) defer os.Remove(f.Name()) s, err := newStore(f)\n\nreport erratum • discuss",
      "content_length": 1328,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 42,
      "content": "Build a Log • 31\n\nrequire.NoError(t, err) _, _, err = s.Append(write) require.NoError(t, err)\n\nf, beforeSize, err := openFile(f.Name()) require.NoError(t, err)\n\nerr = s.Close() require.NoError(t, err)\n\n_, afterSize, err := openFile(f.Name()) require.NoError(t, err) require.True(t, afterSize > beforeSize)\n\n}\n\nfunc openFile(name string) (file *os.File, size int64, err error)\n\n{\n\nf, err := os.OpenFile( name, os.O_RDWR|os.O_CREATE|os.O_APPEND, 0644,\n\n) if err != nil {\n\nreturn nil, 0, err\n\n} fi, err := f.Stat() if err != nil {\n\nreturn nil, 0, err\n\n} return f, fi.Size(), nil\n\n}\n\nAssuming these tests pass, you know that your log can append and read persisted records.\n\nWrite the Index\n\nNext let’s code the index. Create an index.go file inside internal/log that contains the following code:\n\nWriteALogPackage/internal/log/index.go package log\n\nimport (\n\n\"io\" \"os\"\n\n\"github.com/tysontate/gommap\"\n\n)\n\nvar (\n\noffWidth uint64 = 4 posWidth uint64 = 8 entWidth\n\n= offWidth + posWidth\n\n)\n\nreport erratum • discuss",
      "content_length": 1007,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 43,
      "content": "Chapter 3. Write a Log Package • 32\n\ntype index struct {\n\nfile *os.File mmap gommap.MMap size uint64\n\n}\n\nWe use the *Width constants throughout the index, so like with the store’s variables and constants, we put the constants at the top of the file to make them easy to find. The *Width constants define the number of bytes that make up each index entry.\n\nOur index entries contain two fields: the record’s offset and its position in the store file. We store offsets as uint32s and positions as uint64s, so they take up 4 and 8 bytes of space, respectively. We use the entWidth to jump straight to the position of an entry given its offset since the position in the file is offset * entWidth.\n\nindex defines our index file, which comprises a persisted file and a memory- mapped file. The size tells us the size of the index and where to write the next entry appended to the index.\n\nNow add the following newIndex() function below the index:\n\nWriteALogPackage/internal/log/index.go func newIndex(f *os.File, c Config) (*index, error) {\n\nidx := &index{\n\nfile: f,\n\n} fi, err := os.Stat(f.Name()) if err != nil {\n\nreturn nil, err\n\n} idx.size = uint64(fi.Size()) if err = os.Truncate(\n\nf.Name(), int64(c.Segment.MaxIndexBytes),\n\n); err != nil {\n\nreturn nil, err\n\n} if idx.mmap, err = gommap.Map( idx.file.Fd(), gommap.PROT_READ|gommap.PROT_WRITE, gommap.MAP_SHARED,\n\n); err != nil {\n\nreturn nil, err\n\n} return idx, nil\n\n}\n\nreport erratum • discuss",
      "content_length": 1442,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 44,
      "content": "Build a Log • 33\n\nnewIndex(*os.File) creates an index for the given file. We create the index and save the current size of the file so we can track the amount of data in the index file as we add index entries. We grow the file to the max index size before memory-mapping the file and then return the created index to the caller.\n\nNext, add the following Close() method below newIndex():\n\nWriteALogPackage/internal/log/index.go func (i *index) Close() error {\n\nif err := i.mmap.Sync(gommap.MS_SYNC); err != nil {\n\nreturn err\n\n} if err := i.file.Sync(); err != nil {\n\nreturn err\n\n} if err := i.file.Truncate(int64(i.size)); err != nil {\n\nreturn err\n\n} return i.file.Close()\n\n}\n\nClose() makes sure the memory-mapped file has synced its data to the persisted file and that the persisted file has flushed its contents to stable storage. Then it truncates the persisted file to the amount of data that’s actually in it and closes the file.\n\nNow that we’ve seen the code for both opening and closing an index, we can discuss what this growing and truncating the file business is all about.\n\nWhen we start our service, the service needs to know the offset to set on the next record appended to the log. The service learns the next record’s offset by looking at the last entry of the index, a simple process of reading the last 12 bytes of the file. However, we mess up this process when we grow the files so we can memory-map them. (The reason we resize them now is that, once they’re memory-mapped, we can’t resize them, so it’s now or never.) We grow the files by appending empty space at the end of them, so the last entry is no longer at the end of the file—instead, there’s some unknown amount of space between this entry and the file’s end. This space prevents the service from restarting properly. That’s why we shut down the service by truncating the index files to remove the empty space and put the last entry at the end of the file once again. This graceful shutdown returns the service to a state where it can restart properly and efficiently.\n\nreport erratum • discuss",
      "content_length": 2073,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 45,
      "content": "Chapter 3. Write a Log Package • 34\n\nHandling Ungraceful Shutdowns\n\nA graceful shutdown occurs when a service finishes its ongoing tasks, performs its processes to ensure there’s no data loss, and prepares for a restart. If your service crashes or its hardware fails, you’ll experience an ungraceful shutdown. An example of an ungraceful shutdown for the service we’re building would be if it lost power before it finished truncating its index files. You handle ungraceful shutdowns by performing a sanity check when your service restarts to find corrupted data. If you have corrupted data, you can rebuild the data or replicate the data from an uncorrupted source. The log we’re building doesn’t handle ungraceful shut- downs because I wanted to keep the code simple.\n\nAnd now back to our regularly scheduled programming.\n\nAdd the following Read() method below newIndex():\n\nWriteALogPackage/internal/log/index.go func (i *index) Read(in int64) (out uint32, pos uint64, err error) {\n\nif i.size == 0 {\n\nreturn 0, 0, io.EOF\n\n} if in == -1 {\n\nout = uint32((i.size / entWidth) - 1)\n\n} else {\n\nout = uint32(in)\n\n} pos = uint64(out) * entWidth if i.size < pos+entWidth {\n\nreturn 0, 0, io.EOF\n\n} out = enc.Uint32(i.mmap[pos : pos+offWidth]) pos = enc.Uint64(i.mmap[pos+offWidth : pos+entWidth]) return out, pos, nil\n\n}\n\nRead(int64) takes in an offset and returns the associated record’s position in the store. The given offset is relative to the segment’s base offset; 0 is always the offset of the index’s first entry, 1 is the second entry, and so on. We use relative offsets to reduce the size of the indexes by storing offsets as uint32s. If we used absolute offsets, we’d have to store the offsets as uint64s and require four more bytes for each entry. Four bytes doesn’t sound like much, until you multiply it by the number of records people often use distributed logs for, which with a company like LinkedIn is trillions of records every day. Even relatively small companies can make billions of records per day.\n\nreport erratum • discuss",
      "content_length": 2038,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 46,
      "content": "Build a Log • 35\n\nNow add the following Write() method below Read():\n\nWriteALogPackage/internal/log/index.go func (i *index) Write(off uint32, pos uint64) error {\n\nif uint64(len(i.mmap)) < i.size+entWidth {\n\nreturn io.EOF\n\n} enc.PutUint32(i.mmap[i.size:i.size+offWidth], off) enc.PutUint64(i.mmap[i.size+offWidth:i.size+entWidth], pos) i.size += uint64(entWidth) return nil\n\n}\n\nWrite(off uint32, pos uint32) appends the given offset and position to the index. First, we validate that we have space to write the entry. If there’s space, we then encode the offset and position and write them to the memory-mapped file. Then we increment the position where the next write will go.\n\nAdd this Name() method to return the index’s file path:\n\nWriteALogPackage/internal/log/index.go func (i *index) Name() string {\n\nreturn i.file.Name()\n\n}\n\nLet’s test our index. Create an index_test.go file in internal/log starting with the following code:\n\nWriteALogPackage/internal/log/index_test.go package log\n\nimport (\n\n\"io\" \"io/ioutil\" \"os\" \"testing\"\n\n\"github.com/stretchr/testify/require\"\n\n)\n\nfunc TestIndex(t *testing.T) {\n\nf, err := ioutil.TempFile(os.TempDir(), \"index_test\") require.NoError(t, err) defer os.Remove(f.Name())\n\nc := Config{} c.Segment.MaxIndexBytes = 1024 idx, err := newIndex(f, c) require.NoError(t, err) _, _, err = idx.Read(-1) require.Error(t, err) require.Equal(t, f.Name(), idx.Name())\n\nreport erratum • discuss",
      "content_length": 1421,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 47,
      "content": "Chapter 3. Write a Log Package • 36\n\nentries := []struct {\n\nOff uint32 Pos uint64\n\n}{\n\n{Off: 0, Pos: 0}, {Off: 1, Pos: 10},\n\n}\n\nThis code sets up the test. We create an index file and make it big enough to contain our test entries via the Truncate() call. We have to grow the file before we use it because we memory-map the file to a slice of bytes and if we didn’t increase the size of the file before we wrote to it, we’d get an out-of-bounds error.\n\nFinally, add the following code beneath the previous snippet to finish the test:\n\nWriteALogPackage/internal/log/index_test.go\n\nfor _, want := range entries {\n\nerr = idx.Write(want.Off, want.Pos) require.NoError(t, err)\n\n_, pos, err := idx.Read(int64(want.Off)) require.NoError(t, err) require.Equal(t, want.Pos, pos)\n\n}\n\n// index and scanner should error when reading past existing entries _, _, err = idx.Read(int64(len(entries))) require.Equal(t, io.EOF, err) _ = idx.Close()\n\n// index should build its state from the existing file f, _ = os.OpenFile(f.Name(), os.O_RDWR, 0600) idx, err = newIndex(f, c) require.NoError(t, err) off, pos, err := idx.Read(-1) require.NoError(t, err) require.Equal(t, uint32(1), off) require.Equal(t, entries[1].Pos, pos)\n\n}\n\nWe iterate over each entry and write it to the index. We check that we can read the same entry back via the Read() method. Then we verify that the index and scanner error when we try to read beyond the number of entries stored in the index. And we check that the index builds its state from the existing file, for when our service restarts with existing data.\n\nWe need to configure the max size of a segment’s store and index. Let’s add a config struct to centralize the log’s configuration, making it easy to configure\n\nreport erratum • discuss",
      "content_length": 1757,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 48,
      "content": "Build a Log • 37\n\nthe log and use the configs throughout the code. Create an internal/log/config.go file with the following code:\n\nWriteALogPackage/internal/log/config.go package log\n\ntype Config struct {\n\nSegment struct {\n\nMaxStoreBytes uint64 MaxIndexBytes uint64 InitialOffset uint64\n\n}\n\n}\n\nThat wraps up the code for store and index types, which make up the lowest level of our log. Now let’s code the segment.\n\nCreate the Segment\n\nThe segment wraps the index and store types to coordinate operations across the two. For example, when the log appends a record to the active segment, the segment needs to write the data to its store and add a new entry in the index. Similarly for reads, the segment needs to look up the entry from the index and then fetch the data from the store.\n\nTo get started, create a file called segment.go in internal/log that starts with the following code:\n\nWriteALogPackage/internal/log/segment.go package log\n\nimport (\n\n\"fmt\" \"os\" \"path\"\n\napi \"github.com/travisjeffery/proglog/api/v1\" \"google.golang.org/protobuf/proto\"\n\n)\n\ntype segment struct { *store store index *index baseOffset, nextOffset uint64 Config config\n\n}\n\nOur segment needs to call its store and index files, so we keep pointers to those in the first two fields. We need the next and base offsets to know what offset to append new records under and to calculate the relative offsets for the index entries. And we put the config on the segment so we can compare\n\nreport erratum • discuss",
      "content_length": 1482,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 49,
      "content": "Chapter 3. Write a Log Package • 38\n\nthe store file and index sizes to the configured limits, which lets us know when the segment is maxed out.\n\nBelow the previous snippet, add the following newSegment() function:\n\nWriteALogPackage/internal/log/segment.go func newSegment(dir string, baseOffset uint64, c Config) (*segment, error) {\n\ns := &segment{\n\nbaseOffset: baseOffset, config:\n\nc,\n\n} var err error storeFile, err := os.OpenFile(\n\npath.Join(dir, fmt.Sprintf(\"%d%s\", baseOffset, \".store\")), os.O_RDWR|os.O_CREATE|os.O_APPEND, 0644,\n\n) if err != nil {\n\nreturn nil, err\n\n} if s.store, err = newStore(storeFile); err != nil {\n\nreturn nil, err\n\n} indexFile, err := os.OpenFile(\n\npath.Join(dir, fmt.Sprintf(\"%d%s\", baseOffset, \".index\")), os.O_RDWR|os.O_CREATE, 0644,\n\n) if err != nil {\n\nreturn nil, err\n\n} if s.index, err = newIndex(indexFile, c); err != nil {\n\nreturn nil, err\n\n} if off, _, err := s.index.Read(-1); err != nil {\n\ns.nextOffset = baseOffset\n\n} else {\n\ns.nextOffset = baseOffset + uint64(off) + 1\n\n} return s, nil\n\n}\n\nThe log calls newSegment() when it needs to add a new segment, such as when the current active segment hits its max size. We open the store and index files and pass the os.O_CREATE file mode flag as an argument to os.OpenFile() to create the files if they don’t exist yet. When we create the store file, we pass the os.O_APPEND flag to make the operating system append to the file when writing. Then we create our index and store with these files. Finally, we set the segment’s next offset to prepare for the next appended record. If the index\n\nreport erratum • discuss",
      "content_length": 1601,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 50,
      "content": "Build a Log • 39\n\nis empty, then the next record appended to the segment would be the first record and its offset would be the segment’s base offset. If the index has at least one entry, then that means the offset of the next record written should take the offset at the end of the segment, which we get by adding 1 to the base offset and relative offset. Our segment is ready to write to and read from the log—once we’ve written those methods!\n\nNext, below newSegment() put the following Append() method:\n\nWriteALogPackage/internal/log/segment.go func (s *segment) Append(record *api.Record) (offset uint64, err error) {\n\ncur := s.nextOffset record.Offset = cur p, err := proto.Marshal(record) if err != nil {\n\nreturn 0, err\n\n} _, pos, err := s.store.Append(p) if err != nil {\n\nreturn 0, err\n\n} if err = s.index.Write(\n\n// index offsets are relative to base offset uint32(s.nextOffset-uint64(s.baseOffset)), pos,\n\n); err != nil {\n\nreturn 0, err\n\n} s.nextOffset++ return cur, nil\n\n}\n\nAppend() writes the record to the segment and returns the newly appended record’s offset. The log returns the offset to the API response. The segment appends a record in a two-step process: it appends the data to the store and then adds an index entry. Since index offsets are relative to the base offset, we subtract the segment’s next offset from its base offset (which are both absolute offsets) to get the entry’s relative offset in the segment. We then increment the next offset to prep for a future append call.\n\nNow add the following Read() method below Append():\n\nWriteALogPackage/internal/log/segment.go func (s *segment) Read(off uint64) (*api.Record, error) {\n\n_, pos, err := s.index.Read(int64(off - s.baseOffset)) if err != nil {\n\nreturn nil, err\n\n} p, err := s.store.Read(pos)\n\nreport erratum • discuss",
      "content_length": 1800,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 51,
      "content": "Chapter 3. Write a Log Package • 40\n\nif err != nil {\n\nreturn nil, err\n\n} record := &api.Record{} err = proto.Unmarshal(p, record) return record, err\n\n}\n\nRead(off uint64) returns the record for the given offset. Similar to writes, to read a record the segment must first translate the absolute index into a relative offset and get the associated index entry. Once it has the index entry, the segment can go straight to the record’s position in the store and read the proper amount of data.\n\nNext, put the following IsMaxed() method below Read():\n\nWriteALogPackage/internal/log/segment.go func (s *segment) IsMaxed() bool {\n\nreturn s.store.size >= s.config.Segment.MaxStoreBytes ||\n\ns.index.size >= s.config.Segment.MaxIndexBytes\n\n}\n\nIsMaxed() returns whether the segment has reached its max size, either by writing too much to the store or the index. If you wrote a small number of long logs, then you’d hit the segment bytes limit; if you wrote a lot of small logs, then you’d hit the index bytes limit. The log uses this method to know it needs to create a new segment.\n\nWrite this Remove() method below IsMaxed():\n\nWriteALogPackage/internal/log/segment.go func (s *segment) Remove() error {\n\nif err := s.Close(); err != nil {\n\nreturn err\n\n} if err := os.Remove(s.index.Name()); err != nil {\n\nreturn err\n\n} if err := os.Remove(s.store.Name()); err != nil {\n\nreturn err\n\n} return nil\n\n}\n\nRemove() closes the segment and removes the index and store files.\n\nAnd put this Close() method below Remove():\n\nreport erratum • discuss",
      "content_length": 1525,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 52,
      "content": "Build a Log • 41\n\nWriteALogPackage/internal/log/segment.go func (s *segment) Close() error {\n\nif err := s.index.Close(); err != nil {\n\nreturn err\n\n} if err := s.store.Close(); err != nil {\n\nreturn err\n\n} return nil\n\n}\n\nFinally, add this last function at the end of the file:\n\nWriteALogPackage/internal/log/segment.go func nearestMultiple(j, k uint64) uint64 {\n\nif j >= 0 {\n\nreturn (j / k) * k\n\n} return ((j - k + 1) / k) * k\n\n}\n\nnearestMultiple(j uint64, k uint64) returns the nearest and lesser multiple of k in j, for example nearestMultiple(9, 4) == 8. We take the lesser multiple to make sure we stay under the user’s disk capacity.\n\nThat’s all the segment code, so now let’s test it. Create a segment_test.go file inside internal/log with the following test code:\n\nWriteALogPackage/internal/log/segment_test.go package log\n\nimport (\n\n\"io\" \"io/ioutil\" \"os\" \"testing\"\n\n\"github.com/stretchr/testify/require\" api \"github.com/travisjeffery/proglog/api/v1\"\n\n)\n\nfunc TestSegment(t *testing.T) {\n\ndir, _ := ioutil.TempDir(\"\", \"segment-test\") defer os.RemoveAll(dir)\n\nwant := &api.Record{Value: []byte(\"hello world\")}\n\nc := Config{} c.Segment.MaxStoreBytes = 1024 c.Segment.MaxIndexBytes = entWidth * 3\n\ns, err := newSegment(dir, 16, c) require.NoError(t, err)\n\nreport erratum • discuss",
      "content_length": 1282,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 53,
      "content": "Chapter 3. Write a Log Package • 42\n\nrequire.Equal(t, uint64(16), s.nextOffset, s.nextOffset) require.False(t, s.IsMaxed())\n\nfor i := uint64(0); i < 3; i++ {\n\noff, err := s.Append(want) require.NoError(t, err) require.Equal(t, 16+i, off)\n\ngot, err := s.Read(off) require.NoError(t, err) require.Equal(t, want.Value, got.Value)\n\n}\n\n_, err = s.Append(want) require.Equal(t, io.EOF, err)\n\n// maxed index require.True(t, s.IsMaxed())\n\nc.Segment.MaxStoreBytes = uint64(len(want.Value) * 3) c.Segment.MaxIndexBytes = 1024\n\ns, err = newSegment(dir, 16, c) require.NoError(t, err) // maxed store require.True(t, s.IsMaxed())\n\nerr = s.Remove() require.NoError(t, err) s, err = newSegment(dir, 16, c) require.NoError(t, err) require.False(t, s.IsMaxed())\n\n}\n\nWe test that we can append a record to a segment, read back the same record, and eventually hit the configured max size for both the store and index. Calling newSegment() twice with the same base offset and dir also checks that the function loads a segment’s state from the persisted index and log files.\n\nNow that we know that our segment works, we’re ready to create the log.\n\nCode the Log\n\nAll right, one last piece to go and that’s the log, which manages the list of seg- ments. Create a log.go file inside internal/log that starts with the following code:\n\nWriteALogPackage/internal/log/log.go package log\n\nimport (\n\n\"fmt\" \"io\" \"io/ioutil\" \"os\"\n\nreport erratum • discuss",
      "content_length": 1424,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 54,
      "content": "Build a Log • 43\n\n\"path\" \"sort\" \"strconv\" \"strings\" \"sync\"\n\napi \"github.com/travisjeffery/proglog/api/v1\"\n\n)\n\ntype Log struct {\n\nmu sync.RWMutex\n\nDir string Config Config\n\nactiveSegment *segment segments\n\n[]*segment\n\n}\n\nThe log consists of a list of segments and a pointer to the active segment to append writes to. The directory is where we store the segments.\n\nBelow the Log struct, write the following NewLog() function:\n\nWriteALogPackage/internal/log/log.go func NewLog(dir string, c Config) (*Log, error) { if c.Segment.MaxStoreBytes == 0 {\n\nc.Segment.MaxStoreBytes = 1024\n\n} if c.Segment.MaxIndexBytes == 0 {\n\nc.Segment.MaxIndexBytes = 1024\n\n} l := &Log{\n\nDir: Config: c,\n\ndir,\n\n}\n\nreturn l, l.setup()\n\n}\n\nIn NewLog(dir string, c Config), we first set defaults for the configs the caller didn’t specify, create a log instance, and set up that instance.\n\nNext, add this setup() method below NewLog():\n\nWriteALogPackage/internal/log/log.go func (l *Log) setup() error {\n\nfiles, err := ioutil.ReadDir(l.Dir) if err != nil {\n\nreturn err\n\n} var baseOffsets []uint64 for _, file := range files {\n\noffStr := strings.TrimSuffix(\n\nreport erratum • discuss",
      "content_length": 1152,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 55,
      "content": "Chapter 3. Write a Log Package • 44\n\nfile.Name(), path.Ext(file.Name()),\n\n) off, _ := strconv.ParseUint(offStr, 10, 0) baseOffsets = append(baseOffsets, off)\n\n} sort.Slice(baseOffsets, func(i, j int) bool {\n\nreturn baseOffsets[i] < baseOffsets[j]\n\n}) for i := 0; i < len(baseOffsets); i++ {\n\nif err = l.newSegment(baseOffsets[i]); err != nil {\n\nreturn err\n\n} // baseOffset contains dup for index and store so we skip // the dup i++\n\n} if l.segments == nil {\n\nif err = l.newSegment(\n\nl.Config.Segment.InitialOffset,\n\n); err != nil {\n\nreturn err\n\n}\n\n} return nil\n\n}\n\nWhen a log starts, it’s responsible for setting itself up for the segments that already exist on disk or, if the log is new and has no existing segments, for bootstrapping the initial segment. We fetch the list of the segments on disk, parse and sort the base offsets (because we want our slice of segments to be in order from oldest to newest), and then create the segments with the newSegment() helper method, which creates a segment for the base offset you pass in.\n\nNow add the following Append() function below setup():\n\nWriteALogPackage/internal/log/log.go func (l *Log) Append(record *api.Record) (uint64, error) {\n\nl.mu.Lock() defer l.mu.Unlock() off, err := l.activeSegment.Append(record) if err != nil {\n\nreturn 0, err\n\n} if l.activeSegment.IsMaxed() {\n\nerr = l.newSegment(off + 1)\n\n} return off, err\n\n}\n\nreport erratum • discuss",
      "content_length": 1404,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 56,
      "content": "Build a Log • 45\n\nAppend(*api.Record) appends a record to the log. We append the record to the active segment. Afterward, if the segment is at its max size (per the max size configs), then we make a new active segment. Note that we’re wrapping this func (and subsequent funcs) with a mutex to coordinate access to this section of the code. We use a RWMutex to grant access to reads when there isn’t a write holding the lock. If you felt so inclined, you could optimize this further and make the locks per segment rather than across the whole log. (I haven’t done that here because I want to keep this code simple.)\n\nBelow Append(), add this Read() method:\n\nWriteALogPackage/internal/log/log.go func (l *Log) Read(off uint64) (*api.Record, error) {\n\nl.mu.RLock() defer l.mu.RUnlock() var s *segment for _, segment := range l.segments {\n\nif segment.baseOffset <= off && off < segment.nextOffset {\n\ns = segment break\n\n}\n\n} if s == nil || s.nextOffset <= off {\n\nreturn nil, fmt.Errorf(\"offset out of range: %d\", off)\n\n} return s.Read(off)\n\n}\n\nRead(offset uint64) reads the record stored at the given offset. In Read(offset uint64), we first find the segment that contains the given record. Since the segments are in order from oldest to newest and the segment’s base offset is the smallest offset in the segment, we iterate over the segments until we find the first segment whose base offset is less than or equal to the offset we’re looking for. Once we know the segment that contains the record, we get the index entry from the segment’s index, and we read the data out of the segment’s store file and return the data to the caller.\n\nBelow Read(), add this snippet to define the Close(), Remove(), and Reset() methods:\n\nWriteALogPackage/internal/log/log.go func (l *Log) Close() error {\n\nl.mu.Lock() defer l.mu.Unlock() for _, segment := range l.segments {\n\nif err := segment.Close(); err != nil {\n\nreturn err\n\n}\n\n}\n\nreport erratum • discuss",
      "content_length": 1939,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 57,
      "content": "Chapter 3. Write a Log Package • 46\n\nreturn nil\n\n}\n\nfunc (l *Log) Remove() error {\n\nif err := l.Close(); err != nil {\n\nreturn err\n\n} return os.RemoveAll(l.Dir)\n\n}\n\nfunc (l *Log) Reset() error {\n\nif err := l.Remove(); err != nil {\n\nreturn err\n\n} return l.setup()\n\n}\n\nThis snippet implements a few related methods:\n\nClose() iterates over the segments and closes them. • Remove() closes the log and then removes its data. • Reset() removes the log and then creates a new log to replace it.\n\nAfter the previous snippet, add this snippet to implement the LowestOffset() and HighestOffset() methods:\n\nWriteALogPackage/internal/log/log.go func (l *Log) LowestOffset() (uint64, error) {\n\nl.mu.RLock() defer l.mu.RUnlock() return l.segments[0].baseOffset, nil\n\n}\n\nfunc (l *Log) HighestOffset() (uint64, error) {\n\nl.mu.RLock() defer l.mu.RUnlock() off := l.segments[len(l.segments)-1].nextOffset if off == 0 {\n\nreturn 0, nil\n\n} return off - 1, nil\n\n}\n\nThese methods tell us the offset range stored in the log. In Chapter 8, Coordi- nate Your Services with Consensus, on page 141, when we work on supporting a replicated, coordinated cluster, we’ll need this information to know what nodes have the oldest and newest data and what nodes are falling behind and need to replicate.\n\nBelow HighestOffset(), add this Truncate() method:\n\nreport erratum • discuss",
      "content_length": 1345,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 58,
      "content": "Build a Log • 47\n\nWriteALogPackage/internal/log/log.go func (l *Log) Truncate(lowest uint64) error {\n\nl.mu.Lock() defer l.mu.Unlock() var segments []*segment for _, s := range l.segments {\n\nif s.nextOffset <= lowest+1 {\n\nif err := s.Remove(); err != nil {\n\nreturn err\n\n} continue\n\n} segments = append(segments, s)\n\n} l.segments = segments return nil\n\n}\n\nTruncate(lowest uint64) removes all segments whose highest offset is lower than lowest. Because we don’t have disks with infinite space, we’ll periodically call Truncate() to remove old segments whose data we (hopefully) have processed by then and don’t need anymore.\n\nAfter Truncate(), add this snippet:\n\nWriteALogPackage/internal/log/log.go func (l *Log) Reader() io.Reader {\n\nl.mu.RLock() defer l.mu.RUnlock() readers := make([]io.Reader, len(l.segments)) for i, segment := range l.segments {\n\nreaders[i] = &originReader{segment.store, 0}\n\n} return io.MultiReader(readers...)\n\n}\n\ntype originReader struct {\n\nstore off int64\n\n}\n\nfunc (o *originReader) Read(p []byte) (int, error) {\n\nn, err := o.ReadAt(p, o.off) o.off += int64(n) return n, err\n\n}\n\nReader() returns an io.Reader to read the whole log. We’ll need this capability when we implement coordinate consensus and need to support snapshots and restoring a log. Reader() uses an io.MultiReader() call to concatenate the seg- ments’ stores. The segment stores are wrapped by the originReader type for two\n\nreport erratum • discuss",
      "content_length": 1441,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 59,
      "content": "Chapter 3. Write a Log Package • 48\n\nreasons. The first reason is to satisfy the io.Reader interface so we can pass it into the io.MultiReader() call. The second is to ensure that we begin reading from the origin of the store and read its entire file.\n\nWe’ve got one last method to add to our log, and that’s the function to create new segments. Copy the following newSegment() method below Read():\n\nWriteALogPackage/internal/log/log.go func (l *Log) newSegment(off uint64) error {\n\ns, err := newSegment(l.Dir, off, l.Config) if err != nil {\n\nreturn err\n\n} l.segments = append(l.segments, s) l.activeSegment = s return nil\n\n}\n\nnewSegment(off int64) creates a new segment, appends that segment to the log’s slice of segments, and makes the new segment the active segment so that subsequent append calls write to it.\n\nYou know the deal: it’s time to test our log. Create a log_test.go inside internal/log that starts with the following code:\n\nWriteALogPackage/internal/log/log_test.go package log\n\nimport (\n\n\"io/ioutil\" \"os\" \"testing\"\n\n\"github.com/stretchr/testify/require\" api \"github.com/travisjeffery/proglog/api/v1\" \"google.golang.org/protobuf/proto\"\n\n)\n\nfunc TestLog(t *testing.T) {\n\nfor scenario, fn := range map[string]func(\n\nt *testing.T, log *Log,\n\n){\n\n\"append and read a record succeeds\": testAppendRead, \"offset out of range error\": \"init with existing segments\": \"reader\": \"truncate\":\n\ntestOutOfRangeErr, testInitExisting, testReader, testTruncate,\n\n} {\n\nt.Run(scenario, func(t *testing.T) {\n\ndir, err := ioutil.TempDir(\"\", \"store-test\") require.NoError(t, err) defer os.RemoveAll(dir)\n\nreport erratum • discuss",
      "content_length": 1621,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 60,
      "content": "Build a Log • 49\n\nc := Config{} c.Segment.MaxStoreBytes = 32 log, err := NewLog(dir, c) require.NoError(t, err)\n\nfn(t, log)\n\n})\n\n}\n\n}\n\nTestLog(*testing.T) defines a table of tests to, well, test the log. I used a table to write the log tests so we don’t have to repeat the code that creates a new log for every test case.\n\nNow, let’s define the test cases. Put the following test cases below the TestLog() function:\n\nWriteALogPackage/internal/log/log_test.go func testAppendRead(t *testing.T, log *Log) {\n\nappend := &api.Record{\n\nValue: []byte(\"hello world\"),\n\n} off, err := log.Append(append) require.NoError(t, err) require.Equal(t, uint64(0), off)\n\nread, err := log.Read(off) require.NoError(t, err) require.Equal(t, append.Value, read.Value)\n\n}\n\ntestAppendRead(*testing.T, *log.Log) tests that we can successfully append to and read from the log. When we append a record to the log, the log returns the offset it associated that record with. So, when we ask the log for the record at that offset, we expect to get the same record that we appended.\n\nWriteALogPackage/internal/log/log_test.go func testOutOfRangeErr(t *testing.T, log *Log) {\n\nread, err := log.Read(1) require.Nil(t, read) require.Error(t, err)\n\n}\n\ntestOutOfRangeErr(*testing.T, *log.Log) tests that the log returns an error when we try to read an offset that’s outside of the range of offsets the log has stored.\n\nWriteALogPackage/internal/log/log_test.go func testInitExisting(t *testing.T, o *Log) {\n\nappend := &api.Record{\n\nValue: []byte(\"hello world\"),\n\n}\n\nreport erratum • discuss",
      "content_length": 1554,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 61,
      "content": "Chapter 3. Write a Log Package • 50\n\nfor i := 0; i < 3; i++ {\n\n_, err := o.Append(append) require.NoError(t, err)\n\n} require.NoError(t, o.Close())\n\noff, err := o.LowestOffset() require.NoError(t, err) require.Equal(t, uint64(0), off) off, err = o.HighestOffset() require.NoError(t, err) require.Equal(t, uint64(2), off)\n\nn, err := NewLog(o.Dir, o.Config) require.NoError(t, err)\n\noff, err = n.LowestOffset() require.NoError(t, err) require.Equal(t, uint64(0), off) off, err = n.HighestOffset() require.NoError(t, err) require.Equal(t, uint64(2), off)\n\n}\n\ntestInitExisting(*testing.T,*log.Log) tests that when we create a log, the log bootstraps itself from the data stored by prior log instances. We append three records to the original log before closing it. Then we create a new log configured with the same directory as the old log. Finally, we confirm that the new log set itself up from the data stored by the original log.\n\nWriteALogPackage/internal/log/log_test.go func testReader(t *testing.T, log *Log) {\n\nappend := &api.Record{\n\nValue: []byte(\"hello world\"),\n\n} off, err := log.Append(append) require.NoError(t, err) require.Equal(t, uint64(0), off)\n\nreader := log.Reader() b, err := ioutil.ReadAll(reader) require.NoError(t, err)\n\nread := &api.Record{} err = proto.Unmarshal(b[lenWidth:], read) require.NoError(t, err) require.Equal(t, append.Value, read.Value)\n\n}\n\ntestReader(*testing.T, *log.Log) tests that we can read the full, raw log as it’s stored on disk so that we can snapshot and restore the logs in Finite-State Machine, on page 151.\n\nreport erratum • discuss",
      "content_length": 1582,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 62,
      "content": "What You Learned • 51\n\nWriteALogPackage/internal/log/log_test.go func testTruncate(t *testing.T, log *Log) {\n\nappend := &api.Record{\n\nValue: []byte(\"hello world\"),\n\n} for i := 0; i < 3; i++ {\n\n_, err := log.Append(append) require.NoError(t, err)\n\n}\n\nerr := log.Truncate(1) require.NoError(t, err)\n\n_, err = log.Read(0) require.Error(t, err)\n\n}\n\ntestTruncate(*testing.T, *log.Log) tests that we can truncate the log and remove old segments that we don’t need any more.\n\nThat wraps up our log code! We just wrote a log that’s not that watered down from the log that drives Kafka, and we didn’t even have to work too hard.\n\nWhat You Learned\n\nYou now know what logs are, why they’re important, and how they’re used in various applications including distributed services. And then you learned how to build one! This log serves as the foundation of our distributed log. Now we can build a service on our library and make the library’s functionality accessible to people on other computers.\n\nreport erratum • discuss",
      "content_length": 1009,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 63,
      "content": "Part II\n\nNetwork",
      "content_length": 16,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 64,
      "content": "CHAPTER 4\n\nServe Requests with gRPC\n\nWe’ve set up our project and protocol buffers and written our log library. Currently, our library can only be used on a single computer by a single person at a time. Plus that person has to learn our library’s API, run our code, and store the log on their disk—none of which most people will do, which limits our user base. We can solve these problems and appeal to a larger audience by turning our library into a web service. Compared to a program that runs on a single computer, networked services provide three major advantages:\n\nYou can run them across multiple computers for availability and scalability. • They allow multiple people to interact with the same data. • They provide accessible interfaces that are easy for people to use.\n\nSome situations where you’ll want to write services to reap these advantages include providing a public API for your front end to hit, building internal infrastructure tools, and making a service to build your own business on (people rarely pay to use libraries).\n\nIn this chapter, we’ll build on our library and make a service that allows multiple people to interact with the same data and runs across multiple computers. We won’t add support for clusters right now; we’ll do that in Chapter 8, Coordinate Your Services with Consensus, on page 141. The best tool I’ve found for serving requests across distributed services is Google’s gRPC.\n\nWhat Is gRPC?\n\nWhen I was building distributed services in the past, the two common prob- lems that drove me batty were maintaining compatibility and maintaining performance between clients and the server.\n\nreport erratum • discuss",
      "content_length": 1653,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 65,
      "content": "Chapter 4. Serve Requests with gRPC • 56\n\nI wanted to ensure that clients and the server were always compatible—that the client was sending requests that the server understood, and vice versa with the server’s responses. When I made breaking changes to the server, I wanted to ensure that old clients continued to work, and I accomplished this by versioning my API.\n\nFor maintaining good performance, your main priorities are optimizing your database queries and optimizing the algorithms you’ve used to implement your business logic. Once you’ve optimized those though, performance will often come down to how fast your service unmarshals requests and marshals responses, and down to reducing the overhead each time clients and the server communicate—like using a single, long-lasting connection rather than a new connection for each request.\n\nSo I was happy when Google released gRPC, an open source, high-performance RPC (remote procedure call) framework. gRPC has been a great help in solving these problems when building distributed systems, and I think you’ll find that it simplifies your work. How does gRPC help you build services?\n\nGoals When Building a Service\n\nHere are the most important goals to aim for when you’re building a networked service—and some info about how gRPC helps you achieve them:\n\nSimplicity\n\nNetworked communication is technical and complex. When building our service, we want to focus on the problem it solves rather than the technical minutiae of request-response serialization, and so on. You want to work with APIs that abstract these details away. However, when you need to work at lower levels of abstraction, then you need those levels to be accessible.\n\nOn the spectrum of low- to high-level frameworks, in terms of the abstrac- tions you’re working with, gRPC is mid-to-high level. It’s above a framework like Express since gRPC decides how to serialize and structure your end- points and provides features like bidirectional streaming, but below a framework like Rails since Rails handles everything from handling requests to storing your data and structuring your application. gRPC is extendable via middleware, and its active community1 has written middleware2 to\n\n1. 2.\n\nhttps://github.com/grpc-ecosystem\n\nhttps://github.com/grpc-ecosystem/go-grpc-middleware\n\nreport erratum • discuss",
      "content_length": 2329,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 66,
      "content": "Goals When Building a Service • 57\n\nsolve a lot of the problems you’ll face when building services—for example, logging, authentication, rate limiting, and tracing.\n\nMaintainability\n\nWriting the first version of a service is a brief period of the total time you’ll spend working on the service. Once your service is live and people depend on it, you must maintain backward compatibility. With request-response type APIs, the simplest way to maintain backward compatibility is to version and run multiple instances of your API.\n\nWith gRPC, you can easily write and run separate versions of your services when you have major API changes, while still taking advantage of proto- buf’s field versioning for small changes. Having all your requests and responses type checked helps prevent accidentally introducing back- ward-incompatible changes as you and your peers build your service.\n\nSecurity\n\nWhen you expose a service on a network, you expose the service to who- ever is on that network—potentially the whole Internet. It’s important that you control who has access to your service and what they can do.\n\ngRPC supports Secure Sockets Layer/Transport Layer Security (SSL/TLS) to encrypt all data exchanged between the client and server and lets you attach credentials to requests so you know which user is making each request. We’ll discuss security in the next chapter.\n\nEase of use\n\nThe whole point of writing a service is to have people use it and solve some problem of theirs. The easier your service is to use, the more popular it will be. You go a long way toward making your service easy to use by telling your users when they’re doing something wrong, such as calling your API with a bad request.\n\nWith gRPC, everything from your service methods to your requests and responses and their bodies are all defined in types. The compiler copies the comments from your protobuf to your code to help users when the type defi- nitions aren’t good enough. Your users will know whether they’re using the API correctly thanks to their code being type checked. Having every- thing—requests, responses, models, and serialization—type checked is a big help to people learning how to use your service. gRPC also lets users look up the API’s details with godoc. Many frameworks don’t offer either of these handy features.\n\nreport erratum • discuss",
      "content_length": 2339,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 67,
      "content": "Chapter 4. Serve Requests with gRPC • 58\n\nPerformance\n\nYou want your service to be as fast as possible while using as few resources as possible. For example, if you can run your application on an n1-standard-1 (~$35 per month) instance on Google Cloud Platform rather than on an n1-standard-2 (~$71 per month) instance, that cuts your costs in half.\n\ngRPC is built on solid foundations with protobuf and HTTP/2 because protobuf performs very well at serialization and HTTP/2 provides a means for long- lasting connections, which gRPC takes advantage of. So your service runs efficiently and doesn’t cause unnecessarily high server bills.\n\nScalability\n\nScalability can refer to scaling up with load balancing to balance the load across multiple computers and to scaling up the number of people developing a project. gRPC helps make both types of scaling easier.\n\nYou can use different kinds of load balancing with gRPC3 based on your needs, including thick client-side load balancing, proxy load balancing, look-aside load balancing, or service mesh.\n\nFor scaling up the number of people working on your project, gRPC lets you compile your service into clients and servers in the languages that gRPC supports. This allows people to use their own languages to build services that communicate with each other.\n\nWe now know what we want out of building our service, so let’s create a gRPC service that fulfills our goals.\n\nDefine a gRPC Service\n\nA gRPC service is essentially a group of related RPC endpoints—exactly how they’re related is up to you. A common example is a RESTful grouping where the relation is that the endpoints operate on the same resource, but the grouping could be looser than that. In general, it’s just a group of endpoints needed to solve some problem. In our case, the goal is to enable people to write to and read from their log.\n\nCreating a gRPC service involves defining it in protobuf and then compiling your protocol buffers into code comprising the client and server stubs that you then implement. To get started, open log.proto, the file where we defined our Record message, and add the following service definition above those messages:\n\n3.\n\nhttps://grpc.io/blog/grpc-load-balancing\n\nreport erratum • discuss",
      "content_length": 2238,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 68,
      "content": "Define a gRPC Service • 59\n\nServeRequestsWithgRPC/api/v1/log.proto service Log {\n\nrpc Produce(ProduceRequest) returns (ProduceResponse) {} rpc Consume(ConsumeRequest) returns (ConsumeResponse) {} rpc ConsumeStream(ConsumeRequest) returns (stream ConsumeResponse) {} rpc ProduceStream(stream ProduceRequest) returns (stream ProduceResponse) {}\n\n}\n\nThe service keyword says that this is a service for the compiler to generate, and each RPC line is an endpoint in that service, specifying the type of request and response the endpoint accepts. The requests and responses are messages that the compiler turns into Go structs, like the ones we saw in the previous chapter.\n\nWe have two streaming endpoints:\n\nConsumeStream—a server-side streaming RPC where the client sends a request to the server and gets back a stream to read a sequence of messages.\n\nProduceStream—a bidirectional streaming RPC where both the client and server send a sequence of messages using a read-write stream. The two streams operate independently, so the clients and servers can read and write in whatever order they like. For example, the server could wait to receive all of the client’s requests before sending back its response. You’d order your calls this way if your server needed to process the requests in batches or aggregate a response over multiple requests. Alternatively, the server could send back a response for each request in lockstep. You’d order your calls this way if each request required its own corresponding response.\n\nBelow your service definition, add the following code to define our requests and responses:\n\nServeRequestsWithgRPC/api/v1/log.proto message ProduceRequest {\n\nRecord record = 1;\n\n}\n\nmessage ProduceResponse {\n\nuint64 offset = 1;\n\n}\n\nmessage ConsumeRequest {\n\nuint64 offset = 1;\n\n}\n\nmessage ConsumeResponse {\n\nRecord record = 2;\n\n}\n\nreport erratum • discuss",
      "content_length": 1867,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 69,
      "content": "Chapter 4. Serve Requests with gRPC • 60\n\nThe request includes the record to produce to the log, and the response sends back the record’s offset, which is essentially the record’s identifier. Similarly with consuming: the user specifies the offset of the logs they want to consume, and the server responds back with the specified record.\n\nTo generate the client- and server-side code with our Log service definition, we need to tell the protobuf compiler to use the gRPC plugin.\n\nCompile with the gRPC Plugin\n\nThis task takes just a second. Install the gRPC package by running this command:\n\n$ go get google.golang.org/grpc@v1.32.0 $ go get google.golang.org/grpc/cmd/protoc-gen-go-grpc@v1.0.0\n\nThen open up your Makefile and update your compile target to match the fol- lowing to enable the gRPC plugin and compile our gRPC service:\n\nServeRequestsWithgRPC/Makefile compile:\n\nprotoc api/v1/*.proto \\\n\n--go_out=. \\ --go-grpc_out=. \\ --go_opt=paths=source_relative \\ --go-grpc_opt=paths=source_relative \\ --proto_path=.\n\nRun $ make compile, and then open up the log_grpc.pb.go file in the api/v1 directory and check out the generated code. You’ll see a working gRPC log client, and the compiler left the log service API for us to implement.\n\nImplement a gRPC Server\n\nBecause the compiler generated a server stub, the job left for us is to write it. To implement a server, you need to build a struct whose methods match the service definition in your protobuf.\n\nCreate an internal/server directory tree in the root of your project by running mkdir -p internal/server. Internal packages are magical packages in Go that can only be imported by nearby code. For example, you can import code in /a/b/c/internal/d/e/f by code rooted by /a/b/c, but not code rooted by /a/b/g. In this directory, we’ll implement our server in a file called server.go and a package named server. The first order of business is to define our server type and a factory function to create an instance of the server.\n\nHere’s the code we need to add to our server.go file:\n\nreport erratum • discuss",
      "content_length": 2065,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 70,
      "content": "Implement a gRPC Server • 61\n\nServeRequestsWithgRPC/internal/server/server.go package server\n\nimport (\n\n\"context\"\n\napi \"github.com/travisjeffery/proglog/api/v1\" \"google.golang.org/grpc\"\n\n)\n\ntype Config struct {\n\nCommitLog CommitLog\n\n}\n\nvar _ api.LogServer = (*grpcServer)(nil)\n\ntype grpcServer struct {\n\napi.UnimplementedLogServer *Config\n\n}\n\nfunc newgrpcServer(config *Config) (srv *grpcServer, err error) {\n\nsrv = &grpcServer{\n\nConfig: config,\n\n} return srv, nil\n\n}\n\nTo implement the API you saw in log_grpc.pb.go, we need to implement the Con- sume() and Produce() handlers. Our gRPC layer is thin because it defers to our log library, so to implement these methods, you call down to the library and handle any errors. Add the following code below your newgrpcServer function:\n\nServeRequestsWithgRPC/internal/server/server.go func (s *grpcServer) Produce(ctx context.Context, req *api.ProduceRequest) (\n\napi.ProduceResponse, error) { offset, err := s.CommitLog.Append(req.Record) if err != nil { return nil, err\n\n} return &api.ProduceResponse{Offset: offset}, nil\n\n}\n\nfunc (s *grpcServer) Consume(ctx context.Context, req *api.ConsumeRequest) (\n\napi.ConsumeResponse, error) { record, err := s.CommitLog.Read(req.Offset) if err != nil { return nil, err\n\n} return &api.ConsumeResponse{Record: record}, nil\n\n}\n\nreport erratum • discuss",
      "content_length": 1335,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 71,
      "content": "Chapter 4. Serve Requests with gRPC • 62\n\nWith this snippet, we’ve implemented the Produce(context.Context,*api.ProduceRequest) and Consume(context.Context, *api.ConsumeRequest) methods on our server. These methods handle the requests made by clients to produce and consume to the server’s log. Now let’s add the streaming APIs. Put the following code below the previous snippet:\n\nServeRequestsWithgRPC/internal/server/server.go func (s *grpcServer) ProduceStream(\n\nstream api.Log_ProduceStreamServer,\n\n) error {\n\nfor {\n\nreq, err := stream.Recv() if err != nil {\n\nreturn err\n\n} res, err := s.Produce(stream.Context(), req) if err != nil {\n\nreturn err\n\n} if err = stream.Send(res); err != nil {\n\nreturn err\n\n}\n\n}\n\n}\n\nfunc (s *grpcServer) ConsumeStream(\n\nreq *api.ConsumeRequest, stream api.Log_ConsumeStreamServer,\n\n) error {\n\nfor {\n\nselect { case <-stream.Context().Done():\n\nreturn nil\n\ndefault:\n\nres, err := s.Consume(stream.Context(), req) switch err.(type) { case nil: case api.ErrOffsetOutOfRange:\n\ncontinue\n\ndefault:\n\nreturn err\n\n} if err = stream.Send(res); err != nil {\n\nreturn err\n\n} req.Offset++\n\n}\n\n}\n\n}\n\nreport erratum • discuss",
      "content_length": 1139,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 72,
      "content": "Implement a gRPC Server • 63\n\nProduceStream(api.Log_ProduceStreamServer) implements a bidirectional streaming RPC so the client can stream data into the server’s log and the server can tell the client whether each request succeeded. ConsumeStream(*api.ConsumeRequest, api.Log_ConsumeStreamServer) implements a server-side streaming RPC so the client can tell the server where in the log to read records, and then the server will stream every record that follows—even records that aren’t in the log yet! When the server reaches the end of the log, the server will wait until someone appends a record to the log and then continue streaming records to the client.\n\nThe code that makes up our gRPC service is short and simple, which is a sign that we have a clean separation between our networking code and log code. However, one reason our service’s code is so short is because we have the most basic error handling ever: we just send the client whatever error our library returned.\n\nIf a client tried to consume a message but the request failed, the developer would want to know why. Could the server not find the message? Did the server fail unexpectedly? The server communicates this info with a status code. Also, end users need to know when the application fails, so the server should send back a human-readable version of the error for the client to show to the user.\n\nLet’s explore how to improve our service’s error handling, shall we?\n\nError Handling in gRPC\n\nYet another nice feature of gRPC is how it handles errors. In the previous code, we return errors just like you’d see in code from the Go standard library. Even though this code is handling calls between people on different computers, you wouldn’t know it—thanks to gRPC, which abstracts away the networking details. By default your errors will only have a string description, but you may want to include more information such as a status code or some other arbitrary data.\n\nGo’s gRPC implementation has an awesome status package4 that you can use to build errors with status codes or whatever other data you want to include in your errors. To create an error with a status code, you create the error with the Error function from the status package and pass the relevant code from the codes package5 that matches the type of error you have. Any status code you attach on the error here must be a code defined in the codes package—they’re meant to be consistent across the languages gRPC supports.\n\n4. 5.\n\nhttps://godoc.org/google.golang.org/grpc/status\n\nhttps://godoc.org/google.golang.org/grpc/codes\n\nreport erratum • discuss",
      "content_length": 2592,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 73,
      "content": "Chapter 4. Serve Requests with gRPC • 64\n\nFor example, if you couldn’t find a record for some ID, then you’d use the NotFound code like this:\n\nerr := status.Error(codes.NotFound, \"id was not found\") return nil, err\n\nOn the client side, you’d parse out the code from the error with the FromError function from the status package. Your goal is to have as few non-status errors as possible so you know why the errors happen and can handle them gracefully. The non-status errors that are OK are unforeseen, internal server errors. Here’s how to use the FromError function to parse out a status from a gRPC error:\n\nst, ok := status.FromError(err) if !ok {\n\n// Error was not a status error\n\n} // Use st.Message() and st.Code()\n\nWhen you want more than a status code (say you’re trying to debug an error and want more details like logs or traces), then you can use the status pack- age’s WithDetails function, which allows you to attach any protobuf message you want to the error.\n\nThe errdetailspackage6 provides some protobufs you’ll likely find useful when building your service, including messages to use to handle bad requests, debug info, and localized messages.\n\nLet’s use the LocalizedMessage from the errdetails package to change the previous example to respond with an error message that’s safe to return to the user. In the following code, we first create a new not-found status, then we create the localized message specifying the message and locale used. Next we attach the details to the status, and then finally convert and return the status as a Go error:\n\nst := status.New(codes.NotFound, \"id was not found\") d := &errdetails.LocalizedMessage{\n\nLocale: \"en-US\", Message: fmt.Sprintf(\n\n\"We couldn't find a user with the email address: %s\", id,\n\n),\n\n} var err error st, err = st.WithDetails(d)\n\n6.\n\nhttps://godoc.org/google.golang.org/genproto/googleapis/rpc/errdetails\n\nreport erratum • discuss",
      "content_length": 1903,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 74,
      "content": "Implement a gRPC Server • 65\n\nif err != nil {\n\n// If this errored, it will always error // here, so better panic so we can figure // out why than have this silently passing. panic(fmt.Sprintf(\"Unexpected error attaching metadata: %v\", err))\n\n} return st.Err()\n\nTo extract these details on the client side, you need to convert the error back into a status, pull out the details via its Details method, and then convert the type of the details to match the type of the protobuf you set on the server, which in our case is *errdetails.LocalizedMessage.\n\nThe code to do that looks like this:\n\nst := status.Convert(err) for _, detail := range st.Details() {\n\nswitch t := detail.(type) { case *errdetails.LocalizedMessage:\n\n// send t.Message back to the user\n\n}\n\n}\n\nFocusing back on our service, let’s add a custom error named ErrOffsetOutOfRange that the server will send back to the client when the client tries to consume an offset that’s outside of the log. Create an error.go file inside the api/v1 direc- tory with the following code:\n\nServeRequestsWithgRPC/api/v1/error.go package log_v1\n\nimport (\n\n\"fmt\"\n\n\"google.golang.org/genproto/googleapis/rpc/errdetails\" \"google.golang.org/grpc/status\"\n\n)\n\ntype ErrOffsetOutOfRange struct {\n\nOffset uint64\n\n}\n\nfunc (e ErrOffsetOutOfRange) GRPCStatus() *status.Status {\n\nst := status.New(\n\n404, fmt.Sprintf(\"offset out of range: %d\", e.Offset),\n\n) msg := fmt.Sprintf(\n\n\"The requested offset is outside the log's range: %d\", e.Offset,\n\n)\n\nreport erratum • discuss",
      "content_length": 1502,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 75,
      "content": "Chapter 4. Serve Requests with gRPC • 66\n\nd := &errdetails.LocalizedMessage{\n\nLocale: \"en-US\", Message: msg,\n\n} std, err := st.WithDetails(d) if err != nil {\n\nreturn st\n\n} return std\n\n}\n\nfunc (e ErrOffsetOutOfRange) Error() string {\n\nreturn e.GRPCStatus().Err().Error()\n\n}\n\nNext, let’s update your log to use this error. Find this section of the Read(offset uint64) method of your log in internal/log/log.go:\n\nWriteALogPackage/internal/log/log.go if s == nil || s.nextOffset <= off {\n\nreturn nil, fmt.Errorf(\"offset out of range: %d\", off)\n\n}\n\nAnd then change that section to this:\n\nServeRequestsWithgRPC/internal/log/log.go if s == nil || s.nextOffset <= off {\n\nreturn nil, api.ErrOffsetOutOfRange{Offset: off}\n\n}\n\nFinally, we need to update the associated testOutOfRange(*testing.T, *log.Log) test in internal/log/log_test.go to the following code:\n\nServeRequestsWithgRPC/internal/log/log_test.go func testOutOfRangeErr(t *testing.T, log *Log) {\n\nread, err := log.Read(1) require.Nil(t, read) apiErr := err.(api.ErrOffsetOutOfRange) require.Equal(t, uint64(1), apiErr.Offset)\n\n}\n\nWith our custom error, when the client tries to consume an offset that’s out- side of the log, the log returns an error with plenty of useful information: a localized message, a status code, and an error message. Because our error is a struct type, we can type-switch the error returned by the Read(offset uint64) method to know what happened. We already use this feature in our Con- sumeStream(*api.ConsumeRequest,api.Log_ConsumeStreamServer) method to know whether the server has read to the end of the log and just needs to wait until someone produces another record to the client:\n\nreport erratum • discuss",
      "content_length": 1692,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 76,
      "content": "Implement a gRPC Server • 67\n\nServeRequestsWithgRPC/internal/server/server.go func (s *grpcServer) ConsumeStream(\n\nreq *api.ConsumeRequest, stream api.Log_ConsumeStreamServer,\n\n) error {\n\nfor {\n\nselect { case <-stream.Context().Done():\n\nreturn nil\n\ndefault:\n\nres, err := s.Consume(stream.Context(), req) switch err.(type) { case nil: case api.ErrOffsetOutOfRange:\n\ncontinue\n\ndefault:\n\nreturn err\n\n} if err = stream.Send(res); err != nil {\n\nreturn err\n\n} req.Offset++\n\n}\n\n}\n\n}\n\nWe’ve improved our service’s error handling to include status codes and a human-readable, localized error message to help our users know why a failure occurred. Next, let’s define the log field that’s on our service such that we can pass in different log implementations and make the service easier to write tests against.\n\nDependency Inversion with Interfaces\n\nOur server depends on a log abstraction. For example, when running in a production environment—where we need our service to persist our user’s data—the service will depend on our library. But when running in a test environment, where we don’t need to persist our test data, we could use a naive, in-memory log. An in-memory log would also be good for testing because it would make the tests run faster.\n\nAs you can see from these examples, it would be best if our service weren’t tied to a specific log implementation. Instead, we want to pass in a log implementation based on our needs at the time. We can make this possible by having our service depend on a log interface rather than on a concrete type. That way, the service can use any log implementation that satisfies the log interface.\n\nreport erratum • discuss",
      "content_length": 1657,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 77,
      "content": "Chapter 4. Serve Requests with gRPC • 68\n\nAdd this code below your grpcServer methods in server.go:\n\nServeRequestsWithgRPC/internal/server/server.go type CommitLog interface {\n\nAppend(*api.Record) (uint64, error) Read(uint64) (*api.Record, error)\n\n}\n\nThat’s all we need to do to allow our service to use any given log implementa- tion that satisfies our CommitLog interface. Easy, huh?\n\nNow, let’s write an exported API that enables our users to instantiate a new service.\n\nRegister Your Server\n\nWe implemented the server writing nothing gRPC-specific yet. There are just three steps left to get our service working with gRPC, and happily we only need to perform two of them: creating a gRPC server and registering our service with it. The final step is giving the server a listener to accept incoming con- nections from, but we’ll require our users to pass their own listener implemen- tation, as they might like to when testing. Once these three steps are complete, the gRPC server will listen on the network, handle requests, call our server, and respond to the client with the result.\n\nAbove your grpcServer struct in server.go, add the following NewGRPCServer() function to provide your users a way to instantiate your service, create a gRPC server, and register your service to that server (this will give the user a server that just needs a listener for it to accept incoming connections):\n\nServeRequestsWithgRPC/internal/server/server.go func NewGRPCServer(config *Config) (*grpc.Server, error) {\n\ngsrv := grpc.NewServer() srv, err := newgrpcServer(config) if err != nil {\n\nreturn nil, err\n\n} api.RegisterLogServer(gsrv, srv) return gsrv, nil\n\n}\n\nWe’re now done writing our service. Let’s create some tests to verify that it works.\n\nTest a gRPC Server and Client\n\nNow that we’ve finished our gRPC server, we need some tests to check that our client and server work like we expect. We’ve already tested the details of our log’s library implementation in the library, so the tests we’re writing here are at a higher level and focus on ensuring that everything’s hooked up\n\nreport erratum • discuss",
      "content_length": 2103,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 78,
      "content": "Test a gRPC Server and Client • 69\n\nproperly between the gRPC and library bits and that our gRPC client and server can communicate.\n\nIn the grpc directory, create a server_test.go file, and add the following code that will set up your test:\n\nServeRequestsWithgRPC/internal/server/server_test.go package server\n\nimport (\n\n\"context\" \"io/ioutil\" \"net\" \"testing\"\n\n\"github.com/stretchr/testify/require\" api \"github.com/travisjeffery/proglog/api/v1\" \"github.com/travisjeffery/proglog/internal/log\" \"google.golang.org/grpc\"\n\n)\n\nfunc TestServer(t *testing.T) {\n\nfor scenario, fn := range map[string]func(\n\nt *testing.T, client api.LogClient, config *Config,\n\n){\n\n\"produce/consume a message to/from the log succeeeds\":\n\ntestProduceConsume, \"produce/consume stream succeeds\":\n\ntestProduceConsumeStream,\n\n\"consume past log boundary fails\":\n\ntestConsumePastBoundary,\n\n} {\n\nt.Run(scenario, func(t *testing.T) {\n\nclient, config, teardown := setupTest(t, nil) defer teardown() fn(t, client, config)\n\n})\n\n}\n\n}\n\nTestServer(*testing.T) defines our list of test cases and then runs a subtest for each case. Add the following setupTest(*testing.T, func(*Config)) function below Test- Server():\n\nServeRequestsWithgRPC/internal/server/server_test.go func setupTest(t *testing.T, fn func(*Config)) (\n\nclient api.LogClient, cfg *Config, teardown func(),\n\n) {\n\nreport erratum • discuss",
      "content_length": 1360,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 79,
      "content": "Chapter 4. Serve Requests with gRPC • 70\n\nt.Helper()\n\nl, err := net.Listen(\"tcp\", \":0\") require.NoError(t, err)\n\nclientOptions := []grpc.DialOption{grpc.WithInsecure()} cc, err := grpc.Dial(l.Addr().String(), clientOptions...) require.NoError(t, err)\n\ndir, err := ioutil.TempDir(\"\", \"server-test\") require.NoError(t, err)\n\nclog, err := log.NewLog(dir, log.Config{}) require.NoError(t, err)\n\ncfg = &Config{\n\nCommitLog: clog,\n\n} if fn != nil {\n\nfn(cfg)\n\n} server, err := NewGRPCServer(cfg) require.NoError(t, err)\n\ngo func() {\n\nserver.Serve(l)\n\n}()\n\nclient = api.NewLogClient(cc)\n\nreturn client, cfg, func() { server.Stop() cc.Close() l.Close() clog.Remove()\n\n}\n\n}\n\nsetupTest(*testing.T, func(*Config)) is a helper function to set up each test case. Our test setup begins by creating a listener on the local network address that our server will run on. The 0 port is useful for when we don’t care what port we use since 0 will automatically assign us a free port. We then make an insecure connection to our listener and, with it, a client we’ll use to hit our server with. Next we create our server and start serving requests in a goroutine because the Serve method is a blocking call, and if we didn’t run it in a goroutine our tests further down would never run.\n\nNow we’re ready to write some test cases. Add the following code below setupTest():\n\nServeRequestsWithgRPC/internal/server/server_test.go func testProduceConsume(t *testing.T, client api.LogClient, config *Config) {\n\nctx := context.Background()\n\nreport erratum • discuss",
      "content_length": 1534,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 80,
      "content": "Test a gRPC Server and Client • 71\n\nwant := &api.Record{\n\nValue: []byte(\"hello world\"),\n\n}\n\nproduce, err := client.Produce(\n\nctx, &api.ProduceRequest{\n\nRecord: want,\n\n},\n\n) require.NoError(t, err)\n\nconsume, err := client.Consume(ctx, &api.ConsumeRequest{\n\nOffset: produce.Offset,\n\n}) require.NoError(t, err) require.Equal(t, want.Value, consume.Record.Value) require.Equal(t, want.Offset, consume.Record.Offset)\n\n}\n\ntestProduceConsume(*testing.T,api.LogClient,*Config) tests that producing and consuming works by using our client and server to produce a record to the log, consume it back, and then check that the record we sent is the same one we got back.\n\nAdd the following test case below testProduceConsume():\n\nServeRequestsWithgRPC/internal/server/server_test.go func testConsumePastBoundary( t *testing.T, client api.LogClient, config *Config,\n\n) {\n\nctx := context.Background()\n\nproduce, err := client.Produce(ctx, &api.ProduceRequest{\n\nRecord: &api.Record{\n\nValue: []byte(\"hello world\"),\n\n},\n\n}) require.NoError(t, err)\n\nconsume, err := client.Consume(ctx, &api.ConsumeRequest{\n\nOffset: produce.Offset + 1,\n\n}) if consume != nil {\n\nt.Fatal(\"consume not nil\")\n\n} got := grpc.Code(err) want := grpc.Code(api.ErrOffsetOutOfRange{}.GRPCStatus().Err()) if got != want {\n\nt.Fatalf(\"got err: %v, want: %v\", got, want)\n\n}\n\n}\n\nreport erratum • discuss",
      "content_length": 1350,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 81,
      "content": "Chapter 4. Serve Requests with gRPC • 72\n\ntestConsumePastBoundary(*testing.T,api.LogClient, *Config) tests that our server responds with an api.ErrOffsetOutOfRange() error when a client tries to consume beyond the log’s boundaries.\n\nWe have one more test case. Put the following snippet at the bottom of the file:\n\nServeRequestsWithgRPC/internal/server/server_test.go func testProduceConsumeStream( t *testing.T, client api.LogClient, config *Config,\n\n) {\n\nctx := context.Background()\n\nrecords := []*api.Record{{\n\nValue: []byte(\"first message\"), Offset: 0,\n\n}, {\n\nValue: []byte(\"second message\"), Offset: 1,\n\n}}\n\n{\n\nstream, err := client.ProduceStream(ctx) require.NoError(t, err)\n\nfor offset, record := range records {\n\nerr = stream.Send(&api.ProduceRequest{\n\nRecord: record,\n\n}) require.NoError(t, err) res, err := stream.Recv() require.NoError(t, err) if res.Offset != uint64(offset) {\n\nt.Fatalf(\n\n\"got offset: %d, want: %d\", res.Offset, offset,\n\n)\n\n}\n\n}\n\n}\n\n{\n\nstream, err := client.ConsumeStream(\n\nctx, &api.ConsumeRequest{Offset: 0},\n\n) require.NoError(t, err)\n\nfor i, record := range records {\n\nres, err := stream.Recv()\n\nreport erratum • discuss",
      "content_length": 1153,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 82,
      "content": "What You Learned • 73\n\nrequire.NoError(t, err) require.Equal(t, res.Record, &api.Record{\n\nValue: record.Value, Offset: uint64(i),\n\n})\n\n}\n\n}\n\n}\n\ntestProduceConsumeStream(*testing.T, api.LogClient, *Config) is the streaming counterpart to testProduceConsume(), testing that we can produce and consume through streams.\n\nRun $ make test to test your code. In the test output, you’ll see your TestServer test passing.\n\nWahoo! You’ve written and tested your first gRPC service.\n\nWhat You Learned\n\nYou now know how to define a gRPC service in protobuf, compile your gRPC protobufs into code, build a gRPC server, and test that everything works end- to-end across your client and server. You can build a gRPC server and client, and you can use your log over the network.\n\nNext we’ll improve the security of our service by encrypting the data sent between the client and server with SSL/TLS, and authenticating requests so we can know who’s making each request and whether they’re allowed to.\n\nreport erratum • discuss",
      "content_length": 1009,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 83,
      "content": "CHAPTER 5\n\nSecure Your Services\n\nWhen you build a project, your goal is to solve a problem. You may get so focused on this goal that you ignore the other factors you should consider, like security. Security is one of those things that’s super important but easy to ignore.\n\nYes, creating a secure solution is more complicated than building a solution without considering security. But if you want to build something that people will actually use, it has to be secure. And it’s far easier to incorporate security from the start than it is to retrofit security into a finished project. So you need to keep security in mind from the very beginning. In this book, for example, we don’t just want to build a tool to stream data—we want to build a tool that streams data securely.\n\nWhen you start your career as a software engineer, security can seem like a thankless job. If you do it right, no one will know you did it at all, and building it can be scary and even boring at times. Over the years, from building several software-as-a-service startups, I’ve changed my tune—I now consider securing my services as important as the problems they solve. Here’s why:\n\nSecurity saves you from being hacked. When you don’t follow security best practices, breaches and leaks follow with amazing regularity and severity, as we’ve seen in the news. Whenever I’m building a service, I think about what it’d be like if the data I’m trying to protect was publicly posted all over the planet. Picturing this gives me the motivation to make sure that sort of thing doesn’t happen to me, and thankfully it hasn’t yet (knock on wood).\n\nSecurity wins deals. In my experience, the most important factor in whether a potential customer bought software I worked on came down to whether the software fulfilled some security requirement.\n\nreport erratum • discuss",
      "content_length": 1836,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 84,
      "content": "Chapter 5. Secure Your Services • 76\n\nSecurity is painful to tack on. Taking an insecure service that lacks the basic security features most people need and then trying to tack those features on is a painful, tricky process. In contrast, it’s relatively easy to build those features from the start.\n\nThose high stakes get me fired up about building secure services. So let’s get to it.\n\nSecure Services in Three Steps\n\nSecurity in distributed services can be broken down into three steps:\n\n1. Encrypt data in-flight to protect against man-in-the-middle attacks; 2. Authenticate to identify clients; and 3. Authorize to determine the permissions of the identified clients. Let’s talk about these phases in more detail, explore the security benefits they provide, and write the code to build them into our service.\n\nEncrypt In-Flight Data\n\nEncryption of data in-flight prevents man-in-the-middle attacks (MITM).1 An example of a MITM attack is active eavesdropping, where the attacker makes independent connections with the victims to make them think they’re talking directly with each other when in fact the conversation is controlled by the attacker. This is bad because not only can the attacker learn confidential information, but also the attacker can maliciously change the messages sent between the victims. For example, say Bob was trying to send money to Alice using PayPal, but Mallory changed the account the money was sent to from Alice’s to her own.\n\nCryptography’s Conventional Names\n\nBob, Alice, and Mallory are placeholder names commonly used when discussing cryptography (en.wikipedia.org/wiki/ Alice_and_Bob#Cast_of_characters). Typically Alice and Bob want to exchange a message, and Mallory is a malicious attacker. There’s a whole cast of characters, and they’re named with rhyming mnemonics to their role (for example: Mallory the malicious attacker, Eve the eavesdropper, Craig the password cracker).\n\nThe most widely used technology for preventing MITM attacks and encrypting data in-flight is TLS, the successor to SSL. TLS used to be considered necessary\n\n1.\n\nhttps://en.wikipedia.org/wiki/Man-in-the-middle_attack\n\nreport erratum • discuss",
      "content_length": 2164,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 85,
      "content": "Secure Services in Three Steps • 77\n\nonly for \"serious\" websites like online banks, but these days all sites should use TLS.2 Modern browsers highlight websites that don’t use TLS as unsafe and recommend their users to not even use them.\n\nThe process by which a client and server communicate is kicked off by a TLS handshake. During this handshake, the client and server:\n\n1. Specify which version of TLS they’ll use;\n\n2. Decide which cipher suites (the set of encryption algorithms) they’ll use;\n\n3. Authenticate the identity of the server via the server’s private key and the\n\ncertificate authority’s digital signature; and\n\n4. Generate session keys for symmetric encryption after the handshake is\n\ncomplete.\n\nOnce this handshake process is complete, the client and server can commu- nicate securely.\n\nFortunately we don’t have to worry about implementing these TLS handshake steps, as TLS handles them for us behind the scenes. Our job is to obtain certificates for our client and server to use and to tell gRPC to communicate over TLS using the given the certs.\n\nWe’ll build TLS support into our service to encrypt data in-flight and authenticate the server.\n\nAuthenticate to Identify Clients\n\nOnce you’ve secured the communication between your client and server with TLS, the next step to a secure service is authentication. Authentication is the process of identifying who the client is (TLS has already handled authenticating the server). For example, whenever you post a tweet, Twitter needs to verify that the person trying to post the tweet to your account is really you.\n\nMost web services use TLS for one-way authentication and only authenticate the server. The authentication of the client is left to the application to work out, usually by some combination of username-password credentials and tokens. TLS mutual authentication, also commonly referred to as two-way authentication, in which both the server and the client validate the other’s communication, is more commonly used in machine-to-machine communica- tion—like distributed systems! In this setup, both the server and the client use a certificate to prove their identity.\n\n2.\n\nhttps://doesmysiteneedhttps.com\n\nreport erratum • discuss",
      "content_length": 2209,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 86,
      "content": "Chapter 5. Secure Your Services • 78\n\nBecause mutual TLS authentication is so effective, relatively simple, and well adopted (both in terms of how many people use it and the number of technolo- gies that support it), many companies use it to secure the communications3 between their internal distributed services. Because so many people use mutual TLS authentication, it’s important for new services (like ours) to support it. So we’ll build mutual TLS authentication into our service.\n\nAuthorize to Determine the Permissions of Clients\n\nAuthentication and authorization are so closely related that people often use the word “auth” to refer to both. Authentication and authorization are almost always done at the same time in terms of the request’s life-cycle and place in the server’s code base. In fact, for many web services where resources have a single owner, authentication and authorization are the same process. For example, a Twitter account has one owner, so if a client authenticates as that owner, then Twitter lets them do whatever they want with the account.\n\nDifferentiating between authentication and authorization is necessary when you have a resource with shared access and varying levels of ownership. With our log service for example, Alice might be the owner and have both read and write access to the contents of the log, whereas Bob might be allowed to read the contents but isn’t able to write. In this type of situation, you need authorization with granular access control.\n\nIn our service, we’ll build access control list-based authorization to control whether a client is allowed to read from or write to (or both) the log.\n\nNow that you have a general understanding of the three key aspects of securing a distributed system, let’s implement them in our service.\n\nAuthenticate the Server with TLS\n\nYou’ve now seen how TLS works and why to use it, so we’re ready to build TLS support into our service to encrypt data in-flight and authenticate the server. I’ll also cover how to make obtaining and working with certificates easier to manage.\n\nOperate as Your Own CA with CFSSL\n\nBefore changing our server’s code, let’s get some certs. We could use a third- party certificate authority (CA) to get the certs, but that could cost money (depending on the CA) and is a hassle. For internal services (like ours), there’s\n\n3.\n\nhttps://blog.cloudflare.com/how-to-build-your-own-public-key-infrastructure\n\nreport erratum • discuss",
      "content_length": 2448,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 87,
      "content": "Authenticate the Server with TLS • 79\n\nno need to go through a third-party authority. Trusted certificates don’t have to come from Comodo or Let’s Encrypt or any other CA—they can come from a CA you operate yourself. It’s free and easy with the right tools.\n\nCloudFlare4 wrote a toolkit called CFSSL for signing, verifying, and bundling TLS certificates. CloudFlare uses CFSSL for their internal services’ TLS cer- tificates, acting as their own certificate authority. CloudFlare open sourced CFSSL so others, including us, can use it. Even major CA vendors like Let’s Encrypt use CFSSL. Big thanks to CloudFlare because CFSSL is a seriously useful toolkit.\n\nCFSSL has two tools we’ll need:\n\ncfssl to sign, verify, and bundle TLS certificates and output the results as JSON. • cfssljson to take that JSON output and split them into separate key, cer- tificate, CSR, and bundle files.\n\nInstall the CloudFlare CLIs by running the following commands:\n\n$ go get github.com/cloudflare/cfssl/cmd/cfssl@v1.4.1 $ go get github.com/cloudflare/cfssl/cmd/cfssljson@v1.4.1\n\nTo initialize our CA and generate certs, we need to pass various config files to the cfssl commands we’ll run. We need separate config files to generate our CA and server certs and we need a config file containing general config info about our CA. So let’s create a directory in our project to contain these config files by running $ mkdir test.\n\nPut the following JSON into a file called ca-csr.json in your test directory:\n\nSecureYourServices/test/ca-csr.json {\n\n\"CN\": \"My Awesome CA\", \"key\": {\n\n\"algo\": \"rsa\", \"size\": 2048\n\n}, \"names\": [ {\n\n\"C\": \"CA\", \"L\": \"ON\", \"ST\": \"Toronto\", \"O\": \"My Awesome Company\", \"OU\": \"CA Services\"\n\n}\n\n]\n\n}\n\n4.\n\nhttps://www.cloudflare.com\n\nreport erratum • discuss",
      "content_length": 1758,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 88,
      "content": "Chapter 5. Secure Your Services • 80\n\ncfssl will use this file to configure our CA’s certificate. CN stands for Common Name, so we’re saying our CA is called “My Awesome CA.” key specifies the algorithm and size of key to sign the certificate with; names is a list of various name information that’ll be added to the certificate. Each name object should contain at least one “C,” “L,” “O,” “OU,” or “ST” value (or any combination of these). They stand for:\n\nC—country • L—locality or municipality (such as city) • ST—state or province • O—organization • OU—organizational unit (such as the department responsible for owning the key)\n\nCreate a test/ca-config.json that looks like this to define the CA’s policy:\n\nSecureYourServices/test/ca-config.json {\n\n\"signing\": {\n\n\"profiles\": {\n\n\"server\": {\n\n\"expiry\": \"8760h\", \"usages\": [\n\n\"signing\", \"key encipherment\", \"server auth\"\n\n]\n\n}, \"client\": {\n\n\"expiry\": \"8760h\", \"usages\": [\n\n\"signing\", \"key encipherment\", \"client auth\"\n\n]\n\n}\n\n}\n\n}\n\n}\n\nOur CA needs to know what kind of certificates it will issue. The signing section of this configuration file defines your CA’s signing policy. Our configuration file says that the CA can generate client and server certificates that will expire after a year and the certificates may be used for digital signatures, encrypting keys, and auth.\n\nPut the following JSON into a file called server-csr.json in your test directory:\n\nreport erratum • discuss",
      "content_length": 1435,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 89,
      "content": "Authenticate the Server with TLS • 81\n\nSecureYourServices/test/server-csr.json {\n\n\"CN\": \"127.0.0.1\", \"hosts\": [\n\n\"localhost\", \"127.0.0.1\"\n\n], \"key\": {\n\n\"algo\": \"rsa\", \"size\": 2048\n\n}, \"names\": [ {\n\n\"C\": \"CA\", \"L\": \"ON\", \"ST\": \"Toronto\", \"O\": \"My Awesome Company\", \"OU\": \"Distributed Services\"\n\n}\n\n]\n\n}\n\ncfssl will use these configs to configure our server’s certificate. The “hosts” field is a list of the domain names that the certificate should be valid for. Since we’re running our service locally, we just need 127.0.0.1 and localhost.\n\nNow let’s update our Makefile to call cfssl and cfssljson to actually generate the certs. Make your project’s Makefile look like this:\n\nSecureYourServices/Makefile CONFIG_PATH=${HOME}/.proglog/\n\n.PHONY: init init:\n\nmkdir -p ${CONFIG_PATH}\n\n.PHONY: gencert gencert:\n\ncfssl gencert \\\n\ninitca test/ca-csr.json | cfssljson -bare ca\n\ncfssl gencert \\\n\nca=ca.pem \\ -ca-key=ca-key.pem \\ -config=test/ca-config.json \\ -profile=server \\ test/server-csr.json | cfssljson -bare server\n\nmv *.pem *.csr ${CONFIG_PATH}\n\n.PHONY: test test:\n\ngo test -race ./...\n\nreport erratum • discuss",
      "content_length": 1111,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 90,
      "content": "Chapter 5. Secure Your Services • 82\n\n.PHONY: compile compile:\n\nprotoc api/v1/*.proto \\\n\n--go_out=. \\ --go-grpc_out=. \\ --go_opt=paths=source_relative \\ --go-grpc_opt=paths=source_relative \\ --proto_path=.\n\nIn this updated Makefile, we’ve added a CONFIG_PATH variable to specify where we’d like to put our generated certs and an init target to create that directory. With these configs in a static and known location on the filesystem, it’s easier to look up and use the certs in our code. The gencert target calls cfssl to gen- erate the certificate and private keys for our CA and server using the config files we added earlier.\n\nWe’ll reference these config files frequently in our tests, so let’s make a package containing their file paths as variables to make referencing them easy. Create an internal/config directory with a files.go file containing this code:\n\nSecureYourServices/internal/config/files.go package config\n\nimport (\n\n\"os\" \"path/filepath\"\n\n)\n\nvar (\n\nCAFile ServerCertFile ServerKeyFile\n\n= configFile(\"ca.pem\") = configFile(\"server.pem\") = configFile(\"server-key.pem\")\n\n)\n\nfunc configFile(filename string) string {\n\nif dir := os.Getenv(\"CONFIG_DIR\"); dir != \"\" {\n\nreturn filepath.Join(dir, filename)\n\n} homeDir, err := os.UserHomeDir() if err != nil {\n\npanic(err)\n\n} return filepath.Join(homeDir, \".proglog\", filename)\n\n}\n\nThese variables define the paths to the certs we generated and need to look up and parse for our tests. I would use constants and the const keyword if Go allowed using const with function calls.\n\nreport erratum • discuss",
      "content_length": 1562,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 91,
      "content": "Authenticate the Server with TLS • 83\n\nWe’ll use the certificate and key files to build *tls.Configs, so let’s add a helper function and struct for that. In the config directory, create a tls.go file beginning with this code:\n\nSecureYourServices/internal/config/tls.go package config\n\nimport (\n\n\"crypto/tls\" \"crypto/x509\" \"fmt\" \"io/ioutil\"\n\n)\n\nfunc SetupTLSConfig(cfg TLSConfig) (*tls.Config, error) {\n\nvar err error tlsConfig := &tls.Config{} if cfg.CertFile != \"\" && cfg.KeyFile != \"\" {\n\ntlsConfig.Certificates = make([]tls.Certificate, 1) tlsConfig.Certificates[0], err = tls.LoadX509KeyPair(\n\ncfg.CertFile, cfg.KeyFile,\n\n) if err != nil {\n\nreturn nil, err\n\n}\n\n} if cfg.CAFile != \"\" {\n\nb, err := ioutil.ReadFile(cfg.CAFile) if err != nil {\n\nreturn nil, err\n\n} ca := x509.NewCertPool() ok := ca.AppendCertsFromPEM([]byte(b)) if !ok {\n\nreturn nil, fmt.Errorf(\n\n\"failed to parse root certificate: %q\", cfg.CAFile,\n\n)\n\n} if cfg.Server {\n\ntlsConfig.ClientCAs = ca tlsConfig.ClientAuth = tls.RequireAndVerifyClientCert\n\n} else {\n\ntlsConfig.RootCAs = ca\n\n} tlsConfig.ServerName = cfg.ServerAddress\n\n} return tlsConfig, nil\n\n}\n\nreport erratum • discuss",
      "content_length": 1147,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 92,
      "content": "Chapter 5. Secure Your Services • 84\n\nOur tests use a few different *tls.Config configurations, and SetupTLSConfig() allows us to get each type of *tls.Config with one function call. These are the different configurations:\n\nClient *tls.Config is set up to verify the server’s certificate with the client’s\n\nby setting the *tls.Config’s RootCAs.\n\nClient *tls.Config is set up to verify the server’s certificate and allow the server to verify the client’s certificate by setting its RootCAs and its Certificates.\n\nServer *tls.Config is set up to verify the client’s certificate and allow the client to verify the server’s certificate by setting its ClientCAs, Certificate, and ClientAuth mode set to tls.RequireAndVerifyCert.\n\nBelow SetupTLSConfig(), put this struct:\n\nSecureYourServices/internal/config/tls.go type TLSConfig struct { string CertFile string KeyFile string CAFile ServerAddress string Server\n\nbool\n\n}\n\nTLSConfig defines the parameters that SetupTLSConfig() uses to determine what type of *tls.Config to return.\n\nBack to our tests. Let’s test that the client uses our CA to verify the server’s certificate. If the server’s certificate came from a different authority, the client wouldn’t trust the server and wouldn’t make a connection. In setup_test.go, add these imports:\n\nSecureYourServices/internal/server/server_test.go \"github.com/travisjeffery/proglog/internal/config\" \"google.golang.org/grpc/credentials\"\n\nNow replace the code in your existing setupTest() function with the following code:\n\nSecureYourServices/internal/server/server_test.go t.Helper()\n\nl, err := net.Listen(\"tcp\", \"127.0.0.1:0\") require.NoError(t, err)\n\nclientTLSConfig, err := config.SetupTLSConfig(config.TLSConfig{\n\nCAFile: config.CAFile,\n\n}) require.NoError(t, err)\n\nclientCreds := credentials.NewTLS(clientTLSConfig) cc, err := grpc.Dial(\n\nreport erratum • discuss",
      "content_length": 1857,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 93,
      "content": "Authenticate the Server with TLS • 85\n\nl.Addr().String(), grpc.WithTransportCredentials(clientCreds),\n\n) require.NoError(t, err)\n\nclient = api.NewLogClient(cc)\n\nIn this code, we configure our client’s TLS credentials to use our CA as the client’s Root CA (the CA it will use to verify the server). Then we tell the client to use those credentials for its connection.\n\nNext we need to hook up our server with its certificate and enable it to handle TLS connections. Add the following code below the previous snippet:\n\nSecureYourServices/internal/server/server_test.go serverTLSConfig, err := config.SetupTLSConfig(config.TLSConfig{\n\nCertFile: KeyFile: CAFile: ServerAddress: l.Addr().String(),\n\nconfig.ServerCertFile, config.ServerKeyFile, config.CAFile,\n\n}) require.NoError(t, err) serverCreds := credentials.NewTLS(serverTLSConfig)\n\ndir, err := ioutil.TempDir(\"\", \"server-test\") require.NoError(t, err)\n\nclog, err := log.NewLog(dir, log.Config{}) require.NoError(t, err)\n\ncfg = &Config{\n\nCommitLog: clog,\n\n} if fn != nil {\n\nfn(cfg)\n\n} server, err := NewGRPCServer(cfg, grpc.Creds(serverCreds)) require.NoError(t, err)\n\ngo func() {\n\nserver.Serve(l)\n\n}()\n\nreturn client, cfg, func() { server.Stop() cc.Close() l.Close()\n\n}\n\nIn this code, we’re parsing the server’s cert and key, which we then use to configure the server’s TLS credentials. We then pass those credentials as a gRPC server option to our NewGRPCServer() function so it can create our gRPC server with that option. gRPC server options are how you enable features in\n\nreport erratum • discuss",
      "content_length": 1553,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 94,
      "content": "Chapter 5. Secure Your Services • 86\n\ngRPC servers. We’re setting the credentials for the server connections in this case, but there are plenty of other server options5 to configure connection timeouts, keep alive policies, and so on.\n\nFinally, we need to update the NewGRPCServer() function in server.go to take in the given gRPC server options and create the server with them. Change the NewGRPCServer() function to this:\n\nSecureYourServices/internal/server/server.go func NewGRPCServer(config *Config, opts ...grpc.ServerOption) (\n\ngrpc.Server, error,\n\n) {\n\ngsrv := grpc.NewServer(opts...) srv, err := newgrpcServer(config) if err != nil {\n\nreturn nil, err\n\n} api.RegisterLogServer(gsrv, srv) return gsrv, nil\n\n}\n\nAt this point you can run the tests with $ make test, and our tests should pass as they did before the changes we’ve made in this chapter. The difference is that your server is now authenticated and your connection is encrypted. You can verify this by temporarily changing your test code back to using an insecure client connection with the grpc.WithInsecure() dial option, and then running the tests again. This time the tests will fail because the client and server won’t be able to connect with each other because the server is expecting the client to run over TLS.\n\nYour server is authenticated so you know your client is communicating with your actual server and not some middleman’s. Now we’ll use mutual TLS authentication to verify that the client hitting your server really is your client.\n\nAuthenticate the Client with Mutual TLS Authentication\n\nIn the previous section, we used TLS to encrypt our connections and authenticate the server. Now we’ll go one step further and implement mutual TLS authentication (also known as two-way authentication) so the server will use our CA to verify that the client is authentic.\n\nThe first thing we need is a cert for our client, which we can generate with cfssl and cfssljson just like our CA and server’s certificates. Put the following JSON in a file called client-csr.json in your test directory:\n\n5.\n\nhttps://godoc.org/google.golang.org/grpc#ServerOption\n\nreport erratum • discuss",
      "content_length": 2151,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 95,
      "content": "➤ ➤\n\nAuthenticate the Client with Mutual TLS Authentication • 87\n\nSecureYourServices/test/client-csr.json {\n\n\"CN\": \"client\", \"hosts\": [\"\"], \"key\": {\n\n\"algo\": \"rsa\", \"size\": 2048\n\n}, \"names\": [ {\n\n\"C\": \"CA\", \"L\": \"ON\", \"ST\": \"Toronto\", \"O\": \"My Company\", \"OU\": \"Distributed Services\"\n\n}\n\n]\n\n}\n\nThe CN field is the important config because that’s the client’s identity—their username, in a sense. This is the identity we’ll store their permissions under for authorization. (We’ll do this in the next section.)\n\nNext, update the gencert target in your Makefile, to include the following snippet. Place it right below where you generate the server cert:\n\nSecureYourServices/Makefile cfssl gencert \\\n\nca=ca.pem \\ -ca-key=ca-key.pem \\ -config=test/ca-config.json \\ -profile=client \\ test/client-csr.json | cfssljson -bare client\n\nOnce that is done, generate the cert for your client by running $ make gencert.\n\nAdd configuration file variables for your client certificates in internal/config/files.go:\n\nSecureYourServices/internal/config/files.go var (\n\nCAFile ServerCertFile ServerKeyFile ClientCertFile ClientKeyFile\n\n= configFile(\"ca.pem\") = configFile(\"server.pem\") = configFile(\"server-key.pem\") = configFile(\"client.pem\") = configFile(\"client-key.pem\")\n\n)\n\nNext we need to update the server to verify that the certificate the client has sent the server is signed by our CA. Update your server setup in server_test.go like this:\n\nreport erratum • discuss",
      "content_length": 1453,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 96,
      "content": "➤ ➤\n\n➤\n\nChapter 5. Secure Your Services • 88\n\nSecureYourServices/internal/server/server_test.go clientTLSConfig, err := config.SetupTLSConfig(config.TLSConfig{\n\nCertFile: config.ClientCertFile, KeyFile: config.ClientKeyFile, CAFile: config.CAFile,\n\n}) require.NoError(t, err)\n\nclientCreds := credentials.NewTLS(clientTLSConfig) cc, err := grpc.Dial(\n\nl.Addr().String(), grpc.WithTransportCredentials(clientCreds),\n\n) require.NoError(t, err)\n\nclient = api.NewLogClient(cc)\n\nserverTLSConfig, err := config.SetupTLSConfig(config.TLSConfig{\n\nCertFile: KeyFile: CAFile: ServerAddress: l.Addr().String(), Server: true,\n\nconfig.ServerCertFile, config.ServerKeyFile, config.CAFile,\n\n})\n\nNow run your tests again. They’ll still pass because you’re using a valid cert and your tests expect the client to be authentic. For a fun exercise, try generating a cert from a different CA for your client to use and then watch your tests fail. (Okay, maybe I’m the only one who considers this kind of thing fun.)\n\nYour server and client now have mutual TLS authentication with both sides verifying that your CA vouches for their authenticity, so you know it’s your actual client communicating with your server without a middleman eavesdrop- ping. Hooray for security!\n\nAuthorize with Access Control Lists\n\nAuthentication is usually half of what you need from your auth process. You authenticate to know who’s behind the client so you can then complete the auth process by authorizing whoever is behind the client for whatever action they’ve attempted. As I mentioned earlier, authorization is the process of verifying what someone has access to.\n\nThe simplest way to implement authorization is with an access control list (ACL).6 An ACL is a table of rules where each row says something like “Subject A is permitted to do action B on object C.” For example: Alice is permitted to\n\n6.\n\nhttps://en.wikipedia.org/wiki/Access_control_list\n\nreport erratum • discuss",
      "content_length": 1941,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 97,
      "content": "Authorize with Access Control Lists • 89\n\nread Distributed Services with Go. In this example, Alice is the subject, to read is the action, and Distributed Services with Go is the object.\n\nOne of the beautiful things about an ACL is that it’s easy to build. Since it’s just a table, something as simple as a map or a CSV file could back the data—in more complex implementations, a key-value store or relational database would store the data. So building an ACL library from scratch isn’t difficult, but there’s a nice library called Casbin7 that supports enforcing authorization based on various control models8—including ACLs. Plus Casbin is well adopted, tested, and extendable. Casbin is a useful tool to have in your toolkit, so let’s learn how to use it and take advantage of it!\n\nFirst, add Casbin as a dependency by running the following command at the root of your project:\n\n$ go get github.com/casbin/casbin@v1.9.1\n\nWe’ll wrap Casbin with our own internal library. If we later use another authoriza- tion tool, we won’t have to change a bunch of code throughout our project, just the code in our library. Create an auth directory inside your internal directory by running:\n\n$ mkdir internal/auth\n\nThen create a file called authorizer.go in that directory with your favorite text editor and add the following code:\n\nSecureYourServices/internal/auth/authorizer.go package auth\n\nimport (\n\n\"fmt\"\n\n\"github.com/casbin/casbin\" \"google.golang.org/grpc/codes\" \"google.golang.org/grpc/status\"\n\n)\n\nfunc New(model, policy string) *Authorizer {\n\nenforcer := casbin.NewEnforcer(model, policy) return &Authorizer{\n\nenforcer: enforcer,\n\n}\n\n}\n\ntype Authorizer struct {\n\nenforcer *casbin.Enforcer\n\n}\n\n7. 8.\n\nhttps://github.com/casbin/casbin\n\nhttps://github.com/casbin/casbin#supported-models\n\nreport erratum • discuss",
      "content_length": 1807,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 98,
      "content": "Chapter 5. Secure Your Services • 90\n\nfunc (a *Authorizer) Authorize(subject, object, action string) error {\n\nif !a.enforcer.Enforce(subject, object, action) {\n\nmsg := fmt.Sprintf(\n\n\"%s not permitted to %s to %s\", subject, action, object,\n\n) st := status.New(codes.PermissionDenied, msg) return st.Err()\n\n} return nil\n\n}\n\nIn this code, we define an Authorizer type whose sole method, Authorize, defers to Casbin’s Enforce function. This function returns whether the given subject is permitted to run the given action on the given object based on the model and policy you configure Casbin with. The New function’s model and policy arguments are paths to the files where you’ve defined the model (which will configure Casbin’s authorization mechanism—which for us will be ACL) and the policy (which is a CSV file containing your ACL table).\n\nBecause we’re testing authorization, we need multiple clients with different permissions and hence multiple client certs. Having multiple clients with different permissions lets us check whether the server permits or denies a client’s request based on the rules defined in the ACL. So let’s change the cert generation code in your Makefile to generate multiple client certs. To do that, in the gencert target of your Makefile, replace the client cert section to look like this:\n\nSecureYourServices/Makefile cfssl gencert \\\n\nca=ca.pem \\ -ca-key=ca-key.pem \\ -config=test/ca-config.json \\ -profile=client \\ -cn=\"root\" \\ test/client-csr.json | cfssljson -bare root-client\n\ncfssl gencert \\\n\nca=ca.pem \\ -ca-key=ca-key.pem \\ -config=test/ca-config.json \\ -profile=client \\ -cn=\"nobody\" \\ test/client-csr.json | cfssljson -bare nobody-client\n\nThen run $ make gencert to generate the certs.\n\nreport erratum • discuss",
      "content_length": 1749,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 99,
      "content": "Authorize with Access Control Lists • 91\n\nNow let’s update our server tests to test for authorization and check that the tests fail (since our server doesn’t have authorization support yet). Later, when we implement authorization in our server, the tests will pass, and we’ll know we’ve successfully implemented authorization in the server.\n\nFirst, let’s update our client setup in our tests to build two clients we can use for testing our authorization setup. Update your client setup code in server_test.go to look like this:\n\nSecureYourServices/internal/server/server_test.go newClient := func(crtPath, keyPath string) (\n\ngrpc.ClientConn, api.LogClient, []grpc.DialOption,\n\n) {\n\ntlsConfig, err := config.SetupTLSConfig(config.TLSConfig{\n\nCertFile: crtPath, KeyFile: keyPath, CAFile: Server:\n\nconfig.CAFile, false,\n\n}) require.NoError(t, err) tlsCreds := credentials.NewTLS(tlsConfig) opts := []grpc.DialOption{grpc.WithTransportCredentials(tlsCreds)} conn, err := grpc.Dial(l.Addr().String(), opts...) require.NoError(t, err) client := api.NewLogClient(conn) return conn, client, opts\n\n}\n\nvar rootConn *grpc.ClientConn rootConn, rootClient, _ = newClient(\n\nconfig.RootClientCertFile, config.RootClientKeyFile,\n\n)\n\nvar nobodyConn *grpc.ClientConn nobodyConn, nobodyClient, _ = newClient(\n\nconfig.NobodyClientCertFile, config.NobodyClientKeyFile,\n\n)\n\nAnd update the teardown function to close the client connections:\n\nSecureYourServices/internal/server/server_test.go return rootClient, nobodyClient, cfg, func() {\n\nserver.Stop() rootConn.Close() nobodyConn.Close() l.Close()\n\n}\n\nreport erratum • discuss",
      "content_length": 1605,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 100,
      "content": "➤ ➤ ➤ ➤\n\n➤ ➤ ➤ ➤ ➤ ➤\n\nChapter 5. Secure Your Services • 92\n\nWe’re creating two clients: a superuser9 client called root who’s permitted to produce and consume, and a nobody10 client who isn’t permitted to do any- thing. Because the code for creating both clients is the same (aside from which cert and key they’re configured with), we’ve refactored the client creation code into a newClient(crtPath, keyPath string) helper function. Our server now takes in an Authorizer instance that the server will defer its authorization logic to. And we pass both our root and nobody clients to the test functions so they can use whatever client they need based on whether they’re testing how the server works with an authorized or unauthorized client. This last change also requires us to make some changes to our existing tests, so let’s fix those.\n\nChange your TestServer() function to the following so your test functions take in the unauthorized client:\n\nSecureYourServices/internal/server/server_test.go func TestServer(t *testing.T) {\n\nfor scenario, fn := range map[string]func(\n\nt *testing.T, rootClient api.LogClient, nobodyClient api.LogClient, config *Config,\n\n){\n\n// ...\n\n} {\n\nt.Run(scenario, func(t *testing.T) {\n\nrootClient,\n\nnobodyClient, config, teardown := setupTest(t, nil)\n\ndefer teardown() fn(t, rootClient, nobodyClient, config)\n\n})\n\n}\n\n}\n\nWe need to update our existing tests to handle the second client, which we do by changing the arguments of your test functions to the following:\n\nt *testing.T, client, _ api.LogClient, cfg *Config\n\nWe also need to add more variables to specify the locations of our nobody client’s cert and key, along with the configuration files for Casbin. So add these variables to your var declaration in internal/config/files.go:\n\n9. 10. https://en.wikipedia.org/wiki/Nobody_(username)\n\nhttps://en.wikipedia.org/wiki/Superuser\n\nreport erratum • discuss",
      "content_length": 1889,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 101,
      "content": "➤ ➤ ➤ ➤ ➤ ➤\n\nAuthorize with Access Control Lists • 93\n\nSecureYourServices/internal/config/files.go var (\n\nCAFile ServerCertFile ServerKeyFile RootClientCertFile RootClientKeyFile NobodyClientCertFile = configFile(\"nobody-client.pem\") NobodyClientKeyFile = configFile(\"nobody-client-key.pem\") ACLModelFile ACLPolicyFile\n\n= configFile(\"ca.pem\") = configFile(\"server.pem\") = configFile(\"server-key.pem\") = configFile(\"root-client.pem\") = configFile(\"root-client-key.pem\")\n\n= configFile(\"model.conf\") = configFile(\"policy.csv\")\n\n)\n\nSince the ACL policy is specific and used throughout our tests, we’ll put our Casbin configuration in the test directory as well. Inside the test directory, create a file called model.conf with the following configuration:\n\nSecureYourServices/test/model.conf # Request definition [request_definition] r = sub, obj, act\n\n# Policy definition [policy_definition] p = sub, obj, act\n\n# Policy effect [policy_effect] e = some(where (p.eft == allow))\n\n# Matchers [matchers] m = r.sub == p.sub && r.obj == p.obj && r.act == p.act\n\nThis configures Casbin to use ACL as its authorization mechanism.\n\nAlongside the model.conf file, add a policy.csv file with this snippet:\n\nSecureYourServices/test/policy.csv p, root, *, produce p, root, *, consume\n\nThis is your ACL table, with two entries saying that the root client has produce and consume permissions on the * object (which we’re using as a wildcard, meaning any object). All other clients, including nobody, will be denied.\n\nNow we need to install the policy and model files into the CONFIG_PATH so our tests can find them. Update your Makefile’s test target to the following:\n\nreport erratum • discuss",
      "content_length": 1674,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 102,
      "content": "Chapter 5. Secure Your Services • 94\n\nSecureYourServices/Makefile $(CONFIG_PATH)/model.conf:\n\ncp test/model.conf $(CONFIG_PATH)/model.conf\n\n$(CONFIG_PATH)/policy.csv:\n\ncp test/policy.csv $(CONFIG_PATH)/policy.csv\n\n.PHONY: test test: $(CONFIG_PATH)/policy.csv $(CONFIG_PATH)/model.conf\n\ngo test -race ./...\n\nNow your tests are in a runnable state again, so you can run $ make test to see that they still pass! This is because the existing tests use the root client, which is authorized to produce and consume, and our current tests assume the client is authorized, and so they pass.\n\nLet’s add a test to check that unauthorized clients are denied. In server_test.go, import these packages:\n\nSecureYourServices/internal/server/server_test.go \"google.golang.org/grpc/codes\" \"google.golang.org/grpc/status\"\n\nBelow the testProduceConsumeStream() test we added in the last chapter, add this testUnauthorized() test:\n\nSecureYourServices/internal/server/server_test.go func testUnauthorized(\n\nt *testing.T, _, client api.LogClient, config *Config,\n\n) {\n\nctx := context.Background() produce, err := client.Produce(ctx, &api.ProduceRequest{\n\nRecord: &api.Record{\n\nValue: []byte(\"hello world\"),\n\n},\n\n},\n\n) if produce != nil {\n\nt.Fatalf(\"produce response should be nil\")\n\n} gotCode, wantCode := status.Code(err), codes.PermissionDenied if gotCode != wantCode {\n\nt.Fatalf(\"got code: %d, want: %d\", gotCode, wantCode)\n\n} consume, err := client.Consume(ctx, &api.ConsumeRequest{\n\nOffset: 0,\n\n})\n\nreport erratum • discuss",
      "content_length": 1505,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 103,
      "content": "➤\n\nAuthorize with Access Control Lists • 95\n\nif consume != nil {\n\nt.Fatalf(\"consume response should be nil\")\n\n} gotCode, wantCode = status.Code(err), codes.PermissionDenied if gotCode != wantCode {\n\nt.Fatalf(\"got code: %d, want: %d\", gotCode, wantCode)\n\n}\n\n}\n\nIn this test, we use the nobody client, which isn’t permitted to do anything. We try to use the client to produce and consume, just as we did in the suc- cessful test case. Since our client isn’t authorized, we want our server to deny the client, which we verify by checking the code on the returned error.\n\nUpdate the test table in TestServer(*testing.T) to include our unauthorized test by adding the highlighted line:\n\nSecureYourServices/internal/server/server_test.go \"produce/consume a message to/from the log succeeeds\": testProduceConsume, \"produce/consume stream succeeds\": \"consume past log boundary fails\": \"unauthorized fails\": testUnauthorized,\n\ntestProduceConsumeStream, testConsumePastBoundary,\n\nIf we run our tests with $ make test, they’ll fail because our server is still per- mitting all clients to do anything, since we haven’t hooked up its authorization yet. Let’s add authorization to the server now.\n\nUpdate your Config in server.go, update your imports to the following:\n\nSecureYourServices/internal/server/server.go import (\n\n\"context\"\n\napi \"github.com/travisjeffery/proglog/api/v1\"\n\ngrpc_middleware \"github.com/grpc-ecosystem/go-grpc-middleware\" grpc_auth \"github.com/grpc-ecosystem/go-grpc-middleware/auth\" \"google.golang.org/grpc\" \"google.golang.org/grpc/credentials\" \"google.golang.org/grpc/codes\" \"google.golang.org/grpc/peer\" \"google.golang.org/grpc/status\"\n\n)\n\nNow we’ll add a field for the authorizer and some constants we will use for authorization:\n\nSecureYourServices/internal/server/server.go type Config struct {\n\nCommitLog CommitLog Authorizer Authorizer\n\n}\n\nreport erratum • discuss",
      "content_length": 1882,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 104,
      "content": "➤ ➤ ➤ ➤ ➤ ➤ ➤\n\n➤ ➤ ➤ ➤ ➤ ➤ ➤\n\nChapter 5. Secure Your Services • 96\n\nconst (\n\nobjectWildcard = \"*\" produceAction = \"produce\" consumeAction = \"consume\"\n\n)\n\nThe constants match the values we in our ACL policy table, and we’ll reference them a few times in this file so they make sense being constants. The Config’s Authorizer field is an interface we need to define; put the following snippet below the CommitLog interface:\n\nSecureYourServices/internal/server/server.go type Authorizer interface {\n\nAuthorize(subject, object, action string) error\n\n}\n\nWe depend on an interface for the Authorizer so that we can switch out the authorization implementation—same as the CommitLog in Dependency Inversion with Interfaces, on page 67. Update your Produce() method to this snippet, adding the highlighted lines:\n\nSecureYourServices/internal/server/server.go func (s *grpcServer) Produce(ctx context.Context, req *api.ProduceRequest) (\n\napi.ProduceResponse, error) { if err := s.Authorizer.Authorize( subject(ctx), objectWildcard, produceAction,\n\n); err != nil {\n\nreturn nil, err\n\n} offset, err := s.CommitLog.Append(req.Record) if err != nil {\n\nreturn nil, err\n\n} return &api.ProduceResponse{Offset: offset}, nil\n\n}\n\nMake a similar change to your Consume() method, changing the method to this:\n\nSecureYourServices/internal/server/server.go func (s *grpcServer) Consume(ctx context.Context, req *api.ConsumeRequest) (\n\napi.ConsumeResponse, error) { if err := s.Authorizer.Authorize( subject(ctx), objectWildcard, consumeAction,\n\n); err != nil {\n\nreturn nil, err\n\n}\n\nreport erratum • discuss",
      "content_length": 1580,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 105,
      "content": "Authorize with Access Control Lists • 97\n\nrecord, err := s.CommitLog.Read(req.Offset) if err != nil {\n\nreturn nil, err\n\n} return &api.ConsumeResponse{Record: record}, nil\n\n}\n\nWe now have the server checking whether the client (identified by the cert’s subject) is authorized to produce and consume, and if not, sending the per- mission denied error back to the client. When producing, if the client is authorized, then the method will continue and append the given record to the log. And when consuming, if the client is authorized, then the method will consume the record from the log. We take the subject out of the client’s cert with two helper functions. Add the following code at the bottom of server.go:\n\nSecureYourServices/internal/server/server.go func authenticate(ctx context.Context) (context.Context, error) {\n\npeer, ok := peer.FromContext(ctx) if !ok {\n\nreturn ctx, status.New(\n\ncodes.Unknown, \"couldn't find peer info\",\n\n).Err()\n\n}\n\nif peer.AuthInfo == nil {\n\nreturn context.WithValue(ctx, subjectContextKey{}, \"\"), nil\n\n}\n\ntlsInfo := peer.AuthInfo.(credentials.TLSInfo) subject := tlsInfo.State.VerifiedChains[0][0].Subject.CommonName ctx = context.WithValue(ctx, subjectContextKey{}, subject)\n\nreturn ctx, nil\n\n}\n\nfunc subject(ctx context.Context) string {\n\nreturn ctx.Value(subjectContextKey{}).(string)\n\n}\n\ntype subjectContextKey struct{}\n\nThe authenticate(context.Context) function is an interceptor that reads the subject out of the client’s cert and writes it to the RPC’s context. With interceptors, you can intercept and modify the execution of each RPC call, allowing you to break the request handling into smaller, reusable chunks. (Other frameworks name the same concept middleware.) The subject(context.Context) function returns the client’s cert’s subject so we can identify a client and check their access.\n\nUpdate your NewGRPCServer(*Config, ...grpc.ServerOption) function to the following code:\n\nreport erratum • discuss",
      "content_length": 1951,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 106,
      "content": "Chapter 5. Secure Your Services • 98\n\nSecureYourServices/internal/server/server.go func NewGRPCServer(config *Config, opts ...grpc.ServerOption) (\n\ngrpc.Server, error,\n\n) {\n\nopts = append(opts, grpc.StreamInterceptor(\n\ngrpc_middleware.ChainStreamServer(\n\ngrpc_auth.StreamServerInterceptor(authenticate),\n\n)), grpc.UnaryInterceptor(grpc_middleware.ChainUnaryServer( grpc_auth.UnaryServerInterceptor(authenticate),\n\n))) gsrv := grpc.NewServer(opts...) srv, err := newgrpcServer(config) if err != nil {\n\nreturn nil, err\n\n} api.RegisterLogServer(gsrv, srv) return gsrv, nil\n\n}\n\nWe hook up our authenticate() interceptor to our gRPC server so that our server identifies the subject of each RPC to kick off the authorization process.\n\nNow update your test server’s configuration to pass in an authorizer. In set- up_test.go’s setupTest, import your auth package, and update the server’s configu- ration to the following:\n\nSecureYourServices/internal/server/server_test.go authorizer := auth.New(config.ACLModelFile, config.ACLPolicyFile) cfg = &Config{\n\nCommitLog: clog, Authorizer: authorizer,\n\n}\n\nOur server now authorizes its requests! You can verify that everything works by running the tests again: $ make test. Last time we ran them they failed because our server didn’t deny the nobody client who doesn’t have any per- missions. This time the tests pass since our server will now only authorize users who are permitted based on your ACL!\n\nWhat You Learned\n\nYou’ve learned how to secure services in three parts: by encrypting connections with TLS, through mutual TLS authentication to verify the identities of clients and servers, and by using ACL-based authorization to permit client actions. Next we’ll make our service observable by adding metrics, logs, and traces.\n\nreport erratum • discuss",
      "content_length": 1795,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 107,
      "content": "CHAPTER 6\n\nObserve Your Systems\n\nImagine waking up one day and noticing that the last hole in your belt doesn’t fit. You head to your scale and you see that you’ve gained a significant amount of weight overnight. You go on an emergency diet and fitness regimen. A couple of weeks later, you check the scales and see you gained even more weight somehow. What’s going on?\n\nWhat you need is insight into what’s going on in your body. If our body had built-in observability, we’d have metrics on our body, like hormone levels that we could graph on a dashboard. If we could see a sudden imbalance in our hormone levels, with all things being equal, we could surmise that a hormonal imbalance must be the root cause. But without being able to see what had changed, you’d make many changes in search of the problem, each with their own effects.\n\nWe make our systems observable so we can we can ask questions that will give us insight into the system and debug unexpected problems. The keyword is unexpected—making our system observable means we can fix arbitrary problems that haven’t happened before. In this chapter, we’ll make our service observable so we understand what’s going on within it.\n\nThree Types of Telemetry Data\n\nObservability is a measure of how well we understand our system’s inter- nals—its behavior and state—from its external outputs. We use metrics, structured logs, and traces as the outputs to make our systems observable. While there are three types of telemetry data, each with its own use case we’ll talk about, it’ll often derive from the same events. For example, each time a web service handles a request, it may increment a “requests handled” metric, emit a log for the request, and make a trace.\n\nreport erratum • discuss",
      "content_length": 1748,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 108,
      "content": "Chapter 6. Observe Your Systems • 100\n\nMetrics\n\nMetrics measure numeric data over time, such as how many requests failed or how long each request took. Metrics like these help to define service-level indicators (SLI), objectives (SLO), and agreements (SLA). You’ll use metrics to report the health of your system, trigger internal alerts, and graph on dashboards to get an idea of how your system’s doing at a glance.\n\nBecause metrics are numerical data, you can gradually reduce resolution to reduce the storage requirements and time to query. For example, if we ran a book publishing company, we’d have metrics on each book purchase. To ship a customer’s books, we’d need to know the customer’s order, but after we’ve delivered the books and the return policy has passed, we don’t care about the order anymore. When we’re doing accounting or analysis on our business, that’s too much detail. Eventually we’d only need quarterly earnings to do our taxes, calculate year-over-year growth, and know if we can hire more editors and authors to expand our business.\n\nThere are three kinds of metrics:\n\nCounters\n\nCounters track the number of times an event happened, such as the number of requests that failed or the sum of some fact of your system like the number of bytes processed.\n\nYou’ll often take a counter and use it to get a rate: the number of times an event happened in an interval. Who cares about the total requests we’ve received other than to brag about it? What we care about is how many requests we’ve handled in the past second or minute—if that dropped significantly you’d want to check for latency in your system. You’d want to know when your request error rate spikes so you can see what’s wrong and fix it.\n\nHistograms\n\nHistograms show you a distribution of your data. You’ll mainly use his- tograms for measuring the percentiles of your request duration and sizes.\n\nGauges\n\nGauges track the current value of something. You can replace that value entirely. Gauges are useful for saturation-type metrics, like a host’s disk usage percentage or the number of load balancers compared to your cloud provider’s limits.\n\nreport erratum • discuss",
      "content_length": 2156,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 109,
      "content": "Three Types of Telemetry Data • 101\n\nYou could measure just about anything, so what data should you measure? What metrics will provide worthy signals on your system? These are Google’s four golden signals1 to measure:\n\nLatency—the time it takes your service to process requests. If your latency spikes, you often need to scale your system vertically by changing to an instance with more memory, CPUs, or IOPS, or scale your system horizontally by adding more instances to your load balancer.\n\nTraffic—the amount of demand on your service. For a typical web service, this could be requests processed per second. For an online video game or video streaming service, it could be the number of concurrent users. These metrics are good for bragging rights (hopefully), but more important, they can help give you an idea of the scale at which you’re working and when you’ve scaled to the point you need a new design.\n\nErrors—your service’s request failure rate. Internal server errors are par-\n\nticularly important.\n\nSaturation—a measure of your service’s capacity. For example, if your service persists data to disk, at your current ingress rate will you run out of hard drive space soon? If you have an in-memory store, how much memory is your service using compared to the memory available?\n\nWhile most debugging stories begin with metrics—either through an alert or someone noticing abnormalities on the dashboard—you’ll go to your logs and traces to learn more details about the problem. Let’s take a look at those next.\n\nStructured Logs\n\nLogs describe events in your system. You should log any event that gives you useful insight into your service. Logs should help us troubleshoot, audit, and profile so we can learn what went wrong and why, who ran what actions, and how long those actions took. For example, a gRPC service log could log this per RPC call:\n\n{\n\n\"request_id\": \"f47ac10b-58cc-0372-8567-0e02b2c3d479\", \"level\": \"info\", \"ts\": 1600139560.3399575, \"caller\": \"zap/server_interceptors.go:67\", \"msg\": \"finished streaming call with code OK\", \"peer.address\": \"127.0.0.1:54304\",\n\n1.\n\nhttps://landing.google.com/sre/sre-book/chapters/monitoring-distributed-systems/#xref_monitoring_golden-\n\nsignals\n\nreport erratum • discuss",
      "content_length": 2229,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 110,
      "content": "Chapter 6. Observe Your Systems • 102\n\n\"grpc.start_time\": \"2020-09-14T22:12:40-05:00\", \"system\": \"grpc\", \"span.kind\": \"server\", \"grpc.service\": \"log.v1.Log\", \"grpc.method\": \"ConsumeStream\", \"peer.address\": \"127.0.0.1:54304\", \"grpc.code\": \"OK\", \"grpc.time_ns\": 197740\n\n}\n\nIn this log we see when the caller called the method, the caller’s IP address, the service and method they called, if the call succeeded, and how long the request took. In distributed systems, the request ID is helpful for piecing together a complete picture of a request that’s handled by multiple services.\n\nThis gRPC log is a JSON formatted, structured log. A structured log is a set of name and value ordered pairs encoded in consistent schema and format that’s easily read by programs. Structured logs enable us to separate log capturing, transporting, persisting, and querying. For example, we could capture and transport our logs as protocol buffers and then re-encode them in the Parquet2 format and persist them in your columnar database.\n\nI recommend collecting your structured logs in an event streaming platform like Kafka to enable arbitrary processing and transporting of your logs. For example, you can connect Kafka with a database like BigQuery to query your logs while connecting Kafka with an object store like GCS to maintain histor- ical copies.\n\nAt play is a balance between logging too little and being without the informa- tion needed to debug a problem, or logging too much and being overwhelmed by too much information and missing what’s important. I suggest erring on logging too much, and cut back on the logs that aren’t useful as you learn more. That way you’re less likely to be without information you need to trou- bleshoot or audit a problem.\n\nTraces\n\nTraces capture request lifecycles and let you track requests as they flow through your system. Tracing user interfaces like Jaegar,3 Stackdriver,4 and Lightstep5 give you a visual representation of where requests spend time in your system. In distributed systems, this is especially useful as requests\n\n2. 3. 4. 5.\n\nhttps://parquet.apache.org\n\nhttps://www.jaegertracing.io\n\nhttps://cloud.google.com/products/operations\n\nhttps://lightstep.com\n\nreport erratum • discuss",
      "content_length": 2224,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 111,
      "content": "Make Your Service Observable • 103\n\nexecute over multiple services. The following screenshot shows an example of a trace of Jocko’s request handling in Jaegar.\n\nYou can tag your traces with details to know more about each request. A common example is tagging each trace with a user ID so that if users experi- ence a problem, you can easily find their requests.\n\nTraces comprise one or more spans. Spans can have parent/child relationships or be linked as siblings. Each span represents a part of the request’s execution. How detailed you break up those parts is up to you. Go wide to begin: trace requests across all your services end-to-end, with spans that begin and end at the entry and exit points of your services. Then go deep in each service and trace important method calls.\n\nNow, let’s update our code to make your service observable.\n\nMake Your Service Observable\n\nLet’s make your service observable by adding metrics, structured logs, and traces. When you deploy your services to production, you’ll usually configure your metrics, structured logs, and traces to go to external services like Prometheus,6 Elasticsearch,7 and Jaegar. To keep things simple, we’ll just log our observability pieces to files and see what the data looks like.\n\n6. 7.\n\nhttps://prometheus.io\n\nhttps://www.elastic.co/elasticsearch\n\nreport erratum • discuss",
      "content_length": 1343,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 112,
      "content": "➤ ➤ ➤ ➤ ➤ ➤ ➤ ➤ ➤\n\nChapter 6. Observe Your Systems • 104\n\nOpenTelemetry8 is a Cloud Native Computing Foundation (CNCF) project that provides robust and portable APIs and libraries that we can use for metrics and distributed tracing in our service. (OpenCensus and OpenTracing merged to form OpenTelemetry, which is backward-compatible with existing Open- Census integrations.) OpenTelemetry’s Go gRPC integration supports traces but not metrics, so we’ll use the OpenCensus libraries in our service since OpenCensus’s gRPC integration supports them both. Unfortunately, neither OpenTelemetry nor OpenCensus support logging yet. OpenTelemetry should support logging at some point—a special interest group9 is planning Open- Telemetry’s logging specification. In the meantime, we’ll use Uber’s Zap logging library.10\n\nMost Go networking APIs support middleware, so you can wrap request handling with your own logic. This is where I recommend beginning making your service observable by wrapping all requests with metrics, logs, and traces. That’s why we’re using the OpenCensus and Zap integrations’ inter- ceptors.\n\nRun the following commands within your project to add the OpenCensus and Zap dependencies:\n\n$ go get go.uber.org/zap@v1.10.0 $ go get go.opencensus.io@v0.22.2\n\nThen open internal/server/server.go and update your imports to include the high- lighted imports in this snippet:\n\nObserveYourServices/internal/server/server.go import (\n\n\"context\"\n\ngrpc_middleware \"github.com/grpc-ecosystem/go-grpc-middleware\" grpc_auth \"github.com/grpc-ecosystem/go-grpc-middleware/auth\" api \"github.com/travisjeffery/proglog/api/v1\"\n\n\"time\"\n\ngrpc_zap \"github.com/grpc-ecosystem/go-grpc-middleware/logging/zap\" grpc_ctxtags \"github.com/grpc-ecosystem/go-grpc-middleware/tags\" \"go.opencensus.io/plugin/ocgrpc\" \"go.opencensus.io/stats/view\" \"go.opencensus.io/trace\" \"go.uber.org/zap\" \"go.uber.org/zap/zapcore\"\n\n8. 9. 10. https://github.com/uber-go/zap\n\nhttps://opentelemetry.io\n\nhttps://github.com/open-telemetry/community#logs-working-group\n\nreport erratum • discuss",
      "content_length": 2059,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 113,
      "content": "Make Your Service Observable • 105\n\n\"google.golang.org/grpc\" \"google.golang.org/grpc/codes\" \"google.golang.org/grpc/credentials\" \"google.golang.org/grpc/peer\" \"google.golang.org/grpc/status\"\n\n)\n\nNow update NewGRPCServer() to configure Zap:\n\nObserveYourServices/internal/server/server.go func NewGRPCServer(config *Config, grpcOpts ...grpc.ServerOption) (\n\ngrpc.Server, error,\n\n) {\n\nlogger := zap.L().Named(\"server\") zapOpts := []grpc_zap.Option{\n\ngrpc_zap.WithDurationField(\n\nfunc(duration time.Duration) zapcore.Field {\n\nreturn zap.Int64(\n\n\"grpc.time_ns\", duration.Nanoseconds(),\n\n)\n\n},\n\n),\n\n}\n\nWe specify the logger’s name to differentiate the server logs from other logs in our service. Then we add a “grpc.time_ns” field to our structured logs to log the duration of each request in nanoseconds.\n\nAfter the previous snippet, add the following snippet to configure how Open- Census collects metrics and traces:\n\nObserveYourServices/internal/server/server.go trace.ApplyConfig(trace.Config{DefaultSampler: trace.AlwaysSample()}) err := view.Register(ocgrpc.DefaultServerViews...) if err != nil {\n\nreturn nil, err\n\n}\n\nWe’ve configured OpenCensus to always sample the traces because we’re developing our service and we want all of our requests traced.\n\nIn production you may not want to trace every request because it could affect performance, require too much data, or trace confidential data. If tracing too much is the problem, you can use the probability sampler and sample a percentage of the requests. However, one problem with using the probability sampler is that you may miss important requests. We could try to reconcile these trade-offs by writing our own sampler that always traces important\n\nreport erratum • discuss",
      "content_length": 1729,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 114,
      "content": "➤ ➤\n\n➤ ➤\n\n➤\n\nChapter 6. Observe Your Systems • 106\n\nrequests and samples a percentage of the rest of the requests. The code for that would look like this:\n\nhalfSampler := trace.ProbabilitySampler(0.5) trace.ApplyConfig(trace.Config{\n\nDefaultSampler: func(p trace.SamplingParameters) trace.SamplingDecision {\n\nif strings.Contains(p.Name, \"Produce\"){\n\nreturn trace.SamplingDecision{Sample: true}\n\n} return halfSampler(p)\n\n},\n\n})\n\nThe views specify what stats OpenCensus will collect. The default server views track stats on:\n\nReceived bytes per RPC • Sent bytes per RPC • Latency • Completed RPCs\n\nNow, change the grpcOpts after the previous snippet to include the lines high- lighted here:\n\nObserveYourServices/internal/server/server.go grpcOpts = append(grpcOpts,\n\ngrpc.StreamInterceptor(\n\ngrpc_middleware.ChainStreamServer(\n\ngrpc_ctxtags.StreamServerInterceptor(), grpc_zap.StreamServerInterceptor(logger, zapOpts...), grpc_auth.StreamServerInterceptor(authenticate),\n\n)), grpc.UnaryInterceptor(grpc_middleware.ChainUnaryServer( grpc_ctxtags.UnaryServerInterceptor(), grpc_zap.UnaryServerInterceptor(logger, zapOpts...), grpc_auth.UnaryServerInterceptor(authenticate),\n\n)), grpc.StatsHandler(&ocgrpc.ServerHandler{}),\n\n)\n\nThese lines configure gRPC to apply the Zap interceptors that log the gRPC calls and attach OpenCensus as the server’s stat handler so that OpenCensus can record stats on the server’s request handling.\n\nOkay, now we just have to change our test setup to configure the metrics and traces log files. Open internal/server/server_test.go and add these imports:\n\nObserveYourServices/internal/server/server_test.go \"os\" \"time\" \"flag\"\n\nreport erratum • discuss",
      "content_length": 1676,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 115,
      "content": "Make Your Service Observable • 107\n\n\"go.opencensus.io/examples/exporter\"\n\n\"go.uber.org/zap\"\n\nBelow your imports, add this snippet that defines a debug flag to enable observability output:\n\nObserveYourServices/internal/server/server_test.go // imports...\n\nvar debug = flag.Bool(\"debug\", false, \"Enable observability for debugging.\")\n\nfunc TestMain(m *testing.M) { flag.Parse() if *debug {\n\nlogger, err := zap.NewDevelopment() if err != nil {\n\npanic(err)\n\n} zap.ReplaceGlobals(logger)\n\n} os.Exit(m.Run())\n\n}\n\nWhen a test file implements TestMain(m *testing.M), Go will call TestMain(m) instead of running the tests directly. TestMain() gives us a place for setup that applies to all tests in that file, like enabling our debug output. Flag parsing has to go in TestMain() instead of init(), otherwise Go can’t define the flag and your code will error and exit.\n\nIn the setupTest() function, after the authorizer variable, add this snippet:\n\nObserveYourServices/internal/server/server_test.go var telemetryExporter *exporter.LogExporter if *debug {\n\nmetricsLogFile, err := ioutil.TempFile(\"\", \"metrics-*.log\") require.NoError(t, err) t.Logf(\"metrics log file: %s\", metricsLogFile.Name())\n\ntracesLogFile, err := ioutil.TempFile(\"\", \"traces-*.log\") require.NoError(t, err) t.Logf(\"traces log file: %s\", tracesLogFile.Name())\n\ntelemetryExporter, err = exporter.NewLogExporter(exporter.Options{\n\nMetricsLogFile: TracesLogFile: ReportingInterval: time.Second,\n\nmetricsLogFile.Name(), tracesLogFile.Name(),\n\n}) require.NoError(t, err) err = telemetryExporter.Start() require.NoError(t, err)\n\n}\n\nreport erratum • discuss",
      "content_length": 1610,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 116,
      "content": "➤ ➤ ➤ ➤ ➤\n\nChapter 6. Observe Your Systems • 108\n\nThis snippet sets up and starts the telemetry exporter to write to two files. Each test gets its own separate trace and metrics files so we can see each test’s requests.\n\nAt the bottom of setupTest(), update the teardown function to include these highlighted lines:\n\nObserveYourServices/internal/server/server_test.go return rootClient, nobodyClient, cfg, func() {\n\nserver.Stop() rootConn.Close() nobodyConn.Close() l.Close() if telemetryExporter != nil {\n\ntime.Sleep(1500 * time.Millisecond) telemetryExporter.Stop() telemetryExporter.Close()\n\n}\n\n}\n\nWe sleep for 1.5 seconds to give the telemetry exporter enough time to flush its data to disk. Then we stop and close the exporter.\n\nRun your server tests by navigating into the internal/server directory and execut- ing this command:\n\n$ go test -v -debug=true\n\nIn the test output, find these metrics and traces file logs, and open them to see the exported metrics and trace data:\n\nmetrics log file: /tmp/metrics-{{random string}}.log traces log file: /tmp/traces-{{random string}}.log\n\nFor example, here’s the completed RPC stat showing that the server handled two successful produce calls:\n\nMetric: name: grpc.io/server/completed_rpcs, type: TypeCumulativeInt64, unit: ms\n\nLabels: [\n\n{grpc_server_method}={log.v1.Log/Produce true} {grpc_server_status}={OK true}] Value : value=2\n\nAnd here’s a trace for a produce call:\n\nTraceID: SpanID:\n\n3e3343b74193e6a807cac515e82fb3b3 045493d1be3f7188\n\nSpan: Status: Elapsed: 1ms SpanKind: Server\n\nlog.v1.Log.Produce\n\n[0]\n\nreport erratum • discuss",
      "content_length": 1585,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 117,
      "content": "What You Learned • 109\n\nAttributes:\n\nClient=false - FailFast=false\n\nMessageEvents: Received UncompressedByteSize: 15 CompressedByteSize: 0\n\nSent UncompressedByteSize: 0 CompressedByteSize: 5\n\nWe can now observe what’s going on in our service!\n\nWhat You Learned\n\nIn this chapter, you learned about observability and its role in making reliable systems. You’ll find tracing especially useful in distributed systems, as it gives you a complete story of requests that take part over multiple services. You also learned how to make your service observable. Next, we’ll make our server support clustering to the service highly available and scalable.\n\nreport erratum • discuss",
      "content_length": 670,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 118,
      "content": "Part III\n\nDistribute",
      "content_length": 20,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 119,
      "content": "CHAPTER 7\n\nServer-to-Server Service Discovery\n\nSo far we’ve built a secure, stand-alone gRPC web service. Now let’s start our journey toward making our stand-alone service into a distributed one by incorporating service discovery so that our service automatically handles when a node is added to or removed from our cluster.\n\nIf you’re not familiar with service discovery, don’t worry—you will be after reading this chapter. Service discovery is one of the coolest things about dis- tributed services: machines automatically discovering other machines! (When Skynet becomes self aware and takes over, we can thank service discovery for playing a part.) Here’s a quick overview of the many benefits of service discovery.\n\nWhy Use Service Discovery?\n\nService discovery is the process of figuring out how to connect to a service. A service discovery solution must keep an up-to-date list (also known as a registry) of services, their locations, and their health. Downstream services then query this registry to discover the location of upstream services and connect to them—for example, a web service discovering and connecting to its database. This way, even if the upstream services change (scale up or down, or get replaced), downstream services can still connect to them.\n\nIn the pre-cloud days, you could set up “service discovery” with manually managed and configured static addresses, which was workable since applica- tions ran on static hardware. Today, service discovery plays a big part in modern cloud applications where nodes change frequently.\n\nInstead of using service discovery, some developers put load balancers in front of their services so that the load balancers provide static IPs. But for server-to-server communication, where you control the servers and you don’t\n\nreport erratum • discuss",
      "content_length": 1810,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 120,
      "content": "Chapter 7. Server-to-Server Service Discovery • 114\n\nneed a load balancer to act as a trust boundary1 between clients and servers, use service discovery instead. Load balancers add cost, increase latency, introduce single points of failure, and need updates as services scale up and down. If you manage tens or hundreds of microservices, then not using service discovery means you also have to manage tens or hundreds of load balancers and DNS records. For a distributed service like ours, using a load balancer would force us to depend on a load-balancer service like nginx or the various cloud load balancers like AWS’s ELB or Google Cloud’s Load Balancer. This would increase our operational burden, infrastructure costs, and latency.\n\nIn our system, we have two service-discovery problems to solve:\n\nHow will the servers in our cluster discover each other? • How will the clients discover the servers?\n\nIn this chapter, we’ll work on implementing the discovery for the servers. Then, after we implement consensus in Chapter 8, Coordinate Your Services with Consensus, on page 141, we’ll work on the clients’ discovery in Chapter 9, Discover Servers and Load Balance from the Client, on page 171.\n\nNow that you know what service discovery can do, we’re ready to embed it into our service.\n\nEmbed Service Discovery\n\nWhen you have an application that needs to talk to a service, the tool you use for service discovery needs to perform the following tasks:\n\nManage a registry of services containing info such as their IPs and ports; • Help services find other services using the registry; • Health check service instances and remove them if they’re not well; and • Deregister services when they go offline.\n\nHistorically, people who’ve built distributed services have depended on sepa- rate, stand-alone services for service discovery (such as Consul, ZooKeeper, and Etcd). In this architecture, users of your service run two clusters: one for your service and one for your service discovery. The benefit of using a service-discovery service is that you don’t have to build service discovery yourself. The downside to using such a service, from your users’ standpoint, is that they have to learn, launch, and operate an extra service’s cluster. So using a stand-alone service for discovery removes the burden from your shoulders and puts it on your users’. That means many users won’t use your\n\n1.\n\nhttps://en.wikipedia.org/wiki/Trust_boundary\n\nreport erratum • discuss",
      "content_length": 2469,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 121,
      "content": "Embed Service Discovery • 115\n\nservice because the burden is too much for them, and users who do take it on won’t recommend your service to others as often or as highly.\n\nSo why did people who built distributed services use stand-alone service- discovery services, and why did their users put up with the extra burden? Because neither had much of a choice. The people building distributed services didn’t have the libraries they needed to embed service discovery into their services, and users didn’t have other options.\n\nFortunately, times have changed. Today, Gophers have Serf—a library that provides decentralized cluster membership, failure detection, and orchestration that you can use to easily embed service discovery into your distributed ser- vices. Hashicorp, the company that created it, uses Serf to power its own service-discovery product, Consul, so you’re in good company.\n\nUsing Serf to embed service discovery into your services means that you don’t have to implement service discovery yourself and your users don’t have to run an extra cluster. It’s a win-win.\n\nWhen to Depend on a Stand-Alone Service-Discovery Solution\n\nYou may encounter cases where depending on a stand-alone ser- vice for service discovery makes sense—for example, if you need to integrate your service discovery with many platforms. You sink a lot of effort into that kind of work, and that’s likely a poor use of your time when you could just use a service like Consul that provides those integrations. In any case, Serf is always a good place to start. Once you’ve developed your service to solve the core problem it’s targeting and your service is stable or close to it, then you will have a good sense of whether you need to depend on a service-discovery service.\n\nHere are some other benefits of building our service with Serf:\n\nIn the early days of building a service, Serf is faster to set up and build\n\nour service against than having to set up a separate service.\n\nIt’s easier to move from Serf to a stand-alone service than to move from\n\na stand-alone service to Serf, so we still have both options open.\n\nOur service will be easier and more flexible to deploy, making our service\n\nmore accessible.\n\nSo for our service, we’ll use Serf to build service discovery.\n\nNow that we’ve seen the benefits of using Serf, let’s quickly discuss how Serf does its thing.\n\nreport erratum • discuss",
      "content_length": 2385,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 122,
      "content": "Chapter 7. Server-to-Server Service Discovery • 116\n\nDiscover Services with Serf\n\nSerf maintains cluster membership by using an efficient, lightweight gossip protocol to communicate between the service’s nodes. Unlike service registry projects like ZooKeeper and Consul, Serf doesn’t have a central-registry architectural style. Instead, each instance of your service in the cluster runs as a Serf node. These nodes exchange messages with each other in the same way a zombie apocalypse might occur: one infected zombie soon spreads to infect everyone else. With Serf, instead of a spreading zombie virus, you’re spreading information about the nodes in your cluster. You listen to Serf for messages about changes in the cluster and then handle them accordingly.\n\nTo implement service discovery with Serf we need to:\n\n1. Create a Serf node on each server.\n\n2. Configure each Serf node with an address to listen on and accept connec-\n\ntions from other Serf nodes.\n\n3. Configure each Serf node with addresses of other Serf nodes and join their\n\ncluster.\n\n4. Handle Serf’s cluster discovery events, such as when a node joins or fails\n\nin the cluster.\n\nLet’s get coding.\n\nSerf is a lightweight tool that you can use for infinite use cases, but its API can be verbose when you have a specific problem to solve. The specific job we want our discovery layer to solve is to tell us when a server joined or left the cluster and what its ID and address are with as little API as possible. So let’s make a discovery package our server will use.\n\nTo get started, install the Serf package by running this command:\n\n$ go get github.com/hashicorp/serf@v0.8.5\n\nThen create an internal/discovery directory and inside it create a membership.go file, beginning with this code:\n\nServerSideServiceDiscovery/internal/discovery/membership.go package discovery\n\nimport (\n\n\"net\"\n\n\"go.uber.org/zap\"\n\n\"github.com/hashicorp/serf/serf\"\n\n)\n\nreport erratum • discuss",
      "content_length": 1934,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 123,
      "content": "Discover Services with Serf • 117\n\ntype Membership struct {\n\nConfig handler Handler serf events chan serf.Event logger *zap.Logger\n\nserf.Serf\n\n}\n\nfunc New(handler Handler, config Config) (*Membership, error) {\n\nc := &Membership{\n\nConfig: config, handler: handler, logger: zap.L().Named(\"membership\"),\n\n} if err := c.setupSerf(); err != nil {\n\nreturn nil, err\n\n} return c, nil\n\n}\n\nMembership is our type wrapping Serf to provide discovery and cluster member- ship to our service. Users will call New() to create a Membership with the required configuration and event handler.\n\nAdd this code below the New() function to define the configuration type and set up Serf:\n\nLine 1\n\nServerSideServiceDiscovery/internal/discovery/membership.go type Config struct {\n\n\n\n\n\n\n\n5 }- -\n\nNodeName BindAddr Tags StartJoinAddrs []string\n\nstring string map[string]string\n\n\n\nfunc (m *Membership) setupSerf() (err error) {\n\n\n\n10\n\naddr, err := net.ResolveTCPAddr(\"tcp\", m.BindAddr) if err != nil {\n\n\n\nreturn err\n\n\n\n\n\n\n\n15\n\n\n\n\n\n\n\n\n\n20\n\n\n\n} config := serf.DefaultConfig() config.Init() config.MemberlistConfig.BindAddr = addr.IP.String() config.MemberlistConfig.BindPort = addr.Port m.events = make(chan serf.Event) config.EventCh = m.events config.Tags = m.Tags config.NodeName = m.Config.NodeName m.serf, err = serf.Create(config) if err != nil {\n\n\n\n\n\nreturn err\n\n\n\n}\n\nreport erratum • discuss",
      "content_length": 1369,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 124,
      "content": "Chapter 7. Server-to-Server Service Discovery • 118\n\n25\n\n\n\ngo m.eventHandler() if m.StartJoinAddrs != nil {\n\n\n\n\n\n_, err = m.serf.Join(m.StartJoinAddrs, true) if err != nil {\n\n\n\nreturn err\n\n30\n\n}\n\n\n\n}-\n\n} return nil\n\nSerf has a lot of configurable parameters, but the five parameters you’ll typi- cally use are:\n\nNodeName—the node name acts as the node’s unique identifier across the Serf cluster. If you don’t set the node name, Serf uses the hostname.\n\nBindAddr and BindPort—Serf listens on this address and port for gossiping.\n\nTags—Serf shares these tags to the other nodes in the cluster and should use these tags for simple data that informs the cluster how to handle this node. For example, Consul shares each node’s RPC address with Serf tags, and once they know each other’s RPC address, they can make RPCs to each other. Consul shares whether the node is a voter or non-voter, which changes the node’s role in the Raft cluster. We’ll talk about this more in the next chapter when we use Raft to build consensus in our cluster. In our code, similar to Consul, we’ll share each node’s user-con- figured RPC address with a Serf tag so the nodes know which addresses to send their RPCs.\n\nEventCh—the event channel is how you’ll receive Serf’s events when a node joins or leaves the cluster. If you want a snapshot of the members at any point in time, you can call Serf’s Members() method.\n\nStartJoinAddrs—when you have an existing cluster and you create a new node that you want to add to that cluster, you need to point your new node to at least one of the nodes now in the cluster. After the new node connects to one of those nodes in the existing cluster, it’ll learn about the rest of the nodes, and vice versa (the existing nodes learn about the new node). The StartJoinAddrs field is how you configure new nodes to join an existing cluster. You set the field to the addresses of nodes in the cluster, and Serf’s gossip protocol takes care of the rest to join your node to the cluster. In a production environment, specify at least three addresses to make your cluster resilient to one or two node failures or a disrupted network.\n\nsetupSerf() creates and configures a Serf instance and starts the eventsHandler() goroutine to handle Serf’s events.\n\nreport erratum • discuss",
      "content_length": 2284,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 125,
      "content": "Discover Services with Serf • 119\n\nDefine the Handler interface by putting this snippet below setupSerf():\n\nServerSideServiceDiscovery/internal/discovery/membership.go type Handler interface {\n\nJoin(name, addr string) error Leave(name string) error\n\n}\n\nThe Handler represents some component in our service that needs to know when a server joins or leaves the cluster.\n\nIn this chapter we will build a component that replicates the data of servers that join the cluster. In the next chapter, where we will build consensus in our service, Raft needs to know when servers join the cluster to coordi- nate with them.\n\nAdd this snippet below Handler() to define the eventHandler() method:\n\nServerSideServiceDiscovery/internal/discovery/membership.go func (m *Membership) eventHandler() {\n\nfor e := range m.events {\n\nswitch e.EventType() { case serf.EventMemberJoin:\n\nfor _, member := range e.(serf.MemberEvent).Members {\n\nif m.isLocal(member) {\n\ncontinue\n\n} m.handleJoin(member)\n\n}\n\ncase serf.EventMemberLeave, serf.EventMemberFailed:\n\nfor _, member := range e.(serf.MemberEvent).Members {\n\nif m.isLocal(member) { return\n\n} m.handleLeave(member)\n\n}\n\n}\n\n}\n\n}\n\nfunc (m *Membership) handleJoin(member serf.Member) {\n\nif err := m.handler.Join(\n\nmember.Name, member.Tags[\"rpc_addr\"],\n\n); err != nil {\n\nm.logError(err, \"failed to join\", member)\n\n}\n\n}\n\nfunc (m *Membership) handleLeave(member serf.Member) {\n\nif err := m.handler.Leave(\n\nmember.Name,\n\nreport erratum • discuss",
      "content_length": 1463,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 126,
      "content": "Chapter 7. Server-to-Server Service Discovery • 120\n\n); err != nil {\n\nm.logError(err, \"failed to leave\", member)\n\n}\n\n}\n\nThe eventHandler() runs in a loop reading events sent by Serf into the events channel, handling each incoming event according to the event’s type. When a node joins or leaves the cluster, Serf sends an event to all nodes, including the node that joined or left the cluster. We check whether the node we got an event for is the local server so the server doesn’t act on itself—we don’t want the server to try and replicate itself, for example.\n\nNotice that Serf may coalesce multiple members updates into one event. For example, say ten nodes join around the same time; in that case, Serf will send you one join event with ten members, so that’s why we iterate over the event’s members.\n\nPut this code below eventHandler() to implement the rest of Membership:\n\nServerSideServiceDiscovery/internal/discovery/membership.go func (m *Membership) isLocal(member serf.Member) bool { return m.serf.LocalMember().Name == member.Name\n\n}\n\nfunc (m *Membership) Members() []serf.Member {\n\nreturn m.serf.Members()\n\n}\n\nfunc (m *Membership) Leave() error { return m.serf.Leave()\n\n}\n\nfunc (m *Membership) logError(err error, msg string, member serf.Member) {\n\nm.logger.Error(\n\nmsg, zap.Error(err), zap.String(\"name\", member.Name), zap.String(\"rpc_addr\", member.Tags[\"rpc_addr\"]),\n\n)\n\n}\n\nThese methods comprise the rest of Membership:\n\nisLocal() returns whether the given Serf member is the local member by\n\nchecking the members’ names.\n\nMembers() returns a point-in-time snapshot of the cluster’s Serf members.\n\nLeave() tells this member to leave the Serf cluster.\n\nlogError() logs the given error and message.\n\nreport erratum • discuss",
      "content_length": 1739,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 127,
      "content": "Discover Services with Serf • 121\n\nLet’s test our Membership code now. Create a membership_test.go file in the inter- nal/discovery directory, and begin the file with this code:\n\nServerSideServiceDiscovery/internal/discovery/membership_test.go package discovery_test\n\nimport (\n\n\"fmt\" \"testing\" \"time\"\n\n\"github.com/hashicorp/serf/serf\" \"github.com/stretchr/testify/require\" \"github.com/travisjeffery/go-dynaport\" . \"github.com/travisjeffery/proglog/internal/discovery\"\n\n)\n\nfunc TestMembership(t *testing.T) {\n\nm, handler := setupMember(t, nil) m, _ = setupMember(t, m) m, _ = setupMember(t, m)\n\nrequire.Eventually(t, func() bool {\n\nreturn 2 == len(handler.joins) &&\n\n3 == len(m[0].Members()) && 0 == len(handler.leaves)\n\n}, 3*time.Second, 250*time.Millisecond)\n\nrequire.NoError(t, m[2].Leave())\n\nrequire.Eventually(t, func() bool {\n\nreturn 2 == len(handler.joins) &&\n\n3 == len(m[0].Members()) && serf.StatusLeft == m[0].Members()[2].Status && 1 == len(handler.leaves)\n\n}, 3*time.Second, 250*time.Millisecond)\n\nrequire.Equal(t, fmt.Sprintf(\"%d\", 2), <-handler.leaves)\n\n}\n\nOur test sets up a cluster with multiple servers and checks that the Membership returns all the servers that joined the membership and updates after a server leaves the cluster. The handler’s joins and leaves channels tell us how many times each event happened and for what servers. Each member has a status to know how its doing:\n\nAlive—the server is present and healthy. • Leaving—the server is gracefully leaving the cluster. • Left—the server has gracefully left the cluster. • Failed—the server unexpectedly left the cluster.\n\nreport erratum • discuss",
      "content_length": 1626,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 128,
      "content": "Chapter 7. Server-to-Server Service Discovery • 122\n\nTestMembership() relies on a helper method to set up a member each time you call it. Define the helper setupMember() by adding the following code below TestMembership():\n\nServerSideServiceDiscovery/internal/discovery/membership_test.go func setupMember(t *testing.T, members []*Membership) (\n\n[]*Membership, *handler,\n\n) {\n\nid := len(members) ports := dynaport.Get(1) addr := fmt.Sprintf(\"%s:%d\", \"127.0.0.1\", ports[0]) tags := map[string]string{\n\n\"rpc_addr\": addr,\n\n} c := Config{\n\nNodeName: fmt.Sprintf(\"%d\", id), BindAddr: addr, tags, Tags:\n\n} h := &handler{} if len(members) == 0 {\n\nh.joins = make(chan map[string]string, 3) h.leaves = make(chan string, 3)\n\n} else {\n\nc.StartJoinAddrs = []string{ members[0].BindAddr,\n\n}\n\n} m, err := New(h, c) require.NoError(t, err) members = append(members, m) return members, h\n\n}\n\nsetupMember() sets up a new member under a free port and with the member’s length as the node name so the names are unique. The member’s length also tells us whether this member is the cluster’s initial member or we have a cluster to join.\n\nDefine the handler mock and finish the test code by putting this snippet below setupMember():\n\nServerSideServiceDiscovery/internal/discovery/membership_test.go type handler struct {\n\njoins chan map[string]string leaves chan string\n\n}\n\nfunc (h *handler) Join(id, addr string) error {\n\nif h.joins != nil {\n\nreport erratum • discuss",
      "content_length": 1446,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 129,
      "content": "Request Discovered Services and Replicate Logs • 123\n\nh.joins <- map[string]string{\n\n\"id\": \"addr\": addr,\n\nid,\n\n}\n\n} return nil\n\n}\n\nfunc (h *handler) Leave(id string) error {\n\nif h.leaves != nil {\n\nh.leaves <- id\n\n} return nil\n\n}\n\nThe handler mock tracks how many times our Membership calls the handler’s Join() and Leave() methods, and with what IDs and addresses.\n\nRun the Membership’s tests and verify they pass.\n\nNow that we have our discovery and membership package, let’s integrate it with our service and build something we couldn’t before—replication!\n\nRequest Discovered Services and Replicate Logs\n\nLet’s build on our service discovery to add replication in our service so that we store multiple copies of the log data when we have multiple servers in a cluster. Replication makes our service more resilient to failures. For example, if a node’s disk fails and we can’t recover its data, replication can save our butts because it ensures that there’s a copy saved on another disk.\n\nIn the next chapter, we’ll coordinate the servers so our replication will have a defined leader-follower relationship, but for now we simply want the servers to replicate each other when they discover each other and not worry about whether they should, like the scientists from Jurassic Park. Our goal for the rest of this chapter is to build something simple that makes use of our service’s discovery and sets us up for our coordinated replication in the next chapter.\n\nDiscovery alone isn’t useful—so what if a bunch of computers discover each other and they just sit there doing nothing? Discovery is important because the discovery events trigger other processes in our service like replication and consensus. When servers discover other servers, we want to trigger the servers to replicate. We need a component in our service that handles when a server joins (or leaves) the cluster and begins (or ends) replicating from it.\n\nOur replication will be pull-based, with the replication component consum- ing from each discovered server and producing a copy to the local server.\n\nreport erratum • discuss",
      "content_length": 2096,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 130,
      "content": "Chapter 7. Server-to-Server Service Discovery • 124\n\nIn pull-based replication, the consumer periodically polls the data source to check if it has new data to consume. In push-based replication, the data source pushes the data to its replicas. (In the next chapter we’ll integrate Raft to our service—and it’s push-based.)\n\nPull-based systems’ flexibility can be great for log and message systems where the consumers and work loads can differ—for example, if you have a client that stream processes its data and runs continuously and you have a client that batch processes its data and runs every twenty-four hours. When repli- cating between servers, we replicate the newest data with as low latency as possible with homogeneous servers, so pull-based and push-based systems behave about the same. But it’ll be easier to write our own pull-based replica- tion that will highlight why we need consensus.\n\nTo add replication to our cluster, we need a replication component that acts as a membership handler handling when a server joins and leaves the cluster. When a server joins the cluster, the component will connect to the server and run a loop that consumes from the discovered server and produces to the local server.\n\nIn the internal/log directory, create a new file named replicator.go to contain our replication code, beginning with this snippet:\n\nServerSideServiceDiscovery/internal/log/replicator.go package log\n\nimport (\n\n\"context\" \"sync\"\n\n\"go.uber.org/zap\" \"google.golang.org/grpc\"\n\napi \"github.com/travisjeffery/proglog/api/v1\"\n\n)\n\ntype Replicator struct {\n\nDialOptions []grpc.DialOption LocalServer api.LogClient\n\nlogger *zap.Logger\n\nmu servers map[string]chan struct{} closed bool close\n\nsync.Mutex\n\nchan struct{}\n\n}\n\nThe replicator connects to other servers with the gRPC client, and we need to configure the client so it can authenticate with the servers. The clientOptions\n\nreport erratum • discuss",
      "content_length": 1916,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 131,
      "content": "Request Discovered Services and Replicate Logs • 125\n\nfield is how we pass in the options to configure the client. The servers field is a map of server addresses to a channel, which the replicator uses to stop replicating from a server when the server fails or leaves the cluster. The replicator calls the produce function to save a copy of the messages it con- sumes from the other servers.\n\nNext, put the following Join() method below the replicator struct:\n\nServerSideServiceDiscovery/internal/log/replicator.go func (r *Replicator) Join(name, addr string) error {\n\nr.mu.Lock() defer r.mu.Unlock() r.init()\n\nif r.closed {\n\nreturn nil\n\n}\n\nif _, ok := r.servers[name]; ok {\n\n// already replicating so skip return nil\n\n} r.servers[name] = make(chan struct{})\n\ngo r.replicate(addr, r.servers[name])\n\nreturn nil\n\n}\n\nThe Join(name, addr string) method adds the given server address to the list of servers to replicate and kicks off the add goroutine to run the actual replication logic.\n\nNow put the replicate(addr string) method, containing the replication logic, below the previous snippet:\n\nServerSideServiceDiscovery/internal/log/replicator.go func (r *Replicator) replicate(addr string, leave chan struct{}) {\n\ncc, err := grpc.Dial(addr, r.DialOptions...) if err != nil {\n\nr.logError(err, \"failed to dial\", addr) return\n\n} defer cc.Close()\n\nclient := api.NewLogClient(cc)\n\nctx := context.Background() stream, err := client.ConsumeStream(ctx,\n\n&api.ConsumeRequest{\n\nOffset: 0,\n\n},\n\n)\n\nreport erratum • discuss",
      "content_length": 1510,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 132,
      "content": "Chapter 7. Server-to-Server Service Discovery • 126\n\nif err != nil {\n\nr.logError(err, \"failed to consume\", addr) return\n\n}\n\nrecords := make(chan *api.Record) go func() {\n\nfor {\n\nrecv, err := stream.Recv() if err != nil {\n\nr.logError(err, \"failed to receive\", addr) return\n\n} records <- recv.Record\n\n}\n\n}()\n\nYou saw most of this code before when we tested our stream consumer and producer. Here we create a client and open up a stream to consume all logs on the server.\n\nAppend the following snippet to finish implementing replicate():\n\nServerSideServiceDiscovery/internal/log/replicator.go\n\nfor {\n\nselect { case <-r.close:\n\nreturn\n\ncase <-leave:\n\nreturn\n\ncase record := <-records:\n\n_, err = r.LocalServer.Produce(ctx, &api.ProduceRequest{\n\nRecord: record,\n\n},\n\n) if err != nil {\n\nr.logError(err, \"failed to produce\", addr) return\n\n}\n\n}\n\n}\n\n}\n\nThe loop consumes the logs from the discovered server in a stream and then produces to the local server to save a copy. We replicate messages from the other server until that server fails or leaves the cluster and the replicator closes the channel for that server, which breaks the loop and ends the replicate() goroutine. The replicator closes the channel when Serf receives an event\n\nreport erratum • discuss",
      "content_length": 1253,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 133,
      "content": "Request Discovered Services and Replicate Logs • 127\n\nsaying that the other server left the cluster, and then this server calls the Leave() method that we’re about to add.\n\nWrite the Leave(name string) method beneath your replicate() method with the fol- lowing code:\n\nServerSideServiceDiscovery/internal/log/replicator.go func (r *Replicator) Leave(name string) error {\n\nr.mu.Lock() defer r.mu.Unlock() r.init() if _, ok := r.servers[name]; !ok {\n\nreturn nil\n\n} close(r.servers[name]) delete(r.servers, name) return nil\n\n}\n\nThis Leave(name string) method handles the server leaving the cluster by removing the server from the list of servers to replicate and closes the server’s associated channel. Closing the channel signals to the receiver in the replicate() goroutine to stop replicating from that server.\n\nNext, add the following init() helper below your Leave() method:\n\nServerSideServiceDiscovery/internal/log/replicator.go func (r *Replicator) init() {\n\nif r.logger == nil {\n\nr.logger = zap.L().Named(\"replicator\")\n\n} if r.servers == nil {\n\nr.servers = make(map[string]chan struct{})\n\n} if r.close == nil {\n\nr.close = make(chan struct{})\n\n}\n\n}\n\nWe use this init() helper to lazily initialize the server map. You should use lazy initialization to give your structs a useful zero value2 because having a useful zero value reduces the API’s size and complexity while maintaining the same functionality. Without a useful zero value, we’d either have to export a repli- cator constructor function for the user to call or export the servers field on the replicator struct for the user to set—making more API for the user to learn and then requiring them to write more code before they can use our struct.\n\n2.\n\nhttps://dave.cheney.net/2013/01/19/what-is-the-zero-value-and-why-is-it-useful\n\nreport erratum • discuss",
      "content_length": 1817,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 134,
      "content": "Chapter 7. Server-to-Server Service Discovery • 128\n\nAppend the following snippet to implement the Close() method:\n\nServerSideServiceDiscovery/internal/log/replicator.go func (r *Replicator) Close() error {\n\nr.mu.Lock() defer r.mu.Unlock() r.init()\n\nif r.closed {\n\nreturn nil\n\n} r.closed = true close(r.close) return nil\n\n}\n\nClose() closes the replicator so it doesn’t replicate new servers that join the cluster and it stops replicating existing servers by causing the replicate() gorou- tines to return.\n\nWe have one last helper to add to handle errors. Add this logError(err error, msg, addr string) method at the bottom of the file:\n\nServerSideServiceDiscovery/internal/log/replicator.go func (r *Replicator) logError(err error, msg, addr string) {\n\nr.logger.Error(\n\nmsg, zap.String(\"addr\", addr), zap.Error(err),\n\n)\n\n}\n\nWith this method, we just log the errors because we have no other use for them and to keep the code short and simple. If your users need access to the errors, a technique you can use to expose these errors is to export an error channel and send the errors into it for your users to receive and handle.\n\nThat’s it for our replicator. In terms of components, we now have our repli- cator, membership, log, and server. Each service instance must set up and connect these components together to work. For simpler, short-running programs, I’ll make a run package that exports a Run() function that’s responsible for running the program. Rob Pike’s Ivy project3 works this way. For more complex, long-running services, I’ll make an agent package that exports an Agent type that manages the different components and processes\n\n3.\n\nhttps://github.com/robpike/ivy\n\nreport erratum • discuss",
      "content_length": 1705,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 135,
      "content": "Request Discovered Services and Replicate Logs • 129\n\nthat make up the service. Hashicorp’s Consul4 works this way. Let’s write an Agent for our service and then test our log, server, membership, and replicator end-to-end.\n\nCreate an internal/agent directory with a file named agent.go inside that begins with this code:\n\nServerSideServiceDiscovery/internal/agent/agent.go package agent\n\nimport (\n\n\"crypto/tls\" \"fmt\" \"net\" \"sync\"\n\n\"go.uber.org/zap\"\n\n\"google.golang.org/grpc\" \"google.golang.org/grpc/credentials\"\n\napi \"github.com/travisjeffery/proglog/api/v1\" \"github.com/travisjeffery/proglog/internal/auth\" \"github.com/travisjeffery/proglog/internal/discovery\" \"github.com/travisjeffery/proglog/internal/log\" \"github.com/travisjeffery/proglog/internal/server\"\n\n)\n\ntype Agent struct {\n\nConfig\n\nlog server membership *discovery.Membership replicator *log.Replicator\n\nlog.Log *grpc.Server\n\nshutdown shutdowns shutdownLock sync.Mutex\n\nbool chan struct{}\n\n}\n\nAn Agent runs on every service instance, setting up and connecting all the different components. The struct references each component (log, server, membership, replicator) that the Agent manages.\n\nAfter the Agent, add its Config struct:\n\nServerSideServiceDiscovery/internal/agent/agent.go type Config struct {\n\nServerTLSConfig *tls.Config *tls.Config PeerTLSConfig\n\n4.\n\nhttps://github.com/hashicorp/consul\n\nreport erratum • discuss",
      "content_length": 1386,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 136,
      "content": "Chapter 7. Server-to-Server Service Discovery • 130\n\nDataDir BindAddr RPCPort NodeName StartJoinAddrs []string ACLModelFile ACLPolicyFile\n\nstring string int string\n\nstring string\n\n}\n\nfunc (c Config) RPCAddr() (string, error) {\n\nhost, _, err := net.SplitHostPort(c.BindAddr) if err != nil {\n\nreturn \"\", err\n\n} return fmt.Sprintf(\"%s:%d\", host, c.RPCPort), nil\n\n}\n\nThe Agent sets up the components so its Config comprises the components’ parameters to pass them through to the components.\n\nBelow Config, place this Agent creator function:\n\nServerSideServiceDiscovery/internal/agent/agent.go func New(config Config) (*Agent, error) {\n\na := &Agent{\n\nConfig: shutdowns: make(chan struct{}),\n\nconfig,\n\n} setup := []func() error{\n\na.setupLogger, a.setupLog, a.setupServer, a.setupMembership,\n\n} for _, fn := range setup {\n\nif err := fn(); err != nil {\n\nreturn nil, err\n\n}\n\n} return a, nil\n\n}\n\nNew(Config) creates an Agent and runs a set of methods to set up and run the agent’s components. After we run New(), we expect to have a running, function- ing service. We’ve seen most of these setup codes before when testing our components, so we’ll cover them quickly.\n\nreport erratum • discuss",
      "content_length": 1182,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 137,
      "content": "Request Discovered Services and Replicate Logs • 131\n\nFirst, set up the logger with this setupLogger() method. Put setupLogger() under New():\n\nServerSideServiceDiscovery/internal/agent/agent.go func (a *Agent) setupLogger() error {\n\nlogger, err := zap.NewDevelopment() if err != nil {\n\nreturn err\n\n} zap.ReplaceGlobals(logger) return nil\n\n}\n\nThen, we set up the log with this setupLog() method. Put setupLog() under the previous snippet:\n\nServerSideServiceDiscovery/internal/agent/agent.go func (a *Agent) setupLog() error {\n\nvar err error a.log, err = log.NewLog(\n\na.Config.DataDir, log.Config{},\n\n) return err\n\n}\n\nNow we set up the server with setupServer(). Add setupServer() after setupLog():\n\nServerSideServiceDiscovery/internal/agent/agent.go func (a *Agent) setupServer() error {\n\nauthorizer := auth.New(\n\na.Config.ACLModelFile, a.Config.ACLPolicyFile,\n\n) serverConfig := &server.Config{\n\nCommitLog: a.log, Authorizer: authorizer,\n\n} var opts []grpc.ServerOption if a.Config.ServerTLSConfig != nil {\n\ncreds := credentials.NewTLS(a.Config.ServerTLSConfig) opts = append(opts, grpc.Creds(creds))\n\n} var err error a.server, err = server.NewGRPCServer(serverConfig, opts...) if err != nil {\n\nreturn err\n\n} rpcAddr, err := a.RPCAddr() if err != nil {\n\nreturn err\n\n}\n\nreport erratum • discuss",
      "content_length": 1293,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 138,
      "content": "Chapter 7. Server-to-Server Service Discovery • 132\n\nln, err := net.Listen(\"tcp\", rpcAddr) if err != nil {\n\nreturn err\n\n} go func() {\n\nif err := a.server.Serve(ln); err != nil {\n\n_ = a.Shutdown()\n\n}\n\n}() return err\n\n}\n\nThen we set up the membership with setupMembership(). Place setupMembership() after setupServer():\n\nServerSideServiceDiscovery/internal/agent/agent.go func (a *Agent) setupMembership() error {\n\nrpcAddr, err := a.Config.RPCAddr() if err != nil {\n\nreturn err\n\n} var opts []grpc.DialOption if a.Config.PeerTLSConfig != nil {\n\nopts = append(opts, grpc.WithTransportCredentials(\n\ncredentials.NewTLS(a.Config.PeerTLSConfig),\n\n), )\n\n} conn, err := grpc.Dial(rpcAddr, opts...) if err != nil {\n\nreturn err\n\n} client := api.NewLogClient(conn) a.replicator = &log.Replicator{\n\nDialOptions: opts, LocalServer: client,\n\n} a.membership, err = discovery.New(a.replicator, discovery.Config{\n\nNodeName: a.Config.NodeName, BindAddr: a.Config.BindAddr, Tags: map[string]string{\n\n\"rpc_addr\": rpcAddr,\n\n}, StartJoinAddrs: a.Config.StartJoinAddrs,\n\n}) return err\n\n}\n\nsetupMembership() sets up a Replicator with the gRPC dial options needed to connect to other servers and a client so the replicator can connect to other servers,\n\nreport erratum • discuss",
      "content_length": 1251,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 139,
      "content": "Request Discovered Services and Replicate Logs • 133\n\nconsume their data, and produce a copy of the data to the local server. Then we create a Membership passing in the replicator and its handler to notify the replicator when servers join and leave the cluster.\n\nThat’s all of the agent’s setup code. If we call New() now, we’d have a running agent. At some point we’ll want to shut down the agent, so put this Shutdown() method at the bottom of the file:\n\nServerSideServiceDiscovery/internal/agent/agent.go func (a *Agent) Shutdown() error {\n\na.shutdownLock.Lock() defer a.shutdownLock.Unlock() if a.shutdown {\n\nreturn nil\n\n} a.shutdown = true close(a.shutdowns)\n\nshutdown := []func() error{ a.membership.Leave, a.replicator.Close, func() error {\n\na.server.GracefulStop() return nil\n\n}, a.log.Close,\n\n} for _, fn := range shutdown {\n\nif err := fn(); err != nil {\n\nreturn err\n\n}\n\n} return nil\n\n}\n\nThis ensures that the agent will shut down once even if people call Shutdown() multiple times. Then we shut down the agent and its components by:\n\nLeaving the membership so that other servers will see that this server has left the cluster and so that this server doesn’t receive discovery events anymore;\n\nClosing the replicator so it doesn’t continue to replicate;\n\nGracefully stopping the server, which stops the server from accepting new connections and blocks until all the pending RPCs have finished; and\n\nClosing the log.\n\nreport erratum • discuss",
      "content_length": 1450,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 140,
      "content": "Chapter 7. Server-to-Server Service Discovery • 134\n\nWe’ve implemented Serf into our service, so we can now run multiple instances of our service that discover and then replicate each other’s data. Let’s write a test to check that our service discovery and replication works and to prevent us from introducing a regression when we build consensus in Chapter 8, Coordinate Your Services with Consensus, on page 141.\n\nTest Discovery and the Service End-to-End\n\nLet’s test that our service discovery and replication works in an end-to-end test. We’ll set up a cluster with three nodes. We’ll produce a record to one server and verify that we can consume the message from the other servers that have (hopefully) replicated for us.\n\nIn internal/agent, create an agent_test.go file, beginning with this snippet:\n\nServerSideServiceDiscovery/internal/agent/agent_test.go package agent_test\n\nimport (\n\n\"context\" \"crypto/tls\" \"fmt\" \"io/ioutil\" \"os\" \"testing\" \"time\"\n\n\"github.com/stretchr/testify/require\" \"github.com/travisjeffery/go-dynaport\" \"google.golang.org/grpc\" \"google.golang.org/grpc/credentials\"\n\napi \"github.com/travisjeffery/proglog/api/v1\" \"github.com/travisjeffery/proglog/internal/agent\" \"github.com/travisjeffery/proglog/internal/config\"\n\n)\n\nWhat can I say? Our end-to-end test has a lot going on and requires a lot of imports to make it happen.\n\nNow we can write the test beginning with this code:\n\nServerSideServiceDiscovery/internal/agent/agent_test.go func TestAgent(t *testing.T) {\n\nserverTLSConfig, err := config.SetupTLSConfig(config.TLSConfig{\n\nCertFile: KeyFile: CAFile: Server: ServerAddress: \"127.0.0.1\",\n\nconfig.ServerCertFile, config.ServerKeyFile, config.CAFile, true,\n\n})\n\nreport erratum • discuss",
      "content_length": 1718,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 141,
      "content": "Test Discovery and the Service End-to-End • 135\n\nrequire.NoError(t, err)\n\npeerTLSConfig, err := config.SetupTLSConfig(config.TLSConfig{\n\nCertFile: KeyFile: CAFile: Server: ServerAddress: \"127.0.0.1\",\n\nconfig.RootClientCertFile, config.RootClientKeyFile, config.CAFile, false,\n\n}) require.NoError(t, err)\n\nThis snippet defines the certificate configurations used in our test to test our security. The serverTLSConfig defines the configuration of the certificate that’s served to clients. And the peerTLSConfig defines the configuration of the certificate that’s served between servers so they can connect with and replicate each other.\n\nNow set up the cluster by placing this code after the previous snippet:\n\nServerSideServiceDiscovery/internal/agent/agent_test.go var agents []*agent.Agent for i := 0; i < 3; i++ {\n\nports := dynaport.Get(2) bindAddr := fmt.Sprintf(\"%s:%d\", \"127.0.0.1\", ports[0]) rpcPort := ports[1]\n\ndataDir, err := ioutil.TempDir(\"\", \"agent-test-log\") require.NoError(t, err)\n\nvar startJoinAddrs []string if i != 0 {\n\nstartJoinAddrs = append(\n\nstartJoinAddrs, agents[0].Config.BindAddr,\n\n)\n\n}\n\nagent, err := agent.New(agent.Config{\n\nNodeName: StartJoinAddrs: startJoinAddrs, BindAddr: RPCPort: DataDir: ACLModelFile: ACLPolicyFile: ServerTLSConfig: serverTLSConfig, PeerTLSConfig:\n\nfmt.Sprintf(\"%d\", i),\n\nbindAddr, rpcPort, dataDir, config.ACLModelFile, config.ACLPolicyFile,\n\npeerTLSConfig,\n\n}) require.NoError(t, err)\n\nagents = append(agents, agent)\n\n} defer func() {\n\nreport erratum • discuss",
      "content_length": 1515,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 142,
      "content": "Chapter 7. Server-to-Server Service Discovery • 136\n\nfor _, agent := range agents {\n\nerr := agent.Shutdown() require.NoError(t, err) require.NoError(t,\n\nos.RemoveAll(agent.Config.DataDir),\n\n)\n\n}\n\n}() time.Sleep(3 * time.Second)\n\nThis code sets up a three-node cluster. The second and third nodes join the first node’s cluster.\n\nBecause we now have two addresses to configure in our service (the RPC address and the Serf address), and because we run our tests on a single host, we need two ports. We used the 0 port trick in Test a gRPC Server and Client, on page 68, to get a port automatically assigned to a listener by net.Listen,5 but now we just want the port—with no listener—so we use the dynaport library to allocate the two ports we need: one for our gRPC log connections and one for our Serf service discovery connections.\n\nWe defer a function call that runs after the test to verify that the agents suc- cessfully shut down and to delete the test data. We make the test sleep for a few seconds to give the nodes time to discover each other.\n\nNow that we have a cluster, we can test it works. Put this code after the pre- vious snippet:\n\nServerSideServiceDiscovery/internal/agent/agent_test.go leaderClient := client(t, agents[0], peerTLSConfig) produceResponse, err := leaderClient.Produce(\n\ncontext.Background(), &api.ProduceRequest{\n\nRecord: &api.Record{\n\nValue: []byte(\"foo\"),\n\n},\n\n},\n\n) require.NoError(t, err) consumeResponse, err := leaderClient.Consume(\n\ncontext.Background(), &api.ConsumeRequest{\n\nOffset: produceResponse.Offset,\n\n},\n\n) require.NoError(t, err) require.Equal(t, consumeResponse.Record.Value, []byte(\"foo\"))\n\n5.\n\nhttps://golang.org/pkg/net/#Listen\n\nreport erratum • discuss",
      "content_length": 1706,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 143,
      "content": "Test Discovery and the Service End-to-End • 137\n\nThis code is the same as our testProduceConsume() test case in Test a gRPC Server and Client, on page 68: it checks that we can produce to and consume from a single node. Now we need to check that another node replicated the record. We do that by adding this code to the test, below the previous snippet:\n\nServerSideServiceDiscovery/internal/agent/agent_test.go\n\n// wait until replication has finished time.Sleep(3 * time.Second)\n\nfollowerClient := client(t, agents[1], peerTLSConfig) consumeResponse, err = followerClient.Consume(\n\ncontext.Background(), &api.ConsumeRequest{\n\nOffset: produceResponse.Offset,\n\n},\n\n) require.NoError(t, err) require.Equal(t, consumeResponse.Record.Value, []byte(\"foo\"))\n\n}\n\nBecause our replication works asynchronously across servers, the logs pro- duced to one server won’t be immediately available on the replica servers. This process causes latency between when the message is produced to the first server and when it’s replicated to the second. The stupid, simple6 way to fix this (especially since we’re black-box testing7) is to add a big enough delay in the test for the replicator to have replicated the message, but as small a delay as possible to keep our tests fast. Then we check that we can consume the replicated message.\n\nToo Much Sleep Will Make Your Tests Too Slow\n\nIf we had enough test cases that needed a delay like this, eventu- ally our tests would be slow and annoying to run, in which case we’d want to use a different technique. For example, you could retry your test’s assertion in a loop with a small delay between iterations and timeout after a few seconds. Or you could have your server expose an event channel that included when the server produced a message. Then you’d wait to receive an event on that channel in your test so your test blocked and then continued the instant the second server replicated the message.\n\n6. 7.\n\nhttps://en.wikipedia.org/wiki/KISS_principle\n\nhttps://en.wikipedia.org/wiki/Black-box_testing\n\nreport erratum • discuss",
      "content_length": 2057,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 144,
      "content": "Chapter 7. Server-to-Server Service Discovery • 138\n\nLastly, we need to add our client() helper that sets up a client for the service:\n\nServerSideServiceDiscovery/internal/agent/agent_test.go func client(\n\nt *testing.T, agent *agent.Agent, tlsConfig *tls.Config,\n\n) api.LogClient {\n\ntlsCreds := credentials.NewTLS(tlsConfig) opts := []grpc.DialOption{grpc.WithTransportCredentials(tlsCreds)} rpcAddr, err := agent.Config.RPCAddr() require.NoError(t, err) conn, err := grpc.Dial(fmt.Sprintf(\n\n\"%s\", rpcAddr,\n\n), opts...) require.NoError(t, err) client := api.NewLogClient(conn) return client\n\n}\n\nNow, run your tests with $ make test. If all is well, your tests pass and you’ve officially made a distributed service that can replicate data. Congrats!\n\nWhat You Learned\n\nNow when our servers discover other servers, they replicate each other’s data. That’s a problem with our replication implementation: when one server dis- covers another, they replicate each other in a cycle! You can verify it by adding this code at the bottom of your test:\n\nconsumeResponse, err = leaderClient.Consume(\n\ncontext.Background(), &api.ConsumeRequest{\n\nOffset: produceResponse.Offset + 1,\n\n},\n\n) require.Nil(t, consumeResponse) require.Error(t, err) got := grpc.Code(err) want := grpc.Code(api.ErrOffsetOutOfRange{}.GRPCStatus().Err()) require.Equal(t, got, want)\n\nWe only produced one record to our service, and yet we’re able to consume multiple records from the original server because it’s replicated data from another server that replicated its data from the original server. No, Leo, we do not need to go deeper.\n\nreport erratum • discuss",
      "content_length": 1624,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 145,
      "content": "What You Learned • 139\n\nI mentioned that in the next chapter we’ll work on coordinating the servers so that they’d have a defined leader-follower relationship so that only the followers would replicate the leader. We also want to control the number of replicas. Typically in a production deployment, three replicas is ideal: you could lose two and still not lose data, and with only three you won’t be storing more data than necessary.\n\nSo let’s work on building consensus with Raft and coordinating the nodes in our cluster.\n\nreport erratum • discuss",
      "content_length": 551,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 146,
      "content": "CHAPTER 8\n\nCoordinate Your Services with Consensus\n\nDistributed services are like commercial kitchens. Imagine a small restaurant opens up with one stove and one cook. Patrons discover the restaurant and tell their friends, and business is booming. But the kitchen struggles with the mob of customers and sometimes the stove breaks, forcing the restaurant to close for the night and lose business. So the restaurant hires two more cooks and buys two more stoves. The cooks keep up with orders now but they make mistakes: they mix up appetizers and entrees; they mix up tables; they make double of one order while forgetting to make another. They lack coordi- nation. So the kitchen hires a chef to oversee and coordinate the kitchen. When an order comes in, the chef divides the order and assigns the appetizers, entrees, and deserts to the cooks who prepare the food timely and correctly. The patrons love the fast, quality service, and the kitchen becomes world- renowned.\n\nIn this chapter, we look at the chef of distributed services: consensus. Con- sensus algorithms are tools used to get distributed services to agree on shared state even in the face of failures. In Request Discovered Services and Replicate Logs, on page 123, we naively implemented replication in our service, and the servers replicate each other in a cycle, making infinite copies of the same data. We need to put the servers in leader and follower relationships where the followers replicate the leader’s data. We’ll do just that in this chapter using Raft for leader election and replication.\n\nWhat Is Raft and How Does It Work?\n\nRaft is a distributed consensus algorithm designed to be easily understood and implemented. It’s the consensus algorithm behind services like Etcd—the distributed key-value store that backs Kubernetes, Consul, and soon Kafka,\n\nreport erratum • discuss",
      "content_length": 1859,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 147,
      "content": "Chapter 8. Coordinate Your Services with Consensus • 142\n\nwhose team is migrating from ZooKeeper to Raft.1 Because Raft is easy to understand and implement, developers have written many quality Raft libraries used in many projects and it’s become the most widely deployed consensus algorithm today.\n\nLet’s talk about Raft’s leader election first and then talk about its replication, and that’ll transition into coding replication in our service.\n\nLeader Election\n\nA Raft cluster has one leader and the rest of the servers are followers. The leader maintains power by sending heartbeat requests to its followers, effec- tively saying: “I’m still here and I’m still the boss.” If the follower times out waiting for a heartbeat request from the leader, then the follower becomes a candidate and begins an election to decide the next leader. The candidate votes for itself and then requests votes from the followers. “The boss is gone! I’m the new boss, right?” If the candidate receives a majority of the votes, it becomes the leader, and it sends heartbeat requests to the followers to establish authority: “Hey y’all, new boss here.”\n\nFollowers can become candidates simultaneously if they time out at the same time waiting for the leader’s heartbeats. They’ll hold their own elections and the elections might not result in a new leader because of vote splitting. So they’ll hold another election. Candidates will hold elections until there’s a winner that becomes the new leader.\n\nEvery Raft server has a term: a monotonically increasing integer that tells other servers how authoritative and current this server is. The servers’ terms act as a logical clock: a way to capture chronological and causal relationships in distributed systems, where real-time clocks are untrustworthy and unim- portant. Each time a candidate begins an election, it increments its term. If the candidate wins the election and becomes the leader, the followers update their terms to match and the terms don’t change until the next election. Servers vote once per term for the first candidate that requests votes, as long as the candidate’s term is greater than the voters’. These conditions help prevent vote splits and ensure the voters elect an up-to-date leader.\n\nDepending on your use case, you might use Raft just for leader election. Imagine you’ve built a job system with a database of jobs to run and a program that queries the database every second to check if there’s a job to run and, if so, runs the job. You want this system to be highly available and resilient\n\n1.\n\nhttps://cwiki.apache.org/confluence/display/KAFKA/KIP-500:+Replace+ZooKeeper+with+a+Self-Managed+Meta-\n\ndata+Quorum\n\nreport erratum • discuss",
      "content_length": 2699,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 148,
      "content": "What Is Raft and How Does It Work? • 143\n\nto failures, so you run multiple instances of the job runner. But you don’t want all of the runners running simultaneously and duplicating the work. So you use Raft to elect a leader; only the leader runs the jobs, and if the leader fails, Raft elects a new leader that runs the jobs. Most use cases rely on Raft for both its leader election and replication to get consensus on state.\n\nRaft’s leader election can be useful by itself, but usually the point is to elect a leader that’s responsible for replicating a log to its followers and doing something with the log data. Raft breaks consensus into two parts: leader election and log replication. Let’s talk about how Raft’s replication works.\n\nLog Replication\n\nThe leader accepts client requests, each of which represents some command to run across the cluster. (In a key-value service for example, you’d have a command to assign a key’s value.) For each request, the leader appends the command to its log and then requests its followers to append the command to their logs. After a majority of followers have replicated the command—when the leader considers the command committed—the leader executes the com- mand with a finite-state machine and responds to the client with the result. The leader tracks the highest committed offset and sends this in the requests to its followers. When a follower receives a request, it executes all commands up to the highest committed offset with its finite-state machine. All Raft servers run the same finite-state machine that defines how to handle each command.\n\nReplication saves us from losing data when servers fail. There’s a cost-benefit to replication. Like any insurance, replication costs (in complexity, in network bandwidth, in data storage), but the benefit of having replicated data to handle when a server fails makes it worth paying for the time the servers work. A Raft leader replicates to most of its followers, assuring that we won’t lose data unless a majority of the followers fail.\n\nThe recommended number of servers in a Raft cluster is three and five. A Raft cluster of three servers will tolerate a single server failure while a cluster of five will tolerate two server failures. I recommend odd number cluster sizes because Raft will handle (N–1)/2 failures, where N is the size of your cluster. If you had a cluster with four servers, it would handle losing one server, the same as a cluster with three servers—so you’d pay for an extra server that didn’t increase your fault tolerance. For larger clusters, CockRoachDB wrote a layer on top of Raft called MultiRaft2 that divides the database’s data into\n\n2.\n\nhttps://www.cockroachlabs.com/blog/scaling-raft\n\nreport erratum • discuss",
      "content_length": 2744,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 149,
      "content": "Chapter 8. Coordinate Your Services with Consensus • 144\n\nranges, each with its own consensus group. To keep our project simple, we’ll have a single Raft cluster.\n\nOur service’s use case is unique because replicating a log is our end goal. Raft’s algorithm replicates a log, and we could defer all log management to Raft’s internals. This would make our service efficient and easy to code, but wouldn’t teach you how to use Raft to build distributed services that aren’t distributed logs.\n\nIn other services, you’ll use Raft as a means to replicate a log of commands and then execute those commands with state machines. If you were building a distributed SQL database, you’d replicate and execute the insert and update SQL commands; if you were building a key-value store, you’d replicate and execute set commands. Because other services you build will replicate a log as a means rather than an end, we’ll build our service the way you would other types of service, by replicating the transformation commands—which in our service are append commands. Technically we’ll replicate two logs: the log containing Raft’s commands and the log that results from the finite-state machines applying those commands. This service may not be as optimized as it could be, but what you’ll learn will be more useful for when you build other services.\n\nImplement Raft in Our Service\n\nWe have a log that can write and read records on one computer. We want a distributed log that’s replicated on multiple computers, so let’s implement Raft in our service to get that.\n\nInstall Raft by running this command:\n\n$ go get github.com/hashicorp/raft@v1.1.1 $ # use etcd's fork of Ben Johnson's Bolt key/value store, $ # which includes fixes for Go 1.14+ $ go mod edit -replace github.com/hashicorp/raft-boltdb=\\ github.com/travisjeffery/raft-boltdb@v1.0.0\n\nIn the internal/log directory, create a distributed.go file, beginning with this snippet:\n\nCoordinateWithConsensus/internal/log/distributed.go package log\n\nimport (\n\n\"bytes\" \"crypto/tls\" \"fmt\" \"io\" \"net\" \"os\"\n\nreport erratum • discuss",
      "content_length": 2065,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 150,
      "content": "Implement Raft in Our Service • 145\n\n\"path/filepath\" \"time\"\n\nraftboltdb \"github.com/hashicorp/raft-boltdb\" \"google.golang.org/protobuf/proto\"\n\n\"github.com/hashicorp/raft\"\n\napi \"github.com/travisjeffery/proglog/api/v1\"\n\n)\n\ntype DistributedLog struct { config Config log raft\n\nLog *raft.Raft\n\n}\n\nfunc NewDistributedLog(dataDir string, config Config) (\n\nDistributedLog, error,\n\n) {\n\nl := &DistributedLog{\n\nconfig: config,\n\n} if err := l.setupLog(dataDir); err != nil {\n\nreturn nil, err\n\n} if err := l.setupRaft(dataDir); err != nil {\n\nreturn nil, err\n\n} return l, nil\n\n}\n\nThis code defines our distributed log type and a function to create the log. The function defers the logic to the setup methods we’ll write shortly. The log package will contain the single-server, non-replicated log we wrote earlier, and the distributed, replicated log built with Raft.\n\nWrite this setupLog() method under NewDistributedLog():\n\nCoordinateWithConsensus/internal/log/distributed.go func (l *DistributedLog) setupLog(dataDir string) error {\n\nlogDir := filepath.Join(dataDir, \"log\") if err := os.MkdirAll(logDir, 0755); err != nil {\n\nreturn err\n\n} var err error l.log, err = NewLog(logDir, l.config) return err\n\n}\n\nsetupLog(dataDir string) creates the log for this server, where this server will store the user’s records.\n\nreport erratum • discuss",
      "content_length": 1329,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 151,
      "content": "Chapter 8. Coordinate Your Services with Consensus • 146\n\nSet Up Raft\n\nA Raft instance comprises:\n\nA finite-state machine that applies the commands you give Raft;\n\nA log store where Raft stores those commands;\n\nA stable store where Raft stores the cluster’s configuration—the servers\n\nin the cluster, their addresses, and so on;\n\nA snapshot store where Raft stores compact snapshots of its data; and\n\nA transport that Raft uses to connect with the server’s peers.\n\nWe must set these up to create a Raft instance. Below setupLog(), add this setupRaft() method:\n\nCoordinateWithConsensus/internal/log/distributed.go func (l *DistributedLog) setupRaft(dataDir string) error {\n\nfsm := &fsm{log: l.log}\n\nlogDir := filepath.Join(dataDir, \"raft\", \"log\") if err := os.MkdirAll(logDir, 0755); err != nil {\n\nreturn err\n\n} logConfig := l.config logConfig.Segment.InitialOffset = 1 logStore, err := newLogStore(logDir, logConfig) if err != nil {\n\nreturn err\n\n}\n\nsetupRaft(dataDir string) configures and creates the server’s Raft instance.\n\nWe begin by creating our finite-state machine (FSM) that we’ll implement later in this file.\n\nThen we create Raft’s log store, and we use our own log we wrote in Code the Store, on page 26! We configure our log’s initial offset to 1, as required by Raft. Raft needs a specific log interface satisfied, so we’ll wrap our log to provide those APIs (we’ll write that wrapper shortly):\n\nCoordinateWithConsensus/internal/log/distributed.go stableStore, err := raftboltdb.NewBoltStore(\n\nfilepath.Join(dataDir, \"raft\", \"stable\"),\n\n) if err != nil {\n\nreturn err\n\n}\n\nretain := 1\n\nreport erratum • discuss",
      "content_length": 1622,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 152,
      "content": "Implement Raft in Our Service • 147\n\nsnapshotStore, err := raft.NewFileSnapshotStore( filepath.Join(dataDir, \"raft\"), retain, os.Stderr,\n\n) if err != nil {\n\nreturn err\n\n}\n\nmaxPool := 5 timeout := 10 * time.Second transport := raft.NewNetworkTransport(\n\nl.config.Raft.StreamLayer, maxPool, timeout, os.Stderr,\n\n)\n\nThe stable store is a key-value store where Raft stores important metadata, like the server’s current term or the candidate the server voted for. Bolt3 is an embedded and persisted key-value database for Go we’ve used as our stable store.\n\nThen we set up Raft’s snapshot store. Raft snapshots to recover and restore data efficiently, when necessary, like if your server’s EC2 instance failed and an autoscaling group brought up another instance for the Raft server. Rather than streaming all the data from the Raft leader, the new server would restore from the snapshot (which you could store in S3 or a similar storage service) and then get the latest changes from the leader. This is more efficient and less taxing on the leader. You want to snapshot frequently to minimize the difference between the data in the snapshots and on the leader. The retain variable specifies that we’ll keep one snapshot.\n\nWe create our transport that wraps a stream layer—a low-level stream abstraction (we’ll write our own stream layer implementation in Stream Layer, on page 156):\n\nCoordinateWithConsensus/internal/log/distributed.go config := raft.DefaultConfig() config.LocalID = l.config.Raft.LocalID if l.config.Raft.HeartbeatTimeout != 0 {\n\nconfig.HeartbeatTimeout = l.config.Raft.HeartbeatTimeout\n\n} if l.config.Raft.ElectionTimeout != 0 {\n\nconfig.ElectionTimeout = l.config.Raft.ElectionTimeout\n\n}\n\n3.\n\nhttps://github.com/boltdb/bolt\n\nreport erratum • discuss",
      "content_length": 1764,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 153,
      "content": "Chapter 8. Coordinate Your Services with Consensus • 148\n\nif l.config.Raft.LeaderLeaseTimeout != 0 {\n\nconfig.LeaderLeaseTimeout = l.config.Raft.LeaderLeaseTimeout\n\n} if l.config.Raft.CommitTimeout != 0 {\n\nconfig.CommitTimeout = l.config.Raft.CommitTimeout\n\n}\n\nThe config’s LocalID field is the unique ID for this server, and it’s the only config field we must set; the rest are optional, and in normal operation the default config should be fine.\n\nTo make our tests faster, we support overriding a handful of timeout configs to speed up Raft. For example, when we shut down the leader, we want the election to finish within a second, whereas in production you’d need a longer timeout to handle networking latency.\n\nAdd the following code to create the Raft instance and bootstrap the cluster:\n\nCoordinateWithConsensus/internal/log/distributed.go\n\nl.raft, err = raft.NewRaft(\n\nconfig, fsm, logStore, stableStore, snapshotStore, transport,\n\n) if err != nil {\n\nreturn err\n\n} hasState, err := raft.HasExistingState(\n\nlogStore, stableStore, snapshotStore,\n\n) if err != nil {\n\nreturn err\n\n} if l.config.Raft.Bootstrap && !hasState {\n\nconfig := raft.Configuration{\n\nServers: []raft.Server{{\n\nID: Address: transport.LocalAddr(),\n\nconfig.LocalID,\n\n}},\n\n} err = l.raft.BootstrapCluster(config).Error()\n\n} return err\n\n}\n\nreport erratum • discuss",
      "content_length": 1334,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 154,
      "content": "Implement Raft in Our Service • 149\n\nTo support configuring Raft, add these highlighted lines to your log’s Config struct in internal/log/config.go:\n\nCoordinateWithConsensus/internal/log/config.go package log\n\n➤ ➤ )➤\n\nimport (\n\n\"github.com/hashicorp/raft\"\n\ntype Config struct {\n\n➤ ➤ ➤ ➤ ➤\n\nRaft struct {\n\nraft.Config StreamLayer *StreamLayer Bootstrap\n\nbool\n\n} Segment struct {\n\nMaxStoreBytes uint64 MaxIndexBytes uint64 InitialOffset uint64\n\n}\n\n}\n\nGenerally you’ll bootstrap a server configured with itself as the only voter, wait until it becomes the leader, and then tell the leader to add more servers to the cluster. The subsequently added servers don’t bootstrap. That concludes our Raft setup. Let’s continue building our DistributedLog.\n\nLog API\n\nWe’ve written the code to set up a DistributedLog; next we’ll write its public APIs that append records to and read records from the log and wrap Raft. The Dis- tributedLog will have the same API as the Log type to make them interchangeable.\n\nAdd this Append() method below setupRaft():\n\nCoordinateWithConsensus/internal/log/distributed.go func (l *DistributedLog) Append(record *api.Record) (uint64, error) {\n\nres, err := l.apply(\n\nAppendRequestType, &api.ProduceRequest{Record: record},\n\n) if err != nil {\n\nreturn 0, err\n\n} return res.(*api.ProduceResponse).Offset, nil\n\n}\n\nAppend(record*api.Record) appends the record to the log. Unlike in Code the Store, on page 26, where we appended the record directly to this server’s log, we tell\n\nreport erratum • discuss",
      "content_length": 1519,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 155,
      "content": "Chapter 8. Coordinate Your Services with Consensus • 150\n\nRaft to apply a command (we’ve reused for the ProduceRequest for the command) that tells the FSM to append the record to the log. Raft runs the process described in Log Replication, on page 143, to replicate the command to a majority of the Raft servers and ultimately append the record to a majority of Raft servers.\n\nPut this apply() method below Apply():\n\nCoordinateWithConsensus/internal/log/distributed.go func (l *DistributedLog) apply(reqType RequestType, req proto.Message) (\n\ninterface{}, error,\n\n) {\n\nvar buf bytes.Buffer _, err := buf.Write([]byte{byte(reqType)}) if err != nil {\n\nreturn nil, err\n\n} b, err := proto.Marshal(req) if err != nil {\n\nreturn nil, err\n\n} _, err = buf.Write(b) if err != nil {\n\nreturn nil, err\n\n} timeout := 10 * time.Second future := l.raft.Apply(buf.Bytes(), timeout) if future.Error() != nil {\n\nreturn nil, future.Error()\n\n} res := future.Response() if err, ok := res.(error); ok { return nil, err\n\n} return res, nil\n\n}\n\napply(reqType RequestType, req proto.Marshaler) wraps Raft’s API to apply requests and return their responses. Even though we have only one request type, the append request type, I’ve written things that easily support multiple request types to show how you would set up your own services when you have different requests. In apply(), we marshal the request type and request into bytes that Raft uses as the record’s data it replicates. The l.raft.Apply(buf.Bytes(), timeout) call has a lot going on behind the scenes, running the steps described in Log Replication, on page 143, to replicate the record and append the record to the leader’s log.\n\nreport erratum • discuss",
      "content_length": 1691,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 156,
      "content": "Implement Raft in Our Service • 151\n\nThe future.Error() API returns an error when something went wrong with Raft’s replication. For example, it took too long for Raft to process the command or the server had to shutdown—the future.Error() API doesn’t return your service’s errors. The future.Response() API returns what your FSM’s Apply() method returned and, opposed to Go’s convention of using Go’s multiple return values to sep- arate errors, you must return a single value for Raft. In our apply() method we check whether the value is an error with a type assertion.\n\nPut this Read() method below apply():\n\nCoordinateWithConsensus/internal/log/distributed.go func (l *DistributedLog) Read(offset uint64) (*api.Record, error) {\n\nreturn l.log.Read(offset)\n\n}\n\nRead(offset uint64) reads the record for the offset from the server’s log. When you’re okay with relaxed consistency, read operations need not go through Raft. When you need strong consistency, where reads must be up-to-date with writes, then you must go through Raft, but then reads are less efficient and take longer.\n\nFinite-State Machine\n\nRaft defers the running of your business logic to the FSM. After the previous snippet, define your fsm type with this code:\n\nCoordinateWithConsensus/internal/log/distributed.go var _ raft.FSM = (*fsm)(nil)\n\ntype fsm struct {\n\nlog *Log\n\n}\n\nThe FSM must access the data it manages. In our service, that’s a log, and the FSM appends records to the log. If you were writing a key-value service, then your FSM would update the store of your data: an int, a map, Postgres —whatever store you’ve used.\n\nYour FSM must implement three methods:\n\nApply(record *raft.Log)—Raft invokes this method after committing a log entry.\n\nSnapshot()—Raft periodically calls this method to snapshot its state. For most services, you’ll be able to build a compacted log—for example, if we were building a key-value store and we had a bunch of commands saying “set foo to bar,” “set foo to baz,” “set foo to qux,” and so on, we would only set the latest command to restore the current state. Because we’re repli- cating a log itself, we need the full log to restore it.\n\nreport erratum • discuss",
      "content_length": 2174,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 157,
      "content": "Chapter 8. Coordinate Your Services with Consensus • 152\n\nRestore(io.ReadCloser)—Raft calls this to restore an FSM from a snapshot—for instance, if an EC2 instance failed and a new instance took its place.\n\nPut this code below the fsm type to implement Apply():\n\nCoordinateWithConsensus/internal/log/distributed.go type RequestType uint8\n\nconst (\n\nAppendRequestType RequestType = 0\n\n)\n\nfunc (l *fsm) Apply(record *raft.Log) interface{} {\n\nbuf := record.Data reqType := RequestType(buf[0]) switch reqType { case AppendRequestType:\n\nreturn l.applyAppend(buf[1:])\n\n} return nil\n\n}\n\nfunc (l *fsm) applyAppend(b []byte) interface{} {\n\nvar req api.ProduceRequest err := proto.Unmarshal(b, &req) if err != nil {\n\nreturn err\n\n} offset, err := l.log.Append(req.Record) if err != nil {\n\nreturn err\n\n} return &api.ProduceResponse{Offset: offset}\n\n}\n\nAs I mentioned earlier, even though our service has only one command to replicate, I want to develop things to support multiple commands and show you how to do it for your own projects. So in this snippet, we make our own request type and define our append request type. When we send a request to Raft for it to apply, and when we read the request in the FSM’s Apply() method to apply it, these request types identify the request and tell us how to handle it. In Apply(), we switch on the request type and call the corresponding method containing the logic to run the command. In applyAppend([]byte), we unmarshal the request and then append the record to the local log and return the response for Raft to send back to where we called raft.Apply() in Distributed- Log.Append().\n\nreport erratum • discuss",
      "content_length": 1642,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 158,
      "content": "Implement Raft in Our Service • 153\n\nBelow applyAppend(), put this snippet to support snapshots:\n\nCoordinateWithConsensus/internal/log/distributed.go func (f *fsm) Snapshot() (raft.FSMSnapshot, error) {\n\nr := f.log.Reader() return &snapshot{reader: r}, nil\n\n}\n\nvar _ raft.FSMSnapshot = (*snapshot)(nil)\n\ntype snapshot struct {\n\nreader io.Reader\n\n}\n\nfunc (s *snapshot) Persist(sink raft.SnapshotSink) error { if _, err := io.Copy(sink, s.reader); err != nil {\n\n_ = sink.Cancel() return err\n\n} return sink.Close()\n\n}\n\nfunc (s *snapshot) Release() {}\n\nSnapshot() returns an FSMSnapshot that represents a point-in-time snapshot of the FSM’s state. In our case that state is our FSM’s log, so call Reader() to return an io.Reader that will read all the log’s data.\n\nThese snapshots serve two purposes: they allow Raft to compact its log so it doesn’t store logs whose commands Raft has applied already. And they allow Raft to bootstrap new servers more efficiently than if the leader had to replicate its entire log again and again.\n\nRaft calls Snapshot() according to your configured SnapshotInterval (how often Raft checks if it should snapshot—default is two minutes) and SnapshotThreshold (how many logs since the last snapshot before making a new snapshot—default is 8192).\n\nRaft calls Persist() on the FSMSnapshot we created to write its state to some sink that, depending on the snapshot store you configured Raft with, could be in- memory, a file, an S3 bucket—something to store the bytes in. We’re using the file snapshot store so that when the snapshot completes, we’ll have a file containing all the Raft’s log data. A shared state store such as S3 would put the burden of writing and reading the snapshot on S3 rather than the leader and allow new servers to restore snapshots without streaming from the leader. Raft calls Release() when it’s finished with the snapshot.\n\nreport erratum • discuss",
      "content_length": 1904,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 159,
      "content": "Chapter 8. Coordinate Your Services with Consensus • 154\n\nPut this Restore() method below Release():\n\nCoordinateWithConsensus/internal/log/distributed.go func (f *fsm) Restore(r io.ReadCloser) error {\n\nb := make([]byte, lenWidth) var buf bytes.Buffer for i := 0; ; i++ {\n\n_, err := io.ReadFull(r, b) if err == io.EOF {\n\nbreak } else if err != nil {\n\nreturn err\n\n} size := int64(enc.Uint64(b)) if _, err = io.CopyN(&buf, r, size); err != nil {\n\nreturn err\n\n} record := &api.Record{} if err = proto.Unmarshal(buf.Bytes(), record); err != nil {\n\nreturn err\n\n} if i == 0 {\n\nf.log.Config.Segment.InitialOffset = record.Offset if err := f.log.Reset(); err != nil {\n\nreturn err\n\n}\n\n} if _, err = f.log.Append(record); err != nil {\n\nreturn err\n\n} buf.Reset()\n\n} return nil\n\n}\n\nRaft calls Restore() to restore an FSM from a snapshot. For example, if we lost a server and scaled up a new one, we’d want to restore its FSM. The FSM must discard existing state to make sure its state will match the leader’s replicated state.\n\nIn our Restore() implementation, we reset the log and configure its initial offset to the first record’s offset we read from the snapshot so the log’s offsets match. Then we read the records in the snapshot and append them to our new log.\n\nThat’s it for our FSM code.\n\nNext, put this snippet below the FSM to define Raft’s log store:\n\nCoordinateWithConsensus/internal/log/distributed.go var _ raft.LogStore = (*logStore)(nil)\n\nreport erratum • discuss",
      "content_length": 1466,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 160,
      "content": "Implement Raft in Our Service • 155\n\ntype logStore struct {\n\nLog\n\n}\n\nfunc newLogStore(dir string, c Config) (*logStore, error) {\n\nlog, err := NewLog(dir, c) if err != nil {\n\nreturn nil, err\n\n} return &logStore{log}, nil\n\n}\n\nRaft calls your FSM’s Apply() method with *raft.Log’s read from its managed log store. Raft replicates a log and then calls your state machine with the log’s records. We’re using our own log as Raft’s log store, but we need to wrap our log to satisfy the LogStore interface Raft requires. In this snippet, we’ve defined our log store and a function to create it.\n\nBelow newLogStore() add this snippet:\n\nCoordinateWithConsensus/internal/log/distributed.go func (l *logStore) FirstIndex() (uint64, error) {\n\nreturn l.LowestOffset()\n\n}\n\nfunc (l *logStore) LastIndex() (uint64, error) {\n\noff, err := l.HighestOffset() return off, err\n\n}\n\nfunc (l *logStore) GetLog(index uint64, out *raft.Log) error {\n\nin, err := l.Read(index) if err != nil {\n\nreturn err\n\n} out.Data = in.Value out.Index = in.Offset out.Type = raft.LogType(in.Type) out.Term = in.Term return nil\n\n}\n\nRaft uses these APIs to get records and information about the log. We support the functionality on our log already and just needed to wrap our existing methods. What we call offsets, Raft calls indexes.\n\nPut the following snippet below GetLog():\n\nCoordinateWithConsensus/internal/log/distributed.go func (l *logStore) StoreLog(record *raft.Log) error {\n\nreturn l.StoreLogs([]*raft.Log{record})\n\n}\n\nreport erratum • discuss",
      "content_length": 1509,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 161,
      "content": "Chapter 8. Coordinate Your Services with Consensus • 156\n\nfunc (l *logStore) StoreLogs(records []*raft.Log) error {\n\nfor _, record := range records {\n\nif _, err := l.Append(&api.Record{ Value: record.Data, Term: record.Term, Type: uint32(record.Type),\n\n}); err != nil {\n\nreturn err\n\n}\n\n} return nil\n\n}\n\nRaft uses these APIs to append records to its log. Again, we just translate the call to our log’s API and our record type. These changes require adding some fields to our Record type.\n\nChange your Record message in api/v1/log.proto to the following:\n\nCoordinateWithConsensus/api/v1/log.proto message Record {\n\nbytes value = 1; uint64 offset = 2; uint64 term = 3; uint32 type = 4;\n\n}\n\nThen compile your protobuf by running $ make compile.\n\nThe last method on the logStore is a method to delete old records. Below StoreLogs(), put this DeleteRange() method:\n\nCoordinateWithConsensus/internal/log/distributed.go func (l *logStore) DeleteRange(min, max uint64) error {\n\nreturn l.Truncate(max)\n\n}\n\nDeleteRange(min, max uint64) removes the records between the offsets—it’s to remove records that are old or stored in a snapshot.\n\nStream Layer\n\nRaft uses a stream layer in the transport to provide a low-level stream abstraction to connect with Raft servers. Our stream layer must satisfy Raft’s StreamLayer interface:\n\ntype StreamLayer interface {\n\nnet.Listener // Dial is used to create a new outgoing connection Dial(address ServerAddress, timeout time.Duration) (net.Conn, error)\n\n}\n\nreport erratum • discuss",
      "content_length": 1508,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 162,
      "content": "Implement Raft in Our Service • 157\n\nAdd this snippet at the bottom of distributed.go to begin your StreamLayer:\n\nCoordinateWithConsensus/internal/log/distributed.go var _ raft.StreamLayer = (*StreamLayer)(nil)\n\ntype StreamLayer struct {\n\nln serverTLSConfig *tls.Config *tls.Config peerTLSConfig\n\nnet.Listener\n\n}\n\nfunc NewStreamLayer(\n\nln net.Listener, serverTLSConfig, peerTLSConfig *tls.Config,\n\n) *StreamLayer {\n\nreturn &StreamLayer{ ln: serverTLSConfig: serverTLSConfig, peerTLSConfig:\n\nln,\n\npeerTLSConfig,\n\n}\n\n}\n\nThis snippet defines the StreamLayer type and checks that it satisfies the raft.Stream- Layer interface. We want to enable encrypted communication between servers with TLS, so we need to take in the TLS configs used to accept incoming connections (the serverTLSConfig) and create outgoing connections (the peerTLSConfig).\n\nBelow NewStreamLayer(), add this Dial() method and RaftRPC constant:\n\nCoordinateWithConsensus/internal/log/distributed.go const RaftRPC = 1\n\nfunc (s *StreamLayer) Dial(\n\naddr raft.ServerAddress, timeout time.Duration,\n\n) (net.Conn, error) {\n\ndialer := &net.Dialer{Timeout: timeout} var conn, err = dialer.Dial(\"tcp\", string(addr)) if err != nil {\n\nreturn nil, err\n\n} // identify to mux this is a raft rpc _, err = conn.Write([]byte{byte(RaftRPC)}) if err != nil {\n\nreturn nil, err\n\n} if s.peerTLSConfig != nil {\n\nconn = tls.Client(conn, s.peerTLSConfig)\n\n} return conn, err\n\n}\n\nreport erratum • discuss",
      "content_length": 1443,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 163,
      "content": "Chapter 8. Coordinate Your Services with Consensus • 158\n\nDial(addr raft.ServerAddress, timeout time.Duration) makes outgoing connections to other servers in the Raft cluster. When we connect to a server, we write the RaftRPC byte to identify the connection type so we can multiplex Raft on the same port as our Log gRPC requests. (We’ll take a look at multiplexing shortly.) If we configure the stream layer with a peer TLS config, we make a TLS client-side connection.\n\nThe rest of the methods on the stream layer implement the net.Listener interface. Below Dial() add this snippet:\n\nCoordinateWithConsensus/internal/log/distributed.go func (s *StreamLayer) Accept() (net.Conn, error) {\n\nconn, err := s.ln.Accept() if err != nil {\n\nreturn nil, err\n\n} b := make([]byte, 1) _, err = conn.Read(b) if err != nil {\n\nreturn nil, err\n\n} if bytes.Compare([]byte{byte(RaftRPC)}, b) != 0 {\n\nreturn nil, fmt.Errorf(\"not a raft rpc\")\n\n} if s.serverTLSConfig != nil {\n\nreturn tls.Server(conn, s.serverTLSConfig), nil\n\n} return conn, nil\n\n}\n\nfunc (s *StreamLayer) Close() error {\n\nreturn s.ln.Close()\n\n}\n\nfunc (s *StreamLayer) Addr() net.Addr {\n\nreturn s.ln.Addr()\n\n}\n\nAccept() is the mirror of Dial(). We accept the incoming connection and read the byte that identifies the connection and then create a server-side TLS connec- tion. Close() closes the listener. Addr() returns the listener’s address.\n\nDiscovery Integration\n\nThe next step to implement Raft in our service is to integrate our Serf-driven discovery layer with Raft to make the corresponding change in our Raft cluster when the Serf membership changes. Each time you add a server to the cluster, Serf will publish an event saying a member joined, and our discov- ery.Membership will call its handler’s Join(id, addr string) method. When a server\n\nreport erratum • discuss",
      "content_length": 1824,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 164,
      "content": "Implement Raft in Our Service • 159\n\nleaves the cluster, Serf will publish an event saying a member left, and our discovery.Membership will call its handler’s Leave(id string) method. Our distributed log will act as our Membership’s handler, so we need to implement those Join() and Leave() methods to update Raft.\n\nAdd this snippet below DistributedLog.Read(offset uint64) method:\n\nCoordinateWithConsensus/internal/log/distributed.go func (l *DistributedLog) Join(id, addr string) error {\n\nconfigFuture := l.raft.GetConfiguration() if err := configFuture.Error(); err != nil {\n\nreturn err\n\n} serverID := raft.ServerID(id) serverAddr := raft.ServerAddress(addr) for _, srv := range configFuture.Configuration().Servers {\n\nif srv.ID == serverID || srv.Address == serverAddr {\n\nif srv.ID == serverID && srv.Address == serverAddr {\n\n// server has already joined return nil\n\n} // remove the existing server removeFuture := l.raft.RemoveServer(serverID, 0, 0) if err := removeFuture.Error(); err != nil {\n\nreturn err\n\n}\n\n}\n\n} addFuture := l.raft.AddVoter(serverID, serverAddr, 0, 0) if err := addFuture.Error(); err != nil {\n\nreturn err\n\n} return nil\n\n}\n\nfunc (l *DistributedLog) Leave(id string) error {\n\nremoveFuture := l.raft.RemoveServer(raft.ServerID(id), 0, 0) return removeFuture.Error()\n\n}\n\nJoin(id, addr string) adds the server to the Raft cluster. We add every server as a voter, but Raft supports adding servers as non-voters with the AddNonVoter() API. You’d find non-voter servers useful if you wanted to replicate state to many servers to serve read only eventually consistent state. Each time you add more voter servers, you increase the probability that replications and elections will take longer because the leader has more servers it needs to communicate with to reach a majority.\n\nreport erratum • discuss",
      "content_length": 1820,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 165,
      "content": "Chapter 8. Coordinate Your Services with Consensus • 160\n\nLeave(id string) removes the server from the cluster. Removing the leader will trigger a new election.\n\nRaft will error and return ErrNotLeader when you try to change the cluster on non-leader nodes. In our service discovery code we log all handler errors as critical, but if the node is a non-leader, then we should expect these errors and not log them. In internal/discovery/membership.go, import github.com/hashicorp/raft and update your logError() method to this:\n\nCoordinateWithConsensus/internal/discovery/membership.go func (m *Membership) logError(err error, msg string, member serf.Member) {\n\nlog := m.logger.Error if err == raft.ErrNotLeader {\n\nlog = m.logger.Debug\n\n} log(\n\nmsg, zap.Error(err), zap.String(\"name\", member.Name), zap.String(\"rpc_addr\", member.Tags[\"rpc_addr\"]),\n\n)\n\n}\n\nlogError() will log the non-leader errors at the debug level now, and logs like these would be good candidates for removal.\n\nGo back to internal/log/distributed.go and add this WaitForLeader() method below Leave():\n\nCoordinateWithConsensus/internal/log/distributed.go func (l *DistributedLog) WaitForLeader(timeout time.Duration) error {\n\ntimeoutc := time.After(timeout) ticker := time.NewTicker(time.Second) defer ticker.Stop() for {\n\nselect { case <-timeoutc:\n\nreturn fmt.Errorf(\"timed out\")\n\ncase <-ticker.C:\n\nif l := l.raft.Leader(); l != \"\" {\n\nreturn nil\n\n}\n\n}\n\n}\n\n}\n\nWaitForLeader(timeout time.Duration) blocks until the cluster has elected a leader or times out. It’s useful when writing tests because, as we’ve discussed, most operations must run on the leader.\n\nreport erratum • discuss",
      "content_length": 1648,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 166,
      "content": "Implement Raft in Our Service • 161\n\nPut our last method on the DistributedLog under WaitForLeader():\n\nCoordinateWithConsensus/internal/log/distributed.go func (l *DistributedLog) Close() error {\n\nf := l.raft.Shutdown() if err := f.Error(); err != nil {\n\nreturn err\n\n} return l.log.Close()\n\n}\n\nClose() shuts down the Raft instance and closes the local log. And that wraps up the method on our DistributedLog. Now we’ll build out the pieces that the distributed log and Raft depend on, beginning with the FSM.\n\nTest the Distributed Log\n\nNow let’s test our distributed log. In the internal/log directory create a distribut- ed_test.go file, beginning with this code:\n\nCoordinateWithConsensus/internal/log/distributed_test.go package log_test\n\nimport (\n\n\"fmt\" \"io/ioutil\" \"net\" \"os\" \"reflect\" \"testing\" \"time\"\n\n\"github.com/hashicorp/raft\" \"github.com/stretchr/testify/require\" \"github.com/travisjeffery/go-dynaport\" api \"github.com/travisjeffery/proglog/api/v1\" \"github.com/travisjeffery/proglog/internal/log\"\n\n)\n\nfunc TestMultipleNodes(t *testing.T) { var logs []*log.DistributedLog nodeCount := 3 ports := dynaport.Get(nodeCount)\n\nfor i := 0; i < nodeCount; i++ {\n\ndataDir, err := ioutil.TempDir(\"\", \"distributed-log-test\") require.NoError(t, err) defer func(dir string) {\n\n_ = os.RemoveAll(dir)\n\n}(dataDir)\n\nreport erratum • discuss",
      "content_length": 1332,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 167,
      "content": "Chapter 8. Coordinate Your Services with Consensus • 162\n\nln, err := net.Listen( \"tcp\", fmt.Sprintf(\"127.0.0.1:%d\", ports[i]),\n\n) require.NoError(t, err)\n\nconfig := log.Config{} config.Raft.StreamLayer = log.NewStreamLayer(ln, nil, nil) config.Raft.LocalID = raft.ServerID(fmt.Sprintf(\"%d\", i)) config.Raft.HeartbeatTimeout = 50 * time.Millisecond config.Raft.ElectionTimeout = 50 * time.Millisecond config.Raft.LeaderLeaseTimeout = 50 * time.Millisecond config.Raft.CommitTimeout = 5 * time.Millisecond\n\nTo begin TestMultipleServers(*testing.T), we set up a three-server cluster. We shorten the default Raft timeout configs so that Raft elects the leader quickly.\n\nBelow the previous code, add this snippet:\n\nCoordinateWithConsensus/internal/log/distributed_test.go\n\nif i == 0 {\n\nconfig.Raft.Bootstrap = true\n\n}\n\nl, err := log.NewDistributedLog(dataDir, config) require.NoError(t, err)\n\nif i != 0 {\n\nerr = logs[0].Join(\n\nfmt.Sprintf(\"%d\", i), ln.Addr().String(),\n\n) require.NoError(t, err)\n\n} else {\n\nerr = l.WaitForLeader(3 * time.Second) require.NoError(t, err)\n\n}\n\nlogs = append(logs, l)\n\n}\n\nThe first server bootstraps the cluster, becomes the leader, and adds the other two servers to the cluster. The leader then must join other servers to its cluster.\n\nBelow the previous snippet, add this code:\n\nCoordinateWithConsensus/internal/log/distributed_test.go records := []*api.Record{\n\n{Value: []byte(\"first\")}, {Value: []byte(\"second\")},\n\n} for _, record := range records {\n\noff, err := logs[0].Append(record) require.NoError(t, err)\n\nreport erratum • discuss",
      "content_length": 1563,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 168,
      "content": "Multiplex to Run Multiple Services on One Port • 163\n\nrequire.Eventually(t, func() bool {\n\nfor j := 0; j < nodeCount; j++ {\n\ngot, err := logs[j].Read(off) if err != nil {\n\nreturn false\n\n} record.Offset = off if !reflect.DeepEqual(got.Value, record.Value) {\n\nreturn false\n\n}\n\n} return true\n\n}, 500*time.Millisecond, 50*time.Millisecond)\n\n}\n\nWe test our replication by appending some records to our leader server and check that Raft replicated the records to its followers. The Raft followers will apply the append message after a short latency, so we use testify’s Eventually() method to give Raft enough time to finish replicating.\n\nNow, finish the test by adding the following snippet:\n\nCoordinateWithConsensus/internal/log/distributed_test.go err := logs[0].Leave(\"1\") require.NoError(t, err)\n\ntime.Sleep(50 * time.Millisecond)\n\noff, err := logs[0].Append(&api.Record{ Value: []byte(\"third\"),\n\n}) require.NoError(t, err)\n\ntime.Sleep(50 * time.Millisecond)\n\nrecord, err := logs[1].Read(off) require.IsType(t, api.ErrOffsetOutOfRange{}, err) require.Nil(t, record)\n\nrecord, err = logs[2].Read(off) require.NoError(t, err) require.Equal(t, []byte(\"third\"), record.Value) require.Equal(t, off, record.Offset)\n\n}\n\nThis code checks that the leader stops replicating to a server that’s left the cluster, while continuing to replicate to the existing servers.\n\nMultiplex to Run Multiple Services on One Port\n\nMultiplexing allows you to serve different services on the same port. This makes your service easier to use: there’s less documentation, less configuration, and\n\nreport erratum • discuss",
      "content_length": 1589,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 169,
      "content": "Chapter 8. Coordinate Your Services with Consensus • 164\n\nfewer connections to manage. And you can serve multiple services even when a firewall constrains you to one port. There’s a slight perf hit on each new connection because the multiplexer reads the first bytes to identify the connection, but for long-lived connections that performance hit is negligible. And you must be careful you don’t accidentally expose a service.\n\nMany distributed services that use Raft multiplex Raft with other services, like an RPC service. Running gRPC with mutual TLS makes multiplexing tricky because we want to multiplex the connection after the TLS handshake. Before the handshake, we can’t differentiate the connections; we just know they’re both TLS connections. We need to handshake and see the decrypted packets to know more. After the handshake, we can read the connection’s packets to determine whether the connection is a gRPC or Raft connection. The issue with multiplexing mutual TLS gRPC connections is that gRPC needs information taken during the handshake to authenticate clients later on. So we have to multiplex before the handshake and need to make a way to iden- tify Raft from gRPC connections.\n\nWe identify the Raft connections from the gRPC connections by making the Raft connections write a byte to identify them by. We write the number 1 as the first byte of our Raft connections to separate them from the gRPC connec- tions. If we had other services, we could differentiate them from gRPC by passing a custom dialer to the gRPC client to send the number 2 as the first byte. The TLS standards4 don’t assign multiplexing schemes to the values 0–19, saying that they “require coordination,” like we’ve done. It’s better to handle internal services specially because you control the clients and can make them write whatever you need to identify them.\n\nLet’s update our agent to multiplex its Raft and gRPC connections and create a distributed log.\n\nUpdate your imports in internal/agent/agent.go to the following:\n\nCoordinateWithConsensus/internal/agent/agent.go import (\n\n\"bytes\" \"crypto/tls\" \"fmt\" \"io\" \"net\" \"sync\" \"time\"\n\n4.\n\nhttps://tools.ietf.org/html/rfc7983\n\nreport erratum • discuss",
      "content_length": 2199,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 170,
      "content": "➤\n\nMultiplex to Run Multiple Services on One Port • 165\n\n\"go.uber.org/zap\" \"github.com/hashicorp/raft\" \"github.com/soheilhy/cmux\" \"google.golang.org/grpc\" \"google.golang.org/grpc/credentials\"\n\n\"github.com/travisjeffery/proglog/internal/auth\" \"github.com/travisjeffery/proglog/internal/discovery\" \"github.com/travisjeffery/proglog/internal/log\" \"github.com/travisjeffery/proglog/internal/server\"\n\n)\n\nAnd then update your Agent type to this definition:\n\nCoordinateWithConsensus/internal/agent/agent.go type Agent struct {\n\nConfig Config\n\nmux log server membership *discovery.Membership\n\ncmux.CMux *log.DistributedLog *grpc.Server\n\nshutdown shutdowns shutdownLock sync.Mutex\n\nbool chan struct{}\n\n}\n\nHere we’ve added the mux cmux.CMux field, changed the log to a DistributedLog, and removed the replicator.\n\nAdd this field to your Config struct to enable bootstrapping the Raft cluster:\n\nCoordinateWithConsensus/internal/agent/agent.go Bootstrap bool\n\nIn the New() function, add the highlighted code to set up the mux (short for multiplexer):\n\nCoordinateWithConsensus/internal/agent/agent.go setup := []func() error {\n\na.setupLogger, a.setupMux, a.setupLog, a.setupServer, a.setupMembership,\n\n}\n\nThen put setupMux() after the New() function:\n\nreport erratum • discuss",
      "content_length": 1263,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 171,
      "content": "Chapter 8. Coordinate Your Services with Consensus • 166\n\nCoordinateWithConsensus/internal/agent/agent.go func (a *Agent) setupMux() error {\n\nrpcAddr := fmt.Sprintf( \":%d\", a.Config.RPCPort,\n\n) ln, err := net.Listen(\"tcp\", rpcAddr) if err != nil {\n\nreturn err\n\n} a.mux = cmux.New(ln) return nil\n\n}\n\nsetupMux() creates a listener on our RPC address that’ll accept both Raft and gRPC connections and then creates the mux with the listener. The mux will accept connections on that listener and match connections based on your configured rules.\n\nLet’s update setupLog() to configure the rule to match Raft and create the dis- tributed log. Replace your existing setupLog() method and put this snippet in its place:\n\nCoordinateWithConsensus/internal/agent/agent.go func (a *Agent) setupLog() error {\n\nraftLn := a.mux.Match(func(reader io.Reader) bool {\n\nb := make([]byte, 1) if _, err := reader.Read(b); err != nil {\n\nreturn false\n\n} return bytes.Compare(b, []byte{byte(log.RaftRPC)}) == 0\n\n})\n\nIn this snippet, we configure the mux that matches Raft connections. We identify Raft connections by reading one byte and checking that the byte matches the byte we set up our outgoing Raft connections to write in Stream Layer, on page 156:\n\nCoordinateWithConsensus/internal/log/distributed.go // identify to mux this is a raft rpc _, err = conn.Write([]byte{byte(RaftRPC)}) if err != nil {\n\nreturn nil, err\n\n}\n\nIf the mux matches this rule, it will pass the connection to the raftLn listener for Raft to handle the connection. Add the rest of setupLog() after the previous snippet:\n\nreport erratum • discuss",
      "content_length": 1598,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 172,
      "content": "➤ ➤\n\nMultiplex to Run Multiple Services on One Port • 167\n\nCoordinateWithConsensus/internal/agent/agent.go\n\nlogConfig := log.Config{} logConfig.Raft.StreamLayer = log.NewStreamLayer(\n\nraftLn, a.Config.ServerTLSConfig, a.Config.PeerTLSConfig,\n\n) logConfig.Raft.LocalID = raft.ServerID(a.Config.NodeName) logConfig.Raft.Bootstrap = a.Config.Bootstrap var err error a.log, err = log.NewDistributedLog(\n\na.Config.DataDir, logConfig,\n\n) if err != nil {\n\nreturn err\n\n} if a.Config.Bootstrap {\n\nerr = a.log.WaitForLeader(3 * time.Second)\n\n} return err\n\n}\n\nWe configure the distributed log’s Raft to use our multiplexed listener and then configure and create the distributed log.\n\nUpdate your gRPC server to use the mux’s listener by updating setupServer() to the following:\n\nCoordinateWithConsensus/internal/agent/agent.go func (a *Agent) setupServer() error {\n\nauthorizer := auth.New(\n\na.Config.ACLModelFile, a.Config.ACLPolicyFile,\n\n) serverConfig := &server.Config{\n\nCommitLog: a.log, Authorizer: authorizer,\n\n} var opts []grpc.ServerOption if a.Config.ServerTLSConfig != nil {\n\ncreds := credentials.NewTLS(a.Config.ServerTLSConfig) opts = append(opts, grpc.Creds(creds))\n\n} var err error a.server, err = server.NewGRPCServer(serverConfig, opts...) if err != nil {\n\nreturn err\n\n} grpcLn := a.mux.Match(cmux.Any()) go func() {\n\nreport erratum • discuss",
      "content_length": 1347,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 173,
      "content": "➤ ➤ ➤ ➤ ➤\n\nChapter 8. Coordinate Your Services with Consensus • 168\n\nif err := a.server.Serve(grpcLn); err != nil {\n\n_ = a.Shutdown()\n\n}\n\n}() return err\n\n}\n\nBecause we’ve multiplexed two connection types (Raft and gRPC) and we added a matcher for the Raft connections, we know all other connections must be gRPC connections. We use cmux.Any() because it matches any connec- tions. Then we tell our gRPC server to serve on the multiplexed listener.\n\nReplace your setupMembership() method with the following:\n\nCoordinateWithConsensus/internal/agent/agent.go func (a *Agent) setupMembership() error {\n\nrpcAddr, err := a.Config.RPCAddr() if err != nil {\n\nreturn err\n\n} a.membership, err = discovery.New(a.log, discovery.Config{\n\nNodeName: a.Config.NodeName, BindAddr: a.Config.BindAddr, Tags: map[string]string{\n\n\"rpc_addr\": rpcAddr,\n\n}, StartJoinAddrs: a.Config.StartJoinAddrs,\n\n}) return err\n\n}\n\nOur DistributedLog handles coordinated replication, thanks to Raft, so we don’t need the Replicator anymore. Now we need the Membership to tell the DistributedLog when servers join or leave the cluster. Delete the a.replicator.Close line in Shutdown() and delete the internal/log/replicator.go file too. All that’s left is to tell our mux to serve connections. Above the return statement in New(), add this line:\n\nCoordinateWithConsensus/internal/agent/agent.go go a.serve()\n\nAnd then put serve() at the bottom of the file:\n\nCoordinateWithConsensus/internal/agent/agent.go func (a *Agent) serve() error {\n\nif err := a.mux.Serve(); err != nil {\n\n_ = a.Shutdown() return err\n\n} return nil\n\n}\n\nreport erratum • discuss",
      "content_length": 1609,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 174,
      "content": "What You Learned • 169\n\nNow let’s update our agent tests for Raft and test our replication and coordi- nation. In What You Learned, on page 138, I showed you a test snippet that’d fail our test because our servers replicated each other in a cycle instead of adhering to a leader-follower relationship. That snippet will pass now!\n\nIn internal/agent/agent_test.go, add the following line to the agent’s config:\n\nCoordinateWithConsensus/internal/agent/agent_test.go Bootstrap: i == 0,\n\nThis line is all we need to bootstrap the Raft cluster.\n\nAt the bottom of the test, add this snippet:\n\nCoordinateWithConsensus/internal/agent/agent_test.go consumeResponse, err = leaderClient.Consume(\n\ncontext.Background(), &api.ConsumeRequest{\n\nOffset: produceResponse.Offset + 1,\n\n},\n\n) require.Nil(t, consumeResponse) require.Error(t, err) got := grpc.Code(err) want := grpc.Code(api.ErrOffsetOutOfRange{}.GRPCStatus().Err()) require.Equal(t, got, want)\n\nNow we check that Raft has replicated the record we produced to the leader by consuming the record from a follower and that the replication stops there—the leader doesn’t replicate from the followers.\n\nRun your tests with $ make test. Your distributed service now uses Raft for consensus and replication!\n\nWhat You Learned\n\nIn this chapter, you learned how to coordinate distributed services with Raft by adding leader election and replication to our service. We also looked at how to multiplex connections and run multiple services on one port. Next, we’ll talk about client-side discovery, so clients can discover and call our servers.\n\nreport erratum • discuss",
      "content_length": 1605,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 175,
      "content": "CHAPTER 9\n\nDiscover Servers and Load Balance from the Client\n\nWe’ve gone through the belly of a whale and built a distributed service with discovery and consensus—a real distributed service! So far we’ve focused on the servers and haven’t changed the clients beyond what gRPC gives us for free. In this chapter, we’ll work on three client features that will improve our service’s availability, scalability, and user experience. We’ll enable our client to automatically:\n\nDiscover servers in the cluster, • Direct append calls to leaders and consume calls to followers, and • Balance consume calls across followers.\n\nAfter we’ve made these improvements, we’ll be ready to deploy!\n\nThree Load-Balancing Strategies\n\nThree strategies can be used for solving the discovery and load balancing problem:\n\nServer proxying—your client sends its requests to a load balancer that knows the servers (either by querying a service registry or by being the service registry) and proxies the requests to your back-end services.\n\nExternal load balancing—your client queries an external load-balancing service that knows the servers and tells the client which server to send the RPC.\n\nClient-side balancing—your client queries a service registry to learn about the servers, picks the server to send its RPC, and sends its RPC directly to the server.\n\nreport erratum • discuss",
      "content_length": 1356,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 176,
      "content": "Chapter 9. Discover Servers and Load Balance from the Client • 172\n\nUsing a server proxy is the most commonly used discovery and load-balancing pattern. Most servers don’t trust their clients enough to give them control over how load balancing works because these decisions might affect the ser- vice’s availability (for example, allowing a client to target a single server and call it until it’s unavailable). You can put a proxy between clients and servers to act as a trust boundary. The proxy lets you control how your system ingests requests, as all the networking behind the proxy is in your network, trusted, and under your control. The server proxy knows about the servers it proxies to by maintaining or calling a service registry. People often use AWS’s Elastic Load Balancer (ELB) to load balance external traffic from the internet. The ELB is an example of a service-side discovery router—incoming requests hit the ELB, and the ELB proxies that request to one instance registered with the ELB.\n\nFor complex and very accurate load balancing, you can run an external load balancer. The external load balancer knows all the servers and potentially all the clients, so it has all the data to decide the best server for the client to call. You pay for external load balancers with operational burden. I’ve never needed an external load balancer.\n\nAlternatively, you can use client-side load balancing when you trust the clients. Client-side load balancing reduces latency and increases efficiency because requests go directly to their destination—there are no intermediates. This load balancing pattern is resilient because there isn’t a single point of failure. However, you need to work on your network and security to give clients direct access to your servers.\n\nWe’ll build client-side discovery and load balancing into our service because we control both the client and server and we designed our service for low- latency, high-throughput applications.\n\nLoad Balance on the Client in gRPC\n\nThough the ideas we’ll talk about in this chapter can apply to any client and server, because our service is a gRPC service, we’ll use those terms. gRPC separates server discovery, load balancing, and client requests and response handling—often the only code you’ll write is the latter. In gRPC, resolvers discover servers and pickers load balance by picking what server will handle the current request. gRPC also has balancers that manage subconnections but defer the load balancing to the pickers. gRPC provides an API (base.NewBal- ancerBuilderV2) to create a base balancer, but you probably won’t have to write your own balancer.\n\nreport erratum • discuss",
      "content_length": 2661,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 177,
      "content": "Load Balance on the Client in gRPC • 173\n\nWhen you call grpc.Dial, gRPC takes the address and passes it on to the resolver, and the resolver discovers the servers. gRPC’s default resolver is the DNS resolver. If the address you give to gRPC has multiple DNS records associated with it, gRPC will balance the requests across each of those records’ servers. You can write your own resolvers and use resolvers written by the community. For example, Kuberesolver1 resolves the servers by fetching the endpoints from Kubernetes’ API.\n\ngRPC uses round-robin load balancing by default. The round-robin algorithm works by sending the first call to the first server, the second call to the second server, and so on. After the last server, it goes back to the first server again. So, we send each server the same number of calls. Round-robin works well when each request requires the same work by the server—stateless services that defer the work to a separate service like a database, for example. You can always begin with round-robin load balancing and optimize later.\n\nThe issue with round-robin load balancing, however, is that it doesn’t consider what you know about each request, client, and server. For example:\n\nIf your server is a replicated distributed service with a single writer and multiple readers, you’ll want to read from replicas so the writer can focus on the writes. This requires knowing whether the request is a read or a write and whether the server is a primary or a replica.\n\nIf your service is a globally distributed service, you’ll want your clients to prioritize networking with local servers, which means you must know the location of the clients and the servers.\n\nIf your system is latency sensitive, you can track metrics on how many in-flight or queued requests a server has or some other combination of latency metrics and have the client request the server with the smallest number.\n\nNow you’ve seen how client-side discovery and load balancing work in gRPC, and when you might want to go beyond round-robin to load balance more effi- ciently, what can you do with this knowledge when building your own services?\n\nThe service we’re building is a single-writer, multiple-reader distributed ser- vice—the leader server is the only server that can append to the log. Currently our clients connect to a single server, so if we want to call a leader and a fol- lower, we have to create multiple clients. And if we want to balance consume calls across the followers, we have to manage it in our client code.\n\n1.\n\nhttps://github.com/sercand/kuberesolver\n\nreport erratum • discuss",
      "content_length": 2597,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 178,
      "content": "Chapter 9. Discover Servers and Load Balance from the Client • 174\n\nWe can solve some problems by writing our own resolver and picker: the resolver discovers the servers and what server is the leader, and the picker manages directing produce calls to the leader and balancing consume calls across the followers. The resolver and picker will make your service easier to use, and we’ll be able to delete some of our test code too. Hopefully that sounds sweet to you—it does to me—so let’s get started.\n\nMake Servers Discoverable\n\nOur resolver will need a way to discover the cluster’s servers. It needs to know each server’s address and whether or not it is the leader. In Implement Raft in Our Service, on page 144, we built Raft into our service, which knows the cluster’s server and what server is the leader. We can expose this information to the resolver with an endpoint on our gRPC service.\n\nUsing an RPC for discovery will be easy because we built Serf and Raft into our service already. Kafka clients discover the cluster’s brokers by requesting a metadata endpoint. Kafka’s metadata endpoint responds with data that’s stored and coordinated with ZooKeeper, though the Kafka developers plan to remove the dependency on ZooKeeper and build Raft into Kafka to coordinate this data, similar to our service. This will be a big change in how this data works in Kafka, specifically with how it manages what servers are in the cluster and how it elects leaders; however, little to nothing will have to change with how the clients discover the servers, thus showing the benefit of using a service endpoint for client-side discovery.\n\nOpen the api/v1/log.proto file and update the Log service to include the GetServers() endpoint like so:\n\nClientSideServiceDiscovery/api/v1/log.proto service Log {\n\nrpc Produce(ProduceRequest) returns (ProduceResponse) {} rpc Consume(ConsumeRequest) returns (ConsumeResponse) {} rpc ConsumeStream(ConsumeRequest) returns (stream ConsumeResponse) {} rpc ProduceStream(stream ProduceRequest) returns (stream ProduceResponse)\n\n{}\n\nrpc GetServers(GetServersRequest) returns (GetServersResponse) {}\n\n}\n\nThis is the endpoint resolvers will call to get the cluster’s servers.\n\nNow, add this snippet to the end of the file to define the endpoint’s request and response:\n\nClientSideServiceDiscovery/api/v1/log.proto message GetServersRequest {}\n\nreport erratum • discuss",
      "content_length": 2393,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 179,
      "content": "Line 1\n\n\n\n\n\n\n\nMake Servers Discoverable • 175\n\nmessage GetServersResponse {\n\nrepeated Server servers = 1;\n\n}\n\nmessage Server { string id = 1; string rpc_addr = 2; bool is_leader = 3;\n\n}\n\nThe endpoint response includes the server addresses clients should connect to and what server is the leader. This information will tell the picker what server to send the server produce calls and what servers to send consume calls.\n\nWe’ll implement the endpoint on our server, but before we do, we need an API on our DistributedLog that exposes Raft’s server data. Open internal/log/distribut- ed.go and put this GetServers() method below DistributedLog.Close:\n\nClientSideServiceDiscovery/internal/log/distributed.go func (l *DistributedLog) GetServers() ([]*api.Server, error) {\n\nfuture := l.raft.GetConfiguration() if err := future.Error(); err != nil {\n\nreturn nil, err\n\n} var servers []*api.Server for _, server := range future.Configuration().Servers { servers = append(servers, &api.Server{\n\nId: RpcAddr: string(server.Address), IsLeader: l.raft.Leader() == server.Address,\n\nstring(server.ID),\n\n})\n\n} return servers, nil\n\n}\n\nRaft’s configuration comprises the servers in the cluster and includes each server’s ID, address, and suffrage—whether the server votes in Raft elections (we don’t need the suffrage in our project). Raft can tell us the address of the cluster’s leader, too. GetServers() converts the data from Raft’s raft.Server type into our *api.Server type for our API to respond with.\n\nLet’s update the DistributedLog tests to check that GetServers() returns the servers in the cluster as we expect. Open internal/log/distributed_test.go and add the new code in this snippet that surrounds the old lines 8 and 9:\n\nClientSideServiceDiscovery/internal/log/distributed_test.go servers, err := logs[0].GetServers() require.NoError(t, err) require.Equal(t, 3, len(servers)) require.True(t, servers[0].IsLeader)\n\nreport erratum • discuss",
      "content_length": 1937,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 180,
      "content": "5\n\n\n\n\n\n\n\n\n\n10\n\n\n\n\n\n\n\n\n\n15\n\n\n\n\n\nChapter 9. Discover Servers and Load Balance from the Client • 176\n\nrequire.False(t, servers[1].IsLeader) require.False(t, servers[2].IsLeader)\n\nerr = logs[0].Leave(\"1\") require.NoError(t, err)\n\ntime.Sleep(50 * time.Millisecond)\n\nservers, err = logs[0].GetServers() require.NoError(t, err) require.Equal(t, 2, len(servers)) require.True(t, servers[0].IsLeader) require.False(t, servers[1].IsLeader)\n\nThe assertions before line 8 test that GetServers() returns all three servers in the cluster and sets the leader server as the leader. After line 9, we expect the cluster to have two servers because these assertions run after we’ve made one server leave the cluster.\n\nThat’s it for the DistributedLog changes and tests. Next we’ll implement the endpoint on the server that calls DistributedLog.GetServers().\n\nOpen internal/server/server.go and update the Config to:\n\nClientSideServiceDiscovery/internal/server/server.go type Config struct {\n\nCommitLog Authorizer Authorizer GetServerer GetServerer\n\nCommitLog\n\n}\n\nAnd put this snippet below the ConsumeStream() method:\n\nClientSideServiceDiscovery/internal/server/server.go func (s *grpcServer) GetServers(\n\nctx context.Context, req *api.GetServersRequest,\n\n) (\n\napi.GetServersResponse, error) { servers, err := s.GetServerer.GetServers() if err != nil { return nil, err\n\n} return &api.GetServersResponse{Servers: servers}, nil\n\n}\n\ntype GetServerer interface {\n\nGetServers() ([]*api.Server, error)\n\n}\n\nThese two snippets enable us to inject different structs that can get servers. We don’t want to add the GetServers() method to our CommitLog interface because\n\nreport erratum • discuss",
      "content_length": 1665,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 181,
      "content": "➤\n\nResolve the Servers • 177\n\na non-distributed log like our Log type doesn’t know about servers. So we made a new interface whose sole method GetServers() matches DistributedLog.Get- Servers. When we update the end-to-end tests in the agent package, we’ll set our DistributedLog on the config as both the CommitLog and the GetServerer—which our new server endpoint wraps with error handling.\n\nIn agent.go, update your setupServer() method to configure the server to get the cluster’s servers from the DistributedLog:\n\nClientSideServiceDiscovery/internal/agent/agent.go serverConfig := &server.Config{\n\nCommitLog: Authorizer: authorizer, GetServerer: a.log,\n\na.log,\n\n}\n\nNow we have a server endpoint that clients can call to get the cluster’s servers. We’re now ready to build our resolver.\n\nResolve the Servers\n\nThe gRPC resolver we’ll write in this section will call the GetServers() endpoint we made and pass its information to gRPC so that the picker knows what servers it can route requests to.\n\nTo start, create a new package for our resolver and picker code by running $ mkdir internal/loadbalance.\n\ngRPC uses the builder pattern for resolvers and pickers, so each has a builder interface and an implementation interface. Because the builder interfaces have one simple method—Build()—we’ll implement both interfaces with one type. Create a file named resolver.go in internal/loadbalance that begins with this code:\n\nClientSideServiceDiscovery/internal/loadbalance/resolver.go package loadbalance\n\nimport (\n\n\"context\" \"fmt\" \"sync\"\n\n\"go.uber.org/zap\" \"google.golang.org/grpc\" \"google.golang.org/grpc/attributes\" \"google.golang.org/grpc/resolver\" \"google.golang.org/grpc/serviceconfig\"\n\napi \"github.com/travisjeffery/proglog/api/v1\"\n\n)\n\nreport erratum • discuss",
      "content_length": 1765,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 182,
      "content": "Chapter 9. Discover Servers and Load Balance from the Client • 178\n\ntype Resolver struct {\n\nmu clientConn resolverConn *grpc.ClientConn serviceConfig *serviceconfig.ParseResult *zap.Logger logger\n\nsync.Mutex resolver.ClientConn\n\n}\n\nResolver is the type we’ll implement into gRPC’s resolver.Builder and resolver.Resolver interfaces. The clientConn connection is the user’s client connection and gRPC passes it to the resolver for the resolver to update with the servers it discovers. The resolverConn is the resolver’s own client connection to the server so it can call GetServers() and get the servers.\n\nAdd this snippet below the Resolver type to implement gRPC’s resolver.Builder interface:\n\nClientSideServiceDiscovery/internal/loadbalance/resolver.go var _ resolver.Builder = (*Resolver)(nil)\n\nfunc (r *Resolver) Build(\n\ntarget resolver.Target, cc resolver.ClientConn, opts resolver.BuildOptions,\n\n) (resolver.Resolver, error) {\n\nr.logger = zap.L().Named(\"resolver\") r.clientConn = cc var dialOpts []grpc.DialOption if opts.DialCreds != nil { dialOpts = append(\n\ndialOpts, grpc.WithTransportCredentials(opts.DialCreds),\n\n)\n\n} r.serviceConfig = r.clientConn.ParseServiceConfig(\n\nfmt.Sprintf(`{\"loadBalancingConfig\":[{\"%s\":{}}]}`, Name),\n\n) var err error r.resolverConn, err = grpc.Dial(target.Endpoint, dialOpts...) if err != nil {\n\nreturn nil, err\n\n} r.ResolveNow(resolver.ResolveNowOptions{}) return r, nil\n\n}\n\nconst Name = \"proglog\"\n\nfunc (r *Resolver) Scheme() string {\n\nreturn Name\n\n}\n\nreport erratum • discuss",
      "content_length": 1517,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 183,
      "content": "Resolve the Servers • 179\n\nfunc init() {\n\nresolver.Register(&Resolver{})\n\n}\n\nresolver.Builder comprises two methods—Build() and Scheme():\n\nBuild() receives the data needed to build a resolver that can discover the servers (like the target address) and the client connection the resolver will update with the servers it discovers. Build() sets up a client connection to our server so the resolver can call the GetServers() API.\n\nScheme() returns the resolver’s scheme identifier. When you call grpc.Dial, gRPC parses out the scheme from the target address you gave it and tries to find a resolver that matches, defaulting to its DNS resolver. For our resolver, you’ll format the target address like this: proglog://your-service- address.\n\nWe register this resolver with gRPC in init() so gRPC knows about this resolver when it’s looking for resolvers that match the target’s scheme.\n\nPut this snippet below init() to implement gRPC’s resolver.Resolver interface:\n\nClientSideServiceDiscovery/internal/loadbalance/resolver.go var _ resolver.Resolver = (*Resolver)(nil)\n\nfunc (r *Resolver) ResolveNow(resolver.ResolveNowOptions) {\n\nr.mu.Lock() defer r.mu.Unlock() client := api.NewLogClient(r.resolverConn) // get cluster and then set on cc attributes ctx := context.Background() res, err := client.GetServers(ctx, &api.GetServersRequest{}) if err != nil {\n\nr.logger.Error(\n\n\"failed to resolve server\", zap.Error(err),\n\n) return\n\n} var addrs []resolver.Address for _, server := range res.Servers {\n\naddrs = append(addrs, resolver.Address{\n\nAddr: server.RpcAddr, Attributes: attributes.New( \"is_leader\", server.IsLeader,\n\n),\n\n})\n\n}\n\nreport erratum • discuss",
      "content_length": 1652,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 184,
      "content": "Chapter 9. Discover Servers and Load Balance from the Client • 180\n\nr.clientConn.UpdateState(resolver.State{\n\nAddresses: ServiceConfig: r.serviceConfig,\n\naddrs,\n\n})\n\n}\n\nfunc (r *Resolver) Close() {\n\nif err := r.resolverConn.Close(); err != nil {\n\nr.logger.Error(\n\n\"failed to close conn\", zap.Error(err),\n\n)\n\n}\n\n}\n\nresolver.Resolver comprises two methods—ResolveNow() and Close(). gRPC calls ResolveNow() to resolve the target, discover the servers, and update the client connection with the servers. How your resolver will discover the servers depends on your resolver and the service you’re working with. For example, a resolver built for Kubernetes could call Kubernetes’ API to get the list of endpoints. We create a gRPC client for our service and call the GetServers() API to get the cluster’s servers.\n\nServices can specify how clients should balance their calls to the service by updating the state with a service config. We update the state with a service config that specifies to use the “proglog” load balancer we’ll write in Route and Balance Requests with Pickers, on page 183.\n\nYou update the state with a slice of resolver.Address to inform the load balancer what servers it can choose from. A resolver.Address has three fields:\n\nAddr (required)—the address of the server to connect to.\n\nAttributes (optional but useful)—a map containing any data that’s useful for the load balancer. We’ll tell the picker what server is the leader and what servers are followers with this field.\n\nServerName (optional and you likely don’t need to set)—the name used as the transport certificate authority for the address, instead of the hostname taken from the Dial target string.\n\nAfter we’ve discovered the servers, we update the client connection by calling UpdateState() with the resolver.Address’s. We set up the addresses with the data in the api.Server’s. gRPC may call ResolveNow() concurrently, so we use a mutex to protect access across goroutines.\n\nClose() closes the resolver. In our resolver, we close the connection to our server created in Build().\n\nreport erratum • discuss",
      "content_length": 2087,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 185,
      "content": "Resolve the Servers • 181\n\nThat’s it for our resolver’s code. Let’s test it.\n\nCreate a test file named resolver_test.go in internal/loadbalance that begins with this snippet:\n\nLine 1\n\nClientSideServiceDiscovery/internal/loadbalance/resolver_test.go package loadbalance_test\n\n\n\n\n\nimport (\n\n\n\n5\n\n\"net\" \"testing\"\n\n\n\n\n\n\n\n\n\n10\n\n\n\n\n\n\"github.com/stretchr/testify/require\" \"google.golang.org/grpc\" \"google.golang.org/grpc/attributes\" \"google.golang.org/grpc/credentials\" \"google.golang.org/grpc/resolver\" \"google.golang.org/grpc/serviceconfig\"\n\n\n\n\n\n15\n\n\n\n)- -\n\napi \"github.com/travisjeffery/proglog/api/v1\" \"github.com/travisjeffery/proglog/internal/loadbalance\" \"github.com/travisjeffery/proglog/internal/config\" \"github.com/travisjeffery/proglog/internal/server\"\n\n20\n\nfunc TestResolver(t *testing.T) {\n\n\n\n\n\nl, err := net.Listen(\"tcp\", \"127.0.0.1:0\") require.NoError(t, err)\n\n\n\n\n\ntlsConfig, err := config.SetupTLSConfig(config.TLSConfig{\n\n25\n\n\n\n\n\n\n\n\n\nCertFile: KeyFile: CAFile: Server: ServerAddress: \"127.0.0.1\",\n\nconfig.ServerCertFile, config.ServerKeyFile, config.CAFile, true,\n\n30\n\n\n\n\n\n}) require.NoError(t, err) serverCreds := credentials.NewTLS(tlsConfig)\n\n\n\n\n\nsrv, err := server.NewGRPCServer(&server.Config{\n\n35\n\nGetServerer: &getServers{},\n\n\n\n\n\n}, grpc.Creds(serverCreds)) require.NoError(t, err)\n\n\n\n\n\ngo srv.Serve(l)\n\nThis code sets up a server for our test resolver to try and discover some servers from. We pass in a mock GetServerers on line 35 so we can set what servers the resolver should find.\n\nreport erratum • discuss",
      "content_length": 1529,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 186,
      "content": "Chapter 9. Discover Servers and Load Balance from the Client • 182\n\nPut this snippet below the previous snippet to continue writing the test:\n\nClientSideServiceDiscovery/internal/loadbalance/resolver_test.go conn := &clientConn{} tlsConfig, err = config.SetupTLSConfig(config.TLSConfig{\n\nCertFile: KeyFile: CAFile: Server: ServerAddress: \"127.0.0.1\",\n\nconfig.RootClientCertFile, config.RootClientKeyFile, config.CAFile, false,\n\n}) require.NoError(t, err) clientCreds := credentials.NewTLS(tlsConfig) opts := resolver.BuildOptions{\n\nDialCreds: clientCreds,\n\n} r := &loadbalance.Resolver{} _, err = r.Build(\n\nresolver.Target{\n\nEndpoint: l.Addr().String(),\n\n}, conn, opts,\n\n) require.NoError(t, err)\n\nThis code creates and builds the test resolver and configures its target end- point to point to the server we set up in the previous snippet. The resolver will call GetServers() to resolve the servers and update the client connection with the servers’ addresses.\n\nAdd this snippet below the previous snippet to finish writing the test:\n\nClientSideServiceDiscovery/internal/loadbalance/resolver_test.go\n\nwantState := resolver.State{\n\nAddresses: []resolver.Address{{\n\nAddr: Attributes: attributes.New(\"is_leader\", true),\n\n\"localhost:9001\",\n\n}, {\n\nAddr: Attributes: attributes.New(\"is_leader\", false),\n\n\"localhost:9002\",\n\n}},\n\n} require.Equal(t, wantState, conn.state)\n\nconn.state.Addresses = nil r.ResolveNow(resolver.ResolveNowOptions{}) require.Equal(t, wantState, conn.state)\n\n}\n\nreport erratum • discuss",
      "content_length": 1503,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 187,
      "content": "Route and Balance Requests with Pickers • 183\n\nWe check that the resolver updated the client connection with the servers and data we expected. We wanted the resolver to find two servers and mark the 9001 server as the leader.\n\nOur test depended on some mock types. Add this code at the bottom of the file:\n\nClientSideServiceDiscovery/internal/loadbalance/resolver_test.go type getServers struct{}\n\nfunc (s *getServers) GetServers() ([]*api.Server, error) {\n\nreturn []*api.Server{{\n\nId: RpcAddr: \"localhost:9001\", IsLeader: true,\n\n\"leader\",\n\n}, {\n\nId: RpcAddr: \"localhost:9002\",\n\n\"follower\",\n\n}}, nil\n\n}\n\ntype clientConn struct {\n\nresolver.ClientConn state resolver.State\n\n}\n\nfunc (c *clientConn) UpdateState(state resolver.State) {\n\nc.state = state\n\n}\n\nfunc (c *clientConn) ReportError(err error) {}\n\nfunc (c *clientConn) NewAddress(addrs []resolver.Address) {}\n\nfunc (c *clientConn) NewServiceConfig(config string) {}\n\nfunc (c *clientConn) ParseServiceConfig(\n\nconfig string, ) *serviceconfig.ParseResult {\n\nreturn nil\n\n}\n\ngetServers implements GetServerers, whose job is to return a known server set for the resolver to find. clientConn implements resolver.ClientConn, and its job is to keep a reference to the state the resolver updated it with so that we can verify that the resolver updates the client connection with the correct data.\n\nRun the resolver tests to verify that they pass. And now, we’re on to the picker.\n\nRoute and Balance Requests with Pickers\n\nIn the gRPC architecture, pickers handle the RPC balancing logic. They’re called pickers because they pick a server from the servers discovered by the resolver to handle each RPC. Pickers can route RPCs based on information\n\nreport erratum • discuss",
      "content_length": 1715,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 188,
      "content": "Chapter 9. Discover Servers and Load Balance from the Client • 184\n\nabout the RPC, client, and server, so their utility goes beyond balancing to any kind of request-routing logic.\n\nTo implement the picker builder, create a file named picker.go in internal/loadbalance that begins with this code:\n\nClientSideServiceDiscovery/internal/loadbalance/picker.go package loadbalance\n\nimport (\n\n\"strings\" \"sync\" \"sync/atomic\"\n\n\"google.golang.org/grpc/balancer\" \"google.golang.org/grpc/balancer/base\"\n\n)\n\nvar _ base.PickerBuilder = (*Picker)(nil)\n\ntype Picker struct {\n\nmu leader followers []balancer.SubConn current\n\nsync.RWMutex balancer.SubConn\n\nuint64\n\n}\n\nfunc (p *Picker) Build(buildInfo base.PickerBuildInfo) balancer.Picker {\n\np.mu.Lock() defer p.mu.Unlock() var followers []balancer.SubConn for sc, scInfo := range buildInfo.ReadySCs {\n\nisLeader := scInfo.\n\nAddress. Attributes. Value(\"is_leader\").(bool)\n\nif isLeader {\n\np.leader = sc continue\n\n} followers = append(followers, sc)\n\n} p.followers = followers return p\n\n}\n\nPickers use the builder pattern just like resolvers. gRPC passes a map of subconnections with information about those subconnections to Build() to set up the picker—behind the scenes, gRPC connected to the addresses that our resolver discovered. Our picker will route consume RPCs to follower servers and produce RPCs to the leader server. The address attributes help us differ- entiate the servers.\n\nreport erratum • discuss",
      "content_length": 1444,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 189,
      "content": "Route and Balance Requests with Pickers • 185\n\nTo implement the picker, add this snippet below Build():\n\nClientSideServiceDiscovery/internal/loadbalance/picker.go var _ balancer.Picker = (*Picker)(nil)\n\nfunc (p *Picker) Pick(info balancer.PickInfo) (\n\nbalancer.PickResult, error) { p.mu.RLock() defer p.mu.RUnlock() var result balancer.PickResult if strings.Contains(info.FullMethodName, \"Produce\") ||\n\nlen(p.followers) == 0 { result.SubConn = p.leader\n\n} else if strings.Contains(info.FullMethodName, \"Consume\") {\n\nresult.SubConn = p.nextFollower()\n\n} if result.SubConn == nil {\n\nreturn result, balancer.ErrNoSubConnAvailable\n\n} return result, nil\n\n}\n\nfunc (p *Picker) nextFollower() balancer.SubConn {\n\ncur := atomic.AddUint64(&p.current, uint64(1)) len := uint64(len(p.followers)) idx := int(cur % len) return p.followers[idx]\n\n}\n\nPickers have one method: Pick(balancer.PickInfo). gRPC gives Pick() a balancer.PickInfo containing the RPC’s name and context to help the picker know what subcon- nection to pick. If you have header metadata, you can read it from the context. Pick() returns a balancer.PickResult with the subconnection to handle the call. Optionally, you can set a Done callback on the result that gRPC calls when the RPC completes. The callback tells you the RPC’s error, trailer metadata, and whether there were bytes sent and received to and from the server.\n\nWe look at the RPC’s method name to know whether the call is an append or consume call, and if we should pick a leader subconnection or a follower subconnection. We balance the consume calls across the followers with the round-robin algorithm. Put this snippet at the end of the file to register the picker with gRPC and finish the picker’s code:\n\nClientSideServiceDiscovery/internal/loadbalance/picker.go func init() {\n\nbalancer.Register(\n\nbase.NewBalancerBuilder(Name, &Picker{}, base.Config{}),\n\n)\n\n}\n\nreport erratum • discuss",
      "content_length": 1910,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 190,
      "content": "Chapter 9. Discover Servers and Load Balance from the Client • 186\n\nThough pickers handle routing the calls, which we’d traditionally consider handling the balancing, gRPC has a balancer type that takes input from gRPC, manages subconnections, and collects and aggregates connectivity states. gRPC provides a base balancer; you probably don’t need to write your own.\n\nTime to test our picker. Create a test file named picker_test.go in internal/loadbalance that begins with this snippet:\n\nClientSideServiceDiscovery/internal/loadbalance/picker_test.go package loadbalance_test\n\nimport (\n\n\"testing\"\n\n\"google.golang.org/grpc/attributes\" \"google.golang.org/grpc/balancer\" \"google.golang.org/grpc/balancer/base\" \"google.golang.org/grpc/resolver\"\n\n\"github.com/stretchr/testify/require\"\n\n\"github.com/travisjeffery/proglog/internal/loadbalance\"\n\n)\n\nfunc TestPickerNoSubConnAvailable(t *testing.T) {\n\npicker := &loadbalance.Picker{} for _, method := range []string{\n\n\"/log.vX.Log/Produce\", \"/log.vX.Log/Consume\",\n\n} {\n\ninfo := balancer.PickInfo{\n\nFullMethodName: method,\n\n} result, err := picker.Pick(info) require.Equal(t, balancer.ErrNoSubConnAvailable, err) require.Nil(t, result.SubConn)\n\n}\n\n}\n\nTestPickerNoSubConnAvailable() tests that a picker initially returns balancer.ErrNoSub- ConnAvailable before the resolver has discovered servers and updated the picker’s state with available subconnections. balancer.ErrNoSubConnAvailable instructs gRPC to block the client’s RPCs until the picker has an available subconnection to handle them.\n\nNext add this snippet below TestPickerNoSubConnAvailable() to test pickers with subconnections to pick from:\n\nClientSideServiceDiscovery/internal/loadbalance/picker_test.go func TestPickerProducesToLeader(t *testing.T) { picker, subConns := setupTest() info := balancer.PickInfo{\n\nreport erratum • discuss",
      "content_length": 1841,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 191,
      "content": "Route and Balance Requests with Pickers • 187\n\nFullMethodName: \"/log.vX.Log/Produce\",\n\n} for i := 0; i < 5; i++ {\n\ngotPick, err := picker.Pick(info) require.NoError(t, err) require.Equal(t, subConns[0], gotPick.SubConn)\n\n}\n\n}\n\nfunc TestPickerConsumesFromFollowers(t *testing.T) {\n\npicker, subConns := setupTest() info := balancer.PickInfo{\n\nFullMethodName: \"/log.vX.Log/Consume\",\n\n} for i := 0; i < 5; i++ {\n\npick, err := picker.Pick(info) require.NoError(t, err) require.Equal(t, subConns[i%2+1], pick.SubConn)\n\n}\n\n}\n\nTestPickerProducesToLeader() tests that the picker picks the leader subconnection for append calls. TestPickerConsumesFromFollowers() tests that the picker picks the followers subconnections in a round-robin for consume calls.\n\nPut this final snippet at the end of the file to define the tests’ helpers:\n\nClientSideServiceDiscovery/internal/loadbalance/picker_test.go func setupTest() (*loadbalance.Picker, []*subConn) {\n\nvar subConns []*subConn buildInfo := base.PickerBuildInfo{\n\nReadySCs: make(map[balancer.SubConn]base.SubConnInfo),\n\n} for i := 0; i < 3; i++ { sc := &subConn{} addr := resolver.Address{\n\nAttributes: attributes.New(\"is_leader\", i == 0),\n\n} // 0th sub conn is the leader sc.UpdateAddresses([]resolver.Address{addr}) buildInfo.ReadySCs[sc] = base.SubConnInfo{Address: addr} subConns = append(subConns, sc)\n\n} picker := &loadbalance.Picker{} picker.Build(buildInfo) return picker, subConns\n\n}\n\n// subConn implements balancer.SubConn. type subConn struct {\n\naddrs []resolver.Address\n\n}\n\nreport erratum • discuss",
      "content_length": 1547,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 192,
      "content": "➤ ➤ ➤ ➤ ➤\n\nChapter 9. Discover Servers and Load Balance from the Client • 188\n\nfunc (s *subConn) UpdateAddresses(addrs []resolver.Address) {\n\ns.addrs = addrs\n\n}\n\nfunc (s *subConn) Connect() {}\n\nsetupTest() builds the test picker with some mock subconnections. We create the picker with build information that contains addresses with the same attributes as our resolver sets.\n\nRun the picker’s tests to verify they pass. Now we’re ready to put everything together.\n\nTest Discovery and Balancing End-to-End\n\nWe’re ready to update our agent’s tests to test everything end-to-end: the client configuring the resolver and picker, the resolver discovering the servers, and the picker picking subconnections per RPC.\n\nOpen your agent tests in internal/agent/agent_test.go and add this import:\n\nClientSideServiceDiscovery/internal/agent/agent_test.go \"github.com/travisjeffery/proglog/internal/loadbalance\"\n\nThen update the client() function to use your resolver and picker:\n\nClientSideServiceDiscovery/internal/agent/agent_test.go func client(\n\nt *testing.T, agent *agent.Agent, tlsConfig *tls.Config,\n\n) api.LogClient {\n\ntlsCreds := credentials.NewTLS(tlsConfig) opts := []grpc.DialOption{\n\ngrpc.WithTransportCredentials(tlsCreds),\n\n} rpcAddr, err := agent.Config.RPCAddr() require.NoError(t, err) conn, err := grpc.Dial(fmt.Sprintf(\n\n\"%s:///%s\", loadbalance.Name, rpcAddr,\n\n), opts...) require.NoError(t, err) client := api.NewLogClient(conn) return client\n\n}\n\nThe highlighted lines specify our scheme in the URL so gRPC knows to use our resolver.\n\nreport erratum • discuss",
      "content_length": 1568,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 193,
      "content": "What You Learned • 189\n\nRun the agent’s tests by running $ go run ./internal/agent, and you’ll see that the leader client consume call fails now. Why? Before, each client connected to one server. So the leader client connected to the leader. When we produced records, they were immediately available for consuming with the leader client because it consumed from the leader server—we didn’t have to wait for the leader to replicate the record. Now, each client connects to every server and produces to the leader and consumes from the followers, so we must wait for the leader to replicate the record to the followers.\n\nUpdate your test to wait for the servers to replicate the record before consum- ing with the leader client. Move time.Sleep that appears before line 14 to appear before line 4:\n\nLine 1\n\n\n\nClientSideServiceDiscovery/internal/agent/agent_test.go // wait until replication has finished time.Sleep(3 * time.Second)\n\n\n\n\n\nconsumeResponse, err := leaderClient.Consume(\n\n5\n\n\n\ncontext.Background(), &api.ConsumeRequest{\n\n\n\nOffset: produceResponse.Offset,\n\n)-\n\n},\n\n10\n\n\n\nrequire.NoError(t, err) require.Equal(t, consumeResponse.Record.Value, []byte(\"foo\"))\n\n\n\n\n\n\n\nfollowerClient := client(t, agents[1], peerTLSConfig) consumeResponse, err = followerClient.Consume(\n\n15\n\n\n\ncontext.Background(), &api.ConsumeRequest{\n\n\n\nOffset: produceResponse.Offset,\n\n)-\n\n},\n\n20\n\n\n\nrequire.NoError(t, err) require.Equal(t, consumeResponse.Record.Value, []byte(\"foo\"))\n\nRun your tests again with $ make test and watch them pass!\n\nWhat You Learned\n\nNow you know how gRPC resolves services and balances calls across them and how you can build your own resolvers and pickers. You can write your own resolver so that your clients dynamically discover servers. And you saw how pickers are useful for more than just load balancing—you can build your own routing logic with them.\n\nIn the next part of the book, we’ll look at how to deploy our service and make it live.\n\nreport erratum • discuss",
      "content_length": 1978,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 194,
      "content": "Part IV\n\nDeploy",
      "content_length": 15,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 195,
      "content": "CHAPTER 10\n\nDeploy Applications with Kubernetes Locally\n\nAfter Frodo and Sam had trekked from the Shire to Mount Doom, was their task finished? No—the whole journey would’ve been for nothing if they hadn’t thrown that ring into the fire. Likewise, building a service means something only after you’ve deployed it. Therefore, in this chapter, we’ll deploy a cluster of our service. We’ll:\n\nCreate an agent command-line interface (CLI) so we have an executable\n\nto run our service.\n\nGet set up with Kubernetes and Helm so that we can orchestrate our\n\nservice on both our local machine and later on a cloud platform.\n\nRun a cluster of your service on your machine.\n\nReady? Let’s get started.\n\nWhat Is Kubernetes?\n\nWhile entire books are devoted to answering this question, even they can’t cover everything Kubernetes can do. For our purposes in this book, I will touch upon the information you need to know to have a working knowledge of Kubernetes, enough to continue our journey and deploy and operate our service. Why Kuber- netes? Kubernetes is ubiquitous, it’s available on all cloud platforms, and it’s as close to a standard as we have for deploying distributed services.\n\nKubernetes1 is an open source orchestration system for automating deployment, scaling, and operating services running in containers. You tell Kubernetes what\n\n1.\n\nhttps://kubernetes.io\n\nreport erratum • discuss",
      "content_length": 1387,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 196,
      "content": "Chapter 10. Deploy Applications with Kubernetes Locally • 194\n\nto do by using its REST API to create, update, and delete resources that Kubernetes knows how to handle. Kubernetes is a declarative system in that you describe the end-goal state you want and Kubernetes runs the changes to take your system from its current state to your end-goal state.\n\nThe Kubernetes resource that people most commonly see are pods, the smallest deployable unit in Kubernetes. Think of containers as processes and pods as hosts—all containers running in a pod share the same network namespace, the same IP address, and the same interprocess communication (IPC) namespace, and they can share the same volumes. These are logical hosts because a physical host (what Kubernetes calls a node) may run multiple pods. The other resources you’ll work with either configure pods (ConfigMaps, Secrets) or manage a pod set (Deployments, StatefulSets, DaemonSets). You can extend Kubernetes by creating your own custom resources and controllers.\n\nControllers are control loops that watch the state of your resources and make changes where needed. Kubernetes itself is made up of many controllers. For example, the Deployment controller watches your Deployment resources; if you increase the replicas on a Deployment, the controller will schedule more pods.\n\nTo interact with Kubernetes, you’ll need its command-line tool, kubectl, which we’ll look at next.\n\nInstall kubectl\n\nThe Kubernetes command-line tool, kubectl,2 is used to run commands against Kubernetes clusters. You’ll use kubectl to inspect and manage your service’s cluster resources and view logs. Try to use kubectl for one-off operations. For operations you run again and again, like deploying or upgrading a service, you’ll use the Helm package manager or an operator, which we’ll take a look at later in this chapter.\n\nTo install kubectl, run the following:\n\n$ curl -LO \\ https://storage.googleapis.com/kubernetes-release/release/\\ v1.18.0/bin/$(uname)/amd64/kubectl $ chmod +x ./kubectl $ mv ./kubectl /usr/local/bin/kubectl\n\nWe need a Kubernetes cluster and its API for kubectl to call and do anything. In the next section, we’ll use the Kind tool to run a local Kubernetes cluster in Docker.\n\n2.\n\nhttps://kubernetes.io/docs/reference/kubectl/overview\n\nreport erratum • discuss",
      "content_length": 2317,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 197,
      "content": "Use Kind for Local Development and Continuous Integration • 195\n\nUse Kind for Local Development and Continuous Integration\n\nKind3 (an acronym for Kubernetes IN Docker) is a tool developed by the Kubernetes team to run local Kubernetes clusters using Docker containers as nodes. It’s the easiest way to run your own Kubernetes cluster, and it’s great for local development, testing, and continuous integration.\n\nTo install Kind, run the following:\n\n$ curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.8.1/kind-$(uname)-amd64 $ chmod +x ./kind $ mv ./kind /usr/local/bin/kind\n\nTo use Kind, you’ll need to install Docker.4 See Docker’s dedicated install instructions for your operation system.\n\nWith Docker running, you can create a Kind cluster by running:\n\n$ kind create cluster\n\nYou can then verify that Kind created your cluster and configured kubectl to use it by running the following:\n\n$ kubectl cluster-info > Kubernetes master is running at https://127.0.0.1:46023 KubeDNS is running at \\ https://127.0.0.1:46023/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\n\nTo further debug and diagnose cluster problems, use kubectl cluster-info dump.\n\nKind runs one Docker container representing one Kubernetes node in the cluster. By default, Kind runs a single node cluster with everything needed for a functioning Kubernetes cluster. You can see the Node container by running this:\n\n$ docker ps CONTAINER ID IMAGE COMMAND CREATED ... 033de99b1e53 kindest/node:v1.18.2 \"/usr/local/bin/entr…\" 2 minutes...\n\nWe have a running Kubernetes cluster now—let’s run our service on it! To run our service in Kubernetes, we’ll need a Docker image, and our Docker image will need an executable entry point. Let’s write an agent CLI that serves as our service’s executable.\n\n3. 4.\n\nhttps://kind.sigs.k8s.io\n\nhttps://docs.docker.com/install\n\nreport erratum • discuss",
      "content_length": 1857,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 198,
      "content": "➤ ➤ ➤ ➤ ➤\n\nChapter 10. Deploy Applications with Kubernetes Locally • 196\n\nWrite an Agent Command-Line Interface\n\nOur agent CLI will provide just enough features to use as a Docker image’s entry point and run our service, parse flags, and then configure and run the agent.\n\nI use the Cobra5 library to handle commands and flags because it works well for creating both simple CLIs and complex applications. It’s used in the Go community by projects such as Kubernetes, Docker, Helm, Etcd, Hugo, and more. And Cobra integrates with a library called Viper,6 which is a complete configuration solution for Go applications.\n\nThe first step is to create a cmd/proglog/main.go file, beginning with this code:\n\nDeployLocally/cmd/proglog/main.go package main\n\nimport (\n\n\"log\" \"os\" \"os/signal\" \"path\" \"syscall\"\n\n\"github.com/spf13/cobra\" \"github.com/spf13/viper\" \"github.com/travisjeffery/proglog/internal/agent\" \"github.com/travisjeffery/proglog/internal/config\"\n\n)\n\nfunc main() {\n\ncli := &cli{}\n\ncmd := &cobra.Command{\n\nUse: PreRunE: cli.setupConfig, cli.run, RunE:\n\n\"proglog\",\n\n}\n\nif err := setupFlags(cmd); err != nil {\n\nlog.Fatal(err)\n\n}\n\nif err := cmd.Execute(); err != nil {\n\nlog.Fatal(err)\n\n}\n\n}\n\nThe highlighted code defines our sole command. Our CLI is about as simple as it gets. In more complex applications, this command would act as the root\n\n5. 6.\n\nhttps://github.com/spf13/cobra\n\nhttps://github.com/spf13/viper\n\nreport erratum • discuss",
      "content_length": 1440,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 199,
      "content": "Write an Agent Command-Line Interface • 197\n\ncommand tying together your subcommands. Cobra calls the RunE function you set on your command when the command runs. Put or call the command’s primary logic in that function. Cobra enables you to run hook functions to run before and after RunE.\n\nCobra provides persistent flags and hooks for applications with many subcom- mands (so we’re not using them in our program)—persistent flags and hooks apply to the current command and all its children. A common use case for a persistent flag is in API-wrapping CLIs. In these CLIs, every subcommand will need a flag for the API’s endpoint address. In this situation, you’d use an --api-addr persistent flag that you declare once on the root command for all the subcommands to inherit.\n\nTo define our cli and cfg types, add the following code:\n\nDeployLocally/cmd/proglog/main.go type cli struct {\n\ncfg cfg\n\n}\n\ntype cfg struct {\n\nagent.Config ServerTLSConfig config.TLSConfig config.TLSConfig PeerTLSConfig\n\n}\n\nI typically create a cli struct in which I can put logic and data that’s common to all the commands. I created a separate cfg struct from the agent.Config struct to handle the field types that we can’t parse without error handling: the *net.TCPAddr and the *tls.Config.\n\nNow, let’s set up our CLI’s flags.\n\nExpose Flags\n\nBelow the previous snippet, add this code to declare our CLI’s flags:\n\nDeployLocally/cmd/proglog/main.go func setupFlags(cmd *cobra.Command) error {\n\nhostname, err := os.Hostname() if err != nil {\n\nlog.Fatal(err)\n\n}\n\ncmd.Flags().String(\"config-file\", \"\", \"Path to config file.\")\n\ndataDir := path.Join(os.TempDir(), \"proglog\") cmd.Flags().String(\"data-dir\",\n\ndataDir, \"Directory to store log and Raft data.\")\n\nreport erratum • discuss",
      "content_length": 1755,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 200,
      "content": "Chapter 10. Deploy Applications with Kubernetes Locally • 198\n\ncmd.Flags().String(\"node-name\", hostname, \"Unique server ID.\")\n\ncmd.Flags().String(\"bind-addr\",\n\n\"127.0.0.1:8401\", \"Address to bind Serf on.\")\n\ncmd.Flags().Int(\"rpc-port\",\n\n8400, \"Port for RPC clients (and Raft) connections.\")\n\ncmd.Flags().StringSlice(\"start-join-addrs\",\n\nnil, \"Serf addresses to join.\")\n\ncmd.Flags().Bool(\"bootstrap\", false, \"Bootstrap the cluster.\")\n\ncmd.Flags().String(\"acl-model-file\", \"\", \"Path to ACL model.\") cmd.Flags().String(\"acl-policy-file\", \"\", \"Path to ACL policy.\")\n\ncmd.Flags().String(\"server-tls-cert-file\", \"\", \"Path to server tls cert.\") cmd.Flags().String(\"server-tls-key-file\", \"\", \"Path to server tls key.\") cmd.Flags().String(\"server-tls-ca-file\",\n\n\"\", \"Path to server certificate authority.\")\n\ncmd.Flags().String(\"peer-tls-cert-file\", \"\", \"Path to peer tls cert.\") cmd.Flags().String(\"peer-tls-key-file\", \"\", \"Path to peer tls key.\") cmd.Flags().String(\"peer-tls-ca-file\",\n\n\"\", \"Path to peer certificate authority.\")\n\nreturn viper.BindPFlags(cmd.Flags())\n\n}\n\nThese flags allow people calling your CLI to configure the agent and learn the default configuration.\n\nWith the pflag.FlagSet.{{type}}Var() methods, we can set our configuration’s values directly. However, the problem with setting the configurations directly is that not all types have supporting APIs out of the box. Our BindAddr configuration is an example, which is a *net.TCPAddr that we need to parse from a string. You can define custom flag values7 when you have enough flags of the same type, or just use an intermediate value otherwise.\n\nBut what if we want to configure our service with more than flags, such as with a file? We’ll look at how to read in the configuration from a file, too, for dynamic configurations.\n\nManage Your Configuration\n\nViper provides a centralized config registry system where multiple configuration sources can set the configuration but you can read the result in one place.\n\n7.\n\nhttps://golang.org/pkg/flag/#Value\n\nreport erratum • discuss",
      "content_length": 2041,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 201,
      "content": "Write an Agent Command-Line Interface • 199\n\nYou could allow users to set the configuration with flags, a file, or by loading dynamic configs from a service like Consul—Viper supports all of these.\n\nWith a configuration file, you can support dynamic config changes to a running service. The service watches the config file for changes and updates accord- ingly. For example, you may run your service at INFO-level logs by default but need DEBUG-level logs when you’re debugging an issue with the running service. A configuration file also enables other processes to set up the config- uration for the service. We’ll see an example of that with our service where we have an init container that sets up the configuration for the service’s container.\n\nI’ve given usable defaults for the configurations we have to set: the data directory, bind address, the RPC port, and the node name. Try to set usable default flag values instead of requiring users to set them.\n\nAfter declaring the flags, the next step is to execute the root command to parse the process’s arguments and search through the command tree to find the correct command to run. We just have the one command, so we’re not making Cobra work hard.\n\nAdd this snippet to set up the config:\n\nDeployLocally/cmd/proglog/main.go func (c *cli) setupConfig(cmd *cobra.Command, args []string) error {\n\nvar err error\n\nconfigFile, err := cmd.Flags().GetString(\"config-file\") if err != nil {\n\nreturn err\n\n} viper.SetConfigFile(configFile)\n\nif err = viper.ReadInConfig(); err != nil {\n\n// it's ok if config file doesn't exist if _, ok := err.(viper.ConfigFileNotFoundError); !ok {\n\nreturn err\n\n}\n\n}\n\nc.cfg.DataDir = viper.GetString(\"data-dir\") c.cfg.NodeName = viper.GetString(\"node-name\") c.cfg.BindAddr = viper.GetString(\"bind-addr\") c.cfg.RPCPort = viper.GetInt(\"rpc-port\") c.cfg.StartJoinAddrs = viper.GetStringSlice(\"start-join-addrs\") c.cfg.Bootstrap = viper.GetBool(\"bootstrap\") c.cfg.ACLModelFile = viper.GetString(\"acl-mode-file\") c.cfg.ACLPolicyFile = viper.GetString(\"acl-policy-file\") c.cfg.ServerTLSConfig.CertFile = viper.GetString(\"server-tls-cert-file\") c.cfg.ServerTLSConfig.KeyFile = viper.GetString(\"server-tls-key-file\")\n\nreport erratum • discuss",
      "content_length": 2210,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 202,
      "content": "Chapter 10. Deploy Applications with Kubernetes Locally • 200\n\nc.cfg.ServerTLSConfig.CAFile = viper.GetString(\"server-tls-ca-file\") c.cfg.PeerTLSConfig.CertFile = viper.GetString(\"peer-tls-cert-file\") c.cfg.PeerTLSConfig.KeyFile = viper.GetString(\"peer-tls-key-file\") c.cfg.PeerTLSConfig.CAFile = viper.GetString(\"peer-tls-ca-file\")\n\nif c.cfg.ServerTLSConfig.CertFile != \"\" &&\n\nc.cfg.ServerTLSConfig.KeyFile != \"\" { c.cfg.ServerTLSConfig.Server = true c.cfg.Config.ServerTLSConfig, err = config.SetupTLSConfig(\n\nc.cfg.ServerTLSConfig,\n\n) if err != nil {\n\nreturn err\n\n}\n\n}\n\nif c.cfg.PeerTLSConfig.CertFile != \"\" &&\n\nc.cfg.PeerTLSConfig.KeyFile != \"\" { c.cfg.Config.PeerTLSConfig, err = config.SetupTLSConfig(\n\nc.cfg.PeerTLSConfig,\n\n) if err != nil {\n\nreturn err\n\n}\n\n}\n\nreturn nil\n\n}\n\nsetupConfig(cmd *cobra.Command, args []string) reads the configuration and prepares the agent’s configuration. Cobra calls setupConfig() before running the command’s RunE function.\n\nFinish writing the program by including this run() method:\n\nDeployLocally/cmd/proglog/main.go func (c *cli) run(cmd *cobra.Command, args []string) error {\n\nvar err error agent, err := agent.New(c.cfg.Config) if err != nil {\n\nreturn err\n\n} sigc := make(chan os.Signal, 1) signal.Notify(sigc, syscall.SIGINT, syscall.SIGTERM) <-sigc return agent.Shutdown()\n\n}\n\nrun(cmd *cobra.Command, args []string) runs our executable’s logic by:\n\nCreating the agent; • Handling signals from the operating system; and\n\nreport erratum • discuss",
      "content_length": 1491,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 203,
      "content": "Build Your Docker Image • 201\n\nShutting down the agent gracefully when the operating system terminates\n\nthe program.\n\nOkay, we have our executable that we can use as our Docker image’s entry point, so let’s write our Dockerfile and build the image.\n\nBuild Your Docker Image\n\nCreate a Dockerfile with this code:\n\nDeployLocally/Dockerfile FROM golang:1.14-alpine AS build WORKDIR /go/src/proglog COPY . . RUN CGO_ENABLED=0 go build -o /go/bin/proglog ./cmd/proglog\n\nFROM scratch COPY --from=build /go/bin/proglog /bin/proglog ENTRYPOINT [\"/bin/proglog\"]\n\nOur Dockerfile uses multistage builds: one stage builds our service and one stage runs it. This makes our Dockerfile easy to read and maintain while keeping our build efficient and the image small.\n\nThe build stage uses the golang:1.14-alpine image because we need the Go compiler, our dependencies, and perhaps various system libraries. These take up disk space, and we don’t need them after we have compiled our binary. In the second stage, we use the scratch empty image—the smallest Docker image. We copy our binary into this image, and this is the image we deploy.\n\nYou must statically compile your binaries for them to run in the scratch image because it doesn’t contain the system libraries needed to run dynamically linked binaries. That’s why we disable Cgo—the compiler links it dynamically. Using the scratch image helps with thinking of the containers as being immutable. Instead of exec’ing into a container and mutating the image by installing tools or changing the filesystem, you run a short-lived container that has the tool you need.\n\nThe next step is to add a target to your Makefile to build the Docker image by adding this snippet to the bottom of the file:\n\nDeployLocally/Makefile TAG ?= 0.0.1\n\nbuild-docker:\n\ndocker build -t github.com/travisjeffery/proglog:$(TAG) .\n\nreport erratum • discuss",
      "content_length": 1868,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 204,
      "content": "Chapter 10. Deploy Applications with Kubernetes Locally • 202\n\nThen build the image and load it into your Kind cluster by running:\n\n$ make build-docker $ kind load docker-image github.com/travisjeffery/proglog:0.0.1\n\nNow that we have our Docker image, let’s look at how we can configure and run a cluster of our service in Kubernetes with Helm.\n\nConfigure and Deploy Your Service with Helm\n\nHelm8 is the package manager for Kubernetes that enables you to distribute and install services in Kubernetes. Helm packages are called charts. A chart defines all resources needed to run a service in a Kubernetes cluster—for example, its deployments, services, persistent volume claims, and so on. Charts on Kubernetes are like Debian packages on Debian or Homebrew for- mulas on macOS. As a service developer, you’ll want to build and share a Helm chart for your service to make it easier for people to run your service. (And if you’re dogfooding your own service, you’ll get the same benefit.)\n\nA release is a instance of running a chart. Each time you install a chart into Kubernetes, Helm creates a release. In the Debian package and Homebrew formula examples, releases are like processes.\n\nAnd finally, repositories are where you share charts to and install charts from; they’re like Debian sources and Homebrew taps.\n\nTo install Helm, run this command:\n\n$ curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 \\\n\n| bash\n\nBefore we write our own Helm chart, let’s take Helm for a spin and install an existing chart. Bitnami9 maintains a repository of charts for popular applica- tions. Let’s add a Bitnami repository and install the Nginx chart, which is a web and proxy server:\n\n$ helm repo add bitnami https://charts.bitnami.com/bitnami $ helm install my-nginx bitnami/nginx\n\nWe can see the releases by running $ helm list:\n\n$ helm list NAME my-nginx\n\nNAMESPACE default\n\nREVISION 1\n\nUPDATED 2020...\n\nSTATUS...\n\ndeployed...\n\nLet’s request Nginx to confirm that it’s really running:\n\n8. 9.\n\nhttps://helm.sh\n\nhttps://bitnami.com/kubernetes\n\nreport erratum • discuss",
      "content_length": 2082,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 205,
      "content": "Configure and Deploy Your Service with Helm • 203\n\n$ POD_NAME=$(kubectl get pod \\\n\n--selector=app.kubernetes.io/name=nginx \\ --template '{{index .items 0 \"metadata\" \"name\" }}')\n\n$ SERVICE_IP=$(kubectl get svc \\\n\n--namespace default my-nginx --template \"{{ .spec.clusterIP }}\")\n\n$ kubectl exec $POD_NAME curl $SERVICE_IP\n\n% Total\n\n% Received % Xferd Average Speed Dload Upload\n\nTime Total\n\nTime Spent\n\nTime Current Left Speed\n\n100 <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> <style>\n\n612 100\n\n612\n\n0\n\n0\n\n597k\n\n0 --:--:-- --:--:-- --:--:-- 597k\n\nbody {\n\nwidth: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif;\n\n} </style> </head> <body> <h1>Welcome to nginx!</h1> <p>If you see this page, the nginx web server is successfully installed and working. Further configuration is required.</p>\n\n<p>For online documentation and support please refer to <a href=\"http://nginx.org/\">nginx.org</a>.<br/> Commercial support is available at <a href=\"http://nginx.com/\">nginx.com</a>.</p>\n\n<p><em>Thank you for using nginx.</em></p> </body> </html>\n\nWe could use the same technique for deploying Nginx in a production envi- ronment, aside from setting some configuration parameters to fit our use case. Helm made it easy to install and configure an Nginx cluster, and we can manage other services the same way.\n\nUninstall the Nginx release by running the following:\n\n$ helm uninstall my-nginx release \"my-nginx\" uninstalled\n\nNow, let’s build our own chart.\n\nBuild Your Own Helm Chart\n\nIn this section, we’ll build a Helm chart for our service and use it to install a cluster in our Kind cluster.\n\nreport erratum • discuss",
      "content_length": 1646,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 206,
      "content": "Chapter 10. Deploy Applications with Kubernetes Locally • 204\n\nCreate your Helm chart by running these commands:\n\n$ mkdir deploy && cd deploy $ helm create proglog\n\nHelm created a new chart in a new proglog directory that’s bootstrapped with an example that shows you what a Helm chart looks like—to write your own or to tweak for your own services. The proglog directory contains these directo- ries and files:\n\n. └── proglog\n\n├── charts ├── Chart.yaml ├── templates │ │ │ │ │ │ │ │ └── values.yaml\n\n├── deployment.yaml ├── _helpers.tpl ├── ingress.yaml ├── NOTES.txt ├── serviceaccount.yaml ├── service.yaml └── tests\n\n└── test-connection.yaml\n\n4 directories, 9 files\n\nThe Chart.yaml file describes your chart. You can access the data in this file in your templates. The charts directory may contain subcharts, though I’ve never needed subcharts.\n\nThe values.yaml contains your chart’s default values. Users can override these values when they install or upgrade your chart (for example, the port your service listens on, your service’s resource requirements, log level, and so on).\n\nThe templates directory contains template files that you render with your values to generate valid Kubernetes manifest files. Kubernetes applies the rendered manifest files to install the resources needed for your service. You write your Helm templates using the Go template language.\n\nYou can render the templates locally without applying the resources in your Kubernetes cluster by running $ helm template. This is useful when you’re developing your templates or if you want to apply your changes in a two-step plan-then-apply process because you can see the rendered resources that Kubernetes will apply.\n\nreport erratum • discuss",
      "content_length": 1719,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 207,
      "content": "Configure and Deploy Your Service with Helm • 205\n\nTo check out the resources Helm would create with the example chart, run this command:\n\n$ helm template proglog\n\nYou’ll see the following:\n\n--- # Source: proglog/templates/serviceaccount.yaml apiVersion: v1 kind: ServiceAccount metadata:\n\nname: RELEASE-NAME-proglog labels:\n\nhelm.sh/chart: proglog-0.1.0 app.kubernetes.io/name: proglog app.kubernetes.io/instance: RELEASE-NAME app.kubernetes.io/version: \"1.16.0\" app.kubernetes.io/managed-by: Helm\n\n--- # Source: proglog/templates/service.yaml «rest»\n\nWe don’t need the example templates, so remove them by running this command:\n\n$ rm proglog/templates/**/*.yaml proglog/templates/NOTES.txt\n\nGenerally, Helm charts include a template file for each resource type. Our service will require two resource types: a StatefulSet and a Service, so we’ll have a statefulset.yaml file and a service.yaml file. Let’s begin with the StatefulSet.\n\nStatefulSets in Kubernetes\n\nYou use StatefulSets to manage stateful applications in Kubernetes, like our service that persists a log. You need a StatefulSet for any service that requires one or more of the following:\n\nStable, unique network identifiers—each node in our service requires\n\nunique node names as identifiers.\n\nStable, persistent storage—our service expects the data its written to\n\npersist across restarts.\n\nOrdered, graceful deployment and scaling—our service needs initial node\n\nto bootstrap the cluster and join subsequent nodes to its cluster.\n\nOrdered, automated rolling updates—we always want our cluster to have a leader, and when we roll the leader we want to give the cluster enough time to elect a new leader before rolling the next node.\n\nreport erratum • discuss",
      "content_length": 1723,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 208,
      "content": "Chapter 10. Deploy Applications with Kubernetes Locally • 206\n\nAnd by “stable,” I mean persisted across scheduling changes like restarts and scaling.\n\nIf your service isn’t stateful and doesn’t require these features, then you should use a Deployment instead of a StatefulSet. One example is an API service that persists to a relational database, like Postgres. You’d run the API service with a Deployment because it’s stateless, and you’d run Postgres with a StatefulSet.\n\nCreate a deploy/proglog/templates/statefulset.yaml file with this code:\n\nDeployLocally/deploy/proglog/templates/statefulset.yaml apiVersion: apps/v1 kind: StatefulSet metadata:\n\nname: {{ include \"proglog.fullname\" . }} namespace: {{ .Release.Namespace }} labels: {{ include \"proglog.labels\" . | nindent 4 }}\n\nspec:\n\nselector:\n\nmatchLabels: {{ include \"proglog.selectorLabels\" . | nindent 6 }}\n\nserviceName: {{ include \"proglog.fullname\" . }} replicas: {{ .Values.replicas }} template:\n\nmetadata:\n\nname: {{ include \"proglog.fullname\" . }} labels: {{ include \"proglog.labels\" . | nindent 8 }}\n\nspec:\n\n# initContainers... # containers...\n\nvolumeClaimTemplates: - metadata:\n\nname: datadir\n\nspec:\n\naccessModes: [ \"ReadWriteOnce\" ] resources:\n\nrequests:\n\nstorage: {{ .Values.storage }}\n\nI have omitted the spec’s initContainers and containers fields to make the snippet smaller (we will fill those in next). The only thing of note here is that our StatefulSet has a datadir PersistentVolumeClaim—the claim requests storage for our cluster. Based on our configuration, Kubernetes could fulfill the claim with a local disk, a disk provided by your cloud platform, and so on. Kubernetes takes care of obtaining and binding the storage to your containers.\n\nreport erratum • discuss",
      "content_length": 1745,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 209,
      "content": "Configure and Deploy Your Service with Helm • 207\n\nNow, replace initContainers... in the previous snippet with this code:\n\nDeployLocally/deploy/proglog/templates/statefulset.yaml initContainers: - name: {{ include \"proglog.fullname\" . }}-config-init\n\nimage: busybox imagePullPolicy: IfNotPresent command:\n\n/bin/sh - -c - |-\n\nID=$(echo $HOSTNAME | rev | cut -d- -f1 | rev) cat > /var/run/proglog/config.yaml <<EOD data-dir: /var/run/proglog/data rpc-port: {{.Values.rpcPort}} # Make sure the following three key-values are on one line each in # your code. I split them across multiple lines to fit them in # for the book. bind-addr: \\\n\n\"$HOSTNAME.proglog.{{.Release.Namespace}}.\\svc.cluster.local:\\\n\n{{.Values.serfPort}}\"\n\nbootstrap: $([ $ID = 0 ] && echo true || echo false) $([ $ID != 0 ] && echo 'start-join-addrs: \\\n\n\"proglog-0.proglog.{{.Release.Namespace}}.svc.cluster.local:\\\n\n{{.Values.serfPort}}\"')\n\nEOD\n\nvolumeMounts: - name: datadir\n\nmountPath: /var/run/proglog\n\nInit containers run to completion before the StatefulSet’s app containers listed in the containers field. Our config init container sets up our service’s configuration file. We configure the first server to bootstrap the Raft cluster. And we configure the subsequent servers to join the cluster. We mount the datadir volume into the container so we can write to the same configuration file our app container will read from later.\n\nReplace containers... in the previous snippet with this:\n\nDeployLocally/deploy/proglog/templates/statefulset.yaml containers: - name: {{ include \"proglog.fullname\" . }}\n\nimage: \"{{ .Values.image.repository }}:{{ .Values.image.tag }}\" ports: - containerPort: {{ .Values.rpcPort }}\n\nname: rpc\n\ncontainerPort: {{ .Values.serfPort }}\n\nname: serf\n\nargs:\n\n--config-file=/var/run/proglog/config.yaml\n\nreport erratum • discuss",
      "content_length": 1822,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 210,
      "content": "Chapter 10. Deploy Applications with Kubernetes Locally • 208\n\n# probes... volumeMounts: - name: datadir\n\nmountPath: /var/run/proglog\n\nThese containers define our StatefulSet’s app containers; we need one for our service. We mount the volume to the container for reading the configuration file and persisting the log. We use a flag to tell our service where to find its configuration file.\n\nContainer Probes and gRPC Health Check\n\nKubernetes uses probes to know whether it needs to act on a container to improve your service’s reliability. With a service, usually the probe requests a health check endpoint that responds with the health of the service.\n\nThere are three types of probes:\n\nLiveness probes signal that the container is alive, otherwise Kubernetes will restart the container. Kubernetes calls the liveness probe throughout the container’s lifetime.\n\nReadiness probes check that the container is ready to accept traffic, oth- erwise Kubernetes will remove the pod from the service load balancers. Kubernetes calls the readiness probe throughout the container’s lifetime.\n\nStartup probes signal when the container application has started and Kubernetes can begin probing for liveness and readiness. Distributed services often need to go through service discovery and join in consensus with the cluster before they’re initialized. If we had a liveness probe that failed before our service finished initializing, our service would continually restart. After startup, Kubernetes doesn’t call this probe again.\n\nThese probes should help improve your service’s reliability, but they can cause incidents if they’re not carefully implemented (like the example of the liveness probe that restarts the container before it’s finished initializing). The systems dedicated to improving the reliability of the service can cause more incidents than the service by itself.\n\nYou have three ways of running probes:\n\nMaking an HTTP request against a server; • Opening a TCP socket against a server; and • Running a command in the container (for example, Postgres has a com- mand called pg_isready that connects to a Postgres server).\n\nreport erratum • discuss",
      "content_length": 2152,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 211,
      "content": "Configure and Deploy Your Service with Helm • 209\n\nThe first two are lightweight because they don’t require any extra binaries in your image. However, a command can be more precise and necessary if you use your own protocol.\n\ngRPC services conventionally use a grpc_health_probe command that expects your server to satisfy the gRPC health checking protocol.10 Our server needs to export a service defined as:\n\nsyntax = \"proto3\";\n\npackage grpc.health.v1;\n\nmessage HealthCheckRequest {\n\nstring service = 1;\n\n}\n\nmessage HealthCheckResponse {\n\nenum ServingStatus {\n\nUNKNOWN = 0; SERVING = 1; NOT_SERVING = 2;\n\n} ServingStatus status = 1;\n\n}\n\nservice Health {\n\nrpc Check(HealthCheckRequest) returns (HealthCheckResponse);\n\nrpc Watch(HealthCheckRequest) returns (stream HealthCheckResponse);\n\n}\n\nLet’s update our server to export the health check service.\n\nOpen internal/server/server.go and add the highlighted imports:\n\nDeployLocally/internal/server/server.go import (\n\n\"context\" \"time\"\n\napi \"github.com/travisjeffery/proglog/api/v1\"\n\ngrpc_middleware \"github.com/grpc-ecosystem/go-grpc-middleware\" grpc_auth \"github.com/grpc-ecosystem/go-grpc-middleware/auth\" grpc_zap \"github.com/grpc-ecosystem/go-grpc-middleware/logging/zap\" grpc_ctxtags \"github.com/grpc-ecosystem/go-grpc-middleware/tags\"\n\n\"go.opencensus.io/plugin/ocgrpc\" \"go.opencensus.io/stats/view\" \"go.opencensus.io/trace\"\n\n10. https://github.com/grpc/grpc/blob/master/doc/health-checking.md\n\nreport erratum • discuss",
      "content_length": 1472,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 212,
      "content": "➤ ➤\n\nChapter 10. Deploy Applications with Kubernetes Locally • 210\n\n\"go.uber.org/zap\" \"go.uber.org/zap/zapcore\"\n\n\"google.golang.org/grpc\" \"google.golang.org/grpc/codes\" \"google.golang.org/grpc/credentials\" \"google.golang.org/grpc/peer\" \"google.golang.org/grpc/status\"\n\n\"google.golang.org/grpc/health\" healthpb \"google.golang.org/grpc/health/grpc_health_v1\"\n\n)\n\nThen, update the NewGRPCServer() function to include the highlighted lines in this snippet:\n\nDeployLocally/internal/server/server.go func NewGRPCServer(config *Config, grpcOpts ...grpc.ServerOption) (\n\ngrpc.Server, error,\n\n) {\n\nlogger := zap.L().Named(\"server\") zapOpts := []grpc_zap.Option{\n\ngrpc_zap.WithDurationField(\n\nfunc(duration time.Duration) zapcore.Field {\n\nreturn zap.Int64(\n\n\"grpc.time_ns\", duration.Nanoseconds(),\n\n)\n\n},\n\n),\n\n}\n\ntrace.ApplyConfig(trace.Config{\n\nDefaultSampler: trace.AlwaysSample(),\n\n}) err := view.Register(ocgrpc.DefaultServerViews...) if err != nil {\n\nreturn nil, err\n\n}\n\ngrpcOpts = append(grpcOpts,\n\ngrpc.StreamInterceptor(\n\ngrpc_middleware.ChainStreamServer(\n\ngrpc_ctxtags.StreamServerInterceptor(), grpc_zap.StreamServerInterceptor(\n\nlogger, zapOpts...,\n\n), grpc_auth.StreamServerInterceptor(\n\nauthenticate,\n\n),\n\n)), grpc.UnaryInterceptor( grpc_middleware.ChainUnaryServer(\n\nreport erratum • discuss",
      "content_length": 1296,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 213,
      "content": "➤ ➤ ➤\n\n➤\n\nConfigure and Deploy Your Service with Helm • 211\n\ngrpc_ctxtags.UnaryServerInterceptor(), grpc_zap.UnaryServerInterceptor(\n\nlogger, zapOpts...,\n\n), grpc_auth.UnaryServerInterceptor(\n\nauthenticate,\n\n),\n\n)),\n\ngrpc.StatsHandler(&ocgrpc.ServerHandler{}),\n\n) gsrv := grpc.NewServer(grpcOpts...)\n\nhsrv := health.NewServer() hsrv.SetServingStatus(\"\", healthpb.HealthCheckResponse_SERVING) healthpb.RegisterHealthServer(gsrv, hsrv)\n\nsrv, err := newgrpcServer(config) if err != nil {\n\nreturn nil, err\n\n} api.RegisterLogServer(gsrv, srv) return gsrv, nil\n\n}\n\nThese lines create a service that supports the health check protocol. We set its serving status as serving so that the probe knows the service is alive and ready to accept connections. Then we register the service with our server so that gRPC can call this service’s endpoints.\n\nReplace probes... in deploy/proglog/templates/statefulset.yaml with this snippet to tell Kubernetes how to probe our service:\n\nDeployLocally/deploy/proglog/templates/statefulset.yaml readinessProbe:\n\nexec:\n\ncommand: [\"/bin/grpc_health_probe\", \"-addr=:{{ .Values.rpcPort }}\"]\n\ninitialDelaySeconds: 10\n\nlivenessProbe:\n\nexec:\n\ncommand: [\"/bin/grpc_health_probe\", \"-addr=:{{ .Values.rpcPort }}\"]\n\ninitialDelaySeconds: 10\n\nThen add these highlighted lines to your Dockerfile to install the grpc_health_probe executable in your image:\n\nDeployLocally/Dockerfile FROM golang:1.14-alpine AS build WORKDIR /go/src/proglog COPY . . RUN CGO_ENABLED=0 go build -o /go/bin/proglog ./cmd/proglog RUN GRPC_HEALTH_PROBE_VERSION=v0.3.2 && \\\n\nreport erratum • discuss",
      "content_length": 1586,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 214,
      "content": "➤ ➤ ➤ ➤\n\n➤\n\nChapter 10. Deploy Applications with Kubernetes Locally • 212\n\nwget -qO/go/bin/grpc_health_probe \\ https://github.com/grpc-ecosystem/grpc-health-probe/releases/download/\\ ${GRPC_HEALTH_PROBE_VERSION}/grpc_health_probe-linux-amd64 && \\ chmod +x /go/bin/grpc_health_probe\n\nFROM scratch COPY --from=build /go/bin/proglog /bin/proglog COPY --from=build /go/bin/grpc_health_probe /bin/grpc_health_probe ENTRYPOINT [\"/bin/proglog\"]\n\nThe last resource we need to define in our Helm chart is the Service.\n\nKubernetes Services\n\nA Service in Kubernetes exposes an application as a network service. You define a Service with policies that specify what Pods the Service applies to and how to access the Pods.\n\nFour types of services specify how the Service exposes the Pods:\n\nClusterIP exposes the Service on a load-balanced cluster-internal IP so the Service is reachable within the Kubernetes cluster only. This is the default Service type.\n\nNodePort exposes the Service on each Node’s IP on a static port—even if the Node doesn’t have a Pod on it, Kubernetes sets up the routing so if you request a Node at the service’s port, it’ll direct the request to the proper place. You can request NodePort services outside the Kubernetes cluster.\n\nLoadBalancer exposes the Service externally using a cloud provider’s load balancer. A LoadBalancer Service automatically creates ClusterIP and NodeIP services behind the scenes and manages the routes to these ser- vices.\n\nExternalName is a special Service that serves as a way to alias a DNS\n\nname.\n\nI don’t recommend using NodePort services (aside from the ones LoadBalancer services create for you). You have to know your nodes’ IPs to use the services, you must secure all your Nodes, and you have to deal with port conflicts. Instead, I recommend using a LoadBalancer or a ClusterIP service if you’re able to run a Pod that can access your internal network.\n\nCreate a deploy/proglog/templates/service.yaml for your service template with the following code:\n\nreport erratum • discuss",
      "content_length": 2029,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 215,
      "content": "➤\n\nAdvertise Raft on the Fully Qualified Domain Name • 213\n\nDeployLocally/deploy/proglog/templates/service.yaml apiVersion: v1 kind: Service metadata:\n\nname: {{ include \"proglog.fullname\" . }} namespace: {{ .Release.Namespace }} labels: {{ include \"proglog.labels\" . | nindent 4 }}\n\nspec:\n\nclusterIP: None publishNotReadyAddresses: true ports:\n\nname: rpc\n\nport: {{ .Values.rpcPort }} targetPort: {{ .Values.rpcPort }}\n\nname: serf-tcp\n\nprotocol: \"TCP\" port: {{ .Values.serfPort }} targetPort: {{ .Values.serfPort }}\n\nname: serf-udp\n\nprotocol: \"UDP\" port: {{ .Values.serfPort }} targetPort: {{ .Values.serfPort }}\n\nselector: {{ include \"proglog.selectorLabels\" . | nindent 4 }}\n\nThis snippet defines our “headless” Service. A headless Service doesn’t load balance to a single IP. You use a headless Service when your distributed service has its own means for service discovery. By defining selectors on our Service, Kubernetes’ endpoint controller changes the DNS configuration to return records that point to the Pods backing the Service. So, each pod will get its own DNS record similar to proglog-{{id}}.proglog.{{namespace}}.svc.cluster.local, and the servers will use these records to discover each other.\n\nAdvertise Raft on the Fully Qualified Domain Name\n\nCurrently, we configure Raft’s address as the transport’s local address, and the server will advertise its address as ::8400. We want to use the fully qualified domain name instead so the node will properly advertise itself to its cluster and to its clients.\n\nIn internal/log/config.go, change your Config to this:\n\nDeployLocally/internal/log/config.go type Config struct {\n\nRaft struct {\n\nraft.Config BindAddr string StreamLayer *StreamLayer Bootstrap\n\nbool\n\n}\n\nreport erratum • discuss",
      "content_length": 1748,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 216,
      "content": "➤\n\nChapter 10. Deploy Applications with Kubernetes Locally • 214\n\nSegment struct {\n\nMaxStoreBytes uint64 MaxIndexBytes uint64 InitialOffset uint64\n\n}\n\n}\n\nChange your DistributedLog’s bootstrap code to use the configured bind address:\n\nDeployLocally/internal/log/distributed.go if l.config.Raft.Bootstrap && !hasState {\n\nconfig := raft.Configuration{\n\nServers: []raft.Server{{\n\nID: Address: raft.ServerAddress(l.config.Raft.BindAddr),\n\nconfig.LocalID,\n\n}},\n\n} err = l.raft.BootstrapCluster(config).Error()\n\n}\n\nAnd in distributed_test.go, update your log configuration to set the address:\n\nDeployLocally/internal/log/distributed_test.go config := log.Config{} config.Raft.StreamLayer = log.NewStreamLayer(ln, nil, nil) config.Raft.LocalID = raft.ServerID(fmt.Sprintf(\"%d\", i)) config.Raft.HeartbeatTimeout = 50 * time.Millisecond config.Raft.ElectionTimeout = 50 * time.Millisecond config.Raft.LeaderLeaseTimeout = 50 * time.Millisecond config.Raft.CommitTimeout = 5 * time.Millisecond config.Raft.BindAddr = ln.Addr().String()\n\nRun your log tests to verify they pass.\n\nFinally, in agent.go, update setupMux() and setupLog() to configure the mux and Raft instance:\n\nDeployLocally/internal/agent/agent.go func (a *Agent) setupMux() error {\n\naddr, err := net.ResolveTCPAddr(\"tcp\", a.Config.BindAddr) if err != nil {\n\nreturn err\n\n} rpcAddr := fmt.Sprintf( \"%s:%d\", addr.IP.String(), a.Config.RPCPort,\n\n) ln, err := net.Listen(\"tcp\", rpcAddr) if err != nil {\n\nreturn err\n\n}\n\nreport erratum • discuss",
      "content_length": 1493,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 217,
      "content": "➤ ➤ ➤ ➤ ➤\n\nAdvertise Raft on the Fully Qualified Domain Name • 215\n\na.mux = cmux.New(ln) return nil\n\n}\n\nfunc (a *Agent) setupLog() error {\n\n// ... logConfig := log.Config{} logConfig.Raft.StreamLayer = log.NewStreamLayer(\n\nraftLn, a.Config.ServerTLSConfig, a.Config.PeerTLSConfig,\n\n) rpcAddr, err := a.Config.RPCAddr() if err != nil {\n\nreturn err\n\n} logConfig.Raft.BindAddr = rpcAddr logConfig.Raft.LocalID = raft.ServerID(a.Config.NodeName) logConfig.Raft.Bootstrap = a.Config.Bootstrap // ...\n\n}\n\nNow we’re ready to deploy the service in our Kubernetes cluster.\n\nInstall Your Helm Chart\n\nWe’ve finished writing our Helm chart and we can install it in our Kind cluster to run a cluster of our service.\n\nYou can see what Helm renders by running:\n\n$ helm template proglog deploy/proglog\n\nYou’ll see that the repository is still set to the default: nginx. Open up deploy/proglog/values.yaml and replace the entire contents to look like this:\n\nDeployLocally/deploy/proglog/values.yaml # Default values for proglog. image:\n\nrepository: github.com/travisjeffery/proglog tag: 0.0.1 pullPolicy: IfNotPresent\n\nserfPort: 8401 rpcPort: 8400 replicas: 3 storage: 1Gi\n\nThe point of the values.yml is to set good defaults and show what parameters users can set if they must.\n\nreport erratum • discuss",
      "content_length": 1287,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 218,
      "content": "Chapter 10. Deploy Applications with Kubernetes Locally • 216\n\nNow, install the Chart by running this command:\n\n$ helm install proglog deploy/proglog\n\nWait a few seconds and you’ll see Kubernetes set up three pods. You can list them by running $ kubectl get pods. When all three pods are ready, we can try requesting the API.\n\nWe can tell Kubernetes to forward a pod or a Service’s port to a port on your computer so you can request a service running inside Kubernetes without a load balancer:\n\n$ kubectl port-forward pod/proglog-0 8400 8400\n\nNow we can request our service from a program running outside Kubernetes at :8400.\n\nLet’s write a simple executable to get the list of servers. Create a file named cmd/getservers/main.go that looks like this:\n\nDeployLocally/cmd/getservers/main.go package main\n\nimport (\n\n\"context\" \"flag\" \"fmt\" \"log\"\n\napi \"github.com/travisjeffery/proglog/api/v1\" \"google.golang.org/grpc\"\n\n)\n\nfunc main() {\n\naddr := flag.String(\"addr\", \":8400\", \"service address\") flag.Parse() conn, err := grpc.Dial(*addr, grpc.WithInsecure()) if err != nil {\n\nlog.Fatal(err)\n\n} client := api.NewLogClient(conn) ctx := context.Background() res, err := client.GetServers(ctx, &api.GetServersRequest{}) if err != nil {\n\nlog.Fatal(err)\n\n} fmt.Println(\"servers:\") for _, server := range res.Servers {\n\nfmt.Printf(\"\\t- %v\\n\", server)\n\n}\n\n}\n\nreport erratum • discuss",
      "content_length": 1370,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 219,
      "content": "What You Learned • 217\n\nThen, run the command to request our service to get and print the list of servers:\n\n$ go run cmd/getservers/main.go\n\nYou should see the following output:\n\nservers: - id:\"proglog-0\" rpc_addr:\"proglog-0.proglog.default.svc.cluster.local:8400\" - id:\"proglog-1\" rpc_addr:\"proglog-1.proglog.default.svc.cluster.local:8400\" - id:\"proglog-2\" rpc_addr:\"proglog-2.proglog.default.svc.cluster.local:8400\"\n\nThis means all three servers in our cluster have successfully joined the cluster and are coordinating with each other!\n\nWhat You Learned\n\nIn this chapter, you learned the fundamentals of Kubernetes and how to use Kind to set up a Kubernetes cluster that you can run on your machine or on a CI. You also learned how to create a Helm chart and how to install your Helm chart into Kubernetes to run a cluster of your service. You learned quite a lot! In the next chapter, we’ll build on this knowledge and deploy your service on a cloud platform.\n\nreport erratum • discuss",
      "content_length": 989,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 220,
      "content": "CHAPTER 11\n\nDeploy Applications with Kubernetes to the Cloud\n\nIn the previous chapter, we put the work into making our service deployable, but we only deployed it locally. In this chapter, we’ll deploy our service to the cloud and put it on the Internet. Kubernetes abstracts the resources needed for your applications—containers, networking, volumes, and so on—similar to how Go abstracts the operating system and processor architecture so you can run the same program on each. As such, the changes you need to make to take your local Kubernetes cluster to the cloud can be little to nothing.\n\nThree major cloud platforms dominate the landscape: Google Cloud Platform (GCP),1 Amazon Web Services (AWS),2 and Microsoft Azure.3 All three platforms provide similar feature sets and their own Kubernetes services. With Kuber- netes making up the differences between the platforms, we can deploy to any one, easily move between providers (and bargain with the providers for better prices), or run across them all at the same time. In this chapter, we’ll deploy our service to the Google Cloud Platform.\n\nGCP provides a free tier of products, with limitations, along with $300 credit to spend during your twelve-month free trial. What matters to us for purposes of our work in this book is that the free tier includes one Kubernetes cluster and 5 GB of storage—good enough to deploy our service to the cloud. Though Google won’t charge you for the free trial, you need a credit card to sign up, and during the trial, Google displays a banner showing how many credits and how much time you have left so you know your status. Once your trial\n\n1. 2. 3.\n\nhttps://cloud.google.com\n\nhttps://aws.amazon.com\n\nhttps://azure.microsoft.com/en-us\n\nreport erratum • discuss",
      "content_length": 1755,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 221,
      "content": "Chapter 11. Deploy Applications with Kubernetes to the Cloud • 220\n\nis over and/or you decide to purchase the service and use more of the plat- form, Google requires you to enable automatic billing.\n\nCreate a Google Kubernetes Engine Cluster\n\nLet’s start by getting you set up with Google Cloud by creating an account and Google Kubernetes Engine (GKE) cluster and configuring your computer’s Docker and kubectl to work with the cloud services. GKE is GCP’s managed Kubernetes service, enabling you to create a Kubernetes cluster with a single- click. GKE clusters are managed by Google’s Site Reliability Engineers, who ensure that your cluster is available and up-to-date so that you can focus on your applications instead of Kubernetes.\n\nSign Up with Google Cloud\n\nOpen the GCP sign-up form4 and log in to your existing Google account or make a new account. Follow the form instructions, filling in the form with your details until you’ve started your free trial. Then continue to the next step to create a Kubernetes cluster.\n\nCreate a Kubernetes Cluster\n\nNavigate to the Kubernetes Engine service5 and click Create cluster to open the cluster creation form shown in the screenshot that follows. In the form, change the name field from its default cluster-1 to proglog. Keep the location type as its default (Zonal). In the master version section, select the Release channel radio and select the current Regular channel, which is 1.16.11-gke.5 as I write this. Then click the Create button at the bottom of the page. The page will refresh and show a spinner that indicates GCP is provisioning the cluster. You’ll see a green check mark when the cluster is ready, as shown on page 221.\n\nInstall and Authenticate gcloud\n\nGoogle Cloud provides a cloud software development kit (SDK) with various tools and libraries for working with Google’s services. The SDK includes the gcloud CLI, which we need to interact with the Google Cloud APIs and config- ure Docker. Install the latest Cloud SDK by following the installation instructions for your OS from the Google Cloud Developer Tools page.6\n\n4. 5. 6.\n\nhttps://console.cloud.google.com/freetrial/signup/tos?pli=1\n\nhttps://console.cloud.google.com/kubernetes\n\nhttps://cloud.google.com/sdk/docs/downloads-versioned-archives\n\nreport erratum • discuss",
      "content_length": 2297,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 222,
      "content": "Create a Google Kubernetes Engine Cluster • 221\n\nAfter you’ve installed the gcloud CLI, authenticate the CLI for your account by running this command:\n\n$ gcloud auth login\n\nNow that you’ve authenticated the CLI, you can run gcloud commands against resources in your account. Get your project’s ID, and configure gcloud to use the project by default by running the following:\n\n$ PROJECT_ID=$(gcloud projects list | tail -n 1 | cut -d' ' -f1) $ gcloud config set project $PROJECT_ID\n\nWe’ll refer this PROJECT_ID environment variable several times, so if you make a new terminal session, make sure you set the variable again.\n\nreport erratum • discuss",
      "content_length": 648,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 223,
      "content": "Chapter 11. Deploy Applications with Kubernetes to the Cloud • 222\n\nPush Our Service’s Image to Google’s Container Registry\n\nWe need to make our service’s image pullable by our GKE cluster’s nodes by pushing its image to Google’s Container Registry. Run the following to push the image to the registry:\n\n$ gcloud auth configure-docker $ docker tag github.com/travisjeffery/proglog:0.0.1 \\\n\ngcr.io/$PROJECT_ID/proglog:0.0.1\n\n$ docker push gcr.io/$PROJECT_ID/proglog:0.0.1\n\nThe first line configures Docker to use Google’s Container Registry and use gcloud as the credential helper for those registries. You can open your Docker configuration file (at ~/.docker/config.json by default) to see the configuration changes. The second line creates a new tag for the gcr.io registry name. The gcr.io registry hosts images in the United States (though that may change). You’ll also find us.gcr.io, eu.gcr.io, and asia.gcr.io if you need your images in specific regions. The third line pushes the image to the registry.\n\nConfigure kubectl\n\nThe last bit of setup allows kubectl and Helm to call our GKE cluster:\n\n$ gcloud container clusters get-credentials proglog --zone us-central1-c Fetching cluster endpoint and auth data. kubeconfig entry generated for proglog.\n\nThis command updates your kubeconfig file (at ~/.kube/config by default) with the credentials and configuration to point kubectl at your cluster in GKE. Helm uses the kubeconfig file, too.\n\nOkay, we’ve set up our Google Cloud project, created a GKE cluster, and configured our clients to manage the cluster. We could deploy our service as- is to GKE, but Kubernetes won’t make our service available on the Internet with our current deployment setup.\n\nLet’s fix that.\n\nCreate Custom Controllers with Metacontroller\n\nWe could deploy our service with no changes and our service would function the same as it did in our local Kind cluster. But we want to extend our deployment setup to expose our service on the Internet. Because our service load balances client-side, each pod needs its own static IP, so we need a load balancer service for each pod. It’d be nice for Kubernetes to automatically create the load balancers as the pods scale up and delete them as the pods scale down, but Kubernetes doesn’t support this out of the box.\n\nreport erratum • discuss",
      "content_length": 2315,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 224,
      "content": "Create Custom Controllers with Metacontroller • 223\n\nEnter Metacontroller.\n\nMetacontroller7 is a Kubernetes add-on that makes it easy to write and deploy custom controllers with simple scripts. Metacontroller lets us hook into Kubernetes’ changes so that we can compose with our own changes. Metacon- troller handles all the interactions with Kubernetes’ API, including running a level-triggered reconciliation loop on your behalf. You just receive JSON describing Kubernetes’ observed state and return JSON describing your desired state. You can build features in Kubernetes that would require writing an Operator8 (a popular pattern for extending Kubernetes), with less code and effort than an Operator requires.\n\nInstall Metacontroller\n\nTo install Metacontroller, we need to apply a couple YAML files that define Metacontroller’s APIs and RBAC authorization that enable the APIs to manage the Kubernetes cluster’s resources. You can use two Metacontroller APIs:\n\nCompositeController, which is used to manage child resources based on some parent resource. The Deployment and StatefulSet controllers fit this pattern.\n\nDecoratorController, which is used to add behavior to a resource. This is the controller pattern we need and will build for our service-per-pod feature.\n\nNext, we use Helm to install Metacontroller. From the root of your project, run the following commands to define the Metacontroller Helm chart:\n\n$ cd deploy $ helm create metacontroller $ rm metacontroller/templates/**/*.yaml \\\n\nmetacontroller/templates/NOTES.txt \\ metacontroller/values.yaml\n\n$ MC_URL=https://raw.githubusercontent.com\\ /GoogleCloudPlatform/metacontroller/master/manifests/ $ curl -L $MC_URL/metacontroller-rbac.yaml > \\\n\nmetacontroller/templates/metacontroller-rbac.yaml\n\n$ curl -L $MC_URL/metacontroller.yaml > \\\n\nmetacontroller/templates/metacontroller.yaml\n\nThen install the Metacontroller chart by running these:\n\n$ kubectl create namespace metacontroller $ helm install metacontroller metacontroller\n\n7. 8.\n\nhttps://metacontroller.app\n\nhttps://coreos.com/blog/introducing-operators.html\n\nreport erratum • discuss",
      "content_length": 2110,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 225,
      "content": "Chapter 11. Deploy Applications with Kubernetes to the Cloud • 224\n\nNow we can update our proglog chart to support our service-per-pod feature and then deploy our service to the cloud.\n\nAdd Service-per-Pod Load Balancer Hooks\n\nWe’ll create a DecoratorController that adds a load balancer service for each pod in our service’s StatefulSet.\n\nCreate a deploy/proglog/templates/service-per-pod.yaml file with the following code to define our DecoratorController and Metacontroller configuration:\n\nDeployToCloud/deploy/proglog/templates/service-per-pod.yaml {{ if .Values.service.lb }} apiVersion: metacontroller.k8s.io/v1alpha1 kind: DecoratorController metadata:\n\nname: service-per-pod\n\nspec:\n\nresources: - apiVersion: apps/v1\n\nresource: statefulsets annotationSelector: matchExpressions: - {key: service-per-pod-label, operator: Exists} - {key: service-per-pod-ports, operator: Exists}\n\nattachments: - apiVersion: v1\n\nresource: services\n\nhooks:\n\nsync:\n\nwebhook:\n\nurl: \"http://service-per-pod.metacontroller/create-service-per-pod\"\n\nfinalize:\n\nwebhook:\n\nurl: \"http://service-per-pod.metacontroller/delete-service-per-pod\"\n\nOur DecoratorController decorates every StatefulSet with the service-per-pod- label and service-per-pod-ports annotations. The hooks field defines which hooks the controller will call. The sync hook should create and maintain the resources you desire for your StatefulSet. The finalize adds a finalizer to the StatefulSet that prevents Kubernetes from deleting the StatefulSet until the hook has had its chance to run and clean up its resources. Currently Metacontroller supports running webhooks, so we need an internal service and deployment to run the webhooks.\n\nPut this snippet after the previous snippet to define the webhook service and its configuration:\n\nreport erratum • discuss",
      "content_length": 1808,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 226,
      "content": "Create Custom Controllers with Metacontroller • 225\n\nDeployToCloud/deploy/proglog/templates/service-per-pod.yaml --- apiVersion: v1 kind: ConfigMap metadata:\n\nnamespace: metacontroller name: service-per-pod-hooks\n\ndata: {{ (.Files.Glob \"hooks/*\").AsConfig | indent 2 }} --- apiVersion: apps/v1 kind: Deployment metadata:\n\nname: service-per-pod namespace: metacontroller\n\nspec:\n\nreplicas: 1 selector:\n\nmatchLabels:\n\napp: service-per-pod\n\ntemplate:\n\nmetadata: labels:\n\napp: service-per-pod\n\nspec:\n\ncontainers: - name: hooks\n\nimage: metacontroller/jsonnetd:0.1 imagePullPolicy: Always workingDir: /hooks volumeMounts: - name: hooks\n\nmountPath: /hooks\n\nvolumes: - name: hooks configMap:\n\nname: service-per-pod-hooks\n\n--- apiVersion: v1 kind: Service metadata:\n\nname: service-per-pod namespace: metacontroller\n\nspec:\n\nselector:\n\napp: service-per-pod\n\nports: - port: 80\n\ntargetPort: 8080\n\n{{ end }}\n\nreport erratum • discuss",
      "content_length": 918,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 227,
      "content": "Chapter 11. Deploy Applications with Kubernetes to the Cloud • 226\n\nThis code snippet defines our webhook, Deployment and Service, with a ConfigMap that mounts our hook code files. Our controller calls the http://service- per-pod.metacontroller/create-service-per-pod endpoint when the StatefulSet changes, and calls the http://service-per-pod.metacontroller/delete-service-per-pod endpoint when the StatefulSet is deleted. The paths of the endpoints match the names of our hook filenames.\n\nCreate a hooks directory to put the hook code in:\n\n$ mkdir deploy/proglog/hooks\n\nAdd the hook to create the services by adding this create-service-per-pod.jsonnet file in the hooks directory:\n\nDeployToCloud/deploy/proglog/hooks/create-service-per-pod.jsonnet function(request) {\n\nlocal statefulset = request.object, local labelKey = statefulset.metadata.annotations[\"service-per-pod-label\"], local ports = statefulset.metadata.annotations[\"service-per-pod-ports\"],\n\nattachments: [\n\n{\n\napiVersion: \"v1\", kind: \"Service\", metadata: {\n\nname: statefulset.metadata.name + \"-\" + index, labels: {app: \"service-per-pod\"}\n\n}, spec: {\n\ntype: \"LoadBalancer\", selector: {\n\n[labelKey]: statefulset.metadata.name + \"-\" + index\n\n}, ports: [\n\n{\n\nlocal parts = std.split(portnums, \":\"), port: std.parseInt(parts[0]), targetPort: std.parseInt(parts[1]),\n\n} for portnums in std.split(ports, \",\")\n\n]\n\n}\n\n} for index in std.range(0, statefulset.spec.replicas - 1)\n\n]\n\n}\n\nreport erratum • discuss",
      "content_length": 1465,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 228,
      "content": "Create Custom Controllers with Metacontroller • 227\n\nWe’ve implemented our hook in Jsonnet,9 a data templating language that simply extends JSON with variables, conditionals, arithmetic, functions, imports, and errors. Kubernetes passes the StatefulSet we’ve decorated into the function. Our implementation iterates over each replica in the StatefulSet and builds a list of service attachments. We can attach arbitrary resources that are only connected to the target resource through owner references, meaning Kubernetes will delete them if the StatefulSet is deleted.\n\nNext, add the hook to delete the service:\n\nDeployToCloud/deploy/proglog/hooks/delete-service-per-pod.jsonnet function(request) { attachments: [], finalized: std.length(request.attachments['Service.v1']) == 0\n\n}\n\nIf the StatefulSet doesn’t match our decorator selector or the StatefulSet is deleted, then we delete any attachments we’ve made. If we observe that all the services are gone, we mark the StatefulSet as finalized so Kubernetes can delete it.\n\nLast, we must update our StatefulSet and set the annotations that signal Kubernetes to decorate this StatefulSet and create a service for each pod. Change the StatefulSet’s metadata defined in statefulset.yaml to include these annotations:\n\nDeployToCloud/deploy/proglog/templates/statefulset.yaml apiVersion: apps/v1 kind: StatefulSet metadata:\n\nname: {{ include \"proglog.fullname\" . }} namespace: {{ .Release.Namespace }} labels: {{ include \"proglog.labels\" . | nindent 4 }} {{ if .Values.service.lb }} annotations:\n\nservice-per-pod-label: \"statefulset.kubernetes.io/pod-name\" service-per-pod-ports: \"{{.Values.rpcPort}}:{{.Values.rpcPort}}\"\n\n{{ end }}\n\nspec:\n\n# ...\n\nAnd that’s all of our Metacontroller changes. Our service should create a load balancer service for each pod now. Let’s deploy our service to our GKE cluster and try it!\n\n9.\n\nhttps://jsonnet.org\n\nreport erratum • discuss",
      "content_length": 1914,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 229,
      "content": "Chapter 11. Deploy Applications with Kubernetes to the Cloud • 228\n\nDeploy to the Internet\n\nThis is the moment we’ve been building up to over the course of the book: deploying our distributed service to the cloud. Run the following command:\n\n$ helm install proglog proglog \\\n\n--set image.repository=gcr.io/$PROJECT_ID/proglog \\ --set service.lb=true\n\nThis command installs our proglog chart to our GKE cluster. We’ve set the image repository to configure the StatefulSet to pull the image from the Google Container Registry. And we’ve enabled the service-per-pod controller. You can watch as the services come up by passing the -w flag:\n\n$ kubectl get services -w\n\nWhen all three load balancers are up, we can verify that our client connects to our service running in the cloud and that our service nodes discovered each other:\n\n$ ADDR=$(kubectl get service \\\n\nl app=service-per-pod \\ -o go-template=\\ '{{range .items}}\\\n\n{{(index .status.loadBalancer.ingress 0).ip}}{{\"\\n\"}}\\\n\n{{end}}'\\ | head -n 1)\n\n$ go run cmd/getservers/main.go -addr=$ADDR:8400 servers: - id:\"proglog-0\" rpc_addr:\"proglog-0.proglog.default.svc.cluster.local:8400\" - id:\"proglog-1\" rpc_addr:\"proglog-1.proglog.default.svc.cluster.local:8400\" - id:\"proglog-2\" rpc_addr:\"proglog-2.proglog.default.svc.cluster.local:8400\"\n\nWhat You Learned\n\nCongratulations! You deployed your service to the cloud. Now any person on the Internet can use your service. You set up a Google Cloud account, a project, and a GKE cluster. You also learned how to write a simple controller to extend the behavior of Kubernetes resources with Metacontroller.\n\nWe’ve now reached the end of the book, and you’ve accomplished a lot. You’ve made a distributed service from scratch. You’ve learned distributed computing ideas like service discovery, consensus, and load balancing. You’re ready to make your own distributed services and contribute to existing projects.10\n\nGo leave your mark on this growing field!\n\n10. https://github.com/avelino/awesome-go#distributed-systems\n\nreport erratum • discuss",
      "content_length": 2041,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 230,
      "content": "SYMBOLS & operator, 20 * wildcard, 93 . (dot), 20\n\nDIGITS 1.14-alpine image, 201\n\nA Accept(), 158 access control, 78, 88–98,\n\n223\n\naccess control lists, 88–98 ACL tables, 85, 93 active segment, 25, 37 Addr field, 180 Addr(), 158 addresses\n\ndynamic configuration\n\nwith Viper, 199\n\nlisteners, 158 service discovery with custom pickers, 184 service discovery with\n\ncustom resolvers, 174, 179–180\n\nagents\n\nabout, 129 CLI agent for deploying\n\nwith Kubernetes, 193, 196–202\n\nclient-side load balanc-\n\ning, custom, 188\n\ncreating, 130 multiplexing Raft, 164–\n\n169\n\nreplication, 129–139\n\nAmazon Web Services (AWS)\n\ndeploying to, 219 Elastic Load Balancer\n\n(ELB), 172\n\nAny(), 168 api directory, 17 APIv2 (Go), 19 appending\n\nbuilding index, 31–37 building log code, 44–51 building segment, 37–40 building store, 26–31 in prototype for proglog\n\nproject, 7–11\n\nreplication with consen- sus, 149, 152, 155, 163\n\ntesting index, 35 testing log code, 49–51 testing segments, 41 testing store, 29–31 understanding, 25, 33\n\nApply(), finite-state machines,\n\n151–153, 155\n\nasia.gcr.io registry, 222 Attributes field, 180 authentication about, 76 vs. authorization, 78, 88 defined, 77 gcloud CLI, 221 mutual, 77, 86 with TLS, 76–88\n\nauthorization\n\nabout, 76, 78 with access control, 78,\n\n88–98, 223\n\nACL tables, 85, 93\n\nIndex\n\nvs. authentication, 78, 88 Metacontroller, 223 testing, 91–98\n\nAWS (Amazon Web Services)\n\ndeploying to, 219 Elastic Load Balancer\n\n(ELB), 172\n\nAzure, 219\n\nB backward compatibility\n\ngRPC advantages, 57 with protobufs, 13, 15\n\nbase balancers, 172, 186 base offsets, 37, 39, 44–45 Basecamp, 5 bin directory, 17 BindAddr, 118, 198 BindPort, 118 Bitnami, 202 Bolt, 147 bootstrapping\n\nclusters, 149, 162, 205,\n\n207\n\nlogs, 44, 50 segments, 44 servers, 149, 153, 162,\n\n205, 207 Build(), 177–179, 184 bumping major versions, 18\n\nC C (country), certificate author-\n\nity configuration, 80\n\ncandidates, consensus with\n\nRaft, 142\n\ncapacity, metrics, 101",
      "content_length": 1945,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 231,
      "content": "Casbin, 89, 92 certificates\n\ncertificate authority (CA) with CFSSL, 78–86\n\ngenerating, 82, 87 generating multiple, 90 replication testing, 135 Root CA, 85 server names, 180 with TLS, 77–88\n\nCFSSL, certificate authority\n\n(CA) with, 78–86\n\ncfssl tool, 79–82 cfssljson tool, 79–82 Cgo, disabling, 201 channels\n\nclosing, 126–134 replication, 124–134 service discovery with Serf, 118, 120, 124, 126–128\n\nChart.yaml file, 204 charts\n\nbuilding Helm charts,\n\n203–213 defined, 202 deploying locally with\n\nHelm, 202–217\n\ninstalling Helm charts,\n\n215–217\n\nMetacontroller, 223 Nginx, 202 subcharts, 204\n\nclient-side load balancing custom pickers, 175,\n\n177, 183–189\n\ncustom resolvers, 173–\n\n183\n\ndefined, 171 on gPRC, 172–174 testing, 175, 181–183,\n\n186–189\n\nclients, see also client-side\n\nload balancing\n\nauthentication with TLS,\n\n76–77, 86\n\nauthorization, 78, 88–98 defining gPRC service, 59 generating in gPRC, 60 generating multiple certifi-\n\ncates, 90\n\nhandling in gPRC, 63–68 nobody, 92, 95 replication, 124–134 root, 92, 94 service discovery and,\n\n114, 171\n\nsuperuser, 92 testing gPRC, 68–73\n\nclosing\n\nagents, 133, 136 channels, 126–134 consensus with Raft,\n\n158, 161, 168\n\nindexes, 33 listeners, 158 logs, 45, 133 multiplexing Raft, 168 observability, 108 replication, 126–134 resolvers, 180 segments, 40, 46 store, 29–30\n\ncloud\n\ncustom controllers, 222–\n\n227\n\ndeploying to with Kuber-\n\nnetes, 219–228\n\nCloudFlare, 79 ClusterIP Service, 212 clusters\n\nbootstrapping, 149, 162,\n\n205, 207\n\nchecking local member-\n\nship, 120\n\nconfiguring for agents,\n\n132\n\ncreating Google Kuber- netes Engine (GKE), 220\n\ndeploying locally with Kubernetes, setup, 194–195\n\ndeploying locally, CLI agent for, 196–202 deploying locally, with\n\nHelm, 202–217\n\ndeploying to cloud, cus- tom controllers, 222– 227\n\nmultiplexing Raft, 168 number of servers in a Raft cluster, 143 removing servers from Raft cluster, 160\n\nreplication with consen- sus, implementing, 144–161\n\nreplication with consen- sus, testing, 161–163, 169\n\nreplication with consen- sus, understanding, 141–144 resiliency, 118\n\nIndex • 230\n\nservice discovery and replication, testing, 134–139\n\nservice discovery with\n\nSerf, integrating with consensus, 158–161 service discovery with\n\nSerf, replication, 123– 134\n\nservice discovery with\n\nSerf, setup, 116–123 snapshotting with Serf,\n\n120\n\nstatus, 121 clusters, Kubernetes\n\ndebugging, 195 running in Docker, 195,\n\n202, 215\n\nCN (Common Name), certifi-\n\ncates, 80, 87\n\nCobra, 196 CockRoachDB, 143 code\n\nfor this book, xiii working with generated,\n\n20\n\ncodes package, 63 commit logs, see also config- uring; consensus; load bal- ancing; proglog project; replication; service discov- ery\n\nabout, 6, 23, 25 advantages of, 23–25 building index, 31–37 building segments, 37–42 building store, 26–31 closing, 45, 133 coding log, 42–51 interface, 67 observability, 103–109 prototype for proglog\n\nproject, 6–11\n\nremoving, 45 resetting, 45, 50 restoring, 47, 50 setup, 43, 131 testing log code, 48–51\n\nCommon Name (CN), certifi-\n\ncates, 80, 87\n\ncompilers\n\nperformance, 19 protobuf, 16, 19, 58, 60\n\nCompositeController, 223 configuring\n\nagents, 129",
      "content_length": 3118,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 232,
      "content": "authorization with Cas-\n\nbin, 92\n\ncertificate authority (CA) with CFSSL, 79–86 certificates for replication\n\ntesting, 135\n\nclient-side load balanc-\n\ning, custom, 175–177, 180\n\ndefaults, 43 deploying locally with\n\nKubernetes, 197–201, 207\n\ndeploying to cloud, 220–\n\n222, 224\n\ndynamic configuration\n\nwith Viper, 196, 198– 201\n\ngcloud CLI, 221 index, 36 Metacontroller, 224 multiplexing Raft, 166–\n\n169\n\nobservability, 105 Raft, 146–149, 153, 175–\n\n177\n\nreplication, 124 segments, 37 service discovery with\n\nSerf, 116–118\n\nstore, 36 TLS for consensus with\n\nRaft, 157\n\nZap, 105\n\nconsensus\n\nabout, 24 adding servers as non-\n\nvoters, 159 API for, 149–151 errors, 151, 160 leader election, 142 performance, 148, 151,\n\n163\n\nwith Raft, implementing,\n\n144–161\n\nwith Raft, integrating\n\nwith discovery, 158– 161\n\nwith Raft, setup, 144–149 with Raft, testing, 161–\n\n163, 169\n\nwith Raft, understanding,\n\n141–144\n\nreading whole log and, 47 testing, 161–163, 169 timeouts, 148, 160, 162,\n\n189\n\nconsistency, Raft and, 151\n\nconst, 82 Consul, 115, 118, 129, 141 consumers, see also gRPC\n\nclient-side load balanc- ing, custom, 176, 187 configuring for authoriza-\n\ntion, 96–98\n\nerror messages, 66 prototype for proglog\n\nproject, 7–11\n\nreplication, 123–139\n\ncontainers\n\nabout, 193 Google Container Reg-\n\nistry, 222\n\nhealth checks with probes, 208–212\n\ninitContainers, 206 StatefulSets and, 206\n\ncontrollers\n\ndeploying locally, 194 deploying to cloud, cus-\n\ntom, 222–227\n\ncounters, 100 country (C), certificate author-\n\nity configuration, 80\n\ncredentials, security with gR-\n\nPC, 57\n\ncryptography conventions, 76\n\nD debugging\n\nflags, 107 Kubernetes clusters, 195 DecoratorController, 223–227 DeleteRange(), 156 deleting\n\nindexes and stores, 40 logs, 45 records with Raft, 156 segments, 25 StatefulSets, 227 test data, 136\n\ndeploying\n\nCLI agent for, 193 with custom controllers,\n\n222–227\n\nwith Kubernetes, CLI agent for, 196–202 with Kubernetes, Helm,\n\n202–217\n\nwith Kubernetes, setup,\n\n194–195\n\nwith Kubernetes, to cloud, 219–228\n\nIndex • 231\n\nDeployments (Kubernetes),\n\n206\n\nDesigning Data-Intensive Ap-\n\nplications, xi\n\nDetails(), 65 dial options, 86, 132 Dial(), 157, 173 directories\n\nconventions, 17, 19 Helm charts, 204 hooks, 226 protobufs, 17 segments, 43\n\ndiscovery, see service discov-\n\nery\n\ndistributed services, see al-\n\nso configuring; consensus; gRPC; load balancing; replication; service discov- ery\n\nadvantages of, 4, 55 goals, 56–58\n\nDocker\n\n1.14-alpine image, 201 adding probes to, 211 building image, 201 deploying to cloud, 220,\n\n222\n\ndeploying with Kuber-\n\nnetes CLI agent, 196– 202\n\nDockerfile, creating, 201 running Kubernetes\n\nclusters in with Kind, 195, 202, 215\n\nscratch (empty) image, 201\n\ndomain types, defining as\n\nprotobufs, 17–19\n\nDone, 185 dot (.), 20 dynamic configuration, 196,\n\n198–201\n\nE Elastic Load Balancer (ELB),\n\n172\n\nElasticsearch, 5, 103 ELB (Elastic Load Balancer),\n\n172 elections\n\nabout, 142 adding servers as non-\n\nvoters, 159\n\nclient-side load balanc-\n\ning, custom, 175",
      "content_length": 2991,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 233,
      "content": "node tags, 118 timeouts for, 148, 160,\n\n162, 189\n\nenc, 27 encoding, 11, 27 encryption\n\nabout, 76 consensus with Raft,\n\n157, 164–169\n\nwith TLS, 57, 76–86,\n\n157, 164–169\n\nEnforce (Casbin), 90 entWidth, 32 environment variables\n\nPATH, 17 PROJECT_ID, 221\n\nerrdetails package, 64 Error(), 151 errors\n\nclient-side load balanc-\n\ning, custom, 186\n\ncommit log prototype, 7,\n\n10\n\nconsensus, 151, 160 default description, 63 detailed description, 64 handling in gPRC, 63–\n\n68, 72\n\nmetrics, 101 replication, 128, 151 service discovery, 120 status codes, 63–65 testing range errors, 49\n\nEtcd, 5, 141 eu.gcr.io registry, 222 event channels\n\nclosing, 126–134 replication, 124–134 service discovery with\n\nSerf, 118, 120\n\nEventCh, 118 Eventually(), 163 ext files and logs, 23 extensibility and protobufs,\n\n15\n\nexternal load balancing, 171 ExternalName Service, 212\n\nF fields\n\naccessing with . (dot), 20 field versioning, 18, 57 Go syntax, 18 protobuf syntax, 18 file paths, returning index, 35\n\nfiles\n\ncreating, 38 re-creating store from, 27 size, 27\n\nfinalize, 224 finite-state machines calling, 155 creating, 146 defining type, 151 methods, 151 Raft setup, 146–149, 153 replication with consen- sus, 143, 151–154\n\nflags\n\ncustom flag values, 198 deploying with Kuber-\n\nnetes CLI agent, 196– 202\n\nflag parsing, 107 persistent flags with Co-\n\nbra, 197\n\nflushing\n\nclosing indexes, 33 writer buffer, 28\n\nfollowers, in replication with\n\nconsensus, 142\n\nFSM, see finite-state ma-\n\nchines\n\nG gauges, 100 gcloud CLI, 220 GCP (Google Cloud Platform) deploying to, 219–228 sign up, 220 gcr.io registry, 222 gencert, 82, 87, 90 GET, JSON/HTTP servers, 8 getters, 20 GKE (Google Kubernetes En-\n\ngine)\n\ncustom controllers, 222–\n\n227\n\ndeploying to cloud with,\n\n220–228\n\nGo\n\nabout, xi advantages, 3, 5 API version, 19 conventions, 17, 19 converting into protobuf\n\nsyntax, 17\n\nencoding, 11, 27 runtimes, 19 syntax, 18 version, xii\n\nIndex • 232\n\ngodoc, 57 gogoprotobuf compiler, 19 Google, see also gRPC\n\nCloud Developer Tools,\n\n220\n\nContainer Registry, 222 metric recommendations,\n\n101\n\nprotobufs use, 13\n\nGoogle Cloud Platform (GCP) deploying to, 219–228 sign up, 220\n\nGoogle Kubernetes Engine\n\n(GKE)\n\ncustom controllers, 222–\n\n227\n\ndeploying to cloud with,\n\n220–228 gorilla/mux library, 8 goroutines, testing gRPC in,\n\n70 gRPC\n\nabout, 55 advantages, 56–58 client-side load balanc-\n\ning, 172–174\n\nclient-side load balanc- ing, custom pickers, 172, 177, 183–189 client-side load balanc- ing, custom resolvers, 173–183\n\ndial options, 86, 132 error handling, 63–68, 72 health checks with probes, 208–212\n\ninstalling, 60 with interceptors, 97 log interface, 67 multiplexing Raft, 164–\n\n169\n\nprotobufs advantages, 13 resources on, 57–58 server, creating, 68 server, implementing, 60–\n\n63\n\nserver, observability,\n\n103–109\n\nserver, options, 86 server, registering, 68 server, testing, 68–73 service, defining, 58–60 testing, 68–73\n\ngrpc_health_probe, 209",
      "content_length": 2916,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 234,
      "content": "H handlers\n\nprototype for proglog\n\nproject, 7–11\n\nservice discovery integra- tion with consensus, 158–161\n\nservice discovery with\n\nSerf, 119–120 handshake, TLS, 77 Hashicorp, 115, 128 headless Services (Kuber-\n\nnetes), 213\n\nhealth checks, 208–212 heartbeat, leader, 142 Helm\n\nabout, 194, 202 building charts, 203–213 deploying to cloud with,\n\n222–228\n\ndeploying with, 202–217 installing Metacontroller,\n\n223\n\ninstalling charts, 215–\n\n217 HighestOffset(), 46 histograms, 100 hooks\n\nCobra, 197 deploying to cloud with Metacontroller, 224\n\nhosts field, certificate authority\n\nconfiguration, 81\n\nHTTP\n\ngRPC advantages, 58 HTTP/2, 58 JSON/HTTP, role in dis- tributed systems, 4\n\nprobes, 208\n\nI import paths, 5 include directory, 17 indexes\n\nabout, 7, 25 appending records in prototype, 7–11\n\nbuilding, 31–37 building log code, 45 closing, 33 configuring, 36 creating, 32 defined, 26 memory-mapping, 26,\n\n32–33\n\nperformance, 25\n\nremoving, 40 returning file path, 35 size, 32, 34, 36, 40, 42 testing, 35 truncating, 33–34, 36\n\ninit()\n\nreplication, 127 service discovery with custom pickers, 185 service discovery with\n\ncustom resolvers, 179\n\ninitContainers, 206 initialization, lazy, 127 installing\n\nCasbin, 89 CLIs for CFSSL, 79 gcloud CLI, 220 gRPC package, 60 Helm, 202 Helm charts, 215–217 Kind, 195 kubectl, 194 Metacontroller, 223 protobuf compiler, 16, 19 Raft, 144 Serf, 116\n\ninstances, creating, 20 interceptors, 97 interface, log, 67 internal packages, 60 isLocal(), 120 Ivy, 128\n\nJ Jaegar, 102 Jocko, 4 Join, 158 JSON\n\ncertificate authority (CA) with CFSSL, 79–82 JSON/HTTP, role in dis- tributed systems, 4 JSON/HTTP, setup, 5–11 Jsonnet, 227 uses, 13 Jsonnet, 227\n\nK Kafka\n\nconsensus with Raft, 141 Jocko, 4 service discovery, 174 structured logs, 102 key field, certificate authority\n\nconfiguration, 80\n\nIndex • 233\n\nkeys\n\nauthentication with TLS,\n\n77\n\ncertificate authority con-\n\nfiguration, 80\n\ngenerating for certificates,\n\n82\n\nKind, 195, 202, 215 Kleppmann, Martin, xi kubectl, 194, 220, 222 Kuberesolver, 173 Kubernetes\n\nabout, 193 consensus with Raft, 141 deploying locally with CLI\n\nagent, 196–202 deploying locally with\n\nHelm, 202–217\n\ndeploying to cloud, 219–\n\n228\n\ndeploying to cloud, cus- tom controllers, 222– 227\n\nwith gogoprotobuf compil-\n\ner, 19\n\nGoogle Kubernetes En- gine (GKE), creating, 220\n\nhealth checks with probes, 208–212 JSON/HTTP use, 5 Kuberesolver, 173 manifest files, 204 running services inside\n\ncontainers without load balancers, 216 Services, 205, 212 setup, 194–195 StatefulSets, 205–208,\n\n223–227\n\nL L (locality), certificate author-\n\nity configuration, 80\n\nlanguages\n\ngRPC advantages, 58 protobufs advantages, 15 protobufs compiler, 19 status codes, 63\n\nlatency\n\nload balancing and, 172–\n\n173\n\nmetrics, 101 Raft elections, 148, 189 replication, 137 lazy initialization, 127",
      "content_length": 2815,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 235,
      "content": "leaders, election in replication\n\nwith consensus, 142\n\nLeave, 120, 158 lenWidth, 27 Lightstep, 102 Listen, 136 listeners\n\nconsensus with Raft, 158 JSON/HTTP server, building, 7–11\n\nmultiplexing Raft, 166–\n\n169\n\nservice discovery ports,\n\n136\n\ntesting setup, 70 liveness probes, 208 load balancing about, 171 base balancers, 172, 186 client-side, defined, 171 client-side, on gPRC,\n\n172–174\n\ncustom controllers for deploying to cloud, 222–227 external, 171 gRPC advantages, 58 Kubernetes LoadBalancer\n\nService, 212\n\npickers, custom, 175,\n\n177, 183–189\n\nreadiness probes, 208 resolvers, custom, 173–\n\n183\n\nround-robin, 173, 185 scaling and, 114 with server proxies, 171 vs. service discovery, 113 strategies, 171 testing, 175, 181–183,\n\n186–189 types of, 58\n\nLoadBalancer Service, 212 LocalID, 148 locality (L), certificate author-\n\nity configuration, 80\n\nLocalizedMessage, 64 locks, 45 logError(), 120, 160 logs, see also commit logs;\n\nproglog project\n\nadvantages of, 23–25,\n\n101\n\nbootstrapping, 44, 50 compacting, 153 ext files, 23\n\nstructured logs, 99, 101,\n\n103–109\n\nas term, 25–26\n\nLowestOffset(), 46\n\nM maintainability, gRPC advan-\n\ntages, 57\n\nMakefile, creating, 20 man-in-the-middle attacks\n\n(MITM), 76 Members(), 120 membership, see clusters memory\n\nin-memory log interface,\n\n67\n\nmemory-mapping index\n\nfiles, 26, 32–33\n\nmetrics, 101 storing offsets, 34 Metacontroller, 222–227 metadata\n\npickers, 185 Raft, 147\n\nmetrics, 99–101, 103–109 Microsoft Azure, 219 middleware, 97 MITM (man-in-the-middle at-\n\ntacks), 76 model.con file, 93 modules, 5 multiplexing, Raft, 158, 163–\n\n169\n\nMultiRaft, 143 MultiReader(), 47 mutex, wrapping log code in,\n\n45\n\nmutual authentication, 77,\n\n86\n\nN names\n\ncertificates, 80, 87 cryptography conven-\n\ntions, 76\n\nnodes, 118, 122 servers, 180 specifying package names\n\nin protobufs, 18 names field, certificate authori-\n\nty configuration, 80\n\nnearestMultiple, 41 networking, see gRPC new keyword, 20\n\nIndex • 234\n\nnext offsets, 37 Nginx chart, 202 nobody client, 92, 95 NodeName, 118 NodePort Service, 212 nodes, see also replication;\n\nservice discovery\n\ncluster resiliency, 118 dynamic configuration\n\nwith Viper, 199\n\nelections, 118 Kubernetes, 194–195 names, 118, 122 pointing to clusters, 118 tags, 118\n\nO O (organization), certificate\n\nauthority configuration, 80\n\nobservability, 99–109\n\ndefined, 99 importance of, 99 metrics, 99–101, 103–109 output, 108 probability samplers, 105 proglog project, 103–109 structured logs, 99, 101 testing, 106–109 traces, 99, 102–109\n\noffsets\n\nabout, 7 appending records in prototype, 7–11 base, 37, 39, 44–45 building index, 32–33 building log code, 44–45 building segments, 37–40 building store, 28 custom gPRC errors, 65–\n\n68, 72 next, 37 performance, 25 ranges, 46 reading and, 34, 45 relative, 39 replication with consen-\n\nsus, 143 storing, 34 testing errors, 49\n\nOpenCensus, 104–106 OpenTelemetry, 104 OpenTracing, 104 Operators, 223 organization (O), certificate\n\nauthority configuration, 80",
      "content_length": 2968,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 236,
      "content": "organization unit (OU), certifi- cate authority configura- tion, 80 originReader, 47 OU (organizational unit), cer- tificate authority configura- tion, 80\n\noutput, observability, 108\n\nP packages\n\ninternal, 60 specifying package names\n\nin protobufs, 18\n\nParquet, 102 PATH environment variable, 17 peerTLSConfig, 135, 157 performance\n\ncompilers, 19 consensus with Raft,\n\n148, 151, 163\n\ngRPC advantages, 58 multiplexing Raft, 163 protobufs, 13, 16, 19 reading logs, 25 test performance, 137 writing logs, 27 PersistentVolumeClaim, 206 persisting, snapshots, 153 pg_isready, 208 Pick(), 185 pickers\n\nclient-side load balancing on gPRC, 172, 183 client-side load balanc-\n\ning, custom, 175, 177, 183–189\n\nregistering, 185\n\nPike, Rob, 128 plugins, 21 pods\n\nabout, 194 forwarding, 216 listing, 216 service-per-pod load bal-\n\nancers, 222–227 Services and, 212 point-in-time recovery, 24 policy.csv, 93 ports\n\nassigning, 70, 136 consensus with Raft, 158 dynamic configuration\n\nwith Viper, 199\n\nexposing with Kubernetes NodePort Service, 212\n\nforwarding, 216 multiplexing Raft, 163–\n\n169\n\nservice discovery, 118,\n\n136\n\nPOST, JSON/HTTP servers,\n\n8\n\nPostgres, 208 probability samplers, 105 probes\n\nhealth checks with, 208–\n\n212\n\nliveness, 208 readiness, 208 startup, 208\n\nproducers, see also gRPC\n\nconfiguring for authoriza-\n\ntion, 96–98\n\nprototype for proglog\n\nproject, 7–11\n\nreplication, 126–134, 137 proglog project, see also con-\n\nfiguring; testing\n\nauthentication with TLS,\n\n76–88\n\nbuilding, 26–51 building index, 31–37 building segments, 37–42 building store, 26–31 client-side load balanc- ing, pickers, 183–189 client-side load balanc-\n\ning, resolvers, 173–183\n\nclosing logs, 45 coding log, 42–51 deploying with Kuber- netes, CLI agent for, 196–202\n\ndeploying with Kuber-\n\nnetes, Helm, 202–217\n\ndeploying with Kuber-\n\nnetes, setup, 194–195\n\ndeploying with Kuber- netes, to cloud, 219– 228\n\nerror handling with gR-\n\nPC, 63–68\n\nJSON/HTTP setup, 5–11 observability, 103–109 protobufs setup, 16–21 prototype for, 6–11 removing logs, 45 replication, 123–134 replication with consen- sus, API, 149–151\n\nIndex • 235\n\nreplication with consen- sus, implementing, 144–161\n\nreplication with consen- sus, testing, 161–163, 169\n\nresetting logs, 45, 50 running in Kubernetes\n\ncluster, 195\n\nservice discovery and replication, testing, 134–139\n\nservice discovery with\n\nSerf, replication, 123– 134\n\nservice discovery with\n\nSerf, setup, 116–123\n\nservice discovery with\n\nSerf, testing, 121–123, 134–139 setup code, 43\n\nPROJECT_ID environment vari-\n\nable, 221\n\nPrometheus, 103 protobufs\n\nadvantages, 5, 13–16 comments, 57 compiler, 16, 19, 58, 60 defining domain types,\n\n17–19\n\ndefining gPRC service,\n\n58–60\n\ndetailed error description,\n\n64\n\ndirectory structure, 16 performance, 13, 16, 19 proglog project setup, 16–\n\n21\n\nsyntax, 17 versioning with, 15, 18,\n\n57\n\nprotocol buffers, see proto-\n\nbufs\n\npull-based replication,\n\nsee replication\n\nR Raft\n\nabout, 24 adding servers as non-\n\nvoters, 159\n\nadvantages of, 142 configuring, 146–149,\n\n153, 175–177\n\nimplementing, 144–161 integrating with discov-\n\nery, 158–161\n\nleader election, 142",
      "content_length": 3109,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 237,
      "content": "log API, 149–151 metadata, 147 multiplexing, 158, 163–\n\n169\n\nnode tags, 118 number of servers in a Raft cluster, 143\n\nperformance, 148, 151,\n\n163\n\nremoving servers from a\n\ncluster, 160\n\nrestoring with, 152, 154 separating Raft connec- tions from gRPC, 164, 166\n\nSerf setup and, 118–119 service discovery with\n\ncustom resolvers, 174– 175\n\nsetup, 144–149 snapshotting with, 146–\n\n149, 151–153\n\ntesting, 161–163, 169 understanding, 141–144\n\nrate metrics, 100 readiness probes, 208 reading\n\nabout, 25 building index, 34, 36 building log code, 45, 47–\n\n51\n\nbuilding segment, 39, 42 building store, 28 concatenating stores, 47 consensus and, 47 performance, 25 replication with consen-\n\nsus, 151, 153\n\ntesting log code, 49–51 testing store, 29–31 records, see also appending\n\nabout, 6 defined, 26 deleting with Raft, 156 recovery, point-in-time, 24 redo/undo, 24 Redux, 24 reflection, 19, 21 registries\n\nconfig registry system with Viper, 198–201\n\ndefined, 113 Google Container Reg-\n\nistry, 222\n\nrelative offsets, 39 releases, Helm, 202\n\nreleasing, snapshots, 153 reliability, health checks with\n\nprobes, 208–212\n\nreplication\n\nabout, 24 advantages of, 123, 143 closing, 133 with consensus, API,\n\n149–151\n\nwith consensus, imple- menting, 144–161\n\nwith consensus, integrat-\n\ning with discovery, 158–161\n\nwith consensus, testing,\n\n161–163, 169\n\nwith consensus, under- standing, 141–144\n\nerrors, 128, 151 lazy initialization, 127 number of replicas, 139,\n\n143\n\npull-based, 123–134 push-based, 123 with Serf, 123–134 with Serf, testing, 134–\n\n139\n\nrepositories, deploying with\n\nHelm, 202\n\nrequests, see also gRPC JSON/HTTP server, building, 7–11\n\nobservability, 100–109\n\nresetting, logs, 45, 50 resiliency, clusters, 118 ResolveNow(), 180 resolvers\n\nclient-side load balancing\n\non gPRC, 172\n\nclient-side load balanc- ing, custom, 173–183\n\nclosing, 180 default to DNS resolver,\n\n173\n\nregistering, 179\n\nResponse(), 151 restoring\n\nfinite-state machines,\n\n152, 154 logs, 47, 50 testing, 50\n\nRoot CA, 85 root client, 92, 94 round-robin load balancing,\n\n173, 185\n\nIndex • 236\n\nRPC (remote procedure call),\n\nsee gRPC\n\nruntime compiler, protobufs,\n\n19\n\nRWmutex, wrapping log code\n\nin, 45\n\nS S (state), certificate authority\n\nconfiguration, 80\n\nsaturation, metrics, 101 scaling\n\ndefined, 58 gRPC advantages, 58 load balancers and, 58,\n\n114\n\nmetrics for, 101 service discovery and,\n\n114\n\nStatefulSets and, 205 schema-violations, preventing\n\nwith protobufs, 13–15\n\nScheme(), 179 scratch image, 201 Secure Sockets Layer,\n\nsee SSL\n\nsecurity\n\nauthentication, 76–88,\n\n221\n\nauthorization, 76, 78, 88–\n\n98, 223\n\nencryption, 76–86 gRPC advantages, 57 importance of, 75 man-in-the-middle at-\n\ntacks, 76\n\nname conventions, 76\n\nSegment, 5 segments, see also indexes;\n\nstores\n\nactive, 25, 37 bootstrapping, 44 building, 37–42 building log code, 44–51 building store, 28 closing, 40, 46 configuring, 37 defined, 26 deleting, 25 directory for, 43 removing indexes and\n\nstores, 40\n\nsize, 38, 40, 42, 45 testing, 41 truncating, 46, 51 understanding, 25",
      "content_length": 3015,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 238,
      "content": "Serf\n\nadvantages of, 115 integrating with consen-\n\nsus, 158–161\n\nreplication with, 123–134 setup, 116–123 snapshotting, 118, 120 testing service discovery and replication, 134– 139\n\ntesting setup, 121–123\n\nserialization with protobufs,\n\n13\n\nserver proxies, 171 serverTLSConfig, 135, 157 ServerName field, 180 servers, see also consensus; replication; service discov- ery\n\nadding as non-voters,\n\n159\n\nauthentication with TLS,\n\n78–88\n\nbootstrapping, 149, 153,\n\n162, 205, 207\n\nbuilding JSON/HTTP, 7–\n\n11\n\ncreating gPRC, 68 handling in gPRC, 63–68 implementing gPRC, 60–\n\n63\n\nnames, 180 number of servers in a Raft cluster, 143 observability, 103–109 registering gPRC, 68 removing from Raft clus-\n\nter, 160\n\nrunning JSON/HTTP, 10 server options, gRPC, 86 status of in Serf, 121 terms in Raft, 142 testing gPRC, 68–73\n\nservice discovery\n\nadvantages of, 113 advantages of embedded,\n\n114–115\n\nclient-side load balancing and, 172, 174–183 clients and, 114, 171 defined, 113 errors, 120 integrating with consen-\n\nsus, 158–161\n\nvs. load balancers, 113 replication with, 123–134 with Serf, replication,\n\n123–134\n\nwith Serf, setup, 116–123 with Serf, testing, 134–\n\n139\n\nstand-alone, 114–115 startup probes, 208 task overview, 114 testing, 134–139 testing, custom client- side load balancing, 181–183, 186–189 testing, setup, 121–123\n\nservice keyword, 59 service-per-pod-label, 224 service-per-pod-ports, 224 Services (Kubernetes), 205,\n\n212 shutdown\n\nagents, 133, 136 CLI agent for deploying,\n\n201\n\nindexes and, 33 multiplexing Raft, 168 replication and, 133 test data, 136 ungraceful, 34\n\nsize\n\nfile, 27 index, 32, 34, 36, 40, 42 segments, 38, 40, 42, 45 store, 36, 40, 42 sleep, tests, 136–137 snapshot store, Raft setup,\n\n146–149, 153\n\nsnapshotting\n\ndeleting records, 156 finite-state machines, frequency of, 147 finite-state machines,\n\nimplementation, 151– 153\n\nfinite-state machines, setup, 146–149, 153 reading whole log and,\n\n47, 50\n\nwith Serf, 118, 120 testing, 50\n\nSSL, gRPC advantages, 57,\n\nsee also TLS\n\nstable stores, Raft setup,\n\n146–149\n\nStackdriver, 102 StartJoinAddrs, 118 startup probes, 208 Stat, 27 state, updating with service\n\nconfig, 180\n\nIndex • 237\n\nstate (S), certificate authority\n\nconfiguration, 80\n\nStatefulSets, 205–208, 223–\n\n227\n\nstatus codes, error handling,\n\n63–65\n\nstatus package, 63–65 storage, binding in Kuber-\n\nnetes, 206\n\nstores\n\nbuilding, 26–31 building segments, 37–40 closing, 29–30 concatenating, 47 configuring, 36 defined, 26 offsets, 34 Raft setup, 146–149, 153 re-creating from a file\n\nwith existing data, 27\n\nremoving, 40 size, 36, 40, 42 testing, 29–31 understanding, 25 streams, see also gRPC\n\nclient-side load balanc- ing, custom, 176, 187\n\nconsensus with Raft,\n\n147, 156–158 error messages, 66 replication, 126–134 structured logs, 99, 101, 103–\n\n109\n\nsubcharts, 204 superuser clients, 92 sync, 224\n\nT Tag, 118 tags, Serf nodes, 118 TCP, probes, 208 telemetry exporter, 108 templates, Helm charts, 204 terms, in consensus with\n\nRaft, 142\n\ntesting\n\nauthorization, 91–98 certificate authority con- figuration, 80, 84–86, 88\n\nclient-side load balanc-\n\ning, custom, 175, 181– 183, 186–189\n\ncommit log prototype, 11 consensus, 161–163, 169",
      "content_length": 3175,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 239,
      "content": "defining test cases, 49 gRPC, 68–73 helpers, 30, 70, 122 index, 35 log code, 48–51 log interface, 67 multiplexing Raft, 169 observability, 106–109 performance of tests, 137 range errors, 49 restoring, 50 segments, 41 service discovery, 134–\n\n139\n\nservice discovery with\n\nSerf, setup, 121–123\n\nservice discovery with\n\nload balancing, 175, 188\n\nsetup, 70, 188 shutting down and delet-\n\ning test data, 136 sleep and, 136–137 store, 29–31\n\ntimeouts, consensus with Raft, 148, 160, 162, 189\n\nTLS\n\nauthentication with, 76–\n\n88\n\ncertificates for replication\n\ntesting, 135\n\nconsensus with Raft,\n\n157, 164–169\n\ngRPC advantages, 57 handshake, 77\n\nmultiplexing Raft, 164–\n\n169\n\nmutual authentication,\n\n77, 86\n\nstandards, 164 version, 77 traces, 102–109 traffic, metrics, 101 transport\n\nRaft implementation,\n\n156–158\n\nRaft setup, 146–149\n\nTransport Layer Security,\n\nsee TLS truncating\n\nindexes, 33–34, 36 segments, 46, 51 trust boundaries, 114, 172 two-way authentication, 77,\n\n86\n\ntype checking\n\ngRPC advantages, 57 with protobufs, 13, 15\n\nU uint32, 34 undo/redo, 24 UpdateState, 180 updating, StatefulSets and,\n\n205\n\nus.gcr.io registry, 222\n\nIndex • 238\n\nV values.yaml file, 204, 215 versioning\n\nabout, 56 bumping major versions,\n\n18\n\nfield versioning, 18, 57 gRPC advantages, 57 with protobufs, 15, 18,\n\n57 versions\n\nGo, xii Go API, 19 TLS, 77\n\nViper, 196, 198–201\n\nW -w flag, 228 webhooks, 224 *Width constants, 32 wildcard, * for, 93 WithInsecure() dial option, 86 write-ahead logs, see commit\n\nlogs writing\n\nbuilding index, 35 building segment, 39 building store, 27 performance, 27 writer buffer, 27–28\n\nZ Zap, 104–106 ZooKeeper, 174",
      "content_length": 1628,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 240,
      "content": "SAVE 30%!Use coupon codeBUYANOTHER2021\n\nThank you!\n\nHow did you enjoy this book? Please let us know. Take a moment and email us at support@pragprog.com with your feedback. Tell us your story and you could win free ebooks. Please use the subject line “Book Feedback.”\n\nReady for your next great Pragmatic Bookshelf book? Come on over to https://pragprog.com and use the coupon code BUYANOTHER2021 to save 30% on your next ebook.\n\nVoid where prohibited, restricted, or otherwise unwelcome. Do not use ebooks near water. If rash persists, see a doctor. Doesn’t apply to The Pragmatic Programmer ebook because it’s older than the Pragmatic Bookshelf itself. Side effects may include increased knowledge and skill, increased marketability, and deep satisfaction. Increase dosage regularly.\n\nAnd thank you for your continued support,\n\nAndy Hunt, Publisher",
      "content_length": 849,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 241,
      "content": "Explore Software Defined Radio\n\nDo you want to be able to receive satellite images using nothing but your computer, an old TV antenna, and a $20 USB stick? Now you can. At last, the technology exists to turn your computer into a super radio receiv- er, capable of tuning in to FM, shortwave, amateur “ham,” and even satellite frequencies, around the world and above it. Listen to police, fire, and aircraft signals, both in the clear and encoded. And with the book’s advanced antenna design, there’s no limit to the signals you can receive.\n\nWolfram Donat (78 pages) ISBN: 9781680507591. $19.95 https://pragprog.com/book/wdradio\n\nGenetic Algorithms in Elixir\n\nFrom finance to artificial intelligence, genetic algo- rithms are a powerful tool with a wide array of applica- tions. But you don’t need an exotic new language or framework to get started; you can learn about genetic algorithms in a language you’re already familiar with. Join us for an in-depth look at the algorithms, tech- niques, and methods that go into writing a genetic al- gorithm. From introductory problems to real-world applications, you’ll learn the underlying principles of problem solving using genetic algorithms.\n\nSean Moriarity (242 pages) ISBN: 9781680507942. $39.95 https://pragprog.com/book/smgaelixir",
      "content_length": 1282,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 242,
      "content": "Design and Build Great Web APIs\n\nAPIs are transforming the business world at an increas- ing pace. Gain the essential skills needed to quickly design, build, and deploy quality web APIs that are robust, reliable, and resilient. Go from initial design through prototyping and implementation to deployment of mission-critical APIs for your organization. Test, secure, and deploy your API with confidence and avoid the “release into production” panic. Tackle just about any API challenge with more than a dozen open-source utilities and common programming patterns you can apply right away.\n\nMike Amundsen (330 pages) ISBN: 9781680506808. $45.95 https://pragprog.com/book/maapis\n\nQuantum Computing\n\nYou’ve heard that quantum computing is going to change the world. Now you can check it out for your- self. Learn how quantum computing works, and write programs that run on the IBM Q quantum computer, one of the world’s first functioning quantum computers. Develop your intuition to apply quantum concepts for challenging computational tasks. Write programs to trigger quantum effects and speed up finding the right solution for your problem. Get your hands on the fu- ture of computing today.\n\nNihal Mehta, Ph.D. (580 pages) ISBN: 9781680507201. $45.95 https://pragprog.com/book/nmquantum",
      "content_length": 1285,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 243,
      "content": "A Common-Sense Guide to Data Structures and Algorithms, Second Edition\n\nIf you thought that data structures and algorithms were all just theory, you’re missing out on what they can do for your code. Learn to use Big O Notation to make your code run faster by orders of magnitude. Choose from data structures such as hash tables, trees, and graphs to increase your code’s efficiency exponentially. With simple language and clear dia- grams, this book makes this complex topic accessible, no matter your background. This new edition features practice exercises in every chapter, and new chapters on topics such as dynamic programming and heaps and tries. Get the hands-on info you need to master data structures and algorithms for your day-to-day work.\n\nJay Wengrow (506 pages) ISBN: 9781680507225. $45.95 https://pragprog.com/book/jwdsal2\n\nBuild Location-Based Projects for iOS\n\nCoding is awesome. So is being outside. With location- based iOS apps, you can combine the two for an en- hanced outdoor experience. Use Swift to create your own apps that use GPS data, read sensor data from your iPhone, draw on maps, automate with geofences, and store augmented reality world maps. You’ll have a great time without even noticing that you’re learning. And even better, each of the projects is designed to be extended and eventually submitted to the App Store. Explore, share, and have fun.\n\nDominik Hauser (154 pages) ISBN: 9781680507812. $26.95 https://pragprog.com/book/dhios",
      "content_length": 1472,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 244,
      "content": "iOS Unit Testing by Example\n\nFearlessly change the design of your iOS code with solid unit tests. Use Xcode’s built-in test framework XCTest and Swift to get rapid feedback on all your code — including legacy code. Learn the tricks and tech- niques of testing all iOS code, especially view con- trollers (UIViewControllers), which are critical to iOS apps. Learn to isolate and replace dependencies in legacy code written without tests. Practice safe refac- toring that makes these tests possible, and watch all your changes get verified quickly and automatically. Make even the boldest code changes with complete confidence.\n\nJon Reid (300 pages) ISBN: 9781680506815. $47.95 https://pragprog.com/book/jrlegios\n\nBecome an Effective Software Engineering Manager\n\nSoftware startups make global headlines every day. As technology companies succeed and grow, so do their engineering departments. In your career, you’ll may suddenly get the opportunity to lead teams: to become a manager. But this is often uncharted territory. How do you decide whether this career move is right for you? And if you do, what do you need to learn to suc- ceed? Where do you start? How do you know that you’re doing it right? What does “it” even mean? And isn’t management a dirty word? This book will share the secrets you need to know to manage engineers successfully.\n\nJames Stanier (396 pages) ISBN: 9781680507249. $45.95 https://pragprog.com/book/jsengman",
      "content_length": 1437,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 245,
      "content": "The Pragmatic Bookshelf\n\nThe Pragmatic Bookshelf features books written by professional developers for professional developers. The titles continue the well-known Pragmatic Programmer style and continue to garner awards and rave reviews. As development gets more and more difficult, the Prag- matic Programmers will be there with more titles and products to help you stay on top of your game.\n\nVisit Us Online\n\nThis Book’s Home Page https://pragprog.com/book/tjgo Source code from this book, errata, and other resources. Come give us feedback, too!\n\nKeep Up to Date https://pragprog.com Join our announcement mailing list (low volume) or follow us on twitter @pragprog for new titles, sales, coupons, hot tips, and more.\n\nNew and Noteworthy https://pragprog.com/news Check out the latest pragmatic developments, new titles and other offerings.\n\nBuy the Book\n\nIf you liked this ebook, perhaps you’d like to have a paper copy of the book. Paperbacks are available from your local independent bookstore and wherever fine books are sold.\n\nContact Us\n\nOnline Orders:\n\nhttps://pragprog.com/catalog\n\nCustomer Service:\n\nsupport@pragprog.com\n\nInternational Rights:\n\ntranslations@pragprog.com\n\nAcademic Use:\n\nacademic@pragprog.com\n\nWrite for Us:\n\nhttp://write-for-us.pragprog.com\n\nOr Call:\n\n+1 800-699-7764",
      "content_length": 1296,
      "extraction_method": "Unstructured"
    }
  ]
}