{
  "metadata": {
    "title": "operating_systems_three_easy_pieces",
    "author": "Unknown Author",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 643,
    "conversion_date": "2025-11-27T15:35:27.534149",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "operating_systems_three_easy_pieces.pdf"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Segment 1 (pages 1-8)",
      "start_page": 1,
      "end_page": 8,
      "detection_method": "topic_boundary",
      "content": "OPERATING SYSTEMS\nTHREE EASY PIECES\nREMZI H. ARPACI-DUSSEAU\nANDREA C. ARPACI-DUSSEAU\nUNIVERSITY OF WISCONSIN–MADISON\n\n\n. .\nc⃝2014 by Arpaci-Dusseau Books, Inc.\nAll rights reserved\n\n\ni\nTo Vedat S. Arpaci, a lifelong inspiration\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\nPreface\nTo Everyone\nWelcome to this book! We hope you’ll enjoy reading it as much as we enjoyed\nwriting it. The book is called Operating Systems: Three Easy Pieces, and the title\nis obviously an homage to one of the greatest sets of lecture notes ever created, by\none Richard Feynman on the topic of Physics [F96]. While this book will undoubt-\nedly fall short of the high standard set by that famous physicist, perhaps it will be\ngood enough for you in your quest to understand what operating systems (and\nmore generally, systems) are all about.\nThe three easy pieces refer to the three major thematic elements the book is\norganized around: virtualization, concurrency, and persistence. In discussing\nthese concepts, we’ll end up discussing most of the important things an operating\nsystem does; hopefully, you’ll also have some fun along the way. Learning new\nthings is fun, right? At least, it should be.\nEach major concept is divided into a set of chapters, most of which present a\nparticular problem and then show how to solve it. The chapters are short, and try\n(as best as possible) to reference the source material where the ideas really came\nfrom. One of our goals in writing this book is to make the paths of history as clear\nas possible, as we think that helps a student understand what is, what was, and\nwhat will be more clearly. In this case, seeing how the sausage was made is nearly\nas important as understanding what the sausage is good for1.\nThere are a couple devices we use throughout the book which are probably\nworth introducing here. The ﬁrst is the crux of the problem. Anytime we are\ntrying to solve a problem, we ﬁrst try to state what the most important issue is;\nsuch a crux of the problem is explicitly called out in the text, and hopefully solved\nvia the techniques, algorithms, and ideas presented in the rest of the text.\nThere are also numerous asides and tips throughout the text, adding a little\ncolor to the mainline presentation. Asides tend to discuss something relevant (but\nperhaps not essential) to the main text; tips tend to be general lessons that can be\napplied to systems you build. An index at the end of the book lists all of these tips\nand asides (as well as cruces, the odd plural of crux) for your convenience.\nWe use one of the oldest didactic methods, the dialogue, throughout the book,\nas a way of presenting some of the material in a different light. These are used to\nintroduce the major thematic concepts (in a peachy way, as we will see), as well as\nto review material every now and then. They are also a chance to write in a more\n1Hint: eating! Or if you’re a vegetarian, running away from.\niii\n\n\niv\nhumorous style. Whether you ﬁnd them useful, or humorous, well, that’s another\nmatter entirely.\nAt the beginning of each major section, we’ll ﬁrst present an abstraction that an\noperating system provides, and then work in subsequent chapters on the mecha-\nnisms, policies, and other support needed to provide the abstraction. Abstractions\nare fundamental to all aspects of Computer Science, so it is perhaps no surprise\nthat they are also essential in operating systems.\nThroughout the chapters, we try to use real code (not pseudocode) where pos-\nsible, so for virtually all examples, you should be able to type them up yourself\nand run them. Running real code on real systems is the best way to learn about\noperating systems, so we encourage you to do so when you can.\nIn various parts of the text, we have sprinkled in a few homeworks to ensure\nthat you are understanding what is going on. Many of these homeworks are little\nsimulations of pieces of the operating system; you should download the home-\nworks, and run them to quiz yourself. The homework simulators have the follow-\ning feature: by giving them a different random seed, you can generate a virtually\ninﬁnite set of problems; the simulators can also be told to solve the problems for\nyou. Thus, you can test and re-test yourself until you have achieved a good level\nof understanding.\nThe most important addendum to this book is a set of projects in which you\nlearn about how real systems work by designing, implementing, and testing your\nown code. All projects (as well as the code examples, mentioned above) are in\nthe C programming language [KR88]; C is a simple and powerful language that\nunderlies most operating systems, and thus worth adding to your tool-chest of\nlanguages. Two types of projects are available (see the online appendix for ideas).\nThe ﬁrst are systems programming projects; these projects are great for those who\nare new to C and UNIX and want to learn how to do low-level C programming.\nThe second type are based on a real operating system kernel developed at MIT\ncalled xv6 [CK+08]; these projects are great for students that already have some C\nand want to get their hands dirty inside the OS. At Wisconsin, we’ve run the course\nin three different ways: either all systems programming, all xv6 programming, or\na mix of both.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "page_number": 1,
      "chapter_number": 1,
      "summary": "This chapter covers segment 1 (pages 1-8). Key topics include systems, real, and pieces.",
      "keywords": [
        "OPERATING SYSTEMS",
        "SYSTEMS",
        "OPERATING",
        "book",
        "EASY PIECES",
        "real operating system",
        "PIECES",
        "projects",
        "called Operating Systems",
        "EASY",
        "real systems",
        "systems programming",
        "text",
        "real",
        "programming"
      ],
      "concepts": [
        "systems",
        "real",
        "pieces",
        "way",
        "ways",
        "present",
        "running",
        "run",
        "called",
        "important"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 1,
          "title": "Segment 1 (pages 1-20)",
          "relevance_score": 0.74,
          "method": "api"
        },
        {
          "book": "A Philosophy of Software Design",
          "chapter": 2,
          "title": "Segment 2 (pages 10-17)",
          "relevance_score": 0.67,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 2,
          "title": "Segment 2 (pages 21-38)",
          "relevance_score": 0.65,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 2,
          "title": "Segment 2 (pages 9-17)",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "AntiPatterns",
          "chapter": 1,
          "title": "Segment 1 (pages 2-12)",
          "relevance_score": 0.63,
          "method": "api"
        }
      ]
    },
    {
      "number": 2,
      "title": "Segment 2 (pages 9-16)",
      "start_page": 9,
      "end_page": 16,
      "detection_method": "topic_boundary",
      "content": "v\nTo Educators\nIf you are an instructor or professor who wishes to use this book, please feel\nfree to do so. As you may have noticed, they are free and available on-line from\nthe following web page:\nhttp://www.ostep.org\nYou can also purchase a printed copy from lulu.com. Look for it on the web\npage above.\nThe (current) proper citation for the book is as follows:\nOperating Systems: Three Easy Pieces\nRemzi H. Arpaci-Dusseau and Andrea C. Arpaci-Dusseau\nArpaci-Dusseau Books, Inc.\nMay, 2014 (Version 0.8)\nhttp://www.ostep.org\nThe course divides fairly well across a 15-week semester, in which you can\ncover most of the topics within at a reasonable level of depth. Cramming the\ncourse into a 10-week quarter probably requires dropping some detail from each\nof the pieces. There are also a few chapters on virtual machine monitors, which we\nusually squeeze in sometime during the semester, either right at end of the large\nsection on virtualization, or near the end as an aside.\nOne slightly unusual aspect of the book is that concurrency, a topic at the front\nof many OS books, is pushed off herein until the student has built an understand-\ning of virtualization of the CPU and of memory. In our experience in teaching\nthis course for nearly 15 years, students have a hard time understanding how the\nconcurrency problem arises, or why they are trying to solve it, if they don’t yet un-\nderstand what an address space is, what a process is, or why context switches can\noccur at arbitrary points in time. Once they do understand these concepts, how-\never, introducing the notion of threads and the problems that arise due to them\nbecomes rather easy, or at least, easier.\nYou may have noticed there are no slides that go hand-in-hand with the book.\nThe major reason for this omission is that we believe in the most old-fashioned\nof teaching methods: chalk and a blackboard. Thus, when we teach the course,\nwe come to class with a few major ideas and examples in mind and use the board\nto present them; handouts and live code demos sprinkled are also useful. In our\nexperience, using too many slides encourages students to “check out” of lecture\n(and log into facebook.com), as they know the material is there for them to digest\nlater; using the blackboard makes lecture a “live” viewing experience and thus\n(hopefully) more interactive, dynamic, and enjoyable for the students in your class.\nIf you’d like a copy of the notes we use in preparation for class, please drop us\nan email. We have already shared them with many others around the world.\nOne last request: if you use the free online chapters, please just link to them,\ninstead of making a local copy. This helps us track usage (over 1 million chapters\ndownloaded in the past few years!) and also ensures students get the latest and\ngreatest version.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\nvi\nTo Students\nIf you are a student reading this book, thank you! It is an honor for us to\nprovide some material to help you in your pursuit of knowledge about operating\nsystems. We both think back fondly towards some textbooks of our undergraduate\ndays (e.g., Hennessy and Patterson [HP90], the classic book on computer architec-\nture) and hope this book will become one of those positive memories for you.\nYou may have noticed this book is free and available online. There is one major\nreason for this: textbooks are generally too expensive. This book, we hope, is\nthe ﬁrst of a new wave of free materials to help those in pursuit of their education,\nregardless of which part of the world they come from or how much they are willing\nto spend for a book. Failing that, it is one free book, which is better than none.\nWe also hope, where possible, to point you to the original sources of much\nof the material in the book: the great papers and persons who have shaped the\nﬁeld of operating systems over the years. Ideas are not pulled out of the air; they\ncome from smart and hard-working people (including numerous Turing-award\nwinners2), and thus we should strive to celebrate those ideas and people where\npossible. In doing so, we hopefully can better understand the revolutions that\nhave taken place, instead of writing texts as if those thoughts have always been\npresent [K62]. Further, perhaps such references will encourage you to dig deeper\non your own; reading the famous papers of our ﬁeld is certainly one of the best\nways to learn.\n2The Turing Award is the highest award in Computer Science; it is like the Nobel Prize,\nexcept that you have never heard of it.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nvii\nAcknowledgments\nThis section will contain thanks to those who helped us put the book together.\nThe important thing for now: your name could go here! But, you have to help. So\nsend us some feedback and help debug this book. And you could be famous! Or,\nat least, have your name in some book.\nThe people who have helped so far include: Abhirami Senthilkumaran*, Adam\nDrescher, Adam Eggum, Ahmed Fikri*, Ajaykrishna Raghavan, Alex Wyler, Anand\nMundada, B. Brahmananda Reddy (Minnesota), Bala Subrahmanyam Kambala,\nBenita Bose, Biswajit Mazumder (Clemson), Bobby Jack, Bj¨orn Lindberg, Bren-\nnan Payne, Brian Kroth, Cara Lauritzen, Charlotte Kissinger, Chien-Chung Shen\n(Delaware)*, Cody Hanson, Dan Soendergaard (U. Aarhus), David Hanle (Grin-\nnell), Deepika Muthukumar, Dorian Arnold (New Mexico), Dustin Metzler, Dustin\nPassofaro, Emily Jacobson, Emmett Witchel (Texas), Ernst Biersack (France), Finn\nKuusisto*, Guilherme Baptista, Hamid Reza Ghasemi, Henry Abbey, Hrishikesh\nAmur, Huanchen Zhang*, Jake Gillberg, James Perry (U. Michigan-Dearborn)*, Jay\nLim, Jerod Weinman (Grinnell), Joel Sommers (Colgate), Jonathan Perry (MIT), Jun\nHe, Karl Wallinger, Kaushik Kannan, Kevin Liu*, Lei Tian (U. Nebraska-Lincoln),\nLeslie Schultz, Lihao Wang, Martha Ferris, Masashi Kishikawa (Sony), Matt Rei-\nchoff, Matty Williams, Meng Huang, Mike Griepentrog, Ming Chen (Stonybrook),\nMohammed Alali (Delaware), Murugan Kandaswamy, Natasha Eilbert, Nathan\nDipiazza, Nathan Sullivan, Neeraj Badlani (N.C. State), Nelson Gomez, Nghia\nHuynh (Texas), Patricio Jara, Radford Smith, Ripudaman Singh, Ross Aiken, Rus-\nlan Kiselev, Ryland Herrick, Samer Al-Kiswany, Sandeep Ummadi (Minnesota),\nSatish Chebrolu (NetApp), Satyanarayana Shanmugam*, Seth Pollen, Sharad Punuganti,\nShreevatsa R., Sivaraman Sivaraman*, Srinivasan Thirunarayanan*, Suriyhaprakhas\nBalaram Sankari, Sy Jin Cheah, Thomas Griebel, Tongxin Zheng, Tony Adkins,\nTorin Rudeen (Princeton), Tuo Wang, Varun Vats, Xiang Peng, Xu Di, Yue Zhuo\n(Texas A&M), Yufui Ren, Zef RosnBrick, Zuyu Zhang. Special thanks to those\nmarked with an asterisk above, who have gone above and beyond in their sugges-\ntions for improvement.\nSpecial thanks to Professor Joe Meehean (Lynchburg) for his detailed notes on\neach chapter, to Professor Jerod Weinman (Grinnell) and his entire class for their\nincredible booklets, and to Professor Chien-Chung Shen (Delaware) for his invalu-\nable and detailed reading and comments about the book. All three have helped\nthese authors immeasurably in the reﬁnement of the materials herein.\nAlso, many thanks to the hundreds of students who have taken 537 over the\nyears. In particular, the Fall ’08 class who encouraged the ﬁrst written form of\nthese notes (they were sick of not having any kind of textbook to read – pushy\nstudents!), and then praised them enough for us to keep going (including one hi-\nlarious “ZOMG! You should totally write a textbook!” comment in our course\nevaluations that year).\nA great debt of thanks is also owed to the brave few who took the xv6 project\nlab course, much of which is now incorporated into the main 537 course. From\nSpring ’09: Justin Cherniak, Patrick Deline, Matt Czech, Tony Gregerson, Michael\nGriepentrog, Tyler Harter, Ryan Kroiss, Eric Radzikowski, Wesley Reardan, Rajiv\nVaidyanathan, and Christopher Waclawik. From Fall ’09: Nick Bearson, Aaron\nBrown, Alex Bird, David Capel, Keith Gould, Tom Grim, Jeffrey Hugo, Brandon\nJohnson, John Kjell, Boyan Li, James Loethen, Will McCardell, Ryan Szaroletta, Si-\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\nviii\nmon Tso, and Ben Yule. From Spring ’10: Patrick Blesi, Aidan Dennis-Oehling,\nParas Doshi, Jake Friedman, Benjamin Frisch, Evan Hanson, Pikkili Hemanth,\nMichael Jeung, Alex Langenfeld, Scott Rick, Mike Treffert, Garret Staus, Brennan\nWall, Hans Werner, Soo-Young Yang, and Carlos Grifﬁn (almost).\nAlthough they do not directly help with the book, our graduate students have\ntaught us much of what we know about systems. We talk with them regularly\nwhile they are at Wisconsin, but they do all the real work – and by telling us about\nwhat they are doing, we learn new things every week. This list includes the fol-\nlowing collection of current and former students with whom we published pa-\npers; an asterisk marks those who received a Ph.D. under our guidance: Abhishek\nRajimwale, Ao Ma, Brian Forney, Chris Dragga, Deepak Ramamurthi, Florentina\nPopovici*, Haryadi S. Gunawi*, James Nugent, John Bent*, Lanyue Lu, Lakshmi\nBairavasundaram*, Laxman Visampalli, Leo Arulraj, Meenali Rungta, Muthian Si-\nvathanu*, Nathan Burnett*, Nitin Agrawal*, Sriram Subramanian*, Stephen Todd\nJones*, Swaminathan Sundararaman*, Swetha Krishnan, Thanh Do, Thanumalayan\nS. Pillai, Timothy Denehy*, Tyler Harter, Venkat Venkataramani, Vijay Chidambaram,\nVijayan Prabhakaran*, Yiying Zhang*, Yupu Zhang*, Zev Weiss.\nA ﬁnal debt of gratitude is also owed to Aaron Brown, who ﬁrst took this course\nmany years ago (Spring ’09), then took the xv6 lab course (Fall ’09), and ﬁnally was\na graduate teaching assistant for the course for two years or so (Fall ’10 through\nSpring ’12). His tireless work has vastly improved the state of the projects (par-\nticularly those in xv6 land) and thus has helped better the learning experience for\ncountless undergraduates and graduates here at Wisconsin. As Aaron would say\n(in his usual succinct manner): “Thx.”\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nix\nFinal Words\nYeats famously said “Education is not the ﬁlling of a pail but the lighting of a\nﬁre.” He was right but wrong at the same time3. You do have to “ﬁll the pail” a bit,\nand these notes are certainly here to help with that part of your education; after all,\nwhen you go to interview at Google, and they ask you a trick question about how\nto use semaphores, it might be good to actually know what a semaphore is, right?\nBut Yeats’s larger point is obviously on the mark: the real point of education\nis to get you interested in something, to learn something more about the subject\nmatter on your own and not just what you have to digest to get a good grade in\nsome class. As one of our fathers (Remzi’s dad, Vedat Arpaci) used to say, “Learn\nbeyond the classroom”.\nWe created these notes to spark your interest in operating systems, to read more\nabout the topic on your own, to talk to your professor about all the exciting re-\nsearch that is going on in the ﬁeld, and even to get involved with that research. It\nis a great ﬁeld(!), full of exciting and wonderful ideas that have shaped computing\nhistory in profound and important ways. And while we understand this ﬁre won’t\nlight for all of you, we hope it does for many, or even a few. Because once that ﬁre\nis lit, well, that is when you truly become capable of doing something great. And\nthus the real point of the educational process: to go forth, to study many new and\nfascinating topics, to learn, to mature, and most importantly, to ﬁnd something\nthat lights a ﬁre for you.\nAndrea and Remzi\nMarried couple\nProfessors of Computer Science at the University of Wisconsin\nChief Lighters of Fires, hopefully4\n3If he actually said this; as with many famous quotes, the history of this gem is murky.\n4If this sounds like we are admitting some past history as arsonists, you are probably\nmissing the point. Probably. If this sounds cheesy, well, that’s because it is, but you’ll just have\nto forgive us for that.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\nx\nReferences\n[CK+08] “The xv6 Operating System”\nRuss Cox, Frans Kaashoek, Robert Morris, Nickolai Zeldovich\nFrom: http://pdos.csail.mit.edu/6.828/2008/index.html\nxv6 was developed as a port of the original UNIX version 6 and represents a beautiful, clean, and simple\nway to understand a modern operating system.\n[F96] “Six Easy Pieces: Essentials Of Physics Explained By Its Most Brilliant Teacher”\nRichard P. Feynman\nBasic Books, 1996\nThis book reprints the six easiest chapters of Feynman’s Lectures on Physics, from 1963. If you like\nPhysics, it is a fantastic read.\n[HP90] “Computer Architecture a Quantitative Approach” (1st ed.)\nDavid A. Patterson and John L. Hennessy\nMorgan-Kaufman, 1990\nA book that encouraged each of us at our undergraduate institutions to pursue graduate studies; we later\nboth had the pleasure of working with Patterson, who greatly shaped the foundations of our research\ncareers.\n[KR88] “The C Programming Language”\nBrian Kernighan and Dennis Ritchie\nPrentice-Hall, April 1988\nThe C programming reference that everyone should have, by the people who invented the language.\n[K62] “The Structure of Scientiﬁc Revolutions”\nThomas S. Kuhn\nUniversity of Chicago Press, 1962\nA great and famous read about the fundamentals of the scientiﬁc process. Mop-up work, anomaly, crisis,\nand revolution. We are mostly destined to do mop-up work, alas.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nContents\nTo Everyone\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\niii\nTo Educators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nv\nTo Students . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nvi\nAcknowledgments . . . . . . . . . . . . . . . . . . . . . . . . . . . vii\nFinal Words\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nix\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nx\n1\nA Dialogue on the Book\n1\n2\nIntroduction to Operating Systems\n3\n2.1\nVirtualizing the CPU\n. . . . . . . . . . . . . . . . . . . . . .\n5\n2.2\nVirtualizing Memory . . . . . . . . . . . . . . . . . . . . . .\n7\n2.3\nConcurrency . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n2.4\nPersistence . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n2.5\nDesign Goals . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n2.6\nSome History\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n2.7\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\nI\nVirtualization\n21\n3\nA Dialogue on Virtualization\n23\n4\nThe Abstraction: The Process\n25\n4.1\nThe Abstraction: A Process . . . . . . . . . . . . . . . . . . .\n26\n4.2\nProcess API\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\n4.3\nProcess Creation: A Little More Detail\n. . . . . . . . . . . .\n28\n4.4\nProcess States\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n29\n4.5\nData Structures\n. . . . . . . . . . . . . . . . . . . . . . . . .\n30\n4.6\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n32\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\nxi\n\n\nxii\nCONTENTS\n5\nInterlude: Process API\n35\n5.1\nThe fork() System Call . . . . . . . . . . . . . . . . . . . .\n35\n5.2\nAdding wait() System Call . . . . . . . . . . . . . . . . . .\n37\n5.3\nFinally, the exec() System Call . . . . . . . . . . . . . . . .\n38\n5.4\nWhy? Motivating the API\n. . . . . . . . . . . . . . . . . . .\n39\n5.5\nOther Parts of the API . . . . . . . . . . . . . . . . . . . . . .\n42\n5.6\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n42\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n43\n6\nMechanism: Limited Direct Execution\n45\n6.1\nBasic Technique: Limited Direct Execution . . . . . . . . . .\n45\n6.2\nProblem #1: Restricted Operations . . . . . . . . . . . . . . .\n46\n6.3\nProblem #2: Switching Between Processes . . . . . . . . . .\n50\n6.4\nWorried About Concurrency?\n. . . . . . . . . . . . . . . . .\n54\n6.5\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n55\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n57\nHomework (Measurement) . . . . . . . . . . . . . . . . . . . . . .\n58\n7\nScheduling: Introduction\n59\n7.1\nWorkload Assumptions . . . . . . . . . . . . . . . . . . . . .\n59\n7.2\nScheduling Metrics\n. . . . . . . . . . . . . . . . . . . . . . .\n60\n7.3\nFirst In, First Out (FIFO)\n. . . . . . . . . . . . . . . . . . . .\n60\n7.4\nShortest Job First (SJF)\n. . . . . . . . . . . . . . . . . . . . .\n62\n7.5\nShortest Time-to-Completion First (STCF)\n. . . . . . . . . .\n63\n7.6\nRound Robin . . . . . . . . . . . . . . . . . . . . . . . . . . .\n65\n7.7\nIncorporating I/O . . . . . . . . . . . . . . . . . . . . . . . .\n66\n7.8\nNo More Oracle . . . . . . . . . . . . . . . . . . . . . . . . .\n68\n7.9\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n68\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n69\nHomework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n70\n8\nScheduling:The Multi-Level Feedback Queue\n71\n8.1\nMLFQ: Basic Rules\n. . . . . . . . . . . . . . . . . . . . . . .\n72\n8.2\nAttempt #1: How to Change Priority . . . . . . . . . . . . .\n73\n8.3\nAttempt #2: The Priority Boost . . . . . . . . . . . . . . . . .\n76\n8.4\nAttempt #3: Better Accounting . . . . . . . . . . . . . . . . .\n77\n8.5\nTuning MLFQ And Other Issues . . . . . . . . . . . . . . . .\n78\n8.6\nMLFQ: Summary\n. . . . . . . . . . . . . . . . . . . . . . . .\n79\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n81\nHomework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n82\n9\nScheduling: Proportional Share\n83\n9.1\nBasic Concept: Tickets Represent Your Share . . . . . . . . .\n83\n9.2\nTicket Mechanisms\n. . . . . . . . . . . . . . . . . . . . . . .\n85\n9.3\nImplementation . . . . . . . . . . . . . . . . . . . . . . . . .\n86\n9.4\nAn Example . . . . . . . . . . . . . . . . . . . . . . . . . . .\n87\n9.5\nHow To Assign Tickets? . . . . . . . . . . . . . . . . . . . . .\n88\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "page_number": 9,
      "chapter_number": 2,
      "summary": "This chapter covers segment 2 (pages 9-16). Key topics include book, operating, and operations. The major reason for this omission is that we believe in the most old-fashioned\nof teaching methods: chalk and a blackboard.",
      "keywords": [
        "book",
        "Operating Systems",
        "Systems",
        "Operating",
        "students",
        "System Call",
        "process",
        "Version",
        "references",
        "Easy",
        "Pieces",
        "Easy Pieces",
        "Arpaci-Dusseau",
        "free",
        "years"
      ],
      "concepts": [
        "book",
        "operating",
        "operations",
        "process",
        "processes",
        "student",
        "basic",
        "including",
        "include",
        "references"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 1,
          "title": "Segment 1 (pages 1-20)",
          "relevance_score": 0.71,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 2,
          "title": "Segment 2 (pages 9-17)",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 2,
          "title": "Segment 2 (pages 21-38)",
          "relevance_score": 0.61,
          "method": "api"
        },
        {
          "book": "A Philosophy of Software Design",
          "chapter": 2,
          "title": "Segment 2 (pages 10-17)",
          "relevance_score": 0.59,
          "method": "api"
        },
        {
          "book": "AI Agents In Action",
          "chapter": 3,
          "title": "Segment 3 (pages 17-24)",
          "relevance_score": 0.57,
          "method": "api"
        }
      ]
    },
    {
      "number": 3,
      "title": "Segment 3 (pages 17-24)",
      "start_page": 17,
      "end_page": 24,
      "detection_method": "topic_boundary",
      "content": "CONTENTS\nxiii\n9.6\nWhy Not Deterministic?\n. . . . . . . . . . . . . . . . . . . .\n88\n9.7\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n89\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n91\nHomework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n92\n10 Multiprocessor Scheduling (Advanced)\n93\n10.1 Background: Multiprocessor Architecture\n. . . . . . . . . .\n94\n10.2 Don’t Forget Synchronization\n. . . . . . . . . . . . . . . . .\n96\n10.3 One Final Issue: Cache Afﬁnity\n. . . . . . . . . . . . . . . .\n97\n10.4 Single-Queue Scheduling . . . . . . . . . . . . . . . . . . . .\n97\n10.5 Multi-Queue Scheduling . . . . . . . . . . . . . . . . . . . .\n99\n10.6 Linux Multiprocessor Schedulers\n. . . . . . . . . . . . . . . 102\n10.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103\n11 Summary Dialogue on CPU Virtualization\n105\n12 A Dialogue on Memory Virtualization\n107\n13 The Abstraction: Address Spaces\n109\n13.1 Early Systems\n. . . . . . . . . . . . . . . . . . . . . . . . . . 109\n13.2 Multiprogramming and Time Sharing . . . . . . . . . . . . . 110\n13.3 The Address Space\n. . . . . . . . . . . . . . . . . . . . . . . 111\n13.4 Goals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113\n13.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116\n14 Interlude: Memory API\n119\n14.1 Types of Memory . . . . . . . . . . . . . . . . . . . . . . . . 119\n14.2 The malloc() Call . . . . . . . . . . . . . . . . . . . . . . . 120\n14.3 The free() Call\n. . . . . . . . . . . . . . . . . . . . . . . . 122\n14.4 Common Errors . . . . . . . . . . . . . . . . . . . . . . . . . 122\n14.5 Underlying OS Support . . . . . . . . . . . . . . . . . . . . . 125\n14.6 Other Calls . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125\n14.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127\n15 Mechanism: Address Translation\n129\n15.1 Assumptions . . . . . . . . . . . . . . . . . . . . . . . . . . . 130\n15.2 An Example . . . . . . . . . . . . . . . . . . . . . . . . . . . 130\n15.3 Dynamic (Hardware-based) Relocation . . . . . . . . . . . . 133\n15.4 OS Issues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136\n15.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139\nHomework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140\n16 Segmentation\n141\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\nxiv\nCONTENTS\n16.1 Segmentation: Generalized Base/Bounds\n. . . . . . . . . . 141\n16.2 Which Segment Are We Referring To? . . . . . . . . . . . . . 144\n16.3 What About The Stack? . . . . . . . . . . . . . . . . . . . . . 145\n16.4 Support for Sharing . . . . . . . . . . . . . . . . . . . . . . . 146\n16.5 Fine-grained vs. Coarse-grained Segmentation\n. . . . . . . 147\n16.6 OS Support . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147\n16.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150\nHomework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152\n17 Free-Space Management\n153\n17.1 Assumptions . . . . . . . . . . . . . . . . . . . . . . . . . . . 154\n17.2 Low-level Mechanisms . . . . . . . . . . . . . . . . . . . . . 155\n17.3 Basic Strategies\n. . . . . . . . . . . . . . . . . . . . . . . . . 163\n17.4 Other Approaches . . . . . . . . . . . . . . . . . . . . . . . . 165\n17.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168\n18 Paging: Introduction\n169\n18.1 Where Are Page Tables Stored?\n. . . . . . . . . . . . . . . . 172\n18.2 What’s Actually In The Page Table? . . . . . . . . . . . . . . 173\n18.3 Paging: Also Too Slow\n. . . . . . . . . . . . . . . . . . . . . 174\n18.4 A Memory Trace . . . . . . . . . . . . . . . . . . . . . . . . . 176\n18.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180\nHomework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181\n19 Paging: Faster Translations (TLBs)\n183\n19.1 TLB Basic Algorithm\n. . . . . . . . . . . . . . . . . . . . . . 183\n19.2 Example: Accessing An Array . . . . . . . . . . . . . . . . . 185\n19.3 Who Handles The TLB Miss? . . . . . . . . . . . . . . . . . . 187\n19.4 TLB Contents: What’s In There? . . . . . . . . . . . . . . . . 189\n19.5 TLB Issue: Context Switches . . . . . . . . . . . . . . . . . . 190\n19.6 Issue: Replacement Policy\n. . . . . . . . . . . . . . . . . . . 192\n19.7 A Real TLB Entry\n. . . . . . . . . . . . . . . . . . . . . . . . 193\n19.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195\nHomework (Measurement) . . . . . . . . . . . . . . . . . . . . . . 197\n20 Paging: Smaller Tables\n201\n20.1 Simple Solution: Bigger Pages . . . . . . . . . . . . . . . . . 201\n20.2 Hybrid Approach: Paging and Segments . . . . . . . . . . . 202\n20.3 Multi-level Page Tables . . . . . . . . . . . . . . . . . . . . . 205\n20.4 Inverted Page Tables\n. . . . . . . . . . . . . . . . . . . . . . 212\n20.5 Swapping the Page Tables to Disk . . . . . . . . . . . . . . . 213\n20.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 214\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nCONTENTS\nxv\nHomework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215\n21 Beyond Physical Memory: Mechanisms\n217\n21.1 Swap Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218\n21.2 The Present Bit . . . . . . . . . . . . . . . . . . . . . . . . . . 219\n21.3 The Page Fault . . . . . . . . . . . . . . . . . . . . . . . . . . 220\n21.4 What If Memory Is Full?\n. . . . . . . . . . . . . . . . . . . . 221\n21.5 Page Fault Control Flow\n. . . . . . . . . . . . . . . . . . . . 222\n21.6 When Replacements Really Occur . . . . . . . . . . . . . . . 223\n21.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225\n22 Beyond Physical Memory: Policies\n227\n22.1 Cache Management . . . . . . . . . . . . . . . . . . . . . . . 227\n22.2 The Optimal Replacement Policy\n. . . . . . . . . . . . . . . 228\n22.3 A Simple Policy: FIFO\n. . . . . . . . . . . . . . . . . . . . . 230\n22.4 Another Simple Policy: Random . . . . . . . . . . . . . . . . 232\n22.5 Using History: LRU . . . . . . . . . . . . . . . . . . . . . . . 233\n22.6 Workload Examples . . . . . . . . . . . . . . . . . . . . . . . 234\n22.7 Implementing Historical Algorithms . . . . . . . . . . . . . 237\n22.8 Approximating LRU\n. . . . . . . . . . . . . . . . . . . . . . 238\n22.9 Considering Dirty Pages . . . . . . . . . . . . . . . . . . . . 239\n22.10Other VM Policies . . . . . . . . . . . . . . . . . . . . . . . . 240\n22.11Thrashing\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 240\n22.12Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 242\nHomework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 244\n23 The VAX/VMS Virtual Memory System\n245\n23.1 Background\n. . . . . . . . . . . . . . . . . . . . . . . . . . . 245\n23.2 Memory Management Hardware . . . . . . . . . . . . . . . 246\n23.3 A Real Address Space . . . . . . . . . . . . . . . . . . . . . . 247\n23.4 Page Replacement . . . . . . . . . . . . . . . . . . . . . . . . 249\n23.5 Other Neat VM Tricks . . . . . . . . . . . . . . . . . . . . . . 250\n23.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 252\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253\n24 Summary Dialogue on Memory Virtualization\n255\nII Concurrency\n259\n25 A Dialogue on Concurrency\n261\n26 Concurrency: An Introduction\n263\n26.1 An Example: Thread Creation . . . . . . . . . . . . . . . . . 264\n26.2 Why It Gets Worse: Shared Data . . . . . . . . . . . . . . . . 267\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\nxvi\nCONTENTS\n26.3 The Heart of the Problem: Uncontrolled Scheduling\n. . . . 269\n26.4 The Wish For Atomicity . . . . . . . . . . . . . . . . . . . . . 271\n26.5 One More Problem: Waiting For Another . . . . . . . . . . . 273\n26.6 Summary: Why in OS Class? . . . . . . . . . . . . . . . . . . 273\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275\nHomework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 276\n27 Interlude: Thread API\n279\n27.1 Thread Creation . . . . . . . . . . . . . . . . . . . . . . . . . 279\n27.2 Thread Completion . . . . . . . . . . . . . . . . . . . . . . . 280\n27.3 Locks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 283\n27.4 Condition Variables . . . . . . . . . . . . . . . . . . . . . . . 285\n27.5 Compiling and Running\n. . . . . . . . . . . . . . . . . . . . 287\n27.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 287\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289\n28 Locks\n291\n28.1 Locks: The Basic Idea . . . . . . . . . . . . . . . . . . . . . . 291\n28.2 Pthread Locks . . . . . . . . . . . . . . . . . . . . . . . . . . 292\n28.3 Building A Lock . . . . . . . . . . . . . . . . . . . . . . . . . 293\n28.4 Evaluating Locks\n. . . . . . . . . . . . . . . . . . . . . . . . 293\n28.5 Controlling Interrupts . . . . . . . . . . . . . . . . . . . . . . 294\n28.6 Test And Set (Atomic Exchange) . . . . . . . . . . . . . . . . 295\n28.7 Building A Working Spin Lock . . . . . . . . . . . . . . . . . 297\n28.8 Evaluating Spin Locks\n. . . . . . . . . . . . . . . . . . . . . 299\n28.9 Compare-And-Swap\n. . . . . . . . . . . . . . . . . . . . . . 299\n28.10Load-Linked and Store-Conditional . . . . . . . . . . . . . . 300\n28.11Fetch-And-Add . . . . . . . . . . . . . . . . . . . . . . . . . 302\n28.12Summary: So Much Spinning\n. . . . . . . . . . . . . . . . . 303\n28.13A Simple Approach: Just Yield, Baby . . . . . . . . . . . . . 304\n28.14Using Queues: Sleeping Instead Of Spinning . . . . . . . . . 305\n28.15Different OS, Different Support\n. . . . . . . . . . . . . . . . 307\n28.16Two-Phase Locks\n. . . . . . . . . . . . . . . . . . . . . . . . 307\n28.17Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 308\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 309\n29 Lock-based Concurrent Data Structures\n311\n29.1 Concurrent Counters . . . . . . . . . . . . . . . . . . . . . . 311\n29.2 Concurrent Linked Lists\n. . . . . . . . . . . . . . . . . . . . 316\n29.3 Concurrent Queues . . . . . . . . . . . . . . . . . . . . . . . 319\n29.4 Concurrent Hash Table . . . . . . . . . . . . . . . . . . . . . 320\n29.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 322\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 323\n30 Condition Variables\n325\n30.1 Deﬁnition and Routines . . . . . . . . . . . . . . . . . . . . . 326\n30.2 The Producer/Consumer (Bound Buffer) Problem . . . . . . 329\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nCONTENTS\nxvii\n30.3 Covering Conditions\n. . . . . . . . . . . . . . . . . . . . . . 337\n30.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 338\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 339\n31 Semaphores\n341\n31.1 Semaphores: A Deﬁnition\n. . . . . . . . . . . . . . . . . . . 341\n31.2 Binary Semaphores (Locks) . . . . . . . . . . . . . . . . . . . 343\n31.3 Semaphores As Condition Variables . . . . . . . . . . . . . . 344\n31.4 The Producer/Consumer (Bounded-Buffer) Problem . . . . 346\n31.5 Reader-Writer Locks\n. . . . . . . . . . . . . . . . . . . . . . 350\n31.6 The Dining Philosophers . . . . . . . . . . . . . . . . . . . . 352\n31.7 How To Implement Semaphores . . . . . . . . . . . . . . . . 355\n31.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 356\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 357\n32 Common Concurrency Problems\n359\n32.1 What Types Of Bugs Exist? . . . . . . . . . . . . . . . . . . . 359\n32.2 Non-Deadlock Bugs . . . . . . . . . . . . . . . . . . . . . . . 360\n32.3 Deadlock Bugs . . . . . . . . . . . . . . . . . . . . . . . . . . 363\n32.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 370\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 371\n33 Event-based Concurrency (Advanced)\n373\n33.1 The Basic Idea: An Event Loop . . . . . . . . . . . . . . . . . 373\n33.2 An Important API: select() (or poll())\n. . . . . . . . . 374\n33.3 Using select()\n. . . . . . . . . . . . . . . . . . . . . . . . 375\n33.4 Why Simpler? No Locks Needed\n. . . . . . . . . . . . . . . 376\n33.5 A Problem: Blocking System Calls . . . . . . . . . . . . . . . 377\n33.6 A Solution: Asynchronous I/O\n. . . . . . . . . . . . . . . . 377\n33.7 Another Problem: State Management . . . . . . . . . . . . . 380\n33.8 What Is Still Difﬁcult With Events . . . . . . . . . . . . . . . 381\n33.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 381\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 382\n34 Summary Dialogue on Concurrency\n383\nIII Persistence\n385\n35 A Dialogue on Persistence\n387\n36 I/O Devices\n389\n36.1 System Architecture . . . . . . . . . . . . . . . . . . . . . . . 389\n36.2 A Canonical Device . . . . . . . . . . . . . . . . . . . . . . . 390\n36.3 The Canonical Protocol . . . . . . . . . . . . . . . . . . . . . 391\n36.4 Lowering CPU Overhead With Interrupts\n. . . . . . . . . . 392\n36.5 More Efﬁcient Data Movement With DMA . . . . . . . . . . 393\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\nxviii\nCONTENTS\n36.6 Methods Of Device Interaction . . . . . . . . . . . . . . . . . 394\n36.7 Fitting Into The OS: The Device Driver . . . . . . . . . . . . 395\n36.8 Case Study: A Simple IDE Disk Driver . . . . . . . . . . . . 396\n36.9 Historical Notes . . . . . . . . . . . . . . . . . . . . . . . . . 399\n36.10Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 399\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 400\n37 Hard Disk Drives\n403\n37.1 The Interface . . . . . . . . . . . . . . . . . . . . . . . . . . . 403\n37.2 Basic Geometry\n. . . . . . . . . . . . . . . . . . . . . . . . . 404\n37.3 A Simple Disk Drive\n. . . . . . . . . . . . . . . . . . . . . . 404\n37.4 I/O Time: Doing The Math . . . . . . . . . . . . . . . . . . . 408\n37.5 Disk Scheduling . . . . . . . . . . . . . . . . . . . . . . . . . 412\n37.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 416\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 417\nHomework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 418\n38 Redundant Arrays of Inexpensive Disks (RAIDs)\n421\n38.1 Interface And RAID Internals\n. . . . . . . . . . . . . . . . . 422\n38.2 Fault Model\n. . . . . . . . . . . . . . . . . . . . . . . . . . . 423\n38.3 How To Evaluate A RAID\n. . . . . . . . . . . . . . . . . . . 423\n38.4 RAID Level 0: Striping . . . . . . . . . . . . . . . . . . . . . 424\n38.5 RAID Level 1: Mirroring . . . . . . . . . . . . . . . . . . . . 427\n38.6 RAID Level 4: Saving Space With Parity\n. . . . . . . . . . . 430\n38.7 RAID Level 5: Rotating Parity . . . . . . . . . . . . . . . . . 434\n38.8 RAID Comparison: A Summary . . . . . . . . . . . . . . . . 435\n38.9 Other Interesting RAID Issues . . . . . . . . . . . . . . . . . 436\n38.10Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 436\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 437\nHomework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 439\n39 Interlude: File and Directories\n441\n39.1 Files and Directories . . . . . . . . . . . . . . . . . . . . . . . 441\n39.2 The File System Interface . . . . . . . . . . . . . . . . . . . . 443\n39.3 Creating Files\n. . . . . . . . . . . . . . . . . . . . . . . . . . 443\n39.4 Reading and Writing Files\n. . . . . . . . . . . . . . . . . . . 444\n39.5 Reading And Writing, But Not Sequentially . . . . . . . . . 446\n39.6 Writing Immediately with fsync()\n. . . . . . . . . . . . . 447\n39.7 Renaming Files\n. . . . . . . . . . . . . . . . . . . . . . . . . 448\n39.8 Getting Information About Files . . . . . . . . . . . . . . . . 449\n39.9 Removing Files\n. . . . . . . . . . . . . . . . . . . . . . . . . 450\n39.10Making Directories\n. . . . . . . . . . . . . . . . . . . . . . . 450\n39.11Reading Directories . . . . . . . . . . . . . . . . . . . . . . . 451\n39.12Deleting Directories . . . . . . . . . . . . . . . . . . . . . . . 452\n39.13Hard Links . . . . . . . . . . . . . . . . . . . . . . . . . . . . 452\n39.14Symbolic Links\n. . . . . . . . . . . . . . . . . . . . . . . . . 454\n39.15Making and Mounting a File System\n. . . . . . . . . . . . . 456\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nCONTENTS\nxix\n39.16Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 457\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 458\nHomework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 459\n40 File System Implementation\n461\n40.1 The Way To Think . . . . . . . . . . . . . . . . . . . . . . . . 461\n40.2 Overall Organization . . . . . . . . . . . . . . . . . . . . . . 462\n40.3 File Organization: The Inode . . . . . . . . . . . . . . . . . . 464\n40.4 Directory Organization . . . . . . . . . . . . . . . . . . . . . 469\n40.5 Free Space Management\n. . . . . . . . . . . . . . . . . . . . 469\n40.6 Access Paths: Reading and Writing . . . . . . . . . . . . . . 470\n40.7 Caching and Buffering\n. . . . . . . . . . . . . . . . . . . . . 474\n40.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 475\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 476\nHomework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 477\n41 Locality and The Fast File System\n479\n41.1 The Problem: Poor Performance . . . . . . . . . . . . . . . . 479\n41.2 FFS: Disk Awareness Is The Solution\n. . . . . . . . . . . . . 481\n41.3 Organizing Structure: The Cylinder Group . . . . . . . . . . 481\n41.4 Policies: How To Allocate Files and Directories\n. . . . . . . 482\n41.5 Measuring File Locality . . . . . . . . . . . . . . . . . . . . . 483\n41.6 The Large-File Exception . . . . . . . . . . . . . . . . . . . . 484\n41.7 A Few Other Things About FFS . . . . . . . . . . . . . . . . 486\n41.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 488\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 489\n42 Crash Consistency: FSCK and Journaling\n491\n42.1 A Detailed Example . . . . . . . . . . . . . . . . . . . . . . . 492\n42.2 Solution #1: The File System Checker . . . . . . . . . . . . . 495\n42.3 Solution #2: Journaling (or Write-Ahead Logging) . . . . . . 497\n42.4 Solution #3: Other Approaches\n. . . . . . . . . . . . . . . . 507\n42.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 508\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 509\n43 Log-structured File Systems\n511\n43.1 Writing To Disk Sequentially . . . . . . . . . . . . . . . . . . 512\n43.2 Writing Sequentially And Effectively . . . . . . . . . . . . . 513\n43.3 How Much To Buffer? . . . . . . . . . . . . . . . . . . . . . . 514\n43.4 Problem: Finding Inodes . . . . . . . . . . . . . . . . . . . . 515\n43.5 Solution Through Indirection: The Inode Map . . . . . . . . 515\n43.6 The Checkpoint Region . . . . . . . . . . . . . . . . . . . . . 516\n43.7 Reading A File From Disk: A Recap . . . . . . . . . . . . . . 517\n43.8 What About Directories? . . . . . . . . . . . . . . . . . . . . 517\n43.9 A New Problem: Garbage Collection . . . . . . . . . . . . . 518\n43.10Determining Block Liveness . . . . . . . . . . . . . . . . . . 520\n43.11A Policy Question: Which Blocks To Clean, And When?\n. . 521\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\nxx\nCONTENTS\n43.12Crash Recovery And The Log\n. . . . . . . . . . . . . . . . . 521\n43.13Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 522\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 524\n44 Data Integrity and Protection\n527\n44.1 Disk Failure Modes . . . . . . . . . . . . . . . . . . . . . . . 527\n44.2 Handling Latent Sector Errors . . . . . . . . . . . . . . . . . 529\n44.3 Detecting Corruption: The Checksum . . . . . . . . . . . . . 530\n44.4 Using Checksums . . . . . . . . . . . . . . . . . . . . . . . . 533\n44.5 A New Problem: Misdirected Writes\n. . . . . . . . . . . . . 534\n44.6 One Last Problem: Lost Writes . . . . . . . . . . . . . . . . . 535\n44.7 Scrubbing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 535\n44.8 Overheads Of Checksumming . . . . . . . . . . . . . . . . . 536\n44.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 536\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 537\n45 Summary Dialogue on Persistence\n539\n46 A Dialogue on Distribution\n541\n47 Distributed Systems\n543\n47.1 Communication Basics . . . . . . . . . . . . . . . . . . . . . 544\n47.2 Unreliable Communication Layers\n. . . . . . . . . . . . . . 545\n47.3 Reliable Communication Layers . . . . . . . . . . . . . . . . 547\n47.4 Communication Abstractions\n. . . . . . . . . . . . . . . . . 549\n47.5 Remote Procedure Call (RPC)\n. . . . . . . . . . . . . . . . . 551\n47.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 556\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 557\n48 Sun’s Network File System (NFS)\n559\n48.1 A Basic Distributed File System . . . . . . . . . . . . . . . . 560\n48.2 On To NFS . . . . . . . . . . . . . . . . . . . . . . . . . . . . 561\n48.3 Focus: Simple and Fast Server Crash Recovery . . . . . . . . 561\n48.4 Key To Fast Crash Recovery: Statelessness . . . . . . . . . . 562\n48.5 The NFSv2 Protocol . . . . . . . . . . . . . . . . . . . . . . . 563\n48.6 From Protocol to Distributed File System . . . . . . . . . . . 565\n48.7 Handling Server Failure with Idempotent Operations . . . . 567\n48.8 Improving Performance: Client-side Caching\n. . . . . . . . 569\n48.9 The Cache Consistency Problem . . . . . . . . . . . . . . . . 569\n48.10Assessing NFS Cache Consistency . . . . . . . . . . . . . . . 571\n48.11Implications on Server-Side Write Buffering . . . . . . . . . 571\n48.12Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 573\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 574\n49 The Andrew File System (AFS)\n575\n49.1 AFS Version 1\n. . . . . . . . . . . . . . . . . . . . . . . . . . 575\n49.2 Problems with Version 1\n. . . . . . . . . . . . . . . . . . . . 576\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "page_number": 17,
      "chapter_number": 3,
      "summary": "This chapter covers segment 3 (pages 17-24). Key topics include summary, file, and problem.",
      "keywords": [
        "References",
        "Summary",
        "File System",
        "File",
        "Summary Dialogue",
        "System",
        "Problem",
        "Homework",
        "Distributed File System",
        "Memory",
        "Locks",
        "Page Tables",
        "Dialogue",
        "Disk",
        "CONTENTS"
      ],
      "concepts": [
        "summary",
        "file",
        "problem",
        "disk",
        "locks",
        "references",
        "referring",
        "paging",
        "pages",
        "contents"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 1,
          "title": "Segment 1 (pages 1-20)",
          "relevance_score": 0.74,
          "method": "api"
        },
        {
          "book": "AI Agents and Applications",
          "chapter": 11,
          "title": "Segment 11 (pages 84-95)",
          "relevance_score": 0.73,
          "method": "api"
        },
        {
          "book": "More Effective C++",
          "chapter": 8,
          "title": "Segment 8 (pages 61-75)",
          "relevance_score": 0.71,
          "method": "api"
        },
        {
          "book": "Python Distilled",
          "chapter": 38,
          "title": "Segment 38 (pages 345-352)",
          "relevance_score": 0.7,
          "method": "api"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 59,
          "title": "Segment 59 (pages 499-500)",
          "relevance_score": 0.68,
          "method": "api"
        }
      ]
    },
    {
      "number": 4,
      "title": "Segment 4 (pages 25-32)",
      "start_page": 25,
      "end_page": 32,
      "detection_method": "topic_boundary",
      "content": "CONTENTS\nxxi\n49.3 Improving the Protocol . . . . . . . . . . . . . . . . . . . . . 578\n49.4 AFS Version 2\n. . . . . . . . . . . . . . . . . . . . . . . . . . 578\n49.5 Cache Consistency\n. . . . . . . . . . . . . . . . . . . . . . . 580\n49.6 Crash Recovery\n. . . . . . . . . . . . . . . . . . . . . . . . . 582\n49.7 Scale And Performance Of AFSv2 . . . . . . . . . . . . . . . 582\n49.8 AFS: Other Improvements . . . . . . . . . . . . . . . . . . . 584\n49.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 585\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 587\n50 Summary Dialogue on Distribution\n589\nGeneral Index\n591\nAsides\n601\nTips\n603\nCruces\n605\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\nList of Figures\n2.1\nSimple Example: Code That Loops and Prints\n. . . . . . . . .\n5\n2.2\nRunning Many Programs At Once . . . . . . . . . . . . . . . .\n6\n2.3\nA Program that Accesses Memory\n. . . . . . . . . . . . . . . .\n7\n2.4\nRunning The Memory Program Multiple Times . . . . . . . .\n8\n2.5\nA Multi-threaded Program\n. . . . . . . . . . . . . . . . . . . .\n9\n2.6\nA Program That Does I/O . . . . . . . . . . . . . . . . . . . . .\n11\n4.1\nLoading: From Program To Process . . . . . . . . . . . . . . . .\n28\n4.2\nProcess: State Transitions\n. . . . . . . . . . . . . . . . . . . . .\n30\n4.3\nThe xv6 Proc Structure . . . . . . . . . . . . . . . . . . . . . . .\n31\n5.1\np1.c: Calling fork() . . . . . . . . . . . . . . . . . . . . . . .\n36\n5.2\np2.c: Calling fork() And wait() . . . . . . . . . . . . . . .\n37\n5.3\np3.c: Calling fork(), wait(), And exec()\n. . . . . . . . .\n39\n5.4\np4.c: All Of The Above With Redirection\n. . . . . . . . . . .\n41\n6.1\nThe xv6 Context Switch Code . . . . . . . . . . . . . . . . . . .\n54\n7.1\nFIFO Simple Example\n. . . . . . . . . . . . . . . . . . . . . . .\n61\n7.2\nWhy FIFO Is Not That Great\n. . . . . . . . . . . . . . . . . . .\n61\n7.3\nSJF Simple Example\n. . . . . . . . . . . . . . . . . . . . . . . .\n62\n7.4\nSJF With Late Arrivals From B and C . . . . . . . . . . . . . . .\n63\n7.5\nSTCF Simple Example . . . . . . . . . . . . . . . . . . . . . . .\n64\n7.6\nSJF Again (Bad for Response Time)\n. . . . . . . . . . . . . . .\n65\n7.7\nRound Robin (Good for Response Time)\n. . . . . . . . . . . .\n65\n7.8\nPoor Use of Resources . . . . . . . . . . . . . . . . . . . . . . .\n67\n7.9\nOverlap Allows Better Use of Resources . . . . . . . . . . . . .\n67\n8.1\nMLFQ Example . . . . . . . . . . . . . . . . . . . . . . . . . . .\n73\n8.2\nLong-running Job Over Time . . . . . . . . . . . . . . . . . . .\n74\n8.3\nAlong Came An Interactive Job . . . . . . . . . . . . . . . . . .\n74\n8.4\nA Mixed I/O-intensive and CPU-intensive Workload . . . . .\n75\n8.5\nWithout (Left) and With (Right) Priority Boost . . . . . . . . .\n76\nxxiii\n\n\nxxiv\nLIST OF FIGURES\n8.6\nWithout (Left) and With (Right) Gaming Tolerance\n. . . . . .\n77\n8.7\nLower Priority, Longer Quanta . . . . . . . . . . . . . . . . . .\n78\n9.1\nLottery Scheduling Decision Code . . . . . . . . . . . . . . . .\n86\n9.2\nLottery Fairness Study . . . . . . . . . . . . . . . . . . . . . . .\n87\n10.1 Single CPU With Cache\n. . . . . . . . . . . . . . . . . . . . . .\n94\n10.2 Two CPUs With Caches Sharing Memory . . . . . . . . . . . .\n95\n10.3 Simple List Delete Code . . . . . . . . . . . . . . . . . . . . . .\n97\n13.1 Operating Systems: The Early Days . . . . . . . . . . . . . . . 109\n13.2 Three Processes: Sharing Memory . . . . . . . . . . . . . . . . 110\n13.3 An Example Address Space . . . . . . . . . . . . . . . . . . . . 111\n15.1 A Process And Its Address Space . . . . . . . . . . . . . . . . . 132\n15.2 Physical Memory with a Single Relocated Process . . . . . . . 133\n16.1 An Address Space (Again) . . . . . . . . . . . . . . . . . . . . . 142\n16.2 Placing Segments In Physical Memory\n. . . . . . . . . . . . . 143\n16.3 Non-compacted and Compacted Memory . . . . . . . . . . . . 148\n17.1 An Allocated Region Plus Header\n. . . . . . . . . . . . . . . . 157\n17.2 Speciﬁc Contents Of The Header . . . . . . . . . . . . . . . . . 157\n17.3 A Heap With One Free Chunk\n. . . . . . . . . . . . . . . . . . 159\n17.4 A Heap: After One Allocation . . . . . . . . . . . . . . . . . . . 159\n17.5 Free Space With Three Chunks Allocated . . . . . . . . . . . . 160\n17.6 Free Space With Two Chunks Allocated . . . . . . . . . . . . . 161\n17.7 A Non-Coalesced Free List . . . . . . . . . . . . . . . . . . . . . 162\n18.1 A Simple 64-byte Address Space . . . . . . . . . . . . . . . . . 169\n18.2 64-Byte Address Space Placed In Physical Memory\n. . . . . . 170\n18.3 The Address Translation Process . . . . . . . . . . . . . . . . . 172\n18.4 Example: Page Table in Kernel Physical Memory\n. . . . . . . 173\n18.5 An x86 Page Table Entry (PTE)\n. . . . . . . . . . . . . . . . . . 174\n18.6 Accessing Memory With Paging\n. . . . . . . . . . . . . . . . . 175\n18.7 A Virtual (And Physical) Memory Trace . . . . . . . . . . . . . 178\n19.1 TLB Control Flow Algorithm . . . . . . . . . . . . . . . . . . . 184\n19.2 Example: An Array In A Tiny Address Space . . . . . . . . . . 185\n19.3 TLB Control Flow Algorithm (OS Handled)\n. . . . . . . . . . 188\n19.4 A MIPS TLB Entry . . . . . . . . . . . . . . . . . . . . . . . . . 193\n19.5 Discovering TLB Sizes and Miss Costs\n. . . . . . . . . . . . . 198\n20.1 A 16-KB Address Space With 1-KB Pages . . . . . . . . . . . . 203\n20.2 Linear (Left) And Multi-Level (Right) Page Tables . . . . . . . 206\n20.3 A 16-KB Address Space With 64-byte Pages . . . . . . . . . . . 207\n20.4 Multi-level Page Table Control Flow . . . . . . . . . . . . . . . 212\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nLIST OF FIGURES\nxxv\n21.1 Physical Memory and Swap Space . . . . . . . . . . . . . . . . 219\n21.2 Page-Fault Control Flow Algorithm (Hardware) . . . . . . . . 222\n21.3 Page-Fault Control Flow Algorithm (Software) . . . . . . . . . 223\n22.1 Random Performance over 10,000 Trials . . . . . . . . . . . . . 232\n22.2 The No-Locality Workload . . . . . . . . . . . . . . . . . . . . . 235\n22.3 The 80-20 Workload . . . . . . . . . . . . . . . . . . . . . . . . . 236\n22.4 The Looping Workload . . . . . . . . . . . . . . . . . . . . . . . 237\n22.5 The 80-20 Workload With Clock\n. . . . . . . . . . . . . . . . . 239\n23.1 The VAX/VMS Address Space\n. . . . . . . . . . . . . . . . . . 247\n26.1 A Single-Threaded Address Space . . . . . . . . . . . . . . . . 264\n26.2 Simple Thread Creation Code (t0.c)\n. . . . . . . . . . . . . . . 265\n26.3 Sharing Data: Oh Oh (t2)\n. . . . . . . . . . . . . . . . . . . . . 267\n27.1 Creating a Thread . . . . . . . . . . . . . . . . . . . . . . . . . . 281\n27.2 Waiting for Thread Completion . . . . . . . . . . . . . . . . . . 282\n27.3 Simpler Argument Passing to a Thread . . . . . . . . . . . . . 283\n27.4 An Example Wrapper . . . . . . . . . . . . . . . . . . . . . . . . 285\n28.1 First Attempt: A Simple Flag . . . . . . . . . . . . . . . . . . . 296\n28.2 A Simple Spin Lock Using Test-and-set . . . . . . . . . . . . . 298\n28.3 Compare-and-swap . . . . . . . . . . . . . . . . . . . . . . . . . 299\n28.4 Load-linked And Store-conditional\n. . . . . . . . . . . . . . . 301\n28.5 Using LL/SC To Build A Lock . . . . . . . . . . . . . . . . . . . 301\n28.6 Ticket Locks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 303\n28.7 Lock With Test-and-set And Yield\n. . . . . . . . . . . . . . . . 304\n28.8 Lock With Queues, Test-and-set, Yield, And Wakeup . . . . . 306\n28.9 Linux-based Futex Locks . . . . . . . . . . . . . . . . . . . . . . 308\n29.1 A Counter Without Locks . . . . . . . . . . . . . . . . . . . . . 312\n29.2 A Counter With Locks . . . . . . . . . . . . . . . . . . . . . . . 312\n29.3 Performance of Traditional vs. Sloppy Counters . . . . . . . . 313\n29.4 Sloppy Counter Implementation . . . . . . . . . . . . . . . . . 315\n29.5 Scaling Sloppy Counters . . . . . . . . . . . . . . . . . . . . . . 316\n29.6 Concurrent Linked List\n. . . . . . . . . . . . . . . . . . . . . . 317\n29.7 Concurrent Linked List: Rewritten . . . . . . . . . . . . . . . . 318\n29.8 Michael and Scott Concurrent Queue\n. . . . . . . . . . . . . . 320\n29.9 A Concurrent Hash Table\n. . . . . . . . . . . . . . . . . . . . . 321\n29.10Scaling Hash Tables\n. . . . . . . . . . . . . . . . . . . . . . . . 321\n30.1 A Parent Waiting For Its Child\n. . . . . . . . . . . . . . . . . . 325\n30.2 Parent Waiting For Child: Spin-based Approach . . . . . . . . 326\n30.3 Parent Waiting For Child: Use A Condition Variable\n. . . . . 327\n30.4 The Put and Get Routines (Version 1) . . . . . . . . . . . . . . 330\n30.5 Producer/Consumer Threads (Version 1)\n. . . . . . . . . . . . 330\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\nxxvi\nLIST OF FIGURES\n30.6 Producer/Consumer: Single CV and If Statement\n. . . . . . . 331\n30.7 Producer/Consumer: Single CV and While . . . . . . . . . . . 333\n30.8 Producer/Consumer: Two CVs and While . . . . . . . . . . . . 335\n30.9 The Final Put and Get Routines . . . . . . . . . . . . . . . . . . 336\n30.10The Final Working Solution . . . . . . . . . . . . . . . . . . . . 336\n30.11Covering Conditions: An Example . . . . . . . . . . . . . . . . 338\n31.1 Initializing A Semaphore . . . . . . . . . . . . . . . . . . . . . 342\n31.2 Semaphore: Deﬁnitions of Wait and Post . . . . . . . . . . . . 342\n31.3 A Binary Semaphore, a.k.a. a Lock . . . . . . . . . . . . . . . . 343\n31.4 A Parent Waiting For Its Child\n. . . . . . . . . . . . . . . . . . 345\n31.5 The Put and Get Routines . . . . . . . . . . . . . . . . . . . . . 347\n31.6 Adding the Full and Empty Conditions . . . . . . . . . . . . . 347\n31.7 Adding Mutual Exclusion (Incorrectly)\n. . . . . . . . . . . . . 349\n31.8 Adding Mutual Exclusion (Correctly) . . . . . . . . . . . . . . 350\n31.9 A Simple Reader-Writer Lock . . . . . . . . . . . . . . . . . . . 351\n31.10The Dining Philosophers\n. . . . . . . . . . . . . . . . . . . . . 353\n31.11The getforks() and putforks() Routines\n. . . . . . . . . 354\n31.12Implementing Zemaphores with Locks and CVs . . . . . . . . 355\n32.1 The Deadlock Dependency Graph . . . . . . . . . . . . . . . . 364\n33.1 Simple Code using select()\n. . . . . . . . . . . . . . . . . . 376\n36.1 Prototypical System Architecture . . . . . . . . . . . . . . . . . 390\n36.2 A Canonical Device . . . . . . . . . . . . . . . . . . . . . . . . . 391\n36.3 The File System Stack\n. . . . . . . . . . . . . . . . . . . . . . . 396\n36.4 The IDE Interface . . . . . . . . . . . . . . . . . . . . . . . . . . 397\n36.5 The xv6 IDE Disk Driver (Simpliﬁed) . . . . . . . . . . . . . . 398\n37.1 A Disk With Just A Single Track . . . . . . . . . . . . . . . . . 404\n37.2 A Single Track Plus A Head . . . . . . . . . . . . . . . . . . . . 405\n37.3 Three Tracks Plus A Head (Right: With Seek)\n. . . . . . . . . 406\n37.4 Three Tracks: Track Skew Of 2 . . . . . . . . . . . . . . . . . . 407\n37.5 SSTF: Scheduling Requests 21 And 2\n. . . . . . . . . . . . . . 412\n37.6 SSTF: Sometimes Not Good Enough . . . . . . . . . . . . . . . 414\n39.1 An Example Directory Tree\n. . . . . . . . . . . . . . . . . . . . 442\n41.1 FFS Locality For SEER Traces . . . . . . . . . . . . . . . . . . . 483\n41.2 Amortization: How Big Do Chunks Have To Be? . . . . . . . . 486\n41.3 FFS: Standard Versus Parameterized Placement\n. . . . . . . . 487\n47.1 Example UDP/IP Client/Server Code . . . . . . . . . . . . . . . 545\n47.2 A Simple UDP Library . . . . . . . . . . . . . . . . . . . . . . . 546\n47.3 Message Plus Acknowledgment\n. . . . . . . . . . . . . . . . . 547\n47.4 Message Plus Acknowledgment: Dropped Request . . . . . . 548\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nLIST OF FIGURES\nxxvii\n47.5 Message Plus Acknowledgment: Dropped Reply\n. . . . . . . 549\n48.1 A Generic Client/Server System\n. . . . . . . . . . . . . . . . . 559\n48.2 Distributed File System Architecture\n. . . . . . . . . . . . . . 560\n48.3 Client Code: Reading From A File . . . . . . . . . . . . . . . . 562\n48.4 The NFS Protocol: Examples\n. . . . . . . . . . . . . . . . . . . 564\n48.5 The Three Types of Loss . . . . . . . . . . . . . . . . . . . . . . 568\n48.6 The Cache Consistency Problem . . . . . . . . . . . . . . . . . 570\n49.1 AFSv1 Protocol Highlights\n. . . . . . . . . . . . . . . . . . . . 576\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "page_number": 25,
      "chapter_number": 4,
      "summary": "This chapter covers segment 4 (pages 25-32). Key topics include simple, examples, and code.",
      "keywords": [
        "Address Space",
        "Space",
        "Control Flow Algorithm",
        "Physical Memory",
        "Memory",
        "Simple",
        "Address",
        "Control Flow",
        "List",
        "Flow Algorithm",
        "Code",
        "Parent Waiting",
        "TLB Control Flow",
        "Lock",
        "Page Table"
      ],
      "concepts": [
        "simple",
        "examples",
        "code",
        "paging",
        "pages",
        "list",
        "thread",
        "memory",
        "space",
        "version"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 1,
          "title": "Segment 1 (pages 1-20)",
          "relevance_score": 0.77,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 33,
          "title": "Segment 33 (pages 662-682)",
          "relevance_score": 0.75,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 12,
          "title": "Segment 12 (pages 219-238)",
          "relevance_score": 0.73,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 42,
          "title": "Segment 42 (pages 838-857)",
          "relevance_score": 0.73,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 51,
          "title": "Segment 51 (pages 1023-1040)",
          "relevance_score": 0.72,
          "method": "api"
        }
      ]
    },
    {
      "number": 5,
      "title": "Segment 5 (pages 33-40)",
      "start_page": 33,
      "end_page": 40,
      "detection_method": "topic_boundary",
      "content": "List of Tables\n6.1\nDirection Execution Protocol (Without Limits) . . . . . . . . .\n46\n6.2\nLimited Direction Execution Protocol . . . . . . . . . . . . . .\n49\n6.3\nLimited Direction Execution Protocol (Timer Interrupt) . . . .\n53\n9.1\nStride Scheduling: A Trace\n. . . . . . . . . . . . . . . . . . . .\n89\n16.1 Segment Register Values . . . . . . . . . . . . . . . . . . . . . . 143\n16.2 Segment Registers (With Negative-Growth Support) . . . . . 146\n16.3 Segment Register Values (with Protection) . . . . . . . . . . . 147\n20.1 A Page Table For 16-KB Address Space\n. . . . . . . . . . . . . 203\n20.2 A Page Directory, And Pieces Of Page Table\n. . . . . . . . . . 209\n22.1 Tracing the Optimal Policy\n. . . . . . . . . . . . . . . . . . . . 229\n22.2 Tracing the FIFO Policy\n. . . . . . . . . . . . . . . . . . . . . . 231\n22.3 Tracing the Random Policy\n. . . . . . . . . . . . . . . . . . . . 232\n22.4 Tracing the LRU Policy . . . . . . . . . . . . . . . . . . . . . . . 233\n26.1 Thread Trace (1) . . . . . . . . . . . . . . . . . . . . . . . . . . . 266\n26.2 Thread Trace (2) . . . . . . . . . . . . . . . . . . . . . . . . . . . 266\n26.3 Thread Trace (3) . . . . . . . . . . . . . . . . . . . . . . . . . . . 266\n26.4 The Problem: Up Close and Personal\n. . . . . . . . . . . . . . 270\n28.1 Trace: No Mutual Exclusion . . . . . . . . . . . . . . . . . . . . 296\n29.1 Tracing the Sloppy Counters\n. . . . . . . . . . . . . . . . . . . 314\n30.1 Thread Trace: Broken Solution (Version 1)\n. . . . . . . . . . . 332\n30.2 Thread Trace: Broken Solution (Version 2)\n. . . . . . . . . . . 334\n31.1 Thread Trace: Single Thread Using A Semaphore . . . . . . . 343\n31.2 Thread Trace: Two Threads Using A Semaphore . . . . . . . . 344\n31.3 Thread Trace: Parent Waiting For Child (Case 1) . . . . . . . . 346\nxxix\n\n\nxxx\nLIST OF TABLES\n31.4 Thread Trace: Parent Waiting For Child (Case 2) . . . . . . . . 346\n32.1 Bugs In Modern Applications . . . . . . . . . . . . . . . . . . . 360\n37.1 Disk Drive Specs: SCSI Versus SATA . . . . . . . . . . . . . . 409\n37.2 Disk Drive Performance: SCSI Versus SATA . . . . . . . . . . 410\n38.1 RAID-0: Simple Striping\n. . . . . . . . . . . . . . . . . . . . . 424\n38.2 Striping with a Bigger Chunk Size . . . . . . . . . . . . . . . . 424\n38.3 Simple RAID-1: Mirroring\n. . . . . . . . . . . . . . . . . . . . 428\n38.4 Full-stripe Writes In RAID-4\n. . . . . . . . . . . . . . . . . . . 432\n38.5 Example: Writes To 4, 13, And Respective Parity Blocks . . . . 433\n38.6 RAID-5 With Rotated Parity . . . . . . . . . . . . . . . . . . . . 434\n38.7 RAID Capacity, Reliability, and Performance . . . . . . . . . . 435\n40.1 The ext2 inode . . . . . . . . . . . . . . . . . . . . . . . . . . . . 466\n40.2 File System Measurement Summary . . . . . . . . . . . . . . . 468\n40.3 File Read Timeline (Time Increasing Downward) . . . . . . . 471\n40.4 File Creation Timeline (Time Increasing Downward) . . . . . 473\n42.1 Data Journaling Timeline . . . . . . . . . . . . . . . . . . . . . 506\n42.2 Metadata Journaling Timeline\n. . . . . . . . . . . . . . . . . . 507\n44.1 Frequency of LSEs and Block Corruption . . . . . . . . . . . . 528\n48.1 Reading A File: Client-side And File Server Actions\n. . . . . 566\n49.1 Reading A File: Client-side And File Server Actions\n. . . . . 579\n49.2 Cache Consistency Timeline\n. . . . . . . . . . . . . . . . . . . 581\n49.3 Comparison: AFS vs. NFS . . . . . . . . . . . . . . . . . . . . . 583\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\n1\nA Dialogue on the Book\nProfessor: Welcome to this book! It’s called Operating Systems in Three Easy\nPieces, and I am here to teach you the things you need to know about operating\nsystems. I am called “Professor”; who are you?\nStudent: Hi Professor! I am called “Student”, as you might have guessed. And\nI am here and ready to learn!\nProfessor: Sounds good. Any questions?\nStudent: Sure! Why is it called “Three Easy Pieces”?\nProfessor: That’s an easy one. Well, you see, there are these great lectures on\nPhysics by Richard Feynman...\nStudent: Oh! The guy who wrote “Surely You’re Joking, Mr. Feynman”, right?\nGreat book! Is this going to be hilarious like that book was?\nProfessor: Um... well, no. That book was great, and I’m glad you’ve read it.\nHopefully this book is more like his notes on Physics. Some of the basics were\nsummed up in a book called “Six Easy Pieces”. He was talking about Physics;\nwe’re going to do Three Easy Pieces on the ﬁne topic of Operating Systems. This\nis appropriate, as Operating Systems are about half as hard as Physics.\nStudent: Well, I liked physics, so that is probably good. What are those pieces?\nProfessor: They are the three key ideas we’re going to learn about: virtualiza-\ntion, concurrency, and persistence. In learning about these ideas, we’ll learn\nall about how an operating system works, including how it decides what program\nto run next on a CPU, how it handles memory overload in a virtual memory sys-\ntem, how virtual machine monitors work, how to manage information on disks,\nand even a little about how to build a distributed system that works when parts\nhave failed. That sort of stuff.\nStudent: I have no idea what you’re talking about, really.\nProfessor: Good! That means you are in the right class.\nStudent: I have another question: what’s the best way to learn this stuff?\nProfessor: Excellent query! Well, each person needs to ﬁgure this out on their\n1\n\n\n2\nA DIALOGUE ON THE BOOK\nown, of course, but here is what I would do: go to class, to hear the professor\nintroduce the material. Then, say at the end of every week, read these notes,\nto help the ideas sink into your head a bit better. Of course, some time later\n(hint: before the exam!), read the notes again to ﬁrm up your knowledge. Of\ncourse, your professor will no doubt assign some homeworks and projects, so you\nshould do those; in particular, doing projects where you write real code to solve\nreal problems is the best way to put the ideas within these notes into action. As\nConfucius said...\nStudent: Oh, I know! ’I hear and I forget. I see and I remember. I do and I\nunderstand.’ Or something like that.\nProfessor: (surprised) How did you know what I was going to say?!\nStudent: It seemed to follow. Also, I am a big fan of Confucius.\nProfessor: Well, I think we are going to get along just ﬁne! Just ﬁne indeed.\nStudent: Professor – just one more question, if I may. What are these dialogues\nfor? I mean, isn’t this just supposed to be a book? Why not present the material\ndirectly?\nProfessor: Ah, good question, good question! Well, I think it is sometimes\nuseful to pull yourself outside of a narrative and think a bit; these dialogues are\nthose times. So you and I are going to work together to make sense of all of these\npretty complex ideas. Are you up for it?\nStudent: So we have to think? Well, I’m up for that. I mean, what else do I have\nto do anyhow? It’s not like I have much of a life outside of this book.\nProfessor: Me neither, sadly. So let’s get to work!\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\n2\nIntroduction to Operating Systems\nIf you are taking an undergraduate operating systems course, you should\nalready have some idea of what a computer program does when it runs.\nIf not, this book (and the corresponding course) is going to be difﬁcult\n– so you should probably stop reading this book, or run to the nearest\nbookstore and quickly consume the necessary background material be-\nfore continuing (both Patt/Patel [PP03] and particularly Bryant/O’Hallaron\n[BOH10] are pretty great books).\nSo what happens when a program runs?\nWell, a running program does one very simple thing: it executes in-\nstructions. Many millions (and these days, even billions) of times ev-\nery second, the processor fetches an instruction from memory, decodes\nit (i.e., ﬁgures out which instruction this is), and executes it (i.e., it does\nthe thing that it is supposed to do, like add two numbers together, access\nmemory, check a condition, jump to a function, and so forth). After it is\ndone with this instruction, the processor moves on to the next instruction,\nand so on, and so on, until the program ﬁnally completes1.\nThus, we have just described the basics of the Von Neumann model of\ncomputing2. Sounds simple, right? But in this class, we will be learning\nthat while a program runs, a lot of other wild things are going on with\nthe primary goal of making the system easy to use.\nThere is a body of software, in fact, that is responsible for making it\neasy to run programs (even allowing you to seemingly run many at the\nsame time), allowing programs to share memory, enabling programs to\ninteract with devices, and other fun stuff like that. That body of software\n1Of course, modern processors do many bizarre and frightening things underneath the\nhood to make programs run faster, e.g., executing multiple instructions at once, and even issu-\ning and completing them out of order! But that is not our concern here; we are just concerned\nwith the simple model most programs assume: that instructions seemingly execute one at a\ntime, in an orderly and sequential fashion.\n2Von Neumann was one of the early pioneers of computing systems. He also did pioneer-\ning work on game theory and atomic bombs, and played in the NBA for six years. OK, one of\nthose things isn’t true.\n3\n\n\n4\nINTRODUCTION TO OPERATING SYSTEMS\nTHE CRUX OF THE PROBLEM:\nHOW TO VIRTUALIZE RESOURCES\nOne central question we will answer in this book is quite simple: how\ndoes the operating system virtualize resources? This is the crux of our\nproblem. Why the OS does this is not the main question, as the answer\nshould be obvious: it makes the system easier to use. Thus, we focus on\nthe how: what mechanisms and policies are implemented by the OS to\nattain virtualization? How does the OS do so efﬁciently? What hardware\nsupport is needed?\nWe will use the “crux of the problem”, in shaded boxes such as this one,\nas a way to call out speciﬁc problems we are trying to solve in building\nan operating system. Thus, within a note on a particular topic, you may\nﬁnd one or more cruces (yes, this is the proper plural) which highlight the\nproblem. The details within the chapter, of course, present the solution,\nor at least the basic parameters of a solution.\nis called the operating system (OS)3, as it is in charge of making sure the\nsystem operates correctly and efﬁciently in an easy-to-use manner.\nThe primary way the OS does this is through a general technique that\nwe call virtualization. That is, the OS takes a physical resource (such as\nthe processor, or memory, or a disk) and transforms it into a more gen-\neral, powerful, and easy-to-use virtual form of itself. Thus, we sometimes\nrefer to the operating system as a virtual machine.\nOf course, in order to allow users to tell the OS what to do and thus\nmake use of the features of the virtual machine (such as running a pro-\ngram, or allocating memory, or accessing a ﬁle), the OS also provides\nsome interfaces (APIs) that you can call. A typical OS, in fact, exports\na few hundred system calls that are available to applications. Because\nthe OS provides these calls to run programs, access memory and devices,\nand other related actions, we also sometimes say that the OS provides a\nstandard library to applications.\nFinally, because virtualization allows many programs to run (thus shar-\ning the CPU), and many programs to concurrently access their own in-\nstructions and data (thus sharing memory), and many programs to access\ndevices (thus sharing disks and so forth), the OS is sometimes known as\na resource manager. Each of the CPU, memory, and disk is a resource\nof the system; it is thus the operating system’s role to manage those re-\nsources, doing so efﬁciently or fairly or indeed with many other possible\ngoals in mind. To understand the role of the OS a little bit better, let’s take\na look at some examples.\n3Another early name for the OS was the supervisor or even the master control program.\nApparently, the latter sounded a little overzealous (see the movie Tron for details) and thus,\nthankfully, “operating system” caught on instead.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "page_number": 33,
      "chapter_number": 5,
      "summary": "This chapter covers segment 5 (pages 33-40). Key topics include professor, memory, and student.",
      "keywords": [
        "Thread Trace",
        "Operating Systems",
        "Direction Execution Protocol",
        "OPERATING",
        "Trace",
        "System",
        "Professor",
        "Thread",
        "Book",
        "Execution Protocol",
        "Student",
        "Direction Execution",
        "programs",
        "File",
        "memory"
      ],
      "concepts": [
        "professor",
        "memory",
        "student",
        "program",
        "trace",
        "tracing",
        "operating",
        "operates",
        "read",
        "timeline"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 1,
          "title": "Segment 1 (pages 1-20)",
          "relevance_score": 0.76,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 51,
          "title": "Segment 51 (pages 1023-1040)",
          "relevance_score": 0.72,
          "method": "api"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 37,
          "title": "Segment 37 (pages 363-372)",
          "relevance_score": 0.7,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 33,
          "title": "Segment 33 (pages 662-682)",
          "relevance_score": 0.7,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 3,
          "title": "Segment 3 (pages 39-59)",
          "relevance_score": 0.68,
          "method": "api"
        }
      ]
    },
    {
      "number": 6,
      "title": "Segment 6 (pages 41-48)",
      "start_page": 41,
      "end_page": 48,
      "detection_method": "topic_boundary",
      "content": "INTRODUCTION TO OPERATING SYSTEMS\n5\n1\n#include <stdio.h>\n2\n#include <stdlib.h>\n3\n#include <sys/time.h>\n4\n#include <assert.h>\n5\n#include \"common.h\"\n6\n7\nint\n8\nmain(int argc, char *argv[])\n9\n{\n10\nif (argc != 2) {\n11\nfprintf(stderr, \"usage: cpu <string>\\n\");\n12\nexit(1);\n13\n}\n14\nchar *str = argv[1];\n15\nwhile (1) {\n16\nSpin(1);\n17\nprintf(\"%s\\n\", str);\n18\n}\n19\nreturn 0;\n20\n}\nFigure 2.1: Simple Example: Code That Loops and Prints\n2.1\nVirtualizing the CPU\nFigure 2.1 depicts our ﬁrst program. It doesn’t do much. In fact, all\nit does is call Spin(), a function that repeatedly checks the time and\nreturns once it has run for a second. Then, it prints out the string that the\nuser passed in on the command line, and repeats, forever.\nLet’s say we save this ﬁle as cpu.c and decide to compile and run it\non a system with a single processor (or CPU as we will sometimes call it).\nHere is what we will see:\nprompt> gcc -o cpu cpu.c -Wall\nprompt> ./cpu \"A\"\nA\nA\nA\nA\nˆC\nprompt>\nNot too interesting of a run – the system begins running the program,\nwhich repeatedly checks the time until a second has elapsed. Once a sec-\nond has passed, the code prints the input string passed in by the user\n(in this example, the letter “A”), and continues. Note the program will\nrun forever; only by pressing “Control-c” (which on UNIX-based systems\nwill terminate the program running in the foreground) can we halt the\nprogram.\nNow, let’s do the same thing, but this time, let’s run many different in-\nstances of this same program. Figure 2.2 shows the results of this slightly\nmore complicated example.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n6\nINTRODUCTION TO OPERATING SYSTEMS\nprompt> ./cpu A & ; ./cpu B & ; ./cpu C & ; ./cpu D &\n[1] 7353\n[2] 7354\n[3] 7355\n[4] 7356\nA\nB\nD\nC\nA\nB\nD\nC\nA\nC\nB\nD\n...\nFigure 2.2: Running Many Programs At Once\nWell, now things are getting a little more interesting. Even though we\nhave only one processor, somehow all four of these programs seem to be\nrunning at the same time! How does this magic happen?4\nIt turns out that the operating system, with some help from the hard-\nware, is in charge of this illusion, i.e., the illusion that the system has a\nvery large number of virtual CPUs. Turning a single CPU (or small set of\nthem) into a seemingly inﬁnite number of CPUs and thus allowing many\nprograms to seemingly run at once is what we call virtualizing the CPU,\nthe focus of the ﬁrst major part of this book.\nOf course, to run programs, and stop them, and otherwise tell the OS\nwhich programs to run, there need to be some interfaces (APIs) that you\ncan use to communicate your desires to the OS. We’ll talk about these\nAPIs throughout this book; indeed, they are the major way in which most\nusers interact with operating systems.\nYou might also notice that the ability to run multiple programs at once\nraises all sorts of new questions. For example, if two programs want to\nrun at a particular time, which should run? This question is answered by\na policy of the OS; policies are used in many different places within an\nOS to answer these types of questions, and thus we will study them as\nwe learn about the basic mechanisms that operating systems implement\n(such as the ability to run multiple programs at once). Hence the role of\nthe OS as a resource manager.\n4Note how we ran four processes at the same time, by using the & symbol. Doing so runs a\njob in the background in the tcsh shell, which means that the user is able to immediately issue\ntheir next command, which in this case is another program to run. The semi-colon between\ncommands allows us to run multiple programs at the same time in tcsh. If you’re using a\ndifferent shell (e.g., bash), it works slightly differently; read documentation online for details.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nINTRODUCTION TO OPERATING SYSTEMS\n7\n1\n#include <unistd.h>\n2\n#include <stdio.h>\n3\n#include <stdlib.h>\n4\n#include \"common.h\"\n5\n6\nint\n7\nmain(int argc, char *argv[])\n8\n{\n9\nint *p = malloc(sizeof(int));\n// a1\n10\nassert(p != NULL);\n11\nprintf(\"(%d) address of p: %08x\\n\",\n12\ngetpid(), (unsigned) p);\n// a2\n13\n*p = 0;\n// a3\n14\nwhile (1) {\n15\nSpin(1);\n16\n*p = *p + 1;\n17\nprintf(\"(%d) p: %d\\n\", getpid(), *p); // a4\n18\n}\n19\nreturn 0;\n20\n}\nFigure 2.3: A Program that Accesses Memory\n2.2\nVirtualizing Memory\nNow let’s consider memory.\nThe model of physical memory pre-\nsented by modern machines is very simple. Memory is just an array of\nbytes; to read memory, one must specify an address to be able to access\nthe data stored there; to write (or update) memory, one must also specify\nthe data to be written to the given address.\nMemory is accessed all the time when a program is running. A pro-\ngram keeps all of its data structures in memory, and accesses them through\nvarious instructions, like loads and stores or other explicit instructions\nthat access memory in doing their work. Don’t forget that each instruc-\ntion of the program is in memory too; thus memory is accessed on each\ninstruction fetch.\nLet’s take a look at a program (in Figure 2.3) that allocates some mem-\nory by calling malloc(). The output of this program can be found here:\nprompt> ./mem\n(2134) memory address of p: 00200000\n(2134) p: 1\n(2134) p: 2\n(2134) p: 3\n(2134) p: 4\n(2134) p: 5\nˆC\nThe program does a couple of things. First, it allocates some memory\n(line a1). Then, it prints out the address of the memory (a2), and then\nputs the number zero into the ﬁrst slot of the newly allocated memory\n(a3). Finally, it loops, delaying for a second and incrementing the value\nstored at the address held in p. With every print statement, it also prints\nout what is called the process identiﬁer (the PID) of the running program.\nThis PID is unique per running process.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n8\nINTRODUCTION TO OPERATING SYSTEMS\nprompt> ./mem &; ./mem &\n[1] 24113\n[2] 24114\n(24113) memory address of p: 00200000\n(24114) memory address of p: 00200000\n(24113) p: 1\n(24114) p: 1\n(24114) p: 2\n(24113) p: 2\n(24113) p: 3\n(24114) p: 3\n(24113) p: 4\n(24114) p: 4\n...\nFigure 2.4: Running The Memory Program Multiple Times\nAgain, this ﬁrst result is not too interesting. The newly allocated mem-\nory is at address 00200000. As the program runs, it slowly updates the\nvalue and prints out the result.\nNow, we again run multiple instances of this same program to see\nwhat happens (Figure 2.4). We see from the example that each running\nprogram has allocated memory at the same address (00200000), and yet\neach seems to be updating the value at 00200000 independently! It is as\nif each running program has its own private memory, instead of sharing\nthe same physical memory with other running programs5.\nIndeed, that is exactly what is happening here as the OS is virtualiz-\ning memory. Each process accesses its own private virtual address space\n(sometimes just called its address space), which the OS somehow maps\nonto the physical memory of the machine. A memory reference within\none running program does not affect the address space of other processes\n(or the OS itself); as far as the running program is concerned, it has phys-\nical memory all to itself. The reality, however, is that physical memory is\na shared resource, managed by the operating system. Exactly how all of\nthis is accomplished is also the subject of the ﬁrst part of this book, on the\ntopic of virtualization.\n2.3\nConcurrency\nAnother main theme of this book is concurrency. We use this concep-\ntual term to refer to a host of problems that arise, and must be addressed,\nwhen working on many things at once (i.e., concurrently) in the same\nprogram. The problems of concurrency arose ﬁrst within the operating\nsystem itself; as you can see in the examples above on virtualization, the\nOS is juggling many things at once, ﬁrst running one process, then an-\nother, and so forth. As it turns out, doing so leads to some deep and\ninteresting problems.\n5For this example to work, you need to make sure address-space randomization is dis-\nabled; randomization, as it turns out, can be a good defense against certain kinds of security\nﬂaws. Read more about it on your own, especially if you want to learn how to break into\ncomputer systems via stack-smashing attacks. Not that we would recommend such a thing...\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nINTRODUCTION TO OPERATING SYSTEMS\n9\n1\n#include <stdio.h>\n2\n#include <stdlib.h>\n3\n#include \"common.h\"\n4\n5\nvolatile int counter = 0;\n6\nint loops;\n7\n8\nvoid *worker(void *arg) {\n9\nint i;\n10\nfor (i = 0; i < loops; i++) {\n11\ncounter++;\n12\n}\n13\nreturn NULL;\n14\n}\n15\n16\nint\n17\nmain(int argc, char *argv[])\n18\n{\n19\nif (argc != 2) {\n20\nfprintf(stderr, \"usage: threads <value>\\n\");\n21\nexit(1);\n22\n}\n23\nloops = atoi(argv[1]);\n24\npthread_t p1, p2;\n25\nprintf(\"Initial value : %d\\n\", counter);\n26\n27\nPthread_create(&p1, NULL, worker, NULL);\n28\nPthread_create(&p2, NULL, worker, NULL);\n29\nPthread_join(p1, NULL);\n30\nPthread_join(p2, NULL);\n31\nprintf(\"Final value\n: %d\\n\", counter);\n32\nreturn 0;\n33\n}\nFigure 2.5: A Multi-threaded Program\nUnfortunately, the problems of concurrency are no longer limited just\nto the OS itself. Indeed, modern multi-threaded programs exhibit the\nsame problems. Let us demonstrate with an example of a multi-threaded\nprogram (Figure 2.5).\nAlthough you might not understand this example fully at the moment\n(and we’ll learn a lot more about it in later chapters, in the section of the\nbook on concurrency), the basic idea is simple. The main program creates\ntwo threads using Pthread create()6. You can think of a thread as a\nfunction running within the same memory space as other functions, with\nmore than one of them active at a time. In this example, each thread starts\nrunning in a routine called worker(), in which it simply increments a\ncounter in a loop for loops number of times.\nBelow is a transcript of what happens when we run this program with\nthe input value for the variable loops set to 1000. The value of loops\n6The actual call should be to lower-case pthread create(); the upper-case version is\nour own wrapper that calls pthread create() and makes sure that the return code indicates\nthat the call succeeded. See the code for details.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n10\nINTRODUCTION TO OPERATING SYSTEMS\nTHE CRUX OF THE PROBLEM:\nHOW TO BUILD CORRECT CONCURRENT PROGRAMS\nWhen there are many concurrently executing threads within the same\nmemory space, how can we build a correctly working program? What\nprimitives are needed from the OS? What mechanisms should be pro-\nvided by the hardware? How can we use them to solve the problems of\nconcurrency?\ndetermines how many times each of the two workers will increment the\nshared counter in a loop. When the program is run with the value of\nloops set to 1000, what do you expect the ﬁnal value of counter to be?\nprompt> gcc -o thread thread.c -Wall -pthread\nprompt> ./thread 1000\nInitial value : 0\nFinal value\n: 2000\nAs you probably guessed, when the two threads are ﬁnished, the ﬁnal\nvalue of the counter is 2000, as each thread incremented the counter 1000\ntimes. Indeed, when the input value of loops is set to N, we would\nexpect the ﬁnal output of the program to be 2N. But life is not so simple,\nas it turns out. Let’s run the same program, but with higher values for\nloops, and see what happens:\nprompt> ./thread 100000\nInitial value : 0\nFinal value\n: 143012\n// huh??\nprompt> ./thread 100000\nInitial value : 0\nFinal value\n: 137298\n// what the??\nIn this run, when we gave an input value of 100,000, instead of getting\na ﬁnal value of 200,000, we instead ﬁrst get 143,012. Then, when we run\nthe program a second time, we not only again get the wrong value, but\nalso a different value than the last time. In fact, if you run the program\nover and over with high values of loops, you may ﬁnd that sometimes\nyou even get the right answer! So why is this happening?\nAs it turns out, the reason for these odd and unusual outcomes relate\nto how instructions are executed, which is one at a time. Unfortunately, a\nkey part of the program above, where the shared counter is incremented,\ntakes three instructions: one to load the value of the counter from mem-\nory into a register, one to increment it, and one to store it back into mem-\nory. Because these three instructions do not execute atomically (all at\nonce), strange things can happen. It is this problem of concurrency that\nwe will address in great detail in the second part of this book.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nINTRODUCTION TO OPERATING SYSTEMS\n11\n1\n#include <stdio.h>\n2\n#include <unistd.h>\n3\n#include <assert.h>\n4\n#include <fcntl.h>\n5\n#include <sys/types.h>\n6\n7\nint\n8\nmain(int argc, char *argv[])\n9\n{\n10\nint fd = open(\"/tmp/file\", O_WRONLY | O_CREAT | O_TRUNC, S_IRWXU);\n11\nassert(fd > -1);\n12\nint rc = write(fd, \"hello world\\n\", 13);\n13\nassert(rc == 13);\n14\nclose(fd);\n15\nreturn 0;\n16\n}\nFigure 2.6: A Program That Does I/O\n2.4\nPersistence\nThe third major theme of the course is persistence. In system memory,\ndata can be easily lost, as devices such as DRAM store values in a volatile\nmanner; when power goes away or the system crashes, any data in mem-\nory is lost. Thus, we need hardware and software to be able to store data\npersistently; such storage is thus critical to any system as users care a\ngreat deal about their data.\nThe hardware comes in the form of some kind of input/output or I/O\ndevice; in modern systems, a hard drive is a common repository for long-\nlived information, although solid-state drives (SSDs) are making head-\nway in this arena as well.\nThe software in the operating system that usually manages the disk is\ncalled the ﬁle system; it is thus responsible for storing any ﬁles the user\ncreates in a reliable and efﬁcient manner on the disks of the system.\nUnlike the abstractions provided by the OS for the CPU and memory,\nthe OS does not create a private, virtualized disk for each application.\nRather, it is assumed that often times, users will want to share informa-\ntion that is in ﬁles. For example, when writing a C program, you might\nﬁrst use an editor (e.g., Emacs7) to create and edit the C ﬁle (emacs -nw\nmain.c). Once done, you might use the compiler to turn the source code\ninto an executable (e.g., gcc -o main main.c). When you’re ﬁnished,\nyou might run the new executable (e.g., ./main). Thus, you can see how\nﬁles are shared across different processes. First, Emacs creates a ﬁle that\nserves as input to the compiler; the compiler uses that input ﬁle to create\na new executable ﬁle (in many steps – take a compiler course for details);\nﬁnally, the new executable is then run. And thus a new program is born!\nTo understand this better, let’s look at some code. Figure 2.6 presents\ncode to create a ﬁle (/tmp/file) that contains the string “hello world”.\n7You should be using Emacs. If you are using vi, there is probably something wrong with\nyou. If you are using something that is not a real code editor, that is even worse.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n12\nINTRODUCTION TO OPERATING SYSTEMS\nTHE CRUX OF THE PROBLEM:\nHOW TO STORE DATA PERSISTENTLY\nThe ﬁle system is the part of the OS in charge of managing persistent data.\nWhat techniques are needed to do so correctly? What mechanisms and\npolicies are required to do so with high performance? How is reliability\nachieved, in the face of failures in hardware and software?\nTo accomplish this task, the program makes three calls into the oper-\nating system. The ﬁrst, a call to open(), opens the ﬁle and creates it; the\nsecond, write(), writes some data to the ﬁle; the third, close(), sim-\nply closes the ﬁle thus indicating the program won’t be writing any more\ndata to it. These system calls are routed to the part of the operating sys-\ntem called the ﬁle system, which then handles the requests and returns\nsome kind of error code to the user.\nYou might be wondering what the OS does in order to actually write\nto disk. We would show you but you’d have to promise to close your\neyes ﬁrst; it is that unpleasant. The ﬁle system has to do a fair bit of work:\nﬁrst ﬁguring out where on disk this new data will reside, and then keep-\ning track of it in various structures the ﬁle system maintains. Doing so\nrequires issuing I/O requests to the underlying storage device, to either\nread existing structures or update (write) them. As anyone who has writ-\nten a device driver8 knows, getting a device to do something on your\nbehalf is an intricate and detailed process. It requires a deep knowledge\nof the low-level device interface and its exact semantics. Fortunately, the\nOS provides a standard and simple way to access devices through its sys-\ntem calls. Thus, the OS is sometimes seen as a standard library.\nOf course, there are many more details in how devices are accessed,\nand how ﬁle systems manage data persistently atop said devices. For\nperformance reasons, most ﬁle systems ﬁrst delay such writes for a while,\nhoping to batch them into larger groups. To handle the problems of sys-\ntem crashes during writes, most ﬁle systems incorporate some kind of\nintricate write protocol, such as journaling or copy-on-write, carefully\nordering writes to disk to ensure that if a failure occurs during the write\nsequence, the system can recover to reasonable state afterwards. To make\ndifferent common operations efﬁcient, ﬁle systems employ many differ-\nent data structures and access methods, from simple lists to complex b-\ntrees. If all of this doesn’t make sense yet, good! We’ll be talking about\nall of this quite a bit more in the third part of this book on persistence,\nwhere we’ll discuss devices and I/O in general, and then disks, RAIDs,\nand ﬁle systems in great detail.\n8A device driver is some code in the operating system that knows how to deal with a\nspeciﬁc device. We will talk more about devices and device drivers later.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "page_number": 41,
      "chapter_number": 6,
      "summary": "Let’s say we save this ﬁle as cpu.c and decide to compile and run it\non a system with a single processor (or CPU as we will sometimes call it) Key topics include memory, program, and systems.",
      "keywords": [
        "OPERATING SYSTEMS",
        "program",
        "Memory",
        "SYSTEMS",
        "OPERATING",
        "run",
        "OPERATING SYSTEMS prompt",
        "ﬁle system",
        "running program",
        "run multiple programs",
        "INTRODUCTION TO OPERATING",
        "ﬁle",
        "include",
        "cpu",
        "running"
      ],
      "concepts": [
        "memory",
        "program",
        "systems",
        "value",
        "devices",
        "code",
        "run",
        "running",
        "runs",
        "write"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 2",
          "chapter": 26,
          "title": "Segment 26 (pages 249-257)",
          "relevance_score": 0.67,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 1,
          "title": "Segment 1 (pages 1-20)",
          "relevance_score": 0.66,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 2,
          "title": "Segment 2 (pages 21-38)",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 34,
          "title": "Segment 34 (pages 683-702)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 3,
          "title": "Segment 3 (pages 39-59)",
          "relevance_score": 0.59,
          "method": "api"
        }
      ]
    },
    {
      "number": 7,
      "title": "Segment 7 (pages 49-56)",
      "start_page": 49,
      "end_page": 56,
      "detection_method": "topic_boundary",
      "content": "INTRODUCTION TO OPERATING SYSTEMS\n13\n2.5\nDesign Goals\nSo now you have some idea of what an OS actually does: it takes phys-\nical resources, such as a CPU, memory, or disk, and virtualizes them. It\nhandles tough and tricky issues related to concurrency. And it stores ﬁles\npersistently, thus making them safe over the long-term. Given that we\nwant to build such a system, we want to have some goals in mind to help\nfocus our design and implementation and make trade-offs as necessary;\nﬁnding the right set of trade-offs is a key to building systems.\nOne of the most basic goals is to build up some abstractions in order\nto make the system convenient and easy to use. Abstractions are fun-\ndamental to everything we do in computer science. Abstraction makes\nit possible to write a large program by dividing it into small and under-\nstandable pieces, to write such a program in a high-level language like\nC9 without thinking about assembly, to write code in assembly without\nthinking about logic gates, and to build a processor out of gates without\nthinking too much about transistors. Abstraction is so fundamental that\nsometimes we forget its importance, but we won’t here; thus, in each sec-\ntion, we’ll discuss some of the major abstractions that have developed\nover time, giving you a way to think about pieces of the OS.\nOne goal in designing and implementing an operating system is to\nprovide high performance; another way to say this is our goal is to mini-\nmize the overheads of the OS. Virtualization and making the system easy\nto use are well worth it, but not at any cost; thus, we must strive to pro-\nvide virtualization and other OS features without excessive overheads.\nThese overheads arise in a number of forms: extra time (more instruc-\ntions) and extra space (in memory or on disk). We’ll seek solutions that\nminimize one or the other or both, if possible. Perfection, however, is not\nalways attainable, something we will learn to notice and (where appro-\npriate) tolerate.\nAnother goal will be to provide protection between applications, as\nwell as between the OS and applications.\nBecause we wish to allow\nmany programs to run at the same time, we want to make sure that the\nmalicious or accidental bad behavior of one does not harm others; we\ncertainly don’t want an application to be able to harm the OS itself (as\nthat would affect all programs running on the system). Protection is at\nthe heart of one of the main principles underlying an operating system,\nwhich is that of isolation; isolating processes from one another is the key\nto protection and thus underlies much of what an OS must do.\nThe operating system must also run non-stop; when it fails, all appli-\ncations running on the system fail as well. Because of this dependence,\noperating systems often strive to provide a high degree of reliability. As\noperating systems grow evermore complex (sometimes containing mil-\nlions of lines of code), building a reliable operating system is quite a chal-\n9Some of you might object to calling C a high-level language. Remember this is an OS\ncourse, though, where we’re simply happy not to have to code in assembly all the time!\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n14\nINTRODUCTION TO OPERATING SYSTEMS\nlenge – and indeed, much of the on-going research in the ﬁeld (including\nsome of our own work [BS+09, SS+10]) focuses on this exact problem.\nOther goals make sense: energy-efﬁciency is important in our increas-\ningly green world; security (an extension of protection, really) against\nmalicious applications is critical, especially in these highly-networked\ntimes; mobility is increasingly important as OSes are run on smaller and\nsmaller devices. Depending in how the system is used, the OS will have\ndifferent goals and thus likely be implemented in at least slightly differ-\nent ways. However, as we will see, many of the principles we will present\non how to build operating systems are useful in the range of different de-\nvices.\n2.6\nSome History\nBefore closing this introduction, let us present a brief history of how\noperating systems developed. Like any system built by humans, good\nideas accumulated in operating systems over time, as engineers learned\nwhat was important in their design. Here, we discuss a few major devel-\nopments. For a richer treatment, see Brinch Hansen’s excellent history of\noperating systems [BH00].\nEarly Operating Systems: Just Libraries\nIn the beginning, the operating system didn’t do too much. Basically,\nit was just a set of libraries of commonly-used functions; for example,\ninstead of having each programmer of the system write low-level I/O\nhandling code, the “OS” would provide such APIs, and thus make life\neasier for the developer.\nUsually, on these old mainframe systems, one program ran at a time,\nas controlled by a human operator. Much of what you think a modern\nOS would do (e.g., deciding what order to run jobs in) was performed by\nthis operator. If you were a smart developer, you would be nice to this\noperator, so that they might move your job to the front of the queue.\nThis mode of computing was known as batch processing, as a number\nof jobs were set up and then run in a “batch” by the operator. Computers,\nas of that point, were not used in an interactive manner, because of cost:\nit was simply too costly to let a user sit in front of the computer and use it,\nas most of the time it would just sit idle then, costing the facility hundreds\nof thousands of dollars per hour [BH00].\nBeyond Libraries: Protection\nIn moving beyond being a simple library of commonly-used services, op-\nerating systems took on a more central role in managing machines. One\nimportant aspect of this was the realization that code run on behalf of the\nOS was special; it had control of devices and thus should be treated dif-\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nINTRODUCTION TO OPERATING SYSTEMS\n15\nferently than normal application code. Why is this? Well, imagine if you\nallowed any application to read from anywhere on the disk; the notion of\nprivacy goes out the window, as any program could read any ﬁle. Thus,\nimplementing a ﬁle system (to manage your ﬁles) as a library makes little\nsense. Instead, something else was needed.\nThus, the idea of a system call was invented, pioneered by the Atlas\ncomputing system [K+61,L78]. Instead of providing OS routines as a li-\nbrary (where you just make a procedure call to access them), the idea here\nwas to add a special pair of hardware instructions and hardware state to\nmake the transition into the OS a more formal, controlled process.\nThe key difference between a system call and a procedure call is that\na system call transfers control (i.e., jumps) into the OS while simultane-\nously raising the hardware privilege level. User applications run in what\nis referred to as user mode which means the hardware restricts what ap-\nplications can do; for example, an application running in user mode can’t\ntypically initiate an I/O request to the disk, access any physical memory\npage, or send a packet on the network. When a system call is initiated\n(usually through a special hardware instruction called a trap), the hard-\nware transfers control to a pre-speciﬁed trap handler (that the OS set up\npreviously) and simultaneously raises the privilege level to kernel mode.\nIn kernel mode, the OS has full access to the hardware of the system and\nthus can do things like initiate an I/O request or make more memory\navailable to a program. When the OS is done servicing the request, it\npasses control back to the user via a special return-from-trap instruction,\nwhich reverts to user mode while simultaneously passing control back to\nwhere the application left off.\nThe Era of Multiprogramming\nWhere operating systems really took off was in the era of computing be-\nyond the mainframe, that of the minicomputer. Classic machines like\nthe PDP family from Digital Equipment made computers hugely more\naffordable; thus, instead of having one mainframe per large organization,\nnow a smaller collection of people within an organization could likely\nhave their own computer. Not surprisingly, one of the major impacts of\nthis drop in cost was an increase in developer activity; more smart people\ngot their hands on computers and thus made computer systems do more\ninteresting and beautiful things.\nIn particular, multiprogramming became commonplace due to the de-\nsire to make better use of machine resources. Instead of just running one\njob at a time, the OS would load a number of jobs into memory and switch\nrapidly between them, thus improving CPU utilization. This switching\nwas particularly important because I/O devices were slow; having a pro-\ngram wait on the CPU while its I/O was being serviced was a waste of\nCPU time. Instead, why not switch to another job and run it for a while?\nThe desire to support multiprogramming and overlap in the presence\nof I/O and interrupts forced innovation in the conceptual development of\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n16\nINTRODUCTION TO OPERATING SYSTEMS\noperating systems along a number of directions. Issues such as memory\nprotection became important; we wouldn’t want one program to be able\nto access the memory of another program. Understanding how to deal\nwith the concurrency issues introduced by multiprogramming was also\ncritical; making sure the OS was behaving correctly despite the presence\nof interrupts is a great challenge. We will study these issues and related\ntopics later in the book.\nOne of the major practical advances of the time was the introduction\nof the UNIX operating system, primarily thanks to Ken Thompson (and\nDennis Ritchie) at Bell Labs (yes, the phone company). UNIX took many\ngood ideas from different operating systems (particularly from Multics\n[O72], and some from systems like TENEX [B+72] and the Berkeley Time-\nSharing System [S+68]), but made them simpler and easier to use. Soon\nthis team was shipping tapes containing UNIX source code to people\naround the world, many of whom then got involved and added to the\nsystem themselves; see the Aside (next page) for more detail10.\nThe Modern Era\nBeyond the minicomputer came a new type of machine, cheaper, faster,\nand for the masses: the personal computer, or PC as we call it today. Led\nby Apple’s early machines (e.g., the Apple II) and the IBM PC, this new\nbreed of machine would soon become the dominant force in computing,\nas their low-cost enabled one machine per desktop instead of a shared\nminicomputer per workgroup.\nUnfortunately, for operating systems, the PC at ﬁrst represented a\ngreat leap backwards, as early systems forgot (or never knew of) the\nlessons learned in the era of minicomputers. For example, early operat-\ning systems such as DOS (the Disk Operating System, from Microsoft)\ndidn’t think memory protection was important; thus, a malicious (or per-\nhaps just a poorly-programmed) application could scribble all over mem-\nory. The ﬁrst generations of the Mac OS (v9 and earlier) took a coopera-\ntive approach to job scheduling; thus, a thread that accidentally got stuck\nin an inﬁnite loop could take over the entire system, forcing a reboot. The\npainful list of OS features missing in this generation of systems is long,\ntoo long for a full discussion here.\nFortunately, after some years of suffering, the old features of minicom-\nputer operating systems started to ﬁnd their way onto the desktop. For\nexample, Mac OS X has UNIX at its core, including all of the features\none would expect from such a mature system. Windows has similarly\nadopted many of the great ideas in computing history, starting in partic-\nular with Windows NT, a great leap forward in Microsoft OS technology.\nEven today’s cell phones run operating systems (such as Linux) that are\n10We’ll use asides and other related text boxes to call attention to various items that don’t\nquite ﬁt the main ﬂow of the text. Sometimes, we’ll even use them just to make a joke, because\nwhy not have a little fun along the way? Yes, many of the jokes are bad.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nINTRODUCTION TO OPERATING SYSTEMS\n17\nASIDE: THE IMPORTANCE OF UNIX\nIt is difﬁcult to overstate the importance of UNIX in the history of oper-\nating systems. Inﬂuenced by earlier systems (in particular, the famous\nMultics system from MIT), UNIX brought together many great ideas and\nmade a system that was both simple and powerful.\nUnderlying the original “Bell Labs” UNIX was the unifying principle of\nbuilding small powerful programs that could be connected together to\nform larger workﬂows. The shell, where you type commands, provided\nprimitives such as pipes to enable such meta-level programming, and\nthus it became easy to string together programs to accomplish a big-\nger task.\nFor example, to ﬁnd lines of a text ﬁle that have the word\n“foo” in them, and then to count how many such lines exist, you would\ntype: grep foo file.txt|wc -l, thus using the grep and wc (word\ncount) programs to achieve your task.\nThe UNIX environment was friendly for programmers and developers\nalike, also providing a compiler for the new C programming language.\nMaking it easy for programmers to write their own programs, as well as\nshare them, made UNIX enormously popular. And it probably helped a\nlot that the authors gave out copies for free to anyone who asked, an early\nform of open-source software.\nAlso of critical importance was the accessibility and readability of the\ncode. Having a beautiful, small kernel written in C invited others to play\nwith the kernel, adding new and cool features. For example, an enter-\nprising group at Berkeley, led by Bill Joy, made a wonderful distribution\n(the Berkeley Systems Distribution, or BSD) which had some advanced\nvirtual memory, ﬁle system, and networking subsystems. Joy later co-\nfounded Sun Microsystems.\nUnfortunately, the spread of UNIX was slowed a bit as companies tried to\nassert ownership and proﬁt from it, an unfortunate (but common) result\nof lawyers getting involved. Many companies had their own variants:\nSunOS from Sun Microsystems, AIX from IBM, HPUX (a.k.a. “H-Pucks”)\nfrom HP, and IRIX from SGI. The legal wrangling among AT&T/Bell\nLabs and these other players cast a dark cloud over UNIX, and many\nwondered if it would survive, especially as Windows was introduced and\ntook over much of the PC market...\nmuch more like what a minicomputer ran in the 1970s than what a PC\nran in the 1980s (thank goodness); it is good to see that the good ideas de-\nveloped in the heyday of OS development have found their way into the\nmodern world. Even better is that these ideas continue to develop, pro-\nviding more features and making modern systems even better for users\nand applications.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n18\nINTRODUCTION TO OPERATING SYSTEMS\nASIDE: AND THEN CAME LINUX\nFortunately for UNIX, a young Finnish hacker named Linus Torvalds de-\ncided to write his own version of UNIX which borrowed heavily on the\nprinciples and ideas behind the original system, but not from the code\nbase, thus avoiding issues of legality. He enlisted help from many oth-\ners around the world, and soon Linux was born (as well as the modern\nopen-source software movement).\nAs the internet era came into place, most companies (such as Google,\nAmazon, Facebook, and others) chose to run Linux, as it was free and\ncould be readily modiﬁed to suit their needs; indeed, it is hard to imag-\nine the success of these new companies had such a system not existed.\nAs smart phones became a dominant user-facing platform, Linux found\na stronghold there too (via Android), for many of the same reasons. And\nSteve Jobs took his UNIX-based NeXTStep operating environment with\nhim to Apple, thus making UNIX popular on desktops (though many\nusers of Apple technology are probably not even aware of this fact). And\nthus UNIX lives on, more important today than ever before. The comput-\ning gods, if you believe in them, should be thanked for this wonderful\noutcome.\n2.7\nSummary\nThus, we have an introduction to the OS. Today’s operating systems\nmake systems relatively easy to use, and virtually all operating systems\nyou use today have been inﬂuenced by the developments we will discuss\nthroughout the book.\nUnfortunately, due to time constraints, there are a number of parts of\nthe OS we won’t cover in the book. For example, there is a lot of net-\nworking code in the operating system; we leave it to you to take the net-\nworking class to learn more about that. Similarly, graphics devices are\nparticularly important; take the graphics course to expand your knowl-\nedge in that direction. Finally, some operating system books talk a great\ndeal about security; we will do so in the sense that the OS must provide\nprotection between running programs and give users the ability to pro-\ntect their ﬁles, but we won’t delve into deeper security issues that one\nmight ﬁnd in a security course.\nHowever, there are many important topics that we will cover, includ-\ning the basics of virtualization of the CPU and memory, concurrency, and\npersistence via devices and ﬁle systems. Don’t worry! While there is a\nlot of ground to cover, most of it is quite cool, and at the end of the road,\nyou’ll have a new appreciation for how computer systems really work.\nNow get to work!\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nINTRODUCTION TO OPERATING SYSTEMS\n19\nReferences\n[BS+09] “Tolerating File-System Mistakes with EnvyFS”\nLakshmi N. Bairavasundaram, Swaminathan Sundararaman, Andrea C. Arpaci-Dusseau, Remzi\nH. Arpaci-Dusseau\nUSENIX ’09, San Diego, CA, June 2009\nA fun paper about using multiple ﬁle systems at once to tolerate a mistake in any one of them.\n[BH00] “The Evolution of Operating Systems”\nP. Brinch Hansen\nIn Classic Operating Systems: From Batch Processing to Distributed Systems\nSpringer-Verlag, New York, 2000\nThis essay provides an intro to a wonderful collection of papers about historically signiﬁcant systems.\n[B+72] “TENEX, A Paged Time Sharing System for the PDP-10”\nDaniel G. Bobrow, Jerry D. Burchﬁel, Daniel L. Murphy, Raymond S. Tomlinson\nCACM, Volume 15, Number 3, March 1972\nTENEX has much of the machinery found in modern operating systems; read more about it to see how\nmuch innovation was already in place in the early 1970’s.\n[B75] “The Mythical Man-Month”\nFred Brooks\nAddison-Wesley, 1975\nA classic text on software engineering; well worth the read.\n[BOH10] “Computer Systems: A Programmer’s Perspective”\nRandal E. Bryant and David R. O’Hallaron\nAddison-Wesley, 2010\nAnother great intro to how computer systems work. Has a little bit of overlap with this book – so if you’d\nlike, you can skip the last few chapters of that book, or simply read them to get a different perspective\non some of the same material. After all, one good way to build up your own knowledge is to hear as\nmany other perspectives as possible, and then develop your own opinion and thoughts on the matter.\nYou know, by thinking!\n[K+61] “One-Level Storage System”\nT. Kilburn, D.B.G. Edwards, M.J. Lanigan, F.H. Sumner\nIRE Transactions on Electronic Computers, April 1962\nThe Atlas pioneered much of what you see in modern systems. However, this paper is not the best read.\nIf you were to only read one, you might try the historical perspective below [L78].\n[L78] “The Manchester Mark I and Atlas: A Historical Perspective”\nS. H. Lavington\nCommunications of the ACM archive\nVolume 21, Issue 1 (January 1978), pages 4-12\nA nice piece of history on the early development of computer systems and the pioneering efforts of the\nAtlas. Of course, one could go back and read the Atlas papers themselves, but this paper provides a great\noverview and adds some historical perspective.\n[O72] “The Multics System: An Examination of its Structure”\nElliott Organick, 1972\nA great overview of Multics. So many good ideas, and yet it was an over-designed system, shooting for\ntoo much, and thus never really worked as expected. A classic example of what Fred Brooks would call\nthe “second-system effect” [B75].\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n20\nINTRODUCTION TO OPERATING SYSTEMS\n[PP03] “Introduction to Computing Systems:\nFrom Bits and Gates to C and Beyond”\nYale N. Patt and Sanjay J. Patel\nMcGraw-Hill, 2003\nOne of our favorite intro to computing systems books. Starts at transistors and gets you all the way up\nto C; the early material is particularly great.\n[RT74] “The UNIX Time-Sharing System”\nDennis M. Ritchie and Ken Thompson\nCACM, Volume 17, Number 7, July 1974, pages 365-375\nA great summary of UNIX written as it was taking over the world of computing, by the people who\nwrote it.\n[S68] “SDS 940 Time-Sharing System”\nScientiﬁc Data Systems Inc.\nTECHNICAL MANUAL, SDS 90 11168 August 1968\nAvailable: http://goo.gl/EN0Zrn\nYes, a technical manual was the best we could ﬁnd. But it is fascinating to read these old system\ndocuments, and see how much was already in place in the late 1960’s. One of the minds behind the\nBerkeley Time-Sharing System (which eventually became the SDS system) was Butler Lampson, who\nlater won a Turing award for his contributions in systems.\n[SS+10] “Membrane: Operating System Support for Restartable File Systems”\nSwaminathan Sundararaman, Sriram Subramanian, Abhishek Rajimwale,\nAndrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau, Michael M. Swift\nFAST ’10, San Jose, CA, February 2010\nThe great thing about writing your own class notes: you can advertise your own research. But this\npaper is actually pretty neat – when a ﬁle system hits a bug and crashes, Membrane auto-magically\nrestarts it, all without applications or the rest of the system being affected.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "page_number": 49,
      "chapter_number": 7,
      "summary": "Perfection, however, is not\nalways attainable, something we will learn to notice and (where appro-\npriate) tolerate Key topics include systems, operating, and operator.",
      "keywords": [
        "OPERATING SYSTEMS",
        "SYSTEMS",
        "OPERATING",
        "UNIX operating system",
        "UNIX",
        "computer systems",
        "operating systems make",
        "run operating systems",
        "modern operating systems",
        "build operating systems",
        "Disk Operating System",
        "Early Operating Systems",
        "operating system books",
        "Classic Operating Systems",
        "Operating System Support"
      ],
      "concepts": [
        "systems",
        "operating",
        "operator",
        "unix",
        "making",
        "make",
        "time",
        "great",
        "program",
        "programming"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 1,
          "title": "Segment 1 (pages 1-20)",
          "relevance_score": 0.59,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 2,
          "title": "Segment 2 (pages 21-38)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 3,
          "title": "Segment 3 (pages 39-59)",
          "relevance_score": 0.53,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 55,
          "title": "Segment 55 (pages 1103-1105)",
          "relevance_score": 0.49,
          "method": "api"
        },
        {
          "book": "More Effective C++",
          "chapter": 1,
          "title": "Segment 1 (pages 1-8)",
          "relevance_score": 0.49,
          "method": "api"
        }
      ]
    },
    {
      "number": 8,
      "title": "Segment 8 (pages 57-67)",
      "start_page": 57,
      "end_page": 67,
      "detection_method": "topic_boundary",
      "content": "Part I\nVirtualization\n21\n\n\n3\nA Dialogue on Virtualization\nProfessor: And thus we reach the ﬁrst of our three pieces on operating systems:\nvirtualization.\nStudent: But what is virtualization, oh noble professor?\nProfessor: Imagine we have a peach.\nStudent: A peach? (incredulous)\nProfessor: Yes, a peach. Let us call that the physical peach. But we have many\neaters who would like to eat this peach. What we would like to present to each\neater is their own peach, so that they can be happy. We call the peach we give\neaters virtual peaches; we somehow create many of these virtual peaches out of\nthe one physical peach. And the important thing: in this illusion, it looks to each\neater like they have a physical peach, but in reality they don’t.\nStudent: So you are sharing the peach, but you don’t even know it?\nProfessor: Right! Exactly.\nStudent: But there’s only one peach.\nProfessor: Yes. And...?\nStudent: Well, if I was sharing a peach with somebody else, I think I would\nnotice.\nProfessor: Ah yes! Good point. But that is the thing with many eaters; most\nof the time they are napping or doing something else, and thus, you can snatch\nthat peach away and give it to someone else for a while. And thus we create the\nillusion of many virtual peaches, one peach for each person!\nStudent: Sounds like a bad campaign slogan. You are talking about computers,\nright Professor?\nProfessor: Ah, young grasshopper, you wish to have a more concrete example.\nGood idea! Let us take the most basic of resources, the CPU. Assume there is one\nphysical CPU in a system (though now there are often two or four or more). What\nvirtualization does is take that single CPU and make it look like many virtual\nCPUs to the applications running on the system. Thus, while each applications\n23\n\n\n24\nA DIALOGUE ON VIRTUALIZATION\nthinks it has its own CPU to use, there is really only one. And thus the OS has\ncreated a beautiful illusion: it has virtualized the CPU.\nStudent: Wow! That sounds like magic. Tell me more! How does that work?\nProfessor: In time, young student, in good time. Sounds like you are ready to\nbegin.\nStudent: I am! Well, sort of. I must admit, I’m a little worried you are going to\nstart talking about peaches again.\nProfessor: Don’t worry too much; I don’t even like peaches. And thus we be-\ngin...\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\n4\nThe Abstraction: The Process\nIn this note, we discuss one of the most fundamental abstractions that the\nOS provides to users: the process. The deﬁnition of a process, informally,\nis quite simple: it is a running program [V+65,B70]. The program itself is\na lifeless thing: it just sits there on the disk, a bunch of instructions (and\nmaybe some static data), waiting to spring into action. It is the operating\nsystem that takes these bytes and gets them running, transforming the\nprogram into something useful.\nIt turns out that one often wants to run more than one program at\nonce; for example, consider your desktop or laptop where you might like\nto run a web browser, mail program, a game, a music player, and so forth.\nIn fact, a typical system may be seemingly running tens or even hundreds\nof processes at the same time. Doing so makes the system easy to use, as\none never need be concerned with whether a CPU is available; one simply\nruns programs. Hence our challenge:\nTHE CRUX OF THE PROBLEM:\nHOW TO PROVIDE THE ILLUSION OF MANY CPUS?\nAlthough there are only a few physical CPUs available, how can the\nOS provide the illusion of a nearly-endless supply of said CPUs?\nThe OS creates this illusion by virtualizing the CPU. By running one\nprocess, then stopping it and running another, and so forth, the OS can\npromote the illusion that many virtual CPUs exist when in fact there is\nonly one physical CPU (or a few). This basic technique, known as time\nsharing of the CPU, allows users to run as many concurrent processes as\nthey would like; the potential cost is performance, as each will run more\nslowly if the CPU(s) must be shared.\nTo implement virtualization of the CPU, and to implement it well, the\nOS will need both some low-level machinery as well as some high-level\nintelligence. We call the low-level machinery mechanisms; mechanisms\nare low-level methods or protocols that implement a needed piece of\n25\n\n\n26\nTHE ABSTRACTION: THE PROCESS\nTIP: USE TIME SHARING (AND SPACE SHARING)\nTime sharing is one of the most basic techniques used by an OS to share\na resource. By allowing the resource to be used for a little while by one\nentity, and then a little while by another, and so forth, the resource in\nquestion (e.g., the CPU, or a network link) can be shared by many. The\nnatural counterpart of time sharing is space sharing, where a resource is\ndivided (in space) among those who wish to use it. For example, disk\nspace is naturally a space-shared resource, as once a block is assigned to\na ﬁle, it is not likely to be assigned to another ﬁle until the user deletes it.\nfunctionality. For example, we’ll learn below how to implement a con-\ntext switch, which gives the OS the ability to stop running one program\nand start running another on a given CPU; this time-sharing mechanism\nis employed by all modern OSes.\nOn top of these mechanisms resides some of the intelligence in the\nOS, in the form of policies. Policies are algorithms for making some\nkind of decision within the OS. For example, given a number of possi-\nble programs to run on a CPU, which program should the OS run? A\nscheduling policy in the OS will make this decision, likely using histori-\ncal information (e.g., which program has run more over the last minute?),\nworkload knowledge (e.g., what types of programs are run), and perfor-\nmance metrics (e.g., is the system optimizing for interactive performance,\nor throughput?) to make its decision.\n4.1\nThe Abstraction: A Process\nThe abstraction provided by the OS of a running program is something\nwe will call a process. As we said above, a process is simply a running\nprogram; at any instant in time, we can summarize a process by taking an\ninventory of the different pieces of the system it accesses or affects during\nthe course of its execution.\nTo understand what constitutes a process, we thus have to understand\nits machine state: what a program can read or update when it is running.\nAt any given time, what parts of the machine are important to the execu-\ntion of this program?\nOne obvious component of machine state that comprises a process is\nits memory. Instructions lie in memory; the data that the running pro-\ngram reads and writes sits in memory as well. Thus the memory that the\nprocess can address (called its address space) is part of the process.\nAlso part of the process’s machine state are registers; many instructions\nexplicitly read or update registers and thus clearly they are important to\nthe execution of the process.\nNote that there are some particularly special registers that form part\nof this machine state. For example, the program counter (PC) (sometimes\ncalled the instruction pointer or IP) tells us which instruction of the pro-\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nTHE ABSTRACTION: THE PROCESS\n27\nTIP: SEPARATE POLICY AND MECHANISM\nIn many operating systems, a common design paradigm is to separate\nhigh-level policies from their low-level mechanisms [L+75].\nYou can\nthink of the mechanism as providing the answer to a how question about\na system; for example, how does an operating system perform a context\nswitch? The policy provides the answer to a which question; for example,\nwhich process should the operating system run right now? Separating the\ntwo allows one easily to change policies without having to rethink the\nmechanism and is thus a form of modularity, a general software design\nprinciple.\ngram is currently being executed; similarly a stack pointer and associated\nframe pointer are used to manage the stack for function parameters, local\nvariables, and return addresses.\nFinally, programs often access persistent storage devices too. Such I/O\ninformation might include a list of the ﬁles the process currently has open.\n4.2\nProcess API\nThough we defer discussion of a real process API until a subsequent\nchapter, here we ﬁrst give some idea of what must be included in any\ninterface of an operating system. These APIs, in some form, are available\non any modern operating system.\n• Create: An operating system must include some method to cre-\nate new processes. When you type a command into the shell, or\ndouble-click on an application icon, the OS is invoked to create a\nnew process to run the program you have indicated.\n• Destroy: As there is an interface for process creation, systems also\nprovide an interface to destroy processes forcefully. Of course, many\nprocesses will run and just exit by themselves when complete; when\nthey don’t, however, the user may wish to kill them, and thus an in-\nterface to halt a runaway process is quite useful.\n• Wait: Sometimes it is useful to wait for a process to stop running;\nthus some kind of waiting interface is often provided.\n• Miscellaneous Control: Other than killing or waiting for a process,\nthere are sometimes other controls that are possible. For example,\nmost operating systems provide some kind of method to suspend a\nprocess (stop it from running for a while) and then resume it (con-\ntinue it running).\n• Status: There are usually interfaces to get some status information\nabout a process as well, such as how long it has run for, or what\nstate it is in.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n28\nTHE ABSTRACTION: THE PROCESS\nMemory\nCPU\nDisk\ncode\nstatic data\nheap\nstack\nProcess\ncode\nstatic data\nProgram\nLoading:\nTakes on-disk program\nand reads it into the\naddress space of process\nFigure 4.1: Loading: From Program To Process\n4.3\nProcess Creation: A Little More Detail\nOne mystery that we should unmask a bit is how programs are trans-\nformed into processes. Speciﬁcally, how does the OS get a program up\nand running? How does process creation actually work?\nThe ﬁrst thing that the OS must do to run a program is to load its code\nand any static data (e.g., initialized variables) into memory, into the ad-\ndress space of the process. Programs initially reside on disk (or, in some\nmodern systems, ﬂash-based SSDs) in some kind of executable format;\nthus, the process of loading a program and static data into memory re-\nquires the OS to read those bytes from disk and place them in memory\nsomewhere (as shown in Figure 4.1).\nIn early (or simple) operating systems, the loading process is done ea-\ngerly, i.e., all at once before running the program; modern OSes perform\nthe process lazily, i.e., by loading pieces of code or data only as they are\nneeded during program execution. To truly understand how lazy loading\nof pieces of code and data works, you’ll have to understand more about\nthe machinery of paging and swapping, topics we’ll cover in the future\nwhen we discuss the virtualization of memory. For now, just remember\nthat before running anything, the OS clearly must do some work to get\nthe important program bits from disk into memory.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nTHE ABSTRACTION: THE PROCESS\n29\nOnce the code and static data are loaded into memory, there are a few\nother things the OS needs to do before running the process. Some mem-\nory must be allocated for the program’s run-time stack (or just stack).\nAs you should likely already know, C programs use the stack for local\nvariables, function parameters, and return addresses; the OS allocates\nthis memory and gives it to the process. The OS will also likely initial-\nize the stack with arguments; speciﬁcally, it will ﬁll in the parameters to\nthe main() function, i.e., argc and the argv array.\nThe OS may also create some initial memory for the program’s heap.\nIn C programs, the heap is used for explicitly requested dynamically-\nallocated data; programs request such space by calling malloc() and\nfree it explicitly by calling free(). The heap is needed for data struc-\ntures such as linked lists, hash tables, trees, and other interesting data\nstructures. The heap will be small at ﬁrst; as the program runs, and re-\nquests more memory via the malloc() library API, the OS may get in-\nvolved and allocate more memory to the process to help satisfy such calls.\nThe OS will also do some other initialization tasks, particularly as re-\nlated to input/output (I/O). For example, in UNIX systems, each process\nby default has three open ﬁle descriptors, for standard input, output, and\nerror; these descriptors let programs easily read input from the terminal\nas well as print output to the screen. We’ll learn more about I/O, ﬁle\ndescriptors, and the like in the third part of the book on persistence.\nBy loading the code and static data into memory, by creating and ini-\ntializing a stack, and by doing other work as related to I/O setup, the OS\nhas now (ﬁnally) set the stage for program execution. It thus has one last\ntask: to start the program running at the entry point, namely main(). By\njumping to the main() routine (through a specialized mechanism that\nwe will discuss next chapter), the OS transfers control of the CPU to the\nnewly-created process, and thus the program begins its execution.\n4.4\nProcess States\nNow that we have some idea of what a process is (though we will\ncontinue to reﬁne this notion), and (roughly) how it is created, let us talk\nabout the different states a process can be in at a given time. The notion\nthat a process can be in one of these states arose in early computer systems\n[V+65,DV66]. In a simpliﬁed view, a process can be in one of three states:\n• Running: In the running state, a process is running on a processor.\nThis means it is executing instructions.\n• Ready: In the ready state, a process is ready to run but for some\nreason the OS has chosen not to run it at this given moment.\n• Blocked: In the blocked state, a process has performed some kind\nof operation that makes it not ready to run until some other event\ntakes place. A common example: when a process initiates an I/O\nrequest to a disk, it becomes blocked and thus some other process\ncan use the processor.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n30\nTHE ABSTRACTION: THE PROCESS\nRunning\nReady\nBlocked\nDescheduled\nScheduled\nI/O: initiate\nI/O: done\nFigure 4.2: Process: State Transitions\nIf we were to map these states to a graph, we would arrive at the di-\nagram in Figure 4.2. As you can see in the diagram, a process can be\nmoved between the ready and running states at the discretion of the OS.\nBeing moved from ready to running means the process has been sched-\nuled; being moved from running to ready means the process has been\ndescheduled. Once a process has become blocked (e.g., by initiating an\nI/O operation), the OS will keep it as such until some event occurs (e.g.,\nI/O completion); at that point, the process moves to the ready state again\n(and potentially immediately to running again, if the OS so decides).\n4.5\nData Structures\nThe OS is a program, and like any program, it has some key data struc-\ntures that track various relevant pieces of information. To track the state\nof each process, for example, the OS likely will keep some kind of process\nlist for all processes that are ready, as well as some additional informa-\ntion to track which process is currently running. The OS must also track,\nin some way, blocked processes; when an I/O event completes, the OS\nshould make sure to wake the correct process and ready it to run again.\nFigure 4.3 shows what type of information an OS needs to track about\neach process in the xv6 kernel [CK+08]. Similar process structures exist\nin “real” operating systems such as Linux, Mac OS X, or Windows; look\nthem up and see how much more complex they are.\nFrom the ﬁgure, you can see a couple of important pieces of informa-\ntion the OS tracks about a process. The register context will hold, for\na stopped process, the contents of its register state. When a process is\nstopped, its register state will be saved to this memory location; by restor-\ning these registers (i.e., placing their values back into the actual physical\nregisters), the OS can resume running the process. We’ll learn more about\nthis technique known as a context switch in future chapters.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nTHE ABSTRACTION: THE PROCESS\n31\n// the registers xv6 will save and restore\n// to stop and subsequently restart a process\nstruct context {\nint eip;\nint esp;\nint ebx;\nint ecx;\nint edx;\nint esi;\nint edi;\nint ebp;\n};\n// the different states a process can be in\nenum proc_state { UNUSED, EMBRYO, SLEEPING,\nRUNNABLE, RUNNING, ZOMBIE };\n// the information xv6 tracks about each process\n// including its register context and state\nstruct proc {\nchar *mem;\n// Start of process memory\nuint sz;\n// Size of process memory\nchar *kstack;\n// Bottom of kernel stack\n// for this process\nenum proc_state state;\n// Process state\nint pid;\n// Process ID\nstruct proc *parent;\n// Parent process\nvoid *chan;\n// If non-zero, sleeping on chan\nint killed;\n// If non-zero, have been killed\nstruct file *ofile[NOFILE]; // Open files\nstruct inode *cwd;\n// Current directory\nstruct context context;\n// Switch here to run process\nstruct trapframe *tf;\n// Trap frame for the\n// current interrupt\n};\nFigure 4.3: The xv6 Proc Structure\nYou can also see from the ﬁgure that there are some other states a pro-\ncess can be in, beyond running, ready, and blocked. Sometimes a system\nwill have an initial state that the process is in when it is being created.\nAlso, a process could be placed in a ﬁnal state where it has exited but\nhas not yet been cleaned up (in UNIX-based systems, this is called the\nzombie state1). This ﬁnal state can be useful as it allows other processes\n(usually the parent that created the process) to examine the return code\nof the process and see if it the just-ﬁnished process executed successfully\n(usually, programs return zero in UNIX-based systems when they have\naccomplished a task successfully, and non-zero otherwise). When ﬁn-\nished, the parent will make one ﬁnal call (e.g., wait()) to wait for the\ncompletion of the child, and to also indicate to the OS that it can clean up\nany relevant data structures that referred to the now-extinct process.\n1Yes, the zombie state. Just like real zombies, these zombies are relatively easy to kill.\nHowever, different techniques are usually recommended.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "page_number": 57,
      "chapter_number": 8,
      "summary": "This chapter covers segment 8 (pages 57-67). Key topics include process, processes, and program.",
      "keywords": [
        "Process",
        "program",
        "running",
        "CPU",
        "state",
        "systems",
        "operating system",
        "operating",
        "run",
        "memory",
        "peach",
        "data",
        "Professor",
        "ready",
        "Abstraction"
      ],
      "concepts": [
        "process",
        "processes",
        "program",
        "running",
        "run",
        "runs",
        "state",
        "data",
        "systems",
        "initialized"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 1,
          "title": "Segment 1 (pages 1-20)",
          "relevance_score": 0.77,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 2,
          "title": "Segment 2 (pages 9-17)",
          "relevance_score": 0.7,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 33,
          "title": "Segment 33 (pages 662-682)",
          "relevance_score": 0.69,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 27,
          "title": "Segment 27 (pages 537-558)",
          "relevance_score": 0.68,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 26,
          "title": "Segment 26 (pages 249-257)",
          "relevance_score": 0.68,
          "method": "api"
        }
      ]
    },
    {
      "number": 9,
      "title": "Segment 9 (pages 68-77)",
      "start_page": 68,
      "end_page": 77,
      "detection_method": "topic_boundary",
      "content": "32\nTHE ABSTRACTION: THE PROCESS\nASIDE: DATA STRUCTURE – THE PROCESS LIST\nOperating systems are replete with various important data structures\nthat we will discuss in these notes. The process list is the ﬁrst such struc-\nture. It is one of the simpler ones, but certainly any OS that has the ability\nto run multiple programs at once will have something akin to this struc-\nture in order to keep track of all the running programs in the system.\nSometimes people refer to the individual structure that stores informa-\ntion about a process as a Process Control Block (PCB), a fancy way of\ntalking about a C structure that contains information about each process.\n4.6\nSummary\nWe have introduced the most basic abstraction of the OS: the process.\nIt is quite simply viewed as a running program. With this conceptual\nview in mind, we will now move on to the nitty-gritty: the low-level\nmechanisms needed to implement processes, and the higher-level poli-\ncies required to schedule them in an intelligent way. By combining mech-\nanisms and policies, we will build up our understanding of how an oper-\nating system virtualizes the CPU.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nTHE ABSTRACTION: THE PROCESS\n33\nReferences\n[CK+08] “The xv6 Operating System”\nRuss Cox, Frans Kaashoek, Robert Morris, Nickolai Zeldovich\nFrom: http://pdos.csail.mit.edu/6.828/2008/index.html\nThe coolest real and little OS in the world. Download and play with it to learn more about the details of\nhow operating systems actually work.\n[DV66] “Programming Semantics for Multiprogrammed Computations”\nJack B. Dennis and Earl C. Van Horn\nCommunications of the ACM, Volume 9, Number 3, March 1966\nThis paper deﬁned many of the early terms and concepts around building multiprogrammed systems.\n[H70] “The Nucleus of a Multiprogramming System”\nPer Brinch Hansen\nCommunications of the ACM, Volume 13, Number 4, April 1970\nThis paper introduces one of the ﬁrst microkernels in operating systems history, called Nucleus. The\nidea of smaller, more minimal systems is a theme that rears its head repeatedly in OS history; it all began\nwith Brinch Hansen’s work described herein.\n[L+75] “Policy/mechanism separation in Hydra”\nR. Levin, E. Cohen, W. Corwin, F. Pollack, W. Wulf\nSOSP 1975\nAn early paper about how to structure operating systems in a research OS known as Hydra. While\nHydra never became a mainstream OS, some of its ideas inﬂuenced OS designers.\n[V+65] “Structure of the Multics Supervisor”\nV.A. Vyssotsky, F. J. Corbato, R. M. Graham\nFall Joint Computer Conference, 1965\nAn early paper on Multics, which described many of the basic ideas and terms that we ﬁnd in modern\nsystems. Some of the vision behind computing as a utility are ﬁnally being realized in modern cloud\nsystems.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n5\nInterlude: Process API\nASIDE: INTERLUDES\nInterludes will cover more practical aspects of systems, including a par-\nticular focus on operating system APIs and how to use them. If you don’t\nlike practical things, you could skip these interludes. But you should like\npractical things, because, well, they are generally useful in real life; com-\npanies, for example, don’t usually hire you for your non-practical skills.\nIn this interlude, we discuss process creation in UNIX systems. UNIX\npresents one of the most intriguing ways to create a new process with\na pair of system calls: fork() and exec(). A third routine, wait(),\ncan be used by a process wishing to wait for a process it has created to\ncomplete. We now present these interfaces in more detail, with a few\nsimple examples to motivate us. And thus, our problem:\nCRUX: HOW TO CREATE AND CONTROL PROCESSES\nWhat interfaces should the OS present for process creation and con-\ntrol? How should these interfaces be designed to enable ease of use as\nwell as utility?\n5.1\nThe fork() System Call\nThe fork() system call is used to create a new process [C63]. How-\never, be forewarned: it is certainly the strangest routine you will ever\ncall1. More speciﬁcally, you have a running program whose code looks\nlike what you see in Figure 5.1; examine the code, or better yet, type it in\nand run it yourself!\n1Well, OK, we admit that we don’t know that for sure; who knows what routines you\ncall when no one is looking? But fork() is pretty odd, no matter how unusual your routine-\ncalling patterns are.\n35\n\n\n36\nINTERLUDE: PROCESS API\n1\n#include <stdio.h>\n2\n#include <stdlib.h>\n3\n#include <unistd.h>\n4\n5\nint\n6\nmain(int argc, char *argv[])\n7\n{\n8\nprintf(\"hello world (pid:%d)\\n\", (int) getpid());\n9\nint rc = fork();\n10\nif (rc < 0) {\n// fork failed; exit\n11\nfprintf(stderr, \"fork failed\\n\");\n12\nexit(1);\n13\n} else if (rc == 0) { // child (new process)\n14\nprintf(\"hello, I am child (pid:%d)\\n\", (int) getpid());\n15\n} else {\n// parent goes down this path (main)\n16\nprintf(\"hello, I am parent of %d (pid:%d)\\n\",\n17\nrc, (int) getpid());\n18\n}\n19\nreturn 0;\n20\n}\nFigure 5.1: p1.c: Calling fork()\nWhen you run this program (called p1.c), you’ll see the following:\nprompt> ./p1\nhello world (pid:29146)\nhello, I am parent of 29147 (pid:29146)\nhello, I am child (pid:29147)\nprompt>\nLet us understand what happened in more detail in p1.c. When it\nﬁrst started running, the process prints out a hello world message; in-\ncluded in that message is its process identiﬁer, also known as a PID. The\nprocess has a PID of 29146; in UNIX systems, the PID is used to name\nthe process if one wants to do something with the process, such as (for\nexample) stop it from running. So far, so good.\nNow the interesting part begins. The process calls the fork() system\ncall, which the OS provides as a way to create a new process. The odd\npart: the process that is created is an (almost) exact copy of the calling pro-\ncess. That means that to the OS, it now looks like there are two copies of\nthe program p1 running, and both are about to return from the fork()\nsystem call. The newly-created process (called the child, in contrast to the\ncreating parent) doesn’t start running at main(), like you might expect\n(note, the “hello, world” message only got printed out once); rather, it\njust comes into life as if it had called fork() itself.\nYou might have noticed: the child isn’t an exact copy. Speciﬁcally, al-\nthough it now has its own copy of the address space (i.e., its own private\nmemory), its own registers, its own PC, and so forth, the value it returns\nto the caller of fork() is different. Speciﬁcally, while the parent receives\nthe PID of the newly-created child, the child is simply returned a 0. This\ndifferentiation is useful, because it is simple then to write the code that\nhandles the two different cases (as above).\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nINTERLUDE: PROCESS API\n37\n1\n#include <stdio.h>\n2\n#include <stdlib.h>\n3\n#include <unistd.h>\n4\n#include <sys/wait.h>\n5\n6\nint\n7\nmain(int argc, char *argv[])\n8\n{\n9\nprintf(\"hello world (pid:%d)\\n\", (int) getpid());\n10\nint rc = fork();\n11\nif (rc < 0) {\n// fork failed; exit\n12\nfprintf(stderr, \"fork failed\\n\");\n13\nexit(1);\n14\n} else if (rc == 0) { // child (new process)\n15\nprintf(\"hello, I am child (pid:%d)\\n\", (int) getpid());\n16\n} else {\n// parent goes down this path (main)\n17\nint wc = wait(NULL);\n18\nprintf(\"hello, I am parent of %d (wc:%d) (pid:%d)\\n\",\n19\nrc, wc, (int) getpid());\n20\n}\n21\nreturn 0;\n22\n}\nFigure 5.2: p2.c: Calling fork() And wait()\nYou might also have noticed: the output is not deterministic. When\nthe child process is created, there are now two active processes in the sys-\ntem that we care about: the parent and the child. Assuming we are run-\nning on a system with a single CPU (for simplicity), then either the child\nor the parent might run at that point. In our example (above), the parent\ndid and thus printed out its message ﬁrst. In other cases, the opposite\nmight happen, as we show in this output trace:\nprompt> ./p1\nhello world (pid:29146)\nhello, I am child (pid:29147)\nhello, I am parent of 29147 (pid:29146)\nprompt>\nThe CPU scheduler, a topic we’ll discuss in great detail soon, deter-\nmines which process runs at a given moment in time; because the sched-\nuler is complex, we cannot usually make strong assumptions about what\nit will choose to do, and hence which process will run ﬁrst. This non-\ndeterminism, as it turns out, leads to some interesting problems, par-\nticularly in multi-threaded programs; hence, we’ll see a lot more non-\ndeterminism when we study concurrency in the second part of the book.\n5.2\nAdding wait() System Call\nSo far, we haven’t done much: just created a child that prints out a\nmessage and exits. Sometimes, as it turns out, it is quite useful for a\nparent to wait for a child process to ﬁnish what it has been doing. This\ntask is accomplished with the wait() system call (or its more complete\nsibling waitpid()); see Figure 5.2 for details.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n38\nINTERLUDE: PROCESS API\nIn this example (p2.c), the parent process calls wait() to delay its\nexecution until the child ﬁnishes executing.\nWhen the child is done,\nwait() returns to the parent.\nAdding a wait() call to the code above makes the output determin-\nistic. Can you see why? Go ahead, think about it.\n(waiting for you to think .... and done)\nNow that you have thought a bit, here is the output:\nprompt> ./p2\nhello world (pid:29266)\nhello, I am child (pid:29267)\nhello, I am parent of 29267 (wc:29267) (pid:29266)\nprompt>\nWith this code, we now know that the child will always print ﬁrst.\nWhy do we know that? Well, it might simply run ﬁrst, as before, and\nthus print before the parent. However, if the parent does happen to run\nﬁrst, it will immediately call wait(); this system call won’t return until\nthe child has run and exited2. Thus, even when the parent runs ﬁrst, it\npolitely waits for the child to ﬁnish running, then wait() returns, and\nthen the parent prints its message.\n5.3\nFinally, the exec() System Call\nA ﬁnal and important piece of the process creation API is the exec()\nsystem call3. This system call is useful when you want to run a program\nthat is different from the calling program. For example, calling fork()\nin p2.c is only useful if you want to keep running copies of the same\nprogram. However, often you want to run a different program; exec()\ndoes just that (Figure 5.3).\nIn this example, the child process calls execvp() in order to run the\nprogram wc, which is the word counting program. In fact, it runs wc on\nthe source ﬁle p3.c, thus telling us how many lines, words, and bytes are\nfound in the ﬁle:\nprompt> ./p3\nhello world (pid:29383)\nhello, I am child (pid:29384)\n29\n107\n1030 p3.c\nhello, I am parent of 29384 (wc:29384) (pid:29383)\nprompt>\n2There are a few cases where wait() returns before the child exits; read the man page\nfor more details, as always. And beware of any absolute and unqualiﬁed statements this book\nmakes, such as “the child will always print ﬁrst” or “UNIX is the best thing in the world, even\nbetter than ice cream.”\n3Actually, there are six variants of exec(): execl(), execle(), execlp(), execv(),\nand execvp(). Read the man pages to learn more.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nINTERLUDE: PROCESS API\n39\n1\n#include <stdio.h>\n2\n#include <stdlib.h>\n3\n#include <unistd.h>\n4\n#include <string.h>\n5\n#include <sys/wait.h>\n6\n7\nint\n8\nmain(int argc, char *argv[])\n9\n{\n10\nprintf(\"hello world (pid:%d)\\n\", (int) getpid());\n11\nint rc = fork();\n12\nif (rc < 0) {\n// fork failed; exit\n13\nfprintf(stderr, \"fork failed\\n\");\n14\nexit(1);\n15\n} else if (rc == 0) { // child (new process)\n16\nprintf(\"hello, I am child (pid:%d)\\n\", (int) getpid());\n17\nchar *myargs[3];\n18\nmyargs[0] = strdup(\"wc\");\n// program: \"wc\" (word count)\n19\nmyargs[1] = strdup(\"p3.c\"); // argument: file to count\n20\nmyargs[2] = NULL;\n// marks end of array\n21\nexecvp(myargs[0], myargs);\n// runs word count\n22\nprintf(\"this shouldn’t print out\");\n23\n} else {\n// parent goes down this path (main)\n24\nint wc = wait(NULL);\n25\nprintf(\"hello, I am parent of %d (wc:%d) (pid:%d)\\n\",\n26\nrc, wc, (int) getpid());\n27\n}\n28\nreturn 0;\n29\n}\nFigure 5.3: p3.c: Calling fork(), wait(), And exec()\nIf fork() was strange, exec() is not so normal either. What it does:\ngiven the name of an executable (e.g., wc), and some arguments (e.g.,\np3.c), it loads code (and static data) from that executable and over-\nwrites its current code segment (and current static data) with it; the heap\nand stack and other parts of the memory space of the program are re-\ninitialized. Then the OS simply runs that program, passing in any argu-\nments as the argv of that process. Thus, it does not create a new process;\nrather, it transforms the currently running program (formerly p3) into a\ndifferent running program (wc). After the exec() in the child, it is al-\nmost as if p3.c never ran; a successful call to exec() never returns.\n5.4\nWhy? Motivating the API\nOf course, one big question you might have: why would we build\nsuch an odd interface to what should be the simple act of creating a new\nprocess? Well, as it turns out, the separation of fork() and exec() is\nessential in building a UNIX shell, because it lets the shell run code after\nthe call to fork() but before the call to exec(); this code can alter the\nenvironment of the about-to-be-run program, and thus enables a variety\nof interesting features to be readily built.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n40\nINTERLUDE: PROCESS API\nTIP: GETTING IT RIGHT (LAMPSON’S LAW)\nAs Lampson states in his well-regarded “Hints for Computer Systems\nDesign” [L83], “Get it right. Neither abstraction nor simplicity is a substi-\ntute for getting it right.” Sometimes, you just have to do the right thing,\nand when you do, it is way better than the alternatives. There are lots\nof ways to design APIs for process creation; however, the combination\nof fork() and exec() are simple and immensely powerful. Here, the\nUNIX designers simply got it right. And because Lampson so often “got\nit right”, we name the law in his honor.\nThe shell is just a user program4. It shows you a prompt and then\nwaits for you to type something into it. You then type a command (i.e.,\nthe name of an executable program, plus any arguments) into it; in most\ncases, the shell then ﬁgures out where in the ﬁle system the executable\nresides, calls fork() to create a new child process to run the command,\ncalls some variant of exec() to run the command, and then waits for the\ncommand to complete by calling wait(). When the child completes, the\nshell returns from wait() and prints out a prompt again, ready for your\nnext command.\nThe separation of fork() and exec() allows the shell to do a whole\nbunch of useful things rather easily. For example:\nprompt> wc p3.c > newfile.txt\nIn the example above, the output of the program wc is redirected into\nthe output ﬁle newfile.txt (the greater-than sign is how said redirec-\ntion is indicated). The way the shell accomplishes this task is quite sim-\nple: when the child is created, before calling exec(), the shell closes\nstandard output and opens the ﬁle newfile.txt. By doing so, any out-\nput from the soon-to-be-running program wc are sent to the ﬁle instead\nof the screen.\nFigure 5.4 shows a program that does exactly this. The reason this redi-\nrection works is due to an assumption about how the operating system\nmanages ﬁle descriptors. Speciﬁcally, UNIX systems start looking for free\nﬁle descriptors at zero. In this case, STDOUT FILENO will be the ﬁrst\navailable one and thus get assigned when open() is called. Subsequent\nwrites by the child process to the standard output ﬁle descriptor, for ex-\nample by routines such as printf(), will then be routed transparently\nto the newly-opened ﬁle instead of the screen.\nHere is the output of running the p4.c program:\nprompt> ./p4\nprompt> cat p4.output\n32\n109\n846 p4.c\nprompt>\n4And there are lots of shells; tcsh, bash, and zsh to name a few. You should pick one,\nread its man pages, and learn more about it; all UNIX experts do.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nINTERLUDE: PROCESS API\n41\n1\n#include <stdio.h>\n2\n#include <stdlib.h>\n3\n#include <unistd.h>\n4\n#include <string.h>\n5\n#include <fcntl.h>\n6\n#include <sys/wait.h>\n7\n8\nint\n9\nmain(int argc, char *argv[])\n10\n{\n11\nint rc = fork();\n12\nif (rc < 0) {\n// fork failed; exit\n13\nfprintf(stderr, \"fork failed\\n\");\n14\nexit(1);\n15\n} else if (rc == 0) { // child: redirect standard output to a file\n16\nclose(STDOUT_FILENO);\n17\nopen(\"./p4.output\", O_CREAT|O_WRONLY|O_TRUNC, S_IRWXU);\n18\n19\n// now exec \"wc\"...\n20\nchar *myargs[3];\n21\nmyargs[0] = strdup(\"wc\");\n// program: \"wc\" (word count)\n22\nmyargs[1] = strdup(\"p4.c\"); // argument: file to count\n23\nmyargs[2] = NULL;\n// marks end of array\n24\nexecvp(myargs[0], myargs);\n// runs word count\n25\n} else {\n// parent goes down this path (main)\n26\nint wc = wait(NULL);\n27\n}\n28\nreturn 0;\n29\n}\nFigure 5.4: p4.c: All Of The Above With Redirection\nYou’ll notice (at least) two interesting tidbits about this output. First,\nwhen p4 is run, it looks as if nothing has happened; the shell just prints\nthe command prompt and is immediately ready for your next command.\nHowever, that is not the case; the program p4 did indeed call fork() to\ncreate a new child, and then run the wc program via a call to execvp().\nYou don’t see any output printed to the screen because it has been redi-\nrected to the ﬁle p4.output. Second, you can see that when we cat the\noutput ﬁle, all the expected output from running wc is found. Cool, right?\nUNIX pipes are implemented in a similar way, but with the pipe()\nsystem call. In this case, the output of one process is connected to an in-\nkernel pipe (i.e., queue), and the input of another process is connected\nto that same pipe; thus, the output of one process seamlessly is used as\ninput to the next, and long and useful chains of commands can be strung\ntogether. As a simple example, consider the looking for a word in a ﬁle,\nand then counting how many times said word occurs; with pipes and the\nutilities grep and wc, it is easy – just type grep foo file | wc -l\ninto the command prompt and marvel at the result.\nFinally, while we just have sketched out the process API at a high level,\nthere is a lot more detail about these calls out there to be learned and\ndigested; we’ll learn more, for example, about ﬁle descriptors when we\ntalk about ﬁle systems in the third part of the book. For now, sufﬁce it\nto say that the fork()/exec() combination is a powerful way to create\nand manipulate processes.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "page_number": 68,
      "chapter_number": 9,
      "summary": "32\nTHE ABSTRACTION: THE PROCESS\nASIDE: DATA STRUCTURE – THE PROCESS LIST\nOperating systems are replete with various important data structures\nthat we will discuss in these notes Key topics include process, processes, and called.",
      "keywords": [
        "PROCESS",
        "Process API",
        "System Call",
        "child",
        "fork",
        "system",
        "pid",
        "child process",
        "int",
        "program",
        "parent",
        "Call",
        "include",
        "wait",
        "Operating systems"
      ],
      "concepts": [
        "process",
        "processes",
        "called",
        "programs",
        "programming",
        "run",
        "running",
        "runs",
        "prompt",
        "way"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 38,
          "title": "Segment 38 (pages 759-776)",
          "relevance_score": 0.72,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 1,
          "title": "Segment 1 (pages 1-20)",
          "relevance_score": 0.6,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 3,
          "title": "Segment 3 (pages 39-59)",
          "relevance_score": 0.58,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 2,
          "title": "Segment 2 (pages 9-17)",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 26,
          "title": "Segment 26 (pages 249-257)",
          "relevance_score": 0.57,
          "method": "api"
        }
      ]
    },
    {
      "number": 10,
      "title": "Segment 10 (pages 78-89)",
      "start_page": 78,
      "end_page": 89,
      "detection_method": "topic_boundary",
      "content": "42\nINTERLUDE: PROCESS API\nASIDE: RTFM – READ THE MAN PAGES\nMany times in this book, when referring to a particular system call or\nlibrary call, we’ll tell you to read the manual pages, or man pages for\nshort. Man pages are the original form of documentation that exist on\nUNIX systems; realize that they were created before the thing called the\nweb existed.\nSpending some time reading man pages is a key step in the growth of\na systems programmer; there are tons of useful tidbits hidden in those\npages.\nSome particularly useful pages to read are the man pages for\nwhichever shell you are using (e.g., tcsh, or bash), and certainly for any\nsystem calls your program makes (in order to see what return values and\nerror conditions exist).\nFinally, reading the man pages can save you some embarrassment. When\nyou ask colleagues about some intricacy of fork(), they may simply\nreply: “RTFM.” This is your colleagues’ way of gently urging you to Read\nThe Man pages. The F in RTFM just adds a little color to the phrase...\n5.5\nOther Parts of the API\nBeyond fork(), exec(), and wait(), there are a lot of other inter-\nfaces for interacting with processes in UNIX systems. For example, the\nkill() system call is used to send signals to a process, including direc-\ntives to go to sleep, die, and other useful imperatives. In fact, the entire\nsignals subsystem provides a rich infrastructure to deliver external events\nto processes, including ways to receive and process those signals.\nThere are many command-line tools that are useful as well. For exam-\nple, using the ps command allows you to see which processes are run-\nning; read the man pages for some useful ﬂags to pass to ps. The tool\ntop is also quite helpful, as it displays the processes of the system and\nhow much CPU and other resources they are eating up. Humorously,\nmany times when you run it, top claims it is the top resource hog; per-\nhaps it is a bit of an egomaniac. Finally, there are many different kinds of\nCPU meters you can use to get a quick glance understanding of the load\non your system; for example, we always keep MenuMeters (from Raging\nMenace software) running on our Macintosh toolbars, so we can see how\nmuch CPU is being utilized at any moment in time. In general, the more\ninformation about what is going on, the better.\n5.6\nSummary\nWe have introduced some of the APIs dealing with UNIX process cre-\nation: fork(), exec(), and wait(). However, we have just skimmed\nthe surface. For more detail, read Stevens and Rago [SR05], of course,\nparticularly the chapters on Process Control, Process Relationships, and\nSignals. There is much to extract from the wisdom therein.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nINTERLUDE: PROCESS API\n43\nReferences\n[C63] “A Multiprocessor System Design”\nMelvin E. Conway\nAFIPS ’63 Fall Joint Computer Conference\nNew York, USA 1963\nAn early paper on how to design multiprocessing systems; may be the ﬁrst place the term fork() was\nused in the discussion of spawning new processes.\n[DV66] “Programming Semantics for Multiprogrammed Computations”\nJack B. Dennis and Earl C. Van Horn\nCommunications of the ACM, Volume 9, Number 3, March 1966\nA classic paper that outlines the basics of multiprogrammed computer systems. Undoubtedly had great\ninﬂuence on Project MAC, Multics, and eventually UNIX.\n[L83] “Hints for Computer Systems Design”\nButler Lampson\nACM Operating Systems Review, 15:5, October 1983\nLampson’s famous hints on how to design computer systems. You should read it at some point in your\nlife, and probably at many points in your life.\n[SR05] “Advanced Programming in the UNIX Environment”\nW. Richard Stevens and Stephen A. Rago\nAddison-Wesley, 2005\nAll nuances and subtleties of using UNIX APIs are found herein. Buy this book! Read it! And most\nimportantly, live it.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n6\nMechanism: Limited Direct Execution\nIn order to virtualize the CPU, the operating system needs to somehow\nshare the physical CPU among many jobs running seemingly at the same\ntime. The basic idea is simple: run one process for a little while, then\nrun another one, and so forth. By time sharing the CPU in this manner,\nvirtualization is achieved.\nThere are a few challenges, however, in building such virtualization\nmachinery. The ﬁrst is performance: how can we implement virtualiza-\ntion without adding excessive overhead to the system? The second is\ncontrol: how can we run processes efﬁciently while retaining control over\nthe CPU? Control is particularly important to the OS, as it is in charge of\nresources; without control, a process could simply run forever and take\nover the machine, or access information that it should not be allowed to\naccess. Attaining performance while maintaining control is thus one of\nthe central challenges in building an operating system.\nTHE CRUX:\nHOW TO EFFICIENTLY VIRTUALIZE THE CPU WITH CONTROL\nThe OS must virtualize the CPU in an efﬁcient manner, but while re-\ntaining control over the system. To do so, both hardware and operating\nsystems support will be required. The OS will often use a judicious bit of\nhardware support in order to accomplish its work effectively.\n6.1\nBasic Technique: Limited Direct Execution\nTo make a program run as fast as one might expect, not surprisingly\nOS developers came up with a technique, which we call limited direct\nexecution. The “direct execution” part of the idea is simple: just run the\nprogram directly on the CPU. Thus, when the OS wishes to start a pro-\ngram running, it creates a process entry for it in a process list, allocates\nsome memory pages for it, loads the program code into memory (from\n45\n\n\n46\nMECHANISM: LIMITED DIRECT EXECUTION\nOS\nProgram\nCreate entry for process list\nAllocate memory for program\nLoad program into memory\nSet up stack with argc/argv\nClear registers\nExecute call main()\nRun main()\nExecute return from main\nFree memory of process\nRemove from process list\nTable 6.1: Direction Execution Protocol (Without Limits)\ndisk), locates its entry point (i.e., the main() routine or something simi-\nlar), jumps to it, and starts running the user’s code. Table 6.1 shows this\nbasic direct execution protocol (without any limits, yet), using a normal\ncall and return to jump to the program’s main() and later to get back\ninto the kernel.\nSounds simple, no? But this approach gives rise to a few problems\nin our quest to virtualize the CPU. The ﬁrst is simple: if we just run a\nprogram, how can the OS make sure the program doesn’t do anything\nthat we don’t want it to do, while still running it efﬁciently? The second:\nwhen we are running a process, how does the operating system stop it\nfrom running and switch to another process, thus implementing the time\nsharing we require to virtualize the CPU?\nIn answering these questions below, we’ll get a much better sense of\nwhat is needed to virtualize the CPU. In developing these techniques,\nwe’ll also see where the “limited” part of the name arises from; without\nlimits on running programs, the OS wouldn’t be in control of anything\nand thus would be “just a library” – a very sad state of affairs for an\naspiring operating system!\n6.2\nProblem #1: Restricted Operations\nDirect execution has the obvious advantage of being fast; the program\nruns natively on the hardware CPU and thus executes as quickly as one\nwould expect. But running on the CPU introduces a problem: what if\nthe process wishes to perform some kind of restricted operation, such\nas issuing an I/O request to a disk, or gaining access to more system\nresources such as CPU or memory?\nTHE CRUX: HOW TO PERFORM RESTRICTED OPERATIONS\nA process must be able to perform I/O and some other restricted oper-\nations, but without giving the process complete control over the system.\nHow can the OS and hardware work together to do so?\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nMECHANISM: LIMITED DIRECT EXECUTION\n47\nTIP: USE PROTECTED CONTROL TRANSFER\nThe hardware assists the OS by providing different modes of execution.\nIn user mode, applications do not have full access to hardware resources.\nIn kernel mode, the OS has access to the full resources of the machine.\nSpecial instructions to trap into the kernel and return-from-trap back to\nuser-mode programs are also provided, as well instructions that allow the\nOS to tell the hardware where the trap table resides in memory.\nOne approach would simply be to let any process do whatever it wants\nin terms of I/O and other related operations. However, doing so would\nprevent the construction of many kinds of systems that are desirable. For\nexample, if we wish to build a ﬁle system that checks permissions before\ngranting access to a ﬁle, we can’t simply let any user process issue I/Os\nto the disk; if we did, a process could simply read or write the entire disk\nand thus all protections would be lost.\nThus, the approach we take is to introduce a new processor mode,\nknown as user mode; code that runs in user mode is restricted in what it\ncan do. For example, when running in user mode, a process can’t issue\nI/O requests; doing so would result in the processor raising an exception;\nthe OS would then likely kill the process.\nIn contrast to user mode is kernel mode, which the operating system\n(or kernel) runs in. In this mode, code that runs can do what it likes, in-\ncluding privileged operations such as issuing I/O requests and executing\nall types of restricted instructions.\nWe are still left with a challenge, however: what should a user pro-\ncess do when it wishes to perform some kind of privileged operation,\nsuch as reading from disk? To enable this, virtually all modern hard-\nware provides the ability for user programs to perform a system call.\nPioneered on ancient machines such as the Atlas [K+61,L78], system calls\nallow the kernel to carefully expose certain key pieces of functionality to\nuser programs, such as accessing the ﬁle system, creating and destroy-\ning processes, communicating with other processes, and allocating more\nmemory. Most operating systems provide a few hundred calls (see the\nPOSIX standard for details [P10]); early Unix systems exposed a more\nconcise subset of around twenty calls.\nTo execute a system call, a program must execute a special trap instruc-\ntion. This instruction simultaneously jumps into the kernel and raises the\nprivilege level to kernel mode; once in the kernel, the system can now per-\nform whatever privileged operations are needed (if allowed), and thus do\nthe required work for the calling process. When ﬁnished, the OS calls a\nspecial return-from-trap instruction, which, as you might expect, returns\ninto the calling user program while simultaneously reducing the privi-\nlege level back to user mode.\nThe hardware needs to be a bit careful when executing a trap, in that\nit must make sure to save enough of the caller’s register state in order\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n48\nMECHANISM: LIMITED DIRECT EXECUTION\nASIDE: WHY SYSTEM CALLS LOOK LIKE PROCEDURE CALLS\nYou may wonder why a call to a system call, such as open() or read(),\nlooks exactly like a typical procedure call in C; that is, if it looks just like\na procedure call, how does the system know it’s a system call, and do all\nthe right stuff? The simple reason: it is a procedure call, but hidden in-\nside that procedure call is the famous trap instruction. More speciﬁcally,\nwhen you call open() (for example), you are executing a procedure call\ninto the C library. Therein, whether for open() or any of the other sys-\ntem calls provided, the library uses an agreed-upon calling convention\nwith the kernel to put the arguments to open in well-known locations\n(e.g., on the stack, or in speciﬁc registers), puts the system-call number\ninto a well-known location as well (again, onto the stack or a register),\nand then executes the aforementioned trap instruction. The code in the\nlibrary after the trap unpacks return values and returns control to the\nprogram that issued the system call. Thus, the parts of the C library that\nmake system calls are hand-coded in assembly, as they need to carefully\nfollow convention in order to process arguments and return values cor-\nrectly, as well as execute the hardware-speciﬁc trap instruction. And now\nyou know why you personally don’t have to write assembly code to trap\ninto an OS; somebody has already written that assembly for you.\nto be able to return correctly when the OS issues the return-from-trap\ninstruction. On x86, for example, the processor will push the program\ncounter, ﬂags, and a few other registers onto a per-process kernel stack;\nthe return-from-trap will pop these values off the stack and resume exe-\ncution of the user-mode program (see the Intel systems manuals [I11] for\ndetails). Other hardware systems use different conventions, but the basic\nconcepts are similar across platforms.\nThere is one important detail left out of this discussion: how does the\ntrap know which code to run inside the OS? Clearly, the calling process\ncan’t specify an address to jump to (as you would when making a pro-\ncedure call); doing so would allow programs to jump anywhere into the\nkernel which clearly is a bad idea (imagine jumping into code to access\na ﬁle, but just after a permission check; in fact, it is likely such ability\nwould enable a wily programmer to get the kernel to run arbitrary code\nsequences [S07]). Thus the kernel must carefully control what code exe-\ncutes upon a trap.\nThe kernel does so by setting up a trap table at boot time. When the\nmachine boots up, it does so in privileged (kernel) mode, and thus is\nfree to conﬁgure machine hardware as need be. One of the ﬁrst things\nthe OS thus does is to tell the hardware what code to run when certain\nexceptional events occur. For example, what code should run when a\nhard-disk interrupt takes place, when a keyboard interrupt occurs, or\nwhen program makes a system call? The OS informs the hardware of\nthe locations of these trap handlers, usually with some kind of special\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nMECHANISM: LIMITED DIRECT EXECUTION\n49\nOS @ boot\nHardware\n(kernel mode)\ninitialize trap table\nremember address of...\nsyscall handler\nOS @ run\nHardware\nProgram\n(kernel mode)\n(user mode)\nCreate entry for process list\nAllocate memory for program\nLoad program into memory\nSetup user stack with argv\nFill kernel stack with reg/PC\nreturn-from-trap\nrestore regs from kernel stack\nmove to user mode\njump to main\nRun main()\n...\nCall system call\ntrap into OS\nsave regs to kernel stack\nmove to kernel mode\njump to trap handler\nHandle trap\nDo work of syscall\nreturn-from-trap\nrestore regs from kernel stack\nmove to user mode\njump to PC after trap\n...\nreturn from main\ntrap (via exit())\nFree memory of process\nRemove from process list\nTable 6.2: Limited Direction Execution Protocol\ninstruction. Once the hardware is informed, it remembers the location of\nthese handlers until the machine is next rebooted, and thus the hardware\nknows what to do (i.e., what code to jump to) when system calls and other\nexceptional events take place.\nOne last aside: being able to execute the instruction to tell the hard-\nware where the trap tables are is a very powerful capability. Thus, as you\nmight have guessed, it is also a privileged operation. If you try to exe-\ncute this instruction in user mode, the hardware won’t let you, and you\ncan probably guess what will happen (hint: adios, offending program).\nPoint to ponder: what horrible things could you do to a system if you\ncould install your own trap table? Could you take over the machine?\nThe timeline (with time increasing downward, in Table 6.2) summa-\nrizes the protocol. We assume each process has a kernel stack where reg-\nisters (including general purpose registers and the program counter) are\nsaved to and restored from (by the hardware) when transitioning into and\nout of the kernel.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n50\nMECHANISM: LIMITED DIRECT EXECUTION\nThere are two phases in the LDE protocol. In the ﬁrst (at boot time),\nthe kernel initializes the trap table, and the CPU remembers its location\nfor subsequent use. The kernel does so via a privileged instruction (all\nprivileged instructions are highlighted in bold).\nIn the second (when running a process), the kernel sets up a few things\n(e.g., allocating a node on the process list, allocating memory) before us-\ning a return-from-trap instruction to start the execution of the process;\nthis switches the CPU to user mode and begins running the process.\nWhen the process wishes to issue a system call, it traps back into the OS,\nwhich handles it and once again returns control via a return-from-trap\nto the process. The process then completes its work, and returns from\nmain(); this usually will return into some stub code which will properly\nexit the program (say, by calling the exit() system call, which traps into\nthe OS). At this point, the OS cleans up and we are done.\n6.3\nProblem #2: Switching Between Processes\nThe next problem with direct execution is achieving a switch between\nprocesses. Switching between processes should be simple, right? The\nOS should just decide to stop one process and start another. What’s the\nbig deal? But it actually is a little bit tricky: speciﬁcally, if a process is\nrunning on the CPU, this by deﬁnition means the OS is not running. If\nthe OS is not running, how can it do anything at all? (hint: it can’t) While\nthis sounds almost philosophical, it is a real problem: there is clearly no\nway for the OS to take an action if it is not running on the CPU. Thus we\narrive at the crux of the problem.\nTHE CRUX: HOW TO REGAIN CONTROL OF THE CPU\nHow can the operating system regain control of the CPU so that it can\nswitch between processes?\nA Cooperative Approach: Wait For System Calls\nOne approach that some systems have taken in the past (for example,\nearly versions of the Macintosh operating system [M11], or the old Xerox\nAlto system [A79]) is known as the cooperative approach. In this style,\nthe OS trusts the processes of the system to behave reasonably. Processes\nthat run for too long are assumed to periodically give up the CPU so that\nthe OS can decide to run some other task.\nThus, you might ask, how does a friendly process give up the CPU in\nthis utopian world? Most processes, as it turns out, transfer control of\nthe CPU to the OS quite frequently by making system calls, for example,\nto open a ﬁle and subsequently read it, or to send a message to another\nmachine, or to create a new process. Systems like this often include an\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nMECHANISM: LIMITED DIRECT EXECUTION\n51\nTIP: DEALING WITH APPLICATION MISBEHAVIOR\nOperating systems often have to deal with misbehaving processes, those\nthat either through design (maliciousness) or accident (bugs) attempt to\ndo something that they shouldn’t. In modern systems, the way the OS\ntries to handle such malfeasance is to simply terminate the offender. One\nstrike and you’re out! Perhaps brutal, but what else should the OS do\nwhen you try to access memory illegally or execute an illegal instruction?\nexplicit yield system call, which does nothing except to transfer control\nto the OS so it can run other processes.\nApplications also transfer control to the OS when they do something\nillegal. For example, if an application divides by zero, or tries to access\nmemory that it shouldn’t be able to access, it will generate a trap to the\nOS. The OS will then have control of the CPU again (and likely terminate\nthe offending process).\nThus, in a cooperative scheduling system, the OS regains control of\nthe CPU by waiting for a system call or an illegal operation of some kind\nto take place. You might also be thinking: isn’t this passive approach less\nthan ideal? What happens, for example, if a process (whether malicious,\nor just full of bugs) ends up in an inﬁnite loop, and never makes a system\ncall? What can the OS do then?\nA Non-Cooperative Approach: The OS Takes Control\nWithout some additional help from the hardware, it turns out the OS can’t\ndo much at all when a process refuses to make system calls (or mistakes)\nand thus return control to the OS. In fact, in the cooperative approach,\nyour only recourse when a process gets stuck in an inﬁnite loop is to\nresort to the age-old solution to all problems in computer systems: reboot\nthe machine. Thus, we again arrive at a subproblem of our general quest\nto gain control of the CPU.\nTHE CRUX: HOW TO GAIN CONTROL WITHOUT COOPERATION\nHow can the OS gain control of the CPU even if processes are not being\ncooperative? What can the OS do to ensure a rogue process does not take\nover the machine?\nThe answer turns out to be simple and was discovered by a number\nof people building computer systems many years ago: a timer interrupt\n[M+63]. A timer device can be programmed to raise an interrupt every\nso many milliseconds; when the interrupt is raised, the currently running\nprocess is halted, and a pre-conﬁgured interrupt handler in the OS runs.\nAt this point, the OS has regained control of the CPU, and thus can do\nwhat it pleases: stop the current process, and start a different one.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n52\nMECHANISM: LIMITED DIRECT EXECUTION\nTIP: USE THE TIMER INTERRUPT TO REGAIN CONTROL\nThe addition of a timer interrupt gives the OS the ability to run again\non a CPU even if processes act in a non-cooperative fashion. Thus, this\nhardware feature is essential in helping the OS maintain control of the\nmachine.\nAs we discussed before with system calls, the OS must inform the\nhardware of which code to run when the timer interrupt occurs; thus,\nat boot time, the OS does exactly that.\nSecond, also during the boot\nsequence, the OS must start the timer, which is of course a privileged\noperation. Once the timer has begun, the OS can thus feel safe in that\ncontrol will eventually be returned to it, and thus the OS is free to run\nuser programs. The timer can also be turned off (also a privileged opera-\ntion), something we will discuss later when we understand concurrency\nin more detail.\nNote that the hardware has some responsibility when an interrupt oc-\ncurs, in particular to save enough of the state of the program that was\nrunning when the interrupt occurred such that a subsequent return-from-\ntrap instruction will be able to resume the running program correctly.\nThis set of actions is quite similar to the behavior of the hardware during\nan explicit system-call trap into the kernel, with various registers thus\ngetting saved (e.g., onto a kernel stack) and thus easily restored by the\nreturn-from-trap instruction.\nSaving and Restoring Context\nNow that the OS has regained control, whether cooperatively via a sys-\ntem call, or more forcefully via a timer interrupt, a decision has to be\nmade: whether to continue running the currently-running process, or\nswitch to a different one. This decision is made by a part of the operating\nsystem known as the scheduler; we will discuss scheduling policies in\ngreat detail in the next few chapters.\nIf the decision is made to switch, the OS then executes a low-level\npiece of code which we refer to as a context switch. A context switch is\nconceptually simple: all the OS has to do is save a few register values\nfor the currently-executing process (onto its kernel stack, for example)\nand restore a few for the soon-to-be-executing process (from its kernel\nstack). By doing so, the OS thus ensures that when the return-from-trap\ninstruction is ﬁnally executed, instead of returning to the process that was\nrunning, the system resumes execution of another process.\nTo save the context of the currently-running process, the OS will exe-\ncute some low-level assembly code to save the general purpose registers,\nPC, as well as the kernel stack pointer of the currently-running process,\nand then restore said registers, PC, and switch to the kernel stack for the\nsoon-to-be-executing process. By switching stacks, the kernel enters the\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nMECHANISM: LIMITED DIRECT EXECUTION\n53\nOS @ boot\nHardware\n(kernel mode)\ninitialize trap table\nremember addresses of...\nsyscall handler\ntimer handler\nstart interrupt timer\nstart timer\ninterrupt CPU in X ms\nOS @ run\nHardware\nProgram\n(kernel mode)\n(user mode)\nProcess A\n...\ntimer interrupt\nsave regs(A) to k-stack(A)\nmove to kernel mode\njump to trap handler\nHandle the trap\nCall switch() routine\nsave regs(A) to proc-struct(A)\nrestore regs(B) from proc-struct(B)\nswitch to k-stack(B)\nreturn-from-trap (into B)\nrestore regs(B) from k-stack(B)\nmove to user mode\njump to B’s PC\nProcess B\n...\nTable 6.3: Limited Direction Execution Protocol (Timer Interrupt)\ncall to the switch code in the context of one process (the one that was in-\nterrupted) and returns in the context of another (the soon-to-be-executing\none). When the OS then ﬁnally executes a return-from-trap instruction,\nthe soon-to-be-executing process becomes the currently-running process.\nAnd thus the context switch is complete.\nA timeline of the entire process is shown in Table 6.3. In this exam-\nple, Process A is running and then is interrupted by the timer interrupt.\nThe hardware saves its state (onto its kernel stack) and enters the kernel\n(switching to kernel mode). In the timer interrupt handler, the OS decides\nto switch from running Process A to Process B. At that point, it calls the\nswitch() routine, which carefully saves current register values (into the\nprocess structure of A), restores the registers of Process B (from its process\nstructure entry), and then switches contexts, speciﬁcally by changing the\nstack pointer to use B’s kernel stack (and not A’s). Finally, the OS returns-\nfrom-trap, which restores B’s register state and starts running it.\nNote that there are two types of register saves/restores that happen\nduring this protocol. The ﬁrst is when the timer interrupt occurs; in this\ncase, the user register state of the running process is implicitly saved by\nthe hardware, using the kernel stack of that process. The second is when\nthe OS decides to switch from A to B; in this case, the kernel register state\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "page_number": 78,
      "chapter_number": 10,
      "summary": "In fact, the entire\nsignals subsystem provides a rich infrastructure to deliver external events\nto processes, including ways to receive and process those signals Key topics include process, processes.",
      "keywords": [
        "PROCESS",
        "Limited Direct Execution",
        "system call",
        "system",
        "CPU",
        "Direct Execution",
        "kernel",
        "operating system",
        "Limited Direct",
        "call",
        "Control",
        "kernel stack",
        "kernel mode",
        "trap",
        "program"
      ],
      "concepts": [
        "process",
        "processes",
        "run",
        "running",
        "runs",
        "systems",
        "program",
        "programming",
        "hardware",
        "trap"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 40,
          "title": "Segment 40 (pages 795-814)",
          "relevance_score": 0.59,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 38,
          "title": "Segment 38 (pages 759-776)",
          "relevance_score": 0.54,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 39,
          "title": "Segment 39 (pages 777-794)",
          "relevance_score": 0.54,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 52,
          "title": "Segment 52 (pages 506-515)",
          "relevance_score": 0.54,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 7",
          "chapter": 13,
          "title": "Segment 13 (pages 116-124)",
          "relevance_score": 0.53,
          "method": "api"
        }
      ]
    },
    {
      "number": 11,
      "title": "Segment 11 (pages 90-102)",
      "start_page": 90,
      "end_page": 102,
      "detection_method": "topic_boundary",
      "content": "54\nMECHANISM: LIMITED DIRECT EXECUTION\n1\n# void swtch(struct context **old, struct context *new);\n2\n#\n3\n# Save current register context in old\n4\n# and then load register context from new.\n5\n.globl swtch\n6\nswtch:\n7\n# Save old registers\n8\nmovl 4(%esp), %eax\n# put old ptr into eax\n9\npopl 0(%eax)\n# save the old IP\n10\nmovl %esp, 4(%eax)\n# and stack\n11\nmovl %ebx, 8(%eax)\n# and other registers\n12\nmovl %ecx, 12(%eax)\n13\nmovl %edx, 16(%eax)\n14\nmovl %esi, 20(%eax)\n15\nmovl %edi, 24(%eax)\n16\nmovl %ebp, 28(%eax)\n17\n18\n# Load new registers\n19\nmovl 4(%esp), %eax\n# put new ptr into eax\n20\nmovl 28(%eax), %ebp # restore other registers\n21\nmovl 24(%eax), %edi\n22\nmovl 20(%eax), %esi\n23\nmovl 16(%eax), %edx\n24\nmovl 12(%eax), %ecx\n25\nmovl 8(%eax), %ebx\n26\nmovl 4(%eax), %esp\n# stack is switched here\n27\npushl 0(%eax)\n# return addr put in place\n28\nret\n# finally return into new ctxt\nFigure 6.1: The xv6 Context Switch Code\nis explicitly saved by the software (i.e., the OS), but this time into memory\nin the process structure of the process. The latter action moves the system\nfrom running as if it just trapped into the kernel from A to as if it just\ntrapped into the kernel from B.\nTo give you a better sense of how such a switch is enacted, Figure 6.1\nshows the context switch code for xv6. See if you can make sense of it\n(you’ll have to know a bit of x86, as well as some xv6, to do so). The\ncontext structures old and new are found the old and new process’s\nprocess structures, respectively.\n6.4\nWorried About Concurrency?\nSome of you, as attentive and thoughtful readers, may be now think-\ning: “Hmm... what happens when, during a system call, a timer interrupt\noccurs?” or “What happens when you’re handling one interrupt and an-\nother one happens? Doesn’t that get hard to handle in the kernel?” Good\nquestions – we really have some hope for you yet!\nThe answer is yes, the OS does indeed need to be concerned as to what\nhappens if, during interrupt or trap handling, another interrupt occurs.\nThis, in fact, is the exact topic of the entire second piece of this book, on\nconcurrency; we’ll defer a detailed discussion until then.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nMECHANISM: LIMITED DIRECT EXECUTION\n55\nASIDE: HOW LONG CONTEXT SWITCHES TAKE\nA natural question you might have is: how long does something like a\ncontext switch take? Or even a system call? For those of you that are cu-\nrious, there is a tool called lmbench [MS96] that measures exactly those\nthings, as well as a few other performance measures that might be rele-\nvant.\nResults have improved quite a bit over time, roughly tracking processor\nperformance. For example, in 1996 running Linux 1.3.37 on a 200-MHz\nP6 CPU, system calls took roughly 4 microseconds, and a context switch\nroughly 6 microseconds [MS96]. Modern systems perform almost an or-\nder of magnitude better, with sub-microsecond results on systems with\n2- or 3-GHz processors.\nIt should be noted that not all operating-system actions track CPU per-\nformance. As Ousterhout observed, many OS operations are memory\nintensive, and memory bandwidth has not improved as dramatically as\nprocessor speed over time [O90]. Thus, depending on your workload,\nbuying the latest and greatest processor may not speed up your OS as\nmuch as you might hope.\nTo whet your appetite, we’ll just sketch some basics of how the OS\nhandles these tricky situations. One simple thing an OS might do is dis-\nable interrupts during interrupt processing; doing so ensures that when\none interrupt is being handled, no other one will be delivered to the CPU.\nOf course, the OS has to be careful in doing so; disabling interrupts for\ntoo long could lead to lost interrupts, which is (in technical terms) bad.\nOperating systems also have developed a number of sophisticated\nlocking schemes to protect concurrent access to internal data structures.\nThis enables multiple activities to be on-going within the kernel at the\nsame time, particularly useful on multiprocessors. As we’ll see in the\nnext piece of this book on concurrency, though, such locking can be com-\nplicated and lead to a variety of interesting and hard-to-ﬁnd bugs.\n6.5\nSummary\nWe have described some key low-level mechanisms to implement CPU\nvirtualization, a set of techniques which we collectively refer to as limited\ndirect execution. The basic idea is straightforward: just run the program\nyou want to run on the CPU, but ﬁrst make sure to set up the hardware\nso as to limit what the process can do without OS assistance.\nThis general approach is taken in real life as well. For example, those\nof you who have children, or, at least, have heard of children, may be\nfamiliar with the concept of baby prooﬁng a room: locking cabinets con-\ntaining dangerous stuff and covering electrical sockets. When the room is\nthus readied, you can let your baby roam freely, secure in the knowledge\nthat the most dangerous aspects of the room have been restricted.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n56\nMECHANISM: LIMITED DIRECT EXECUTION\nTIP: REBOOT IS USEFUL\nEarlier on, we noted that the only solution to inﬁnite loops (and similar\nbehaviors) under cooperative preemption is to reboot the machine. While\nyou may scoff at this hack, researchers have shown that reboot (or in gen-\neral, starting over some piece of software) can be a hugely useful tool in\nbuilding robust systems [C+04].\nSpeciﬁcally, reboot is useful because it moves software back to a known\nand likely more tested state.\nReboots also reclaim stale or leaked re-\nsources (e.g., memory) which may otherwise be hard to handle. Finally,\nreboots are easy to automate. For all of these reasons, it is not uncommon\nin large-scale cluster Internet services for system management software\nto periodically reboot sets of machines in order to reset them and thus\nobtain the advantages listed above.\nThus, next time you reboot, you are not just enacting some ugly hack.\nRather, you are using a time-tested approach to improving the behavior\nof a computer system. Well done!\nIn an analogous manner, the OS “baby proofs” the CPU, by ﬁrst (dur-\ning boot time) setting up the trap handlers and starting an interrupt timer,\nand then by only running processes in a restricted mode. By doing so, the\nOS can feel quite assured that processes can run efﬁciently, only requir-\ning OS intervention to perform privileged operations or when they have\nmonopolized the CPU for too long and thus need to be switched out.\nWe thus have the basic mechanisms for virtualizing the CPU in place.\nBut a major question is left unanswered: which process should we run at\na given time? It is this question that the scheduler must answer, and thus\nthe next topic of our study.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nMECHANISM: LIMITED DIRECT EXECUTION\n57\nReferences\n[A79] “Alto User’s Handbook”\nXerox Palo Alto Research Center, September 1979\nAvailable: http://history-computer.com/Library/AltoUsersHandbook.pdf\nAn amazing system, way ahead of its time. Became famous because Steve Jobs visited, took notes, and\nbuilt Lisa and eventually Mac.\n[C+04] “Microreboot – A Technique for Cheap Recovery”\nGeorge Candea, Shinichi Kawamoto, Yuichi Fujiki, Greg Friedman, Armando Fox\nOSDI ’04, San Francisco, CA, December 2004\nAn excellent paper pointing out how far one can go with reboot in building more robust systems.\n[I11] “Intel 64 and IA-32 Architectures Software Developer’s Manual”\nVolume 3A and 3B: System Programming Guide\nIntel Corporation, January 2011\n[K+61] “One-Level Storage System”\nT. Kilburn, D.B.G. Edwards, M.J. Lanigan, F.H. Sumner\nIRE Transactions on Electronic Computers, April 1962\nThe Atlas pioneered much of what you see in modern systems. However, this paper is not the best one\nto read. If you were to only read one, you might try the historical perspective below [L78].\n[L78] “The Manchester Mark I and Atlas: A Historical Perspective”\nS. H. Lavington\nCommunications of the ACM, 21:1, January 1978\nA history of the early development of computers and the pioneering efforts of Atlas.\n[M+63] “A Time-Sharing Debugging System for a Small Computer”\nJ. McCarthy, S. Boilen, E. Fredkin, J. C. R. Licklider\nAFIPS ’63 (Spring), May, 1963, New York, USA\nAn early paper about time-sharing that refers to using a timer interrupt; the quote that discusses it:\n“The basic task of the channel 17 clock routine is to decide whether to remove the current user from core\nand if so to decide which user program to swap in as he goes out.”\n[MS96] “lmbench: Portable tools for performance analysis”\nLarry McVoy and Carl Staelin\nUSENIX Annual Technical Conference, January 1996\nA fun paper about how to measure a number of different things about your OS and its performance.\nDownload lmbench and give it a try.\n[M11] “Mac OS 9”\nJanuary 2011\nAvailable: http://en.wikipedia.org/wiki/Mac OS 9\n[O90] “Why Aren’t Operating Systems Getting Faster as Fast as Hardware?”\nJ. Ousterhout\nUSENIX Summer Conference, June 1990\nA classic paper on the nature of operating system performance.\n[P10] “The Single UNIX Speciﬁcation, Version 3”\nThe Open Group, May 2010\nAvailable: http://www.unix.org/version3/\nThis is hard and painful to read, so probably avoid it if you can.\n[S07] “The Geometry of Innocent Flesh on the Bone:\nReturn-into-libc without Function Calls (on the x86)”\nHovav Shacham\nCCS ’07, October 2007\nOne of those awesome, mind-blowing ideas that you’ll see in research from time to time. The author\nshows that if you can jump into code arbitrarily, you can essentially stitch together any code sequence\nyou like (given a large code base) – read the paper for the details. The technique makes it even harder to\ndefend against malicious attacks, alas.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n58\nMECHANISM: LIMITED DIRECT EXECUTION\nHomework (Measurement)\nASIDE: MEASUREMENT HOMEWORKS\nMeasurement homeworks are small exercises where you write code to\nrun on a real machine, in order to measure some aspect of OS or hardware\nperformance. The idea behind such homeworks is to give you a little bit\nof hands-on experience with a real operating system.\nIn this homework, you’ll measure the costs of a system call and context\nswitch. Measuring the cost of a system call is relatively easy. For example,\nyou could repeatedly call a simple system call (e.g., performing a 0-byte\nread), and time how long it takes; dividing the time by the number of\niterations gives you an estimate of the cost of a system call.\nOne thing you’ll have to take into account is the precision and accu-\nracy of your timer. A typical timer that you can use is gettimeofday();\nread the man page for details. What you’ll see there is that gettimeofday()\nreturns the time in microseconds since 1970; however, this does not mean\nthat the timer is precise to the microsecond. Measure back-to-back calls\nto gettimeofday() to learn something about how precise the timer re-\nally is; this will tell you how many iterations of your null system-call\ntest you’ll have to run in order to get a good measurement result. If\ngettimeofday() is not precise enough for you, you might look into\nusing the rdtsc instruction available on x86 machines.\nMeasuring the cost of a context switch is a little trickier. The lmbench\nbenchmark does so by running two processes on a single CPU, and set-\nting up two UNIX pipes between them; a pipe is just one of many ways\nprocesses in a UNIX system can communicate with one another. The ﬁrst\nprocess then issues a write to the ﬁrst pipe, and waits for a read on the\nsecond; upon seeing the ﬁrst process waiting for something to read from\nthe second pipe, the OS puts the ﬁrst process in the blocked state, and\nswitches to the other process, which reads from the ﬁrst pipe and then\nwrites to the second. When the second process tries to read from the ﬁrst\npipe again, it blocks, and thus the back-and-forth cycle of communication\ncontinues. By measuring the cost of communicating like this repeatedly,\nlmbench can make a good estimate of the cost of a context switch. You\ncan try to re-create something similar here, using pipes, or perhaps some\nother communication mechanism such as UNIX sockets.\nOne difﬁculty in measuring context-switch cost arises in systems with\nmore than one CPU; what you need to do on such a system is ensure that\nyour context-switching processes are located on the same processor. For-\ntunately, most operating systems have calls to bind a process to a partic-\nular processor; on Linux, for example, the sched setaffinity() call\nis what you’re looking for. By ensuring both processes are on the same\nprocessor, you are making sure to measure the cost of the OS stopping\none process and restoring another on the same CPU.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\n7\nScheduling: Introduction\nBy now low-level mechanisms of running processes (e.g., context switch-\ning) should be clear; if they are not, go back a chapter or two, and read the\ndescription of how that stuff works again. However, we have yet to un-\nderstand the high-level policies that an OS scheduler employs. We will\nnow do just that, presenting a series of scheduling policies (sometimes\ncalled disciplines) that various smart and hard-working people have de-\nveloped over the years.\nThe origins of scheduling, in fact, predate computer systems; early\napproaches were taken from the ﬁeld of operations management and ap-\nplied to computers. This reality should be no surprise: assembly lines\nand many other human endeavors also require scheduling, and many of\nthe same concerns exist therein, including a laser-like desire for efﬁciency.\nAnd thus, our problem:\nTHE CRUX: HOW TO DEVELOP SCHEDULING POLICY\nHow should we develop a basic framework for thinking about\nscheduling policies? What are the key assumptions? What metrics are\nimportant? What basic approaches have been used in the earliest of com-\nputer systems?\n7.1\nWorkload Assumptions\nBefore getting into the range of possible policies, let us ﬁrst make a\nnumber of simplifying assumptions about the processes running in the\nsystem, sometimes collectively called the workload. Determining the\nworkload is a critical part of building policies, and the more you know\nabout workload, the more ﬁne-tuned your policy can be.\nThe workload assumptions we make here are clearly unrealistic, but\nthat is alright (for now), because we will relax them as we go, and even-\ntually develop what we will refer to as ... (dramatic pause) ...\n59\n\n\n60\nSCHEDULING: INTRODUCTION\na fully-operational scheduling discipline1.\nWe will make the following assumptions about the processes, some-\ntimes called jobs, that are running in the system:\n1. Each job runs for the same amount of time.\n2. All jobs arrive at the same time.\n3. All jobs only use the CPU (i.e., they perform no I/O)\n4. The run-time of each job is known.\nWe said all of these assumptions were unrealistic, but just as some an-\nimals are more equal than others in Orwell’s Animal Farm [O45], some\nassumptions are more unrealistic than others in this chapter. In particu-\nlar, it might bother you that the run-time of each job is known: this would\nmake the scheduler omniscient, which, although it would be great (prob-\nably), is not likely to happen anytime soon.\n7.2\nScheduling Metrics\nBeyond making workload assumptions, we also need one more thing\nto enable us to compare different scheduling policies: a scheduling met-\nric. A metric is just something that we use to measure something, and\nthere are a number of different metrics that make sense in scheduling.\nFor now, however, let us also simplify our life by simply having a sin-\ngle metric: turnaround time. The turnaround time of a job is deﬁned\nas the time at which the job completes minus the time at which the job\narrived in the system. More formally, the turnaround time Tturnaround is:\nTturnaround = Tcompletion −Tarrival\n(7.1)\nBecause we have assumed that all jobs arrive at the same time, for now\nTarrival = 0 and hence Tturnaround = Tcompletion. This fact will change\nas we relax the aforementioned assumptions.\nYou should note that turnaround time is a performance metric, which\nwill be our primary focus this chapter. Another metric of interest is fair-\nness, as measured (for example) by Jain’s Fairness Index [J91]. Perfor-\nmance and fairness are often at odds in scheduling; a scheduler, for ex-\nample, may optimize performance but at the cost of preventing a few jobs\nfrom running, thus decreasing fairness. This conundrum shows us that\nlife isn’t always perfect.\n7.3\nFirst In, First Out (FIFO)\nThe most basic algorithm a scheduler can implement is known as First\nIn, First Out (FIFO) scheduling or sometimes First Come, First Served\n1Said in the same way you would say “A fully-operational Death Star.”\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nSCHEDULING: INTRODUCTION\n61\n(FCFS). FIFO has a number of positive properties: it is clearly very simple\nand thus easy to implement. Given our assumptions, it works pretty well.\nLet’s do a quick example together. Imagine three jobs arrive in the\nsystem, A, B, and C, at roughly the same time (Tarrival = 0). Because\nFIFO has to put some job ﬁrst, let’s assume that while they all arrived\nsimultaneously, A arrived just a hair before B which arrived just a hair\nbefore C. Assume also that each job runs for 10 seconds. What will the\naverage turnaround time be for these jobs?\n0\n20\n40\n60\n80\n100\n120\nTime\nA\nB\nC\nFigure 7.1: FIFO Simple Example\nFrom Figure 7.1, you can see that A ﬁnished at 10, B at 20, and C at 30.\nThus, the average turnaround time for the three jobs is simply 10+20+30\n3\n=\n20. Computing turnaround time is as easy as that.\nNow let’s relax one of our assumptions. In particular, let’s relax as-\nsumption 1, and thus no longer assume that each job runs for the same\namount of time. How does FIFO perform now? What kind of workload\ncould you construct to make FIFO perform poorly?\n(think about this before reading on ... keep thinking ... got it?!)\nPresumably you’ve ﬁgured this out by now, but just in case, let’s do\nan example to show how jobs of different lengths can lead to trouble for\nFIFO scheduling. In particular, let’s again assume three jobs (A, B, and\nC), but this time A runs for 100 seconds while B and C run for 10 each.\n0\n20\n40\n60\n80\n100\n120\nTime\nA\nB\nC\nFigure 7.2: Why FIFO Is Not That Great\nAs you can see in Figure 7.2, Job A runs ﬁrst for the full 100 seconds\nbefore B or C even get a chance to run. Thus, the average turnaround\ntime for the system is high: a painful 110 seconds ( 100+110+120\n3\n= 110).\nThis problem is generally referredto as the convoy effect [B+79], where\na number of relatively-short potential consumers of a resource get queued\nbehind a heavyweight resource consumer. This scheduling scenario might\nremind you of a single line at a grocery store and what you feel like when\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n62\nSCHEDULING: INTRODUCTION\nTIP: THE PRINCIPLE OF SJF\nShortest Job First represents a general scheduling principle that can be\napplied to any system where the perceived turnaround time per customer\n(or, in our case, a job) matters. Think of any line you have waited in: if\nthe establishment in question cares about customer satisfaction, it is likely\nthey have taken SJF into account. For example, grocery stores commonly\nhave a “ten-items-or-less” line to ensure that shoppers with only a few\nthings to purchase don’t get stuck behind the family preparing for some\nupcoming nuclear winter.\nyou see the person in front of you with three carts full of provisions and\na checkbook out; it’s going to be a while2.\nSo what should we do? How can we develop a better algorithm to\ndeal with our new reality of jobs that run for different amounts of time?\nThink about it ﬁrst; then read on.\n7.4\nShortest Job First (SJF)\nIt turns out that a very simple approach solves this problem; in fact\nit is an idea stolen from operations research [C54,PV56] and applied to\nscheduling of jobs in computer systems. This new scheduling discipline\nis known as Shortest Job First (SJF), and the name should be easy to\nremember because it describes the policy quite completely: it runs the\nshortest job ﬁrst, then the next shortest, and so on.\n0\n20\n40\n60\n80\n100\n120\nTime\nB\nC\nA\nFigure 7.3: SJF Simple Example\nLet’s take our example above but with SJF as our scheduling policy.\nFigure 7.3 shows the results of running A, B, and C. Hopefully the dia-\ngram makes it clear why SJF performs much better with regards to aver-\nage turnaround time. Simply by running B and C before A, SJF reduces\naverage turnaround from 110 seconds to 50 ( 10+20+120\n3\n= 50), more than\na factor of two improvement.\nIn fact, given our assumptions about jobs all arriving at the same time,\nwe could prove that SJF is indeed an optimal scheduling algorithm. How-\n2Recommended action in this case: either quickly switch to a different line, or take a long,\ndeep, and relaxing breath. That’s right, breathe in, breathe out. It will be OK, don’t worry.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nSCHEDULING: INTRODUCTION\n63\nASIDE: PREEMPTIVE SCHEDULERS\nIn the old days of batch computing, a number of non-preemptive sched-\nulers were developed; such systems would run each job to completion\nbefore considering whether to run a new job. Virtually all modern sched-\nulers are preemptive, and quite willing to stop one process from run-\nning in order to run another. This implies that the scheduler employs the\nmechanisms we learned about previously; in particular, the scheduler can\nperform a context switch, stopping one running process temporarily and\nresuming (or starting) another.\never, you are in a systems class, not theory or operations research; no\nproofs are allowed.\nThus we arrive upon a good approach to scheduling with SJF, but our\nassumptions are still fairly unrealistic. Let’s relax another. In particular,\nwe can target assumption 2, and now assume that jobs can arrive at any\ntime instead of all at once. What problems does this lead to?\n(Another pause to think ... are you thinking? Come on, you can do it)\nHere we can illustrate the problem again with an example. This time,\nassume A arrives at t = 0 and needs to run for 100 seconds, whereas B\nand C arrive at t = 10 and each need to run for 10 seconds. With pure\nSJF, we’d get the schedule seen in Figure 7.4.\n0\n20\n40\n60\n80\n100\n120\nTime\nA\nB\nC\n[B,C arrive]\nFigure 7.4: SJF With Late Arrivals From B and C\nAs you can see from the ﬁgure, even though B and C arrived shortly\nafter A, they still are forced to wait until A has completed, and thus suffer\nthe same convoy problem. Average turnaround time for these three jobs\nis 103.33 seconds ( 100+(110−10)+(120−10)\n3\n). What can a scheduler do?\n7.5\nShortest Time-to-Completion First (STCF)\nAs you might have guessed, given our previous discussion about mech-\nanisms such as timer interrupts and context switching, the scheduler can\ncertainly do something else when B and C arrive: it can preempt job A\nand decide to run another job, perhaps continuing A later. SJF by our deﬁ-\nnition is a non-preemptive scheduler, and thus suffers from the problems\ndescribed above.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n64\nSCHEDULING: INTRODUCTION\n0\n20\n40\n60\n80\n100\n120\nTime\nA\nB\nC\nA\n[B,C arrive]\nFigure 7.5: STCF Simple Example\nFortunately, there is a scheduler which does exactly that: add preemp-\ntion to SJF, known as the Shortest Time-to-Completion First (STCF) or\nPreemptive Shortest Job First (PSJF) scheduler [CK68]. Any time a new\njob enters the system, it determines of the remaining jobs and new job,\nwhich has the least time left, and then schedules that one. Thus, in our\nexample, STCF would preempt A and run B and C to completion; only\nwhen they are ﬁnished would A’s remaining time be scheduled. Figure\n7.5 shows an example.\nThe result is a much-improved average turnaround time: 50 seconds\n( (120−0)+(20−10)+(30−10)\n3\n). And as before, given our new assumptions,\nSTCF is provably optimal; given that SJF is optimal if all jobs arrive at\nthe same time, you should probably be able to see the intuition behind\nthe optimality of STCF.\nThus, if we knew that job lengths, and jobs only used the CPU, and our\nonly metric was turnaround time, STCF would be a great policy. In fact,\nfor a number of early batch computing systems, these types of scheduling\nalgorithms made some sense. However, the introduction of time-shared\nmachines changed all that. Now users would sit at a terminal and de-\nmand interactive performance from the system as well. And thus, a new\nmetric was born: response time.\nResponse time is deﬁned as the time from when the job arrives in a\nsystem to the ﬁrst time it is scheduled. More formally:\nTresponse = Tfirstrun −Tarrival\n(7.2)\nFor example, if we had the schedule above (with A arriving at time 0,\nand B and C at time 10), the response time of each job is as follows: 0 for\njob A, 0 for B, and 10 for C (average: 3.33).\nAs you might be thinking, STCF and related disciplines are not par-\nticularly good for response time. If three jobs arrive at the same time,\nfor example, the third job has to wait for the previous two jobs to run in\ntheir entirety before being scheduled just once. While great for turnaround\ntime, this approach is quite bad for response time and interactivity. In-\ndeed, imagine sitting at a terminal, typing, and having to wait 10 seconds\nto see a response from the system just because some other job got sched-\nuled in front of yours: not too pleasant.\nThus, we are left with another problem: how can we build a scheduler\nthat is sensitive to response time?\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nSCHEDULING: INTRODUCTION\n65\n0\n5\n10\n15\n20\n25\n30\nTime\nA\nB\nC\nFigure 7.6: SJF Again (Bad for Response Time)\n0\n5\n10\n15\n20\n25\n30\nTime\nABCABCABCABCABC\nFigure 7.7: Round Robin (Good for Response Time)\n7.6\nRound Robin\nTo solve this problem, we will introduce a new scheduling algorithm.\nThis approach is classically known as Round-Robin (RR) scheduling [K64].\nThe basic idea is simple: instead of running jobs to completion, RR runs\na job for a time slice (sometimes called a scheduling quantum) and then\nswitches to the next job in the run queue.\nIt repeatedly does so un-\ntil the jobs are ﬁnished. For this reason, RR is sometimes called time-\nslicing. Note that the length of a time slice must be a multiple of the\ntimer-interrupt period; thus if the timer interrupts every 10 milliseconds,\nthe time slice could be 10, 20, or any other multiple of 10 ms.\nTo understand RR in more detail, let’s look at an example. Assume\nthree jobs A, B, and C arrive at the same time in the system, and that\nthey each wish to run for 5 seconds. An SJF scheduler runs each job to\ncompletion before running another (Figure 7.6). In contrast, RR with a\ntime-slice of 1 second would cycle through the jobs quickly (Figure 7.7).\nThe average response time of RR is:\n0+1+2\n3\n= 1; for SJF, average re-\nsponse time is: 0+5+10\n3\n= 5.\nAs you can see, the length of the time slice is critical for RR. The shorter\nit is, the better the performance of RR under the response-time metric.\nHowever, making the time slice too short is problematic: suddenly the\ncost of context switching will dominate overall performance. Thus, de-\nciding on the length of the time slice presents a trade-off to a system de-\nsigner, making it long enough to amortize the cost of switching without\nmaking it so long that the system is no longer responsive.\nNote that the cost of context switching does not arise solely from the\nOS actions of saving and restoring a few registers. When programs run,\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n66\nSCHEDULING: INTRODUCTION\nTIP: AMORTIZATION CAN REDUCE COSTS\nThe general technique of amortization is commonly used in systems\nwhen there is a ﬁxed cost to some operation. By incurring that cost less\noften (i.e., by performing the operation fewer times), the total cost to the\nsystem is reduced. For example, if the time slice is set to 10 ms, and the\ncontext-switch cost is 1 ms, roughly 10% of time is spent context switch-\ning and is thus wasted. If we want to amortize this cost, we can increase\nthe time slice, e.g., to 100 ms. In this case, less than 1% of time is spent\ncontext switching, and thus the cost of time-slicing has been amortized.\nthey build up a great deal of state in CPU caches, TLBs, branch predictors,\nand other on-chip hardware. Switching to another job causes this state\nto be ﬂushed and new state relevant to the currently-running job to be\nbrought in, which may exact a noticeable performance cost [MB91].\nRR, with a reasonable time slice, is thus an excellent scheduler if re-\nsponse time is our only metric. But what about our old friend turnaround\ntime? Let’s look at our example above again. A, B, and C, each with run-\nning times of 5 seconds, arrive at the same time, and RR is the scheduler\nwith a (long) 1-second time slice. We can see from the picture above that\nA ﬁnishes at 13, B at 14, and C at 15, for an average of 14. Pretty awful!\nIt is not surprising, then, that RR is indeed one of the worst policies if\nturnaround time is our metric. Intuitively, this should make sense: what\nRR is doing is stretching out each job as long as it can, by only running\neach job for a short bit before moving to the next. Because turnaround\ntime only cares about when jobs ﬁnish, RR is nearly pessimal, even worse\nthan simple FIFO in many cases.\nMore generally, any policy (such as RR) that is fair, i.e., that evenly di-\nvides the CPU among active processes on a small time scale, will perform\npoorly on metrics such as turnaround time. Indeed, this is an inherent\ntrade-off: if you are willing to be unfair, you can run shorter jobs to com-\npletion, but at the cost of response time; if you instead value fairness,\nresponse time is lowered, but at the cost of turnaround time. This type of\ntrade-off is common in systems; you can’t have your cake and eat it too.\nWe have developed two types of schedulers. The ﬁrst type (SJF, STCF)\noptimizes turnaround time, but is bad for response time. The second type\n(RR) optimizes response time but is bad for turnaround. And we still\nhave two assumptions which need to be relaxed: assumption 3 (that jobs\ndo no I/O), and assumption 4 (that the run-time of each job is known).\nLet’s tackle those assumptions next.\n7.7\nIncorporating I/O\nFirst we will relax assumption 3 – of course all programs perform I/O.\nImagine a program that didn’t take any input: it would produce the same\noutput each time. Imagine one without output: it is the proverbial tree\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "page_number": 90,
      "chapter_number": 11,
      "summary": "This chapter covers segment 11 (pages 90-102). Key topics include time, scheduler, and schedule. To give you a better sense of how such a switch is enacted, Figure 6.1\nshows the context switch code for xv6.",
      "keywords": [
        "time",
        "turnaround time",
        "job",
        "system",
        "response time",
        "Jobs",
        "eax",
        "Scheduling",
        "LIMITED DIRECT EXECUTION",
        "SJF",
        "movl",
        "time slice",
        "average turnaround time",
        "CPU",
        "run"
      ],
      "concepts": [
        "time",
        "scheduler",
        "schedule",
        "jobs",
        "performance",
        "perform",
        "operating",
        "operations",
        "operation",
        "running"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 7",
          "chapter": 13,
          "title": "Segment 13 (pages 116-124)",
          "relevance_score": 0.6,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 1,
          "title": "Segment 1 (pages 1-15)",
          "relevance_score": 0.58,
          "method": "api"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 37,
          "title": "Segment 37 (pages 363-372)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 34,
          "title": "Segment 34 (pages 319-327)",
          "relevance_score": 0.54,
          "method": "api"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 33,
          "title": "Segment 33 (pages 330-337)",
          "relevance_score": 0.53,
          "method": "api"
        }
      ]
    },
    {
      "number": 12,
      "title": "Segment 12 (pages 103-116)",
      "start_page": 103,
      "end_page": 116,
      "detection_method": "topic_boundary",
      "content": "SCHEDULING: INTRODUCTION\n67\nfalling in the forest, with no one to see it; it doesn’t matter that it ran.\nA scheduler clearly has a decision to make when a job initiates an I/O\nrequest, because the currently-running job won’t be using the CPU dur-\ning the I/O; it is blocked waiting for I/O completion. If the I/O is sent to\na hard disk drive, the process might be blocked for a few milliseconds or\nlonger, depending on the current I/O load of the drive. Thus, the sched-\nuler should probably schedule another job on the CPU at that time.\nThe scheduler also has to make a decision when the I/O completes.\nWhen that occurs, an interrupt is raised, and the OS runs and moves\nthe process that issued the I/O from blocked back to the ready state. Of\ncourse, it could even decide to run the job at that point. How should the\nOS treat each job?\nTo understand this issue better, let us assume we have two jobs, A and\nB, which each need 50 ms of CPU time. However, there is one obvious\ndifference: A runs for 10 ms and then issues an I/O request (assume here\nthat I/Os each take 10 ms), whereas B simply uses the CPU for 50 ms and\nperforms no I/O. The scheduler runs A ﬁrst, then B after (Figure 7.8).\n0\n20\n40\n60\n80\n100\n120\n140\nTime\nA\nA\nA\nA\nA B B B B B\nCPU\nDisk\nFigure 7.8: Poor Use of Resources\nAssume we are trying to build a STCF scheduler. How should such a\nscheduler account for the fact that A is broken up into 5 10-ms sub-jobs,\nwhereas B is just a single 50-ms CPU demand? Clearly, just running one\njob and then the other without considering how to take I/O into account\nmakes little sense.\n0\n20\n40\n60\n80\n100\n120\n140\nTime\nA\nA\nA\nA\nA\nB\nB\nB\nB\nB\nCPU\nDisk\nFigure 7.9: Overlap Allows Better Use of Resources\nA common approach is to treat each 10-ms sub-job of A as an indepen-\ndent job. Thus, when the system starts, its choice is whether to schedule\na 10-ms A or a 50-ms B. With STCF, the choice is clear: choose the shorter\none, in this case A. Then, when the ﬁrst sub-job of A has completed, only\nB is left, and it begins running. Then a new sub-job of A is submitted,\nand it preempts B and runs for 10 ms. Doing so allows for overlap, with\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n68\nSCHEDULING: INTRODUCTION\nTIP: OVERLAP ENABLES HIGHER UTILIZATION\nWhen possible, overlap operations to maximize the utilization of sys-\ntems. Overlap is useful in many different domains, including when per-\nforming disk I/O or sending messages to remote machines; in either case,\nstarting the operation and then switching to other work is a good idea,\nand improved the overall utilization and efﬁciency of the system.\nthe CPU being used by one process while waiting for the I/O of another\nprocess to complete; the system is thus better utilized (see Figure 7.9).\nAnd thus we see how a scheduler might incorporate I/O. By treating\neach CPU burst as a job, the scheduler makes sure processes that are “in-\nteractive” get run frequently. While those interactive jobs are performing\nI/O, other CPU-intensive jobs run, thus better utilizing the processor.\n7.8\nNo More Oracle\nWith a basic approach to I/O in place, we come to our ﬁnal assump-\ntion: that the scheduler knows the length of each job. As we said before,\nthis is likely the worst assumption we could make. In fact, in a general-\npurpose OS (like the ones we care about), the OS usually knows very little\nabout the length of each job. Thus, how can we build an approach that be-\nhaves like SJF/STCF without such a priori knowledge? Further, how can\nwe incorporate some of the ideas we have seen with the RR scheduler so\nthat response time is also quite good?\n7.9\nSummary\nWe have introduced the basic ideas behind scheduling and developed\ntwo families of approaches. The ﬁrst runs the shortest job remaining and\nthus optimizes turnaround time; the second alternates between all jobs\nand thus optimizes response time. Both are bad where the other is good,\nalas, an inherent trade-off common in systems. We have also seen how we\nmight incorporate I/O into the picture, but have still not solved the prob-\nlem of the fundamental inability of the OS to see into the future. Shortly,\nwe will see how to overcome this problem, by building a scheduler that\nuses the recent past to predict the future. This scheduler is known as the\nmulti-level feedback queue, and it is the topic of the next chapter.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nSCHEDULING: INTRODUCTION\n69\nReferences\n[B+79] “The Convoy Phenomenon”\nM. Blasgen, J. Gray, M. Mitoma, T. Price\nACM Operating Systems Review, 13:2, April 1979\nPerhaps the ﬁrst reference to convoys, which occurs in databases as well as the OS.\n[C54] “Priority Assignment in Waiting Line Problems”\nA. Cobham\nJournal of Operations Research, 2:70, pages 70–76, 1954\nThe pioneering paper on using an SJF approach in scheduling the repair of machines.\n[K64] “Analysis of a Time-Shared Processor”\nLeonard Kleinrock\nNaval Research Logistics Quarterly, 11:1, pages 59–73, March 1964\nMay be the ﬁrst reference to the round-robin scheduling algorithm; certainly one of the ﬁrst analyses of\nsaid approach to scheduling a time-shared system.\n[CK68] “Computer Scheduling Methods and their Countermeasures”\nEdward G. Coffman and Leonard Kleinrock\nAFIPS ’68 (Spring), April 1968\nAn excellent early introduction to and analysis of a number of basic scheduling disciplines.\n[J91] “The Art of Computer Systems Performance Analysis:\nTechniques for Experimental Design, Measurement, Simulation, and Modeling”\nR. Jain\nInterscience, New York, April 1991\nThe standard text on computer systems measurement. A great reference for your library, for sure.\n[O45] “Animal Farm”\nGeorge Orwell\nSecker and Warburg (London), 1945\nA great but depressing allegorical book about power and its corruptions. Some say it is a critique of\nStalin and the pre-WWII Stalin era in the U.S.S.R; we say it’s a critique of pigs.\n[PV56] “Machine Repair as a Priority Waiting-Line Problem”\nThomas E. Phipps Jr. and W. R. Van Voorhis\nOperations Research, 4:1, pages 76–86, February 1956\nFollow-on work that generalizes the SJF approach to machine repair from Cobham’s original work; also\npostulates the utility of an STCF approach in such an environment. Speciﬁcally, “There are certain\ntypes of repair work, ... involving much dismantling and covering the ﬂoor with nuts and bolts, which\ncertainly should not be interrupted once undertaken; in other cases it would be inadvisable to continue\nwork on a long job if one or more short ones became available (p.81).”\n[MB91] “The effect of context switches on cache performance”\nJeffrey C. Mogul and Anita Borg\nASPLOS, 1991\nA nice study on how cache performance can be affected by context switching; less of an issue in today’s\nsystems where processors issue billions of instructions per second but context-switches still happen in\nthe millisecond time range.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n70\nSCHEDULING: INTRODUCTION\nHomework\nASIDE: SIMULATION HOMEWORKS\nSimulation homeworks come in the form of simulators you run to\nmake sure you understand some piece of the material. The simulators\nare generally python programs that enable you both to generate different\nproblems (using different random seeds) as well as to have the program\nsolve the problem for you (with the -c ﬂag) so that you can check your\nanswers. Running any simulator with a -h or --help ﬂag will provide\nwith more information as to all the options the simulator gives you.\nThis program, scheduler.py, allows you to see how different sched-\nulers perform under scheduling metrics such as response time, turnaround\ntime, and total wait time. See the README for details.\nQuestions\n1. Compute the response time and turnaround time when running\nthree jobs of length 200 with the SJF and FIFO schedulers.\n2. Now do the same but with jobs of different lengths: 100, 200, and\n300.\n3. Now do the same, but also with the RR scheduler and a time-slice\nof 1.\n4. For what types of workloads does SJF deliver the same turnaround\ntimes as FIFO?\n5. For what types of workloads and quantum lengths does SJF deliver\nthe same response times as RR?\n6. What happens to response time with SJF as job lengths increase?\nCan you use the simulator to demonstrate the trend?\n7. What happens to response time with RR as quantum lengths in-\ncrease? Can you write an equation that gives the worst-case re-\nsponse time, given N jobs?\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\n8\nScheduling:\nThe Multi-Level Feedback Queue\nIn this chapter, we’ll tackle the problem of developing one of the most\nwell-known approaches to scheduling, known as the Multi-level Feed-\nback Queue (MLFQ). The Multi-level Feedback Queue (MLFQ) sched-\nuler was ﬁrst described by Corbato et al. in 1962 [C+62] in a system\nknown as the Compatible Time-Sharing System (CTSS), and this work,\nalong with later work on Multics, led the ACM to award Corbato its\nhighest honor, the Turing Award. The scheduler has subsequently been\nreﬁned throughout the years to the implementations you will encounter\nin some modern systems.\nThe fundamental problem MLFQ tries to address is two-fold. First, it\nwould like to optimize turnaround time, which, as we saw in the previous\nnote, is done by running shorter jobs ﬁrst; unfortunately, the OS doesn’t\ngenerally know how long a job will run for, exactly the knowledge that\nalgorithms like SJF (or STCF) require. Second, MLFQ would like to make\na system feel responsive to interactive users (i.e., users sitting and staring\nat the screen, waiting for a process to ﬁnish), and thus minimize response\ntime; unfortunately, algorithms like Round Robin reduce response time\nbut are terrible for turnaround time. Thus, our problem: given that we\nin general do not know anything about a process, how can we build a\nscheduler to achieve these goals? How can the scheduler learn, as the\nsystem runs, the characteristics of the jobs it is running, and thus make\nbetter scheduling decisions?\nTHE CRUX:\nHOW TO SCHEDULE WITHOUT PERFECT KNOWLEDGE?\nHow can we design a scheduler that both minimizes response time for\ninteractive jobs while also minimizing turnaround time without a priori\nknowledge of job length?\n71\n\n\n72\nSCHEDULING:\nTHE MULTI-LEVEL FEEDBACK QUEUE\nTIP: LEARN FROM HISTORY\nThe multi-level feedback queue is an excellent example of a system that\nlearns from the past to predict the future. Such approaches are com-\nmon in operating systems (and many other places in Computer Science,\nincluding hardware branch predictors and caching algorithms).\nSuch\napproaches work when jobs have phases of behavior and are thus pre-\ndictable; of course, one must be careful with such techniques, as they can\neasily be wrong and drive a system to make worse decisions than they\nwould have with no knowledge at all.\n8.1\nMLFQ: Basic Rules\nTo build such a scheduler, in this chapter we will describe the basic\nalgorithms behind a multi-level feedback queue; although the speciﬁcs of\nmany implemented MLFQs differ [E95], most approaches are similar.\nIn our treatment, the MLFQ has a number of distinct queues, each\nassigned a different priority level. At any given time, a job that is ready\nto run is on a single queue. MLFQ uses priorities to decide which job\nshould run at a given time: a job with higher priority (i.e., a job on a\nhigher queue) is chosen to run.\nOf course, more than one job may be on a given queue, and thus have\nthe same priority. In this case, we will just use round-robin scheduling\namong those jobs.\nThus, the key to MLFQ scheduling lies in how the scheduler sets pri-\norities. Rather than giving a ﬁxed priority to each job, MLFQ varies the\npriority of a job based on its observed behavior. If, for example, a job repeat-\nedly relinquishes the CPU while waiting for input from the keyboard,\nMLFQ will keep its priority high, as this is how an interactive process\nmight behave. If, instead, a job uses the CPU intensively for long periods\nof time, MLFQ will reduce its priority. In this way, MLFQ will try to learn\nabout processes as they run, and thus use the history of the job to predict\nits future behavior.\nThus, we arrive at the ﬁrst two basic rules for MLFQ:\n• Rule 1: If Priority(A) > Priority(B), A runs (B doesn’t).\n• Rule 2: If Priority(A) = Priority(B), A & B run in RR.\nIf we were to put forth a picture of what the queues might look like at\na given instant, we might see something like the following (Figure 8.1).\nIn the ﬁgure, two jobs (A and B) are at the highest priority level, while job\nC is in the middle and Job D is at the lowest priority. Given our current\nknowledge of how MLFQ works, the scheduler would just alternate time\nslices between A and B because they are the highest priority jobs in the\nsystem; poor jobs C and D would never even get to run – an outrage!\nOf course, just showing a static snapshot of some queues does not re-\nally give you an idea of how MLFQ works. What we need is to under-\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nSCHEDULING:\nTHE MULTI-LEVEL FEEDBACK QUEUE\n73\nQ1\nQ2\nQ3\nQ4\nQ5\nQ6\nQ7\nQ8\n[Low Priority]\n[High Priority]\nD\nC\nA\nB\nFigure 8.1: MLFQ Example\nstand how job priority changes over time. And that, in a surprise only\nto those who are reading a chapter from this book for the ﬁrst time, is\nexactly what we will do next.\n8.2\nAttempt #1: How to Change Priority\nWe now must decide how MLFQ is going to change the priority level\nof a job (and thus which queue it is on) over the lifetime of a job. To do\nthis, we must keep in mind our workload: a mix of interactive jobs that\nare short-running (and may frequently relinquish the CPU), and some\nlonger-running “CPU-bound” jobs that need a lot of CPU time but where\nresponse time isn’t important.\nHere is our ﬁrst attempt at a priority-\nadjustment algorithm:\n• Rule 3: When a job enters the system, it is placed at the highest\npriority (the topmost queue).\n• Rule 4a: If a job uses up an entire time slice while running, its pri-\nority is reduced (i.e., it moves down one queue).\n• Rule 4b: If a job gives up the CPU before the time slice is up, it stays\nat the same priority level.\nExample 1: A Single Long-Running Job\nLet’s look at some examples. First, we’ll look at what happens when there\nhas been a long running job in the system. Figure 8.2 shows what happens\nto this job over time in a three-queue scheduler.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n74\nSCHEDULING:\nTHE MULTI-LEVEL FEEDBACK QUEUE\nQ2\nQ1\nQ0\n0\n50\n100\n150\n200\nFigure 8.2: Long-running Job Over Time\nAs you can see in the example, the job enters at the highest priority\n(Q2). After a single time-slice of 10 ms, the scheduler reduces the job’s\npriority by one, and thus the job is on Q1. After running at Q1 for a time\nslice, the job is ﬁnally lowered to the lowest priority in the system (Q0),\nwhere it remains. Pretty simple, no?\nExample 2: Along Came A Short Job\nNow let’s look at a more complicated example, and hopefully see how\nMLFQ tries to approximate SJF. In this example, there are two jobs: A,\nwhich is a long-running CPU-intensive job, and B, which is a short-running\ninteractive job. Assume A has been running for some time, and then B ar-\nrives. What will happen? Will MLFQ approximate SJF for B?\nFigure 8.3 plots the results of this scenario. A (shown in black) is run-\nning along in the lowest-priority queue (as would any long-running CPU-\nintensive jobs); B (shown in gray) arrives at time T = 100, and thus is\nQ2\nQ1\nQ0\n0\n50\n100\n150\n200\nFigure 8.3: Along Came An Interactive Job\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nSCHEDULING:\nTHE MULTI-LEVEL FEEDBACK QUEUE\n75\nQ2\nQ1\nQ0\n0\n50\n100\n150\n200\nFigure 8.4: A Mixed I/O-intensive and CPU-intensive Workload\ninserted into the highest queue; as its run-time is short (only 20 ms), B\ncompletes before reaching the bottom queue, in two time slices; then A\nresumes running (at low priority).\nFrom this example, you can hopefully understand one of the major\ngoals of the algorithm: because it doesn’t know whether a job will be a\nshort job or a long-running job, it ﬁrst assumes it might be a short job, thus\ngiving the job high priority. If it actually is a short job, it will run quickly\nand complete; if it is not a short job, it will slowly move down the queues,\nand thus soon prove itself to be a long-running more batch-like process.\nIn this manner, MLFQ approximates SJF.\nExample 3: What About I/O?\nLet’s now look at an example with some I/O. As Rule 4b states above, if a\nprocess gives up the processor before using up its time slice, we keep it at\nthe same priority level. The intent of this rule is simple: if an interactive\njob, for example, is doing a lot of I/O (say by waiting for user input from\nthe keyboard or mouse), it will relinquish the CPU before its time slice is\ncomplete; in such case, we don’t wish to penalize the job and thus simply\nkeep it at the same level.\nFigure 8.4 shows an example of how this works, with an interactive job\nB (shown in gray) that needs the CPU only for 1 ms before performing an\nI/O competing for the CPU with a long-running batch job A (shown in\nblack). The MLFQ approach keeps B at the highest priority because B\nkeeps releasing the CPU; if B is an interactive job, MLFQ further achieves\nits goal of running interactive jobs quickly.\nProblems With Our Current MLFQ\nWe thus have a basic MLFQ. It seems to do a fairly good job, sharing the\nCPU fairly between long-running jobs, and letting short or I/O-intensive\ninteractive jobs run quickly. Unfortunately, the approach we have devel-\noped thus far contains serious ﬂaws. Can you think of any?\n(This is where you pause and think as deviously as you can)\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n76\nSCHEDULING:\nTHE MULTI-LEVEL FEEDBACK QUEUE\nQ2\nQ1\nQ0\n0\n50\n100\n150\n200\nQ2\nQ1\nQ0\n0\n50\n100\n150\n200\nFigure 8.5: Without (Left) and With (Right) Priority Boost\nFirst, there is the problem of starvation: if there are “too many” in-\nteractive jobs in the system, they will combine to consume all CPU time,\nand thus long-running jobs will never receive any CPU time (they starve).\nWe’d like to make some progress on these jobs even in this scenario.\nSecond, a smart user could rewrite their program to game the sched-\nuler. Gaming the scheduler generally refers to the idea of doing some-\nthing sneaky to trick the scheduler into giving you more than your fair\nshare of the resource. The algorithm we have described is susceptible to\nthe following attack: before the time slice is over, issue an I/O operation\n(to some ﬁle you don’t care about) and thus relinquish the CPU; doing so\nallows you to remain in the same queue, and thus gain a higher percent-\nage of CPU time. When done right (e.g., by running for 99% of a time slice\nbefore relinquishing the CPU), a job could nearly monopolize the CPU.\nFinally, a program may change its behavior over time; what was CPU-\nbound may transition to a phase of interactivity. With our current ap-\nproach, such a job would be out of luck and not be treated like the other\ninteractive jobs in the system.\n8.3\nAttempt #2: The Priority Boost\nLet’s try to change the rules and see if we can avoid the problem of\nstarvation. What could we do in order to guarantee that CPU-bound jobs\nwill make some progress (even if it is not much?).\nThe simple idea here is to periodically boost the priority of all the jobs\nin system. There are many ways to achieve this, but let’s just do some-\nthing simple: throw them all in the topmost queue; hence, a new rule:\n• Rule 5: After some time period S, move all the jobs in the system\nto the topmost queue.\nOur new rule solves two problems at once. First, processes are guar-\nanteed not to starve: by sitting in the top queue, a job will share the CPU\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nSCHEDULING:\nTHE MULTI-LEVEL FEEDBACK QUEUE\n77\nQ2\nQ1\nQ0\n0\n50\n100\n150\n200\nQ2\nQ1\nQ0\n0\n50\n100\n150\n200\nFigure 8.6: Without (Left) and With (Right) Gaming Tolerance\nwith other high-priority jobs in a round-robin fashion, and thus eventu-\nally receive service. Second, if a CPU-bound job has become interactive,\nthe scheduler treats it properly once it has received the priority boost.\nLet’s see an example. In this scenario, we just show the behavior of\na long-running job when competing for the CPU with two short-running\ninteractive jobs. Two graphs are shown in Figure 8.5. On the left, there is\nno priority boost, and thus the long-running job gets starved once the two\nshort jobs arrive; on the right, there is a priority boost every 50 ms (which\nis likely too small of a value, but used here for the example), and thus\nwe at least guarantee that the long-running job will make some progress,\ngetting boosted to the highest priority every 50 ms and thus getting to\nrun periodically.\nOf course, the addition of the time period S leads to the obvious ques-\ntion: what should S be set to? John Ousterhout, a well-regarded systems\nresearcher [O11], used to call such values in systems voo-doo constants,\nbecause they seemed to require some form of black magic to set them cor-\nrectly. Unfortunately, S has that ﬂavor. If it is set too high, long-running\njobs could starve; too low, and interactive jobs may not get a proper share\nof the CPU.\n8.4\nAttempt #3: Better Accounting\nWe now have one more problem to solve: how to prevent gaming of\nour scheduler? The real culprit here, as you might have guessed, are\nRules 4a and 4b, which let a job retain its priority by relinquishing the\nCPU before the time slice expires. So what should we do?\nThe solution here is to perform better accounting of CPU time at each\nlevel of the MLFQ. Instead of forgetting how much of a time slice a pro-\ncess used at a given level, the scheduler should keep track; once a process\nhas used its allotment, it is demoted to the next priority queue. Whether\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n78\nSCHEDULING:\nTHE MULTI-LEVEL FEEDBACK QUEUE\nQ2\nQ1\nQ0\n0\n50\n100\n150\n200\nFigure 8.7: Lower Priority, Longer Quanta\nit uses the time slice in one long burst or many small ones does not matter.\nWe thus rewrite Rules 4a and 4b to the following single rule:\n• Rule 4: Once a job uses up its time allotment at a given level (re-\ngardless of how many times it has given up the CPU), its priority is\nreduced (i.e., it moves down one queue).\nLet’s look at an example. Figure 8.6 shows what happens when a\nworkload tries to game the scheduler with the old Rules 4a and 4b (on\nthe left) as well the new anti-gaming Rule 4. Without any protection from\ngaming, a process can issue an I/O just before a time slice ends and thus\ndominate CPU time. With such protections in place, regardless of the\nI/O behavior of the process, it slowly moves down the queues, and thus\ncannot gain an unfair share of the CPU.\n8.5\nTuning MLFQ And Other Issues\nA few other issues arise with MLFQ scheduling. One big question is\nhow to parameterize such a scheduler. For example, how many queues\nshould there be? How big should the time slice be per queue? How often\nshould priority be boosted in order to avoid starvation and account for\nchanges in behavior? There are no easy answers to these questions, and\nthus only some experience with workloads and subsequent tuning of the\nscheduler will lead to a satisfactory balance.\nFor example, most MLFQ variants allow for varying time-slice length\nacross different queues. The high-priority queues are usually given short\ntime slices; they are comprised of interactive jobs, after all, and thus\nquickly alternating between them makes sense (e.g., 10 or fewer millisec-\nonds). The low-priority queues, in contrast, contain long-running jobs\nthat are CPU-bound; hence, longer time slices work well (e.g., 100s of\nms). Figure 8.7 shows an example in which two long-running jobs run\nfor 10 ms at the highest queue, 20 in the middle, and 40 at the lowest.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nSCHEDULING:\nTHE MULTI-LEVEL FEEDBACK QUEUE\n79\nTIP: AVOID VOO-DOO CONSTANTS (OUSTERHOUT’S LAW)\nAvoiding voo-doo constants is a good idea whenever possible. Unfor-\ntunately, as in the example above, it is often difﬁcult. One could try to\nmake the system learn a good value, but that too is not straightforward.\nThe frequent result: a conﬁguration ﬁle ﬁlled with default parameter val-\nues that a seasoned administrator can tweak when something isn’t quite\nworking correctly. As you can imagine, these are often left unmodiﬁed,\nand thus we are left to hope that the defaults work well in the ﬁeld. This\ntip brought to you by our old OS professor, John Ousterhout, and hence\nwe call it Ousterhout’s Law.\nThe Solaris MLFQ implementation – the Time-Sharing scheduling class,\nor TS – is particularly easy to conﬁgure; it provides a set of tables that\ndetermine exactly how the priority of a process is altered throughout its\nlifetime, how long each time slice is, and how often to boost the priority of\na job [AD00]; an administrator can muck with this table in order to make\nthe scheduler behave in different ways. Default values for the table are\n60 queues, with slowly increasing time-slice lengths from 20 milliseconds\n(highest priority) to a few hundred milliseconds (lowest), and priorities\nboosted around every 1 second or so.\nOther MLFQ schedulers don’t use a table or the exact rules described\nin this chapter; rather they adjust priorities using mathematical formu-\nlae. For example, the FreeBSD scheduler (version 4.3) uses a formula to\ncalculate the current priority level of a job, basing it on how much CPU\nthe process has used [LM+89]; in addition, usage is decayed over time,\nproviding the desired priority boost in a different manner than described\nherein. See [E95] for an excellent overview of such decay-usage algo-\nrithms and their properties.\nFinally, many schedulers have a few other features that you might en-\ncounter. For example, some schedulers reserve the highest priority levels\nfor operating system work; thus typical user jobs can never obtain the\nhighest levels of priority in the system. Some systems also allow some\nuser advice to help set priorities; for example, by using the command-line\nutility nice you can increase or decrease the priority of a job (somewhat)\nand thus increase or decrease its chances of running at any given time.\nSee the man page for more.\n8.6\nMLFQ: Summary\nWe have described a scheduling approach known as the Multi-Level\nFeedback Queue (MLFQ). Hopefully you can now see why it is called\nthat: it has multiple levels of queues, and uses feedback to determine the\npriority of a given job. History is its guide: pay attention to how jobs\nbehave over time and treat them accordingly.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n80\nSCHEDULING:\nTHE MULTI-LEVEL FEEDBACK QUEUE\nTIP: USE ADVICE WHERE POSSIBLE\nAs the operating system rarely knows what is best for each and every\nprocess of the system, it is often useful to provide interfaces to allow users\nor administrators to provide some hints to the OS. We often call such\nhints advice, as the OS need not necessarily pay attention to it, but rather\nmight take the advice into account in order to make a better decision.\nSuch hints are useful in many parts of the OS, including the scheduler\n(e.g., with nice), memory manager (e.g., madvise), and ﬁle system (e.g.,\nTIP [P+95]).\nThe reﬁned set of MLFQ rules, spread throughout the chapter, are re-\nproduced here for your viewing pleasure:\n• Rule 1: If Priority(A) > Priority(B), A runs (B doesn’t).\n• Rule 2: If Priority(A) = Priority(B), A & B run in RR.\n• Rule 3: When a job enters the system, it is placed at the highest\npriority (the topmost queue).\n• Rule 4: Once a job uses up its time allotment at a given level (re-\ngardless of how many times it has given up the CPU), its priority is\nreduced (i.e., it moves down one queue).\n• Rule 5: After some time period S, move all the jobs in the system\nto the topmost queue.\nMLFQ is interesting because instead of demanding a priori knowledge\nof the nature of a job, it instead observes the execution of a job and pri-\noritizes it accordingly. In this way, it manages to achieve the best of both\nworlds: it can deliver excellent overall performance (similar to SJF/STCF)\nfor short-running interactive jobs, and is fair and makes progress for long-\nrunning CPU-intensive workloads. For this reason, many systems, in-\ncluding BSD UNIX derivatives [LM+89, B86], Solaris [M06], and Win-\ndows NT and subsequent Windows operating systems [CS97] use a form\nof MLFQ as their base scheduler.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "page_number": 103,
      "chapter_number": 12,
      "summary": "This chapter covers segment 12 (pages 103-116). Key topics include jobs, scheduling.",
      "keywords": [
        "job",
        "multi-level feedback queue",
        "Priority",
        "time",
        "CPU",
        "MLFQ",
        "jobs",
        "CPU time",
        "feedback queue",
        "queue",
        "SCHEDULING",
        "interactive jobs",
        "system",
        "multi-level feedback",
        "time slice"
      ],
      "concepts": [
        "job",
        "jobs",
        "scheduling",
        "schedule",
        "time",
        "priority",
        "priorities",
        "run",
        "running",
        "cpu"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 7",
          "chapter": 13,
          "title": "Segment 13 (pages 116-124)",
          "relevance_score": 0.73,
          "method": "api"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 33,
          "title": "Segment 33 (pages 330-337)",
          "relevance_score": 0.68,
          "method": "api"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 29,
          "title": "Segment 29 (pages 298-305)",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 60,
          "title": "Segment 60 (pages 591-598)",
          "relevance_score": 0.63,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 1,
          "title": "Segment 1 (pages 1-15)",
          "relevance_score": 0.62,
          "method": "api"
        }
      ]
    },
    {
      "number": 13,
      "title": "Segment 13 (pages 117-125)",
      "start_page": 117,
      "end_page": 125,
      "detection_method": "topic_boundary",
      "content": "SCHEDULING:\nTHE MULTI-LEVEL FEEDBACK QUEUE\n81\nReferences\n[AD00] “Multilevel Feedback Queue Scheduling in Solaris”\nAndrea Arpaci-Dusseau\nAvailable: http://www.cs.wisc.edu/˜remzi/solaris-notes.pdf\nA great short set of notes by one of the authors on the details of the Solaris scheduler. OK, we are\nprobably biased in this description, but the notes are pretty darn good.\n[B86] “The Design of the UNIX Operating System”\nM.J. Bach\nPrentice-Hall, 1986\nOne of the classic old books on how a real UNIX operating system is built; a deﬁnite must-read for kernel\nhackers.\n[C+62] “An Experimental Time-Sharing System”\nF. J. Corbato, M. M. Daggett, R. C. Daley\nIFIPS 1962\nA bit hard to read, but the source of many of the ﬁrst ideas in multi-level feedback scheduling. Much\nof this later went into Multics, which one could argue was the most inﬂuential operating system of all\ntime.\n[CS97] “Inside Windows NT”\nHelen Custer and David A. Solomon\nMicrosoft Press, 1997\nThe NT book, if you want to learn about something other than UNIX. Of course, why would you? OK,\nwe’re kidding; you might actually work for Microsoft some day you know.\n[E95] “An Analysis of Decay-Usage Scheduling in Multiprocessors”\nD.H.J. Epema\nSIGMETRICS ’95\nA nice paper on the state of the art of scheduling back in the mid 1990s, including a good overview of\nthe basic approach behind decay-usage schedulers.\n[LM+89] “The Design and Implementation of the 4.3BSD UNIX Operating System”\nS.J. Lefﬂer, M.K. McKusick, M.J. Karels, J.S. Quarterman\nAddison-Wesley, 1989\nAnother OS classic, written by four of the main people behind BSD. The later versions of this book,\nwhile more up to date, don’t quite match the beauty of this one.\n[M06] “Solaris Internals: Solaris 10 and OpenSolaris Kernel Architecture”\nRichard McDougall\nPrentice-Hall, 2006\nA good book about Solaris and how it works.\n[O11] “John Ousterhout’s Home Page”\nJohn Ousterhout\nAvailable: http://www.stanford.edu/˜ouster/\nThe home page of the famous Professor Ousterhout. The two co-authors of this book had the pleasure of\ntaking graduate operating systems from Ousterhout while in graduate school; indeed, this is where the\ntwo co-authors got to know each other, eventually leading to marriage, kids, and even this book. Thus,\nyou really can blame Ousterhout for this entire mess you’re in.\n[P+95] “Informed Prefetching and Caching”\nR.H. Patterson, G.A. Gibson, E. Ginting, D. Stodolsky, J. Zelenka\nSOSP ’95\nA fun paper about some very cool ideas in ﬁle systems, including how applications can give the OS\nadvice about what ﬁles it is accessing and how it plans to access them.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n82\nSCHEDULING:\nTHE MULTI-LEVEL FEEDBACK QUEUE\nHomework\nThis program, mlfq.py, allows you to see how the MLFQ scheduler\npresented in this chapter behaves. See the README for details.\nQuestions\n1. Run a few randomly-generated problems with just two jobs and\ntwo queues; compute the MLFQ execution trace for each. Make\nyour life easier by limiting the length of each job and turning off\nI/Os.\n2. How would you run the scheduler to reproduce each of the exam-\nples in the chapter?\n3. How would you conﬁgure the scheduler parameters to behave just\nlike a round-robin scheduler?\n4. Craft a workload with two jobs and scheduler parameters so that\none job takes advantage of the older Rules 4a and 4b (turned on\nwith the -S ﬂag) to game the scheduler and obtain 99% of the CPU\nover a particular time interval.\n5. Given a system with a quantum length of 10 ms in its highest queue,\nhow often would you have to boost jobs back to the highest priority\nlevel (with the -B ﬂag) in order to guarantee that a single long-\nrunning (and potentially-starving) job gets at least 5% of the CPU?\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\n9\nScheduling: Proportional Share\nIn this chapter, we’ll examine a different type of scheduler known as a\nproportional-share scheduler, also sometimes referred to as a fair-share\nscheduler. Proportional-share is based around a simple concept: instead\nof optimizing for turnaround or response time, a scheduler might instead\ntry to guarantee that each job obtain a certain percentage of CPU time.\nAn excellent modern example of proportional-share scheduling is found\nin research by Waldspurger and Weihl [WW94], and is known as lottery\nscheduling; however, the idea is certainly much older [KL88]. The basic\nidea is quite simple: every so often, hold a lottery to determine which pro-\ncess should get to run next; processes that should run more often should\nbe given more chances to win the lottery. Easy, no? Now, onto the details!\nBut not before our crux:\nCRUX: HOW TO SHARE THE CPU PROPORTIONALLY\nHow can we design a scheduler to share the CPU in a proportional\nmanner? What are the key mechanisms for doing so? How effective are\nthey?\n9.1\nBasic Concept: Tickets Represent Your Share\nUnderlying lottery scheduling is one very basic concept: tickets, which\nare used to represent the share of a resource that a process (or user or\nwhatever) should receive. The percent of tickets that a process has repre-\nsents its share of the system resource in question.\nLet’s look at an example. Imagine two processes, A and B, and further\nthat A has 75 tickets while B has only 25. Thus, what we would like is for\nA to receive 75% of the CPU and B the remaining 25%.\nLottery scheduling achieves this probabilistically (but not determinis-\ntically) by holding a lottery every so often (say, every time slice). Holding\na lottery is straightforward: the scheduler must know how many total\ntickets there are (in our example, there are 100). The scheduler then picks\n83\n\n\n84\nSCHEDULING: PROPORTIONAL SHARE\nTIP: USE RANDOMNESS\nOne of the most beautiful aspects of lottery scheduling is its use of ran-\ndomness. When you have to make a decision, using such a randomized\napproach is often a robust and simple way of doing so.\nRandom approaches has at least three advantages over more traditional\ndecisions. First, random often avoids strange corner-case behaviors that\na more traditional algorithm may have trouble handling. For example,\nconsider LRU page replacement (studied in more detail in a future chap-\nter on virtual memory); while often a good replacement algorithm, LRU\nperforms pessimally for some cyclic-sequential workloads. Random, on\nthe other hand, has no such worst case.\nSecond, random also is lightweight, requiring little state to track alter-\nnatives. In a traditional fair-share scheduling algorithm, tracking how\nmuch CPU each process has received requires per-process accounting,\nwhich must be updated after running each process. Doing so randomly\nnecessitates only the most minimal of per-process state (e.g., the number\nof tickets each has).\nFinally, random can be quite fast. As long as generating a random num-\nber is quick, making the decision is also, and thus random can be used\nin a number of places where speed is required. Of course, the faster the\nneed, the more random tends towards pseudo-random.\na winning ticket, which is a number from 0 to 991. Assuming A holds\ntickets 0 through 74 and B 75 through 99, the winning ticket simply de-\ntermines whether A or B runs. The scheduler then loads the state of that\nwinning process and runs it.\nHere is an example output of a lottery scheduler’s winning tickets:\n63 85 70 39 76 17 29 41 36 39 10 99 68 83 63 62 43\n0 49 49\nHere is the resulting schedule:\nA\nB\nA\nA\nB\nA\nA\nA\nA\nA\nA\nB\nA\nB\nA\nA\nA\nA\nA\nA\nAs you can see from the example, the use of randomness in lottery\nscheduling leads to a probabilistic correctness in meeting the desired pro-\nportion, but no guarantee. In our example above, B only gets to run 4 out\nof 20 time slices (20%), instead of the desired 25% allocation. However,\nthe longer these two jobs compete, the more likely they are to achieve the\ndesired percentages.\n1Computer Scientists always start counting at 0. It is so odd to non-computer-types that\nfamous people have felt obliged to write about why we do it this way [D82].\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nSCHEDULING: PROPORTIONAL SHARE\n85\nTIP: USE TICKETS TO REPRESENT SHARES\nOne of the most powerful (and basic) mechanisms in the design of lottery\n(and stride) scheduling is that of the ticket. The ticket is used to represent\na process’s share of the CPU in these examples, but can be applied much\nmore broadly. For example, in more recent work on virtual memory man-\nagement for hypervisors, Waldspurger shows how tickets can be used to\nrepresent a guest operating system’s share of memory [W02]. Thus, if you\nare ever in need of a mechanism to represent a proportion of ownership,\nthis concept just might be ... (wait for it) ... the ticket.\n9.2\nTicket Mechanisms\nLottery scheduling also provides a number of mechanisms to manip-\nulate tickets in different and sometimes useful ways. One way is with\nthe concept of ticket currency. Currency allows a user with a set of tick-\nets to allocate tickets among their own jobs in whatever currency they\nwould like; the system then automatically converts said currency into the\ncorrect global value.\nFor example, assume users A and B have each been given 100 tickets.\nUser A is running two jobs, A1 and A2, and gives them each 500 tickets\n(out of 1000 total) in User A’s own currency. User B is running only 1 job\nand gives it 10 tickets (out of 10 total). The system will convert A1’s and\nA2’s allocation from 500 each in A’s currency to 50 each in the global cur-\nrency; similarly, B1’s 10 tickets will be converted to 100 tickets. The lottery\nwill then be held over the global ticket currency (200 total) to determine\nwhich job runs.\nUser A -> 500 (A’s currency) to A1 ->\n50 (global currency)\n-> 500 (A’s currency) to A2 ->\n50 (global currency)\nUser B ->\n10 (B’s currency) to B1 -> 100 (global currency)\nAnother useful mechanism is ticket transfer. With transfers, a process\ncan temporarily hand off its tickets to another process. This ability is\nespecially useful in a client/server setting, where a client process sends\na message to a server asking it to do some work on the client’s behalf.\nTo speed up the work, the client can pass the tickets to the server and\nthus try to maximize the performance of the server while the server is\nhandling the client’s request. When ﬁnished, the server then transfers the\ntickets back to the client and all is as before.\nFinally, ticket inﬂation can sometimes be a useful technique. With\ninﬂation, a process can temporarily raise or lower the number of tickets\nit owns. Of course, in a competitive scenario with processes that do not\ntrust one another, this makes little sense; one greedy process could give\nitself a vast number of tickets and take over the machine. Rather, inﬂation\ncan be applied in an environment where a group of processes trust one\nanother; in such a case, if any one process knows it needs more CPU time,\nit can boost its ticket value as a way to reﬂect that need to the system, all\nwithout communicating with any other processes.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n86\nSCHEDULING: PROPORTIONAL SHARE\n1\n// counter: used to track if we’ve found the winner yet\n2\nint counter\n= 0;\n3\n4\n// winner: use some call to a random number generator to\n5\n//\nget a value, between 0 and the total # of tickets\n6\nint winner\n= getrandom(0, totaltickets);\n7\n8\n// current: use this to walk through the list of jobs\n9\nnode_t *current = head;\n10\n11\n// loop until the sum of ticket values is > the winner\n12\nwhile (current) {\n13\ncounter = counter + current->tickets;\n14\nif (counter > winner)\n15\nbreak; // found the winner\n16\ncurrent = current->next;\n17\n}\n18\n// ’current’ is the winner: schedule it...\nFigure 9.1: Lottery Scheduling Decision Code\n9.3\nImplementation\nProbably the most amazing thing about lottery scheduling is the sim-\nplicity of its implementation. All you need is a good random number\ngenerator to pick the winning ticket, a data structure to track the pro-\ncesses of the system (e.g., a list), and the total number of tickets.\nLet’s assume we keep the processes in a list. Here is an example com-\nprised of three processes, A, B, and C, each with some number of tickets.\nhead\nJob:A\nTix:100\nJob:B\nTix:50\nJob:C\nTix:250\nNULL\nTo make a scheduling decision, we ﬁrst have to pick a random number\n(the winner) from the total number of tickets (400)2 Let’s say we pick the\nnumber 300. Then, we simply traverse the list, with a simple counter\nused to help us ﬁnd the winner (Figure 9.1).\nThe code walks the list of processes, adding each ticket value to counter\nuntil the value exceeds winner. Once that is the case, the current list el-\nement is the winner. With our example of the winning ticket being 300,\nthe following takes place. First, counter is incremented to 100 to ac-\ncount for A’s tickets; because 100 is less than 300, the loop continues.\nThen counter would be updated to 150 (B’s tickets), still less than 300\nand thus again we continue. Finally, counter is updated to 400 (clearly\ngreater than 300), and thus we break out of the loop with current point-\ning at C (the winner).\nTo make this process most efﬁcient, it might generally be best to or-\nganize the list in sorted order, from the highest number of tickets to the\n2Surprisingly, as pointed out by Bj¨orn Lindberg, this can be challenging to do\ncorrectly; for more details, see http://stackoverflow.com/questions/2509679/\nhow-to-generate-a-random-number-from-within-a-range.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nSCHEDULING: PROPORTIONAL SHARE\n87\n1\n10\n100\n1000\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nJob Length\nUnfairness (Average)\nFigure 9.2: Lottery Fairness Study\nlowest. The ordering does not affect the correctness of the algorithm;\nhowever, it does ensure in general that the fewest number of list itera-\ntions are taken, especially if there are a few processes that possess most\nof the tickets.\n9.4\nAn Example\nTo make the dynamics of lottery scheduling more understandable, we\nnow perform a brief study of the completion time of two jobs competing\nagainst one another, each with the same number of tickets (100) and same\nrun time (R, which we will vary).\nIn this scenario, we’d like for each job to ﬁnish at roughly the same\ntime, but due to the randomness of lottery scheduling, sometimes one\njob ﬁnishes before the other.\nTo quantify this difference, we deﬁne a\nsimple unfairness metric, U which is simply the time the ﬁrst job com-\npletes divided by the time that the second job completes. For example,\nif R = 10, and the ﬁrst job ﬁnishes at time 10 (and the second job at 20),\nU = 10\n20 = 0.5. When both jobs ﬁnish at nearly the same time, U will be\nquite close to 1. In this scenario, that is our goal: a perfectly fair scheduler\nwould achieve U = 1.\nFigure 9.2 plots the average unfairness as the length of the two jobs\n(R) is varied from 1 to 1000 over thirty trials (results are generated via the\nsimulator provided at the end of the chapter). As you can see from the\ngraph, when the job length is not very long, average unfairness can be\nquite severe. Only as the jobs run for a signiﬁcant number of time slices\ndoes the lottery scheduler approach the desired outcome.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n88\nSCHEDULING: PROPORTIONAL SHARE\n9.5\nHow To Assign Tickets?\nOne problem we have not addressed with lottery scheduling is: how\nto assign tickets to jobs? This problem is a tough one, because of course\nhow the system behaves is strongly dependent on how tickets are allo-\ncated. One approach is to assume that the users know best; in such a\ncase, each user is handed some number of tickets, and a user can allocate\ntickets to any jobs they run as desired. However, this solution is a non-\nsolution: it really doesn’t tell you what to do. Thus, given a set of jobs,\nthe “ticket-assignment problem” remains open.\n9.6\nWhy Not Deterministic?\nYou might also be wondering: why use randomness at all? As we saw\nabove, while randomness gets us a simple (and approximately correct)\nscheduler, it occasionally will not deliver the exact right proportions, es-\npecially over short time scales. For this reason, Waldspurger invented\nstride scheduling, a deterministic fair-share scheduler [W95].\nStride scheduling is also straightforward. Each job in the system has\na stride, which is inverse in proportion to the number of tickets it has. In\nour example above, with jobs A, B, and C, with 100, 50, and 250 tickets,\nrespectively, we can compute the stride of each by dividing some large\nnumber by the number of tickets each process has been assigned. For\nexample, if we divide 10,000 by each of those ticket values, we obtain\nthe following stride values for A, B, and C: 100, 200, and 40. We call\nthis value the stride of each process; every time a process runs, we will\nincrement a counter for it (called its pass value) by its stride to track its\nglobal progress.\nThe scheduler then uses the stride and pass to determine which pro-\ncess should run next. The basic idea is simple: at any given time, pick\nthe process to run that has the lowest pass value so far; when you run\na process, increment its pass counter by its stride. A pseudocode imple-\nmentation is provided by Waldspurger [W95]:\ncurrent = remove_min(queue);\n// pick client with minimum pass\nschedule(current);\n// use resource for quantum\ncurrent->pass += current->stride; // compute next pass using stride\ninsert(queue, current);\n// put back into the queue\nIn our example, we start with three processes (A, B, and C), with stride\nvalues of 100, 200, and 40, and all with pass values initially at 0. Thus, at\nﬁrst, any of the processes might run, as their pass values are equally low.\nAssume we pick A (arbitrarily; any of the processes with equal low pass\nvalues can be chosen). A runs; when ﬁnished with the time slice, we\nupdate its pass value to 100. Then we run B, whose pass value is then\nset to 200. Finally, we run C, whose pass value is incremented to 40. At\nthis point, the algorithm will pick the lowest pass value, which is C’s, and\nrun it, updating its pass to 80 (C’s stride is 40, as you recall). Then C will\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nSCHEDULING: PROPORTIONAL SHARE\n89\nPass(A)\nPass(B)\nPass(C)\nWho Runs?\n(stride=100)\n(stride=200)\n(stride=40)\n0\n0\n0\nA\n100\n0\n0\nB\n100\n200\n0\nC\n100\n200\n40\nC\n100\n200\n80\nC\n100\n200\n120\nA\n200\n200\n120\nC\n200\n200\n160\nC\n200\n200\n200\n...\nTable 9.1: Stride Scheduling: A Trace\nrun again (still the lowest pass value), raising its pass to 120. A will run\nnow, updating its pass to 200 (now equal to B’s). Then C will run twice\nmore, updating its pass to 160 then 200. At this point, all pass values are\nequal again, and the process will repeat, ad inﬁnitum. Table 9.1 traces the\nbehavior of the scheduler over time.\nAs we can see from the table, C ran ﬁve times, A twice, and B just once,\nexactly in proportion to their ticket values of 250, 100, and 50. Lottery\nscheduling achieves the proportions probabilistically over time; stride\nscheduling gets them exactly right at the end of each scheduling cycle.\nSo you might be wondering: given the precision of stride scheduling,\nwhy use lottery scheduling at all? Well, lottery scheduling has one nice\nproperty that stride scheduling does not: no global state. Imagine a new\njob enters in the middle of our stride scheduling example above; what\nshould its pass value be? Should it be set to 0? If so, it will monopolize\nthe CPU. With lottery scheduling, there is no global state per process;\nwe simply add a new process with whatever tickets it has, update the\nsingle global variable to track how many total tickets we have, and go\nfrom there. In this way, lottery makes it much easier to incorporate new\nprocesses in a sensible manner.\n9.7\nSummary\nWe have introduced the concept of proportional-share scheduling and\nbrieﬂy discussed two implementations: lottery and stride scheduling.\nLottery uses randomness in a clever way to achieve proportional share;\nstride does so deterministically. Although both are conceptually inter-\nesting, they have not achieved wide-spread adoption as CPU schedulers\nfor a variety of reasons. One is that such approaches do not particularly\nmesh well with I/O [AC97]; another is that they leave open the hard prob-\nlem of ticket assignment, i.e., how do you know how many tickets your\nbrowser should be allocated? General-purpose schedulers (such as the\nMLFQ we discussed previously, and other similar Linux schedulers) do\nso more gracefully and thus are more widely deployed.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "page_number": 117,
      "chapter_number": 13,
      "summary": "This chapter covers segment 13 (pages 117-125). Key topics include scheduling, schedule, and tickets. Bach\nPrentice-Hall, 1986\nOne of the classic old books on how a real UNIX operating system is built; a deﬁnite must-read for kernel\nhackers.",
      "keywords": [
        "lottery scheduling",
        "SCHEDULING",
        "Tickets",
        "stride scheduling",
        "lottery",
        "Proportional Share",
        "stride",
        "job",
        "UNIX Operating System",
        "number",
        "Share",
        "process",
        "time",
        "Run",
        "jobs"
      ],
      "concepts": [
        "scheduling",
        "schedule",
        "tickets",
        "jobs",
        "job",
        "time",
        "randomly",
        "randomized",
        "current",
        "processes"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 1,
          "title": "Segment 1 (pages 1-20)",
          "relevance_score": 0.63,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 2,
          "title": "Segment 2 (pages 21-38)",
          "relevance_score": 0.58,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 52,
          "title": "Segment 52 (pages 1041-1063)",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 55,
          "title": "Segment 55 (pages 1103-1105)",
          "relevance_score": 0.56,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 1",
          "chapter": 14,
          "title": "Segment 14 (pages 124-131)",
          "relevance_score": 0.56,
          "method": "api"
        }
      ]
    },
    {
      "number": 14,
      "title": "Segment 14 (pages 126-137)",
      "start_page": 126,
      "end_page": 137,
      "detection_method": "topic_boundary",
      "content": "90\nSCHEDULING: PROPORTIONAL SHARE\nAs a result, proportional-share schedulers are more useful in domains\nwhere some of these problems (such as assignment of shares) are rela-\ntively easy to solve. For example, in a virtualized data center, where you\nmight like to assign one-quarter of your CPU cycles to the Windows VM\nand the rest to your base Linux installation, proportional sharing can be\nsimple and effective. See Waldspurger [W02] for further details on how\nsuch a scheme is used to proportionally share memory in VMWare’s ESX\nServer.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nSCHEDULING: PROPORTIONAL SHARE\n91\nReferences\n[AC97] “Extending Proportional-Share Scheduling to a Network of Workstations”\nAndrea C. Arpaci-Dusseau and David E. Culler\nPDPTA’97, June 1997\nA paper by one of the authors on how to extend proportional-share scheduling to work better in a\nclustered environment.\n[D82] “Why Numbering Should Start At Zero”\nEdsger Dijkstra, August 1982\nhttp://www.cs.utexas.edu/users/EWD/ewd08xx/EWD831.PDF\nA short note from E. Dijkstra, one of the pioneers of computer science. We’ll be hearing much more\non this guy in the section on Concurrency. In the meanwhile, enjoy this note, which includes this\nmotivating quote: “One of my colleagues – not a computing scientist – accused a number of younger\ncomputing scientists of ’pedantry’ because they started numbering at zero.” The note explains why\ndoing so is logical.\n[KL88] “A Fair Share Scheduler”\nJ. Kay and P. Lauder\nCACM, Volume 31 Issue 1, January 1988\nAn early reference to a fair-share scheduler.\n[WW94] “Lottery Scheduling: Flexible Proportional-Share Resource Management”\nCarl A. Waldspurger and William E. Weihl\nOSDI ’94, November 1994\nThe landmark paper on lottery scheduling that got the systems community re-energized about schedul-\ning, fair sharing, and the power of simple randomized algorithms.\n[W95] “Lottery and Stride Scheduling: Flexible\nProportional-Share Resource Management”\nCarl A. Waldspurger\nPh.D. Thesis, MIT, 1995\nThe award-winning thesis of Waldspurger’s that outlines lottery and stride scheduling. If you’re think-\ning of writing a Ph.D. dissertation at some point, you should always have a good example around, to\ngive you something to strive for: this is such a good one.\n[W02] “Memory Resource Management in VMware ESX Server”\nCarl A. Waldspurger\nOSDI ’02, Boston, Massachusetts\nThe paper to read about memory management in VMMs (a.k.a., hypervisors). In addition to being\nrelatively easy to read, the paper contains numerous cool ideas about this new type of VMM-level\nmemory management.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n92\nSCHEDULING: PROPORTIONAL SHARE\nHomework\nThis program, lottery.py, allows you to see how a lottery scheduler\nworks. See the README for details.\nQuestions\n1. Compute the solutions for simulations with 3 jobs and random seeds\nof 1, 2, and 3.\n2. Now run with two speciﬁc jobs: each of length 10, but one (job 0)\nwith just 1 ticket and the other (job 1) with 100 (e.g., -l 10:1,10:100).\nWhat happens when the number of tickets is so imbalanced? Will\njob 0 ever run before job 1 completes? How often? In general, what\ndoes such a ticket imbalance do to the behavior of lottery schedul-\ning?\n3. When running with two jobs of length 100 and equal ticket alloca-\ntions of 100 (-l 100:100,100:100), how unfair is the scheduler?\nRun with some different random seeds to determine the (probabilis-\ntic) answer; let unfairness be determined by how much earlier one\njob ﬁnishes than the other.\n4. How does your answer to the previous question change as the quan-\ntum size (-q) gets larger?\n5. Can you make a version of the graph that is found in the chapter?\nWhat else would be worth exploring? How would the graph look\nwith a stride scheduler?\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\n10\nMultiprocessor Scheduling (Advanced)\nThis chapter will introduce the basics of multiprocessor scheduling. As\nthis topic is relatively advanced, it may be best to cover it after you have\nstudied the topic of concurrency in some detail (i.e., the second major\n“easy piece” of the book).\nAfter years of existence only in the high-end of the computing spec-\ntrum, multiprocessor systems are increasingly commonplace, and have\nfound their way into desktop machines, laptops, and even mobile de-\nvices. The rise of the multicore processor, in which multiple CPU cores\nare packed onto a single chip, is the source of this proliferation; these\nchips have become popular as computer architects have had a difﬁcult\ntime making a single CPU much faster without using (way) too much\npower. And thus we all now have a few CPUs available to us, which is a\ngood thing, right?\nOf course, there are many difﬁculties that arise with the arrival of more\nthan a single CPU. A primary one is that a typical application (i.e., some C\nprogram you wrote) only uses a single CPU; adding more CPUs does not\nmake that single application run faster. To remedy this problem, you’ll\nhave to rewrite your application to run in parallel, perhaps using threads\n(as discussed in great detail in the second piece of this book). Multi-\nthreaded applications can spread work across multiple CPUs and thus\nrun faster when given more CPU resources.\nASIDE: ADVANCED CHAPTERS\nAdvanced chapters require material from a broad swath of the book to\ntruly understand, while logically ﬁtting into a section that is earlier than\nsaid set of prerequisite materials. For example, this chapter on multipro-\ncessor scheduling makes much more sense if you’ve ﬁrst read the middle\npiece on concurrency; however, it logically ﬁts into the part of the book\non virtualization (generally) and CPU scheduling (speciﬁcally). Thus, it\nis recommended such chapters be covered out of order; in this case, after\nthe second piece of the book.\n93\n\n\n94\nMULTIPROCESSOR SCHEDULING (ADVANCED)\nMemory\nCPU\nCache\nFigure 10.1: Single CPU With Cache\nBeyond applications, a new problem that arises for the operating sys-\ntem is (not surprisingly!) that of multiprocessor scheduling. Thus far\nwe’ve discussed a number of principles behind single-processor schedul-\ning; how can we extend those ideas to work on multiple CPUs? What\nnew problems must we overcome? And thus, our problem:\nCRUX: HOW TO SCHEDULE JOBS ON MULTIPLE CPUS\nHow should the OS schedule jobs on multiple CPUs? What new prob-\nlems arise? Do the same old techniques work, or are new ideas required?\n10.1\nBackground: Multiprocessor Architecture\nTo understand the new issues surrounding multiprocessor schedul-\ning, we have to understand a new and fundamental difference between\nsingle-CPU hardware and multi-CPU hardware. This difference centers\naround the use of hardware caches (e.g., Figure 10.1), and exactly how\ndata is shared across multiple processors. We now discuss this issue fur-\nther, at a high level. Details are available elsewhere [CSG99], in particular\nin an upper-level or perhaps graduate computer architecture course.\nIn a system with a single CPU, there are a hierarchy of hardware\ncaches that in general help the processor run programs faster. Caches\nare small, fast memories that (in general) hold copies of popular data that\nis found in the main memory of the system. Main memory, in contrast,\nholds all of the data, but access to this larger memory is slower. By keep-\ning frequently accessed data in a cache, the system can make the large,\nslow memory appear to be a fast one.\nAs an example, consider a program that issues an explicit load instruc-\ntion to fetch a value from memory, and a simple system with only a single\nCPU; the CPU has a small cache (say 64 KB) and a large main memory.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nMULTIPROCESSOR SCHEDULING (ADVANCED)\n95\nMemory\nCPU\nCPU\nCache\nCache\nBus\nFigure 10.2: Two CPUs With Caches Sharing Memory\nThe ﬁrst time a program issues this load, the data resides in main mem-\nory, and thus takes a long time to fetch (perhaps in the tens of nanosec-\nonds, or even hundreds). The processor, anticipating that the data may\nbe reused, puts a copy of the loaded data into the CPU cache. If the pro-\ngram later fetches this same data item again, the CPU ﬁrst checks for it in\nthe cache; because it ﬁnds it there, the data is fetched much more quickly\n(say, just a few nanoseconds), and thus the program runs faster.\nCaches are thus based on the notion of locality, of which there are\ntwo kinds: temporal locality and spatial locality. The idea behind tem-\nporal locality is that when a piece of data is accessed, it is likely to be\naccessed again in the near future; imagine variables or even instructions\nthemselves being accessed over and over again in a loop. The idea be-\nhind spatial locality is that if a program accesses a data item at address\nx, it is likely to access data items near x as well; here, think of a program\nstreaming through an array, or instructions being executed one after the\nother. Because locality of these types exist in many programs, hardware\nsystems can make good guesses about which data to put in a cache and\nthus work well.\nNow for the tricky part: what happens when you have multiple pro-\ncessors in a single system, with a single shared main memory, as we see\nin Figure 10.2?\nAs it turns out, caching with multiple CPUs is much more compli-\ncated. Imagine, for example, that a program running on CPU 1 reads\na data item (with value D) at address A; because the data is not in the\ncache on CPU 1, the system fetches it from main memory, and gets the\nvalue D. The program then modiﬁes the value at address A, just updat-\ning its cache with the new value D′; writing the data through all the way\nto main memory is slow, so the system will (usually) do that later. Then\nassume the OS decides to stop running the program and move it to CPU\n2. The program then re-reads the value at address A; there is no such data\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n96\nMULTIPROCESSOR SCHEDULING (ADVANCED)\nCPU 2’s cache, and thus the system fetches the value from main memory,\nand gets the old value D instead of the correct value D′. Oops!\nThis general problem is called the problem of cache coherence, and\nthere is a vast research literature that describes many different subtleties\ninvolved with solving the problem [SHW11]. Here, we will skip all of the\nnuance and make some major points; take a computer architecture class\n(or three) to learn more.\nThe basic solution is provided by the hardware: by monitoring mem-\nory accesses, hardware can ensure that basically the “right thing” hap-\npens and that the view of a single shared memory is preserved. One way\nto do this on a bus-based system (as described above) is to use an old\ntechnique known as bus snooping [G83]; each cache pays attention to\nmemory updates by observing the bus that connects them to main mem-\nory. When a CPU then sees an update for a data item it holds in its cache,\nit will notice the change and either invalidate its copy (i.e., remove it\nfrom its own cache) or update it (i.e., put the new value into its cache\ntoo). Write-back caches, as hinted at above, make this more complicated\n(because the write to main memory isn’t visible until later), but you can\nimagine how the basic scheme might work.\n10.2\nDon’t Forget Synchronization\nGiven that the caches do all of this work to provide coherence, do pro-\ngrams (or the OS itself) have to worry about anything when they access\nshared data? The answer, unfortunately, is yes, and is documented in\ngreat detail in the second piece of this book on the topic of concurrency.\nWhile we won’t get into the details here, we’ll sketch/review some of the\nbasic ideas here (assuming you’re familiar with concurrency).\nWhen accessing (and in particular, updating) shared data items or\nstructures across CPUs, mutual exclusion primitives (such as locks) should\nlikely be used to guarantee correctness (other approaches, such as build-\ning lock-free data structures, are complex and only used on occasion;\nsee the chapter on deadlock in the piece on concurrency for details). For\nexample, assume we have a shared queue being accessed on multiple\nCPUs concurrently. Without locks, adding or removing elements from\nthe queue concurrently will not work as expected, even with the under-\nlying coherence protocols; one needs locks to atomically update the data\nstructure to its new state.\nTo make this more concrete, imagine this code sequence, which is used\nto remove an element from a shared linked list, as we see in Figure 10.3.\nImagine if threads on two CPUs enter this routine at the same time. If\nThread 1 executes the ﬁrst line, it will have the current value of head\nstored in its tmp variable; if Thread 2 then executes the ﬁrst line as well,\nit also will have the same value of head stored in its own private tmp\nvariable (tmp is allocated on the stack, and thus each thread will have\nits own private storage for it). Thus, instead of each thread removing\nan element from the head of the list, each thread will try to remove the\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nMULTIPROCESSOR SCHEDULING (ADVANCED)\n97\n1\ntypedef struct __Node_t {\n2\nint\nvalue;\n3\nstruct __Node_t *next;\n4\n} Node_t;\n5\n6\nint List_Pop() {\n7\nNode_t *tmp = head;\n// remember old head ...\n8\nint value\n= head->value;\n// ... and its value\n9\nhead\n= head->next;\n// advance head to next pointer\n10\nfree(tmp);\n// free old head\n11\nreturn value;\n// return value at head\n12\n}\nFigure 10.3: Simple List Delete Code\nsame head element, leading to all sorts of problems (such as an attempted\ndouble free of the head element at line 4, as well as potentially returning\nthe same data value twice).\nThe solution, of course, is to make such routines correct via lock-\ning.\nIn this case, allocating a simple mutex (e.g., pthread mutex t\nm;) and then adding a lock(&m) at the beginning of the routine and\nan unlock(&m) at the end will solve the problem, ensuring that the code\nwill execute as desired. Unfortunately, as we will see, such an approach is\nnot without problems, in particular with regards to performance. Speciﬁ-\ncally, as the number of CPUs grows, access to a synchronized shared data\nstructure becomes quite slow.\n10.3\nOne Final Issue: Cache Afﬁnity\nOne ﬁnal issue arises in building a multiprocessor cache scheduler,\nknown as cache afﬁnity. This notion is simple: a process, when run on a\nparticular CPU, builds up a fair bit of state in the caches (and TLBs) of the\nCPU. The next time the process runs, it is often advantageous to run it on\nthe same CPU, as it will run faster if some of its state is already present in\nthe caches on that CPU. If, instead, one runs a process on a different CPU\neach time, the performance of the process will be worse, as it will have to\nreload the state each time it runs (note it will run correctly on a different\nCPU thanks to the cache coherence protocols of the hardware). Thus, a\nmultiprocessor scheduler should consider cache afﬁnity when making its\nscheduling decisions, perhaps preferring to keep a process on the same\nCPU if at all possible.\n10.4\nSingle-Queue Scheduling\nWith this background in place, we now discuss how to build a sched-\nuler for a multiprocessor system. The most basic approach is to simply\nreuse the basic framework for single processor scheduling, by putting all\njobs that need to be scheduled into a single queue; we call this single-\nqueue multiprocessor scheduling or SQMS for short. This approach\nhas the advantage of simplicity; it does not require much work to take an\nexisting policy that picks the best job to run next and adapt it to work on\nmore than one CPU (where it might pick the best two jobs to run, if there\nare two CPUs, for example).\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n98\nMULTIPROCESSOR SCHEDULING (ADVANCED)\nHowever, SQMS has obvious shortcomings. The ﬁrst problem is a lack\nof scalability. To ensure the scheduler works correctly on multiple CPUs,\nthe developers will have inserted some form of locking into the code, as\ndescribed above. Locks ensure that when SQMS code accesses the single\nqueue (say, to ﬁnd the next job to run), the proper outcome arises.\nLocks, unfortunately, can greatly reduce performance, particularly as\nthe number of CPUs in the systems grows [A91]. As contention for such\na single lock increases, the system spends more and more time in lock\noverhead and less time doing the work the system should be doing (note:\nit would be great to include a real measurement of this in here someday).\nThe second main problem with SQMS is cache afﬁnity. For example,\nlet us assume we have ﬁve jobs to run (A, B, C, D, E) and four processors.\nOur scheduling queue thus looks like this:\nQueue\nA\nB\nC\nD\nE\nNULL\nOver time, assuming each job runs for a time slice and then another\njob is chosen, here is a possible job schedule across CPUs:\nCPU 3\nCPU 2\nCPU 1\nCPU 0\nD\nC\nB\nA\nE\nC\nB\nA\nE\nD\nB\nA\nE\nD\nC\nA\nE\nD\nC\nB\n ... (repeat) ...\n ... (repeat) ...\n ... (repeat) ...\n ... (repeat) ...\nBecause each CPU simply picks the next job to run from the globally-\nshared queue, each job ends up bouncing around from CPU to CPU, thus\ndoing exactly the opposite of what would make sense from the stand-\npoint of cache afﬁnity.\nTo handle this problem, most SQMS schedulers include some kind of\nafﬁnity mechanism to try to make it more likely that process will continue\nto run on the same CPU if possible. Speciﬁcally, one might provide afﬁn-\nity for some jobs, but move others around to balance load. For example,\nimagine the same ﬁve jobs scheduled as follows:\nCPU 3\nCPU 2\nCPU 1\nCPU 0\nD\nD\nD\nD\nE\nC\nC\nC\nE\nC\nB\nB\nE\nB\nB\nA\nE\nA\nA\nA\n ... (repeat) ...\n ... (repeat) ...\n ... (repeat) ...\n ... (repeat) ...\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nMULTIPROCESSOR SCHEDULING (ADVANCED)\n99\nIn this arrangement, jobs A through D are not moved across proces-\nsors, with only job E migrating from CPU to CPU, thus preserving afﬁn-\nity for most. You could then decide to migrate a different job the next\ntime through, thus achieving some kind of afﬁnity fairness as well. Im-\nplementing such a scheme, however, can be complex.\nThus, we can see the SQMS approach has its strengths and weak-\nnesses. It is straightforward to implement given an existing single-CPU\nscheduler, which by deﬁnition has only a single queue. However, it does\nnot scale well (due to synchronization overheads), and it does not readily\npreserve cache afﬁnity.\n10.5\nMulti-Queue Scheduling\nBecause of the problems caused in single-queue schedulers, some sys-\ntems opt for multiple queues, e.g., one per CPU. We call this approach\nmulti-queue multiprocessor scheduling (or MQMS).\nIn MQMS, our basic scheduling framework consists of multiple schedul-\ning queues. Each queue will likely follow a particular scheduling disci-\npline, such as round robin, though of course any algorithm can be used.\nWhen a job enters the system, it is placed on exactly one scheduling\nqueue, according to some heuristic (e.g., random, or picking one with\nfewer jobs than others). Then it is scheduled essentially independently,\nthus avoiding the problems of information sharing and synchronization\nfound in the single-queue approach.\nFor example, assume we have a system where there are just two CPUs\n(labeled CPU 0 and CPU 1), and some number of jobs enter the system:\nA, B, C, and D for example. Given that each CPU has a scheduling queue\nnow, the OS has to decide into which queue to place each job. It might do\nsomething like this:\nQ0\nA\nC\nQ1\nB\nD\nDepending on the queue scheduling policy, each CPU now has two\njobs to choose from when deciding what should run. For example, with\nround robin, the system might produce a schedule that looks like this:\nCPU 1\nCPU 0\nA\nA\nC\nC\nA\nA\nC\nC\nA\nA\nC\nC\nB\nB\nD\nD\nB\nB\nD\nD\nB\nB\nD\nD\n ... \n ... \nMQMS has a distinct advantage of SQMS in that it should be inher-\nently more scalable. As the number of CPUs grows, so too does the num-\nber of queues, and thus lock and cache contention should not become a\ncentral problem. In addition, MQMS intrinsically provides cache afﬁnity;\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n100\nMULTIPROCESSOR SCHEDULING (ADVANCED)\njobs stay on the same CPU and thus reap the advantage of reusing cached\ncontents therein.\nBut, if you’ve been paying attention, you might see that we have a new\nproblem, which is fundamental in the multi-queue based approach: load\nimbalance. Let’s assume we have the same set up as above (four jobs,\ntwo CPUs), but then one of the jobs (say C) ﬁnishes. We now have the\nfollowing scheduling queues:\nQ0\nA\nQ1\nB\nD\nIf we then run our round-robin policy on each queue of the system, we\nwill see this resulting schedule:\nCPU 1\nCPU 0\nA\nA\nA\nA\nA\nA\nA\nA\nA\nA\nA\nA\nB\nB\nD\nD\nB\nB\nD\nD\nB\nB\nD\nD\n ... \n ... \nAs you can see from this diagram, A gets twice as much CPU as B and\nD, which is not the desired outcome. Even worse, let’s imagine that both\nA and C ﬁnish, leaving just jobs B and D in the system. The scheduling\nqueues will look like this:\nQ0\nQ1\nB\nD\nAs a result, CPU 0 will be left idle! (insert dramatic and sinister music here)\nAnd hence our CPU usage timeline looks sad:\nCPU 0\nCPU 1\nB\nB\nD\nD\nB\nB\nD\nD\nB\nB\nD\nD\n ... \nSo what should a poor multi-queue multiprocessor scheduler do? How\ncan we overcome the insidious problem of load imbalance and defeat the\nevil forces of ... the Decepticons1? How do we stop asking questions that\nare hardly relevant to this otherwise wonderful book?\n1Little known fact is that the home planet of Cybertron was destroyed by bad CPU\nscheduling decisions. And now let that be the ﬁrst and last reference to Transformers in this\nbook, for which we sincerely apologize.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nMULTIPROCESSOR SCHEDULING (ADVANCED)\n101\nCRUX: HOW TO DEAL WITH LOAD IMBALANCE\nHow should a multi-queue multiprocessor scheduler handle load im-\nbalance, so as to better achieve its desired scheduling goals?\nThe obvious answer to this query is to move jobs around, a technique\nwhich we (once again) refer to as migration. By migrating a job from one\nCPU to another, true load balance can be achieved.\nLet’s look at a couple of examples to add some clarity. Once again, we\nhave a situation where one CPU is idle and the other has some jobs.\nQ0\nQ1\nB\nD\nIn this case, the desired migration is easy to understand: the OS should\nsimply move one of B or D to CPU 0. The result of this single job migra-\ntion is evenly balanced load and everyone is happy.\nA more tricky case arises in our earlier example, where A was left\nalone on CPU 0 and B and D were alternating on CPU 1:\nQ0\nA\nQ1\nB\nD\nIn this case, a single migration does not solve the problem. What\nwould you do in this case? The answer, alas, is continuous migration\nof one or more jobs. One possible solution is to keep switching jobs, as\nwe see in the following timeline. In the ﬁgure, ﬁrst A is alone on CPU 0,\nand B and D alternate on CPU 1. After a few time slices, B is moved to\ncompete with A on CPU 0, while D enjoys a few time slices alone on CPU\n1. And thus load is balanced:\nCPU 0\nCPU 1\nA\nA\nA\nA\nB\nA\nB\nA\nB\nB\nB\nB\nB\nD\nB\nD\nD\nD\nD\nD\nA\nD\nA\nD\n ... \n ... \nOf course, many other possible migration patterns exist. But now for\nthe tricky part: how should the system decide to enact such a migration?\nOne basic approach is to use a technique known as work stealing\n[FLR98]. With a work-stealing approach, a (source) queue that is low\non jobs will occasionally peek at another (target) queue, to see how full\nit is. If the target queue is (notably) more full than the source queue, the\nsource will “steal” one or more jobs from the target to help balance load.\nOf course, there is a natural tension in such an approach. If you look\naround at other queues too often, you will suffer from high overhead and\nhave trouble scaling, which was the entire purpose of implementing the\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "page_number": 126,
      "chapter_number": 14,
      "summary": "In the meanwhile, enjoy this note, which includes this\nmotivating quote: “One of my colleagues – not a computing scientist – accused a number of younger\ncomputing scientists of ’pedantry’ because they started numbering at zero.” The note explains why\ndoing so is logical Key topics include scheduling, schedule.",
      "keywords": [
        "CPU",
        "Multiprocessor Scheduling",
        "CPUs",
        "SCHEDULING",
        "single CPU",
        "multiple CPUs",
        "Cache",
        "CPU scheduling",
        "Multiprocessor",
        "jobs",
        "system",
        "data",
        "job",
        "memory",
        "single"
      ],
      "concepts": [
        "scheduling",
        "schedule",
        "cpu",
        "jobs",
        "job",
        "cache",
        "caching",
        "share",
        "sharing",
        "memory"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 27,
          "title": "Segment 27 (pages 278-289)",
          "relevance_score": 0.56,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 36,
          "title": "Segment 36 (pages 333-342)",
          "relevance_score": 0.54,
          "method": "api"
        },
        {
          "book": "More Effective C++",
          "chapter": 11,
          "title": "Segment 11 (pages 108-115)",
          "relevance_score": 0.5,
          "method": "api"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 18,
          "title": "Segment 18 (pages 168-175)",
          "relevance_score": 0.49,
          "method": "api"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 33,
          "title": "Segment 33 (pages 330-337)",
          "relevance_score": 0.49,
          "method": "api"
        }
      ]
    },
    {
      "number": 15,
      "title": "Segment 15 (pages 138-145)",
      "start_page": 138,
      "end_page": 145,
      "detection_method": "topic_boundary",
      "content": "102\nMULTIPROCESSOR SCHEDULING (ADVANCED)\nmultiple queue scheduling in the ﬁrst place! If, on the other hand, you\ndon’t look at other queues very often, you are in danger of suffering from\nsevere load balances. Finding the right threshold remains, as is common\nin system policy design, a black art.\n10.6\nLinux Multiprocessor Schedulers\nInterestingly, in the Linux community, no common solution has ap-\nproached to building a multiprocessor scheduler. Over time, three dif-\nferent schedulers arose: the O(1) scheduler, the Completely Fair Sched-\nuler (CFS), and the BF Scheduler (BFS)2. See Meehean’s dissertation for\nan excellent overview of the strengths and weaknesses of said schedulers\n[M11]; here we just summarize a few of the basics.\nBoth O(1) and CFS uses multiple queues, whereas BFS uses a single\nqueue, showing that both approaches can be successful. Of course, there\nare many other details which separate these schedulers. For example, the\nO(1) scheduler is a priority-based scheduler (similar to the MLFQ dis-\ncussed before), changing a process’s priority over time and then schedul-\ning those with highest priority in order to meet various scheduling objec-\ntives; interactivity is a particular focus. CFS, in contrast, is a deterministic\nproportional-share approach (more like Stride scheduling, as discussed\nearlier). BFS, the only single-queue approach among the three, is also\nproportional-share, but based on a more complicated scheme known as\nEarliest Eligible Virtual Deadline First (EEVDF) [SA96]. Read more about\nthese modern algorithms on your own; you should be able to understand\nhow they work now!\n10.7\nSummary\nWe have seen various approaches to multiprocessor scheduling. The\nsingle-queue approach (SQMS) is rather straightforward to build and bal-\nances load well but inherently has difﬁculty with scaling to many pro-\ncessors and cache afﬁnity. The multiple-queue approach (MQMS) scales\nbetter and handles cache afﬁnity well, but has trouble with load imbal-\nance and is more complicated. Whichever approach you take, there is no\nsimple answer: building a general purpose scheduler remains a daunting\ntask, as small code changes can lead to large behavioral differences. Only\nundertake such an exercise if you know exactly what you are doing, or,\nat least, are getting paid a large amount of money to do so.\n2Look up what BF stands for on your own; be forewarned, it is not for the faint of heart.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nMULTIPROCESSOR SCHEDULING (ADVANCED)\n103\nReferences\n[A90] “The Performance of Spin Lock Alternatives for Shared-Memory Multiprocessors”\nThomas E. Anderson\nIEEE TPDS Volume 1:1, January 1990\nA classic paper on how different locking alternatives do and don’t scale. By Tom Anderson, very well\nknown researcher in both systems and networking. And author of a very ﬁne OS textbook, we must say.\n[B+10] “An Analysis of Linux Scalability to Many Cores Abstract”\nSilas Boyd-Wickizer, Austin T. Clements, Yandong Mao, Aleksey Pesterev, M. Frans Kaashoek,\nRobert Morris, Nickolai Zeldovich\nOSDI ’10, Vancouver, Canada, October 2010\nA terriﬁc modern paper on the difﬁculties of scaling Linux to many cores.\n[CSG99] “Parallel Computer Architecture: A Hardware/Software Approach”\nDavid E. Culler, Jaswinder Pal Singh, and Anoop Gupta\nMorgan Kaufmann, 1999\nA treasure ﬁlled with details about parallel machines and algorithms. As Mark Hill humorously ob-\nserves on the jacket, the book contains more information than most research papers.\n[FLR98] “The Implementation of the Cilk-5 Multithreaded Language”\nMatteo Frigo, Charles E. Leiserson, Keith Randall\nPLDI ’98, Montreal, Canada, June 1998\nCilk is a lightweight language and runtime for writing parallel programs, and an excellent example of\nthe work-stealing paradigm.\n[G83] “Using Cache Memory To Reduce Processor-Memory Trafﬁc”\nJames R. Goodman\nISCA ’83, Stockholm, Sweden, June 1983\nThe pioneering paper on how to use bus snooping, i.e., paying attention to requests you see on the bus, to\nbuild a cache coherence protocol. Goodman’s research over many years at Wisconsin is full of cleverness,\nthis being but one example.\n[M11] “Towards Transparent CPU Scheduling”\nJoseph T. Meehean\nDoctoral Dissertation at University of Wisconsin–Madison, 2011\nA dissertation that covers a lot of the details of how modern Linux multiprocessor scheduling works.\nPretty awesome! But, as co-advisors of Joe’s, we may be a bit biased here.\n[SHW11] “A Primer on Memory Consistency and Cache Coherence”\nDaniel J. Sorin, Mark D. Hill, and David A. Wood\nSynthesis Lectures in Computer Architecture\nMorgan and Claypool Publishers, May 2011\nA deﬁnitive overview of memory consistency and multiprocessor caching. Required reading for anyone\nwho likes to know way too much about a given topic.\n[SA96] “Earliest Eligible Virtual Deadline First: A Flexible and Accurate Mechanism for Pro-\nportional Share Resource Allocation”\nIon Stoica and Hussein Abdel-Wahab\nTechnical Report TR-95-22, Old Dominion University, 1996\nA tech report on this cool scheduling idea, from Ion Stoica, now a professor at U.C. Berkeley and world\nexpert in networking, distributed systems, and many other things.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n11\nSummary Dialogue on CPU Virtualization\nProfessor: So, Student, did you learn anything?\nStudent: Well, Professor, that seems like a loaded question. I think you only\nwant me to say “yes.”\nProfessor: That’s true. But it’s also still an honest question. Come on, give a\nprofessor a break, will you?\nStudent: OK, OK. I think I did learn a few things. First, I learned a little about\nhow the OS virtualizes the CPU. There are a bunch of important mechanisms\nthat I had to understand to make sense of this: traps and trap handlers, timer\ninterrupts, and how the OS and the hardware have to carefully save and restore\nstate when switching between processes.\nProfessor: Good, good!\nStudent: All those interactions do seem a little complicated though; how can I\nlearn more?\nProfessor: Well, that’s a good question. I think there is no substitute for doing;\njust reading about these things doesn’t quite give you the proper sense. Do the\nclass projects and I bet by the end it will all kind of make sense.\nStudent: Sounds good. What else can I tell you?\nProfessor: Well, did you get some sense of the philosophy of the OS in your\nquest to understand its basic machinery?\nStudent: Hmm... I think so. It seems like the OS is fairly paranoid. It wants\nto make sure it stays in charge of the machine. While it wants a program to run\nas efﬁciently as possible (and hence the whole reasoning behind limited direct\nexecution), the OS also wants to be able to say “Ah! Not so fast my friend”\nin case of an errant or malicious process. Paranoia rules the day, and certainly\nkeeps the OS in charge of the machine. Perhaps that is why we think of the OS\nas a resource manager.\nProfessor: Yes indeed – sounds like you are starting to put it together! Nice.\nStudent: Thanks.\n105\n\n\n106\nSUMMARY DIALOGUE ON CPU VIRTUALIZATION\nProfessor: And what about the policies on top of those mechanisms – any inter-\nesting lessons there?\nStudent: Some lessons to be learned there for sure. Perhaps a little obvious, but\nobvious can be good. Like the notion of bumping short jobs to the front of the\nqueue – I knew that was a good idea ever since the one time I was buying some\ngum at the store, and the guy in front of me had a credit card that wouldn’t work.\nHe was no short job, let me tell you.\nProfessor: That sounds oddly rude to that poor fellow. What else?\nStudent: Well, that you can build a smart scheduler that tries to be like SJF and\nRR all at once – that MLFQ was pretty neat. Building up a real scheduler seems\ndifﬁcult.\nProfessor: Indeed it is. That’s why there is still controversy to this day over\nwhich scheduler to use; see the Linux battles between CFS, BFS, and the O(1)\nscheduler, for example. And no, I will not spell out the full name of BFS.\nStudent: And I won’t ask you to! These policy battles seem like they could rage\nforever; is there really a right answer?\nProfessor: Probably not. After all, even our own metrics are at odds: if your\nscheduler is good at turnaround time, it’s bad at response time, and vice versa.\nAs Lampson said, perhaps the goal isn’t to ﬁnd the best solution, but rather to\navoid disaster.\nStudent: That’s a little depressing.\nProfessor: Good engineering can be that way. And it can also be uplifting!\nIt’s just your perspective on it, really. I personally think being pragmatic is a\ngood thing, and pragmatists realize that not all problems have clean and easy\nsolutions. Anything else that caught your fancy?\nStudent: I really liked the notion of gaming the scheduler; it seems like that\nmight be something to look into when I’m next running a job on Amazon’s EC2\nservice. Maybe I can steal some cycles from some other unsuspecting (and more\nimportantly, OS-ignorant) customer!\nProfessor: It looks like I might have created a monster! Professor Frankenstein\nis not what I’d like to be called, you know.\nStudent: But isn’t that the idea? To get us excited about something, so much so\nthat we look into it on our own? Lighting ﬁres and all that?\nProfessor: I guess so. But I didn’t think it would work!\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\n12\nA Dialogue on Memory Virtualization\nStudent: So, are we done with virtualization?\nProfessor: No!\nStudent: Hey, no reason to get so excited; I was just asking a question. Students\nare supposed to do that, right?\nProfessor: Well, professors do always say that, but really they mean this: ask\nquestions, if they are good questions, and you have actually put a little thought\ninto them.\nStudent: Well, that sure takes the wind out of my sails.\nProfessor: Mission accomplished. In any case, we are not nearly done with\nvirtualization! Rather, you have just seen how to virtualize the CPU, but really\nthere is a big monster waiting in the closet: memory. Virtualizing memory is\ncomplicated and requires us to understand many more intricate details about\nhow the hardware and OS interact.\nStudent: That sounds cool. Why is it so hard?\nProfessor: Well, there are a lot of details, and you have to keep them straight\nin your head to really develop a mental model of what is going on. We’ll start\nsimple, with very basic techniques like base/bounds, and slowly add complexity\nto tackle new challenges, including fun topics like TLBs and multi-level page\ntables. Eventually, we’ll be able to describe the workings of a fully-functional\nmodern virtual memory manager.\nStudent: Neat! Any tips for the poor student, inundated with all of this infor-\nmation and generally sleep-deprived?\nProfessor: For the sleep deprivation, that’s easy: sleep more (and party less).\nFor understanding virtual memory, start with this: every address generated\nby a user program is a virtual address. The OS is just providing an illusion\nto each process, speciﬁcally that it has its own large and private memory; with\nsome hardware help, the OS will turn these pretend virtual addresses into real\nphysical addresses, and thus be able to locate the desired information.\n107\n\n\n108\nA DIALOGUE ON MEMORY VIRTUALIZATION\nStudent: OK, I think I can remember that... (to self) every address from a user\nprogram is virtual, every address from a user program is virtual, every ...\nProfessor: What are you mumbling about?\nStudent: Oh nothing.... (awkward pause) ... Anyway, why does the OS want\nto provide this illusion again?\nProfessor: Mostly ease of use: the OS will give each program the view that it\nhas a large contiguous address space to put its code and data into; thus, as a\nprogrammer, you never have to worry about things like “where should I store this\nvariable?” because the virtual address space of the program is large and has lots\nof room for that sort of thing. Life, for a programmer, becomes much more tricky\nif you have to worry about ﬁtting all of your code data into a small, crowded\nmemory.\nStudent: Why else?\nProfessor: Well, isolation and protection are big deals, too. We don’t want\none errant program to be able to read, or worse, overwrite, some other program’s\nmemory, do we?\nStudent: Probably not. Unless it’s a program written by someone you don’t\nlike.\nProfessor: Hmmm.... I think we might need to add a class on morals and ethics\nto your schedule for next semester. Perhaps OS class isn’t getting the right mes-\nsage across.\nStudent: Maybe we should. But remember, it’s not me who taught us that the\nproper OS response to errant process behavior is to kill the offending process!\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\n13\nThe Abstraction: Address Spaces\nIn the early days, building computer systems was easy. Why, you ask?\nBecause users didn’t expect much. It is those darned users with their\nexpectations of “ease of use”, “high performance”, “reliability”, etc., that\nreally have led to all these headaches. Next time you meet one of those\ncomputer users, thank them for all the problems they have caused.\n13.1\nEarly Systems\nFrom the perspective of memory, early machines didn’t provide much\nof an abstraction to users. Basically, the physical memory of the machine\nlooked something like what you see in Figure 13.1.\nThe OS was a set of routines (a library, really) that sat in memory (start-\ning at physical address 0 in this example), and there would be one run-\nning program (a process) that currently sat in physical memory (starting\nat physical address 64k in this example) and used the rest of memory.\nThere were few illusions here, and the user didn’t expect much from the\nOS. Life was sure easy for OS developers in those days, wasn’t it?\nmax\n64KB\n0KB\nCurrent Program\n(code, data, etc.)\nOperating System\n(code, data, etc.)\nFigure 13.1: Operating Systems: The Early Days\n109\n",
      "page_number": 138,
      "chapter_number": 15,
      "summary": "This chapter covers segment 15 (pages 138-145). Key topics include scheduling, schedule, and student.",
      "keywords": [
        "professor",
        "Student",
        "Memory",
        "scheduler",
        "n’t",
        "MULTIPROCESSOR SCHEDULING",
        "program",
        "Good",
        "Linux Multiprocessor Schedulers",
        "Virtual",
        "address",
        "MULTIPROCESSOR",
        "SYSTEMS",
        "Linux",
        "BFS"
      ],
      "concepts": [
        "scheduling",
        "schedule",
        "student",
        "professor",
        "memory",
        "likes",
        "liked",
        "virtual",
        "process",
        "processes"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 1,
          "title": "Segment 1 (pages 1-20)",
          "relevance_score": 0.72,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 2,
          "title": "Segment 2 (pages 21-38)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 37,
          "title": "Segment 37 (pages 363-372)",
          "relevance_score": 0.6,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 1",
          "chapter": 9,
          "title": "Segment 9 (pages 78-85)",
          "relevance_score": 0.58,
          "method": "api"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 41,
          "title": "Segment 41 (pages 830-852)",
          "relevance_score": 0.57,
          "method": "api"
        }
      ]
    },
    {
      "number": 16,
      "title": "Segment 16 (pages 146-153)",
      "start_page": 146,
      "end_page": 153,
      "detection_method": "topic_boundary",
      "content": "110\nTHE ABSTRACTION: ADDRESS SPACES\n512KB\n448KB\n384KB\n320KB\n256KB\n192KB\n128KB\n64KB\n0KB\n(free)\n(free)\n(free)\n(free)\nOperating System\n(code, data, etc.)\nProcess A\n(code, data, etc.)\nProcess B\n(code, data, etc.)\nProcess C\n(code, data, etc.)\nFigure 13.2: Three Processes: Sharing Memory\n13.2\nMultiprogramming and Time Sharing\nAfter a time, because machines were expensive, people began to share\nmachines more effectively. Thus the era of multiprogramming was born\n[DV66], in which multiple processes were ready to run at a given time,\nand the OS would switch between them, for example when one decided\nto perform an I/O. Doing so increased the effective utilization of the\nCPU. Such increases in efﬁciency were particularly important in those\ndays where each machine cost hundreds of thousands or even millions of\ndollars (and you thought your Mac was expensive!).\nSoon enough, however, people began demanding more of machines,\nand the era of time sharing was born [S59, L60, M62, M83]. Speciﬁcally,\nmany realized the limitations of batch computing, particularly on pro-\ngrammers themselves [CV65], who were tired of long (and hence ineffec-\ntive) program-debug cycles. The notion of interactivity became impor-\ntant, as many users might be concurrently using a machine, each waiting\nfor (or hoping for) a timely response from their currently-executing tasks.\nOne way to implement time sharing would be to run one process for\na short while, giving it full access to all memory (as in Figure 13.1), then\nstop it, save all of its state to some kind of disk (including all of physical\nmemory), load some other process’s state, run it for a while, and thus\nimplement some kind of crude sharing of the machine [M+63].\nUnfortunately, this approach has a big problem: it is way too slow, par-\nticularly as memory grew. While saving and restoring register-level state\n(e.g., the PC, general-purpose registers, etc.) is relatively fast, saving the\nentire contents of memory to disk is brutally non-performant. Thus, what\nwe’d rather do is leave processes in memory while switching between\nthem, allowing the OS to implement time sharing efﬁciently (Figure 13.2).\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nTHE ABSTRACTION: ADDRESS SPACES\n111\n16KB\n15KB\n2KB\n1KB\n0KB\nStack\n(free)\nHeap\nProgram Code\nthe code segment:\nwhere instructions live\nthe heap segment:\ncontains malloc’d data\ndynamic data structures\n(it grows downward)\n(it grows upward)\nthe stack segment:\ncontains local variables\narguments to routines, \nreturn values, etc.\nFigure 13.3: An Example Address Space\nIn the diagram, there are three processes (A, B, and C) and each of\nthem have a small part of the 512-KB physical memory carved out for\nthem. Assuming a single CPU, the OS chooses to run one of the processes\n(say A), while the others (B and C) sit in the ready queue waiting to run.\nAs time sharing became more popular, you can probably guess that\nnew demands were placed on the operating system. In particular, allow-\ning multiple programs to reside concurrently in memory makes protec-\ntion an important issue; you don’t want a process to be able to read, or\nworse, write some other process’s memory.\n13.3\nThe Address Space\nHowever, we have to keep those pesky users in mind, and doing so\nrequires the OS to create an easy to use abstraction of physical memory.\nWe call this abstraction the address space, and it is the running program’s\nview of memory in the system. Understanding this fundamental OS ab-\nstraction of memory is key to understanding how memory is virtualized.\nThe address space of a process contains all of the memory state of the\nrunning program. For example, the code of the program (the instruc-\ntions) have to live in memory somewhere, and thus they are in the ad-\ndress space. The program, while it is running, uses a stack to keep track\nof where it is in the function call chain as well as to allocate local variables\nand pass parameters and return values to and from routines. Finally, the\nheap is used for dynamically-allocated, user-managed memory, such as\nthat you might receive from a call to malloc() in C or new in an object-\noriented language such as C++ or Java. Of course, there are other things\nin there too (e.g., statically-initialized variables), but for now let us just\nassume those three components: code, stack, and heap.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n112\nTHE ABSTRACTION: ADDRESS SPACES\nIn the example in Figure 13.3, we have a tiny address space (only 16\nKB)1. The program code lives at the top of the address space (starting at\n0 in this example, and is packed into the ﬁrst 1K of the address space).\nCode is static (and thus easy to place in memory), so we can place it at\nthe top of the address space and know that it won’t need any more space\nas the program runs.\nNext, we have the two regions of the address space that may grow\n(and shrink) while the program runs. Those are the heap (at the top) and\nthe stack (at the bottom). We place them like this because each wishes to\nbe able to grow, and by putting them at opposite ends of the address\nspace, we can allow such growth: they just have to grow in opposite\ndirections. The heap thus starts just after the code (at 1KB) and grows\ndownward (say when a user requests more memory via malloc()); the\nstack starts at 16KB and grows upward (say when a user makes a proce-\ndure call). However, this placement of stack and heap is just a convention;\nyou could arrange the address space in a different way if you’d like (as\nwe’ll see later, when multiple threads co-exist in an address space, no\nnice way to divide the address space like this works anymore, alas).\nOf course, when we describe the address space, what we are describ-\ning is the abstraction that the OS is providing to the running program.\nThe program really isn’t in memory at physical addresses 0 through 16KB;\nrather it is loaded at some arbitrary physical address(es). Examine pro-\ncesses A, B, and C in Figure 13.2; there you can see how each process is\nloaded into memory at a different address. And hence the problem:\nTHE CRUX: HOW TO VIRTUALIZE MEMORY\nHow can the OS build this abstraction of a private, potentially large\naddress space for multiple running processes (all sharing memory) on\ntop of a single, physical memory?\nWhen the OS does this, we say the OS is virtualizing memory, because\nthe running program thinks it is loaded into memory at a particular ad-\ndress (say 0) and has a potentially very large address space (say 32-bits or\n64-bits); the reality is quite different.\nWhen, for example, process A in Figure 13.2 tries to perform a load\nat address 0 (which we will call a virtual address), somehow the OS, in\ntandem with some hardware support, will have to make sure the load\ndoesn’t actually go to physical address 0 but rather to physical address\n320KB (where A is loaded into memory). This is the key to virtualization\nof memory, which underlies every modern computer system in the world.\n1We will often use small examples like this because it is a pain to represent a 32-bit address\nspace and the numbers start to become hard to handle.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nTHE ABSTRACTION: ADDRESS SPACES\n113\nTIP: THE PRINCIPLE OF ISOLATION\nIsolation is a key principle in building reliable systems. If two entities are\nproperly isolated from one another, this implies that one can fail with-\nout affecting the other. Operating systems strive to isolate processes from\neach other and in this way prevent one from harming the other. By using\nmemory isolation, the OS further ensures that running programs cannot\naffect the operation of the underlying OS. Some modern OS’s take iso-\nlation even further, by walling off pieces of the OS from other pieces of\nthe OS. Such microkernels [BH70, R+89, S+03] thus may provide greater\nreliability than typical monolithic kernel designs.\n13.4\nGoals\nThus we arrive at the job of the OS in this set of notes: to virtualize\nmemory. The OS will not only virtualize memory, though; it will do so\nwith style. To make sure the OS does so, we need some goals to guide us.\nWe have seen these goals before (think of the Introduction), and we’ll see\nthem again, but they are certainly worth repeating.\nOne major goal of a virtual memory (VM) system is transparency2.\nThe OS should implement virtual memory in a way that is invisible to\nthe running program. Thus, the program shouldn’t be aware of the fact\nthat memory is virtualized; rather, the program behaves as if it has its\nown private physical memory. Behind the scenes, the OS (and hardware)\ndoes all the work to multiplex memory among many different jobs, and\nhence implements the illusion.\nAnother goal of VM is efﬁciency. The OS should strive to make the\nvirtualization as efﬁcient as possible, both in terms of time (i.e., not mak-\ning programs run much more slowly) and space (i.e., not using too much\nmemory for structures needed to support virtualization). In implement-\ning time-efﬁcient virtualization, the OS will have to rely on hardware\nsupport, including hardware features such as TLBs (which we will learn\nabout in due course).\nFinally, a third VM goal is protection. The OS should make sure to\nprotect processes from one another as well as the OS itself from pro-\ncesses. When one process performs a load, a store, or an instruction fetch,\nit should not be able to access or affect in any way the memory contents\nof any other process or the OS itself (that is, anything outside its address\nspace). Protection thus enables us to deliver the property of isolation\namong processes; each process should be running in its own isolated co-\ncoon, safe from the ravages of other faulty or even malicious processes.\n2This usage of transparency is sometimes confusing; some students think that “being\ntransparent” means keeping everything out in the open, i.e., what government should be like.\nHere, it means the opposite: that the illusion provided by the OS should not be visible to ap-\nplications. Thus, in common usage, a transparent system is one that is hard to notice, not one\nthat responds to requests as stipulated by the Freedom of Information Act.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n114\nTHE ABSTRACTION: ADDRESS SPACES\nASIDE: EVERY ADDRESS YOU SEE IS VIRTUAL\nEver write a C program that prints out a pointer? The value you see\n(some large number, often printed in hexadecimal), is a virtual address.\nEver wonder where the code of your program is found? You can print\nthat out too, and yes, if you can print it, it also is a virtual address. In\nfact, any address you can see as a programmer of a user-level program\nis a virtual address. It’s only the OS, through its tricky techniques of\nvirtualizing memory, that knows where in the physical memory of the\nmachine these instructions and data values lie. So never forget: if you\nprint out an address in a program, it’s a virtual one, an illusion of how\nthings are laid out in memory; only the OS (and the hardware) knows the\nreal truth.\nHere’s a little program that prints out the locations of the main() rou-\ntine (where code lives), the value of a heap-allocated value returned from\nmalloc(), and the location of an integer on the stack:\n1\n#include <stdio.h>\n2\n#include <stdlib.h>\n3\nint main(int argc, char *argv[]) {\n4\nprintf(\"location of code\n: %p\\n\", (void *) main);\n5\nprintf(\"location of heap\n: %p\\n\", (void *) malloc(1));\n6\nint x = 3;\n7\nprintf(\"location of stack : %p\\n\", (void *) &x);\n8\nreturn x;\n9\n}\nWhen run on a 64-bit Mac OS X machine, we get the following output:\nlocation of code\n: 0x1095afe50\nlocation of heap\n: 0x1096008c0\nlocation of stack : 0x7fff691aea64\nFrom this, you can see that code comes ﬁrst in the address space, then\nthe heap, and the stack is all the way at the other end of this large virtual\nspace. All of these addresses are virtual, and will be translated by the OS\nand hardware in order to fetch values from their true physical locations.\nIn the next chapters, we’ll focus our exploration on the basic mecha-\nnisms needed to virtualize memory, including hardware and operating\nsystems support. We’ll also investigate some of the more relevant poli-\ncies that you’ll encounter in operating systems, including how to manage\nfree space and which pages to kick out of memory when you run low on\nspace. In doing so, we’ll build up your understanding of how a modern\nvirtual memory system really works3.\n3Or, we’ll convince you to drop the course. But hold on; if you make it through VM, you’ll\nlikely make it all the way!\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nTHE ABSTRACTION: ADDRESS SPACES\n115\n13.5\nSummary\nWe have seen the introduction of a major OS subsystem: virtual mem-\nory. The VM system is responsible for providing the illusion of a large,\nsparse, private address space to programs, which hold all of their instruc-\ntions and data therein. The OS, with some serious hardware help, will\ntake each of these virtual memory references, and turn them into physi-\ncal addresses, which can be presented to the physical memory in order to\nfetch the desired information. The OS will do this for many processes at\nonce, making sure to protect programs from one another, as well as pro-\ntect the OS. The entire approach requires a great deal of mechanism (lots\nof low-level machinery) as well as some critical policies to work; we’ll\nstart from the bottom up, describing the critical mechanisms ﬁrst. And\nthus we proceed!\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n116\nTHE ABSTRACTION: ADDRESS SPACES\nReferences\n[BH70] “The Nucleus of a Multiprogramming System”\nPer Brinch Hansen\nCommunications of the ACM, 13:4, April 1970\nThe ﬁrst paper to suggest that the OS, or kernel, should be a minimal and ﬂexible substrate for building\ncustomized operating systems; this theme is revisited throughout OS research history.\n[CV65] “Introduction and Overview of the Multics System”\nF. J. Corbato and V. A. Vyssotsky\nFall Joint Computer Conference, 1965\nA great early Multics paper. Here is the great quote about time sharing: “The impetus for time-sharing\nﬁrst arose from professional programmers because of their constant frustration in debugging programs\nat batch processing installations. Thus, the original goal was to time-share computers to allow simulta-\nneous access by several persons while giving to each of them the illusion of having the whole machine at\nhis disposal.”\n[DV66] “Programming Semantics for Multiprogrammed Computations”\nJack B. Dennis and Earl C. Van Horn\nCommunications of the ACM, Volume 9, Number 3, March 1966\nAn early paper (but not the ﬁrst) on multiprogramming.\n[L60] “Man-Computer Symbiosis”\nJ. C. R. Licklider\nIRE Transactions on Human Factors in Electronics, HFE-1:1, March 1960\nA funky paper about how computers and people are going to enter into a symbiotic age; clearly well\nahead of its time but a fascinating read nonetheless.\n[M62] “Time-Sharing Computer Systems”\nJ. McCarthy\nManagement and the Computer of the Future, MIT Press, Cambridge, Mass, 1962\nProbably McCarthy’s earliest recorded paper on time sharing. However, in another paper [M83], he\nclaims to have been thinking of the idea since 1957. McCarthy left the systems area and went on to be-\ncome a giant in Artiﬁcial Intelligence at Stanford, including the creation of the LISP programming lan-\nguage. See McCarthy’s home page for more info: http://www-formal.stanford.edu/jmc/\n[M+63] “A Time-Sharing Debugging System for a Small Computer”\nJ. McCarthy, S. Boilen, E. Fredkin, J. C. R. Licklider\nAFIPS ’63 (Spring), May, 1963, New York, USA\nA great early example of a system that swapped program memory to the “drum” when the program\nwasn’t running, and then back into “core” memory when it was about to be run.\n[M83] “Reminiscences on the History of Time Sharing”\nJohn McCarthy\nWinter or Spring of 1983\nAvailable: http://www-formal.stanford.edu/jmc/history/timesharing/timesharing.html\nA terriﬁc historical note on where the idea of time-sharing might have come from, including some doubts\ntowards those who cite Strachey’s work [S59] as the pioneering work in this area.\n[R+89] “Mach: A System Software kernel”\nRichard Rashid, Daniel Julin, Douglas Orr, Richard Sanzi, Robert Baron, Alessandro Forin,\nDavid Golub, Michael Jones\nCOMPCON 89, February 1989\nAlthough not the ﬁrst project on microkernels per se, the Mach project at CMU was well-known and\ninﬂuential; it still lives today deep in the bowels of Mac OS X.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nTHE ABSTRACTION: ADDRESS SPACES\n117\n[S59] “Time Sharing in Large Fast Computers”\nC. Strachey\nProceedings of the International Conference on Information Processing, UNESCO, June 1959\nOne of the earliest references on time sharing.\n[S+03] “Improving the Reliability of Commodity Operating Systems”\nMichael M. Swift, Brian N. Bershad, Henry M. Levy\nSOSP 2003\nThe ﬁrst paper to show how microkernel-like thinking can improve operating system reliability.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "page_number": 146,
      "chapter_number": 16,
      "summary": "This chapter covers segment 16 (pages 146-153). Key topics include program, programming, and memory. Thus the era of multiprogramming was born\n[DV66], in which multiple processes were ready to run at a given time,\nand the OS would switch between them, for example when one decided\nto perform an I/O.",
      "keywords": [
        "Address Space",
        "ADDRESS",
        "Memory",
        "Space",
        "large address space",
        "Time Sharing",
        "Program",
        "physical memory",
        "virtual address",
        "System",
        "Operating systems",
        "ABSTRACTION",
        "code",
        "Time",
        "virtual memory"
      ],
      "concepts": [
        "program",
        "programming",
        "memory",
        "process",
        "processes",
        "processing",
        "spaces",
        "address",
        "addresses",
        "code"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 1,
          "title": "Segment 1 (pages 1-20)",
          "relevance_score": 0.72,
          "method": "api"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 37,
          "title": "Segment 37 (pages 363-372)",
          "relevance_score": 0.68,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 55,
          "title": "Segment 55 (pages 1103-1105)",
          "relevance_score": 0.68,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 26,
          "title": "Segment 26 (pages 249-257)",
          "relevance_score": 0.68,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 43,
          "title": "Segment 43 (pages 858-879)",
          "relevance_score": 0.66,
          "method": "api"
        }
      ]
    },
    {
      "number": 17,
      "title": "Segment 17 (pages 154-161)",
      "start_page": 154,
      "end_page": 161,
      "detection_method": "topic_boundary",
      "content": "14\nInterlude: Memory API\nIn this interlude, we discuss the memory allocation interfaces in UNIX\nsystems. The interfaces provided are quite simple, and hence the chapter\nis short and to the point1. The main problem we address is this:\nCRUX: HOW TO ALLOCATE AND MANAGE MEMORY\nIn UNIX/C programs, understanding how to allocate and manage\nmemory is critical in building robust and reliable software. What inter-\nfaces are commonly used? What mistakes should be avoided?\n14.1\nTypes of Memory\nIn running a C program, there are two types of memory that are allo-\ncated. The ﬁrst is called stack memory, and allocations and deallocations\nof it are managed implicitly by the compiler for you, the programmer; for\nthis reason it is sometimes called automatic memory.\nDeclaring memory on the stack in C is easy. For example, let’s say you\nneed some space in a function func() for an integer, called x. To declare\nsuch a piece of memory, you just do something like this:\nvoid func() {\nint x; // declares an integer on the stack\n...\n}\nThe compiler does the rest, making sure to make space on the stack\nwhen you call into func(). When your return from the function, the\ncompiler deallocates the memory for you; thus, if you want some infor-\nmation to live beyond the call invocation, you had better not leave that\ninformation on the stack.\nIt is this need for long-lived memory that gets us to the second type\nof memory, called heap memory, where all allocations and deallocations\n1Indeed, we hope all chapters are! But this one is shorter and pointier, we think.\n119\n\n\n120\nINTERLUDE: MEMORY API\nare explicitly handled by you, the programmer. A heavy responsibility,\nno doubt! And certainly the cause of many bugs. But if you are careful\nand pay attention, you will use such interfaces correctly and without too\nmuch trouble. Here is an example of how one might allocate a pointer to\nan integer on the heap:\nvoid func() {\nint *x = (int *) malloc(sizeof(int));\n...\n}\nA couple of notes about this small code snippet. First, you might no-\ntice that both stack and heap allocation occur on this line: ﬁrst the com-\npiler knows to make room for a pointer to an integer when it sees your\ndeclaration of said pointer (int *x); subsequently, when the program\ncalls malloc(), it requests space for an integer on the heap; the routine\nreturns the address of such an integer (upon success, or NULL on failure),\nwhich is then stored on the stack for use by the program.\nBecause of its explicit nature, and because of its more varied usage,\nheap memory presents more challenges to both users and systems. Thus,\nit is the focus of the remainder of our discussion.\n14.2\nThe malloc() Call\nThe malloc() call is quite simple: you pass it a size asking for some\nroom on the heap, and it either succeeds and gives you back a pointer to\nthe newly-allocated space, or fails and returns NULL2.\nThe manual page shows what you need to do to use malloc; type man\nmalloc at the command line and you will see:\n#include <stdlib.h>\n...\nvoid *malloc(size_t size);\nFrom this information, you can see that all you need to do is include\nthe header ﬁle stdlib.h to use malloc. In fact, you don’t really need to\neven do this, as the C library, which all C programs link with by default,\nhas the code for malloc() inside of it; adding the header just lets the\ncompiler check whether you are calling malloc() correctly (e.g., passing\nthe right number of arguments to it, of the right type).\nThe single parameter malloc() takes is of type size t which sim-\nply describes how many bytes you need. However, most programmers\ndo not type in a number here directly (such as 10); indeed, it would be\nconsidered poor form to do so. Instead, various routines and macros are\nutilized. For example, to allocate space for a double-precision ﬂoating\npoint value, you simply do this:\ndouble *d = (double *) malloc(sizeof(double));\n2Note that NULL in C isn’t really anything special at all, just a macro for the value zero.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nINTERLUDE: MEMORY API\n121\nTIP: WHEN IN DOUBT, TRY IT OUT\nIf you aren’t sure how some routine or operator you are using behaves,\nthere is no substitute for simply trying it out and making sure it behaves\nas you expect. While reading the manual pages or other documentation\nis useful, how it works in practice is what matters. Write some code and\ntest it! That is no doubt the best way to make sure your code behaves as\nyou desire. Indeed, that is what we did to double-check the things we\nwere saying about sizeof() were actually true!\nWow, that’s lot of double-ing! This invocation of malloc() uses the\nsizeof() operator to request the right amount of space; in C, this is\ngenerally thought of as a compile-time operator, meaning that the actual\nsize is known at compile time and thus a number (in this case, 8, for a\ndouble) is substituted as the argument to malloc(). For this reason,\nsizeof() is correctly thought of as an operator and not a function call\n(a function call would take place at run time).\nYou can also pass in the name of a variable (and not just a type) to\nsizeof(), but in some cases you may not get the desired results, so be\ncareful. For example, let’s look at the following code snippet:\nint *x = malloc(10 * sizeof(int));\nprintf(\"%d\\n\", sizeof(x));\nIn the ﬁrst line, we’ve declared space for an array of 10 integers, which\nis ﬁne and dandy. However, when we use sizeof() in the next line,\nit returns a small value, such as 4 (on 32-bit machines) or 8 (on 64-bit\nmachines). The reason is that in this case, sizeof() thinks we are sim-\nply asking how big a pointer to an integer is, not how much memory we\nhave dynamically allocated. However, sometimes sizeof() does work\nas you might expect:\nint x[10];\nprintf(\"%d\\n\", sizeof(x));\nIn this case, there is enough static information for the compiler to\nknow that 40 bytes have been allocated.\nAnother place to be careful is with strings. When declaring space for a\nstring, use the following idiom: malloc(strlen(s) + 1), which gets\nthe length of the string using the function strlen(), and adds 1 to it\nin order to make room for the end-of-string character. Using sizeof()\nmay lead to trouble here.\nYou might also notice that malloc() returns a pointer to type void.\nDoing so is just the way in C to pass back an address and let the pro-\ngrammer decide what to do with it. The programmer further helps out\nby using what is called a cast; in our example above, the programmer\ncasts the return type of malloc() to a pointer to a double. Casting\ndoesn’t really accomplish anything, other than tell the compiler and other\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n122\nINTERLUDE: MEMORY API\nprogrammers who might be reading your code: “yeah, I know what I’m\ndoing.” By casting the result of malloc(), the programmer is just giving\nsome reassurance; the cast is not needed for the correctness.\n14.3\nThe free() Call\nAs it turns out, allocating memory is the easy part of the equation;\nknowing when, how, and even if to free memory is the hard part. To free\nheap memory that is no longer in use, programmers simply call free():\nint *x = malloc(10 * sizeof(int));\n...\nfree(x);\nThe routine takes one argument, a pointer that was returned by malloc().\nThus, you might notice, the size of the allocated region is not passed in\nby the user, and must be tracked by the memory-allocation library itself.\n14.4\nCommon Errors\nThere are a number of common errors that arise in the use of malloc()\nand free(). Here are some we’ve seen over and over again in teaching\nthe undergraduate operating systems course. All of these examples com-\npile and run with nary a peep from the compiler; while compiling a C\nprogram is necessary to build a correct C program, it is far from sufﬁ-\ncient, as you will learn (often in the hard way).\nCorrect memory management has been such a problem, in fact, that\nmany newer languages have support for automatic memory manage-\nment. In such languages, while you call something akin to malloc()\nto allocate memory (usually new or something similar to allocate a new\nobject), you never have to call something to free space; rather, a garbage\ncollector runs and ﬁgures out what memory you no longer have refer-\nences to and frees it for you.\nForgetting To Allocate Memory\nMany routines expect memory to be allocated before you call them. For\nexample, the routine strcpy(dst, src) copies a string from a source\npointer to a destination pointer. However, if you are not careful, you\nmight do this:\nchar *src = \"hello\";\nchar *dst;\n// oops! unallocated\nstrcpy(dst, src); // segfault and die\nWhen you run this code, it will likely lead to a segmentation fault3,\nwhich is a fancy term for YOU DID SOMETHING WRONG WITH\nMEMORY YOU FOOLISH PROGRAMMER AND I AM ANGRY.\n3Although it sounds arcane, you will soon learn why such an illegal memory access is\ncalled a segmentation fault; if that isn’t incentive to read on, what is?\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nINTERLUDE: MEMORY API\n123\nTIP: IT COMPILED OR IT RAN ̸= IT IS CORRECT\nJust because a program compiled(!) or even ran once or many times cor-\nrectly does not mean the program is correct. Many events may have con-\nspired to get you to a point where you believe it works, but then some-\nthing changes and it stops. A common student reaction is to say (or yell)\n“But it worked before!” and then blame the compiler, operating system,\nhardware, or even (dare we say it) the professor. But the problem is usu-\nally right where you think it would be, in your code. Get to work and\ndebug it before you blame those other components.\nIn this case, the proper code might instead look like this:\nchar *src = \"hello\";\nchar *dst = (char *) malloc(strlen(src) + 1);\nstrcpy(dst, src); // work properly\nAlternately, you could use strdup() and make your life even easier.\nRead the strdup man page for more information.\nNot Allocating Enough Memory\nA related error is not allocating enough memory, sometimes called a buffer\noverﬂow. In the example above, a common error is to make almost enough\nroom for the destination buffer.\nchar *src = \"hello\";\nchar *dst = (char *) malloc(strlen(src)); // too small!\nstrcpy(dst, src); // work properly\nOddly enough, depending on how malloc is implemented and many\nother details, this program will often run seemingly correctly. In some\ncases, when the string copy executes, it writes one byte too far past the\nend of the allocated space, but in some cases this is harmless, perhaps\noverwriting a variable that isn’t used anymore. In some cases, these over-\nﬂows can be incredibly harmful, and in fact are the source of many secu-\nrity vulnerabilities in systems [W06]. In other cases, the malloc library\nallocated a little extra space anyhow, and thus your program actually\ndoesn’t scribble on some other variable’s value and works quite ﬁne. In\neven other cases, the program will indeed fault and crash. And thus we\nlearn another valuable lesson: even though it ran correctly once, doesn’t\nmean it’s correct.\nForgetting to Initialize Allocated Memory\nWith this error, you call malloc() properly, but forget to ﬁll in some val-\nues into your newly-allocated data type. Don’t do this! If you do forget,\nyour program will eventually encounter an uninitialized read, where it\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n124\nINTERLUDE: MEMORY API\nreads from the heap some data of unknown value. Who knows what\nmight be in there? If you’re lucky, some value such that the program still\nworks (e.g., zero). If you’re not lucky, something random and harmful.\nForgetting To Free Memory\nAnother common error is known as a memory leak, and it occurs when\nyou forget to free memory. In long-running applications or systems (such\nas the OS itself), this is a huge problem, as slowly leaking memory even-\ntually leads one to run out of memory, at which point a restart is required.\nThus, in general, when you are done with a chunk of memory, you should\nmake sure to free it. Note that using a garbage-collected language doesn’t\nhelp here: if you still have a reference to some chunk of memory, no\ngarbage collector will ever free it, and thus memory leaks remain a prob-\nlem even in more modern languages.\nNote that not all memory need be freed, at least, in certain cases. For\nexample, when you write a short-lived program, you might allocate some\nspace using malloc(). The program runs and is about to complete: is\nthere need to call free() a bunch of times just before exiting? While\nit seems wrong not to, it is in this case quite ﬁne to simply exit. After\nall, when your program exits, the OS will clean up everything about this\nprocess, including any memory it has allocated. Calling free() a bunch\nof times and then exiting is thus pointless, and, if you do so incorrectly,\nwill cause the program to crash. Just call exit and be happy instead.\nFreeing Memory Before You Are Done With It\nSometimes a program will free memory before it is ﬁnished using it; such\na mistake is called a dangling pointer, and it, as you can guess, is also a\nbad thing. The subsequent use can crash the program, or overwrite valid\nmemory (e.g., you called free(), but then called malloc() again to\nallocate something else, which then recycles the errantly-freed memory).\nFreeing Memory Repeatedly\nPrograms also sometimes free memory more than once; this is known as\nthe double free. The result of doing so is undeﬁned. As you can imag-\nine, the memory-allocation library might get confused and do all sorts of\nweird things; crashes are a common outcome.\nCalling free() Incorrectly\nOne last problem we discuss is the call of free() incorrectly. After all,\nfree() expects you only to pass to it one of the pointers you received\nfrom malloc() earlier. When you pass in some other value, bad things\ncan (and do) happen. Thus, such invalid frees are dangerous and of\ncourse should also be avoided.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nINTERLUDE: MEMORY API\n125\nSummary\nAs you can see, there are lots of ways to abuse memory. Because of fre-\nquent errors with memory, a whole ecosphere of tools have developed to\nhelp ﬁnd such problems in your code. Check out both purify [HJ92] and\nvalgrind [SN05]; both are excellent at helping you locate the source of\nyour memory-related problems. Once you become accustomed to using\nthese powerful tools, you will wonder how you survived without them.\n14.5\nUnderlying OS Support\nYou might have noticed that we haven’t been talking about system\ncalls when discussing malloc() and free(). The reason for this is sim-\nple: they are not system calls, but rather library calls. Thus the malloc li-\nbrary manages space within your virtual address space, but itself is built\non top of some system calls which call into the OS to ask for more mem-\nory or release some back to the system.\nOne such system call is called brk, which is used to change the loca-\ntion of the program’s break: the location of the end of the heap. It takes\none argument (the address of the new break), and thus either increases or\ndecreases the size of the heap based on whether the new break is larger\nor smaller than the current break. An additional call sbrk is passed an\nincrement but otherwise serves a similar purpose.\nNote that you should never directly call either brk or sbrk. They\nare used by the memory-allocation library; if you try to use them, you\nwill likely make something go (horribly) wrong. Stick to malloc() and\nfree() instead.\nFinally, you can also obtain memory from the operating system via the\nmmap() call. By passing in the correct arguments, mmap() can create an\nanonymous memory region within your program – a region which is not\nassociated with any particular ﬁle but rather with swap space, something\nwe’ll discuss in detail later on in virtual memory. This memory can then\nalso be treated like a heap and managed as such. Read the manual page\nof mmap() for more details.\n14.6\nOther Calls\nThere are a few other calls that the memory-allocation library sup-\nports. For example, calloc() allocates memory and also zeroes it be-\nfore returning; this prevents some errors where you assume that memory\nis zeroed and forget to initialize it yourself (see the paragraph on “unini-\ntialized reads” above). The routine realloc() can also be useful, when\nyou’ve allocated space for something (say, an array), and then need to\nadd something to it: realloc() makes a new larger region of memory,\ncopies the old region into it, and returns the pointer to the new region.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "page_number": 154,
      "chapter_number": 17,
      "summary": "This chapter covers segment 17 (pages 154-161). Key topics include memory, allocation, and allocate. The main problem we address is this:\nCRUX: HOW TO ALLOCATE AND MANAGE MEMORY\nIn UNIX/C programs, understanding how to allocate and manage\nmemory is critical in building robust and reliable software.",
      "keywords": [
        "Memory",
        "malloc",
        "Memory API",
        "free",
        "call",
        "free memory",
        "program",
        "space",
        "sizeof",
        "pointer",
        "Interlude",
        "heap memory",
        "API",
        "heap",
        "n’t"
      ],
      "concepts": [
        "memory",
        "allocation",
        "allocate",
        "allocations",
        "allocated",
        "called",
        "correctly",
        "free",
        "programs",
        "space"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 7",
          "chapter": 6,
          "title": "Segment 6 (pages 48-56)",
          "relevance_score": 0.74,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 1,
          "title": "Segment 1 (pages 1-20)",
          "relevance_score": 0.7,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 43,
          "title": "Segment 43 (pages 858-879)",
          "relevance_score": 0.7,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 44,
          "title": "Segment 44 (pages 880-902)",
          "relevance_score": 0.66,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 8,
          "title": "Segment 8 (pages 63-70)",
          "relevance_score": 0.66,
          "method": "api"
        }
      ]
    },
    {
      "number": 18,
      "title": "Segment 18 (pages 162-173)",
      "start_page": 162,
      "end_page": 173,
      "detection_method": "topic_boundary",
      "content": "126\nINTERLUDE: MEMORY API\n14.7\nSummary\nWe have introduced some of the APIs dealing with memory allocation.\nAs always, we have just covered the basics; more details are available\nelsewhere. Read the C book [KR88] and Stevens [SR05] (Chapter 7) for\nmore information. For a cool modern paper on how to detect and correct\nmany of these problems automatically, see Novark et al. [N+07]; this\npaper also contains a nice summary of common problems and some neat\nideas on how to ﬁnd and ﬁx them.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nINTERLUDE: MEMORY API\n127\nReferences\n[HJ92] Purify: Fast Detection of Memory Leaks and Access Errors\nR. Hastings and B. Joyce\nUSENIX Winter ’92\nThe paper behind the cool Purify tool, now a commercial product.\n[KR88] “The C Programming Language”\nBrian Kernighan and Dennis Ritchie\nPrentice-Hall 1988\nThe C book, by the developers of C. Read it once, do some programming, then read it again, and then\nkeep it near your desk or wherever you program.\n[N+07] “Exterminator: Automatically Correcting Memory Errors with High Probability”\nGene Novark, Emery D. Berger, and Benjamin G. Zorn\nPLDI 2007\nA cool paper on ﬁnding and correcting memory errors automatically, and a great overview of many\ncommon errors in C and C++ programs.\n[SN05] “Using Valgrind to Detect Undeﬁned Value Errors with Bit-precision”\nJ. Seward and N. Nethercote\nUSENIX ’05\nHow to use valgrind to ﬁnd certain types of errors.\n[SR05] “Advanced Programming in the UNIX Environment”\nW. Richard Stevens and Stephen A. Rago\nAddison-Wesley, 2005\nWe’ve said it before, we’ll say it again: read this book many times and use it as a reference whenever you\nare in doubt. The authors are always surprised at how each time they read something in this book, they\nlearn something new, even after many years of C programming.\n[W06] “Survey on Buffer Overﬂow Attacks and Countermeasures”\nTim Werthman\nAvailable: www.nds.rub.de/lehre/seminar/SS06/Werthmann BufferOverﬂow.pdf\nA nice survey of buffer overﬂows and some of the security problems they cause. Refers to many of the\nfamous exploits.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n15\nMechanism: Address Translation\nIn developing the virtualization of the CPU, we focused on a general\nmechanism known as limited direct execution (or LDE). The idea be-\nhind LDE is simple: for the most part, let the program run directly on the\nhardware; however, at certain key points in time (such as when a process\nissues a system call, or a timer interrupt occurs), arrange so that the OS\ngets involved and makes sure the “right” thing happens. Thus, the OS,\nwith a little hardware support, tries its best to get out of the way of the\nrunning program, to deliver an efﬁcient virtualization; however, by inter-\nposing at those critical points in time, the OS ensures that it maintains\ncontrol over the hardware. Efﬁciency and control together are two of the\nmain goals of any modern operating system.\nIn virtualizing memory, we will pursue a similar strategy, attaining\nboth efﬁciency and control while providing the desired virtualization. Ef-\nﬁciency dictates that we make use of hardware support, which at ﬁrst\nwill be quite rudimentary (e.g., just a few registers) but will grow to be\nfairly complex (e.g., TLBs, page-table support, and so forth, as you will\nsee). Control implies that the OS ensures that no application is allowed\nto access any memory but its own; thus, to protect applications from one\nanother, and the OS from applications, we will need help from the hard-\nware here too. Finally, we will need a little more from the VM system, in\nterms of ﬂexibility; speciﬁcally, we’d like for programs to be able to use\ntheir address spaces in whatever way they would like, thus making the\nsystem easier to program. And thus arrive at the reﬁned crux:\nTHE CRUX:\nHOW TO EFFICIENTLY AND FLEXIBLY VIRTUALIZE MEMORY\nHow can we build an efﬁcient virtualization of memory? How do\nwe provide the ﬂexibility needed by applications? How do we maintain\ncontrol over which memory locations an application can access, and thus\nensure that application memory accesses are properly restricted? How\ndo we do all of this efﬁciently?\n129\n\n\n130\nMECHANISM: ADDRESS TRANSLATION\nThe generic technique we will use, which you can consider an addition\nto our general approach of limited direct execution, is something that is\nreferred to as hardware-based address translation, or just address trans-\nlation for short. With address translation, the hardware transforms each\nmemory access (e.g., an instruction fetch, load, or store), changing the vir-\ntual address provided by the instruction to a physical address where the\ndesired information is actually located. Thus, on each and every memory\nreference, an address translation is performed by the hardware to redirect\napplication memory references to their actual locations in memory.\nOf course, the hardware alone cannot virtualize memory, as it just pro-\nvides the low-level mechanism for doing so efﬁciently. The OS must get\ninvolved at key points to set up the hardware so that the correct trans-\nlations take place; it must thus manage memory, keeping track of which\nlocations are free and which are in use, and judiciously intervening to\nmaintain control over how memory is used.\nOnce again the goal of all of this work is to create a beautiful illu-\nsion: that the program has its own private memory, where its own code\nand data reside. Behind that virtual reality lies the ugly physical truth:\nthat many programs are actually sharing memory at the same time, as\nthe CPU (or CPUs) switches between running one program and the next.\nThrough virtualization, the OS (with the hardware’s help) turns the ugly\nmachine reality into something that is a useful, powerful, and easy to use\nabstraction.\n15.1\nAssumptions\nOur ﬁrst attempts at virtualizing memory will be very simple, almost\nlaughably so. Go ahead, laugh all you want; pretty soon it will be the OS\nlaughing at you, when you try to understand the ins and outs of TLBs,\nmulti-level page tables, and other technical wonders. Don’t like the idea\nof the OS laughing at you? Well, you may be out of luck then; that’s just\nhow the OS rolls.\nSpeciﬁcally, we will assume for now that the user’s address space must\nbe placed contiguously in physical memory. We will also assume, for sim-\nplicity, that the size of the address space is not too big; speciﬁcally, that\nit is less than the size of physical memory. Finally, we will also assume that\neach address space is exactly the same size. Don’t worry if these assump-\ntions sound unrealistic; we will relax them as we go, thus achieving a\nrealistic virtualization of memory.\n15.2\nAn Example\nTo understand better what we need to do to implement address trans-\nlation, and why we need such a mechanism, let’s look at a simple exam-\nple. Imagine there is a process whose address space as indicated in Figure\n15.1. What we are going to examine here is a short code sequence that\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nMECHANISM: ADDRESS TRANSLATION\n131\nTIP: INTERPOSITION IS POWERFUL\nInterposition is a generic and powerful technique that is often used to\ngreat effect in computer systems. In virtualizing memory, the hardware\nwill interpose on each memory access, and translate each virtual address\nissued by the process to a physical address where the desired informa-\ntion is actually stored. However, the general technique of interposition is\nmuch more broadly applicable; indeed, almost any well-deﬁned interface\ncan be interposed upon, to add new functionality or improve some other\naspect of the system. One of the usual beneﬁts of such an approach is\ntransparency; the interposition often is done without changing the client\nof the interface, thus requiring no changes to said client.\nloads a value from memory, increments it by three, and then stores the\nvalue back into memory. You can imagine the C-language representation\nof this code might look like this:\nvoid func()\nint x;\n...\nx = x + 3; // this is the line of code we are interested in\nThe compiler turns this line of code into assembly, which might look\nsomething like this (in x86 assembly). Use objdump on Linux or otool\non Mac OS X to disassemble it:\n128: movl 0x0(%ebx), %eax\n;load 0+ebx into eax\n132: addl $0x03, %eax\n;add 3 to eax register\n135: movl %eax, 0x0(%ebx)\n;store eax back to mem\nThis code snippet is relatively straightforward; it presumes that the\naddress of x has been placed in the register ebx, and then loads the value\nat that address into the general-purpose register eax using the movl in-\nstruction (for “longword” move). The next instruction adds 3 to eax,\nand the ﬁnal instruction stores the value in eax back into memory at that\nsame location.\nIn Figure 15.1, you can see how both the code and data are laid out in\nthe process’s address space; the three-instruction code sequence is located\nat address 128 (in the code section near the top), and the value of the\nvariable x at address 15 KB (in the stack near the bottom). In the ﬁgure,\nthe initial value of x is 3000, as shown in its location on the stack.\nWhen these instructions run, from the perspective of the process, the\nfollowing memory accesses take place.\n• Fetch instruction at address 128\n• Execute this instruction (load from address 15 KB)\n• Fetch instruction at address 132\n• Execute this instruction (no memory reference)\n• Fetch the instruction at address 135\n• Execute this instruction (store to address 15 KB)\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n132\nMECHANISM: ADDRESS TRANSLATION\n16KB\n15KB\n14KB\n4KB\n3KB\n2KB\n1KB\n0KB\nStack\n(free)\nHeap\nProgram Code\n128\n132\n135\nmovl 0x0(%ebx),%eax\naddl 0x03, %eax\nmovl %eax,0x0(%ebx)\n3000\nFigure 15.1: A Process And Its Address Space\nFrom the program’s perspective, its address space starts at address 0\nand grows to a maximum of 16 KB; all memory references it generates\nshould be within these bounds. However, to virtualize memory, the OS\nwants to place the process somewhere else in physical memory, not nec-\nessarily at address 0. Thus, we have the problem: how can we relocate\nthis process in memory in a way that is transparent to the process? How\ncan provide the illusion of a virtual address space starting at 0, when in\nreality the address space is located at some other physical address?\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nMECHANISM: ADDRESS TRANSLATION\n133\n64KB\n48KB\n32KB\n16KB\n0KB\n(not in use)\n(not in use)\nOperating System\nStack\nCode\nHeap\n(allocated but not in use)\nRelocated Process\nFigure 15.2: Physical Memory with a Single Relocated Process\nAn example of what physical memory might look like once this pro-\ncess’s address space has been placed in memory is found in Figure 15.2.\nIn the ﬁgure, you can see the OS using the ﬁrst slot of physical memory\nfor itself, and that it has relocated the process from the example above\ninto the slot starting at physical memory address 32 KB. The other two\nslots are free (16 KB-32 KB and 48 KB-64 KB).\n15.3\nDynamic (Hardware-based) Relocation\nTo gain some understanding of hardware-based address translation,\nwe’ll ﬁrst discuss its ﬁrst incarnation. Introduced in the ﬁrst time-sharing\nmachines of the late 1950’s is a simple idea referred to as base and bounds\n(the technique is also referred to as dynamic relocation; we’ll use both\nterms interchangeably) [SS74].\nSpeciﬁcally, we’ll need two hardware registers within each CPU: one\nis called the base register, and the other the bounds (sometimes called a\nlimit register). This base-and-bounds pair is going to allow us to place the\naddress space anywhere we’d like in physical memory, and do so while\nensuring that the process can only access its own address space.\nIn this setup, each program is written and compiled as if it is loaded at\naddress zero. However, when a program starts running, the OS decides\nwhere in physical memory it should be loaded and sets the base register\nto that value. In the example above, the OS decides to load the process at\nphysical address 32 KB and thus sets the base register to this value.\nInteresting things start to happen when the process is running. Now,\nwhen any memory reference is generated by the process, it is translated\nby the processor in the following manner:\nphysical address = virtual address + base\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n134\nMECHANISM: ADDRESS TRANSLATION\nASIDE: SOFTWARE-BASED RELOCATION\nIn the early days, before hardware support arose, some systems per-\nformed a crude form of relocation purely via software methods.\nThe\nbasic technique is referred to as static relocation, in which a piece of soft-\nware known as the loader takes an executable that is about to be run and\nrewrites its addresses to the desired offset in physical memory.\nFor example, if an instruction was a load from address 1000 into a reg-\nister (e.g., movl 1000, %eax), and the address space of the program\nwas loaded starting at address 3000 (and not 0, as the program thinks),\nthe loader would rewrite the instruction to offset each address by 3000\n(e.g., movl 4000, %eax). In this way, a simple static relocation of the\nprocess’s address space is achieved.\nHowever, static relocation has numerous problems. First and most im-\nportantly, it does not provide protection, as processes can generate bad\naddresses and thus illegally access other process’s or even OS memory; in\ngeneral, hardware support is likely needed for true protection [WL+93].\nA smaller negative is that once placed, it is difﬁcult to later relocate an\naddress space to another location [M65].\nEach memory reference generated by the process is a virtual address;\nthe hardware in turn adds the contents of the base register to this address\nand the result is a physical address that can be issued to the memory\nsystem.\nTo understand this better, let’s trace through what happens when a\nsingle instruction is executed. Speciﬁcally, let’s look at one instruction\nfrom our earlier sequence:\n128: movl 0x0(%ebx), %eax\nThe program counter (PC) is set to 128; when the hardware needs to\nfetch this instruction, it ﬁrst adds the value to the the base register value\nof 32 KB (32768) to get a physical address of 32896; the hardware then\nfetches the instruction from that physical address. Next, the processor\nbegins executing the instruction. At some point, the process then issues\nthe load from virtual address 15 KB, which the processor takes and again\nadds to the base register (32 KB), getting the ﬁnal physical address of\n47 KB and thus the desired contents.\nTransforming a virtual address into a physical address is exactly the\ntechnique we refer to as address translation; that is, the hardware takes a\nvirtual address the process thinks it is referencing and transforms it into\na physical address which is where the data actually resides. Because this\nrelocation of the address happens at runtime, and because we can move\naddress spaces even after the process has started running, the technique\nis often referred to as dynamic relocation [M65].\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nMECHANISM: ADDRESS TRANSLATION\n135\nTIP: HARDWARE-BASED DYNAMIC RELOCATION\nWith dynamic relocation, we can see how a little hardware goes a long\nway. Namely, a base register is used to transform virtual addresses (gen-\nerated by the program) into physical addresses. A bounds (or limit) reg-\nister ensures that such addresses are within the conﬁnes of the address\nspace. Together, they combine to provide a simple and efﬁcient virtual-\nization of memory.\nNow you might be asking: what happened to that bounds (limit) reg-\nister? After all, isn’t this supposed to be the base-and-bounds approach?\nIndeed, it is. And as you might have guessed, the bounds register is there\nto help with protection. Speciﬁcally, the processor will ﬁrst check that\nthe memory reference is within bounds to make sure it is legal; in the sim-\nple example above, the bounds register would always be set to 16 KB. If\na process generates a virtual address that is greater than the bounds, or\none that is negative, the CPU will raise an exception, and the process will\nlikely be terminated. The point of the bounds is thus to make sure that all\naddresses generated by the process are legal and within the “bounds” of\nthe process.\nWe should note that the base and bounds registers are hardware struc-\ntures kept on the chip (one pair per CPU). Sometimes people call the\npart of the processor that helps with address translation the memory\nmanagement unit (MMU); as we develop more sophisticated memory-\nmanagement techniques, we will be adding more circuitry to the MMU.\nA small aside about bound registers, which can be deﬁned in one of\ntwo ways. In one way (as above), it holds the size of the address space,\nand thus the hardware checks the virtual address against it ﬁrst before\nadding the base. In the second way, it holds the physical address of the\nend of the address space, and thus the hardware ﬁrst adds the base and\nthen makes sure the address is within bounds. Both methods are logically\nequivalent; for simplicity, we’ll usually assume that the bounds register\nholds the size of the address space.\nExample Translations\nTo understand address translation via base-and-bounds in more detail,\nlet’s take a look at an example. Imagine a process with an address space of\nsize 4 KB (yes, unrealistically small) has been loaded at physical address\n16 KB. Here are the results of a number of address translations:\n• Virtual Address 0 →Physical Address 16 KB\n• VA 1 KB →PA 17 KB\n• VA 3000 →PA 19384\n• VA 4400 →Fault (out of bounds)\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n136\nMECHANISM: ADDRESS TRANSLATION\nASIDE: DATA STRUCTURE – THE FREE LIST\nThe OS must track which parts of free memory are not in use, so as to\nbe able to allocate memory to processes. Many different data structures\ncan of course be used for such a task; the simplest (which we will assume\nhere) is a free list, which simply is a list of the ranges of the physical\nmemory which are not currently in use.\nAs you can see from the example, it is easy for you to simply add the\nbase address to the virtual address (which can rightly be viewed as an\noffset into the address space) to get the resulting physical address. Only if\nthe virtual address is “too big” or negative will the result be a fault (e.g.,\n4400 is greater than the 4 KB bounds), causing an exception to be raised\nand the process to be terminated.\n15.4\nOS Issues\nThere are a number of new OS issues that arise when using base and\nbounds to implement a simple virtual memory. Speciﬁcally, there are\nthree critical junctures where the OS must take action to implement this\nbase-and-bounds approach to virtualizing memory.\nFirst, The OS must take action when a process is created, ﬁnding space\nfor its address space in memory. Fortunately, given our assumptions that\neach address space is (a) smaller than the size of physical memory and\n(b) the same size, this is quite easy for the OS; it can simply view physical\nmemory as an array of slots, and track whether each one is free or in use.\nWhen a new process is created, the OS will have to search a data structure\n(often called a free list) to ﬁnd room for the new address space and then\nmark it used.\nAn example of what physical memory might look like can be found\nin Figure 15.2. In the ﬁgure, you can see the OS using the ﬁrst slot of\nphysical memory for itself, and that it has relocated the process from the\nexample above into the slot starting at physical memory address 32 KB.\nThe other two slots are free (16 KB-32 KB and 48 KB-64 KB); thus, the free\nlist should consist of these two entries.\nSecond, the OS must take action when a process is terminated, reclaim-\ning all of its memory for use in other processes or the OS. Upon termina-\ntion of a process, the OS thus puts its memory back on the free list, and\ncleans up any associated data structures as need be.\nThird, the OS must also take action when a context switch occurs.\nThere is only one base and bounds register on each CPU, after all, and\ntheir values differ for each running program, as each program is loaded at\na different physical address in memory. Thus, the OS must save and restore\nthe base-and-bounds pair when it switches between processes. Speciﬁ-\ncally, when the OS decides to stop running a process, it must save the\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nMECHANISM: ADDRESS TRANSLATION\n137\nvalues of the base and bounds registers to memory, in some per-process\nstructure such as the process structure or process control block (PCB).\nSimilarly, when the OS resumes a running process (or runs it the ﬁrst\ntime), it must set the values of the base and bounds on the CPU to the\ncorrect values for this process.\nWe should note that when a process is stopped (i.e., not running), it is\npossible for the OS to move an address space from one location in mem-\nory to another rather easily. To move a process’s address space, the OS\nﬁrst deschedules the process; then, the OS copies the address space from\nthe current location to the new location; ﬁnally, the OS updates the saved\nbase register (in the process structure) to point to the new location. When\nthe process is resumed, its (new) base register is restored, and it begins\nrunning again, oblivious that its instructions and data are now in a com-\npletely new spot in memory!\nWe should also note that access to the base and bounds registers is ob-\nviously privileged. Special hardware instructions are required to access\nbase-and-bounds registers; if a process, running in user mode, attempts\nto do so, the CPU will raise an exception and the OS will likely termi-\nnate the process. Only in kernel (or privileged) mode can such registers\nbe modiﬁed. Imagine the havoc a user process could wreak1 if it could\narbitrarily change the base register while running. Imagine it! And then\nquickly ﬂush such dark thoughts from your mind, as they are the ghastly\nstuff of which nightmares are made.\n15.5\nSummary\nIn this chapter, we have extended the concept of limited direct exe-\ncution with a speciﬁc mechanism used in virtual memory, known as ad-\ndress translation. With address translation, the OS can control each and\nevery memory access from a process, ensuring the accesses stay within\nthe bounds of the address space. Key to the efﬁciency of this technique\nis hardware support, which performs the translation quickly for each ac-\ncess, turning virtual addresses (the process’s view of memory) into phys-\nical ones (the actual view). All of this is performed in a way that is trans-\nparent to the process that has been relocated; the process has no idea its\nmemory references are being translated, making for a wonderful illusion.\nWe have also seen one particular form of virtualization, known as base\nand bounds or dynamic relocation. Base-and-bounds virtualization is\nquite efﬁcient, as only a little more hardware logic is required to add a\nbase register to the virtual address and check that the address generated\nby the process is in bounds. Base-and-bounds also offers protection; the\nOS and hardware combine to ensure no process can generate memory\nreferences outside its own address space. Protection is certainly one of\nthe most important goals of the OS; without it, the OS could not control\n1Is there anything other than “havoc” that can be “wreaked”?\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "page_number": 162,
      "chapter_number": 18,
      "summary": "This chapter covers segment 18 (pages 162-173). Key topics include memory, address, and addresses. [KR88] “The C Programming Language”\nBrian Kernighan and Dennis Ritchie\nPrentice-Hall 1988\nThe C book, by the developers of C.",
      "keywords": [
        "Address",
        "address space",
        "Address Translation",
        "physical address",
        "MEMORY",
        "physical memory",
        "process",
        "virtual address",
        "physical memory address",
        "physical",
        "space",
        "Translation",
        "hardware",
        "memory address",
        "bounds"
      ],
      "concepts": [
        "memory",
        "address",
        "addresses",
        "hardware",
        "virtual",
        "programming",
        "program",
        "process",
        "processes",
        "base"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 1,
          "title": "Segment 1 (pages 1-20)",
          "relevance_score": 0.75,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 1,
          "title": "Segment 1 (pages 1-8)",
          "relevance_score": 0.71,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 12,
          "title": "Segment 12 (pages 219-238)",
          "relevance_score": 0.69,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 55,
          "title": "Segment 55 (pages 1103-1105)",
          "relevance_score": 0.69,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 26,
          "title": "Segment 26 (pages 249-257)",
          "relevance_score": 0.69,
          "method": "api"
        }
      ]
    },
    {
      "number": 19,
      "title": "Segment 19 (pages 174-184)",
      "start_page": 174,
      "end_page": 184,
      "detection_method": "topic_boundary",
      "content": "138\nMECHANISM: ADDRESS TRANSLATION\nthe machine (if processes were free to overwrite memory, they could eas-\nily do nasty things like overwrite the trap table and take over the system).\nUnfortunately, this simple technique of dynamic relocation does have\nits inefﬁciencies. For example, as you can see in Figure 15.2 (back a few\npages), the relocated process is using physical memory from 32 KB to\n48 KB; however, because the process stack and heap are not too big, all of\nthe space between the two is simply wasted. This type of waste is usually\ncalled internal fragmentation, as the space inside the allocated unit is not\nall used (i.e., is fragmented) and thus wasted. In our current approach, al-\nthough there might be enough physical memory for more processes, we\nare currently restricted to placing an address space in a ﬁxed-sized slot\nand thus internal fragmentation can arise2. Thus, we are going to need\nmore sophisticated machinery, to try to better utilize physical memory\nand avoid internal fragmentation. Our ﬁrst attempt will be a slight gen-\neralization of base and bounds known as segmentation, which we will\ndiscuss next.\n2A different solution might instead place a ﬁxed-sized stack within the address space,\njust below the code region, and a growing heap below that. However, this limits ﬂexibility\nby making recursion and deeply-nested function calls challenging, and thus is something we\nhope to avoid.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nMECHANISM: ADDRESS TRANSLATION\n139\nReferences\n[M65] “On Dynamic Program Relocation”\nW.C. McGee\nIBM Systems Journal\nVolume 4, Number 3, 1965, pages 184–199\nThis paper is a nice summary of early work on dynamic relocation, as well as some basics on static\nrelocation.\n[P90] “Relocating loader for MS-DOS .EXE executable ﬁles”\nKenneth D. A. Pillay\nMicroprocessors & Microsystems archive\nVolume 14, Issue 7 (September 1990)\nAn example of a relocating loader for MS-DOS. Not the ﬁrst one, but just a relatively modern example\nof how such a system works.\n[SS74] “The Protection of Information in Computer Systems”\nJ. Saltzer and M. Schroeder\nCACM, July 1974\nFrom this paper: “The concepts of base-and-bound register and hardware-interpreted descriptors ap-\npeared, apparently independently, between 1957 and 1959 on three projects with diverse goals. At\nM.I.T., McCarthy suggested the base-and-bound idea as part of the memory protection system nec-\nessary to make time-sharing feasible. IBM independently developed the base-and-bound register as a\nmechanism to permit reliable multiprogramming of the Stretch (7030) computer system. At Burroughs,\nR. Barton suggested that hardware-interpreted descriptors would provide direct support for the naming\nscope rules of higher level languages in the B5000 computer system.” We found this quote on Mark\nSmotherman’s cool history pages [S04]; see them for more information.\n[S04] “System Call Support”\nMark Smotherman, May 2004\nhttp://people.cs.clemson.edu/˜mark/syscall.html\nA neat history of system call support. Smotherman has also collected some early history on items like\ninterrupts and other fun aspects of computing history. See his web pages for more details.\n[WL+93] “Efﬁcient Software-based Fault Isolation”\nRobert Wahbe, Steven Lucco, Thomas E. Anderson, Susan L. Graham\nSOSP ’93\nA terriﬁc paper about how you can use compiler support to bound memory references from a program,\nwithout hardware support. The paper sparked renewed interest in software techniques for isolation of\nmemory references.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n140\nMECHANISM: ADDRESS TRANSLATION\nHomework\nThe program relocation.py allows you to see how address trans-\nlations are performed in a system with base and bounds registers. See the\nREADME for details.\nQuestions\n• Run with seeds 1, 2, and 3, and compute whether each virtual ad-\ndress generated by the process is in or out of bounds. If in bounds,\ncompute the translation.\n• Run with these ﬂags: -s 0 -n 10. What value do you have set\n-l (the bounds register) to in order to ensure that all the generated\nvirtual addresses are within bounds?\n• Run with these ﬂags: -s 1 -n 10 -l 100. What is the maxi-\nmum value that bounds can be set to, such that the address space\nstill ﬁts into physical memory in its entirety?\n• Run some of the same problems above, but with larger address\nspaces (-a) and physical memories (-p).\n• What fraction of randomly-generated virtual addresses are valid,\nas a function of the value of the bounds register? Make a graph\nfrom running with different random seeds, with limit values rang-\ning from 0 up to the maximum size of the address space.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\n16\nSegmentation\nSo far we have been putting the entire address space of each process in\nmemory. With the base and bounds registers, the OS can easily relocate\nprocesses to different parts of physical memory. However, you might\nhave noticed something interesting about these address spaces of ours:\nthere is a big chunk of “free” space right in the middle, between the stack\nand the heap.\nAs you can imagine from Figure 16.1, although the space between the\nstack and heap is not being used by the process, it is still taking up phys-\nical memory when we relocate the entire address space somewhere in\nphysical memory; thus, the simple approach of using a base and bounds\nregister pair to virtualize memory is wasteful. It also makes it quite hard\nto run a program when the entire address space doesn’t ﬁt into memory;\nthus, base and bounds is not as ﬂexible as we would like. And thus:\nTHE CRUX: HOW TO SUPPORT A LARGE ADDRESS SPACE\nHow do we support a large address space with (potentially) a lot of\nfree space between the stack and the heap? Note that in our examples,\nwith tiny (pretend) address spaces, the waste doesn’t seem too bad. Imag-\nine, however, a 32-bit address space (4 GB in size); a typical program will\nonly use megabytes of memory, but still would demand that the entire\naddress space be resident in memory.\n16.1\nSegmentation: Generalized Base/Bounds\nTo solve this problem, an idea was born, and it is called segmenta-\ntion. It is quite an old idea, going at least as far back as the very early\n1960’s [H61, G62]. The idea is simple: instead of having just one base\nand bounds pair in our MMU, why not have a base and bounds pair per\nlogical segment of the address space? A segment is just a contiguous\nportion of the address space of a particular length, and in our canonical\n141\n\n\n142\nSEGMENTATION\n16KB\n15KB\n14KB\n6KB\n5KB\n4KB\n3KB\n2KB\n1KB\n0KB\nProgram Code\nHeap\n(free)\nStack\nFigure 16.1: An Address Space (Again)\naddress space, we have three logically-different segments: code, stack,\nand heap. What segmentation allows the OS to do is to place each one\nof those segments in different parts of physical memory, and thus avoid\nﬁlling physical memory with unused virtual address space.\nLet’s look at an example. Assume we want to place the address space\nfrom Figure 16.1 into physical memory. With a base and bounds pair per\nsegment, we can place each segment independently in physical memory.\nFor example, see Figure 16.2; there you see a 64-KB physical memory\nwith those three segments within it (and 16KB reserved for the OS).\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nSEGMENTATION\n143\n64KB\n48KB\n32KB\n16KB\n0KB\n(not in use)\n(not in use)\n(not in use)\nOperating System\nStack\nCode\nHeap\nFigure 16.2: Placing Segments In Physical Memory\nAs you can see in the diagram, only used memory is allocated space\nin physical memory, and thus large address spaces with large amounts of\nunused address space (which we sometimes call sparse address spaces)\ncan be accommodated.\nThe hardware structure in our MMU required to support segmenta-\ntion is just what you’d expect: in this case, a set of three base and bounds\nregister pairs. Table 16.1 below shows the register values for the example\nabove; each bounds register holds the size of a segment.\nSegment\nBase\nSize\nCode\n32K\n2K\nHeap\n34K\n2K\nStack\n28K\n2K\nTable 16.1: Segment Register Values\nYou can see from the table that the code segment is placed at physical\naddress 32KB and has a size of 2KB and the heap segment is placed at\n34KB and also has a size of 2KB.\nLet’s do an example translation, using the address space in Figure 16.1.\nAssume a reference is made to virtual address 100 (which is in the code\nsegment). When the reference takes place (say, on an instruction fetch),\nthe hardware will add the base value to the offset into this segment (100 in\nthis case) to arrive at the desired physical address: 100 + 32KB, or 32868.\nIt will then check that the address is within bounds (100 is less than 2KB),\nﬁnd that it is, and issue the reference to physical memory address 32868.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n144\nSEGMENTATION\nASIDE: THE SEGMENTATION FAULT\nThe term segmentation fault or violation arises from a memory access\non a segmented machine to an illegal address. Humorously, the term\npersists, even on machines with no support for segmentation at all. Or\nnot so humorously, if you can’t ﬁgure why your code keeps faulting.\nNow let’s look at an address in the heap, virtual address 4200 (again\nrefer to Figure 16.1). If we just add the virtual address 4200 to the base\nof the heap (34KB), we get a physical address of 39016, which is not the\ncorrect physical address. What we need to ﬁrst do is extract the offset into\nthe heap, i.e., which byte(s) in this segment the address refers to. Because\nthe heap starts at virtual address 4KB (4096), the offset of 4200 is actually\n4200 – 4096 or 104. We then take this offset (104) and add it to the base\nregister physical address (34K or 34816) to get the desired result: 34920.\nWhat if we tried to refer to an illegal address, such as 7KB which is be-\nyond the end of the heap? You can imagine what will happen: the hard-\nware detects that the address is out of bounds, traps into the OS, likely\nleading to the termination of the offending process. And now you know\nthe origin of the famous term that all C programmers learn to dread: the\nsegmentation violation or segmentation fault.\n16.2\nWhich Segment Are We Referring To?\nThe hardware uses segment registers during translation. How does it\nknow the offset into a segment, and to which segment an address refers?\nOne common approach, sometimes referred to as an explicit approach,\nis to chop up the address space into segments based on the top few bits\nof the virtual address; this technique was used in the VAX/VMS system\n[LL82]. In our example above, we have three segments; thus we need two\nbits to accomplish our task. If we use the top two bits of our 14-bit virtual\naddress to select the segment, our virtual address looks like this:\n13 12 11 10\n9\n8\n7\n6\n5\n4\n3\n2\n1\n0\nSegment\nOffset\nIn our example, then, if the top two bits are 00, the hardware knows\nthe virtual address is in the code segment, and thus uses the code base\nand bounds pair to relocate the address to the correct physical location.\nIf the top two bits are 01, the hardware knows the address is in the heap,\nand thus uses the heap base and bounds. Let’s take our example heap\nvirtual address from above (4200) and translate it, just to make sure this\nis clear. The virtual address 4200, in binary form, can be seen here:\n13\n0\n12\n1\n11\n0\n10\n0\n9\n0\n8\n0\n7\n0\n6\n1\n5\n1\n4\n0\n3\n1\n2\n0\n1\n0\n0\n0\nSegment\nOffset\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nSEGMENTATION\n145\nAs you can see from the picture, the top two bits (01) tell the hardware\nwhich segment we are referring to. The bottom 12 bits are the offset into\nthe segment: 0000 0110 1000, or hex 0x068, or 104 in decimal. Thus, the\nhardware simply takes the ﬁrst two bits to determine which segment reg-\nister to use, and then takes the next 12 bits as the offset into the segment.\nBy adding the base register to the offset, the hardware arrives at the ﬁ-\nnal physical address. Note the offset eases the bounds check too: we can\nsimply check if the offset is less than the bounds; if not, the address is ille-\ngal. Thus, if base and bounds were arrays (with one entry per segment),\nthe hardware would be doing something like this to obtain the desired\nphysical address:\n1\n// get top 2 bits of 14-bit VA\n2\nSegment = (VirtualAddress & SEG_MASK) >> SEG_SHIFT\n3\n// now get offset\n4\nOffset\n= VirtualAddress & OFFSET_MASK\n5\nif (Offset >= Bounds[Segment])\n6\nRaiseException(PROTECTION_FAULT)\n7\nelse\n8\nPhysAddr = Base[Segment] + Offset\n9\nRegister = AccessMemory(PhysAddr)\nIn our running example, we can ﬁll in values for the constants above.\nSpeciﬁcally, SEG MASK would be set to 0x3000, SEG SHIFT to 12, and\nOFFSET MASK to 0xFFF.\nYou may also have noticed that when we use the top two bits, and we\nonly have three segments (code, heap, stack), one segment of the address\nspace goes unused. Thus, some systems put code in the same segment as\nthe heap and thus use only one bit to select which segment to use [LL82].\nThere are other ways for the hardware to determine which segment\na particular address is in. In the implicit approach, the hardware deter-\nmines the segment by noticing how the address was formed. If, for ex-\nample, the address was generated from the program counter (i.e., it was\nan instruction fetch), then the address is within the code segment; if the\naddress is based off of the stack or base pointer, it must be in the stack\nsegment; any other address must be in the heap.\n16.3\nWhat About The Stack?\nThus far, we’ve left out one important component of the address space:\nthe stack. The stack has been relocated to physical address 28KB in the di-\nagram above, but with one critical difference: it grows backwards – in phys-\nical memory, it starts at 28KB and grows back to 26KB, corresponding to\nvirtual addresses 16KB to 14KB; translation has to proceed differently.\nThe ﬁrst thing we need is a little extra hardware support. Instead of\njust base and bounds values, the hardware also needs to know which way\nthe segment grows (a bit, for example, that is set to 1 when the segment\ngrows in the positive direction, and 0 for negative). Our updated view of\nwhat the hardware tracks is seen in Table 16.2.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n146\nSEGMENTATION\nSegment\nBase\nSize\nGrows Positive?\nCode\n32K\n2K\n1\nHeap\n34K\n2K\n1\nStack\n28K\n2K\n0\nTable 16.2: Segment Registers (With Negative-Growth Support)\nWith the hardware understanding that segments can grow in the neg-\native direction, the hardware must now translate such virtual addresses\nslightly differently. Let’s take an example stack virtual address and trans-\nlate it to understand the process.\nIn this example, assume we wish to access virtual address 15KB, which\nshould map to physical address 27KB. Our virtual address, in binary\nform, thus looks like this: 11 1100 0000 0000 (hex 0x3C00). The hard-\nware uses the top two bits (11) to designate the segment, but then we\nare left with an offset of 3KB. To obtain the correct negative offset, we\nmust subtract the maximum segment size from 3KB: in this example, a\nsegment can be 4KB, and thus the correct negative offset is 3KB - 4KB\nwhich equals -1KB. We simply add the negative offset (-1KB) to the base\n(28KB) to arrive at the correct physical address: 27KB. The bounds check\ncan be calculated by ensuring the absolute value of the negative offset is\nless than the segment’s size.\n16.4\nSupport for Sharing\nAs support for segmentation grew, system designers soon realized that\nthey could realize new types of efﬁciencies with a little more hardware\nsupport. Speciﬁcally, to save memory, sometimes it is useful to share\ncertain memory segments between address spaces. In particular, code\nsharing is common and still in use in systems today.\nTo support sharing, we need a little extra support from the hardware,\nin the form of protection bits. Basic support adds a few bits per segment,\nindicating whether or not a program can read or write a segment, or per-\nhaps execute code that lies within the segment. By setting a code segment\nto read-only, the same code can be shared across multiple processes, with-\nout worry of harming isolation; while each process still thinks that it is ac-\ncessing its own private memory, the OS is secretly sharing memory which\ncannot be modiﬁed by the process, and thus the illusion is preserved.\nAn example of the additional information tracked by the hardware\n(and OS) is shown in Figure 16.3. As you can see, the code segment is\nset to read and execute, and thus the same physical segment in memory\ncould be mapped into multiple virtual address spaces.\nWith protection bits, the hardware algorithm described earlier would\nalso have to change. In addition to checking whether a virtual address is\nwithin bounds, the hardware also has to check whether a particular ac-\ncess is permissible. If a user process tries to write to a read-only page, or\nexecute from a non-executable page, the hardware should raise an excep-\ntion, and thus let the OS deal with the offending process.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nSEGMENTATION\n147\nSegment\nBase\nSize\nGrows Positive?\nProtection\nCode\n32K\n2K\n1\nRead-Execute\nHeap\n34K\n2K\n1\nRead-Write\nStack\n28K\n2K\n0\nRead-Write\nTable 16.3: Segment Register Values (with Protection)\n16.5\nFine-grained vs. Coarse-grained Segmentation\nMost of our examples thus far have focused on systems with just a\nfew segments (i.e., code, stack, heap); we can think of this segmentation\nas coarse-grained, as it chops up the address space into relatively large,\ncoarse chunks. However, some early systems (e.g., Multics [CV65,DD68])\nwere more ﬂexible and allowed for address spaces to consist of a large\nnumber smaller segments, referred to as ﬁne-grained segmentation.\nSupporting many segments requires even further hardware support,\nwith a segment table of some kind stored in memory. Such segment ta-\nbles usually support the creation of a very large number of segments, and\nthus enable a system to use segments in more ﬂexible ways than we have\nthus far discussed. For example, early machines like the Burroughs B5000\nhad support for thousands of segments, and expected a compiler to chop\ncode and data into separate segments which the OS and hardware would\nthen support [RK68]. The thinking at the time was that by having ﬁne-\ngrained segments, the OS could better learn about which segments are in\nuse and which are not and thus utilize main memory more effectively.\n16.6\nOS Support\nYou now should have a basic idea as to how segmentation works.\nPieces of the address space are relocated into physical memory as the\nsystem runs, and thus a huge savings of physical memory is achieved\nrelative to our simpler approach with just a single base/bounds pair for\nthe entire address space. Speciﬁcally, all the unused space between the\nstack and the heap need not be allocated in physical memory, allowing\nus to ﬁt more address spaces into physical memory.\nHowever, segmentation raises a number of new issues. We’ll ﬁrst de-\nscribe the new OS issues that must be addressed. The ﬁrst is an old one:\nwhat should the OS do on a context switch? You should have a good\nguess by now: the segment registers must be saved and restored. Clearly,\neach process has its own virtual address space, and the OS must make\nsure to set up these registers correctly before letting the process run again.\nThe second, and more important, issue is managing free space in phys-\nical memory. When a new address space is created, the OS has to be\nable to ﬁnd space in physical memory for its segments. Previously, we\nassumed that each address space was the same size, and thus physical\nmemory could be thought of as a bunch of slots where processes would\nﬁt in. Now, we have a number of segments per process, and each segment\nmight be a different size.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n148\nSEGMENTATION\n64KB\n56KB\n48KB\n40KB\n32KB\n24KB\n16KB\n8KB\n0KB\nOperating System\nNot Compacted\n(not in use)\n(not in use)\n(not in use)\nAllocated\nAllocated\nAllocated\n64KB\n56KB\n48KB\n40KB\n32KB\n24KB\n16KB\n8KB\n0KB\n(not in use)\nAllocated\nOperating System\nCompacted\nFigure 16.3: Non-compacted and Compacted Memory\nThe general problem that arises is that physical memory quickly be-\ncomes full of little holes of free space, making it difﬁcult to allocate new\nsegments, or to grow existing ones. We call this problem external frag-\nmentation [R69]; see Figure 16.3 (left).\nIn the example, a process comes along and wishes to allocate a 20KB\nsegment. In that example, there is 24KB free, but not in one contiguous\nsegment (rather, in three non-contiguous chunks). Thus, the OS cannot\nsatisfy the 20KB request.\nOne solution to this problem would be to compact physical memory\nby rearranging the existing segments. For example, the OS could stop\nwhichever processes are running, copy their data to one contiguous re-\ngion of memory, change their segment register values to point to the\nnew physical locations, and thus have a large free extent of memory with\nwhich to work. By doing so, the OS enables the new allocation request\nto succeed. However, compaction is expensive, as copying segments is\nmemory-intensive and thus would use a fair amount of processor time.\nSee Figure 16.3 (right) for a diagram of compacted physical memory.\nA simpler approach is to use a free-list management algorithm that\ntries to keep large extents of memory available for allocation. There are\nliterally hundreds of approaches that people have taken, including clas-\nsic algorithms like best-ﬁt (which keeps a list of free spaces and returns\nthe one closest in size that satisﬁes the desired allocation to the requester),\nworst-ﬁt, ﬁrst-ﬁt, and more complex schemes like buddy algorithm [K68].\nAn excellent survey by Wilson et al. is a good place to start if you want to\nlearn more about such algorithms [W+95], or you can wait until we cover\nsome of the basics ourselves in a later chapter. Unfortunately, though, no\nmatter how smart the algorithm, external fragmentation will still exist;\nthus, a good algorithm simply attempts to minimize it.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "page_number": 174,
      "chapter_number": 19,
      "summary": "Our ﬁrst attempt will be a slight gen-\neralization of base and bounds known as segmentation, which we will\ndiscuss next Key topics include segmentation, segment, and address.",
      "keywords": [
        "address space",
        "ADDRESS",
        "segment",
        "physical memory",
        "virtual address",
        "physical address",
        "memory",
        "space",
        "physical",
        "entire address space",
        "segments",
        "bounds",
        "virtual address space",
        "base",
        "offset"
      ],
      "concepts": [
        "segmentation",
        "segment",
        "address",
        "addresses",
        "addressed",
        "memory",
        "memories",
        "support",
        "hardware",
        "likely"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 42,
          "title": "Segment 42 (pages 838-857)",
          "relevance_score": 0.74,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 43,
          "title": "Segment 43 (pages 858-879)",
          "relevance_score": 0.71,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 33,
          "title": "Segment 33 (pages 662-682)",
          "relevance_score": 0.7,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 41,
          "title": "Segment 41 (pages 815-837)",
          "relevance_score": 0.67,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 38,
          "title": "Segment 38 (pages 351-358)",
          "relevance_score": 0.67,
          "method": "api"
        }
      ]
    },
    {
      "number": 20,
      "title": "Segment 20 (pages 185-192)",
      "start_page": 185,
      "end_page": 192,
      "detection_method": "topic_boundary",
      "content": "SEGMENTATION\n149\nTIP: IF 1000 SOLUTIONS EXIST, NO GREAT ONE DOES\nThe fact that so many different algorithms exist to try to minimize exter-\nnal fragmentation is indicative of a stronger underlying truth: there is no\none “best” way to solve the problem. Thus, we settle for something rea-\nsonable and hope it is good enough. The only real solution (as we will\nsee in forthcoming chapters) is to avoid the problem altogether, by never\nallocating memory in variable-sized chunks.\n16.7\nSummary\nSegmentation solves a number of problems, and helps us build a more\neffective virtualization of memory. Beyond just dynamic relocation, seg-\nmentation can better support sparse address spaces, by avoiding the huge\npotential waste of memory between logical segments of the address space.\nIt is also fast, as doing the arithmetic segmentation requires in hardware\nis easy and well-suited to hardware; the overheads of translation are min-\nimal. A fringe beneﬁt arises too: code sharing. If code is placed within\na separate segment, such a segment could potentially be shared across\nmultiple running programs.\nHowever, as we learned, allocating variable-sized segments in mem-\nory leads to some problems that we’d like to overcome. The ﬁrst, as dis-\ncussed above, is external fragmentation. Because segments are variable-\nsized, free memory gets chopped up into odd-sized pieces, and thus sat-\nisfying a memory-allocation request can be difﬁcult. One can try to use\nsmart algorithms [W+95] or periodically compact memory, but the prob-\nlem is fundamental and hard to avoid.\nThe second and perhaps more important problem is that segmentation\nstill isn’t ﬂexible enough to support our fully generalized, sparse address\nspace. For example, if we have a large but sparsely-used heap all in one\nlogical segment, the entire heap must still reside in memory in order to be\naccessed. In other words, if our model of how the address space is being\nused doesn’t exactly match how the underlying segmentation has been\ndesigned to support it, segmentation doesn’t work very well. We thus\nneed to ﬁnd some new solutions. Ready to ﬁnd them?\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n150\nSEGMENTATION\nReferences\n[CV65] “Introduction and Overview of the Multics System”\nF. J. Corbato and V. A. Vyssotsky\nFall Joint Computer Conference, 1965\nOne of ﬁve papers presented on Multics at the Fall Joint Computer Conference; oh to be a ﬂy on the wall\nin that room that day!\n[DD68] “Virtual Memory, Processes, and Sharing in Multics”\nRobert C. Daley and Jack B. Dennis\nCommunications of the ACM, Volume 11, Issue 5, May 1968\nAn early paper on how to perform dynamic linking in Multics, which was way ahead of its time. Dy-\nnamic linking ﬁnally found its way back into systems about 20 years later, as the large X-windows\nlibraries demanded it. Some say that these large X11 libraries were MIT’s revenge for removing support\nfor dynamic linking in early versions of UNIX!\n[G62] “Fact Segmentation”\nM. N. Greenﬁeld\nProceedings of the SJCC, Volume 21, May 1962\nAnother early paper on segmentation; so early that it has no references to other work.\n[H61] “Program Organization and Record Keeping for Dynamic Storage”\nA. W. Holt\nCommunications of the ACM, Volume 4, Issue 10, October 1961\nAn incredibly early and difﬁcult to read paper about segmentation and some of its uses.\n[I09] “Intel 64 and IA-32 Architectures Software Developer’s Manuals”\nIntel, 2009\nAvailable: http://www.intel.com/products/processor/manuals\nTry reading about segmentation in here (Chapter 3 in Volume 3a); it’ll hurt your head, at least a little\nbit.\n[K68] “The Art of Computer Programming: Volume I”\nDonald Knuth\nAddison-Wesley, 1968\nKnuth is famous not only for his early books on the Art of Computer Programming but for his typeset-\nting system TeX which is still a powerhouse typesetting tool used by professionals today, and indeed to\ntypeset this very book. His tomes on algorithms are a great early reference to many of the algorithms\nthat underly computing systems today.\n[L83] “Hints for Computer Systems Design”\nButler Lampson\nACM Operating Systems Review, 15:5, October 1983\nA treasure-trove of sage advice on how to build systems. Hard to read in one sitting; take it in a little at\na time, like a ﬁne wine, or a reference manual.\n[LL82] “Virtual Memory Management in the VAX/VMS Operating System”\nHenry M. Levy and Peter H. Lipman\nIEEE Computer, Volume 15, Number 3 (March 1982)\nA classic memory management system, with lots of common sense in its design. We’ll study it in more\ndetail in a later chapter.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nSEGMENTATION\n151\n[RK68] “Dynamic Storage Allocation Systems”\nB. Randell and C.J. Kuehner\nCommunications of the ACM\nVolume 11(5), pages 297-306, May 1968\nA nice overview of the differences between paging and segmentation, with some historical discussion of\nvarious machines.\n[R69] “A note on storage fragmentation and program segmentation”\nBrian Randell\nCommunications of the ACM\nVolume 12(7), pages 365-372, July 1969\nOne of the earliest papers to discuss fragmentation.\n[W+95] “Dynamic Storage Allocation: A Survey and Critical Review”\nPaul R. Wilson, Mark S. Johnstone, Michael Neely, and David Boles\nIn International Workshop on Memory Management\nScotland, United Kingdom, September 1995\nA great survey paper on memory allocators.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n152\nSEGMENTATION\nHomework\nThis program allows you to see how address translations are performed\nin a system with segmentation. See the README for details.\nQuestions\n• First let’s use a tiny address space to translate some addresses. Here’s\na simple set of parameters with a few different random seeds; can\nyou translate the addresses?\nsegmentation.py -a 128 -p 512 -b 0 -l 20 -B 512 -L 20 -s 0\nsegmentation.py -a 128 -p 512 -b 0 -l 20 -B 512 -L 20 -s 1\nsegmentation.py -a 128 -p 512 -b 0 -l 20 -B 512 -L 20 -s 2\n• Now, let’s see if we understand this tiny address space we’ve con-\nstructed (using the parameters from the question above). What is\nthe highest legal virtual address in segment 0? What about the low-\nest legal virtual address in segment 1? What are the lowest and\nhighest illegal addresses in this entire address space? Finally, how\nwould you run segmentation.py with the -A ﬂag to test if you\nare right?\n• Let’s say we have a tiny 16-byte address space in a 128-byte physical\nmemory. What base and bounds would you set up so as to get\nthe simulator to generate the following translation results for the\nspeciﬁed address stream: valid, valid, violation, ..., violation, valid,\nvalid? Assume the following parameters:\nsegmentation.py -a 16 -p 128\n-A 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15\n--b0 ? --l0 ? --b1 ? --l1 ?\n• Assuming we want to generate a problem where roughly 90% of the\nrandomly-generated virtual addresses are valid (i.e., not segmenta-\ntion violations). How should you conﬁgure the simulator to do so?\nWhich parameters are important?\n• Can you run the simulator such that no virtual addresses are valid?\nHow?\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\n17\nFree-Space Management\nIn this chapter, we take a small detour from our discussion of virtual-\nizing memory to discuss a fundamental aspect of any memory manage-\nment system, whether it be a malloc library (managing pages of a pro-\ncess’s heap) or the OS itself (managing portions of the address space of a\nprocess). Speciﬁcally, we will discuss the issues surrounding free-space\nmanagement.\nLet us make the problem more speciﬁc. Managing free space can cer-\ntainly be easy, as we will see when we discuss the concept of paging. It is\neasy when the space you are managing is divided into ﬁxed-sized units;\nin such a case, you just keep a list of these ﬁxed-sized units; when a client\nrequests one of them, return the ﬁrst entry.\nWhere free-space management becomes more difﬁcult (and interest-\ning) is when the free space you are managing consists of variable-sized\nunits; this arises in a user-level memory-allocation library (as in malloc()\nand free()) and in an OS managing physical memory when using seg-\nmentation to implement virtual memory. In either case, the problem that\nexists is known as external fragmentation: the free space gets chopped\ninto little pieces of different sizes and is thus fragmented; subsequent re-\nquests may fail because there is no single contiguous space that can sat-\nisfy the request, even though the total amount of free space exceeds the\nsize of the request.\nfree\nused\nfree\n0\n10\n20\n30\nThe ﬁgure shows an example of this problem. In this case, the total\nfree space available is 20 bytes; unfortunately, it is fragmented into two\nchunks of size 10 each. As a result, a request for 15 bytes will fail even\nthough there are 20 bytes free. And thus we arrive at the problem ad-\ndressed in this chapter.\n153\n\n\n154\nFREE-SPACE MANAGEMENT\nCRUX: HOW TO MANAGE FREE SPACE\nHow should free space be managed, when satisfying variable-sized re-\nquests? What strategies can be used to minimize fragmentation? What\nare the time and space overheads of alternate approaches?\n17.1\nAssumptions\nMost of this discussion will focus on the great history of allocators\nfound in user-level memory-allocation libraries. We draw on Wilson’s\nexcellent survey [W+95] but encourage interested readers to go to the\nsource document itself for more details1.\nWe assume a basic interface such as that provided by malloc() and\nfree(). Speciﬁcally, void *malloc(size t size) takes a single pa-\nrameter, size, which is the number of bytes requested by the applica-\ntion; it hands back a pointer (of no particular type, or a void pointer in\nC lingo) to a region of that size (or greater). The complementary routine\nvoid free(void *ptr) takes a pointer and frees the corresponding\nchunk. Note the implication of the interface: the user, when freeing the\nspace, does not inform the library of its size; thus, the library must be able\nto ﬁgure out how big a chunk of memory is when handed just a pointer\nto it. We’ll discuss how to do this a bit later on in the chapter.\nThe space that this library manages is known historically as the heap,\nand the generic data structure used to manage free space in the heap is\nsome kind of free list. This structure contains references to all of the free\nchunks of space in the managed region of memory. Of course, this data\nstructure need not be a list per se, but just some kind of data structure to\ntrack free space.\nWe further assume that primarily we are concerned with external frag-\nmentation, as described above. Allocators could of course also have the\nproblem of internal fragmentation; if an allocator hands out chunks of\nmemory bigger than that requested, any unasked for (and thus unused)\nspace in such a chunk is considered internal fragmentation (because the\nwaste occurs inside the allocated unit) and is another example of space\nwaste. However, for the sake of simplicity, and because it is the more in-\nteresting of the two types of fragmentation, we’ll mostly focus on external\nfragmentation.\nWe’ll also assume that once memory is handed out to a client, it cannot\nbe relocated to another location in memory. For example, if a program\ncalls malloc() and is given a pointer to some space within the heap,\nthat memory region is essentially “owned” by the program (and cannot\nbe moved by the library) until the program returns it via a correspond-\ning call to free(). Thus, no compaction of free space is possible, which\n1It is nearly 80 pages long; thus, you really have to be interested!\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nFREE-SPACE MANAGEMENT\n155\nwould be useful to combat fragmentation2. Compaction could, however,\nbe used in the OS to deal with fragmentation when implementing seg-\nmentation; see the chapter on segmentation for details.\nFinally, we’ll assume that the allocator manages a contiguous region\nof bytes. In some cases, an allocator could ask for that region to grow;\nfor example, a user-level memory-allocation library might call into the\nkernel to grow the heap (via a system call such as sbrk) when it runs out\nof space. However, for simplicity, we’ll just assume that the region is a\nsingle ﬁxed size throughout its life.\n17.2\nLow-level Mechanisms\nBefore delving into some policy details, we’ll ﬁrst cover some com-\nmon mechanisms used in most allocators. First, we’ll discuss the basics of\nsplitting and coalescing, common techniques in most any allocator. Sec-\nond, we’ll show how one can track the size of allocated regions quickly\nand with relative ease. Finally, we’ll discuss how to build a simple list\ninside the free space to keep track of what is free and what isn’t.\nSplitting and Coalescing\nA free list contains a set of elements that describe the free space still re-\nmaining in the heap. Thus, assume the following 30-byte heap:\nfree\nused\nfree\n0\n10\n20\n30\nThe free list for this heap would have two elements on it. One entry de-\nscribes the ﬁrst 10-byte free segment (bytes 0-9), and one entry describes\nthe other free segment (bytes 20-29):\nhead\naddr:0\nlen:10\naddr:20\nlen:10\nNULL\nAs described above, a request for anything greater than 10 bytes will\nfail (returning NULL); there just isn’t a single contiguous chunk of mem-\nory of that size available. A request for exactly that size (10 bytes) could\nbe satisﬁed easily by either of the free chunks. But what happens if the\nrequest is for something smaller than 10 bytes?\nAssume we have a request for just a single byte of memory. In this\ncase, the allocator will perform an action known as splitting: it will ﬁnd\n2Once you hand a pointer to a chunk of memory to a C program, it is generally difﬁcult\nto determine all references (pointers) to that region, which may be stored in other variables\nor even in registers at a given point in execution. This may not be the case in more strongly-\ntyped, garbage-collected languages, which would thus enable compaction as a technique to\ncombat fragmentation.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n156\nFREE-SPACE MANAGEMENT\na free chunk of memory that can satisfy the request and split it into two.\nThe ﬁrst chunk it will return to the caller; the second chunk will remain\non the list. Thus, in our example above, if a request for 1 byte were made,\nand the allocator decided to use the second of the two elements on the list\nto satisfy the request, the call to malloc() would return 20 (the address of\nthe 1-byte allocated region) and the list would end up looking like this:\nhead\naddr:0\nlen:10\naddr:21\nlen:9\nNULL\nIn the picture, you can see the list basically stays intact; the only change\nis that the free region now starts at 21 instead of 20, and the length of that\nfree region is now just 93. Thus, the split is commonly used in allocators\nwhen requests are smaller than the size of any particular free chunk.\nA corollary mechanism found in many allocators is known as coalesc-\ning of free space. Take our example from above once more (free 10 bytes,\nused 10 bytes, and another free 10 bytes).\nGiven this (tiny) heap, what happens when an application calls free(10),\nthus returning the space in the middle of the heap? If we simply add this\nfree space back into our list without too much thinking, we might end up\nwith a list that looks like this:\nhead\naddr:10\nlen:10\naddr:0\nlen:10\naddr:20\nlen:10\nNULL\nNote the problem: while the entire heap is now free, it is seemingly\ndivided into three chunks of 10 bytes each. Thus, if a user requests 20\nbytes, a simple list traversal will not ﬁnd such a free chunk, and return\nfailure.\nWhat allocators do in order to avoid this problem is coalesce free space\nwhen a chunk of memory is freed. The idea is simple: when returning a\nfree chunk in memory, look carefully at the addresses of the chunk you\nare returning as well as the nearby chunks of free space; if the newly-\nfreed space sits right next to one (or two, as in this example) existing free\nchunks, merge them into a single larger free chunk. Thus, with coalesc-\ning, our ﬁnal list should look like this:\nhead\naddr:0\nlen:30\nNULL\nIndeed, this is what the heap list looked like at ﬁrst, before any allo-\ncations were made. With coalescing, an allocator can better ensure that\nlarge free extents are available for the application.\n3This discussion assumes that there are no headers, an unrealistic but simplifying assump-\ntion we make for now.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "page_number": 185,
      "chapter_number": 20,
      "summary": "The only real solution (as we will\nsee in forthcoming chapters) is to avoid the problem altogether, by never\nallocating memory in variable-sized chunks Key topics include segmentation, segment, and frees.",
      "keywords": [
        "free space",
        "free",
        "space",
        "memory",
        "address space",
        "SEGMENTATION",
        "free chunk",
        "chunk",
        "MANAGE FREE SPACE",
        "bytes",
        "address",
        "list",
        "systems",
        "heap",
        "size"
      ],
      "concepts": [
        "segmentation",
        "segment",
        "frees",
        "management",
        "manage",
        "allocation",
        "allocators",
        "allocated",
        "byte",
        "spaces"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 8",
          "chapter": 38,
          "title": "Segment 38 (pages 351-358)",
          "relevance_score": 0.6,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 43,
          "title": "Segment 43 (pages 858-879)",
          "relevance_score": 0.59,
          "method": "api"
        },
        {
          "book": "Effective-Python",
          "chapter": 35,
          "title": "Segment 35 (pages 369-376)",
          "relevance_score": 0.59,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 7",
          "chapter": 6,
          "title": "Segment 6 (pages 48-56)",
          "relevance_score": 0.59,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 33,
          "title": "Segment 33 (pages 662-682)",
          "relevance_score": 0.57,
          "method": "api"
        }
      ]
    },
    {
      "number": 21,
      "title": "Segment 21 (pages 193-203)",
      "start_page": 193,
      "end_page": 203,
      "detection_method": "topic_boundary",
      "content": "FREE-SPACE MANAGEMENT\n157\nptr\nThe header used by malloc library\nThe 20 bytes returned to caller\nFigure 17.1: An Allocated Region Plus Header\nsize:\n20\nmagic: 1234567\nhptr\nptr\nThe 20 bytes returned to caller\nFigure 17.2: Speciﬁc Contents Of The Header\nTracking The Size Of Allocated Regions\nYou might have noticed that the interface to free(void *ptr) does\nnot take a size parameter; thus it is assumed that given a pointer, the\nmalloc library can quickly determine the size of the region of memory\nbeing freed and thus incorporate the space back into the free list.\nTo accomplish this task, most allocators store a little bit of extra infor-\nmation in a header block which is kept in memory, usually just before\nthe handed-out chunk of memory. Let’s look at an example again (Fig-\nure 17.1). In this example, we are examining an allocated block of size 20\nbytes, pointed to by ptr; imagine the user called malloc() and stored\nthe results in ptr, e.g., ptr = malloc(20);.\nThe header minimally contains the size of the allocated region (in this\ncase, 20); it may also contain additional pointers to speed up dealloca-\ntion, a magic number to provide additional integrity checking, and other\ninformation. Let’s assume a simple header which contains the size of the\nregion and a magic number, like this:\ntypedef struct __header_t {\nint size;\nint magic;\n} header_t;\nThe example above would look like what you see in Figure 17.2. When\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n158\nFREE-SPACE MANAGEMENT\nthe user calls free(ptr), the library then uses simple pointer arithmetic\nto ﬁgure out where the header begins:\nvoid free(void *ptr) {\nheader_t *hptr = (void *)ptr - sizeof(header_t);\n}\nAfter obtaining such a pointer to the header, the library can easily de-\ntermine whether the magic number matches the expected value as a san-\nity check (assert(hptr->magic == 1234567)) and calculate the to-\ntal size of the newly-freed region via simple math (i.e., adding the size of\nthe header to size of the region). Note the small but critical detail in the\nlast sentence: the size of the free region is the size of the header plus the\nsize of the space allocated to the user. Thus, when a user requests N bytes\nof memory, the library does not search for a free chunk of size N; rather,\nit searches for a free chunk of size N plus the size of the header.\nEmbedding A Free List\nThus far we have treated our simple free list as a conceptual entity; it is\njust a list describing the free chunks of memory in the heap. But how do\nwe build such a list inside the free space itself?\nIn a more typical list, when allocating a new node, you would just call\nmalloc() when you need space for the node. Unfortunately, within the\nmemory-allocation library, you can’t do this! Instead, you need to build\nthe list inside the free space itself. Don’t worry if this sounds a little weird;\nit is, but not so weird that you can’t do it!\nAssume we have a 4096-byte chunk of memory to manage (i.e., the\nheap is 4KB). To manage this as a free list, we ﬁrst have to initialize said\nlist; initially, the list should have one entry, of size 4096 (minus the header\nsize). Here is the description of a node of the list:\ntypedef struct __node_t {\nint\nsize;\nstruct __node_t *next;\n} node_t;\nNow let’s look at some code that initializes the heap and puts the ﬁrst\nelement of the free list inside that space. We are assuming that the heap is\nbuilt within some free space acquired via a call to the system call mmap();\nthis is not the only way to build such a heap but serves us well in this\nexample. Here is the code:\n// mmap() returns a pointer to a chunk of free space\nnode_t *head = mmap(NULL, 4096, PROT_READ|PROT_WRITE,\nMAP_ANON|MAP_PRIVATE, -1, 0);\nhead->size\n= 4096 - sizeof(node_t);\nhead->next\n= NULL;\nAfter running this code, the status of the list is that it has a single entry,\nof size 4088. Yes, this is a tiny heap, but it serves as a ﬁne example for us\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nFREE-SPACE MANAGEMENT\n159\nsize:\n4088\nnext:\n0\n...\nhead\n[virtual address: 16KB]\nheader: size field\nheader: next field (NULL is 0)\nthe rest of the 4KB chunk\nFigure 17.3: A Heap With One Free Chunk\nsize:\n100\nmagic: 1234567\n. . .\nsize:\n3980\nnext:\n0\n. . .\nptr\n[virtual address: 16KB]\nhead\nThe 100 bytes now allocated\nThe free 3980 byte chunk\nFigure 17.4: A Heap: After One Allocation\nhere. The head pointer contains the beginning address of this range; let’s\nassume it is 16KB (though any virtual address would be ﬁne). Visually,\nthe heap thus looks like what you see in Figure 17.3.\nNow, let’s imagine that a chunk of memory is requested, say of size\n100 bytes. To service this request, the library will ﬁrst ﬁnd a chunk that is\nlarge enough to accommodate the request; because there is only one free\nchunk (size: 4088), this chunk will be chosen. Then, the chunk will be\nsplit into two: one chunk big enough to service the request (and header,\nas described above), and the remaining free chunk. Assuming an 8-byte\nheader (an integer size and an integer magic number), the space in the\nheap now looks like what you see in Figure 17.4.\nThus, upon the request for 100 bytes, the library allocated 108 bytes\nout of the existing one free chunk, returns a pointer (marked ptr in the\nﬁgure above) to it, stashes the header information immediately before the\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n160\nFREE-SPACE MANAGEMENT\nsize:\n100\nmagic: 1234567\n. . .\nsize:\n100\nmagic: 1234567\n. . .\nsize:\n100\nmagic: 1234567\n. . .\nsize:\n3764\nnext:\n0\n. . .\nsptr\n[virtual address: 16KB]\nhead\n100 bytes still allocated\n100 bytes still allocated\n (but about to be freed)\n100-bytes still allocated\nThe free 3764-byte chunk\nFigure 17.5: Free Space With Three Chunks Allocated\nallocated space for later use upon free(), and shrinks the one free node\nin the list to 3980 bytes (4088 minus 108).\nNow let’s look at the heap when there are three allocated regions, each\nof 100 bytes (or 108 including the header). A visualization of this heap is\nshown in Figure 17.5.\nAs you can see therein, the ﬁrst 324 bytes of the heap are now allo-\ncated, and thus we see three headers in that space as well as three 100-\nbyte regions being used by the calling program. The free list remains\nuninteresting: just a single node (pointed to by head), but now only 3764\nbytes in size after the three splits. But what happens when the calling\nprogram returns some memory via free()?\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nFREE-SPACE MANAGEMENT\n161\nsize:\n100\nmagic: 1234567\n. . .\nsize:\n100\nnext:\n16708\n. . .\nsize:\n100\nmagic: 1234567\n. . .\nsize:\n3764\nnext:\n0\n. . .\n[virtual address: 16KB]\nhead\nsptr\n100 bytes still allocated\n(now a free chunk of memory)\n100-bytes still allocated\nThe free 3764-byte chunk\nFigure 17.6: Free Space With Two Chunks Allocated\nIn this example, the application returns the middle chunk of allocated\nmemory, by calling free(16500) (the value 16500 is arrived upon by\nadding the start of the memory region, 16384, to the 108 of the previous\nchunk and the 8 bytes of the header for this chunk). This value is shown\nin the previous diagram by the pointer sptr.\nThe library immediately ﬁgures out the size of the free region, and\nthen adds the free chunk back onto the free list. Assuming we insert at\nthe head of the free list, the space now looks like this (Figure 17.6).\nAnd now we have a list that starts with a small free chunk (100 bytes,\npointed to by the head of the list) and a large free chunk (3764 bytes).\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n162\nFREE-SPACE MANAGEMENT\nsize:\n100\nnext:\n16492\n. . .\nsize:\n100\nnext:\n16708\n. . .\nsize:\n100\nnext:\n16384\n. . .\nsize:\n3764\nnext:\n0\n. . .\n[virtual address: 16KB]\nhead\n(now free)\n(now free)\n(now free)\nThe free 3764-byte chunk\nFigure 17.7: A Non-Coalesced Free List\nOur list ﬁnally has more than one element on it! And yes, the free space\nis fragmented, an unfortunate but common occurrence.\nOne last example: let’s assume now that the last two in-use chunks are\nfreed. Without coalescing, you might end up with a free list that is highly\nfragmented (see Figure 17.7).\nAs you can see from the ﬁgure, we now have a big mess! Why? Simple,\nwe forgot to coalesce the list. Although all of the memory is free, it is\nchopped up into pieces, thus appearing as a fragmented memory despite\nnot being one. The solution is simple: go through the list and merge\nneighboring chunks; when ﬁnished, the heap will be whole again.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nFREE-SPACE MANAGEMENT\n163\nGrowing The Heap\nWe should discuss one last mechanism found within many allocation li-\nbraries. Speciﬁcally, what should you do if the heap runs out of space?\nThe simplest approach is just to fail. In some cases this is the only option,\nand thus returning NULL is an honorable approach. Don’t feel bad! You\ntried, and though you failed, you fought the good ﬁght.\nMost traditional allocators start with a small-sized heap and then re-\nquest more memory from the OS when they run out. Typically, this means\nthey make some kind of system call (e.g., sbrk in most UNIX systems) to\ngrow the heap, and then allocate the new chunks from there. To service\nthe sbrk request, the OS ﬁnds free physical pages, maps them into the\naddress space of the requesting process, and then returns the value of\nthe end of the new heap; at that point, a larger heap is available, and the\nrequest can be successfully serviced.\n17.3\nBasic Strategies\nNow that we have some machinery under our belt, let’s go over some\nbasic strategies for managing free space. These approaches are mostly\nbased on pretty simple policies that you could think up yourself; try it\nbefore reading and see if you come up with all of the alternatives (or\nmaybe some new ones!).\nThe ideal allocator is both fast and minimizes fragmentation. Unfortu-\nnately, because the stream of allocation and free requests can be arbitrary\n(after all, they are determined by the programmer), any particular strat-\negy can do quite badly given the wrong set of inputs. Thus, we will not\ndescribe a “best” approach, but rather talk about some basics and discuss\ntheir pros and cons.\nBest Fit\nThe best ﬁt strategy is quite simple: ﬁrst, search through the free list and\nﬁnd chunks of free memory that are as big or bigger than the requested\nsize. Then, return the one that is the smallest in that group of candidates;\nthis is the so called best-ﬁt chunk (it could be called smallest ﬁt too). One\npass through the free list is enough to ﬁnd the correct block to return.\nThe intuition behind best ﬁt is simple: by returning a block that is close\nto what the user asks, best ﬁt tries to reduce wasted space. However, there\nis a cost; naive implementations pay a heavy performance penalty when\nperforming an exhaustive search for the correct free block.\nWorst Fit\nThe worst ﬁt approach is the opposite of best ﬁt; ﬁnd the largest chunk\nand return the requested amount; keep the remaining (large) chunk on\nthe free list. Worst ﬁt tries to thus leave big chunks free instead of lots of\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n164\nFREE-SPACE MANAGEMENT\nsmall chunks that can arise from a best-ﬁt approach. Once again, how-\never, a full search of free space is required, and thus this approach can be\ncostly. Worse, most studies show that it performs badly, leading to excess\nfragmentation while still having high overheads.\nFirst Fit\nThe ﬁrst ﬁt method simply ﬁnds the ﬁrst block that is big enough and\nreturns the requested amount to the user. As before, the remaining free\nspace is kept free for subsequent requests.\nFirst ﬁt has the advantage of speed – no exhaustive search of all the\nfree spaces are necessary – but sometimes pollutes the beginning of the\nfree list with a small objects. Thus, how the allocator manages the free\nlist’s order becomes an issue. One approach is to use address-based or-\ndering; by keeping the list ordered by the address of the free space, coa-\nlescing becomes easier, and fragmentation tends to be reduced.\nNext Fit\nInstead of always beginning the ﬁrst-ﬁt search at the beginning of the list,\nthe next ﬁt algorithm keeps an extra pointer to the location within the\nlist where one was looking last. The idea is to spread the searches for\nfree space throughout the list more uniformly, thus avoiding splintering\nof the beginning of the list. The performance of such an approach is quite\nsimilar to ﬁrst ﬁt, as an exhaustive search is once again avoided.\nExamples\nHere are a few examples of the above strategies. Envision a free list with\nthree elements on it, of sizes 10, 30, and 20 (we’ll ignore headers and other\ndetails here, instead just focusing on how strategies operate):\nhead\n10\n30\n20\nNULL\nAssume an allocation request of size 15. A best-ﬁt approach would\nsearch the entire list and ﬁnd that 20 was the best ﬁt, as it is the smallest\nfree space that can accommodate the request. The resulting free list:\nhead\n10\n30\n5\nNULL\nAs happens in this example, and often happens with a best-ﬁt ap-\nproach, a small free chunk is now left over. A worst-ﬁt approach is similar\nbut instead ﬁnds the largest chunk, in this example 30. The resulting list:\nhead\n10\n15\n20\nNULL\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nFREE-SPACE MANAGEMENT\n165\nThe ﬁrst-ﬁt strategy, in this example, does the same thing as worst-ﬁt,\nalso ﬁnding the ﬁrst free block that can satisfy the request. The difference\nis in the search cost; both best-ﬁt and worst-ﬁt look through the entire list;\nﬁrst-ﬁt only examines free chunks until it ﬁnds one that ﬁts, thus reducing\nsearch cost.\nThese examples just scratch the surface of allocation policies. More\ndetailed analysis with real workloads and more complex allocator behav-\niors (e.g., coalescing) are required for a deeper understanding. Perhaps\nsomething for a homework section, you say?\n17.4\nOther Approaches\nBeyond the basic approaches described above, there have been a host\nof suggested techniques and algorithms to improve memory allocation in\nsome way. We list a few of them here for your consideration (i.e., to make\nyou think about a little more than just best-ﬁt allocation).\nSegregated Lists\nOne interesting approach that has been around for some time is the use\nof segregated lists. The basic idea is simple: if a particular application\nhas one (or a few) popular-sized request that it makes, keep a separate\nlist just to manage objects of that size; all other requests are forwarded to\na more general memory allocator.\nThe beneﬁts of such an approach are obvious. By having a chunk of\nmemory dedicated for one particular size of requests, fragmentation is\nmuch less of a concern; moreover, allocation and free requests can be\nserved quite quickly when they are of the right size, as no complicated\nsearch of a list is required.\nJust like any good idea, this approach introduces new complications\ninto a system as well. For example, how much memory should one ded-\nicate to the pool of memory that serves specialized requests of a given\nsize, as opposed to the general pool? One particular allocator, the slab\nallocator by uber-engineer Jeff Bonwick (which was designed for use in\nthe Solaris kernel), handles this issue in a rather nice way [B94].\nSpeciﬁcally, when the kernel boots up, it allocates a number of object\ncaches for kernel objects that are likely to be requested frequently (such as\nlocks, ﬁle-system inodes, etc.); the object caches thus are each segregated\nfree lists of a given size and serve memory allocation and free requests\nquickly. When a given cache is running low on free space, it requests\nsome slabs of memory from a more general memory allocator (the to-\ntal amount requested being a multiple of the page size and the object in\nquestion). Conversely, when the reference counts of the objects within\na given slab all go to zero, the general allocator can reclaim them from\nthe specialized allocator, which is often done when the VM system needs\nmore memory.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n166\nFREE-SPACE MANAGEMENT\nASIDE: GREAT ENGINEERS ARE REALLY GREAT\nEngineers like Jeff Bonwick (who not only wrote the slab allocator men-\ntioned herein but also was the lead of an amazing ﬁle system, ZFS) are\nthe heart of Silicon Valley. Behind almost any great product or technol-\nogy is a human (or small group of humans) who are way above average\nin their talents, abilities, and dedication. As Mark Zuckerberg (of Face-\nbook) says: “Someone who is exceptional in their role is not just a little\nbetter than someone who is pretty good. They are 100 times better.” This\nis why, still today, one or two people can start a company that changes\nthe face of the world forever (think Google, Apple, or Facebook). Work\nhard and you might become such a “100x” person as well. Failing that,\nwork with such a person; you’ll learn more in day than most learn in a\nmonth. Failing that, feel sad.\nThe slab allocator also goes beyond most segregated list approaches\nby keeping free objects on the lists in a pre-initialized state. Bonwick\nshows that initialization and destruction of data structures is costly [B94];\nby keeping freed objects in a particular list in their initialized state, the\nslab allocator thus avoids frequent initialization and destruction cycles\nper object and thus lowers overheads noticeably.\nBuddy Allocation\nBecause coalescing is critical for an allocator, some approaches have been\ndesigned around making coalescing simple. One good example is found\nin the binary buddy allocator [K65].\nIn such a system, free memory is ﬁrst conceptually thought of as one\nbig space of size 2N. When a request for memory is made, the search for\nfree space recursively divides free space by two until a block that is big\nenough to accommodate the request is found (and a further split into two\nwould result in a space that is too small). At this point, the requested\nblock is returned to the user. Here is an example of a 64KB free space\ngetting divided in the search for a 7KB block:\n64 KB\n32 KB\n32 KB\n16 KB\n16 KB\n8 KB 8 KB\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nFREE-SPACE MANAGEMENT\n167\nIn the example, the leftmost 8KB block is allocated (as indicated by the\ndarker shade of gray) and returned to the user; note that this scheme can\nsuffer from internal fragmentation, as you are only allowed to give out\npower-of-two-sized blocks.\nThe beauty of buddy allocation is found in what happens when that\nblock is freed. When returning the 8KB block to the free list, the allocator\nchecks whether the “buddy” 8KB is free; if so, it coalesces the two blocks\ninto a 16KB block. The allocator then checks if the buddy of the 16KB\nblock is still free; if so, it coalesces those two blocks. This recursive coa-\nlescing process continues up the tree, either restoring the entire free space\nor stopping when a buddy is found to be in use.\nThe reason buddy allocation works so well is that it is simple to de-\ntermine the buddy of a particular block. How, you ask? Think about the\naddresses of the blocks in the free space above. If you think carefully\nenough, you’ll see that the address of each buddy pair only differs by\na single bit; which bit is determined by the level in the buddy tree. And\nthus you have a basic idea of how binary buddy allocation schemes work.\nFor more detail, as always, see the Wilson survey [W+95].\nOther Ideas\nOne major problem with many of the approaches described above is their\nlack of scaling.\nSpeciﬁcally, searching lists can be quite slow.\nThus,\nadvanced allocators use more complex data structures to address these\ncosts, trading simplicity for performance. Examples include balanced bi-\nnary trees, splay trees, or partially-ordered trees [W+95].\nGiven that modern systems often have multiple processors and run\nmulti-threaded workloads (something you’ll learn about in great detail\nin the section of the book on Concurrency), it is not surprising that a lot\nof effort has been spent making allocators work well on multiprocessor-\nbased systems. Two wonderful examples are found in Berger et al. [B+00]\nand Evans [E06]; check them out for the details.\nThese are but two of the thousands of ideas people have had over time\nabout memory allocators. Read on your own if you are curious.\n17.5\nSummary\nIn this chapter, we’ve discussed the most rudimentary forms of mem-\nory allocators. Such allocators exist everywhere, linked into every C pro-\ngram you write, as well as in the underlying OS which is managing mem-\nory for its own data structures. As with many systems, there are many\ntrade-offs to be made in building such a system, and the more you know\nabout the exact workload presented to an allocator, the more you could do\nto tune it to work better for that workload. Making a fast, space-efﬁcient,\nscalable allocator that works well for a broad range of workloads remains\nan on-going challenge in modern computer systems.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "page_number": 193,
      "chapter_number": 21,
      "summary": "This chapter covers segment 21 (pages 193-203). Key topics include allocated, allocators, and allocation.",
      "keywords": [
        "free",
        "free list",
        "free space",
        "free chunk",
        "size",
        "list",
        "chunk",
        "space",
        "Free Chunk size",
        "header",
        "memory",
        "FREE-SPACE MANAGEMENT",
        "heap",
        "Allocated",
        "small free chunk"
      ],
      "concepts": [
        "allocated",
        "allocators",
        "allocation",
        "allocate",
        "free",
        "size",
        "list",
        "chunk",
        "header",
        "requests"
      ],
      "similar_chapters": [
        {
          "book": "Game Programming Gems 7",
          "chapter": 6,
          "title": "Segment 6 (pages 48-56)",
          "relevance_score": 0.78,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 38,
          "title": "Segment 38 (pages 351-358)",
          "relevance_score": 0.76,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 33,
          "title": "Segment 33 (pages 662-682)",
          "relevance_score": 0.75,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 43,
          "title": "Segment 43 (pages 858-879)",
          "relevance_score": 0.72,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 44,
          "title": "Segment 44 (pages 880-902)",
          "relevance_score": 0.72,
          "method": "api"
        }
      ]
    },
    {
      "number": 22,
      "title": "Segment 22 (pages 204-211)",
      "start_page": 204,
      "end_page": 211,
      "detection_method": "topic_boundary",
      "content": "168\nFREE-SPACE MANAGEMENT\nReferences\n[B+00] “Hoard: A Scalable Memory Allocator for Multithreaded Applications”\nEmery D. Berger, Kathryn S. McKinley, Robert D. Blumofe, and Paul R. Wilson\nASPLOS-IX, November 2000\nBerger and company’s excellent allocator for multiprocessor systems. Beyond just being a fun paper,\nalso used in practice!\n[B94] “The Slab Allocator: An Object-Caching Kernel Memory Allocator”\nJeff Bonwick\nUSENIX ’94\nA cool paper about how to build an allocator for an operating system kernel, and a great example of how\nto specialize for particular common object sizes.\n[E06] “A Scalable Concurrent malloc(3) Implementation for FreeBSD”\nJason Evans\nhttp://people.freebsd.org/˜jasone/jemalloc/bsdcan2006/jemalloc.pdf\nApril 2006\nA detailed look at how to build a real modern allocator for use in multiprocessors. The “jemalloc”\nallocator is in widespread use today, within FreeBSD, NetBSD, Mozilla Firefox, and within Facebook.\n[K65] “A Fast Storage Allocator”\nKenneth C. Knowlton\nCommunications of the ACM, Volume 8, Number 10, October 1965\nThe common reference for buddy allocation. Random strange fact: Knuth gives credit for the idea to not\nto Knowlton but to Harry Markowitz, a Nobel-prize winning economist. Another strange fact: Knuth\ncommunicates all of his emails via a secretary; he doesn’t send email himself, rather he tells his secretary\nwhat email to send and then the secretary does the work of emailing. Last Knuth fact: he created TeX,\nthe tool used to typeset this book. It is an amazing piece of software4.\n[W+95] “Dynamic Storage Allocation: A Survey and Critical Review”\nPaul R. Wilson, Mark S. Johnstone, Michael Neely, David Boles\nInternational Workshop on Memory Management\nKinross, Scotland, September 1995\nAn excellent and far-reaching survey of many facets of memory allocation. Far too much detail to go\ninto in this tiny chapter!\n4Actually we use LaTeX, which is based on Lamport’s additions to TeX, but close enough.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\n18\nPaging: Introduction\nRemember our goal: to virtualize memory. Segmentation (a generaliza-\ntion of dynamic relocation) helped us do this, but has some problems; in\nparticular, managing free space becomes quite a pain as memory becomes\nfragmented and segmentation is not as ﬂexible as we might like. Is there\na better solution?\nTHE CRUX:\nHOW TO VIRTUALIZE MEMORY WITHOUT SEGMENTS\nHow can we virtualize memory in a way as to avoid the problems of\nsegmentation? What are the basic techniques? How do we make those\ntechniques work well?\nThus comes along the idea of paging, which goes back to the earliest\nof computer systems, namely the Atlas [KE+62,L78]. Instead of splitting\nup our address space into three logical segments (each of variable size),\nwe split up our address space into ﬁxed-sized units we call a page. Here\nin Figure 18.1 an example of a tiny address space, only 64 bytes total in\nsize, with 16 byte pages (real address spaces are much bigger, of course,\ncommonly 32 bits and thus 4-GB of address space, or even 64 bits). We’ll\nuse tiny examples to make them easier to digest (at ﬁrst).\n64\n48\n32\n16\n0\n(page 3)\n(page 2)\n(page 1)\n(page 0 of the address space)\nFigure 18.1: A Simple 64-byte Address Space\n169\n\n\n170\nPAGING: INTRODUCTION\n128\n112\n96\n80\n64\n48\n32\n16\n0\npage frame 7\npage frame 6\npage frame 5\npage frame 4\npage frame 3\npage frame 2\npage frame 1\npage frame 0 of physical memory\nreserved for OS\n(unused)\npage 3 of AS\npage 0 of AS\n(unused)\npage 2 of AS\n(unused)\npage 1 of AS\nFigure 18.2: 64-Byte Address Space Placed In Physical Memory\nThus, we have an address space that is split into four pages (0 through\n3). With paging, physical memory is also split into some number of pages\nas well; we sometimes will call each page of physical memory a page\nframe. For an example, let’s examine Figure 18.2.\nPaging, as we will see, has a number of advantages over our previous\napproaches. Probably the most important improvement will be ﬂexibility:\nwith a fully-developed paging approach, the system will be able to sup-\nport the abstraction of an address space effectively, regardless of how the\nprocesses uses the address space; we won’t, for example, have to make\nassumptions about how the heap and stack grow and how they are used.\nAnother advantage is the simplicity of free-space management that pag-\ning affords. For example, when the OS wishes to place our tiny 64-byte\naddress space from above into our 8-page physical memory, it simply\nﬁnds four free pages; perhaps the OS keeps a free list of all free pages for\nthis, and just grabs the ﬁrst four free pages off of this list. In the exam-\nple above, the OS has placed virtual page 0 of the address space (AS) in\nphysical page 3, virtual page 1 of the AS on physical page 7, page 2 on\npage 5, and page 3 on page 2.\nTo record where each virtual page of the address space is placed in\nphysical memory, the operating system keeps a per-process data structure\nknown as a page table. The major role of the page table is to store address\ntranslations for each of the virtual pages of the address space, thus letting\nus know where in physical memory they live. For our simple example\nabove (Figure 18.2), the page table would thus have the following entries:\n(Virtual Page 0 →Physical Frame 3), (VP 1 →PF 7), (VP 2 →PF 5), and\n(VP 3 →PF 2).\nIt is important to remember that this page table is a per-process data\nstructure (most page table structures we discuss are per-process struc-\ntures; an exception we’ll touch on is the inverted page table). If another\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nPAGING: INTRODUCTION\n171\nprocess were to run in our example above, the OS would have to manage\na different page table for it, as its virtual pages obviously map to different\nphysical pages (modulo any sharing going on).\nNow, we know enough to perform an address-translation example.\nLet’s imagine the process with that tiny address space (64 bytes) is per-\nforming a memory access:\nmovl <virtual address>, %eax\nSpeciﬁcally, let’s pay attention to the explicit load of the data at <virtual\naddress> into the register eax (and thus ignore the instruction fetch that\nmust have happened prior).\nTo translate this virtual address that the process generated, we have to\nﬁrst split it into two components: the virtual page number (VPN), and\nthe offset within the page. For this example, because the virtual address\nspace of the process is 64 bytes, we need 6 bits total for our virtual address\n(26 = 64). Thus, our virtual address:\nVa5 Va4 Va3 Va2 Va1 Va0\nwhere Va5 is the highest-order bit of the virtual address, and Va0 the\nlowest order bit. Because we know the page size (16 bytes), we can further\ndivide the virtual address as follows:\nVa5 Va4 Va3 Va2 Va1 Va0\nVPN\noffset\nThe page size is 16 bytes in a 64-byte address space; thus we need to\nbe able to select 4 pages, and the top 2 bits of the address do just that.\nThus, we have a 2-bit virtual page number (VPN). The remaining bits tell\nus which byte of the page we are interested in, 4 bits in this case; we call\nthis the offset.\nWhen a process generates a virtual address, the OS and hardware\nmust combine to translate it into a meaningful physical address. For ex-\nample, let us assume the load above was to virtual address 21:\nmovl 21, %eax\nTurning “21” into binary form, we get “010101”, and thus we can ex-\namine this virtual address and see how it breaks down into a virtual page\nnumber (VPN) and offset:\n0\n1\n0\n1\n0\n1\nVPN\noffset\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n172\nPAGING: INTRODUCTION\n0\n1\n0\n1\n0\n1\nVPN\noffset\n1\n1\n1\n0\n1\n0\n1\nAddress\nTranslation\nPFN\noffset\nVirtual\nAddress\nPhysical\nAddress\nFigure 18.3: The Address Translation Process\nThus, the virtual address “21” is on the 5th (“0101”th) byte of vir-\ntual page “01” (or 1). With our virtual page number, we can now index\nour page table and ﬁnd which physical page that virtual page 1 resides\nwithin. In the page table above the physical page number (PPN) (a.k.a.\nphysical frame number or PFN) is 7 (binary 111). Thus, we can translate\nthis virtual address by replacing the VPN with the PFN and then issue\nthe load to physical memory (Figure 18.3).\nNote the offset stays the same (i.e., it is not translated), because the\noffset just tells us which byte within the page we want. Our ﬁnal physical\naddress is 1110101 (117 in decimal), and is exactly where we want our\nload to fetch data from (Figure 18.2).\n18.1\nWhere Are Page Tables Stored?\nPage tables can get awfully large, much bigger than the small segment\ntable or base/bounds pair we have discussed previously. For example,\nimagine a typical 32-bit address space, with 4-KB pages. This virtual ad-\ndress splits into a 20-bit VPN and 12-bit offset (recall that 10 bits would\nbe needed for a 1-KB page size, and just add two more to get to 4 KB).\nA 20-bit VPN implies that there are 220 translations that the OS would\nhave to manage for each process (that’s roughly a million); assuming we\nneed 4 bytes per page table entry (PTE) to hold the physical translation\nplus any other useful stuff, we get an immense 4MB of memory needed\nfor each page table! That is pretty big. Now imagine there are 100 pro-\ncesses running: this means the OS would need 400MB of memory just for\nall those address translations! Even in the modern era, where machines\nhave gigabytes of memory, it seems a little crazy to use a large chunk of\nif just for translations, no?\nBecause page tables are so big, we don’t keep any special on-chip hard-\nware in the MMU to store the page table of the currently-running process.\nInstead, we store the page table for each process in memory somewhere.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nPAGING: INTRODUCTION\n173\n128\n112\n96\n80\n64\n48\n32\n16\n0\npage frame 7\npage frame 6\npage frame 5\npage frame 4\npage frame 3\npage frame 2\npage frame 1\npage frame 0 of physical memory\n(unused)\npage 3 of AS\npage 0 of AS\n(unused)\npage 2 of AS\n(unused)\npage 1 of AS\npage table:\n3 7 5 2\nFigure 18.4: Example: Page Table in Kernel Physical Memory\nLet’s assume for now that the page tables live in physical memory that\nthe OS manages. In Figure 18.4 is a picture of what that might look like.\n18.2\nWhat’s Actually In The Page Table?\nLet’s talk a little about page table organization. The page table is just a\ndata structure that is used to map virtual addresses (or really, virtual page\nnumbers) to physical addresses (physical page numbers). Thus, any data\nstructure could work. The simplest form is called a linear page table,\nwhich is just an array. The OS indexes the array by the VPN, and looks up\nthe page-table entry (PTE) at that index in order to ﬁnd the desired PFN.\nFor now, we will assume this simple linear structure; in later chapters,\nwe will make use of more advanced data structures to help solve some\nproblems with paging.\nAs for the contents of each PTE, we have a number of different bits\nin there worth understanding at some level. A valid bit is common to\nindicate whether the particular translation is valid; for example, when\na program starts running, it will have code and heap at one end of its\naddress space, and the stack at the other. All the unused space in-between\nwill be marked invalid, and if the process tries to access such memory, it\nwill generate a trap to the OS which will likely terminate the process.\nThus, the valid bit is crucial for supporting a sparse address space; by\nsimply marking all the unused pages in the address space invalid, we\nremove the need to allocate physical frames for those pages and thus save\na great deal of memory.\nWe also might have protection bits, indicating whether the page could\nbe read from, written to, or executed from. Again, accessing a page in a\nway not allowed by these bits will generate a trap to the OS.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n174\nPAGING: INTRODUCTION\n31 30 29 28 27 26 25 24 23 22 21 20 19 18 17 16 15 14 13 12 11 10\n9\n8\n7\n6\n5\n4\n3\n2\n1\n0\nPFN\nG\nPAT\nD\nA\nPCD\nPWT\nU/S\nR/W\nP\nFigure 18.5: An x86 Page Table Entry (PTE)\nThere are a couple of other bits that are important but we won’t talk\nabout much for now. A present bit indicates whether this page is in phys-\nical memory or on disk (swapped out); we will understand this in more\ndetail when we study how to move parts of the address space to disk\nand back in order to support address spaces that are larger than physical\nmemory and allow for the pages of processes that aren’t actively being\nrun to be swapped out. A dirty bit is also common, indicating whether\nthe page has been modiﬁed since it was brought into memory.\nA reference bit (a.k.a. accessed bit) is sometimes used to track whether\na page has been accessed, and is useful in determining which pages are\npopular and thus should be kept in memory; such knowledge is critical\nduring page replacement, a topic we will study in great detail in subse-\nquent chapters.\nFigure 18.5 shows an example page table entry from the x86 architec-\nture [I09]. It contains a present bit (P); a read/write bit (R/W) which\ndetermines if writes are allowed to this page; a user/supervisor bit (U/S)\nwhich determines if user-mode processes can access the page; a few bits\n(PWT, PCD, PAT, and G) that determine how hardware caching works for\nthese pages; an accessed bit (A) and a dirty bit (D); and ﬁnally, the page\nframe number (PFN) itself.\nRead the Intel Architecture Manuals [I09] for more details on x86 pag-\ning support. Be forewarned, however; reading manuals such as these,\nwhile quite informative (and certainly necessary for those who write code\nto use such page tables in the OS), can be challenging at ﬁrst. A little pa-\ntience, and a lot of desire, is required.\n18.3\nPaging: Also Too Slow\nWith page tables in memory, we already know that they might be too\nbig. Turns out they can slow things down too. For example, take our\nsimple instruction:\nmovl 21, %eax\nAgain, let’s just examine the explicit reference to address 21 and not\nworry about the instruction fetch. In this example, we will assume the\nhardware performs the translation for us. To fetch the desired data, the\nsystem must ﬁrst translate the virtual address (21) into the correct physi-\ncal address (117). Thus, before issuing the load to address 117, the system\nmust ﬁrst fetch the proper page table entry from the process’s page ta-\nble, perform the translation, and then ﬁnally get the desired data from\nphysical memory.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nPAGING: INTRODUCTION\n175\nTo do so, the hardware must know where the page table is for the\ncurrently-running process. Let’s assume for now that a single page-table\nbase register contains the physical address of the starting location of the\npage table. To ﬁnd the location of the desired PTE, the hardware will thus\nperform the following functions:\nVPN\n= (VirtualAddress & VPN_MASK) >> SHIFT\nPTEAddr = PageTableBaseRegister + (VPN * sizeof(PTE))\nIn our example, VPN MASK would be set to 0x30 (hex 30, or binary\n110000) which picks out the VPN bits from the full virtual address; SHIFT\nis set to 4 (the number of bits in the offset), such that we move the VPN\nbits down to form the correct integer virtual page number. For exam-\nple, with virtual address 21 (010101), and masking turns this value into\n010000; the shift turns it into 01, or virtual page 1, as desired. We then use\nthis value as an index into the array of PTEs pointed to by the page table\nbase register.\nOnce this physical address is known, the hardware can fetch the PTE\nfrom memory, extract the PFN, and concatenate it with the offset from\nthe virtual address to form the desired physical address. Speciﬁcally, you\ncan think of the PFN being left-shifted by SHIFT, and then logically OR’d\nwith the offset to form the ﬁnal address as follows:\noffset\n= VirtualAddress & OFFSET_MASK\nPhysAddr = (PFN << SHIFT) | offset\n1\n// Extract the VPN from the virtual address\n2\nVPN = (VirtualAddress & VPN_MASK) >> SHIFT\n3\n4\n// Form the address of the page-table entry (PTE)\n5\nPTEAddr = PTBR + (VPN * sizeof(PTE))\n6\n7\n// Fetch the PTE\n8\nPTE = AccessMemory(PTEAddr)\n9\n10\n// Check if process can access the page\n11\nif (PTE.Valid == False)\n12\nRaiseException(SEGMENTATION_FAULT)\n13\nelse if (CanAccess(PTE.ProtectBits) == False)\n14\nRaiseException(PROTECTION_FAULT)\n15\nelse\n16\n// Access is OK: form physical address and fetch it\n17\noffset\n= VirtualAddress & OFFSET_MASK\n18\nPhysAddr = (PTE.PFN << PFN_SHIFT) | offset\n19\nRegister = AccessMemory(PhysAddr)\nFigure 18.6: Accessing Memory With Paging\nFinally, the hardware can fetch the desired data from memory and put\nit into register eax. The program has now succeeded at loading a value\nfrom memory!\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "page_number": 204,
      "chapter_number": 22,
      "summary": "This chapter covers segment 22 (pages 204-211). Key topics include paging, bits. [B94] “The Slab Allocator: An Object-Caching Kernel Memory Allocator”\nJeff Bonwick\nUSENIX ’94\nA cool paper about how to build an allocator for an operating system kernel, and a great example of how\nto specialize for particular common object sizes.",
      "keywords": [
        "page table",
        "address",
        "address space",
        "virtual address",
        "page frame",
        "virtual page",
        "virtual page number",
        "Memory",
        "physical memory",
        "virtual",
        "VPN",
        "physical",
        "pages",
        "page table entry",
        "physical page"
      ],
      "concepts": [
        "paging",
        "bits",
        "bit",
        "address",
        "addresses",
        "memory",
        "allocator",
        "allocation",
        "allocate",
        "offset"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 42,
          "title": "Segment 42 (pages 838-857)",
          "relevance_score": 0.8,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 7",
          "chapter": 6,
          "title": "Segment 6 (pages 48-56)",
          "relevance_score": 0.77,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 43,
          "title": "Segment 43 (pages 858-879)",
          "relevance_score": 0.75,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 38,
          "title": "Segment 38 (pages 351-358)",
          "relevance_score": 0.74,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 33,
          "title": "Segment 33 (pages 662-682)",
          "relevance_score": 0.72,
          "method": "api"
        }
      ]
    },
    {
      "number": 23,
      "title": "Segment 23 (pages 212-221)",
      "start_page": 212,
      "end_page": 221,
      "detection_method": "topic_boundary",
      "content": "176\nPAGING: INTRODUCTION\nASIDE: DATA STRUCTURE – THE PAGE TABLE\nOne of the most important data structures in the memory management\nsubsystem of a modern OS is the page table. In general, a page table\nstores virtual-to-physical address translations, thus letting the system\nknow where each page of an address space actually resides in physical\nmemory. Because each address space requires such translations, in gen-\neral there is one page table per process in the system. The exact structure\nof the page table is either determined by the hardware (older systems) or\ncan be more ﬂexibly managed by the OS (modern systems).\nTo summarize, we now describe the initial protocol for what happens\non each memory reference. Figure 18.6 shows the basic approach. For\nevery memory reference (whether an instruction fetch or an explicit load\nor store), paging requires us to perform one extra memory reference in\norder to ﬁrst fetch the translation from the page table. That is a lot of\nwork! Extra memory references are costly, and in this case will likely\nslow down the process by a factor of two or more.\nAnd now you can hopefully see that there are two real problems that\nwe must solve. Without careful design of both hardware and software,\npage tables will cause the system to run too slowly, as well as take up\ntoo much memory. While seemingly a great solution for our memory\nvirtualization needs, these two crucial problems must ﬁrst be overcome.\n18.4\nA Memory Trace\nBefore closing, we now trace through a simple memory access exam-\nple to demonstrate all of the resulting memory accesses that occur when\nusing paging. The code snippet (in C, in a ﬁle called array.c) that are\ninterested in is as follows:\nint array[1000];\n...\nfor (i = 0; i < 1000; i++)\narray[i] = 0;\nWe could then compile array.c and run it with the following com-\nmands:\nprompt> gcc -o array array.c -Wall -O\nprompt> ./array\nOf course, to truly understand what memory accesses this code snip-\npet (which simply initializes an array) will make, we’ll have to know (or\nassume) a few more things. First, we’ll have to disassemble the result-\ning binary (using objdump on Linux, or otool on a Mac) to see what\nassembly instructions are used to initialize the array in a loop. Here it the\nresulting assembly code:\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nPAGING: INTRODUCTION\n177\n0x1024 movl $0x0,(%edi,%eax,4)\n0x1028 incl %eax\n0x102c cmpl $0x03e8,%eax\n0x1030 jne\n0x1024\nThe code, if you know a little x86, is actually quite easy to understand.\nThe ﬁrst instruction moves the value zero (shown as $0x0) into the vir-\ntual memory address of the location of the array; this address is computed\nby taking the contents of %edi and adding %eax multiplied by four to it.\nThus, %edi holds the base address of the array, whereas %eax holds the\narray index (i); we multiply by four because the array is an array of inte-\ngers, each size four bytes (note we are cheating a little bit here, assuming\neach instruction is four bytes in size for simplicity; in actuality, x86 in-\nstructions are variable-sized).\nThe second instruction increments the array index held in %eax, and\nthe third instruction compares the contents of that register to the hex\nvalue 0x03e8, or decimal 1000. If the comparison shows that that two\nvalues are not yet equal (which is what the jne instruction tests), the\nfourth instruction jumps back to the top of the loop.\nTo understand which memory accesses this instruction sequence makes\n(at both the virtual and physical levels), we’ll have assume something\nabout where in virtual memory the code snippet and array are found, as\nwell as the contents and location of the page table.\nFor this example, we assume a virtual address space of size 64 KB\n(unrealistically small). We also assume a page size of 1 KB.\nAll we need to know now are the contents of the page table, and its\nlocation in physical memory. Let’s assume we have a linear (array-based)\npage table and that it is located at physical address 1 KB (1024).\nAs for its contents, there are just a few virtual pages we need to worry\nabout having mapped for this example. First, there is the virtual page the\ncode lives on. Because the page size is 1 KB, virtual address 1024 resides\non the the second page of the virtual address space (VPN=1, as VPN=0 is\nthe ﬁrst page). Let’s assume this virtual page maps to physical frame 4\n(VPN 1 →PFN 4).\nNext, there is the array itself. Its size is 4000 bytes (1000 integers), and\nit lives at virtual addresses 40000 through 44000 (not including the last\nbyte). The virtual pages for this decimal range is VPN=39 ... VPN=42.\nThus, we need mappings for these pages. Let’s assume these virtual-to-\nphysical mappings for the example: (VPN 39 →PFN 7), (VPN 40 →PFN 8),\n(VPN 41 →PFN 9), (VPN 42 →PFN 10).\nWe are now ready to trace the memory references of the program.\nWhen it runs, each instruction fetch will generate two memory references:\none to the page table to ﬁnd the physical frame that the instruction resides\nwithin, and one to the instruction itself to fetch it to the CPU for process-\ning. In addition, there is one explicit memory reference in the form of\nthe mov instruction; this adds another page table access ﬁrst (to translate\nthe array virtual address to the correct physical one) and then the array\naccess itself.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n178\nPAGING: INTRODUCTION\n0\n10\n20\n30\n40\n50\n1024\n1074\n1124\nMemory Access\nCode (VA)\n40000\n40050\n40100\nArray (VA)\n1024\n1074\n1124\n1174\n1224\nPage Table (PA)\n4096\n4146\n4196\nCode (PA)\n7232\n7282\n7132\nArray (PA)\nmov\ninc\ncmp\njne\nmov\nPageTable[1]\nPageTable[39]\nFigure 18.7: A Virtual (And Physical) Memory Trace\nThe entire process, for the ﬁrst ﬁve loop iterations, is depicted in Fig-\nure 18.7. The bottom most graph shows the instruction memory refer-\nences on the y-axis in black (with virtual addresses on the left, and the\nactual physical addresses on the right); the middle graph shows array\naccesses in dark gray (again with virtual on left and physical on right); ﬁ-\nnally, the topmost graph shows page table memory accesses in light gray\n(just physical, as the page table in this example resides in physical mem-\nory). The x-axis, for the entire trace, shows memory accesses across the\nﬁrst ﬁve iterations of the loop (there are 10 memory accesses per loop,\nwhich includes four instruction fetches, one explicit update of memory,\nand ﬁve page table accesses to translate those four fetches and one explicit\nupdate).\nSee if you can make sense of the patterns that show up in this visu-\nalization. In particular, what will change as the loop continues to run\nbeyond these ﬁrst ﬁve iterations? Which new memory locations will be\naccessed? Can you ﬁgure it out?\nThis has just been the simplest of examples (only a few lines of C code),\nand yet you might already be able to sense the complexity of understand-\ning the actual memory behavior of real applications. Don’t worry: it deﬁ-\nnitely gets worse, because the mechanisms we are about to introduce only\ncomplicate this already complex machinery. Sorry!\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nPAGING: INTRODUCTION\n179\n18.5\nSummary\nWe have introduced the concept of paging as a solution to our chal-\nlenge of virtualizing memory. Paging has many advantages over previ-\nous approaches (such as segmentation). First, it does not lead to external\nfragmentation, as paging (by design) divides memory into ﬁxed-sized\nunits. Second, it is quite ﬂexible, enabling the sparse use of virtual ad-\ndress spaces.\nHowever, implementing paging support without care will lead to a\nslower machine (with many extra memory accesses to access the page\ntable) as well as memory waste (with memory ﬁlled with page tables in-\nstead of useful application data). We’ll thus have to think a little harder\nto come up with a paging system that not only works, but works well.\nThe next two chapters, fortunately, will show us how to do so.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n180\nPAGING: INTRODUCTION\nReferences\n[KE+62] “One-level Storage System”\nT. Kilburn, and D.B.G. Edwards and M.J. Lanigan and F.H. Sumner\nIRE Trans. EC-11, 2 (1962), pp. 223-235\n(Reprinted in Bell and Newell, “Computer Structures: Readings and Examples” McGraw-Hill,\nNew York, 1971).\nThe Atlas pioneered the idea of dividing memory into ﬁxed-sized pages and in many senses was an early\nform of the memory-management ideas we see in modern computer systems.\n[I09] “Intel 64 and IA-32 Architectures Software Developer’s Manuals”\nIntel, 2009\nAvailable: http://www.intel.com/products/processor/manuals\nIn particular, pay attention to “Volume 3A: System Programming Guide Part 1” and “Volume 3B:\nSystem Programming Guide Part 2”\n[L78] “The Manchester Mark I and atlas: a historical perspective”\nS. H. Lavington\nCommunications of the ACM archive\nVolume 21, Issue 1 (January 1978), pp. 4-12\nSpecial issue on computer architecture\nThis paper is a great retrospective of some of the history of the development of some important computer\nsystems. As we sometimes forget in the US, many of these new ideas came from overseas.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nPAGING: INTRODUCTION\n181\nHomework\nIn this homework, you will use a simple program, which is known as\npaging-linear-translate.py, to see if you understand how simple\nvirtual-to-physical address translation works with linear page tables. See\nthe README for details.\nQuestions\n• Before doing any translations, let’s use the simulator to study how\nlinear page tables change size given different parameters. Compute\nthe size of linear page tables as different parameters change. Some\nsuggested inputs are below; by using the -v flag, you can see\nhow many page-table entries are ﬁlled.\nFirst, to understand how linear page table size changes as the ad-\ndress space grows:\npaging-linear-translate.py -P 1k -a 1m -p 512m -v -n 0\npaging-linear-translate.py -P 1k -a 2m -p 512m -v -n 0\npaging-linear-translate.py -P 1k -a 4m -p 512m -v -n 0\nThen, to understand how linear page table size changes as page size\ngrows:\npaging-linear-translate.py -P 1k -a 1m -p 512m -v -n 0\npaging-linear-translate.py -P 2k -a 1m -p 512m -v -n 0\npaging-linear-translate.py -P 4k -a 1m -p 512m -v -n 0\nBefore running any of these, try to think about the expected trends.\nHow should page-table size change as the address space grows? As\nthe page size grows? Why shouldn’t we just use really big pages in\ngeneral?\n• Now let’s do some translations. Start with some small examples,\nand change the number of pages that are allocated to the address\nspace with the -u flag. For example:\npaging-linear-translate.py -P 1k -a 16k -p 32k -v -u 0\npaging-linear-translate.py -P 1k -a 16k -p 32k -v -u 25\npaging-linear-translate.py -P 1k -a 16k -p 32k -v -u 50\npaging-linear-translate.py -P 1k -a 16k -p 32k -v -u 75\npaging-linear-translate.py -P 1k -a 16k -p 32k -v -u 100\nWhat happens as you increase the percentage of pages that are al-\nlocated in each address space?\n• Now let’s try some different random seeds, and some different (and\nsometimes quite crazy) address-space parameters, for variety:\npaging-linear-translate.py -P 8\n-a 32\n-p 1024 -v -s 1\npaging-linear-translate.py -P 8k -a 32k\n-p 1m\n-v -s 2\npaging-linear-translate.py -P 1m -a 256m -p 512m -v -s 3\nWhich of these parameter combinations are unrealistic? Why?\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n182\nPAGING: INTRODUCTION\n• Use the program to try out some other problems. Can you ﬁnd the\nlimits of where the program doesn’t work anymore? For example,\nwhat happens if the address-space size is bigger than physical mem-\nory?\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\n19\nPaging: Faster Translations (TLBs)\nUsing paging as the core mechanism to support virtual memory can lead\nto high performance overheads. By chopping the address space into small,\nﬁxed-sized units (i.e., pages), paging requires a large amount of mapping\ninformation. Because that mapping information is generally stored in\nphysical memory, paging logically requires an extra memory lookup for\neach virtual address generated by the program. Going to memory for\ntranslation information before every instruction fetch or explicit load or\nstore is prohibitively slow. And thus our problem:\nTHE CRUX:\nHOW TO SPEED UP ADDRESS TRANSLATION\nHow can we speed up address translation, and generally avoid the\nextra memory reference that paging seems to require? What hardware\nsupport is required? What OS involvement is needed?\nWhen we want to make things fast, the OS usually needs some help.\nAnd help often comes from the OS’s old friend: the hardware. To speed\naddress translation, we are going to add what is called (for historical rea-\nsons [CP78]) a translation-lookaside buffer, or TLB [C68, C95]. A TLB\nis part of the chip’s memory-management unit (MMU), and is simply a\nhardware cache of popular virtual-to-physical address translations; thus,\na better name would be an address-translation cache. Upon each virtual\nmemory reference, the hardware ﬁrst checks the TLB to see if the desired\ntranslation is held therein; if so, the translation is performed (quickly)\nwithout having to consult the page table (which has all translations). Be-\ncause of their tremendous performance impact, TLBs in a real sense make\nvirtual memory possible [C95].\n19.1\nTLB Basic Algorithm\nFigure 19.1 shows a rough sketch of how hardware might handle a\nvirtual address translation, assuming a simple linear page table (i.e., the\npage table is an array) and a hardware-managed TLB (i.e., the hardware\nhandles much of the responsibility of page table accesses; we’ll explain\nmore about this below).\n183\n\n\n184\nPAGING: FASTER TRANSLATIONS (TLBS)\n1\nVPN = (VirtualAddress & VPN_MASK) >> SHIFT\n2\n(Success, TlbEntry) = TLB_Lookup(VPN)\n3\nif (Success == True)\n// TLB Hit\n4\nif (CanAccess(TlbEntry.ProtectBits) == True)\n5\nOffset\n= VirtualAddress & OFFSET_MASK\n6\nPhysAddr = (TlbEntry.PFN << SHIFT) | Offset\n7\nAccessMemory(PhysAddr)\n8\nelse\n9\nRaiseException(PROTECTION_FAULT)\n10\nelse\n// TLB Miss\n11\nPTEAddr = PTBR + (VPN * sizeof(PTE))\n12\nPTE = AccessMemory(PTEAddr)\n13\nif (PTE.Valid == False)\n14\nRaiseException(SEGMENTATION_FAULT)\n15\nelse if (CanAccess(PTE.ProtectBits) == False)\n16\nRaiseException(PROTECTION_FAULT)\n17\nelse\n18\nTLB_Insert(VPN, PTE.PFN, PTE.ProtectBits)\n19\nRetryInstruction()\nFigure 19.1: TLB Control Flow Algorithm\nThe algorithm the hardware follows works like this: ﬁrst, extract the\nvirtual page number (VPN) from the virtual address (Line 1 in Figure 19.1),\nand check if the TLB holds the translation for this VPN (Line 2). If it does,\nwe have a TLB hit, which means the TLB holds the translation. Success!\nWe can now extract the page frame number (PFN) from the relevant TLB\nentry, concatenate that onto the offset from the original virtual address,\nand form the desired physical address (PA), and access memory (Lines\n5–7), assuming protection checks do not fail (Line 4).\nIf the CPU does not ﬁnd the translation in the TLB (a TLB miss), we\nhave some more work to do. In this example, the hardware accesses the\npage table to ﬁnd the translation (Lines 11–12), and, assuming that the\nvirtual memory reference generated by the process is valid and accessi-\nble (Lines 13, 15), updates the TLB with the translation (Line 18). These\nset of actions are costly, primarily because of the extra memory reference\nneeded to access the page table (Line 12). Finally, once the TLB is up-\ndated, the hardware retries the instruction; this time, the translation is\nfound in the TLB, and the memory reference is processed quickly.\nThe TLB, like all caches, is built on the premise that in the common\ncase, translations are found in the cache (i.e., are hits). If so, little over-\nhead is added, as the TLB is found near the processing core and is de-\nsigned to be quite fast. When a miss occurs, the high cost of paging is\nincurred; the page table must be accessed to ﬁnd the translation, and an\nextra memory reference (or more, with more complex page tables) results.\nIf this happens often, the program will likely run noticeably more slowly;\nmemory accesses, relative to most CPU instructions, are quite costly, and\nTLB misses lead to more memory accesses. Thus, it is our hope to avoid\nTLB misses as much as we can.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nPAGING: FASTER TRANSLATIONS (TLBS)\n185\nVPN = 15\nVPN = 14\nVPN = 13\nVPN = 12\nVPN = 11\nVPN = 10\nVPN = 09\nVPN = 08\nVPN = 07\nVPN = 06\nVPN = 05\nVPN = 04\nVPN = 03\nVPN = 02\nVPN = 01\nVPN = 00\n00\n04\n08\n12\n16\nOffset\na[0]\na[1]\na[2]\na[3]\na[4]\na[5]\na[6]\na[7]\na[8]\na[9]\nFigure 19.2: Example: An Array In A Tiny Address Space\n19.2\nExample: Accessing An Array\nTo make clear the operation of a TLB, let’s examine a simple virtual\naddress trace and see how a TLB can improve its performance. In this\nexample, let’s assume we have an array of 10 4-byte integers in memory,\nstarting at virtual address 100. Assume further that we have a small 8-bit\nvirtual address space, with 16-byte pages; thus, a virtual address breaks\ndown into a 4-bit VPN (there are 16 virtual pages) and a 4-bit offset (there\nare 16 bytes on each of those pages).\nFigure 19.2 shows the array laid out on the 16 16-byte pages of the sys-\ntem. As you can see, the array’s ﬁrst entry (a[0]) begins on (VPN=06, off-\nset=04); only three 4-byte integers ﬁt onto that page. The array continues\nonto the next page (VPN=07), where the next four entries (a[3] ... a[6])\nare found. Finally, the last three entries of the 10-entry array (a[7] ... a[9])\nare located on the next page of the address space (VPN=08).\nNow let’s consider a simple loop that accesses each array element,\nsomething that would look like this in C:\nint sum = 0;\nfor (i = 0; i < 10; i++) {\nsum += a[i];\n}\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "page_number": 212,
      "chapter_number": 23,
      "summary": "This chapter covers segment 23 (pages 212-221). Key topics include paging, pages, and instruction. The code snippet (in C, in a ﬁle called array.c) that are\ninterested in is as follows:\nint array[1000];.",
      "keywords": [
        "PAGE TABLE",
        "VPN",
        "memory",
        "TLB",
        "virtual address",
        "address",
        "virtual",
        "array",
        "PAGING",
        "memory reference",
        "memory accesses",
        "address space",
        "virtual memory",
        "pages",
        "page table memory"
      ],
      "concepts": [
        "paging",
        "pages",
        "instruction",
        "instructions",
        "array",
        "addresses",
        "translations",
        "translation",
        "translate",
        "lines"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 42,
          "title": "Segment 42 (pages 838-857)",
          "relevance_score": 0.76,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 41,
          "title": "Segment 41 (pages 815-837)",
          "relevance_score": 0.73,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 12,
          "title": "Segment 12 (pages 219-238)",
          "relevance_score": 0.72,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 55,
          "title": "Segment 55 (pages 1103-1105)",
          "relevance_score": 0.72,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 54,
          "title": "Segment 54 (pages 1084-1102)",
          "relevance_score": 0.71,
          "method": "api"
        }
      ]
    },
    {
      "number": 24,
      "title": "Segment 24 (pages 222-230)",
      "start_page": 222,
      "end_page": 230,
      "detection_method": "topic_boundary",
      "content": "186\nPAGING: FASTER TRANSLATIONS (TLBS)\nFor the sake of simplicity, we will pretend that the only memory ac-\ncesses the loop generates are to the array (ignoring the variables i and\nsum, as well as the instructions themselves). When the ﬁrst array element\n(a[0]) is accessed, the CPU will see a load to virtual address 100. The\nhardware extracts the VPN from this (VPN=06), and uses that to check\nthe TLB for a valid translation. Assuming this is the ﬁrst time the pro-\ngram accesses the array, the result will be a TLB miss.\nThe next access is to a[1], and there is some good news here: a TLB\nhit! Because the second element of the array is packed next to the ﬁrst, it\nlives on the same page; because we’ve already accessed this page when\naccessing the ﬁrst element of the array, the translation is already loaded\ninto the TLB. And hence the reason for our success. Access to a[2] en-\ncounters similar success (another hit), because it too lives on the same\npage as a[0] and a[1].\nUnfortunately, when the program accesses a[3], we encounter an-\nother TLB miss. However, once again, the next entries (a[4] ... a[6])\nwill hit in the TLB, as they all reside on the same page in memory.\nFinally, access to a[7] causes one last TLB miss. The hardware once\nagain consults the page table to ﬁgure out the location of this virtual page\nin physical memory, and updates the TLB accordingly. The ﬁnal two ac-\ncesses (a[8] and a[9]) receive the beneﬁts of this TLB update; when the\nhardware looks in the TLB for their translations, two more hits result.\nLet us summarize TLB activity during our ten accesses to the array:\nmiss, hit, hit, miss, hit, hit, hit, miss, hit, hit. Thus, our TLB hit rate,\nwhich is the number of hits divided by the total number of accesses, is\n70%. Although this is not too high (indeed, we desire hit rates that ap-\nproach 100%), it is non-zero, which may be a surprise. Even though this\nis the ﬁrst time the program accesses the array, TLB performance gains\nbeneﬁt from spatial locality. The elements of the array are packed tightly\ninto pages (i.e., they are close to one another in space), and thus only the\nﬁrst access to an element on a page yields a TLB miss.\nAlso note the role that page size plays in this example. If the page size\nhad simply been twice as big (32 bytes, not 16), the array access would\nsuffer even fewer misses. As typical page sizes are more like 4KB, these\ntypes of dense, array-based accesses achieve excellent TLB performance,\nencountering only a single miss per page of accesses.\nOne last point about TLB performance: if the program, soon after this\nloop completes, accesses the array again, we’d likely see an even bet-\nter result, assuming that we have a big enough TLB to cache the needed\ntranslations: hit, hit, hit, hit, hit, hit, hit, hit, hit, hit. In this case, the\nTLB hit rate would be high because of temporal locality, i.e., the quick\nre-referencing of memory items in time. Like any cache, TLBs rely upon\nboth spatial and temporal locality for success, which are program proper-\nties. If the program of interest exhibits such locality (and many programs\ndo), the TLB hit rate will likely be high.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nPAGING: FASTER TRANSLATIONS (TLBS)\n187\nTIP: USE CACHING WHEN POSSIBLE\nCaching is one of the most fundamental performance techniques in com-\nputer systems, one that is used again and again to make the “common-\ncase fast” [HP06]. The idea behind hardware caches is to take advantage\nof locality in instruction and data references. There are usually two types\nof locality: temporal locality and spatial locality. With temporal locality,\nthe idea is that an instruction or data item that has been recently accessed\nwill likely be re-accessed soon in the future. Think of loop variables or in-\nstructions in a loop; they are accessed repeatedly over time. With spatial\nlocality, the idea is that if a program accesses memory at address x, it will\nlikely soon access memory near x. Imagine here streaming through an\narray of some kind, accessing one element and then the next. Of course,\nthese properties depend on the exact nature of the program, and thus are\nnot hard-and-fast laws but more like rules of thumb.\nHardware caches, whether for instructions, data, or address translations\n(as in our TLB) take advantage of locality by keeping copies of memory in\nsmall, fast on-chip memory. Instead of having to go to a (slow) memory\nto satisfy a request, the processor can ﬁrst check if a nearby copy exists\nin a cache; if it does, the processor can access it quickly (i.e., in a few cy-\ncles) and avoid spending the costly time it takes to access memory (many\nnanoseconds).\nYou might be wondering: if caches (like the TLB) are so great, why don’t\nwe just make bigger caches and keep all of our data in them? Unfor-\ntunately, this is where we run into more fundamental laws like those of\nphysics. If you want a fast cache, it has to be small, as issues like the\nspeed-of-light and other physical constraints become relevant. Any large\ncache by deﬁnition is slow, and thus defeats the purpose. Thus, we are\nstuck with small, fast caches; the question that remains is how to best use\nthem to improve performance.\n19.3\nWho Handles The TLB Miss?\nOne question that we must answer: who handles a TLB miss? Two an-\nswers are possible: the hardware, or the software (OS). In the olden days,\nthe hardware had complex instruction sets (sometimes called CISC, for\ncomplex-instruction set computers) and the people who built the hard-\nware didn’t much trust those sneaky OS people. Thus, the hardware\nwould handle the TLB miss entirely. To do this, the hardware has to\nknow exactly where the page tables are located in memory (via a page-\ntable base register, used in Line 11 in Figure 19.1), as well as their exact\nformat; on a miss, the hardware would “walk” the page table, ﬁnd the cor-\nrect page-table entry and extract the desired translation, update the TLB\nwith the translation, and retry the instruction. An example of an “older”\narchitecture that has hardware-managed TLBs is the Intel x86 architec-\nture, which uses a ﬁxed multi-level page table (see the next chapter for\ndetails); the current page table is pointed to by the CR3 register [I09].\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n188\nPAGING: FASTER TRANSLATIONS (TLBS)\n1\nVPN = (VirtualAddress & VPN_MASK) >> SHIFT\n2\n(Success, TlbEntry) = TLB_Lookup(VPN)\n3\nif (Success == True)\n// TLB Hit\n4\nif (CanAccess(TlbEntry.ProtectBits) == True)\n5\nOffset\n= VirtualAddress & OFFSET_MASK\n6\nPhysAddr = (TlbEntry.PFN << SHIFT) | Offset\n7\nRegister = AccessMemory(PhysAddr)\n8\nelse\n9\nRaiseException(PROTECTION_FAULT)\n10\nelse\n// TLB Miss\n11\nRaiseException(TLB_MISS)\nFigure 19.3: TLB Control Flow Algorithm (OS Handled)\nMore modern architectures (e.g., MIPS R10k [H93] or Sun’s SPARC v9\n[WG00], both RISC or reduced-instruction set computers) have what is\nknown as a software-managed TLB. On a TLB miss, the hardware sim-\nply raises an exception (line 11 in Figure 19.3), which pauses the current\ninstruction stream, raises the privilege level to kernel mode, and jumps\nto a trap handler. As you might guess, this trap handler is code within\nthe OS that is written with the express purpose of handling TLB misses.\nWhen run, the code will lookup the translation in the page table, use spe-\ncial “privileged” instructions to update the TLB, and return from the trap;\nat this point, the hardware retries the instruction (resulting in a TLB hit).\nLet’s discuss a couple of important details. First, the return-from-trap\ninstruction needs to be a little different than the return-from-trap we saw\nbefore when servicing a system call. In the latter case, the return-from-\ntrap should resume execution at the instruction after the trap into the OS,\njust as a return from a procedure call returns to the instruction imme-\ndiately following the call into the procedure. In the former case, when\nreturning from a TLB miss-handling trap, the hardware must resume ex-\necution at the instruction that caused the trap; this retry thus lets the in-\nstruction run again, this time resulting in a TLB hit. Thus, depending on\nhow a trap or exception was caused, the hardware must save a different\nPC when trapping into the OS, in order to resume properly when the time\nto do so arrives.\nSecond, when running the TLB miss-handling code, the OS needs to be\nextra careful not to cause an inﬁnite chain of TLB misses to occur. Many\nsolutions exist; for example, you could keep TLB miss handlers in physi-\ncal memory (where they are unmapped and not subject to address trans-\nlation), or reserve some entries in the TLB for permanently-valid transla-\ntions and use some of those permanent translation slots for the handler\ncode itself; these wired translations always hit in the TLB.\nThe primary advantage of the software-managed approach is ﬂexibil-\nity: the OS can use any data structure it wants to implement the page\ntable, without necessitating hardware change. Another advantage is sim-\nplicity; as you can see in the TLB control ﬂow (line 11 in Figure 19.3, in\ncontrast to lines 11–19 in Figure 19.1), the hardware doesn’t have to do\nmuch on a miss; it raises an exception, and the OS TLB miss handler does\nthe rest.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nPAGING: FASTER TRANSLATIONS (TLBS)\n189\nASIDE: RISC VS. CISC\nIn the 1980’s, a great battle took place in the computer architecture com-\nmunity.\nOn one side was the CISC camp, which stood for Complex\nInstruction Set Computing; on the other side was RISC, for Reduced\nInstruction Set Computing [PS81]. The RISC side was spear-headed by\nDavid Patterson at Berkeley and John Hennessy at Stanford (who are also\nco-authors of some famous books [HP06]), although later John Cocke was\nrecognized with a Turing award for his earliest work on RISC [CM00].\nCISC instruction sets tend to have a lot of instructions in them, and each\ninstruction is relatively powerful. For example, you might see a string\ncopy, which takes two pointers and a length and copies bytes from source\nto destination. The idea behind CISC was that instructions should be\nhigh-level primitives, to make the assembly language itself easier to use,\nand to make code more compact.\nRISC instruction sets are exactly the opposite. A key observation behind\nRISC is that instruction sets are really compiler targets, and all compil-\ners really want are a few simple primitives that they can use to gener-\nate high-performance code. Thus, RISC proponents argued, let’s rip out\nas much from the hardware as possible (especially the microcode), and\nmake what’s left simple, uniform, and fast.\nIn the early days, RISC chips made a huge impact, as they were noticeably\nfaster [BC91]; many papers were written; a few companies were formed\n(e.g., MIPS and Sun). However, as time progressed, CISC manufacturers\nsuch as Intel incorporated many RISC techniques into the core of their\nprocessors, for example by adding early pipeline stages that transformed\ncomplex instructions into micro-instructions which could then be pro-\ncessed in a RISC-like manner. These innovations, plus a growing number\nof transistors on each chip, allowed CISC to remain competitive. The end\nresult is that the debate died down, and today both types of processors\ncan be made to run fast.\n19.4\nTLB Contents: What’s In There?\nLet’s look at the contents of the hardware TLB in more detail. A typical\nTLB might have 32, 64, or 128 entries and be what is called fully associa-\ntive. Basically, this just means that any given translation can be anywhere\nin the TLB, and that the hardware will search the entire TLB in parallel to\nﬁnd the desired translation. A typical TLB entry might look like this:\nVPN\nPFN\nother bits\nNote that both the VPN and PFN are present in each entry, as a trans-\nlation could end up in any of these locations (in hardware terms, the TLB\nis known as a fully-associative cache). The hardware searches the entries\nin parallel to see if there is a match.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n190\nPAGING: FASTER TRANSLATIONS (TLBS)\nASIDE: TLB VALID BIT ̸= PAGE TABLE VALID BIT\nA common mistake is to confuse the valid bits found in a TLB with\nthose found in a page table. In a page table, when a page-table entry\n(PTE) is marked invalid, it means that the page has not been allocated by\nthe process, and should not be accessed by a correctly-working program.\nThe usual response when an invalid page is accessed is to trap to the OS,\nwhich will respond by killing the process.\nA TLB valid bit, in contrast, simply refers to whether a TLB entry has a\nvalid translation within it. When a system boots, for example, a common\ninitial state for each TLB entry is to be set to invalid, because no address\ntranslations are yet cached there. Once virtual memory is enabled, and\nonce programs start running and accessing their virtual address spaces,\nthe TLB is slowly populated, and thus valid entries soon ﬁll the TLB.\nThe TLB valid bit is quite useful when performing a context switch too,\nas we’ll discuss further below. By setting all TLB entries to invalid, the\nsystem can ensure that the about-to-be-run process does not accidentally\nuse a virtual-to-physical translation from a previous process.\nMore interesting are the “other bits”. For example, the TLB commonly\nhas a valid bit, which says whether the entry has a valid translation or\nnot. Also common are protection bits, which determine how a page can\nbe accessed (as in the page table). For example, code pages might be\nmarked read and execute, whereas heap pages might be marked read and\nwrite. There may also be a few other ﬁelds, including an address-space\nidentiﬁer, a dirty bit, and so forth; see below for more information.\n19.5\nTLB Issue: Context Switches\nWith TLBs, some new issues arise when switching between processes\n(and hence address spaces). Speciﬁcally, the TLB contains virtual-to-physical\ntranslations that are only valid for the currently running process; these\ntranslations are not meaningful for other processes. As a result, when\nswitching from one process to another, the hardware or OS (or both) must\nbe careful to ensure that the about-to-be-run process does not accidentally\nuse translations from some previously run process.\nTo understand this situation better, let’s look at an example. When one\nprocess (P1) is running, it assumes the TLB might be caching translations\nthat are valid for it, i.e., that come from P1’s page table. Assume, for this\nexample, that the 10th virtual page of P1 is mapped to physical frame 100.\nIn this example, assume another process (P2) exists, and the OS soon\nmight decide to perform a context switch and run it. Assume here that\nthe 10th virtual page of P2 is mapped to physical frame 170. If entries for\nboth processes were in the TLB, the contents of the TLB would be:\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nPAGING: FASTER TRANSLATIONS (TLBS)\n191\nVPN\nPFN\nvalid\nprot\n10\n100\n1\nrwx\n—\n—\n0\n—\n10\n170\n1\nrwx\n—\n—\n0\n—\nIn the TLB above, we clearly have a problem: VPN 10 translates to\neither PFN 100 (P1) or PFN 170 (P2), but the hardware can’t distinguish\nwhich entry is meant for which process. Thus, we need to do some more\nwork in order for the TLB to correctly and efﬁciently support virtualiza-\ntion across multiple processes. And thus, a crux:\nTHE CRUX:\nHOW TO MANAGE TLB CONTENTS ON A CONTEXT SWITCH\nWhen context-switching between processes, the translations in the TLB\nfor the last process are not meaningful to the about-to-be-run process.\nWhat should the hardware or OS do in order to solve this problem?\nThere are a number of possible solutions to this problem. One ap-\nproach is to simply ﬂush the TLB on context switches, thus emptying\nit before running the next process.\nOn a software-based system, this\ncan be accomplished with an explicit (and privileged) hardware instruc-\ntion; with a hardware-managed TLB, the ﬂush could be enacted when the\npage-table base register is changed (note the OS must change the PTBR\non a context switch anyhow). In either case, the ﬂush operation simply\nsets all valid bits to 0, essentially clearing the contents of the TLB.\nBy ﬂushing the TLB on each context switch, we now have a working\nsolution, as a process will never accidentally encounter the wrong trans-\nlations in the TLB. However, there is a cost: each time a process runs, it\nmust incur TLB misses as it touches its data and code pages. If the OS\nswitches between processes frequently, this cost may be high.\nTo reduce this overhead, some systems add hardware support to en-\nable sharing of the TLB across context switches. In particular, some hard-\nware systems provide an address space identiﬁer (ASID) ﬁeld in the\nTLB. You can think of the ASID as a process identiﬁer (PID), but usu-\nally it has fewer bits (e.g., 8 bits for the ASID versus 32 bits for a PID).\nIf we take our example TLB from above and add ASIDs, it is clear\nprocesses can readily share the TLB: only the ASID ﬁeld is needed to dif-\nferentiate otherwise identical translations. Here is a depiction of a TLB\nwith the added ASID ﬁeld:\nVPN\nPFN\nvalid\nprot\nASID\n10\n100\n1\nrwx\n1\n—\n—\n0\n—\n—\n10\n170\n1\nrwx\n2\n—\n—\n0\n—\n—\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n192\nPAGING: FASTER TRANSLATIONS (TLBS)\nThus, with address-space identiﬁers, the TLB can hold translations\nfrom different processes at the same time without any confusion.\nOf\ncourse, the hardware also needs to know which process is currently run-\nning in order to perform translations, and thus the OS must, on a context\nswitch, set some privileged register to the ASID of the current process.\nAs an aside, you may also have thought of another case where two\nentries of the TLB are remarkably similar. In this example, there are two\nentries for two different processes with two different VPNs that point to\nthe same physical page:\nVPN\nPFN\nvalid\nprot\nASID\n10\n101\n1\nr-x\n1\n—\n—\n0\n—\n—\n50\n101\n1\nr-x\n2\n—\n—\n0\n—\n—\nThis situation might arise, for example, when two processes share a\npage (a code page, for example). In the example above, Process 1 is shar-\ning physical page 101 with Process 2; P1 maps this page into the 10th\npage of its address space, whereas P2 maps it to the 50th page of its ad-\ndress space. Sharing of code pages (in binaries, or shared libraries) is\nuseful as it reduces the number of physical pages in use, thus reducing\nmemory overheads.\n19.6\nIssue: Replacement Policy\nAs with any cache, and thus also with the TLB, one more issue that we\nmust consider is cache replacement. Speciﬁcally, when we are installing\na new entry in the TLB, we have to replace an old one, and thus the\nquestion: which one to replace?\nTHE CRUX: HOW TO DESIGN TLB REPLACEMENT POLICY\nWhich TLB entry should be replaced when we add a new TLB entry?\nThe goal, of course, being to minimize the miss rate (or increase hit rate)\nand thus improve performance.\nWe will study such policies in some detail when we tackle the problem\nof swapping pages to disk in a virtual memory system; here we’ll just\nhighlight a few of typical policies. One common approach is to evict the\nleast-recently-used or LRU entry. The idea here is to take advantage of\nlocality in the memory-reference stream; thus, it is likely that an entry that\nhas not recently been used is a good candidate for eviction as (perhaps)\nit won’t soon be referenced again. Another typical approach is to use a\nrandom policy. Randomness sometimes makes a bad decision but has the\nnice property that there are not any weird corner case behaviors that can\ncause pessimal behavior, e.g., think of a loop accessing n+1 pages, a TLB\nof size n, and an LRU replacement policy.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nPAGING: FASTER TRANSLATIONS (TLBS)\n193\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31\nVPN\nG\nASID\nPFN\nC\nD V\nFigure 19.4: A MIPS TLB Entry\n19.7\nA Real TLB Entry\nFinally, let’s brieﬂy look at a real TLB. This example is from the MIPS\nR4000 [H93], a modern system that uses software-managed TLBs. All 64\nbits of this TLB entry can be seen in Figure 19.4.\nThe MIPS R4000 supports a 32-bit address space with 4KB pages. Thus,\nwe would expect a 20-bit VPN and 12-bit offset in our typical virtual ad-\ndress. However, as you can see in the TLB, there are only 19 bits for the\nVPN; as it turns out, user addresses will only come from half the address\nspace (the rest reserved for the kernel) and hence only 19 bits of VPN\nare needed. The VPN translates to up to a 24-bit physical frame number\n(PFN), and hence can support systems with up to 64GB of (physical) main\nmemory (224 4KB pages).\nThere are a few other interesting bits in the MIPS TLB. We see a global\nbit (G), which is used for pages that are globally-shared among processes.\nThus, if the global bit is set, the ASID is ignored. We also see the 8-bit\nASID, which the OS can use to distinguish between address spaces (as\ndescribed above). One question for you: what should the OS do if there\nare more than 256 (28) processes running at a time? Finally, we see 3\nCoherence (C) bits, which determine how a page is cached by the hardware\n(a bit beyond the scope of these notes); a dirty bit which is marked when\nthe page has been written to (we’ll see the use of this later); a valid bit\nwhich tells the hardware if there is a valid translation present in the entry.\nThere is also a page mask ﬁeld (not shown), which supports multiple page\nsizes; we’ll see later why having larger pages might be useful. Finally,\nsome of the 64 bits are unused (shaded gray in the diagram).\nMIPS TLBs usually have 32 or 64 of these entries, most of which are\nused by user processes as they run. However, a few are reserved for the\nOS. A wired register can be set by the OS to tell the hardware how many\nslots of the TLB to reserve for the OS; the OS uses these reserved map-\npings for code and data that it wants to access during critical times, where\na TLB miss would be problematic (e.g., in the TLB miss handler).\nBecause the MIPS TLB is software managed, there needs to be instruc-\ntions to update the TLB. The MIPS provides four such instructions: TLBP,\nwhich probes the TLB to see if a particular translation is in there; TLBR,\nwhich reads the contents of a TLB entry into registers; TLBWI, which re-\nplaces a speciﬁc TLB entry; and TLBWR, which replaces a random TLB\nentry. The OS uses these instructions to manage the TLB’s contents. It is\nof course critical that these instructions are privileged; imagine what a\nuser process could do if it could modify the contents of the TLB (hint: just\nabout anything, including take over the machine, run its own malicious\n“OS”, or even make the Sun disappear).\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n194\nPAGING: FASTER TRANSLATIONS (TLBS)\nTIP: RAM ISN’T ALWAYS RAM (CULLER’S LAW)\nThe term random-access memory, or RAM, implies that you can access\nany part of RAM just as quickly as another. While it is generally good to\nthink of RAM in this way, because of hardware/OS features such as the\nTLB, accessing a particular page of memory may be costly, particularly if\nthat page isn’t currently mapped by your TLB. Thus, it is always good to\nremember the implementation tip: RAM isn’t always RAM. Sometimes\nrandomly accessing your address space, particular if the number of pages\naccessed exceeds the TLB coverage, can lead to severe performance penal-\nties. Because one of our advisors, David Culler, used to always point to\nthe TLB as the source of many performance problems, we name this law\nin his honor: Culler’s Law.\n19.8\nSummary\nWe have seen how hardware can help us make address translation\nfaster. By providing a small, dedicated on-chip TLB as an address-translation\ncache, most memory references will hopefully be handled without having\nto access the page table in main memory. Thus, in the common case,\nthe performance of the program will be almost as if memory isn’t being\nvirtualized at all, an excellent achievement for an operating system, and\ncertainly essential to the use of paging in modern systems.\nHowever, TLBs do not make the world rosy for every program that\nexists. In particular, if the number of pages a program accesses in a short\nperiod of time exceeds the number of pages that ﬁt into the TLB, the pro-\ngram will generate a large number of TLB misses, and thus run quite a\nbit more slowly. We refer to this phenomenon as exceeding the TLB cov-\nerage, and it can be quite a problem for certain programs. One solution,\nas we’ll discuss in the next chapter, is to include support for larger page\nsizes; by mapping key data structures into regions of the program’s ad-\ndress space that are mapped by larger pages, the effective coverage of the\nTLB can be increased. Support for large pages is often exploited by pro-\ngrams such as a database management system (a DBMS), which have\ncertain data structures that are both large and randomly-accessed.\nOne other TLB issue worth mentioning: TLB access can easily be-\ncome a bottleneck in the CPU pipeline, in particular with what is called a\nphysically-indexed cache. With such a cache, address translation has to\ntake place before the cache is accessed, which can slow things down quite\na bit. Because of this potential problem, people have looked into all sorts\nof clever ways to access caches with virtual addresses, thus avoiding the\nexpensive step of translation in the case of a cache hit. Such a virtually-\nindexed cache solves some performance problems, but introduces new\nissues into hardware design as well. See Wiggins’s ﬁne survey for more\ndetails [W03].\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "page_number": 222,
      "chapter_number": 24,
      "summary": "186\nPAGING: FASTER TRANSLATIONS (TLBS)\nFor the sake of simplicity, we will pretend that the only memory ac-\ncesses the loop generates are to the array (ignoring the variables i and\nsum, as well as the instructions themselves) Key topics include paging, pages, and translations.",
      "keywords": [
        "TLB",
        "TLB miss",
        "TLB entry",
        "TLB hit",
        "TLB VALID BIT",
        "TLBS",
        "TLB hit rate",
        "MIPS TLB",
        "TLB VALID",
        "FASTER TRANSLATIONS",
        "MIPS TLB Entry",
        "hit",
        "TLB miss handler",
        "hardware",
        "page table"
      ],
      "concepts": [
        "paging",
        "pages",
        "translations",
        "translation",
        "translates",
        "accessed",
        "accesses",
        "access",
        "hardware",
        "memory"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 30,
          "title": "Segment 30 (pages 601-619)",
          "relevance_score": 0.54,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 42,
          "title": "Segment 42 (pages 838-857)",
          "relevance_score": 0.54,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 33,
          "title": "Segment 33 (pages 662-682)",
          "relevance_score": 0.52,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 41,
          "title": "Segment 41 (pages 815-837)",
          "relevance_score": 0.52,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 27,
          "title": "Segment 27 (pages 537-558)",
          "relevance_score": 0.51,
          "method": "api"
        }
      ]
    },
    {
      "number": 25,
      "title": "Segment 25 (pages 231-249)",
      "start_page": 231,
      "end_page": 249,
      "detection_method": "topic_boundary",
      "content": "PAGING: FASTER TRANSLATIONS (TLBS)\n195\nReferences\n[BC91] “Performance from Architecture: Comparing a RISC and a CISC\nwith Similar Hardware Organization”\nD. Bhandarkar and Douglas W. Clark\nCommunications of the ACM, September 1991\nA great and fair comparison between RISC and CISC. The bottom line: on similar hardware, RISC was\nabout a factor of three better in performance.\n[CM00] “The evolution of RISC technology at IBM”\nJohn Cocke and V. Markstein\nIBM Journal of Research and Development, 44:1/2\nA summary of the ideas and work behind the IBM 801, which many consider the ﬁrst true RISC micro-\nprocessor.\n[C95] “The Core of the Black Canyon Computer Corporation”\nJohn Couleur\nIEEE Annals of History of Computing, 17:4, 1995\nIn this fascinating historical note, Couleur talks about how he invented the TLB in 1964 while working\nfor GE, and the fortuitous collaboration that thus ensued with the Project MAC folks at MIT.\n[CG68] “Shared-access Data Processing System”\nJohn F. Couleur and Edward L. Glaser\nPatent 3412382, November 1968\nThe patent that contains the idea for an associative memory to store address translations. The idea,\naccording to Couleur, came in 1964.\n[CP78] “The architecture of the IBM System/370”\nR.P. Case and A. Padegs\nCommunications of the ACM. 21:1, 73-96, January 1978\nPerhaps the ﬁrst paper to use the term translation lookaside buffer. The name arises from the his-\ntorical name for a cache, which was a lookaside buffer as called by those developing the Atlas system\nat the University of Manchester; a cache of address translations thus became a translation lookaside\nbuffer. Even though the term lookaside buffer fell out of favor, TLB seems to have stuck, for whatever\nreason.\n[H93] “MIPS R4000 Microprocessor User’s Manual”.\nJoe Heinrich, Prentice-Hall, June 1993\nAvailable: http://cag.csail.mit.edu/raw/\ndocuments/R4400 Uman book Ed2.pdf\n[HP06] “Computer Architecture: A Quantitative Approach”\nJohn Hennessy and David Patterson\nMorgan-Kaufmann, 2006\nA great book about computer architecture. We have a particular attachment to the classic ﬁrst edition.\n[I09] “Intel 64 and IA-32 Architectures Software Developer’s Manuals”\nIntel, 2009\nAvailable: http://www.intel.com/products/processor/manuals\nIn particular, pay attention to “Volume 3A: System Programming Guide Part 1” and “Volume 3B:\nSystem Programming Guide Part 2”\n[PS81] “RISC-I: A Reduced Instruction Set VLSI Computer”\nD.A. Patterson and C.H. Sequin\nISCA ’81, Minneapolis, May 1981\nThe paper that introduced the term RISC, and started the avalanche of research into simplifying com-\nputer chips for performance.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n196\nPAGING: FASTER TRANSLATIONS (TLBS)\n[SB92] “CPU Performance Evaluation and Execution Time Prediction\nUsing Narrow Spectrum Benchmarking”\nRafael H. Saavedra-Barrera\nEECS Department, University of California, Berkeley\nTechnical Report No. UCB/CSD-92-684, February 1992\nwww.eecs.berkeley.edu/Pubs/TechRpts/1992/CSD-92-684.pdf\nA great dissertation about how to predict execution time of applications by breaking them down into\nconstituent pieces and knowing the cost of each piece. Probably the most interesting part that comes out\nof this work is the tool to measure details of the cache hierarchy (described in Chapter 5). Make sure to\ncheck out the wonderful diagrams therein.\n[W03] “A Survey on the Interaction Between Caching, Translation and Protection”\nAdam Wiggins\nUniversity of New South Wales TR UNSW-CSE-TR-0321, August, 2003\nAn excellent survey of how TLBs interact with other parts of the CPU pipeline, namely hardware caches.\n[WG00] “The SPARC Architecture Manual: Version 9”\nDavid L. Weaver and Tom Germond, September 2000\nSPARC International, San Jose, California\nAvailable: http://www.sparc.org/standards/SPARCV9.pdf\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nPAGING: FASTER TRANSLATIONS (TLBS)\n197\nHomework (Measurement)\nIn this homework, you are to measure the size and cost of accessing\na TLB. The idea is based on work by Saavedra-Barrera [SB92], who de-\nveloped a simple but beautiful method to measure numerous aspects of\ncache hierarchies, all with a very simple user-level program. Read his\nwork for more details.\nThe basic idea is to access some number of pages within large data\nstructure (e.g., an array) and to time those accesses. For example, let’s say\nthe TLB size of a machine happens to be 4 (which would be very small,\nbut useful for the purposes of this discussion). If you write a program\nthat touches 4 or fewer pages, each access should be a TLB hit, and thus\nrelatively fast. However, once you touch 5 pages or more, repeatedly in a\nloop, each access will suddenly jump in cost, to that of a TLB miss.\nThe basic code to loop through an array once should look like this:\nint jump = PAGESIZE / sizeof(int);\nfor (i = 0; i < NUMPAGES * jump; i += jump) {\na[i] += 1;\n}\nIn this loop, one integer per page of the the array a is updated, up\nto the number of pages speciﬁed by NUMPAGES. By timing such a loop\nrepeatedly (say, a few hundred million times in another loop around this\none, or however many loops are needed to run for a few seconds), you\ncan time how long each access takes (on average). By looking for jumps\nin cost as NUMPAGES increases, you can roughly determine how big the\nﬁrst-level TLB is, determine whether a second-level TLB exists (and how\nbig it is if it does), and in general get a good sense of how TLB hits and\nmisses can affect performance.\nHere is an example graph:\nAs you can see in the graph, when just a few pages are accessed (8\nor fewer), the average access time is roughly 5 nanoseconds. When 16\nor more pages are accessed, there is a sudden jump to about 20 nanosec-\nonds per access. A ﬁnal jump in cost occurs at around 1024 pages, at\nwhich point each access takes around 70 nanoseconds. From this data,\nwe can conclude that there is a two-level TLB hierarchy; the ﬁrst is quite\nsmall (probably holding between 8 and 16 entries); the second is larger\nbut slower (holding roughly 512 entries). The overall difference between\nhits in the ﬁrst-level TLB and misses is quite large, roughly a factor of\nfourteen. TLB performance matters!\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n198\nPAGING: FASTER TRANSLATIONS (TLBS)\n1\n4\n16\n64\n256 1024\n0\n20\n40\n60\n80\nTLB Size Measurement\nNumber Of Pages\nTime Per Access (ns)\nFigure 19.5: Discovering TLB Sizes and Miss Costs\nQuestions\n• For timing, you’ll need to use a timer such as that made available\nby gettimeofday(). How precise is such a timer? How long\ndoes an operation have to take in order for you to time it precisely?\n(this will help determine how many times, in a loop, you’ll have to\nrepeat a page access in order to time it successfully)\n• Write the program, called tlb.c, that can roughly measure the cost\nof accessing each page. Inputs to the program should be: the num-\nber of pages to touch and the number of trials.\n• Now write a script in your favorite scripting language (csh, python,\netc.) to run this program, while varying the number of pages ac-\ncessed from 1 up to a few thousand, perhaps incrementing by a\nfactor of two per iteration. Run the script on different machines\nand gather some data. How many trials are needed to get reliable\nmeasurements?\n• Next, graph the results, making a graph that looks similar to the\none above. Use a good tool like ploticus. Visualization usually\nmakes the data much easier to digest; why do you think that is?\n• One thing to watch out for is compiler optimzation. Compilers do\nall sorts of clever things, including removing loops which incre-\nment values that no other part of the program subsequently uses.\nHow can you ensure the compiler does not remove the main loop\nabove from your TLB size estimator?\n• Another thing to watch out for is the fact that most systems today\nship with multiple CPUs, and each CPU, of course, has its own TLB\nhierarchy. To really get good measurements, you have to run your\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nPAGING: FASTER TRANSLATIONS (TLBS)\n199\ncode on just one CPU, instead of letting the scheduler bounce it\nfrom one CPU to the next. How can you do that? (hint: look up\n“pinning a thread” on Google for some clues) What will happen if\nyou don’t do this, and the code moves from one CPU to the other?\n• Another issue that might arise relates to initialization. If you don’t\ninitialize the array a above before accessing it, the ﬁrst time you\naccess it will be very expensive, due to initial access costs such as\ndemand zeroing. Will this affect your code and its timing? What\ncan you do to counterbalance these potential costs?\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n20\nPaging: Smaller Tables\nWe now tackle the second problem that paging introduces: page tables\nare too big and thus consume too much memory. Let’s start out with\na linear page table. As you might recall1, linear page tables get pretty\nbig. Assume again a 32-bit address space (232 bytes), with 4KB (212 byte)\npages and a 4-byte page-table entry. An address space thus has roughly\none million virtual pages in it ( 232\n212 ); multiply by the page-table size and\nyou see that our page table is 4MB in size. Recall also: we usually have\none page table for every process in the system! With a hundred active pro-\ncesses (not uncommon on a modern system), we will be allocating hun-\ndreds of megabytes of memory just for page tables! As a result, we are in\nsearch of some techniques to reduce this heavy burden. There are a lot of\nthem, so let’s get going. But not before our crux:\nCRUX: HOW TO MAKE PAGE TABLES SMALLER?\nSimple array-based page tables (usually called linear page tables) are\ntoo big, taking up far too much memory on typical systems. How can we\nmake page tables smaller? What are the key ideas? What inefﬁciencies\narise as a result of these new data structures?\n20.1\nSimple Solution: Bigger Pages\nWe could reduce the size of the page table in one simple way: use\nbigger pages. Take our 32-bit address space again, but this time assume\n16KB pages. We would thus have an 18-bit VPN plus a 14-bit offset. As-\nsuming the same size for each PTE (4 bytes), we now have 218 entries in\nour linear page table and thus a total size of 1MB per page table, a factor\n1Or indeed, you might not; this paging thing is getting out of control, no? That said,\nalways make sure you understand the problem you are solving before moving onto the solution;\nindeed, if you understand the problem, you can often derive the solution yourself. Here, the\nproblem should be clear: simple linear (array-based) page tables are too big.\n201\n\n\n202\nPAGING: SMALLER TABLES\nASIDE: MULTIPLE PAGE SIZES\nAs an aside, do note that many architectures (e.g., MIPS, SPARC, x86-64)\nnow support multiple page sizes. Usually, a small (4KB or 8KB) page\nsize is used. However, if a “smart” application requests it, a single large\npage (e.g., of size 4MB) can be used for a speciﬁc portion of the address\nspace, enabling such applications to place a frequently-used (and large)\ndata structure in such a space while consuming only a single TLB en-\ntry. This type of large page usage is common in database management\nsystems and other high-end commercial applications. The main reason\nfor multiple page sizes is not to save page table space, however; it is to\nreduce pressure on the TLB, enabling a program to access more of its ad-\ndress space without suffering from too many TLB misses. However, as\nresearchers have shown [N+02], using multiple page sizes makes the OS\nvirtual memory manager notably more complex, and thus large pages\nare sometimes most easily used simply by exporting a new interface to\napplications to request large pages directly.\nof four reduction in size of the page table (not surprisingly, the reduction\nexactly mirrors the factor of four increase in page size).\nThe major problem with this approach, however, is that big pages lead\nto waste within each page, a problem known as internal fragmentation\n(as the waste is internal to the unit of allocation). Applications thus end\nup allocating pages but only using little bits and pieces of each, and mem-\nory quickly ﬁlls up with these overly-large pages. Thus, most systems use\nrelatively small page sizes in the common case: 4KB (as in x86) or 8KB (as\nin SPARCv9). Our problem will not be solved so simply, alas.\n20.2\nHybrid Approach: Paging and Segments\nWhenever you have two reasonable but different approaches to some-\nthing in life, you should always examine the combination of the two to\nsee if you can obtain the best of both worlds. We call such a combination a\nhybrid. For example, why eat just chocolate or plain peanut butter when\nyou can instead combine the two in a lovely hybrid known as the Reese’s\nPeanut Butter Cup [M28]?\nYears ago, the creators of Multics (in particular Jack Dennis) chanced\nupon such an idea in the construction of the Multics virtual memory sys-\ntem [M07]. Speciﬁcally, Dennis had the idea of combining paging and\nsegmentation in order to reduce the memory overhead of page tables.\nWe can see why this might work by examining a typical linear page ta-\nble in more detail. Assume we have an address space in which the used\nportions of the heap and stack are small. For the example, we use a tiny\n16KB address space with 1KB pages (Figure 20.1); the page table for this\naddress space is in Table 20.1.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nPAGING: SMALLER TABLES\n203\ncode\nheap\nstack\nVirtual Address Space\nPhysical Memory\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\nFigure 20.1: A 16-KB Address Space With 1-KB Pages\nThis example assumes the single code page (VPN 0) is mapped to\nphysical page 10, the single heap page (VPN 4) to physical page 23, and\nthe two stack pages at the other end of the address space (VPNs 14 and\n15) are mapped to physical pages 28 and 4, respectively. As you can see\nfrom the picture, most of the page table is unused, full of invalid entries.\nWhat a waste! And this is for a tiny 16KB address space. Imagine the\npage table of a 32-bit address space and all the potential wasted space in\nthere! Actually, don’t imagine such a thing; it’s far too gruesome.\nPFN\nvalid\nprot\npresent\ndirty\n10\n1\nr-x\n1\n0\n-\n0\n—\n-\n-\n-\n0\n—\n-\n-\n-\n0\n—\n-\n-\n23\n1\nrw-\n1\n1\n-\n0\n—\n-\n-\n-\n0\n—\n-\n-\n-\n0\n—\n-\n-\n-\n0\n—\n-\n-\n-\n0\n—\n-\n-\n-\n0\n—\n-\n-\n-\n0\n—\n-\n-\n-\n0\n—\n-\n-\n-\n0\n—\n-\n-\n28\n1\nrw-\n1\n1\n4\n1\nrw-\n1\n1\nTable 20.1: A Page Table For 16-KB Address Space\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n204\nPAGING: SMALLER TABLES\nThus, our hybrid approach: instead of having a single page table for\nthe entire address space of the process, why not have one per logical seg-\nment? In this example, we might thus have three page tables, one for the\ncode, heap, and stack parts of the address space.\nNow, remember with segmentation, we had a base register that told\nus where each segment lived in physical memory, and a bound or limit\nregister that told us the size of said segment. In our hybrid, we still have\nthose structures in the MMU; here, we use the base not to point to the\nsegment itself but rather to hold the physical address of the page table of that\nsegment. The bounds register is used to indicate the end of the page table\n(i.e., how many valid pages it has).\nLet’s do a simple example to clarify. Assume a 32-bit virtual address\nspace with 4KB pages, and an address space split into four segments.\nWe’ll only use three segments for this example: one for code, one for\nheap, and one for stack.\nTo determine which segment an address refers to, we’ll use the top\ntwo bits of the address space. Let’s assume 00 is the unused segment,\nwith 01 for code, 10 for the heap, and 11 for the stack. Thus, a virtual\naddress looks like this:\n31 30 29 28 27 26 25 24 23 22 21 20 19 18 17 16 15 14 13 12 11 10\n9\n8\n7\n6\n5\n4\n3\n2\n1\n0\nSeg\nVPN\nOffset\nIn the hardware, assume that there are thus three base/bounds pairs,\none each for code, heap, and stack. When a process is running, the base\nregister for each of these segments contains the physical address of a lin-\near page table for that segment; thus, each process in the system now has\nthree page tables associated with it. On a context switch, these registers\nmust be changed to reﬂect the location of the page tables of the newly-\nrunning process.\nOn a TLB miss (assuming a hardware-managed TLB, i.e., where the\nhardware is responsible for handling TLB misses), the hardware uses the\nsegment bits (SN) to determine which base and bounds pair to use. The\nhardware then takes the physical address therein and combines it with\nthe VPN as follows to form the address of the page table entry (PTE):\nSN\n= (VirtualAddress & SEG_MASK) >> SN_SHIFT\nVPN\n= (VirtualAddress & VPN_MASK) >> VPN_SHIFT\nAddressOfPTE = Base[SN] + (VPN * sizeof(PTE))\nThis sequence should look familiar; it is virtually identical to what we\nsaw before with linear page tables. The only difference, of course, is the\nuse of one of three segment base registers instead of the single page table\nbase register.\nThe critical difference in our hybrid scheme is the presence of a bounds\nregister per segment; each bounds register holds the value of the maxi-\nmum valid page in the segment. For example, if the code segment is\nusing its ﬁrst three pages (0, 1, and 2), the code segment page table will\nonly have three entries allocated to it and the bounds register will be set\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nPAGING: SMALLER TABLES\n205\nTIP: USE HYBRIDS\nWhen you have two good and seemingly opposing ideas, you should\nalways see if you can combine them into a hybrid that manages to achieve\nthe best of both worlds. Hybrid corn species, for example, are known to\nbe more robust than any naturally-occurring species. Of course, not all\nhybrids are a good idea; see the Zeedonk (or Zonkey), which is a cross of\na Zebra and a Donkey. If you don’t believe such a creature exists, look it\nup, and prepare to be amazed.\nto 3; memory accesses beyond the end of the segment will generate an ex-\nception and likely lead to the termination of the process. In this manner,\nour hybrid approach realizes a signiﬁcant memory savings compared to\nthe linear page table; unallocated pages between the stack and the heap\nno longer take up space in a page table (just to mark them as not valid).\nHowever, as you might notice, this approach is not without problems.\nFirst, it still requires us to use segmentation; as we discussed before, seg-\nmentation is not quite as ﬂexible as we would like, as it assumes a certain\nusage pattern of the address space; if we have a large but sparsely-used\nheap, for example, we can still end up with a lot of page table waste.\nSecond, this hybrid causes external fragmentation to arise again. While\nmost of memory is managed in page-sized units, page tables now can be\nof arbitrary size (in multiples of PTEs). Thus, ﬁnding free space for them\nin memory is more complicated. For these reasons, people continued to\nlook for better approaches to implementing smaller page tables.\n20.3\nMulti-level Page Tables\nA different approach doesn’t rely on segmentation but attacks the same\nproblem: how to get rid of all those invalid regions in the page table in-\nstead of keeping them all in memory? We call this approach a multi-level\npage table, as it turns the linear page table into something like a tree. This\napproach is so effective that many modern systems employ it (e.g., x86\n[BOH10]). We now describe this approach in detail.\nThe basic idea behind a multi-level page table is simple. First, chop up\nthe page table into page-sized units; then, if an entire page of page-table\nentries (PTEs) is invalid, don’t allocate that page of the page table at all.\nTo track whether a page of the page table is valid (and if valid, where it\nis in memory), use a new structure, called the page directory. The page\ndirectory thus either can be used to tell you where a page of the page\ntable is, or that the entire page of the page table contains no valid pages.\nFigure 20.2 shows an example. On the left of the ﬁgure is the classic\nlinear page table; even though most of the middle regions of the address\nspace are not valid, we still have to have page-table space allocated for\nthose regions (i.e., the middle two pages of the page table). On the right\nis a multi-level page table. The page directory marks just two pages of\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n206\nPAGING: SMALLER TABLES\nvalid\nprot\nPFN\n1\nrx\n12\n1\nrx\n13\n0\n-\n-\n1\nrw\n100\n0\n-\n-\n0\n-\n-\n0\n-\n-\n0\n-\n-\n0\n-\n-\n0\n-\n-\n0\n-\n-\n0\n-\n-\n0\n-\n-\n0\n-\n-\n1\nrw\n86\n1\nrw\n15\nLinear Page Table\nPTBR\n201\nPFN 201\nPFN 202\nPFN 203\nPFN 204\nvalid\nprot\nPFN\n1\nrx\n12\n1\nrx\n13\n0\n-\n-\n1\nrw\n100\n0\n-\n-\n0\n-\n-\n1\nrw\n86\n1\nrw\n15\n[Page 1 of PT: Not Allocated]\n[Page 2 of PT: Not Allocated]\nPFN 201\nPFN 204\nMulti-level Page Table\nPDBR\n200\nvalid\nPFN\n1\n201\n0\n-\n0\n-\n1\n204\nPFN 200\nThe Page Directory\nFigure 20.2: Linear (Left) And Multi-Level (Right) Page Tables\nthe page table as valid (the ﬁrst and last); thus, just those two pages of the\npage table reside in memory. And thus you can see one way to visualize\nwhat a multi-level table is doing: it just makes parts of the linear page\ntable disappear (freeing those frames for other uses), and tracks which\npages of the page table are allocated with the page directory.\nThe page directory, in a simple two-level table, contains one entry per\npage of the page table. It consists of a number of page directory entries\n(PDE). A PDE (minimally) has a valid bit and a page frame number\n(PFN), similar to a PTE. However, as hinted at above, the meaning of\nthis valid bit is slightly different: if the PDE entry is valid, it means that\nat least one of the pages of the page table that the entry points to (via the\nPFN) is valid, i.e., in at least one PTE on that page pointed to by this PDE,\nthe valid bit in that PTE is set to one. If the PDE entry is not valid (i.e.,\nequal to zero), the rest of the PDE is not deﬁned.\nMulti-level page tables have some obvious advantages over approaches\nwe’ve seen thus far. First, and perhaps most obviously, the multi-level ta-\nble only allocates page-table space in proportion to the amount of address\nspace you are using; thus it is generally compact and supports sparse ad-\ndress spaces.\nSecond, if carefully constructed, each portion of the page table ﬁts\nneatly within a page, making it easier to manage memory; the OS can\nsimply grab the next free page when it needs to allocate or grow a page\ntable. Contrast this to a simple (non-paged) linear page table2, which\nis just an array of PTEs indexed by VPN; with such a structure, the en-\ntire linear page table must reside contiguously in physical memory. For\na large page table (say 4MB), ﬁnding such a large chunk of unused con-\ntiguous free physical memory can be quite a challenge. With a multi-level\n2We are making some assumptions here, i.e., that all page tables reside in their entirety in\nphysical memory (i.e., they are not swapped to disk); we’ll soon relax this assumption.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nPAGING: SMALLER TABLES\n207\nTIP: UNDERSTAND TIME-SPACE TRADE-OFFS\nWhen building a data structure, one should always consider time-space\ntrade-offs in its construction. Usually, if you wish to make access to a par-\nticular data structure faster, you will have to pay a space-usage penalty\nfor the structure.\nstructure, we add a level of indirection through use of the page directory,\nwhich points to pieces of the page table; that indirection allows us to place\npage-table pages wherever we would like in physical memory.\nIt should be noted that there is a cost to multi-level tables; on a TLB\nmiss, two loads from memory will be required to get the right translation\ninformation from the page table (one for the page directory, and one for\nthe PTE itself), in contrast to just one load with a linear page table. Thus,\nthe multi-level table is a small example of a time-space trade-off. We\nwanted smaller tables (and got them), but not for free; although in the\ncommon case (TLB hit), performance is obviously identical, a TLB miss\nsuffers from a higher cost with this smaller table.\nAnother obvious negative is complexity. Whether it is the hardware or\nOS handling the page-table lookup (on a TLB miss), doing so is undoubt-\nedly more involved than a simple linear page-table lookup. Often we are\nwilling to increase complexity in order to improve performance or reduce\noverheads; in the case of a multi-level table, we make page-table lookups\nmore complicated in order to save valuable memory.\nA Detailed Multi-Level Example\nTo understand the idea behind multi-level page tables better, let’s do an\nexample. Imagine a small address space of size 16 KB, with 64-byte pages.\nThus, we have a 14-bit virtual address space, with 8 bits for the VPN and\n6 bits for the offset. A linear page table would have 28 (256) entries, even\nif only a small portion of the address space is in use. Figure 20.3 presents\none example of such an address space.\nstack\nstack\n(free)\n(free)\n... all free ...\n(free)\n(free)\nheap\nheap\n(free)\n(free)\ncode\ncode\n1111 1111\n1111 1110\n1111 1101\n1111 1100\n0000 0111\n0000 0110\n0000 0101\n0000 0100\n0000 0011\n0000 0010\n0000 0001\n0000 0000\n................\nFigure 20.3: A 16-KB Address Space With 64-byte Pages\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n208\nPAGING: SMALLER TABLES\nTIP: BE WARY OF COMPLEXITY\nSystem designers should be wary of adding complexity into their sys-\ntem. What a good systems builder does is implement the least complex\nsystem that achieves the task at hand. For example, if disk space is abun-\ndant, you shouldn’t design a ﬁle system that works hard to use as few\nbytes as possible; similarly, if processors are fast, it is better to write a\nclean and understandable module within the OS than perhaps the most\nCPU-optimized, hand-assembled code for the task at hand. Be wary of\nneedless complexity, in prematurely-optimized code or other forms; such\napproaches make systems harder to understand, maintain, and debug.\nAs Antoine de Saint-Exupery famously wrote: “Perfection is ﬁnally at-\ntained not when there is no longer anything to add, but when there is no\nlonger anything to take away.” What he didn’t write: “It’s a lot easier to\nsay something about perfection than to actually achieve it.”\nIn this example, virtual pages 0 and 1 are for code, virtual pages 4 and\n5 for the heap, and virtual pages 254 and 255 for the stack; the rest of the\npages of the address space are unused.\nTo build a two-level page table for this address space, we start with\nour full linear page table and break it up into page-sized units. Recall our\nfull table (in this example) has 256 entries; assume each PTE is 4 bytes in\nsize. Thus, our page table is 1KB (256 × 4 bytes) in size. Given that we\nhave 64-byte pages, the 1-KB page table can be divided into 16 64-byte\npages; each page can hold 16 PTEs.\nWhat we need to understand now is how to take a VPN and use it to\nindex ﬁrst into the page directory and then into the page of the page table.\nRemember that each is an array of entries; thus, all we need to ﬁgure out\nis how to construct the index for each from pieces of the VPN.\nLet’s ﬁrst index into the page directory. Our page table in this example\nis small: 256 entries, spread across 16 pages. The page directory needs one\nentry per page of the page table; thus, it has 16 entries. As a result, we\nneed four bits of the VPN to index into the directory; we use the top four\nbits of the VPN, as follows:\n13\n12\n11\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\n0\nVPN\noffset\nPage Directory Index\nOnce we extract the page-directory index (PDIndex for short) from\nthe VPN, we can use it to ﬁnd the address of the page-directory entry\n(PDE) with a simple calculation: PDEAddr = PageDirBase + (PDIndex\n* sizeof(PDE)). This results in our page directory, which we now ex-\namine to make further progress in our translation.\nIf the page-directory entry is marked invalid, we know that the access\nis invalid, and thus raise an exception. If, however, the PDE is valid,\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nPAGING: SMALLER TABLES\n209\nwe have more work to do. Speciﬁcally, we now have to fetch the page-\ntable entry (PTE) from the page of the page table pointed to by this page-\ndirectory entry. To ﬁnd this PTE, we have to index into the portion of the\npage table using the remaining bits of the VPN:\n13\n12\n11\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\n0\nVPN\noffset\nPage Directory Index\nPage Table Index\nThis page-table index (PTIndex for short) can then be used to index\ninto the page table itself, giving us the address of our PTE:\nPTEAddr = (PDE.PFN << SHIFT) + (PTIndex * sizeof(PTE))\nNote that the page-frame number (PFN) obtained from the page-directory\nentry must be left-shifted into place before combining it with the page-\ntable index to form the address of the PTE.\nTo see if this all makes sense, we’ll now ﬁll in a multi-level page ta-\nble with some actual values, and translate a single virtual address. Let’s\nbegin with the page directory for this example (left side of Table 20.2).\nIn the ﬁgure, you can see that each page directory entry (PDE) de-\nscribes something about a page of the page table for the address space.\nIn this example, we have two valid regions in the address space (at the\nbeginning and end), and a number of invalid mappings in-between.\nIn physical page 100 (the physical frame number of the 0th page of the\npage table), we have the ﬁrst page of 16 page table entries for the ﬁrst 16\nVPNs in the address space. See Table 20.2 (middle part) for the contents\nof this portion of the page table.\nPage Directory\nPage of PT (@PFN:100)\nPage of PT (@PFN:101)\nPFN\nvalid?\nPFN\nvalid\nprot\nPFN\nvalid\nprot\n100\n1\n10\n1\nr-x\n–\n0\n—\n——\n0\n23\n1\nr-x\n–\n0\n—\n——\n0\n–\n0\n—\n–\n0\n—\n——\n0\n–\n0\n—\n–\n0\n—\n——\n0\n80\n1\nrw-\n–\n0\n—\n——\n0\n59\n1\nrw-\n–\n0\n—\n——\n0\n–\n0\n—\n–\n0\n—\n——\n0\n–\n0\n—\n–\n0\n—\n——\n0\n–\n0\n—\n–\n0\n—\n——\n0\n–\n0\n—\n–\n0\n—\n——\n0\n–\n0\n—\n–\n0\n—\n——\n0\n–\n0\n—\n–\n0\n—\n——\n0\n–\n0\n—\n–\n0\n—\n——\n0\n–\n0\n—\n–\n0\n—\n——\n0\n–\n0\n—\n55\n1\nrw-\n101\n1\n–\n0\n—\n45\n1\nrw-\nTable 20.2: A Page Directory, And Pieces Of Page Table\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n210\nPAGING: SMALLER TABLES\nThis page of the page table contains the mappings for the ﬁrst 16\nVPNs; in our example, VPNs 0 and 1 are valid (the code segment), as\nare 4 and 5 (the heap). Thus, the table has mapping information for each\nof those pages. The rest of the entries are marked invalid.\nThe other valid page of page table is found inside PFN 101. This page\ncontains mappings for the last 16 VPNs of the address space; see Table\n20.2 (right) for details.\nIn the example, VPNs 254 and 255 (the stack) have valid mappings.\nHopefully, what we can see from this example is how much space savings\nare possible with a multi-level indexed structure. In this example, instead\nof allocating the full sixteen pages for a linear page table, we allocate only\nthree: one for the page directory, and two for the chunks of the page table\nthat have valid mappings. The savings for large (32-bit or 64-bit) address\nspaces could obviously be much greater.\nFinally, let’s use this information in order to perform a translation.\nHere is an address that refers to the 0th byte of VPN 254: 0x3F80, or\n11 1111 1000 0000 in binary.\nRecall that we will use the top 4 bits of the VPN to index into the\npage directory. Thus, 1111 will choose the last (15th, if you start at the\n0th) entry of the page directory above. This points us to a valid page\nof the page table located at address 101. We then use the next 4 bits\nof the VPN (1110) to index into that page of the page table and ﬁnd\nthe desired PTE. 1110 is the next-to-last (14th) entry on the page, and\ntells us that page 254 of our virtual address space is mapped at physi-\ncal page 55. By concatenating PFN=55 (or hex 0x37) with offset=000000,\nwe can thus form our desired physical address and issue the request to\nthe memory system: PhysAddr = (PTE.PFN << SHIFT) + offset\n= 00 1101 1100 0000 = 0x0DC0.\nYou should now have some idea of how to construct a two-level page\ntable, using a page directory which points to pages of the page table. Un-\nfortunately, however, our work is not done. As we’ll now discuss, some-\ntimes two levels of page table is not enough!\nMore Than Two Levels\nIn our example thus far, we’ve assumed that multi-level page tables only\nhave two levels: a page directory and then pieces of the page table. In\nsome cases, a deeper tree is possible (and indeed, needed).\nLet’s take a simple example and use it to show why a deeper multi-\nlevel table can be useful. In this example, assume we have a 30-bit virtual\naddress space, and a small (512 byte) page. Thus our virtual address has\na 21-bit virtual page number component and a 9-bit offset.\nRemember our goal in constructing a multi-level page table: to make\neach piece of the page table ﬁt within a single page. Thus far, we’ve only\nconsidered the page table itself; however, what if the page directory gets\ntoo big?\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nPAGING: SMALLER TABLES\n211\nTo determine how many levels are needed in a multi-level table to\nmake all pieces of the page table ﬁt within a page, we start by determining\nhow many page-table entries ﬁt within a page. Given our page size of 512\nbytes, and assuming a PTE size of 4 bytes, you should see that you can ﬁt\n128 PTEs on a single page. When we index into a page of the page table,\nwe can thus conclude we’ll need the least signiﬁcant 7 bits (log2128) of\nthe VPN as an index:\n29 28 27 26 25 24 23 22 21 20 19 18 17 16 15 14 13 12 11 10 9\n8\n7\n6\n5\n4\n3\n2\n1\n0\nVPN\noffset\nPage Directory Index\nPage Table Index\nWhat you also might notice from the diagram above is how many bits\nare left into the (large) page directory: 14. If our page directory has 214\nentries, it spans not one page but 128, and thus our goal of making every\npiece of the multi-level page table ﬁt into a page vanishes.\nTo remedy this problem, we build a further level of the tree, by split-\nting the page directory itself into multiple pages, and then adding another\npage directory on top of that, to point to the pages of the page directory.\nWe can thus split up our virtual address as follows:\n29 28 27 26 25 24 23 22 21 20 19 18 17 16 15 14 13 12 11 10 9\n8\n7\n6\n5\n4\n3\n2\n1\n0\nVPN\noffset\nPD Index 0\nPD Index 1\nPage Table Index\nNow, when indexing the upper-level page directory, we use the very\ntop bits of the virtual address (PD Index 0 in the diagram); this index\ncan be used to fetch the page-directory entry from the top-level page di-\nrectory. If valid, the second level of the page directory is consulted by\ncombining the physical frame number from the top-level PDE and the\nnext part of the VPN (PD Index 1). Finally, if valid, the PTE address\ncan be formed by using the page-table index combined with the address\nfrom the second-level PDE. Whew! That’s a lot of work. And all just to\nlook something up in a multi-level table.\nThe Translation Process: Remember the TLB\nTo summarize the entire process of address translation using a two-level\npage table, we once again present the control ﬂow in algorithmic form\n(Figure 20.4). The ﬁgure shows what happens in hardware (assuming a\nhardware-managed TLB) upon every memory reference.\nAs you can see from the ﬁgure, before any of the complicated multi-\nlevel page table access occurs, the hardware ﬁrst checks the TLB; upon\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n212\nPAGING: SMALLER TABLES\n1\nVPN = (VirtualAddress & VPN_MASK) >> SHIFT\n2\n(Success, TlbEntry) = TLB_Lookup(VPN)\n3\nif (Success == True)\n// TLB Hit\n4\nif (CanAccess(TlbEntry.ProtectBits) == True)\n5\nOffset\n= VirtualAddress & OFFSET_MASK\n6\nPhysAddr = (TlbEntry.PFN << SHIFT) | Offset\n7\nRegister = AccessMemory(PhysAddr)\n8\nelse\n9\nRaiseException(PROTECTION_FAULT)\n10\nelse\n// TLB Miss\n11\n// first, get page directory entry\n12\nPDIndex = (VPN & PD_MASK) >> PD_SHIFT\n13\nPDEAddr = PDBR + (PDIndex * sizeof(PDE))\n14\nPDE\n= AccessMemory(PDEAddr)\n15\nif (PDE.Valid == False)\n16\nRaiseException(SEGMENTATION_FAULT)\n17\nelse\n18\n// PDE is valid: now fetch PTE from page table\n19\nPTIndex = (VPN & PT_MASK) >> PT_SHIFT\n20\nPTEAddr = (PDE.PFN << SHIFT) + (PTIndex * sizeof(PTE))\n21\nPTE\n= AccessMemory(PTEAddr)\n22\nif (PTE.Valid == False)\n23\nRaiseException(SEGMENTATION_FAULT)\n24\nelse if (CanAccess(PTE.ProtectBits) == False)\n25\nRaiseException(PROTECTION_FAULT)\n26\nelse\n27\nTLB_Insert(VPN, PTE.PFN, PTE.ProtectBits)\n28\nRetryInstruction()\nFigure 20.4: Multi-level Page Table Control Flow\na hit, the physical address is formed directly without accessing the page\ntable at all, as before. Only upon a TLB miss does the hardware need to\nperform the full multi-level lookup. On this path, you can see the cost of\nour traditional two-level page table: two additional memory accesses to\nlook up a valid translation.\n20.4\nInverted Page Tables\nAn even more extreme space savings in the world of page tables is\nfound with inverted page tables. Here, instead of having many page\ntables (one per process of the system), we keep a single page table that\nhas an entry for each physical page of the system. The entry tells us which\nprocess is using this page, and which virtual page of that process maps to\nthis physical page.\nFinding the correct entry is now a matter of searching through this\ndata structure. A linear scan would be expensive, and thus a hash table is\noften built over the base structure to speed lookups. The PowerPC is one\nexample of such an architecture [JM98].\nMore generally, inverted page tables illustrate what we’ve said from\nthe beginning: page tables are just data structures. You can do lots of\ncrazy things with data structures, making them smaller or bigger, making\nthem slower or faster. Multi-level and inverted page tables are just two\nexamples of the many things one could do.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nPAGING: SMALLER TABLES\n213\n20.5\nSwapping the Page Tables to Disk\nFinally, we discuss the relaxation of one ﬁnal assumption. Thus far,\nwe have assumed that page tables reside in kernel-owned physical mem-\nory. Even with our many tricks to reduce the size of page tables, it is still\npossible, however, that they may be too big to ﬁt into memory all at once.\nThus, some systems place such page tables in kernel virtual memory,\nthereby allowing the system to swap some of these page tables to disk\nwhen memory pressure gets a little tight. We’ll talk more about this in\na future chapter (namely, the case study on VAX/VMS), once we under-\nstand how to move pages in and out of memory in more detail.\n20.6\nSummary\nWe have now seen how real page tables are built; not necessarily just\nas linear arrays but as more complex data structures. The trade-offs such\ntables present are in time and space – the bigger the table, the faster a TLB\nmiss can be serviced, as well as the converse – and thus the right choice of\nstructure depends strongly on the constraints of the given environment.\nIn a memory-constrained system (like many older systems), small struc-\ntures make sense; in a system with a reasonable amount of memory and\nwith workloads that actively use a large number of pages, a bigger ta-\nble that speeds up TLB misses might be the right choice. With software-\nmanaged TLBs, the entire space of data structures opens up to the delight\nof the operating system innovator (hint: that’s you). What new struc-\ntures can you come up with? What problems do they solve? Think of\nthese questions as you fall asleep, and dream the big dreams that only\noperating-system developers can dream.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "page_number": 231,
      "chapter_number": 25,
      "summary": "This chapter covers segment 25 (pages 231-249). Key topics include paging, pages, and tables. [CG68] “Shared-access Data Processing System”\nJohn F.",
      "keywords": [
        "page table",
        "linear page table",
        "page directory",
        "multi-level page table",
        "Page Table Index",
        "pages",
        "PAGE TABLES SMALLER",
        "address space",
        "Tables",
        "Smaller Tables",
        "linear page",
        "TLB",
        "two-level page table",
        "Inverted Page Tables",
        "Page Directory Index"
      ],
      "concepts": [
        "paging",
        "pages",
        "tables",
        "space",
        "systems",
        "memory",
        "entries",
        "entry",
        "size",
        "make"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 59,
          "title": "Segment 59 (pages 499-500)",
          "relevance_score": 0.71,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 6",
          "chapter": 8,
          "title": "Segment 8 (pages 127-144)",
          "relevance_score": 0.71,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 11,
          "title": "Segment 11 (pages 89-108)",
          "relevance_score": 0.7,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 6",
          "chapter": 30,
          "title": "Segment 30 (pages 523-540)",
          "relevance_score": 0.7,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 7,
          "title": "Segment 7 (pages 52-61)",
          "relevance_score": 0.69,
          "method": "api"
        }
      ]
    },
    {
      "number": 26,
      "title": "Segment 26 (pages 250-260)",
      "start_page": 250,
      "end_page": 260,
      "detection_method": "topic_boundary",
      "content": "214\nPAGING: SMALLER TABLES\nReferences\n[BOH10] “Computer Systems: A Programmer’s Perspective”\nRandal E. Bryant and David R. O’Hallaron\nAddison-Wesley, 2010\nWe have yet to ﬁnd a good ﬁrst reference to the multi-level page table. However, this great textbook by\nBryant and O’Hallaron dives into the details of x86, which at least is an early system that used such\nstructures. It’s also just a great book to have.\n[JM98] “Virtual Memory: Issues of Implementation”\nBruce Jacob and Trevor Mudge\nIEEE Computer, June 1998\nAn excellent survey of a number of different systems and their approach to virtualizing memory. Plenty\nof details on x86, PowerPC, MIPS, and other architectures.\n[LL82] “Virtual Memory Management in the VAX/VMS Operating System”\nHank Levy and P. Lipman\nIEEE Computer, Vol. 15, No. 3, March 1982\nA terriﬁc paper about a real virtual memory manager in a classic operating system, VMS. So terriﬁc, in\nfact, that we’ll use it to review everything we’ve learned about virtual memory thus far a few chapters\nfrom now.\n[M28] “Reese’s Peanut Butter Cups”\nMars Candy Corporation.\nApparently these ﬁne confections were invented in 1928 by Harry Burnett Reese, a former dairy farmer\nand shipping foreman for one Milton S. Hershey. At least, that is what it says on Wikipedia. If true,\nHershey and Reese probably hated each other’s guts, as any two chocolate barons should.\n[N+02] “Practical, Transparent Operating System Support for Superpages”\nJuan Navarro, Sitaram Iyer, Peter Druschel, Alan Cox\nOSDI ’02, Boston, Massachusetts, October 2002\nA nice paper showing all the details you have to get right to incorporate large pages, or superpages,\ninto a modern OS. Not as easy as you might think, alas.\n[M07] “Multics: History”\nAvailable: http://www.multicians.org/history.html\nThis amazing web site provides a huge amount of history on the Multics system, certainly one of the\nmost inﬂuential systems in OS history. The quote from therein: “Jack Dennis of MIT contributed\ninﬂuential architectural ideas to the beginning of Multics, especially the idea of combining paging and\nsegmentation.” (from Section 1.2.1)\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nPAGING: SMALLER TABLES\n215\nHomework\nThis fun little homework tests if you understand how a multi-level\npage table works. And yes, there is some debate over the use of the term\n“fun” in the previous sentence. The program is called, perhaps unsur-\nprisingly: paging-multilevel-translate.py; see the README for\ndetails.\nQuestions\n• With a linear page table, you need a single register to locate the\npage table, assuming that hardware does the lookup upon a TLB\nmiss. How many registers do you need to locate a two-level page\ntable? A three-level table?\n• Use the simulator to perform translations given random seeds 0,\n1, and 2, and check your answers using the -c ﬂag. How many\nmemory references are needed to perform each lookup?\n• Given your understanding of how cache memory works, how do\nyou think memory references to the page table will behave in the\ncache? Will they lead to lots of cache hits (and thus fast accesses?)\nOr lots of misses (and thus slow accesses)?\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n21\nBeyond Physical Memory: Mechanisms\nThus far, we’ve assumed that an address space is unrealistically small\nand ﬁts into physical memory. In fact, we’ve been assuming that every\naddress space of every running process ﬁts into memory. We will now\nrelax these big assumptions, and assume that we wish to support many\nconcurrently-running large address spaces.\nTo do so, we require an additional level in the memory hierarchy.\nThus far, we have assumed that all pages reside in physical memory.\nHowever, to support large address spaces, the OS will need a place to\nstash away portions of address spaces that currently aren’t in great de-\nmand. In general, the characteristics of such a location are that it should\nhave more capacity than memory; as a result, it is generally slower (if it\nwere faster, we would just use it as memory, no?). In modern systems,\nthis role is usually served by a hard disk drive. Thus, in our memory\nhierarchy, big and slow hard drives sit at the bottom, with memory just\nabove. And thus we arrive at the crux of the problem:\nTHE CRUX: HOW TO GO BEYOND PHYSICAL MEMORY\nHow can the OS make use of a larger, slower device to transparently pro-\nvide the illusion of a large virtual address space?\nOne question you might have: why do we want to support a single\nlarge address space for a process? Once again, the answer is convenience\nand ease of use. With a large address space, you don’t have to worry\nabout if there is room enough in memory for your program’s data struc-\ntures; rather, you just write the program naturally, allocating memory as\nneeded. It is a powerful illusion that the OS provides, and makes your\nlife vastly simpler. You’re welcome! A contrast is found in older systems\nthat used memory overlays, which required programmers to manually\nmove pieces of code or data in and out of memory as they were needed\n[D97]. Try imagining what this would be like: before calling a function or\naccessing some data, you need to ﬁrst arrange for the code or data to be\nin memory; yuck!\n217\n\n\n218\nBEYOND PHYSICAL MEMORY: MECHANISMS\nASIDE: STORAGE TECHNOLOGIES\nWe’ll delve much more deeply into how I/O devices actually work later\n(see the chapter on I/O devices). So be patient! And of course the slower\ndevice need not be a hard disk, but could be something more modern\nsuch as a Flash-based SSD. We’ll talk about those things too. For now,\njust assume we have a big and relatively-slow device which we can use\nto help us build the illusion of a very large virtual memory, even bigger\nthan physical memory itself.\nBeyond just a single process, the addition of swap space allows the OS\nto support the illusion of a large virtual memory for multiple concurrently-\nrunning processes. The invention of multiprogramming (running multi-\nple programs “at once”, to better utilize the machine) almost demanded\nthe ability to swap out some pages, as early machines clearly could not\nhold all the pages needed by all processes at once. Thus, the combina-\ntion of multiprogramming and ease-of-use leads us to want to support\nusing more memory than is physically available. It is something that all\nmodern VM systems do; it is now something we will learn more about.\n21.1\nSwap Space\nThe ﬁrst thing we will need to do is to reserve some space on the disk\nfor moving pages back and forth. In operating systems, we generally refer\nto such space as swap space, because we swap pages out of memory to it\nand swap pages into memory from it. Thus, we will simply assume that\nthe OS can read from and write to the swap space, in page-sized units. To\ndo so, the OS will need to remember the disk address of a given page.\nThe size of the swap space is important, as ultimately it determines\nthe maximum number of memory pages that can be in use by a system at\na given time. Let us assume for simplicity that it is very large for now.\nIn the tiny example (Figure 21.1), you can see a little example of a 4-\npage physical memory and an 8-page swap space. In the example, three\nprocesses (Proc 0, Proc 1, and Proc 2) are actively sharing physical mem-\nory; each of the three, however, only have some of their valid pages in\nmemory, with the rest located in swap space on disk. A fourth process\n(Proc 3) has all of its pages swapped out to disk, and thus clearly isn’t\ncurrently running. One block of swap remains free. Even from this tiny\nexample, hopefully you can see how using swap space allows the system\nto pretend that memory is larger than it actually is.\nWe should note that swap space is not the only on-disk location for\nswapping trafﬁc. For example, assume you are running a program binary\n(e.g., ls, or your own compiled main program). The code pages from this\nbinary are initially found on disk, and when the program runs, they are\nloaded into memory (either all at once when the program starts execution,\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nBEYOND PHYSICAL MEMORY: MECHANISMS\n219\nPhysical\nMemory\nPFN 0\nProc 0\n[VPN 0]\nPFN 1\nProc 1\n[VPN 2]\nPFN 2\nProc 1\n[VPN 3]\nPFN 3\nProc 2\n[VPN 0]\nSwap\nSpace\nProc 0\n[VPN 1]\nBlock 0\nProc 0\n[VPN 2]\nBlock 1\n[Free]\nBlock 2\nProc 1\n[VPN 0]\nBlock 3\nProc 1\n[VPN 1]\nBlock 4\nProc 3\n[VPN 0]\nBlock 5\nProc 2\n[VPN 1]\nBlock 6\nProc 3\n[VPN 1]\nBlock 7\nFigure 21.1: Physical Memory and Swap Space\nor, as in modern systems, one page at a time when needed). However, if\nthe system needs to make room in physical memory for other needs, it\ncan safely re-use the memory space for these code pages, knowing that it\ncan later swap them in again from the on-disk binary in the ﬁle system.\n21.2\nThe Present Bit\nNow that we have some space on the disk, we need to add some ma-\nchinery higher up in the system in order to support swapping pages to\nand from the disk. Let us assume, for simplicity, that we have a system\nwith a hardware-managed TLB.\nRecall ﬁrst what happens on a memory reference. The running pro-\ncess generates virtual memory references (for instruction fetches, or data\naccesses), and, in this case, the hardware translates them into physical\naddresses before fetching the desired data from memory.\nRemember that the hardware ﬁrst extracts the VPN from the virtual\naddress, checks the TLB for a match (a TLB hit), and if a hit, produces the\nresulting physical address and fetches it from memory. This is hopefully\nthe common case, as it is fast (requiring no additional memory accesses).\nIf the VPN is not found in the TLB (i.e., a TLB miss), the hardware\nlocates the page table in memory (using the page table base register)\nand looks up the page table entry (PTE) for this page using the VPN\nas an index. If the page is valid and present in physical memory, the\nhardware extracts the PFN from the PTE, installs it in the TLB, and retries\nthe instruction, this time generating a TLB hit; so far, so good.\nIf we wish to allow pages to be swapped to disk, however, we must\nadd even more machinery. Speciﬁcally, when the hardware looks in the\nPTE, it may ﬁnd that the page is not present in physical memory. The way\nthe hardware (or the OS, in a software-managed TLB approach) deter-\nmines this is through a new piece of information in each page-table entry,\nknown as the present bit. If the present bit is set to one, it means the\npage is present in physical memory and everything proceeds as above; if\nit is set to zero, the page is not in memory but rather on disk somewhere.\nThe act of accessing a page that is not in physical memory is commonly\nreferred to as a page fault.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n220\nBEYOND PHYSICAL MEMORY: MECHANISMS\nASIDE: SWAPPING TERMINOLOGY AND OTHER THINGS\nTerminology in virtual memory systems can be a little confusing and vari-\nable across machines and operating systems. For example, a page fault\nmore generally could refer to any reference to a page table that generates\na fault of some kind: this could include the type of fault we are discussing\nhere, i.e., a page-not-present fault, but sometimes can refer to illegal mem-\nory accesses. Indeed, it is odd that we call what is deﬁnitely a legal access\n(to a page mapped into the virtual address space of a process, but simply\nnot in physical memory at the time) a “fault” at all; really, it should be\ncalled a page miss. But often, when people say a program is “page fault-\ning”, they mean that it is accessing parts of its virtual address space that\nthe OS has swapped out to disk.\nWe suspect the reason that this behavior became known as a “fault” re-\nlates to the machinery in the operating system to handle it. When some-\nthing unusual happens, i.e., when something the hardware doesn’t know\nhow to handle occurs, the hardware simply transfers control to the OS,\nhoping it can make things better. In this case, a page that a process wants\nto access is missing from memory; the hardware does the only thing it\ncan, which is raise an exception, and the OS takes over from there. As\nthis is identical to what happens when a process does something illegal,\nit is perhaps not surprising that we term the activity a “fault.”\nUpon a page fault, the OS is invoked to service the page fault. A partic-\nular piece of code, known as a page-fault handler, runs, and must service\nthe page fault, as we now describe.\n21.3\nThe Page Fault\nRecall that with TLB misses, we have two types of systems: hardware-\nmanaged TLBs (where the hardware looks in the page table to ﬁnd the\ndesired translation) and software-managed TLBs (where the OS does). In\neither type of system, if a page is not present, the OS is put in charge to\nhandle the page fault. The appropriately-named OS page-fault handler\nruns to determine what to do. Virtually all systems handle page faults in\nsoftware; even with a hardware-managed TLB, the hardware trusts the\nOS to manage this important duty.\nIf a page is not present and has been swapped to disk, the OS will need\nto swap the page into memory in order to service the page fault. Thus, a\nquestion arises: how will the OS know where to ﬁnd the desired page? In\nmany systems, the page table is a natural place to store such information.\nThus, the OS could use the bits in the PTE normally used for data such as\nthe PFN of the page for a disk address. When the OS receives a page fault\nfor a page, it looks in the PTE to ﬁnd the address, and issues the request\nto disk to fetch the page into memory.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nBEYOND PHYSICAL MEMORY: MECHANISMS\n221\nASIDE: WHY HARDWARE DOESN’T HANDLE PAGE FAULTS\nWe know from our experience with the TLB that hardware designers are\nloathe to trust the OS to do much of anything. So why do they trust the\nOS to handle a page fault? There are a few main reasons. First, page\nfaults to disk are slow; even if the OS takes a long time to handle a fault,\nexecuting tons of instructions, the disk operation itself is traditionally so\nslow that the extra overheads of running software are minimal. Second,\nto be able to handle a page fault, the hardware would have to understand\nswap space, how to issue I/Os to the disk, and a lot of other details which\nit currently doesn’t know much about. Thus, for both reasons of perfor-\nmance and simplicity, the OS handles page faults, and even hardware\ntypes can be happy.\nWhen the disk I/O completes, the OS will then update the page table\nto mark the page as present, update the PFN ﬁeld of the page-table entry\n(PTE) to record the in-memory location of the newly-fetched page, and\nretry the instruction. This next attempt may generate a TLB miss, which\nwould then be serviced and update the TLB with the translation (one\ncould alternately update the TLB upon when servicing the page fault,\nto avoid this step). Finally, a last restart would ﬁnd the translation in\nthe TLB and thus proceed to fetch the desired data or instruction from\nmemory at the translated physical address.\nNote that while the I/O is in ﬂight, the process will be in the blocked\nstate. Thus, the OS will be free to run other ready processes while the\npage fault is being serviced. Because I/O is expensive, this overlap of\nthe I/O (page fault) of one process and the execution of another is yet\nanother way a multiprogrammed system can make the most effective use\nof its hardware.\n21.4\nWhat If Memory Is Full?\nIn the process described above, you may notice that we assumed there\nis plenty of free memory in which to page in a page from swap space.\nOf course, this may not be the case; memory may be full (or close to it).\nThus, the OS might like to ﬁrst page out one or more pages to make room\nfor the new page(s) the OS is about to bring in. The process of picking a\npage to kick out, or replace is known as the page-replacement policy.\nAs it turns out, a lot of thought has been put into creating a good page-\nreplacement policy, as kicking out the wrong page can exact a great cost\non program performance. Making the wrong decision can cause a pro-\ngram to run at disk-like speeds instead of memory-like speeds; in cur-\nrent technology that means a program could run 10,000 or 100,000 times\nslower. Thus, such a policy is something we should study in some detail;\nindeed, that is exactly what we will do in the next chapter. For now, it is\ngood enough to understand that such a policy exists, built on top of the\nmechanisms described here.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n222\nBEYOND PHYSICAL MEMORY: MECHANISMS\n1\nVPN = (VirtualAddress & VPN_MASK) >> SHIFT\n2\n(Success, TlbEntry) = TLB_Lookup(VPN)\n3\nif (Success == True)\n// TLB Hit\n4\nif (CanAccess(TlbEntry.ProtectBits) == True)\n5\nOffset\n= VirtualAddress & OFFSET_MASK\n6\nPhysAddr = (TlbEntry.PFN << SHIFT) | Offset\n7\nRegister = AccessMemory(PhysAddr)\n8\nelse\n9\nRaiseException(PROTECTION_FAULT)\n10\nelse\n// TLB Miss\n11\nPTEAddr = PTBR + (VPN * sizeof(PTE))\n12\nPTE = AccessMemory(PTEAddr)\n13\nif (PTE.Valid == False)\n14\nRaiseException(SEGMENTATION_FAULT)\n15\nelse\n16\nif (CanAccess(PTE.ProtectBits) == False)\n17\nRaiseException(PROTECTION_FAULT)\n18\nelse if (PTE.Present == True)\n19\n// assuming hardware-managed TLB\n20\nTLB_Insert(VPN, PTE.PFN, PTE.ProtectBits)\n21\nRetryInstruction()\n22\nelse if (PTE.Present == False)\n23\nRaiseException(PAGE_FAULT)\nFigure 21.2: Page-Fault Control Flow Algorithm (Hardware)\n21.5\nPage Fault Control Flow\nWith all of this knowledge in place, we can now roughly sketch the\ncomplete control ﬂow of memory access. In other words, when some-\nbody asks you “what happens when a program fetches some data from\nmemory?”, you should have a pretty good idea of all the different pos-\nsibilities. See the control ﬂow in Figures 21.2 and 21.3 for more details;\nthe ﬁrst ﬁgure shows what the hardware does during translation, and the\nsecond what the OS does upon a page fault.\nFrom the hardware control ﬂow diagram in Figure 21.2, notice that\nthere are now three important cases to understand when a TLB miss oc-\ncurs. First, that the page was both present and valid (Lines 18–21); in\nthis case, the TLB miss handler can simply grab the PFN from the PTE,\nretry the instruction (this time resulting in a TLB hit), and thus continue\nas described (many times) before. In the second case (Lines 22–23), the\npage fault handler must be run; although this was a legitimate page for\nthe process to access (it is valid, after all), it is not present in physical\nmemory. Third (and ﬁnally), the access could be to an invalid page, due\nfor example to a bug in the program (Lines 13–14). In this case, no other\nbits in the PTE really matter; the hardware traps this invalid access, and\nthe OS trap handler runs, likely terminating the offending process.\nFrom the software control ﬂow in Figure 21.3, we can see what the OS\nroughly must do in order to service the page fault. First, the OS must ﬁnd\na physical frame for the soon-to-be-faulted-in page to reside within; if\nthere is no such page, we’ll have to wait for the replacement algorithm to\nrun and kick some pages out of memory, thus freeing them for use here.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nBEYOND PHYSICAL MEMORY: MECHANISMS\n223\n1\nPFN = FindFreePhysicalPage()\n2\nif (PFN == -1)\n// no free page found\n3\nPFN = EvictPage()\n// run replacement algorithm\n4\nDiskRead(PTE.DiskAddr, pfn) // sleep (waiting for I/O)\n5\nPTE.present = True\n// update page table with present\n6\nPTE.PFN\n= PFN\n// bit and translation (PFN)\n7\nRetryInstruction()\n// retry instruction\nFigure 21.3: Page-Fault Control Flow Algorithm (Software)\nWith a physical frame in hand, the handler then issues the I/O request\nto read in the page from swap space. Finally, when that slow operation\ncompletes, the OS updates the page table and retries the instruction. The\nretry will result in a TLB miss, and then, upon another retry, a TLB hit, at\nwhich point the hardware will be able to access the desired item.\n21.6\nWhen Replacements Really Occur\nThus far, the way we’ve described how replacements occur assumes\nthat the OS waits until memory is entirely full, and only then replaces\n(evicts) a page to make room for some other page. As you can imagine,\nthis is a little bit unrealistic, and there are many reasons for the OS to keep\na small portion of memory free more proactively.\nTo keep a small amount of memory free, most operating systems thus\nhave some kind of high watermark (HW ) and low watermark (LW ) to\nhelp decide when to start evicting pages from memory. How this works is\nas follows: when the OS notices that there are fewer than LW pages avail-\nable, a background thread that is responsible for freeing memory runs.\nThe thread evicts pages until there are HW pages available. The back-\nground thread, sometimes called the swap daemon or page daemon1,\nthen goes to sleep, happy that is has freed some memory for running pro-\ncesses and the OS to use.\nBy performing a number of replacements at once, new performance\noptimizations become possible. For example, many systems will cluster\nor group a number of pages and write them out at once to the swap parti-\ntion, thus increasing the efﬁciency of the disk [LL82]; as we will see later\nwhen we discuss disks in more detail, such clustering reduces seek and\nrotational overheads of a disk and thus increases performance noticeably.\nTo work with the background paging thread, the control ﬂow in Figure\n21.3 should be modiﬁed slightly; instead of performing a replacement\ndirectly, the algorithm would instead simply check if there are any free\npages available. If not, it would signal that the background paging thread\nthat free pages are needed; when the thread frees up some pages, it would\nre-awaken the original thread, which could then page in the desired page\nand go about its work.\n1The word “daemon”, usually pronounced “demon”, is an old term for a background\nthread or process that does something useful. Turns out (once again!) that the source of the\nterm is Multics [CS94].\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n224\nBEYOND PHYSICAL MEMORY: MECHANISMS\nTIP: DO WORK IN THE BACKGROUND\nWhen you have some work to do, it is often a good idea to do it in the\nbackground to increase efﬁciency and to allow for grouping of opera-\ntions. Operating systems often do work in the background; for example,\nmany systems buffer ﬁle writes in memory before actually writing the\ndata to disk. Doing so has many possible beneﬁts: increased disk efﬁ-\nciency, as the disk may now receive many writes at once and thus better\nbe able to schedule them; improved latency of writes, as the application\nthinks the writes completed quite quickly; the possibility of work reduc-\ntion, as the writes may need never to go to disk (i.e., if the ﬁle is deleted);\nand better use of idle time, as the background work may possibly be\ndone when the system is otherwise idle, thus better utilizing the hard-\nware [G+95].\n21.7\nSummary\nIn this brief chapter, we have introduced the notion of accessing more\nmemory than is physically present within a system. To do so requires\nmore complexity in page-table structures, as a present bit (of some kind)\nmust be included to tell us whether the page is present in memory or not.\nWhen not, the operating system page-fault handler runs to service the\npage fault, and thus arranges for the transfer of the desired page from\ndisk to memory, perhaps ﬁrst replacing some pages in memory to make\nroom for those soon to be swapped in.\nRecall, importantly (and amazingly!), that these actions all take place\ntransparently to the process. As far as the process is concerned, it is just\naccessing its own private, contiguous virtual memory. Behind the scenes,\npages are placed in arbitrary (non-contiguous) locations in physical mem-\nory, and sometimes they are not even present in memory, requiring a fetch\nfrom disk. While we hope that in the common case a memory access is\nfast, in some cases it will take multiple disk operations to service it; some-\nthing as simple as performing a single instruction can, in the worst case,\ntake many milliseconds to complete.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "page_number": 250,
      "chapter_number": 26,
      "summary": "[M07] “Multics: History”\nAvailable: http://www.multicians.org/history.html\nThis amazing web site provides a huge amount of history on the Multics system, certainly one of the\nmost inﬂuential systems in OS history Key topics include paging, pages, and memory.",
      "keywords": [
        "Memory",
        "Physical Memory",
        "page fault",
        "page table",
        "TLB",
        "pages",
        "Physical",
        "fault",
        "page physical memory",
        "Virtual Memory",
        "VPN",
        "swap space",
        "space",
        "Systems",
        "disk"
      ],
      "concepts": [
        "paging",
        "pages",
        "memory",
        "hardware",
        "disk",
        "runs",
        "run",
        "swap",
        "swapped",
        "operating"
      ],
      "similar_chapters": [
        {
          "book": "Generative AI with LangChain_2e",
          "chapter": 15,
          "title": "Segment 15 (pages 122-129)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 33,
          "title": "Segment 33 (pages 662-682)",
          "relevance_score": 0.53,
          "method": "api"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 15,
          "title": "Segment 15 (pages 115-122)",
          "relevance_score": 0.49,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 41,
          "title": "Segment 41 (pages 815-837)",
          "relevance_score": 0.48,
          "method": "api"
        },
        {
          "book": "Game_Engine_Architecture",
          "chapter": 38,
          "title": "Segment 38 (pages 764-785)",
          "relevance_score": 0.48,
          "method": "api"
        }
      ]
    },
    {
      "number": 27,
      "title": "Segment 27 (pages 261-275)",
      "start_page": 261,
      "end_page": 275,
      "detection_method": "topic_boundary",
      "content": "BEYOND PHYSICAL MEMORY: MECHANISMS\n225\nReferences\n[CS94] “Take Our Word For It”\nF. Corbato and R. Steinberg\nAvailable: http://www.takeourword.com/TOW146/page4.html\nRichard Steinberg writes: “Someone has asked me the origin of the word daemon as it applies to comput-\ning. Best I can tell based on my research, the word was ﬁrst used by people on your team at Project MAC\nusing the IBM 7094 in 1963.” Professor Corbato replies: “Our use of the word daemon was inspired\nby the Maxwell’s daemon of physics and thermodynamics (my background is in physics). Maxwell’s\ndaemon was an imaginary agent which helped sort molecules of different speeds and worked tirelessly\nin the background. We fancifully began to use the word daemon to describe background processes which\nworked tirelessly to perform system chores.”\n[D97] “Before Memory Was Virtual”\nPeter Denning\nFrom In the Beginning: Recollections of Software Pioneers, Wiley, November 1997\nAn excellent historical piece by one of the pioneers of virtual memory and working sets.\n[G+95] “Idleness is not sloth”\nRichard Golding, Peter Bosch, Carl Staelin, Tim Sullivan, John Wilkes\nUSENIX ATC ’95, New Orleans, Louisiana\nA fun and easy-to-read discussion of how idle time can be better used in systems, with lots of good\nexamples.\n[LL82] “Virtual Memory Management in the VAX/VMS Operating System”\nHank Levy and P. Lipman\nIEEE Computer, Vol. 15, No. 3, March 1982\nNot the ﬁrst place where such clustering was used, but a clear and simple explanation of how such a\nmechanism works.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n22\nBeyond Physical Memory: Policies\nIn a virtual memory manager, life is easy when you have a lot of free\nmemory. A page fault occurs, you ﬁnd a free page on the free-page list,\nand assign it to the faulting page. Hey, Operating System, congratula-\ntions! You did it again.\nUnfortunately, things get a little more interesting when little memory\nis free. In such a case, this memory pressure forces the OS to start paging\nout pages to make room for actively-used pages. Deciding which page\n(or pages) to evict is encapsulated within the replacement policy of the\nOS; historically, it was one of the most important decisions the early vir-\ntual memory systems made, as older systems had little physical memory.\nMinimally, it is an interesting set of policies worth knowing a little more\nabout. And thus our problem:\nTHE CRUX: HOW TO DECIDE WHICH PAGE TO EVICT\nHow can the OS decide which page (or pages) to evict from memory?\nThis decision is made by the replacement policy of the system, which usu-\nally follows some general principles (discussed below) but also includes\ncertain tweaks to avoid corner-case behaviors.\n22.1\nCache Management\nBefore diving into policies, we ﬁrst describe the problem we are trying\nto solve in more detail. Given that main memory holds some subset of\nall the pages in the system, it can rightly be viewed as a cache for virtual\nmemory pages in the system. Thus, our goal in picking a replacement\npolicy for this cache is to minimize the number of cache misses; that is,\nto minimize the number of times that we have to go to disk to fetch the\ndesired page. Alternately, one can view our goal as maximizing the num-\nber of cache hits, i.e., the number of times a page that is read or written\nis found in memory.\n227\n\n\n228\nBEYOND PHYSICAL MEMORY: POLICIES\nKnowing the number of cache hits and misses let us calculate the av-\nerage memory access time (AMAT) for a program (a metric computer\narchitects compute for hardware caches [HP06]). Speciﬁcally, given these\nvalues, we can compute the AMAT of a program as follows:\nAMAT = (Hit% · TM) + (Miss% · TD)\n(22.1)\nwhere TM represents the cost of accessing memory, and represents TD the\ncost of accessing disk.\nFor example, let us imagine a machine with a (tiny) address space:\n4KB, with 256-byte pages. Thus, a virtual address has two components: a\n4-bit VPN (the most-signiﬁcant bits) and an 8-bit offset (the least-signiﬁcant\nbits). Thus, a process in this example can access 24 or 16 total virtual\npages. In this example, the process generates the following memory ref-\nerences (i.e., virtual addresses): 0x000, 0x100, 0x200, 0x300, 0x400, 0x500,\n0x600, 0x700, 0x800, 0x900. These virtual addresses refer to the ﬁrst byte\nof each of the ﬁrst ten pages of the address space (the page number being\nthe ﬁrst hex digit of each virtual address).\nLet us further assume that every page except virtual page 3 are already\nin memory. Thus, our sequence of memory references will encounter the\nfollowing behavior: hit, hit, hit, miss, hit, hit, hit, hit, hit, hit. We can\ncompute the hit rate (the percent of references found in memory): 90%,\nas 9 out of 10 references are in memory. The miss rate is obviously 10%.\nTo calculate AMAT, we simply need to know the cost of accessing\nmemory and the cost of accessing disk. Assuming the cost of access-\ning memory (TM) is around 100 nanoseconds, and the cost of access-\ning disk (TD) is about 10 milliseconds, we have the following AMAT:\n0.9 · 100ns + 0.1 · 10ms, which is 90ns + 1ms, or 1.00009 ms, or about\n1 millisecond. If our hit rate had instead been 99.9%, the result is quite\ndifferent: AMAT is 10.1 microseconds, or roughly 100 times faster. As the\nhit rate approaches 100%, AMAT approaches 100 nanoseconds.\nUnfortunately, as you can see in this example, the cost of disk access\nis so high in modern systems that even a tiny miss rate will quickly dom-\ninate the overall AMAT of running programs. Clearly, we need to avoid\nas many misses as possible or run slowly, at the rate of the disk. One way\nto help with this is to carefully develop a smart policy, as we now do.\n22.2\nThe Optimal Replacement Policy\nTo better understand how a particular replacement policy works, it\nwould be nice to compare it to the best possible replacement policy. As it\nturns out, such an optimal policy was developed by Belady many years\nago [B66] (he originally called it MIN). The optimal replacement policy\nleads to the fewest number of misses overall. Belady showed that a sim-\nple (but, unfortunately, difﬁcult to implement!) approach that replaces\nthe page that will be accessed furthest in the future is the optimal policy,\nresulting in the fewest-possible cache misses.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nBEYOND PHYSICAL MEMORY: POLICIES\n229\nTIP: COMPARING AGAINST OPTIMAL IS USEFUL\nAlthough optimal is not very practical as a real policy, it is incredibly\nuseful as a comparison point in simulation or other studies. Saying that\nyour fancy new algorithm has a 80% hit rate isn’t meaningful in isolation;\nsaying that optimal achieves an 82% hit rate (and thus your new approach\nis quite close to optimal) makes the result more meaningful and gives it\ncontext. Thus, in any study you perform, knowing what the optimal is\nlets you perform a better comparison, showing how much improvement\nis still possible, and also when you can stop making your policy better,\nbecause it is close enough to the ideal [AD03].\nHopefully, the intuition behind the optimal policy makes sense. Think\nabout it like this: if you have to throw out some page, why not throw\nout the one that is needed the furthest from now? By doing so, you are\nessentially saying that all the other pages in the cache are more important\nthan the one furthest out. The reason this is true is simple: you will refer\nto the other pages before you refer to the one furthest out.\nLet’s trace through a simple example to understand the decisions the\noptimal policy makes. Assume a program accesses the following stream\nof virtual pages: 0, 1, 2, 0, 1, 3, 0, 3, 1, 2, 1. Table 22.1 shows the behavior\nof optimal, assuming a cache that ﬁts three pages.\nIn the table, you can see the following actions. Not surprisingly, the\nﬁrst three accesses are misses, as the cache begins in an empty state; such\na miss is sometimes referred to as a cold-start miss (or compulsory miss).\nThen we refer again to pages 0 and 1, which both hit in the cache. Finally,\nwe reach another miss (to page 3), but this time the cache is full; a re-\nplacement must take place! Which begs the question: which page should\nwe replace? With the optimal policy, we examine the future for each page\ncurrently in the cache (0, 1, and 2), and see that 0 is accessed almost imme-\ndiately, 1 is accessed a little later, and 2 is accessed furthest in the future.\nThus the optimal policy has an easy choice: evict page 2, resulting in\npages 0, 1, and 3 in the cache. The next three references are hits, but then\nResulting\nAccess\nHit/Miss?\nEvict\nCache State\n0\nMiss\n0\n1\nMiss\n0, 1\n2\nMiss\n0, 1, 2\n0\nHit\n0, 1, 2\n1\nHit\n0, 1, 2\n3\nMiss\n2\n0, 1, 3\n0\nHit\n0, 1, 3\n3\nHit\n0, 1, 3\n1\nHit\n0, 1, 3\n2\nMiss\n3\n0, 1, 2\n1\nHit\n0, 1, 2\nTable 22.1: Tracing the Optimal Policy\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n230\nBEYOND PHYSICAL MEMORY: POLICIES\nASIDE: TYPES OF CACHE MISSES\nIn the computer architecture world, architects sometimes ﬁnd it useful\nto characterize misses by type, into one of three categories: compulsory,\ncapacity, and conﬂict misses, sometimes called the Three C’s [H87]. A\ncompulsory miss (or cold-start miss [EF78]) occurs because the cache is\nempty to begin with and this is the ﬁrst reference to the item; in con-\ntrast, a capacity miss occurs because the cache ran out of space and had\nto evict an item to bring a new item into the cache. The third type of\nmiss (a conﬂict miss) arises in hardware because of limits on where an\nitem can be placed in a hardware cache, due to something known as set-\nassociativity; it does not arise in the OS page cache because such caches\nare always fully-associative, i.e., there are no restrictions on where in\nmemory a page can be placed. See H&P for details [HP06].\nwe get to page 2, which we evicted long ago, and suffer another miss.\nHere the optimal policy again examines the future for each page in the\ncache (0, 1, and 3), and sees that as long as it doesn’t evict page 1 (which\nis about to be accessed), we’ll be OK. The example shows page 3 getting\nevicted, although 0 would have been a ﬁne choice too. Finally, we hit on\npage 1 and the trace completes.\nWe can also calculate the hit rate for the cache: with 6 hits and 5 misses,\nthe hit rate is\nHits\nHits+Misses which is\n6\n6+5 or 54.6%. You can also compute\nthe hit rate modulo compulsory misses (i.e., ignore the ﬁrst miss to a given\npage), resulting in a 85.7% hit rate.\nUnfortunately, as we saw before in the development of scheduling\npolicies, the future is not generally known; you can’t build the optimal\npolicy for a general-purpose operating system1. Thus, in developing a\nreal, deployable policy, we will focus on approaches that ﬁnd some other\nway to decide which page to evict. The optimal policy will thus serve\nonly as a comparison point, to know how close we are to “perfect”.\n22.3\nA Simple Policy: FIFO\nMany early systems avoided the complexity of trying to approach\noptimal and employed very simple replacement policies. For example,\nsome systems used FIFO (ﬁrst-in, ﬁrst-out) replacement, where pages\nwere simply placed in a queue when they enter the system; when a re-\nplacement occurs, the page on the tail of the queue (the “ﬁrst-in” page) is\nevicted. FIFO has one great strength: it is quite simple to implement.\nLet’s examine how FIFO does on our example reference stream (Table\n22.2). We again begin our trace with three compulsory misses to pages 0,\n1, and 2, and then hit on both 0 and 1. Next, page 3 is referenced, causing\na miss; the replacement decision is easy with FIFO: pick the page that\n1If you can, let us know! We can become rich together. Or, like the scientists who “discov-\nered” cold fusion, widely scorned and mocked.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nBEYOND PHYSICAL MEMORY: POLICIES\n231\nResulting\nAccess\nHit/Miss?\nEvict\nCache State\n0\nMiss\nFirst-in→\n0\n1\nMiss\nFirst-in→\n0, 1\n2\nMiss\nFirst-in→\n0, 1, 2\n0\nHit\nFirst-in→\n0, 1, 2\n1\nHit\nFirst-in→\n0, 1, 2\n3\nMiss\n0\nFirst-in→\n1, 2, 3\n0\nMiss\n1\nFirst-in→\n2, 3, 0\n3\nHit\nFirst-in→\n2, 3, 0\n1\nMiss\n2\nFirst-in→\n3, 0, 1\n2\nMiss\n3\nFirst-in→\n0, 1, 2\n1\nHit\nFirst-in→\n0, 1, 2\nTable 22.2: Tracing the FIFO Policy\nwas the “ﬁrst one” in (the cache state in the table is kept in FIFO order,\nwith the ﬁrst-in page on the left), which is page 0. Unfortunately, our next\naccess is to page 0, causing another miss and replacement (of page 1). We\nthen hit on page 3, but miss on 1 and 2, and ﬁnally hit on 3.\nComparing FIFO to optimal, FIFO does notably worse: a 36.4% hit\nrate (or 57.1% excluding compulsory misses). FIFO simply can’t deter-\nmine the importance of blocks: even though page 0 had been accessed\na number of times, FIFO still kicks it out, simply because it was the ﬁrst\none brought into memory.\nASIDE: BELADY’S ANOMALY\nBelady (of the optimal policy) and colleagues found an interesting refer-\nence stream that behaved a little unexpectedly [BNS69]. The memory-\nreference stream: 1, 2, 3, 4, 1, 2, 5, 1, 2, 3, 4, 5. The replacement policy\nthey were studying was FIFO. The interesting part: how the cache hit\nrate changed when moving from a cache size of 3 to 4 pages.\nIn general, you would expect the cache hit rate to increase (get better)\nwhen the cache gets larger. But in this case, with FIFO, it gets worse! Cal-\nculate the hits and misses yourself and see. This odd behavior is generally\nreferred to as Belady’s Anomaly (to the chagrin of his co-authors).\nSome other policies, such as LRU, don’t suffer from this problem. Can\nyou guess why? As it turns out, LRU has what is known as a stack prop-\nerty [M+70]. For algorithms with this property, a cache of size N + 1\nnaturally includes the contents of a cache of size N. Thus, when increas-\ning the cache size, hit rate will either stay the same or improve. FIFO and\nRandom (among others) clearly do not obey the stack property, and thus\nare susceptible to anomalous behavior.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n232\nBEYOND PHYSICAL MEMORY: POLICIES\nResulting\nAccess\nHit/Miss?\nEvict\nCache State\n0\nMiss\n0\n1\nMiss\n0, 1\n2\nMiss\n0, 1, 2\n0\nHit\n0, 1, 2\n1\nHit\n0, 1, 2\n3\nMiss\n0\n1, 2, 3\n0\nMiss\n1\n2, 3, 0\n3\nHit\n2, 3, 0\n1\nMiss\n3\n2, 0, 1\n2\nHit\n2, 0, 1\n1\nHit\n2, 0, 1\nTable 22.3: Tracing the Random Policy\n22.4\nAnother Simple Policy: Random\nAnother similar replacement policy is Random, which simply picks a\nrandom page to replace under memory pressure. Random has properties\nsimilar to FIFO; it is simple to implement, but it doesn’t really try to be\ntoo intelligent in picking which blocks to evict. Let’s look at how Random\ndoes on our famous example reference stream (see Table 22.3).\nOf course, how Random does depends entirely upon how lucky (or\nunlucky) Random gets in its choices. In the example above, Random does\na little better than FIFO, and a little worse than optimal. In fact, we can\nrun the Random experiment thousands of times and determine how it\ndoes in general. Figure 22.1 shows how many hits Random achieves over\n10,000 trials, each with a different random seed. As you can see, some-\ntimes (just over 40% of the time), Random is as good as optimal, achieving\n6 hits on the example trace; sometimes it does much worse, achieving 2\nhits or fewer. How Random does depends on the luck of the draw.\n0\n1\n2\n3\n4\n5\n6\n7\n0\n10\n20\n30\n40\n50\nNumber of Hits\nFrequency\nFigure 22.1: Random Performance over 10,000 Trials\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nBEYOND PHYSICAL MEMORY: POLICIES\n233\nResulting\nAccess\nHit/Miss?\nEvict\nCache State\n0\nMiss\nLRU→\n0\n1\nMiss\nLRU→\n0, 1\n2\nMiss\nLRU→\n0, 1, 2\n0\nHit\nLRU→\n1, 2, 0\n1\nHit\nLRU→\n2, 0, 1\n3\nMiss\n2\nLRU→\n0, 1, 3\n0\nHit\nLRU→\n1, 3, 0\n3\nHit\nLRU→\n1, 0, 3\n1\nHit\nLRU→\n0, 3, 1\n2\nMiss\n0\nLRU→\n3, 1, 2\n1\nHit\nLRU→\n3, 2, 1\nTable 22.4: Tracing the LRU Policy\n22.5\nUsing History: LRU\nUnfortunately, any policy as simple as FIFO or Random is likely to\nhave a common problem: it might kick out an important page, one that\nis about to be referenced again. FIFO kicks out the page that was ﬁrst\nbrought in; if this happens to be a page with important code or data\nstructures upon it, it gets thrown out anyhow, even though it will soon be\npaged back in. Thus, FIFO, Random, and similar policies are not likely to\napproach optimal; something smarter is needed.\nAs we did with scheduling policy, to improve our guess at the future,\nwe once again lean on the past and use history as our guide. For example,\nif a program has accessed a page in the near past, it is likely to access it\nagain in the near future.\nOne type of historical information a page-replacement policy could\nuse is frequency; if a page has been accessed many times, perhaps it\nshould not be replaced as it clearly has some value. A more commonly-\nused property of a page is its recency of access; the more recently a page\nhas been accessed, perhaps the more likely it will be accessed again.\nThis family of policies is based on what people refer to as the prin-\nciple of locality [D70], which basically is just an observation about pro-\ngrams and their behavior. What this principle says, quite simply, is that\nprograms tend to access certain code sequences (e.g., in a loop) and data\nstructures (e.g., an array accessed by the loop) quite frequently; we should\nthus try to use history to ﬁgure out which pages are important, and keep\nthose pages in memory when it comes to eviction time.\nAnd thus, a family of simple historically-based algorithms are born.\nThe Least-Frequently-Used (LFU) policy replaces the least-frequently-\nused page when an eviction must take place. Similarly, the Least-Recently-\nUsed (LRU) policy replaces the least-recently-used page.\nThese algo-\nrithms are easy to remember: once you know the name, you know exactly\nwhat it does, which is an excellent property for a name.\nTo better understand LRU, let’s examine how LRU does on our exam-\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n234\nBEYOND PHYSICAL MEMORY: POLICIES\nASIDE: TYPES OF LOCALITY\nThere are two types of locality that programs tend to exhibit. The ﬁrst\nis known as spatial locality, which states that if a page P is accessed,\nit is likely the pages around it (say P −1 or P + 1) will also likely be\naccessed. The second is temporal locality, which states that pages that\nhave been accessed in the near past are likely to be accessed again in the\nnear future. The assumption of the presence of these types of locality\nplays a large role in the caching hierarchies of hardware systems, which\ndeploy many levels of instruction, data, and address-translation caching\nto help programs run fast when such locality exists.\nOf course, the principle of locality, as it is often called, is no hard-and-\nfast rule that all programs must obey. Indeed, some programs access\nmemory (or disk) in rather random fashion and don’t exhibit much or\nany locality in their access streams. Thus, while locality is a good thing to\nkeep in mind while designing caches of any kind (hardware or software),\nit does not guarantee success. Rather, it is a heuristic that often proves\nuseful in the design of computer systems.\nple reference stream. Table 22.4 shows the results. From the table, you\ncan see how LRU can use history to do better than stateless policies such\nas Random or FIFO. In the example, LRU evicts page 2 when it ﬁrst has\nto replace a page, because 0 and 1 have been accessed more recently. It\nthen replaces page 0 because 1 and 3 have been accessed more recently.\nIn both cases, LRU’s decision, based on history, turns out to be correct,\nand the next references are thus hits. Thus, in our simple example, LRU\ndoes as well as possible, matching optimal in its performance.\nWe should also note that the opposites of these algorithms exist: Most-\nFrequently-Used (MFU) and Most-Recently-Used (MRU). In most cases\n(not all!), these policies do not work well, as they ignore the locality most\nprograms exhibit instead of embracing it.\n22.6\nWorkload Examples\nLet’s look at a few more examples in order to better understand how\nsome of these policies behave. We’ll look at more complex workloads\ninstead just a small trace of references. However, even these workloads\nare greatly simpliﬁed; a real study would include application traces.\nOur ﬁrst workload has no locality, which means that each reference\nis to a random page within the set of accessed pages. In this simple ex-\nample, the workload accesses 100 unique pages over time, choosing the\nnext page to refer to at random; overall, 10,000 pages are accessed. In the\nexperiment, we vary the cache size from very small (1 page) to enough\nto hold all the unique pages (100 page), in order to see how each policy\nbehaves over the range of cache sizes.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nBEYOND PHYSICAL MEMORY: POLICIES\n235\n0\n20\n40\n60\n80\n100\n0%\n20%\n40%\n60%\n80%\n100%\nThe No-Locality Workload\nCache Size (Blocks)\nHit Rate\nOPT\nLRU\nFIFO\nRAND\nFigure 22.2: The No-Locality Workload\nFigure 22.2 plots the results of the experiment for optimal, LRU, Ran-\ndom, and FIFO. The y-axis of the ﬁgure shows the hit rate that each policy\nachieves; the x-axis varies the cache size as described above.\nWe can draw a number of conclusions from the graph. First, when\nthere is no locality in the workload, it doesn’t matter much which realistic\npolicy you are using; LRU, FIFO, and Random all perform the same, with\nthe hit rate exactly determined by the size of the cache. Second, when\nthe cache is large enough to ﬁt the entire workload, it also doesn’t matter\nwhich policy you use; all policies (even optimal) converge to a 100% hit\nrate when all the referenced blocks ﬁt in cache. Finally, you can see that\noptimal performs noticeably better than the realistic policies; peeking into\nthe future, if it were possible, does a much better job of replacement.\nThe next workload we examine is called the “80-20” workload, which\nexhibits locality: 80% of the references are made to 20% of the pages (the\n“hot” pages); the remaining 20% of the references are made to the re-\nmaining 80% of the pages (the “cold” pages). In our workload, there are\na total 100 unique pages again; thus, “hot” pages are referred to most of\nthe time, and “cold” pages the remainder. Figure 22.3 shows how the\npolicies perform with this workload.\nAs you can see from the ﬁgure, while both random and FIFO do rea-\nsonably well, LRU does better, as it is more likely to hold onto the hot\npages; as those pages have been referred to frequently in the past, they\nare likely to be referred to again in the near future. Optimal once again\ndoes better, showing that LRU’s historical information is not perfect.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n236\nBEYOND PHYSICAL MEMORY: POLICIES\n0\n20\n40\n60\n80\n100\n0%\n20%\n40%\n60%\n80%\n100%\nThe 80-20 Workload\nCache Size (Blocks)\nHit Rate\nOPT\nLRU\nFIFO\nRAND\nFigure 22.3: The 80-20 Workload\nYou might now be wondering: is LRU’s improvement over Random\nand FIFO really that big of a deal? The answer, as usual, is “it depends.” If\neach miss is very costly (not uncommon), then even a small increase in hit\nrate (reduction in miss rate) can make a huge difference on performance.\nIf misses are not so costly, then of course the beneﬁts possible with LRU\nare not nearly as important.\nLet’s look at one ﬁnal workload. We call this one the “looping sequen-\ntial” workload, as in it, we refer to 50 pages in sequence, starting at 0,\nthen 1, ..., up to page 49, and then we loop, repeating those accesses, for a\ntotal of 10,000 accesses to 50 unique pages. The last graph in Figure 22.4\nshows the behavior of the policies under this workload.\nThis workload, common in many applications (including important\ncommercial applications such as databases [CD85]), represents a worst-\ncase for both LRU and FIFO. These algorithms, under a looping-sequential\nworkload, kick out older pages; unfortunately, due to the looping nature\nof the workload, these older pages are going to be accessed sooner than\nthe pages that the policies prefer to keep in cache. Indeed, even with\na cache of size 49, a looping-sequential workload of 50 pages results in\na 0% hit rate. Interestingly, Random fares notably better, not quite ap-\nproaching optimal, but at least achieving a non-zero hit rate. Turns out\nthat random has some nice properties; one such property is not having\nweird corner-case behaviors.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nBEYOND PHYSICAL MEMORY: POLICIES\n237\n0\n20\n40\n60\n80\n100\n0%\n20%\n40%\n60%\n80%\n100%\nThe Looping-Sequential Workload\nCache Size (Blocks)\nHit Rate\nOPT\nLRU\nFIFO\nRAND\nFigure 22.4: The Looping Workload\n22.7\nImplementing Historical Algorithms\nAs you can see, an algorithm such as LRU can generally do a better\njob than simpler policies like FIFO or Random, which may throw out\nimportant pages. Unfortunately, historical policies present us with a new\nchallenge: how do we implement them?\nLet’s take, for example, LRU. To implement it perfectly, we need to\ndo a lot of work. Speciﬁcally, upon each page access (i.e., each memory\naccess, whether an instruction fetch or a load or store), we must update\nsome data structure to move this page to the front of the list (i.e., the\nMRU side). Contrast this to FIFO, where the FIFO list of pages is only\naccessed when a page is evicted (by removing the ﬁrst-in page) or when\na new page is added to the list (to the last-in side). To keep track of which\npages have been least- and most-recently used, the system has to do some\naccounting work on every memory reference. Clearly, without great care,\nsuch accounting could greatly reduce performance.\nOne method that could help speed this up is to add a little bit of hard-\nware support. For example, a machine could update, on each page access,\na time ﬁeld in memory (for example, this could be in the per-process page\ntable, or just in some separate array in memory, with one entry per phys-\nical page of the system). Thus, when a page is accessed, the time ﬁeld\nwould be set, by hardware, to the current time. Then, when replacing a\npage, the OS could simply scan all the time ﬁelds in the system to ﬁnd the\nleast-recently-used page.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n238\nBEYOND PHYSICAL MEMORY: POLICIES\nUnfortunately, as the number of pages in a system grows, scanning a\nhuge array of times just to ﬁnd the absolute least-recently-used page is\nprohibitively expensive. Imagine a modern machine with 4GB of mem-\nory, chopped into 4KB pages. This machine has 1 million pages, and thus\nﬁnding the LRU page will take a long time, even at modern CPU speeds.\nWhich begs the question: do we really need to ﬁnd the absolute oldest\npage to replace? Can we instead survive with an approximation?\nCRUX: HOW TO IMPLEMENT AN LRU REPLACEMENT POLICY\nGiven that it will be expensive to implement perfect LRU, can we ap-\nproximate it in some way, and still obtain the desired behavior?\n22.8\nApproximating LRU\nAs it turns out, the answer is yes: approximating LRU is more fea-\nsible from a computational-overhead standpoint, and indeed it is what\nmany modern systems do. The idea requires some hardware support,\nin the form of a use bit (sometimes called the reference bit), the ﬁrst of\nwhich was implemented in the ﬁrst system with paging, the Atlas one-\nlevel store [KE+62]. There is one use bit per page of the system, and the\nuse bits live in memory somewhere (they could be in the per-process page\ntables, for example, or just in an array somewhere). Whenever a page is\nreferenced (i.e., read or written), the use bit is set by hardware to 1. The\nhardware never clears the bit, though (i.e., sets it to 0); that is the respon-\nsibility of the OS.\nHow does the OS employ the use bit to approximate LRU? Well, there\ncould be a lot of ways, but with the clock algorithm [C69], one simple\napproach was suggested. Imagine all the pages of the system arranged in\na circular list. A clock hand points to some particular page to begin with\n(it doesn’t really matter which). When a replacement must occur, the OS\nchecks if the currently-pointed to page P has a use bit of 1 or 0. If 1, this\nimplies that page P was recently used and thus is not a good candidate\nfor replacement. Thus, the clock hand is incremented to the next page\nP + 1, and the use bit for P set to 0 (cleared). The algorithm continues\nuntil it ﬁnds a use bit that is set to 0, implying this page has not been\nrecently used (or, in the worst case, that all pages have been and that we\nhave now searched through the entire set of pages, clearing all the bits).\nNote that this approach is not the only way to employ a use bit to\napproximate LRU. Indeed, any approach which periodically clears the\nuse bits and then differentiates between which pages have use bits of 1\nversus 0 to decide which to replace would be ﬁne. The clock algorithm of\nCorbato’s was just one early approach which met with some success, and\nhad the nice property of not repeatedly scanning through all of memory\nlooking for an unused page.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nBEYOND PHYSICAL MEMORY: POLICIES\n239\n0\n20\n40\n60\n80\n100\n0%\n20%\n40%\n60%\n80%\n100%\nThe 80-20 Workload\nCache Size (Blocks)\nHit Rate\nOPT\nLRU\nFIFO\nRAND\nClock\nFigure 22.5: The 80-20 Workload With Clock\nThe behavior of a clock algorithm variant is shown in Figure 22.5. This\nvariant randomly scans pages when doing a replacement; when it en-\ncounters a page with a reference bit set to 1, it clears the bit (i.e., sets it\nto 0); when it ﬁnds a page with the reference bit set to 0, it chooses it as\nits victim. As you can see, although it doesn’t do quite as well as perfect\nLRU, it does better than approaches that don’t consider history at all.\n22.9\nConsidering Dirty Pages\nOne small modiﬁcation to the clock algorithm (also originally sug-\ngested by Corbato [C69]) that is commonly made is the additional con-\nsideration of whether a page has been modiﬁed or not while in memory.\nThe reason for this: if a page has been modiﬁed and is thus dirty, it must\nbe written back to disk to evict it, which is expensive. If it has not been\nmodiﬁed (and is thus clean), the eviction is free; the physical frame can\nsimply be reused for other purposes without additional I/O. Thus, some\nVM systems prefer to evict clean pages over dirty pages.\nTo support this behavior, the hardware should include a modiﬁed bit\n(a.k.a. dirty bit). This bit is set any time a page is written, and thus can be\nincorporated into the page-replacement algorithm. The clock algorithm,\nfor example, could be changed to scan for pages that are both unused\nand clean to evict ﬁrst; failing to ﬁnd those, then for unused pages that\nare dirty; etc.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "page_number": 261,
      "chapter_number": 27,
      "summary": "This chapter covers segment 27 (pages 261-275). Key topics include paging, pages, and paged.",
      "keywords": [
        "LRU",
        "pages",
        "Hit",
        "hit rate",
        "PHYSICAL MEMORY",
        "MEMORY",
        "FIFO",
        "Cache",
        "Miss",
        "policy",
        "Random",
        "Policies",
        "optimal policy",
        "Optimal",
        "rate"
      ],
      "concepts": [
        "paging",
        "pages",
        "paged",
        "policies",
        "memory",
        "misses",
        "miss",
        "cache",
        "caching",
        "random"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 33,
          "title": "Segment 33 (pages 662-682)",
          "relevance_score": 0.7,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 42,
          "title": "Segment 42 (pages 838-857)",
          "relevance_score": 0.68,
          "method": "api"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 40,
          "title": "Segment 40 (pages 808-829)",
          "relevance_score": 0.63,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 41,
          "title": "Segment 41 (pages 815-837)",
          "relevance_score": 0.63,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 37,
          "title": "Segment 37 (pages 343-350)",
          "relevance_score": 0.58,
          "method": "api"
        }
      ]
    },
    {
      "number": 28,
      "title": "Segment 28 (pages 276-284)",
      "start_page": 276,
      "end_page": 284,
      "detection_method": "topic_boundary",
      "content": "240\nBEYOND PHYSICAL MEMORY: POLICIES\n22.10\nOther VM Policies\nPage replacement is not the only policy the VM subsystem employs\n(though it may be the most important). For example, the OS also has to\ndecide when to bring a page into memory. This policy, sometimes called\nthe page selection policy (as it was called by Denning [D70]), presents\nthe OS with some different options.\nFor most pages, the OS simply uses demand paging, which means the\nOS brings the page into memory when it is accessed, “on demand” as\nit were. Of course, the OS could guess that a page is about to be used,\nand thus bring it in ahead of time; this behavior is known as prefetching\nand should only be done when there is reasonable chance of success. For\nexample, some systems will assume that if a code page P is brought into\nmemory, that code page P +1 will likely soon be accessed and thus should\nbe brought into memory too.\nAnother policy determines how the OS writes pages out to disk. Of\ncourse, they could simply be written out one at a time; however, many\nsystems instead collect a number of pending writes together in memory\nand write them to disk in one (more efﬁcient) write. This behavior is\nusually called clustering or simply grouping of writes, and is effective\nbecause of the nature of disk drives, which perform a single large write\nmore efﬁciently than many small ones.\n22.11\nThrashing\nBefore closing, we address one ﬁnal question: what should the OS do\nwhen memory is simply oversubscribed, and the memory demands of the\nset of running processes simply exceeds the available physical memory?\nIn this case, the system will constantly be paging, a condition sometimes\nreferred to as thrashing [D70].\nSome earlier operating systems had a fairly sophisticated set of mech-\nanisms to both detect and cope with thrashing when it took place. For\nexample, given a set of processes, a system could decide not to run a sub-\nset of processes, with the hope that the reduced set of processes working\nsets (the pages that they are using actively) ﬁt in memory and thus can\nmake progress. This approach, generally known as admission control,\nstates that it is sometimes better to do less work well than to try to do\neverything at once poorly, a situation we often encounter in real life as\nwell as in modern computer systems (sadly).\nSome current systems take more a draconian approach to memory\noverload. For example, some versions of Linux run an out-of-memory\nkiller when memory is oversubscribed; this daemon chooses a memory-\nintensive process and kills it, thus reducing memory in a none-too-subtle\nmanner. While successful at reducing memory pressure, this approach\ncan have problems, if, for example, it kills the X server and thus renders\nany applications requiring the display unusable.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nBEYOND PHYSICAL MEMORY: POLICIES\n241\n22.12\nSummary\nWe have seen the introduction of a number of page-replacement (and\nother) policies, which are part of the VM subsystem of all modern operat-\ning systems. Modern systems add some tweaks to straightforward LRU\napproximations like clock; for example, scan resistance is an important\npart of many modern algorithms, such as ARC [MM03]. Scan-resistant al-\ngorithms are usually LRU-like but also try to avoid the worst-case behav-\nior of LRU, which we saw with the looping-sequential workload. Thus,\nthe evolution of page-replacement algorithms continues.\nHowever, in many cases the importance of said algorithms has de-\ncreased, as the discrepancy between memory-access and disk-access times\nhas increased. Because paging to disk is so expensive, the cost of frequent\npaging is prohibitive. Thus, the best solution to excessive paging is often\na simple (if intellectually dissatisfying) one: buy more memory.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n242\nBEYOND PHYSICAL MEMORY: POLICIES\nReferences\n[AD03] “Run-Time Adaptation in River”\nRemzi H. Arpaci-Dusseau\nACM TOCS, 21:1, February 2003\nA summary of one of the authors’ dissertation work on a system named River. Certainly one place where\nhe learned that comparison against the ideal is an important technique for system designers.\n[B66] “A Study of Replacement Algorithms for Virtual-Storage Computer”\nLaszlo A. Belady\nIBM Systems Journal 5(2): 78-101, 1966\nThe paper that introduces the simple way to compute the optimal behavior of a policy (the MIN algo-\nrithm).\n[BNS69] “An Anomaly in Space-time Characteristics of Certain Programs Running in a Paging\nMachine”\nL. A. Belady and R. A. Nelson and G. S. Shedler\nCommunications of the ACM, 12:6, June 1969\nIntroduction of the little sequence of memory references known as Belady’s Anomaly. How do Nelson\nand Shedler feel about this name, we wonder?\n[CD85] “An Evaluation of Buffer Management Strategies for Relational Database Systems”\nHong-Tai Chou and David J. DeWitt\nVLDB ’85, Stockholm, Sweden, August 1985\nA famous database paper on the different buffering strategies you should use under a number of common\ndatabase access patterns. The more general lesson: if you know something about a workload, you can\ntailor policies to do better than the general-purpose ones usually found in the OS.\n[C69] “A Paging Experiment with the Multics System”\nF.J. Corbato\nIncluded in a Festschrift published in honor of Prof. P.M. Morse\nMIT Press, Cambridge, MA, 1969\nThe original (and hard to ﬁnd!) reference to the clock algorithm, though not the ﬁrst usage of a use bit.\nThanks to H. Balakrishnan of MIT for digging up this paper for us.\n[D70] “Virtual Memory”\nPeter J. Denning\nComputing Surveys, Vol. 2, No. 3, September 1970\nDenning’s early and famous survey on virtual memory systems.\n[EF78] “Cold-start vs. Warm-start Miss Ratios”\nMalcolm C. Easton and Ronald Fagin\nCommunications of the ACM, 21:10, October 1978\nA good discussion of cold-start vs. warm-start misses.\n[HP06] “Computer Architecture: A Quantitative Approach”\nJohn Hennessy and David Patterson\nMorgan-Kaufmann, 2006\nA great and marvelous book about computer architecture. Read it!\n[H87] “Aspects of Cache Memory and Instruction Buffer Performance”\nMark D. Hill\nPh.D. Dissertation, U.C. Berkeley, 1987\nMark Hill, in his dissertation work, introduced the Three C’s, which later gained wide popularity with\nits inclusion in H&P [HP06]. The quote from therein: “I have found it useful to partition misses ... into\nthree components intuitively based on the cause of the misses (page 49).”\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nBEYOND PHYSICAL MEMORY: POLICIES\n243\n[KE+62] “One-level Storage System”\nT. Kilburn, and D.B.G. Edwards and M.J. Lanigan and F.H. Sumner\nIRE Trans. EC-11:2, 1962\nAlthough Atlas had a use bit, it only had a very small number of pages, and thus the scanning of the\nuse bits in large memories was not a problem the authors solved.\n[M+70] “Evaluation Techniques for Storage Hierarchies”\nR. L. Mattson, J. Gecsei, D. R. Slutz, I. L. Traiger\nIBM Systems Journal, Volume 9:2, 1970\nA paper that is mostly about how to simulate cache hierarchies efﬁciently; certainly a classic in that\nregard, as well for its excellent discussion of some of the properties of various replacement algorithms.\nCan you ﬁgure out why the stack property might be useful for simulating a lot of different-sized caches\nat once?\n[MM03] “ARC: A Self-Tuning, Low Overhead Replacement Cache”\nNimrod Megiddo and Dharmendra S. Modha\nFAST 2003, February 2003, San Jose, California\nAn excellent modern paper about replacement algorithms, which includes a new policy, ARC, that is\nnow used in some systems. Recognized in 2014 as a “Test of Time” award winner by the storage systems\ncommunity at the FAST ’14 conference.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n244\nBEYOND PHYSICAL MEMORY: POLICIES\nHomework\nThis simulator, paging-policy.py, allows you to play around with\ndifferent page-replacement policies. See the README for details.\nQuestions\n• Generate random addresses with the following arguments: -s 0\n-n 10, -s 1 -n 10, and -s 2 -n 10. Change the policy from\nFIFO, to LRU, to OPT. Compute whether each access in said address\ntraces are hits or misses.\n• For a cache of size 5, generate worst-case address reference streams\nfor each of the following policies: FIFO, LRU, and MRU (worst-case\nreference streams cause the most misses possible. For the worst case\nreference streams, how much bigger of a cache is needed to improve\nperformance dramatically and approach OPT?\n• Generate a random trace (use python or perl). How would you\nexpect the different policies to perform on such a trace?\n• Now generate a trace with some locality. How can you generate\nsuch a trace? How does LRU perform on it? How much better than\nRAND is LRU? How does CLOCK do? How about CLOCK with\ndifferent numbers of clock bits?\n• Use a program like valgrind to instrument a real application and\ngenerate a virtual page reference stream.\nFor example, running\nvalgrind --tool=lackey --trace-mem=yes ls will output\na nearly-complete reference trace of every instruction and data ref-\nerence made by the program ls. To make this useful for the sim-\nulator above, you’ll have to ﬁrst transform each virtual memory\nreference into a virtual page-number reference (done by masking\noff the offset and shifting the resulting bits downward). How big\nof a cache is needed for your application trace in order to satisfy a\nlarge fraction of requests? Plot a graph of its working set as the size\nof the cache increases.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\n23\nThe VAX/VMS Virtual Memory System\nBefore we end our study of virtual memory, let us take a closer look at one\nparticularly clean and well done virtual memory manager, that found in\nthe VAX/VMS operating system [LL82]. In this note, we will discuss the\nsystem to illustrate how some of the concepts brought forth in earlier\nchapters together in a complete memory manager.\n23.1\nBackground\nThe VAX-11 minicomputer architecture was introduced in the late 1970’s\nby Digital Equipment Corporation (DEC). DEC was a massive player\nin the computer industry during the era of the mini-computer; unfortu-\nnately, a series of bad decisions and the advent of the PC slowly (but\nsurely) led to their demise [C03]. The architecture was realized in a num-\nber of implementations, including the VAX-11/780 and the less powerful\nVAX-11/750.\nThe OS for the system was known as VAX/VMS (or just plain VMS),\none of whose primary architects was Dave Cutler, who later led the effort\nto develop Microsoft’s Windows NT [C93]. VMS had the general prob-\nlem that it would be run on a broad range of machines, including very\ninexpensive VAXen (yes, that is the proper plural) to extremely high-end\nand powerful machines in the same architecture family. Thus, the OS had\nto have mechanisms and policies that worked (and worked well) across\nthis huge range of systems.\nTHE CRUX: HOW TO AVOID THE CURSE OF GENERALITY\nOperating systems often have a problem known as “the curse of gen-\nerality”, where they are tasked with general support for a broad class of\napplications and systems. The fundamental result of the curse is that the\nOS is not likely to support any one installation very well. In the case of\nVMS, the curse was very real, as the VAX-11 architecture was realized in\na number of different implementations. Thus, how can an OS be built so\nas to run effectively on a wide range of systems?\n245\n\n\n246\nTHE VAX/VMS VIRTUAL MEMORY SYSTEM\nAs an additional issue, VMS is an excellent example of software inno-\nvations used to hide some of the inherent ﬂaws of the architecture. Al-\nthough the OS often relies on the hardware to build efﬁcient abstractions\nand illusions, sometimes the hardware designers don’t quite get every-\nthing right; in the VAX hardware, we’ll see a few examples of this, and\nwhat the VMS operating system does to build an effective, working sys-\ntem despite these hardware ﬂaws.\n23.2\nMemory Management Hardware\nThe VAX-11 provided a 32-bit virtual address space per process, di-\nvided into 512-byte pages. Thus, a virtual address consisted of a 23-bit\nVPN and a 9-bit offset. Further, the upper two bits of the VPN were used\nto differentiate which segment the page resided within; thus, the system\nwas a hybrid of paging and segmentation, as we saw previously.\nThe lower-half of the address space was known as “process space” and\nis unique to each process. In the ﬁrst half of process space (known as P0),\nthe user program is found, as well as a heap which grows downward.\nIn the second half of process space (P1), we ﬁnd the stack, which grows\nupwards. The upper-half of the address space is known as system space\n(S), although only half of it is used. Protected OS code and data reside\nhere, and the OS is in this way shared across processes.\nOne major concern of the VMS designers was the incredibly small size\nof pages in the VAX hardware (512 bytes). This size, chosen for historical\nreasons, has the fundamental problem of making simple linear page ta-\nbles excessively large. Thus, one of the ﬁrst goals of the VMS designers\nwas to make sure that VMS would not overwhelm memory with page\ntables.\nThe system reduced the pressure page tables place on memory in two\nways. First, by segmenting the user address space into two, the VAX-11\nprovides a page table for each of these regions (P0 and P1) per process;\nthus, no page-table space is needed for the unused portion of the address\nspace between the stack and the heap. The base and bounds registers\nare used as you would expect; a base register holds the address of the\npage table for that segment, and the bounds holds its size (i.e., number of\npage-table entries).\nSecond, the OS reduces memory pressure even further by placing user\npage tables (for P0 and P1, thus two per process) in kernel virtual mem-\nory. Thus, when allocating or growing a page table, the kernel allocates\nspace out of its own virtual memory, in segment S. If memory comes un-\nder severe pressure, the kernel can swap pages of these page tables out to\ndisk, thus making physical memory available for other uses.\nPutting page tables in kernel virtual memory means that address trans-\nlation is even further complicated. For example, to translate a virtual ad-\ndress in P0 or P1, the hardware has to ﬁrst try to look up the page-table\nentry for that page in its page table (the P0 or P1 page table for that pro-\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nTHE VAX/VMS VIRTUAL MEMORY SYSTEM\n247\nPage 0: Invalid\nUser Code\nUser Heap\nUser Stack\nTrap Tables\nKernel Data\nKernel Code\nKernel Heap\nUnused\nSystem (S)\nUser (P1)\nUser (P0)\n0\n230\n231\n232\nFigure 23.1: The VAX/VMS Address Space\ncess); in doing so, however, the hardware may ﬁrst have to consult the\nsystem page table (which lives in physical memory); with that transla-\ntion complete, the hardware can learn the address of the page of the page\ntable, and then ﬁnally learn the address of the desired memory access.\nAll of this, fortunately, is made faster by the VAX’s hardware-managed\nTLBs, which usually (hopefully) circumvent this laborious lookup.\n23.3\nA Real Address Space\nOne neat aspect of studying VMS is that we can see how a real address\nspace is constructed (Figure 23.1. Thus far, we have assumed a simple\naddress space of just user code, user data, and user heap, but as we can\nsee above, a real address space is notably more complex.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n248\nTHE VAX/VMS VIRTUAL MEMORY SYSTEM\nASIDE: WHY NULL POINTER ACCESSES CAUSE SEG FAULTS\nYou should now have a good understanding of exactly what happens on\na null-pointer dereference. A process generates a virtual address of 0, by\ndoing something like this:\nint *p = NULL; // set p = 0\n*p = 10;\n// try to store value 10 to virtual address 0\nThe hardware tries to look up the VPN (also 0 here) in the TLB, and suf-\nfers a TLB miss. The page table is consulted, and the entry for VPN 0\nis found to be marked invalid. Thus, we have an invalid access, which\ntransfers control to the OS, which likely terminates the process (on UNIX\nsystems, processes are sent a signal which allows them to react to such a\nfault; if uncaught, however, the process is killed).\nFor example, the code segment never begins at page 0. This page,\ninstead, is marked inaccessible, in order to provide some support for de-\ntecting null-pointer accesses. Thus, one concern when designing an ad-\ndress space is support for debugging, which the inaccessible zero page\nprovides here in some form.\nPerhaps more importantly, the kernel virtual address space (i.e., its\ndata structures and code) is a part of each user address space. On a con-\ntext switch, the OS changes the P0 and P1 registers to point to the ap-\npropriate page tables of the soon-to-be-run process; however, it does not\nchange the S base and bound registers, and as a result the “same” kernel\nstructures are mapped into each user address space.\nThe kernel is mapped into each address space for a number of reasons.\nThis construction makes life easier for the kernel; when, for example, the\nOS is handed a pointer from a user program (e.g., on a write() system\ncall), it is easy to copy data from that pointer to its own structures. The\nOS is naturally written and compiled, without worry of where the data\nit is accessing comes from. If in contrast the kernel were located entirely\nin physical memory, it would be quite hard to do things like swap pages\nof the page table to disk; if the kernel were given its own address space,\nmoving data between user applications and the kernel would again be\ncomplicated and painful. With this construction (now used widely), the\nkernel appears almost as a library to applications, albeit a protected one.\nOne last point about this address space relates to protection. Clearly,\nthe OS does not want user applications reading or writing OS data or\ncode. Thus, the hardware must support different protection levels for\npages to enable this. The VAX did so by specifying, in protection bits\nin the page table, what privilege level the CPU must be at in order to\naccess a particular page. Thus, system data and code are set to a higher\nlevel of protection than user data and code; an attempted access to such\ninformation from user code will generate a trap into the OS, and (you\nguessed it) the likely termination of the offending process.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "page_number": 276,
      "chapter_number": 28,
      "summary": "This policy, sometimes called\nthe page selection policy (as it was called by Denning [D70]), presents\nthe OS with some different options Key topics include pages, paging, and memory.",
      "keywords": [
        "VMS Virtual Memory",
        "Virtual Memory System",
        "MEMORY",
        "Virtual Memory",
        "address space",
        "PHYSICAL MEMORY",
        "VMS operating system",
        "systems",
        "page table",
        "Memory System",
        "address",
        "VMS",
        "space",
        "Virtual",
        "user address space"
      ],
      "concepts": [
        "pages",
        "paging",
        "memory",
        "memories",
        "systems",
        "policies",
        "policy",
        "address",
        "addresses",
        "referred"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 42,
          "title": "Segment 42 (pages 838-857)",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 41,
          "title": "Segment 41 (pages 815-837)",
          "relevance_score": 0.61,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 43,
          "title": "Segment 43 (pages 858-879)",
          "relevance_score": 0.58,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 1",
          "chapter": 9,
          "title": "Segment 9 (pages 78-85)",
          "relevance_score": 0.53,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 7",
          "chapter": 6,
          "title": "Segment 6 (pages 48-56)",
          "relevance_score": 0.52,
          "method": "api"
        }
      ]
    },
    {
      "number": 29,
      "title": "Segment 29 (pages 285-293)",
      "start_page": 285,
      "end_page": 293,
      "detection_method": "topic_boundary",
      "content": "THE VAX/VMS VIRTUAL MEMORY SYSTEM\n249\n23.4\nPage Replacement\nThe page table entry (PTE) in VAX contains the following bits: a valid\nbit, a protection ﬁeld (4 bits), a modify (or dirty) bit, a ﬁeld reserved for\nOS use (5 bits), and ﬁnally a physical frame number (PFN) to store the\nlocation of the page in physical memory. The astute reader might note:\nno reference bit! Thus, the VMS replacement algorithm must make do\nwithout hardware support for determining which pages are active.\nThe developers were also concerned about memory hogs, programs\nthat use a lot of memory and make it hard for other programs to run.\nMost of the policies we have looked at thus far are susceptible to such\nhogging; for example, LRU is a global policy that doesn’t share memory\nfairly among processes.\nSegmented FIFO\nTo address these two problems, the developers came up with the seg-\nmented FIFO replacement policy [RL81]. The idea is simple: each pro-\ncess has a maximum number of pages it can keep in memory, known as\nits resident set size (RSS). Each of these pages is kept on a FIFO list; when\na process exceeds its RSS, the “ﬁrst-in” page is evicted. FIFO clearly does\nnot need any support from the hardware, and is thus easy to implement.\nOf course, pure FIFO does not perform particularly well, as we saw\nearlier. To improve FIFO’s performance, VMS introduced two second-\nchance lists where pages are placed before getting evicted from memory,\nspeciﬁcally a global clean-page free list and dirty-page list. When a process\nP exceeds its RSS, a page is removed from its per-process FIFO; if clean\n(not modiﬁed), it is placed on the end of the clean-page list; if dirty (mod-\niﬁed), it is placed on the end of the dirty-page list.\nIf another process Q needs a free page, it takes the ﬁrst free page off\nof the global clean list. However, if the original process P faults on that\npage before it is reclaimed, P reclaims it from the free (or dirty) list, thus\navoiding a costly disk access. The bigger these global second-chance lists\nare, the closer the segmented FIFO algorithm performs to LRU [RL81].\nPage Clustering\nAnother optimization used in VMS also helps overcome the small page\nsize in VMS. Speciﬁcally, with such small pages, disk I/O during swap-\nping could be highly inefﬁcient, as disks do better with large transfers.\nTo make swapping I/O more efﬁcient, VMS adds a number of optimiza-\ntions, but most important is clustering. With clustering, VMS groups\nlarge batches of pages together from the global dirty list, and writes them\nto disk in one fell swoop (thus making them clean). Clustering is used\nin most modern systems, as the freedom to place pages anywhere within\nswap space lets the OS group pages, perform fewer and bigger writes,\nand thus improve performance.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n250\nTHE VAX/VMS VIRTUAL MEMORY SYSTEM\nASIDE: EMULATING REFERENCE BITS\nAs it turns out, you don’t need a hardware reference bit in order to get\nsome notion of which pages are in use in a system. In fact, in the early\n1980’s, Babaoglu and Joy showed that protection bits on the VAX can be\nused to emulate reference bits [BJ81]. The basic idea: if you want to gain\nsome understanding of which pages are actively being used in a system,\nmark all of the pages in the page table as inaccessible (but keep around\nthe information as to which pages are really accessible by the process,\nperhaps in the “reserved OS ﬁeld” portion of the page table entry). When\na process accesses a page, it will generate a trap into the OS; the OS will\nthen check if the page really should be accessible, and if so, revert the\npage to its normal protections (e.g., read-only, or read-write). At the time\nof a replacement, the OS can check which pages remain marked inacces-\nsible, and thus get an idea of which pages have not been recently used.\nThe key to this “emulation” of reference bits is reducing overhead while\nstill obtaining a good idea of page usage. The OS must not be too aggres-\nsive in marking pages inaccessible, or overhead would be too high. The\nOS also must not be too passive in such marking, or all pages will end up\nreferenced; the OS will again have no good idea which page to evict.\n23.5\nOther Neat VM Tricks\nVMS had two other now-standard tricks: demand zeroing and copy-\non-write. We now describe these lazy optimizations.\nOne form of laziness in VMS (and most modern systems) is demand\nzeroing of pages. To understand this better, let’s consider the example\nof adding a page to your address space, say in your heap. In a naive\nimplementation, the OS responds to a request to add a page to your heap\nby ﬁnding a page in physical memory, zeroing it (required for security;\notherwise you’d be able to see what was on the page from when some\nother process used it!), and then mapping it into your address space (i.e.,\nsetting up the page table to refer to that physical page as desired). But the\nnaive implementation can be costly, particularly if the page does not get\nused by the process.\nWith demand zeroing, the OS instead does very little work when the\npage is added to your address space; it puts an entry in the page table\nthat marks the page inaccessible. If the process then reads or writes the\npage, a trap into the OS takes place. When handling the trap, the OS no-\ntices (usually through some bits marked in the “reserved for OS” portion\nof the page table entry) that this is actually a demand-zero page; at this\npoint, the OS then does the needed work of ﬁnding a physical page, ze-\nroing it, and mapping it into the process’s address space. If the process\nnever accesses the page, all of this work is avoided, and thus the virtue of\ndemand zeroing.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nTHE VAX/VMS VIRTUAL MEMORY SYSTEM\n251\nTIP: BE LAZY\nBeing lazy can be a virtue in both life as well as in operating systems.\nLaziness can put off work until later, which is beneﬁcial within an OS for\na number of reasons. First, putting off work might reduce the latency of\nthe current operation, thus improving responsiveness; for example, op-\nerating systems often report that writes to a ﬁle succeeded immediately,\nand only write them to disk later in the background. Second, and more\nimportantly, laziness sometimes obviates the need to do the work at all;\nfor example, delaying a write until the ﬁle is deleted removes the need to\ndo the write at all. Laziness is also good in life: for example, by putting\noff your OS project, you may ﬁnd that the project speciﬁcation bugs are\nworked out by your fellow classmates; however, the class project is un-\nlikely to get canceled, so being too lazy may be problematic, leading to a\nlate project, bad grade, and a sad professor. Don’t make professors sad!\nAnother cool optimization found in VMS (and again, in virtually every\nmodern OS) is copy-on-write (COW for short). The idea, which goes at\nleast back to the TENEX operating system [BB+72], is simple: when the\nOS needs to copy a page from one address space to another, instead of\ncopying it, it can map it into the target address space and mark it read-\nonly in both address spaces. If both address spaces only read the page, no\nfurther action is taken, and thus the OS has affected a fast copy without\nactually moving any data.\nIf, however, one of the address spaces does indeed try to write to the\npage, it will trap into the OS. The OS will then notice that the page is a\nCOW page, and thus (lazily) allocate a new page, ﬁll it with the data, and\nmap this new page into the address space of the faulting process. The\nprocess then continues and now has its own private copy of the page.\nCOW is useful for a number of reasons. Certainly any sort of shared\nlibrary can be mapped copy-on-write into the address spaces of many\nprocesses, saving valuable memory space. In UNIX systems, COW is\neven more critical, due to the semantics of fork() and exec(). As\nyou might recall, fork() creates an exact copy of the address space of\nthe caller; with a large address space, making such a copy is slow and\ndata intensive. Even worse, most of the address space is immediately\nover-written by a subsequent call to exec(), which overlays the calling\nprocess’s address space with that of the soon-to-be-exec’d program. By\ninstead performing a copy-on-write fork(), the OS avoids much of the\nneedless copying and thus retains the correct semantics while improving\nperformance.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n252\nTHE VAX/VMS VIRTUAL MEMORY SYSTEM\n23.6\nSummary\nYou have now seen a top-to-bottom review of an entire virtual mem-\nory system. Hopefully, most of the details were easy to follow, as you\nshould have already had a good understanding of most of the basic mech-\nanisms and policies. More detail is available in the excellent (and short)\npaper by Levy and Lipman [LL82]; we encourage you to read it, a great\nway to see what the source material behind these chapters is like.\nYou should also learn more about the state of the art by reading about\nLinux and other modern systems when possible. There is a lot of source\nmaterial out there, including some reasonable books [BC05]. One thing\nthat will amaze you: how classic ideas, found in old papers such as\nthis one on VAX/VMS, still inﬂuence how modern operating systems are\nbuilt.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nTHE VAX/VMS VIRTUAL MEMORY SYSTEM\n253\nReferences\n[BB+72] “TENEX, A Paged Time Sharing System for the PDP-10”\nDaniel G. Bobrow, Jerry D. Burchﬁel, Daniel L. Murphy, Raymond S. Tomlinson\nCommunications of the ACM, Volume 15, March 1972\nAn early time-sharing OS where a number of good ideas came from. Copy-on-write was just one of\nthose; inspiration for many other aspects of modern systems, including process management, virtual\nmemory, and ﬁle systems are found herein.\n[BJ81] “Converting a Swap-Based System to do Paging\nin an Architecture Lacking Page-Reference Bits”\nOzalp Babaoglu and William N. Joy\nSOSP ’81, December 1981, Paciﬁc Grove, California\nA clever idea paper on how to exploit existing protection machinery within a machine in order to emulate\nreference bits. The idea came from the group at Berkeley working on their own version of UNIX, known\nas the Berkeley Systems Distribution, or BSD. The group was heavily inﬂuential in the development of\nUNIX, in virtual memory, ﬁle systems, and networking.\n[BC05] “Understanding the Linux Kernel (Third Edition)”\nDaniel P. Bovet and Marco Cesati\nO’Reilly Media, November 2005\nOne of the many books you can ﬁnd on Linux. They go out of date quickly, but many of the basics\nremain and are worth reading about.\n[C03] “The Innovator’s Dilemma”\nClayton M. Christenson\nHarper Paperbacks, January 2003\nA fantastic book about the disk-drive industry and how new innovations disrupt existing ones. A good\nread for business majors and computer scientists alike. Provides insight on how large and successful\ncompanies completely fail.\n[C93] “Inside Windows NT”\nHelen Custer and David Solomon\nMicrosoft Press, 1993\nThe book about Windows NT that explains the system top to bottom, in more detail than you might like.\nBut seriously, a pretty good book.\n[LL82] “Virtual Memory Management in the VAX/VMS Operating System”\nHenry M. Levy, Peter H. Lipman\nIEEE Computer, Volume 15, Number 3 (March 1982) Read the original source of most of this ma-\nterial; tt is a concise and easy read. Particularly important if you wish to go to graduate school, where\nall you do is read papers, work, read some more papers, work more, eventually write a paper, and then\nwork some more. But it is fun!\n[RL81] “Segmented FIFO Page Replacement”\nRollins Turner and Henry Levy\nSIGMETRICS ’81\nA short paper that shows for some workloads, segmented FIFO can approach the performance of LRU.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n24\nSummary Dialogue on Memory Virtualization\nStudent: (Gulps) Wow, that was a lot of material.\nProfessor: Yes, and?\nStudent: Well, how am I supposed to remember it all? You know, for the exam?\nProfessor: Goodness, I hope that’s not why you are trying to remember it.\nStudent: Why should I then?\nProfessor: Come on, I thought you knew better. You’re trying to learn some-\nthing here, so that when you go off into the world, you’ll understand how systems\nactually work.\nStudent: Hmm... can you give an example?\nProfessor: Sure! One time back in graduate school, my friends and I were\nmeasuring how long memory accesses took, and once in a while the numbers\nwere way higher than we expected; we thought all the data was ﬁtting nicely into\nthe second-level hardware cache, you see, and thus should have been really fast\nto access.\nStudent: (nods)\nProfessor: We couldn’t ﬁgure out what was going on. So what do you do in such\na case? Easy, ask a professor! So we went and asked one of our professors, who\nlooked at the graph we had produced, and simply said “TLB”. Aha! Of course,\nTLB misses! Why didn’t we think of that? Having a good model of how virtual\nmemory works helps diagnose all sorts of interesting performance problems.\nStudent: I think I see. I’m trying to build these mental models of how things\nwork, so that when I’m out there working on my own, I won’t be surprised when\na system doesn’t quite behave as expected. I should even be able to anticipate how\nthe system will work just by thinking about it.\nProfessor: Exactly. So what have you learned? What’s in your mental model of\nhow virtual memory works?\nStudent: Well, I think I now have a pretty good idea of what happens when\nmemory is referenced by a process, which, as you’ve said many times, happens\n255\n\n\n256\nSUMMARY DIALOGUE ON MEMORY VIRTUALIZATION\non each instruction fetch as well as explicit loads and stores.\nProfessor: Sounds good – tell me more.\nStudent: Well, one thing I’ll always remember is that the addresses we see in a\nuser program, written in C for example...\nProfessor: What other language is there?\nStudent: (continuing) ... Yes, I know you like C. So do I! Anyhow, as I was\nsaying, I now really know that all addresses that we can observe within a program\nare virtual addresses; that I, as a programmer, am just given this illusion of where\ndata and code are in memory. I used to think it was cool that I could print the\naddress of a pointer, but now I ﬁnd it frustrating – it’s just a virtual address! I\ncan’t see the real physical address where the data lives.\nProfessor: Nope, the OS deﬁnitely hides that from you. What else?\nStudent: Well, I think the TLB is a really key piece, providing the system with\na small hardware cache of address translations. Page tables are usually quite\nlarge and hence live in big and slow memories. Without that TLB, programs\nwould certainly run a great deal more slowly. Seems like the TLB truly makes\nvirtualizing memory possible. I couldn’t imagine building a system without one!\nAnd I shudder at the thought of a program with a working set that exceeds the\ncoverage of the TLB: with all those TLB misses, it would be hard to watch.\nProfessor: Yes, cover the eyes of the children! Beyond the TLB, what did you\nlearn?\nStudent: I also now understand that the page table is one of those data structures\nyou need to know about; it’s just a data structure, though, and that means almost\nany structure could be used. We started with simple structures, like arrays (a.k.a.\nlinear page tables), and advanced all the way up to multi-level tables (which look\nlike trees), and even crazier things like pageable page tables in kernel virtual\nmemory. All to save a little space in memory!\nProfessor: Indeed.\nStudent: And here’s one more important thing: I learned that the address trans-\nlation structures need to be ﬂexible enough to support what programmers want\nto do with their address spaces. Structures like the multi-level table are perfect\nin this sense; they only create table space when the user needs a portion of the\naddress space, and thus there is little waste. Earlier attempts, like the simple base\nand bounds register, just weren’t ﬂexible enough; the structures need to match\nwhat users expect and want out of their virtual memory system.\nProfessor: That’s a nice perspective. What about all of the stuff we learned\nabout swapping to disk?\nStudent: Well, it’s certainly fun to study, and good to know how page replace-\nment works. Some of the basic policies are kind of obvious (like LRU, for ex-\nample), but building a real virtual memory system seems more interesting, like\nwe saw in the VMS case study. But somehow, I found the mechanisms more\ninteresting, and the policies less so.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nSUMMARY DIALOGUE ON MEMORY VIRTUALIZATION\n257\nProfessor: Oh, why is that?\nStudent: Well, as you said, in the end the best solution to policy problems is\nsimple: buy more memory. But the mechanisms you need to understand to know\nhow stuff really works. Speaking of which...\nProfessor: Yes?\nStudent: Well, my machine is running a little slowly these days... and memory\ncertainly doesn’t cost that much...\nProfessor: Oh ﬁne, ﬁne! Here’s a few bucks. Go and get yourself some DRAM,\ncheapskate.\nStudent: Thanks professor! I’ll never swap to disk again – or, if I do, at least I’ll\nknow what’s actually going on!\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "page_number": 285,
      "chapter_number": 29,
      "summary": "This chapter covers segment 29 (pages 285-293). Key topics include pages, paged, and paging. The developers were also concerned about memory hogs, programs\nthat use a lot of memory and make it hard for other programs to run.",
      "keywords": [
        "VIRTUAL MEMORY SYSTEM",
        "VMS VIRTUAL MEMORY",
        "VIRTUAL MEMORY",
        "MEMORY",
        "MEMORY SYSTEM",
        "VMS",
        "pages",
        "address space",
        "VMS VIRTUAL",
        "page table",
        "SYSTEM",
        "address",
        "FIFO Page Replacement",
        "VIRTUAL",
        "FIFO"
      ],
      "concepts": [
        "pages",
        "paged",
        "paging",
        "professor",
        "memory",
        "memories",
        "student",
        "work",
        "systems",
        "virtual"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 1,
          "title": "Segment 1 (pages 1-20)",
          "relevance_score": 0.75,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 41,
          "title": "Segment 41 (pages 815-837)",
          "relevance_score": 0.75,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 42,
          "title": "Segment 42 (pages 838-857)",
          "relevance_score": 0.75,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 33,
          "title": "Segment 33 (pages 662-682)",
          "relevance_score": 0.7,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 43,
          "title": "Segment 43 (pages 858-879)",
          "relevance_score": 0.7,
          "method": "api"
        }
      ]
    },
    {
      "number": 30,
      "title": "Segment 30 (pages 294-301)",
      "start_page": 294,
      "end_page": 301,
      "detection_method": "topic_boundary",
      "content": "Part II\nConcurrency\n259\n\n\n25\nA Dialogue on Concurrency\nProfessor: And thus we reach the second of our three pillars of operating sys-\ntems: concurrency.\nStudent: I thought there were four pillars...?\nProfessor: Nope, that was in an older version of the book.\nStudent: Umm... OK. So what is concurrency, oh wonderful professor?\nProfessor: Well, imagine we have a peach –\nStudent: (interrupting) Peaches again! What is it with you and peaches?\nProfessor: Ever read T.S. Eliot? The Love Song of J. Alfred Prufrock, “Do I dare\nto eat a peach”, and all that fun stuff?\nStudent: Oh yes! In English class in high school. Great stuff! I really liked the\npart where –\nProfessor: (interrupting) This has nothing to do with that – I just like peaches.\nAnyhow, imagine there are a lot of peaches on a table, and a lot of people who\nwish to eat them. Let’s say we did it this way: each eater ﬁrst identiﬁes a peach\nvisually, and then tries to grab it and eat it. What is wrong with this approach?\nStudent: Hmmm... seems like you might see a peach that somebody else also\nsees. If they get there ﬁrst, when you reach out, no peach for you!\nProfessor: Exactly! So what should we do about it?\nStudent: Well, probably develop a better way of going about this. Maybe form a\nline, and when you get to the front, grab a peach and get on with it.\nProfessor: Good! But what’s wrong with your approach?\nStudent: Sheesh, do I have to do all the work?\nProfessor: Yes.\nStudent: OK, let me think. Well, we used to have many people grabbing for\npeaches all at once, which is faster. But in my way, we just go one at a time,\nwhich is correct, but quite a bit slower. The best kind of approach would be fast\nand correct, probably.\n261\n\n\n262\nA DIALOGUE ON CONCURRENCY\nProfessor: You are really starting to impress. In fact, you just told us everything\nwe need to know about concurrency! Well done.\nStudent: I did? I thought we were just talking about peaches. Remember, this\nis usually a part where you make it about computers again.\nProfessor: Indeed. My apologies! One must never forget the concrete. Well,\nas it turns out, there are certain types of programs that we call multi-threaded\napplications; each thread is kind of like an independent agent running around\nin this program, doing things on the program’s behalf. But these threads access\nmemory, and for them, each spot of memory is kind of like one of those peaches. If\nwe don’t coordinate access to memory between threads, the program won’t work\nas expected. Make sense?\nStudent: Kind of. But why do we talk about this in an OS class? Isn’t that just\napplication programming?\nProfessor: Good question! A few reasons, actually. First, the OS must support\nmulti-threaded applications with primitives such as locks and condition vari-\nables, which we’ll talk about soon. Second, the OS itself was the ﬁrst concurrent\nprogram – it must access its own memory very carefully or many strange and\nterrible things will happen. Really, it can get quite grisly.\nStudent: I see. Sounds interesting. There are more details, I imagine?\nProfessor: Indeed there are...\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\n26\nConcurrency: An Introduction\nThus far, we have seen the development of the basic abstractions that the\nOS performs. We have seen how to take a single physical CPU and turn\nit into multiple virtual CPUs, thus enabling the illusion of multiple pro-\ngrams running at the same time. We have also seen how to create the\nillusion of a large, private virtual memory for each process; this abstrac-\ntion of the address space enables each program to behave as if it has its\nown memory when indeed the OS is secretly multiplexing address spaces\nacross physical memory (and sometimes, disk).\nIn this note, we introduce a new abstraction for a single running pro-\ncess: that of a thread. Instead of our classic view of a single point of\nexecution within a program (i.e., a single PC where instructions are be-\ning fetched from and executed), a multi-threaded program has more than\none point of execution (i.e., multiple PCs, each of which is being fetched\nand executed from). Perhaps another way to think of this is that each\nthread is very much like a separate process, except for one difference:\nthey share the same address space and thus can access the same data.\nThe state of a single thread is thus very similar to that of a process.\nIt has a program counter (PC) that tracks where the program is fetch-\ning instructions from. Each thread has its own private set of registers it\nuses for computation; thus, if there are two threads that are running on\na single processor, when switching from running one (T1) to running the\nother (T2), a context switch must take place. The context switch between\nthreads is quite similar to the context switch between processes, as the\nregister state of T1 must be saved and the register state of T2 restored\nbefore running T2. With processes, we saved state to a process control\nblock (PCB); now, we’ll need one or more thread control blocks (TCBs)\nto store the state of each thread of a process. There is one major difference,\nthough, in the context switch we perform between threads as compared\nto processes: the address space remains the same (i.e., there is no need to\nswitch which page table we are using).\nOne other major difference between threads and processes concerns\nthe stack. In our simple model of the address space of a classic process\n(which we can now call a single-threaded process), there is a single stack,\nusually residing at the bottom of the address space (Figure 26.1, left).\n263\n\n\n264\nCONCURRENCY: AN INTRODUCTION\n16KB\n15KB\n2KB\n1KB\n0KB\nStack\n(free)\nHeap\nProgram Code\nthe code segment:\nwhere instructions live\nthe heap segment:\ncontains malloc’d data\ndynamic data structures\n(it grows downward)\n(it grows upward)\nthe stack segment:\ncontains local variables\narguments to routines, \nreturn values, etc.\n16KB\n15KB\n2KB\n1KB\n0KB\nStack (1)\nStack (2)\n(free)\n(free)\nHeap\nProgram Code\nFigure 26.1: A Single-Threaded Address Space\nHowever, in a multi-threaded process, each thread runs independently\nand of course may call into various routines to do whatever work it is do-\ning. Instead of a single stack in the address space, there will be one per\nthread. Let’s say we have a multi-threaded process that has two threads\nin it; the resulting address space looks different (Figure 26.1, right).\nIn this ﬁgure, you can see two stacks spread throughout the address\nspace of the process. Thus, any stack-allocated variables, parameters, re-\nturn values, and other things that we put on the stack will be placed in\nwhat is sometimes called thread-local storage, i.e., the stack of the rele-\nvant thread.\nYou might also notice how this ruins our beautiful address space lay-\nout. Before, the stack and heap could grow independently and trouble\nonly arose when you ran out of room in the address space. Here, we\nno longer have such a nice situation. Fortunately, this is usually OK, as\nstacks do not generally have to be very large (the exception being in pro-\ngrams that make heavy use of recursion).\n26.1\nAn Example: Thread Creation\nLet’s say we wanted to run a program that created two threads, each\nof which was doing some independent work, in this case printing “A” or\n“B”. The code is shown in Figure 26.2.\nThe main program creates two threads, each of which will run the\nfunction mythread(), though with different arguments (the string A or\nB). Once a thread is created, it may start running right away (depending\non the whims of the scheduler); alternately, it may be put in a “ready” but\nnot “running” state and thus not run yet. After creating the two threads\n(T1 and T2), the main thread calls pthread join(), which waits for a\nparticular thread to complete.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nCONCURRENCY: AN INTRODUCTION\n265\n1\n#include <stdio.h>\n2\n#include <assert.h>\n3\n#include <pthread.h>\n4\n5\nvoid *mythread(void *arg) {\n6\nprintf(\"%s\\n\", (char *) arg);\n7\nreturn NULL;\n8\n}\n9\n10\nint\n11\nmain(int argc, char *argv[]) {\n12\npthread_t p1, p2;\n13\nbr int rc;\n14\nprintf(\"main: begin\\n\");\n15\nrc = pthread_create(&p1, NULL, mythread, \"A\"); assert(rc == 0);\n16\nrc = pthread_create(&p2, NULL, mythread, \"B\"); assert(rc == 0);\n17\n// join waits for the threads to finish\n18\nrc = pthread_join(p1, NULL); assert(rc == 0);\n19\nrc = pthread_join(p2, NULL); assert(rc == 0);\n20\nprintf(\"main: end\\n\");\n21\nreturn 0;\n22\n}\nFigure 26.2: Simple Thread Creation Code (t0.c)\nLet us examine the possible execution ordering of this little program.\nIn the execution diagram (Table 26.1), time increases in the downwards\ndirection, and each column shows when a different thread (the main one,\nor Thread 1, or Thread 2) is running.\nNote, however, that this ordering is not the only possible ordering. In\nfact, given a sequence of instructions, there are quite a few, depending on\nwhich thread the scheduler decides to run at a given point. For example,\nonce a thread is created, it may run immediately, which would lead to the\nexecution shown in Table 26.2.\nWe also could even see “B” printed before “A”, if, say, the scheduler\ndecided to run Thread 2 ﬁrst even though Thread 1 was created earlier;\nthere is no reason to assume that a thread that is created ﬁrst will run ﬁrst.\nTable 26.3 shows this ﬁnal execution ordering, with Thread 2 getting to\nstrut its stuff before Thread 1.\nAs you might be able to see, one way to think about thread creation\nis that it is a bit like making a function call; however, instead of ﬁrst ex-\necuting the function and then returning to the caller, the system instead\ncreates a new thread of execution for the routine that is being called, and\nit runs independently of the caller, perhaps before returning from the cre-\nate, but perhaps much later.\nAs you also might be able to tell from this example, threads make life\ncomplicated: it is already hard to tell what will run when! Computers are\nhard enough to understand without concurrency. Unfortunately, with\nconcurrency, it gets worse. Much worse.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "page_number": 294,
      "chapter_number": 30,
      "summary": "This chapter covers segment 30 (pages 294-301). Key topics include thread, student, and running.",
      "keywords": [
        "thread",
        "address space",
        "Professor",
        "Concurrency",
        "program",
        "Student",
        "address",
        "space",
        "stack",
        "process",
        "running",
        "Thread Creation",
        "single",
        "run",
        "Peaches"
      ],
      "concepts": [
        "thread",
        "student",
        "running",
        "run",
        "professor",
        "programs",
        "programming",
        "concurrency",
        "concurrent",
        "spaces"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 37,
          "title": "Segment 37 (pages 363-372)",
          "relevance_score": 0.73,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 51,
          "title": "Segment 51 (pages 1023-1040)",
          "relevance_score": 0.73,
          "method": "api"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 1,
          "title": "Segment 1 (pages 1-8)",
          "relevance_score": 0.71,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 1,
          "title": "Segment 1 (pages 1-20)",
          "relevance_score": 0.71,
          "method": "api"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 29,
          "title": "Segment 29 (pages 298-305)",
          "relevance_score": 0.7,
          "method": "api"
        }
      ]
    },
    {
      "number": 31,
      "title": "Segment 31 (pages 302-309)",
      "start_page": 302,
      "end_page": 309,
      "detection_method": "topic_boundary",
      "content": "266\nCONCURRENCY: AN INTRODUCTION\nmain\nThread 1\nThread2\nstarts running\nprints “main: begin”\ncreates Thread 1\ncreates Thread 2\nwaits for T1\nruns\nprints “A”\nreturns\nwaits for T2\nruns\nprints “B”\nreturns\nprints “main: end”\nTable 26.1: Thread Trace (1)\nmain\nThread 1\nThread2\nstarts running\nprints “main: begin”\ncreates Thread 1\nruns\nprints “A”\nreturns\ncreates Thread 2\nruns\nprints “B”\nreturns\nwaits for T1\nreturns immediately; T1 is done\nwaits for T2\nreturns immediately; T2 is done\nprints “main: end”\nTable 26.2: Thread Trace (2)\nmain\nThread 1\nThread2\nstarts running\nprints “main: begin”\ncreates Thread 1\ncreates Thread 2\nruns\nprints “B”\nreturns\nwaits for T1\nruns\nprints “A”\nreturns\nwaits for T2\nreturns immediately; T2 is done\nprints “main: end”\nTable 26.3: Thread Trace (3)\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nCONCURRENCY: AN INTRODUCTION\n267\n1\n#include <stdio.h>\n2\n#include <pthread.h>\n3\n#include \"mythreads.h\"\n4\n5\nstatic volatile int counter = 0;\n6\n7\n//\n8\n// mythread()\n9\n//\n10\n// Simply adds 1 to counter repeatedly, in a loop\n11\n// No, this is not how you would add 10,000,000 to\n12\n// a counter, but it shows the problem nicely.\n13\n//\n14\nvoid *\n15\nmythread(void *arg)\n16\n{\n17\nprintf(\"%s: begin\\n\", (char *) arg);\n18\nint i;\n19\nfor (i = 0; i < 1e7; i++) {\n20\ncounter = counter + 1;\n21\n}\n22\nprintf(\"%s: done\\n\", (char *) arg);\n23\nreturn NULL;\n24\n}\n25\n26\n//\n27\n// main()\n28\n//\n29\n// Just launches two threads (pthread_create)\n30\n// and then waits for them (pthread_join)\n31\n//\n32\nint\n33\nmain(int argc, char *argv[])\n34\n{\n35\npthread_t p1, p2;\n36\nprintf(\"main: begin (counter = %d)\\n\", counter);\n37\nPthread_create(&p1, NULL, mythread, \"A\");\n38\nPthread_create(&p2, NULL, mythread, \"B\");\n39\n40\n// join waits for the threads to finish\n41\nPthread_join(p1, NULL);\n42\nPthread_join(p2, NULL);\n43\nprintf(\"main: done with both (counter = %d)\\n\", counter);\n44\nreturn 0;\n45\n}\nFigure 26.3: Sharing Data: Oh Oh (t2)\n26.2\nWhy It Gets Worse: Shared Data\nThe simple thread example we showed above was useful in showing\nhow threads are created and how they can run in different orders depend-\ning on how the scheduler decides to run them. What it doesn’t show you,\nthough, is how threads interact when they access shared data.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n268\nCONCURRENCY: AN INTRODUCTION\nLet us imagine a simple example where two threads wish to update a\nglobal shared variable. The code we’ll study is in Figure 26.3.\nHere are a few notes about the code. First, as Stevens suggests [SR05],\nwe wrap the thread creation and join routines to simply exit on failure;\nfor a program as simple as this one, we want to at least notice an error\noccurred (if it did), but not do anything very smart about it (e.g., just\nexit). Thus, Pthread create() simply calls pthread create() and\nmakes sure the return code is 0; if it isn’t, Pthread create() just prints\na message and exits.\nSecond, instead of using two separate function bodies for the worker\nthreads, we just use a single piece of code, and pass the thread an argu-\nment (in this case, a string) so we can have each thread print a different\nletter before its messages.\nFinally, and most importantly, we can now look at what each worker is\ntrying to do: add a number to the shared variable counter, and do so 10\nmillion times (1e7) in a loop. Thus, the desired ﬁnal result is: 20,000,000.\nWe now compile and run the program, to see how it behaves. Some-\ntimes, everything works how we might expect:\nprompt> gcc -o main main.c -Wall -pthread\nprompt> ./main\nmain: begin (counter = 0)\nA: begin\nB: begin\nA: done\nB: done\nmain: done with both (counter = 20000000)\nUnfortunately, when we run this code, even on a single processor, we\ndon’t necessarily get the desired result. Sometimes, we get:\nprompt> ./main\nmain: begin (counter = 0)\nA: begin\nB: begin\nA: done\nB: done\nmain: done with both (counter = 19345221)\nLet’s try it one more time, just to see if we’ve gone crazy. After all,\naren’t computers supposed to produce deterministic results, as you have\nbeen taught?! Perhaps your professors have been lying to you? (gasp)\nprompt> ./main\nmain: begin (counter = 0)\nA: begin\nB: begin\nA: done\nB: done\nmain: done with both (counter = 19221041)\nNot only is each run wrong, but also yields a different result! A big\nquestion remains: why does this happen?\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nCONCURRENCY: AN INTRODUCTION\n269\nTIP: KNOW AND USE YOUR TOOLS\nYou should always learn new tools that help you write, debug, and un-\nderstand computer systems. Here, we use a neat tool called a disassem-\nbler. When you run a disassembler on an executable, it shows you what\nassembly instructions make up the program. For example, if we wish to\nunderstand the low-level code to update a counter (as in our example),\nwe run objdump (Linux) to see the assembly code:\nprompt> objdump -d main\nDoing so produces a long listing of all the instructions in the program,\nneatly labeled (particularly if you compiled with the -g ﬂag), which in-\ncludes symbol information in the program. The objdump program is just\none of many tools you should learn how to use; a debugger like gdb,\nmemory proﬁlers like valgrind or purify, and of course the compiler\nitself are others that you should spend time to learn more about; the better\nyou are at using your tools, the better systems you’ll be able to build.\n26.3\nThe Heart of the Problem: Uncontrolled Scheduling\nTo understand why this happens, we must understand the code se-\nquence that the compiler generates for the update to counter. In this\ncase, we wish to simply add a number (1) to counter. Thus, the code\nsequence for doing so might look something like this (in x86);\nmov 0x8049a1c, %eax\nadd $0x1, %eax\nmov %eax, 0x8049a1c\nThis example assumes that the variable counter is located at address\n0x8049a1c. In this three-instruction sequence, the x86 mov instruction is\nused ﬁrst to get the memory value at the address and put it into register\neax. Then, the add is performed, adding 1 (0x1) to the contents of the\neax register, and ﬁnally, the contents of eax are stored back into memory\nat the same address.\nLet us imagine one of our two threads (Thread 1) enters this region of\ncode, and is thus about to increment counter by one. It loads the value\nof counter (let’s say it’s 50 to begin with) into its register eax. Thus,\neax=50 for Thread 1. Then it adds one to the register; thus eax=51.\nNow, something unfortunate happens: a timer interrupt goes off; thus,\nthe OS saves the state of the currently running thread (its PC, its registers\nincluding eax, etc.) to the thread’s TCB.\nNow something worse happens: Thread 2 is chosen to run, and it en-\nters this same piece of code. It also executes the ﬁrst instruction, getting\nthe value of counter and putting it into its eax (remember: each thread\nwhen running has its own private registers; the registers are virtualized\nby the context-switch code that saves and restores them). The value of\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n270\nCONCURRENCY: AN INTRODUCTION\n(after instruction)\nOS\nThread 1\nThread 2\nPC\n%eax counter\nbefore critical section\n100\n0\n50\nmov 0x8049a1c, %eax\n105\n50\n50\nadd $0x1, %eax\n108\n51\n50\ninterrupt\nsave T1’s state\nrestore T2’s state\n100\n0\n50\nmov 0x8049a1c, %eax\n105\n50\n50\nadd $0x1, %eax\n108\n51\n50\nmov %eax, 0x8049a1c\n113\n51\n51\ninterrupt\nsave T2’s state\nrestore T1’s state\n108\n51\n50\nmov %eax, 0x8049a1c\n113\n51\n51\nTable 26.4: The Problem: Up Close and Personal\ncounter is still 50 at this point, and thus Thread 2 has eax=50. Let’s\nthen assume that Thread 2 executes the next two instructions, increment-\ning eax by 1 (thus eax=51), and then saving the contents of eax into\ncounter (address 0x8049a1c). Thus, the global variable counter now\nhas the value 51.\nFinally, another context switch occurs, and Thread 1 resumes running.\nRecall that it had just executed the mov and add, and is now about to\nperform the ﬁnal mov instruction. Recall also that eax=51. Thus, the ﬁnal\nmov instruction executes, and saves the value to memory; the counter is\nset to 51 again.\nPut simply, what has happened is this: the code to increment counter\nhas been run twice, but counter, which started at 50, is now only equal\nto 51. A “correct” version of this program should have resulted in counter\nequal to 52.\nHere is a pictorial depiction of what happened and when in the ex-\nample above. Assume, for this depiction, that the above code is loaded at\naddress 100 in memory, like the following sequence (note for those of you\nused to nice, RISC-like instruction sets: x86 has variable-length instruc-\ntions; the mov instructions here take up 5 bytes of memory, whereas the\nadd takes only 3):\n100 mov\n0x8049a1c, %eax\n105 add\n$0x1, %eax\n108 mov\n%eax, 0x8049a1c\nWith these assumptions, what happens is seen in Table 26.4. Assume\nthe counter starts at value 50, and trace through this example to make\nsure you understand what is going on.\nWhat we have demonstrated here is called a race condition: the results\ndepend on the timing execution of the code. With some bad luck (i.e.,\ncontext switches that occur at untimely points in the execution), we get\nthe wrong result. In fact, we may get a different result each time; thus,\ninstead of a nice deterministic computation (which we are used to from\ncomputers), we call this result indeterminate, where it is not known what\nthe output will be and it is indeed likely to be different across runs.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nCONCURRENCY: AN INTRODUCTION\n271\nBecause multiple threads executing this code can result in a race con-\ndition, we call this code a critical section. A critical section is a piece of\ncode that accesses a shared variable (or more generally, a shared resource)\nand must not be concurrently executed by more than one thread.\nWhat we really want for this code is what we call mutual exclusion.\nThis property guarantees that if one thread is executing within the critical\nsection, the others will be prevented from doing so.\nVirtually all of these terms, by the way, were coined by Edsger Dijk-\nstra, who was a pioneer in the ﬁeld and indeed won the Turing Award\nbecause of this and other work; see his 1968 paper on “Cooperating Se-\nquential Processes” [D68] for an amazingly clear description of the prob-\nlem. We’ll be hearing more about Dijkstra in this section of the book.\n26.4\nThe Wish For Atomicity\nOne way to solve this problem would be to have more powerful in-\nstructions that, in a single step, did exactly whatever we needed done\nand thus removed the possibility of an untimely interrupt. For example,\nwhat if we had a super instruction that looked like this?\nmemory-add 0x8049a1c, $0x1\nAssume this instruction adds a value to a memory location, and the\nhardware guarantees that it executes atomically; when the instruction\nexecuted, it would perform the update as desired. It could not be inter-\nrupted mid-instruction, because that is precisely the guarantee we receive\nfrom the hardware: when an interrupt occurs, either the instruction has\nnot run at all, or it has run to completion; there is no in-between state.\nHardware can be a beautiful thing, no?\nAtomically, in this context, means “as a unit”, which sometimes we\ntake as “all or none.” What we’d like is to execute the three instruction\nsequence atomically:\nmov 0x8049a1c, %eax\nadd $0x1, %eax\nmov %eax, 0x8049a1c\nAs we said, if we had a single instruction to do this, we could just\nissue that instruction and be done. But in the general case, we won’t have\nsuch an instruction. Imagine we were building a concurrent B-tree, and\nwished to update it; would we really want the hardware to support an\n“atomic update of B-tree” instruction? Probably not, at least in a sane\ninstruction set.\nThus, what we will instead do is ask the hardware for a few useful\ninstructions upon which we can build a general set of what we call syn-\nchronization primitives. By using these hardware synchronization prim-\nitives, in combination with some help from the operating system, we will\nbe able to build multi-threaded code that accesses critical sections in a\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n272\nCONCURRENCY: AN INTRODUCTION\nASIDE: KEY CONCURRENCY TERMS\nCRITICAL SECTION, RACE CONDITION,\nINDETERMINATE, MUTUAL EXCLUSION\nThese four terms are so central to concurrent code that we thought it\nworth while to call them out explicitly. See some of Dijkstra’s early work\n[D65,D68] for more details.\n• A critical section is a piece of code that accesses a shared resource,\nusually a variable or data structure.\n• A race condition arises if multiple threads of execution enter the\ncritical section at roughly the same time; both attempt to update\nthe shared data structure, leading to a surprising (and perhaps un-\ndesirable) outcome.\n• An indeterminate program consists of one or more race conditions;\nthe output of the program varies from run to run, depending on\nwhich threads ran when. The outcome is thus not deterministic,\nsomething we usually expect from computer systems.\n• To avoid these problems, threads should use some kind of mutual\nexclusion primitives; doing so guarantees that only a single thread\never enters a critical section, thus avoiding races, and resulting in\ndeterministic program outputs.\nsynchronized and controlled manner, and thus reliably produces the cor-\nrect result despite the challenging nature of concurrent execution. Pretty\nawesome, right?\nThis is the problem we will study in this section of the book. It is a\nwonderful and hard problem, and should make your mind hurt (a bit).\nIf it doesn’t, then you don’t understand! Keep working until your head\nhurts; you then know you’re headed in the right direction. At that point,\ntake a break; we don’t want your head hurting too much.\nTHE CRUX:\nHOW TO PROVIDE SUPPORT FOR SYNCHRONIZATION\nWhat support do we need from the hardware in order to build use-\nful synchronization primitives? What support do we need from the OS?\nHow can we build these primitives correctly and efﬁciently? How can\nprograms use them to get the desired results?\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nCONCURRENCY: AN INTRODUCTION\n273\n26.5\nOne More Problem: Waiting For Another\nThis chapter has set up the problem of concurrency as if only one type\nof interaction occurs between threads, that of accessing shared variables\nand the need to support atomicity for critical sections. As it turns out,\nthere is another common interaction that arises, where one thread must\nwait for another to complete some action before it continues. This inter-\naction arises, for example, when a process performs a disk I/O and is put\nto sleep; when the I/O completes, the process needs to be roused from its\nslumber so it can continue.\nThus, in the coming chapters, we’ll be not only studying how to build\nsupport for synchronization primitives to support atomicity but also for\nmechanisms to support this type of sleeping/waking interaction that is\ncommon in multi-threaded programs. If this doesn’t make sense right\nnow, that is OK! It will soon enough, when you read the chapter on con-\ndition variables. If it doesn’t by then, well, then it is less OK, and you\nshould read that chapter again (and again) until it does make sense.\n26.6\nSummary: Why in OS Class?\nBefore wrapping up, one question that you might have is: why are we\nstudying this in OS class? “History” is the one-word answer; the OS was\nthe ﬁrst concurrent program, and many techniques were created for use\nwithin the OS. Later, with multi-threaded processes, application program-\nmers also had to consider such things.\nFor example, imagine the case where there are two processes running.\nAssume they both call write() to write to the ﬁle, and both wish to\nappend the data to the ﬁle (i.e., add the data to the end of the ﬁle, thus in-\ncreasing its length). To do so, both must allocate a new block, record in the\ninode of the ﬁle where this block lives, and change the size of the ﬁle to re-\nﬂect the new larger size (among other things; we’ll learn more about ﬁles\nin the third part of the book). Because an interrupt may occur at any time,\nthe code that updates to these shared structures (e.g., a bitmap for alloca-\ntion, or the ﬁle’s inode) are critical sections; thus, OS designers, from the\nvery beginning of the introduction of the interrupt, had to worry about\nhow the OS updates internal structures. An untimely interrupt causes all\nof the problems described above. Not surprisingly, page tables, process\nlists, ﬁle system structures, and virtually every kernel data structure has\nto be carefully accessed, with the proper synchronization primitives, to\nwork correctly.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "page_number": 302,
      "chapter_number": 31,
      "summary": "This chapter covers segment 31 (pages 302-309). Key topics include thread, instructions, and instruction. What it doesn’t show you,\nthough, is how threads interact when they access shared data.",
      "keywords": [
        "Thread",
        "counter",
        "main",
        "eax",
        "creates Thread",
        "code",
        "Thread Trace",
        "prints",
        "runs prints",
        "instruction",
        "mov",
        "CONCURRENCY",
        "begin",
        "run",
        "critical section"
      ],
      "concepts": [
        "thread",
        "instructions",
        "instruction",
        "counter",
        "program",
        "running",
        "runs",
        "run",
        "result",
        "main"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 37,
          "title": "Segment 37 (pages 363-372)",
          "relevance_score": 0.74,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 51,
          "title": "Segment 51 (pages 1023-1040)",
          "relevance_score": 0.71,
          "method": "api"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 29,
          "title": "Segment 29 (pages 298-305)",
          "relevance_score": 0.69,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 12,
          "title": "Segment 12 (pages 219-238)",
          "relevance_score": 0.69,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 26,
          "title": "Segment 26 (pages 249-257)",
          "relevance_score": 0.66,
          "method": "api"
        }
      ]
    },
    {
      "number": 32,
      "title": "Segment 32 (pages 310-319)",
      "start_page": 310,
      "end_page": 319,
      "detection_method": "topic_boundary",
      "content": "274\nCONCURRENCY: AN INTRODUCTION\nTIP: USE ATOMIC OPERATIONS\nAtomic operations are one of the most powerful underlying techniques\nin building computer systems, from the computer architecture, to concur-\nrent code (what we are studying here), to ﬁle systems (which we’ll study\nsoon enough), database management systems, and even distributed sys-\ntems [L+93].\nThe idea behind making a series of actions atomic is simply expressed\nwith the phrase “all or nothing”; it should either appear as if all of the ac-\ntions you wish to group together occurred, or that none of them occurred,\nwith no in-between state visible. Sometimes, the grouping of many ac-\ntions into a single atomic action is called a transaction, an idea devel-\noped in great detail in the world of databases and transaction processing\n[GR92].\nIn our theme of exploring concurrency, we’ll be using synchronization\nprimitives to turn short sequences of instructions into atomic blocks of\nexecution, but the idea of atomicity is much bigger than that, as we will\nsee. For example, ﬁle systems use techniques such as journaling or copy-\non-write in order to atomically transition their on-disk state, critical for\noperating correctly in the face of system failures. If that doesn’t make\nsense, don’t worry – it will, in some future chapter.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nCONCURRENCY: AN INTRODUCTION\n275\nReferences\n[D65] “Solution of a problem in concurrent programming control”\nE. W. Dijkstra\nCommunications of the ACM, 8(9):569, September 1965\nPointed to as the ﬁrst paper of Dijkstra’s where he outlines the mutual exclusion problem and a solution.\nThe solution, however, is not widely used; advanced hardware and OS support is needed, as we will see\nin the coming chapters.\n[D68] “Cooperating sequential processes”\nEdsger W. Dijkstra, 1968\nAvailable: http://www.cs.utexas.edu/users/EWD/ewd01xx/EWD123.PDF\nDijkstra has an amazing number of his old papers, notes, and thoughts recorded (for posterity) on this\nwebsite at the last place he worked, the University of Texas. Much of his foundational work, however,\nwas done years earlier while he was at the Technische Hochshule of Eindhoven (THE), including this\nfamous paper on “cooperating sequential processes”, which basically outlines all of the thinking that\nhas to go into writing multi-threaded programs. Dijkstra discovered much of this while working on an\noperating system named after his school: the “THE” operating system (said “T”, “H”, “E”, and not\nlike the word “the”).\n[GR92] “Transaction Processing: Concepts and Techniques”\nJim Gray and Andreas Reuter\nMorgan Kaufmann, September 1992\nThis book is the bible of transaction processing, written by one of the legends of the ﬁeld, Jim Gray. It is,\nfor this reason, also considered Jim Gray’s “brain dump”, in which he wrote down everything he knows\nabout how database management systems work. Sadly, Gray passed away tragically a few years back,\nand many of us lost a friend and great mentor, including the co-authors of said book, who were lucky\nenough to interact with Gray during their graduate school years.\n[L+93] “Atomic Transactions”\nNancy Lynch, Michael Merritt, William Weihl, Alan Fekete\nMorgan Kaufmann, August 1993\nA nice text on some of the theory and practice of atomic transactions for distributed systems. Perhaps a\nbit formal for some, but lots of good material is found herein.\n[SR05] “Advanced Programming in the UNIX Environment”\nW. Richard Stevens and Stephen A. Rago\nAddison-Wesley, 2005\nAs we’ve said many times, buy this book, and read it, in little chunks, preferably before going to bed.\nThis way, you will actually fall asleep more quickly; more importantly, you learn a little more about\nhow to become a serious UNIX programmer.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n276\nCONCURRENCY: AN INTRODUCTION\nHomework\nThis program, x86.py, allows you to see how different thread inter-\nleavings either cause or avoid race conditions. See the README for de-\ntails on how the program works and its basic inputs, then answer the\nquestions below.\nQuestions\n1. To start, let’s examine a simple program, “loop.s”. First, just look at\nthe program, and see if you can understand it: cat loop.s. Then,\nrun it with these arguments:\n./x86.py -p loop.s -t 1 -i 100 -R dx\nTthis speciﬁes a single thread, an interrupt every 100 instructions,\nand tracing of register %dx. Can you ﬁgure out what the value of\n%dx will be during the run? Once you have, run the same above\nand use the -c ﬂag to check your answers; note the answers, on\nthe left, show the value of the register (or memory value) after the\ninstruction on the right has run.\n2. Now run the same code but with these ﬂags:\n./x86.py -p loop.s -t 2 -i 100 -a dx=3,dx=3 -R dx\nTthis speciﬁes two threads, and initializes each %dx register to 3.\nWhat values will %dx see? Run with the -c ﬂag to see the answers.\nDoes the presence of multiple threads affect anything about your\ncalculations? Is there a race condition in this code?\n3. Now run the following:\n./x86.py -p loop.s -t 2 -i 3 -r -a dx=3,dx=3 -R dx\nThis makes the interrupt interval quite small and random; use dif-\nferent seeds with -s to see different interleavings. Does the fre-\nquency of interruption change anything about this program?\n4. Next we’ll examine a different program (looping-race-nolock.s).\nThis program accesses a shared variable located at memory address\n2000; we’ll call this variable x for simplicity. Run it with a single\nthread and make sure you understand what it does, like this:\n./x86.py -p looping-race-nolock.s -t 1 -M 2000\nWhat value is found in x (i.e., at memory address 2000) throughout\nthe run? Use -c to check your answer.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nCONCURRENCY: AN INTRODUCTION\n277\n5. Now run with multiple iterations and threads:\n./x86.py -p looping-race-nolock.s -t 2 -a bx=3 -M 2000\nDo you understand why the code in each thread loops three times?\nWhat will the ﬁnal value of x be?\n6. Now run with random interrupt intervals:\n./x86.py -p looping-race-nolock.s -t 2 -M 2000 -i 4 -r -s 0\nThen change the random seed, setting -s 1, then -s 2, etc. Can\nyou tell, just by looking at the thread interleaving, what the ﬁnal\nvalue of x will be? Does the exact location of the interrupt matter?\nWhere can it safely occur? Where does an interrupt cause trouble?\nIn other words, where is the critical section exactly?\n7. Now use a ﬁxed interrupt interval to explore the program further.\nRun:\n./x86.py -p looping-race-nolock.s -a bx=1 -t 2 -M 2000 -i 1\nSee if you can guess what the ﬁnal value of the shared variable\nx will be. What about when you change -i 2, -i 3, etc.? For\nwhich interrupt intervals does the program give the “correct” ﬁnal\nanswer?\n8. Now run the same code for more loops (e.g., set -a bx=100). What\ninterrupt intervals, set with the -i ﬂag, lead to a “correct” outcome?\nWhich intervals lead to surprising results?\n9. We’ll examine one last program in this homework (wait-for-me.s).\nRun the code like this:\n./x86.py -p wait-for-me.s -a ax=1,ax=0 -R ax -M 2000\nThis sets the %ax register to 1 for thread 0, and 0 for thread 1, and\nwatches the value of %ax and memory location 2000 throughout\nthe run. How should the code behave? How is the value at location\n2000 being used by the threads? What will its ﬁnal value be?\n10. Now switch the inputs:\n./x86.py -p wait-for-me.s -a ax=0,ax=1 -R ax -M 2000\nHow do the threads behave? What is thread 0 doing? How would\nchanging the interrupt interval (e.g., -i 1000, or perhaps to use\nrandom intervals) change the trace outcome? Is the program efﬁ-\nciently using the CPU?\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n27\nInterlude: Thread API\nThis chapter brieﬂy covers the main portions of the thread API. Each part\nwill be explained further in the subsequent chapters, as we show how\nto use the API. More details can be found in various books and online\nsources [B97, B+96, K+96]. We should note that the subsequent chapters\nintroduce the concepts of locks and condition variables more slowly, with\nmany examples; this chapter is thus better used as a reference.\nCRUX: HOW TO CREATE AND CONTROL THREADS\nWhat interfaces should the OS present for thread creation and control?\nHow should these interfaces be designed to enable ease of use as well as\nutility?\n27.1\nThread Creation\nThe ﬁrst thing you have to be able to do to write a multi-threaded\nprogram is to create new threads, and thus some kind of thread creation\ninterface must exist. In POSIX, it is easy:\n#include <pthread.h>\nint\npthread_create(\npthread_t *\nthread,\nconst pthread_attr_t *\nattr,\nvoid *\n(*start_routine)(void*),\nvoid *\narg);\nThis declaration might look a little complex (particularly if you haven’t\nused function pointers in C), but actually it’s not too bad.\nThere are\nfour arguments: thread, attr, start routine, and arg. The ﬁrst,\nthread, is a pointer to a structure of type pthread t; we’ll use this\nstructure to interact with this thread, and thus we need to pass it to\npthread create() in order to initialize it.\n279\n\n\n280\nINTERLUDE: THREAD API\nThe second argument, attr, is used to specify any attributes this thread\nmight have. Some examples include setting the stack size or perhaps in-\nformation about the scheduling priority of the thread. An attribute is\ninitialized with a separate call to pthread attr init(); see the man-\nual page for details. However, in most cases, the defaults will be ﬁne; in\nthis case, we will simply pass the value NULL in.\nThe third argument is the most complex, but is really just asking: which\nfunction should this thread start running in? In C, we call this a function\npointer, and this one tells us the following is expected: a function name\n(start routine), which is passed a single argument of type void * (as\nindicated in the parentheses after start routine), and which returns a\nvalue of type void * (i.e., a void pointer).\nIf this routine instead required an integer argument, instead of a void\npointer, the declaration would look like this:\nint pthread_create(..., // first two args are the same\nvoid *\n(*start_routine)(int),\nint\narg);\nIf instead the routine took a void pointer as an argument, but returned\nan integer, it would look like this:\nint pthread_create(..., // first two args are the same\nint\n(*start_routine)(void *),\nvoid *\narg);\nFinally, the fourth argument, arg, is exactly the argument to be passed\nto the function where the thread begins execution. You might ask: why\ndo we need these void pointers? Well, the answer is quite simple: having\na void pointer as an argument to the function start routine allows us\nto pass in any type of argument; having it as a return value allows the\nthread to return any type of result.\nLet’s look at an example in Figure 27.1. Here we just create a thread\nthat is passed two arguments, packaged into a single type we deﬁne our-\nselves (myarg t). The thread, once created, can simply cast its argument\nto the type it expects and thus unpack the arguments as desired.\nAnd there it is! Once you create a thread, you really have another\nlive executing entity, complete with its own call stack, running within the\nsame address space as all the currently existing threads in the program.\nThe fun thus begins!\n27.2\nThread Completion\nThe example above shows how to create a thread. However, what\nhappens if you want to wait for a thread to complete? You need to do\nsomething special in order to wait for completion; in particular, you must\ncall the routine pthread join().\nint pthread_join(pthread_t thread, void **value_ptr);\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nINTERLUDE: THREAD API\n281\n1\n#include <pthread.h>\n2\n3\ntypedef struct __myarg_t {\n4\nint a;\n5\nint b;\n6\n} myarg_t;\n7\n8\nvoid *mythread(void *arg) {\n9\nmyarg_t *m = (myarg_t *) arg;\n10\nprintf(\"%d %d\\n\", m->a, m->b);\n11\nreturn NULL;\n12\n}\n13\n14\nint\n15\nmain(int argc, char *argv[]) {\n16\npthread_t p;\n17\nint rc;\n18\n19\nmyarg_t args;\n20\nargs.a = 10;\n21\nargs.b = 20;\n22\nrc = pthread_create(&p, NULL, mythread, &args);\n23\n...\n24\n}\nFigure 27.1: Creating a Thread\nThis routine takes only two arguments. The ﬁrst is of type pthread t,\nand is used to specify which thread to wait for. This value is exactly what\nyou passed into the thread library during creation; if you held onto it,\nyou can now use it to wait for the thread to stop running.\nThe second argument is a pointer to the return value you expect to get\nback. Because the routine can return anything, it is deﬁned to return a\npointer to void; because the pthread join() routine changes the value\nof the passed in argument, you need to pass in a pointer to that value, not\njust the value itself.\nLet’s look at another example (Figure 27.2). In the code, a single thread\nis again created, and passed a couple of arguments via the myarg t struc-\nture. To return values, the myret t type is used. Once the thread is\nﬁnished running, the main thread, which has been waiting inside of the\npthread join() routine1, then returns, and we can access the values\nreturned from the thread, namely whatever is in myret t.\nA few things to note about this example. First, often times we don’t\nhave to do all of this painful packing and unpacking of arguments. For\nexample, if we just create a thread with no arguments, we can pass NULL\nin as an argument when the thread is created. Similarly, we can pass NULL\ninto pthread join() if we don’t care about the return value.\nSecond, if we are just passing in a single value (e.g., an int), we don’t\nhave to package it up as an argument. Figure 27.3 shows an example. In\n1Note we use wrapper functions here; speciﬁcally, we call Malloc(), Pthread join(), and\nPthread create(), which just call their similarly-named lower-case versions and make sure the\nroutines did not return anything unexpected.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n282\nINTERLUDE: THREAD API\n1\n#include <stdio.h>\n2\n#include <pthread.h>\n3\n#include <assert.h>\n4\n#include <stdlib.h>\n5\n6\ntypedef struct __myarg_t {\n7\nint a;\n8\nint b;\n9\n} myarg_t;\n10\n11\ntypedef struct __myret_t {\n12\nint x;\n13\nint y;\n14\n} myret_t;\n15\n16\nvoid *mythread(void *arg) {\n17\nmyarg_t *m = (myarg_t *) arg;\n18\nprintf(\"%d %d\\n\", m->a, m->b);\n19\nmyret_t *r = Malloc(sizeof(myret_t));\n20\nr->x = 1;\n21\nr->y = 2;\n22\nreturn (void *) r;\n23\n}\n24\n25\nint\n26\nmain(int argc, char *argv[]) {\n27\nint rc;\n28\npthread_t p;\n29\nmyret_t *m;\n30\n31\nmyarg_t args;\n32\nargs.a = 10;\n33\nargs.b = 20;\n34\nPthread_create(&p, NULL, mythread, &args);\n35\nPthread_join(p, (void **) &m);\n36\nprintf(\"returned %d %d\\n\", m->x, m->y);\n37\nreturn 0;\n38\n}\nFigure 27.2: Waiting for Thread Completion\nthis case, life is a bit simpler, as we don’t have to package arguments and\nreturn values inside of structures.\nThird, we should note that one has to be extremely careful with how\nvalues are returned from a thread. In particular, never return a pointer\nwhich refers to something allocated on the thread’s call stack. If you do,\nwhat do you think will happen? (think about it!) Here is an example of a\ndangerous piece of code, modiﬁed from the example in Figure 27.2.\n1\nvoid *mythread(void *arg) {\n2\nmyarg_t *m = (myarg_t *) arg;\n3\nprintf(\"%d %d\\n\", m->a, m->b);\n4\nmyret_t r; // ALLOCATED ON STACK: BAD!\n5\nr.x = 1;\n6\nr.y = 2;\n7\nreturn (void *) &r;\n8\n}\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nINTERLUDE: THREAD API\n283\nvoid *mythread(void *arg) {\nint m = (int) arg;\nprintf(\"%d\\n\", m);\nreturn (void *) (arg + 1);\n}\nint main(int argc, char *argv[]) {\npthread_t p;\nint rc, m;\nPthread_create(&p, NULL, mythread, (void *) 100);\nPthread_join(p, (void **) &m);\nprintf(\"returned %d\\n\", m);\nreturn 0;\n}\nFigure 27.3: Simpler Argument Passing to a Thread\nIn this case, the variable r is allocated on the stack of mythread. How-\never, when it returns, the value is automatically deallocated (that’s why\nthe stack is so easy to use, after all!), and thus, passing back a pointer to\na now deallocated variable will lead to all sorts of bad results. Certainly,\nwhen you print out the values you think you returned, you’ll probably\n(but not necessarily!) be surprised. Try it and ﬁnd out for yourself2!\nFinally, you might notice that the use of pthread create() to create\na thread, followed by an immediate call to pthread join(), is a pretty\nstrange way to create a thread. In fact, there is an easier way to accom-\nplish this exact task; it’s called a procedure call. Clearly, we’ll usually be\ncreating more than just one thread and waiting for it to complete, other-\nwise there is not much purpose to using threads at all.\nWe should note that not all code that is multi-threaded uses the join\nroutine. For example, a multi-threaded web server might create a number\nof worker threads, and then use the main thread to accept requests and\npass them to the workers, indeﬁnitely. Such long-lived programs thus\nmay not need to join. However, a parallel program that creates threads\nto execute a particular task (in parallel) will likely use join to make sure\nall such work completes before exiting or moving onto the next stage of\ncomputation.\n27.3\nLocks\nBeyond thread creation and join, probably the next most useful set of\nfunctions provided by the POSIX threads library are those for providing\nmutual exclusion to a critical section via locks. The most basic pair of\nroutines to use for this purpose is provided by this pair of routines:\nint pthread_mutex_lock(pthread_mutex_t *mutex);\nint pthread_mutex_unlock(pthread_mutex_t *mutex);\n2Fortunately the compiler gcc will likely complain when you write code like this, which\nis yet another reason to pay attention to compiler warnings.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "page_number": 310,
      "chapter_number": 32,
      "summary": "In our theme of exploring concurrency, we’ll be using synchronization\nprimitives to turn short sequences of instructions into atomic blocks of\nexecution, but the idea of atomicity is much bigger than that, as we will\nsee Key topics include thread, programming, and programs.",
      "keywords": [
        "thread",
        "pthread",
        "void",
        "Thread API",
        "int",
        "CREATE",
        "int pthread",
        "argument",
        "run",
        "arg",
        "program",
        "pthread join",
        "routine",
        "systems",
        "myarg"
      ],
      "concepts": [
        "thread",
        "programming",
        "programs",
        "uses",
        "useful",
        "arguments",
        "argument",
        "atomic",
        "including",
        "include"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 3,
          "title": "Segment 3 (pages 17-25)",
          "relevance_score": 0.74,
          "method": "api"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 16,
          "title": "Segment 16 (pages 141-154)",
          "relevance_score": 0.68,
          "method": "api"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 23,
          "title": "Segment 23 (pages 227-245)",
          "relevance_score": 0.67,
          "method": "api"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 37,
          "title": "Segment 37 (pages 363-372)",
          "relevance_score": 0.65,
          "method": "api"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 18,
          "title": "Segment 18 (pages 168-175)",
          "relevance_score": 0.64,
          "method": "api"
        }
      ]
    },
    {
      "number": 33,
      "title": "Segment 33 (pages 320-329)",
      "start_page": 320,
      "end_page": 329,
      "detection_method": "topic_boundary",
      "content": "284\nINTERLUDE: THREAD API\nThe routines should be easy to understand and use. When you have a\nregion of code you realize is a critical section, and thus needs to be pro-\ntected by locks in order to operate as desired. You can probably imagine\nwhat the code looks like:\npthread_mutex_t lock;\npthread_mutex_lock(&lock);\nx = x + 1; // or whatever your critical section is\npthread_mutex_unlock(&lock);\nThe intent of the code is as follows: if no other thread holds the lock\nwhen pthread mutex lock() is called, the thread will acquire the lock\nand enter the critical section. If another thread does indeed hold the lock,\nthe thread trying to grab the lock will not return from the call until it has\nacquired the lock (implying that the thread holding the lock has released\nit via the unlock call). Of course, many threads may be stuck waiting\ninside the lock acquisition function at a given time; only the thread with\nthe lock acquired, however, should call unlock.\nUnfortunately, this code is broken, in two important ways. The ﬁrst\nproblem is a lack of proper initialization. All locks must be properly\ninitialized in order to guarantee that they have the correct values to begin\nwith and thus work as desired when lock and unlock are called.\nWith POSIX threads, there are two ways to initialize locks. One way\nto do this is to use PTHREAD MUTEX INITIALIZER, as follows:\npthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;\nDoing so sets the lock to the default values and thus makes the lock\nusable. The dynamic way to do it (i.e., at run time) is to make a call to\npthread mutex init(), as follows:\nint rc = pthread_mutex_init(&lock, NULL);\nassert(rc == 0); // always check success!\nThe ﬁrst argument to this routine is the address of the lock itself, whereas\nthe second is an optional set of attributes. Read more about the attributes\nyourself; passing NULL in simply uses the defaults. Either way works, but\nwe usually use the dynamic (latter) method. Note that a corresponding\ncall to pthread cond destroy() should also be made, when you are\ndone with the lock; see the manual page for all of details.\nThe second problem with the code above is that it fails to check errors\ncode when calling lock and unlock. Just like virtually any library rou-\ntine you call in a UNIX system, these routines can also fail! If your code\ndoesn’t properly check error codes, the failure will happen silently, which\nin this case could allow multiple threads into a critical section. Minimally,\nuse wrappers, which assert that the routine succeeded (e.g., as in Fig-\nure 27.4); more sophisticated (non-toy) programs, which can’t simply exit\nwhen something goes wrong, should check for failure and do something\nappropriate when the lock or unlock does not succeed.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nINTERLUDE: THREAD API\n285\n// Use this to keep your code clean but check for failures\n// Only use if exiting program is OK upon failure\nvoid Pthread_mutex_lock(pthread_mutex_t *mutex) {\nint rc = pthread_mutex_lock(mutex);\nassert(rc == 0);\n}\nFigure 27.4: An Example Wrapper\nThe lock and unlock routines are not the only routines that pthreads\nhas to interact with locks. In particular, here are two more routines which\nmay be of interest:\nint pthread_mutex_trylock(pthread_mutex_t *mutex);\nint pthread_mutex_timedlock(pthread_mutex_t *mutex,\nstruct timespec *abs_timeout);\nThese two calls are used in lock acquisition. The trylock version re-\nturns failure if the lock is already held; the timedlock version of acquir-\ning a lock returns after a timeout or after acquiring the lock, whichever\nhappens ﬁrst. Thus, the timedlock with a timeout of zero degenerates\nto the trylock case. Both of these versions should generally be avoided;\nhowever, there are a few cases where avoiding getting stuck (perhaps in-\ndeﬁnitely) in a lock acquisition routine can be useful, as we’ll see in future\nchapters (e.g., when we study deadlock).\n27.4\nCondition Variables\nThe other major component of any threads library, and certainly the\ncase with POSIX threads, is the presence of a condition variable. Con-\ndition variables are useful when some kind of signaling must take place\nbetween threads, if one thread is waiting for another to do something be-\nfore it can continue. Two primary routines are used by programs wishing\nto interact in this way:\nint pthread_cond_wait(pthread_cond_t *cond, pthread_mutex_t *mutex);\nint pthread_cond_signal(pthread_cond_t *cond);\nTo use a condition variable, one has to in addition have a lock that is\nassociated with this condition. When calling either of the above routines,\nthis lock should be held.\nThe ﬁrst routine, pthread cond wait(), puts the calling thread to\nsleep, and thus waits for some other thread to signal it, usually when\nsomething in the program has changed that the now-sleeping thread might\ncare about. For example, a typical usage looks like this:\npthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;\npthread_cond_t\ninit = PTHREAD_COND_INITIALIZER;\nPthread_mutex_lock(&lock);\nwhile (initialized == 0)\nPthread_cond_wait(&init, &lock);\nPthread_mutex_unlock(&lock);\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n286\nINTERLUDE: THREAD API\nIn this code, after initialization of the relevant lock and condition3,\na thread checks to see if the variable initialized has yet been set to\nsomething other than zero. If not, the thread simply calls the wait routine\nin order to sleep until some other thread wakes it.\nThe code to wake a thread, which would run in some other thread,\nlooks like this:\nPthread_mutex_lock(&lock);\ninitialized = 1;\nPthread_cond_signal(&init);\nPthread_mutex_unlock(&lock);\nA few things to note about this code sequence. First, when signal-\ning (as well as when modifying the global variable initialized), we\nalways make sure to have the lock held. This ensures that we don’t acci-\ndentally introduce a race condition into our code.\nSecond, you might notice that the wait call takes a lock as its second\nparameter, whereas the signal call only takes a condition. The reason\nfor this difference is that the wait call, in addition to putting the call-\ning thread to sleep, releases the lock when putting said caller to sleep.\nImagine if it did not: how could the other thread acquire the lock and\nsignal it to wake up? However, before returning after being woken, the\npthread cond wait() re-acquires the lock, thus ensuring that any time\nthe waiting thread is running between the lock acquire at the beginning\nof the wait sequence, and the lock release at the end, it holds the lock.\nOne last oddity: the waiting thread re-checks the condition in a while\nloop, instead of a simple if statement. We’ll discuss this issue in detail\nwhen we study condition variables in a future chapter, but in general,\nusing a while loop is the simple and safe thing to do. Although it rechecks\nthe condition (perhaps adding a little overhead), there are some pthread\nimplementations that could spuriously wake up a waiting thread; in such\na case, without rechecking, the waiting thread will continue thinking that\nthe condition has changed even though it has not. It is safer thus to view\nwaking up as a hint that something might have changed, rather than an\nabsolute fact.\nNote that sometimes it is tempting to use a simple ﬂag to signal be-\ntween two threads, instead of a condition variable and associated lock.\nFor example, we could rewrite the waiting code above to look more like\nthis in the waiting code:\nwhile (initialized == 0)\n; // spin\nThe associated signaling code would look like this:\ninitialized = 1;\n3Note\nthat\none\ncould\nuse\npthread cond init()\n(and\ncorrespond-\ning\nthe\npthread cond destroy()\ncall)\ninstead\nof\nthe\nstatic\ninitializer\nPTHREAD COND INITIALIZER. Sound like more work? It is.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nINTERLUDE: THREAD API\n287\nDon’t ever do this, for the following reasons. First, it performs poorly\nin many cases (spinning for a long time just wastes CPU cycles). Sec-\nond, it is error prone. As recent research shows [X+10], it is surprisingly\neasy to make mistakes when using ﬂags (as above) to synchronize be-\ntween threads; roughly half the uses of these ad hoc synchronizations were\nbuggy! Don’t be lazy; use condition variables even when you think you\ncan get away without doing so.\n27.5\nCompiling and Running\nAll of the code examples in this chapter are relatively easy to get up\nand running. To compile them, you must include the header pthread.h\nin your code. On the link line, you must also explicitly link with the\npthreads library, by adding the -pthread ﬂag.\nFor example, to compile a simple multi-threaded program, all you\nhave to do is the following:\nprompt> gcc -o main main.c -Wall -pthread\nAs long as main.c includes the pthreads header, you have now suc-\ncessfully compiled a concurrent program. Whether it works or not, as\nusual, is a different matter entirely.\n27.6\nSummary\nWe have introduced the basics of the pthread library, including thread\ncreation, building mutual exclusion via locks, and signaling and waiting\nvia condition variables. You don’t need much else to write robust and\nefﬁcient multi-threaded code, except patience and a great deal of care!\nWe now end the chapter with a set of tips that might be useful to you\nwhen you write multi-threaded code (see the aside on the following page\nfor details). There are other aspects of the API that are interesting; if you\nwant more information, type man -k pthread on a Linux system to\nsee over one hundred APIs that make up the entire interface. However,\nthe basics discussed herein should enable you to build sophisticated (and\nhopefully, correct and performant) multi-threaded programs. The hard\npart with threads is not the APIs, but rather the tricky logic of how you\nbuild concurrent programs. Read on to learn more.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n288\nINTERLUDE: THREAD API\nASIDE: THREAD API GUIDELINES\nThere are a number of small but important things to remember when\nyou use the POSIX thread library (or really, any thread library) to build a\nmulti-threaded program. They are:\n• Keep it simple. Above all else, any code to lock or signal between\nthreads should be as simple as possible. Tricky thread interactions\nlead to bugs.\n• Minimize thread interactions. Try to keep the number of ways\nin which threads interact to a minimum. Each interaction should\nbe carefully thought out and constructed with tried and true ap-\nproaches (many of which we will learn about in the coming chap-\nters).\n• Initialize locks and condition variables. Failure to do so will lead\nto code that sometimes works and sometimes fails in very strange\nways.\n• Check your return codes. Of course, in any C and UNIX program-\nming you do, you should be checking each and every return code,\nand it’s true here as well. Failure to do so will lead to bizarre and\nhard to understand behavior, making you likely to (a) scream, (b)\npull some of your hair out, or (c) both.\n• Be careful with how you pass arguments to, and return values\nfrom, threads. In particular, any time you are passing a reference to\na variable allocated on the stack, you are probably doing something\nwrong.\n• Each thread has its own stack. As related to the point above, please\nremember that each thread has its own stack. Thus, if you have a\nlocally-allocated variable inside of some function a thread is exe-\ncuting, it is essentially private to that thread; no other thread can\n(easily) access it. To share data between threads, the values must be\nin the heap or otherwise some locale that is globally accessible.\n• Always use condition variables to signal between threads. While\nit is often tempting to use a simple ﬂag, don’t do it.\n• Use the manual pages. On Linux, in particular, the pthread man\npages are highly informative and discuss much of the nuances pre-\nsented here, often in even more detail. Read them carefully!\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nINTERLUDE: THREAD API\n289\nReferences\n[B97] “Programming with POSIX Threads”\nDavid R. Butenhof\nAddison-Wesley, May 1997\nAnother one of these books on threads.\n[B+96] “PThreads Programming:\nA POSIX Standard for Better Multiprocessing”\nDick Buttlar, Jacqueline Farrell, Bradford Nichols\nO’Reilly, September 1996\nA reasonable book from the excellent, practical publishing house O’Reilly. Our bookshelves certainly\ncontain a great deal of books from this company, including some excellent offerings on Perl, Python, and\nJavascript (particularly Crockford’s “Javascript: The Good Parts”.)\n[K+96] “Programming With Threads”\nSteve Kleiman, Devang Shah, Bart Smaalders\nPrentice Hall, January 1996\nProbably one of the better books in this space. Get it at your local library.\n[X+10] “Ad Hoc Synchronization Considered Harmful”\nWeiwei Xiong, Soyeon Park, Jiaqi Zhang, Yuanyuan Zhou, Zhiqiang Ma\nOSDI 2010, Vancouver, Canada\nThis paper shows how seemingly simple synchronization code can lead to a surprising number of bugs.\nUse condition variables and do the signaling correctly!\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n28\nLocks\nFrom the introduction to concurrency, we saw one of the fundamental\nproblems in concurrent programming: we would like to execute a series\nof instructions atomically, but due to the presence of interrupts on a single\nprocessor (or multiple threads executing on multiple processors concur-\nrently), we couldn’t. In this chapter, we thus attack this problem directly,\nwith the introduction of something referred to as a lock. Programmers\nannotate source code with locks, putting them around critical sections,\nand thus ensure that any such critical section executes as if it were a sin-\ngle atomic instruction.\n28.1\nLocks: The Basic Idea\nAs an example, assume our critical section looks like this, the canonical\nupdate of a shared variable:\nbalance = balance + 1;\nOf course, other critical sections are possible, such as adding an ele-\nment to a linked list or other more complex updates to shared structures,\nbut we’ll just keep to this simple example for now. To use a lock, we add\nsome code around the critical section like this:\n1\nlock_t mutex; // some globally-allocated lock ’mutex’\n2\n...\n3\nlock(&mutex);\n4\nbalance = balance + 1;\n5\nunlock(&mutex);\nA lock is just a variable, and thus to use one, you must declare a lock\nvariable of some kind (such as mutex above). This lock variable (or just\n“lock” for short) holds the state of the lock at any instant in time. It is ei-\nther available (or unlocked or free) and thus no thread holds the lock, or\nacquired (or locked or held), and thus exactly one thread holds the lock\nand presumably is in a critical section. We could store other information\n291\n\n\n292\nLOCKS\nin the data type as well, such as which thread holds the lock, or a queue\nfor ordering lock acquisition, but information like that is hidden from the\nuser of the lock.\nThe semantics of the lock() and unlock() routines are simple. Call-\ning the routine lock() tries to acquire the lock; if no other thread holds\nthe lock (i.e., it is free), the thread will acquire the lock and enter the crit-\nical section; this thread is sometimes said to be the owner of the lock. If\nanother thread then calls lock() on that same lock variable (mutex in\nthis example), it will not return while the lock is held by another thread;\nin this way, other threads are prevented from entering the critical section\nwhile the ﬁrst thread that holds the lock is in there.\nOnce the owner of the lock calls unlock(), the lock is now available\n(free) again. If no other threads are waiting for the lock (i.e., no other\nthread has called lock() and is stuck therein), the state of the lock is\nsimply changed to free. If there are waiting threads (stuck in lock()),\none of them will (eventually) notice (or be informed of) this change of the\nlock’s state, acquire the lock, and enter the critical section.\nLocks provide some minimal amount of control over scheduling to\nprogrammers. In general, we view threads as entities created by the pro-\ngrammer but scheduled by the OS, in any fashion that the OS chooses.\nLocks yield some of that control back to the programmer; by putting\na lock around a section of code, the programmer can guarantee that no\nmore than a single thread can ever be active within that code. Thus locks\nhelp transform the chaos that is traditional OS scheduling into a more\ncontrolled activity.\n28.2\nPthread Locks\nThe name that the POSIX library uses for a lock is a mutex, as it is used\nto provide mutual exclusion between threads, i.e., if one thread is in the\ncritical section, it excludes the others from entering until it has completed\nthe section. Thus, when you see the following POSIX threads code, you\nshould understand that it is doing the same thing as above (we again use\nour wrappers that check for errors upon lock and unlock):\n1\npthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;\n2\n3\nPthread_mutex_lock(&lock);\n// wrapper for pthread_mutex_lock()\n4\nbalance = balance + 1;\n5\nPthread_mutex_unlock(&lock);\nYou might also notice here that the POSIX version passes a variable\nto lock and unlock, as we may be using different locks to protect different\nvariables. Doing so can increase concurrency: instead of one big lock that\nis used any time any critical section is accessed (a coarse-grained locking\nstrategy), one will often protect different data and data structures with\ndifferent locks, thus allowing more threads to be in locked code at once\n(a more ﬁne-grained approach).\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nLOCKS\n293\n28.3\nBuilding A Lock\nBy now, you should have some understanding of how a lock works,\nfrom the perspective of a programmer. But how should we build a lock?\nWhat hardware support is needed? What OS support? It is this set of\nquestions we address in the rest of this chapter.\nThe Crux: HOW TO BUILD A LOCK\nHow can we build an efﬁcient lock? Efﬁcient locks provided mutual\nexclusion at low cost, and also might attain a few other properties we\ndiscuss below. What hardware support is needed? What OS support?\nTo build a working lock, we will need some help from our old friend,\nthe hardware, as well as our good pal, the OS. Over the years, a num-\nber of different hardware primitives have been added to the instruction\nsets of various computer architectures; while we won’t study how these\ninstructions are implemented (that, after all, is the topic of a computer\narchitecture class), we will study how to use them in order to build a mu-\ntual exclusion primitive like a lock. We will also study how the OS gets\ninvolved to complete the picture and enable us to build a sophisticated\nlocking library.\n28.4\nEvaluating Locks\nBefore building any locks, we should ﬁrst understand what our goals\nare, and thus we ask how to evaluate the efﬁcacy of a particular lock\nimplementation. To evaluate whether a lock works (and works well), we\nshould ﬁrst establish some basic criteria. The ﬁrst is whether the lock does\nits basic task, which is to provide mutual exclusion. Basically, does the\nlock work, preventing multiple threads from entering a critical section?\nThe second is fairness. Does each thread contending for the lock get\na fair shot at acquiring it once it is free? Another way to look at this is\nby examining the more extreme case: does any thread contending for the\nlock starve while doing so, thus never obtaining it?\nThe ﬁnal criterion is performance, speciﬁcally the time overheads added\nby using the lock. There are a few different cases that are worth con-\nsidering here. One is the case of no contention; when a single thread\nis running and grabs and releases the lock, what is the overhead of do-\ning so? Another is the case where multiple threads are contending for\nthe lock on a single CPU; in this case, are there performance concerns? Fi-\nnally, how does the lock perform when there are multiple CPUs involved,\nand threads on each contending for the lock? By comparing these differ-\nent scenarios, we can better understand the performance impact of using\nvarious locking techniques, as described below.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "page_number": 320,
      "chapter_number": 33,
      "summary": "This chapter covers segment 33 (pages 320-329). Key topics include locks, thread, and pthreads. When you have a\nregion of code you realize is a critical section, and thus needs to be pro-\ntected by locks in order to operate as desired.",
      "keywords": [
        "lock",
        "THREAD",
        "pthread",
        "mutex",
        "THREAD API",
        "code",
        "pthread cond",
        "POSIX threads",
        "pthread mutex lock",
        "cond",
        "pthread mutex",
        "critical section",
        "Condition",
        "section",
        "variable"
      ],
      "concepts": [
        "locks",
        "thread",
        "pthreads",
        "code",
        "initialization",
        "initialized",
        "initialize",
        "programs",
        "programming",
        "variables"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 37,
          "title": "Segment 37 (pages 363-372)",
          "relevance_score": 0.77,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 51,
          "title": "Segment 51 (pages 1023-1040)",
          "relevance_score": 0.77,
          "method": "api"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 48,
          "title": "Segment 48 (pages 529-537)",
          "relevance_score": 0.72,
          "method": "api"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 49,
          "title": "Segment 49 (pages 538-555)",
          "relevance_score": 0.72,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 50,
          "title": "Segment 50 (pages 1000-1022)",
          "relevance_score": 0.72,
          "method": "api"
        }
      ]
    },
    {
      "number": 34,
      "title": "Segment 34 (pages 330-344)",
      "start_page": 330,
      "end_page": 344,
      "detection_method": "topic_boundary",
      "content": "294\nLOCKS\n28.5\nControlling Interrupts\nOne of the earliest solutions used to provide mutual exclusion was\nto disable interrupts for critical sections; this solution was invented for\nsingle-processor systems. The code would look like this:\n1\nvoid lock() {\n2\nDisableInterrupts();\n3\n}\n4\nvoid unlock() {\n5\nEnableInterrupts();\n6\n}\nAssume we are running on such a single-processor system. By turn-\ning off interrupts (using some kind of special hardware instruction) be-\nfore entering a critical section, we ensure that the code inside the critical\nsection will not be interrupted, and thus will execute as if it were atomic.\nWhen we are ﬁnished, we re-enable interrupts (again, via a hardware in-\nstruction) and thus the program proceeds as usual.\nThe main positive of this approach is its simplicity. You certainly don’t\nhave to scratch your head too hard to ﬁgure out why this works. Without\ninterruption, a thread can be sure that the code it executes will execute\nand that no other thread will interfere with it.\nThe negatives, unfortunately, are many. First, this approach requires\nus to allow any calling thread to perform a privileged operation (turning\ninterrupts on and off), and thus trust that this facility is not abused. As\nyou already know, any time we are required to trust an arbitrary pro-\ngram, we are probably in trouble. Here, the trouble manifests in numer-\nous ways: a greedy program could call lock() at the beginning of its\nexecution and thus monopolize the processor; worse, an errant or mali-\ncious program could call lock() and go into an endless loop. In this\nlatter case, the OS never regains control of the system, and there is only\none recourse: restart the system. Using interrupt disabling as a general-\npurpose synchronization solution requires too much trust in applications.\nSecond, the approach does not work on multiprocessors. If multiple\nthreads are running on different CPUs, and each try to enter the same\ncritical section, it does not matter whether interrupts are disabled; threads\nwill be able to run on other processors, and thus could enter the critical\nsection. As multiprocessors are now commonplace, our general solution\nwill have to do better than this.\nThird, and probably least important, this approach can be inefﬁcient.\nCompared to normal instruction execution, code that masks or unmasks\ninterrupts tends to be executed slowly by modern CPUs.\nFor these reasons, turning off interrupts is only used in limited con-\ntexts as a mutual-exclusion primitive. For example, in some cases an\noperating system itself will sometimes use interrupt masking to guaran-\ntee atomicity when accessing its own data structures, or at least to pre-\nvent certain messy interrupt handling situations from arising. This usage\nmakes sense, as the trust issue disappears inside the OS, which always\ntrusts itself to perform privileged operations anyhow.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nLOCKS\n295\nASIDE: DEKKER’S AND PETERSON’S ALGORITHMS\nIn the 1960’s, Dijkstra posed the concurrency problem to his friends,\nand one of them, a mathematician named Theodorus Jozef Dekker, came\nup with a solution [D68]. Unlike the solutions we discuss here, which use\nspecial hardware instructions and even OS support, Dekker’s approach\nuses just loads and stores (assuming they are atomic with respect to each\nother, which was true on early hardware).\nDekker’s approach was later reﬁned by Peterson [P81] (and thus “Pe-\nterson’s algorithm”), shown here. Once again, just loads and stores are\nused, and the idea is to ensure that two threads never enter a critical sec-\ntion at the same time. Here is Peterson’s algorithm (for two threads); see\nif you can understand it.\nint flag[2];\nint turn;\nvoid init() {\nflag[0] = flag[1] = 0;\n// 1->thread wants to grab lock\nturn = 0;\n// whose turn? (thread 0 or 1?)\n}\nvoid lock() {\nflag[self] = 1;\n// self: thread ID of caller\nturn = 1 - self;\n// make it other thread’s turn\nwhile ((flag[1-self] == 1) && (turn == 1 - self))\n; // spin-wait\n}\nvoid unlock() {\nflag[self] = 0;\n// simply undo your intent\n}\nFor some reason, developing locks that work without special hard-\nware support became all the rage for a while, giving theory-types a lot\nof problems to work on. Of course, this all became quite useless when\npeople realized it is much easier to assume a little hardware support (and\nindeed that support had been around from the very earliest days of multi-\nprocessing). Further, algorithms like the ones above don’t work on mod-\nern hardware (due to relaxed memory consistency models), thus making\nthem even less useful than they were before. Yet more research relegated\nto the dustbin of history...\n28.6\nTest And Set (Atomic Exchange)\nBecause disabling interrupts does not work on multiple processors,\nsystem designers started to invent hardware support for locking. The\nearliest multiprocessor systems, such as the Burroughs B5000 in the early\n1960’s [M82], had such support; today all systems provide this type of\nsupport, even for single CPU systems.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n296\nLOCKS\n1\ntypedef struct __lock_t { int flag; } lock_t;\n2\n3\nvoid init(lock_t *mutex) {\n4\n// 0 -> lock is available, 1 -> held\n5\nmutex->flag = 0;\n6\n}\n7\n8\nvoid lock(lock_t *mutex) {\n9\nwhile (mutex->flag == 1)\n// TEST the flag\n10\n; // spin-wait (do nothing)\n11\nmutex->flag = 1;\n// now SET it!\n12\n}\n13\n14\nvoid unlock(lock_t *mutex) {\n15\nmutex->flag = 0;\n16\n}\nFigure 28.1: First Attempt: A Simple Flag\nThe simplest bit of hardware support to understand is what is known\nas a test-and-set instruction, also known as atomic exchange. To under-\nstand how test-and-set works, let’s ﬁrst try to build a simple lock without\nit. In this failed attempt, we use a simple ﬂag variable to denote whether\nthe lock is held or not.\nIn this ﬁrst attempt (Figure 28.1), the idea is quite simple: use a simple\nvariable to indicate whether some thread has possession of a lock. The\nﬁrst thread that enters the critical section will call lock(), which tests\nwhether the ﬂag is equal to 1 (in this case, it is not), and then sets the ﬂag\nto 1 to indicate that the thread now holds the lock. When ﬁnished with\nthe critical section, the thread calls unlock() and clears the ﬂag, thus\nindicating that the lock is no longer held.\nIf another thread happens to call lock() while that ﬁrst thread is in\nthe critical section, it will simply spin-wait in the while loop for that\nthread to call unlock() and clear the ﬂag. Once that ﬁrst thread does\nso, the waiting thread will fall out of the while loop, set the ﬂag to 1 for\nitself, and proceed into the critical section.\nUnfortunately, the code has two problems: one of correctness, and an-\nother of performance. The correctness problem is simple to see once you\nget used to thinking about concurrent programming. Imagine the code\ninterleaving in Table 28.1 (assume flag=0 to begin).\nThread 1\nThread 2\ncall lock()\nwhile (ﬂag == 1)\ninterrupt: switch to Thread 2\ncall lock()\nwhile (ﬂag == 1)\nﬂag = 1;\ninterrupt: switch to Thread 1\nﬂag = 1; // set ﬂag to 1 (too!)\nTable 28.1: Trace: No Mutual Exclusion\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nLOCKS\n297\nTIP: THINK ABOUT CONCURRENCY AS MALICIOUS SCHEDULER\nWhat we also get from this example is a sense of the approach we\nneed to take when trying to understand concurrent execution. What you\nare really trying to do is to pretend you are a malicious scheduler, one\nthat interrupts threads at the most inopportune of times in order to foil\ntheir feeble attempts at building synchronization primitives. Although\nthe exact sequence of interrupts may be improbable, it is possible, and that\nis all we need to show to demonstrate that a particular approach does not\nwork.\nAs you can see from this interleaving, with timely (untimely?) inter-\nrupts, we can easily produce a case where both threads set their ﬂags to 1\nand both threads are thus able to enter the critical section. This is bad! We\nhave obviously failed to provide the most basic requirement: providing\nmutual exclusion.\nThe performance problem, which we will address more later on, is the\nfact that the way a thread waits to acquire a lock that is already held:\nit endlessly checks the value of ﬂag, a technique known as spin-waiting.\nSpin-waiting wastes time waiting for another thread to release a lock. The\nwaste is exceptionally high on a uniprocessor, where the thread that the\nwaiter is waiting for cannot even run (at least, until a context switch oc-\ncurs)! Thus, as we move forward and develop more sophisticated solu-\ntions, we should also consider ways to avoid this kind of waste.\n28.7\nBuilding A Working Spin Lock\nWhile the idea behind the example above is a good one, it is not possi-\nble to implement without some support from the hardware. Fortunately,\nsome systems provide an instruction to support the creation of simple\nlocks based on this concept. This more powerful instruction has differ-\nent names – on SPARC, it is the load/store unsigned byte instruction\n(ldstub), whereas on x86, it is the atomic exchange instruction (xchg)\n– but basically does the same thing across platforms, and is usually gen-\nerally referred to as test-and-set. We deﬁne what the test-and-set instruc-\ntion does with the following C code snippet:\n1\nint TestAndSet(int *ptr, int new) {\n2\nint old = *ptr; // fetch old value at ptr\n3\n*ptr = new;\n// store ’new’ into ptr\n4\nreturn old;\n// return the old value\n5\n}\nWhat the test-and-set instruction does is as follows. It returns the old\nvalue pointed to by the ptr, and simultaneously updates said value to\nnew. The key, of course, is that this sequence of operations is performed\natomically. The reason it is called “test and set” is that it enables you\nto “test” the old value (which is what is returned) while simultaneously\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n298\nLOCKS\n1\ntypedef struct __lock_t {\n2\nint flag;\n3\n} lock_t;\n4\n5\nvoid init(lock_t *lock) {\n6\n// 0 indicates that lock is available, 1 that it is held\n7\nlock->flag = 0;\n8\n}\n9\n10\nvoid lock(lock_t *lock) {\n11\nwhile (TestAndSet(&lock->flag, 1) == 1)\n12\n; // spin-wait (do nothing)\n13\n}\n14\n15\nvoid unlock(lock_t *lock) {\n16\nlock->flag = 0;\n17\n}\nFigure 28.2: A Simple Spin Lock Using Test-and-set\n“setting” the memory location to a new value; as it turns out, this slightly\nmore powerful instruction is enough to build a simple spin lock, as we\nnow examine in Figure 28.2.\nLet’s make sure we understand why this works. Imagine ﬁrst the case\nwhere a thread calls lock() and no other thread currently holds the lock;\nthus, flag should be 0. When the thread then calls TestAndSet(flag,\n1), the routine will return the old value of flag, which is 0; thus, the call-\ning thread, which is testing the value of ﬂag, will not get caught spinning\nin the while loop and will acquire the lock. The thread will also atomi-\ncally set the value to 1, thus indicating that the lock is now held. When\nthe thread is ﬁnished with its critical section, it calls unlock() to set the\nﬂag back to zero.\nThe second case we can imagine arises when one thread already has\nthe lock held (i.e., flag is 1). In this case, this thread will call lock() and\nthen call TestAndSet(flag, 1) as well. This time, TestAndSet()\nwill return the old value at ﬂag, which is 1 (because the lock is held),\nwhile simultaneously setting it to 1 again. As long as the lock is held by\nanother thread, TestAndSet() will repeatedly return 1, and thus this\nthread will spin and spin until the lock is ﬁnally released. When the ﬂag is\nﬁnally set to 0 by some other thread, this thread will call TestAndSet()\nagain, which will now return 0 while atomically setting the value to 1 and\nthus acquire the lock and enter the critical section.\nBy making both the test (of the old lock value) and set (of the new\nvalue) a single atomic operation, we ensure that only one thread acquires\nthe lock. And that’s how to build a working mutual exclusion primitive!\nYou may also now understand why this type of lock is usually referred\nto as a spin lock. It is the simplest type of lock to build, and simply spins,\nusing CPU cycles, until the lock becomes available. To work correctly\non a single processor, it requires a preemptive scheduler (i.e., one that\nwill interrupt a thread via a timer, in order to run a different thread, from\ntime to time). Without preemption, spin locks don’t make much sense on\na single CPU, as a thread spinning on a CPU will never relinquish it.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nLOCKS\n299\n28.8\nEvaluating Spin Locks\nGiven our basic spin lock, we can now evaluate how effective it is\nalong our previously described axes. The most important aspect of a lock\nis correctness: does it provide mutual exclusion? The answer here is ob-\nviously yes: the spin lock only allows a single thread to enter the critical\nsection at a time. Thus, we have a correct lock.\nThe next axis is fairness. How fair is a spin lock to a waiting thread?\nCan you guarantee that a waiting thread will ever enter the critical sec-\ntion? The answer here, unfortunately, is bad news: spin locks don’t pro-\nvide any fairness guarantees. Indeed, a thread spinning may spin forever,\nunder contention. Spin locks are not fair and may lead to starvation.\nThe ﬁnal axis is performance. What are the costs of using a spin lock?\nTo analyze this more carefully, we suggest thinking about a few different\ncases. In the ﬁrst, imagine threads competing for the lock on a single\nprocessor; in the second, consider the threads as spread out across many\nprocessors.\nFor spin locks, in the single CPU case, performance overheads can\nbe quite painful; imagine the case where the thread holding the lock is\npre-empted within a critical section. The scheduler might then run every\nother thread (imagine there are N −1 others), each of which tries to ac-\nquire the lock. In this case, each of those threads will spin for the duration\nof a time slice before giving up the CPU, a waste of CPU cycles.\nHowever, on multiple CPUs, spin locks work reasonably well (if the\nnumber of threads roughly equals the number of CPUs). The thinking\ngoes as follows: imagine Thread A on CPU 1 and Thread B on CPU 2,\nboth contending for a lock. If Thread A (CPU 1) grabs the lock, and then\nThread B tries to, B will spin (on CPU 2). However, presumably the crit-\nical section is short, and thus soon the lock becomes available, and is ac-\nquired by Thread B. Spinning to wait for a lock held on another processor\ndoesn’t waste many cycles in this case, and thus can be quite effective.\n28.9\nCompare-And-Swap\nAnother hardware primitive that some systems provide is known as\nthe compare-and-swap instruction (as it is called on SPARC, for exam-\nple), or compare-and-exchange (as it called on x86). The C pseudocode\nfor this single instruction is found in Figure 28.3.\nThe basic idea is for compare-and-swap to test whether the value at the\n1\nint CompareAndSwap(int *ptr, int expected, int new) {\n2\nint actual = *ptr;\n3\nif (actual == expected)\n4\n*ptr = new;\n5\nreturn actual;\n6\n}\nFigure 28.3: Compare-and-swap\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n300\nLOCKS\naddress speciﬁed by ptr is equal to expected; if so, update the memory\nlocation pointed to by ptr with the new value. If not, do nothing. In\neither case, return the actual value at that memory location, thus allowing\nthe code calling compare-and-swap to know whether it succeeded or not.\nWith the compare-and-swap instruction, we can build a lock in a man-\nner quite similar to that with test-and-set. For example, we could just\nreplace the lock() routine above with the following:\n1\nvoid lock(lock_t *lock) {\n2\nwhile (CompareAndSwap(&lock->flag, 0, 1) == 1)\n3\n; // spin\n4\n}\nThe rest of the code is the same as the test-and-set example above.\nThis code works quite similarly; it simply checks if the ﬂag is 0 and if\nso, atomically swaps in a 1 thus acquiring the lock. Threads that try to\nacquire the lock while it is held will get stuck spinning until the lock is\nﬁnally released.\nIf you want to see how to really make a C-callable x86-version of\ncompare-and-swap, this code sequence might be useful (from [S05]):\n1\nchar CompareAndSwap(int *ptr, int old, int new) {\n2\nunsigned char ret;\n3\n4\n// Note that sete sets a ’byte’ not the word\n5\n__asm__ __volatile__ (\n6\n\"\nlock\\n\"\n7\n\"\ncmpxchgl %2,%1\\n\"\n8\n\"\nsete %0\\n\"\n9\n: \"=q\" (ret), \"=m\" (*ptr)\n10\n: \"r\" (new), \"m\" (*ptr), \"a\" (old)\n11\n: \"memory\");\n12\nreturn ret;\n13\n}\nFinally, as you may have sensed, compare-and-swap is a more power-\nful instruction than test-and-set. We will make some use of this power in\nthe future when we brieﬂy delve into wait-free synchronization [H91].\nHowever, if we just build a simple spin lock with it, its behavior is iden-\ntical to the spin lock we analyzed above.\n28.10\nLoad-Linked and Store-Conditional\nSome platforms provide a pair of instructions that work in concert to\nhelp build critical sections. On the MIPS architecture [H93], for example,\nthe load-linked and store-conditional instructions can be used in tandem\nto build locks and other concurrent structures. The C pseudocode for\nthese instructions is as found in Figure 28.4. Alpha, PowerPC, and ARM\nprovide similar instructions [W09].\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nLOCKS\n301\n1\nint LoadLinked(int *ptr) {\n2\nreturn *ptr;\n3\n}\n4\n5\nint StoreConditional(int *ptr, int value) {\n6\nif (no one has updated *ptr since the LoadLinked to this address) {\n7\n*ptr = value;\n8\nreturn 1; // success!\n9\n} else {\n10\nreturn 0; // failed to update\n11\n}\n12\n}\nFigure 28.4: Load-linked And Store-conditional\n1\nvoid lock(lock_t *lock) {\n2\nwhile (1) {\n3\nwhile (LoadLinked(&lock->flag) == 1)\n4\n; // spin until it’s zero\n5\nif (StoreConditional(&lock->flag, 1) == 1)\n6\nreturn; // if set-it-to-1 was a success: all done\n7\n// otherwise: try it all over again\n8\n}\n9\n}\n10\n11\nvoid unlock(lock_t *lock) {\n12\nlock->flag = 0;\n13\n}\nFigure 28.5: Using LL/SC To Build A Lock\nThe load-linked operates much like a typical load instruction, and sim-\nply fetches a value from memory and places it in a register. The key differ-\nence comes with the store-conditional, which only succeeds (and updates\nthe value stored at the address just load-linked from) if no intermittent\nstore to the address has taken place. In the case of success, the store-\nconditional returns 1 and updates the value at ptr to value; if it fails,\nthe value at ptr is not updated and 0 is returned.\nAs a challenge to yourself, try thinking about how to build a lock using\nload-linked and store-conditional. Then, when you are ﬁnished, look at\nthe code below which provides one simple solution. Do it! The solution\nis in Figure 28.5.\nThe lock() code is the only interesting piece. First, a thread spins\nwaiting for the ﬂag to be set to 0 (and thus indicate the lock is not held).\nOnce so, the thread tries to acquire the lock via the store-conditional; if it\nsucceeds, the thread has atomically changed the ﬂag’s value to 1 and thus\ncan proceed into the critical section.\nNote how failure of the store-conditional might arise. One thread calls\nlock() and executes the load-linked, returning 0 as the lock is not held.\nBefore it can attempt the store-conditional, it is interrupted and another\nthread enters the lock code, also executing the load-linked instruction,\nand also getting a 0 and continuing. At this point, two threads have\neach executed the load-linked and each are about to attempt the store-\nconditional. The key feature of these instructions is that only one of these\nthreads will succeed in updating the ﬂag to 1 and thus acquire the lock;\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n302\nLOCKS\nTIP: LESS CODE IS BETTER CODE (LAUER’S LAW)\nProgrammers tend to brag about how much code they wrote to do some-\nthing. Doing so is fundamentally broken. What one should brag about,\nrather, is how little code one wrote to accomplish a given task. Short,\nconcise code is always preferred; it is likely easier to understand and has\nfewer bugs. As Hugh Lauer said, when discussing the construction of\nthe Pilot operating system: “If the same people had twice as much time,\nthey could produce as good of a system in half the code.” [L81] We’ll call\nthis Lauer’s Law, and it is well worth remembering. So next time you’re\nbragging about how much code you wrote to ﬁnish the assignment, think\nagain, or better yet, go back, rewrite, and make the code as clear and con-\ncise as possible.\nthe second thread to attempt the store-conditional will fail (because the\nother thread updated the value of ﬂag between its load-linked and store-\nconditional) and thus have to try to acquire the lock again.\nIn class a few years ago, undergraduate student David Capel sug-\ngested a more concise form of the above, for those of you who enjoy\nshort-circuiting boolean conditionals. See if you can ﬁgure out why it\nis equivalent. It certainly is shorter!\n1\nvoid lock(lock_t *lock) {\n2\nwhile (LoadLinked(&lock->flag)||!StoreConditional(&lock->flag, 1))\n3\n; // spin\n4\n}\n28.11\nFetch-And-Add\nOne ﬁnal hardware primitive is the fetch-and-add instruction, which\natomically increments a value while returning the old value at a partic-\nular address. The C pseudocode for the fetch-and-add instruction looks\nlike this:\n1\nint FetchAndAdd(int *ptr) {\n2\nint old = *ptr;\n3\n*ptr = old + 1;\n4\nreturn old;\n5\n}\nIn this example, we’ll use fetch-and-add to build a more interesting\nticket lock, as introduced by Mellor-Crummey and Scott [MS91]. The\nlock and unlock code looks like what you see in Figure 28.6.\nInstead of a single value, this solution uses a ticket and turn variable in\ncombination to build a lock. The basic operation is pretty simple: when\na thread wishes to acquire a lock, it ﬁrst does an atomic fetch-and-add\non the ticket value; that value is now considered this thread’s “turn”\n(myturn). The globally shared lock->turn is then used to determine\nwhich thread’s turn it is; when (myturn == turn) for a given thread,\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nLOCKS\n303\n1\ntypedef struct __lock_t {\n2\nint ticket;\n3\nint turn;\n4\n} lock_t;\n5\n6\nvoid lock_init(lock_t *lock) {\n7\nlock->ticket = 0;\n8\nlock->turn\n= 0;\n9\n}\n10\n11\nvoid lock(lock_t *lock) {\n12\nint myturn = FetchAndAdd(&lock->ticket);\n13\nwhile (lock->turn != myturn)\n14\n; // spin\n15\n}\n16\n17\nvoid unlock(lock_t *lock) {\n18\nFetchAndAdd(&lock->turn);\n19\n}\nFigure 28.6: Ticket Locks\nit is that thread’s turn to enter the critical section. Unlock is accomplished\nsimply by incrementing the turn such that the next waiting thread (if\nthere is one) can now enter the critical section.\nNote one important difference with this solution versus our previous\nattempts: it ensures progress for all threads. Once a thread is assigned its\nticket value, it will be scheduled at some point in the future (once those in\nfront of it have passed through the critical section and released the lock).\nIn our previous attempts, no such guarantee existed; a thread spinning\non test-and-set (for example) could spin forever even as other threads\nacquire and release the lock.\n28.12\nSummary: So Much Spinning\nOur simple hardware-based locks are simple (only a few lines of code)\nand they work (you could even prove that if you’d like to, by writing\nsome code), which are two excellent properties of any system or code.\nHowever, in some cases, these solutions can be quite inefﬁcient. Imagine\nyou are running two threads on a single processor. Now imagine that\none thread (thread 0) is in a critical section and thus has a lock held, and\nunfortunately gets interrupted. The second thread (thread 1) now tries to\nacquire the lock, but ﬁnds that it is held. Thus, it begins to spin. And spin.\nThen it spins some more. And ﬁnally, a timer interrupt goes off, thread\n0 is run again, which releases the lock, and ﬁnally (the next time it runs,\nsay), thread 1 won’t have to spin so much and will be able to acquire the\nlock. Thus, any time a thread gets caught spinning in a situation like this,\nit wastes an entire time slice doing nothing but checking a value that isn’t\ngoing to change! The problem gets worse with N threads contending\nfor a lock; N −1 time slices may be wasted in a similar manner, simply\nspinning and waiting for a single thread to release the lock. And thus,\nour next problem:\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n304\nLOCKS\nTHE CRUX: HOW TO AVOID SPINNING\nHow can we develop a lock that doesn’t needlessly waste time spin-\nning on the CPU?\nHardware support alone cannot solve the problem. We’ll need OS sup-\nport too! Let’s now ﬁgure out just how that might work.\n28.13\nA Simple Approach: Just Yield, Baby\nHardware support got us pretty far: working locks, and even (as with\nthe case of the ticket lock) fairness in lock acquisition. However, we still\nhave a problem: what to do when a context switch occurs in a critical\nsection, and threads start to spin endlessly, waiting for the interrupt (lock-\nholding) thread to be run again?\nOur ﬁrst try is a simple and friendly approach: when you are going to\nspin, instead give up the CPU to another thread. Or, as Al Davis might\nsay, “just yield, baby!” [D91]. Figure 28.7 presents the approach.\nIn this approach, we assume an operating system primitive yield()\nwhich a thread can call when it wants to give up the CPU and let an-\nother thread run. Because a thread can be in one of three states (running,\nready, or blocked), you can think of this as an OS system call that moves\nthe caller from the running state to the ready state, and thus promotes\nanother thread to running.\nThink about the example with two threads on one CPU; in this case,\nour yield-based approach works quite well. If a thread happens to call\nlock() and ﬁnd a lock held, it will simply yield the CPU, and thus the\nother thread will run and ﬁnish its critical section. In this simple case, the\nyielding approach works well.\nLet us now consider the case where there are many threads (say 100)\ncontending for a lock repeatedly.\nIn this case, if one thread acquires\nthe lock and is preempted before releasing it, the other 99 will each call\n1\nvoid init() {\n2\nflag = 0;\n3\n}\n4\n5\nvoid lock() {\n6\nwhile (TestAndSet(&flag, 1) == 1)\n7\nyield(); // give up the CPU\n8\n}\n9\n10\nvoid unlock() {\n11\nflag = 0;\n12\n}\nFigure 28.7: Lock With Test-and-set And Yield\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nLOCKS\n305\nlock(), ﬁnd the lock held, and yield the CPU. Assuming some kind\nof round-robin scheduler, each of the 99 will execute this run-and-yield\npattern before the thread holding the lock gets to run again. While better\nthan our spinning approach (which would waste 99 time slices spinning),\nthis approach is still costly; the cost of a context switch can be substantial,\nand there is thus plenty of waste.\nWorse, we have not tackled the starvation problem at all. A thread\nmay get caught in an endless yield loop while other threads repeatedly\nenter and exit the critical section. We clearly will need an approach that\naddresses this problem directly.\n28.14\nUsing Queues: Sleeping Instead Of Spinning\nThe real problem with our previous approaches is that they leave too\nmuch to chance. The scheduler determines which thread runs next; if\nthe scheduler makes a bad choice, a thread runs that must either spin\nwaiting for the lock (our ﬁrst approach), or yield the CPU immediately\n(our second approach). Either way, there is potential for waste and no\nprevention of starvation.\nThus, we must explicitly exert some control over who gets to acquire\nthe lock next after the current holder releases it. To do this, we will need a\nlittle more OS support, as well as a queue to keep track of which threads\nare waiting to enter the lock.\nFor simplicity, we will use the support provided by Solaris, in terms of\ntwo calls: park() to put a calling thread to sleep, and unpark(threadID)\nto wake a particular thread as designated by threadID. These two rou-\ntines can be used in tandem to build a lock that puts a caller to sleep if it\ntries to acquire a held lock and wakes it when the lock is free. Let’s look at\nthe code in Figure 28.8 to understand one possible use of such primitives.\nWe do a couple of interesting things in this example. First, we combine\nthe old test-and-set idea with an explicit queue of lock waiters to make a\nmore efﬁcient lock. Second, we use a queue to help control who gets the\nlock next and thus avoid starvation.\nYou might notice how the guard is used, basically as a spin-lock around\nthe ﬂag and queue manipulations the lock is using. This approach thus\ndoesn’t avoid spin-waiting entirely; a thread might be interrupted while\nacquiring or releasing the lock, and thus cause other threads to spin-wait\nfor this one to run again. However, the time spent spinning is quite lim-\nited (just a few instructions inside the lock and unlock code, instead of the\nuser-deﬁned critical section), and thus this approach may be reasonable.\nSecond, you might notice that in lock(), when a thread can not ac-\nquire the lock (it is already held), we are careful to add ourselves to a\nqueue (by calling the gettid() call to get the thread ID of the current\nthread), set guard to 0, and yield the CPU. A question for the reader:\nWhat would happen if the release of the guard lock came after the park(),\nand not before? Hint: something bad.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n306\nLOCKS\n1\ntypedef struct __lock_t {\n2\nint flag;\n3\nint guard;\n4\nqueue_t *q;\n5\n} lock_t;\n6\n7\nvoid lock_init(lock_t *m) {\n8\nm->flag\n= 0;\n9\nm->guard = 0;\n10\nqueue_init(m->q);\n11\n}\n12\n13\nvoid lock(lock_t *m) {\n14\nwhile (TestAndSet(&m->guard, 1) == 1)\n15\n; //acquire guard lock by spinning\n16\nif (m->flag == 0) {\n17\nm->flag = 1; // lock is acquired\n18\nm->guard = 0;\n19\n} else {\n20\nqueue_add(m->q, gettid());\n21\nm->guard = 0;\n22\npark();\n23\n}\n24\n}\n25\n26\nvoid unlock(lock_t *m) {\n27\nwhile (TestAndSet(&m->guard, 1) == 1)\n28\n; //acquire guard lock by spinning\n29\nif (queue_empty(m->q))\n30\nm->flag = 0; // let go of lock; no one wants it\n31\nelse\n32\nunpark(queue_remove(m->q)); // hold lock (for next thread!)\n33\nm->guard = 0;\n34\n}\nFigure 28.8: Lock With Queues, Test-and-set, Yield, And Wakeup\nYou might also notice the interesting fact that the ﬂag does not get set\nback to 0 when another thread gets woken up. Why is this? Well, it is not\nan error, but rather a necessity! When a thread is woken up, it will be as\nif it is returning from park(); however, it does not hold the guard at that\npoint in the code and thus cannot even try to set the ﬂag to 1. Thus, we\njust pass the lock directly from the thread releasing the lock to the next\nthread acquiring it; ﬂag is not set to 0 in-between.\nFinally, you might notice the perceived race condition in the solution,\njust before the call to park(). With just the wrong timing, a thread will\nbe about to park, assuming that it should sleep until the lock is no longer\nheld. A switch at that time to another thread (say, a thread holding the\nlock) could lead to trouble, for example, if that thread then released the\nlock. The subsequent park by the ﬁrst thread would then sleep forever\n(potentially). This problem is sometimes called the wakeup/waiting race;\nto avoid it, we need to do some extra work.\nSolaris solves this problem by adding a third system call: setpark().\nBy calling this routine, a thread can indicate it is about to park. If it then\nhappens to be interrupted and another thread calls unpark before park is\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nLOCKS\n307\nactually called, the subsequent park returns immediately instead of sleep-\ning. The code modiﬁcation, inside of lock(), is quite small:\n1\nqueue_add(m->q, gettid());\n2\nsetpark(); // new code\n3\nm->guard = 0;\nA different solution could pass the guard into the kernel. In that case,\nthe kernel could take precautions to atomically release the lock and de-\nqueue the running thread.\n28.15\nDifferent OS, Different Support\nWe have thus far seen one type of support that an OS can provide in\norder to build a more efﬁcient lock in a thread library. Other OS’s provide\nsimilar support; the details vary.\nFor example, Linux provides something called a futex which is simi-\nlar to the Solaris interface but provides a bit more in-kernel functionality.\nSpeciﬁcally, each futex has associated with it a speciﬁc physical mem-\nory location; associated with each such memory location is an in-kernel\nqueue. Callers can use futex calls (described below) to sleep and wake as\nneed be.\nSpeciﬁcally, two calls are available. The call to futex wait(address,\nexpected) puts the calling thread to sleep, assuming the value at address\nis equal to expected. If it is not equal, the call returns immediately. The\ncall to the routine futex wake(address) wakes one thread that is wait-\ning on the queue. The usage of these in Linux is as found in 28.9.\nThis code snippet from lowlevellock.h in the nptl library (part of\nthe gnu libc library) [L09] is pretty interesting. Basically, it uses a single\ninteger to track both whether the lock is held or not (the high bit of the\ninteger) and the number of waiters on the lock (all the other bits). Thus,\nif the lock is negative, it is held (because the high bit is set and that bit\ndetermines the sign of the integer). The code is also interesting because it\nshows how to optimize for the common case where there is no contention:\nwith only one thread acquiring and releasing a lock, very little work is\ndone (the atomic bit test-and-set to lock and an atomic add to release the\nlock). See if you can puzzle through the rest of this “real-world” lock to\nsee how it works.\n28.16\nTwo-Phase Locks\nOne ﬁnal note: the Linux approach has the ﬂavor of an old approach\nthat has been used on and off for years, going at least as far back to Dahm\nLocks in the early 1960’s [M82], and is now referred to as a two-phase\nlock. A two-phase lock realizes that spinning can be useful, particularly\nif the lock is about to be released. So in the ﬁrst phase, the lock spins for\na while, hoping that it can acquire the lock.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n308\nLOCKS\n1\nvoid mutex_lock (int *mutex) {\n2\nint v;\n3\n/* Bit 31 was clear, we got the mutex (this is the fastpath)\n*/\n4\nif (atomic_bit_test_set (mutex, 31) == 0)\n5\nreturn;\n6\natomic_increment (mutex);\n7\nwhile (1) {\n8\nif (atomic_bit_test_set (mutex, 31) == 0) {\n9\natomic_decrement (mutex);\n10\nreturn;\n11\n}\n12\n/* We have to wait now. First make sure the futex value\n13\nwe are monitoring is truly negative (i.e. locked). */\n14\nv = *mutex;\n15\nif (v >= 0)\n16\ncontinue;\n17\nfutex_wait (mutex, v);\n18\n}\n19\n}\n20\n21\nvoid mutex_unlock (int *mutex) {\n22\n/* Adding 0x80000000 to the counter results in 0 if and only if\n23\nthere are not other interested threads */\n24\nif (atomic_add_zero (mutex, 0x80000000))\n25\nreturn;\n26\n27\n/* There are other threads waiting for this mutex,\n28\nwake one of them up.\n*/\n29\nfutex_wake (mutex);\nFigure 28.9: Linux-based Futex Locks\nHowever, if the lock is not acquired during the ﬁrst spin phase, a sec-\nond phase is entered, where the caller is put to sleep, and only woken up\nwhen the lock becomes free later. The Linux lock above is a form of such\na lock, but it only spins once; a generalization of this could spin in a loop\nfor a ﬁxed amount of time before using futex support to sleep.\nTwo-phase locks are yet another instance of a hybrid approach, where\ncombining two good ideas may indeed yield a better one. Of course,\nwhether it does depends strongly on many things, including the hard-\nware environment, number of threads, and other workload details. As\nalways, making a single general-purpose lock, good for all possible use\ncases, is quite a challenge.\n28.17\nSummary\nThe above approach shows how real locks are built these days: some\nhardware support (in the form of a more powerful instruction) plus some\noperating system support (e.g., in the form of park() and unpark()\nprimitives on Solaris, or futex on Linux). Of course, the details differ, and\nthe exact code to perform such locking is usually highly tuned. Check\nout the Solaris or Linux open source code bases if you want to see more\ndetails; they are a fascinating read [L09, S09].\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "page_number": 330,
      "chapter_number": 34,
      "summary": "This chapter covers segment 34 (pages 330-344). Key topics include locks, thread, and spin. The code would look like this:\n1\nvoid lock() {\n2\nDisableInterrupts();\n3\n}\n4\nvoid unlock() {\n5\nEnableInterrupts();\n6\n}\nAssume we are running on such a single-processor system.",
      "keywords": [
        "lock",
        "thread",
        "Spin Lock",
        "void lock",
        "thread calls lock",
        "call lock",
        "Simple Spin Lock",
        "Spin",
        "CPU",
        "flag",
        "code",
        "critical section",
        "int",
        "lock held",
        "critical"
      ],
      "concepts": [
        "locks",
        "thread",
        "spin",
        "spinning",
        "code",
        "instruction",
        "instructions",
        "interrupts",
        "interruption",
        "returns"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 37,
          "title": "Segment 37 (pages 363-372)",
          "relevance_score": 0.71,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 51,
          "title": "Segment 51 (pages 1023-1040)",
          "relevance_score": 0.68,
          "method": "api"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 48,
          "title": "Segment 48 (pages 529-537)",
          "relevance_score": 0.66,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 52,
          "title": "Segment 52 (pages 1041-1063)",
          "relevance_score": 0.66,
          "method": "api"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 49,
          "title": "Segment 49 (pages 538-555)",
          "relevance_score": 0.65,
          "method": "api"
        }
      ]
    },
    {
      "number": 35,
      "title": "Segment 35 (pages 345-354)",
      "start_page": 345,
      "end_page": 354,
      "detection_method": "topic_boundary",
      "content": "LOCKS\n309\nReferences\n[D91] “Just Win, Baby: Al Davis and His Raiders”\nGlenn Dickey, Harcourt 1991\nThere is even an undoubtedly bad book about Al Davis and his famous “just win” quote. Or, we suppose,\nthe book is more about Al Davis and the Raiders, and maybe not just the quote. Read the book to ﬁnd\nout?\n[D68] “Cooperating sequential processes”\nEdsger W. Dijkstra, 1968\nAvailable: http://www.cs.utexas.edu/users/EWD/ewd01xx/EWD123.PDF\nOne of the early seminal papers in the area. Discusses how Dijkstra posed the original concurrency\nproblem, and Dekker’s solution.\n[H93] “MIPS R4000 Microprocessor User’s Manual”.\nJoe Heinrich, Prentice-Hall, June 1993\nAvailable: http://cag.csail.mit.edu/raw/\ndocuments/R4400 Uman book Ed2.pdf\n[H91] “Wait-free Synchronization”\nMaurice Herlihy\nACM Transactions on Programming Languages and Systems (TOPLAS)\nVolume 13, Issue 1, January 1991\nA landmark paper introducing a different approach to building concurrent data structures. However,\nbecause of the complexity involved, many of these ideas have been slow to gain acceptance in deployed\nsystems.\n[L81] “Observations on the Development of an Operating System”\nHugh Lauer\nSOSP ’81\nA must-read retrospective about the development of the Pilot OS, an early PC operating system. Fun\nand full of insights.\n[L09] “glibc 2.9 (include Linux pthreads implementation)”\nAvailable: http://ftp.gnu.org/gnu/glibc/\nIn particular, take a look at the nptl subdirectory where you will ﬁnd most of the pthread support in\nLinux today.\n[M82] “The Architecture of the Burroughs B5000\n20 Years Later and Still Ahead of the Times?”\nAlastair J.W. Mayer, 1982\nwww.ajwm.net/amayer/papers/B5000.html\nFrom the paper: “One particularly useful instruction is the RDLK (read-lock). It is an indivisible\noperation which reads from and writes into a memory location.” RDLK is thus an early test-and-set\nprimitive, if not the earliest. Some credit here goes to an engineer named Dave Dahm, who apparently\ninvented a number of these things for the Burroughs systems, including a form of spin locks (called\n“Buzz Locks” as well as a two-phase lock eponymously called “Dahm Locks.”)\n[MS91] “Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors”\nJohn M. Mellor-Crummey and M. L. Scott\nACM TOCS, February 1991\nAn excellent survey on different locking algorithms. However, no OS support is used, just fancy hard-\nware instructions.\n[P81] “Myths About the Mutual Exclusion Problem”\nG.L. Peterson\nInformation Processing Letters. 12(3) 1981, 115–116\nPeterson’s algorithm introduced here.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n310\nLOCKS\n[S05] “Guide to porting from Solaris to Linux on x86”\nAjay Sood, April 29, 2005\nAvailable: http://www.ibm.com/developerworks/linux/library/l-solar/\n[S09] “OpenSolaris Thread Library”\nAvailable: http://src.opensolaris.org/source/xref/onnv/onnv-gate/\nusr/src/lib/libc/port/threads/synch.c\nThis is also pretty interesting to look at, though who knows what will happen to it now that Oracle owns\nSun. Thanks to Mike Swift for the pointer to the code.\n[W09] “Load-Link, Store-Conditional”\nWikipedia entry on said topic, as of October 22, 2009\nhttp://en.wikipedia.org/wiki/Load-Link/Store-Conditional\nCan you believe we referenced wikipedia? Pretty shabby. But, we found the information there ﬁrst,\nand it felt wrong not to cite it. Further, they even listed the instructions for the different architec-\ntures: ldl l/stl c and ldq l/stq c (Alpha), lwarx/stwcx (PowerPC), ll/sc (MIPS), and\nldrex/strex (ARM version 6 and above).\n[WG00] “The SPARC Architecture Manual: Version 9”\nDavid L. Weaver and Tom Germond, September 2000\nSPARC International, San Jose, California\nAvailable: http://www.sparc.org/standards/SPARCV9.pdf\nAlso see: http://developers.sun.com/solaris/articles/atomic sparc/ for some\nmore details on Sparc atomic operations.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\n29\nLock-based Concurrent Data Structures\nBefore moving beyond locks, we’ll ﬁrst describe how to use locks in some\ncommon data structures. Adding locks to a data structure to make it us-\nable by threads makes the structure thread safe. Of course, exactly how\nsuch locks are added determines both the correctness and performance of\nthe data structure. And thus, our challenge:\nCRUX: HOW TO ADD LOCKS TO DATA STRUCTURES\nWhen given a particular data structure, how should we add locks to\nit, in order to make it work correctly? Further, how do we add locks such\nthat the data structure yields high performance, enabling many threads\nto access the structure at once, i.e., concurrently?\nOf course, we will be hard pressed to cover all data structures or all\nmethods for adding concurrency, as this is a topic that has been studied\nfor years, with (literally) thousands of research papers published about\nit. Thus, we hope to provide a sufﬁcient introduction to the type of think-\ning required, and refer you to some good sources of material for further\ninquiry on your own. We found Moir and Shavit’s survey to be a great\nsource of information [MS04].\n29.1\nConcurrent Counters\nOne of the simplest data structures is a counter. It is a structure that\nis commonly used and has a simple interface. We deﬁne a simple non-\nconcurrent counter in Figure 29.1.\nSimple But Not Scalable\nAs you can see, the non-synchronized counter is a trivial data structure,\nrequiring a tiny amount of code to implement. We now have our next\nchallenge: how can we make this code thread safe? Figure 29.2 shows\nhow we do so.\n311\n\n\n312\nLOCK-BASED CONCURRENT DATA STRUCTURES\n1\ntypedef struct __counter_t {\n2\nint value;\n3\n} counter_t;\n4\n5\nvoid init(counter_t *c) {\n6\nc->value = 0;\n7\n}\n8\n9\nvoid increment(counter_t *c) {\n10\nc->value++;\n11\n}\n12\n13\nvoid decrement(counter_t *c) {\n14\nc->value--;\n15\n}\n16\n17\nint get(counter_t *c) {\n18\nreturn c->value;\n19\n}\nFigure 29.1: A Counter Without Locks\n1\ntypedef struct __counter_t {\n2\nint\nvalue;\n3\npthread_lock_t lock;\n4\n} counter_t;\n5\n6\nvoid init(counter_t *c) {\n7\nc->value = 0;\n8\nPthread_mutex_init(&c->lock, NULL);\n9\n}\n10\n11\nvoid increment(counter_t *c) {\n12\nPthread_mutex_lock(&c->lock);\n13\nc->value++;\n14\nPthread_mutex_unlock(&c->lock);\n15\n}\n16\n17\nvoid decrement(counter_t *c) {\n18\nPthread_mutex_lock(&c->lock);\n19\nc->value--;\n20\nPthread_mutex_unlock(&c->lock);\n21\n}\n22\n23\nint get(counter_t *c) {\n24\nPthread_mutex_lock(&c->lock);\n25\nint rc = c->value;\n26\nPthread_mutex_unlock(&c->lock);\n27\nreturn rc;\n28\n}\nFigure 29.2: A Counter With Locks\nThis concurrent counter is simple and works correctly. In fact, it fol-\nlows a design pattern common to the simplest and most basic concurrent\ndata structures: it simply adds a single lock, which is acquired when call-\ning a routine that manipulates the data structure, and is released when\nreturning from the call. In this manner, it is similar to a data structure\nbuilt with monitors [BH73], where locks are acquired and released auto-\nmatically as you call and return from object methods.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nLOCK-BASED CONCURRENT DATA STRUCTURES\n313\n1\n2\n3\n4\n0\n5\n10\n15\nThreads\nTime (seconds)\nPrecise\nSloppy\nFigure 29.3: Performance of Traditional vs. Sloppy Counters\nAt this point, you have a working concurrent data structure. The prob-\nlem you might have is performance. If your data structure is too slow,\nyou’ll have to do more than just add a single lock; such optimizations, if\nneeded, are thus the topic of the rest of the chapter. Note that if the data\nstructure is not too slow, you are done! No need to do something fancy if\nsomething simple will work.\nTo understand the performance costs of the simple approach, we run a\nbenchmark in which each thread updates a single shared counter a ﬁxed\nnumber of times; we then vary the number of threads. Figure 29.3 shows\nthe total time taken, with one to four threads active; each thread updates\nthe counter one million times. This experiment was run upon an iMac\nwith four Intel 2.7 GHz i5 CPUs; with more CPUs active, we hope to get\nmore total work done per unit time.\nFrom the top line in the ﬁgure (labeled precise), you can see that the\nperformance of the synchronized counter scales poorly. Whereas a single\nthread can complete the million counter updates in a tiny amount of time\n(roughly 0.03 seconds), having two threads each update the counter one\nmillion times concurrently leads to a massive slowdown (taking over 5\nseconds!). It only gets worse with more threads.\nIdeally, you’d like to see the threads complete just as quickly on mul-\ntiple processors as the single thread does on one. Achieving this end is\ncalled perfect scaling; even though more work is done, it is done in par-\nallel, and hence the time taken to complete the task is not increased.\nScalable Counting\nAmazingly, researchers have studied how to build more scalable coun-\nters for years [MS04]. Even more amazing is the fact that scalable coun-\nters matter, as recent work in operating system performance analysis has\nshown [B+10]; without scalable counting, some workloads running on\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n314\nLOCK-BASED CONCURRENT DATA STRUCTURES\nTime\nL1\nL2\nL3\nL4\nG\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n1\n0\n2\n1\n0\n2\n1\n0\n3\n2\n0\n3\n1\n0\n4\n3\n0\n3\n2\n0\n5\n4\n1\n3\n3\n0\n6\n5 →0\n1\n3\n4\n5 (from L1)\n7\n0\n2\n4\n5 →0\n10 (from L4)\nTable 29.1: Tracing the Sloppy Counters\nLinux suffer from serious scalability problems on multicore machines.\nThough many techniques have been developed to attack this problem,\nwe’ll now describe one particular approach. The idea, introduced in re-\ncent research [B+10], is known as a sloppy counter.\nThe sloppy counter works by representing a single logical counter via\nnumerous local physical counters, one per CPU core, as well as a single\nglobal counter. Speciﬁcally, on a machine with four CPUs, there are four\nlocal counters and one global one. In addition to these counters, there are\nalso locks: one for each local counter, and one for the global counter.\nThe basic idea of sloppy counting is as follows. When a thread running\non a given core wishes to increment the counter, it increments its local\ncounter; access to this local counter is synchronized via the corresponding\nlocal lock. Because each CPU has its own local counter, threads across\nCPUs can update local counters without contention, and thus counter\nupdates are scalable.\nHowever, to keep the global counter up to date (in case a thread wishes\nto read its value), the local values are periodically transferred to the global\ncounter, by acquiring the global lock and incrementing it by the local\ncounter’s value; the local counter is then reset to zero.\nHow often this local-to-global transfer occurs is determined by a thresh-\nold, which we call S here (for sloppiness). The smaller S is, the more the\ncounter behaves like the non-scalable counter above; the bigger S is, the\nmore scalable the counter, but the further off the global value might be\nfrom the actual count. One could simply acquire all the local locks and\nthe global lock (in a speciﬁed order, to avoid deadlock) to get an exact\nvalue, but that is not scalable.\nTo make this clear, let’s look at an example (Table 29.1). In this exam-\nple, the threshold S is set to 5, and there are threads on each of four CPUs\nupdating their local counters L1 ... L4. The global counter value (G) is\nalso shown in the trace, with time increasing downward. At each time\nstep, a local counter may be incremented; if the local value reaches the\nthreshold S, the local value is transferred to the global counter and the\nlocal counter is reset.\nThe lower line in Figure 29.3 (labeled sloppy) shows the performance of\nsloppy counters with a threshold S of 1024. Performance is excellent; the\ntime taken to update the counter four million times on four processors is\nhardly higher than the time taken to update it one million times on one\nprocessor.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nLOCK-BASED CONCURRENT DATA STRUCTURES\n315\n1\ntypedef struct __counter_t {\n2\nint\nglobal;\n// global count\n3\npthread_mutex_t glock;\n// global lock\n4\nint\nlocal[NUMCPUS];\n// local count (per cpu)\n5\npthread_mutex_t llock[NUMCPUS];\n// ... and locks\n6\nint\nthreshold;\n// update frequency\n7\n} counter_t;\n8\n9\n// init: record threshold, init locks, init values\n10\n//\nof all local counts and global count\n11\nvoid init(counter_t *c, int threshold) {\n12\nc->threshold = threshold;\n13\n14\nc->global = 0;\n15\npthread_mutex_init(&c->glock, NULL);\n16\n17\nint i;\n18\nfor (i = 0; i < NUMCPUS; i++) {\n19\nc->local[i] = 0;\n20\npthread_mutex_init(&c->llock[i], NULL);\n21\n}\n22\n}\n23\n24\n// update: usually, just grab local lock and update local amount\n25\n//\nonce local count has risen by ’threshold’, grab global\n26\n//\nlock and transfer local values to it\n27\nvoid update(counter_t *c, int threadID, int amt) {\n28\npthread_mutex_lock(&c->llock[threadID]);\n29\nc->local[threadID] += amt;\n// assumes amt > 0\n30\nif (c->local[threadID] >= c->threshold) { // transfer to global\n31\npthread_mutex_lock(&c->glock);\n32\nc->global += c->local[threadID];\n33\npthread_mutex_unlock(&c->glock);\n34\nc->local[threadID] = 0;\n35\n}\n36\npthread_mutex_unlock(&c->llock[threadID]);\n37\n}\n38\n39\n// get: just return global amount (which may not be perfect)\n40\nint get(counter_t *c) {\n41\npthread_mutex_lock(&c->glock);\n42\nint val = c->global;\n43\npthread_mutex_unlock(&c->glock);\n44\nreturn val; // only approximate!\n45\n}\nFigure 29.4: Sloppy Counter Implementation\nFigure 29.5 shows the importance of the threshold value S, with four\nthreads each incrementing the counter 1 million times on four CPUs. If S\nis low, performance is poor (but the global count is always quite accurate);\nif S is high, performance is excellent, but the global count lags (by the\nnumber of CPUs multiplied by S). This accuracy/performance trade-off\nis what sloppy counters enables.\nA rough version of such a sloppy counter is found in Figure 29.4. Read\nit, or better yet, run it yourself in some experiments to better understand\nhow it works.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n316\nLOCK-BASED CONCURRENT DATA STRUCTURES\n1\n2\n4\n8\n16\n32\n64 128 256\n1024\n512\n0\n5\n10\n15\nSloppiness\nTime (seconds)\nFigure 29.5: Scaling Sloppy Counters\n29.2\nConcurrent Linked Lists\nWe next examine a more complicated structure, the linked list. Let’s\nstart with a basic approach once again. For simplicity, we’ll omit some of\nthe obvious routines that such a list would have and just focus on concur-\nrent insert; we’ll leave it to the reader to think about lookup, delete, and\nso forth. Figure 29.6 shows the code for this rudimentary data structure.\nAs you can see in the code, the code simply acquires a lock in the insert\nroutine upon entry, and releases it upon exit. One small tricky issue arises\nif malloc() happens to fail (a rare case); in this case, the code must also\nrelease the lock before failing the insert.\nThis kind of exceptional control ﬂow has been shown to be quite error\nprone; a recent study of Linux kernel patches found that a huge fraction of\nbugs (nearly 40%) are found on such rarely-taken code paths (indeed, this\nobservation sparked some of our own research, in which we removed all\nmemory-failing paths from a Linux ﬁle system, resulting in a more robust\nsystem [S+11]).\nThus, a challenge: can we rewrite the insert and lookup routines to re-\nmain correct under concurrent insert but avoid the case where the failure\npath also requires us to add the call to unlock?\nThe answer, in this case, is yes. Speciﬁcally, we can rearrange the code\na bit so that the lock and release only surround the actual critical section\nin the insert code, and that a common exit path is used in the lookup code.\nThe former works because part of the lookup actually need not be locked;\nassuming that malloc() itself is thread-safe, each thread can call into it\nwithout worry of race conditions or other concurrency bugs. Only when\nupdating the shared list does a lock need to be held. See Figure 29.7 for\nthe details of these modiﬁcations.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nLOCK-BASED CONCURRENT DATA STRUCTURES\n317\n1\n// basic node structure\n2\ntypedef struct __node_t {\n3\nint\nkey;\n4\nstruct __node_t\n*next;\n5\n} node_t;\n6\n7\n// basic list structure (one used per list)\n8\ntypedef struct __list_t {\n9\nnode_t\n*head;\n10\npthread_mutex_t\nlock;\n11\n} list_t;\n12\n13\nvoid List_Init(list_t *L) {\n14\nL->head = NULL;\n15\npthread_mutex_init(&L->lock, NULL);\n16\n}\n17\n18\nint List_Insert(list_t *L, int key) {\n19\npthread_mutex_lock(&L->lock);\n20\nnode_t *new = malloc(sizeof(node_t));\n21\nif (new == NULL) {\n22\nperror(\"malloc\");\n23\npthread_mutex_unlock(&L->lock);\n24\nreturn -1; // fail\n25\n}\n26\nnew->key\n= key;\n27\nnew->next = L->head;\n28\nL->head\n= new;\n29\npthread_mutex_unlock(&L->lock);\n30\nreturn 0; // success\n31\n}\n32\n33\nint List_Lookup(list_t *L, int key) {\n34\npthread_mutex_lock(&L->lock);\n35\nnode_t *curr = L->head;\n36\nwhile (curr) {\n37\nif (curr->key == key) {\n38\npthread_mutex_unlock(&L->lock);\n39\nreturn 0; // success\n40\n}\n41\ncurr = curr->next;\n42\n}\n43\npthread_mutex_unlock(&L->lock);\n44\nreturn -1; // failure\n45\n}\nFigure 29.6: Concurrent Linked List\nAs for the lookup routine, it is a simple code transformation to jump\nout of the main search loop to a single return path. Doing so again re-\nduces the number of lock acquire/release points in the code, and thus\ndecreases the chances of accidentally introducing bugs (such as forget-\nting to unlock before returning) into the code.\nScaling Linked Lists\nThough we again have a basic concurrent linked list, once again we\nare in a situation where it does not scale particularly well. One technique\nthat researchers have explored to enable more concurrency within a list is\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n318\nLOCK-BASED CONCURRENT DATA STRUCTURES\n1\nvoid List_Init(list_t *L) {\n2\nL->head = NULL;\n3\npthread_mutex_init(&L->lock, NULL);\n4\n}\n5\n6\nvoid List_Insert(list_t *L, int key) {\n7\n// synchronization not needed\n8\nnode_t *new = malloc(sizeof(node_t));\n9\nif (new == NULL) {\n10\nperror(\"malloc\");\n11\nreturn;\n12\n}\n13\nnew->key = key;\n14\n15\n// just lock critical section\n16\npthread_mutex_lock(&L->lock);\n17\nnew->next = L->head;\n18\nL->head\n= new;\n19\npthread_mutex_unlock(&L->lock);\n20\n}\n21\n22\nint List_Lookup(list_t *L, int key) {\n23\nint rv = -1;\n24\npthread_mutex_lock(&L->lock);\n25\nnode_t *curr = L->head;\n26\nwhile (curr) {\n27\nif (curr->key == key) {\n28\nrv = 0;\n29\nbreak;\n30\n}\n31\ncurr = curr->next;\n32\n}\n33\npthread_mutex_unlock(&L->lock);\n34\nreturn rv; // now both success and failure\n35\n}\nFigure 29.7: Concurrent Linked List: Rewritten\nsomething called hand-over-hand locking (a.k.a. lock coupling) [MS04].\nThe idea is pretty simple. Instead of having a single lock for the entire\nlist, you instead add a lock per node of the list. When traversing the\nlist, the code ﬁrst grabs the next node’s lock and then releases the current\nnode’s lock (which inspires the name hand-over-hand).\nConceptually, a hand-over-hand linked list makes some sense; it en-\nables a high degree of concurrency in list operations. However, in prac-\ntice, it is hard to make such a structure faster than the simple single lock\napproach, as the overheads of acquiring and releasing locks for each node\nof a list traversal is prohibitive. Even with very large lists, and a large\nnumber of threads, the concurrency enabled by allowing multiple on-\ngoing traversals is unlikely to be faster than simply grabbing a single\nlock, performing an operation, and releasing it. Perhaps some kind of hy-\nbrid (where you grab a new lock every so many nodes) would be worth\ninvestigating.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "page_number": 345,
      "chapter_number": 35,
      "summary": "Discusses how Dijkstra posed the original concurrency\nproblem, and Dekker’s solution Key topics include locks, counters, and listed.",
      "keywords": [
        "concurrent data structures",
        "counter",
        "lock",
        "data structures",
        "concurrent data",
        "Lock-based Concurrent Data",
        "mutex",
        "list",
        "data",
        "local counter",
        "local",
        "pthread",
        "structure",
        "concurrent",
        "Concurrent Linked List"
      ],
      "concepts": [
        "locks",
        "counters",
        "listed",
        "concurrency",
        "concurrently",
        "thread",
        "structures",
        "times",
        "value",
        "local"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 21,
          "title": "Segment 21 (pages 193-216)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 2,
          "title": "Segment 2 (pages 9-16)",
          "relevance_score": 0.59,
          "method": "api"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 23,
          "title": "Segment 23 (pages 227-245)",
          "relevance_score": 0.59,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 51,
          "title": "Segment 51 (pages 1023-1040)",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 26,
          "title": "Segment 26 (pages 270-277)",
          "relevance_score": 0.55,
          "method": "api"
        }
      ]
    },
    {
      "number": 36,
      "title": "Segment 36 (pages 355-364)",
      "start_page": 355,
      "end_page": 364,
      "detection_method": "topic_boundary",
      "content": "LOCK-BASED CONCURRENT DATA STRUCTURES\n319\nTIP: MORE CONCURRENCY ISN’T NECESSARILY FASTER\nIf the scheme you design adds a lot of overhead (for example, by acquir-\ning and releasing locks frequently, instead of once), the fact that it is more\nconcurrent may not be important. Simple schemes tend to work well,\nespecially if they use costly routines rarely. Adding more locks and com-\nplexity can be your downfall. All of that said, there is one way to really\nknow: build both alternatives (simple but less concurrent, and complex\nbut more concurrent) and measure how they do. In the end, you can’t\ncheat on performance; your idea is either faster, or it isn’t.\nTIP: BE WARY OF LOCKS AND CONTROL FLOW\nA general design tip, which is useful in concurrent code as well as\nelsewhere, is to be wary of control ﬂow changes that lead to function re-\nturns, exits, or other similar error conditions that halt the execution of\na function. Because many functions will begin by acquiring a lock, al-\nlocating some memory, or doing other similar stateful operations, when\nerrors arise, the code has to undo all of the state before returning, which\nis error-prone. Thus, it is best to structure code to minimize this pattern.\n29.3\nConcurrent Queues\nAs you know by now, there is always a standard method to make a\nconcurrent data structure: add a big lock. For a queue, we’ll skip that\napproach, assuming you can ﬁgure it out.\nInstead, we’ll take a look at a slightly more concurrent queue designed\nby Michael and Scott [MS98]. The data structures and code used for this\nqueue are found in Figure 29.8 on the following page.\nIf you study this code carefully, you’ll notice that there are two locks,\none for the head of the queue, and one for the tail. The goal of these two\nlocks is to enable concurrency of enqueue and dequeue operations. In\nthe common case, the enqueue routine will only access the tail lock, and\ndequeue only the head lock.\nOne trick used by the Michael and Scott is to add a dummy node (allo-\ncated in the queue initialization code); this dummy enables the separation\nof head and tail operations. Study the code, or better yet, type it in, run\nit, and measure it, to understand how it works deeply.\nQueues are commonly used in multi-threaded applications. However,\nthe type of queue used here (with just locks) often does not completely\nmeet the needs of such programs.\nA more fully developed bounded\nqueue, that enables a thread to wait if the queue is either empty or overly\nfull, is the subject of our intense study in the next chapter on condition\nvariables. Watch for it!\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n320\nLOCK-BASED CONCURRENT DATA STRUCTURES\n1\ntypedef struct __node_t {\n2\nint\nvalue;\n3\nstruct __node_t\n*next;\n4\n} node_t;\n5\n6\ntypedef struct __queue_t {\n7\nnode_t\n*head;\n8\nnode_t\n*tail;\n9\npthread_mutex_t\nheadLock;\n10\npthread_mutex_t\ntailLock;\n11\n} queue_t;\n12\n13\nvoid Queue_Init(queue_t *q) {\n14\nnode_t *tmp = malloc(sizeof(node_t));\n15\ntmp->next = NULL;\n16\nq->head = q->tail = tmp;\n17\npthread_mutex_init(&q->headLock, NULL);\n18\npthread_mutex_init(&q->tailLock, NULL);\n19\n}\n20\n21\nvoid Queue_Enqueue(queue_t *q, int value) {\n22\nnode_t *tmp = malloc(sizeof(node_t));\n23\nassert(tmp != NULL);\n24\ntmp->value = value;\n25\ntmp->next\n= NULL;\n26\n27\npthread_mutex_lock(&q->tailLock);\n28\nq->tail->next = tmp;\n29\nq->tail = tmp;\n30\npthread_mutex_unlock(&q->tailLock);\n31\n}\n32\n33\nint Queue_Dequeue(queue_t *q, int *value) {\n34\npthread_mutex_lock(&q->headLock);\n35\nnode_t *tmp = q->head;\n36\nnode_t *newHead = tmp->next;\n37\nif (newHead == NULL) {\n38\npthread_mutex_unlock(&q->headLock);\n39\nreturn -1; // queue was empty\n40\n}\n41\n*value = newHead->value;\n42\nq->head = newHead;\n43\npthread_mutex_unlock(&q->headLock);\n44\nfree(tmp);\n45\nreturn 0;\n46\n}\nFigure 29.8: Michael and Scott Concurrent Queue\n29.4\nConcurrent Hash Table\nWe end our discussion with a simple and widely applicable concurrent\ndata structure, the hash table. We’ll focus on a simple hash table that does\nnot resize; a little more work is required to handle resizing, which we\nleave as an exercise for the reader (sorry!).\nThis concurrent hash table is straightforward, is built using the con-\ncurrent lists we developed earlier, and works incredibly well. The reason\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nLOCK-BASED CONCURRENT DATA STRUCTURES\n321\n1\n#define BUCKETS (101)\n2\n3\ntypedef struct __hash_t {\n4\nlist_t lists[BUCKETS];\n5\n} hash_t;\n6\n7\nvoid Hash_Init(hash_t *H) {\n8\nint i;\n9\nfor (i = 0; i < BUCKETS; i++) {\n10\nList_Init(&H->lists[i]);\n11\n}\n12\n}\n13\n14\nint Hash_Insert(hash_t *H, int key) {\n15\nint bucket = key % BUCKETS;\n16\nreturn List_Insert(&H->lists[bucket], key);\n17\n}\n18\n19\nint Hash_Lookup(hash_t *H, int key) {\n20\nint bucket = key % BUCKETS;\n21\nreturn List_Lookup(&H->lists[bucket], key);\n22\n}\nFigure 29.9: A Concurrent Hash Table\nfor its good performance is that instead of having a single lock for the en-\ntire structure, it uses a lock per hash bucket (each of which is represented\nby a list). Doing so enables many concurrent operations to take place.\nFigure 29.10 shows the performance of the hash table under concur-\nrent updates (from 10,000 to 50,000 concurrent updates from each of four\nthreads, on the same iMac with four CPUs). Also shown, for the sake\nof comparison, is the performance of a linked list (with a single lock).\nAs you can see from the graph, this simple concurrent hash table scales\nmagniﬁcently; the linked list, in contrast, does not.\n0\n10\n20\n30\n40\n0\n5\n10\n15\nInserts (Thousands)\nTime (seconds)\nSimple Concurrent List\nConcurrent Hash Table\nFigure 29.10: Scaling Hash Tables\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n322\nLOCK-BASED CONCURRENT DATA STRUCTURES\nTIP: AVOID PREMATURE OPTIMIZATION (KNUTH’S LAW)\nWhen building a concurrent data structure, start with the most basic ap-\nproach, which is to add a single big lock to provide synchronized access.\nBy doing so, you are likely to build a correct lock; if you then ﬁnd that it\nsuffers from performance problems, you can reﬁne it, thus only making\nit fast if need be. As Knuth famously stated, “Premature optimization is\nthe root of all evil.”\nMany operating systems added a single lock when transitioning to multi-\nprocessors, including Sun OS and Linux. In the latter, it even had a name,\nthe big kernel lock (BKL), and was the source of performance problems\nfor many years until it was ﬁnally removed in 2011. In SunOS (which\nwas a BSD variant), the notion of removing the single lock protecting\nthe kernel was so painful that the Sun engineers decided on a different\nroute: building the entirely new Solaris operating system, which was\nmulti-threaded from day one. Read the Linux and Solaris kernel books\nfor more information [BC05, MM00].\n29.5\nSummary\nWe have introduced a sampling of concurrent data structures, from\ncounters, to lists and queues, and ﬁnally to the ubiquitous and heavily-\nused hash table. We have learned a few important lessons along the way:\nto be careful with acquisition and release of locks around control ﬂow\nchanges; that enabling more concurrency does not necessarily increase\nperformance; that performance problems should only be remedied once\nthey exist. This last point, of avoiding premature optimization, is cen-\ntral to any performance-minded developer; there is no value in making\nsomething faster if doing so will not improve the overall performance of\nthe application.\nOf course, we have just scratched the surface of high performance\nstructures. See Moir and Shavit’s excellent survey for more information,\nas well as links to other sources [MS04]. In particular, you might be inter-\nested in other structures (such as B-trees); for this knowledge, a database\nclass is your best bet. You also might be interested in techniques that don’t\nuse traditional locks at all; such non-blocking data structures are some-\nthing we’ll get a taste of in the chapter on common concurrency bugs,\nbut frankly this topic is an entire area of knowledge requiring more study\nthan is possible in this humble book. Find out more on your own if you\nare interested (as always!).\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nLOCK-BASED CONCURRENT DATA STRUCTURES\n323\nReferences\n[B+10] “An Analysis of Linux Scalability to Many Cores”\nSilas Boyd-Wickizer, Austin T. Clements, Yandong Mao, Aleksey Pesterev, M. Frans Kaashoek,\nRobert Morris, Nickolai Zeldovich\nOSDI ’10, Vancouver, Canada, October 2010\nA great study of how Linux performs on multicore machines, as well as some simple solutions.\n[BH73] “Operating System Principles”\nPer Brinch Hansen, Prentice-Hall, 1973\nAvailable: http://portal.acm.org/citation.cfm?id=540365\nOne of the ﬁrst books on operating systems; certainly ahead of its time. Introduced monitors as a\nconcurrency primitive.\n[BC05] “Understanding the Linux Kernel (Third Edition)”\nDaniel P. Bovet and Marco Cesati\nO’Reilly Media, November 2005\nThe classic book on the Linux kernel. You should read it.\n[L+13] “A Study of Linux File System Evolution”\nLanyue Lu, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau, Shan Lu\nFAST ’13, San Jose, CA, February 2013\nOur paper that studies every patch to Linux ﬁle systems over nearly a decade. Lots of fun ﬁndings in\nthere; read it to see! The work was painful to do though; the poor graduate student, Lanyue Lu, had to\nlook through every single patch by hand in order to understand what they did.\n[MS98] “Nonblocking Algorithms and Preemption-safe Locking on Multiprogrammed Shared-\nmemory Multiprocessors”\nM. Michael and M. Scott\nJournal of Parallel and Distributed Computing, Vol. 51, No. 1, 1998\nProfessor Scott and his students have been at the forefront of concurrent algorithms and data structures\nfor many years; check out his web page, numerous papers, or books to ﬁnd out more.\n[MS04] “Concurrent Data Structures”\nMark Moir and Nir Shavit\nIn Handbook of Data Structures and Applications\n(Editors D. Metha and S.Sahni)\nChapman and Hall/CRC Press, 2004\nAvailable: www.cs.tau.ac.il/˜shanir/concurrent-data-structures.pdf\nA short but relatively comprehensive reference on concurrent data structures. Though it is missing\nsome of the latest works in the area (due to its age), it remains an incredibly useful reference.\n[MM00] “Solaris Internals: Core Kernel Architecture”\nJim Mauro and Richard McDougall\nPrentice Hall, October 2000\nThe Solaris book. You should also read this, if you want to learn in great detail about something other\nthan Linux.\n[S+11] “Making the Common Case the Only Case with Anticipatory Memory Allocation”\nSwaminathan Sundararaman, Yupu Zhang, Sriram Subramanian,\nAndrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau\nFAST ’11, San Jose, CA, February 2011\nOur work on removing possibly-failing calls to malloc from kernel code paths. The idea is to allocate all\npotentially needed memory before doing any of the work, thus avoiding failure deep down in the storage\nstack.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n30\nCondition Variables\nThus far we have developed the notion of a lock and seen how one can be\nproperly built with the right combination of hardware and OS support.\nUnfortunately, locks are not the only primitives that are needed to build\nconcurrent programs.\nIn particular, there are many cases where a thread wishes to check\nwhether a condition is true before continuing its execution. For example,\na parent thread might wish to check whether a child thread has completed\nbefore continuing (this is often called a join()); how should such a wait\nbe implemented? Let’s look at Figure 30.1.\n1\nvoid *child(void *arg) {\n2\nprintf(\"child\\n\");\n3\n// XXX how to indicate we are done?\n4\nreturn NULL;\n5\n}\n6\n7\nint main(int argc, char *argv[]) {\n8\nprintf(\"parent: begin\\n\");\n9\npthread_t c;\n10\nPthread_create(&c, NULL, child, NULL); // create child\n11\n// XXX how to wait for child?\n12\nprintf(\"parent: end\\n\");\n13\nreturn 0;\n14\n}\nFigure 30.1: A Parent Waiting For Its Child\nWhat we would like to see here is the following output:\nparent: begin\nchild\nparent: end\nWe could try using a shared variable, as you see in Figure 30.2. This\nsolution will generally work, but it is hugely inefﬁcient as the parent spins\nand wastes CPU time. What we would like here instead is some way to\nput the parent to sleep until the condition we are waiting for (e.g., the\nchild is done executing) comes true.\n325\n\n\n326\nCONDITION VARIABLES\n1\nvolatile int done = 0;\n2\n3\nvoid *child(void *arg) {\n4\nprintf(\"child\\n\");\n5\ndone = 1;\n6\nreturn NULL;\n7\n}\n8\n9\nint main(int argc, char *argv[]) {\n10\nprintf(\"parent: begin\\n\");\n11\npthread_t c;\n12\nPthread_create(&c, NULL, child, NULL); // create child\n13\nwhile (done == 0)\n14\n; // spin\n15\nprintf(\"parent: end\\n\");\n16\nreturn 0;\n17\n}\nFigure 30.2: Parent Waiting For Child: Spin-based Approach\nTHE CRUX: HOW TO WAIT FOR A CONDITION\nIn multi-threaded programs, it is often useful for a thread to wait for\nsome condition to become true before proceeding. The simple approach,\nof just spinning until the condition becomes true, is grossly inefﬁcient\nand wastes CPU cycles, and in some cases, can be incorrect. Thus, how\nshould a thread wait for a condition?\n30.1\nDeﬁnition and Routines\nTo wait for a condition to become true, a thread can make use of what\nis known as a condition variable. A condition variable is an explicit\nqueue that threads can put themselves on when some state of execution\n(i.e., some condition) is not as desired (by waiting on the condition);\nsome other thread, when it changes said state, can then wake one (or\nmore) of those waiting threads and thus allow them to continue (by sig-\nnaling on the condition). The idea goes back to Dijkstra’s use of “private\nsemaphores” [D68]; a similar idea was later named a “condition variable”\nby Hoare in his work on monitors [H74].\nTo declare such a condition variable, one simply writes something\nlike this: pthread cond t c;, which declares c as a condition variable\n(note: proper initialization is also required). A condition variable has two\noperations associated with it: wait() and signal(). The wait() call\nis executed when a thread wishes to put itself to sleep; the signal() call\nis executed when a thread has changed something in the program and\nthus wants to wake a sleeping thread waiting on this condition. Speciﬁ-\ncally, the POSIX calls look like this:\npthread_cond_wait(pthread_cond_t *c, pthread_mutex_t *m);\npthread_cond_signal(pthread_cond_t *c);\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nCONDITION VARIABLES\n327\n1\nint done\n= 0;\n2\npthread_mutex_t m = PTHREAD_MUTEX_INITIALIZER;\n3\npthread_cond_t c\n= PTHREAD_COND_INITIALIZER;\n4\n5\nvoid thr_exit() {\n6\nPthread_mutex_lock(&m);\n7\ndone = 1;\n8\nPthread_cond_signal(&c);\n9\nPthread_mutex_unlock(&m);\n10\n}\n11\n12\nvoid *child(void *arg) {\n13\nprintf(\"child\\n\");\n14\nthr_exit();\n15\nreturn NULL;\n16\n}\n17\n18\nvoid thr_join() {\n19\nPthread_mutex_lock(&m);\n20\nwhile (done == 0)\n21\nPthread_cond_wait(&c, &m);\n22\nPthread_mutex_unlock(&m);\n23\n}\n24\n25\nint main(int argc, char *argv[]) {\n26\nprintf(\"parent: begin\\n\");\n27\npthread_t p;\n28\nPthread_create(&p, NULL, child, NULL);\n29\nthr_join();\n30\nprintf(\"parent: end\\n\");\n31\nreturn 0;\n32\n}\nFigure 30.3: Parent Waiting For Child: Use A Condition Variable\nWe will often refer to these as wait() and signal() for simplicity.\nOne thing you might notice about the wait() call is that it also takes a\nmutex as a parameter; it assumes that this mutex is locked when wait()\nis called. The responsibility of wait() is to release the lock and put the\ncalling thread to sleep (atomically); when the thread wakes up (after some\nother thread has signaled it), it must re-acquire the lock before returning\nto the caller. This complexity stems from the desire to prevent certain\nrace conditions from occurring when a thread is trying to put itself to\nsleep. Let’s take a look at the solution to the join problem (Figure 30.3) to\nunderstand this better.\nThere are two cases to consider. In the ﬁrst, the parent creates the child\nthread but continues running itself (assume we have only a single pro-\ncessor) and thus immediately calls into thr join() to wait for the child\nthread to complete. In this case, it will acquire the lock, check if the child\nis done (it is not), and put itself to sleep by calling wait() (hence releas-\ning the lock). The child will eventually run, print the message “child”,\nand call thr exit() to wake the parent thread; this code just grabs the\nlock, sets the state variable done, and signals the parent thus waking it.\nFinally, the parent will run (returning from wait() with the lock held),\nunlock the lock, and print the ﬁnal message “parent: end”.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n328\nCONDITION VARIABLES\nIn the second case, the child runs immediately upon creation, sets\ndone to 1, calls signal to wake a sleeping thread (but there is none, so\nit just returns), and is done. The parent then runs, calls thr join(), sees\nthat done is 1, and thus does not wait and returns.\nOne last note: you might observe the parent uses a while loop instead\nof just an if statement when deciding whether to wait on the condition.\nWhile this does not seem strictly necessary per the logic of the program,\nit is always a good idea, as we will see below.\nTo make sure you understand the importance of each piece of the\nthr exit() and thr join() code, let’s try a few alternate implemen-\ntations. First, you might be wondering if we need the state variable done.\nWhat if the code looked like the example below? Would this work?\n1\nvoid thr_exit() {\n2\nPthread_mutex_lock(&m);\n3\nPthread_cond_signal(&c);\n4\nPthread_mutex_unlock(&m);\n5\n}\n6\n7\nvoid thr_join() {\n8\nPthread_mutex_lock(&m);\n9\nPthread_cond_wait(&c, &m);\n10\nPthread_mutex_unlock(&m);\n11\n}\nUnfortunately this approach is broken. Imagine the case where the\nchild runs immediately and calls thr exit() immediately; in this case,\nthe child will signal, but there is no thread asleep on the condition. When\nthe parent runs, it will simply call wait and be stuck; no thread will ever\nwake it. From this example, you should appreciate the importance of\nthe state variable done; it records the value the threads are interested in\nknowing. The sleeping, waking, and locking all are built around it.\nHere is another poor implementation. In this example, we imagine\nthat one does not need to hold a lock in order to signal and wait. What\nproblem could occur here? Think about it!\n1\nvoid thr_exit() {\n2\ndone = 1;\n3\nPthread_cond_signal(&c);\n4\n}\n5\n6\nvoid thr_join() {\n7\nif (done == 0)\n8\nPthread_cond_wait(&c);\n9\n}\nThe issue here is a subtle race condition. Speciﬁcally, if the parent calls\nthr join() and then checks the value of done, it will see that it is 0 and\nthus try to go to sleep. But just before it calls wait to go to sleep, the parent\nis interrupted, and the child runs. The child changes the state variable\ndone to 1 and signals, but no thread is waiting and thus no thread is\nwoken. When the parent runs again, it sleeps forever, which is sad.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "page_number": 355,
      "chapter_number": 36,
      "summary": "This chapter covers segment 36 (pages 355-364). Key topics include lock, concurrent, and concurrency. Because many functions will begin by acquiring a lock, al-\nlocating some memory, or doing other similar stateful operations, when\nerrors arise, the code has to undo all of the state before returning, which\nis error-prone.",
      "keywords": [
        "CONCURRENT DATA STRUCTURES",
        "Concurrent Hash Table",
        "CONCURRENT DATA",
        "pthread",
        "DATA STRUCTURES",
        "CONCURRENT",
        "condition",
        "Hash Table",
        "child",
        "lock",
        "parent",
        "Hash",
        "Concurrent Hash",
        "NULL",
        "LOCK-BASED CONCURRENT DATA"
      ],
      "concepts": [
        "lock",
        "concurrent",
        "concurrency",
        "structures",
        "operations",
        "operating",
        "queues",
        "parent",
        "void",
        "conditions"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 2,
          "title": "Segment 2 (pages 9-16)",
          "relevance_score": 0.76,
          "method": "api"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 21,
          "title": "Segment 21 (pages 193-216)",
          "relevance_score": 0.76,
          "method": "api"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 37,
          "title": "Segment 37 (pages 363-372)",
          "relevance_score": 0.75,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 51,
          "title": "Segment 51 (pages 1023-1040)",
          "relevance_score": 0.74,
          "method": "api"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 23,
          "title": "Segment 23 (pages 227-245)",
          "relevance_score": 0.72,
          "method": "api"
        }
      ]
    },
    {
      "number": 37,
      "title": "Segment 37 (pages 365-372)",
      "start_page": 365,
      "end_page": 372,
      "detection_method": "topic_boundary",
      "content": "CONDITION VARIABLES\n329\nTIP: ALWAYS HOLD THE LOCK WHILE SIGNALING\nAlthough it is strictly not necessary in all cases, it is likely simplest and\nbest to hold the lock while signaling when using condition variables. The\nexample above shows a case where you must hold the lock for correct-\nness; however, there are some other cases where it is likely OK not to, but\nprobably is something you should avoid. Thus, for simplicity, hold the\nlock when calling signal.\nThe converse of this tip, i.e., hold the lock when calling wait, is not just\na tip, but rather mandated by the semantics of wait, because wait always\n(a) assumes the lock is held when you call it, (b) releases said lock when\nputting the caller to sleep, and (c) re-acquires the lock just before return-\ning. Thus, the generalization of this tip is correct: hold the lock when\ncalling signal or wait, and you will always be in good shape.\nHopefully, from this simple join example, you can see some of the ba-\nsic requirements of using condition variables properly. To make sure you\nunderstand, we now go through a more complicated example: the pro-\nducer/consumer or bounded-buffer problem.\n30.2\nThe Producer/Consumer (Bound Buffer) Problem\nThe next synchronization problem we will confront in this chapter is\nknown as the producer/consumer problem, or sometimes as the bounded\nbuffer problem, which was ﬁrst posed by Dijkstra [D72]. Indeed, it was\nthis very producer/consumer problem that led Dijkstra and his co-workers\nto invent the generalized semaphore (which can be used as either a lock\nor a condition variable) [D01]; we will learn more about semaphores later.\nImagine one or more producer threads and one or more consumer\nthreads. Producers produce data items and wish to place them in a buffer;\nconsumers grab data items out of the buffer consume them in some way.\nThis arrangement occurs in many real systems.\nFor example, in a\nmulti-threaded web server, a producer puts HTTP requests into a work\nqueue (i.e., the bounded buffer); consumer threads take requests out of\nthis queue and process them.\nA bounded buffer is also used when you pipe the output of one pro-\ngram into another, e.g., grep foo file.txt | wc -l. This example\nruns two processes concurrently; grep writes lines from file.txt with\nthe string foo in them to what it thinks is standard output; the UNIX\nshell redirects the output to what is called a UNIX pipe (created by the\npipe system call). The other end of this pipe is connected to the stan-\ndard input of the process wc, which simply counts the number of lines in\nthe input stream and prints out the result. Thus, the grep process is the\nproducer; the wc process is the consumer; between them is an in-kernel\nbounded buffer; you, in this example, are just the happy user.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n330\nCONDITION VARIABLES\n1\nint buffer;\n2\nint count = 0; // initially, empty\n3\n4\nvoid put(int value) {\n5\nassert(count == 0);\n6\ncount = 1;\n7\nbuffer = value;\n8\n}\n9\n10\nint get() {\n11\nassert(count == 1);\n12\ncount = 0;\n13\nreturn buffer;\n14\n}\nFigure 30.4: The Put and Get Routines (Version 1)\n1\nvoid *producer(void *arg) {\n2\nint i;\n3\nint loops = (int) arg;\n4\nfor (i = 0; i < loops; i++) {\n5\nput(i);\n6\n}\n7\n}\n8\n9\nvoid *consumer(void *arg) {\n10\nint i;\n11\nwhile (1) {\n12\nint tmp = get();\n13\nprintf(\"%d\\n\", tmp);\n14\n}\n15\n}\nFigure 30.5: Producer/Consumer Threads (Version 1)\nBecause the bounded buffer is a shared resource, we must of course\nrequire synchronized access to it, lest1 a race condition arise. To begin to\nunderstand this problem better, let us examine some actual code.\nThe ﬁrst thing we need is a shared buffer, into which a producer puts\ndata, and out of which a consumer takes data. Let’s just use a single\ninteger for simplicity (you can certainly imagine placing a pointer to a\ndata structure into this slot instead), and the two inner routines to put\na value into the shared buffer, and to get a value out of the buffer. See\nFigure 30.4 for details.\nPretty simple, no? The put() routine assumes the buffer is empty\n(and checks this with an assertion), and then simply puts a value into the\nshared buffer and marks it full by setting count to 1. The get() routine\ndoes the opposite, setting the buffer to empty (i.e., setting count to 0)\nand returning the value. Don’t worry that this shared buffer has just a\nsingle entry; later, we’ll generalize it to a queue that can hold multiple\nentries, which will be even more fun than it sounds.\nNow we need to write some routines that know when it is OK to access\nthe buffer to either put data into it or get data out of it. The conditions for\n1This is where we drop some serious Old English on you, and the subjunctive form.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nCONDITION VARIABLES\n331\n1\ncond_t\ncond;\n2\nmutex_t mutex;\n3\n4\nvoid *producer(void *arg) {\n5\nint i;\n6\nfor (i = 0; i < loops; i++) {\n7\nPthread_mutex_lock(&mutex);\n// p1\n8\nif (count == 1)\n// p2\n9\nPthread_cond_wait(&cond, &mutex); // p3\n10\nput(i);\n// p4\n11\nPthread_cond_signal(&cond);\n// p5\n12\nPthread_mutex_unlock(&mutex);\n// p6\n13\n}\n14\n}\n15\n16\nvoid *consumer(void *arg) {\n17\nint i;\n18\nfor (i = 0; i < loops; i++) {\n19\nPthread_mutex_lock(&mutex);\n// c1\n20\nif (count == 0)\n// c2\n21\nPthread_cond_wait(&cond, &mutex); // c3\n22\nint tmp = get();\n// c4\n23\nPthread_cond_signal(&cond);\n// c5\n24\nPthread_mutex_unlock(&mutex);\n// c6\n25\nprintf(\"%d\\n\", tmp);\n26\n}\n27\n}\nFigure 30.6: Producer/Consumer: Single CV and If Statement\nthis should be obvious: only put data into the buffer when count is zero\n(i.e., when the buffer is empty), and only get data from the buffer when\ncount is one (i.e., when the buffer is full). If we write the synchronization\ncode such that a producer puts data into a full buffer, or a consumer gets\ndata from an empty one, we have done something wrong (and in this\ncode, an assertion will ﬁre).\nThis work is going to be done by two types of threads, one set of which\nwe’ll call the producer threads, and the other set which we’ll call con-\nsumer threads. Figure 30.5 shows the code for a producer that puts an\ninteger into the shared buffer loops number of times, and a consumer\nthat gets the data out of that shared buffer (forever), each time printing\nout the data item it pulled from the shared buffer.\nA Broken Solution\nNow imagine that we have just a single producer and a single consumer.\nObviously the put() and get() routines have critical sections within\nthem, as put() updates the buffer, and get() reads from it. However,\nputting a lock around the code doesn’t work; we need something more.\nNot surprisingly, that something more is some condition variables. In this\n(broken) ﬁrst try (Figure 30.6), we have a single condition variable cond\nand associated lock mutex.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n332\nCONDITION VARIABLES\nTc1\nState\nTc2\nState\nTp\nState\nCount\nComment\nc1\nRunning\nReady\nReady\n0\nc2\nRunning\nReady\nReady\n0\nc3\nSleep\nReady\nReady\n0\nNothing to get\nSleep\nReady\np1\nRunning\n0\nSleep\nReady\np2\nRunning\n0\nSleep\nReady\np4\nRunning\n1\nBuffer now full\nReady\nReady\np5\nRunning\n1\nTc1 awoken\nReady\nReady\np6\nRunning\n1\nReady\nReady\np1\nRunning\n1\nReady\nReady\np2\nRunning\n1\nReady\nReady\np3\nSleep\n1\nBuffer full; sleep\nReady\nc1\nRunning\nSleep\n1\nTc2 sneaks in ...\nReady\nc2\nRunning\nSleep\n1\nReady\nc4\nRunning\nSleep\n0\n... and grabs data\nReady\nc5\nRunning\nReady\n0\nTp awoken\nReady\nc6\nRunning\nReady\n0\nc4\nRunning\nReady\nReady\n0\nOh oh! No data\nTable 30.1: Thread Trace: Broken Solution (Version 1)\nLet’s examine the signaling logic between producers and consumers.\nWhen a producer wants to ﬁll the buffer, it waits for it to be empty (p1–\np3). The consumer has the exact same logic, but waits for a different\ncondition: fullness (c1–c3).\nWith just a single producer and a single consumer, the code in Figure\n30.6 works. However, if we have more than one of these threads (e.g.,\ntwo consumers), the solution has two critical problems. What are they?\n... (pause here to think) ...\nLet’s understand the ﬁrst problem, which has to do with the if state-\nment before the wait. Assume there are two consumers (Tc1 and Tc2) and\none producer (Tp). First, a consumer (Tc1) runs; it acquires the lock (c1),\nchecks if any buffers are ready for consumption (c2), and ﬁnding that\nnone are, waits (c3) (which releases the lock).\nThen the producer (Tp) runs. It acquires the lock (p1), checks if all\nbuffers are full (p2), and ﬁnding that not to be the case, goes ahead and\nﬁlls the buffer (p4). The producer then signals that a buffer has been\nﬁlled (p5). Critically, this moves the ﬁrst consumer (Tc1) from sleeping\non a condition variable to the ready queue; Tc1 is now able to run (but\nnot yet running). The producer then continues until realizing the buffer\nis full, at which point it sleeps (p6, p1–p3).\nHere is where the problem occurs: another consumer (Tc2) sneaks in\nand consumes the one existing value in the buffer (c1, c2, c4, c5, c6, skip-\nping the wait at c3 because the buffer is full). Now assume Tc1 runs; just\nbefore returning from the wait, it re-acquires the lock and then returns. It\nthen calls get() (c4), but there are no buffers to consume! An assertion\ntriggers, and the code has not functioned as desired. Clearly, we should\nhave somehow prevented Tc1 from trying to consume because Tc2 snuck\nin and consumed the one value in the buffer that had been produced. Ta-\nble 30.1 shows the action each thread takes, as well as its scheduler state\n(Ready, Running, or Sleeping) over time.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nCONDITION VARIABLES\n333\n1\ncond_t\ncond;\n2\nmutex_t mutex;\n3\n4\nvoid *producer(void *arg) {\n5\nint i;\n6\nfor (i = 0; i < loops; i++) {\n7\nPthread_mutex_lock(&mutex);\n// p1\n8\nwhile (count == 1)\n// p2\n9\nPthread_cond_wait(&cond, &mutex); // p3\n10\nput(i);\n// p4\n11\nPthread_cond_signal(&cond);\n// p5\n12\nPthread_mutex_unlock(&mutex);\n// p6\n13\n}\n14\n}\n15\n16\nvoid *consumer(void *arg) {\n17\nint i;\n18\nfor (i = 0; i < loops; i++) {\n19\nPthread_mutex_lock(&mutex);\n// c1\n20\nwhile (count == 0)\n// c2\n21\nPthread_cond_wait(&cond, &mutex); // c3\n22\nint tmp = get();\n// c4\n23\nPthread_cond_signal(&cond);\n// c5\n24\nPthread_mutex_unlock(&mutex);\n// c6\n25\nprintf(\"%d\\n\", tmp);\n26\n}\n27\n}\nFigure 30.7: Producer/Consumer: Single CV and While\nThe problem arises for a simple reason: after the producer woke Tc1,\nbut before Tc1 ever ran, the state of the bounded buffer changed (thanks to\nTc2). Signaling a thread only wakes them up; it is thus a hint that the state\nof the world has changed (in this case, that a value has been placed in the\nbuffer), but there is no guarantee that when the woken thread runs, the\nstate will still be as desired. This interpretation of what a signal means\nis often referred to as Mesa semantics, after the ﬁrst research that built\na condition variable in such a manner [LR80]; the contrast, referred to as\nHoare semantics, is harder to build but provides a stronger guarantee\nthat the woken thread will run immediately upon being woken [H74].\nVirtually every system ever built employs Mesa semantics.\nBetter, But Still Broken: While, Not If\nFortunately, this ﬁx is easy (Figure 30.7): change the if to a while. Think\nabout why this works; now consumer Tc1 wakes up and (with the lock\nheld) immediately re-checks the state of the shared variable (c2). If the\nbuffer is empty at that point, the consumer simply goes back to sleep\n(c3). The corollary if is also changed to a while in the producer (p2).\nThanks to Mesa semantics, a simple rule to remember with condition\nvariables is to always use while loops. Sometimes you don’t have to re-\ncheck the condition, but it is always safe to do so; just do it and be happy.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n334\nCONDITION VARIABLES\nTc1\nState\nTc2\nState\nTp\nState\nCount\nComment\nc1\nRunning\nReady\nReady\n0\nc2\nRunning\nReady\nReady\n0\nc3\nSleep\nReady\nReady\n0\nNothing to get\nSleep\nc1\nRunning\nReady\n0\nSleep\nc2\nRunning\nReady\n0\nSleep\nc3\nSleep\nReady\n0\nNothing to get\nSleep\nSleep\np1\nRunning\n0\nSleep\nSleep\np2\nRunning\n0\nSleep\nSleep\np4\nRunning\n1\nBuffer now full\nReady\nSleep\np5\nRunning\n1\nTc1 awoken\nReady\nSleep\np6\nRunning\n1\nReady\nSleep\np1\nRunning\n1\nReady\nSleep\np2\nRunning\n1\nReady\nSleep\np3\nSleep\n1\nMust sleep (full)\nc2\nRunning\nSleep\nSleep\n1\nRecheck condition\nc4\nRunning\nSleep\nSleep\n0\nTc1 grabs data\nc5\nRunning\nReady\nSleep\n0\nOops! Woke Tc2\nc6\nRunning\nReady\nSleep\n0\nc1\nRunning\nReady\nSleep\n0\nc2\nRunning\nReady\nSleep\n0\nc3\nSleep\nReady\nSleep\n0\nNothing to get\nSleep\nc2\nRunning\nSleep\n0\nSleep\nc3\nSleep\nSleep\n0\nEveryone asleep...\nTable 30.2: Thread Trace: Broken Solution (Version 2)\nHowever, this code still has a bug, the second of two problems men-\ntioned above. Can you see it? It has something to do with the fact that\nthere is only one condition variable. Try to ﬁgure out what the problem\nis, before reading ahead. DO IT!\n... (another pause for you to think, or close your eyes for a bit) ...\nLet’s conﬁrm you ﬁgured it out correctly, or perhaps let’s conﬁrm that\nyou are now awake and reading this part of the book. The problem oc-\ncurs when two consumers run ﬁrst (Tc1 and Tc2), and both go to sleep\n(c3). Then, a producer runs, put a value in the buffer, wakes one of the\nconsumers (say Tc1), and goes back to sleep. Now we have one consumer\nready to run (Tc1), and two threads sleeping on a condition (Tc2 and Tp).\nAnd we are about to cause a problem to occur: things are getting exciting!\nThe consumer Tc1 then wakes by returning from wait() (c3), re-checks\nthe condition (c2), and ﬁnding the buffer full, consumes the value (c4).\nThis consumer then, critically, signals on the condition (c5), waking one\nthread that is sleeping. However, which thread should it wake?\nBecause the consumer has emptied the buffer, it clearly should wake\nthe producer. However, if it wakes the consumer Tc2 (which is deﬁnitely\npossible, depending on how the wait queue is managed), we have a prob-\nlem.\nSpeciﬁcally, the consumer Tc2 will wake up and ﬁnd the buffer\nempty (c2), and go back to sleep (c3). The producer Tp, which has a value\nto put into the buffer, is left sleeping. The other consumer thread, Tc1,\nalso goes back to sleep. All three threads are left sleeping, a clear bug; see\nTable 30.2 for the brutal step-by-step of this terrible calamity.\nSignaling is clearly needed, but must be more directed. A consumer\nshould not wake other consumers, only producers, and vice-versa.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nCONDITION VARIABLES\n335\n1\ncond_t\nempty, fill;\n2\nmutex_t mutex;\n3\n4\nvoid *producer(void *arg) {\n5\nint i;\n6\nfor (i = 0; i < loops; i++) {\n7\nPthread_mutex_lock(&mutex);\n8\nwhile (count == 1)\n9\nPthread_cond_wait(&empty, &mutex);\n10\nput(i);\n11\nPthread_cond_signal(&fill);\n12\nPthread_mutex_unlock(&mutex);\n13\n}\n14\n}\n15\n16\nvoid *consumer(void *arg) {\n17\nint i;\n18\nfor (i = 0; i < loops; i++) {\n19\nPthread_mutex_lock(&mutex);\n20\nwhile (count == 0)\n21\nPthread_cond_wait(&fill, &mutex);\n22\nint tmp = get();\n23\nPthread_cond_signal(&empty);\n24\nPthread_mutex_unlock(&mutex);\n25\nprintf(\"%d\\n\", tmp);\n26\n}\n27\n}\nFigure 30.8: Producer/Consumer: Two CVs and While\nThe Single Buffer Producer/Consumer Solution\nThe solution here is once again a small one: use two condition variables,\ninstead of one, in order to properly signal which type of thread should\nwake up when the state of the system changes. Figure 30.8 shows the\nresulting code.\nIn the code above, producer threads wait on the condition empty, and\nsignals ﬁll. Conversely, consumer threads wait on ﬁll and signal empty.\nBy doing so, the second problem above is avoided by design: a consumer\ncan never accidentally wake a consumer, and a producer can never acci-\ndentally wake a producer.\nThe Final Producer/Consumer Solution\nWe now have a working producer/consumer solution, albeit not a fully\ngeneral one. The last change we make is to enable more concurrency and\nefﬁciency; speciﬁcally, we add more buffer slots, so that multiple values\ncan be produced before sleeping, and similarly multiple values can be\nconsumed before sleeping. With just a single producer and consumer, this\napproach is more efﬁcient as it reduces context switches; with multiple\nproducers or consumers (or both), it even allows concurrent producing\nor consuming to take place, thus increasing concurrency. Fortunately, it\nis a small change from our current solution.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n336\nCONDITION VARIABLES\n1\nint buffer[MAX];\n2\nint fill\n= 0;\n3\nint use\n= 0;\n4\nint count = 0;\n5\n6\nvoid put(int value) {\n7\nbuffer[fill] = value;\n8\nfill = (fill + 1) % MAX;\n9\ncount++;\n10\n}\n11\n12\nint get() {\n13\nint tmp = buffer[use];\n14\nuse = (use + 1) % MAX;\n15\ncount--;\n16\nreturn tmp;\n17\n}\nFigure 30.9: The Final Put and Get Routines\n1\ncond_t empty, fill;\n2\nmutex_t mutex;\n3\n4\nvoid *producer(void *arg) {\n5\nint i;\n6\nfor (i = 0; i < loops; i++) {\n7\nPthread_mutex_lock(&mutex);\n// p1\n8\nwhile (count == MAX)\n// p2\n9\nPthread_cond_wait(&empty, &mutex); // p3\n10\nput(i);\n// p4\n11\nPthread_cond_signal(&fill);\n// p5\n12\nPthread_mutex_unlock(&mutex);\n// p6\n13\n}\n14\n}\n15\n16\nvoid *consumer(void *arg) {\n17\nint i;\n18\nfor (i = 0; i < loops; i++) {\n19\nPthread_mutex_lock(&mutex);\n// c1\n20\nwhile (count == 0)\n// c2\n21\nPthread_cond_wait(&fill, &mutex);\n// c3\n22\nint tmp = get();\n// c4\n23\nPthread_cond_signal(&empty);\n// c5\n24\nPthread_mutex_unlock(&mutex);\n// c6\n25\nprintf(\"%d\\n\", tmp);\n26\n}\n27\n}\nFigure 30.10: The Final Working Solution\nThe ﬁrst change for this ﬁnal solution is within the buffer structure\nitself and the corresponding put() and get() (Figure 30.9). We also\nslightly change the conditions that producers and consumers check in or-\nder to determine whether to sleep or not. Figure 30.10 shows the ﬁnal\nwaiting and signaling logic. A producer only sleeps if all buffers are cur-\nrently ﬁlled (p2); similarly, a consumer only sleeps if all buffers are cur-\nrently empty (c2). And thus we solve the producer/consumer problem.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "page_number": 365,
      "chapter_number": 37,
      "summary": "30.2\nThe Producer/Consumer (Bound Buffer) Problem\nThe next synchronization problem we will confront in this chapter is\nknown as the producer/consumer problem, or sometimes as the bounded\nbuffer problem, which was ﬁrst posed by Dijkstra [D72] Key topics include buffer, consumer, and consume.",
      "keywords": [
        "Buffer",
        "mutex",
        "consumer",
        "Producer",
        "sleep",
        "Ready",
        "CONDITION VARIABLES",
        "Pthread",
        "CONDITION",
        "Running",
        "int",
        "cond",
        "LOCK",
        "void",
        "count"
      ],
      "concepts": [
        "buffer",
        "consumer",
        "consume",
        "producer",
        "threads",
        "problem",
        "counts",
        "condition",
        "conditions",
        "runs"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 51,
          "title": "Segment 51 (pages 1023-1040)",
          "relevance_score": 0.56,
          "method": "api"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 37,
          "title": "Segment 37 (pages 363-372)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 34,
          "title": "Segment 34 (pages 338-346)",
          "relevance_score": 0.53,
          "method": "api"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 10,
          "title": "Segment 10 (pages 75-82)",
          "relevance_score": 0.53,
          "method": "api"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 2,
          "title": "Segment 2 (pages 9-16)",
          "relevance_score": 0.52,
          "method": "api"
        }
      ]
    },
    {
      "number": 38,
      "title": "Segment 38 (pages 373-387)",
      "start_page": 373,
      "end_page": 387,
      "detection_method": "topic_boundary",
      "content": "CONDITION VARIABLES\n337\nTIP: USE WHILE (NOT IF) FOR CONDITIONS\nWhen checking for a condition in a multi-threaded program, using\na while loop is always correct; using an if statement only might be,\ndepending on the semantics of signaling. Thus, always use while and\nyour code will behave as expected.\nUsing while loops around conditional checks also handles the case\nwhere spurious wakeups occur. In some thread packages, due to de-\ntails of the implementation, it is possible that two threads get woken up\nthough just a single signal has taken place [L11]. Spurious wakeups are\nfurther reason to re-check the condition a thread is waiting on.\n30.3\nCovering Conditions\nWe’ll now look at one more example of how condition variables can\nbe used. This code study is drawn from Lampson and Redell’s paper on\nPilot [LR80], the same group who ﬁrst implemented the Mesa semantics\ndescribed above (the language they used was Mesa, hence the name).\nThe problem they ran into is best shown via simple example, in this\ncase in a simple multi-threaded memory allocation library. Figure 30.11\nshows a code snippet which demonstrates the issue.\nAs you might see in the code, when a thread calls into the memory\nallocation code, it might have to wait in order for more memory to be-\ncome free. Conversely, when a thread frees memory, it signals that more\nmemory is free. However, our code above has a problem: which waiting\nthread (there can be more than one) should be woken up?\nConsider the following scenario. Assume there are zero bytes free;\nthread Ta calls allocate(100), followed by thread Tb which asks for\nless memory by calling allocate(10). Both Ta and Tb thus wait on the\ncondition and go to sleep; there aren’t enough free bytes to satisfy either\nof these requests.\nAt that point, assume a third thread, Tc, calls free(50). Unfortu-\nnately, when it calls signal to wake a waiting thread, it might not wake\nthe correct waiting thread, Tb, which is waiting for only 10 bytes to be\nfreed; Ta should remain waiting, as not enough memory is yet free. Thus,\nthe code in the ﬁgure does not work, as the thread waking other threads\ndoes not know which thread (or threads) to wake up.\nThe solution suggested by Lampson and Redell is straightforward: re-\nplace the pthread cond signal() call in the code above with a call to\npthread cond broadcast(), which wakes up all waiting threads. By\ndoing so, we guarantee that any threads that should be woken are. The\ndownside, of course, can be a negative performance impact, as we might\nneedlessly wake up many other waiting threads that shouldn’t (yet) be\nawake. Those threads will simply wake up, re-check the condition, and\nthen go immediately back to sleep.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n338\nCONDITION VARIABLES\n1\n// how many bytes of the heap are free?\n2\nint bytesLeft = MAX_HEAP_SIZE;\n3\n4\n// need lock and condition too\n5\ncond_t\nc;\n6\nmutex_t m;\n7\n8\nvoid *\n9\nallocate(int size) {\n10\nPthread_mutex_lock(&m);\n11\nwhile (bytesLeft < size)\n12\nPthread_cond_wait(&c, &m);\n13\nvoid *ptr = ...; // get mem from heap\n14\nbytesLeft -= size;\n15\nPthread_mutex_unlock(&m);\n16\nreturn ptr;\n17\n}\n18\n19\nvoid free(void *ptr, int size) {\n20\nPthread_mutex_lock(&m);\n21\nbytesLeft += size;\n22\nPthread_cond_signal(&c); // whom to signal??\n23\nPthread_mutex_unlock(&m);\n24\n}\nFigure 30.11: Covering Conditions: An Example\nLampson and Redell call such a condition a covering condition, as it\ncovers all the cases where a thread needs to wake up (conservatively);\nthe cost, as we’ve discussed, is that too many threads might be woken.\nThe astute reader might also have noticed we could have used this ap-\nproach earlier (see the producer/consumer problem with only a single\ncondition variable). However, in that case, a better solution was avail-\nable to us, and thus we used it. In general, if you ﬁnd that your program\nonly works when you change your signals to broadcasts (but you don’t\nthink it should need to), you probably have a bug; ﬁx it! But in cases like\nthe memory allocator above, broadcast may be the most straightforward\nsolution available.\n30.4\nSummary\nWe have seen the introduction of another important synchronization\nprimitive beyond locks: condition variables. By allowing threads to sleep\nwhen some program state is not as desired, CVs enable us to neatly solve\na number of important synchronization problems, including the famous\n(and still important) producer/consumer problem, as well as covering\nconditions. A more dramatic concluding sentence would go here, such as\n“He loved Big Brother” [O49].\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nCONDITION VARIABLES\n339\nReferences\n[D72] “Information Streams Sharing a Finite Buffer”\nE.W. Dijkstra\nInformation Processing Letters 1: 179180, 1972\nAvailable: http://www.cs.utexas.edu/users/EWD/ewd03xx/EWD329.PDF\nThe famous paper that introduced the producer/consumer problem.\n[D01] “My recollections of operating system design”\nE.W. Dijkstra\nApril, 2001\nAvailable: http://www.cs.utexas.edu/users/EWD/ewd13xx/EWD1303.PDF\nA fascinating read for those of you interested in how the pioneers of our ﬁeld came up with some very\nbasic and fundamental concepts, including ideas like “interrupts” and even “a stack”!\n[H74] “Monitors: An Operating System Structuring Concept”\nC.A.R. Hoare\nCommunications of the ACM, 17:10, pages 549–557, October 1974\nHoare did a fair amount of theoretical work in concurrency. However, he is still probably most known\nfor his work on Quicksort, the coolest sorting algorithm in the world, at least according to these authors.\n[L11] “Pthread cond signal Man Page”\nAvailable: http://linux.die.net/man/3/pthread cond signal\nMarch, 2011\nThe Linux man page shows a nice simple example of why a thread might get a spurious wakeup, due to\nrace conditions within the signal/wakeup code.\n[LR80] “Experience with Processes and Monitors in Mesa”\nB.W. Lampson, D.R. Redell\nCommunications of the ACM. 23:2, pages 105-117, February 1980\nA terriﬁc paper about how to actually implement signaling and condition variables in a real system,\nleading to the term “Mesa” semantics for what it means to be woken up; the older semantics, developed\nby Tony Hoare [H74], then became known as “Hoare” semantics, which is hard to say out loud in class\nwith a straight face.\n[O49] “1984”\nGeorge Orwell, 1949, Secker and Warburg\nA little heavy-handed, but of course a must read. That said, we kind of gave away the ending by quoting\nthe last sentence. Sorry! And if the government is reading this, let us just say that we think that the\ngovernment is “double plus good”. Hear that, our pals at the NSA?\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n31\nSemaphores\nAs we know now, one needs both locks and condition variables to solve\na broad range of relevant and interesting concurrency problems. One of\nthe ﬁrst people to realize this years ago was Edsger Dijkstra (though it\nis hard to know the exact history [GR92]), known among other things for\nhis famous “shortest paths” algorithm in graph theory [D59], an early\npolemic on structured programming entitled “Goto Statements Consid-\nered Harmful” [D68a] (what a great title!), and, in the case we will study\nhere, the introduction of a synchronization primitive called the semaphore\n[D68b,D72]. Indeed, Dijkstra and colleagues invented the semaphore as a\nsingle primitive for all things related to synchronization; as you will see,\none can use semaphores as both locks and condition variables.\nTHE CRUX: HOW TO USE SEMAPHORES\nHow can we use semaphores instead of locks and condition variables?\nWhat is the deﬁnition of a semaphore? What is a binary semaphore?\nIs it straightforward to build a semaphore out of locks and condition\nvariables?\nWhat about building locks and condition variables out of\nsemaphores?\n31.1\nSemaphores: A Deﬁnition\nA semaphore is as an object with an integer value that we can ma-\nnipulate with two routines; in the POSIX standard, these routines are\nsem wait() and sem post()1. Because the initial value of the semaphore\ndetermines its behavior, before calling any other routine to interact with\nthe semaphore, we must ﬁrst initialize it to some value, as the code in\nFigure 31.1 does.\n1Historically, sem wait() was ﬁrst called P() by Dijkstra (for the Dutch word “to probe”)\nand sem post() was called V() (for the Dutch word “to test”). Sometimes, people call them\ndown and up, too. Use the Dutch versions to impress your friends.\n341\n\n\n342\nSEMAPHORES\n1\n#include <semaphore.h>\n2\nsem_t s;\n3\nsem_init(&s, 0, 1);\nFigure 31.1: Initializing A Semaphore\nIn the ﬁgure, we declare a semaphore s and initialize it to the value 1\nby passing 1 in as the third argument. The second argument to sem init()\nwill be set to 0 in all of the examples we’ll see; this indicates that the\nsemaphore is shared between threads in the same process. See the man\npage for details on other usages of semaphores (namely, how they can\nbe used to synchronize access across different processes), which require a\ndifferent value for that second argument.\nAfter a semaphore is initialized, we can call one of two functions to\ninteract with it, sem wait() or sem post(). The behavior of these two\nfunctions is seen in Figure 31.2.\nFor now, we are not concerned with the implementation of these rou-\ntines, which clearly requires some care; with multiple threads calling into\nsem wait() and sem post(), there is the obvious need for managing\nthese critical sections. We will now focus on how to use these primitives;\nlater we may discuss how they are built.\nWe should discuss a few salient aspects of the interfaces here. First, we\ncan see that sem wait() will either return right away (because the value\nof the semaphore was one or higher when we called sem wait()), or it\nwill cause the caller to suspend execution waiting for a subsequent post.\nOf course, multiple calling threads may call into sem wait(), and thus\nall be queued waiting to be woken.\nSecond, we can see that sem post() does not wait for some particular\ncondition to hold like sem wait() does. Rather, it simply increments the\nvalue of the semaphore and then, if there is a thread waiting to be woken,\nwakes one of them up.\nThird, the value of the semaphore, when negative, is equal to the num-\nber of waiting threads [D68b]. Though the value generally isn’t seen by\nusers of the semaphores, this invariant is worth knowing and perhaps\ncan help you remember how a semaphore functions.\nDon’t worry (yet) about the seeming race conditions possible within\nthe semaphore; assume that the actions they make are performed atomi-\ncally. We will soon use locks and condition variables to do just this.\n1\nint sem_wait(sem_t *s) {\n2\ndecrement the value of semaphore s by one\n3\nwait if value of semaphore s is negative\n4\n}\n5\n6\nint sem_post(sem_t *s) {\n7\nincrement the value of semaphore s by one\n8\nif there are one or more threads waiting, wake one\n9\n}\nFigure 31.2: Semaphore: Deﬁnitions of Wait and Post\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nSEMAPHORES\n343\n1\nsem_t m;\n2\nsem_init(&m, 0, X); // initialize semaphore to X; what should X be?\n3\n4\nsem_wait(&m);\n5\n// critical section here\n6\nsem_post(&m);\nFigure 31.3: A Binary Semaphore, a.k.a. a Lock\n31.2\nBinary Semaphores (Locks)\nWe are now ready to use a semaphore. Our ﬁrst use will be one with\nwhich we are already familiar: using a semaphore as a lock. See Figure\n31.3 for a code snippet; therein, you’ll see that we simply surround the\ncritical section of interest with a sem wait()/sem post() pair. Criti-\ncal to making this work, though, is the initial value of the semaphore m\n(initialized to X in the ﬁgure). What should X be?\n... (Try thinking about it before going on) ...\nLooking back at deﬁnition of the sem wait() and sem post() rou-\ntines above, we can see that the initial value should be 1.\nTo make this clear, let’s imagine a scenario with two threads. The ﬁrst\nthread (Thread 0) calls sem wait(); it will ﬁrst decrement the value of\nthe semaphore, changing it to 0. Then, it will wait only if the value is\nnot greater than or equal to 0; because the value is 0, the calling thread\nwill simply return and continue; Thread 0 is now free to enter the critical\nsection. If no other thread tries to acquire the lock while Thread 0 is inside\nthe critical section, when it calls sem post(), it will simply restore the\nvalue of the semaphore to 1 (and not wake any waiting thread, because\nthere are none). Table 31.1 shows a trace of this scenario.\nA more interesting case arises when Thread 0 “holds the lock” (i.e.,\nit has called sem wait() but not yet called sem post()), and another\nthread (Thread 1) tries to enter the critical section by calling sem wait().\nIn this case, Thread 1 will decrement the value of the semaphore to -1, and\nthus wait (putting itself to sleep and relinquishing the processor). When\nThread 0 runs again, it will eventually call sem post(), incrementing the\nvalue of the semaphore back to zero, and then wake the waiting thread\n(Thread 1), which will then be able to acquire the lock for itself. When\nThread 1 ﬁnishes, it will again increment the value of the semaphore,\nrestoring it to 1 again.\nValue of Semaphore\nThread 0\nThread 1\n1\n1\ncall sem wait()\n0\nsem wait() returns\n0\n(crit sect)\n0\ncall sem post()\n1\nsem post() returns\nTable 31.1: Thread Trace: Single Thread Using A Semaphore\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n344\nSEMAPHORES\nValue\nThread 0\nState\nThread 1\nState\n1\nRunning\nReady\n1\ncall sem wait()\nRunning\nReady\n0\nsem wait() returns\nRunning\nReady\n0\n(crit sect:\nbegin)\nRunning\nReady\n0\nInterrupt; Switch→T1\nReady\nRunning\n0\nReady\ncall sem wait()\nRunning\n-1\nReady\ndecrement sem\nRunning\n-1\nReady\n(sem<0)→sleep\nSleeping\n-1\nRunning\nSwitch→T0\nSleeping\n-1\n(crit sect:\nend)\nRunning\nSleeping\n-1\ncall sem post()\nRunning\nSleeping\n0\nincrement sem\nRunning\nSleeping\n0\nwake(T1)\nRunning\nReady\n0\nsem post() returns\nRunning\nReady\n0\nInterrupt; Switch→T1\nReady\nRunning\n0\nReady\nsem wait() returns\nRunning\n0\nReady\n(crit sect)\nRunning\n0\nReady\ncall sem post()\nRunning\n1\nReady\nsem post() returns\nRunning\nTable 31.2: Thread Trace: Two Threads Using A Semaphore\nTable 31.2 shows a trace of this example. In addition to thread actions,\nthe table shows the scheduler state of each thread: Running, Ready (i.e.,\nrunnable but not running), and Sleeping. Note in particular that Thread 1\ngoes into the sleeping state when it tries to acquire the already-held lock;\nonly when Thread 0 runs again can Thread 1 be awoken and potentially\nrun again.\nIf you want to work through your own example, try a scenario where\nmultiple threads queue up waiting for a lock. What would the value of\nthe semaphore be during such a trace?\nThus we are able to use semaphores as locks. Because locks only have\ntwo states (held and not held), this usage is sometimes known as a binary\nsemaphore and in fact can be implemented in a more simpliﬁed manner\nthan discussed here; we instead use the generalized semaphore as a lock.\n31.3\nSemaphores As Condition Variables\nSemaphores are also useful when a thread wants to halt its progress\nwaiting for a condition to become true. For example, a thread may wish\nto wait for a list to become non-empty, so it can delete an element from it.\nIn this pattern of usage, we often ﬁnd a thread waiting for something to\nhappen, and a different thread making that something happen and then\nsignaling that it has happened, thus waking the waiting thread. Because\nthe waiting thread (or threads) is waiting for some condition in the pro-\ngram to change, we are using the semaphore as a condition variable.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nSEMAPHORES\n345\n1\nsem_t s;\n2\n3\nvoid *\n4\nchild(void *arg) {\n5\nprintf(\"child\\n\");\n6\nsem_post(&s); // signal here: child is done\n7\nreturn NULL;\n8\n}\n9\n10\nint\n11\nmain(int argc, char *argv[]) {\n12\nsem_init(&s, 0, X); // what should X be?\n13\nprintf(\"parent: begin\\n\");\n14\npthread_t c;\n15\nPthread_create(c, NULL, child, NULL);\n16\nsem_wait(&s); // wait here for child\n17\nprintf(\"parent: end\\n\");\n18\nreturn 0;\n19\n}\nFigure 31.4: A Parent Waiting For Its Child\nA simple example is as follows.\nImagine a thread creates another\nthread and then wants to wait for it to complete its execution (Figure\n31.4). When this program runs, we would like to see the following:\nparent: begin\nchild\nparent: end\nThe question, then, is how to use a semaphore to achieve this effect,\nand is it turns out, it is relatively easy to understand. As you can see in\nthe code, the parent simply calls sem wait() and the child sem post()\nto wait for the condition of the child ﬁnishing its execution to become\ntrue. However, this raises the question: what should the initial value of\nthis semaphore be?\n(Again, think about it here, instead of reading ahead)\nThe answer, of course, is that the value of the semaphore should be\nset to is 0. There are two cases to consider. First, let us assume that the\nparent creates the child but the child has not run yet (i.e., it is sitting in\na ready queue but not running). In this case (Table 31.3), the parent will\ncall sem wait() before the child has called sem post(); we’d like the\nparent to wait for the child to run. The only way this will happen is if the\nvalue of the semaphore is not greater than 0; hence, 0 is the initial value.\nThe parent runs, decrements the semaphore (to -1), then waits (sleeping).\nWhen the child ﬁnally runs, it will call sem post(), increment the value\nof the semaphore to 0, and wake the parent, which will then return from\nsem wait() and ﬁnish the program.\nThe second case (Table 31.4) occurs when the child runs to comple-\ntion before the parent gets a chance to call sem wait(). In this case,\nthe child will ﬁrst call sem post(), thus incrementing the value of the\nsemaphore from 0 to 1. When the parent then gets a chance to run, it\nwill call sem wait() and ﬁnd the value of the semaphore to be 1; the\nparent will thus decrement the value (to 0) and return from sem wait()\nwithout waiting, also achieving the desired effect.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n346\nSEMAPHORES\nValue\nParent\nState\nChild\nState\n0\ncreate(Child)\nRunning\n(Child exists; is runnable)\nReady\n0\ncall sem wait()\nRunning\nReady\n-1\ndecrement sem\nRunning\nReady\n-1\n(sem<0)→sleep\nSleeping\nReady\n-1\nSwitch→Child\nSleeping\nchild runs\nRunning\n-1\nSleeping\ncall sem post()\nRunning\n0\nSleeping\nincrement sem\nRunning\n0\nReady\nwake(Parent)\nRunning\n0\nReady\nsem post() returns\nRunning\n0\nReady\nInterrupt; Switch→Parent\nReady\n0\nsem wait() returns\nReady\nReady\nTable 31.3: Thread Trace: Parent Waiting For Child (Case 1)\nValue\nParent\nState\nChild\nState\n0\ncreate(Child)\nRunning\n(Child exists; is runnable)\nReady\n0\nInterrupt; Switch→Child\nReady\nchild runs\nRunning\n0\nReady\ncall sem post()\nRunning\n1\nReady\nincrement sem\nRunning\n1\nReady\nwake(nobody)\nRunning\n1\nReady\nsem post() returns\nRunning\n1\nparent runs\nRunning\nInterrupt; Switch→Parent\nReady\n1\ncall sem wait()\nRunning\nReady\n0\ndecrement sem\nRunning\nReady\n0\n(sem≥0)→awake\nRunning\nReady\n0\nsem wait() returns\nRunning\nReady\nTable 31.4: Thread Trace: Parent Waiting For Child (Case 2)\n31.4\nThe Producer/Consumer (Bounded-Buffer) Problem\nThe next problem we will confront in this chapter is known as the pro-\nducer/consumer problem, or sometimes as the bounded buffer problem\n[D72]. This problem is described in detail in the previous chapter on con-\ndition variables; see there for details.\nFirst Attempt\nOur ﬁrst attempt at solving the problem introduces two semaphores, empty\nand full, which the threads will use to indicate when a buffer entry has\nbeen emptied or ﬁlled, respectively. The code for the put and get routines\nis in Figure 31.5, and our attempt at solving the producer and consumer\nproblem is in Figure 31.6.\nIn this example, the producer ﬁrst waits for a buffer to become empty\nin order to put data into it, and the consumer similarly waits for a buffer\nto become ﬁlled before using it. Let us ﬁrst imagine that MAX=1 (there is\nonly one buffer in the array), and see if this works.\nImagine again there are two threads, a producer and a consumer. Let\nus examine a speciﬁc scenario on a single CPU. Assume the consumer\ngets to run ﬁrst. Thus, the consumer will hit line c1 in the ﬁgure above,\ncalling sem wait(&full). Because full was initialized to the value 0,\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nSEMAPHORES\n347\n1\nint buffer[MAX];\n2\nint fill = 0;\n3\nint use\n= 0;\n4\n5\nvoid put(int value) {\n6\nbuffer[fill] = value;\n// line f1\n7\nfill = (fill + 1) % MAX; // line f2\n8\n}\n9\n10\nint get() {\n11\nint tmp = buffer[use];\n// line g1\n12\nuse = (use + 1) % MAX;\n// line g2\n13\nreturn tmp;\n14\n}\nFigure 31.5: The Put and Get Routines\n1\nsem_t empty;\n2\nsem_t full;\n3\n4\nvoid *producer(void *arg) {\n5\nint i;\n6\nfor (i = 0; i < loops; i++) {\n7\nsem_wait(&empty);\n// line P1\n8\nput(i);\n// line P2\n9\nsem_post(&full);\n// line P3\n10\n}\n11\n}\n12\n13\nvoid *consumer(void *arg) {\n14\nint i, tmp = 0;\n15\nwhile (tmp != -1) {\n16\nsem_wait(&full);\n// line C1\n17\ntmp = get();\n// line C2\n18\nsem_post(&empty);\n// line C3\n19\nprintf(\"%d\\n\", tmp);\n20\n}\n21\n}\n22\n23\nint main(int argc, char *argv[]) {\n24\n// ...\n25\nsem_init(&empty, 0, MAX); // MAX buffers are empty to begin with...\n26\nsem_init(&full, 0, 0);\n// ... and 0 are full\n27\n// ...\n28\n}\nFigure 31.6: Adding the Full and Empty Conditions\nthe call will decrement full (to -1), block the consumer, and wait for\nanother thread to call sem post() on full, as desired.\nAssume the producer then runs. It will hit line P1, thus calling the\nsem wait(&empty) routine. Unlike the consumer, the producer will\ncontinue through this line, because empty was initialized to the value\nMAX (in this case, 1). Thus, empty will be decremented to 0 and the\nproducer will put a data value into the ﬁrst entry of buffer (line P2). The\nproducer will then continue on to P3 and call sem post(&full), chang-\ning the value of the full semaphore from -1 to 0 and waking the consumer\n(e.g., move it from blocked to ready).\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n348\nSEMAPHORES\nIn this case, one of two things could happen. If the producer contin-\nues to run, it will loop around and hit line P1 again. This time, how-\never, it would block, as the empty semaphore’s value is 0. If the producer\ninstead was interrupted and the consumer began to run, it would call\nsem wait(&full) (line c1) and ﬁnd that the buffer was indeed full and\nthus consume it. In either case, we achieve the desired behavior.\nYou can try this same example with more threads (e.g., multiple pro-\nducers, and multiple consumers). It should still work.\nLet us now imagine that MAX is greater than 1 (say MAX = 10). For this\nexample, let us assume that there are multiple producers and multiple\nconsumers. We now have a problem: a race condition. Do you see where\nit occurs? (take some time and look for it) If you can’t see it, here’s a hint:\nlook more closely at the put() and get() code.\nOK, let’s understand the issue. Imagine two producers (Pa and Pb)\nboth calling into put() at roughly the same time. Assume producer Pa gets\nto run ﬁrst, and just starts to ﬁll the ﬁrst buffer entry (ﬁll = 0 at line f1).\nBefore Pa gets a chance to increment the ﬁll counter to 1, it is interrupted.\nProducer Pb starts to run, and at line f1 it also puts its data into the 0th\nelement of buffer, which means that the old data there is overwritten!\nThis is a no-no; we don’t want any data from the producer to be lost.\nA Solution: Adding Mutual Exclusion\nAs you can see, what we’ve forgotten here is mutual exclusion. The\nﬁlling of a buffer and incrementing of the index into the buffer is a critical\nsection, and thus must be guarded carefully. So let’s use our friend the\nbinary semaphore and add some locks. Figure 31.7 shows our attempt.\nNow we’ve added some locks around the entire put()/get() parts of\nthe code, as indicated by the NEW LINE comments. That seems like the\nright idea, but it also doesn’t work. Why? Deadlock. Why does deadlock\noccur? Take a moment to consider it; try to ﬁnd a case where deadlock\narises. What sequence of steps must happen for the program to deadlock?\nAvoiding Deadlock\nOK, now that you ﬁgured it out, here is the answer. Imagine two threads,\none producer and one consumer. The consumer gets to run ﬁrst. It ac-\nquires the mutex (line c0), and then calls sem wait() on the full semaphore\n(line c1); because there is no data yet, this call causes the consumer to\nblock and thus yield the CPU; importantly, though, the consumer still\nholds the lock.\nA producer then runs. It has data to produce and if it were able to run,\nit would be able to wake the consumer thread and all would be good.\nUnfortunately, the ﬁrst thing it does is call sem wait() on the binary\nmutex semaphore (line p0). The lock is already held. Hence, the producer\nis now stuck waiting too.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nSEMAPHORES\n349\n1\nsem_t empty;\n2\nsem_t full;\n3\nsem_t mutex;\n4\n5\nvoid *producer(void *arg) {\n6\nint i;\n7\nfor (i = 0; i < loops; i++) {\n8\nsem_wait(&mutex);\n// line p0 (NEW LINE)\n9\nsem_wait(&empty);\n// line p1\n10\nput(i);\n// line p2\n11\nsem_post(&full);\n// line p3\n12\nsem_post(&mutex);\n// line p4 (NEW LINE)\n13\n}\n14\n}\n15\n16\nvoid *consumer(void *arg) {\n17\nint i;\n18\nfor (i = 0; i < loops; i++) {\n19\nsem_wait(&mutex);\n// line c0 (NEW LINE)\n20\nsem_wait(&full);\n// line c1\n21\nint tmp = get();\n// line c2\n22\nsem_post(&empty);\n// line c3\n23\nsem_post(&mutex);\n// line c4 (NEW LINE)\n24\nprintf(\"%d\\n\", tmp);\n25\n}\n26\n}\n27\n28\nint main(int argc, char *argv[]) {\n29\n// ...\n30\nsem_init(&empty, 0, MAX); // MAX buffers are empty to begin with...\n31\nsem_init(&full, 0, 0);\n// ... and 0 are full\n32\nsem_init(&mutex, 0, 1);\n// mutex=1 because it is a lock (NEW LINE)\n33\n// ...\n34\n}\nFigure 31.7: Adding Mutual Exclusion (Incorrectly)\nThere is a simple cycle here. The consumer holds the mutex and is\nwaiting for the someone to signal full. The producer could signal full but\nis waiting for the mutex. Thus, the producer and consumer are each stuck\nwaiting for each other: a classic deadlock.\nFinally, A Working Solution\nTo solve this problem, we simply must reduce the scope of the lock. Fig-\nure 31.8 shows the ﬁnal working solution. As you can see, we simply\nmove the mutex acquire and release to be just around the critical section;\nthe full and empty wait and signal code is left outside. The result is a\nsimple and working bounded buffer, a commonly-used pattern in multi-\nthreaded programs. Understand it now; use it later. You will thank us for\nyears to come. Or at least, you will thank us when the same question is\nasked on the ﬁnal exam.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n350\nSEMAPHORES\n1\nsem_t empty;\n2\nsem_t full;\n3\nsem_t mutex;\n4\n5\nvoid *producer(void *arg) {\n6\nint i;\n7\nfor (i = 0; i < loops; i++) {\n8\nsem_wait(&empty);\n// line p1\n9\nsem_wait(&mutex);\n// line p1.5 (MOVED MUTEX HERE...)\n10\nput(i);\n// line p2\n11\nsem_post(&mutex);\n// line p2.5 (... AND HERE)\n12\nsem_post(&full);\n// line p3\n13\n}\n14\n}\n15\n16\nvoid *consumer(void *arg) {\n17\nint i;\n18\nfor (i = 0; i < loops; i++) {\n19\nsem_wait(&full);\n// line c1\n20\nsem_wait(&mutex);\n// line c1.5 (MOVED MUTEX HERE...)\n21\nint tmp = get();\n// line c2\n22\nsem_post(&mutex);\n// line c2.5 (... AND HERE)\n23\nsem_post(&empty);\n// line c3\n24\nprintf(\"%d\\n\", tmp);\n25\n}\n26\n}\n27\n28\nint main(int argc, char *argv[]) {\n29\n// ...\n30\nsem_init(&empty, 0, MAX); // MAX buffers are empty to begin with...\n31\nsem_init(&full, 0, 0);\n// ... and 0 are full\n32\nsem_init(&mutex, 0, 1);\n// mutex=1 because it is a lock\n33\n// ...\n34\n}\nFigure 31.8: Adding Mutual Exclusion (Correctly)\n31.5\nReader-Writer Locks\nAnother classic problem stems from the desire for a more ﬂexible lock-\ning primitive that admits that different data structure accesses might re-\nquire different kinds of locking. For example, imagine a number of con-\ncurrent list operations, including inserts and simple lookups. While in-\nserts change the state of the list (and thus a traditional critical section\nmakes sense), lookups simply read the data structure; as long as we can\nguarantee that no insert is on-going, we can allow many lookups to pro-\nceed concurrently. The special type of lock we will now develop to sup-\nport this type of operation is known as a reader-writer lock [CHP71]. The\ncode for such a lock is available in Figure 31.9.\nThe code is pretty simple. If some thread wants to update the data\nstructure in question, it should call the new pair of synchronization op-\nerations: rwlock acquire writelock(), to acquire a write lock, and\nrwlock release writelock(), to release it. Internally, these simply\nuse the writelock semaphore to ensure that only a single writer can ac-\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nSEMAPHORES\n351\n1\ntypedef struct _rwlock_t {\n2\nsem_t lock;\n// binary semaphore (basic lock)\n3\nsem_t writelock; // used to allow ONE writer or MANY readers\n4\nint\nreaders;\n// count of readers reading in critical section\n5\n} rwlock_t;\n6\n7\nvoid rwlock_init(rwlock_t *rw) {\n8\nrw->readers = 0;\n9\nsem_init(&rw->lock, 0, 1);\n10\nsem_init(&rw->writelock, 0, 1);\n11\n}\n12\n13\nvoid rwlock_acquire_readlock(rwlock_t *rw) {\n14\nsem_wait(&rw->lock);\n15\nrw->readers++;\n16\nif (rw->readers == 1)\n17\nsem_wait(&rw->writelock); // first reader acquires writelock\n18\nsem_post(&rw->lock);\n19\n}\n20\n21\nvoid rwlock_release_readlock(rwlock_t *rw) {\n22\nsem_wait(&rw->lock);\n23\nrw->readers--;\n24\nif (rw->readers == 0)\n25\nsem_post(&rw->writelock); // last reader releases writelock\n26\nsem_post(&rw->lock);\n27\n}\n28\n29\nvoid rwlock_acquire_writelock(rwlock_t *rw) {\n30\nsem_wait(&rw->writelock);\n31\n}\n32\n33\nvoid rwlock_release_writelock(rwlock_t *rw) {\n34\nsem_post(&rw->writelock);\n35\n}\nFigure 31.9: A Simple Reader-Writer Lock\nquire the lock and thus enter the critical section to update the data struc-\nture in question.\nMore interesting is the pair of routines to acquire and release read\nlocks. When acquiring a read lock, the reader ﬁrst acquires lock and\nthen increments the readers variable to track how many readers are\ncurrently inside the data structure. The important step then taken within\nrwlock acquire readlock() occurs when the ﬁrst reader acquires\nthe lock; in that case, the reader also acquires the write lock by calling\nsem wait() on the writelock semaphore, and then ﬁnally releasing\nthe lock by calling sem post().\nThus, once a reader has acquired a read lock, more readers will be\nallowed to acquire the read lock too; however, any thread that wishes to\nacquire the write lock will have to wait until all readers are ﬁnished; the\nlast one to exit the critical section calls sem post() on “writelock” and\nthus enables a waiting writer to acquire the lock.\nThis approach works (as desired), but does have some negatives, espe-\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "page_number": 373,
      "chapter_number": 38,
      "summary": "Figure 30.11\nshows a code snippet which demonstrates the issue Key topics include thread, semaphores, and lock. Thus, always use while and\nyour code will behave as expected.",
      "keywords": [
        "sem",
        "sem wait",
        "sem post",
        "call sem wait",
        "call sem post",
        "thread",
        "semaphore",
        "call sem",
        "wait",
        "post",
        "line",
        "lock",
        "Running",
        "Ready sem post",
        "ready"
      ],
      "concepts": [
        "thread",
        "semaphores",
        "lock",
        "runs",
        "running",
        "run",
        "void",
        "waiting",
        "line",
        "reader"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 16,
          "title": "Segment 16 (pages 141-154)",
          "relevance_score": 0.52,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 51,
          "title": "Segment 51 (pages 1023-1040)",
          "relevance_score": 0.52,
          "method": "api"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 37,
          "title": "Segment 37 (pages 363-372)",
          "relevance_score": 0.48,
          "method": "api"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 41,
          "title": "Segment 41 (pages 405-412)",
          "relevance_score": 0.48,
          "method": "api"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 33,
          "title": "Segment 33 (pages 330-337)",
          "relevance_score": 0.47,
          "method": "api"
        }
      ]
    },
    {
      "number": 39,
      "title": "Segment 39 (pages 388-398)",
      "start_page": 388,
      "end_page": 398,
      "detection_method": "topic_boundary",
      "content": "352\nSEMAPHORES\nTIP: SIMPLE AND DUMB CAN BE BETTER (HILL’S LAW)\nYou should never underestimate the notion that the simple and dumb\napproach can be the best one. With locking, sometimes a simple spin lock\nworks best, because it is easy to implement and fast. Although something\nlike reader/writer locks sounds cool, they are complex, and complex can\nmean slow. Thus, always try the simple and dumb approach ﬁrst.\nThis idea, of appealing to simplicity, is found in many places. One early\nsource is Mark Hill’s dissertation [H87], which studied how to design\ncaches for CPUs. Hill found that simple direct-mapped caches worked\nbetter than fancy set-associative designs (one reason is that in caching,\nsimpler designs enable faster lookups). As Hill succinctly summarized\nhis work: “Big and dumb is better.” And thus we call this similar advice\nHill’s Law.\ncially when it comes to fairness. In particular, it would be relatively easy\nfor readers to starve writers. More sophisticated solutions to this prob-\nlem exist; perhaps you can think of a better implementation? Hint: think\nabout what you would need to do to prevent more readers from entering\nthe lock once a writer is waiting.\nFinally, it should be noted that reader-writer locks should be used\nwith some caution. They often add more overhead (especially with more\nsophisticated implementations), and thus do not end up speeding up\nperformance as compared to just using simple and fast locking primi-\ntives [CB08].\nEither way, they showcase once again how we can use\nsemaphores in an interesting and useful way.\n31.6\nThe Dining Philosophers\nOne of the most famous concurrency problems posed, and solved, by\nDijkstra, is known as the dining philosopher’s problem [DHO71]. The\nproblem is famous because it is fun and somewhat intellectually inter-\nesting; however, its practical utility is low. However, its fame forces its\ninclusion here; indeed, you might be asked about it on some interview,\nand you’d really hate your OS professor if you miss that question and\ndon’t get the job. Conversely, if you get the job, please feel free to send\nyour OS professor a nice note, or some stock options.\nThe basic setup for the problem is this (as shown in Figure 31.10): as-\nsume there are ﬁve “philosophers” sitting around a table. Between each\npair of philosophers is a single fork (and thus, ﬁve total). The philoso-\nphers each have times where they think, and don’t need any forks, and\ntimes where they eat. In order to eat, a philosopher needs two forks, both\nthe one on their left and the one on their right. The contention for these\nforks, and the synchronization problems that ensue, are what makes this\na problem we study in concurrent programming.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nSEMAPHORES\n353\nP0\nP1\nP2\nP3\nP4\nf0\nf1\nf2\nf3\nf4\nFigure 31.10: The Dining Philosophers\nHere is the basic loop of each philosopher:\nwhile (1) {\nthink();\ngetforks();\neat();\nputforks();\n}\nThe key challenge, then, is to write the routines getforks() and\nputforks() such that there is no deadlock, no philosopher starves and\nnever gets to eat, and concurrency is high (i.e., as many philosophers can\neat at the same time as possible).\nFollowing Downey’s solutions [D08], we’ll use a few helper functions\nto get us towards a solution. They are:\nint left(int p)\n{ return p; }\nint right(int p) { return (p + 1) % 5; }\nWhen philosopher p wishes to refer to the fork on their left, they sim-\nply call left(p). Similarly, the fork on the right of a philosopher p is\nreferred to by calling right(p); the modulo operator therein handles\nthe one case where the last philosopher (p=4) tries to grab the fork on\ntheir right, which is fork 0.\nWe’ll also need some semaphores to solve this problem. Let us assume\nwe have ﬁve, one for each fork: sem t forks[5].\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n354\nSEMAPHORES\n1\nvoid getforks() {\n2\nsem_wait(forks[left(p)]);\n3\nsem_wait(forks[right(p)]);\n4\n}\n5\n6\nvoid putforks() {\n7\nsem_post(forks[left(p)]);\n8\nsem_post(forks[right(p)]);\n9\n}\nFigure 31.11: The getforks() and putforks() Routines\nBroken Solution\nWe attempt our ﬁrst solution to the problem. Assume we initialize each\nsemaphore (in the forks array) to a value of 1. Assume also that each\nphilosopher knows its own number (p). We can thus write the getforks()\nand putforks() routine as shown in Figure 31.11.\nThe intuition behind this (broken) solution is as follows. To acquire\nthe forks, we simply grab a “lock” on each one: ﬁrst the one on the left,\nand then the one on the right. When we are done eating, we release them.\nSimple, no? Unfortunately, in this case, simple means broken. Can you\nsee the problem that arises? Think about it.\nThe problem is deadlock. If each philosopher happens to grab the fork\non their left before any philosopher can grab the fork on their right, each\nwill be stuck holding one fork and waiting for another, forever. Speciﬁ-\ncally, philosopher 0 grabs fork 0, philosopher 1 grabs fork 1, philosopher\n2 grabs fork 2, philosopher 3 grabs fork 3, and philosopher 4 grabs fork 4;\nall the forks are acquired, and all the philosophers are stuck waiting for\na fork that another philosopher possesses. We’ll study deadlock in more\ndetail soon; for now, it is safe to say that this is not a working solution.\nA Solution: Breaking The Dependency\nThe simplest way to attack this problem is to change how forks are ac-\nquired by at least one of the philosophers; indeed, this is how Dijkstra\nhimself solved the problem. Speciﬁcally, let’s assume that philosopher\n4 (the highest numbered one) acquires the forks in a different order. The\ncode to do so is as follows:\n1\nvoid getforks() {\n2\nif (p == 4) {\n3\nsem_wait(forks[right(p)]);\n4\nsem_wait(forks[left(p)]);\n5\n} else {\n6\nsem_wait(forks[left(p)]);\n7\nsem_wait(forks[right(p)]);\n8\n}\n9\n}\nBecause the last philosopher tries to grab right before left, there is no\nsituation where each philosopher grabs one fork and is stuck waiting for\nanother; the cycle of waiting is broken. Think through the ramiﬁcations\nof this solution, and convince yourself that it works.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nSEMAPHORES\n355\n1\ntypedef struct __Zem_t {\n2\nint value;\n3\npthread_cond_t cond;\n4\npthread_mutex_t lock;\n5\n} Zem_t;\n6\n7\n// only one thread can call this\n8\nvoid Zem_init(Zem_t *s, int value) {\n9\ns->value = value;\n10\nCond_init(&s->cond);\n11\nMutex_init(&s->lock);\n12\n}\n13\n14\nvoid Zem_wait(Zem_t *s) {\n15\nMutex_lock(&s->lock);\n16\nwhile (s->value <= 0)\n17\nCond_wait(&s->cond, &s->lock);\n18\ns->value--;\n19\nMutex_unlock(&s->lock);\n20\n}\n21\n22\nvoid Zem_post(Zem_t *s) {\n23\nMutex_lock(&s->lock);\n24\ns->value++;\n25\nCond_signal(&s->cond);\n26\nMutex_unlock(&s->lock);\n27\n}\nFigure 31.12: Implementing Zemaphores with Locks and CVs\nThere are other “famous” problems like this one, e.g., the cigarette\nsmoker’s problem or the sleeping barber problem. Most of them are\njust excuses to think about concurrency; some of them have fascinating\nnames. Look them up if you are interested in learning more, or just get-\nting more practice thinking in a concurrent manner [D08].\n31.7\nHow To Implement Semaphores\nFinally, let’s use our low-level synchronization primitives, locks and\ncondition variables, to build our own version of semaphores called ...\n(drum roll here) ... Zemaphores. This task is fairly straightforward, as\nyou can see in Figure 31.12.\nAs you can see from the ﬁgure, we use just one lock and one condition\nvariable, plus a state variable to track the value of the semaphore. Study\nthe code for yourself until you really understand it. Do it!\nOne subtle difference between our Zemaphore and pure semaphores\nas deﬁned by Dijkstra is that we don’t maintain the invariant that the\nvalue of the semaphore, when negative, reﬂects the number of waiting\nthreads; indeed, the value will never be lower than zero. This behavior is\neasier to implement and matches the current Linux implementation.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n356\nSEMAPHORES\nTIP: BE CAREFUL WITH GENERALIZATION\nThe abstract technique of generalization can thus be quite useful in sys-\ntems design, where one good idea can be made slightly broader and thus\nsolve a larger class of problems. However, be careful when generalizing;\nas Lampson warns us “Don’t generalize; generalizations are generally\nwrong” [L83].\nOne could view semaphores as a generalization of locks and condition\nvariables; however, is such a generalization needed? And, given the dif-\nﬁculty of realizing a condition variable on top of a semaphore, perhaps\nthis generalization is not as general as you might think.\nCuriously, building locks and condition variables out of semaphores\nis a much trickier proposition. Some highly experienced concurrent pro-\ngrammers tried to do this in the Windows environment, and many differ-\nent bugs ensued [B04]. Try it yourself, and see if you can ﬁgure out why\nbuilding condition variables out of semaphores is more challenging than\nit might appear.\n31.8\nSummary\nSemaphores are a powerful and ﬂexible primitive for writing concur-\nrent programs. Some programmers use them exclusively, shunning locks\nand condition variables, due to their simplicity and utility.\nIn this chapter, we have presented just a few classic problems and solu-\ntions. If you are interested in ﬁnding out more, there are many other ma-\nterials you can reference. One great (and free reference) is Allen Downey’s\nbook on concurrency and programming with semaphores [D08]. This\nbook has lots of puzzles you can work on to improve your understand-\ning of both semaphores in speciﬁc and concurrency in general. Becoming\na real concurrency expert takes years of effort; going beyond what you\nlearn in this class is undoubtedly the key to mastering such a topic.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nSEMAPHORES\n357\nReferences\n[B04] “Implementing Condition Variables with Semaphores”\nAndrew Birrell\nDecember 2004\nAn interesting read on how difﬁcult implementing CVs on top of semaphores really is, and the mistakes\nthe author and co-workers made along the way. Particularly relevant because the group had done a ton\nof concurrent programming; Birrell, for example, is known for (among other things) writing various\nthread-programming guides.\n[CB08] “Real-world Concurrency”\nBryan Cantrill and Jeff Bonwick\nACM Queue. Volume 6, No. 5. September 2008\nA nice article by some kernel hackers from a company formerly known as Sun on the real problems faced\nin concurrent code.\n[CHP71] “Concurrent Control with Readers and Writers”\nP.J. Courtois, F. Heymans, D.L. Parnas\nCommunications of the ACM, 14:10, October 1971\nThe introduction of the reader-writer problem, and a simple solution. Later work introduced more\ncomplex solutions, skipped here because, well, they are pretty complex.\n[D59] “A Note on Two Problems in Connexion with Graphs”\nE. W. Dijkstra\nNumerische Mathematik 1, 269271, 1959\nAvailable: http://www-m3.ma.tum.de/twiki/pub/MN0506/WebHome/dijkstra.pdf\nCan you believe people worked on algorithms in 1959? We can’t. Even before computers were any fun\nto use, these people had a sense that they would transform the world...\n[D68a] “Go-to Statement Considered Harmful”\nE.W. Dijkstra\nCommunications of the ACM, volume 11(3): pages 147148, March 1968\nAvailable: http://www.cs.utexas.edu/users/EWD/ewd02xx/EWD215.PDF\nSometimes thought as the beginning of the ﬁeld of software engineering.\n[D68b] “The Structure of the THE Multiprogramming System”\nE.W. Dijkstra\nCommunications of the ACM, volume 11(5), pages 341346, 1968\nOne of the earliest papers to point out that systems work in computer science is an engaging intellectual\nendeavor. Also argues strongly for modularity in the form of layered systems.\n[D72] “Information Streams Sharing a Finite Buffer”\nE.W. Dijkstra\nInformation Processing Letters 1: 179180, 1972\nAvailable: http://www.cs.utexas.edu/users/EWD/ewd03xx/EWD329.PDF\nDid Dijkstra invent everything? No, but maybe close. He certainly was the ﬁrst to clearly write down\nwhat the problems were in concurrent code. However, it is true that practitioners in operating system\ndesign knew of many of the problems described by Dijkstra, so perhaps giving him too much credit\nwould be a misrepresentation of history.\n[D08] “The Little Book of Semaphores”\nA.B. Downey\nAvailable: http://greenteapress.com/semaphores/\nA nice (and free!) book about semaphores. Lots of fun problems to solve, if you like that sort of thing.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n358\nSEMAPHORES\n[DHO71] “Hierarchical ordering of sequential processes”\nE.W. Dijkstra\nAvailable: http://www.cs.utexas.edu/users/EWD/ewd03xx/EWD310.PDF\nPresents numerous concurrency problems, including the Dining Philosophers. The wikipedia page\nabout this problem is also quite informative.\n[GR92] “Transaction Processing: Concepts and Techniques”\nJim Gray and Andreas Reuter\nMorgan Kaufmann, September 1992\nThe exact quote that we ﬁnd particularly humorous is found on page 485, at the top of Section 8.8:\n“The ﬁrst multiprocessors, circa 1960, had test and set instructions ... presumably the OS imple-\nmentors worked out the appropriate algorithms, although Dijkstra is generally credited with inventing\nsemaphores many years later.”\n[H87] “Aspects of Cache Memory and Instruction Buffer Performance”\nMark D. Hill\nPh.D. Dissertation, U.C. Berkeley, 1987\nHill’s dissertation work, for those obsessed with caching in early systems. A great example of a quanti-\ntative dissertation.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\n32\nCommon Concurrency Problems\nResearchers have spent a great deal of time and effort looking into con-\ncurrency bugs over many years.\nMuch of the early work focused on\ndeadlock, a topic which we’ve touched on in the past chapters but will\nnow dive into deeply [C+71].\nMore recent work focuses on studying\nother types of common concurrency bugs (i.e., non-deadlock bugs). In\nthis chapter, we take a brief look at some example concurrency problems\nfound in real code bases, to better understand what problems to look out\nfor. And thus our problem:\nCRUX: HOW TO HANDLE COMMON CONCURRENCY BUGS\nConcurrency bugs tend to come in a variety of common patterns.\nKnowing which ones to look out for is the ﬁrst step to writing more ro-\nbust, correct concurrent code.\n32.1\nWhat Types Of Bugs Exist?\nThe ﬁrst, and most obvious, question is this: what types of concur-\nrency bugs manifest in complex, concurrent programs? This question is\ndifﬁcult to answer in general, but fortunately, some others have done the\nwork for us. Speciﬁcally, we rely upon a study by Lu et al. [L+08], which\nanalyzes a number of popular concurrent applications in great detail to\nunderstand what types of bugs arise in practice.\nThe study focuses on four major and important open-source applica-\ntions: MySQL (a popular database management system), Apache (a well-\nknown web server), Mozilla (the famous web browser), and OpenOfﬁce\n(a free version of the MS Ofﬁce suite, which some people actually use).\nIn the study, the authors examine concurrency bugs that have been found\nand ﬁxed in each of these code bases, turning the developers’ work into a\nquantitative bug analysis; understanding these results can help you un-\nderstand what types of problems actually occur in mature code bases.\n359\n\n\n360\nCOMMON CONCURRENCY PROBLEMS\nApplication\nWhat it does\nNon-Deadlock\nDeadlock\nMySQL\nDatabase Server\n14\n9\nApache\nWeb Server\n13\n4\nMozilla\nWeb Browser\n41\n16\nOpenOfﬁce\nOfﬁce Suite\n6\n2\nTotal\n74\n31\nTable 32.1: Bugs In Modern Applications\nTable 32.1 shows a summary of the bugs Lu and colleagues studied.\nFrom the table, you can see that there were 105 total bugs, most of which\nwere not deadlock (74); the remaining 31 were deadlock bugs. Further,\nyou can see that the number of bugs studied from each application; while\nOpenOfﬁce only had 8 total concurrency bugs, Mozilla had nearly 60.\nWe now dive into these different classes of bugs (non-deadlock, dead-\nlock) a bit more deeply. For the ﬁrst class of non-deadlock bugs, we use\nexamples from the study to drive our discussion. For the second class of\ndeadlock bugs, we discuss the long line of work that has been done in\neither preventing, avoiding, or handling deadlock.\n32.2\nNon-Deadlock Bugs\nNon-deadlock bugs make up a majority of concurrency bugs, accord-\ning to Lu’s study. But what types of bugs are these? How do they arise?\nHow can we ﬁx them? We now discuss the two major types of non-\ndeadlock bugs found by Lu et al.: atomicity violation bugs and order\nviolation bugs.\nAtomicity-Violation Bugs\nThe ﬁrst type of problem encountered is referred to as an atomicity vi-\nolation. Here is a simple example, found in MySQL. Before reading the\nexplanation, try ﬁguring out what the bug is. Do it!\n1\nThread 1::\n2\nif (thd->proc_info) {\n3\n...\n4\nfputs(thd->proc_info, ...);\n5\n...\n6\n}\n7\n8\nThread 2::\n9\nthd->proc_info = NULL;\nIn the example, two different threads access the ﬁeld proc info in\nthe structure thd. The ﬁrst thread checks if the value is non-NULL and\nthen prints its value; the second thread sets it to NULL. Clearly, if the\nﬁrst thread performs the check but then is interrupted before the call to\nfputs, the second thread could run in-between, thus setting the pointer\nto NULL; when the ﬁrst thread resumes, it will crash, as a NULL pointer\nwill be dereferenced by fputs.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nCOMMON CONCURRENCY PROBLEMS\n361\nThe more formal deﬁnition of an atomicity violation, according to Lu\net al, is this: “The desired serializability among multiple memory accesses\nis violated (i.e. a code region is intended to be atomic, but the atomicity\nis not enforced during execution).” In our example above, the code has\nan atomicity assumption (in Lu’s words) about the check for non-NULL\nof proc info and the usage of proc info in the fputs() call; when\nassumption is broken, the code will not work as desired.\nFinding a ﬁx for this type of problem is often (but not always) straight-\nforward. Can you think of how to ﬁx the code above?\nIn this solution, we simply add locks around the shared-variable ref-\nerences, ensuring that when either thread accesses the proc info ﬁeld,\nit has a lock held. Of course (not shown), any other code that accesses the\nstructure should also acquire this lock before doing so.\n1\npthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;\n2\n3\nThread 1::\n4\npthread_mutex_lock(&lock);\n5\nif (thd->proc_info) {\n6\n...\n7\nfputs(thd->proc_info, ...);\n8\n...\n9\n}\n10\npthread_mutex_unlock(&lock);\n11\n12\nThread 2::\n13\npthread_mutex_lock(&lock);\n14\nthd->proc_info = NULL;\n15\npthread_mutex_unlock(&lock);\nOrder-Violation Bugs\nAnother common type of non-deadlock bug found by Lu et al. is known\nas an order violation. Here is another simple example; once again, see if\nyou can ﬁgure out why the code below has a bug in it.\n1\nThread 1::\n2\nvoid init() {\n3\n...\n4\nmThread = PR_CreateThread(mMain, ...);\n5\n...\n6\n}\n7\n8\nThread 2::\n9\nvoid mMain(...) {\n10\n...\n11\nmState = mThread->State;\n12\n...\n13\n}\nAs you probably ﬁgured out, the code in Thread 2 seems to assume\nthat the variable mThread has already been initialized (and is not NULL);\nhowever, if Thread 1 does not happen to run ﬁrst, we are out of luck, and\nThread 2 will likely crash with a NULL pointer dereference (assuming\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n362\nCOMMON CONCURRENCY PROBLEMS\nthat the value of mThread is initially NULL; if not, even stranger things\ncould happen as arbitrary memory locations are read through the deref-\nerence in Thread 2).\nThe more formal deﬁnition of an order violation is this: “The desired\norder between two (groups of) memory accesses is ﬂipped (i.e., A should\nalways be executed before B, but the order is not enforced during execu-\ntion).” [L+08]\nThe ﬁx to this type of bug is generally to enforce ordering. As we\ndiscussed in detail previously, using condition variables is an easy and\nrobust way to add this style of synchronization into modern code bases.\nIn the example above, we could thus rewrite the code as follows:\n1\npthread_mutex_t mtLock = PTHREAD_MUTEX_INITIALIZER;\n2\npthread_cond_t\nmtCond = PTHREAD_COND_INITIALIZER;\n3\nint mtInit\n= 0;\n4\n5\nThread 1::\n6\nvoid init() {\n7\n...\n8\nmThread = PR_CreateThread(mMain, ...);\n9\n10\n// signal that the thread has been created...\n11\npthread_mutex_lock(&mtLock);\n12\nmtInit = 1;\n13\npthread_cond_signal(&mtCond);\n14\npthread_mutex_unlock(&mtLock);\n15\n...\n16\n}\n17\n18\nThread 2::\n19\nvoid mMain(...) {\n20\n...\n21\n// wait for the thread to be initialized...\n22\npthread_mutex_lock(&mtLock);\n23\nwhile (mtInit == 0)\n24\npthread_cond_wait(&mtCond, &mtLock);\n25\npthread_mutex_unlock(&mtLock);\n26\n27\nmState = mThread->State;\n28\n...\n29\n}\nIn this ﬁxed-up code sequence, we have added a lock (mtLock) and\ncorresponding condition variable (mtCond), as well as a state variable\n(mtInit). When the initialization code runs, it sets the state of mtInit\nto 1 and signals that it has done so. If Thread 2 had run before this point,\nit will be waiting for this signal and corresponding state change; if it runs\nlater, it will check the state and see that the initialization has already oc-\ncurred (i.e., mtInit is set to 1), and thus continue as is proper. Note that\nwe could likely use mThread as the state variable itself, but do not do so\nfor the sake of simplicity here. When ordering matters between threads,\ncondition variables (or semaphores) can come to the rescue.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "page_number": 388,
      "chapter_number": 39,
      "summary": "This chapter covers segment 39 (pages 388-398). Key topics include concurrent, bugs.",
      "keywords": [
        "bugs",
        "SEMAPHORES",
        "lock",
        "Common Concurrency Problems",
        "concurrency problems",
        "philosopher",
        "problems",
        "thread",
        "forks",
        "concurrency bugs",
        "concurrency",
        "pthread",
        "mutex",
        "code",
        "common concurrency bugs"
      ],
      "concepts": [
        "concurrent",
        "bugs",
        "bug",
        "semaphores",
        "locking",
        "thread",
        "worked",
        "philosophers",
        "implement",
        "implementation"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 51,
          "title": "Segment 51 (pages 1023-1040)",
          "relevance_score": 0.8,
          "method": "api"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 37,
          "title": "Segment 37 (pages 363-372)",
          "relevance_score": 0.78,
          "method": "api"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 2,
          "title": "Segment 2 (pages 9-16)",
          "relevance_score": 0.71,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 52,
          "title": "Segment 52 (pages 1041-1063)",
          "relevance_score": 0.7,
          "method": "api"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 21,
          "title": "Segment 21 (pages 193-216)",
          "relevance_score": 0.68,
          "method": "api"
        }
      ]
    },
    {
      "number": 40,
      "title": "Segment 40 (pages 399-406)",
      "start_page": 399,
      "end_page": 406,
      "detection_method": "topic_boundary",
      "content": "COMMON CONCURRENCY PROBLEMS\n363\nNon-Deadlock Bugs: Summary\nA large fraction (97%) of non-deadlock bugs studied by Lu et al. are either\natomicity or order violations. Thus, by carefully thinking about these\ntypes of bug patterns, programmers can likely do a better job of avoiding\nthem. Moreover, as more automated code-checking tools develop, they\nshould likely focus on these two types of bugs as they constitute such a\nlarge fraction of non-deadlock bugs found in deployment.\nUnfortunately, not all bugs are as easily ﬁxable as the examples we\nlooked at above. Some require a deeper understanding of what the pro-\ngram is doing, or a larger amount of code or data structure reorganization\nto ﬁx. Read Lu et al.’s excellent (and readable) paper for more details.\n32.3\nDeadlock Bugs\nBeyond the concurrency bugs mentioned above, a classic problem that\narises in many concurrent systems with complex locking protocols is known\nas deadlock. Deadlock occurs, for example, when a thread (say Thread\n1) is holding a lock (L1) and waiting for another one (L2); unfortunately,\nthe thread (Thread 2) that holds lock L2 is waiting for L1 to be released.\nHere is a code snippet that demonstrates such a potential deadlock:\nThread 1:\nThread 2:\nlock(L1);\nlock(L2);\nlock(L2);\nlock(L1);\nNote that if this code runs, deadlock does not necessarily occur; rather,\nit may occur, if, for example, Thread 1 grabs lock L1 and then a context\nswitch occurs to Thread 2. At that point, Thread 2 grabs L2, and tries to\nacquire L1. Thus we have a deadlock, as each thread is waiting for the\nother and neither can run. See Figure 32.1 for details; the presence of a\ncycle in the graph is indicative of the deadlock.\nThe ﬁgure should make clear the problem. How should programmers\nwrite code so as to handle deadlock in some way?\nCRUX: HOW TO DEAL WITH DEADLOCK\nHow should we build systems to prevent, avoid, or at least detect and\nrecover from deadlock? Is this a real problem in systems today?\nWhy Do Deadlocks Occur?\nAs you may be thinking, simple deadlocks such as the one above seem\nreadily avoidable. For example, if Thread 1 and 2 both made sure to grab\nlocks in the same order, the deadlock would never arise. So why do dead-\nlocks happen?\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n364\nCOMMON CONCURRENCY PROBLEMS\nThread 1\nThread 2\nLock L1\nLock L2\nHolds\nHolds\nWanted by\nWanted by\nFigure 32.1: The Deadlock Dependency Graph\nOne reason is that in large code bases, complex dependencies arise\nbetween components. Take the operating system, for example. The vir-\ntual memory system might need to access the ﬁle system in order to page\nin a block from disk; the ﬁle system might subsequently require a page\nof memory to read the block into and thus contact the virtual memory\nsystem. Thus, the design of locking strategies in large systems must be\ncarefully done to avoid deadlock in the case of circular dependencies that\nmay occur naturally in the code.\nAnother reason is due to the nature of encapsulation. As software de-\nvelopers, we are taught to hide details of implementations and thus make\nsoftware easier to build in a modular way. Unfortunately, such modular-\nity does not mesh well with locking. As Jula et al. point out [J+08], some\nseemingly innocuous interfaces almost invite you to deadlock. For exam-\nple, take the Java Vector class and the method AddAll(). This routine\nwould be called as follows:\nVector v1, v2;\nv1.AddAll(v2);\nInternally, because the method needs to be multi-thread safe, locks for\nboth the vector being added to (v1) and the parameter (v2) need to be\nacquired. The routine acquires said locks in some arbitrary order (say v1\nthen v2) in order to add the contents of v2 to v1. If some other thread\ncalls v2.AddAll(v1) at nearly the same time, we have the potential for\ndeadlock, all in a way that is quite hidden from the calling application.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nCOMMON CONCURRENCY PROBLEMS\n365\nConditions for Deadlock\nFour conditions need to hold for a deadlock to occur [C+71]:\n• Mutual exclusion: Threads claim exclusive control of resources that\nthey require (e.g., a thread grabs a lock).\n• Hold-and-wait: Threads hold resources allocated to them (e.g., locks\nthat they have already acquired) while waiting for additional re-\nsources (e.g., locks that they wish to acquire).\n• No preemption: Resources (e.g., locks) cannot be forcibly removed\nfrom threads that are holding them.\n• Circular wait: There exists a circular chain of threads such that\neach thread holds one more resources (e.g., locks) that are being\nrequested by the next thread in the chain.\nIf any of these four conditions are not met, deadlock cannot occur.\nThus, we ﬁrst explore techniques to prevent deadlock; each of these strate-\ngies seeks to prevent one of the above conditions from arising and thus is\none approach to handling the deadlock problem.\nPrevention\nCircular Wait\nProbably the most practical prevention technique (and certainly one that\nis used frequently) is to write your locking code such that you never in-\nduce a circular wait. The way to do that is to provide a total ordering on\nlock acquisition. For example, if there are only two locks in the system (L1\nand L2), we can prevent deadlock by always acquiring L1 before L2. Such\nstrict ordering ensures that no cyclical wait arises; hence, no deadlock.\nAs you can imagine, this approach requires careful design of global\nlocking strategies and must be done with great care. Further, it is just a\nconvention, and a sloppy programmer can easily ignore the locking pro-\ntocol and potentially cause deadlock. Finally, it requires a deep under-\nstanding of the code base, and how various routines are called; just one\nmistake could result in the wrong ordering of lock acquisition, and hence\ndeadlock.\nHold-and-wait\nThe hold-and-wait requirement for deadlock can be avoided by acquiring\nall locks at once, atomically. In practice, this could be achieved as follows:\n1\nlock(prevention);\n2\nlock(L1);\n3\nlock(L2);\n4\n...\n5\nunlock(prevention);\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n366\nCOMMON CONCURRENCY PROBLEMS\nBy ﬁrst grabbing the lock prevention, this code guarantees that no\nuntimely thread switch can occur in the midst of lock acquisition and thus\ndeadlock can once again be avoided. Of course, it requires that any time\nany thread grabs a lock, it ﬁrst acquires the global prevention lock. For\nexample, if another thread was trying to grab locks L1 and L2 in a dif-\nferent order, it would be OK, because it would be holding the prevention\nlock while doing so.\nNote that the solution is problematic for a number of reasons. As be-\nfore, encapsulation works against us: this approach requires us to know\nwhen calling a routine exactly which locks must be held and to acquire\nthem ahead of time. Further, the approach likely decreases concurrency\nas all locks must be acquired early on (at once) instead of when they are\ntruly needed.\nNo Preemption\nBecause we generally view locks as held until unlock is called, multiple\nlock acquisition often gets us into trouble because when waiting for one\nlock we are holding another. Many thread libraries provide a more ﬂexi-\nble set of interfaces to help avoid this situation. Speciﬁcally, a trylock()\nroutine will grab the lock (if it is available) or return -1 indicating that the\nlock is held right now and that you should try again later if you want to\ngrab that lock.\nSuch an interface could be used as follows to build a deadlock-free,\nordering-robust lock acquisition protocol:\n1\ntop:\n2\nlock(L1);\n3\nif (trylock(L2) == -1) {\n4\nunlock(L1);\n5\ngoto top;\n6\n}\nNote that another thread could follow the same protocol but grab the\nlocks in the other order (L2 then L1) and the program would still be dead-\nlock free. One new problem does arise, however: livelock. It is possible\n(though perhaps unlikely) that two threads could both be repeatedly at-\ntempting this sequence and repeatedly failing to acquire both locks. In\nthis case, both systems are running through this code sequence over and\nover again (and thus it is not a deadlock), but progress is not being made,\nhence the name livelock. There are solutions to the livelock problem, too:\nfor example, one could add a random delay before looping back and try-\ning the entire thing over again, thus decreasing the odds of repeated in-\nterference among competing threads.\nOne ﬁnal point about this solution: it skirts around the hard parts of\nusing a trylock approach. The ﬁrst problem that would likely exist again\narises due to encapsulation: if one of these locks is buried in some routine\nthat is getting called, the jump back to the beginning becomes more com-\nplex to implement. If the code had acquired some resources (other than\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nCOMMON CONCURRENCY PROBLEMS\n367\nL1) along the way, it must make sure to carefully release them as well;\nfor example, if after acquiring L1, the code had allocated some memory,\nit would have to release that memory upon failure to acquire L2, before\njumping back to the top to try the entire sequence again. However, in\nlimited circumstances (e.g., the Java vector method above), this type of\napproach could work well.\nMutual Exclusion\nThe ﬁnal prevention technique would be to avoid the need for mutual\nexclusion at all. In general, we know this is difﬁcult, because the code we\nwish to run does indeed have critical sections. So what can we do?\nHerlihy had the idea that one could design various data structures to\nbe wait-free [H91]. The idea here is simple: using powerful hardware in-\nstructions, you can build data structures in a manner that does not require\nexplicit locking.\nAs a simple example, let us assume we have a compare-and-swap in-\nstruction, which as you may recall is an atomic instruction provided by\nthe hardware that does the following:\n1\nint CompareAndSwap(int *address, int expected, int new) {\n2\nif (*address == expected) {\n3\n*address = new;\n4\nreturn 1; // success\n5\n}\n6\nreturn 0; // failure\n7\n}\nImagine we now wanted to atomically increment a value by a certain\namount. We could do it as follows:\n1\nvoid AtomicIncrement(int *value, int amount) {\n2\ndo {\n3\nint old = *value;\n4\n} while (CompareAndSwap(value, old, old + amount) == 0);\n5\n}\nInstead of acquiring a lock, doing the update, and then releasing it, we\nhave instead built an approach that repeatedly tries to update the value to\nthe new amount and uses the compare-and-swap to do so. In this manner,\nno lock is acquired, and no deadlock can arise (though livelock is still a\npossibility).\nLet us consider a slightly more complex example: list insertion. Here\nis code that inserts at the head of a list:\n1\nvoid insert(int value) {\n2\nnode_t *n = malloc(sizeof(node_t));\n3\nassert(n != NULL);\n4\nn->value = value;\n5\nn->next\n= head;\n6\nhead\n= n;\n7\n}\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n368\nCOMMON CONCURRENCY PROBLEMS\nThis code performs a simple insertion, but if called by multiple threads\nat the “same time”, has a race condition (see if you can ﬁgure out why). Of\ncourse, we could solve this by surrounding this code with a lock acquire\nand release:\n1\nvoid insert(int value) {\n2\nnode_t *n = malloc(sizeof(node_t));\n3\nassert(n != NULL);\n4\nn->value = value;\n5\nlock(listlock);\n// begin critical section\n6\nn->next\n= head;\n7\nhead\n= n;\n8\nunlock(listlock); // end of critical section\n9\n}\nIn this solution, we are using locks in the traditional manner1. Instead,\nlet us try to perform this insertion in a wait-free manner simply using the\ncompare-and-swap instruction. Here is one possible approach:\n1\nvoid insert(int value) {\n2\nnode_t *n = malloc(sizeof(node_t));\n3\nassert(n != NULL);\n4\nn->value = value;\n5\ndo {\n6\nn->next = head;\n7\n} while (CompareAndSwap(&head, n->next, n));\n8\n}\nThe code here updates the next pointer to point to the current head,\nand then tries to swap the newly-created node into position as the new\nhead of the list. However, this will fail if some other thread successfully\nswapped in a new head in the meanwhile, causing this thread to retry\nagain with the new head.\nOf course, building a useful list requires more than just a list insert,\nand not surprisingly building a list that you can insert into, delete from,\nand perform lookups on in a wait-free manner is non-trivial. Read the\nrich literature on wait-free synchronization if you ﬁnd this interesting.\nDeadlock Avoidance via Scheduling\nInstead of deadlock prevention, in some scenarios deadlock avoidance\nis preferable. Avoidance requires some global knowledge of which locks\nvarious threads might grab during their execution, and subsequently sched-\nules said threads in a way as to guarantee no deadlock can occur.\nFor example, assume we have two processors and four threads which\nmust be scheduled upon them. Assume further we know that Thread\n1 (T1) grabs locks L1 and L2 (in some order, at some point during its\nexecution), T2 grabs L1 and L2 as well, T3 grabs just L2, and T4 grabs no\n1The astute reader might be asking why we grabbed the lock so late, instead of right when\nentering the insert() routine; can you, astute reader, ﬁgure out why that is likely OK?\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nCOMMON CONCURRENCY PROBLEMS\n369\nlocks at all. We can show these lock acquisition demands of the threads\nin tabular form:\nT1\nT2\nT3\nT4\nL1\nyes\nyes\nno\nno\nL2\nyes\nyes\nyes\nno\nA smart scheduler could thus compute that as long as T1 and T2 are\nnot run at the same time, no deadlock could ever arise. Here is one such\nschedule:\nCPU 1\nCPU 2\nT1\nT2\nT3\nT4\nNote that it is OK for (T3 and T1) or (T3 and T2) to overlap. Even\nthough T3 grabs lock L2, it can never cause a deadlock by running con-\ncurrently with other threads because it only grabs one lock.\nLet’s look at one more example. In this one, there is more contention\nfor the same resources (again, locks L1 and L2), as indicated by the fol-\nlowing contention table:\nT1\nT2\nT3\nT4\nL1\nyes\nyes\nyes\nno\nL2\nyes\nyes\nyes\nno\nIn particular, threads T1, T2, and T3 all need to grab both locks L1 and\nL2 at some point during their execution. Here is a possible schedule that\nguarantees that no deadlock could ever occur:\nCPU 1\nCPU 2\nT1\nT2\nT3\nT4\nAs you can see, static scheduling leads to a conservative approach\nwhere T1, T2, and T3 are all run on the same processor, and thus the\ntotal time to complete the jobs is lengthened considerably. Though it may\nhave been possible to run these tasks concurrently, the fear of deadlock\nprevents us from doing so, and the cost is performance.\nOne famous example of an approach like this is Dijkstra’s Banker’s Al-\ngorithm [D64], and many similar approaches have been described in the\nliterature. Unfortunately, they are only useful in very limited environ-\nments, for example, in an embedded system where one has full knowl-\nedge of the entire set of tasks that must be run and the locks that they\nneed. Further, such approaches can limit concurrency, as we saw in the\nsecond example above. Thus, avoidance of deadlock via scheduling is\nnot a widely-used general-purpose solution.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n370\nCOMMON CONCURRENCY PROBLEMS\nTIP: DON’T ALWAYS DO IT PERFECTLY (TOM WEST’S LAW)\nTom West, famous as the subject of the classic computer-industry book\n“Soul of a New Machine” [K81], says famously: “Not everything worth\ndoing is worth doing well”, which is a terriﬁc engineering maxim. If a\nbad thing happens rarely, certainly one should not spend a great deal of\neffort to prevent it, particularly if the cost of the bad thing occurring is\nsmall.\nDetect and Recover\nOne ﬁnal general strategy is to allow deadlocks to occasionally occur, and\nthen take some action once such a deadlock has been detected. For exam-\nple, if an OS froze once a year, you would just reboot it and get happily (or\ngrumpily) on with your work. If deadlocks are rare, such a non-solution\nis indeed quite pragmatic.\nMany database systems employ deadlock detection and recovery tech-\nniques. A deadlock detector runs periodically, building a resource graph\nand checking it for cycles. In the event of a cycle (deadlock), the system\nneeds to be restarted. If more intricate repair of data structures is ﬁrst\nrequired, a human being may be involved to ease the process.\n32.4\nSummary\nIn this chapter, we have studied the types of bugs that occur in con-\ncurrent programs. The ﬁrst type, non-deadlock bugs, are surprisingly\ncommon, but often are easier to ﬁx. They include atomicity violations,\nin which a sequence of instructions that should have been executed to-\ngether was not, and order violations, in which the needed order between\ntwo threads was not enforced.\nWe have also brieﬂy discussed deadlock: why it occurs, and what can\nbe done about it. The problem is as old as concurrency itself, and many\nhundreds of papers have been written about the topic. The best solu-\ntion in practice is to be careful, develop a lock acquisition total order,\nand thus prevent deadlock from occurring in the ﬁrst place. Wait-free\napproaches also have promise, as some wait-free data structures are now\nﬁnding their way into commonly-used libraries and critical systems, in-\ncluding Linux. However, their lack of generality and the complexity to\ndevelop a new wait-free data structure will likely limit the overall util-\nity of this approach. Perhaps the best solution is to develop new concur-\nrent programming models: in systems such as MapReduce (from Google)\n[GD02], programmers can describe certain types of parallel computations\nwithout any locks whatsoever. Locks are problematic by their very na-\nture; perhaps we should seek to avoid using them unless we truly must.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "page_number": 399,
      "chapter_number": 40,
      "summary": "This chapter covers segment 40 (pages 399-406). Key topics include deadlock, locking, and thread. Thus, by carefully thinking about these\ntypes of bug patterns, programmers can likely do a better job of avoiding\nthem.",
      "keywords": [
        "COMMON CONCURRENCY PROBLEMS",
        "Deadlock",
        "lock",
        "thread",
        "CONCURRENCY PROBLEMS",
        "COMMON CONCURRENCY",
        "CONCURRENCY",
        "code",
        "lock acquisition",
        "systems",
        "Bugs",
        "COMMON",
        "PROBLEMS",
        "occur",
        "Non-Deadlock Bugs"
      ],
      "concepts": [
        "deadlock",
        "locking",
        "thread",
        "problems",
        "order",
        "ordering",
        "code",
        "prevent",
        "prevention",
        "free"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 37,
          "title": "Segment 37 (pages 363-372)",
          "relevance_score": 0.71,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 51,
          "title": "Segment 51 (pages 1023-1040)",
          "relevance_score": 0.7,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 29,
          "title": "Segment 29 (pages 272-282)",
          "relevance_score": 0.67,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 52,
          "title": "Segment 52 (pages 1041-1063)",
          "relevance_score": 0.66,
          "method": "api"
        },
        {
          "book": "AntiPatterns",
          "chapter": 9,
          "title": "Segment 9 (pages 74-81)",
          "relevance_score": 0.62,
          "method": "api"
        }
      ]
    },
    {
      "number": 41,
      "title": "Segment 41 (pages 407-414)",
      "start_page": 407,
      "end_page": 414,
      "detection_method": "topic_boundary",
      "content": "COMMON CONCURRENCY PROBLEMS\n371\nReferences\n[C+71] “System Deadlocks”\nE.G. Coffman, M.J. Elphick, A. Shoshani\nACM Computing Surveys, 3:2, June 1971\nThe classic paper outlining the conditions for deadlock and how you might go about dealing with it.\nThere are certainly some earlier papers on this topic; see the references within this paper for details.\n[D64] “Een algorithme ter voorkoming van de dodelijke omarming”\nCirculated privately, around 1964\nAvailable: http://www.cs.utexas.edu/users/EWD/ewd01xx/EWD108.PDF\nIndeed, not only did Dijkstra come up with a number of solutions to the deadlock problem, he was the\nﬁrst to note its existence, at least in written form. However, he called it the “deadly embrace”, which\n(thankfully) did not catch on.\n[GD02] “MapReduce: Simpliﬁed Data Processing on Large Clusters”\nSanjay Ghemawhat and Jeff Dean\nOSDI ’04, San Francisco, CA, October 2004\nThe MapReduce paper ushered in the era of large-scale data processing, and proposes a framework for\nperforming such computations on clusters of generally unreliable machines.\n[H91] “Wait-free Synchronization”\nMaurice Herlihy\nACM TOPLAS, 13(1), pages 124-149, January 1991\nHerlihy’s work pioneers the ideas behind wait-free approaches to writing concurrent programs. These\napproaches tend to be complex and hard, often more difﬁcult than using locks correctly, probably limiting\ntheir success in the real world.\n[J+08] “Deadlock Immunity: Enabling Systems To Defend Against Deadlocks”\nHoratiu Jula, Daniel Tralamazza, Cristian Zamﬁr, George Candea\nOSDI ’08, San Diego, CA, December 2008\nAn excellent recent paper on deadlocks and how to avoid getting caught in the same ones over and over\nagain in a particular system.\n[K81] “Soul of a New Machine”\nTracy Kidder, 1980\nA must-read for any systems builder or engineer, detailing the early days of how a team inside Data\nGeneral (DG), led by Tom West, worked to produce a “new machine.” Kidder’s other book are also\nexcellent, in particular, “Mountains beyond Mountains”. Or maybe you don’t agree with me, comma?\n[L+08] “Learning from Mistakes – A Comprehensive Study on\nReal World Concurrency Bug Characteristics”\nShan Lu, Soyeon Park, Eunsoo Seo, Yuanyuan Zhou\nASPLOS ’08, March 2008, Seattle, Washington\nThe ﬁrst in-depth study of concurrency bugs in real software, and the basis for this chapter. Look at Y.Y.\nZhou’s or Shan Lu’s web pages for many more interesting papers on bugs.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n33\nEvent-based Concurrency (Advanced)\nThus far, we’ve written about concurrency as if the only way to build\nconcurrent applications is to use threads. Like many things in life, this\nis not completely true. Speciﬁcally, a different style of concurrent pro-\ngramming is often used in both GUI-based applications [O96] as well as\nsome types of internet servers [PDZ99]. This style, known as event-based\nconcurrency, has become popular in some modern systems, including\nserver-side frameworks such as node.js [N13], but its roots are found in\nC/UNIX systems that we’ll discuss below.\nThe problem that event-based concurrency addresses is two-fold. The\nﬁrst is that managing concurrency correctly in multi-threaded applica-\ntions can be challenging; as we’ve discussed, missing locks, deadlock,\nand other nasty problems can arise. The second is that in a multi-threaded\napplication, the developer has little or no control over what is scheduled\nat a given moment in time; rather, the programmer simply creates threads\nand then hopes that the underlying OS schedules them in a reasonable\nmanner across available CPUs. Given the difﬁculty of building a general-\npurpose scheduler that works well in all cases for all workloads, some-\ntimes the OS will schedule work in a manner that is less than optimal.\nThe crux:\nTHE CRUX:\nHOW TO BUILD CONCURRENT SERVERS WITHOUT THREADS\nHow can we build a concurrent server without using threads, and thus\nretain control over concurrency as well as avoid some of the problems\nthat seem to plague multi-threaded applications?\n33.1\nThe Basic Idea: An Event Loop\nThe basic approach we’ll use, as stated above, is called event-based\nconcurrency. The approach is quite simple: you simply wait for some-\nthing (i.e., an “event”) to occur; when it does, you check what type of\n373\n\n\n374\nEVENT-BASED CONCURRENCY (ADVANCED)\nevent it is and do the small amount of work it requires (which may in-\nclude issuing I/O requests, or scheduling other events for future han-\ndling, etc.). That’s it!\nBefore getting into the details, let’s ﬁrst examine what a canonical\nevent-based server looks like. Such applications are based around a sim-\nple construct known as the event loop. Pseudocode for an event loop\nlooks like this:\nwhile (1) {\nevents = getEvents();\nfor (e in events)\nprocessEvent(e);\n}\nIt’s really that simple. The main loop simply waits for something to do\n(by calling getEvents() in the code above) and then, for each event re-\nturned, processes them, one at a time; the code that processes each event\nis known as an event handler. Importantly, when a handler processes\nan event, it is the only activity taking place in the system; thus, deciding\nwhich event to handle next is equivalent to scheduling. This explicit con-\ntrol over scheduling is one of the fundamental advantages of the event-\nbased approach.\nBut this discussion leaves us with a bigger question: how exactly does\nan event-based server determine which events are taking place, in par-\nticular with regards to network and disk I/O? Speciﬁcally, how can an\nevent server tell if a message has arrived for it?\n33.2\nAn Important API: select() (or poll())\nWith that basic event loop in mind, we next must address the question\nof how to receive events. In most systems, a basic API is available, via\neither the select() or poll() system calls.\nWhat these interfaces enable a program to do is simple: check whether\nthere is any incoming I/O that should be attended to. For example, imag-\nine that a network application (such as a web server) wishes to check\nwhether any network packets have arrived, in order to service them.\nThese system calls let you do exactly that.\nTake select() for example. The manual page (on Mac OS X) de-\nscribes the API in this manner:\nint select(int nfds,\nfd_set *restrict readfds,\nfd_set *restrict writefds,\nfd_set *restrict errorfds,\nstruct timeval *restrict timeout);\nThe actual description from the man page: select() examines the I/O de-\nscriptor sets whose addresses are passed in readfds, writefds, and errorfds to see\nif some of their descriptors are ready for reading, are ready for writing, or have\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nEVENT-BASED CONCURRENCY (ADVANCED)\n375\nASIDE: BLOCKING VS. NON-BLOCKING INTERFACES\nBlocking (or synchronous) interfaces do all of their work before returning\nto the caller; non-blocking (or asynchronous) interfaces begin some work\nbut return immediately, thus letting whatever work that needs to be done\nget done in the background.\nThe usual culprit in blocking calls is I/O of some kind. For example, if a\ncall must read from disk in order to complete, it might block, waiting for\nthe I/O request that has been sent to the disk to return.\nNon-blocking interfaces can be used in any style of programming (e.g.,\nwith threads), but are essential in the event-based approach, as a call that\nblocks will halt all progress.\nan exceptional condition pending, respectively. The ﬁrst nfds descriptors are\nchecked in each set, i.e., the descriptors from 0 through nfds-1 in the descriptor\nsets are examined. On return, select() replaces the given descriptor sets with\nsubsets consisting of those descriptors that are ready for the requested operation.\nselect() returns the total number of ready descriptors in all the sets.\nA couple of points about select(). First, note that it lets you check\nwhether descriptors can be read from as well as written to; the former\nlets a server determine that a new packet has arrived and is in need of\nprocessing, whereas the latter lets the service know when it is OK to reply\n(i.e., the outbound queue is not full).\nSecond, note the timeout argument. One common usage here is to\nset the timeout to NULL, which causes select() to block indeﬁnitely,\nuntil some descriptor is ready. However, more robust servers will usually\nspecify some kind of timeout; one common technique is to set the timeout\nto zero, and thus use the call to select() to return immediately.\nThe poll() system call is quite similar. See its manual page, or Stevens\nand Rago [SR05], for details.\nEither way, these basic primitives give us a way to build a non-blocking\nevent loop, which simply checks for incoming packets, reads from sockets\nwith messages upon them, and replies as needed.\n33.3\nUsing select()\nTo make this more concrete, let’s examine how to use select() to see\nwhich network descriptors have incoming messages upon them. Figure\n33.1 shows a simple example.\nThis code is actually fairly simple to understand. After some initial-\nization, the server enters an inﬁnite loop. Inside the loop, it uses the\nFD ZERO() macro to ﬁrst clear the set of ﬁle descriptors, and then uses\nFD SET() to include all of the ﬁle descriptors from minFD to maxFD in\nthe set. This set of descriptors might represent, for example, all of the net-\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n376\nEVENT-BASED CONCURRENCY (ADVANCED)\n1\n#include <stdio.h>\n2\n#include <stdlib.h>\n3\n#include <sys/time.h>\n4\n#include <sys/types.h>\n5\n#include <unistd.h>\n6\n7\nint main(void) {\n8\n// open and set up a bunch of sockets (not shown)\n9\n// main loop\n10\nwhile (1) {\n11\n// initialize the fd_set to all zero\n12\nfd_set readFDs;\n13\nFD_ZERO(&readFDs);\n14\n15\n// now set the bits for the descriptors\n16\n// this server is interested in\n17\n// (for simplicity, all of them from min to max)\n18\nint fd;\n19\nfor (fd = minFD; fd < maxFD; fd++)\n20\nFD_SET(fd, &readFDs);\n21\n22\n// do the select\n23\nint rc = select(maxFD+1, &readFDs, NULL, NULL, NULL);\n24\n25\n// check which actually have data using FD_ISSET()\n26\nint fd;\n27\nfor (fd = minFD; fd < maxFD; fd++)\n28\nif (FD_ISSET(fd, &readFDs))\n29\nprocessFD(fd);\n30\n}\n31\n}\nFigure 33.1: Simple Code using select()\nwork sockets to which the server is paying attention. Finally, the server\ncalls select() to see which of the connections have data available upon\nthem. By then using FD ISSET() in a loop, the event server can see\nwhich of the descriptors have data ready and process the incoming data.\nOf course, a real server would be more complicated than this, and\nrequire logic to use when sending messages, issuing disk I/O, and many\nother details. For further information, see Stevens and Rago [SR05] for\nAPI information, or Pai et. al or Welsh et al. for a good overview of the\ngeneral ﬂow of event-based servers [PDZ99, WCB01].\n33.4\nWhy Simpler? No Locks Needed\nWith a single CPU and an event-based application, the problems found\nin concurrent programs are no longer present. Speciﬁcally, because only\none event is being handled at a time, there is no need to acquire or release\nlocks; the event-based server cannot be interrupted by another thread be-\ncause it is decidedly single threaded. Thus, concurrency bugs common in\nthreaded programs do not manifest in the basic event-based approach.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nEVENT-BASED CONCURRENCY (ADVANCED)\n377\nTIP: DON’T BLOCK IN EVENT-BASED SERVERS\nEvent-based servers enable ﬁne-grained control over scheduling of tasks.\nHowever, to maintain such control, no call that blocks the execution the\ncaller can ever be made; failing to obey this design tip will result in a\nblocked event-based server, frustrated clients, and serious questions as to\nwhether you ever read this part of the book.\n33.5\nA Problem: Blocking System Calls\nThus far, event-based programming sounds great, right? You program\na simple loop, and handle events as they arise. You don’t even need to\nthink about locking! But there is an issue: what if an event requires that\nyou issue a system call that might block?\nFor example, imagine a request comes from a client into a server to\nread a ﬁle from disk and return its contents to the requesting client (much\nlike a simple HTTP request). To service such a request, some event han-\ndler will eventually have to issue an open() system call to open the ﬁle,\nfollowed by a series of read() calls to read the ﬁle. When the ﬁle is read\ninto memory, the server will likely start sending the results to the client.\nBoth the open() and read() calls may issue I/O requests to the stor-\nage system (when the needed metadata or data is not in memory already),\nand thus may take a long time to service. With a thread-based server, this\nis no issue: while the thread issuing the I/O request suspends (waiting\nfor the I/O to complete), other threads can run, thus enabling the server\nto make progress. Indeed, this natural overlap of I/O and other computa-\ntion is what makes thread-based programming quite natural and straight-\nforward.\nWith an event-based approach, however, there are no other threads to\nrun: just the main event loop. And this implies that if an event handler\nissues a call that blocks, the entire server will do just that: block until the\ncall completes. When the event loop blocks, the system sits idle, and thus\nis a huge potential waste of resources. We thus have a rule that must be\nobeyed in event-based systems: no blocking calls are allowed.\n33.6\nA Solution: Asynchronous I/O\nTo overcome this limit, many modern operating systems have intro-\nduced new ways to issue I/O requests to the disk system, referred to\ngenerically as asynchronous I/O. These interfaces enable an application\nto issue an I/O request and return control immediately to the caller, be-\nfore the I/O has completed; additional interfaces enable an application to\ndetermine whether various I/Os have completed.\nFor example, let us examine the interface provided on Mac OS X (other\nsystems have similar APIs). The APIs revolve around a basic structure,\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n378\nEVENT-BASED CONCURRENCY (ADVANCED)\nthe struct aiocb or AIO control block in common terminology. A\nsimpliﬁed version of the structure looks like this (see the manual pages\nfor more information):\nstruct aiocb {\nint\naio_fildes;\n/* File descriptor */\noff_t\naio_offset;\n/* File offset */\nvolatile void\n*aio_buf;\n/* Location of buffer */\nsize_t\naio_nbytes;\n/* Length of transfer */\n};\nTo issue an asynchronous read to a ﬁle, an application should ﬁrst\nﬁll in this structure with the relevant information: the ﬁle descriptor of\nthe ﬁle to be read (aio fildes), the offset within the ﬁle (aio offset)\nas well as the length of the request (aio nbytes), and ﬁnally the tar-\nget memory location into which the results of the read should be copied\n(aio buf).\nAfter this structure is ﬁlled in, the application must issue the asyn-\nchronous call to read the ﬁle; on Mac OS X, this API is simply the asyn-\nchronous read API:\nint aio_read(struct aiocb *aiocbp);\nThis call tries to issue the I/O; if successful, it simply returns right\naway and the application (i.e., the event-based server) can continue with\nits work.\nThere is one last piece of the puzzle we must solve, however. How can\nwe tell when an I/O is complete, and thus that the buffer (pointed to by\naio buf) now has the requested data within it?\nOne last API is needed. On Mac OS X, it is referred to (somewhat\nconfusingly) as aio error(). The API looks like this:\nint aio_error(const struct aiocb *aiocbp);\nThis system call checks whether the request referred to by aiocbp has\ncompleted. If it has, the routine returns success (indicated by a zero);\nif not, EINPROGRESS is returned. Thus, for every outstanding asyn-\nchronous I/O, an application can periodically poll the system via a call\nto aio error() to determine whether said I/O has yet completed.\nOne thing you might have noticed is that it is painful to check whether\nan I/O has completed; if a program has tens or hundreds of I/Os issued\nat a given point in time, should it simply keep checking each of them\nrepeatedly, or wait a little while ﬁrst, or ... ?\nTo remedy this issue, some systems provide an approach based on the\ninterrupt. This method uses UNIX signals to inform applications when\nan asynchronous I/O completes, thus removing the need to repeatedly\nask the system. This polling vs. interrupts issue is seen in devices too, as\nyou will see (or already have seen) in the chapter on I/O devices.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "page_number": 407,
      "chapter_number": 41,
      "summary": "[L+08] “Learning from Mistakes – A Comprehensive Study on\nReal World Concurrency Bug Characteristics”\nShan Lu, Soyeon Park, Eunsoo Seo, Yuanyuan Zhou\nASPLOS ’08, March 2008, Seattle, Washington\nThe ﬁrst in-depth study of concurrency bugs in real software, and the basis for this chapter Key topics include event, servers, and concurrency.",
      "keywords": [
        "Event-based Concurrency",
        "Event",
        "Event-based",
        "server",
        "CONCURRENCY",
        "Event Loop",
        "System",
        "event-based server",
        "select",
        "call",
        "system call",
        "AIO",
        "read",
        "Loop",
        "descriptors"
      ],
      "concepts": [
        "event",
        "servers",
        "concurrency",
        "concurrent",
        "select",
        "threads",
        "aio",
        "paper",
        "blocking",
        "data"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 16,
          "title": "Segment 16 (pages 141-154)",
          "relevance_score": 0.58,
          "method": "api"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 54,
          "title": "Segment 54 (pages 590-592)",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 10,
          "title": "Segment 10 (pages 75-82)",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 37,
          "title": "Segment 37 (pages 363-372)",
          "relevance_score": 0.56,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 50,
          "title": "Segment 50 (pages 498-509)",
          "relevance_score": 0.55,
          "method": "api"
        }
      ]
    },
    {
      "number": 42,
      "title": "Segment 42 (pages 415-422)",
      "start_page": 415,
      "end_page": 422,
      "detection_method": "topic_boundary",
      "content": "EVENT-BASED CONCURRENCY (ADVANCED)\n379\nASIDE: UNIX SIGNALS\nA huge and fascinating infrastructure known as signals is present in all mod-\nern UNIX variants. At its simplest, signals provide a way to communicate with a\nprocess. Speciﬁcally, a signal can be delivered to an application; doing so stops the\napplication from whatever it is doing to run a signal handler, i.e., some code in\nthe application to handle that signal. When ﬁnished, the process just resumes its\nprevious behavior.\nEach signal has a name, such as HUP (hang up), INT (interrupt), SEGV (seg-\nmentation violation), etc; see the manual page for details. Interestingly, sometimes\nit is the kernel itself that does the signaling. For example, when your program en-\ncounters a segmentation violation, the OS sends it a SIGSEGV (prepending SIG\nto signal names is common); if your program is conﬁgured to catch that signal,\nyou can actually run some code in response to this erroneous program behavior\n(which can be useful for debugging). When a signal is sent to a process not conﬁg-\nured to handle that signal, some default behavior is enacted; for SEGV, the process\nis killed.\nHere is a simple program that goes into an inﬁnite loop, but has ﬁrst set up a\nsignal handler to catch SIGHUP:\n#include <stdio.h>\n#include <signal.h>\nvoid handle(int arg) {\nprintf(\"stop wakin’ me up...\\n\");\n}\nint main(int argc, char *argv[]) {\nsignal(SIGHUP, handle);\nwhile (1)\n; // doin’ nothin’ except catchin’ some sigs\nreturn 0;\n}\nYou can send signals to it with the kill command line tool (yes, this is an odd\nand aggressive name). Doing so will interrupt the main while loop in the program\nand run the handler code handle():\nprompt> ./main &\n[3] 36705\nprompt> kill -HUP 36705\nstop wakin’ me up...\nprompt> kill -HUP 36705\nstop wakin’ me up...\nprompt> kill -HUP 36705\nstop wakin’ me up...\nThere is a lot more to learn about signals, so much that a single page, much\nless a single chapter, does not nearly sufﬁce. As always, there is one great source:\nStevens and Rago [SR05]. Read more if interested.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n380\nEVENT-BASED CONCURRENCY (ADVANCED)\nIn systems without asynchronous I/O, the pure event-based approach\ncannot be implemented. However, clever researchers have derived meth-\nods that work fairly well in their place. For example, Pai et al. [PDZ99]\ndescribe a hybrid approach in which events are used to process network\npackets, and a thread pool is used to manage outstanding I/Os. Read\ntheir paper for details.\n33.7\nAnother Problem: State Management\nAnother issue with the event-based approach is that such code is gen-\nerally more complicated to write than traditional thread-based code. The\nreason is as follows: when an event handler issues an asynchronous I/O,\nit must package up some program state for the next event handler to use\nwhen the I/O ﬁnally completes; this additional work is not needed in\nthread-based programs, as the state the program needs is on the stack of\nthe thread. Adya et al. call this work manual stack management, and it\nis fundamental to event-based programming [A+02].\nTo make this point more concrete, let’s look at a simple example in\nwhich a thread-based server needs to read from a ﬁle descriptor (fd) and,\nonce complete, write the data that it read from the ﬁle to a network socket\ndescriptor (sd). The code (ignoring error checking) looks like this:\nint rc = read(fd, buffer, size);\nrc = write(sd, buffer, size);\nAs you can see, in a multi-threaded program, doing this kind of work\nis trivial; when the read() ﬁnally returns, the code immediately knows\nwhich socket to write to because that information is on the stack of the\nthread (in the variable sd).\nIn an event-based system, life is not so easy. To perform the same task,\nwe’d ﬁrst issue the read asynchronously, using the AIO calls described\nabove. Let’s say we then periodically check for completion of the read\nusing the aio error() call; when that call informs us that the read is\ncomplete, how does the event-based server know what to do?\nThe solution, as described by Adya et al. [A+02], is to use an old pro-\ngramming language construct known as a continuation [FHK84]. Though\nit sounds complicated, the idea is rather simple: basically, record the\nneeded information to ﬁnish processing this event in some data struc-\nture; when the event happens (i.e., when the disk I/O completes), look\nup the needed information and process the event.\nIn this speciﬁc case, the solution would be to record the socket de-\nscriptor (sd) in some kind of data structure (e.g., a hash table), indexed\nby the ﬁle descriptor (fd). When the disk I/O completes, the event han-\ndler would use the ﬁle descriptor to look up the continuation, which will\nreturn the value of the socket descriptor to the caller. At this point (ﬁ-\nnally), the server can then do the last bit of work to write the data to the\nsocket.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nEVENT-BASED CONCURRENCY (ADVANCED)\n381\n33.8\nWhat Is Still Difﬁcult With Events\nThere are a few other difﬁculties with the event-based approach that\nwe should mention. For example, when systems moved from a single\nCPU to multiple CPUs, some of the simplicity of the event-based ap-\nproach disappeared. Speciﬁcally, in order to utilize more than one CPU,\nthe event server has to run multiple event handlers in parallel; when do-\ning so, the usual synchronization problems (e.g., critical sections) arise,\nand the usual solutions (e.g., locks) must be employed. Thus, on mod-\nern multicore systems, simple event handling without locks is no longer\npossible.\nAnother problem with the event-based approach is that it does not\nintegrate well with certain kinds of systems activity, such as paging. For\nexample, if an event-handler page faults, it will block, and thus the server\nwill not make progress until the page fault completes. Even though the\nserver has been structured to avoid explicit blocking, this type of implicit\nblocking due to page faults is hard to avoid and thus can lead to large\nperformance problems when prevalent.\nA third issue is that event-based code can be hard to manage over time,\nas the exact semantics of various routines changes [A+02]. For example,\nif a routine changes from non-blocking to blocking, the event handler\nthat calls that routine must also change to accommodate its new nature,\nby ripping itself into two pieces. Because blocking is so disastrous for\nevent-based servers, a programmer must always be on the lookout for\nsuch changes in the semantics of the APIs each event uses.\nFinally, though asynchronous disk I/O is now possible on most plat-\nforms, it has taken a long time to get there [PDZ99], and it never quite\nintegrates with asynchronous network I/O in as simple and uniform a\nmanner as you might think. For example, while one would simply like\nto use the select() interface to manage all outstanding I/Os, usually\nsome combination of select() for networking and the AIO calls for\ndisk I/O are required.\n33.9\nSummary\nWe’ve presented a bare bones introduction to a different style of con-\ncurrency based on events. Event-based servers give control of schedul-\ning to the application itself, but do so at some cost in complexity and\ndifﬁculty of integration with other aspects of modern systems (e.g., pag-\ning). Because of these challenges, no single approach has emerged as\nbest; thus, both threads and events are likely to persist as two different\napproaches to the same concurrency problem for many years to come.\nRead some research papers (e.g., [A+02, PDZ99, vB+03, WCB01]) or bet-\nter yet, write some event-based code, to learn more.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n382\nEVENT-BASED CONCURRENCY (ADVANCED)\nReferences\n[A+02] “Cooperative Task Management Without Manual Stack Management”\nAtul Adya, Jon Howell, Marvin Theimer, William J. Bolosky, John R. Douceur\nUSENIX ATC ’02, Monterey, CA, June 2002\nThis gem of a paper is the ﬁrst to clearly articulate some of the difﬁculties of event-based concurrency,\nand suggests some simple solutions, as well explores the even crazier idea of combining the two types of\nconcurrency management into a single application!\n[FHK84] “Programming With Continuations”\nDaniel P. Friedman, Christopher T. Haynes, Eugene E. Kohlbecker\nIn Program Transformation and Programming Environments, Springer Verlag, 1984\nThe classic reference to this old idea from the world of programming languages. Now increasingly\npopular in some modern languages.\n[N13] “Node.js Documentation”\nBy the folks who build node.js\nAvailable: http://nodejs.org/api/\nOne of the many cool new frameworks that help you readily build web services and applications. Every\nmodern systems hacker should be proﬁcient in frameworks such as this one (and likely, more than one).\nSpend the time and do some development in one of these worlds and become an expert.\n[O96] “Why Threads Are A Bad Idea (for most purposes)”\nJohn Ousterhout\nInvited Talk at USENIX ’96, San Diego, CA, January 1996\nA great talk about how threads aren’t a great match for GUI-based applications (but the ideas are more\ngeneral). Ousterhout formed many of these opinions while he was developing Tcl/Tk, a cool scripting\nlanguage and toolkit that made it 100x easier to develop GUI-based applications than the state of the\nart at the time. While the Tk GUI toolkit lives on (in Python for example), Tcl seems to be slowly dying\n(unfortunately).\n[PDZ99] “Flash: An Efﬁcient and Portable Web Server”\nVivek S. Pai, Peter Druschel, Willy Zwaenepoel\nUSENIX ’99, Monterey, CA, June 1999\nA pioneering paper on how to structure web servers in the then-burgeoning Internet era. Read it to\nunderstand the basics as well as to see the authors’ ideas on how to build hybrids when support for\nasynchronous I/O is lacking.\n[SR05] “Advanced Programming in the UNIX Environment”\nW. Richard Stevens and Stephen A. Rago\nAddison-Wesley, 2005\nOnce again, we refer to the classic must-have-on-your-bookshelf book of UNIX systems programming.\nIf there is some detail you need to know, it is in here.\n[vB+03] “Capriccio: Scalable Threads for Internet Services”\nRob von Behren, Jeremy Condit, Feng Zhou, George C. Necula, Eric Brewer\nSOSP ’03, Lake George, New York, October 2003\nA paper about how to make threads work at extreme scale; a counter to all the event-based work ongoing\nat the time.\n[WCB01] “SEDA: An Architecture for Well-Conditioned, Scalable Internet Services”\nMatt Welsh, David Culler, and Eric Brewer\nSOSP ’01, Banff, Canada, October 2001\nA nice twist on event-based serving that combines threads, queues, and event-based hanlding into one\nstreamlined whole. Some of these ideas have found their way into the infrastructures of companies such\nas Google, Amazon, and elsewhere.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\n34\nSummary Dialogue on Concurrency\nProfessor: So, does your head hurt now?\nStudent: (taking two Motrin tablets) Well, some. It’s hard to think about all the\nways threads can interleave.\nProfessor: Indeed it is. I am always amazed at how so few line of code, when\nconcurrent execution is involved, can become nearly impossible to understand.\nStudent: Me too! It’s kind of embarrassing, as a Computer Scientist, not to be\nable to make sense of ﬁve lines of code.\nProfessor: Oh, don’t feel too badly. If you look through the ﬁrst papers on con-\ncurrent algorithms, they are sometimes wrong! And the authors often professors!\nStudent: (gasps) Professors can be ... umm... wrong?\nProfessor: Yes, it is true. Though don’t tell anybody – it’s one of our trade\nsecrets.\nStudent: I am sworn to secrecy. But if concurrent code is so hard to think about,\nand so hard to get right, how are we supposed to write correct concurrent code?\nProfessor: Well that is the real question, isn’t it? I think it starts with a few\nsimple things. First, keep it simple! Avoid complex interactions between threads,\nand use well-known and tried-and-true ways to manage thread interactions.\nStudent: Like simple locking, and maybe a producer-consumer queue?\nProfessor: Exactly! Those are common paradigms, and you should be able to\nproduce the working solutions given what you’ve learned. Second, only use con-\ncurrency when absolutely needed; avoid it if at all possible. There is nothing\nworse than premature optimization of a program.\nStudent: I see – why add threads if you don’t need them?\nProfessor: Exactly. Third, if you really need parallelism, seek it in other sim-\npliﬁed forms. For example, the Map-Reduce method for writing parallel data\nanalysis code is an excellent example of achieving parallelism without having to\nhandle any of the horriﬁc complexities of locks, condition variables, and the other\nnasty things we’ve talked about.\n383\n\n\n384\nSUMMARY DIALOGUE ON CONCURRENCY\nStudent: Map-Reduce, huh? Sounds interesting – I’ll have to read more about\nit on my own.\nProfessor: Good! You should. In the end, you’ll have to do a lot of that, as\nwhat we learn together can only serve as the barest introduction to the wealth of\nknowledge that is out there. Read, read, and read some more! And then try things\nout, write some code, and then write some more too. As Gladwell talks about in\nhis book “Outliers”, you need to put roughly 10,000 hours into something in\norder to become a real expert. You can’t do that all inside of class time!\nStudent: Wow, I’m not sure if that is depressing, or uplifting. But I’ll assume\nthe latter, and get to work! Time to write some more concurrent code...\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nPart III\nPersistence\n385\n",
      "page_number": 415,
      "chapter_number": 42,
      "summary": "This chapter covers segment 42 (pages 415-422). Key topics include thread, program, and programming. Speciﬁcally, a signal can be delivered to an application; doing so stops the\napplication from whatever it is doing to run a signal handler, i.e., some code in\nthe application to handle that signal.",
      "keywords": [
        "EVENT-BASED",
        "EVENT-BASED CONCURRENCY",
        "code",
        "Read",
        "signal",
        "event",
        "systems",
        "CONCURRENCY",
        "program",
        "write",
        "threads",
        "event-based approach",
        "simple",
        "event-based code",
        "signal handler"
      ],
      "concepts": [
        "thread",
        "program",
        "programming",
        "based",
        "student",
        "event",
        "professor",
        "systems",
        "signals",
        "simple"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 37,
          "title": "Segment 37 (pages 363-372)",
          "relevance_score": 0.56,
          "method": "api"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 29,
          "title": "Segment 29 (pages 298-305)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 40,
          "title": "Segment 40 (pages 795-814)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 51,
          "title": "Segment 51 (pages 1023-1040)",
          "relevance_score": 0.54,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 2",
          "chapter": 55,
          "title": "Segment 55 (pages 536-543)",
          "relevance_score": 0.53,
          "method": "api"
        }
      ]
    },
    {
      "number": 43,
      "title": "Segment 43 (pages 423-434)",
      "start_page": 423,
      "end_page": 434,
      "detection_method": "topic_boundary",
      "content": "35\nA Dialogue on Persistence\nProfessor: And thus we reach the third of our four ... err... three pillars of\noperating systems: persistence.\nStudent: Did you say there were three pillars, or four? What is the fourth?\nProfessor: No. Just three, young student, just three. Trying to keep it simple\nhere.\nStudent: OK, ﬁne. But what is persistence, oh ﬁne and noble professor?\nProfessor: Actually, you probably know what it means in the traditional sense,\nright? As the dictionary would say: “a ﬁrm or obstinate continuance in a course\nof action in spite of difﬁculty or opposition.”\nStudent: It’s kind of like taking your class: some obstinance required.\nProfessor: Ha! Yes. But persistence here means something else. Let me explain.\nImagine you are outside, in a ﬁeld, and you pick a –\nStudent: (interrupting) I know! A peach! From a peach tree!\nProfessor: I was going to say apple, from an apple tree. Oh well; we’ll do it your\nway, I guess.\nStudent: (stares blankly)\nProfessor: Anyhow, you pick a peach; in fact, you pick many many peaches,\nbut you want to make them last for a long time. Winter is hard and cruel in\nWisconsin, after all. What do you do?\nStudent: Well, I think there are some different things you can do. You can pickle\nit! Or bake a pie. Or make a jam of some kind. Lots of fun!\nProfessor: Fun? Well, maybe. Certainly, you have to do a lot more work to make\nthe peach persist. And so it is with information as well; making information\npersist, despite computer crashes, disk failures, or power outages is a tough and\ninteresting challenge.\nStudent: Nice segue; you’re getting quite good at that.\nProfessor: Thanks! A professor can always use a few kind words, you know.\n387\n\n\n388\nA DIALOGUE ON PERSISTENCE\nStudent: I’ll try to remember that. I guess it’s time to stop talking peaches, and\nstart talking computers?\nProfessor: Yes, it is that time...\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\n36\nI/O Devices\nBefore delving into the main content of this part of the book (on persis-\ntence), we ﬁrst introduce the concept of an input/output (I/O) device and\nshow how the operating system might interact with such an entity. I/O is\nquite critical to computer systems, of course; imagine a program without\nany input (it produces the same result each time); now imagine a pro-\ngram with no output (what was the purpose of it running?). Clearly, for\ncomputer systems to be interesting, both input and output are required.\nAnd thus, our general problem:\nCRUX: HOW TO INTEGRATE I/O INTO SYSTEMS\nHow should I/O be integrated into systems? What are the general\nmechanisms? How can we make them efﬁcient?\n36.1\nSystem Architecture\nTo begin our discussion, let’s look at the structure of a typical system\n(Figure 36.1). The picture shows a single CPU attached to the main mem-\nory of the system via some kind of memory bus or interconnect. Some\ndevices are connected to the system via a general I/O bus, which in many\nmodern systems would be PCI (or one if its many derivatives); graph-\nics and some other higher-performance I/O devices might be found here.\nFinally, even lower down are one or more of what we call a peripheral\nbus, such as SCSI, SATA, or USB. These connect the slowest devices to\nthe system, including disks, mice, and other similar components.\nOne question you might ask is: why do we need a hierarchical struc-\nture like this? Put simply: physics, and cost. The faster a bus is, the\nshorter it must be; thus, a high-performance memory bus does not have\nmuch room to plug devices and such into it. In addition, engineering\na bus for high performance is quite costly. Thus, system designers have\nadopted this hierarchical approach, where components that demands high\nperformance (such as the graphics card) are nearer the CPU. Lower per-\n389\n\n\n390\nI/O DEVICES\nGraphics\nMemory\nCPU\nMemory Bus\n(proprietary)\nGeneral I/O Bus\n(e.g., PCI)\nPeripheral I/O Bus\n(e.g., SCSI, SATA, USB)\nFigure 36.1: Prototypical System Architecture\nformance components are further away. The beneﬁts of placing disks and\nother slow devices on a peripheral bus are manifold; in particular, you\ncan place a large number of devices on it.\n36.2\nA Canonical Device\nLet us now look at a canonical device (not a real one), and use this\ndevice to drive our understanding of some of the machinery required\nto make device interaction efﬁcient. From Figure 36.2, we can see that a\ndevice has two important components. The ﬁrst is the hardware interface\nit presents to the rest of the system. Just like a piece of software, hardware\nmust also present some kind of interface that allows the system software\nto control its operation. Thus, all devices have some speciﬁed interface\nand protocol for typical interaction.\nThe second part of any device is its internal structure. This part of\nthe device is implementation speciﬁc and is responsible for implement-\ning the abstraction the device presents to the system. Very simple devices\nwill have one or a few hardware chips to implement their functionality;\nmore complex devices will include a simple CPU, some general purpose\nmemory, and other device-speciﬁc chips to get their job done. For exam-\nple, modern RAID controllers might consist of hundreds of thousands of\nlines of ﬁrmware (i.e., software within a hardware device) to implement\nits functionality.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nI/O DEVICES\n391\nOther Hardware-specific Chips\nMemory (DRAM or SRAM or both)\nMicro-controller (CPU)\nRegisters\nStatus\nCommand\nData\nInterface\nInternals\nFigure 36.2: A Canonical Device\n36.3\nThe Canonical Protocol\nIn the picture above, the (simpliﬁed) device interface is comprised of\nthree registers: a status register, which can be read to see the current sta-\ntus of the device; a command register, to tell the device to perform a cer-\ntain task; and a data register to pass data to the device, or get data from\nthe device. By reading and writing these registers, the operating system\ncan control device behavior.\nLet us now describe a typical interaction that the OS might have with\nthe device in order to get the device to do something on its behalf. The\nprotocol is as follows:\nWhile (STATUS == BUSY)\n; // wait until device is not busy\nWrite data to DATA register\nWrite command to COMMAND register\n(Doing so starts the device and executes the command)\nWhile (STATUS == BUSY)\n; // wait until device is done with your request\nThe protocol has four steps. In the ﬁrst, the OS waits until the device is\nready to receive a command by repeatedly reading the status register; we\ncall this polling the device (basically, just asking it what is going on). Sec-\nond, the OS sends some data down to the data register; one can imagine\nthat if this were a disk, for example, that multiple writes would need to\ntake place to transfer a disk block (say 4KB) to the device. When the main\nCPU is involved with the data movement (as in this example protocol),\nwe refer to it as programmed I/O (PIO). Third, the OS writes a command\nto the command register; doing so implicitly lets the device know that\nboth the data is present and that it should begin working on the com-\nmand. Finally, the OS waits for the device to ﬁnish by again polling it\nin a loop, waiting to see if it is ﬁnished (it may then get an error code to\nindicate success or failure).\nThis basic protocol has the positive aspect of being simple and work-\ning. However, there are some inefﬁciencies and inconveniences involved.\nThe ﬁrst problem you might notice in the protocol is that polling seems\ninefﬁcient; speciﬁcally, it wastes a great deal of CPU time just waiting for\nthe (potentially slow) device to complete its activity, instead of switching\nto another ready process and thus better utilizing the CPU.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n392\nI/O DEVICES\nTHE CRUX: HOW TO AVOID THE COSTS OF POLLING\nHow can the OS check device status without frequent polling, and\nthus lower the CPU overhead required to manage the device?\n36.4\nLowering CPU Overhead With Interrupts\nThe invention that many engineers came upon years ago to improve\nthis interaction is something we’ve seen already: the interrupt. Instead\nof polling the device repeatedly, the OS can issue a request, put the call-\ning process to sleep, and context switch to another task. When the device\nis ﬁnally ﬁnished with the operation, it will raise a hardware interrupt,\ncausing the CPU to jump into the OS at a pre-determined interrupt ser-\nvice routine (ISR) or more simply an interrupt handler. The handler is\njust a piece of operating system code that will ﬁnish the request (for ex-\nample, by reading data and perhaps an error code from the device) and\nwake the process waiting for the I/O, which can then proceed as desired.\nInterrupts thus allow for overlap of computation and I/O, which is\nkey for improved utilization. This timeline shows the problem:\nCPU\nDisk\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\np\np\np\np\np\n1\n1\n1\n1\n1\nIn the diagram, Process 1 runs on the CPU for some time (indicated by\na repeated 1 on the CPU line), and then issues an I/O request to the disk\nto read some data. Without interrupts, the system simply spins, polling\nthe status of the device repeatedly until the I/O is complete (indicated by\na p). The disk services the request and ﬁnally Process 1 can run again.\nIf instead we utilize interrupts and allow for overlap, the OS can do\nsomething else while waiting for the disk:\nCPU\nDisk\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n2\n2\n2\n2\n2\n1\n1\n1\n1\n1\nIn this example, the OS runs Process 2 on the CPU while the disk ser-\nvices Process 1’s request. When the disk request is ﬁnished, an interrupt\noccurs, and the OS wakes up Process 1 and runs it again. Thus, both the\nCPU and the disk are properly utilized during the middle stretch of time.\nNote that using interrupts is not always the best solution. For example,\nimagine a device that performs its tasks very quickly: the ﬁrst poll usually\nﬁnds the device to be done with task. Using an interrupt in this case will\nactually slow down the system: switching to another process, handling the\ninterrupt, and switching back to the issuing process is expensive. Thus, if\na device is fast, it may be best to poll; if it is slow, interrupts, which allow\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nI/O DEVICES\n393\nTIP: INTERRUPTS NOT ALWAYS BETTER THAN PIO\nAlthough interrupts allow for overlap of computation and I/O, they only\nreally make sense for slow devices. Otherwise, the cost of interrupt han-\ndling and context switching may outweigh the beneﬁts interrupts pro-\nvide. There are also cases where a ﬂood of interrupts may overload a sys-\ntem and lead it to livelock [MR96]; in such cases, polling provides more\ncontrol to the OS in its scheduling and thus is again useful.\noverlap, are best. If the speed of the device is not known, or sometimes\nfast and sometimes slow, it may be best to use a hybrid that polls for a\nlittle while and then, if the device is not yet ﬁnished, uses interrupts. This\ntwo-phased approach may achieve the best of both worlds.\nAnother reason not to use interrupts arises in networks [MR96]. When\na huge stream of incoming packets each generate an interrupt, it is pos-\nsible for the OS to livelock, that is, ﬁnd itself only processing interrupts\nand never allowing a user-level process to run and actually service the\nrequests. For example, imagine a web server that suddenly experiences\na high load due to the “slashdot effect”. In this case, it is better to occa-\nsionally use polling to better control what is happening in the system and\nallow the web server to service some requests before going back to the\ndevice to check for more packet arrivals.\nAnother interrupt-based optimization is coalescing. In such a setup, a\ndevice which needs to raise an interrupt ﬁrst waits for a bit before deliv-\nering the interrupt to the CPU. While waiting, other requests may soon\ncomplete, and thus multiple interrupts can be coalesced into a single in-\nterrupt delivery, thus lowering the overhead of interrupt processing. Of\ncourse, waiting too long will increase the latency of a request, a common\ntrade-off in systems. See Ahmad et al. [A+11] for an excellent summary.\n36.5\nMore Efﬁcient Data Movement With DMA\nUnfortunately, there is one other aspect of our canonical protocol that\nrequires our attention. In particular, when using programmed I/O (PIO)\nto transfer a large chunk of data to a device, the CPU is once again over-\nburdened with a rather trivial task, and thus wastes a lot of time and\neffort that could better be spent running other processes. This timeline\nillustrates the problem:\nCPU\nDisk\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\nc\nc\nc\n2\n2\n2\n2\n2\n1\n1\nIn the timeline, Process 1 is running and then wishes to write some data to\nthe disk. It then initiates the I/O, which must copy the data from memory\nto the device explicitly, one word at a time (marked c in the diagram).\nWhen the copy is complete, the I/O begins on the disk and the CPU can\nﬁnally be used for something else.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n394\nI/O DEVICES\nTHE CRUX: HOW TO LOWER PIO OVERHEADS\nWith PIO, the CPU spends too much time moving data to and from\ndevices by hand. How can we ofﬂoad this work and thus allow the CPU\nto be more effectively utilized?\nThe solution to this problem is something we refer to as Direct Mem-\nory Access (DMA). A DMA engine is essentially a very speciﬁc device\nwithin a system that can orchestrate transfers between devices and main\nmemory without much CPU intervention.\nDMA works as follows. To transfer data to the device, for example, the\nOS would program the DMA engine by telling it where the data lives in\nmemory, how much data to copy, and which device to send it to. At that\npoint, the OS is done with the transfer and can proceed with other work.\nWhen the DMA is complete, the DMA controller raises an interrupt, and\nthe OS thus knows the transfer is complete. The revised timeline:\nCPU\nDMA\nDisk\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n2\n2\n2\n2\n2\n2\n2\n2\n1\n1\nc\nc\nc\nFrom the timeline, you can see that the copying of data is now handled\nby the DMA controller. Because the CPU is free during that time, the OS\ncan do something else, here choosing to run Process 2. Process 2 thus gets\nto use more CPU before Process 1 runs again.\n36.6\nMethods Of Device Interaction\nNow that we have some sense of the efﬁciency issues involved with\nperforming I/O, there are a few other problems we need to handle to\nincorporate devices into modern systems. One problem you may have\nnoticed thus far: we have not really said anything about how the OS ac-\ntually communicates with the device! Thus, the problem:\nTHE CRUX: HOW TO COMMUNICATE WITH DEVICES\nHow should the hardware communicate with a device? Should there\nbe explicit instructions? Or are there other ways to do it?\nOver time, two primary methods of device communication have de-\nveloped. The ﬁrst, oldest method (used by IBM mainframes for many\nyears) is to have explicit I/O instructions. These instructions specify a\nway for the OS to send data to speciﬁc device registers and thus allow the\nconstruction of the protocols described above.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nI/O DEVICES\n395\nFor example, on x86, the in and out instructions can be used to com-\nmunicate with devices. For example, to send data to a device, the caller\nspeciﬁes a register with the data in it, and a speciﬁc port which names the\ndevice. Executing the instruction leads to the desired behavior.\nSuch instructions are usually privileged. The OS controls devices, and\nthe OS thus is the only entity allowed to directly communicate with them.\nImagine if any program could read or write the disk, for example: total\nchaos (as always), as any user program could use such a loophole to gain\ncomplete control over the machine.\nThe second method to interact with devices is known as memory-\nmapped I/O. With this approach, the hardware makes device registers\navailable as if they were memory locations. To access a particular register,\nthe OS issues a load (to read) or store (to write) the address; the hardware\nthen routes the load/store to the device instead of main memory.\nThere is not some great advantage to one approach or the other. The\nmemory-mapped approach is nice in that no new instructions are needed\nto support it, but both approaches are still in use today.\n36.7\nFitting Into The OS: The Device Driver\nOne ﬁnal problem we will discuss: how to ﬁt devices, each of which\nhave very speciﬁc interfaces, into the OS, which we would like to keep\nas general as possible. For example, consider a ﬁle system. We’d like\nto build a ﬁle system that worked on top of SCSI disks, IDE disks, USB\nkeychain drives, and so forth, and we’d like the ﬁle system to be relatively\noblivious to all of the details of how to issue a read or write request to\nthese difference types of drives. Thus, our problem:\nTHE CRUX: HOW TO BUILD A DEVICE-NEUTRAL OS\nHow can we keep most of the OS device-neutral, thus hiding the de-\ntails of device interactions from major OS subsystems?\nThe problem is solved through the age-old technique of abstraction.\nAt the lowest level, a piece of software in the OS must know in detail\nhow a device works. We call this piece of software a device driver, and\nany speciﬁcs of device interaction are encapsulated within.\nLet us see how this abstraction might help OS design and implemen-\ntation by examining the Linux ﬁle system software stack. Figure 36.3 is\na rough and approximate depiction of the Linux software organization.\nAs you can see from the diagram, a ﬁle system (and certainly, an appli-\ncation above) is completely oblivious to the speciﬁcs of which disk class\nit is using; it simply issues block read and write requests to the generic\nblock layer, which routes them to the appropriate device driver, which\nhandles the details of issuing the speciﬁc request. Although simpliﬁed,\nthe diagram shows how such detail can be hidden from most of the OS.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n396\nI/O DEVICES\nApplication\nFile System\nGeneric Block Layer\nDevice Driver [SCSI, ATA, etc.]\nPOSIX API [open, read, write, close, etc.]\nGeneric Block Interface [block read/write]\nSpecific Block Interface [protocol-specific read/write]\nuser\nkernel mode\nFigure 36.3: The File System Stack\nNote that such encapsulation can have its downside as well. For ex-\nample, if there is a device that has many special capabilities, but has to\npresent a generic interface to the rest of the kernel, those special capabili-\nties will go unused. This situation arises, for example, in Linux with SCSI\ndevices, which have very rich error reporting; because other block de-\nvices (e.g., ATA/IDE) have much simpler error handling, all that higher\nlevels of software ever receive is a generic EIO (generic IO error) error\ncode; any extra detail that SCSI may have provided is thus lost to the ﬁle\nsystem [G08].\nInterestingly, because device drivers are needed for any device you\nmight plug into your system, over time they have come to represent a\nhuge percentage of kernel code. Studies of the Linux kernel reveal that\nover 70% of OS code is found in device drivers [C01]; for Windows-based\nsystems, it is likely quite high as well. Thus, when people tell you that the\nOS has millions of lines of code, what they are really saying is that the OS\nhas millions of lines of device-driver code. Of course, for any given in-\nstallation, most of that code may not be active (i.e., only a few devices are\nconnected to the system at a time). Perhaps more depressingly, as drivers\nare often written by “amateurs” (instead of full-time kernel developers),\nthey tend to have many more bugs and thus are a primary contributor to\nkernel crashes [S03].\n36.8\nCase Study: A Simple IDE Disk Driver\nTo dig a little deeper here, let’s take a quick look at an actual device: an\nIDE disk drive [L94]. We summarize the protocol as described in this ref-\nerence [W10]; we’ll also peek at the xv6 source code for a simple example\nof a working IDE driver [CK+08].\nAn IDE disk presents a simple interface to the system, consisting of\nfour types of register: control, command block, status, and error. These\nregisters are available by reading or writing to speciﬁc “I/O addresses”\n(such as 0x3F6 below) using (on x86) the in and out I/O instructions.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nI/O DEVICES\n397\nControl Register:\nAddress 0x3F6 = 0x80 (0000 1RE0): R=reset, E=0 means \"enable interrupt\"\nCommand Block Registers:\nAddress 0x1F0 = Data Port\nAddress 0x1F1 = Error\nAddress 0x1F2 = Sector Count\nAddress 0x1F3 = LBA low byte\nAddress 0x1F4 = LBA mid byte\nAddress 0x1F5 = LBA hi\nbyte\nAddress 0x1F6 = 1B1D TOP4LBA: B=LBA, D=drive\nAddress 0x1F7 = Command/status\nStatus Register (Address 0x1F7):\n7\n6\n5\n4\n3\n2\n1\n0\nBUSY\nREADY FAULT SEEK\nDRQ\nCORR IDDEX ERROR\nError Register (Address 0x1F1): (check when Status ERROR==1)\n7\n6\n5\n4\n3\n2\n1\n0\nBBK\nUNC\nMC\nIDNF\nMCR\nABRT T0NF AMNF\nBBK\n= Bad Block\nUNC\n= Uncorrectable data error\nMC\n= Media Changed\nIDNF = ID mark Not Found\nMCR\n= Media Change Requested\nABRT = Command aborted\nT0NF = Track 0 Not Found\nAMNF = Address Mark Not Found\nFigure 36.4: The IDE Interface\nThe basic protocol to interact with the device is as follows, assuming\nit has already been initialized.\n• Wait for drive to be ready. Read Status Register (0x1F7) until drive\nis not busy and READY.\n• Write parameters to command registers. Write the sector count,\nlogical block address (LBA) of the sectors to be accessed, and drive\nnumber (master=0x00 or slave=0x10, as IDE permits just two drives)\nto command registers (0x1F2-0x1F6).\n• Start the I/O. by issuing read/write to command register. Write\nREAD—WRITE command to command register (0x1F7).\n• Data transfer (for writes): Wait until drive status is READY and\nDRQ (drive request for data); write data to data port.\n• Handle interrupts. In the simplest case, handle an interrupt for\neach sector transferred; more complex approaches allow batching\nand thus one ﬁnal interrupt when the entire transfer is complete.\n• Error handling. After each operation, read the status register. If the\nERROR bit is on, read the error register for details.\nMost of this protocol is found in the xv6 IDE driver (Figure 36.5),\nwhich (after initialization) works through four primary functions. The\nﬁrst is ide rw(), which queues a request (if there are others pending),\nor issues it directly to the disk (via ide start request()); in either\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n398\nI/O DEVICES\nstatic int ide_wait_ready() {\nwhile (((int r = inb(0x1f7)) & IDE_BSY) || !(r & IDE_DRDY))\n;\n// loop until drive isn’t busy\n}\nstatic void ide_start_request(struct buf *b) {\nide_wait_ready();\noutb(0x3f6, 0);\n// generate interrupt\noutb(0x1f2, 1);\n// how many sectors?\noutb(0x1f3, b->sector & 0xff);\n// LBA goes here ...\noutb(0x1f4, (b->sector >> 8) & 0xff);\n// ... and here\noutb(0x1f5, (b->sector >> 16) & 0xff);\n// ... and here!\noutb(0x1f6, 0xe0 | ((b->dev&1)<<4) | ((b->sector>>24)&0x0f));\nif(b->flags & B_DIRTY){\noutb(0x1f7, IDE_CMD_WRITE);\n// this is a WRITE\noutsl(0x1f0, b->data, 512/4);\n// transfer data too!\n} else {\noutb(0x1f7, IDE_CMD_READ);\n// this is a READ (no data)\n}\n}\nvoid ide_rw(struct buf *b) {\nacquire(&ide_lock);\nfor (struct buf **pp = &ide_queue; *pp; pp=&(*pp)->qnext)\n;\n// walk queue\n*pp = b;\n// add request to end\nif (ide_queue == b)\n// if q is empty\nide_start_request(b);\n// send req to disk\nwhile ((b->flags & (B_VALID|B_DIRTY)) != B_VALID)\nsleep(b, &ide_lock);\n// wait for completion\nrelease(&ide_lock);\n}\nvoid ide_intr() {\nstruct buf *b;\nacquire(&ide_lock);\nif (!(b->flags & B_DIRTY) && ide_wait_ready(1) >= 0)\ninsl(0x1f0, b->data, 512/4);\n// if READ: get data\nb->flags |= B_VALID;\nb->flags &= ˜B_DIRTY;\nwakeup(b);\n// wake waiting process\nif ((ide_queue = b->qnext) != 0) // start next request\nide_start_request(ide_queue);\n// (if one exists)\nrelease(&ide_lock);\n}\nFigure 36.5: The xv6 IDE Disk Driver (Simpliﬁed)\ncase, the routine waits for the request to complete and the calling pro-\ncess is put to sleep. The second is ide start request(), which is\nused to send a request (and perhaps data, in the case of a write) to the\ndisk; the in and out x86 instructions are called to read and write device\nregisters, respectively. The start request routine uses the third function,\nide wait ready(), to ensure the drive is ready before issuing a request\nto it. Finally, ide intr() is invoked when an interrupt takes place; it\nreads data from the device (if the request is a read, not a write), wakes the\nprocess waiting for the I/O to complete, and (if there are more requests\nin the I/O queue), launches the next I/O via ide start request().\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "page_number": 423,
      "chapter_number": 43,
      "summary": "This chapter covers segment 43 (pages 423-434). Key topics include devices, interrupting, and disk. As the dictionary would say: “a ﬁrm or obstinate continuance in a course\nof action in spite of difﬁculty or opposition.”\nStudent: It’s kind of like taking your class: some obstinance required.",
      "keywords": [
        "device",
        "CPU",
        "IDE",
        "Data",
        "system",
        "disk",
        "request",
        "register",
        "Write",
        "Device Driver",
        "device registers",
        "process",
        "interrupt",
        "read",
        "IDE Disk"
      ],
      "concepts": [
        "devices",
        "interrupting",
        "disk",
        "cpu",
        "data",
        "error",
        "writing",
        "write",
        "professor",
        "block"
      ],
      "similar_chapters": [
        {
          "book": "C++ Concurrency in Action",
          "chapter": 37,
          "title": "Segment 37 (pages 363-372)",
          "relevance_score": 0.58,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 1,
          "title": "Segment 1 (pages 1-20)",
          "relevance_score": 0.56,
          "method": "api"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 16,
          "title": "Segment 16 (pages 141-154)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 59,
          "title": "Segment 59 (pages 583-590)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "More Effective C++",
          "chapter": 8,
          "title": "Segment 8 (pages 61-75)",
          "relevance_score": 0.55,
          "method": "api"
        }
      ]
    },
    {
      "number": 44,
      "title": "Segment 44 (pages 435-443)",
      "start_page": 435,
      "end_page": 443,
      "detection_method": "topic_boundary",
      "content": "I/O DEVICES\n399\n36.9\nHistorical Notes\nBefore ending, we include a brief historical note on the origin of some\nof these fundamental ideas. If you are interested in learning more, read\nSmotherman’s excellent summary [S08].\nInterrupts are an ancient idea, existing on the earliest of machines. For\nexample, the UNIVAC in the early 1950’s had some form of interrupt vec-\ntoring, although it is unclear in exactly which year this feature was avail-\nable [S08]. Sadly, even in its infancy, we are beginning to lose the origins\nof computing history.\nThere is also some debate as to which machine ﬁrst introduced the idea\nof DMA. For example, Knuth and others point to the DYSEAC (a “mo-\nbile” machine, which at the time meant it could be hauled in a trailer),\nwhereas others think the IBM SAGE may have been the ﬁrst [S08]. Ei-\nther way, by the mid 50’s, systems with I/O devices that communicated\ndirectly with memory and interrupted the CPU when ﬁnished existed.\nThe history here is difﬁcult to trace because the inventions are tied to\nreal, and sometimes obscure, machines. For example, some think that the\nLincoln Labs TX-2 machine was ﬁrst with vectored interrupts [S08], but\nthis is hardly clear.\nBecause the ideas are relatively obvious – no Einsteinian leap is re-\nquired to come up with the idea of letting the CPU do something else\nwhile a slow I/O is pending – perhaps our focus on “who ﬁrst?” is mis-\nguided. What is certainly clear: as people built these early machines, it\nbecame obvious that I/O support was needed. Interrupts, DMA, and re-\nlated ideas are all direct outcomes of the nature of fast CPUs and slow\ndevices; if you were there at the time, you might have had similar ideas.\n36.10\nSummary\nYou should now have a very basic understanding of how an OS inter-\nacts with a device. Two techniques, the interrupt and DMA, have been\nintroduced to help with device efﬁciency, and two approaches to access-\ning device registers, explicit I/O instructions and memory-mapped I/O,\nhave been described. Finally, the notion of a device driver has been pre-\nsented, showing how the OS itself can encapsulate low-level details and\nthus make it easier to build the rest of the OS in a device-neutral fashion.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n400\nI/O DEVICES\nReferences\n[A+11] “vIC: Interrupt Coalescing for Virtual Machine Storage Device IO”\nIrfan Ahmad, Ajay Gulati, Ali Mashtizadeh\nUSENIX ’11\nA terriﬁc survey of interrupt coalescing in traditional and virtualized environments.\n[C01] “An Empirical Study of Operating System Errors”\nAndy Chou, Junfeng Yang, Benjamin Chelf, Seth Hallem, Dawson Engler\nSOSP ’01\nOne of the ﬁrst papers to systematically explore how many bugs are in modern operating systems.\nAmong other neat ﬁndings, the authors show that device drivers have something like seven times more\nbugs than mainline kernel code.\n[CK+08] “The xv6 Operating System”\nRuss Cox, Frans Kaashoek, Robert Morris, Nickolai Zeldovich\nFrom: http://pdos.csail.mit.edu/6.828/2008/index.html\nSee ide.c for the IDE device driver, with a few more details therein.\n[D07] “What Every Programmer Should Know About Memory”\nUlrich Drepper\nNovember, 2007\nAvailable: http://www.akkadia.org/drepper/cpumemory.pdf\nA fantastic read about modern memory systems, starting at DRAM and going all the way up to virtu-\nalization and cache-optimized algorithms.\n[G08] “EIO: Error-handling is Occasionally Correct”\nHaryadi Gunawi, Cindy Rubio-Gonzalez, Andrea Arpaci-Dusseau, Remzi Arpaci-Dusseau,\nBen Liblit\nFAST ’08, San Jose, CA, February 2008\nOur own work on building a tool to ﬁnd code in Linux ﬁle systems that does not handle error return\nproperly. We found hundreds and hundreds of bugs, many of which have now been ﬁxed.\n[L94] “AT Attachment Interface for Disk Drives”\nLawrence J. Lamers, X3T10 Technical Editor\nAvailable: ftp://ftp.t10.org/t13/project/d0791r4c-ATA-1.pdf\nReference number: ANSI X3.221 - 1994 A rather dry document about device interfaces. Read it at\nyour own peril.\n[MR96] “Eliminating Receive Livelock in an Interrupt-driven Kernel”\nJeffrey Mogul and K. K. Ramakrishnan\nUSENIX ’96, San Diego, CA, January 1996\nMogul and colleagues did a great deal of pioneering work on web server network performance. This\npaper is but one example.\n[S08] “Interrupts”\nMark Smotherman, as of July ’08\nAvailable: http://people.cs.clemson.edu/˜mark/interrupts.html\nA treasure trove of information on the history of interrupts, DMA, and related early ideas in computing.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nI/O DEVICES\n401\n[S03] “Improving the Reliability of Commodity Operating Systems”\nMichael M. Swift, Brian N. Bershad, and Henry M. Levy\nSOSP ’03\nSwift’s work revived interest in a more microkernel-like approach to operating systems; minimally, it\nﬁnally gave some good reasons why address-space based protection could be useful in a modern OS.\n[W10] “Hard Disk Driver”\nWashington State Course Homepage\nAvailable: http://eecs.wsu.edu/˜cs460/cs560/HDdriver.html\nA nice summary of a simple IDE disk drive’s interface and how to build a device driver for it.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n37\nHard Disk Drives\nThe last chapter introduced the general concept of an I/O device and\nshowed you how the OS might interact with such a beast. In this chapter,\nwe dive into more detail about one device in particular: the hard disk\ndrive. These drives have been the main form of persistent data storage in\ncomputer systems for decades and much of the development of ﬁle sys-\ntem technology (coming soon) is predicated on their behavior. Thus, it\nis worth understanding the details of a disk’s operation before building\nthe ﬁle system software that manages it. Many of these details are avail-\nable in excellent papers by Ruemmler and Wilkes [RW92] and Anderson,\nDykes, and Riedel [ADR03].\nCRUX: HOW TO STORE AND ACCESS DATA ON DISK\nHow do modern hard-disk drives store data? What is the interface?\nHow is the data actually laid out and accessed? How does disk schedul-\ning improve performance?\n37.1\nThe Interface\nLet’s start by understanding the interface to a modern disk drive. The\nbasic interface for all modern drives is straightforward. The drive consists\nof a large number of sectors (512-byte blocks), each of which can be read\nor written. The sectors are numbered from 0 to n −1 on a disk with n\nsectors. Thus, we can view the disk as an array of sectors; 0 to n −1 is\nthus the address space of the drive.\nMulti-sector operations are possible; indeed, many ﬁle systems will\nread or write 4KB at a time (or more). However, when updating the\ndisk, the only guarantee drive manufactures make is that a single 512-\nbyte write is atomic (i.e., it will either complete in its entirety or it won’t\ncomplete at all); thus, if an untimely power loss occurs, only a portion of\na larger write may complete (sometimes called a torn write).\n403\n\n\n404\nHARD DISK DRIVES\n0\n11\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\nSpindle\nFigure 37.1: A Disk With Just A Single Track\nThere are some assumptions most clients of disk drives make, but\nthat are not speciﬁed directly in the interface; Schlosser and Ganger have\ncalled this the “unwritten contract” of disk drives [SG04]. Speciﬁcally,\none can usually assume that accessing two blocks that are near one-another\nwithin the drive’s address space will be faster than accessing two blocks\nthat are far apart. One can also usually assume that accessing blocks in\na contiguous chunk (i.e., a sequential read or write) is the fastest access\nmode, and usually much faster than any more random access pattern.\n37.2\nBasic Geometry\nLet’s start to understand some of the components of a modern disk.\nWe start with a platter, a circular hard surface on which data is stored\npersistently by inducing magnetic changes to it. A disk may have one\nor more platters; each platter has 2 sides, each of which is called a sur-\nface. These platters are usually made of some hard material (such as\naluminum), and then coated with a thin magnetic layer that enables the\ndrive to persistently store bits even when the drive is powered off.\nThe platters are all bound together around the spindle, which is con-\nnected to a motor that spins the platters around (while the drive is pow-\nered on) at a constant (ﬁxed) rate. The rate of rotation is often measured in\nrotations per minute (RPM), and typical modern values are in the 7,200\nRPM to 15,000 RPM range. Note that we will often be interested in the\ntime of a single rotation, e.g., a drive that rotates at 10,000 RPM means\nthat a single rotation takes about 6 milliseconds (6 ms).\nData is encoded on each surface in concentric circles of sectors; we call\none such concentric circle a track. A single surface contains many thou-\nsands and thousands of tracks, tightly packed together, with hundreds of\ntracks ﬁtting into the width of a human hair.\nTo read and write from the surface, we need a mechanism that allows\nus to either sense (i.e., read) the magnetic patterns on the disk or to in-\nduce a change in (i.e., write) them. This process of reading and writing is\naccomplished by the disk head; there is one such head per surface of the\ndrive. The disk head is attached to a single disk arm, which moves across\nthe surface to position the head over the desired track.\n37.3\nA Simple Disk Drive\nLet’s understand how disks work by building up a model one track at\na time. Assume we have a simple disk with a single track (Figure 37.1).\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nHARD DISK DRIVES\n405\nHead\nArm\n0\n11\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\nSpindle\nRotates this way\nFigure 37.2: A Single Track Plus A Head\nThis track has just 12 sectors, each of which is 512 bytes in size (our\ntypical sector size, recall) and addressed therefore by the numbers 0 through\n11. The single platter we have here rotates around the spindle, to which\na motor is attached. Of course, the track by itself isn’t too interesting; we\nwant to be able to read or write those sectors, and thus we need a disk\nhead, attached to a disk arm, as we now see (Figure 37.2).\nIn the ﬁgure, the disk head, attached to the end of the arm, is posi-\ntioned over sector 6, and the surface is rotating counter-clockwise.\nSingle-track Latency: The Rotational Delay\nTo understand how a request would be processed on our simple, one-\ntrack disk, imagine we now receive a request to read block 0. How should\nthe disk service this request?\nIn our simple disk, the disk doesn’t have to do much. In particular, it\nmust just wait for the desired sector to rotate under the disk head. This\nwait happens often enough in modern drives, and is an important enough\ncomponent of I/O service time, that it has a special name: rotational de-\nlay (sometimes rotation delay, though that sounds weird). In the exam-\nple, if the full rotational delay is R, the disk has to incur a rotational delay\nof about R\n2 to wait for 0 to come under the read/write head (if we start at\n6). A worst-case request on this single track would be to sector 5, causing\nnearly a full rotational delay in order to service such a request.\nMultiple Tracks: Seek Time\nSo far our disk just has a single track, which is not too realistic; modern\ndisks of course have many millions. Let’s thus look at ever-so-slightly\nmore realistic disk surface, this one with three tracks (Figure 37.3, left).\nIn the ﬁgure, the head is currently positioned over the innermost track\n(which contains sectors 24 through 35); the next track over contains the\nnext set of sectors (12 through 23), and the outermost track contains the\nﬁrst sectors (0 through 11).\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n406\nHARD DISK DRIVES\n0\n11\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\n12\n23\n22\n21\n20\n19\n18\n17\n16\n15\n14\n13\n24\n35\n34\n33\n32\n31\n30\n29\n28\n27\n26\n25\nSpindle\nRotates this way\nSeek\nRemaining rotation\n3\n2\n1\n0\n11\n10\n9\n8\n7\n6\n5\n4\n15\n14\n13\n12\n23\n22\n21\n20\n19\n18\n17\n16\n27\n26\n25\n24\n35\n34\n33\n32\n31\n30\n29\n28\nSpindle\nRotates this way\nFigure 37.3: Three Tracks Plus A Head (Right: With Seek)\nTo understand how the drive might access a given sector, we now trace\nwhat would happen on a request to a distant sector, e.g., a read to sector\n11. To service this read, the drive has to ﬁrst move the disk arm to the cor-\nrect track (in this case, the outermost one), in a process known as a seek.\nSeeks, along with rotations, are one of the most costly disk operations.\nThe seek, it should be noted, has many phases: ﬁrst an acceleration\nphase as the disk arm gets moving; then coasting as the arm is moving\nat full speed, then deceleration as the arm slows down; ﬁnally settling as\nthe head is carefully positioned over the correct track. The settling time\nis often quite signiﬁcant, e.g., 0.5 to 2 ms, as the drive must be certain to\nﬁnd the right track (imagine if it just got close instead!).\nAfter the seek, the disk arm has positioned the head over the right\ntrack. A depiction of the seek is found in Figure 37.3 (right).\nAs we can see, during the seek, the arm has been moved to the desired\ntrack, and the platter of course has rotated, in this case about 3 sectors.\nThus, sector 9 is just about to pass under the disk head, and we must\nonly endure a short rotational delay to complete the transfer.\nWhen sector 11 passes under the disk head, the ﬁnal phase of I/O\nwill take place, known as the transfer, where data is either read from or\nwritten to the surface. And thus, we have a complete picture of I/O time:\nﬁrst a seek, then waiting for the rotational delay, and ﬁnally the transfer.\nSome Other Details\nThough we won’t spend too much time on it, there are some other inter-\nesting details about how hard drives operate. Many drives employ some\nkind of track skew to make sure that sequential reads can be properly\nserviced even when crossing track boundaries. In our simple example\ndisk, this might appear as seen in Figure 37.4.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nHARD DISK DRIVES\n407\nTrack skew: 2 blocks\n0\n11\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\n22\n21\n20\n19\n18\n17\n16\n15\n14\n13\n12\n23\n32\n31\n30\n29\n28\n27\n26\n25\n24\n35\n34\n33\nSpindle\nRotates this way\nFigure 37.4: Three Tracks: Track Skew Of 2\nSectors are often skewed like this because when switching from one\ntrack to another, the disk needs time to reposition the head (even to neigh-\nboring tracks). Without such skew, the head would be moved to the next\ntrack but the desired next block would have already rotated under the\nhead, and thus the drive would have to wait almost the entire rotational\ndelay to access the next block.\nAnother reality is that outer tracks tend to have more sectors than\ninner tracks, which is a result of geometry; there is simply more room\nout there. These tracks are often referred to as multi-zoned disk drives,\nwhere the disk is organized into multiple zones, and where a zone is con-\nsecutive set of tracks on a surface. Each zone has the same number of\nsectors per track, and outer zones have more sectors than inner zones.\nFinally, an important part of any modern disk drive is its cache, for\nhistorical reasons sometimes called a track buffer. This cache is just some\nsmall amount of memory (usually around 8 or 16 MB) which the drive\ncan use to hold data read from or written to the disk. For example, when\nreading a sector from the disk, the drive might decide to read in all of the\nsectors on that track and cache them in its memory; doing so allows the\ndrive to quickly respond to any subsequent requests to the same track.\nOn writes, the drive has a choice: should it acknowledge the write has\ncompleted when it has put the data in its memory, or after the write has\nactually been written to disk? The former is called write back caching\n(or sometimes immediate reporting), and the latter write through. Write\nback caching sometimes makes the drive appear “faster”, but can be dan-\ngerous; if the ﬁle system or applications require that data be written to\ndisk in a certain order for correctness, write-back caching can lead to\nproblems (read the chapter on ﬁle-system journaling for details).\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "page_number": 435,
      "chapter_number": 44,
      "summary": "This chapter covers segment 44 (pages 435-443). Key topics include track, disk, and drives.",
      "keywords": [
        "Disk",
        "Hard Disk Drives",
        "Disk Drives",
        "Track",
        "disk head",
        "drive",
        "Hard Disk",
        "modern disk drive",
        "head",
        "sectors",
        "disk arm",
        "Single Track",
        "read",
        "device",
        "IDE disk drive"
      ],
      "concepts": [
        "track",
        "disk",
        "drives",
        "sectors",
        "devices",
        "hardly",
        "interrupts",
        "ideas",
        "write",
        "writing"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 33,
          "title": "Segment 33 (pages 662-682)",
          "relevance_score": 0.63,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 10,
          "title": "Segment 10 (pages 80-87)",
          "relevance_score": 0.63,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 11,
          "title": "Segment 11 (pages 89-108)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 59,
          "title": "Segment 59 (pages 583-590)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "Building LLM Powered Applications",
          "chapter": 59,
          "title": "Segment 59 (pages 499-500)",
          "relevance_score": 0.61,
          "method": "api"
        }
      ]
    },
    {
      "number": 45,
      "title": "Segment 45 (pages 444-451)",
      "start_page": 444,
      "end_page": 451,
      "detection_method": "topic_boundary",
      "content": "408\nHARD DISK DRIVES\nASIDE: DIMENSIONAL ANALYSIS\nRemember in Chemistry class, how you solved virtually every prob-\nlem by simply setting up the units such that they canceled out, and some-\nhow the answers popped out as a result? That chemical magic is known\nby the highfalutin name of dimensional analysis and it turns out it is\nuseful in computer systems analysis too.\nLet’s do an example to see how dimensional analysis works and why\nit is useful. In this case, assume you have to ﬁgure out how long, in mil-\nliseconds, a single rotation of a disk takes. Unfortunately, you are given\nonly the RPM of the disk, or rotations per minute. Let’s assume we’re\ntalking about a 10K RPM disk (i.e., it rotates 10,000 times per minute).\nHow do we set up the dimensional analysis so that we get time per rota-\ntion in milliseconds?\nTo do so, we start by putting the desired units on the left; in this case,\nwe wish to obtain the time (in milliseconds) per rotation, so that is ex-\nactly what we write down:\nT ime (ms)\n1 Rotation. We then write down everything\nwe know, making sure to cancel units where possible. First, we obtain\n1 minute\n10,000 Rotations (keeping rotation on the bottom, as that’s where it is on\nthe left), then transform minutes into seconds with 60 seconds\n1 minute , and then\nﬁnally transform seconds in milliseconds with 1000 ms\n1 second. The ﬁnal result is\nthis equation, with units nicely canceled, is:\nT ime (ms)\n1 Rot.\n=\n1\u0018\u0018\u0018\nminute\n10,000 Rot. · 60\u0018\u0018\u0018\nseconds\n1\u0018\u0018\u0018\nminute · 1000 ms\n1\u0018\u0018\u0018\nsecond =\n60,000 ms\n10,000 Rot. =\n6 ms\nRotation\nAs you can see from this example, dimensional analysis makes what\nseems obvious into a simple and repeatable process. Beyond the RPM\ncalculation above, it comes in handy with I/O analysis regularly. For\nexample, you will often be given the transfer rate of a disk, e.g.,\n100 MB/second, and then asked: how long does it take to transfer a\n512 KB block (in milliseconds)? With dimensional analysis, it’s easy:\nT ime (ms)\n1 Request =\n512\b\b\nKB\n1 Request ·\n1\b\b\nMB\n1024\b\b\nKB · 1\u0018\u0018\u0018\nsecond\n100\b\b\nMB · 1000 ms\n1\u0018\u0018\u0018\nsecond =\n5 ms\nRequest\n37.4\nI/O Time: Doing The Math\nNow that we have an abstract model of the disk, we can use a little\nanalysis to better understand disk performance. In particular, we can\nnow represent I/O time as the sum of three major components:\nTI/O = Tseek + Trotation + Ttransfer\n(37.1)\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nHARD DISK DRIVES\n409\nCheetah 15K.5\nBarracuda\nCapacity\n300 GB\n1 TB\nRPM\n15,000\n7,200\nAverage Seek\n4 ms\n9 ms\nMax Transfer\n125 MB/s\n105 MB/s\nPlatters\n4\n4\nCache\n16 MB\n16/32 MB\nConnects via\nSCSI\nSATA\nTable 37.1: Disk Drive Specs: SCSI Versus SATA\nNote that the rate of I/O (RI/O), which is often more easily used for\ncomparison between drives (as we will do below), is easily computed\nfrom the time. Simply divide the size of the transfer by the time it took:\nRI/O = SizeT ransfer\nTI/O\n(37.2)\nTo get a better feel for I/O time, let us perform the following calcu-\nlation. Assume there are two workloads we are interested in. The ﬁrst,\nknown as the random workload, issues small (e.g., 4KB) reads to random\nlocations on the disk. Random workloads are common in many impor-\ntant applications, including database management systems. The second,\nknown as the sequential workload, simply reads a large number of sec-\ntors consecutively from the disk, without jumping around. Sequential\naccess patterns are quite common and thus important as well.\nTo understand the difference in performance between random and se-\nquential workloads, we need to make a few assumptions about the disk\ndrive ﬁrst. Let’s look at a couple of modern disks from Seagate. The ﬁrst,\nknown as the Cheetah 15K.5 [S09b], is a high-performance SCSI drive.\nThe second, the Barracuda [S09a], is a drive built for capacity. Details on\nboth are found in Table 37.1.\nAs you can see, the drives have quite different characteristics, and\nin many ways nicely summarize two important components of the disk\ndrive market. The ﬁrst is the “high performance” drive market, where\ndrives are engineered to spin as fast as possible, deliver low seek times,\nand transfer data quickly. The second is the “capacity” market, where\ncost per byte is the most important aspect; thus, the drives are slower but\npack as many bits as possible into the space available.\nFrom these numbers, we can start to calculate how well the drives\nwould do under our two workloads outlined above. Let’s start by looking\nat the random workload. Assuming each 4 KB read occurs at a random\nlocation on disk, we can calculate how long each such read would take.\nOn the Cheetah:\nTseek = 4 ms, Trotation = 2 ms, Ttransfer = 30 microsecs\n(37.3)\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n410\nHARD DISK DRIVES\nTIP: USE DISKS SEQUENTIALLY\nWhen at all possible, transfer data to and from disks in a sequential man-\nner. If sequential is not possible, at least think about transferring data\nin large chunks: the bigger, the better. If I/O is done in little random\npieces, I/O performance will suffer dramatically. Also, users will suffer.\nAlso, you will suffer, knowing what suffering you have wrought with\nyour careless random I/Os.\nThe average seek time (4 milliseconds) is just taken as the average time\nreported by the manufacturer; note that a full seek (from one end of the\nsurface to the other) would likely take two or three times longer. The\naverage rotational delay is calculated from the RPM directly. 15000 RPM\nis equal to 250 RPS (rotations per second); thus, each rotation takes 4 ms.\nOn average, the disk will encounter a half rotation and thus 2 ms is the\naverage time. Finally, the transfer time is just the size of the transfer over\nthe peak transfer rate; here it is vanishingly small (30 microseconds; note\nthat we need 1000 microseconds just to get 1 millisecond!).\nThus, from our equation above, TI/O for the Cheetah roughly equals\n6 ms. To compute the rate of I/O, we just divide the size of the transfer\nby the average time, and thus arrive at RI/O for the Cheetah under the\nrandom workload of about 0.66 MB/s. The same calculation for the Bar-\nracuda yields a TI/O of about 13.2 ms, more than twice as slow, and thus\na rate of about 0.31 MB/s.\nNow let’s look at the sequential workload. Here we can assume there\nis a single seek and rotation before a very long transfer. For simplicity,\nassume the size of the transfer is 100 MB. Thus, TI/O for the Barracuda\nand Cheetah is about 800 ms and 950 ms, respectively. The rates of I/O\nare thus very nearly the peak transfer rates of 125 MB/s and 105 MB/s,\nrespectively. Table 37.2 summarizes these numbers.\nThe table shows us a number of important things. First, and most\nimportantly, there is a huge gap in drive performance between random\nand sequential workloads, almost a factor of 200 or so for the Cheetah\nand more than a factor 300 difference for the Barracuda. And thus we\narrive at the most obvious design tip in the history of computing.\nA second, more subtle point: there is a large difference in performance\nbetween high-end “performance” drives and low-end “capacity” drives.\nFor this reason (and others), people are often willing to pay top dollar for\nthe former while trying to get the latter as cheaply as possible.\nCheetah\nBarracuda\nRI/O Random\n0.66 MB/s\n0.31 MB/s\nRI/O Sequential\n125 MB/s\n105 MB/s\nTable 37.2: Disk Drive Performance: SCSI Versus SATA\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nHARD DISK DRIVES\n411\nASIDE: COMPUTING THE “AVERAGE” SEEK\nIn many books and papers, you will see average disk-seek time cited\nas being roughly one-third of the full seek time. Where does this come\nfrom?\nTurns out it arises from a simple calculation based on average seek\ndistance, not time. Imagine the disk as a set of tracks, from 0 to N. The\nseek distance between any two tracks x and y is thus computed as the\nabsolute value of the difference between them: |x −y|.\nTo compute the average seek distance, all you need to do is to ﬁrst add\nup all possible seek distances:\nN\nX\nx=0\nN\nX\ny=0\n|x −y|.\n(37.4)\nThen, divide this by the number of different possible seeks: N 2. To\ncompute the sum, we’ll just use the integral form:\nZ N\nx=0\nZ N\ny=0\n|x −y| dy dx.\n(37.5)\nTo compute the inner integral, let’s break out the absolute value:\nZ x\ny=0\n(x −y) dy +\nZ N\ny=x\n(y −x) dy.\n(37.6)\nSolving this leads to (xy −1\n2y2)\n\f\fx\n0 + ( 1\n2y2 −xy)\n\f\fN\nx which can be sim-\npliﬁed to (x2 −Nx + 1\n2N 2). Now we have to compute the outer integral:\nZ N\nx=0\n(x2 −Nx + 1\n2N 2) dx,\n(37.7)\nwhich results in:\n(1\n3x3 −N\n2 x2 + N 2\n2 x)\n\f\f\f\f\nN\n0\n= N 3\n3 .\n(37.8)\nRemember that we still have to divide by the total number of seeks\n(N 2) to compute the average seek distance: ( N3\n3 )/(N 2) =\n1\n3N. Thus the\naverage seek distance on a disk, over all possible seeks, is one-third the\nfull distance. And now when you hear that an average seek is one-third\nof a full seek, you’ll know where it came from.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n412\nHARD DISK DRIVES\n0\n11\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\n12\n23\n22\n21\n20\n19\n18\n17\n16\n15\n14\n13\n24\n35\n34\n33\n32\n31\n30\n29\n28\n27\n26\n25\nSpindle\nRotates this way\nFigure 37.5: SSTF: Scheduling Requests 21 And 2\n37.5\nDisk Scheduling\nBecause of the high cost of I/O, the OS has historically played a role in\ndeciding the order of I/Os issued to the disk. More speciﬁcally, given a\nset of I/O requests, the disk scheduler examines the requests and decides\nwhich one to schedule next [SCO90, JW91].\nUnlike job scheduling, where the length of each job is usually un-\nknown, with disk scheduling, we can make a good guess at how long\na “job” (i.e., disk request) will take. By estimating the seek and possible\nthe rotational delay of a request, the disk scheduler can know how long\neach request will take, and thus (greedily) pick the one that will take the\nleast time to service ﬁrst. Thus, the disk scheduler will try to follow the\nprinciple of SJF (shortest job ﬁrst) in its operation.\nSSTF: Shortest Seek Time First\nOne early disk scheduling approach is known as shortest-seek-time-ﬁrst\n(SSTF) (also called shortest-seek-ﬁrst or SSF). SSTF orders the queue of\nI/O requests by track, picking requests on the nearest track to complete\nﬁrst. For example, assuming the current position of the head is over the\ninner track, and we have requests for sectors 21 (middle track) and 2\n(outer track), we would then issue the request to 21 ﬁrst, wait for it to\ncomplete, and then issue the request to 2 (Figure 37.5).\nSSTF works well in this example, seeking to the middle track ﬁrst and\nthen the outer track. However, SSTF is not a panacea, for the following\nreasons. First, the drive geometry is not available to the host OS; rather,\nit sees an array of blocks. Fortunately, this problem is rather easily ﬁxed.\nInstead of SSTF, an OS can simply implement nearest-block-ﬁrst (NBF),\nwhich schedules the request with the nearest block address next.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nHARD DISK DRIVES\n413\nThe second problem is more fundamental: starvation.\nImagine in\nour example above if there were a steady stream of requests to the in-\nner track, where the head currently is positioned. Requests to any other\ntracks would then be ignored completely by a pure SSTF approach. And\nthus the crux of the problem:\nCRUX: HOW TO HANDLE DISK STARVATION\nHow can we implement SSTF-like scheduling but avoid starvation?\nElevator (a.k.a. SCAN or C-SCAN)\nThe answer to this query was developed some time ago (see [CKR72]\nfor example), and is relatively straightforward. The algorithm, originally\ncalled SCAN, simply moves across the disk servicing requests in order\nacross the tracks. Let us call a single pass across the disk a sweep. Thus, if\na request comes for a block on a track that has already been serviced on\nthis sweep of the disk, it is not handled immediately, but rather queued\nuntil the next sweep.\nSCAN has a number of variants, all of which do about the same thing.\nFor example, Coffman et al. introduced F-SCAN, which freezes the queue\nto be serviced when it is doing a sweep [CKR72]; this action places re-\nquests that come in during the sweep into a queue to be serviced later.\nDoing so avoids starvation of far-away requests, by delaying the servic-\ning of late-arriving (but nearer by) requests.\nC-SCAN is another common variant, short for Circular SCAN. In-\nstead of sweeping in one direction across the disk, the algorithm sweeps\nfrom outer-to-inner, and then inner-to-outer, etc.\nFor reasons that should now be obvious, this algorithm (and its vari-\nants) is sometimes referred to as the elevator algorithm, because it be-\nhaves like an elevator which is either going up or down and not just ser-\nvicing requests to ﬂoors based on which ﬂoor is closer. Imagine how an-\nnoying it would be if you were going down from ﬂoor 10 to 1, and some-\nbody got on at 3 and pressed 4, and the elevator went up to 4 because it\nwas “closer” than 1! As you can see, the elevator algorithm, when used\nin real life, prevents ﬁghts from taking place on elevators. In disks, it just\nprevents starvation.\nUnfortunately, SCAN and its cousins do not represent the best schedul-\ning technology. In particular, SCAN (or SSTF even) do not actually adhere\nas closely to the principle of SJF as they could. In particular, they ignore\nrotation. And thus, another crux:\nCRUX: HOW TO ACCOUNT FOR DISK ROTATION COSTS\nHow can we implement an algorithm that more closely approximates SJF\nby taking both seek and rotation into account?\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n414\nHARD DISK DRIVES\n0\n11\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\n12\n23\n22\n21\n20\n19\n18\n17\n16\n15\n14\n13\n24\n35\n34\n33\n32\n31\n30\n29\n28\n27\n26\n25\nSpindle\nRotates this way\nFigure 37.6: SSTF: Sometimes Not Good Enough\nSPTF: Shortest Positioning Time First\nBefore discussing shortest positioning time ﬁrst or SPTF scheduling (some-\ntimes also called shortest access time ﬁrst or SATF), which is the solution\nto our problem, let us make sure we understand the problem in more de-\ntail. Figure 37.6 presents an example.\nIn the example, the head is currently positioned over sector 30 on the\ninner track. The scheduler thus has to decide: should it schedule sector 16\n(on the middle track) or sector 8 (on the outer track) for its next request.\nSo which should it service next?\nThe answer, of course, is “it depends”. In engineering, it turns out\n“it depends” is almost always the answer, reﬂecting that trade-offs are\npart of the life of the engineer; such maxims are also good in a pinch,\ne.g., when you don’t know an answer to your boss’s question, you might\nwant to try this gem. However, it is almost always better to know why it\ndepends, which is what we discuss here.\nWhat it depends on here is the relative time of seeking as compared\nto rotation. If, in our example, seek time is much higher than rotational\ndelay, then SSTF (and variants) are just ﬁne. However, imagine if seek is\nquite a bit faster than rotation. Then, in our example, it would make more\nsense to seek further to service request 8 on the outer track than it would\nto perform the shorter seek to the middle track to service 16, which has to\nrotate all the way around before passing under the disk head.\nOn modern drives, as we saw above, both seek and rotation are roughly\nequivalent (depending, of course, on the exact requests), and thus SPTF\nis useful and improves performance. However, it is even more difﬁcult\nto implement in an OS, which generally does not have a good idea where\ntrack boundaries are or where the disk head currently is (in a rotational\nsense). Thus, SPTF is usually performed inside a drive, described below.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nHARD DISK DRIVES\n415\nTIP: IT ALWAYS DEPENDS (LIVNY’S LAW)\nAlmost any question can be answered with “it depends”, as our colleague\nMiron Livny always says. However, use with caution, as if you answer\ntoo many questions this way, people will stop asking you questions alto-\ngether. For example, somebody asks: “want to go to lunch?” You reply:\n“it depends, are you coming along?”\nOther Scheduling Issues\nThere are many other issues we do not discuss in this brief description\nof basic disk operation, scheduling, and related topics.\nOne such is-\nsue is this: where is disk scheduling performed on modern systems? In\nolder systems, the operating system did all the scheduling; after looking\nthrough the set of pending requests, the OS would pick the best one, and\nissue it to the disk. When that request completed, the next one would be\nchosen, and so forth. Disks were simpler then, and so was life.\nIn modern systems, disks can accommodate multiple outstanding re-\nquests, and have sophisticated internal schedulers themselves (which can\nimplement SPTF accurately; inside the disk controller, all relevant details\nare available, including exact head position). Thus, the OS scheduler usu-\nally picks what it thinks the best few requests are (say 16) and issues them\nall to disk; the disk then uses its internal knowledge of head position and\ndetailed track layout information to service said requests in the best pos-\nsible (SPTF) order.\nAnother important related task performed by disk schedulers is I/O\nmerging. For example, imagine a series of requests to read blocks 33,\nthen 8, then 34, as in Figure 37.6. In this case, the scheduler should merge\nthe requests for blocks 33 and 34 into a single two-block request; any re-\nordering that the scheduler does is performed upon the merged requests.\nMerging is particularly important at the OS level, as it reduces the num-\nber of requests sent to the disk and thus lowers overheads.\nOne ﬁnal problem that modern schedulers address is this: how long\nshould the system wait before issuing an I/O to disk? One might naively\nthink that the disk, once it has even a single I/O, should immediately\nissue the request to the drive; this approach is called work-conserving, as\nthe disk will never be idle if there are requests to serve. However, research\non anticipatory disk scheduling has shown that sometimes it is better to\nwait for a bit [ID01], in what is called a non-work-conserving approach.\nBy waiting, a new and “better” request may arrive at the disk, and thus\noverall efﬁciency is increased. Of course, deciding when to wait, and for\nhow long, can be tricky; see the research paper for details, or check out\nthe Linux kernel implementation to see how such ideas are transitioned\ninto practice (if you are the ambitious sort).\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "page_number": 444,
      "chapter_number": 45,
      "summary": "This chapter covers segment 45 (pages 444-451). Key topics include disk, request, and requests.",
      "keywords": [
        "HARD DISK DRIVES",
        "DISK",
        "DISK DRIVES",
        "HARD DISK",
        "time",
        "Seek",
        "Requests",
        "DRIVES",
        "Disk Scheduling",
        "Request",
        "Average Seek",
        "seek time",
        "SSTF",
        "Disk Drive Performance",
        "track"
      ],
      "concepts": [
        "disk",
        "request",
        "requests",
        "scheduling",
        "schedule",
        "times",
        "rotation",
        "rotations",
        "rotates",
        "drives"
      ],
      "similar_chapters": [
        {
          "book": "Building LLM Powered Applications",
          "chapter": 59,
          "title": "Segment 59 (pages 499-500)",
          "relevance_score": 0.63,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 14,
          "title": "Segment 14 (pages 127-134)",
          "relevance_score": 0.61,
          "method": "api"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 12,
          "title": "Segment 12 (pages 91-98)",
          "relevance_score": 0.61,
          "method": "api"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 26,
          "title": "Segment 26 (pages 208-215)",
          "relevance_score": 0.61,
          "method": "api"
        },
        {
          "book": "AI Engineering Building Applications",
          "chapter": 26,
          "title": "Segment 26 (pages 518-535)",
          "relevance_score": 0.6,
          "method": "api"
        }
      ]
    },
    {
      "number": 46,
      "title": "Segment 46 (pages 452-475)",
      "start_page": 452,
      "end_page": 475,
      "detection_method": "topic_boundary",
      "content": "416\nHARD DISK DRIVES\n37.6\nSummary\nWe have presented a summary of how disks work. The summary is\nactually a detailed functional model; it does not describe the amazing\nphysics, electronics, and material science that goes into actual drive de-\nsign. For those interested in even more details of that nature, we suggest\na different major (or perhaps minor); for those that are happy with this\nmodel, good! We can now proceed to using the model to build more in-\nteresting systems on top of these incredible devices.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nHARD DISK DRIVES\n417\nReferences\n[ADR03] “More Than an Interface: SCSI vs. ATA”\nDave Anderson, Jim Dykes, Erik Riedel\nFAST ’03, 2003\nOne of the best recent-ish references on how modern disk drives really work; a must read for anyone\ninterested in knowing more.\n[CKR72] “Analysis of Scanning Policies for Reducing Disk Seek Times”\nE.G. Coffman, L.A. Klimko, B. Ryan\nSIAM Journal of Computing, September 1972, Vol 1. No 3.\nSome of the early work in the ﬁeld of disk scheduling.\n[ID01] “Anticipatory Scheduling: A Disk-scheduling Framework\nTo Overcome Deceptive Idleness In Synchronous I/O”\nSitaram Iyer, Peter Druschel\nSOSP ’01, October 2001\nA cool paper showing how waiting can improve disk scheduling: better requests may be on their way!\n[JW91] “Disk Scheduling Algorithms Based On Rotational Position”\nD. Jacobson, J. Wilkes\nTechnical Report HPL-CSP-91-7rev1, Hewlett-Packard (February 1991)\nA more modern take on disk scheduling. It remains a technical report (and not a published paper)\nbecause the authors were scooped by Seltzer et al. [SCO90].\n[RW92] “An Introduction to Disk Drive Modeling”\nC. Ruemmler, J. Wilkes\nIEEE Computer, 27:3, pp. 17-28, March 1994\nA terriﬁc introduction to the basics of disk operation. Some pieces are out of date, but most of the basics\nremain.\n[SCO90] “Disk Scheduling Revisited”\nMargo Seltzer, Peter Chen, John Ousterhout\nUSENIX 1990\nA paper that talks about how rotation matters too in the world of disk scheduling.\n[SG04] “MEMS-based storage devices and standard disk interfaces: A square peg in a round\nhole?”\nSteven W. Schlosser, Gregory R. Ganger\nFAST ’04, pp. 87-100, 2004\nWhile the MEMS aspect of this paper hasn’t yet made an impact, the discussion of the contract between\nﬁle systems and disks is wonderful and a lasting contribution.\n[S09a] “Barracuda ES.2 data sheet”\nhttp://www.seagate.com/docs/pdf/datasheet/disc/ds cheetah 15k 5.pdf A data\nsheet; read at your own risk. Risk of what? Boredom.\n[S09b] “Cheetah 15K.5”\nhttp://www.seagate.com/docs/pdf/datasheet/disc/ds barracuda es.pdf See above\ncommentary on data sheets.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n418\nHARD DISK DRIVES\nHomework\nThis homework uses disk.py to familiarize you with how a modern\nhard drive works. It has a lot of different options, and unlike most of\nthe other simulations, has a graphical animator to show you exactly what\nhappens when the disk is in action. See the README for details.\n1. Compute the seek, rotation, and transfer times for the following\nsets of requests: -a 0, -a 6, -a 30, -a 7,30,8, and ﬁnally -a\n10,11,12,13.\n2. Do the same requests above, but change the seek rate to different\nvalues: -S 2, -S 4, -S 8, -S 10, -S 40, -S 0.1. How do the\ntimes change?\n3. Do the same requests above, but change the rotation rate: -R 0.1,\n-R 0.5, -R 0.01. How do the times change?\n4. You might have noticed that some request streams would be bet-\nter served with a policy better than FIFO. For example, with the\nrequest stream -a 7,30,8, what order should the requests be pro-\ncessed in? Now run the shortest seek-time ﬁrst (SSTF) scheduler\n(-p SSTF) on the same workload; how long should it take (seek,\nrotation, transfer) for each request to be served?\n5. Now do the same thing, but using the shortest access-time ﬁrst\n(SATF) scheduler (-p SATF). Does it make any difference for the\nset of requests as speciﬁed by -a 7,30,8? Find a set of requests\nwhere SATF does noticeably better than SSTF; what are the condi-\ntions for a noticeable difference to arise?\n6. You might have noticed that the request stream -a 10,11,12,13\nwasn’t particularly well handled by the disk. Why is that? Can you\nintroduce a track skew to address this problem (-o skew, where\nskew is a non-negative integer)? Given the default seek rate, what\nshould the skew be to minimize the total time for this set of re-\nquests? What about for different seek rates (e.g., -S 2, -S 4)? In\ngeneral, could you write a formula to ﬁgure out the skew, given the\nseek rate and sector layout information?\n7. Multi-zone disks pack more sectors into the outer tracks. To conﬁg-\nure this disk in such a way, run with the -z ﬂag. Speciﬁcally, try\nrunning some requests against a disk run with -z 10,20,30 (the\nnumbers specify the angular space occupied by a sector, per track;\nin this example, the outer track will be packed with a sector every\n10 degrees, the middle track every 20 degrees, and the inner track\nwith a sector every 30 degrees). Run some random requests (e.g.,\n-a -1 -A 5,-1,0, which speciﬁes that random requests should\nbe used via the -a -1 ﬂag and that ﬁve requests ranging from 0 to\nthe max be generated), and see if you can compute the seek, rota-\ntion, and transfer times. Use different random seeds (-s 1, -s 2,\netc.). What is the bandwidth (in sectors per unit time) on the outer,\nmiddle, and inner tracks?\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nHARD DISK DRIVES\n419\n8. Scheduling windows determine how many sector requests a disk\ncan examine at once in order to determine which sector to serve\nnext. Generate some random workloads of a lot of requests (e.g.,\n-A 1000,-1,0, with different seeds perhaps) and see how long\nthe SATF scheduler takes when the scheduling window is changed\nfrom 1 up to the number of requests (e.g., -w 1 up to -w 1000,\nand some values in between). How big of scheduling window is\nneeded to approach the best possible performance? Make a graph\nand see. Hint: use the -c ﬂag and don’t turn on graphics with -G\nto run these more quickly. When the scheduling window is set to 1,\ndoes it matter which policy you are using?\n9. Avoiding starvation is important in a scheduler. Can you think of a\nseries of requests such that a particular sector is delayed for a very\nlong time given a policy such as SATF? Given that sequence, how\ndoes it perform if you use a bounded SATF or BSATF scheduling\napproach? In this approach, you specify the scheduling window\n(e.g., -w 4) as well as the BSATF policy (-p BSATF); the scheduler\nthen will only move onto the next window of requests when all of\nthe requests in the current window have been serviced. Does this\nsolve the starvation problem? How does it perform, as compared\nto SATF? In general, how should a disk make this trade-off between\nperformance and starvation avoidance?\n10. All the scheduling policies we have looked at thus far are greedy,\nin that they simply pick the next best option instead of looking for\nthe optimal schedule over a set of requests. Can you ﬁnd a set of\nrequests in which this greedy approach is not optimal?\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n38\nRedundant Arrays of Inexpensive Disks\n(RAIDs)\nWhen we use a disk, we sometimes wish it to be faster; I/O operations\nare slow and thus can be the bottleneck for the entire system. When we\nuse a disk, we sometimes wish it to be larger; more and more data is being\nput online and thus our disks are getting fuller and fuller. When we use\na disk, we sometimes wish for it to be more reliable; when a disk fails, if\nour data isn’t backed up, all that valuable data is gone.\nCRUX: HOW TO MAKE A LARGE, FAST, RELIABLE DISK\nHow can we make a large, fast, and reliable storage system? What are\nthe key techniques? What are trade-offs between different approaches?\nIn this chapter, we introduce the Redundant Array of Inexpensive\nDisks better known as RAID [P+88], a technique to use multiple disks in\nconcert to build a faster, bigger, and more reliable disk system. The term\nwas introduced in the late 1980s by a group of researchers at U.C. Berke-\nley (led by Professors David Patterson and Randy Katz and then student\nGarth Gibson); it was around this time that many different researchers si-\nmultaneously arrived upon the basic idea of using multiple disks to build\na better storage system [BG88, K86,K88,PB86,SG86].\nExternally, a RAID looks like a disk: a group of blocks one can read\nor write. Internally, the RAID is a complex beast, consisting of multiple\ndisks, memory (both volatile and non-), and one or more processors to\nmanage the system. A hardware RAID is very much like a computer\nsystem, specialized for the task of managing a group of disks.\nRAIDs offer a number of advantages over a single disk. One advan-\ntage is performance. Using multiple disks in parallel can greatly speed\nup I/O times. Another beneﬁt is capacity. Large data sets demand large\ndisks. Finally, RAIDs can improve reliability; spreading data across mul-\ntiple disks (without RAID techniques) makes the data vulnerable to the\nloss of a single disk; with some form of redundancy, RAIDs can tolerate\nthe loss of a disk and keep operating as if nothing were wrong.\n421\n\n\n422\nREDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAIDS)\nTIP: TRANSPARENCY ENABLES DEPLOYMENT\nWhen considering how to add new functionality to a system, one should\nalways consider whether such functionality can be added transparently,\nin a way that demands no changes to the rest of the system. Requiring a\ncomplete rewrite of the existing software (or radical hardware changes)\nlessens the chance of impact of an idea. RAID is a perfect example, and\ncertainly its transparency contributed to its success; administrators could\ninstall a SCSI-based RAID storage array instead of a SCSI disk, and the\nrest of the system (host computer, OS, etc.) did not have to change one bit\nto start using it. By solving this problem of deployment, RAID was made\nmore successful from day one.\nAmazingly, RAIDs provide these advantages transparently to systems\nthat use them, i.e., a RAID just looks like a big disk to the host system. The\nbeauty of transparency, of course, is that it enables one to simply replace\na disk with a RAID and not change a single line of software; the operat-\ning system and client applications continue to operate without modiﬁca-\ntion. In this manner, transparency greatly improves the deployability of\nRAID, enabling users and administrators to put a RAID to use without\nworries of software compatibility.\nWe now discuss some of the important aspects of RAIDs. We begin\nwith the interface, fault model, and then discuss how one can evaluate a\nRAID design along three important axes: capacity, reliability, and perfor-\nmance. We then discuss a number of other issues that are important to\nRAID design and implementation.\n38.1\nInterface And RAID Internals\nTo a ﬁle system above, a RAID looks like a big, (hopefully) fast, and\n(hopefully) reliable disk. Just as with a single disk, it presents itself as\na linear array of blocks, each of which can be read or written by the ﬁle\nsystem (or other client).\nWhen a ﬁle system issues a logical I/O request to the RAID, the RAID\ninternally must calculate which disk (or disks) to access in order to com-\nplete the request, and then issue one or more physical I/Os to do so. The\nexact nature of these physical I/Os depends on the RAID level, as we will\ndiscuss in detail below. However, as a simple example, consider a RAID\nthat keeps two copies of each block (each one on a separate disk); when\nwriting to such a mirrored RAID system, the RAID will have to perform\ntwo physical I/Os for every one logical I/O it is issued.\nA RAID system is often built as a separate hardware box, with a stan-\ndard connection (e.g., SCSI, or SATA) to a host.\nInternally, however,\nRAIDs are fairly complex, consisting of a microcontroller that runs ﬁrmware\nto direct the operation of the RAID, volatile memory such as DRAM\nto buffer data blocks as they are read and written, and in some cases,\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nREDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAIDS)\n423\nnon-volatile memory to buffer writes safely and perhaps even special-\nized logic to perform parity calculations (useful in some RAID levels, as\nwe will also see below). At a high level, a RAID is very much a special-\nized computer system: it has a processor, memory, and disks; however,\ninstead of running applications, it runs specialized software designed to\noperate the RAID.\n38.2\nFault Model\nTo understand RAID and compare different approaches, we must have\na fault model in mind. RAIDs are designed to detect and recover from\ncertain kinds of disk faults; thus, knowing exactly which faults to expect\nis critical in arriving upon a working design.\nThe ﬁrst fault model we will assume is quite simple, and has been\ncalled the fail-stop fault model [S84]. In this model, a disk can be in\nexactly one of two states: working or failed. With a working disk, all\nblocks can be read or written. In contrast, when a disk has failed, we\nassume it is permanently lost.\nOne critical aspect of the fail-stop model is what it assumes about fault\ndetection. Speciﬁcally, when a disk has failed, we assume that this is\neasily detected. For example, in a RAID array, we would assume that the\nRAID controller hardware (or software) can immediately observe when a\ndisk has failed.\nThus, for now, we do not have to worry about more complex “silent”\nfailures such as disk corruption. We also do not have to worry about a sin-\ngle block becoming inaccessible upon an otherwise working disk (some-\ntimes called a latent sector error). We will consider these more complex\n(and unfortunately, more realistic) disk faults later.\n38.3\nHow To Evaluate A RAID\nAs we will soon see, there are a number of different approaches to\nbuilding a RAID. Each of these approaches has different characteristics\nwhich are worth evaluating, in order to understand their strengths and\nweaknesses.\nSpeciﬁcally, we will evaluate each RAID design along three axes. The\nﬁrst axis is capacity; given a set of N disks, how much useful capacity is\navailable to systems that use the RAID? Without redundancy, the answer\nis obviously N; however, if we have a system that keeps a two copies of\neach block, we will obtain a useful capacity of N/2. Different schemes\n(e.g., parity-based ones) tend to fall in between.\nThe second axis of evaluation is reliability. How many disk faults can\nthe given design tolerate? In alignment with our fault model, we assume\nonly that an entire disk can fail; in later chapters (i.e., on data integrity),\nwe’ll think about how to handle more complex failure modes.\nFinally, the third axis is performance. Performance is somewhat chal-\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n424\nREDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAIDS)\nlenging to evaluate, because it depends heavily on the workload pre-\nsented to the disk array. Thus, before evaluating performance, we will\nﬁrst present a set of typical workloads that one should consider.\nWe now consider three important RAID designs: RAID Level 0 (strip-\ning), RAID Level 1 (mirroring), and RAID Levels 4/5 (parity-based re-\ndundancy). The naming of each of these designs as a “level” stems from\nthe pioneering work of Patterson, Gibson, and Katz at Berkeley [P+88].\n38.4\nRAID Level 0: Striping\nThe ﬁrst RAID level is actually not a RAID level at all, in that there is\nno redundancy. However, RAID level 0, or striping as it is better known,\nserves as an excellent upper-bound on performance and capacity and\nthus is worth understanding.\nThe simplest form of striping will stripe blocks across the disks of the\nsystem as follows (assume here a 4-disk array):\nDisk 0\nDisk 1\nDisk 2\nDisk 3\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\nTable 38.1: RAID-0: Simple Striping\nFrom Table 38.1, you get the basic idea: spread the blocks of the array\nacross the disks in a round-robin fashion. This approach is designed to\nextract the most parallelism from the array when requests are made for\ncontiguous chunks of the array (as in a large, sequential read, for exam-\nple). We call the blocks in the same row a stripe; thus, blocks 0, 1, 2, and\n3 are in the same stripe above.\nIn the example, we have made the simplifying assumption that only 1\nblock (each of say size 4KB) is placed on each disk before moving on to\nthe next. However, this arrangement need not be the case. For example,\nwe could arrange the blocks across disks as in Table 38.2:\nDisk 0\nDisk 1\nDisk 2\nDisk 3\n0\n2\n4\n6\nchunk size:\n1\n3\n5\n7\n2 blocks\n8\n10\n12\n14\n9\n11\n13\n15\nTable 38.2: Striping with a Bigger Chunk Size\nIn this example, we place two 4KB blocks on each disk before moving\non to the next disk. Thus, the chunk size of this RAID array is 8KB, and\na stripe thus consists of 4 chunks or 32KB of data.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nREDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAIDS)\n425\nASIDE: THE RAID MAPPING PROBLEM\nBefore studying the capacity, reliability, and performance characteristics\nof the RAID, we ﬁrst present an aside on what we call the mapping prob-\nlem. This problem arises in all RAID arrays; simply put, given a logical\nblock to read or write, how does the RAID know exactly which physical\ndisk and offset to access?\nFor these simple RAID levels, we do not need much sophistication in\norder to correctly map logical blocks onto their physical locations. Take\nthe ﬁrst striping example above (chunk size = 1 block = 4KB). In this case,\ngiven a logical block address A, the RAID can easily compute the desired\ndisk and offset with two simple equations:\nDisk\n= A % number_of_disks\nOffset = A / number_of_disks\nNote that these are all integer operations (e.g., 4 / 3 = 1 not 1.33333...).\nLet’s see how these equations work for a simple example. Imagine in the\nﬁrst RAID above that a request arrives for block 14. Given that there are\n4 disks, this would mean that the disk we are interested in is (14 % 4 = 2):\ndisk 2. The exact block is calculated as (14 / 4 = 3): block 3. Thus, block\n14 should be found on the fourth block (block 3, starting at 0) of the third\ndisk (disk 2, starting at 0), which is exactly where it is.\nYou can think about how these equations would be modiﬁed to support\ndifferent chunk sizes. Try it! It’s not too hard.\nChunk Sizes\nChunk size mostly affects performance of the array. For example, a small\nchunk size implies that many ﬁles will get striped across many disks, thus\nincreasing the parallelism of reads and writes to a single ﬁle; however, the\npositioning time to access blocks across multiple disks increases, because\nthe positioning time for the entire request is determined by the maximum\nof the positioning times of the requests across all drives.\nA big chunk size, on the other hand, reduces such intra-ﬁle paral-\nlelism, and thus relies on multiple concurrent requests to achieve high\nthroughput. However, large chunk sizes reduce positioning time; if, for\nexample, a single ﬁle ﬁts within a chunk and thus is placed on a single\ndisk, the positioning time incurred while accessing it will just be the po-\nsitioning time of a single disk.\nThus, determining the “best” chunk size is hard to do, as it requires a\ngreat deal of knowledge about the workload presented to the disk system\n[CL95]. For the rest of this discussion, we will assume that the array uses\na chunk size of a single block (4KB). Most arrays use larger chunk sizes\n(e.g., 64 KB), but for the issues we discuss below, the exact chunk size\ndoes not matter; thus we use a single block for the sake of simplicity.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n426\nREDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAIDS)\nBack To RAID-0 Analysis\nLet us now evaluate the capacity, reliability, and performance of striping.\nFrom the perspective of capacity, it is perfect: given N disks, striping de-\nlivers N disks worth of useful capacity. From the standpoint of reliability,\nstriping is also perfect, but in the bad way: any disk failure will lead to\ndata loss. Finally, performance is excellent: all disks are utilized, often in\nparallel, to service user I/O requests.\nEvaluating RAID Performance\nIn analyzing RAID performance, one can consider two different perfor-\nmance metrics. The ﬁrst is single-request latency. Understanding the la-\ntency of a single I/O request to a RAID is useful as it reveals how much\nparallelism can exist during a single logical I/O operation. The second\nis steady-state throughput of the RAID, i.e., the total bandwidth of many\nconcurrent requests. Because RAIDs are often used in high-performance\nenvironments, the steady-state bandwidth is critical, and thus will be the\nmain focus of our analyses.\nTo understand throughput in more detail, we need to put forth some\nworkloads of interest. We will assume, for this discussion, that there\nare two types of workloads: sequential and random. With a sequential\nworkload, we assume that requests to the array come in large contiguous\nchunks; for example, a request (or series of requests) that accesses 1 MB\nof data, starting at block (B) and ending at block (B + 1 MB), would be\ndeemed sequential. Sequential workloads are common in many environ-\nments (think of searching through a large ﬁle for a keyword), and thus\nare considered important.\nFor random workloads, we assume that each request is rather small,\nand that each request is to a different random location on disk. For exam-\nple, a random stream of requests may ﬁrst access 4KB at logical address\n10, then at logical address 550,000, then at 20,100, and so forth. Some im-\nportant workloads, such as transactional workloads on a database man-\nagement system (DBMS), exhibit this type of access pattern, and thus it is\nconsidered an important workload.\nOf course, real workloads are not so simple, and often have a mix\nof sequential and random-seeming components as well as behaviors in-\nbetween the two. For simplicity, we just consider these two possibilities.\nAs you can tell, sequential and random workloads will result in widely\ndifferent performance characteristics from a disk. With sequential access,\na disk operates in its most efﬁcient mode, spending little time seeking and\nwaiting for rotation and most of its time transferring data. With random\naccess, just the opposite is true: most time is spent seeking and waiting\nfor rotation and relatively little time is spent transferring data. To capture\nthis difference in our analysis, we will assume that a disk can transfer\ndata at S MB/s under a sequential workload, and R MB/s when under a\nrandom workload. In general, S is much greater than R.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nREDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAIDS)\n427\nTo make sure we understand this difference, let’s do a simple exer-\ncise. Speciﬁcally, lets calculate S and R given the following disk charac-\nteristics. Assume a sequential transfer of size 10 MB on average, and a\nrandom transfer of 10 KB on average. Also, assume the following disk\ncharacteristics:\nAverage seek time\n7 ms\nAverage rotational delay\n3 ms\nTransfer rate of disk\n50 MB/s\nTo compute S, we need to ﬁrst ﬁgure out how time is spent in a typical\n10 MB transfer. First, we spend 7 ms seeking, and then 3 ms rotating.\nFinally, transfer begins; 10 MB @ 50 MB/s leads to 1/5th of a second, or\n200 ms, spent in transfer. Thus, for each 10 MB request, we spend 210 ms\ncompleting the request. To compute S, we just need to divide:\nS = Amount of Data\nT ime to access\n= 10 MB\n210 ms = 47.62 MB/s\nAs we can see, because of the large time spent transferring data, S is\nvery near the peak bandwidth of the disk (the seek and rotational costs\nhave been amortized).\nWe can compute R similarly. Seek and rotation are the same; we then\ncompute the time spent in transfer, which is 10 KB @ 50 MB/s, or 0.195\nms.\nR = Amount of Data\nT ime to access =\n10 KB\n10.195 ms = 0.981 MB/s\nAs we can see, R is less than 1 MB/s, and S/R is almost 50.\nBack To RAID-0 Analysis, Again\nLet’s now evaluate the performance of striping. As we said above, it is\ngenerally good. From a latency perspective, for example, the latency of a\nsingle-block request should be just about identical to that of a single disk;\nafter all, RAID-0 will simply redirect that request to one of its disks.\nFrom the perspective of steady-state throughput, we’d expect to get\nthe full bandwidth of the system. Thus, throughput equals N (the number\nof disks) multiplied by S (the sequential bandwidth of a single disk). For\na large number of random I/Os, we can again use all of the disks, and\nthus obtain N · R MB/s. As we will see below, these values are both\nthe simplest to calculate and will serve as an upper bound in comparison\nwith other RAID levels.\n38.5\nRAID Level 1: Mirroring\nOur ﬁrst RAID level beyond striping is known as RAID level 1, or\nmirroring. With a mirrored system, we simply make more than one copy\nof each block in the system; each copy should be placed on a separate\ndisk, of course. By doing so, we can tolerate disk failures.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n428\nREDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAIDS)\nIn a typical mirrored system, we will assume that for each logical\nblock, the RAID keeps two physical copies of it. Here is an example:\nDisk 0\nDisk 1\nDisk 2\nDisk 3\n0\n0\n1\n1\n2\n2\n3\n3\n4\n4\n5\n5\n6\n6\n7\n7\nTable 38.3: Simple RAID-1: Mirroring\nIn the example, disk 0 and disk 1 have identical contents, and disk 2\nand disk 3 do as well; the data is striped across these mirror pairs. In fact,\nyou may have noticed that there are a number of different ways to place\nblock copies across the disks. The arrangement above is a common one\nand is sometimes called RAID-10 or (RAID 1+0) because it uses mirrored\npairs (RAID-1) and then stripes (RAID-0) on top of them; another com-\nmon arrangement is RAID-01 (or RAID 0+1), which contains two large\nstriping (RAID-0) arrays, and then mirrors (RAID-1) on top of them. For\nnow, we will just talk about mirroring assuming the above layout.\nWhen reading a block from a mirrored array, the RAID has a choice: it\ncan read either copy. For example, if a read to logical block 5 is issued to\nthe RAID, it is free to read it from either disk 2 or disk 3. When writing\na block, though, no such choice exists: the RAID must update both copies\nof the data, in order to preserve reliability. Do note, though, that these\nwrites can take place in parallel; for example, a write to logical block 5\ncould proceed to disks 2 and 3 at the same time.\nRAID-1 Analysis\nLet us assess RAID-1. From a capacity standpoint, RAID-1 is expensive;\nwith the mirroring level = 2, we only obtain half of our peak useful ca-\npacity. Thus, with N disks, the useful capacity of mirroring is N/2.\nFrom a reliability standpoint, RAID-1 does well. It can tolerate the fail-\nure of any one disk. You may also notice RAID-1 can actually do better\nthan this, with a little luck. Imagine, in the ﬁgure above, that disk 0 and\ndisk 2 both failed. In such a situation, there is no data loss! More gen-\nerally, a mirrored system (with mirroring level of 2) can tolerate 1 disk\nfailure for certain, and up to N/2 failures depending on which disks fail.\nIn practice, we generally don’t like to leave things like this to chance; thus\nmost people consider mirroring to be good for handling a single failure.\nFinally, we analyze performance. From the perspective of the latency\nof a single read request, we can see it is the same as the latency on a single\ndisk; all the RAID-1 does is direct the read to one of its copies. A write\nis a little different: it requires two physical writes to complete before it\nis done. These two writes happen in parallel, and thus the time will be\nroughly equivalent to the time of a single write; however, because the\nlogical write must wait for both physical writes to complete, it suffers the\nworst-case seek and rotational delay of the two requests, and thus (on\naverage) will be slightly higher than a write to a single disk.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nREDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAIDS)\n429\nASIDE: THE RAID CONSISTENT-UPDATE PROBLEM\nBefore analyzing RAID-1, let us ﬁrst discuss a problem that arises in\nany multi-disk RAID system, known as the consistent-update problem\n[DAA05]. The problem occurs on a write to any RAID that has to up-\ndate multiple disks during a single logical operation. In this case, let us\nassume we are considering a mirrored disk array.\nImagine the write is issued to the RAID, and then the RAID decides that\nit must be written to two disks, disk 0 and disk 1. The RAID then issues\nthe write to disk 0, but just before the RAID can issue the request to disk\n1, a power loss (or system crash) occurs. In this unfortunate case, let us\nassume that the request to disk 0 completed (but clearly the request to\ndisk 1 did not, as it was never issued).\nThe result of this untimely power loss is that the two copies of the block\nare now inconsistent; the copy on disk 0 is the new version, and the copy\non disk 1 is the old. What we would like to happen is for the state of both\ndisks to change atomically, i.e., either both should end up as the new\nversion or neither.\nThe general way to solve this problem is to use a write-ahead log of some\nkind to ﬁrst record what the RAID is about to do (i.e., update two disks\nwith a certain piece of data) before doing it. By taking this approach, we\ncan ensure that in the presence of a crash, the right thing will happen; by\nrunning a recovery procedure that replays all pending transactions to the\nRAID, we can ensure that no two mirrored copies (in the RAID-1 case)\nare out of sync.\nOne last note: because logging to disk on every write is prohibitively\nexpensive, most RAID hardware includes a small amount of non-volatile\nRAM (e.g., battery-backed) where it performs this type of logging. Thus,\nconsistent update is provided without the high cost of logging to disk.\nTo analyze steady-state throughput, let us start with the sequential\nworkload. When writing out to disk sequentially, each logical write must\nresult in two physical writes; for example, when we write logical block\n0 (in the ﬁgure above), the RAID internally would write it to both disk\n0 and disk 1. Thus, we can conclude that the maximum bandwidth ob-\ntained during sequential writing to a mirrored array is ( N\n2 · S), or half the\npeak bandwidth.\nUnfortunately, we obtain the exact same performance during a se-\nquential read. One might think that a sequential read could do better,\nbecause it only needs to read one copy of the data, not both. However,\nlet’s use an example to illustrate why this doesn’t help much. Imagine we\nneed to read blocks 0, 1, 2, 3, 4, 5, 6, and 7. Let’s say we issue the read of\n0 to disk 0, the read of 1 to disk 2, the read of 2 to disk 1, and the read of\n3 to disk 3. We continue by issuing reads to 4, 5, 6, and 7 to disks 0, 2, 1,\nand 3, respectively. One might naively think that because we are utilizing\nall disks, we are achieving the full bandwidth of the array.\nTo see that this is not the case, however, consider the requests a single\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n430\nREDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAIDS)\ndisk receives (say disk 0). First, it gets a request for block 0; then, it gets a\nrequest for block 4 (skipping block 2). In fact, each disk receives a request\nfor every other block. While it is rotating over the skipped block, it is\nnot delivering useful bandwidth to the client. Thus, each disk will only\ndeliver half its peak bandwidth. And thus, the sequential read will only\nobtain a bandwidth of ( N\n2 · S) MB/s.\nRandom reads are the best case for a mirrored RAID. In this case, we\ncan distribute the reads across all the disks, and thus obtain the full pos-\nsible bandwidth. Thus, for random reads, RAID-1 delivers N · R MB/s.\nFinally, random writes perform as you might expect: N\n2 ·R MB/s. Each\nlogical write must turn into two physical writes, and thus while all the\ndisks will be in use, the client will only perceive this as half the available\nbandwidth. Even though a write to logical block X turns into two parallel\nwrites to two different physical disks, the bandwidth of many small re-\nquests only achieves half of what we saw with striping. As we will soon\nsee, getting half the available bandwidth is actually pretty good!\n38.6\nRAID Level 4: Saving Space With Parity\nWe now present a different method of adding redundancy to a disk ar-\nray known as parity. Parity-based approaches attempt to use less capac-\nity and thus overcome the huge space penalty paid by mirrored systems.\nThey do so at a cost, however: performance.\nIn a ﬁve-disk RAID-4 system, we might observe the following layout:\nDisk 0\nDisk 1\nDisk 2\nDisk 3\nDisk 4\n0\n1\n2\n3\nP0\n4\n5\n6\n7\nP1\n8\n9\n10\n11\nP2\n12\n13\n14\n15\nP3\nAs you can see, for each stripe of data, we have added a single par-\nity block that stores the redundant information for that stripe of blocks.\nFor example, parity block P1 has redundant information that it calculated\nfrom blocks 4, 5, 6, and 7.\nTo compute parity, we need to use a mathematical function that en-\nables us to withstand the loss of any one block from our stripe. It turns\nout the simple function XOR does the trick quite nicely. For a given set of\nbits, the XOR of all of those bits returns a 0 if there are an even number of\n1’s in the bits, and a 1 if there are an odd number of 1’s. For example:\nC0\nC1\nC2\nC3\nP\n0\n0\n1\n1\nXOR(0,0,1,1) = 0\n0\n1\n0\n0\nXOR(0,1,0,0) = 1\nIn the ﬁrst row (0,0,1,1), there are two 1’s (C2, C3), and thus XOR of\nall of those values will be 0 (P); similarly, in the second row there is only\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nREDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAIDS)\n431\none 1 (C1), and thus the XOR must be 1 (P). You can remember this in a\nvery simple way: that the number of 1’s in any row must be an even (not\nodd) number; that is the invariant that the RAID must maintain in order\nfor parity to be correct.\nFrom the example above, you might also be able to guess how parity\ninformation can be used to recover from a failure. Imagine the column la-\nbeled C2 is lost. To ﬁgure out what values must have been in the column,\nwe simply have to read in all the other values in that row (including the\nXOR’d parity bit) and reconstruct the right answer. Speciﬁcally, assume\nthe ﬁrst row’s value in column C2 is lost (it is a 1); by reading the other\nvalues in that row (0 from C0, 0 from C1, 1 from C3, and 0 from the parity\ncolumn P), we get the values 0, 0, 1, and 0. Because we know that XOR\nkeeps an even number of 1’s in each row, we know what the missing data\nmust be: a 1. And that is how reconstruction works in a XOR-based par-\nity scheme! Note also how we compute the reconstructed value: we just\nXOR the data bits and the parity bits together, in the same way that we\ncalculated the parity in the ﬁrst place.\nNow you might be wondering: we are talking about XORing all of\nthese bits, and yet above we know that the RAID places 4KB (or larger)\nblocks on each disk; how do we apply XOR to a bunch of blocks to com-\npute the parity? It turns out this is easy as well. Simply perform a bitwise\nXOR across each bit of the data blocks; put the result of each bitwise XOR\ninto the corresponding bit slot in the parity block. For example, if we had\nblocks of size 4 bits (yes, this is still quite a bit smaller than a 4KB block,\nbut you get the picture), they might look something like this:\nBlock0\nBlock1\nBlock2\nBlock3\nParity\n00\n10\n11\n10\n11\n10\n01\n00\n01\n10\nAs you can see from the ﬁgure, the parity is computed for each bit of\neach block and the result placed in the parity block.\nRAID-4 Analysis\nLet us now analyze RAID-4. From a capacity standpoint, RAID-4 uses 1\ndisk for parity information for every group of disks it is protecting. Thus,\nour useful capacity for a RAID group is (N-1).\nReliability is also quite easy to understand: RAID-4 tolerates 1 disk\nfailure and no more. If more than one disk is lost, there is simply no way\nto reconstruct the lost data.\nFinally, there is performance. This time, let us start by analyzing steady-\nstate throughput. Sequential read performance can utilize all of the disks\nexcept for the parity disk, and thus deliver a peak effective bandwidth of\n(N −1) · S MB/s (an easy case).\nTo understand the performance of sequential writes, we must ﬁrst un-\nderstand how they are done. When writing a big chunk of data to disk,\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n432\nREDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAIDS)\nRAID-4 can perform a simple optimization known as a full-stripe write.\nFor example, imagine the case where the blocks 0, 1, 2, and 3 have been\nsent to the RAID as part of a write request (Table 38.4).\nDisk 0\nDisk 1\nDisk 2\nDisk 3\nDisk 4\n0\n1\n2\n3\nP0\n4\n5\n6\n7\nP1\n8\n9\n10\n11\nP2\n12\n13\n14\n15\nP3\nTable 38.4: Full-stripe Writes In RAID-4\nIn this case, the RAID can simply calculate the new value of P0 (by\nperforming an XOR across the blocks 0, 1, 2, and 3) and then write all of\nthe blocks (including the parity block) to the ﬁve disks above in parallel\n(highlighted in gray in the ﬁgure). Thus, full-stripe writes are the most\nefﬁcient way for RAID-4 to write to disk.\nOnce we understand the full-stripe write, calculating the performance\nof sequential writes on RAID-4 is easy; the effective bandwidth is also\n(N −1)·S MB/s. Even though the parity disk is constantly in use during\nthe operation, the client does not gain performance advantage from it.\nNow let us analyze the performance of random reads. As you can also\nsee from the ﬁgure above, a set of 1-block random reads will be spread\nacross the data disks of the system but not the parity disk. Thus, the\neffective performance is: (N −1) · R MB/s.\nRandom writes, which we have saved for last, present the most in-\nteresting case for RAID-4. Imagine we wish to overwrite block 1 in the\nexample above. We could just go ahead and overwrite it, but that would\nleave us with a problem: the parity block P0 would no longer accurately\nreﬂect the correct parity value for the stripe. Thus, in this example, P0\nmust also be updated. But how can we update it both correctly and efﬁ-\nciently?\nIt turns out there are two methods. The ﬁrst, known as additive parity,\nrequires us to do the following. To compute the value of the new parity\nblock, read in all of the other data blocks in the stripe in parallel (in the\nexample, blocks 0, 2, and 3) and XOR those with the new block (1). The\nresult is your new parity block. To complete the write, you can then write\nthe new data and new parity to their respective disks, also in parallel.\nThe problem with this technique is that it scales with the number of\ndisks, and thus in larger RAIDs requires a high number of reads to com-\npute parity. Thus, the subtractive parity method.\nFor example, imagine this string of bits (4 data bits, one parity):\nC0\nC1\nC2\nC3\nP\n0\n0\n1\n1\nXOR(0,0,1,1) = 0\nLet’s imagine that we wish to overwrite bit C2 with a new value which\nwe will call C2(new). The subtractive method works in three steps. First,\nwe read in the old data at C2 (C2(old) = 1) and the old parity (P(old) =\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nREDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAIDS)\n433\n0). Then, we compare the old data and the new data; if they are the same\n(e.g., C2(new) = C2(old)), then we know the parity bit will also remain\nthe same (i.e., P(new) = P(old)). If, however, they are different, then we\nmust ﬂip the old parity bit to the opposite of its current state, that is, if\n(P(old) == 1), P(new) will be set to 0; if (P(old) == 0), P(new) will be set to\n1. We can express this whole mess neatly with XOR as it turns out (if you\nunderstand XOR, this will now make sense to you):\nP(new) = (C(old) XOR C(new)) XOR P(old)\nBecause we are dealing with blocks, not bits, we perform this calcula-\ntion over all the bits in the block (e.g., 4096 bytes in each block multiplied\nby 8 bits per byte). Thus, in most cases, the new block will be different\nthan the old block and thus the new parity block will too.\nYou should now be able to ﬁgure out when we would use the additive\nparity calculation and when we would use the subtractive method. Think\nabout how many disks would need to be in the system so that the additive\nmethod performs fewer I/Os than the subtractive method; what is the\ncross-over point?\nFor this performance analysis, let us assume we are using the subtrac-\ntive method. Thus, for each write, the RAID has to perform 4 physical\nI/Os (two reads and two writes). Now imagine there are lots of writes\nsubmitted to the RAID; how many can RAID-4 perform in parallel? To\nunderstand, let us again look at the RAID-4 layout (Figure 38.5).\nDisk 0\nDisk 1\nDisk 2\nDisk 3\nDisk 4\n0\n1\n2\n3\nP0\n∗4\n5\n6\n7\n+P1\n8\n9\n10\n11\nP2\n12\n∗13\n14\n15\n+P3\nTable 38.5: Example: Writes To 4, 13, And Respective Parity Blocks\nNow imagine there were 2 small writes submitted to the RAID-4 at\nabout the same time, to blocks 4 and 13 (marked with ∗in the diagram).\nThe data for those disks is on disks 0 and 1, and thus the read and write\nto data could happen in parallel, which is good. The problem that arises\nis with the parity disk; both the requests have to read the related parity\nblocks for 4 and 13, parity blocks 1 and 3 (marked with +). Hopefully, the\nissue is now clear: the parity disk is a bottleneck under this type of work-\nload; we sometimes thus call this the small-write problem for parity-\nbased RAIDs. Thus, even though the data disks could be accessed in\nparallel, the parity disk prevents any parallelism from materializing; all\nwrites to the system will be serialized because of the parity disk. Because\nthe parity disk has to perform two I/Os (one read, one write) per logical\nI/O, we can compute the performance of small random writes in RAID-4\nby computing the parity disk’s performance on those two I/Os, and thus\nwe achieve (R/2) MB/s. RAID-4 throughput under random small writes\nis terrible; it does not improve as you add disks to the system.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n434\nREDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAIDS)\nWe conclude by analyzing I/O latency in RAID-4. As you now know,\na single read (assuming no failure) is just mapped to a single disk, and\nthus its latency is equivalent to the latency of a single disk request. The\nlatency of a single write requires two reads and then two writes; the reads\ncan happen in parallel, as can the writes, and thus total latency is about\ntwice that of a single disk (with some differences because we have to wait\nfor both reads to complete and thus get the worst-case positioning time,\nbut then the updates don’t incur seek cost and thus may be a better-than-\naverage positioning cost).\n38.7\nRAID Level 5: Rotating Parity\nTo address the small-write problem (at least, partially), Patterson, Gib-\nson, and Katz introduced RAID-5. RAID-5 works almost identically to\nRAID-4, except that it rotates the parity block across drives (Figure 38.6).\nDisk 0\nDisk 1\nDisk 2\nDisk 3\nDisk 4\n0\n1\n2\n3\nP0\n5\n6\n7\nP1\n4\n10\n11\nP2\n8\n9\n15\nP3\n12\n13\n14\nP4\n16\n17\n18\n19\nTable 38.6: RAID-5 With Rotated Parity\nAs you can see, the parity block for each stripe is now rotated across\nthe disks, in order to remove the parity-disk bottleneck for RAID-4.\nRAID-5 Analysis\nMuch of the analysis for RAID-5 is identical to RAID-4. For example, the\neffective capacity and failure tolerance of the two levels are identical. So\nare sequential read and write performance. The latency of a single request\n(whether a read or a write) is also the same as RAID-4.\nRandom read performance is a little better, because we can utilize all of\nthe disks. Finally, random write performance improves noticeably over\nRAID-4, as it allows for parallelism across requests. Imagine a write to\nblock 1 and a write to block 10; this will turn into requests to disk 1 and\ndisk 4 (for block 1 and its parity) and requests to disk 0 and disk 2 (for\nblock 10 and its parity). Thus, they can proceed in parallel. In fact, we\ncan generally assume that that given a large number of random requests,\nwe will be able to keep all the disks about evenly busy. If that is the case,\nthen our total bandwidth for small writes will be N\n4 · R MB/s. The factor\nof four loss is due to the fact that each RAID-5 write still generates 4 total\nI/O operations, which is simply the cost of using parity-based RAID.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nREDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAIDS)\n435\nRAID-0\nRAID-1\nRAID-4\nRAID-5\nCapacity\nN\nN/2\nN −1\nN −1\nReliability\n0\n1 (for sure)\n1\n1\nN\n2 (if lucky)\nThroughput\nSequential Read\nN · S\n(N/2) · S\n(N −1) · S\n(N −1) · S\nSequential Write\nN · S\n(N/2) · S\n(N −1) · S\n(N −1) · S\nRandom Read\nN · R\nN · R\n(N −1) · R\nN · R\nRandom Write\nN · R\n(N/2) · R\n1\n2 · R\nN\n4 R\nLatency\nRead\nD\nD\nD\nD\nWrite\nD\nD\n2D\n2D\nTable 38.7: RAID Capacity, Reliability, and Performance\nBecause RAID-5 is basically identical to RAID-4 except in the few cases\nwhere it is better, it has almost completely replaced RAID-4 in the market-\nplace. The only place where it has not is in systems that know they will\nnever perform anything other than a large write, thus avoiding the small-\nwrite problem altogether [HLM94]; in those cases, RAID-4 is sometimes\nused as it is slightly simpler to build.\n38.8\nRAID Comparison: A Summary\nWe now summarize our simpliﬁed comparison of RAID levels in Ta-\nble 38.7. Note that we have omitted a number of details to simplify our\nanalysis. For example, when writing in a mirrored system, the average\nseek time is a little higher than when writing to just a single disk, because\nthe seek time is the max of two seeks (one on each disk). Thus, random\nwrite performance to two disks will generally be a little less than random\nwrite performance of a single disk. Also, when updating the parity disk\nin RAID-4/5, the ﬁrst read of the old parity will likely cause a full seek\nand rotation, but the second write of the parity will only result in rotation.\nHowever, our comparison does capture the essential differences, and\nis useful for understanding tradeoffs across RAID levels. We present a\nsummary in the table below; for the latency analysis, we simply use D to\nrepresent the time that a request to a single disk would take.\nTo conclude, if you strictly want performance and do not care about\nreliability, striping is obviously best. If, however, you want random I/O\nperformance and reliability, mirroring is the best; the cost you pay is in\nlost capacity. If capacity and reliability are your main goals, then RAID-\n5 is the winner; the cost you pay is in small-write performance. Finally,\nif you are always doing sequential I/O and want to maximize capacity,\nRAID-5 also makes the most sense.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n436\nREDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAIDS)\n38.9\nOther Interesting RAID Issues\nThere are a number of other interesting ideas that one could (and per-\nhaps should) discuss when thinking about RAID. Here are some things\nwe might eventually write about.\nFor example, there are many other RAID designs, including Levels 2\nand 3 from the original taxonomy, and Level 6 to tolerate multiple disk\nfaults [C+04]. There is also what the RAID does when a disk fails; some-\ntimes it has a hot spare sitting around to ﬁll in for the failed disk. What\nhappens to performance under failure, and performance during recon-\nstruction of the failed disk? There are also more realistic fault models,\nto take into account latent sector errors or block corruption [B+08], and\nlots of techniques to handle such faults (see the data integrity chapter for\ndetails). Finally, you can even build raid as a software layer: such soft-\nware RAID systems are cheaper but have other problems, including the\nconsistent-update problem [DAA05].\n38.10\nSummary\nWe have discussed RAID. RAID transforms a number of independent\ndisks into a large, more capacious, and more reliable single entity; impor-\ntantly, it does so transparently, and thus hardware and software above is\nrelatively oblivious to the change.\nThere are many possible RAID levels to choose from, and the exact\nRAID level to use depends heavily on what is important to the end-user.\nFor example, mirrored RAID is simple, reliable, and generally provides\ngood performance but at a high capacity cost. RAID-5, in contrast, is\nreliable and better from a capacity standpoint, but performs quite poorly\nwhen there are small writes in the workload. Picking a RAID and setting\nits parameters (chunk size, number of disks, etc.) properly for a particular\nworkload is challenging, and remains more of an art than a science.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nREDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAIDS)\n437\nReferences\n[B+08] “An Analysis of Data Corruption in the Storage Stack”\nLakshmi N. Bairavasundaram, Garth R. Goodson, Bianca Schroeder, Andrea C. Arpaci-Dusseau,\nRemzi H. Arpaci-Dusseau\nFAST ’08, San Jose, CA, February 2008\nOur own work analyzing how often disks actually corrupt your data. Not often, but sometimes! And\nthus something a reliable storage system must consider.\n[BJ88] “Disk Shadowing”\nD. Bitton and J. Gray\nVLDB 1988\nOne of the ﬁrst papers to discuss mirroring, herein called “shadowing”.\n[CL95] “Striping in a RAID level 5 disk array”\nPeter M. Chen, Edward K. Lee\nSIGMETRICS 1995\nA nice analysis of some of the important parameters in a RAID-5 disk array.\n[C+04] “Row-Diagonal Parity for Double Disk Failure Correction”\nP. Corbett, B. English, A. Goel, T. Grcanac, S. Kleiman, J. Leong, S. Sankar\nFAST ’04, February 2004\nThough not the ﬁrst paper on a RAID system with two disks for parity, it is a recent and highly-\nunderstandable version of said idea. Read it to learn more.\n[DAA05] “Journal-guided Resynchronization for Software RAID”\nTimothy E. Denehy, A. Arpaci-Dusseau, R. Arpaci-Dusseau\nFAST 2005\nOur own work on the consistent-update problem. Here we solve it for Software RAID by integrating\nthe journaling machinery of the ﬁle system above with the software RAID beneath it.\n[HLM94] “File System Design for an NFS File Server Appliance”\nDave Hitz, James Lau, Michael Malcolm\nUSENIX Winter 1994, San Francisco, California, 1994\nThe sparse paper introducing a landmark product in storage, the write-anywhere ﬁle layout or WAFL\nﬁle system that underlies the NetApp ﬁle server.\n[K86] “Synchronized Disk Interleaving”\nM.Y. Kim.\nIEEE Transactions on Computers, Volume C-35: 11, November 1986\nSome of the earliest work on RAID is found here.\n[K88] “Small Disk Arrays - The Emerging Approach to High Performance”\nF. Kurzweil.\nPresentation at Spring COMPCON ’88, March 1, 1988, San Francisco, California\nAnother early RAID reference.\n[P+88] “Redundant Arrays of Inexpensive Disks”\nD. Patterson, G. Gibson, R. Katz.\nSIGMOD 1988\nThis is considered the RAID paper, written by famous authors Patterson, Gibson, and Katz. The paper\nhas since won many test-of-time awards and ushered in the RAID era, including the name RAID itself!\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n438\nREDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAIDS)\n[PB86] “Providing Fault Tolerance in Parallel Secondary Storage Systems”\nA. Park and K. Balasubramaniam\nDepartment of Computer Science, Princeton, CS-TR-O57-86, November 1986\nAnother early work on RAID.\n[SG86] “Disk Striping”\nK. Salem and H. Garcia-Molina.\nIEEE International Conference on Data Engineering, 1986\nAnd yes, another early RAID work. There are a lot of these, which kind of came out of the woodwork\nwhen the RAID paper was published in SIGMOD.\n[S84] “Byzantine Generals in Action: Implementing Fail-Stop Processors”\nF.B. Schneider.\nACM Transactions on Computer Systems, 2(2):145154, May 1984\nFinally, a paper that is not about RAID! This paper is actually about how systems fail, and how to make\nsomething behave in a fail-stop manner.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nREDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAIDS)\n439\nHomework\nThis section introduces raid.py, a simple RAID simulator you can\nuse to shore up your knowledge of how RAID systems work. See the\nREADME for details.\nQuestions\n1. Use the simulator to perform some basic RAID mapping tests. Run\nwith different levels (0, 1, 4, 5) and see if you can ﬁgure out the\nmappings of a set of requests. For RAID-5, see if you can ﬁgure out\nthe difference between left-symmetric and left-asymmetric layouts.\nUse some different random seeds to generate different problems\nthan above.\n2. Do the same as the ﬁrst problem, but this time vary the chunk size\nwith -C. How does chunk size change the mappings?\n3. Do the same as above, but use the -r ﬂag to reverse the nature of\neach problem.\n4. Now use the reverse ﬂag but increase the size of each request with\nthe -S ﬂag. Try specifying sizes of 8k, 12k, and 16k, while varying\nthe RAID level. What happens to the underlying I/O pattern when\nthe size of the request increases? Make sure to try this with the\nsequential workload too (-W sequential); for what request sizes\nare RAID-4 and RAID-5 much more I/O efﬁcient?\n5. Use the timing mode of the simulator (-t) to estimate the perfor-\nmance of 100 random reads to the RAID, while varying the RAID\nlevels, using 4 disks.\n6. Do the same as above, but increase the number of disks. How does\nthe performance of each RAID level scale as the number of disks\nincreases?\n7. Do the same as above, but use all writes (-w 100) instead of reads.\nHow does the performance of each RAID level scale now? Can you\ndo a rough estimate of the time it will take to complete the workload\nof 100 random writes?\n8. Run the timing mode one last time, but this time with a sequen-\ntial workload (-W sequential). How does the performance vary\nwith RAID level, and when doing reads versus writes? How about\nwhen varying the size of each request? What size should you write\nto a RAID when using RAID-4 or RAID-5?\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "page_number": 452,
      "chapter_number": 46,
      "summary": "This chapter covers segment 46 (pages 452-475). Key topics include raids, disk, and blocks. The summary is\nactually a detailed functional model; it does not describe the amazing\nphysics, electronics, and material science that goes into actual drive de-\nsign.",
      "keywords": [
        "RAID",
        "DISK",
        "RAID level",
        "Inexpensive Disks",
        "Redundant Arrays",
        "parity disk",
        "RAID system",
        "single disk",
        "block",
        "parity",
        "Arrays of Inexpensive",
        "write",
        "system",
        "ﬁrst RAID level",
        "RAID array"
      ],
      "concepts": [
        "raids",
        "disk",
        "blocks",
        "different",
        "difference",
        "write",
        "writing",
        "performance",
        "perform",
        "data"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 33,
          "title": "Segment 33 (pages 662-682)",
          "relevance_score": 0.56,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 31,
          "title": "Segment 31 (pages 620-638)",
          "relevance_score": 0.53,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 14,
          "title": "Segment 14 (pages 127-134)",
          "relevance_score": 0.53,
          "method": "api"
        },
        {
          "book": "Software Architecture",
          "chapter": 36,
          "title": "Segment 36 (pages 358-366)",
          "relevance_score": 0.53,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 62,
          "title": "Segment 62 (pages 607-613)",
          "relevance_score": 0.52,
          "method": "api"
        }
      ]
    },
    {
      "number": 47,
      "title": "Segment 47 (pages 476-485)",
      "start_page": 476,
      "end_page": 485,
      "detection_method": "topic_boundary",
      "content": "39\nInterlude: File and Directories\nThus far we have seen the development of two key operating system ab-\nstractions: the process, which is a virtualization of the CPU, and the ad-\ndress space, which is a virtualization of memory. In tandem, these two\nabstractions allow a program to run as if it is in its own private, isolated\nworld; as if it has its own processor (or processors); as if it has its own\nmemory. This illusion makes programming the system much easier and\nthus is prevalent today not only on desktops and servers but increasingly\non all programmable platforms including mobile phones and the like.\nIn this section, we add one more critical piece to the virtualization puz-\nzle: persistent storage. A persistent-storage device, such as a classic hard\ndisk drive or a more modern solid-state storage device, stores informa-\ntion permanently (or at least, for a long time). Unlike memory, whose\ncontents are lost when there is a power loss, a persistent-storage device\nkeeps such data intact. Thus, the OS must take extra care with such a\ndevice: this is where users keep data that they really care about.\nCRUX: HOW TO MANAGE A PERSISTENT DEVICE\nHow should the OS manage a persistent device? What are the APIs?\nWhat are the important aspects of the implementation?\nThus, in the next few chapters, we will explore critical techniques for\nmanaging persistent data, focusing on methods to improve performance\nand reliability. We begin, however, with an overview of the API: the in-\nterfaces you’ll expect to see when interacting with a UNIX ﬁle system.\n39.1\nFiles and Directories\nTwo key abstractions have developed over time in the virtualization\nof storage. The ﬁrst is the ﬁle. A ﬁle is simply a linear array of bytes,\neach of which you can read or write. Each ﬁle has some kind of low-level\n441\n\n\n442\nINTERLUDE: FILE AND DIRECTORIES\nname, usually a number of some kind; often, the user is not aware of\nthis name (as we will see). For historical reasons, the low-level name of a\nﬁle is often referred to as its inode number. We’ll be learning a lot more\nabout inodes in future chapters; for now, just assume that each ﬁle has an\ninode number associated with it.\nIn most systems, the OS does not know much about the structure of\nthe ﬁle (e.g., whether it is a picture, or a text ﬁle, or C code); rather, the\nresponsibility of the ﬁle system is simply to store such data persistently\non disk and make sure that when you request the data again, you get\nwhat you put there in the ﬁrst place. Doing so is not as simple as it seems!\nThe second abstraction is that of a directory. A directory, like a ﬁle,\nalso has a low-level name (i.e., an inode number), but its contents are\nquite speciﬁc: it contains a list of (user-readable name, low-level name)\npairs. For example, let’s say there is a ﬁle with the low-level name “10”,\nand it is referred to by the user-readable name of “foo”. The directory\n“foo” resides in thus would have an entry (“foo”, “10”) that maps the\nuser-readable name to the low-level name. Each entry in a directory refers\nto either ﬁles or other directories. By placing directories within other di-\nrectories, users are able to build an arbitrary directory tree (or directory\nhierarchy), under which all ﬁles and directories are stored.\n/\nfoo\nbar.txt\nbar\nfoo\nbar\nbar.txt\nFigure 39.1: An Example Directory Tree\nThe directory hierarchy starts at a root directory (in UNIX-based sys-\ntems, the root directory is simply referred to as /) and uses some kind\nof separator to name subsequent sub-directories until the desired ﬁle or\ndirectory is named. For example, if a user created a directory foo in the\nroot directory /, and then created a ﬁle bar.txt in the directory foo,\nwe could refer to the ﬁle by its absolute pathname, which in this case\nwould be /foo/bar.txt. See Figure 39.1 for a more complex directory\ntree; valid directories in the example are /, /foo, /bar, /bar/bar,\n/bar/foo and valid ﬁles are /foo/bar.txt and /bar/foo/bar.txt.\nDirectories and ﬁles can have the same name as long as they are in dif-\nferent locations in the ﬁle-system tree (e.g., there are two ﬁles named\nbar.txt in the ﬁgure, /foo/bar.txt and /bar/foo/bar.txt).\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nINTERLUDE: FILE AND DIRECTORIES\n443\nTIP: THINK CAREFULLY ABOUT NAMING\nNaming is an important aspect of computer systems [SK09]. In UNIX\nsystems, virtually everything that you can think of is named through the\nﬁle system. Beyond just ﬁles, devices, pipes, and even processes [K84]\ncan be found in what looks like a plain old ﬁle system. This uniformity\nof naming eases your conceptual model of the system, and makes the\nsystem simpler and more modular. Thus, whenever creating a system or\ninterface, think carefully about what names you are using.\nYou may also notice that the ﬁle name in this example often has two\nparts: bar and txt, separated by a period. The ﬁrst part is an arbitrary\nname, whereas the second part of the ﬁle name is usually used to indi-\ncate the type of the ﬁle, e.g., whether it is C code (e.g., .c), or an image\n(e.g., .jpg), or a music ﬁle (e.g., .mp3). However, this is usually just a\nconvention: there is usually no enforcement that the data contained in a\nﬁle named main.c is indeed C source code.\nThus, we can see one great thing provided by the ﬁle system: a conve-\nnient way to name all the ﬁles we are interested in. Names are important\nin systems as the ﬁrst step to accessing any resource is being able to name\nit. In UNIX systems, the ﬁle system thus provides a uniﬁed way to access\nﬁles on disk, USB stick, CD-ROM, many other devices, and in fact many\nother things, all located under the single directory tree.\n39.2\nThe File System Interface\nLet’s now discuss the ﬁle system interface in more detail. We’ll start\nwith the basics of creating, accessing, and deleting ﬁles. You may think\nthis straightforward, but along the way we’ll discover the mysterious call\nthat is used to remove ﬁles, known as unlink(). Hopefully, by the end\nof this chapter, this mystery won’t be so mysterious to you!\n39.3\nCreating Files\nWe’ll start with the most basic of operations: creating a ﬁle. This can be\naccomplished with the open system call; by calling open() and passing\nit the O CREAT ﬂag, a program can create a new ﬁle. Here is some exam-\nple code to create a ﬁle called “foo” in the current working directory.\nint fd = open(\"foo\", O_CREAT | O_WRONLY | O_TRUNC);\nThe routine open() takes a number of different ﬂags. In this exam-\nple, the program creates the ﬁle (O CREAT), can only write to that ﬁle\nwhile opened in this manner (O WRONLY), and, if the ﬁle already exists,\nﬁrst truncate it to a size of zero bytes thus removing any existing content\n(O TRUNC).\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n444\nINTERLUDE: FILE AND DIRECTORIES\nASIDE: THE CREAT() SYSTEM CALL\nThe older way of creating a ﬁle is to call creat(), as follows:\nint fd = creat(\"foo\");\nYou can think of creat() as open() with the following ﬂags:\nO CREAT | O WRONLY | O TRUNC. Because open() can create a ﬁle,\nthe usage of creat() has somewhat fallen out of favor (indeed, it could\njust be implemented as a library call to open()); however, it does hold a\nspecial place in UNIX lore. Speciﬁcally, when Ken Thompson was asked\nwhat he would do differently if he were redesigning UNIX, he replied:\n“I’d spell creat with an e.”\nOne important aspect of open() is what it returns: a ﬁle descriptor. A\nﬁle descriptor is just an integer, private per process, and is used in UNIX\nsystems to access ﬁles; thus, once a ﬁle is opened, you use the ﬁle de-\nscriptor to read or write the ﬁle, assuming you have permission to do so.\nIn this way, a ﬁle descriptor is a capability [L84], i.e., an opaque handle\nthat gives you the power to perform certain operations. Another way to\nthink of a ﬁle descriptor is as a pointer to an object of type ﬁle; once you\nhave such an object, you can call other “methods” to access the ﬁle, like\nread() and write(). We’ll see just how a ﬁle descriptor is used below.\n39.4\nReading and Writing Files\nOnce we have some ﬁles, of course we might like to read or write them.\nLet’s start by reading an existing ﬁle. If we were typing at a command\nline, we might just use the program cat to dump the contents of the ﬁle\nto the screen.\nprompt> echo hello > foo\nprompt> cat foo\nhello\nprompt>\nIn this code snippet, we redirect the output of the program echo to\nthe ﬁle foo, which then contains the word “hello” in it. We then use cat\nto see the contents of the ﬁle. But how does the cat program access the\nﬁle foo?\nTo ﬁnd this out, we’ll use an incredibly useful tool to trace the system\ncalls made by a program. On Linux, the tool is called strace; other sys-\ntems have similar tools (see dtruss on Mac OS X, or truss on some older\nUNIX variants). What strace does is trace every system call made by a\nprogram while it runs, and dump the trace to the screen for you to see.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nINTERLUDE: FILE AND DIRECTORIES\n445\nTIP: USE STRACE (AND SIMILAR TOOLS)\nThe strace tool provides an awesome way to see what programs are up\nto. By running it, you can trace which system calls a program makes, see\nthe arguments and return codes, and generally get a very good idea of\nwhat is going on.\nThe tool also takes some arguments which can be quite useful. For ex-\nample, -f follows any fork’d children too; -t reports the time of day\nat each call; -e trace=open,close,read,write only traces calls to\nthose system calls and ignores all others. There are many more powerful\nﬂags – read the man pages and ﬁnd out how to harness this wonderful\ntool.\nHere is an example of using strace to ﬁgure out what cat is doing\n(some calls removed for readability):\nprompt> strace cat foo\n...\nopen(\"foo\", O_RDONLY|O_LARGEFILE)\n= 3\nread(3, \"hello\\n\", 4096)\n= 6\nwrite(1, \"hello\\n\", 6)\n= 6\nhello\nread(3, \"\", 4096)\n= 0\nclose(3)\n= 0\n...\nprompt>\nThe ﬁrst thing that cat does is open the ﬁle for reading. A couple\nof things we should note about this; ﬁrst, that the ﬁle is only opened for\nreading (not writing), as indicated by the O RDONLY ﬂag; second, that\nthe 64-bit offset be used (O LARGEFILE); third, that the call to open()\nsucceeds and returns a ﬁle descriptor, which has the value of 3.\nWhy does the ﬁrst call to open() return 3, not 0 or perhaps 1 as you\nmight expect? As it turns out, each running process already has three\nﬁles open, standard input (which the process can read to receive input),\nstandard output (which the process can write to in order to dump infor-\nmation to the screen), and standard error (which the process can write\nerror messages to). These are represented by ﬁle descriptors 0, 1, and 2,\nrespectively. Thus, when you ﬁrst open another ﬁle (as cat does above),\nit will almost certainly be ﬁle descriptor 3.\nAfter the open succeeds, cat uses the read() system call to repeat-\nedly read some bytes from a ﬁle. The ﬁrst argument to read() is the ﬁle\ndescriptor, thus telling the ﬁle system which ﬁle to read; a process can of\ncourse have multiple ﬁles open at once, and thus the descriptor enables\nthe operating system to know which ﬁle a particular read refers to. The\nsecond argument points to a buffer where the result of the read() will be\nplaced; in the system-call trace above, strace shows the results of the read\nin this spot (“hello”). The third argument is the size of the buffer, which\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n446\nINTERLUDE: FILE AND DIRECTORIES\nin this case is 4 KB. The call to read() returns successfully as well, here\nreturning the number of bytes it read (6, which includes 5 for the letters\nin the word “hello” and one for an end-of-line marker).\nAt this point, you see another interesting result of the strace: a single\ncall to the write() system call, to the ﬁle descriptor 1. As we mentioned\nabove, this descriptor is known as the standard output, and thus is used\nto write the word “hello” to the screen as the program cat is meant to\ndo. But does it call write() directly? Maybe (if it is highly optimized).\nBut if not, what cat might do is call the library routine printf(); in-\nternally, printf() ﬁgures out all the formatting details passed to it, and\neventually calls write on the standard output to print the results to the\nscreen.\nThe cat program then tries to read more from the ﬁle, but since there\nare no bytes left in the ﬁle, the read() returns 0 and the program knows\nthat this means it has read the entire ﬁle. Thus, the program calls close()\nto indicate that it is done with the ﬁle “foo”, passing in the corresponding\nﬁle descriptor. The ﬁle is thus closed, and the reading of it thus complete.\nWriting a ﬁle is accomplished via a similar set of steps. First, a ﬁle\nis opened for writing, then the write() system call is called, perhaps\nrepeatedly for larger ﬁles, and then close(). Use strace to trace writes\nto a ﬁle, perhaps of a program you wrote yourself, or by tracing the dd\nutility, e.g., dd if=foo of=bar.\n39.5\nReading And Writing, But Not Sequentially\nThus far, we’ve discussed how to read and write ﬁles, but all access\nhas been sequential; that is, we have either read a ﬁle from the beginning\nto the end, or written a ﬁle out from beginning to end.\nSometimes, however, it is useful to be able to read or write to a spe-\nciﬁc offset within a ﬁle; for example, if you build an index over a text\ndocument, and use it to look up a speciﬁc word, you may end up reading\nfrom some random offsets within the document. To do so, we will use\nthe lseek() system call. Here is the function prototype:\noff_t lseek(int fildes, off_t offset, int whence);\nThe ﬁrst argument is familiar (a ﬁle descriptor). The second argu-\nment is the offset, which positions the ﬁle offset to a particular location\nwithin the ﬁle. The third argument, called whence for historical reasons,\ndetermines exactly how the seek is performed. From the man page:\nIf whence is SEEK_SET, the offset is set to offset bytes.\nIf whence is SEEK_CUR, the offset is set to its current\nlocation plus offset bytes.\nIf whence is SEEK_END, the offset is set to the size of\nthe file plus offset bytes.\nAs you can tell from this description, for each ﬁle a process opens, the\nOS tracks a “current” offset, which determines where the next read or\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nINTERLUDE: FILE AND DIRECTORIES\n447\nASIDE: CALLING LSEEK() DOES NOT PERFORM A DISK SEEK\nThe poorly-named system call lseek() confuses many a student try-\ning to understand disks and how the ﬁle systems atop them work. Do\nnot confuse the two! The lseek() call simply changes a variable in OS\nmemory that tracks, for a particular process, at which offset to which its\nnext read or write will start. A disk seek occurs when a read or write\nissued to the disk is not on the same track as the last read or write, and\nthus necessitates a head movement. Making this even more confusing is\nthe fact that calling lseek() to read or write from/to random parts of a\nﬁle, and then reading/writing to those random parts, will indeed lead to\nmore disk seeks. Thus, calling lseek() can certainly lead to a seek in an\nupcoming read or write, but absolutely does not cause any disk I/O to\noccur itself.\nwrite will begin reading from or writing to within the ﬁle. Thus, part\nof the abstraction of an open ﬁle is that it has a current offset, which\nis updated in one of two ways. The ﬁrst is when a read or write of N\nbytes takes place, N is added to the current offset; thus each read or write\nimplicitly updates the offset. The second is explicitly with lseek, which\nchanges the offset as speciﬁed above.\nNote that this call lseek() has nothing to do with the seek operation\nof a disk, which moves the disk arm. The call to lseek() simply changes\nthe value of a variable within the kernel; when the I/O is performed,\ndepending on where the disk head is, the disk may or may not perform\nan actual seek to fulﬁll the request.\n39.6\nWriting Immediately with fsync()\nMost times when a program calls write(), it is just telling the ﬁle\nsystem: please write this data to persistent storage, at some point in the\nfuture. The ﬁle system, for performance reasons, will buffer such writes\nin memory for some time (say 5 seconds, or 30); at that later point in\ntime, the write(s) will actually be issued to the storage device. From the\nperspective of the calling application, writes seem to complete quickly,\nand only in rare cases (e.g., the machine crashes after the write() call\nbut before the write to disk) will data be lost.\nHowever, some applications require something more than this even-\ntual guarantee. For example, in a database management system (DBMS),\ndevelopment of a correct recovery protocol requires the ability to force\nwrites to disk from time to time.\nTo support these types of applications, most ﬁle systems provide some\nadditional control APIs. In the UNIX world, the interface provided to ap-\nplications is known as fsync(int fd). When a process calls fsync()\nfor a particular ﬁle descriptor, the ﬁle system responds by forcing all dirty\n(i.e., not yet written) data to disk, for the ﬁle referred to by the speciﬁed\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n448\nINTERLUDE: FILE AND DIRECTORIES\nﬁle descriptor. The fsync() routine returns once all of these writes are\ncomplete.\nHere is a simple example of how to use fsync(). The code opens\nthe ﬁle foo, writes a single chunk of data to it, and then calls fsync()\nto ensure the writes are forced immediately to disk. Once the fsync()\nreturns, the application can safely move on, knowing that the data has\nbeen persisted (if fsync() is correctly implemented, that is).\nint fd = open(\"foo\", O_CREAT | O_WRONLY | O_TRUNC);\nassert(fd > -1);\nint rc = write(fd, buffer, size);\nassert(rc == size);\nrc = fsync(fd);\nassert(rc == 0);\nInterestingly, this sequence does not guarantee everything that you\nmight expect; in some cases, you also need to fsync() the directory that\ncontains the ﬁle foo. Adding this step ensures not only that the ﬁle itself\nis on disk, but that the ﬁle, if newly created, also is durably a part of the\ndirectory. Not surprisingly, this type of detail is often overlooked, leading\nto many application-level bugs [P+13].\n39.7\nRenaming Files\nOnce we have a ﬁle, it is sometimes useful to be able to give a ﬁle a\ndifferent name. When typing at the command line, this is accomplished\nwith mv command; in this example, the ﬁle foo is renamed bar:\nprompt> mv foo bar\nUsing strace, we can see that mv uses the system call rename(char\n*old, char *new), which takes precisely two arguments: the original\nname of the ﬁle (old) and the new name (new).\nOne interesting guarantee provided by the rename() call is that it is\n(usually) implemented as an atomic call with respect to system crashes;\nif the system crashes during the renaming, the ﬁle will either be named\nthe old name or the new name, and no odd in-between state can arise.\nThus, rename() is critical for supporting certain kinds of applications\nthat require an atomic update to ﬁle state.\nLet’s be a little more speciﬁc here. Imagine that you are using a ﬁle ed-\nitor (e.g., emacs), and you insert a line into the middle of a ﬁle. The ﬁle’s\nname, for the example, is foo.txt. The way the editor might update the\nﬁle to guarantee that the new ﬁle has the original contents plus the line\ninserted is as follows (ignoring error-checking for simplicity):\nint fd = open(\"foo.txt.tmp\", O_WRONLY|O_CREAT|O_TRUNC);\nwrite(fd, buffer, size); // write out new version of file\nfsync(fd);\nclose(fd);\nrename(\"foo.txt.tmp\", \"foo.txt\");\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nINTERLUDE: FILE AND DIRECTORIES\n449\nWhat the editor does in this example is simple: write out the new\nversion of the ﬁle under temporary name (foot.txt.tmp), force it to\ndisk with fsync(), and then, when the application is certain the new\nﬁle metadata and contents are on the disk, rename the temporary ﬁle to\nthe original ﬁle’s name. This last step atomically swaps the new ﬁle into\nplace, while concurrently deleting the old version of the ﬁle, and thus an\natomic ﬁle update is achieved.\n39.8\nGetting Information About Files\nBeyond ﬁle access, we expect the ﬁle system to keep a fair amount of\ninformation about each ﬁle it is storing. We generally call such data about\nﬁles metadata. To see the metadata for a certain ﬁle, we can use stat()\nor fstat() system call – read their man pages for details on how to call\nthem. These calls take a pathname (or ﬁle descriptor) to a ﬁle and ﬁll in a\nstat structure as seen here:\nstruct stat {\ndev_t\nst_dev;\n/* ID of device containing file */\nino_t\nst_ino;\n/* inode number */\nmode_t\nst_mode;\n/* protection */\nnlink_t\nst_nlink;\n/* number of hard links */\nuid_t\nst_uid;\n/* user ID of owner */\ngid_t\nst_gid;\n/* group ID of owner */\ndev_t\nst_rdev;\n/* device ID (if special file) */\noff_t\nst_size;\n/* total size, in bytes */\nblksize_t st_blksize; /* blocksize for filesystem I/O */\nblkcnt_t\nst_blocks;\n/* number of blocks allocated */\ntime_t\nst_atime;\n/* time of last access */\ntime_t\nst_mtime;\n/* time of last modification */\ntime_t\nst_ctime;\n/* time of last status change */\n};\nYou can see that there is a lot of information kept about each ﬁle, in-\ncluding its size (in bytes), its low-level name (i.e., inode number), some\nownership information, and some information about when the ﬁle was\naccessed or modiﬁed, among other things. To see this information, you\ncan use the command line tool stat:\nprompt> echo hello > file\nprompt> stat file\nFile: ‘file’\nSize: 6 Blocks: 8\nIO Block: 4096\nregular file\nDevice: 811h/2065d Inode: 67158084\nLinks: 1\nAccess: (0640/-rw-r-----) Uid: (30686/ remzi) Gid: (30686/ remzi)\nAccess: 2011-05-03 15:50:20.157594748 -0500\nModify: 2011-05-03 15:50:20.157594748 -0500\nChange: 2011-05-03 15:50:20.157594748 -0500\nAs it turns out, each ﬁle system usually keeps this type of information\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "page_number": 476,
      "chapter_number": 47,
      "summary": "Thus, in the next few chapters, we will explore critical techniques for\nmanaging persistent data, focusing on methods to improve performance\nand reliability Key topics include write, writing, and directories.",
      "keywords": [
        "ﬁle",
        "ﬁle system",
        "ﬁle descriptor",
        "system",
        "system call",
        "ﬁles",
        "write",
        "call",
        "read",
        "foo",
        "ﬁle foo",
        "UNIX ﬁle system",
        "File",
        "Directories",
        "directory"
      ],
      "concepts": [
        "write",
        "writing",
        "directories",
        "directory",
        "calling",
        "open",
        "file",
        "time",
        "read",
        "device"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 31,
          "title": "Segment 31 (pages 291-298)",
          "relevance_score": 0.61,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 55,
          "title": "Segment 55 (pages 546-554)",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 3,
          "title": "Segment 3 (pages 18-26)",
          "relevance_score": 0.54,
          "method": "api"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 6,
          "title": "Segment 6 (pages 45-52)",
          "relevance_score": 0.54,
          "method": "api"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 15,
          "title": "Segment 15 (pages 115-122)",
          "relevance_score": 0.53,
          "method": "api"
        }
      ]
    },
    {
      "number": 48,
      "title": "Segment 48 (pages 486-493)",
      "start_page": 486,
      "end_page": 493,
      "detection_method": "topic_boundary",
      "content": "450\nINTERLUDE: FILE AND DIRECTORIES\nin a structure called an inode1. We’ll be learning a lot more about inodes\nwhen we talk about ﬁle system implementation. For now, you should just\nthink of an inode as a persistent data structure kept by the ﬁle system that\nhas information like we see above inside of it.\n39.9\nRemoving Files\nAt this point, we know how to create ﬁles and access them, either se-\nquentially or not. But how do you delete ﬁles? If you’ve used UNIX, you\nprobably think you know: just run the program rm. But what system call\ndoes rm use to remove a ﬁle?\nLet’s use our old friend strace again to ﬁnd out. Here we remove\nthat pesky ﬁle “foo”:\nprompt> strace rm foo\n...\nunlink(\"foo\")\n= 0\n...\nWe’ve removed a bunch of unrelated cruft from the traced output,\nleaving just a single call to the mysteriously-named system call unlink().\nAs you can see, unlink() just takes the name of the ﬁle to be removed,\nand returns zero upon success. But this leads us to a great puzzle: why\nis this system call named “unlink”? Why not just “remove” or “delete”.\nTo understand the answer to this puzzle, we must ﬁrst understand more\nthan just ﬁles, but also directories.\n39.10\nMaking Directories\nBeyond ﬁles, a set of directory-related system calls enable you to make,\nread, and delete directories. Note you can never write to a directory di-\nrectly; because the format of the directory is considered ﬁle system meta-\ndata, you can only update a directory indirectly by, for example, creating\nﬁles, directories, or other object types within it. In this way, the ﬁle system\nmakes sure that the contents of the directory always are as expected.\nTo create a directory, a single system call, mkdir(), is available. The\neponymous mkdir program can be used to create such a directory. Let’s\ntake a look at what happens when we run the mkdir program to make a\nsimple directory called foo:\nprompt> strace mkdir foo\n...\nmkdir(\"foo\", 0777)\n= 0\n...\nprompt>\n1Some ﬁle systems call these structures similar, but slightly different, names, such as\ndnodes; the basic idea is similar however.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nINTERLUDE: FILE AND DIRECTORIES\n451\nTIP: BE WARY OF POWERFUL COMMANDS\nThe program rm provides us with a great example of powerful com-\nmands, and how sometimes too much power can be a bad thing. For\nexample, to remove a bunch of ﬁles at once, you can type something like:\nprompt> rm *\nwhere the * will match all ﬁles in the current directory. But sometimes\nyou want to also delete the directories too, and in fact all of their contents.\nYou can do this by telling rm to recursively descend into each directory,\nand remove its contents too:\nprompt> rm -rf *\nWhere you get into trouble with this small string of characters is when\nyou issue the command, accidentally, from the root directory of a ﬁle sys-\ntem, thus removing every ﬁle and directory from it. Oops!\nThus, remember the double-edged sword of powerful commands; while\nthey give you the ability to do a lot of work with a small number of\nkeystrokes, they also can quickly and readily do a great deal of harm.\nWhen such a directory is created, it is considered “empty”, although it\ndoes have a bare minimum of contents. Speciﬁcally, an empty directory\nhas two entries: one entry that refers to itself, and one entry that refers\nto its parent. The former is referred to as the “.” (dot) directory, and the\nlatter as “..” (dot-dot). You can see these directories by passing a ﬂag (-a)\nto the program ls:\nprompt> ls -a\n./\n../\nprompt> ls -al\ntotal 8\ndrwxr-x---\n2 remzi remzi\n6 Apr 30 16:17 ./\ndrwxr-x--- 26 remzi remzi 4096 Apr 30 16:17 ../\n39.11\nReading Directories\nNow that we’ve created a directory, we might wish to read one too.\nIndeed, that is exactly what the program ls does. Let’s write our own\nlittle tool like ls and see how it is done.\nInstead of just opening a directory as if it were a ﬁle, we instead use\na new set of calls. Below is an example program that prints the contents\nof a directory. The program uses three calls, opendir(), readdir(),\nand closedir(), to get the job done, and you can see how simple the\ninterface is; we just use a simple loop to read one directory entry at a time,\nand print out the name and inode number of each ﬁle in the directory.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n452\nINTERLUDE: FILE AND DIRECTORIES\nint main(int argc, char *argv[]) {\nDIR *dp = opendir(\".\");\nassert(dp != NULL);\nstruct dirent *d;\nwhile ((d = readdir(dp)) != NULL) {\nprintf(\"%d %s\\n\", (int) d->d_ino, d->d_name);\n}\nclosedir(dp);\nreturn 0;\n}\nThe declaration below shows the information available within each\ndirectory entry in the struct dirent data structure:\nstruct dirent {\nchar\nd_name[256]; /* filename */\nino_t\nd_ino;\n/* inode number */\noff_t\nd_off;\n/* offset to the next dirent */\nunsigned short d_reclen;\n/* length of this record */\nunsigned char\nd_type;\n/* type of file */\n};\nBecause directories are light on information (basically, just mapping\nthe name to the inode number, along with a few other details), a program\nmay want to call stat() on each ﬁle to get more information on each,\nsuch as its length or other detailed information. Indeed, this is exactly\nwhat ls does when you pass it the -l ﬂag; try strace on ls with and\nwithout that ﬂag to see for yourself.\n39.12\nDeleting Directories\nFinally, you can delete a directory with a call to rmdir() (which is\nused by the program of the same name, rmdir). Unlike ﬁle deletion,\nhowever, removing directories is more dangerous, as you could poten-\ntially delete a large amount of data with a single command. Thus, rmdir()\nhas the requirement that the directory be empty (i.e., only has “.” and “..”\nentries) before it is deleted. If you try to delete a non-empty directory, the\ncall to rmdir() simply will fail.\n39.13\nHard Links\nWe now come back to the mystery of why removing a ﬁle is performed\nvia unlink(), by understanding a new way to make an entry in the\nﬁle system tree, through a system call known as link(). The link()\nsystem call takes two arguments, an old pathname and a new one; when\nyou “link” a new ﬁle name to an old one, you essentially create another\nway to refer to the same ﬁle. The command-line program ln is used to\ndo this, as we see in this example:\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nINTERLUDE: FILE AND DIRECTORIES\n453\nprompt> echo hello > file\nprompt> cat file\nhello\nprompt> ln file file2\nprompt> cat file2\nhello\nHere we created a ﬁle with the word “hello” in it, and called the ﬁle\nfile2. We then create a hard link to that ﬁle using the ln program. After\nthis, we can examine the ﬁle by either opening file or file2.\nThe way link works is that it simply creates another name in the di-\nrectory you are creating the link to, and refers it to the same inode number\n(i.e., low-level name) of the original ﬁle. The ﬁle is not copied in any way;\nrather, you now just have two human names (file and file2) that both\nrefer to the same ﬁle. We can even see this in the directory itself, by print-\ning out the inode number of each ﬁle:\nprompt> ls -i file file2\n67158084 file\n67158084 file2\nprompt>\nBy passing the -i ﬂag to ls, it prints out the inode number of each ﬁle\n(as well as the ﬁle name). And thus you can see what link really has done:\njust make a new reference to the same exact inode number (67158084 in\nthis example).\nBy now you might be starting to see why unlink() is called unlink().\nWhen you create a ﬁle, you are really doing two things. First, you are\nmaking a structure (the inode) that will track virtually all relevant infor-\nmation about the ﬁle, including its size, where its blocks are on disk, and\nso forth. Second, you are linking a human-readable name to that ﬁle, and\nputting that link into a directory.\nAfter creating a hard link to a ﬁle, to the ﬁle system, there is no dif-\nference between the original ﬁle name (file) and the newly created ﬁle\nname (file2); indeed, they are both just links to the underlying meta-\ndata about the ﬁle, which is found in inode number 67158084.\nThus, to remove a ﬁle from the ﬁle system, we call unlink(). In the\nexample above, we could for example remove the ﬁle named file, and\nstill access the ﬁle without difﬁculty:\nprompt> rm file\nremoved ‘file’\nprompt> cat file2\nhello\nThe reason this works is because when the ﬁle system unlinks ﬁle, it\nchecks a reference count within the inode number. This reference count\n2Note how creative the authors of this book are. We also used to have a cat named “Cat”\n(true story). However, she died, and we now have a hamster named “Hammy.”\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n454\nINTERLUDE: FILE AND DIRECTORIES\n(sometimes called the link count) allows the ﬁle system to track how\nmany different ﬁle names have been linked to this particular inode. When\nunlink() is called, it removes the “link” between the human-readable\nname (the ﬁle that is being deleted) to the given inode number, and decre-\nments the reference count; only when the reference count reaches zero\ndoes the ﬁle system also free the inode and related data blocks, and thus\ntruly “delete” the ﬁle.\nYou can see the reference count of a ﬁle using stat() of course. Let’s\nsee what it is when we create and delete hard links to a ﬁle. In this exam-\nple, we’ll create three links to the same ﬁle, and then delete them. Watch\nthe link count!\nprompt> echo hello > file\nprompt> stat file\n... Inode: 67158084\nLinks: 1 ...\nprompt> ln file file2\nprompt> stat file\n... Inode: 67158084\nLinks: 2 ...\nprompt> stat file2\n... Inode: 67158084\nLinks: 2 ...\nprompt> ln file2 file3\nprompt> stat file\n... Inode: 67158084\nLinks: 3 ...\nprompt> rm file\nprompt> stat file2\n... Inode: 67158084\nLinks: 2 ...\nprompt> rm file2\nprompt> stat file3\n... Inode: 67158084\nLinks: 1 ...\nprompt> rm file3\n39.14\nSymbolic Links\nThere is one other type of link that is really useful, and it is called a\nsymbolic link or sometimes a soft link. As it turns out, hard links are\nsomewhat limited: you can’t create one to a directory (for fear that you\nwill create a cycle in the directory tree); you can’t hard link to ﬁles in\nother disk partitions (because inode numbers are only unique within a\nparticular ﬁle system, not across ﬁle systems); etc. Thus, a new type of\nlink called the symbolic link was created.\nTo create such a link, you can use the same program ln, but with the\n-s ﬂag. Here is an example:\nprompt> echo hello > file\nprompt> ln -s file file2\nprompt> cat file2\nhello\nAs you can see, creating a soft link looks much the same, and the orig-\ninal ﬁle can now be accessed through the ﬁle name file as well as the\nsymbolic link name file2.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nINTERLUDE: FILE AND DIRECTORIES\n455\nHowever, beyond this surface similarity, symbolic links are actually\nquite different from hard links. The ﬁrst difference is that a symbolic\nlink is actually a ﬁle itself, of a different type. We’ve already talked about\nregular ﬁles and directories; symbolic links are a third type the ﬁle system\nknows about. A stat on the symlink reveals all:\nprompt> stat file\n... regular file ...\nprompt> stat file2\n... symbolic link ...\nRunning ls also reveals this fact. If you look closely at the ﬁrst char-\nacter of the long-form of the output from ls, you can see that the ﬁrst\ncharacter in the left-most column is a - for regular ﬁles, a d for directo-\nries, and an l for soft links. You can also see the size of the symbolic link\n(4 bytes in this case), as well as what the link points to (the ﬁle named\nfile).\nprompt> ls -al\ndrwxr-x---\n2 remzi remzi\n29 May\n3 19:10 ./\ndrwxr-x--- 27 remzi remzi 4096 May\n3 15:14 ../\n-rw-r-----\n1 remzi remzi\n6 May\n3 19:10 file\nlrwxrwxrwx\n1 remzi remzi\n4 May\n3 19:10 file2 -> file\nThe reason that file2 is 4 bytes is because the way a symbolic link is\nformed is by holding the pathname of the linked-to ﬁle as the data of the\nlink ﬁle. Because we’ve linked to a ﬁle named file, our link ﬁle file2\nis small (4 bytes). If we link to a longer pathname, our link ﬁle would be\nbigger:\nprompt> echo hello > alongerfilename\nprompt> ln -s alongerfilename file3\nprompt> ls -al alongerfilename file3\n-rw-r----- 1 remzi remzi\n6 May\n3 19:17 alongerfilename\nlrwxrwxrwx 1 remzi remzi 15 May\n3 19:17 file3 -> alongerfilename\nFinally, because of the way symbolic links are created, they leave the\npossibility for what is known as a dangling reference:\nprompt> echo hello > file\nprompt> ln -s file file2\nprompt> cat file2\nhello\nprompt> rm file\nprompt> cat file2\ncat: file2: No such file or directory\nAs you can see in this example, quite unlike hard links, removing the\noriginal ﬁle named file causes the link to point to a pathname that no\nlonger exists.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n456\nINTERLUDE: FILE AND DIRECTORIES\n39.15\nMaking and Mounting a File System\nWe’ve now toured the basic interfaces to access ﬁles, directories, and\ncertain types of special types of links. But there is one more topic we\nshould discuss: how to assemble a full directory tree from many under-\nlying ﬁle systems. This task is accomplished via ﬁrst making ﬁle systems,\nand then mounting them to make their contents accessible.\nTo make a ﬁle system, most ﬁle systems provide a tool, usually re-\nferred to as mkfs (pronounced “make fs”), that performs exactly this task.\nThe idea is as follows: give the tool, as input, a device (such as a disk\npartition, e.g., /dev/sda1) a ﬁle system type (e.g., ext3), and it simply\nwrites an empty ﬁle system, starting with a root directory, onto that disk\npartition. And mkfs said, let there be a ﬁle system!\nHowever, once such a ﬁle system is created, it needs to be made ac-\ncessible within the uniform ﬁle-system tree. This task is achieved via the\nmount program (which makes the underlying system call mount() to do\nthe real work). What mount does, quite simply is take an existing direc-\ntory as a target mount point and essentially paste a new ﬁle system onto\nthe directory tree at that point.\nAn example here might be useful. Imagine we have an unmounted\next3 ﬁle system, stored in device partition /dev/sda1, that has the fol-\nlowing contents: a root directory which contains two sub-directories, a\nand b, each of which in turn holds a single ﬁle named foo. Let’s say we\nwish to mount this ﬁle system at the mount point /home/users. We\nwould type something like this:\nprompt> mount -t ext3 /dev/sda1 /home/users\nIf successful, the mount would thus make this new ﬁle system avail-\nable. However, note how the new ﬁle system is now accessed. To look at\nthe contents of the root directory, we would use ls like this:\nprompt> ls /home/users/\na b\nAs you can see, the pathname /home/users/ now refers to the root\nof the newly-mounted directory. Similarly, we could access ﬁles a and\nb with the pathnames /home/users/a and /home/users/b. Finally,\nthe ﬁles named foo could be accessed via /home/users/a/foo and\n/home/users/b/foo. And thus the beauty of mount: instead of having\na number of separate ﬁle systems, mount uniﬁes all ﬁle systems into one\ntree, making naming uniform and convenient.\nTo see what is mounted on your system, and at which points, simply\nrun the mount program. You’ll see something like this:\n/dev/sda1 on / type ext3 (rw)\nproc on /proc type proc (rw)\nsysfs on /sys type sysfs (rw)\n/dev/sda5 on /tmp type ext3 (rw)\n/dev/sda7 on /var/vice/cache type ext3 (rw)\ntmpfs on /dev/shm type tmpfs (rw)\nAFS on /afs type afs (rw)\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nINTERLUDE: FILE AND DIRECTORIES\n457\nThis crazy mix shows that a whole number of different ﬁle systems,\nincluding ext3 (a standard disk-based ﬁle system), the proc ﬁle system (a\nﬁle system for accessing information about current processes), tmpfs (a\nﬁle system just for temporary ﬁles), and AFS (a distributed ﬁle system)\nare all glued together onto this one machine’s ﬁle-system tree.\n39.16\nSummary\nThe ﬁle system interface in UNIX systems (and indeed, in any system)\nis seemingly quite rudimentary, but there is a lot to understand if you\nwish to master it. Nothing is better, of course, than simply using it (a lot).\nSo please do so! Of course, read more; as always, Stevens [SR05] is the\nplace to begin.\nWe’ve toured the basic interfaces, and hopefully understood a little bit\nabout how they work. Even more interesting is how to implement a ﬁle\nsystem that meets the needs of the API, a topic we will delve into in great\ndetail next.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "page_number": 486,
      "chapter_number": 48,
      "summary": "This chapter covers segment 48 (pages 486-493). Key topics include directories, directory, and prompt. For now, you should just\nthink of an inode as a persistent data structure kept by the ﬁle system that\nhas information like we see above inside of it.",
      "keywords": [
        "ﬁle system",
        "ﬁle",
        "FILE",
        "prompt",
        "system",
        "link",
        "directory",
        "DIRECTORIES",
        "file prompt",
        "ﬁle named file",
        "ﬁles",
        "inode",
        "system call",
        "link ﬁle",
        "remzi"
      ],
      "concepts": [
        "directories",
        "directory",
        "prompt",
        "file",
        "types",
        "links",
        "named",
        "names",
        "naming",
        "making"
      ],
      "similar_chapters": [
        {
          "book": "A Philosophy of Software Design",
          "chapter": 17,
          "title": "Segment 17 (pages 144-151)",
          "relevance_score": 0.45,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 34,
          "title": "Segment 34 (pages 683-702)",
          "relevance_score": 0.44,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 10,
          "title": "Segment 10 (pages 80-87)",
          "relevance_score": 0.44,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 34,
          "title": "Segment 34 (pages 317-324)",
          "relevance_score": 0.43,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 15,
          "title": "Segment 15 (pages 135-143)",
          "relevance_score": 0.42,
          "method": "api"
        }
      ]
    },
    {
      "number": 49,
      "title": "Segment 49 (pages 494-511)",
      "start_page": 494,
      "end_page": 511,
      "detection_method": "topic_boundary",
      "content": "458\nINTERLUDE: FILE AND DIRECTORIES\nReferences\n[K84] “Processes as Files”\nTom J. Killian\nUSENIX, June 1984\nThe paper that introduced the /proc ﬁle system, where each process can be treated as a ﬁle within a\npseudo ﬁle system. A clever idea that you can still see in modern UNIX systems.\n[L84] “Capability-Based Computer Systems”\nHenry M. Levy\nDigital Press, 1984\nAvailable: http://homes.cs.washington.edu/ levy/capabook\nAn excellent overview of early capability-based systems.\n[P+13] “Towards Efﬁcient, Portable Application-Level Consistency”\nThanumalayan S. Pillai, Vijay Chidambaram, Joo-Young Hwang, Andrea C. Arpaci-Dusseau,\nand Remzi H. Arpaci-Dusseau\nHotDep ’13, November 2013\nOur own work that shows how readily applications can make mistakes in committing data to disk; in\nparticular, assumptions about the ﬁle system creep into applications and thus make the applications\nwork correctly only if they are running on a speciﬁc ﬁle system.\n[SK09] “Principles of Computer System Design”\nJerome H. Saltzer and M. Frans Kaashoek\nMorgan-Kaufmann, 2009\nThis tour de force of systems is a must-read for anybody interested in the ﬁeld. It’s how they teach\nsystems at MIT. Read it once, and then read it a few more times to let it all soak in.\n[SR05] “Advanced Programming in the UNIX Environment”\nW. Richard Stevens and Stephen A. Rago\nAddison-Wesley, 2005\nWe have probably referenced this book a few hundred thousand times. It is that useful to you, if you care\nto become an awesome systems programmer.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nINTERLUDE: FILE AND DIRECTORIES\n459\nHomework\nIn this homework, we’ll just familiarize ourselves with how the APIs\ndescribed in the chapter work. To do so, you’ll just write a few different\nprograms, mostly based on various UNIX utilities.\nQuestions\n1. Stat: Write your own version of the command line program stat,\nwhich simply calls the stat() system call on a given ﬁle or di-\nrectory. Print out ﬁle size, number of blocks allocated, reference\n(link) count, and so forth. What is the link count of a directory, as\nthe number of entries in the directory changes? Useful interfaces:\nstat()\n2. List Files: Write a program that lists ﬁles in the given directory.\nWhen called without any arguments, the program should just print\nthe ﬁle names. When invoked with the -l ﬂag, the program should\nprint out information about each ﬁle, such as the owner, group, per-\nmissions, and other information obtained from the stat() system\ncall. The program should take one additional argument, which is\nthe directory to read, e.g., myls -l directory. If no directory is\ngiven, the program should just use the current working directory.\nUseful interfaces: stat(), opendir(), readdir(), getcwd().\n3. Tail: Write a program that prints out the last few lines of a ﬁle. The\nprogram should be efﬁcient, in that it seeks to near the end of the\nﬁle, reads in a block of data, and then goes backwards until it ﬁnds\nthe requested number of lines; at this point, it should print out those\nlines from beginning to the end of the ﬁle. To invoke the program,\none should type: mytail -n file, where n is the number of lines\nat the end of the ﬁle to print. Useful interfaces: stat(), lseek(),\nopen(), read(), close().\n4. Recursive Search: Write a program that prints out the names of\neach ﬁle and directory in the ﬁle system tree, starting at a given\npoint in the tree. For example, when run without arguments, the\nprogram should start with the current working directory and print\nits contents, as well as the contents of any sub-directories, etc., until\nthe entire tree, root at the CWD, is printed. If given a single argu-\nment (of a directory name), use that as the root of the tree instead.\nReﬁne your recursive search with more fun options, similar to the\npowerful find command line tool. Useful interfaces: you ﬁgure it\nout.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n40\nFile System Implementation\nIn this chapter, we introduce a simple ﬁle system implementation, known\nas vsfs (the Very Simple File System). This ﬁle system is a simpliﬁed\nversion of a typical UNIX ﬁle system and thus serves to introduce some\nof the basic on-disk structures, access methods, and various policies that\nyou will ﬁnd in many ﬁle systems today.\nThe ﬁle system is pure software; unlike our development of CPU and\nmemory virtualization, we will not be adding hardware features to make\nsome aspect of the ﬁle system work better (though we will want to pay at-\ntention to device characteristics to make sure the ﬁle system works well).\nBecause of the great ﬂexibility we have in building a ﬁle system, many\ndifferent ones have been built, literally from AFS (the Andrew File Sys-\ntem) [H+88] to ZFS (Sun’s Zettabyte File System) [B07]. All of these ﬁle\nsystems have different data structures and do some things better or worse\nthan their peers. Thus, the way we will be learning about ﬁle systems is\nthrough case studies: ﬁrst, a simple ﬁle system (vsfs) in this chapter to\nintroduce most concepts, and then a series of studies of real ﬁle systems\nto understand how they can differ in practice.\nTHE CRUX: HOW TO IMPLEMENT A SIMPLE FILE SYSTEM\nHow can we build a simple ﬁle system? What structures are needed\non the disk? What do they need to track? How are they accessed?\n40.1\nThe Way To Think\nTo think about ﬁle systems, we usually suggest thinking about two\ndifferent aspects of them; if you understand both of these aspects, you\nprobably understand how the ﬁle system basically works.\nThe ﬁrst is the data structures of the ﬁle system. In other words, what\ntypes of on-disk structures are utilized by the ﬁle system to organize its\ndata and metadata? The ﬁrst ﬁle systems we’ll see (including vsfs below)\nemploy simple structures, like arrays of blocks or other objects, whereas\n461\n\n\n462\nFILE SYSTEM IMPLEMENTATION\nASIDE: MENTAL MODELS OF FILE SYSTEMS\nAs we’ve discussed before, mental models are what you are really trying\nto develop when learning about systems. For ﬁle systems, your mental\nmodel should eventually include answers to questions like: what on-disk\nstructures store the ﬁle system’s data and metadata? What happens when\na process opens a ﬁle? Which on-disk structures are accessed during a\nread or write? By working on and improving your mental model, you\ndevelop an abstract understanding of what is going on, instead of just\ntrying to understand the speciﬁcs of some ﬁle-system code (though that\nis also useful, of course!).\nmore sophisticated ﬁle systems, like SGI’s XFS, use more complicated\ntree-based structures [S+96].\nThe second aspect of a ﬁle system is its access methods. How does\nit map the calls made by a process, such as open(), read(), write(),\netc., onto its structures? Which structures are read during the execution\nof a particular system call? Which are written? How efﬁciently are all of\nthese steps performed?\nIf you understand the data structures and access methods of a ﬁle sys-\ntem, you have developed a good mental model of how it truly works, a\nkey part of the systems mindset. Try to work on developing your mental\nmodel as we delve into our ﬁrst implementation.\n40.2\nOverall Organization\nWe now develop the overall on-disk organization of the data struc-\ntures of the vsfs ﬁle system. The ﬁrst thing we’ll need to do is divide the\ndisk into blocks; simple ﬁle systems use just one block size, and that’s\nexactly what we’ll do here. Let’s choose a commonly-used size of 4 KB.\nThus, our view of the disk partition where we’re building our ﬁle sys-\ntem is simple: a series of blocks, each of size 4 KB. The blocks are ad-\ndressed from 0 to N −1, in a partition of size N 4-KB blocks. Assume we\nhave a really small disk, with just 64 blocks:\n0\n7\n8\n15 16\n23 24\n31\n32\n39 40\n47 48\n55 56\n63\nLet’s now think about what we need to store in these blocks to build\na ﬁle system. Of course, the ﬁrst thing that comes to mind is user data.\nIn fact, most of the space in any ﬁle system is (and should be) user data.\nLet’s call the region of the disk we use for user data the data region, and,\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nFILE SYSTEM IMPLEMENTATION\n463\nagain for simplicity, reserve a ﬁxed portion of the disk for these blocks,\nsay the last 56 of 64 blocks on the disk:\n0\n7\nD\n8\nD D D D D D D\n15\nD\n16\nD D D D D D D\n23\nD\n24\nD D D D D D D\n31\nD\n32\nD D D D D D D\n39\nD\n40\nD D D D D D D\n47\nD\n48\nD D D D D D D\n55\nD\n56\nD D D D D D D\n63\nData Region\nData Region\nAs we learned about (a little) last chapter, the ﬁle system has to track\ninformation about each ﬁle. This information is a key piece of metadata,\nand tracks things like which data blocks (in the data region) comprise\na ﬁle, the size of the ﬁle, its owner and access rights, access and mod-\nify times, and other similar kinds of information. To store this informa-\ntion, ﬁle system usually have a structure called an inode (we’ll read more\nabout inodes below).\nTo accommodate inodes, we’ll need to reserve some space on the disk\nfor them as well. Let’s call this portion of the disk the inode table, which\nsimply holds an array of on-disk inodes. Thus, our on-disk image now\nlooks like this picture, assuming that we use 5 of our 64 blocks for inodes\n(denoted by I’s in the diagram):\n0\nI\nI\nI\nI\nI\n7\nD\n8\nD D D D D D D\n15\nD\n16\nD D D D D D D\n23\nD\n24\nD D D D D D D\n31\nD\n32\nD D D D D D D\n39\nD\n40\nD D D D D D D\n47\nD\n48\nD D D D D D D\n55\nD\n56\nD D D D D D D\n63\nData Region\nData Region\nInodes\nWe should note here that inodes are typically not that big, for example\n128 or 256 bytes. Assuming 256 bytes per inode, a 4-KB block can hold 16\ninodes, and our ﬁle system above contains 80 total inodes. In our simple\nﬁle system, built on a tiny 64-block partition, this number represents the\nmaximum number of ﬁles we can have in our ﬁle system; however, do\nnote that the same ﬁle system, built on a larger disk, could simply allocate\na larger inode table and thus accommodate more ﬁles.\nOur ﬁle system thus far has data blocks (D), and inodes (I), but a few\nthings are still missing. One primary component that is still needed, as\nyou might have guessed, is some way to track whether inodes or data\nblocks are free or allocated. Such allocation structures are thus a requisite\nelement in any ﬁle system.\nMany allocation-tracking methods are possible, of course. For exam-\nple, we could use a free list that points to the ﬁrst free block, which then\npoints to the next free block, and so forth. We instead choose a simple and\npopular structure known as a bitmap, one for the data region (the data\nbitmap), and one for the inode table (the inode bitmap). A bitmap is a\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n464\nFILE SYSTEM IMPLEMENTATION\nsimple structure: each bit is used to indicate whether the corresponding\nobject/block is free (0) or in-use (1). And thus our new on-disk layout,\nwith an inode bitmap (i) and a data bitmap (d):\n0\ni d\nI\nI\nI\nI\nI\n7\nD\n8\nD D D D D D D\n15\nD\n16\nD D D D D D D\n23\nD\n24\nD D D D D D D\n31\nD\n32\nD D D D D D D\n39\nD\n40\nD D D D D D D\n47\nD\n48\nD D D D D D D\n55\nD\n56\nD D D D D D D\n63\nData Region\nData Region\nInodes\nYou may notice that it is a bit of overkill to use an entire 4-KB block for\nthese bitmaps; such a bitmap can track whether 32K objects are allocated,\nand yet we only have 80 inodes and 56 data blocks. However, we just use\nan entire 4-KB block for each of these bitmaps for simplicity.\nThe careful reader (i.e., the reader who is still awake) may have no-\nticed there is one block left in the design of the on-disk structure of our\nvery simple ﬁle system. We reserve this for the superblock, denoted by\nan S in the diagram below. The superblock contains information about\nthis particular ﬁle system, including, for example, how many inodes and\ndata blocks are in the ﬁle system (80 and 56, respectively in this instance),\nwhere the inode table begins (block 3), and so forth. It will likely also\ninclude a magic number of some kind to identify the ﬁle system type (in\nthis case, vsfs).\nS\n0\ni d\nI\nI\nI\nI\nI\n7\nD\n8\nD D D D D D D\n15\nD\n16\nD D D D D D D\n23\nD\n24\nD D D D D D D\n31\nD\n32\nD D D D D D D\n39\nD\n40\nD D D D D D D\n47\nD\n48\nD D D D D D D\n55\nD\n56\nD D D D D D D\n63\nData Region\nData Region\nInodes\nThus, when mounting a ﬁle system, the operating system will read\nthe superblock ﬁrst, to initialize various parameters, and then attach the\nvolume to the ﬁle-system tree. When ﬁles within the volume are accessed,\nthe system will thus know exactly where to look for the needed on-disk\nstructures.\n40.3\nFile Organization: The Inode\nOne of the most important on-disk structures of a ﬁle system is the\ninode; virtually all ﬁle systems have a structure similar to this. The name\ninode is short for index node, the historical name given to it by UNIX in-\nventor Ken Thompson [RT74], used because these nodes were originally\narranged in an array, and the array indexed into when accessing a partic-\nular inode.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nFILE SYSTEM IMPLEMENTATION\n465\nASIDE: DATA STRUCTURE – THE INODE\nThe inode is the generic name that is used in many ﬁle systems to de-\nscribe the structure that holds the metadata for a given ﬁle, such as its\nlength, permissions, and the location of its constituent blocks. The name\ngoes back at least as far as UNIX (and probably further back to Multics\nif not earlier systems); it is short for index node, as the inode number is\nused to index into an array of on-disk inodes in order to ﬁnd the inode\nof that number. As we’ll see, design of the inode is one key part of ﬁle\nsystem design. Most modern systems have some kind of structure like\nthis for every ﬁle they track, but perhaps call them different things (such\nas dnodes, fnodes, etc.).\nEach inode is implicitly referred to by a number (called the inumber),\nwhich we’ve earlier called the low-level name of the ﬁle. In vsfs (and\nother simple ﬁle systems), given an i-number, you should directly be able\nto calculate where on the disk the corresponding inode is located. For ex-\nample, take the inode table of vsfs as above: 20-KB in size (5 4-KB blocks)\nand thus consisting of 80 inodes (assuming each inode is 256 bytes); fur-\nther assume that the inode region starts at 12KB (i.e, the superblock starts\nat 0KB, the inode bitmap is at address 4KB, the data bitmap at 8KB, and\nthus the inode table comes right after). In vsfs, we thus have the following\nlayout for the beginning of the ﬁle system partition (in closeup view):\nSuper\ni-bmap d-bmap\n0KB\n4KB\n8KB\n12KB\n16KB\n20KB\n24KB\n28KB\n32KB\nThe Inode Table (Closeup)\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10 11\n12 13 14 15\n16 17 18 19\n20 21 22 23\n24 25 26 27\n28 29 30 31\n32 33 34 35\n36 37 38 39\n40 41 42 43\n44 45 46 47\n48 49 50 51\n52 53 54 55\n56 57 58 59\n60 61 62 63\n64 65 66 67\n68 69 70 71\n72 73 74 75\n76 77 78 79\niblock 0\niblock 1\niblock 2\niblock 3\niblock 4\nTo read inode number 32, the ﬁle system would ﬁrst calculate the offset\ninto the inode region (32·sizeof(inode) or 8192, add it to the start address\nof the inode table on disk (inodeStartAddr = 12KB), and thus arrive\nupon the correct byte address of the desired block of inodes: 20KB. Re-\ncall that disks are not byte addressable, but rather consist of a large num-\nber of addressable sectors, usually 512 bytes. Thus, to fetch the block of\ninodes that contains inode 32, the ﬁle system would issue a read to sector\n20×1024\n512\n, or 40, to fetch the desired inode block. More generally, the sector\naddress iaddr of the inode block can be calculated as follows:\nblk\n= (inumber * sizeof(inode_t)) / blockSize;\nsector = ((blk * blockSize) + inodeStartAddr) / sectorSize;\nInside each inode is virtually all of the information you need about a\nﬁle: its type (e.g., regular ﬁle, directory, etc.), its size, the number of blocks\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n466\nFILE SYSTEM IMPLEMENTATION\nSize\nName\nWhat is this inode ﬁeld for?\n2\nmode\ncan this ﬁle be read/written/executed?\n2\nuid\nwho owns this ﬁle?\n4\nsize\nhow many bytes are in this ﬁle?\n4\ntime\nwhat time was this ﬁle last accessed?\n4\nctime\nwhat time was this ﬁle created?\n4\nmtime\nwhat time was this ﬁle last modiﬁed?\n4\ndtime\nwhat time was this inode deleted?\n2\ngid\nwhich group does this ﬁle belong to?\n2\nlinks count\nhow many hard links are there to this ﬁle?\n4\nblocks\nhow many blocks have been allocated to this ﬁle?\n4\nﬂags\nhow should ext2 use this inode?\n4\nosd1\nan OS-dependent ﬁeld\n60\nblock\na set of disk pointers (15 total)\n4\ngeneration\nﬁle version (used by NFS)\n4\nﬁle acl\na new permissions model beyond mode bits\n4\ndir acl\ncalled access control lists\n4\nfaddr\nan unsupported ﬁeld\n12\ni osd2\nanother OS-dependent ﬁeld\nTable 40.1: The ext2 inode\nallocated to it, protection information (such as who owns the ﬁle, as well\nas who can access it), some time information, including when the ﬁle was\ncreated, modiﬁed, or last accessed, as well as information about where its\ndata blocks reside on disk (e.g., pointers of some kind). We refer to all\nsuch information about a ﬁle as metadata; in fact, any information inside\nthe ﬁle system that isn’t pure user data is often referred to as such. An\nexample inode from ext2 [P09] is shown below in Table 40.1.\nOne of the most important decisions in the design of the inode is how\nit refers to where data blocks are. One simple approach would be to\nhave one or more direct pointers (disk addresses) inside the inode; each\npointer refers to one disk block that belongs to the ﬁle. Such an approach\nis limited: for example, if you want to have a ﬁle that is really big (e.g.,\nbigger than the size of a block multiplied by the number of direct point-\ners), you are out of luck.\nThe Multi-Level Index\nTo support bigger ﬁles, ﬁle system designers have had to introduce dif-\nferent structures within inodes. One common idea is to have a special\npointer known as an indirect pointer. Instead of pointing to a block that\ncontains user data, it points to a block that contains more pointers, each\nof which point to user data. Thus, an inode may have some ﬁxed number\nof direct pointers (e.g., 12), and a single indirect pointer. If a ﬁle grows\nlarge enough, an indirect block is allocated (from the data-block region\nof the disk), and the inode’s slot for an indirect pointer is set to point to\nit. Assuming that a block is 4KB and 4-byte disk addresses, that adds\nanother 1024 pointers; the ﬁle can grow to be (12 + 1024) · 4K or 4144KB.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nFILE SYSTEM IMPLEMENTATION\n467\nTIP: CONSIDER EXTENT-BASED APPROACHES\nA different approach is to use extents instead of pointers. An extent is\nsimply a disk pointer plus a length (in blocks); thus, instead of requiring\na pointer for every block of a ﬁle, all one needs is a pointer and a length\nto specify the on-disk location of a ﬁle. Just a single extent is limiting, as\none may have trouble ﬁnding a contiguous chunk of on-disk free space\nwhen allocating a ﬁle. Thus, extent-based ﬁle systems often allow for\nmore than one extent, thus giving more freedom to the ﬁle system during\nﬁle allocation.\nIn comparing the two approaches, pointer-based approaches are the most\nﬂexible but use a large amount of metadata per ﬁle (particularly for large\nﬁles). Extent-based approaches are less ﬂexible but more compact; in par-\nticular, they work well when there is enough free space on the disk and\nﬁles can be laid out contiguously (which is the goal for virtually any ﬁle\nallocation policy anyhow).\nNot surprisingly, in such an approach, you might want to support\neven larger ﬁles. To do so, just add another pointer to the inode: the dou-\nble indirect pointer. This pointer refers to a block that contains pointers\nto indirect blocks, each of which contain pointers to data blocks. A dou-\nble indirect block thus adds the possibility to grow ﬁles with an additional\n1024 · 1024 or 1-million 4KB blocks, in other words supporting ﬁles that\nare over 4GB in size. You may want even more, though, and we bet you\nknow where this is headed: the triple indirect pointer.\nOverall, this imbalanced tree is referred to as the multi-level index ap-\nproach to pointing to ﬁle blocks. Let’s examine an example with twelve\ndirect pointers, as well as both a single and a double indirect block. As-\nsuming a block size of 4 KB, and 4-byte pointers, this structure can accom-\nmodate a ﬁle of just over 4 GB in size (i.e., (12 + 1024 + 10242) × 4 KB).\nCan you ﬁgure out how big of a ﬁle can be handled with the addition of\na triple-indirect block? (hint: pretty big)\nMany ﬁle systems use a multi-level index, including commonly-used\nﬁle systems such as Linux ext2 [P09] and ext3, NetApp’s WAFL, as well as\nthe original UNIX ﬁle system. Other ﬁle systems, including SGI XFS and\nLinux ext4, use extents instead of simple pointers; see the earlier aside for\ndetails on how extent-based schemes work (they are akin to segments in\nthe discussion of virtual memory).\nYou might be wondering: why use an imbalanced tree like this? Why\nnot a different approach? Well, as it turns out, many researchers have\nstudied ﬁle systems and how they are used, and virtually every time they\nﬁnd certain “truths” that hold across the decades. One such ﬁnding is\nthat most ﬁles are small. This imbalanced design reﬂects such a reality; if\nmost ﬁles are indeed small, it makes sense to optimize for this case. Thus,\nwith a small number of direct pointers (12 is a typical number), an inode\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n468\nFILE SYSTEM IMPLEMENTATION\nASIDE: LINKED-BASED APPROACHES\nAnother simpler approach in designing inodes is to use a linked list.\nThus, inside an inode, instead of having multiple pointers, you just need\none, to point to the ﬁrst block of the ﬁle. To handle larger ﬁles, add an-\nother pointer at the end of that data block, and so on, and thus you can\nsupport large ﬁles.\nAs you might have guessed, linked ﬁle allocation performs poorly for\nsome workloads; think about reading the last block of a ﬁle, for example,\nor just doing random access. Thus, to make linked allocation work better,\nsome systems will keep an in-memory table of link information, instead\nof storing the next pointers with the data blocks themselves. The table\nis indexed by the address of a data block D; the content of an entry is\nsimply D’s next pointer, i.e., the address of the next block in a ﬁle which\nfollows D. A null-value could be there too (indicating an end-of-ﬁle), or\nsome other marker to indicate that a particular block is free. Having such\na table of next pointers makes it so that a linked allocation scheme can\neffectively do random ﬁle accesses, simply by ﬁrst scanning through the\n(in memory) table to ﬁnd the desired block, and then accessing (on disk)\nit directly.\nDoes such a table sound familiar? What we have described is the basic\nstructure of what is known as the ﬁle allocation table, or FAT ﬁle system.\nYes, this classic old Windows ﬁle system, before NTFS [C94], is based on a\nsimple linked-based allocation scheme. There are other differences from\na standard UNIX ﬁle system too; for example, there are no inodes per se,\nbut rather directory entries which store metadata about a ﬁle and refer\ndirectly to the ﬁrst block of said ﬁle, which makes creating hard links\nimpossible. See Brouwer [B02] for more of the inelegant details.\ncan directly point to 48 KB of data, needing one (or more) indirect blocks\nfor larger ﬁles. See Agrawal et. al [A+07] for a recent study; Table 40.2\nsummarizes those results.\nOf course, in the space of inode design, many other possibilities ex-\nist; after all, the inode is just a data structure, and any data structure that\nstores the relevant information, and can query it effectively, is sufﬁcient.\nAs ﬁle system software is readily changed, you should be willing to ex-\nplore different designs should workloads or technologies change.\nMost ﬁles are small\nRoughly 2K is the most common size\nAverage ﬁle size is growing\nAlmost 200K is the average\nMost bytes are stored in large ﬁles\nA few big ﬁles use most of the space\nFile systems contains lots of ﬁles\nAlmost 100K on average\nFile systems are roughly half full\nEven as disks grow, ﬁle systems remain ˜50% full\nDirectories are typically small\nMany have few entries; most have 20 or fewer\nTable 40.2: File System Measurement Summary\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nFILE SYSTEM IMPLEMENTATION\n469\n40.4\nDirectory Organization\nIn vsfs (as in many ﬁle systems), directories have a simple organiza-\ntion; a directory basically just contains a list of (entry name, inode num-\nber) pairs. For each ﬁle or directory in a given directory, there is a string\nand a number in the data block(s) of the directory. For each string, there\nmay also be a length (assuming variable-sized names).\nFor example, assume a directory dir (inode number 5) has three ﬁles\nin it (foo, bar, and foobar), and their inode numbers are 12, 13, and 24\nrespectively. The on-disk data for dir might look like this:\ninum | reclen | strlen | name\n5\n4\n2\n.\n2\n4\n3\n..\n12\n4\n4\nfoo\n13\n4\n4\nbar\n24\n8\n7\nfoobar\nIn this example, each entry has an inode number, record length (the\ntotal bytes for the name plus any left over space), string length (the actual\nlength of the name), and ﬁnally the name of the entry. Note that each di-\nrectory has two extra entries, . “dot” and .. “dot-dot”; the dot directory\nis just the current directory (in this example, dir), whereas dot-dot is the\nparent directory (in this case, the root).\nDeleting a ﬁle (e.g., calling unlink()) can leave an empty space in\nthe middle of the directory, and hence there should be some way to mark\nthat as well (e.g., with a reserved inode number such as zero). Such a\ndelete is one reason the record length is used: a new entry may reuse an\nold, bigger entry and thus have extra space within.\nYou might be wondering where exactly directories are stored. Often,\nﬁle systems treat directories as a special type of ﬁle. Thus, a directory has\nan inode, somewhere in the inode table (with the type ﬁeld of the inode\nmarked as “directory” instead of “regular ﬁle”). The directory has data\nblocks pointed to by the inode (and perhaps, indirect blocks); these data\nblocks live in the data block region of our simple ﬁle system. Our on-disk\nstructure thus remains unchanged.\nWe should also note again that this simple linear list of directory en-\ntries is not the only way to store such information. As before, any data\nstructure is possible. For example, XFS [S+96] stores directories in B-tree\nform, making ﬁle create operations (which have to ensure that a ﬁle name\nhas not been used before creating it) faster than systems with simple lists\nthat must be scanned in their entirety.\n40.5\nFree Space Management\nA ﬁle system must track which inodes and data blocks are free, and\nwhich are not, so that when a new ﬁle or directory is allocated, it can ﬁnd\nspace for it. Thus free space management is important for all ﬁle systems.\nIn vsfs, we have two simple bitmaps for this task.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n470\nFILE SYSTEM IMPLEMENTATION\nASIDE: FREE SPACE MANAGEMENT\nThere are many ways to manage free space; bitmaps are just one way.\nSome early ﬁle systems used free lists, where a single pointer in the super\nblock was kept to point to the ﬁrst free block; inside that block the next\nfree pointer was kept, thus forming a list through the free blocks of the\nsystem. When a block was needed, the head block was used and the list\nupdated accordingly.\nModern ﬁle systems use more sophisticated data structures. For example,\nSGI’s XFS [S+96] uses some form of a B-tree to compactly represent which\nchunks of the disk are free. As with any data structure, different time-\nspace trade-offs are possible.\nFor example, when we create a ﬁle, we will have to allocate an inode\nfor that ﬁle. The ﬁle system will thus search through the bitmap for an in-\node that is free, and allocate it to the ﬁle; the ﬁle system will have to mark\nthe inode as used (with a 1) and eventually update the on-disk bitmap\nwith the correct information. A similar set of activities take place when a\ndata block is allocated.\nSome other considerations might also come into play when allocating\ndata blocks for a new ﬁle. For example, some Linux ﬁle systems, such\nas ext2 and ext3, will look for a sequence of blocks (say 8) that are free\nwhen a new ﬁle is created and needs data blocks; by ﬁnding such a se-\nquence of free blocks, and then allocating them to the newly-created ﬁle,\nthe ﬁle system guarantees that a portion of the ﬁle will be on the disk and\ncontiguous, thus improving performance. Such a pre-allocation policy is\nthus a commonly-used heuristic when allocating space for data blocks.\n40.6\nAccess Paths: Reading and Writing\nNow that we have some idea of how ﬁles and directories are stored on\ndisk, we should be able to follow the ﬂow of operation during the activity\nof reading or writing a ﬁle. Understanding what happens on this access\npath is thus the second key in developing an understanding of how a ﬁle\nsystem works; pay attention!\nFor the following examples, let us assume that the ﬁle system has been\nmounted and thus that the superblock is already in memory. Everything\nelse (i.e., inodes, directories) is still on the disk.\nReading A File From Disk\nIn this simple example, let us ﬁrst assume that you want to simply open\na ﬁle (e.g., /foo/bar, read it, and then close it. For this simple example,\nlet’s assume the ﬁle is just 4KB in size (i.e., 1 block).\nWhen you issue an open(\"/foo/bar\", O RDONLY) call, the ﬁle sys-\ntem ﬁrst needs to ﬁnd the inode for the ﬁle bar, to obtain some basic in-\nformation about the ﬁle (permissions information, ﬁle size, etc.). To do so,\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nFILE SYSTEM IMPLEMENTATION\n471\ndata\ninode\nroot\nfoo\nbar\nroot\nfoo\nbar\nbar\nbar\nbitmap bitmap inode inode inode data data data[0] data[1] data[1]\nread\nread\nopen(bar)\nread\nread\nread\nread\nread()\nread\nwrite\nread\nread()\nread\nwrite\nread\nread()\nread\nwrite\nTable 40.3: File Read Timeline (Time Increasing Downward)\nthe ﬁle system must be able to ﬁnd the inode, but all it has right now is\nthe full pathname. The ﬁle system must traverse the pathname and thus\nlocate the desired inode.\nAll traversals begin at the root of the ﬁle system, in the root directory\nwhich is simply called /. Thus, the ﬁrst thing the FS will read from disk\nis the inode of the root directory. But where is this inode? To ﬁnd an\ninode, we must know its i-number. Usually, we ﬁnd the i-number of a ﬁle\nor directory in its parent directory; the root has no parent (by deﬁnition).\nThus, the root inode number must be “well known”; the FS must know\nwhat it is when the ﬁle system is mounted. In most UNIX ﬁle systems,\nthe root inode number is 2. Thus, to begin the process, the FS reads in the\nblock that contains inode number 2 (the ﬁrst inode block).\nOnce the inode is read in, the FS can look inside of it to ﬁnd pointers to\ndata blocks, which contain the contents of the root directory. The FS will\nthus use these on-disk pointers to read through the directory, in this case\nlooking for an entry for foo. By reading in one or more directory data\nblocks, it will ﬁnd the entry for foo; once found, the FS will also have\nfound the inode number of foo (say it is 44) which it will need next.\nThe next step is to recursively traverse the pathname until the desired\ninode is found. In this example, the FS would next read the block contain-\ning the inode of foo and then read in its directory data, ﬁnally ﬁnding the\ninode number of bar. The ﬁnal step of open(), then, is to read its inode\ninto memory; the FS can then do a ﬁnal permissions check, allocate a ﬁle\ndescriptor for this process in the per-process open-ﬁle table, and return it\nto the user.\nOnce open, the program can then issue a read() system call to read\nfrom the ﬁle. The ﬁrst read (at offset 0 unless lseek() has been called)\nwill thus read in the ﬁrst block of the ﬁle, consulting the inode to ﬁnd\nthe location of such a block; it may also update the inode with a new last-\naccessed time. The read will further update the in-memory open ﬁle table\nfor this ﬁle descriptor, updating the ﬁle offset such that the next read will\nread the second ﬁle block, etc.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n472\nFILE SYSTEM IMPLEMENTATION\nASIDE: READS DON’T ACCESS ALLOCATION STRUCTURES\nWe’ve seen many students get confused by allocation structures such\nas bitmaps. In particular, many often think that when you are simply\nreading a ﬁle, and not allocating any new blocks, that the bitmap will still\nbe consulted. This is not true! Allocation structures, such as bitmaps,\nare only accessed when allocation is needed. The inodes, directories, and\nindirect blocks have all the information they need to complete a read re-\nquest; there is no need to make sure a block is allocated when the inode\nalready points to it.\nAt some point, the ﬁle will be closed. There is much less work to be\ndone here; clearly, the ﬁle descriptor should be deallocated, but for now,\nthat is all the FS really needs to do. No disk I/Os take place.\nA depiction of this entire process is found in Figure 40.3 (time increases\ndownward). In the ﬁgure, the open causes numerous reads to take place\nin order to ﬁnally locate the inode of the ﬁle. Afterwards, reading each\nblock requires the ﬁle system to ﬁrst consult the inode, then read the\nblock, and then update the inode’s last-accessed-time ﬁeld with a write.\nSpend some time and try to understand what is going on.\nAlso note that the amount of I/O generated by the open is propor-\ntional to the length of the pathname. For each additional directory in the\npath, we have to read its inode as well as its data. Making this worse\nwould be the presence of large directories; here, we only have to read one\nblock to get the contents of a directory, whereas with a large directory, we\nmight have to read many data blocks to ﬁnd the desired entry. Yes, life\ncan get pretty bad when reading a ﬁle; as you’re about to ﬁnd out, writing\nout a ﬁle (and especially, creating a new one) is even worse.\nWriting to Disk\nWriting to a ﬁle is a similar process. First, the ﬁle must be opened (as\nabove). Then, the application can issue write() calls to update the ﬁle\nwith new contents. Finally, the ﬁle is closed.\nUnlike reading, writing to the ﬁle may also allocate a block (unless\nthe block is being overwritten, for example). When writing out a new\nﬁle, each write not only has to write data to disk but has to ﬁrst decide\nwhich block to allocate to the ﬁle and thus update other structures of the\ndisk accordingly (e.g., the data bitmap). Thus, each write to a ﬁle logically\ngenerates three I/Os: one to read the data bitmap, which is then updated\nto mark the newly-allocated block as used, one to write the bitmap (to\nreﬂect its new state to disk), and one to write the actual block itself.\nThe amount of write trafﬁc is even worse when one considers a sim-\nple and common operation such as ﬁle creation. To create a ﬁle, the ﬁle\nsystem must not only allocate an inode, but also allocate space within\nthe directory containing the new ﬁle. The total amount of I/O trafﬁc to\ndo so is quite high: one read to the inode bitmap (to ﬁnd a free inode),\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nFILE SYSTEM IMPLEMENTATION\n473\ndata\ninode\nroot\nfoo\nbar\nroot\nfoo\nbar\nbar\nbar\nbitmap bitmap inode inode inode data\ndata\ndata[0] data[1] data[1]\nread\nread\nread\nread\ncreate\nread\n(/foo/bar)\nwrite\nwrite\nread\nwrite\nwrite\nread\nread\nwrite()\nwrite\nwrite\nwrite\nread\nread\nwrite()\nwrite\nwrite\nwrite\nread\nread\nwrite()\nwrite\nwrite\nwrite\nTable 40.4: File Creation Timeline (Time Increasing Downward)\none write to the inode bitmap (to mark it allocated), one write to the new\ninode itself (to initialize it), one to the data of the directory (to link the\nhigh-level name of the ﬁle to its inode number), and one read and write\nto the directory inode to update it. If the directory needs to grow to ac-\ncommodate the new entry, additional I/Os (i.e., to the data bitmap, and\nthe new directory block) will be needed too. All that just to create a ﬁle!\nLet’s look at a speciﬁc example, where the ﬁle /foo/bar is created,\nand three blocks are written to it. Figure 40.4 shows what happens during\nthe open() (which creates the ﬁle) and during each of three 4KB writes.\nIn the ﬁgure, reads and writes to the disk are grouped under which\nsystem call caused them to occur, and the rough ordering they might take\nplace in goes from top to bottom of the ﬁgure. You can see how much\nwork it is to create the ﬁle: 10 I/Os in this case, to walk the pathname\nand then ﬁnally create the ﬁle. You can also see that each allocating write\ncosts 5 I/Os: a pair to read and update the inode, another pair to read\nand update the data bitmap, and then ﬁnally the write of the data itself.\nHow can a ﬁle system accomplish any of this with reasonable efﬁciency?\nTHE CRUX: HOW TO REDUCE FILE SYSTEM I/O COSTS\nEven the simplest of operations like opening, reading, or writing a ﬁle\nincurs a huge number of I/O operations, scattered over the disk. What\ncan a ﬁle system do to reduce the high costs of doing so many I/Os?\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n474\nFILE SYSTEM IMPLEMENTATION\n40.7\nCaching and Buffering\nAs the examples above show, reading and writing ﬁles can be expen-\nsive, incurring many I/Os to the (slow) disk. To remedy what would\nclearly be a huge performance problem, most ﬁle systems aggressively\nuse system memory (DRAM) to cache important blocks.\nImagine the open example above: without caching, every ﬁle open\nwould require at least two reads for every level in the directory hierarchy\n(one to read the inode of the directory in question, and at least one to read\nits data). With a long pathname (e.g., /1/2/3/ ... /100/ﬁle.txt), the ﬁle\nsystem would literally perform hundreds of reads just to open the ﬁle!\nEarly ﬁle systems thus introduced a ﬁx-sized cache to hold popular\nblocks. As in our discussion of virtual memory, strategies such as LRU\nand different variants would decide which blocks to keep in cache. This\nﬁx-sized cache would usually be allocated at boot time to be roughly 10%\nof total memory. Modern systems integrate virtual memory pages and ﬁle\nsystem pages into a uniﬁed page cache [S00]. In this way, memory can be\nallocated more ﬂexibly across virtual memory and ﬁle system, depending\non which needs more memory at a given time.\nNow imagine the ﬁle open example with caching. The ﬁrst open may\ngenerate a lot of I/O trafﬁc to read in directory inode and data, but sub-\nsequent ﬁle opens of that same ﬁle (or ﬁles in the same directory) will\nmostly hit in the cache and thus no I/O is needed.\nLet us also consider the effect of caching on writes. Whereas read I/O\ncan be avoided altogether with a sufﬁciently large cache, write trafﬁc has\nto go to disk in order to become persistent. Thus, a cache does not serve\nas the same kind of ﬁlter on write trafﬁc that it does for reads. That said,\nwrite buffering (as it is sometimes called) certainly has a number of per-\nformance beneﬁts. First, by delaying writes, the ﬁle system can batch\nsome updates into a smaller set of I/Os; for example, if an inode bitmap\nis updated when one ﬁle is created and then updated moments later as\nanother ﬁle is created, the ﬁle system saves an I/O by delaying the write\nafter the ﬁrst update. Second, by buffering a number of writes in memory,\nthe system can then schedule the subsequent I/Os and thus increase per-\nformance. Finally, some writes are avoided altogether by delaying them;\nfor example, if an application creates a ﬁle and then deletes it, delaying\nthe writes to reﬂect the ﬁle creation to disk avoids them entirely. In this\ncase, laziness (in writing blocks to disk) is a virtue.\nFor the reasons above, most modern ﬁle systems buffer writes in mem-\nory for anywhere between ﬁve and thirty seconds, representing yet an-\nother trade-off: if the system crashes before the updates have been prop-\nagated to disk, the updates are lost; however, by keeping writes in mem-\nory longer, performance can be improved by batching, scheduling, and\neven avoiding writes.\nSome applications (such as databases) don’t enjoy this trade-off. Thus,\nto avoid unexpected data loss due to write buffering, they simply force\nwrites to disk, by calling fsync(), by using direct I/O interfaces that\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nFILE SYSTEM IMPLEMENTATION\n475\nwork around the cache, or by using the raw disk interface and avoiding\nthe ﬁle system altogether1. While most applications live with the trade-\noffs made by the ﬁle system, there are enough controls in place to get the\nsystem to do what you want it to, should the default not be satisfying.\n40.8\nSummary\nWe have seen the basic machinery required in building a ﬁle system.\nThere needs to be some information about each ﬁle (metadata), usually\nstored in a structure called an inode. Directories are just a speciﬁc type\nof ﬁle that store name→inode-number mappings. And other structures\nare needed too; for example, ﬁle systems often use a structure such as a\nbitmap to track which inodes or data blocks are free or allocated.\nThe terriﬁc aspect of ﬁle system design is its freedom; the ﬁle systems\nwe explore in the coming chapters each take advantage of this freedom\nto optimize some aspect of the ﬁle system. There are also clearly many\npolicy decisions we have left unexplored. For example, when a new ﬁle\nis created, where should it be placed on disk? This policy and others will\nalso be the subject of future chapters. Or will they?\n1Take a database class to learn more about old-school databases and their former insis-\ntence on avoiding the OS and controlling everything themselves.\nBut watch out!\nThose\ndatabase types are always trying to bad mouth the OS. Shame on you, database people. Shame.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "page_number": 494,
      "chapter_number": 49,
      "summary": "This chapter covers segment 49 (pages 494-511). Key topics include blocks, directories, and directory. [SR05] “Advanced Programming in the UNIX Environment”\nW.",
      "keywords": [
        "ﬁle system",
        "ﬁle",
        "File System Implementation",
        "system",
        "File System",
        "inode",
        "simple ﬁle system",
        "System Implementation",
        "UNIX ﬁle system",
        "data",
        "data blocks",
        "block",
        "Read",
        "ﬁles",
        "FILE"
      ],
      "concepts": [
        "blocks",
        "directories",
        "directory",
        "systems",
        "write",
        "writing",
        "disk",
        "read",
        "data",
        "allocated"
      ],
      "similar_chapters": [
        {
          "book": "A Philosophy of Software Design",
          "chapter": 17,
          "title": "Segment 17 (pages 144-151)",
          "relevance_score": 0.68,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 10,
          "title": "Segment 10 (pages 80-87)",
          "relevance_score": 0.64,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 1,
          "title": "Segment 1 (pages 1-20)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 34,
          "title": "Segment 34 (pages 683-702)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 33,
          "title": "Segment 33 (pages 662-682)",
          "relevance_score": 0.6,
          "method": "api"
        }
      ]
    },
    {
      "number": 50,
      "title": "Segment 50 (pages 512-520)",
      "start_page": 512,
      "end_page": 520,
      "detection_method": "topic_boundary",
      "content": "476\nFILE SYSTEM IMPLEMENTATION\nReferences\n[A+07] Nitin Agrawal, William J. Bolosky, John R. Douceur, Jacob R. Lorch\nA Five-Year Study of File-System Metadata\nFAST ’07, pages 31–45, February 2007, San Jose, CA\nAn excellent recent analysis of how ﬁle systems are actually used. Use the bibliography within to follow\nthe trail of ﬁle-system analysis papers back to the early 1980s.\n[B07] “ZFS: The Last Word in File Systems”\nJeff Bonwick and Bill Moore\nAvailable: http://opensolaris.org/os/community/zfs/docs/zfs last.pdf\nOne of the most recent important ﬁle systems, full of features and awesomeness. We should have a\nchapter on it, and perhaps soon will.\n[B02] “The FAT File System”\nAndries Brouwer\nSeptember, 2002\nAvailable: http://www.win.tue.nl/˜aeb/linux/fs/fat/fat.html\nA nice clean description of FAT. The ﬁle system kind, not the bacon kind. Though you have to admit,\nbacon fat probably tastes better.\n[C94] “Inside the Windows NT File System”\nHelen Custer\nMicrosoft Press, 1994\nA short book about NTFS; there are probably ones with more technical details elsewhere.\n[H+88] “Scale and Performance in a Distributed File System”\nJohn H. Howard, Michael L. Kazar, Sherri G. Menees, David A. Nichols, M. Satyanarayanan,\nRobert N. Sidebotham, Michael J. West.\nACM Transactions on Computing Systems (ACM TOCS), page 51-81, Volume 6, Number 1,\nFebruary 1988\nA classic distributed ﬁle system; we’ll be learning more about it later, don’t worry.\n[P09] “The Second Extended File System: Internal Layout”\nDave Poirier, 2009\nAvailable: http://www.nongnu.org/ext2-doc/ext2.html\nSome details on ext2, a very simple Linux ﬁle system based on FFS, the Berkeley Fast File System. We’ll\nbe reading about it in the next chapter.\n[RT74] “The UNIX Time-Sharing System”\nM. Ritchie and K. Thompson\nCACM, Volume 17:7, pages 365-375, 1974\nThe original paper about UNIX. Read it to see the underpinnings of much of modern operating systems.\n[S00] “UBC: An Efﬁcient Uniﬁed I/O and Memory Caching Subsystem for NetBSD”\nChuck Silvers\nFREENIX, 2000\nA nice paper about NetBSD’s integration of ﬁle-system buffer caching and the virtual-memory page\ncache. Many other systems do the same type of thing.\n[S+96] “Scalability in the XFS File System”\nAdan Sweeney, Doug Doucette, Wei Hu, Curtis Anderson,\nMike Nishimoto, Geoff Peck\nUSENIX ’96, January 1996, San Diego, CA\nThe ﬁrst attempt to make scalability of operations, including things like having millions of ﬁles in a\ndirectory, a central focus. A great example of pushing an idea to the extreme. The key idea behind this\nﬁle system: everything is a tree. We should have a chapter on this ﬁle system too.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nFILE SYSTEM IMPLEMENTATION\n477\nHomework\nUse this tool, vsfs.py, to study how ﬁle system state changes as var-\nious operations take place. The ﬁle system begins in an empty state, with\njust a root directory. As the simulation takes place, various operations are\nperformed, thus slowly changing the on-disk state of the ﬁle system. See\nthe README for details.\nQuestions\n1. Run the simulator with some different random seeds (say 17, 18, 19,\n20), and see if you can ﬁgure out which operations must have taken\nplace between each state change.\n2. Now do the same, using different random seeds (say 21, 22, 23,\n24), except run with the -r ﬂag, thus making you guess the state\nchange while being shown the operation. What can you conclude\nabout the inode and data-block allocation algorithms, in terms of\nwhich blocks they prefer to allocate?\n3. Now reduce the number of data blocks in the ﬁle system, to very\nlow numbers (say two), and run the simulator for a hundred or so\nrequests. What types of ﬁles end up in the ﬁle system in this highly-\nconstrained layout? What types of operations would fail?\n4. Now do the same, but with inodes. With very few inodes, what\ntypes of operations can succeed? Which will usually fail? What is\nthe ﬁnal state of the ﬁle system likely to be?\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n41\nLocality and The Fast File System\nWhen the UNIX operating system was ﬁrst introduced, the UNIX wizard\nhimself Ken Thompson wrote the ﬁrst ﬁle system. We will call that the\n“old UNIX ﬁle system”, and it was really simple. Basically, its data struc-\ntures looked like this on the disk:\nS\nInodes\nData\nThe super block (S) contained information about the entire ﬁle system:\nhow big the volume is, how many inodes there are, a pointer to the head\nof a free list of blocks, and so forth. The inode region of the disk contained\nall the inodes for the ﬁle system. Finally, most of the disk was taken up\nby data blocks.\nThe good thing about the old ﬁle system was that it was simple, and\nsupported the basic abstractions the ﬁle system was trying to deliver:\nﬁles and the directory hierarchy. This easy-to-use system was a real step\nforward from the clumsy, record-based storage systems of the past, and\nthe directory hierarchy a true advance over simpler, one-level hierarchies\nprovided by earlier systems.\n41.1\nThe Problem: Poor Performance\nThe problem: performance was terrible. As measured by Kirk McKu-\nsick and his colleagues at Berkeley [MJLF84], performance started off bad\nand got worse over time, to the point where the ﬁle system was delivering\nonly 2% of overall disk bandwidth!\nThe main issue was that the old UNIX ﬁle system treated the disk like it\nwas a random-access memory; data was spread all over the place without\nregard to the fact that the medium holding the data was a disk, and thus\nhad real and expensive positioning costs. For example, the data blocks of\na ﬁle were often very far away from its inode, thus inducing an expensive\nseek whenever one ﬁrst read the inode and then the data blocks of a ﬁle\n(a pretty common operation).\n479\n\n\n480\nLOCALITY AND THE FAST FILE SYSTEM\nWorse, the ﬁle system would end up getting quite fragmented, as the\nfree space was not carefully managed. The free list would end up point-\ning to a bunch of blocks spread across the disk, and as ﬁles got allocated,\nthey would simply take the next free block. The result was that a logi-\ncally contiguous ﬁle would be accessed by going back and forth across\nthe disk, thus reducing performance dramatically.\nFor example, imagine the following data block region, which contains\nfour ﬁles (A, B, C, and D), each of size 2 blocks:\nA1\nA2\nB1\nB2\nC1\nC2\nD1\nD2\nIf B and D are deleted, the resulting layout is:\nA1\nA2\nC1\nC2\nAs you can see, the free space is fragmented into two chunks of two\nblocks, instead of one nice contiguous chunk of four. Let’s say we now\nwish to allocate a ﬁle E, of size four blocks:\nA1\nA2\nE1\nE2\nC1\nC2\nE3\nE4\nYou can see what happens: E gets spread across the disk, and as a\nresult, when accessing E, you don’t get peak (sequential) performance\nfrom the disk. Rather, you ﬁrst read E1 and E2, then seek, then read E3\nand E4. This fragmentation problem happened all the time in the old\nUNIX ﬁle system, and it hurt performance. (A side note: this problem is\nexactly what disk defragmentation tools help with; they will reorganize\non-disk data to place ﬁles contiguously and make free space one or a few\ncontiguous regions, moving data around and then rewriting inodes and\nsuch to reﬂect the changes)\nOne other problem: the original block size was too small (512 bytes).\nThus, transferring data from the disk was inherently inefﬁcient. Smaller\nblocks were good because they minimized internal fragmentation (waste\nwithin the block), but bad for transfer as each block might require a posi-\ntioning overhead to reach it. We can summarize the problem as follows:\nTHE CRUX:\nHOW TO ORGANIZE ON-DISK DATA TO IMPROVE PERFORMANCE\nHow can we organize ﬁle system data structures so as to improve per-\nformance? What types of allocation policies do we need on top of those\ndata structures? How do we make the ﬁle system “disk aware”?\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nLOCALITY AND THE FAST FILE SYSTEM\n481\n41.2\nFFS: Disk Awareness Is The Solution\nA group at Berkeley decided to build a better, faster ﬁle system, which\nthey cleverly called the Fast File System (FFS). The idea was to design\nthe ﬁle system structures and allocation policies to be “disk aware” and\nthus improve performance, which is exactly what they did. FFS thus ush-\nered in a new era of ﬁle system research; by keeping the same interface\nto the ﬁle system (the same APIs, including open(), read(), write(),\nclose(), and other ﬁle system calls) but changing the internal implemen-\ntation, the authors paved the path for new ﬁle system construction, work\nthat continues today. Virtually all modern ﬁle systems adhere to the ex-\nisting interface (and thus preserve compatibility with applications) while\nchanging their internals for performance, reliability, or other reasons.\n41.3\nOrganizing Structure: The Cylinder Group\nThe ﬁrst step was to change the on-disk structures. FFS divides the\ndisk into a bunch of groups known as cylinder groups (some modern ﬁle\nsystems like Linux ext2 and ext3 just call them block groups). We can\nthus imagine a disk with ten cylinder groups:\nG0\nG1\nG2\nG3\nG4\nG5\nG6\nG7\nG8\nG9\nThese groups are the central mechanism that FFS uses to improve per-\nformance; by placing two ﬁles within the same group, FFS can ensure that\naccessing one after the other will not result in long seeks across the disk.\nThus, FFS needs to have the ability to allocate ﬁles and directories\nwithin each of these groups. Each group looks like this:\nS ib db\nInodes\nData\nWe now describe the components of a cylinder group. A copy of the\nsuper block (S) is found in each group for reliability reasons (e.g., if one\ngets corrupted or scratched, you can still mount and access the ﬁle system\nby using one of the others).\nThe inode bitmap (ib) and data bitmap (db) track whether each inode\nor data block is free, respectively. Bitmaps are an excellent way to manage\nfree space in a ﬁle system because it is easy to ﬁnd a large chunk of free\nspace and allocate it to a ﬁle, perhaps avoiding some of the fragmentation\nproblems of the free list in the old ﬁle system.\nFinally, the inode and data block regions are just like in the previous\nvery simple ﬁle system. Most of each cylinder group, as usual, is com-\nprised of data blocks.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n482\nLOCALITY AND THE FAST FILE SYSTEM\nASIDE: FFS FILE CREATION\nAs an example, think about what data structures must be updated when\na ﬁle is created; assume, for this example, that the user creates a new ﬁle\n/foo/bar.txt and that the ﬁle is one block long (4KB). The ﬁle is new,\nand thus needs a new inode; thus, both the inode bitmap and the newly-\nallocated inode will be written to disk. The ﬁle also has data in it and\nthus it too must be allocated; the data bitmap and a data block will thus\n(eventually) be written to disk. Hence, at least four writes to the current\ncylinder group will take place (recall that these writes may be buffered\nin memory for a while before the write takes place). But this is not all!\nIn particular, when creating a new ﬁle, we must also place the ﬁle in the\nﬁle-system hierarchy; thus, the directory must be updated. Speciﬁcally,\nthe parent directory foo must be updated to add the entry for bar.txt;\nthis update may ﬁt in an existing data block of foo or require a new block\nto be allocated (with associated data bitmap). The inode of foo must also\nbe updated, both to reﬂect the new length of the directory as well as to\nupdate time ﬁelds (such as last-modiﬁed-time). Overall, it is a lot of work\njust to create a new ﬁle! Perhaps next time you do so, you should be more\nthankful, or at least surprised that it all works so well.\n41.4\nPolicies: How To Allocate Files and Directories\nWith this group structure in place, FFS now has to decide how to place\nﬁles and directories and associated metadata on disk to improve perfor-\nmance. The basic mantra is simple: keep related stuff together (and its corol-\nlary, keep unrelated stuff far apart).\nThus, to obey the mantra, FFS has to decide what is “related” and\nplace it within the same block group; conversely, unrelated items should\nbe placed into different block groups. To achieve this end, FFS makes use\nof a few simple placement heuristics.\nThe ﬁrst is the placement of directories. FFS employs a simple ap-\nproach: ﬁnd the cylinder group with a low number of allocated directo-\nries (because we want to balance directories across groups) and a high\nnumber of free inodes (because we want to subsequently be able to allo-\ncate a bunch of ﬁles), and put the directory data and inode in that group.\nOf course, other heuristics could be used here (e.g., taking into account\nthe number of free data blocks).\nFor ﬁles, FFS does two things. First, it makes sure (in the general case)\nto allocate the data blocks of a ﬁle in the same group as its inode, thus\npreventing long seeks between inode and data (as in the old ﬁle sys-\ntem). Second, it places all ﬁles that are in the same directory in the cylin-\nder group of the directory they are in. Thus, if a user creates four ﬁles,\n/dir1/1.txt, /dir1/2.txt, /dir1/3.txt, and /dir99/4.txt, FFS\nwould try to place the ﬁrst three near one another (same group) and the\nfourth far away (in some other group).\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nLOCALITY AND THE FAST FILE SYSTEM\n483\n0\n2\n4\n6\n8\n10\n0%\n20%\n40%\n60%\n80%\n100%\nFFS Locality\nPath Difference\nCumulative Frequency\nTrace\nRandom\nFigure 41.1: FFS Locality For SEER Traces\nIt should be noted that these heuristics are not based on extensive\nstudies of ﬁle-system trafﬁc or anything particularly nuanced; rather, they\nare based on good old-fashioned common sense (isn’t that what CS stands\nfor after all?). Files in a directory are often accessed together (imagine\ncompiling a bunch of ﬁles and then linking them into a single executable).\nBecause they are, FFS will often improve performance, making sure that\nseeks between related ﬁles are short.\n41.5\nMeasuring File Locality\nTo understand better whether these heuristics make sense, we decided\nto analyze some traces of ﬁle system access and see if indeed there is\nnamespace locality; for some reason, there doesn’t seem to be a good\nstudy of this topic in the literature.\nSpeciﬁcally, we took the SEER traces [K94] and analyzed how “far\naway” ﬁle accesses were from one another in the directory tree. For ex-\nample, if ﬁle f is opened, and then re-opened next in the trace (before\nany other ﬁles are opened), the distance between these two opens in the\ndirectory tree is zero (as they are the same ﬁle). If a ﬁle f in directory\ndir (i.e., dir/f) is opened, and followed by an open of ﬁle g in the same\ndirectory (i.e., dir/g), the distance between the two ﬁle accesses is one,\nas they share the same directory but are not the same ﬁle. Our distance\nmetric, in other words, measures how far up the directory tree you have\nto travel to ﬁnd the common ancestor of two ﬁles; the closer they are in the\ntree, the lower the metric.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n484\nLOCALITY AND THE FAST FILE SYSTEM\nFigure 41.1 shows the locality observed in the SEER traces over all\nworkstations in the SEER cluster over the entirety of all traces. The graph\nplots the difference metric along the x-axis, and shows the cumulative\npercentage of ﬁle opens that were of that difference along the y-axis.\nSpeciﬁcally, for the SEER traces (marked “Trace” in the graph), you can\nsee that about 7% of ﬁle accesses were to the ﬁle that was opened previ-\nously, and that nearly 40% of ﬁle accesses were to either the same ﬁle or\nto one in the same directory (i.e., a difference of zero or one). Thus, the\nFFS locality assumption seems to make sense (at least for these traces).\nInterestingly, another 25% or so of ﬁle accesses were to ﬁles that had a\ndistance of two. This type of locality occurs when the user has structured\na set of related directories in a multi-level fashion and consistently jumps\nbetween them. For example, if a user has a src directory and builds\nobject ﬁles (.o ﬁles) into a obj directory, and both of these directories\nare sub-directories of a main proj directory, a common access pattern\nwill be proj/src/foo.c followed by proj/obj/foo.o. The distance\nbetween these two accesses is two, as proj is the common ancestor. FFS\ndoes not capture this type of locality in its policies, and thus more seeking\nwill occur between such accesses.\nWe also show what locality would be for a “Random” trace for the\nsake of comparison. We generated the random trace by selecting ﬁles\nfrom within an existing SEER trace in random order, and calculating the\ndistance metric between these randomly-ordered accesses. As you can\nsee, there is less namespace locality in the random traces, as expected.\nHowever, because eventually every ﬁle shares a common ancestor (e.g.,\nthe root), there is some locality eventually, and thus random trace is use-\nful as a comparison point.\n41.6\nThe Large-File Exception\nIn FFS, there is one important exception to the general policy of ﬁle\nplacement, and it arises for large ﬁles. Without a different rule, a large\nﬁle would entirely ﬁll the block group it is ﬁrst placed within (and maybe\nothers). Filling a block group in this manner is undesirable, as it prevents\nsubsequent “related” ﬁles from being placed within this block group, and\nthus may hurt ﬁle-access locality.\nThus, for large ﬁles, FFS does the following. After some number of\nblocks are allocated into the ﬁrst block group (e.g., 12 blocks, or the num-\nber of direct pointers available within an inode), FFS places the next “large”\nchunk of the ﬁle (e.g., those pointed to by the ﬁrst indirect block) in an-\nother block group (perhaps chosen for its low utilization). Then, the next\nchunk of the ﬁle is placed in yet another different block group, and so on.\nLet’s look at some pictures to understand this policy better. Without\nthe large-ﬁle exception, a single large ﬁle would place all of its blocks into\none part of the disk. We use a small example of a ﬁle with 10 blocks to\nillustrate the behavior visually.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "page_number": 512,
      "chapter_number": 50,
      "summary": "This chapter covers segment 50 (pages 512-520). Key topics include blocks, file, and directory. Lorch\nA Five-Year Study of File-System Metadata\nFAST ’07, pages 31–45, February 2007, San Jose, CA\nAn excellent recent analysis of how ﬁle systems are actually used.",
      "keywords": [
        "ﬁle system",
        "Fast File System",
        "ﬁle",
        "FILE SYSTEM",
        "SYSTEM",
        "UNIX ﬁle system",
        "ﬁles",
        "FFS",
        "data",
        "Fast File",
        "FILE SYSTEM IMPLEMENTATION",
        "ﬁle system data",
        "FILE",
        "block",
        "group"
      ],
      "concepts": [
        "blocks",
        "file",
        "directory",
        "directories",
        "performance",
        "performed",
        "locality",
        "disk",
        "operating",
        "operations"
      ],
      "similar_chapters": [
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 10,
          "title": "Segment 10 (pages 80-87)",
          "relevance_score": 0.65,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 33,
          "title": "Segment 33 (pages 662-682)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "Game_Engine_Architecture",
          "chapter": 15,
          "title": "Segment 15 (pages 289-307)",
          "relevance_score": 0.61,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 42,
          "title": "Segment 42 (pages 416-425)",
          "relevance_score": 0.6,
          "method": "api"
        },
        {
          "book": "A Philosophy of Software Design",
          "chapter": 17,
          "title": "Segment 17 (pages 144-151)",
          "relevance_score": 0.59,
          "method": "api"
        }
      ]
    },
    {
      "number": 51,
      "title": "Segment 51 (pages 521-532)",
      "start_page": 521,
      "end_page": 532,
      "detection_method": "topic_boundary",
      "content": "LOCALITY AND THE FAST FILE SYSTEM\n485\nHere is the depiction of FFS without the large-ﬁle exception:\nG0\nG1\nG2\nG3\nG4\nG5\nG6\nG7\nG8\nG9\n0 1 2 3 4\n5 6 7 8 9\nWith the large-ﬁle exception, we might see something more like this, with\nthe ﬁle spread across the disk in chunks:\nG0\nG1\nG2\nG3\nG4\nG5\nG6\nG7\nG8\nG9\n0 1\n2 3\n4 5\n6 7\n8 9\nThe astute reader will note that spreading blocks of a ﬁle across the\ndisk will hurt performance, particularly in the relatively common case\nof sequential ﬁle access (e.g., when a user or application reads chunks 0\nthrough 9 in order). And you are right! It will. We can help this a little,\nby choosing our chunk size carefully.\nSpeciﬁcally, if the chunk size is large enough, we will still spend most\nof our time transferring data from disk and just a relatively little time\nseeking between chunks of the block. This process of reducing an over-\nhead by doing more work per overhead paid is called amortization and\nis a common technique in computer systems.\nLet’s do an example: assume that the average positioning time (i.e.,\nseek and rotation) for a disk is 10 ms. Assume further that the disk trans-\nfers data at 40 MB/s. If our goal was to spend half our time seeking be-\ntween chunks and half our time transferring data (and thus achieve 50%\nof peak disk performance), we would thus need to spend 10 ms transfer-\nring data for every 10 ms positioning. So the question becomes: how big\ndoes a chunk have to be in order to spend 10 ms in transfer? Easy, just\nuse our old friend, math, in particular the dimensional analysis we spoke\nof in the chapter on disks:\n40 \u0018\u0018\nMB\n\b\b\nsec\n· 1024 KB\n1 \u0018\u0018\nMB\n·\n1 \b\b\nsec\n1000 \b\b\nms · 10 \b\b\nms = 409.6 KB\n(41.1)\nBasically, what this equation says is this: if you transfer data at 40\nMB/s, you need to transfer only 409.6 KB every time you seek in order to\nspend half your time seeking and half your time transferring. Similarly,\nyou can compute the size of the chunk you would need to achieve 90%\nof peak bandwidth (turns out it is about 3.69 MB), or even 99% of peak\nbandwidth (40.6 MB!). As you can see, the closer you want to get to peak,\nthe bigger these chunks get (see Figure 41.2 for a plot of these values).\nFFS did not use this type of calculation in order to spread large ﬁles\nacross groups, however. Instead, it took a simple approach, based on the\nstructure of the inode itself. The ﬁrst twelve direct blocks were placed\nin the same group as the inode; each subsequent indirect block, and all\nthe blocks it pointed to, was placed in a different group. With a block\nsize of 4-KB, and 32-bit disk addresses, this strategy implies that every\n1024 blocks of the ﬁle (4 MB) were placed in separate groups, the lone\nexception being the ﬁrst 48-KB of the ﬁle as pointed to by direct pointers.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n486\nLOCALITY AND THE FAST FILE SYSTEM\n0%\n25%\n50%\n75%\n100%\n1K\n32K\n1M\n10M\nThe Challenges of Amortization\nPercent Bandwidth (Desired)\nLog(Chunk Size Needed)\n50%, 409.6K\n90%, 3.69M\nFigure 41.2: Amortization: How Big Do Chunks Have To Be?\nWe should note that the trend in disk drives is that transfer rate im-\nproves fairly rapidly, as disk manufacturers are good at cramming more\nbits into the same surface, but the mechanical aspects of drives related\nto seeks (disk arm speed and the rate of rotation) improve rather slowly\n[P98]. The implication is that over time, mechanical costs become rel-\natively more expensive, and thus, to amortize said costs, you have to\ntransfer more data between seeks.\n41.7\nA Few Other Things About FFS\nFFS introduced a few other innovations too. In particular, the design-\ners were extremely worried about accommodating small ﬁles; as it turned\nout, many ﬁles were 2 KB or so in size back then, and using 4-KB blocks,\nwhile good for transferring data, was not so good for space efﬁciency.\nThis internal fragmentation could thus lead to roughly half the disk be-\ning wasted for a typical ﬁle system.\nThe solution the FFS designers hit upon was simple and solved the\nproblem. They decided to introduce sub-blocks, which were 512-byte lit-\ntle blocks that the ﬁle system could allocate to ﬁles. Thus, if you created a\nsmall ﬁle (say 1 KB in size), it would occupy two sub-blocks and thus not\nwaste an entire 4-KB block. As the ﬁle grew, the ﬁle system will continue\nallocating 512-byte blocks to it until it acquires a full 4-KB of data. At that\npoint, FFS will ﬁnd a 4-KB block, copy the sub-blocks into it, and free the\nsub-blocks for future use.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nLOCALITY AND THE FAST FILE SYSTEM\n487\n0\n11\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\nSpindle\n0\n11\n5\n10\n4\n9\n3\n8\n2\n7\n1\n6\nSpindle\nFigure 41.3: FFS: Standard Versus Parameterized Placement\nYou might observe that this process is inefﬁcient, requiring a lot of ex-\ntra work for the ﬁle system (in particular, a lot of extra I/O to perform the\ncopy). And you’d be right again! Thus, FFS generally avoided this pes-\nsimal behavior by modifying the libc library; the library would buffer\nwrites and then issue them in 4-KB chunks to the ﬁle system, thus avoid-\ning the sub-block specialization entirely in most cases.\nA second neat thing that FFS introduced was a disk layout that was\noptimized for performance. In those times (before SCSI and other more\nmodern device interfaces), disks were much less sophisticated and re-\nquired the host CPU to control their operation in a more hands-on way.\nA problem arose in FFS when a ﬁle was placed on consecutive sectors of\nthe disk, as on the left in Figure 41.3.\nIn particular, the problem arose during sequential reads. FFS would\nﬁrst issue a read to block 0; by the time the read was complete, and FFS\nissued a read to block 1, it was too late: block 1 had rotated under the\nhead and now the read to block 1 would incur a full rotation.\nFFS solved this problem with a different layout, as you can see on the\nright in Figure 41.3. By skipping over every other block (in the example),\nFFS has enough time to request the next block before it went past the\ndisk head. In fact, FFS was smart enough to ﬁgure out for a particular\ndisk how many blocks it should skip in doing layout in order to avoid the\nextra rotations; this technique was called parameterization, as FFS would\nﬁgure out the speciﬁc performance parameters of the disk and use those\nto decide on the exact staggered layout scheme.\nYou might be thinking: this scheme isn’t so great after all. In fact, you\nwill only get 50% of peak bandwidth with this type of layout, because\nyou have to go around each track twice just to read each block once. For-\ntunately, modern disks are much smarter: they internally read the entire\ntrack in and buffer it in an internal disk cache (often called a track buffer\nfor this very reason). Then, on subsequent reads to the track, the disk will\njust return the desired data from its cache. File systems thus no longer\nhave to worry about these incredibly low-level details. Abstraction and\nhigher-level interfaces can be a good thing, when designed properly.\nSome other usability improvements were added as well. FFS was one\nof the ﬁrst ﬁle systems to allow for long ﬁle names, thus enabling more\nexpressive names in the ﬁle system instead of a the traditional ﬁxed-size\napproach (e.g., 8 characters).\nFurther, a new concept was introduced\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n488\nLOCALITY AND THE FAST FILE SYSTEM\nTIP: MAKE THE SYSTEM USABLE\nProbably the most basic lesson from FFS is that not only did it intro-\nduce the conceptually good idea of disk-aware layout, but it also added\na number of features that simply made the system more usable. Long ﬁle\nnames, symbolic links, and a rename operation that worked atomically\nall improved the utility of a system; while hard to write a research pa-\nper about (imagine trying to read a 14-pager about “The Symbolic Link:\nHard Link’s Long Lost Cousin”), such small features made FFS more use-\nful and thus likely increased its chances for adoption. Making a system\nusable is often as or more important than its deep technical innovations.\ncalled a symbolic link. As discussed in a previous chapter, hard links are\nlimited in that they both could not point to directories (for fear of intro-\nducing loops in the ﬁle system hierarchy) and that they can only point to\nﬁles within the same volume (i.e., the inode number must still be mean-\ningful). Symbolic links allow the user to create an “alias” to any other\nﬁle or directory on a system and thus are much more ﬂexible. FFS also\nintroduced an atomic rename() operation for renaming ﬁles. Usabil-\nity improvements, beyond the basic technology, also likely gained FFS a\nstronger user base.\n41.8\nSummary\nThe introduction of FFS was a watershed moment in ﬁle system his-\ntory, as it made clear that the problem of ﬁle management was one of the\nmost interesting issues within an operating system, and showed how one\nmight begin to deal with that most important of devices, the hard disk.\nSince that time, hundreds of new ﬁle systems have developed, but still\ntoday many ﬁle systems take cues from FFS (e.g., Linux ext2 and ext3 are\nobvious intellectual descendants). Certainly all modern systems account\nfor the main lesson of FFS: treat the disk like it’s a disk.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nLOCALITY AND THE FAST FILE SYSTEM\n489\nReferences\n[MJLF84] “A Fast File System for UNIX”\nMarshall K. McKusick, William N. Joy, Sam J. Lefﬂer, Robert S. Fabry\nACM Transactions on Computing Systems.\nAugust, 1984. Volume 2, Number 3.\npages 181-197.\nMcKusick was recently honored with the IEEE Reynold B. Johnson award for his contributions to ﬁle\nsystems, much of which was based on his work building FFS. In his acceptance speech, he discussed the\noriginal FFS software: only 1200 lines of code! Modern versions are a little more complex, e.g., the BSD\nFFS descendant now is in the 50-thousand lines-of-code range.\n[P98] “Hardware Technology Trends and Database Opportunities”\nDavid A. Patterson\nKeynote Lecture at the ACM SIGMOD Conference (SIGMOD ’98)\nJune, 1998\nA great and simple overview of disk technology trends and how they change over time.\n[K94] “The Design of the SEER Predictive Caching System”\nG. H. Kuenning\nMOBICOMM ’94, Santa Cruz, California, December 1994\nAccording to Kuenning, this is the best overview of the SEER project, which led to (among other things)\nthe collection of these traces.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n42\nCrash Consistency: FSCK and Journaling\nAs we’ve seen thus far, the ﬁle system manages a set of data structures to\nimplement the expected abstractions: ﬁles, directories, and all of the other\nmetadata needed to support the basic abstraction that we expect from a\nﬁle system. Unlike most data structures (for example, those found in\nmemory of a running program), ﬁle system data structures must persist,\ni.e., they must survive over the long haul, stored on devices that retain\ndata despite power loss (such as hard disks or ﬂash-based SSDs).\nOne major challenge faced by a ﬁle system is how to update persis-\ntent data structures despite the presence of a power loss or system crash.\nSpeciﬁcally, what happens if, right in the middle of updating on-disk\nstructures, someone trips over the power cord and the machine loses\npower? Or the operating system encounters a bug and crashes? Because\nof power losses and crashes, updating a persistent data structure can be\nquite tricky, and leads to a new and interesting problem in ﬁle system\nimplementation, known as the crash-consistency problem.\nThis problem is quite simple to understand. Imagine you have to up-\ndate two on-disk structures, A and B, in order to complete a particular\noperation. Because the disk only services a single request at a time, one\nof these requests will reach the disk ﬁrst (either A or B). If the system\ncrashes or loses power after one write completes, the on-disk structure\nwill be left in an inconsistent state. And thus, we have a problem that all\nﬁle systems need to solve:\nTHE CRUX: HOW TO UPDATE THE DISK DESPITE CRASHES\nThe system may crash or lose power between any two writes, and\nthus the on-disk state may only partially get updated. After the crash,\nthe system boots and wishes to mount the ﬁle system again (in order to\naccess ﬁles and such). Given that crashes can occur at arbitrary points\nin time, how do we ensure the ﬁle system keeps the on-disk image in a\nreasonable state?\n491\n\n\n492\nCRASH CONSISTENCY: FSCK AND JOURNALING\nIn this chapter, we’ll describe this problem in more detail, and look\nat some methods ﬁle systems have used to overcome it. We’ll begin by\nexamining the approach taken by older ﬁle systems, known as fsck or the\nﬁle system checker. We’ll then turn our attention to another approach,\nknown as journaling (also known as write-ahead logging), a technique\nwhich adds a little bit of overhead to each write but recovers more quickly\nfrom crashes or power losses. We will discuss the basic machinery of\njournaling, including a few different ﬂavors of journaling that Linux ext3\n[T98,PAA05] (a relatively modern journaling ﬁle system) implements.\n42.1\nA Detailed Example\nTo kick off our investigation of journaling, let’s look at an example.\nWe’ll need to use a workload that updates on-disk structures in some\nway. Assume here that the workload is simple: the append of a single\ndata block to an existing ﬁle. The append is accomplished by opening the\nﬁle, calling lseek() to move the ﬁle offset to the end of the ﬁle, and then\nissuing a single 4KB write to the ﬁle before closing it.\nLet’s also assume we are using standard simple ﬁle system structures\non the disk, similar to ﬁle systems we have seen before. This tiny example\nincludes an inode bitmap (with just 8 bits, one per inode), a data bitmap\n(also 8 bits, one per data block), inodes (8 total, numbered 0 to 7, and\nspread across four blocks), and data blocks (8 total, numbered 0 to 7).\nHere is a diagram of this ﬁle system:\nInode\nBmap\nData\nBmap\nInodes\nData Blocks\nI[v1]\nDa\nIf you look at the structures in the picture, you can see that a single inode\nis allocated (inode number 2), which is marked in the inode bitmap, and a\nsingle allocated data block (data block 4), also marked in the data bitmap.\nThe inode is denoted I[v1], as it is the ﬁrst version of this inode; it will\nsoon be updated (due to the workload described above).\nLet’s peek inside this simpliﬁed inode too. Inside of I[v1], we see:\nowner\n: remzi\npermissions : read-only\nsize\n: 1\npointer\n: 4\npointer\n: null\npointer\n: null\npointer\n: null\nIn this simpliﬁed inode, the size of the ﬁle is 1 (it has one block al-\nlocated), the ﬁrst direct pointer points to block 4 (the ﬁrst data block of\nthe ﬁle, Da), and all three other direct pointers are set to null (indicating\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nCRASH CONSISTENCY: FSCK AND JOURNALING\n493\nthat they are not used). Of course, real inodes have many more ﬁelds; see\nprevious chapters for more information.\nWhen we append to the ﬁle, we are adding a new data block to it, and\nthus must update three on-disk structures: the inode (which must point\nto the new block as well as have a bigger size due to the append), the\nnew data block Db, and a new version of the data bitmap (call it B[v2]) to\nindicate that the new data block has been allocated.\nThus, in the memory of the system, we have three blocks which we\nmust write to disk. The updated inode (inode version 2, or I[v2] for short)\nnow looks like this:\nowner\n: remzi\npermissions : read-only\nsize\n: 2\npointer\n: 4\npointer\n: 5\npointer\n: null\npointer\n: null\nThe updated data bitmap (B[v2]) now looks like this: 00001100. Finally,\nthere is the data block (Db), which is just ﬁlled with whatever it is users\nput into ﬁles. Stolen music perhaps?\nWhat we would like is for the ﬁnal on-disk image of the ﬁle system to\nlook like this:\nInode\nBmap\nData\nBmap\nInodes\nData Blocks\nI[v2]\nDa\nDb\nTo achieve this transition, the ﬁle system must perform three sepa-\nrate writes to the disk, one each for the inode (I[v2]), bitmap (B[v2]), and\ndata block (Db). Note that these writes usually don’t happen immedi-\nately when the user issues a write() system call; rather, the dirty in-\node, bitmap, and new data will sit in main memory (in the page cache\nor buffer cache) for some time ﬁrst; then, when the ﬁle system ﬁnally\ndecides to write them to disk (after say 5 seconds or 30 seconds), the ﬁle\nsystem will issue the requisite write requests to the disk. Unfortunately,\na crash may occur and thus interfere with these updates to the disk. In\nparticular, if a crash happens after one or two of these writes have taken\nplace, but not all three, the ﬁle system could be left in a funny state.\nCrash Scenarios\nTo understand the problem better, let’s look at some example crash sce-\nnarios. Imagine only a single write succeeds; there are thus three possible\noutcomes, which we list here:\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n494\nCRASH CONSISTENCY: FSCK AND JOURNALING\n• Just the data block (Db) is written to disk. In this case, the data is\non disk, but there is no inode that points to it and no bitmap that\neven says the block is allocated. Thus, it is as if the write never\noccurred. This case is not a problem at all, from the perspective of\nﬁle-system crash consistency1.\n• Just the updated inode (I[v2]) is written to disk. In this case, the\ninode points to the disk address (5) where Db was about to be writ-\nten, but Db has not yet been written there. Thus, if we trust that\npointer, we will read garbage data from the disk (the old contents\nof disk address 5).\nFurther, we have a new problem, which we call a ﬁle-system incon-\nsistency. The on-disk bitmap is telling us that data block 5 has not\nbeen allocated, but the inode is saying that it has. This disagree-\nment in the ﬁle system data structures is an inconsistency in the\ndata structures of the ﬁle system; to use the ﬁle system, we must\nsomehow resolve this problem (more on that below).\n• Just the updated bitmap (B[v2]) is written to disk. In this case, the\nbitmap indicates that block 5 is allocated, but there is no inode that\npoints to it. Thus the ﬁle system is inconsistent again; if left unre-\nsolved, this write would result in a space leak, as block 5 would\nnever be used by the ﬁle system.\nThere are also three more crash scenarios in this attempt to write three\nblocks to disk. In these cases, two writes succeed and the last one fails:\n• The inode (I[v2]) and bitmap (B[v2]) are written to disk, but not\ndata (Db). In this case, the ﬁle system metadata is completely con-\nsistent: the inode has a pointer to block 5, the bitmap indicates that\n5 is in use, and thus everything looks OK from the perspective of\nthe ﬁle system’s metadata. But there is one problem: 5 has garbage\nin it again.\n• The inode (I[v2]) and the data block (Db) are written, but not the\nbitmap (B[v2]). In this case, we have the inode pointing to the cor-\nrect data on disk, but again have an inconsistency between the in-\node and the old version of the bitmap (B1). Thus, we once again\nneed to resolve the problem before using the ﬁle system.\n• The bitmap (B[v2]) and data block (Db) are written, but not the\ninode (I[v2]). In this case, we again have an inconsistency between\nthe inode and the data bitmap. However, even though the block\nwas written and the bitmap indicates its usage, we have no idea\nwhich ﬁle it belongs to, as no inode points to the ﬁle.\n1However, it might be a problem for the user, who just lost some data!\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nCRASH CONSISTENCY: FSCK AND JOURNALING\n495\nThe Crash Consistency Problem\nHopefully, from these crash scenarios, you can see the many problems\nthat can occur to our on-disk ﬁle system image because of crashes: we can\nhave inconsistency in ﬁle system data structures; we can have space leaks;\nwe can return garbage data to a user; and so forth. What we’d like to do\nideally is move the ﬁle system from one consistent state (e.g., before the\nﬁle got appended to) to another atomically (e.g., after the inode, bitmap,\nand new data block have been written to disk). Unfortunately, we can’t\ndo this easily because the disk only commits one write at a time, and\ncrashes or power loss may occur between these updates. We call this\ngeneral problem the crash-consistency problem (we could also call it the\nconsistent-update problem).\n42.2\nSolution #1: The File System Checker\nEarly ﬁle systems took a simple approach to crash consistency. Basi-\ncally, they decided to let inconsistencies happen and then ﬁx them later\n(when rebooting). A classic example of this lazy approach is found in a\ntool that does this: fsck2. fsck is a UNIX tool for ﬁnding such inconsis-\ntencies and repairing them [M86]; similar tools to check and repair a disk\npartition exist on different systems. Note that such an approach can’t ﬁx\nall problems; consider, for example, the case above where the ﬁle system\nlooks consistent but the inode points to garbage data. The only real goal\nis to make sure the ﬁle system metadata is internally consistent.\nThe tool fsck operates in a number of phases, as summarized in\nMcKusick and Kowalski’s paper [MK96]. It is run before the ﬁle system\nis mounted and made available (fsck assumes that no other ﬁle-system\nactivity is on-going while it runs); once ﬁnished, the on-disk ﬁle system\nshould be consistent and thus can be made accessible to users.\nHere is a basic summary of what fsck does:\n• Superblock: fsck ﬁrst checks if the superblock looks reasonable,\nmostly doing sanity checks such as making sure the ﬁle system size\nis greater than the number of blocks allocated. Usually the goal of\nthese sanity checks is to ﬁnd a suspect (corrupt) superblock; in this\ncase, the system (or administrator) may decide to use an alternate\ncopy of the superblock.\n• Free blocks: Next, fsck scans the inodes, indirect blocks, double\nindirect blocks, etc., to build an understanding of which blocks are\ncurrently allocated within the ﬁle system. It uses this knowledge\nto produce a correct version of the allocation bitmaps; thus, if there\nis any inconsistency between bitmaps and inodes, it is resolved by\ntrusting the information within the inodes. The same type of check\nis performed for all the inodes, making sure that all inodes that look\nlike they are in use are marked as such in the inode bitmaps.\n2Pronounced either “eff-ess-see-kay”, “eff-ess-check”, or, if you don’t like the tool, “eff-\nsuck”. Yes, serious professional people use this term.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n496\nCRASH CONSISTENCY: FSCK AND JOURNALING\n• Inode state: Each inode is checked for corruption or other prob-\nlems. For example, fsck makes sure that each allocated inode has\na valid type ﬁeld (e.g., regular ﬁle, directory, symbolic link, etc.). If\nthere are problems with the inode ﬁelds that are not easily ﬁxed, the\ninode is considered suspect and cleared by fsck; the inode bitmap\nis correspondingly updated.\n• Inode links: fsck also veriﬁes the link count of each allocated in-\node. As you may recall, the link count indicates the number of dif-\nferent directories that contain a reference (i.e., a link) to this par-\nticular ﬁle. To verify the link count, fsck scans through the en-\ntire directory tree, starting at the root directory, and builds its own\nlink counts for every ﬁle and directory in the ﬁle system. If there\nis a mismatch between the newly-calculated count and that found\nwithin an inode, corrective action must be taken, usually by ﬁxing\nthe count within the inode. If an allocated inode is discovered but\nno directory refers to it, it is moved to the lost+found directory.\n• Duplicates: fsck also checks for duplicate pointers, i.e., cases where\ntwo different inodes refer to the same block. If one inode is obvi-\nously bad, it may be cleared. Alternately, the pointed-to block could\nbe copied, thus giving each inode its own copy as desired.\n• Bad blocks: A check for bad block pointers is also performed while\nscanning through the list of all pointers. A pointer is considered\n“bad” if it obviously points to something outside its valid range,\ne.g., it has an address that refers to a block greater than the parti-\ntion size. In this case, fsck can’t do anything too intelligent; it just\nremoves (clears) the pointer from the inode or indirect block.\n• Directory checks: fsck does not understand the contents of user\nﬁles; however, directories hold speciﬁcally formatted information\ncreated by the ﬁle system itself. Thus, fsck performs additional\nintegrity checks on the contents of each directory, making sure that\n“.” and “..” are the ﬁrst entries, that each inode referred to in a\ndirectory entry is allocated, and ensuring that no directory is linked\nto more than once in the entire hierarchy.\nAs you can see, building a working fsck requires intricate knowledge\nof the ﬁle system; making sure such a piece of code works correctly in all\ncases can be challenging [G+08]. However, fsck (and similar approaches)\nhave a bigger and perhaps more fundamental problem: they are too slow.\nWith a very large disk volume, scanning the entire disk to ﬁnd all the\nallocated blocks and read the entire directory tree may take many minutes\nor hours. Performance of fsck, as disks grew in capacity and RAIDs\ngrew in popularity, became prohibitive (despite recent advances [M+13]).\nAt a higher level, the basic premise of fsck seems just a tad irra-\ntional. Consider our example above, where just three blocks are written\nto the disk; it is incredibly expensive to scan the entire disk to ﬁx prob-\nlems that occurred during an update of just three blocks. This situation is\nakin to dropping your keys on the ﬂoor in your bedroom, and then com-\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "page_number": 521,
      "chapter_number": 51,
      "summary": "Speciﬁcally, if the chunk size is large enough, we will still spend most\nof our time transferring data from disk and just a relatively little time\nseeking between chunks of the block Key topics include disk, blocks, and directories.",
      "keywords": [
        "ﬁle system",
        "ﬁle",
        "SYSTEM",
        "ﬁle system data",
        "data block",
        "inode",
        "data",
        "FAST FILE SYSTEM",
        "disk",
        "FFS",
        "block",
        "FILE SYSTEM",
        "system data structures",
        "on-disk ﬁle system",
        "FSCK"
      ],
      "concepts": [
        "disk",
        "blocks",
        "directories",
        "directory",
        "systems",
        "problem",
        "crash",
        "crashes",
        "writes",
        "bitmap"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 31,
          "title": "Segment 31 (pages 620-638)",
          "relevance_score": 0.63,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 33,
          "title": "Segment 33 (pages 662-682)",
          "relevance_score": 0.56,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 31,
          "title": "Segment 31 (pages 291-298)",
          "relevance_score": 0.55,
          "method": "api"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 2,
          "title": "Segment 2 (pages 9-16)",
          "relevance_score": 0.53,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 15,
          "title": "Segment 15 (pages 135-143)",
          "relevance_score": 0.53,
          "method": "api"
        }
      ]
    },
    {
      "number": 52,
      "title": "Segment 52 (pages 533-546)",
      "start_page": 533,
      "end_page": 546,
      "detection_method": "topic_boundary",
      "content": "CRASH CONSISTENCY: FSCK AND JOURNALING\n497\nmencing a search-the-entire-house-for-keys recovery algorithm, starting in\nthe basement and working your way through every room. It works but is\nwasteful. Thus, as disks (and RAIDs) grew, researchers and practitioners\nstarted to look for other solutions.\n42.3\nSolution #2: Journaling (or Write-Ahead Logging)\nProbably the most popular solution to the consistent update problem\nis to steal an idea from the world of database management systems. That\nidea, known as write-ahead logging, was invented to address exactly this\ntype of problem. In ﬁle systems, we usually call write-ahead logging jour-\nnaling for historical reasons. The ﬁrst ﬁle system to do this was Cedar\n[H87], though many modern ﬁle systems use the idea, including Linux\next3 and ext4, reiserfs, IBM’s JFS, SGI’s XFS, and Windows NTFS.\nThe basic idea is as follows. When updating the disk, before over-\nwriting the structures in place, ﬁrst write down a little note (somewhere\nelse on the disk, in a well-known location) describing what you are about\nto do. Writing this note is the “write ahead” part, and we write it to a\nstructure that we organize as a “log”; hence, write-ahead logging.\nBy writing the note to disk, you are guaranteeing that if a crash takes\nplaces during the update (overwrite) of the structures you are updating,\nyou can go back and look at the note you made and try again; thus, you\nwill know exactly what to ﬁx (and how to ﬁx it) after a crash, instead\nof having to scan the entire disk. By design, journaling thus adds a bit\nof work during updates to greatly reduce the amount of work required\nduring recovery.\nWe’ll now describe how Linux ext3, a popular journaling ﬁle system,\nincorporates journaling into the ﬁle system. Most of the on-disk struc-\ntures are identical to Linux ext2, e.g., the disk is divided into block groups,\nand each block group has an inode and data bitmap as well as inodes and\ndata blocks. The new key structure is the journal itself, which occupies\nsome small amount of space within the partition or on another device.\nThus, an ext2 ﬁle system (without journaling) looks like this:\nSuper\nGroup 0\nGroup 1\n. . .\nGroup N\nAssuming the journal is placed within the same ﬁle system image\n(though sometimes it is placed on a separate device, or as a ﬁle within\nthe ﬁle system), an ext3 ﬁle system with a journal looks like this:\nSuper\nJournal\nGroup 0\nGroup 1\n. . .\nGroup N\nThe real difference is just the presence of the journal, and of course,\nhow it is used.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n498\nCRASH CONSISTENCY: FSCK AND JOURNALING\nData Journaling\nLet’s look at a simple example to understand how data journaling works.\nData journaling is available as a mode with the Linux ext3 ﬁle system,\nfrom which much of this discussion is based.\nSay we have our canonical update again, where we wish to write the\n‘inode (I[v2]), bitmap (B[v2]), and data block (Db) to disk again. Before\nwriting them to their ﬁnal disk locations, we are now ﬁrst going to write\nthem to the log (a.k.a. journal). This is what this will look like in the log:\nJournal\nTxB\nI[v2]\nB[v2]\nDb\nTxE\nYou can see we have written ﬁve blocks here. The transaction begin\n(TxB) tells us about this update, including information about the pend-\ning update to the ﬁle system (e.g., the ﬁnal addresses of the blocks I[v2],\nB[v2], and Db), as well as some kind of transaction identiﬁer (TID). The\nmiddle three blocks just contain the exact contents of the blocks them-\nselves; this is known as physical logging as we are putting the exact\nphysical contents of the update in the journal (an alternate idea, logi-\ncal logging, puts a more compact logical representation of the update in\nthe journal, e.g., “this update wishes to append data block Db to ﬁle X”,\nwhich is a little more complex but can save space in the log and perhaps\nimprove performance). The ﬁnal block (TxE) is a marker of the end of this\ntransaction, and will also contain the TID.\nOnce this transaction is safely on disk, we are ready to overwrite the\nold structures in the ﬁle system; this process is called checkpointing.\nThus, to checkpoint the ﬁle system (i.e., bring it up to date with the pend-\ning update in the journal), we issue the writes I[v2], B[v2], and Db to\ntheir disk locations as seen above; if these writes complete successfully,\nwe have successfully checkpointed the the ﬁle system and are basically\ndone. Thus, our initial sequence of operations:\n1. Journal write: Write the transaction, including a transaction-begin\nblock, all pending data and metadata updates, and a transaction-\nend block, to the log; wait for these writes to complete.\n2. Checkpoint: Write the pending metadata and data updates to their\nﬁnal locations in the ﬁle system.\nIn our example, we would write TxB, I[v2], B[v2], Db, and TxE to the\njournal ﬁrst. When these writes complete, we would complete the update\nby checkpointing I[v2], B[v2], and Db, to their ﬁnal locations on disk.\nThings get a little trickier when a crash occurs during the writes to\nthe journal. Here, we are trying to write the set of blocks in the transac-\ntion (e.g., TxB, I[v2], B[v2], Db, TxE) to disk. One simple way to do this\nwould be to issue each one at a time, waiting for each to complete, and\nthen issuing the next. However, this is slow. Ideally, we’d like to issue\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nCRASH CONSISTENCY: FSCK AND JOURNALING\n499\nASIDE: FORCING WRITES TO DISK\nTo enforce ordering between two disk writes, modern ﬁle systems have\nto take a few extra precautions. In olden times, forcing ordering between\ntwo writes, A and B, was easy: just issue the write of A to the disk, wait\nfor the disk to interrupt the OS when the write is complete, and then issue\nthe write of B.\nThings got slightly more complex due to the increased use of write caches\nwithin disks. With write buffering enabled (sometimes called immediate\nreporting), a disk will inform the OS the write is complete when it simply\nhas been placed in the disk’s memory cache, and has not yet reached\ndisk. If the OS then issues a subsequent write, it is not guaranteed to\nreach the disk after previous writes; thus ordering between writes is not\npreserved. One solution is to disable write buffering. However, more\nmodern systems take extra precautions and issue explicit write barriers;\nsuch a barrier, when it completes, guarantees that all writes issued before\nthe barrier will reach disk before any writes issued after the barrier.\nAll of this machinery requires a great deal of trust in the correct oper-\nation of the disk. Unfortunately, recent research shows that some disk\nmanufacturers, in an effort to deliver “higher performing” disks, explic-\nitly ignore write-barrier requests, thus making the disks seemingly run\nfaster but at the risk of incorrect operation [C+13, R+11]. As Kahan said,\nthe fast almost always beats out the slow, even if the fast is wrong.\nall ﬁve block writes at once, as this would turn ﬁve writes into a single\nsequential write and thus be faster. However, this is unsafe, for the fol-\nlowing reason: given such a big write, the disk internally may perform\nscheduling and complete small pieces of the big write in any order. Thus,\nthe disk internally may (1) write TxB, I[v2], B[v2], and TxE and only later\n(2) write Db. Unfortunately, if the disk loses power between (1) and (2),\nthis is what ends up on disk:\nJournal\nTxB\nid=1\nI[v2]\nB[v2]\n??\nTxE\nid=1\nWhy is this a problem? Well, the transaction looks like a valid trans-\naction (it has a begin and an end with matching sequence numbers). Fur-\nther, the ﬁle system can’t look at that fourth block and know it is wrong;\nafter all, it is arbitrary user data. Thus, if the system now reboots and\nruns recovery, it will replay this transaction, and ignorantly copy the con-\ntents of the garbage block ’??’ to the location where Db is supposed to\nlive. This is bad for arbitrary user data in a ﬁle; it is much worse if it hap-\npens to a critical piece of ﬁle system, such as the superblock, which could\nrender the ﬁle system unmountable.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n500\nCRASH CONSISTENCY: FSCK AND JOURNALING\nASIDE: OPTIMIZING LOG WRITES\nYou may have noticed a particular inefﬁciency of writing to the log.\nNamely, the ﬁle system ﬁrst has to write out the transaction-begin block\nand contents of the transaction; only after these writes complete can the\nﬁle system send the transaction-end block to disk. The performance im-\npact is clear, if you think about how a disk works: usually an extra rota-\ntion is incurred (think about why).\nOne of our former graduate students, Vijayan Prabhakaran, had a simple\nidea to ﬁx this problem [P+05]. When writing a transaction to the journal,\ninclude a checksum of the contents of the journal in the begin and end\nblocks. Doing so enables the ﬁle system to write the entire transaction at\nonce, without incurring a wait; if, during recovery, the ﬁle system sees\na mismatch in the computed checksum versus the stored checksum in\nthe transaction, it can conclude that a crash occurred during the write\nof the transaction and thus discard the ﬁle-system update. Thus, with a\nsmall tweak in the write protocol and recovery system, a ﬁle system can\nachieve faster common-case performance; on top of that, the system is\nslightly more reliable, as any reads from the journal are now protected by\na checksum.\nThis simple ﬁx was attractive enough to gain the notice of Linux ﬁle sys-\ntem developers, who then incorporated it into the next generation Linux\nﬁle system, called (you guessed it!) Linux ext4. It now ships on mil-\nlions of machines worldwide, including the Android handheld platform.\nThus, every time you write to disk on many Linux-based systems, a little\ncode developed at Wisconsin makes your system a little faster and more\nreliable.\nTo avoid this problem, the ﬁle system issues the transactional write in\ntwo steps. First, it writes all blocks except the TxE block to the journal,\nissuing these writes all at once. When these writes complete, the journal\nwill look something like this (assuming our append workload again):\nJournal\nTxB\nid=1\nI[v2]\nB[v2]\nDb\nWhen those writes complete, the ﬁle system issues the write of the TxE\nblock, thus leaving the journal in this ﬁnal, safe state:\nJournal\nTxB\nid=1\nI[v2]\nB[v2]\nDb\nTxE\nid=1\nAn important aspect of this process is the atomicity guarantee pro-\nvided by the disk. It turns out that the disk guarantees that any 512-byte\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nCRASH CONSISTENCY: FSCK AND JOURNALING\n501\nwrite will either happen or not (and never be half-written); thus, to make\nsure the write of TxE is atomic, one should make it a single 512-byte block.\nThus, our current protocol to update the ﬁle system, with each of its three\nphases labeled:\n1. Journal write: Write the contents of the transaction (including TxB,\nmetadata, and data) to the log; wait for these writes to complete.\n2. Journal commit: Write the transaction commit block (containing\nTxE) to the log; wait for write to complete; transaction is said to be\ncommitted.\n3. Checkpoint: Write the contents of the update (metadata and data)\nto their ﬁnal on-disk locations.\nRecovery\nLet’s now understand how a ﬁle system can use the contents of the jour-\nnal to recover from a crash. A crash may happen at any time during this\nsequence of updates. If the crash happens before the transaction is writ-\nten safely to the log (i.e., before Step 2 above completes), then our job\nis easy: the pending update is simply skipped. If the crash happens af-\nter the transaction has committed to the log, but before the checkpoint is\ncomplete, the ﬁle system can recover the update as follows. When the\nsystem boots, the ﬁle system recovery process will scan the log and look\nfor transactions that have committed to the disk; these transactions are\nthus replayed (in order), with the ﬁle system again attempting to write\nout the blocks in the transaction to their ﬁnal on-disk locations. This form\nof logging is one of the simplest forms there is, and is called redo logging.\nBy recovering the committed transactions in the journal, the ﬁle system\nensures that the on-disk structures are consistent, and thus can proceed\nby mounting the ﬁle system and readying itself for new requests.\nNote that it is ﬁne for a crash to happen at any point during check-\npointing, even after some of the updates to the ﬁnal locations of the blocks\nhave completed. In the worst case, some of these updates are simply per-\nformed again during recovery. Because recovery is a rare operation (only\ntaking place after an unexpected system crash), a few redundant writes\nare nothing to worry about3.\nBatching Log Updates\nYou might have noticed that the basic protocol could add a lot of extra\ndisk trafﬁc. For example, imagine we create two ﬁles in a row, called\nfile1 and file2, in the same directory. To create one ﬁle, one has to\nupdate a number of on-disk structures, minimally including: the inode\nbitmap (to allocated a new inode), the newly-created inode of the ﬁle, the\n3Unless you worry about everything, in which case we can’t help you. Stop worrying so\nmuch, it is unhealthy! But now you’re probably worried about over-worrying.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n502\nCRASH CONSISTENCY: FSCK AND JOURNALING\ndata block of the parent directory containing the new directory entry, as\nwell as the parent directory inode (which now has a new modiﬁcation\ntime). With journaling, we logically commit all of this information to\nthe journal for each of our two ﬁle creations; because the ﬁles are in the\nsame directory, and let’s assume even have inodes within the same inode\nblock, this means that if we’re not careful, we’ll end up writing these same\nblocks over and over.\nTo remedy this problem, some ﬁle systems do not commit each update\nto disk one at a time (e.g., Linux ext3); rather, one can buffer all updates\ninto a global transaction. In our example above, when the two ﬁles are\ncreated, the ﬁle system just marks the in-memory inode bitmap, inodes\nof the ﬁles, directory data, and directory inode as dirty, and adds them to\nthe list of blocks that form the current transaction. When it is ﬁnally time\nto write these blocks to disk (say, after a timeout of 5 seconds), this single\nglobal transaction is committed containing all of the updates described\nabove. Thus, by buffering updates, a ﬁle system can avoid excessive write\ntrafﬁc to disk in many cases.\nMaking The Log Finite\nWe thus have arrived at a basic protocol for updating ﬁle-system on-disk\nstructures. The ﬁle system buffers updates in memory for some time;\nwhen it is ﬁnally time to write to disk, the ﬁle system ﬁrst carefully writes\nout the details of the transaction to the journal (a.k.a. write-ahead log);\nafter the transaction is complete, the ﬁle system checkpoints those blocks\nto their ﬁnal locations on disk.\nHowever, the log is of a ﬁnite size. If we keep adding transactions to\nit (as in this ﬁgure), it will soon ﬁll. What do you think happens then?\nJournal\nTx1\nTx2\nTx3\nTx4\nTx5\n...\nTwo problems arise when the log becomes full. The ﬁrst is simpler,\nbut less critical: the larger the log, the longer recovery will take, as the\nrecovery process must replay all the transactions within the log (in order)\nto recover. The second is more of an issue: when the log is full (or nearly\nfull), no further transactions can be committed to the disk, thus making\nthe ﬁle system “less than useful” (i.e., useless).\nTo address these problems, journaling ﬁle systems treat the log as a\ncircular data structure, re-using it over and over; this is why the journal is\nsometimes referred to as a circular log. To do so, the ﬁle system must take\naction some time after a checkpoint. Speciﬁcally, once a transaction has\nbeen checkpointed, the ﬁle system should free the space it was occupying\nwithin the journal, allowing the log space to be reused. There are many\nways to achieve this end; for example, you could simply mark the oldest\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nCRASH CONSISTENCY: FSCK AND JOURNALING\n503\nand newest transactions in the log in a journal superblock; all other space\nis free. Here is a graphical depiction of such a mechanism:\nJournal\nJournal\nSuper\nTx1\nTx2\nTx3\nTx4\nTx5\n...\nIn the journal superblock (not to be confused with the main ﬁle system\nsuperblock), the journaling system records enough information to know\nwhich transactions have not yet been checkpointed, and thus reduces re-\ncovery time as well as enables re-use of the log in a circular fashion. And\nthus we add another step to our basic protocol:\n1. Journal write: Write the contents of the transaction (containing TxB\nand the contents of the update) to the log; wait for these writes to\ncomplete.\n2. Journal commit: Write the transaction commit block (containing\nTxE) to the log; wait for the write to complete; the transaction is\nnow committed.\n3. Checkpoint: Write the contents of the update to their ﬁnal locations\nwithin the ﬁle system.\n4. Free: Some time later, mark the transaction free in the journal by\nupdating the journal superblock.\nThus we have our ﬁnal data journaling protocol. But there is still a\nproblem: we are writing each data block to the disk twice, which is a\nheavy cost to pay, especially for something as rare as a system crash. Can\nyou ﬁgure out a way to retain consistency without writing data twice?\nMetadata Journaling\nAlthough recovery is now fast (scanning the journal and replaying a few\ntransactions as opposed to scanning the entire disk), normal operation\nof the ﬁle system is slower than we might desire. In particular, for each\nwrite to disk, we are now also writing to the journal ﬁrst, thus doubling\nwrite trafﬁc; this doubling is especially painful during sequential write\nworkloads, which now will proceed at half the peak write bandwidth of\nthe drive. Further, between writes to the journal and writes to the main\nﬁle system, there is a costly seek, which adds noticeable overhead for\nsome workloads.\nBecause of the high cost of writing every data block to disk twice, peo-\nple have tried a few different things in order to speed up performance.\nFor example, the mode of journaling we described above is often called\ndata journaling (as in Linux ext3), as it journals all user data (in addition\nto the metadata of the ﬁle system). A simpler (and more common) form\nof journaling is sometimes called ordered journaling (or just metadata\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n504\nCRASH CONSISTENCY: FSCK AND JOURNALING\njournaling), and it is nearly the same, except that user data is not writ-\nten to the journal. Thus, when performing the same update as above, the\nfollowing information would be written to the journal:\nJournal\nTxB\nI[v2]\nB[v2]\nTxE\nThe data block Db, previously written to the log, would instead be\nwritten to the ﬁle system proper, avoiding the extra write; given that most\nI/O trafﬁc to the disk is data, not writing data twice substantially reduces\nthe I/O load of journaling. The modiﬁcation does raise an interesting\nquestion, though: when should we write data blocks to disk?\nLet’s again consider our example append of a ﬁle to understand the\nproblem better. The update consists of three blocks: I[v2], B[v2], and\nDb. The ﬁrst two are both metadata and will be logged and then check-\npointed; the latter will only be written once to the ﬁle system. When\nshould we write Db to disk? Does it matter?\nAs it turns out, the ordering of the data write does matter for metadata-\nonly journaling. For example, what if we write Db to disk after the trans-\naction (containing I[v2] and B[v2]) completes? Unfortunately, this ap-\nproach has a problem: the ﬁle system is consistent but I[v2] may end up\npointing to garbage data. Speciﬁcally, consider the case where I[v2] and\nB[v2] are written but Db did not make it to disk. The ﬁle system will then\ntry to recover. Because Db is not in the log, the ﬁle system will replay\nwrites to I[v2] and B[v2], and produce a consistent ﬁle system (from the\nperspective of ﬁle-system metadata). However, I[v2] will be pointing to\ngarbage data, i.e., at whatever was in the the slot where Db was headed.\nTo ensure this situation does not arise, some ﬁle systems (e.g., Linux\next3) write data blocks (of regular ﬁles) to the disk ﬁrst, before related\nmetadata is written to disk. Speciﬁcally, the protocol is as follows:\n1. Data write: Write data to ﬁnal location; wait for completion\n(the wait is optional; see below for details).\n2. Journal metadata write: Write the begin block and metadata to the\nlog; wait for writes to complete.\n3. Journal commit: Write the transaction commit block (containing\nTxE) to the log; wait for the write to complete; the transaction (in-\ncluding data) is now committed.\n4. Checkpoint metadata: Write the contents of the metadata update\nto their ﬁnal locations within the ﬁle system.\n5. Free: Later, mark the transaction free in journal superblock.\nBy forcing the data write ﬁrst, a ﬁle system can guarantee that a pointer\nwill never point to garbage. Indeed, this rule of “write the pointed to ob-\nject before the object with the pointer to it” is at the core of crash consis-\ntency, and is exploited even further by other crash consistency schemes\n[GP94] (see below for details).\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nCRASH CONSISTENCY: FSCK AND JOURNALING\n505\nIn most systems, metadata journaling (akin to ordered journaling of\next3) is more popular than full data journaling. For example, Windows\nNTFS and SGI’s XFS both use non-ordered metadata journaling. Linux\next3 gives you the option of choosing either data, ordered, or unordered\nmodes (in unordered mode, data can be written at any time). All of these\nmodes keep metadata consistent; they vary in their semantics for data.\nFinally, note that forcing the data write to complete (Step 1) before\nissuing writes to the journal (Step 2) is not required for correctness, as\nindicated in the protocol above. Speciﬁcally, it would be ﬁne to issue data\nwrites as well as the transaction-begin block and metadata to the journal;\nthe only real requirement is that Steps 1 and 2 complete before the issuing\nof the journal commit block (Step 3).\nTricky Case: Block Reuse\nThere are some interesting corner cases that make journaling more tricky,\nand thus are worth discussing. A number of them revolve around block\nreuse; as Stephen Tweedie (one of the main forces behind ext3) said:\n“What’s the hideous part of the entire system? ... It’s deleting ﬁles.\nEverything to do with delete is hairy. Everything to do with delete...\nyou have nightmares around what happens if blocks get deleted and\nthen reallocated.” [T00]\nThe particular example Tweedie gives is as follows. Suppose you are\nusing some form of metadata journaling (and thus data blocks for ﬁles\nare not journaled). Let’s say you have a directory called foo. The user\nadds an entry to foo (say by creating a ﬁle), and thus the contents of\nfoo (because directories are considered metadata) are written to the log;\nassume the location of the foo directory data is block 1000. The log thus\ncontains something like this:\nJournal\nTxB\nid=1\nI[foo]\nptr:1000\nD[foo]\n[final addr:1000]\nTxE\nid=1\nAt this point, the user deletes everything in the directory as well as the\ndirectory itself, freeing up block 1000 for reuse. Finally, the user creates a\nnew ﬁle (say foobar), which ends up reusing the same block (1000) that\nused to belong to foo. The inode of foobar is committed to disk, as is\nits data; note, however, because metadata journaling is in use, only the\ninode of foobar is committed to the journal; the newly-written data in\nblock 1000 in the ﬁle foobar is not journaled.\nJournal\nTxB\nid=1\nI[foo]\nptr:1000\nD[foo]\n[final addr:1000]\nTxE\nid=1\nTxB\nid=2\nI[foobar]\nptr:1000\nTxE\nid=2\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n506\nCRASH CONSISTENCY: FSCK AND JOURNALING\nJournal\nFile System\nTxB\nContents\nTxE\nMetadata\nData\n(metadata)\n(data)\nissue\nissue\nissue\ncomplete\ncomplete\ncomplete\nissue\ncomplete\nissue\nissue\ncomplete\ncomplete\nTable 42.1: Data Journaling Timeline\nNow assume a crash occurs and all of this information is still in the\nlog. During replay, the recovery process simply replays everything in\nthe log, including the write of directory data in block 1000; the replay\nthus overwrites the user data of current ﬁle foobar with old directory\ncontents! Clearly this is not a correct recovery action, and certainly it will\nbe a surprise to the user when reading the ﬁle foobar.\nThere are a number of solutions to this problem. One could, for ex-\nample, never reuse blocks until the delete of said blocks is checkpointed\nout of the journal. What Linux ext3 does instead is to add a new type\nof record to the journal, known as a revoke record. In the case above,\ndeleting the directory would cause a revoke record to be written to the\njournal. When replaying the journal, the system ﬁrst scans for such re-\nvoke records; any such revoked data is never replayed, thus avoiding the\nproblem mentioned above.\nWrapping Up Journaling: A Timeline\nBefore ending our discussion of journaling, we summarize the protocols\nwe have discussed with timelines depicting each of them.\nTable 42.1\nshows the protocol when journaling data as well as metadata, whereas\nTable 42.2 shows the protocol when journaling only metadata.\nIn each table, time increases in the downward direction, and each row\nin the table shows the logical time that a write can be issued or might\ncomplete. For example, in the data journaling protocol (42.1), the writes\nof the transaction begin block (TxB) and the contents of the transaction\ncan logically be issued at the same time, and thus can be completed in\nany order; however, the write to the transaction end block (TxE) must not\nbe issued until said previous writes complete. Similarly, the checkpoint-\ning writes to data and metadata blocks cannot begin until the transaction\nend block has committed. Horizontal dashed lines show where write-\nordering requirements must be obeyed.\nA similar timeline is shown for the metadata journaling protocol. Note\nthat the data write can logically be issued at the same time as the writes\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nCRASH CONSISTENCY: FSCK AND JOURNALING\n507\nJournal\nFile System\nTxB\nContents\nTxE\nMetadata\nData\n(metadata)\nissue\nissue\nissue\ncomplete\ncomplete\ncomplete\nissue\ncomplete\nissue\ncomplete\nTable 42.2: Metadata Journaling Timeline\nto the transaction begin and the contents of the journal; however, it must\nbe issued and complete before the transaction end has been issued.\nFinally, note that the time of completion marked for each write in the\ntimelines is arbitrary. In a real system, completion time is determined by\nthe I/O subsystem, which may reorder writes to improve performance.\nThe only guarantees about ordering that we have are those that must\nbe enforced for protocol correctness (and are shown via the horizontal\ndashed lines in the tables).\n42.4\nSolution #3: Other Approaches\nWe’ve thus far described two options in keeping ﬁle system metadata\nconsistent: a lazy approach based on fsck, and a more active approach\nknown as journaling. However, these are not the only two approaches.\nOne such approach, known as Soft Updates [GP94], was introduced by\nGanger and Patt. This approach carefully orders all writes to the ﬁle sys-\ntem to ensure that the on-disk structures are never left in an inconsis-\ntent state. For example, by writing a pointed-to data block to disk before\nthe inode that points to it, we can ensure that the inode never points to\ngarbage; similar rules can be derived for all the structures of the ﬁle sys-\ntem. Implementing Soft Updates can be a challenge, however; whereas\nthe journaling layer described above can be implemented with relatively\nlittle knowledge of the exact ﬁle system structures, Soft Updates requires\nintricate knowledge of each ﬁle system data structure and thus adds a fair\namount of complexity to the system.\nAnother approach is known as copy-on-write (yes, COW), and is used\nin a number of popular ﬁle systems, including Sun’s ZFS [B07]. This tech-\nnique never overwrites ﬁles or directories in place; rather, it places new\nupdates to previously unused locations on disk. After a number of up-\ndates are completed, COW ﬁle systems ﬂip the root structure of the ﬁle\nsystem to include pointers to the newly updated structures. Doing so\nmakes keeping the ﬁle system consistent straightforward. We’ll be learn-\ning more about this technique when we discuss the log-structured ﬁle\nsystem (LFS) in a future chapter; LFS is an early example of a COW.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n508\nCRASH CONSISTENCY: FSCK AND JOURNALING\nAnother approach is one we just developed here at Wisconsin. In this\ntechnique, entitled backpointer-based consistency (or BBC), no ordering\nis enforced between writes. To achieve consistency, an additional back\npointer is added to every block in the system; for example, each data\nblock has a reference to the inode to which it belongs. When accessing\na ﬁle, the ﬁle system can determine if the ﬁle is consistent by checking if\nthe forward pointer (e.g., the address in the inode or direct block) points\nto a block that refers back to it. If so, everything must have safely reached\ndisk and thus the ﬁle is consistent; if not, the ﬁle is inconsistent, and an\nerror is returned. By adding back pointers to the ﬁle system, a new form\nof lazy crash consistency can be attained [C+12].\nFinally, we also have explored techniques to reduce the number of\ntimes a journal protocol has to wait for disk writes to complete. Entitled\noptimistic crash consistency [C+13], this new approach issues as many\nwrites to disk as possible and uses a generalized form of the transaction\nchecksum [P+05], as well as a few other techniques, to detect inconsisten-\ncies should they arise. For some workloads, these optimistic techniques\ncan improve performance by an order of magnitude. However, to truly\nfunction well, a slightly different disk interface is required [C+13].\n42.5\nSummary\nWe have introduced the problem of crash consistency, and discussed\nvarious approaches to attacking this problem. The older approach of\nbuilding a ﬁle system checker works but is likely too slow to recover on\nmodern systems. Thus, many ﬁle systems now use journaling. Journaling\nreduces recovery time from O(size-of-the-disk-volume) to O(size-of-the-\nlog), thus speeding recovery substantially after a crash and restart. For\nthis reason, many modern ﬁle systems use journaling. We have also seen\nthat journaling can come in many different forms; the most commonly\nused is ordered metadata journaling, which reduces the amount of trafﬁc\nto the journal while still preserving reasonable consistency guarantees for\nboth ﬁle system metadata as well as user data.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nCRASH CONSISTENCY: FSCK AND JOURNALING\n509\nReferences\n[B07] “ZFS: The Last Word in File Systems”\nJeff Bonwick and Bill Moore\nAvailable: http://opensolaris.org/os/community/zfs/docs/zfs last.pdf\nZFS uses copy-on-write and journaling, actually, as in some cases, logging writes to disk will perform\nbetter.\n[C+12] “Consistency Without Ordering”\nVijay Chidambaram, Tushar Sharma, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau\nFAST ’12, San Jose, California\nA recent paper of ours about a new form of crash consistency based on back pointers. Read it for the\nexciting details!\n[C+13] “Optimistic Crash Consistency”\nVijay Chidambaram, Thanu S. Pillai, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau\nSOSP ’13, Nemacolin Woodlands Resort, PA, November 2013\nOur work on a more optimistic and higher performance journaling protocol. For workloads that call\nfsync() a lot, performance can be greatly improved.\n[GP94] “Metadata Update Performance in File Systems”\nGregory R. Ganger and Yale N. Patt\nOSDI ’94\nA clever paper about using careful ordering of writes as the main way to achieve consistency. Imple-\nmented later in BSD-based systems.\n[G+08] “SQCK: A Declarative File System Checker”\nHaryadi S. Gunawi, Abhishek Rajimwale, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau\nOSDI ’08, San Diego, California\nOur own paper on a new and better way to build a ﬁle system checker using SQL queries. We also show\nsome problems with the existing checker, ﬁnding numerous bugs and odd behaviors, a direct result of\nthe complexity of fsck.\n[H87] “Reimplementing the Cedar File System Using Logging and Group Commit”\nRobert Hagmann\nSOSP ’87, Austin, Texas, November 1987\nThe ﬁrst work (that we know of) that applied write-ahead logging (a.k.a. journaling) to a ﬁle system.\n[M+13] “ffsck: The Fast File System Checker”\nAo Ma, Chris Dragga, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau\nFAST ’13, San Jose, California, February 2013\nA recent paper of ours detailing how to make fsck an order of magnitude faster. Some of the ideas have\nalready been incorporated into the BSD ﬁle system checker [MK96] and are deployed today.\n[MK96] “Fsck - The UNIX File System Check Program”\nMarshall Kirk McKusick and T. J. Kowalski\nRevised in 1996\nDescribes the ﬁrst comprehensive ﬁle-system checking tool, the eponymous fsck. Written by some of\nthe same people who brought you FFS.\n[MJLF84] “A Fast File System for UNIX”\nMarshall K. McKusick, William N. Joy, Sam J. Lefﬂer, Robert S. Fabry\nACM Transactions on Computing Systems.\nAugust 1984, Volume 2:3\nYou already know enough about FFS, right? But yeah, it is OK to reference papers like this more than\nonce in a book.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n510\nCRASH CONSISTENCY: FSCK AND JOURNALING\n[P+05] “IRON File Systems”\nVijayan Prabhakaran, Lakshmi N. Bairavasundaram, Nitin Agrawal, Haryadi S. Gunawi, An-\ndrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau\nSOSP ’05, Brighton, England, October 2005\nA paper mostly focused on studying how ﬁle systems react to disk failures. Towards the end, we intro-\nduce a transaction checksum to speed up logging, which was eventually adopted into Linux ext4.\n[PAA05] “Analysis and Evolution of Journaling File Systems”\nVijayan Prabhakaran, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau\nUSENIX ’05, Anaheim, California, April 2005\nAn early paper we wrote analyzing how journaling ﬁle systems work.\n[R+11] “Coerced Cache Eviction and Discreet-Mode Journaling”\nAbhishek Rajimwale, Vijay Chidambaram, Deepak Ramamurthi,\nAndrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau\nDSN ’11, Hong Kong, China, June 2011\nOur own paper on the problem of disks that buffer writes in a memory cache instead of forcing them to\ndisk, even when explicitly told not to do that! Our solution to overcome this problem: if you want A to\nbe written to disk before B, ﬁrst write A, then send a lot of “dummy” writes to disk, hopefully causing\nA to be forced to disk to make room for them in the cache. A neat if impractical solution.\n[T98] “Journaling the Linux ext2fs File System”\nStephen C. Tweedie\nThe Fourth Annual Linux Expo, May 1998\nTweedie did much of the heavy lifting in adding journaling to the Linux ext2 ﬁle system; the result,\nnot surprisingly, is called ext3. Some nice design decisions include the strong focus on backwards\ncompatibility, e.g., you can just add a journaling ﬁle to an existing ext2 ﬁle system and then mount it\nas an ext3 ﬁle system.\n[T00] “EXT3, Journaling Filesystem”\nStephen Tweedie\nTalk at the Ottawa Linux Symposium, July 2000\nolstrans.sourceforge.net/release/OLS2000-ext3/OLS2000-ext3.html\nA transcript of a talk given by Tweedie on ext3.\n[T01] “The Linux ext2 File System”\nTheodore Ts’o, June, 2001.\nAvailable: http://e2fsprogs.sourceforge.net/ext2.html\nA simple Linux ﬁle system based on the ideas found in FFS. For a while it was quite heavily used; now\nit is really just in the kernel as an example of a simple ﬁle system.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "page_number": 533,
      "chapter_number": 52,
      "summary": "This chapter covers segment 52 (pages 533-546). Key topics include journal, write, and writing. 42.3\nSolution #2: Journaling (or Write-Ahead Logging)\nProbably the most popular solution to the consistent update problem\nis to steal an idea from the world of database management systems.",
      "keywords": [
        "ﬁle system",
        "system",
        "ﬁle",
        "write",
        "CRASH CONSISTENCY",
        "JOURNALING",
        "journaling ﬁle systems",
        "Linux ﬁle system",
        "disk",
        "data",
        "File System",
        "ﬁle system metadata",
        "CRASH",
        "block",
        "ﬁle system checker"
      ],
      "concepts": [
        "journal",
        "write",
        "writing",
        "disks",
        "data",
        "logging",
        "log",
        "systems",
        "block",
        "updating"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 28,
          "title": "Segment 28 (pages 264-271)",
          "relevance_score": 0.66,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 55,
          "title": "Segment 55 (pages 546-554)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 31,
          "title": "Segment 31 (pages 291-298)",
          "relevance_score": 0.6,
          "method": "api"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 15,
          "title": "Segment 15 (pages 115-122)",
          "relevance_score": 0.59,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 36,
          "title": "Segment 36 (pages 341-350)",
          "relevance_score": 0.57,
          "method": "api"
        }
      ]
    },
    {
      "number": 53,
      "title": "Segment 53 (pages 547-558)",
      "start_page": 547,
      "end_page": 558,
      "detection_method": "topic_boundary",
      "content": "43\nLog-structured File Systems\nIn the early 90’s, a group at Berkeley led by Professor John Ousterhout\nand graduate student Mendel Rosenblum developed a new ﬁle system\nknown as the log-structured ﬁle system [RO91]. Their motivation to do\nso was based on the following observations:\n• Memory sizes were growing: As memory got bigger, more data\ncould be cached in memory. As more data is cached, disk trafﬁc\nwould increasingly consist of writes, as reads would be serviced in\nthe cache. Thus, ﬁle system performance would largely be deter-\nmined by its performance for writes.\n• There was a large and growing gap between random I/O perfor-\nmance and sequential I/O performance: Transfer bandwidth in-\ncreases roughly 50%-100% every year; seek and rotational delay\ncosts decrease much more slowly, maybe at 5%-10% per year [P98].\nThus, if one is able to use disks in a sequential manner, one gets a\nhuge performance advantage, which grows over time.\n• Existing ﬁle systems perform poorly on many common workloads:\nFor example, FFS [MJLF84] would perform a large number of writes\nto create a new ﬁle of size one block: one for a new inode, one to\nupdate the inode bitmap, one to the directory data block that the\nﬁle is in, one to the directory inode to update it, one to the new data\nblock that is apart of the new ﬁle, and one to the data bitmap to\nmark the data block as allocated. Thus, although FFS would place\nall of these blocks within the same block group, FFS would incur\nmany short seeks and subsequent rotational delays and thus per-\nformance would fall far short of peak sequential bandwidth.\n• File systems were not RAID-aware: For example, RAID-4 and RAID-\n5 have the small-write problem where a logical write to a single\nblock causes 4 physical I/Os to take place. Existing ﬁle systems do\nnot try to avoid this worst-case RAID writing behavior.\nAn ideal ﬁle system would thus focus on write performance, and try\nto make use of the sequential bandwidth of the disk. Further, it would\nperform well on common workloads that not only write out data but also\n511\n\n\n512\nLOG-STRUCTURED FILE SYSTEMS\nupdate on-disk metadata structures frequently. Finally, it would work\nwell on RAIDs as well as single disks.\nThe new type of ﬁle system Rosenblum and Ousterhout introduced\nwas called LFS, short for the Log-structured File System. When writ-\ning to disk, LFS ﬁrst buffers all updates (including metadata!) in an in-\nmemory segment; when the segment is full, it is written to disk in one\nlong, sequential transfer to an unused part of the disk, i.e., LFS never\noverwrites existing data, but rather always writes segments to free loca-\ntions. Because segments are large, the disk is used efﬁciently, and perfor-\nmance of the ﬁle system approaches its zenith.\nTHE CRUX:\nHOW TO MAKE ALL WRITES SEQUENTIAL WRITES?\nHow can a ﬁle system turns all writes into sequential writes?\nFor\nreads, this task is impossible, as the desired block to be read may be any-\nwhere on disk. For writes, however, the ﬁle system always has a choice,\nand it is exactly this choice we hope to exploit.\n43.1\nWriting To Disk Sequentially\nWe thus have our ﬁrst challenge: how do we transform all updates to\nﬁle-system state into a series of sequential writes to disk? To understand\nthis better, let’s use a simple example. Imagine we are writing a data block\nD to a ﬁle. Writing the data block to disk might result in the following\non-disk layout, with D written at disk address A0:\nD\nA0\nHowever, when a user writes a data block, it is not only data that gets\nwritten to disk; there is also other metadata that needs to be updated.\nIn this case, let’s also write the inode (I) of the ﬁle to disk, and have it\npoint to the data block D. When written to disk, the data block and inode\nwould look something like this (note that the inode looks as big as the\ndata block, which generally isn’t the case; in most systems, data blocks\nare 4 KB in size, whereas an inode is much smaller, around 128 bytes):\nD\nA0\nI\nblk[0]:A0\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nLOG-STRUCTURED FILE SYSTEMS\n513\nTIP: DETAILS MATTER\nAll interesting systems are comprised of a few general ideas and a\nnumber of details. Sometimes, when you are learning about these sys-\ntems, you think to yourself “Oh, I get the general idea; the rest is just de-\ntails,” and you use this to only half-learn how things really work. Don’t\ndo this! Many times, the details are critical. As we’ll see with LFS, the\ngeneral idea is easy to understand, but to really build a working system,\nyou have to think through all of the tricky cases.\nThis basic idea, of simply writing all updates (such as data blocks,\ninodes, etc.) to the disk sequentially, sits at the heart of LFS. If you un-\nderstand this, you get the basic idea. But as with all complicated systems,\nthe devil is in the details.\n43.2\nWriting Sequentially And Effectively\nUnfortunately, writing to disk sequentially is not (alone) enough to\nguarantee efﬁcient writes. For example, imagine if we wrote a single\nblock to address A, at time T. We then wait a little while, and write to\nthe disk at address A + 1 (the next block address in sequential order),\nbut at time T + δ. In-between the ﬁrst and second writes, unfortunately,\nthe disk has rotated; when you issue the second write, it will thus wait\nfor most of a rotation before being committed (speciﬁcally, if the rotation\ntakes time Trotation, the disk will wait Trotation −δ before it can commit\nthe second write to the disk surface). And thus you can hopefully see\nthat simply writing to disk in sequential order is not enough to achieve\npeak performance; rather, you must issue a large number of contiguous\nwrites (or one large write) to the drive in order to achieve good write\nperformance.\nTo achieve this end, LFS uses an ancient technique known as write\nbuffering1. Before writing to the disk, LFS keeps track of updates in\nmemory; when it has received a sufﬁcient number of updates, it writes\nthem to disk all at once, thus ensuring efﬁcient use of the disk.\nThe large chunk of updates LFS writes at one time is referred to by\nthe name of a segment. Although this term is over-used in computer\nsystems, here it just means a large-ish chunk which LFS uses to group\nwrites. Thus, when writing to disk, LFS buffers updates in an in-memory\nsegment, and then writes the segment all at once to the disk. As long as\nthe segment is large enough, these writes will be efﬁcient.\nHere is an example, in which LFS buffers two sets updates into a small\nsegment; actual segments are larger (a few MB). The ﬁrst update is of\n1Indeed, it is hard to ﬁnd a good citation for this idea, since it was likely invented by many\nand very early on in the history of computing. For a study of the beneﬁts of write buffering,\nsee Solworth and Orji [SO90]; to learn about its potential harms, see Mogul [M94].\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n514\nLOG-STRUCTURED FILE SYSTEMS\nfour block writes to ﬁle j; the second is one block being added to ﬁle k.\nLFS then commits the entire segment of seven blocks to disk at once. The\nresulting on-disk layout of these blocks is as follows:\nD[j,0]\nA0\nD[j,1]\nA1\nD[j,2]\nA2\nD[j,3]\nA3\nblk[0]:A0\nblk[1]:A1\nblk[2]:A2\nblk[3]:A3\nInode[j]\nD[k,0]\nA5\nblk[0]:A5\nInode[k]\n43.3\nHow Much To Buffer?\nThis raises the following question: how many updates LFS should\nbuffer before writing to disk? The answer, of course, depends on the disk\nitself, speciﬁcally how high the positioning overhead is in comparison to\nthe transfer rate; see the FFS chapter for a similar analysis.\nFor example, assume that positioning (i.e., rotation and seek over-\nheads) before each write takes roughly Tposition seconds. Assume further\nthat the disk transfer rate is Rpeak MB/s. How much should LFS buffer\nbefore writing when running on such a disk?\nThe way to think about this is that every time you write, you pay a\nﬁxed overhead of the positioning cost. Thus, how much do you have\nto write in order to amortize that cost? The more you write, the better\n(obviously), and the closer you get to achieving peak bandwidth.\nTo obtain a concrete answer, let’s assume we are writing out D MB.\nThe time to write out this chunk of data (Twrite) is the positioning time\nTposition plus the time to transfer D (\nD\nRpeak ), or:\nTwrite = Tposition +\nD\nRpeak\n(43.1)\nAnd thus the effective rate of writing (Reffective), which is just the\namount of data written divided by the total time to write it, is:\nReffective =\nD\nTwrite =\nD\nTposition +\nD\nRpeak\n.\n(43.2)\nWhat we’re interested in is getting the effective rate (Reffective) close\nto the peak rate. Speciﬁcally, we want the effective rate to be some fraction\nF of the peak rate, where 0 < F < 1 (a typical F might be 0.9, or 90% of\nthe peak rate). In mathematical form, this means we want Reffective =\nF × Rpeak.\nAt this point, we can solve for D:\nReffective =\nD\nTposition +\nD\nRpeak\n= F × Rpeak\n(43.3)\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nLOG-STRUCTURED FILE SYSTEMS\n515\nD = F × Rpeak × (Tposition +\nD\nRpeak\n)\n(43.4)\nD = (F × Rpeak × Tposition) + (F × Rpeak ×\nD\nRpeak\n)\n(43.5)\nD =\nF\n1 −F × Rpeak × Tposition\n(43.6)\nLet’s do an example, with a disk with a positioning time of 10 mil-\nliseconds and peak transfer rate of 100 MB/s; assume we want an ef-\nfective bandwidth of 90% of peak (F = 0.9). In this case, D =\n0.9\n0.1 ×\n100 MB/s × 0.01 seconds = 9 MB. Try some different values to see\nhow much we need to buffer in order to approach peak bandwidth. How\nmuch is needed to reach 95% of peak? 99%?\n43.4\nProblem: Finding Inodes\nTo understand how we ﬁnd an inode in LFS, let us brieﬂy review how\nto ﬁnd an inode in a typical UNIX ﬁle system. In a typical ﬁle system such\nas FFS, or even the old UNIX ﬁle system, ﬁnding inodes is easy, because\nthey are organized in an array and placed on disk at ﬁxed locations.\nFor example, the old UNIX ﬁle system keeps all inodes at a ﬁxed por-\ntion of the disk. Thus, given an inode number and the start address, to\nﬁnd a particular inode, you can calculate its exact disk address simply by\nmultiplying the inode number by the size of an inode, and adding that\nto the start address of the on-disk array; array-based indexing, given an\ninode number, is fast and straightforward.\nFinding an inode given an inode number in FFS is only slightly more\ncomplicated, because FFS splits up the inode table into chunks and places\na group of inodes within each cylinder group. Thus, one must know how\nbig each chunk of inodes is and the start addresses of each. After that, the\ncalculations are similar and also easy.\nIn LFS, life is more difﬁcult. Why? Well, we’ve managed to scatter the\ninodes all throughout the disk! Worse, we never overwrite in place, and\nthus the latest version of an inode (i.e., the one we want) keeps moving.\n43.5\nSolution Through Indirection: The Inode Map\nTo remedy this, the designers of LFS introduced a level of indirection\nbetween inode numbers and the inodes through a data structure called\nthe inode map (imap). The imap is a structure that takes an inode number\nas input and produces the disk address of the most recent version of the\ninode. Thus, you can imagine it would often be implemented as a simple\narray, with 4 bytes (a disk pointer) per entry. Any time an inode is written\nto disk, the imap is updated with its new location.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n516\nLOG-STRUCTURED FILE SYSTEMS\nTIP: USE A LEVEL OF INDIRECTION\nPeople often say that the solution to all problems in Computer Science\nis simply a level of indirection. This is clearly not true; it is just the\nsolution to most problems. You certainly can think of every virtualization\nwe have studied, e.g., virtual memory, as simply a level of indirection.\nAnd certainly the inode map in LFS is a virtualization of inode numbers.\nHopefully you can see the great power of indirection in these examples,\nallowing us to freely move structures around (such as pages in the VM\nexample, or inodes in LFS) without having to change every reference to\nthem. Of course, indirection can have a downside too: extra overhead. So\nnext time you have a problem, try solving it with indirection. But make\nsure to think about the overheads of doing so ﬁrst.\nThe imap, unfortunately, needs to be kept persistent (i.e., written to\ndisk); doing so allows LFS to keep track of the locations of inodes across\ncrashes, and thus operate as desired. Thus, a question: where should the\nimap reside on disk?\nIt could live on a ﬁxed part of the disk, of course. Unfortunately, as it\ngets updated frequently, this would then require updates to ﬁle structures\nto be followed by writes to the imap, and hence performance would suffer\n(i.e., there would be more disk seeks, between each update and the ﬁxed\nlocation of the imap).\nInstead, LFS places chunks of the inode map right next to where it is\nwriting all of the other new information. Thus, when appending a data\nblock to a ﬁle k, LFS actually writes the new data block, its inode, and a\npiece of the inode map all together onto the disk, as follows:\nD\nA0\nI[k]\nblk[0]:A0\nA1\nimap\nmap[k]:A1\nIn this picture, the piece of the imap array stored in the block marked\nimap tells LFS that the inode k is at disk address A1; this inode, in turn,\ntells LFS that its data block D is at address A0.\n43.6\nThe Checkpoint Region\nThe clever reader (that’s you, right?) might have noticed a problem\nhere. How do we ﬁnd the inode map, now that pieces of it are also now\nspread across the disk? In the end, there is no magic: the ﬁle system must\nhave some ﬁxed and known location on disk to begin a ﬁle lookup.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nLOG-STRUCTURED FILE SYSTEMS\n517\nLFS has just such a ﬁxed place on disk for this, known as the check-\npoint region (CR). The checkpoint region contains pointers to (i.e., ad-\ndresses of) the latest pieces of the inode map, and thus the inode map\npieces can be found by reading the CR ﬁrst. Note the checkpoint region\nis only updated periodically (say every 30 seconds or so), and thus perfor-\nmance is not ill-affected. Thus, the overall structure of the on-disk layout\ncontains a checkpoint region (which points to the latest pieces of the in-\node map); the inode map pieces each contain addresses of the inodes; the\ninodes point to ﬁles (and directories) just like typical UNIX ﬁle systems.\nHere is an example of the checkpoint region (note it is all the way at\nthe beginning of the disk, at address 0), and a single imap chunk, inode,\nand data block. A real ﬁle system would of course have a much bigger\nCR (indeed, it would have two, as we’ll come to understand later), many\nimap chunks, and of course many more inodes, data blocks, etc.\nimap\n[k...k+N]:\nA2\nCR\n0\nD\nA0\nI[k]\nblk[0]:A0\nA1\nimap\nmap[k]:A1\nA2\n43.7\nReading A File From Disk: A Recap\nTo make sure you understand how LFS works, let us now walk through\nwhat must happen to read a ﬁle from disk. Assume we have nothing in\nmemory to begin. The ﬁrst on-disk data structure we must read is the\ncheckpoint region. The checkpoint region contains pointers (i.e., disk ad-\ndresses) to the entire inode map, and thus LFS then reads in the entire in-\node map and caches it in memory. After this point, when given an inode\nnumber of a ﬁle, LFS simply looks up the inode-number to inode-disk-\naddress mapping in the imap, and reads in the most recent version of the\ninode. To read a block from the ﬁle, at this point, LFS proceeds exactly\nas a typical UNIX ﬁle system, by using direct pointers or indirect pointers\nor doubly-indirect pointers as need be. In the common case, LFS should\nperform the same number of I/Os as a typical ﬁle system when reading a\nﬁle from disk; the entire imap is cached and thus the extra work LFS does\nduring a read is to look up the inode’s address in the imap.\n43.8\nWhat About Directories?\nThus far, we’ve simpliﬁed our discussion a bit by only considering in-\nodes and data blocks. However, to access a ﬁle in a ﬁle system (such as\n/home/remzi/foo, one of our favorite fake ﬁle names), some directo-\nries must be accessed too. So how does LFS store directory data?\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n518\nLOG-STRUCTURED FILE SYSTEMS\nFortunately, directory structure is basically identical to classic UNIX\nﬁle systems, in that a directory is just a collection of (name, inode number)\nmappings. For example, when creating a ﬁle on disk, LFS must both write\na new inode, some data, as well as the directory data and its inode that\nrefer to this ﬁle. Remember that LFS will do so sequentially on the disk\n(after buffering the updates for some time). Thus, creating a ﬁle foo in a\ndirectory would lead to the following new structures on disk:\nD[k]\nA0\nI[k]\nblk[0]:A0\nA1\n(foo, k)\nD[dir]\nA2\nI[dir]\nblk[0]:A2\nA3\nmap[k]:A1\nmap[dir]:A3\nimap\nThe piece of the inode map contains the information for the location of\nboth the directory ﬁle dir as well as the newly-created ﬁle f. Thus, when\naccessing ﬁle foo (with inode number f), you would ﬁrst look in the\ninode map (usually cached in memory) to ﬁnd the location of the inode\nof directory dir (A3); you then read the directory inode, which gives you\nthe location of the directory data (A2); reading this data block gives you\nthe name-to-inode-number mapping of (foo, k). You then consult the\ninode map again to ﬁnd the location of inode number k (A1), and ﬁnally\nread the desired data block at address A0.\nThere is one other serious problem in LFS that the inode map solves,\nknown as the recursive update problem [Z+12].\nThe problem arises\nin any ﬁle system that never updates in place (such as LFS), but rather\nmoves updates to new locations on the disk.\nSpeciﬁcally, whenever an inode is updated, its location on disk changes.\nIf we hadn’t been careful, this would have also entailed an update to\nthe directory that points to this ﬁle, which then would have mandated\na change to the parent of that directory, and so on, all the way up the ﬁle\nsystem tree.\nLFS cleverly avoids this problem with the inode map. Even though\nthe location of an inode may change, the change is never reﬂected in the\ndirectory itself; rather, the imap structure is updated while the directory\nholds the same name-to-inumber mapping. Thus, through indirection,\nLFS avoids the recursive update problem.\n43.9\nA New Problem: Garbage Collection\nYou may have noticed another problem with LFS; it keeps writing\nnewer version of a ﬁle, its inode, and in fact all data to new parts of the\ndisk. This process, while keeping writes efﬁcient, implies that LFS leaves\nolder versions of ﬁle structures all over the disk, scattered throughout the\ndisk. We call such old stuff garbage.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nLOG-STRUCTURED FILE SYSTEMS\n519\nFor example, let’s imagine the case where we have an existing ﬁle re-\nferred to by inode number k, which points to a single data block D0. We\nnow overwrite that block, generating both a new inode and a new data\nblock. The resulting on-disk layout of LFS would look something like this\n(note we omit the imap and other structures for simplicity; a new chunk\nof imap would also have to be written to disk to point to the new inode):\nD0\nA0\nI[k]\nblk[0]:A0\n(both garbage)\nD0\nA4\nI[k]\nblk[0]:A4\nIn the diagram, you can see that both the inode and data block have\ntwo versions on disk, one old (the one on the left) and one current and\nthus live (the one on the right). By the simple act of overwriting a data\nblock, a number of new structures must be persisted by LFS, thus leaving\nold versions of said blocks on the disk.\nAs another example, imagine we instead append a block to that orig-\ninal ﬁle k. In this case, a new version of the inode is generated, but the\nold data block is still pointed to by the inode. Thus, it is still live and very\nmuch apart of the current ﬁle system:\nD0\nA0\nI[k]\nblk[0]:A0\n(garbage)\nD1\nA4\nI[k]\nblk[0]:A0\nblk[1]:A4\nSo what should we do with these older versions of inodes, data blocks,\nand so forth? One could keep those older versions around and allow\nusers to restore old ﬁle versions (for example, when they accidentally\noverwrite or delete a ﬁle, it could be quite handy to do so); such a ﬁle\nsystem is known as a versioning ﬁle system because it keeps track of the\ndifferent versions of a ﬁle.\nHowever, LFS instead keeps only the latest live version of a ﬁle; thus\n(in the background), LFS must periodically ﬁnd these old dead versions\nof ﬁle data, inodes, and other structures, and clean them; cleaning should\nthus make blocks on disk free again for use in a subsequent writes. Note\nthat the process of cleaning is a form of garbage collection, a technique\nthat arises in programming languages that automatically free unused mem-\nory for programs.\nEarlier we discussed segments as important as they are the mechanism\nthat enables large writes to disk in LFS. As it turns out, they are also quite\nintegral to effective cleaning. Imagine what would happen if the LFS\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n520\nLOG-STRUCTURED FILE SYSTEMS\ncleaner simply went through and freed single data blocks, inodes, etc.,\nduring cleaning. The result: a ﬁle system with some number of free holes\nmixed between allocated space on disk. Write performance would drop\nconsiderably, as LFS would not be able to ﬁnd a large contiguous region\nto write to disk sequentially and with high performance.\nInstead, the LFS cleaner works on a segment-by-segment basis, thus\nclearing up large chunks of space for subsequent writing. The basic clean-\ning process works as follows. Periodically, the LFS cleaner reads in a\nnumber of old (partially-used) segments, determines which blocks are\nlive within these segments, and then write out a new set of segments\nwith just the live blocks within them, freeing up the old ones for writing.\nSpeciﬁcally, we expect the cleaner to read in M existing segments, com-\npact their contents into N new segments (where N < M), and then write\nthe N segments to disk in new locations. The old M segments are then\nfreed and can be used by the ﬁle system for subsequent writes.\nWe are now left with two problems, however. The ﬁrst is mechanism:\nhow can LFS tell which blocks within a segment are live, and which are\ndead? The second is policy: how often should the cleaner run, and which\nsegments should it pick to clean?\n43.10\nDetermining Block Liveness\nWe address the mechanism ﬁrst. Given a data block D within an on-\ndisk segment S, LFS must be able to determine whether D is live. To do\nso, LFS adds a little extra information to each segment that describes each\nblock. Speciﬁcally, LFS includes, for each data block D, its inode number\n(which ﬁle it belongs to) and its offset (which block of the ﬁle this is). This\ninformation is recorded in a structure at the head of the segment known\nas the segment summary block.\nGiven this information, it is straightforward to determine whether a\nblock is live or dead. For a block D located on disk at address A, look\nin the segment summary block and ﬁnd its inode number N and offset\nT. Next, look in the imap to ﬁnd where N lives and read N from disk\n(perhaps it is already in memory, which is even better). Finally, using\nthe offset T, look in the inode (or some indirect block) to see where the\ninode thinks the Tth block of this ﬁle is on disk. If it points exactly to disk\naddress A, LFS can conclude that the block D is live. If it points anywhere\nelse, LFS can conclude that D is not in use (i.e., it is dead) and thus know\nthat this version is no longer needed. A pseudocode summary of this\nprocess is shown here:\n(N, T) = SegmentSummary[A];\ninode\n= Read(imap[N]);\nif (inode[T] == A)\n// block D is alive\nelse\n// block D is garbage\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nLOG-STRUCTURED FILE SYSTEMS\n521\nHere is a diagram depicting the mechanism, in which the segment\nsummary block (marked SS) records that the data block at address A0\nis actually a part of ﬁle k at offset 0. By checking the imap for k, you can\nﬁnd the inode, and see that it does indeed point to that location.\nD\nA0\nI[k]\nblk[0]:A0\nA1\nimap\nmap[k]:A1\nss\nA0:\n(k,0)\nThere are some shortcuts LFS takes to make the process of determining\nliveness more efﬁcient. For example, when a ﬁle is truncated or deleted,\nLFS increases its version number and records the new version number in\nthe imap. By also recording the version number in the on-disk segment,\nLFS can short circuit the longer check described above simply by compar-\ning the on-disk version number with a version number in the imap, thus\navoiding extra reads.\n43.11\nA Policy Question: Which Blocks To Clean, And When?\nOn top of the mechanism described above, LFS must include a set of\npolicies to determine both when to clean and which blocks are worth\ncleaning. Determining when to clean is easier; either periodically, dur-\ning idle time, or when you have to because the disk is full.\nDetermining which blocks to clean is more challenging, and has been\nthe subject of many research papers. In the original LFS paper [RO91],\nthe authors describe an approach which tries to segregate hot and cold\nsegment. A hot segment is one in which the contents are being frequently\nover-written; thus, for such a segment, the best policy is to wait a long\ntime before cleaning it, as more and more blocks are getting over-written\n(in new segments) and thus being freed for use. A cold segment, in con-\ntrast, may have a few dead blocks but the rest of its contents are relatively\nstable. Thus, the authors conclude that one should clean cold segments\nsooner and hot segments later, and develop a heuristic that does exactly\nthat. However, as with most policies, this is just one approach, and by\ndeﬁnition is not “the best” approach; later approaches show how to do\nbetter [MR+97].\n43.12\nCrash Recovery And The Log\nOne ﬁnal problem: what happens if the system crashes while LFS is\nwriting to disk? As you may recall in the previous chapter about jour-\nnaling, crashes during updates are tricky for ﬁle systems, and thus some-\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n522\nLOG-STRUCTURED FILE SYSTEMS\nthing LFS must consider as well.\nDuring normal operation, LFS buffers writes in a segment, and then\n(when the segment is full, or when some amount of time has elapsed),\nwrites the segment to disk. LFS organizes these writes in a log, i.e., the\ncheckpoint region points to a head and tail segment, and each segment\npoints to the next segment to be written. LFS also periodically updates the\ncheckpoint region. Crashes could clearly happen during either of these\noperations (write to a segment, write to the CR). So how does LFS handle\ncrashes during writes to these structures?\nLet’s cover the second case ﬁrst. To ensure that the CR update happens\natomically, LFS actually keeps two CRs, one at either end of the disk, and\nwrites to them alternately. LFS also implements a careful protocol when\nupdating the CR with the latest pointers to the inode map and other infor-\nmation; speciﬁcally, it ﬁrst writes out a header (with timestamp), then the\nbody of the CR, and then ﬁnally one last block (also with a timestamp). If\nthe system crashes during a CR update, LFS can detect this by seeing an\ninconsistent pair of timestamps. LFS will always choose to use the most\nrecent CR that has consistent timestamps, and thus consistent update of\nthe CR is achieved.\nLet’s now address the ﬁrst case. Because LFS writes the CR every 30\nseconds or so, the last consistent snapshot of the ﬁle system may be quite\nold. Thus, upon reboot, LFS can easily recover by simply reading in the\ncheckpoint region, the imap pieces it points to, and subsequent ﬁles and\ndirectories; however, the last many seconds of updates would be lost.\nTo improve upon this, LFS tries to rebuild many of those segments\nthrough a technique known as roll forward in the database community.\nThe basic idea is to start with the last checkpoint region, ﬁnd the end of\nthe log (which is included in the CR), and then use that to read through\nthe next segments and see if there are any valid updates within it. If there\nare, LFS updates the ﬁle system accordingly and thus recovers much of\nthe data and metadata written since the last checkpoint. See Rosenblum’s\naward-winning dissertation for details [R92].\n43.13\nSummary\nLFS introduces a new approach to updating the disk. Instead of over-\nwriting ﬁles in places, LFS always writes to an unused portion of the\ndisk, and then later reclaims that old space through cleaning. This ap-\nproach, which in database systems is called shadow paging [L77] and in\nﬁle-system-speak is sometimes called copy-on-write, enables highly efﬁ-\ncient writing, as LFS can gather all updates into an in-memory segment\nand then write them out together sequentially.\nThe downside to this approach is that it generates garbage; old copies\nof the data are scattered throughout the disk, and if one wants to reclaim\nsuch space for subsequent usage, one must clean old segments periodi-\ncally. Cleaning became the focus of much controversy in LFS, and con-\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "page_number": 547,
      "chapter_number": 53,
      "summary": "This chapter covers segment 53 (pages 547-558). Key topics include writes, writing. Their motivation to do\nso was based on the following observations:\n• Memory sizes were growing: As memory got bigger, more data\ncould be cached in memory.",
      "keywords": [
        "LFS",
        "ﬁle system",
        "disk",
        "inode",
        "Log-structured File Systems",
        "ﬁle",
        "data block",
        "File Systems",
        "block",
        "UNIX ﬁle system",
        "system",
        "Inode Map",
        "data",
        "writes",
        "inode number"
      ],
      "concepts": [
        "lfs",
        "writes",
        "writing",
        "disk",
        "segment",
        "segments",
        "block",
        "update",
        "updated",
        "old"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 31,
          "title": "Segment 31 (pages 620-638)",
          "relevance_score": 0.67,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 33,
          "title": "Segment 33 (pages 662-682)",
          "relevance_score": 0.66,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 53,
          "title": "Segment 53 (pages 526-536)",
          "relevance_score": 0.61,
          "method": "api"
        },
        {
          "book": "Designing Machine Learning Systems An Iterative Process for Production-Ready Applications",
          "chapter": 10,
          "title": "Segment 10 (pages 80-87)",
          "relevance_score": 0.6,
          "method": "api"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 15,
          "title": "Segment 15 (pages 115-122)",
          "relevance_score": 0.59,
          "method": "api"
        }
      ]
    },
    {
      "number": 54,
      "title": "Segment 54 (pages 559-566)",
      "start_page": 559,
      "end_page": 566,
      "detection_method": "topic_boundary",
      "content": "LOG-STRUCTURED FILE SYSTEMS\n523\nTIP: TURN FLAWS INTO VIRTUES\nWhenever your system has a fundamental ﬂaw, see if you can turn it\naround into a feature or something useful. NetApp’s WAFL does this\nwith old ﬁle contents; by making old versions available, WAFL no longer\nhas to worry about cleaning, and thus provides a cool feature and re-\nmoves the LFS cleaning problem all in one wonderful twist. Are there\nother examples of this in systems? Undoubtedly, but you’ll have to think\nof them yourself, because this chapter is over with a capital “O”. Over.\nDone. Kaput. We’re out. Peace!\ncerns over cleaning costs [SS+95] perhaps limited LFS’s initial impact on\nthe ﬁeld. However, some modern commercial ﬁle systems, including Ne-\ntApp’s WAFL [HLM94], Sun’s ZFS [B07], and Linux btrfs [M07] adopt\na similar copy-on-write approach to writing to disk, and thus the intel-\nlectual legacy of LFS lives on in these modern ﬁle systems. In particular,\nWAFL got around cleaning problems by turning them into a feature; by\nproviding old versions of the ﬁle system via snapshots, users could ac-\ncess old ﬁles whenever they deleted current ones accidentally.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n524\nLOG-STRUCTURED FILE SYSTEMS\nReferences\n[B07] “ZFS: The Last Word in File Systems”\nJeff Bonwick and Bill Moore\nAvailable: http://opensolaris.org/os/community/zfs/docs/zfs last.pdf\nSlides on ZFS; unfortunately, there is no great ZFS paper.\n[HLM94] “File System Design for an NFS File Server Appliance”\nDave Hitz, James Lau, Michael Malcolm\nUSENIX Spring ’94\nWAFL takes many ideas from LFS and RAID and puts it into a high-speed NFS appliance for the\nmulti-billion dollar storage company NetApp.\n[L77] “Physical Integrity in a Large Segmented Database”\nR. Lorie\nACM Transactions on Databases, 1977, Volume 2:1, pages 91-104\nThe original idea of shadow paging is presented here.\n[M07] “The Btrfs Filesystem”\nChris Mason\nSeptember 2007\nAvailable: oss.oracle.com/projects/btrfs/dist/documentation/btrfs-ukuug.pdf\nA recent copy-on-write Linux ﬁle system, slowly gaining in importance and usage.\n[MJLF84] “A Fast File System for UNIX”\nMarshall K. McKusick, William N. Joy, Sam J. Lefﬂer, Robert S. Fabry\nACM TOCS, August, 1984, Volume 2, Number 3\nThe original FFS paper; see the chapter on FFS for more details.\n[MR+97] “Improving the Performance of Log-structured File Systems with Adaptive Meth-\nods” Jeanna Neefe Matthews, Drew Roselli, Adam M. Costello, Randolph Y. Wang, Thomas E.\nAnderson\nSOSP 1997, pages 238-251, October, Saint Malo, France\nA more recent paper detailing better policies for cleaning in LFS.\n[M94] “A Better Update Policy”\nJeffrey C. Mogul\nUSENIX ATC ’94, June 1994\nIn this paper, Mogul ﬁnds that read workloads can be harmed by buffering writes for too long and then\nsending them to the disk in a big burst. Thus, he recommends sending writes more frequently and in\nsmaller batches.\n[P98] “Hardware Technology Trends and Database Opportunities”\nDavid A. Patterson\nACM SIGMOD ’98 Keynote Address, Presented June 3, 1998, Seattle, Washington\nAvailable: http://www.cs.berkeley.edu/˜pattrsn/talks/keynote.html\nA great set of slides on technology trends in computer systems. Hopefully, Patterson will create another\nof these sometime soon.\n[RO91] “Design and Implementation of the Log-structured File System”\nMendel Rosenblum and John Ousterhout\nSOSP ’91, Paciﬁc Grove, CA, October 1991\nThe original SOSP paper about LFS, which has been cited by hundreds of other papers and inspired\nmany real systems.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nLOG-STRUCTURED FILE SYSTEMS\n525\n[R92] “Design and Implementation of the Log-structured File System”\nMendel Rosenblum\nhttp://www.eecs.berkeley.edu/Pubs/TechRpts/1992/CSD-92-696.pdf\nThe award-winning dissertation about LFS, with many of the details missing from the paper.\n[SS+95] “File system logging versus clustering: a performance comparison”\nMargo Seltzer, Keith A. Smith, Hari Balakrishnan, Jacqueline Chang, Sara McMains, Venkata\nPadmanabhan\nUSENIX 1995 Technical Conference, New Orleans, Louisiana, 1995\nA paper that showed the LFS performance sometimes has problems, particularly for workloads with\nmany calls to fsync() (such as database workloads). The paper was controversial at the time.\n[SO90] “Write-Only Disk Caches”\nJon A. Solworth, Cyril U. Orji\nSIGMOD ’90, Atlantic City, New Jersey, May 1990\nAn early study of write buffering and its beneﬁts. However, buffering for too long can be harmful: see\nMogul [M94] for details.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n44\nData Integrity and Protection\nBeyond the basic advances found in the ﬁle systems we have studied thus\nfar, a number of features are worth studying. In this chapter, we focus on\nreliability once again (having previously studied storage system reliabil-\nity in the RAID chapter). Speciﬁcally, how should a ﬁle system or storage\nsystem ensure that data is safe, given the unreliable nature of modern\nstorage devices?\nThis general area is referred to as data integrity or data protection.\nThus, we will now investigate techniques used to ensure that the data\nyou put into your storage system is the same when the storage system\nreturns it to you.\nCRUX: HOW TO ENSURE DATA INTEGRITY\nHow should systems ensure that the data written to storage is pro-\ntected? What techniques are required? How can such techniques be made\nefﬁcient, with both low space and time overheads?\n44.1\nDisk Failure Modes\nAs you learned in the chapter about RAID, disks are not perfect, and\ncan fail (on occasion). In early RAID systems, the model of failure was\nquite simple: either the entire disk is working, or it fails completely, and\nthe detection of such a failure is straightforward. This fail-stop model of\ndisk failure makes building RAID relatively simple [S90].\nWhat you didn’t learn is about all of the other types of failure modes\nmodern disks exhibit. Speciﬁcally, as Bairavasundaram et al. studied\nin great detail [B+07, B+08], modern disks will occasionally seem to be\nmostly working but have trouble successfully accessing one or more blocks.\nSpeciﬁcally, two types of single-block failures are common and worthy of\nconsideration: latent-sector errors (LSEs) and block corruption. We’ll\nnow discuss each in more detail.\n527\n\n\n528\nDATA INTEGRITY AND PROTECTION\nCheap\nCostly\nLSEs\n9.40%\n1.40%\nCorruption\n0.50%\n0.05%\nTable 44.1: Frequency of LSEs and Block Corruption\nLSEs arise when a disk sector (or group of sectors) has been damaged\nin some way. For example, if the disk head touches the surface for some\nreason (a head crash, something which shouldn’t happen during nor-\nmal operation), it may damage the surface, making the bits unreadable.\nCosmic rays can also ﬂip bits, leading to incorrect contents. Fortunately,\nin-disk error correcting codes (ECC) are used by the drive to determine\nwhether the on-disk bits in a block are good, and in some cases, to ﬁx\nthem; if they are not good, and the drive does not have enough informa-\ntion to ﬁx the error, the disk will return an error when a request is issued\nto read them.\nThere are also cases where a disk block becomes corrupt in a way not\ndetectable by the disk itself. For example, buggy disk ﬁrmware may write\na block to the wrong location; in such a case, the disk ECC indicates the\nblock contents are ﬁne, but from the client’s perspective the wrong block\nis returned when subsequently accessed. Similarly, a block may get cor-\nrupted when it is transferred from the host to the disk across a faulty\nbus; the resulting corrupt data is stored by the disk, but it is not what\nthe client desires. These types of faults are particularly insidious because\nthe are silent faults; the disk gives no indication of the problem when\nreturning the faulty data.\nPrabhakaran et al. describes this more modern view of disk failure as\nthe fail-partial disk failure model [P+05]. In this view, disks can still fail\nin their entirety (as was the case in the traditional fail-stop model); how-\never, disks can also seemingly be working and have one or more blocks\nbecome inaccessible (i.e., LSEs) or hold the wrong contents (i.e., corrup-\ntion). Thus, when accessing a seemingly-working disk, once in a while\nit may either return an error when trying to read or write a given block\n(a non-silent partial fault), and once in a while it may simply return the\nwrong data (a silent partial fault).\nBoth of these types of faults are somewhat rare, but just how rare? Ta-\nble 44.1 summarizes some of the ﬁndings from the two Bairavasundaram\nstudies [B+07,B+08].\nThe table shows the percent of drives that exhibited at least one LSE\nor block corruption over the course of the study (about 3 years, over\n1.5 million disk drives). The table further sub-divides the results into\n“cheap” drives (usually SATA drives) and “costly” drives (usually SCSI\nor FibreChannel). As you can see from the table, while buying better\ndrives reduces the frequency of both types of problem (by about an or-\nder of magnitude), they still happen often enough that you need to think\ncarefully about them.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nDATA INTEGRITY AND PROTECTION\n529\nSome additional ﬁndings about LSEs are:\n• Costly drives with more than one LSE are as likely to develop ad-\nditional errors as cheaper drives\n• For most drives, annual error rate increases in year two\n• LSEs increase with disk size\n• Most disks with LSEs have less than 50\n• Disks with LSEs are more likely to develop additional LSEs\n• There exists a signiﬁcant amount of spatial and temporal locality\n• Disk scrubbing is useful (most LSEs were found this way)\nSome ﬁndings about corruption:\n• Chance of corruption varies greatly across different drive models\nwithin the same drive class\n• Age affects are different across models\n• Workload and disk size have little impact on corruption\n• Most disks with corruption only have a few corruptions\n• Corruption is not independent with a disk or across disks in RAID\n• There exists spatial locality, and some temporal locality\n• There is a weak correlation with LSEs\nTo learn more about these failures, you should likely read the original\npapers [B+07,B+08]. But hopefully the main point should be clear: if you\nreally wish to build a reliable storage system, you must include machin-\nery to detect and recovery from both LSEs and block corruption.\n44.2\nHandling Latent Sector Errors\nGiven these two new modes of partial disk failure, we should now try\nto see what we can do about them. Let’s ﬁrst tackle the easier of the two,\nnamely latent sector errors.\nCRUX: HOW TO HANDLE LATENT SECTOR ERRORS\nHow should a storage system handle latent sector errors? How much\nextra machinery is needed to handle this form of partial failure?\nAs it turns out, latent sector errors are rather straightforward to han-\ndle, as they are (by deﬁnition) easily detected. When a storage system\ntries to access a block, and the disk returns an error, the storage system\nshould simply use whatever redundancy mechanism it has to return the\ncorrect data. In a mirrored RAID, for example, the system should access\nthe alternate copy; in a RAID-4 or RAID-5 system based on parity, the\nsystem should reconstruct the block from the other blocks in the parity\ngroup. Thus, easily detected problems such as LSEs are readily recovered\nthrough standard redundancy mechanisms.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n530\nDATA INTEGRITY AND PROTECTION\nThe growing prevalence of LSEs has inﬂuenced RAID designs over the\nyears. One particularly interesting problem arises in RAID-4/5 systems\nwhen both full-disk faults and LSEs occur in tandem. Speciﬁcally, when\nan entire disk fails, the RAID tries to reconstruct the disk (say, onto a\nhot spare) by reading through all of the other disks in the parity group\nand recomputing the missing values. If, during reconstruction, an LSE\nis encountered on any one of the other disks, we have a problem: the\nreconstruction cannot successfully complete.\nTo combat this issue, some systems add an extra degree of redundancy.\nFor example, NetApp’s RAID-DP has the equivalent of two parity disks\ninstead of one [C+04]. When an LSE is discovered during reconstruction,\nthe extra parity helps to reconstruct the missing block. As always, there is\na cost, in that maintaining two parity blocks for each stripe is more costly;\nhowever, the log-structured nature of the NetApp WAFL ﬁle system mit-\nigates that cost in many cases [HLM94]. The remaining cost is space, in\nthe form of an extra disk for the second parity block.\n44.3\nDetecting Corruption: The Checksum\nLet’s now tackle the more challenging problem, that of silent failures\nvia data corruption. How can we prevent users from getting bad data\nwhen corruption arises, and thus leads to disks returning bad data?\nCRUX: HOW TO PRESERVE DATA INTEGRITY DESPITE CORRUPTION\nGiven the silent nature of such failures, what can a storage system do\nto detect when corruption arises? What techniques are needed? How can\none implement them efﬁciently?\nUnlike latent sector errors, detection of corruption is a key problem.\nHow can a client tell that a block has gone bad? Once it is known that a\nparticular block is bad, recovery is the same as before: you need to have\nsome other copy of the block around (and hopefully, one that is not cor-\nrupt!). Thus, we focus here on detection techniques.\nThe primary mechanism used by modern storage systems to preserve\ndata integrity is called the checksum. A checksum is simply the result\nof a function that takes a chunk of data (say a 4KB block) as input and\ncomputes a function over said data, producing a small summary of the\ncontents of the data (say 4 or 8 bytes). This summary is referred to as the\nchecksum. The goal of such a computation is to enable a system to detect\nif data has somehow been corrupted or altered by storing the checksum\nwith the data and then conﬁrming upon later access that the data’s cur-\nrent checksum matches the original storage value.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "page_number": 559,
      "chapter_number": 54,
      "summary": "NetApp’s WAFL does this\nwith old ﬁle contents; by making old versions available, WAFL no longer\nhas to worry about cleaning, and thus provides a cool feature and re-\nmoves the LFS cleaning problem all in one wonderful twist Key topics include disk, data, and systems.",
      "keywords": [
        "LOG-STRUCTURED FILE SYSTEMS",
        "disk",
        "system",
        "Data",
        "FILE SYSTEMS",
        "storage system",
        "Data Integrity",
        "LOG-STRUCTURED FILE",
        "block",
        "LSEs",
        "FILE",
        "corruption",
        "Disk Failure",
        "File System Design",
        "ﬁle system"
      ],
      "concepts": [
        "disk",
        "data",
        "systems",
        "drive",
        "raid",
        "blocks",
        "corruption",
        "corrupt",
        "corruptions",
        "errors"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 31,
          "title": "Segment 31 (pages 291-298)",
          "relevance_score": 0.51,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 55,
          "title": "Segment 55 (pages 546-554)",
          "relevance_score": 0.43,
          "method": "api"
        },
        {
          "book": "Game_Engine_Architecture",
          "chapter": 38,
          "title": "Segment 38 (pages 764-785)",
          "relevance_score": 0.43,
          "method": "api"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 8,
          "title": "Segment 8 (pages 62-69)",
          "relevance_score": 0.43,
          "method": "api"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 15,
          "title": "Segment 15 (pages 115-122)",
          "relevance_score": 0.42,
          "method": "api"
        }
      ]
    },
    {
      "number": 55,
      "title": "Segment 55 (pages 567-574)",
      "start_page": 567,
      "end_page": 574,
      "detection_method": "topic_boundary",
      "content": "DATA INTEGRITY AND PROTECTION\n531\nTIP: THERE’S NO FREE LUNCH\nThere’s No Such Thing As A Free Lunch, or TNSTAAFL for short, is\nan old American idiom that implies that when you are seemingly get-\nting something for free, in actuality you are likely paying some cost for\nit. It comes from the old days when diners would advertise a free lunch\nfor customers, hoping to draw them in; only when you went in, did you\nrealize that to acquire the “free” lunch, you had to purchase one or more\nalcoholic beverages. Of course, this may not actually be a problem, partic-\nularly if you are an aspiring alcoholic (or typical undergraduate student).\nCommon Checksum Functions\nA number of different functions are used to compute checksums, and\nvary in strength (i.e., how good they are at protecting data integrity) and\nspeed (i.e., how quickly can they be computed). A trade-off that is com-\nmon in systems arises here: usually, the more protection you get, the\ncostlier it is. There is no such thing as a free lunch.\nOne simple checksum function that some use is based on exclusive\nor (XOR). With XOR-based checksums, the checksum is computed sim-\nply by XOR’ing each chunk of the data block being checksummed, thus\nproducing a single value that represents the XOR of the entire block.\nTo make this more concrete, imagine we are computing a 4-byte check-\nsum over a block of 16 bytes (this block is of course too small to really be a\ndisk sector or block, but it will serve for the example). The 16 data bytes,\nin hex, look like this:\n365e c4cd ba14 8a92 ecef 2c3a 40be f666\nIf we view them in binary, we get the following:\n0011 0110 0101 1110\n1100 0100 1100 1101\n1011 1010 0001 0100\n1000 1010 1001 0010\n1110 1100 1110 1111\n0010 1100 0011 1010\n0100 0000 1011 1110\n1111 0110 0110 0110\nBecause we’ve lined up the data in groups of 4 bytes per row, it is easy\nto see what the resulting checksum will be: simply perform an XOR over\neach column to get the ﬁnal checksum value:\n0010 0000 0001 1011\n1001 0100 0000 0011\nThe result, in hex, is 0x201b9403.\nXOR is a reasonable checksum but has its limitations. If, for example,\ntwo bits in the same position within each checksummed unit change, the\nchecksum will not detect the corruption. For this reason, people have\ninvestigated other checksum functions.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n532\nDATA INTEGRITY AND PROTECTION\nAnother simple checksum function is addition. This approach has\nthe advantage of being fast; computing it just requires performing 2’s-\ncomplement addition over each chunk of the data, ignoring overﬂow. It\ncan detect many changes in data, but is not good if the data, for example,\nis shifted.\nA slightly more complex algorithm is known as the Fletcher check-\nsum, named (as you might guess) for the inventor, John G. Fletcher [F82].\nIt is quite simple and involves the computation of two check bytes, s1\nand s2. Speciﬁcally, assume a block D consists of bytes d1 ... dn; s1 is\nsimply deﬁned as follows: s1 = s1 + di mod 255 (computed over all di);\ns2 in turn is: s2 = s2 + s1 mod 255 (again over all di) [F04]. The ﬂetcher\nchecksum is known to be almost as strong as the CRC (described next),\ndetecting all single-bit errors, all double-bit errors, and a large percentage\nof burst errors [F04].\nOne ﬁnal commonly-used checksum is known as a cyclic redundancy\ncheck (CRC). While this sounds fancy, the basic idea is quite simple. As-\nsume you wish to compute the checksum over a data block D. All you do\nis treat D as if it is a large binary number (it is just a string of bits after all)\nand divide it by an agreed upon value (k). The remainder of this division\nis the value of the CRC. As it turns out, one can implement this binary\nmodulo operation rather efﬁciently, and hence the popularity of the CRC\nin networking as well. See elsewhere for more details [M13].\nWhatever the method used, it should be obvious that there is no per-\nfect checksum: it is possible two data blocks with non-identical contents\nwill have identical checksums, something referred to as a collision. This\nfact should be intuitive: after all, computing a checksum is taking some-\nthing large (e.g., 4KB) and producing a summary that is much smaller\n(e.g., 4 or 8 bytes). In choosing a good checksum function, we are thus\ntrying to ﬁnd one that minimizes the chance of collisions while remain-\ning easy to compute.\nChecksum Layout\nNow that you understand a bit about how to compute a checksum, let’s\nnext analyze how to use checksums in a storage system. The ﬁrst question\nwe must address is the layout of the checksum, i.e., how should check-\nsums be stored on disk?\nThe most basic approach simply stores a checksum with each disk sec-\ntor (or block). Given a data block D, let us call the checksum over that\ndata C(D). Thus, without checksums, the disk layout looks like this:\nD0\nD1\nD2\nD3\nD4\nD5\nD6\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nDATA INTEGRITY AND PROTECTION\n533\nWith checksums, the layout adds a single checksum for every block:\nC[D0]\nD0\nC[D1]\nD1\nC[D2]\nD2\nC[D3]\nD3\nC[D4]\nD4\nBecause checksums are usually small (e.g., 8 bytes), and disks only can\nwrite in sector-sized chunks (512 bytes) or multiples thereof, one problem\nthat arises is how to achieve the above layout. One solution employed by\ndrive manufacturers is to format the drive with 520-byte sectors; an extra\n8 bytes per sector can be used to store the checksum.\nIn disks that don’t have such functionality, the ﬁle system must ﬁgure\nout a way to store the checksums packed into 512-byte blocks. One such\npossibility is as follows:\nC[D0]\nC[D1]\nC[D2]\nC[D3]\nC[D4]\nD0\nD1\nD2\nD3\nD4\nIn this scheme, the n checksums are stored together in a sector, fol-\nlowed by n data blocks, followed by another checksum sector for the next\nn blocks, and so forth. This scheme has the beneﬁt of working on all disks,\nbut can be less efﬁcient; if the ﬁle system, for example, wants to overwrite\nblock D1, it has to read in the checksum sector containing C(D1), update\nC(D1) in it, and then write out the checksum sector as well as the new\ndata block D1 (thus, one read and two writes). The earlier approach (of\none checksum per sector) just performs a single write.\n44.4\nUsing Checksums\nWith a checksum layout decided upon, we can now proceed to actu-\nally understand how to use the checksums. When reading a block D, the\nclient (i.e., ﬁle system or storage controller) also reads its checksum from\ndisk Cs(D), which we call the stored checksum (hence the subscript Cs).\nThe client then computes the checksum over the retrieved block D, which\nwe call the computed checksum Cc(D). At this point, the client com-\npares the stored and computed checksums; if they are equal (i.e., Cs(D)\n== Cc(D), the data has likely not been corrupted, and thus can be safely\nreturned to the user. If they do not match (i.e., Cs(D) != Cc(D)), this im-\nplies the data has changed since the time it was stored (since the stored\nchecksum reﬂects the value of the data at that time). In this case, we have\na corruption, which our checksum has helped us to detect.\nGiven a corruption, the natural question is what should we do about\nit? If the storage system has a redundant copy, the answer is easy: try to\nuse it instead. If the storage system has no such copy, the likely answer is\nto return an error. In either case, realize that corruption detection is not a\nmagic bullet; if there is no other way to get the non-corrupted data, you\nare simply out of luck.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n534\nDATA INTEGRITY AND PROTECTION\n44.5\nA New Problem: Misdirected Writes\nThe basic scheme described above works well in the general case of\ncorrupted blocks. However, modern disks have a couple of unusual fail-\nure modes that require different solutions.\nThe ﬁrst failure mode of interest is called a misdirected write. This\narises in disk and RAID controllers which write the data to disk correctly,\nexcept in the wrong location. In a single-disk system, this means that the\ndisk wrote block Dx not to address x (as desired) but rather to address\ny (thus “corrupting” Dy); in addition, within a multi-disk system, the\ncontroller may also write Di,x not to address x of disk i but rather to\nsome other disk j. Thus our question:\nCRUX: HOW TO HANDLE MISDIRECTED WRITES\nHow should a storage system or disk controller detect misdirected\nwrites? What additional features are required from the checksum?\nThe answer, not surprisingly, is simple: add a little more information\nto each checksum. In this case, adding a physical identiﬁer (physical\nID) is quite helpful. For example, if the stored information now contains\nthe checksum C(D) as well as the disk and sector number of the block,\nit is easy for the client to determine whether the correct information re-\nsides within the block. Speciﬁcally, if the client is reading block 4 on disk\n10 (D10,4), the stored information should include that disk number and\nsector offset, as shown below. If the information does not match, a misdi-\nrected write has taken place, and a corruption is now detected. Here is an\nexample of what this added information would look like on a two-disk\nsystem. Note that this ﬁgure, like the others before it, is not to scale, as the\nchecksums are usually small (e.g., 8 bytes) whereas the blocks are much\nlarger (e.g., 4 KB or bigger):\nDisk 0\nDisk 1\nC[D0]\ndisk=0\nblock=0\nD0\nC[D1]\ndisk=0\nblock=1\nD1\nC[D2]\ndisk=0\nblock=2\nD2\nC[D0]\ndisk=1\nblock=0\nD0\nC[D1]\ndisk=1\nblock=1\nD1\nC[D2]\ndisk=1\nblock=2\nD2\nYou can see from the on-disk format that there is now a fair amount of\nredundancy on disk: for each block, the disk number is repeated within\neach block, and the offset of the block in question is also kept next to the\nblock itself. The presence of redundant information should be no sur-\nprise, though; redundancy is the key to error detection (in this case) and\nrecovery (in others). A little extra information, while not strictly needed\nwith perfect disks, can go a long ways in helping detect problematic situ-\nations should they arise.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nDATA INTEGRITY AND PROTECTION\n535\n44.6\nOne Last Problem: Lost Writes\nUnfortunately, misdirected writes are not the last problem we will\naddress. Speciﬁcally, some modern storage devices also have an issue\nknown as a lost write, which occurs when the device informs the upper\nlayer that a write has completed but in fact it never is persisted; thus,\nwhat remains is left is the old contents of the block rather than the up-\ndated new contents.\nThe obvious question here is: do any of our checksumming strategies\nfrom above (e.g., basic checksums, or physical identity) help to detect\nlost writes? Unfortunately, the answer is no: the old block likely has a\nmatching checksum, and the physical ID used above (disk number and\nblock offset) will also be correct. Thus our ﬁnal problem:\nCRUX: HOW TO HANDLE LOST WRITES\nHow should a storage system or disk controller detect lost writes?\nWhat additional features are required from the checksum?\nThere are a number of possible solutions that can help [K+08]. One\nclassic approach [BS04] is to perform a write verify or read-after-write;\nby immediately reading back the data after a write, a system can ensure\nthat the data indeed reached the disk surface. This approach, however, is\nquite slow, doubling the number of I/Os needed to complete a write.\nSome systems add a checksum elsewhere in the system to detect lost\nwrites. For example, Sun’s Zettabyte File System (ZFS) includes a check-\nsum in each ﬁle system inode and indirect block for every block included\nwithin a ﬁle. Thus, even if the write to a data block itself is lost, the check-\nsum within the inode will not match the old data. Only if the writes to\nboth the inode and the data are lost simultaneously will such a scheme\nfail, an unlikely (but unfortunately, possible!) situation.\n44.7\nScrubbing\nGiven all of this discussion, you might be wondering: when do these\nchecksums actually get checked? Of course, some amount of checking\noccurs when data is accessed by applications, but most data is rarely\naccessed, and thus would remain unchecked. Unchecked data is prob-\nlematic for a reliable storage system, as bit rot could eventually affect all\ncopies of a particular piece of data.\nTo remedy this problem, many systems utilize disk scrubbing of var-\nious forms [K+08]. By periodically reading through every block of the\nsystem, and checking whether checksums are still valid, the disk system\ncan reduce the chances that all copies of a certain data item become cor-\nrupted. Typical systems schedule scans on a nightly or weekly basis.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n536\nDATA INTEGRITY AND PROTECTION\n44.8\nOverheads Of Checksumming\nBefore closing, we now discuss some of the overheads of using check-\nsums for data protection. There are two distinct kinds of overheads, as is\ncommon in computer systems: space and time.\nSpace overheads come in two forms. The ﬁrst is on the disk (or other\nstorage medium) itself; each stored checksum takes up room on the disk,\nwhich can no longer be used for user data. A typical ratio might be an 8-\nbyte checksum per 4 KB data block, for a 0.19% on-disk space overhead.\nThe second type of space overhead comes in the memory of the sys-\ntem. When accessing data, there must now be room in memory for the\nchecksums as well as the data itself. However, if the system simply checks\nthe checksum and then discards it once done, this overhead is short-lived\nand not much of a concern. Only if checksums are kept in memory (for\nan added level of protection against memory corruption [Z+13]) will this\nsmall overhead be observable.\nWhile space overheads are small, the time overheads induced by check-\nsumming can be quite noticeable. Minimally, the CPU must compute the\nchecksum over each block, both when the data is stored (to determine\nthe value of the stored checksum) as well as when it is accessed (to com-\npute the checksum again and compare it against the stored checksum).\nOne approach to reducing CPU overheads, employed by many systems\nthat use checksums (including network stacks), is to combine data copy-\ning and checksumming into one streamlined activity; because the copy is\nneeded anyhow (e.g., to copy the data from the kernel page cache into a\nuser buffer), combined copying/checksumming can be quite effective.\nBeyond CPU overheads, some checksumming schemes can induce ex-\ntra I/O overheads, particularly when checksums are stored distinctly from\nthe data (thus requiring extra I/Os to access them), and for any extra I/O\nneeded for background scrubbing. The former can be reduced by design;\nthe latter can be tuned and thus its impact limited, perhaps by control-\nling when such scrubbing activity takes place. The middle of the night,\nwhen most (not all!) productive workers have gone to bed, may be a\ngood time to perform such scrubbing activity and increase the robustness\nof the storage system.\n44.9\nSummary\nWe have discussed data protection in modern storage systems, focus-\ning on checksum implementation and usage. Different checksums protect\nagainst different types of faults; as storage devices evolve, new failure\nmodes will undoubtedly arise. Perhaps such change will force the re-\nsearch community and industry to revisit some of these basic approaches,\nor invent entirely new approaches altogether. Time will tell. Or it won’t.\nTime is funny that way.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nDATA INTEGRITY AND PROTECTION\n537\nReferences\n[B+08] “An Analysis of Data Corruption in the Storage Stack”\nLakshmi N. Bairavasundaram, Garth R. Goodson, Bianca Schroeder,\nAndrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau\nFAST ’08, San Jose, CA, February 2008\nThe ﬁrst paper to truly study disk corruption in great detail, focusing on how often such corruption\noccurs over three years for over 1.5 million drives. Lakshmi did this work while a graduate student at\nWisconsin under our supervision, but also in collaboration with his colleagues at NetApp where he was\nan intern for multiple summers. A great example of how working with industry can make for much\nmore interesting and relevant research.\n[BS04] “Commercial Fault Tolerance: A Tale of Two Systems”\nWendy Bartlett, Lisa Spainhower\nIEEE Transactions on Dependable and Secure Computing, Vol. 1, No. 1, January 2004\nThis classic in building fault tolerant systems is an excellent overview of the state of the art from both\nIBM and Tandem. Another must read for those interested in the area.\n[C+04] “Row-Diagonal Parity for Double Disk Failure Correction”\nP. Corbett, B. English, A. Goel, T. Grcanac, S. Kleiman, J. Leong, S. Sankar\nFAST ’04, San Jose, CA, February 2004\nAn early paper on how extra redundancy helps to solve the combined full-disk-failure/partial-disk-failure\nproblem. Also a nice example of how to mix more theoretical work with practical.\n[F04] “Checksums and Error Control”\nPeter M. Fenwick\nAvailable: www.cs.auckland.ac.nz/compsci314s2c/resources/Checksums.pdf\nA great simple tutorial on checksums, available to you for the amazing cost of free.\n[F82] “An Arithmetic Checksum for Serial Transmissions”\nJohn G. Fletcher\nIEEE Transactions on Communication, Vol. 30, No. 1, January 1982\nFletcher’s original work on his eponymous checksum. Of course, he didn’t call it the Fletcher checksum,\nrather he just didn’t call it anything, and thus it became natural to name it after the inventor. So don’t\nblame old Fletch for this seeming act of braggadocio.\n[HLM94] “File System Design for an NFS File Server Appliance”\nDave Hitz, James Lau, Michael Malcolm\nUSENIX Spring ’94\nThe pioneering paper that describes the ideas and product at the heart of NetApp’s core. Based on this\nsystem, NetApp has grown into a multi-billion dollar storage company. If you’re interested in learning\nmore about its founding, read Hitz’s autobiography “How to Castrate a Bull: Unexpected Lessons on\nRisk, Growth, and Success in Business” (which is the actual title, no joking). And you thought you\ncould avoid bull castration by going into Computer Science.\n[K+08] “Parity Lost and Parity Regained”\nAndrew Krioukov, Lakshmi N. Bairavasundaram, Garth R. Goodson, Kiran Srinivasan,\nRandy Thelen, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau\nFAST ’08, San Jose, CA, February 2008\nThis work of ours, joint with colleagues at NetApp, explores how different checksum schemes work (or\ndon’t work) in protecting data. We reveal a number of interesting ﬂaws in current protection strategies,\nsome of which have led to ﬁxes in commercial products.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n538\nDATA INTEGRITY AND PROTECTION\n[M13] “Cyclic Redundancy Checks”\nAuthor Unknown\nAvailable: http://www.mathpages.com/home/kmath458.htm\nNot sure who wrote this, but a super clear and concise description of CRCs is available here. The internet\nis full of information, as it turns out.\n[P+05] “IRON File Systems”\nVijayan Prabhakaran, Lakshmi N. Bairavasundaram, Nitin Agrawal, Haryadi S. Gunawi, An-\ndrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau\nSOSP ’05, Brighton, England, October 2005\nOur paper on how disks have partial failure modes, which includes a detailed study of how ﬁle systems\nsuch as Linux ext3 and Windows NTFS react to such failures. As it turns out, rather poorly! We found\nnumerous bugs, design ﬂaws, and other oddities in this work. Some of this has fed back into the Linux\ncommunity, thus helping to yield a new more robust group of ﬁle systems to store your data.\n[RO91] “Design and Implementation of the Log-structured File System”\nMendel Rosenblum and John Ousterhout\nSOSP ’91, Paciﬁc Grove, CA, October 1991\nAnother reference to this ground-breaking paper on how to improve write performance in ﬁle systems.\n[S90] “Implementing Fault-Tolerant Services Using The State Machine Approach: A Tutorial”\nFred B. Schneider\nACM Surveys, Vol. 22, No. 4, December 1990\nThis classic paper talks generally about how to build fault tolerant services, and includes many basic\ndeﬁnitions of terms. A must read for those building distributed systems.\n[Z+13] “Zettabyte Reliability with Flexible End-to-end Data Integrity”\nYupu Zhang, Daniel S. Myers, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau\nMSST ’13, Long Beach, California, May 2013\nOur own work on adding data protection to the page cache of a system, which protects against memory\ncorruption as well as on-disk corruption.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "page_number": 567,
      "chapter_number": 55,
      "summary": "With XOR-based checksums, the checksum is computed sim-\nply by XOR’ing each chunk of the data block being checksummed, thus\nproducing a single value that represents the XOR of the entire block Key topics include checksum, disks, and data.",
      "keywords": [
        "Checksum",
        "DATA",
        "DATA INTEGRITY",
        "block",
        "disk",
        "data block",
        "system",
        "storage system",
        "stored checksum",
        "PROTECTION",
        "data protection",
        "write",
        "INTEGRITY AND PROTECTION",
        "FREE LUNCH",
        "INTEGRITY"
      ],
      "concepts": [
        "checksum",
        "disks",
        "data",
        "block",
        "systems",
        "overheads",
        "corruption",
        "corrupted",
        "storage",
        "write"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 31,
          "title": "Segment 31 (pages 291-298)",
          "relevance_score": 0.47,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 55,
          "title": "Segment 55 (pages 546-554)",
          "relevance_score": 0.46,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 15,
          "title": "Segment 15 (pages 135-143)",
          "relevance_score": 0.45,
          "method": "api"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 2,
          "title": "Segment 2 (pages 9-16)",
          "relevance_score": 0.44,
          "method": "api"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 17,
          "title": "Segment 17 (pages 155-167)",
          "relevance_score": 0.44,
          "method": "api"
        }
      ]
    },
    {
      "number": 56,
      "title": "Segment 56 (pages 575-582)",
      "start_page": 575,
      "end_page": 582,
      "detection_method": "topic_boundary",
      "content": "45\nSummary Dialogue on Persistence\nStudent: Wow, ﬁle systems seem interesting(!), and yet complicated.\nProfessor: That’s why me and my spouse do our research in this space.\nStudent: Hold on. Are you one of the professors who wrote this book? I thought\nwe were both just fake constructs, used to summarize some main points, and\nperhaps add a little levity in the study of operating systems.\nProfessor: Uh... er... maybe. And none of your business! And who did you\nthink was writing these things? (sighs) Anyhow, let’s get on with it: what did\nyou learn?\nStudent: Well, I think I got one of the main points, which is that it is much\nharder to manage data for a long time (persistently) than it is to manage data\nthat isn’t persistent (like the stuff in memory). After all, if your machines crashes,\nmemory contents disappear! But the stuff in the ﬁle system needs to live forever.\nProfessor: Well, as my friend Kevin Hultquist used to say, “Forever is a long\ntime”; while he was talking about plastic golf tees, it’s especially true for the\ngarbage that is found in most ﬁle systems.\nStudent: Well, you know what I mean! For a long time at least. And even simple\nthings, such as updating a persistent storage device, are complicated, because you\nhave to care what happens if you crash. Recovery, something I had never even\nthought of when we were virtualizing memory, is now a big deal!\nProfessor: Too true. Updates to persistent storage have always been, and re-\nmain, a fun and challenging problem.\nStudent: I also learned about cool things like disk scheduling, and about data\nprotection techniques like RAID and even checksums. That stuff is cool.\nProfessor: I like those topics too. Though, if you really get into it, they can get a\nlittle mathematical. Check out some the latest on erasure codes if you want your\nbrain to hurt.\nStudent: I’ll get right on that.\n539\n\n\n540\nSUMMARY DIALOGUE ON PERSISTENCE\nProfessor: (frowns) I think you’re being sarcastic. Well, what else did you like?\nStudent: And I also liked all the thought that has gone into building technology-\naware systems, like FFS and LFS. Neat stuff! Being disk aware seems cool. But\nwill it matter anymore, with Flash and all the newest, latest technologies?\nProfessor: Good question! And a reminder to get working on that Flash chap-\nter... (scribbles note down to self) ... But yes, even with Flash, all of this stuff\nis still relevant, amazingly. For example, Flash Translation Layers (FTLs) use\nlog-structuring internally, to improve performance and reliability of Flash-based\nSSDs. And thinking about locality is always useful. So while the technology\nmay be changing, many of the ideas we have studied will continue to be useful,\nfor a while at least.\nStudent: That’s good. I just spent all this time learning it, and I didn’t want it\nto all be for no reason!\nProfessor: Professors wouldn’t do that to you, would they?\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\n46\nA Dialogue on Distribution\nProfessor: And thus we reach our ﬁnal little piece in the world of operating\nsystems: distributed systems. Since we can’t cover much here, we’ll sneak in a\nlittle intro here in the section on persistence, and focus mostly on distributed ﬁle\nsystems. Hope that is OK!\nStudent: Sounds OK. But what is a distributed system exactly, oh glorious and\nall-knowing professor?\nProfessor: Well, I bet you know how this is going to go...\nStudent: There’s a peach?\nProfessor: Exactly! But this time, it’s far away from you, and may take some\ntime to get the peach. And there are a lot of them! Even worse, sometimes a\npeach becomes rotten. But you want to make sure that when anybody bites into\na peach, they will get a mouthful of deliciousness.\nStudent: This peach analogy is working less and less for me.\nProfessor: Come on! It’s the last one, just go with it.\nStudent: Fine.\nProfessor: So anyhow, forget about the peaches. Building distributed systems\nis hard, because things fail all the time. Messages get lost, machines go down,\ndisks corrupt data. It’s like the whole world is working against you!\nStudent: But I use distributed systems all the time, right?\nProfessor: Yes! You do. And... ?\nStudent: Well, it seems like they mostly work. After all, when I send a search\nrequest to google, it usually comes back in a snap, with some great results! Same\nthing when I use facebook, or Amazon, and so forth.\n541\n\n\n542\nA DIALOGUE ON DISTRIBUTION\nProfessor: Yes, it is amazing. And that’s despite all of those failures taking\nplace! Those companies build a huge amount of machinery into their systems so\nas to ensure that even though some machines have failed, the entire system stays\nup and running. They use a lot of techniques to do this: replication, retry, and\nvarious other tricks people have developed over time to detect and recover from\nfailures.\nStudent: Sounds interesting. Time to learn something for real?\nProfessor: It does seem so. Let’s get to work! But ﬁrst things ﬁrst ...\n(bites into peach he has been holding, which unfortunately is rotten)\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\n47\nDistributed Systems\nDistributed systems have changed the face of the world. When your web\nbrowser connects to a web server somewhere else on the planet, it is par-\nticipating in what seems to be a simple form of a client/server distributed\nsystem. When you contact a modern web service such as Google or face-\nbook, you are not just interacting with a single machine, however; be-\nhind the scenes, these complex services are built from a large collection\n(i.e., thousands) of machines, each of which cooperate to provide the par-\nticular service of the site. Thus, it should be clear what makes studying\ndistributed systems interesting. Indeed, it is worthy of an entire class;\nhere, we just introduce a few of the major topics.\nA number of new challenges arise when building a distributed system.\nThe major one we focus on is failure; machines, disks, networks, and\nsoftware all fail from time to time, as we do not (and likely, will never)\nknow how to build “perfect” components and systems. However, when\nwe build a modern web service, we’d like it to appear to clients as if it\nnever fails; how can we accomplish this task?\nTHE CRUX:\nHOW TO BUILD SYSTEMS THAT WORK WHEN COMPONENTS FAIL\nHow can we build a working system out of parts that don’t work correctly\nall the time? The basic question should remind you of some of the topics\nwe discussed in RAID storage arrays; however, the problems here tend\nto be more complex, as are the solutions.\nInterestingly, while failure is a central challenge in constructing dis-\ntributed systems, it also represents an opportunity. Yes, machines fail;\nbut the mere fact that a machine fails does not imply the entire system\nmust fail. By collecting together a set of machines, we can build a sys-\ntem that appears to rarely fail, despite the fact that its components fail\nregularly. This reality is the central beauty and value of distributed sys-\ntems, and why they underly virtually every modern web service you use,\nincluding Google, Facebook, etc.\n543\n\n\n544\nDISTRIBUTED SYSTEMS\nTIP: COMMUNICATION IS INHERENTLY UNRELIABLE\nIn virtually all circumstances, it is good to view communication as a\nfundamentally unreliable activity. Bit corruption, down or non-working\nlinks and machines, and lack of buffer space for incoming packets all lead\nto the same result: packets sometimes do not reach their destination. To\nbuild reliable services atop such unreliable networks, we must consider\ntechniques that can cope with packet loss.\nOther important issues exist as well. System performance is often crit-\nical; with a network connecting our distributed system together, system\ndesigners must often think carefully about how to accomplish their given\ntasks, trying to reduce the number of messages sent and further make\ncommunication as efﬁcient (low latency, high bandwidth) as possible.\nFinally, security is also a necessary consideration. When connecting\nto a remote site, having some assurance that the remote party is who\nthey say they are becomes a central problem. Further, ensuring that third\nparties cannot monitor or alter an on-going communication between two\nothers is also a challenge.\nIn this introduction, we’ll cover the most basic new aspect that is new\nin a distributed system: communication. Namely, how should machines\nwithin a distributed system communicate with one another? We’ll start\nwith the most basic primitives available, messages, and build a few higher-\nlevel primitives on top of them. As we said above, failure will be a central\nfocus: how should communication layers handle failures?\n47.1\nCommunication Basics\nThe central tenet of modern networking is that communication is fun-\ndamentally unreliable. Whether in the wide-area Internet, or a local-area\nhigh-speed network such as Inﬁniband, packets are regularly lost, cor-\nrupted, or otherwise do not reach their destination.\nThere are a multitude of causes for packet loss or corruption. Some-\ntimes, during transmission, some bits get ﬂipped due to electrical or other\nsimilar problems. Sometimes, an element in the system, such as a net-\nwork link or packet router or even the remote host, are somehow dam-\naged or otherwise not working correctly; network cables do accidentally\nget severed, at least sometimes.\nMore fundamental however is packet loss due to lack of buffering\nwithin a network switch, router, or endpoint. Speciﬁcally, even if we\ncould guarantee that all links worked correctly, and that all the compo-\nnents in the system (switches, routers, end hosts) were up and running as\nexpected, loss is still possible, for the following reason. Imagine a packet\narrives at a router; for the packet to be processed, it must be placed in\nmemory somewhere within the router. If many such packets arrive at\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nDISTRIBUTED SYSTEMS\n545\n// client code\nint main(int argc, char *argv[]) {\nint sd = UDP_Open(20000);\nstruct sockaddr_in addr, addr2;\nint rc = UDP_FillSockAddr(&addr, \"machine.cs.wisc.edu\", 10000);\nchar message[BUFFER_SIZE];\nsprintf(message, \"hello world\");\nrc = UDP_Write(sd, &addr, message, BUFFER_SIZE);\nif (rc > 0) {\nint rc = UDP_Read(sd, &addr2, buffer, BUFFER_SIZE);\n}\nreturn 0;\n}\n// server code\nint main(int argc, char *argv[]) {\nint sd = UDP_Open(10000);\nassert(sd > -1);\nwhile (1) {\nstruct sockaddr_in s;\nchar buffer[BUFFER_SIZE];\nint rc = UDP_Read(sd, &s, buffer, BUFFER_SIZE);\nif (rc > 0) {\nchar reply[BUFFER_SIZE];\nsprintf(reply, \"reply\");\nrc = UDP_Write(sd, &s, reply, BUFFER_SIZE);\n}\n}\nreturn 0;\n}\nFigure 47.1: Example UDP/IP Client/Server Code\nonce, it is possible that the memory within the router cannot accommo-\ndate all of the packets. The only choice the router has at that point is\nto drop one or more of the packets. This same behavior occurs at end\nhosts as well; when you send a large number of messages to a single ma-\nchine, the machine’s resources can easily become overwhelmed, and thus\npacket loss again arises.\nThus, packet loss is fundamental in networking. The question thus\nbecomes: how should we deal with it?\n47.2\nUnreliable Communication Layers\nOne simple way is this: we don’t deal with it. Because some appli-\ncations know how to deal with packet loss, it is sometimes useful to let\nthem communicate with a basic unreliable messaging layer, an example\nof the end-to-end argument one often hears about (see the Aside at end\nof chapter). One excellent example of such an unreliable layer is found\nin the UDP/IP networking stack available today on virtually all modern\nsystems. To use UDP, a process uses the sockets API in order to create a\ncommunication endpoint; processes on other machines (or on the same\nmachine) send UDP datagrams to the original process (a datagram is a\nﬁxed-sized message up to some max size).\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n546\nDISTRIBUTED SYSTEMS\nint UDP_Open(int port) {\nint sd;\nif ((sd = socket(AF_INET, SOCK_DGRAM, 0)) == -1) { return -1; }\nstruct sockaddr_in myaddr;\nbzero(&myaddr, sizeof(myaddr));\nmyaddr.sin_family\n= AF_INET;\nmyaddr.sin_port\n= htons(port);\nmyaddr.sin_addr.s_addr = INADDR_ANY;\nif (bind(sd, (struct sockaddr *) &myaddr, sizeof(myaddr)) == -1) {\nclose(sd);\nreturn -1;\n}\nreturn sd;\n}\nint UDP_FillSockAddr(struct sockaddr_in *addr, char *hostName, int port) {\nbzero(addr, sizeof(struct sockaddr_in));\naddr->sin_family = AF_INET;\n// host byte order\naddr->sin_port\n= htons(port);\n// short, network byte order\nstruct in_addr *inAddr;\nstruct hostent *hostEntry;\nif ((hostEntry = gethostbyname(hostName)) == NULL) { return -1; }\ninAddr = (struct in_addr *) hostEntry->h_addr;\naddr->sin_addr = *inAddr;\nreturn 0;\n}\nint UDP_Write(int sd, struct sockaddr_in *addr, char *buffer, int n) {\nint addrLen = sizeof(struct sockaddr_in);\nreturn sendto(sd, buffer, n, 0, (struct sockaddr *) addr, addrLen);\n}\nint UDP_Read(int sd, struct sockaddr_in *addr, char *buffer, int n) {\nint len = sizeof(struct sockaddr_in);\nreturn recvfrom(sd, buffer, n, 0, (struct sockaddr *) addr,\n(socklen_t *) &len);\nreturn rc;\n}\nFigure 47.2: A Simple UDP Library\nFigures 47.1 and 47.2 show a simple client and server built on top of\nUDP/IP. The client can send a message to the server, which then responds\nwith a reply. With this small amount of code, you have all you need to\nbegin building distributed systems!\nUDP is a great example of an unreliable communication layer. If you\nuse it, you will encounter situations where packets get lost (dropped) and\nthus do not reach their destination; the sender is never thus informed of\nthe loss. However, that does not mean that UDP does not guard against\nany failures at all. For example, UDP includes a checksum to detect some\nforms of packet corruption.\nHowever, because many applications simply want to send data to a\ndestination and not worry about packet loss, we need more. Speciﬁcally,\nwe need reliable communication on top of an unreliable network.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "page_number": 575,
      "chapter_number": 56,
      "summary": "This chapter covers segment 56 (pages 575-582). Key topics include professor, systems, and student. Student: Well, I think I got one of the main points, which is that it is much\nharder to manage data for a long time (persistently) than it is to manage data\nthat isn’t persistent (like the stuff in memory).",
      "keywords": [
        "UDP",
        "distributed systems",
        "systems",
        "int",
        "distributed",
        "Student",
        "struct sockaddr",
        "buffer",
        "addr",
        "Professor",
        "int UDP",
        "struct",
        "time",
        "sockaddr",
        "COMMUNICATION"
      ],
      "concepts": [
        "professor",
        "systems",
        "student",
        "networks",
        "communication",
        "communicate",
        "machines",
        "distribution",
        "distributed",
        "messages"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 3,
          "title": "Segment 3 (pages 18-26)",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 31,
          "title": "Segment 31 (pages 291-298)",
          "relevance_score": 0.56,
          "method": "api"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 8,
          "title": "Segment 8 (pages 62-69)",
          "relevance_score": 0.54,
          "method": "api"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 2,
          "title": "Segment 2 (pages 9-16)",
          "relevance_score": 0.53,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 51,
          "title": "Segment 51 (pages 510-517)",
          "relevance_score": 0.53,
          "method": "api"
        }
      ]
    },
    {
      "number": 57,
      "title": "Segment 57 (pages 583-591)",
      "start_page": 583,
      "end_page": 591,
      "detection_method": "topic_boundary",
      "content": "DISTRIBUTED SYSTEMS\n547\nTIP: USE CHECKSUMS FOR INTEGRITY\nChecksums are a commonly-used method to detect corruption quickly\nand effectively in modern systems. A simple checksum is addition: just\nsum up the bytes of a chunk of data; of course, many other more sophis-\nticated checksums have been created, including basic cyclic redundancy\ncodes (CRCs), the Fletcher checksum, and many others [MK09].\nIn networking, checksums are used as follows. Before sending a message\nfrom one machine to another, compute a checksum over the bytes of the\nmessage. Then send both the message and the checksum to the desti-\nnation. At the destination, the receiver computes a checksum over the\nincoming message as well; if this computed checksum matches the sent\nchecksum, the receiver can feel some assurance that the data likely did\nnot get corrupted during transmission.\nChecksums can be evaluated along a number of different axes. Effective-\nness is one primary consideration: does a change in the data lead to a\nchange in the checksum? The stronger the checksum, the harder it is for\nchanges in the data to go unnoticed. Performance is the other important\ncriterion: how costly is the checksum to compute? Unfortunately, effec-\ntiveness and performance are often at odds, meaning that checksums of\nhigh quality are often expensive to compute. Life, again, isn’t perfect.\n47.3\nReliable Communication Layers\nTo build a reliable communication layer, we need some new mech-\nanisms and techniques to handle packet loss. Let us consider a simple\nexample in which a client is sending a message to a server over an unreli-\nable connection. The ﬁrst question we must answer: how does the sender\nknow that the receiver has actually received the message?\nThe technique that we will use is known as an acknowledgment, or\nack for short. The idea is simple: the sender sends a message to the re-\nceiver; the receiver then sends a short message back to acknowledge its\nreceipt. Figure 47.3 depicts the process.\nSender\n[send message]\nReceiver\n[receive message]\n[send ack]\n[receive ack]\nFigure 47.3: Message Plus Acknowledgment\nWhen the sender receives an acknowledgment of the message, it can\nthen rest assured that the message did indeed receive the original mes-\nsage. However, what should the sender do if it does not receive an ac-\nknowledgment?\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n548\nDISTRIBUTED SYSTEMS\nSender\n[send message;\n keep copy;\n set timer]\nReceiver\n...\n (waiting for ack)\n...\n[timer goes off;\n set timer/retry]\n[receive message]\n[send ack]\n[receive ack;\n delete copy/timer off]\nFigure 47.4: Message Plus Acknowledgment: Dropped Request\nTo handle this case, we need an additional mechanism, known as a\ntimeout. When the sender sends a message, the sender now sets a timer\nto go off after some period of time. If, in that time, no acknowledgment\nhas been received, the sender concludes that the message has been lost.\nThe sender then simply performs a retry of the send, sending the same\nmessage again with hopes that this time, it will get through. For this\napproach to work, the sender must keep a copy of the message around,\nin case it needs to send it again. The combination of the timeout and\nthe retry have led some to call the approach timeout/retry; pretty clever\ncrowd, those networking types, no? Figure 47.4 shows an example.\nUnfortunately, timeout/retry in this form is not quite enough. Figure\n47.5 shows an example of packet loss which could lead to trouble. In this\nexample, it is not the original message that gets lost, but the acknowledg-\nment. From the perspective of the sender, the situation seems the same:\nno ack was received, and thus a timeout and retry are in order. But from\nthe perspective of the receiver, it is quite different: now the same message\nhas been received twice! While there may be cases where this is OK, in\ngeneral it is not; imagine what would happen when you are downloading\na ﬁle and extra packets are repeated inside the download. Thus, when we\nare aiming for a reliable message layer, we also usually want to guarantee\nthat each message is received exactly once by the receiver.\nTo enable the receiver to detect duplicate message transmission, the\nsender has to identify each message in some unique way, and the receiver\nneeds some way to track whether it has already seen each message be-\nfore. When the receiver sees a duplicate transmission, it simply acks the\nmessage, but (critically) does not pass the message to the application that\nreceives the data. Thus, the sender receives the ack but the message is not\nreceived twice, preserving the exactly-once semantics mentioned above.\nThere are myriad ways to detect duplicate messages. For example, the\nsender could generate a unique ID for each message; the receiver could\ntrack every ID it has ever seen. This approach could work, but it is pro-\nhibitively costly, requiring unbounded memory to track all IDs.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nDISTRIBUTED SYSTEMS\n549\nSender\n[send message;\n keep copy;\n set timer]\nReceiver\n[receive message]\n[send ack]\n...\n (waiting for ack)\n...\n[timer goes off;\n set timer/retry]\n[receive message]\n[send ack]\n[receive ack;\n delete copy/timer off]\nFigure 47.5: Message Plus Acknowledgment: Dropped Reply\nA simpler approach, requiring little memory, solves this problem, and\nthe mechanism is known as a sequence counter. With a sequence counter,\nthe sender and receiver agree upon a start value (e.g., 1) for a counter\nthat each side will maintain. Whenever a message is sent, the current\nvalue of the counter is sent along with the message; this counter value\n(N) serves as an ID for the message. After the message is sent, the sender\nthen increments the value (to N + 1).\nThe receiver uses its counter value as the expected value for the ID\nof the incoming message from that sender. If the ID of a received mes-\nsage (N) matches the receiver’s counter (also N), it acks the message and\npasses it up to the application; in this case, the receiver concludes this\nis the ﬁrst time this message has been received. The receiver then incre-\nments its counter (to N + 1), and waits for the next message.\nIf the ack is lost, the sender will timeout and re-send message N. This\ntime, the receiver’s counter is higher (N +1), and thus the receiver knows\nit has already received this message. Thus it acks the message but does\nnot pass it up to the application. In this simple manner, sequence counters\ncan be used to avoid duplicates.\nThe most commonly used reliable communication layer is known as\nTCP/IP, or just TCP for short. TCP has a great deal more sophistication\nthan we describe above, including machinery to handle congestion in the\nnetwork [VJ90], multiple outstanding requests, and hundreds of other\nsmall tweaks and optimizations. Read more about it if you’re curious;\nbetter yet, take a networking course and learn that material well.\n47.4\nCommunication Abstractions\nGiven a basic messaging layer, we now approach the next question\nin this chapter: what abstraction of communication should we use when\nbuilding a distributed system?\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n550\nDISTRIBUTED SYSTEMS\nTIP: BE CAREFUL SETTING THE TIMEOUT VALUE\nAs you can probably guess from the discussion, setting the timeout value\ncorrectly is an important aspect of using timeouts to retry message sends.\nIf the timeout is too small, the sender will re-send messages needlessly,\nthus wasting CPU time on the sender and network resources. If the time-\nout is too large, the sender waits too long to re-send and thus perceived\nperformance at the sender is reduced. The “right” value, from the per-\nspective of a single client and server, is thus to wait just long enough to\ndetect packet loss but no longer.\nHowever, there are often more than just a single client and server in a\ndistributed system, as we will see in future chapters. In a scenario with\nmany clients sending to a single server, packet loss at the server may be\nan indicator that the server is overloaded. If true, clients might retry in\na different adaptive manner; for example, after the ﬁrst timeout, a client\nmight increase its timeout value to a higher amount, perhaps twice as\nhigh as the original value. Such an exponential back-off scheme, pio-\nneered in the early Aloha network and adopted in early Ethernet [A70],\navoid situations where resources are being overloaded by an excess of\nre-sends. Robust systems strive to avoid overload of this nature.\nThe systems community developed a number of approaches over the\nyears. One body of work took OS abstractions and extended them to\noperate in a distributed environment. For example, distributed shared\nmemory (DSM) systems enable processes on different machines to share\na large, virtual address space [LH89]. This abstraction turns a distributed\ncomputation into something that looks like a multi-threaded application;\nthe only difference is that these threads run on different machines instead\nof different processors within the same machine.\nThe way most DSM systems work is through the virtual memory sys-\ntem of the OS. When a page is accessed on one machine, two things can\nhappen. In the ﬁrst (best) case, the page is already local on the machine,\nand thus the data is fetched quickly. In the second case, the page is cur-\nrently on some other machine. A page fault occurs, and the page fault\nhandler sends a message to some other machine to fetch the page, install\nit in the page table of the requesting process, and continue execution.\nThis approach is not widely in use today for a number of reasons. The\nlargest problem for DSM is how it handles failure. Imagine, for example,\nif a machine fails; what happens to the pages on that machine? What if\nthe data structures of the distributed computation are spread across the\nentire address space? In this case, parts of these data structures would\nsuddenly become unavailable. Dealing with failure when parts of your\naddress space go missing is hard; imagine a linked list that where a next\npointer points into a portion of the address space that is gone. Yikes!\nA further problem is performance. One usually assumes, when writ-\ning code, that access to memory is cheap. In DSM systems, some accesses\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nDISTRIBUTED SYSTEMS\n551\nare inexpensive, but others cause page faults and expensive fetches from\nremote machines. Thus, programmers of such DSM systems had to be\nvery careful to organize computations such that almost no communica-\ntion occurred at all, defeating much of the point of such an approach.\nThough much research was performed in this space, there was little prac-\ntical impact; nobody builds reliable distributed systems using DSM today.\n47.5\nRemote Procedure Call (RPC)\nWhile OS abstractions turned out to be a poor choice for building dis-\ntributed systems, programming language (PL) abstractions make much\nmore sense. The most dominant abstraction is based on the idea of a re-\nmote procedure call, or RPC for short [BN84]1.\nRemote procedure call packages all have a simple goal: to make the\nprocess of executing code on a remote machine as simple and straight-\nforward as calling a local function. Thus, to a client, a procedure call is\nmade, and some time later, the results are returned. The server simply\ndeﬁnes some routines that it wishes to export. The rest of the magic is\nhandled by the RPC system, which in general has two pieces: a stub gen-\nerator (sometimes called a protocol compiler), and the run-time library.\nWe’ll now take a look at each of these pieces in more detail.\nStub Generator\nThe stub generator’s job is simple: to remove some of the pain of packing\nfunction arguments and results into messages by automating it. Numer-\nous beneﬁts arise: one avoids, by design, the simple mistakes that occur\nin writing such code by hand; further, a stub compiler can perhaps opti-\nmize such code and thus improve performance.\nThe input to such a compiler is simply the set of calls a server wishes\nto export to clients. Conceptually, it could be something as simple as this:\ninterface {\nint func1(int arg1);\nint func2(int arg1, int arg2);\n};\nThe stub generator takes an interface like this and generates a few dif-\nferent pieces of code. For the client, a client stub is generated, which\ncontains each of the functions speciﬁed in the interface; a client program\nwishing to use this RPC service would link with this client stub and call\ninto it in order to make RPCs.\nInternally, each of these functions in the client stub do all of the work\nneeded to perform the remote procedure call. To the client, the code just\n1In modern programming languages, we might instead say remote method invocation\n(RMI), but who likes these languages anyhow, with all of their fancy objects?\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n552\nDISTRIBUTED SYSTEMS\nappears as a function call (e.g., the client calls func1(x)); internally, the\ncode in the client stub for func1() does this:\n• Create a message buffer. A message buffer is usually just a con-\ntiguous array of bytes of some size.\n• Pack the needed information into the message buffer. This infor-\nmation includes some kind of identiﬁer for the function to be called,\nas well as all of the arguments that the function needs (e.g., in our\nexample above, one integer for func1). The process of putting all\nof this information into a single contiguous buffer is sometimes re-\nferred to as the marshaling of arguments or the serialization of the\nmessage.\n• Send the message to the destination RPC server. The communi-\ncation with the RPC server, and all of the details required to make\nit operate correctly, are handled by the RPC run-time library, de-\nscribed further below.\n• Wait for the reply. Because function calls are usually synchronous,\nthe call will wait for its completion.\n• Unpack return code and other arguments. If the function just re-\nturns a single return code, this process is straightforward; however,\nmore complex functions might return more complex results (e.g., a\nlist), and thus the stub might need to unpack those as well. This\nstep is also known as unmarshaling or deserialization.\n• Return to the caller. Finally, just return from the client stub back\ninto the client code.\nFor the server, code is also generated. The steps taken on the server\nare as follows:\n• Unpack the message. This step, called unmarshaling or deserial-\nization, takes the information out of the incoming message. The\nfunction identiﬁer and arguments are extracted.\n• Call into the actual function. Finally! We have reached the point\nwhere the remote function is actually executed. The RPC runtime\ncalls into the function speciﬁed by the ID and passes in the desired\narguments.\n• Package the results. The return argument(s) are marshaled back\ninto a single reply buffer.\n• Send the reply. The reply is ﬁnally sent to the caller.\nThere are a few other important issues to consider in a stub compiler.\nThe ﬁrst is complex arguments, i.e., how does one package and send\na complex data structure? For example, when one calls the write()\nsystem call, one passes in three arguments: an integer ﬁle descriptor, a\npointer to a buffer, and a size indicating how many bytes (starting at the\npointer) are to be written. If an RPC package is passed a pointer, it needs\nto be able to ﬁgure out how to interpret that pointer, and perform the\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nDISTRIBUTED SYSTEMS\n553\ncorrect action. Usually this is accomplished through either well-known\ntypes (e.g., a buffer t that is used to pass chunks of data given a size,\nwhich the RPC compiler understands), or by annotating the data struc-\ntures with more information, enabling the compiler to know which bytes\nneed to be serialized.\nAnother important issue is the organization of the server with regards\nto concurrency. A simple server just waits for requests in a simple loop,\nand handles each request one at a time. However, as you might have\nguessed, this can be grossly inefﬁcient; if one RPC call blocks (e.g., on\nI/O), server resources are wasted. Thus, most servers are constructed in\nsome sort of concurrent fashion. A common organization is a thread pool.\nIn this organization, a ﬁnite set of threads are created when the server\nstarts; when a message arrives, it is dispatched to one of these worker\nthreads, which then does the work of the RPC call, eventually replying;\nduring this time, a main thread keeps receiving other requests, and per-\nhaps dispatching them to other workers. Such an organization enables\nconcurrent execution within the server, thus increasing its utilization; the\nstandard costs arise as well, mostly in programming complexity, as the\nRPC calls may now need to use locks and other synchronization primi-\ntives in order to ensure their correct operation.\nRun-Time Library\nThe run-time library handles much of the heavy lifting in an RPC system;\nmost performance and reliability issues are handled herein. We’ll now\ndiscuss some of the major challenges in building such a run-time layer.\nOne of the ﬁrst challenges we must overcome is how to locate a re-\nmote service. This problem, of naming, is a common one in distributed\nsystems, and in some sense goes beyond the scope of our current discus-\nsion. The simplest of approaches build on existing naming systems, e.g.,\nhostnames and port numbers provided by current internet protocols. In\nsuch a system, the client must know the hostname or IP address of the\nmachine running the desired RPC service, as well as the port number it is\nusing (a port number is just a way of identifying a particular communica-\ntion activity taking place on a machine, allowing multiple communication\nchannels at once). The protocol suite must then provide a mechanism to\nroute packets to a particular address from any other machine in the sys-\ntem. For a good discussion of naming, read either the Grapevine paper\nor about DNS and name resolution on the Internet, or better yet just read\nthe excellent chapter in Saltzer and Kaashoek’s book [SK09].\nOnce a client knows which server it should talk to for a particular re-\nmote service, the next question is which transport-level protocol should\nRPC be built upon. Speciﬁcally, should the RPC system use a reliable pro-\ntocol such as TCP/IP, or be built upon an unreliable communication layer\nsuch as UDP/IP?\nNaively the choice would seem easy: clearly we would like for a re-\nquest to be reliably delivered to the remote server, and clearly we would\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n554\nDISTRIBUTED SYSTEMS\nlike to reliably receive a reply. Thus we should choose the reliable trans-\nport protocol such as TCP, right?\nUnfortunately, building RPC on top of a reliable communication layer\ncan lead to a major inefﬁciency in performance. Recall from the discus-\nsion above how reliable communication layers work: with acknowledg-\nments plus timeout/retry. Thus, when the client sends an RPC request\nto the server, the server responds with an acknowledgment so that the\ncaller knows the request was received. Similarly, when the server sends\nthe reply to the client, the client acks it so that the server knows it was\nreceived. By building a request/response protocol (such as RPC) on top\nof a reliable communication layer, two “extra” messages are sent.\nFor this reason, many RPC packages are built on top of unreliable com-\nmunication layers, such as UDP. Doing so enables a more efﬁcient RPC\nlayer, but does add the responsibility of providing reliability to the RPC\nsystem. The RPC layer achieves the desired level of responsibility by us-\ning timeout/retry and acknowledgments much like we described above.\nBy using some form of sequence numbering, the communication layer\ncan guarantee that each RPC takes place exactly once (in the case of no\nfailure), or at most once (in the case where failure arises).\nOther Issues\nThere are some other issues an RPC run-time must handle as well. For\nexample, what happens when a remote call takes a long time to com-\nplete? Given our timeout machinery, a long-running remote call might\nappear as a failure to a client, thus triggering a retry, and thus the need\nfor some care here. One solution is to use an explicit acknowledgment\n(from the receiver to sender) when the reply isn’t immediately generated;\nthis lets the client know the server received the request. Then, after some\ntime has passed, the client can periodically ask whether the server is still\nworking on the request; if the server keeps saying “yes”, the client should\nbe happy and continue to wait (after all, sometimes a procedure call can\ntake a long time to ﬁnish executing).\nThe run-time must also handle procedure calls with large arguments,\nlarger than what can ﬁt into a single packet. Some lower-level network\nprotocols provide such sender-side fragmentation (of larger packets into\na set of smaller ones) and receiver-side reassembly (of smaller parts into\none larger logical whole); if not, the RPC run-time may have to implement\nsuch functionality itself. See Birrell and Nelson’s excellent RPC paper for\ndetails [BN84].\nOne issue that many systems handle is that of byte ordering. As you\nmay know, some machines store values in what is known as big endian\nordering, whereas others use little endian ordering. Big endian stores\nbytes (say, of an integer) from most signiﬁcant to least signiﬁcant bits,\nmuch like Arabic numerals; little endian does the opposite. Both are\nequally valid ways of storing numeric information; the question here is\nhow to communicate between machines of different endianness.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nDISTRIBUTED SYSTEMS\n555\nAside: The End-to-End Argument\nThe end-to-end argument makes the case that the highest level in a\nsystem, i.e., usually the application at “the end”, is ultimately the only\nlocale within a layered system where certain functionality can truly be\nimplemented. In their landmark paper, Saltzer et al. argue this through\nan excellent example: reliable ﬁle transfer between two machines. If you\nwant to transfer a ﬁle from machine A to machine B, and make sure that\nthe bytes that end up on B are exactly the same as those that began on\nA, you must have an “end-to-end” check of this; lower-level reliable ma-\nchinery, e.g., in the network or disk, provides no such guarantee.\nThe contrast is an approach which tries to solve the reliable-ﬁle-\ntransfer problem by adding reliability to lower layers of the system. For\nexample, say we build a reliable communication protocol and use it to\nbuild our reliable ﬁle transfer. The communication protocol guarantees\nthat every byte sent by a sender will be received in order by the receiver,\nsay using timeout/retry, acknowledgments, and sequence numbers. Un-\nfortunately, using such a protocol does not a reliable ﬁle transfer make;\nimagine the bytes getting corrupted in sender memory before the com-\nmunication even takes place, or something bad happening when the re-\nceiver writes the data to disk. In those cases, even though the bytes were\ndelivered reliably across the network, our ﬁle transfer was ultimately\nnot reliable. To build a reliable ﬁle transfer, one must include end-to-\nend checks of reliability, e.g., after the entire transfer is complete, read\nback the ﬁle on the receiver disk, compute a checksum, and compare that\nchecksum to that of the ﬁle on the sender.\nThe corollary to this maxim is that sometimes having lower layers pro-\nvide extra functionality can indeed improve system performance or oth-\nerwise optimize a system. Thus, you should not rule out having such\nmachinery at a lower-level in a system; rather, you should carefully con-\nsider the utility of such machinery, given its eventual usage in an overall\nsystem or application.\nRPC packages often handle this by providing a well-deﬁned endian-\nness within their message formats. In Sun’s RPC package, the XDR (eX-\nternal Data Representation) layer provides this functionality. If the ma-\nchine sending or receiving a message matches the endianness of XDR,\nmessages are just sent and received as expected. If, however, the machine\ncommunicating has a different endianness, each piece of information in\nthe message must be converted. Thus, the difference in endianness can\nhave a small performance cost.\nA ﬁnal issue is whether to expose the asynchronous nature of com-\nmunication to clients, thus enabling some performance optimizations.\nSpeciﬁcally, typical RPCs are made synchronously, i.e., when a client\nissues the procedure call, it must wait for the procedure call to return\nbefore continuing. Because this wait can be long, and because the client\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "page_number": 583,
      "chapter_number": 57,
      "summary": "The technique that we will use is known as an acknowledgment, or\nack for short Key topics include received, receive, and receives. A simple checksum is addition: just\nsum up the bytes of a chunk of data; of course, many other more sophis-\nticated checksums have been created, including basic cyclic redundancy\ncodes (CRCs), the Fletcher checksum, and many others [MK09].",
      "keywords": [
        "message",
        "DISTRIBUTED SYSTEMS",
        "RPC",
        "RPC system",
        "SYSTEMS",
        "sender",
        "client",
        "server",
        "receiver",
        "DISTRIBUTED",
        "call",
        "send message",
        "receive message",
        "checksum",
        "DSM systems"
      ],
      "concepts": [
        "received",
        "receive",
        "receives",
        "message",
        "messaging",
        "systems",
        "reliable",
        "reliability",
        "reliably",
        "server"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 36,
          "title": "Segment 36 (pages 341-350)",
          "relevance_score": 0.46,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 32,
          "title": "Segment 32 (pages 299-308)",
          "relevance_score": 0.45,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 3",
          "chapter": 56,
          "title": "Segment 56 (pages 532-540)",
          "relevance_score": 0.45,
          "method": "api"
        },
        {
          "book": "Building Python Microservices with FastAPI",
          "chapter": 32,
          "title": "Segment 32 (pages 292-300)",
          "relevance_score": 0.44,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 4,
          "title": "Segment 4 (pages 27-35)",
          "relevance_score": 0.44,
          "method": "api"
        }
      ]
    },
    {
      "number": 58,
      "title": "Segment 58 (pages 592-602)",
      "start_page": 592,
      "end_page": 602,
      "detection_method": "topic_boundary",
      "content": "556\nDISTRIBUTED SYSTEMS\nmay have other work it could be doing, some RPC packages enable you\nto invoke an RPC asynchronously. When an asynchronous RPC is is-\nsued, the RPC package sends the request and returns immediately; the\nclient is then free to do other work, such as call other RPCs or other use-\nful computation. The client at some point will want to see the results of\nthe asynchronous RPC; it thus calls back into the RPC layer, telling it to\nwait for outstanding RPCs to complete, at which point return arguments\ncan be accessed.\n47.6\nSummary\nWe have seen the introduction of a new topic, distributed systems, and\nits major issue: how to handle failure which is now a commonplace event.\nAs they say inside of Google, when you have just your desktop machine,\nfailure is rare; when you’re in a data center with thousands of machines,\nfailure is happening all the time. The key to any distributed system is\nhow you deal with that failure.\nWe have also seen that communication forms the heart of any dis-\ntributed system. A common abstraction of that communication is found\nin remote procedure call (RPC), which enables clients to make remote\ncalls on servers; the RPC package handles all of the gory details, includ-\ning timeout/retry and acknowledgment, in order to deliver a service that\nclosely mirrors a local procedure call.\nThe best way to really understand an RPC package is of course to use\none yourself. Sun’s RPC system, using the stub compiler rpcgen, is a\ncommon one, and is widely available on systems today, including Linux.\nTry it out, and see what all the fuss is about.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nDISTRIBUTED SYSTEMS\n557\nReferences\n[A70] “The ALOHA System – Another Alternative for Computer Communications”\nNorman Abramson\nThe 1970 Fall Joint Computer Conference\nThe ALOHA network pioneered some basic concepts in networking, including exponential back-off and\nretransmit, which formed the basis for communication in shared-bus Ethernet networks for years.\n[BN84] “Implementing Remote Procedure Calls”\nAndrew D. Birrell, Bruce Jay Nelson\nACM TOCS, Volume 2:1, February 1984\nThe foundational RPC system upon which all others build. Yes, another pioneering effort from our\nfriends at Xerox PARC.\n[MK09] “The Effectiveness of Checksums for Embedded Control Networks”\nTheresa C. Maxino and Philip J. Koopman\nIEEE Transactions on Dependable and Secure Computing, 6:1, January ’09\nA nice overview of basic checksum machinery and some performance and robustness comparisons be-\ntween them.\n[LH89] “Memory Coherence in Shared Virtual Memory Systems”\nKai Li and Paul Hudak\nACM TOCS, 7:4, November 1989\nThe introduction of software-based shared memory via virtual memory. An intriguing idea for sure, but\nnot a lasting or good one in the end.\n[SK09] “Principles of Computer System Design”\nJerome H. Saltzer and M. Frans Kaashoek\nMorgan-Kaufmann, 2009\nAn excellent book on systems, and a must for every bookshelf. One of the few terriﬁc discussions on\nnaming we’ve seen.\n[SRC84] “End-To-End Arguments in System Design”\nJerome H. Saltzer, David P. Reed, David D. Clark\nACM TOCS, 2:4, November 1984\nA beautiful discussion of layering, abstraction, and where functionality must ultimately reside in com-\nputer systems.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n48\nSun’s Network File System (NFS)\nOne of the ﬁrst uses of distributed client/server computing was in the\nrealm of distributed ﬁle systems. In such an environment, there are a\nnumber of client machines and one server (or a few); the server stores the\ndata on its disks, and clients request data through well-formed protocol\nmessages. Figure 48.1 depicts the basic setup.\nClient 0\nClient 1\nClient 2\nClient 3\nServer\nNetwork\nFigure 48.1: A Generic Client/Server System\nAs you can see from the picture, the server has the disks, and clients\nsend messages network to access their directories and ﬁles on those disks.\nWhy do we bother with this arrangement? (i.e., why don’t we just let\nclients use their local disks?) Well, primarily this setup allows for easy\nsharing of data across clients. Thus, if you access a ﬁle on one machine\n(Client 0) and then later use another (Client 2), you will have the same\nview of the ﬁle system. Your data is naturally shared across these dif-\nferent machines. A secondary beneﬁt is centralized administration; for\nexample, backing up ﬁles can be done from the few server machines in-\nstead of from the multitude of clients. Another advantage could be secu-\nrity; having all servers in a locked machine room prevents certain types\nof problems from arising.\n559\n\n\n560\nSUN’S NETWORK FILE SYSTEM (NFS)\nCRUX: HOW TO BUILD A DISTRIBUTED FILE SYSTEM\nHow do you build a distributed ﬁle system? What are the key aspects\nto think about? What is easy to get wrong? What can we learn from\nexisting systems?\n48.1\nA Basic Distributed File System\nWe now will study the architecture of a simpliﬁed distributed ﬁle sys-\ntem. A simple client/server distributed ﬁle system has more components\nthan the ﬁle systems we have studied so far. On the client side, there are\nclient applications which access ﬁles and directories through the client-\nside ﬁle system. A client application issues system calls to the client-side\nﬁle system (such as open(), read(), write(), close(), mkdir(),\netc.) in order to access ﬁles which are stored on the server. Thus, to client\napplications, the ﬁle system does not appear to be any different than a lo-\ncal (disk-based) ﬁle system, except perhaps for performance; in this way,\ndistributed ﬁle systems provide transparent access to ﬁles, an obvious\ngoal; after all, who would want to use a ﬁle system that required a differ-\nent set of APIs or otherwise was a pain to use?\nThe role of the client-side ﬁle system is to execute the actions needed\nto service those system calls. For example, if the client issues a read()\nrequest, the client-side ﬁle system may send a message to the server-side\nﬁle system (or, more commonly, the ﬁle server) to read a particular block;\nthe ﬁle server will then read the block from disk (or its own in-memory\ncache), and send a message back to the client with the requested data.\nThe client-side ﬁle system will then copy the data into the user buffer\nsupplied to the read() system call and thus the request will complete.\nNote that a subsequent read() of the same block on the client may be\ncached in client memory or on the client’s disk even; in the best such case,\nno network trafﬁc need be generated.\nClient Application\nClient-side File System\nNetworking Layer\nFile Server\nNetworking Layer\nDisks\nFigure 48.2: Distributed File System Architecture\nFrom this simple overview, you should get a sense that there are two\nimportant pieces of software in a client/server distributed ﬁle system: the\nclient-side ﬁle system and the ﬁle server. Together their behavior deter-\nmines the behavior of the distributed ﬁle system. Now it’s time to study\none particular system: Sun’s Network File System (NFS).\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nSUN’S NETWORK FILE SYSTEM (NFS)\n561\nASIDE: WHY SERVERS CRASH\nBefore getting into the details of the NFSv2 protocol, you might be\nwondering: why do servers crash? Well, as you might guess, there are\nplenty of reasons. Servers may simply suffer from a power outage (tem-\nporarily); only when power is restored can the machines be restarted.\nServers are often comprised of hundreds of thousands or even millions\nof lines of code; thus, they have bugs (even good software has a few\nbugs per hundred or thousand lines of code), and thus they eventually\nwill trigger a bug that will cause them to crash. They also have memory\nleaks; even a small memory leak will cause a system to run out of mem-\nory and crash. And, ﬁnally, in distributed systems, there is a network\nbetween the client and the server; if the network acts strangely (for ex-\nample, if it becomes partitioned and clients and servers are working but\ncannot communicate), it may appear as if a remote machine has crashed,\nbut in reality it is just not currently reachable through the network.\n48.2\nOn To NFS\nOne of the earliest and quite successful distributed systems was devel-\noped by Sun Microsystems, and is known as the Sun Network File Sys-\ntem (or NFS) [S86]. In deﬁning NFS, Sun took an unusual approach: in-\nstead of building a proprietary and closed system, Sun instead developed\nan open protocol which simply speciﬁed the exact message formats that\nclients and servers would use to communicate. Different groups could\ndevelop their own NFS servers and thus compete in an NFS marketplace\nwhile preserving interoperability. It worked: today there are many com-\npanies that sell NFS servers (including Oracle/Sun, NetApp [HLM94],\nEMC, IBM, and others), and the widespread success of NFS is likely at-\ntributed to this “open market” approach.\n48.3\nFocus: Simple and Fast Server Crash Recovery\nIn this chapter, we will discuss the classic NFS protocol (version 2,\na.k.a. NFSv2), which was the standard for many years; small changes\nwere made in moving to NFSv3, and larger-scale protocol changes were\nmade in moving to NFSv4. However, NFSv2 is both wonderful and frus-\ntrating and thus serves as our focus.\nIn NFSv2, the main goal in the design of the protocol was simple and\nfast server crash recovery. In a multiple-client, single-server environment,\nthis goal makes a great deal of sense; any minute that the server is down\n(or unavailable) makes all the client machines (and their users) unhappy\nand unproductive. Thus, as the server goes, so goes the entire system.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n562\nSUN’S NETWORK FILE SYSTEM (NFS)\n48.4\nKey To Fast Crash Recovery: Statelessness\nThis simple goal is realized in NFSv2 by designing what we refer to\nas a stateless protocol. The server, by design, does not keep track of any-\nthing about what is happening at each client. For example, the server\ndoes not know which clients are caching which blocks, or which ﬁles are\ncurrently open at each client, or the current ﬁle pointer position for a ﬁle,\netc. Simply put, the server does not track anything about what clients are\ndoing; rather, the protocol is designed to deliver in each protocol request\nall the information that is needed in order to complete the request. If it\ndoesn’t now, this stateless approach will make more sense as we discuss\nthe protocol in more detail below.\nFor an example of a stateful (not stateless) protocol, consider the open()\nsystem call. Given a pathname, open() returns a ﬁle descriptor (an inte-\nger). This descriptor is used on subsequent read() or write() requests\nto access various ﬁle blocks, as in this application code (note that proper\nerror checking of the system calls is omitted for space reasons):\nchar buffer[MAX];\nint fd = open(\"foo\", O_RDONLY); // get descriptor \"fd\"\nread(fd, buffer, MAX);\n// read MAX bytes from foo (via fd)\nread(fd, buffer, MAX);\n// read MAX bytes from foo\n...\nread(fd, buffer, MAX);\n// read MAX bytes from foo\nclose(fd);\n// close file\nFigure 48.3: Client Code: Reading From A File\nNow imagine that the client-side ﬁle system opens the ﬁle by sending\na protocol message to the server saying “open the ﬁle ’foo’ and give me\nback a descriptor”. The ﬁle server then opens the ﬁle locally on its side\nand sends the descriptor back to the client. On subsequent reads, the\nclient application uses that descriptor to call the read() system call; the\nclient-side ﬁle system then passes the descriptor in a message to the ﬁle\nserver, saying “read some bytes from the ﬁle that is referred to by the\ndescriptor I am passing you here”.\nIn this example, the ﬁle descriptor is a piece of shared state between\nthe client and the server (Ousterhout calls this distributed state [O91]).\nShared state, as we hinted above, complicates crash recovery. Imagine\nthe server crashes after the ﬁrst read completes, but before the client\nhas issued the second one. After the server is up and running again,\nthe client then issues the second read. Unfortunately, the server has no\nidea to which ﬁle fd is referring; that information was ephemeral (i.e.,\nin memory) and thus lost when the server crashed. To handle this situa-\ntion, the client and server would have to engage in some kind of recovery\nprotocol, where the client would make sure to keep enough information\naround in its memory to be able to tell the server what it needs to know\n(in this case, that ﬁle descriptor fd refers to ﬁle foo).\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nSUN’S NETWORK FILE SYSTEM (NFS)\n563\nIt gets even worse when you consider the fact that a stateful server has\nto deal with client crashes. Imagine, for example, a client that opens a ﬁle\nand then crashes. The open() uses up a ﬁle descriptor on the server; how\ncan the server know it is OK to close a given ﬁle? In normal operation, a\nclient would eventually call close() and thus inform the server that the\nﬁle should be closed. However, when a client crashes, the server never\nreceives a close(), and thus has to notice the client has crashed in order\nto close the ﬁle.\nFor these reasons, the designers of NFS decided to pursue a stateless\napproach: each client operation contains all the information needed to\ncomplete the request. No fancy crash recovery is needed; the server just\nstarts running again, and a client, at worst, might have to retry a request.\n48.5\nThe NFSv2 Protocol\nWe thus arrive at the NFSv2 protocol deﬁnition. Our problem state-\nment is simple:\nTHE CRUX: HOW TO DEFINE A STATELESS FILE PROTOCOL\nHow can we deﬁne the network protocol to enable stateless operation?\nClearly, stateful calls like open() can’t be a part of the discussion (as it\nwould require the server to track open ﬁles); however, the client appli-\ncation will want to call open(), read(), write(), close() and other\nstandard API calls to access ﬁles and directories. Thus, as a reﬁned ques-\ntion, how do we deﬁne the protocol to both be stateless and support the\nPOSIX ﬁle system API?\nOne key to understanding the design of the NFS protocol is under-\nstanding the ﬁle handle. File handles are used to uniquely describe the\nﬁle or directory a particular operation is going to operate upon; thus,\nmany of the protocol requests include a ﬁle handle.\nYou can think of a ﬁle handle as having three important components: a\nvolume identiﬁer, an inode number, and a generation number; together, these\nthree items comprise a unique identiﬁer for a ﬁle or directory that a client\nwishes to access. The volume identiﬁer informs the server which ﬁle sys-\ntem the request refers to (an NFS server can export more than one ﬁle\nsystem); the inode number tells the server which ﬁle within that partition\nthe request is accessing. Finally, the generation number is needed when\nreusing an inode number; by incrementing it whenever an inode num-\nber is reused, the server ensures that a client with an old ﬁle handle can’t\naccidentally access the newly-allocated ﬁle.\nHere is a summary of some of the important pieces of the protocol; the\nfull protocol is available elsewhere (see Callaghan’s book for an excellent\nand detailed overview of NFS [C00]).\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n564\nSUN’S NETWORK FILE SYSTEM (NFS)\nNFSPROC_GETATTR\nexpects: file handle\nreturns: attributes\nNFSPROC_SETATTR\nexpects: file handle, attributes\nreturns: nothing\nNFSPROC_LOOKUP\nexpects: directory file handle, name of file/directory to look up\nreturns: file handle\nNFSPROC_READ\nexpects: file handle, offset, count\nreturns: data, attributes\nNFSPROC_WRITE\nexpects: file handle, offset, count, data\nreturns: attributes\nNFSPROC_CREATE\nexpects: directory file handle, name of file, attributes\nreturns: nothing\nNFSPROC_REMOVE\nexpects: directory file handle, name of file to be removed\nreturns: nothing\nNFSPROC_MKDIR\nexpects: directory file handle, name of directory, attributes\nreturns: file handle\nNFSPROC_RMDIR\nexpects: directory file handle, name of directory to be removed\nreturns: nothing\nNFSPROC_READDIR\nexpects: directory handle, count of bytes to read, cookie\nreturns: directory entries, cookie (to get more entries)\nFigure 48.4: The NFS Protocol: Examples\nWe brieﬂy highlight the important components of the protocol. First,\nthe LOOKUP protocol message is used to obtain a ﬁle handle, which is\nthen subsequently used to access ﬁle data. The client passes a directory\nﬁle handle and name of a ﬁle to look up, and the handle to that ﬁle (or\ndirectory) plus its attributes are passed back to the client from the server.\nFor example, assume the client already has a directory ﬁle handle for\nthe root directory of a ﬁle system (/) (indeed, this would be obtained\nthrough the NFS mount protocol, which is how clients and servers ﬁrst\nare connected together; we do not discuss the mount protocol here for\nsake of brevity). If an application running on the client opens the ﬁle\n/foo.txt, the client-side ﬁle system sends a lookup request to the server,\npassing it the root ﬁle handle and the name foo.txt; if successful, the\nﬁle handle (and attributes) for foo.txt will be returned.\nIn case you are wondering, attributes are just the metadata that the ﬁle\nsystem tracks about each ﬁle, including ﬁelds such as ﬁle creation time,\nlast modiﬁcation time, size, ownership and permissions information, and\nso forth, i.e., the same type of information that you would get back if you\ncalled stat() on a ﬁle.\nOnce a ﬁle handle is available, the client can issue READ and WRITE\nprotocol messages on a ﬁle to read or write the ﬁle, respectively. The\nREAD protocol message requires the protocol to pass along the ﬁle handle\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nSUN’S NETWORK FILE SYSTEM (NFS)\n565\nof the ﬁle along with the offset within the ﬁle and number of bytes to read.\nThe server then will be able to issue the read (after all, the handle tells the\nserver which volume and which inode to read from, and the offset and\ncount tells it which bytes of the ﬁle to read) and return the data to the\nclient (or an error if there was a failure). WRITE is handled similarly,\nexcept the data is passed from the client to the server, and just a success\ncode is returned.\nOne last interesting protocol message is the GETATTR request; given a\nﬁle handle, it simply fetches the attributes for that ﬁle, including the last\nmodiﬁed time of the ﬁle. We will see why this protocol request is impor-\ntant in NFSv2 below when we discuss caching (can you guess why?).\n48.6\nFrom Protocol to Distributed File System\nHopefully you are now getting some sense of how this protocol is\nturned into a ﬁle system across the client-side ﬁle system and the ﬁle\nserver. The client-side ﬁle system tracks open ﬁles, and generally trans-\nlates application requests into the relevant set of protocol messages. The\nserver simply responds to each protocol message, each of which has all\nthe information needed to complete request.\nFor example, let us consider a simple application which reads a ﬁle.\nIn the diagram (Figure 48.1), we show what system calls the application\nmakes, and what the client-side ﬁle system and ﬁle server do in respond-\ning to such calls.\nA few comments about the ﬁgure. First, notice how the client tracks all\nrelevant state for the ﬁle access, including the mapping of the integer ﬁle\ndescriptor to an NFS ﬁle handle as well as the current ﬁle pointer. This\nenables the client to turn each read request (which you may have noticed\ndo not specify the offset to read from explicitly) into a properly-formatted\nread protocol message which tells the server exactly which bytes from\nthe ﬁle to read. Upon a successful read, the client updates the current\nﬁle position; subsequent reads are issued with the same ﬁle handle but a\ndifferent offset.\nSecond, you may notice where server interactions occur. When the ﬁle\nis opened for the ﬁrst time, the client-side ﬁle system sends a LOOKUP\nrequest message. Indeed, if a long pathname must be traversed (e.g.,\n/home/remzi/foo.txt), the client would send three LOOKUPs: one\nto look up home in the directory /, one to look up remzi in home, and\nﬁnally one to look up foo.txt in remzi.\nThird, you may notice how each server request has all the information\nneeded to complete the request in its entirety. This design point is critical\nto be able to gracefully recover from server failure, as we will now discuss\nin more detail; it ensures that the server does not need state to be able to\nrespond to the request.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n566\nSUN’S NETWORK FILE SYSTEM (NFS)\nClient\nServer\nfd = open(”/foo”, ...);\nSend LOOKUP (rootdir FH, ”foo”)\nReceive LOOKUP request\nlook for ”foo” in root dir\nreturn foo’s FH + attributes\nReceive LOOKUP reply\nallocate ﬁle desc in open ﬁle table\nstore foo’s FH in table\nstore current ﬁle position (0)\nreturn ﬁle descriptor to application\nread(fd, buffer, MAX);\nIndex into open ﬁle table with fd\nget NFS ﬁle handle (FH)\nuse current ﬁle position as offset\nSend READ (FH, offset=0, count=MAX)\nReceive READ request\nuse FH to get volume/inode num\nread inode from disk (or cache)\ncompute block location (using offset)\nread data from disk (or cache)\nreturn data to client\nReceive READ reply\nupdate ﬁle position (+bytes read)\nset current ﬁle position = MAX\nreturn data/error code to app\nread(fd, buffer, MAX);\nSame except offset=MAX and set current ﬁle position = 2*MAX\nread(fd, buffer, MAX);\nSame except offset=2*MAX and set current ﬁle position = 3*MAX\nclose(fd);\nJust need to clean up local structures\nFree descriptor ”fd” in open ﬁle table\n(No need to talk to server)\nTable 48.1: Reading A File: Client-side And File Server Actions\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "page_number": 592,
      "chapter_number": 58,
      "summary": "This chapter covers segment 58 (pages 592-602). Key topics include servers, client, and read. As they say inside of Google, when you have just your desktop machine,\nfailure is rare; when you’re in a data center with thousands of machines,\nfailure is happening all the time.",
      "keywords": [
        "ﬁle system",
        "ﬁle",
        "client-side ﬁle system",
        "Network File System",
        "system",
        "File System",
        "client",
        "distributed ﬁle system",
        "server",
        "ﬁle handle",
        "File",
        "NFS",
        "ﬁle server",
        "read",
        "NFS ﬁle handle"
      ],
      "concepts": [
        "servers",
        "client",
        "read",
        "protocol",
        "returns",
        "network",
        "request",
        "requested",
        "open",
        "including"
      ],
      "similar_chapters": [
        {
          "book": "A Philosophy of Software Design",
          "chapter": 24,
          "title": "Segment 24 (pages 201-208)",
          "relevance_score": 0.66,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 31,
          "title": "Segment 31 (pages 291-298)",
          "relevance_score": 0.57,
          "method": "api"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 8,
          "title": "Segment 8 (pages 59-66)",
          "relevance_score": 0.56,
          "method": "api"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 15,
          "title": "Segment 15 (pages 115-122)",
          "relevance_score": 0.53,
          "method": "api"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 19,
          "title": "Segment 19 (pages 147-156)",
          "relevance_score": 0.52,
          "method": "api"
        }
      ]
    },
    {
      "number": 59,
      "title": "Segment 59 (pages 603-610)",
      "start_page": 603,
      "end_page": 610,
      "detection_method": "topic_boundary",
      "content": "SUN’S NETWORK FILE SYSTEM (NFS)\n567\nTIP: IDEMPOTENCY IS POWERFUL\nIdempotency is a useful property when building reliable systems. When\nan operation can be issued more than once, it is much easier to handle\nfailure of the operation; you can just retry it. If an operation is not idem-\npotent, life becomes more difﬁcult.\n48.7\nHandling Server Failure with Idempotent Operations\nWhen a client sends a message to the server, it sometimes does not re-\nceive a reply. There are many possible reasons for this failure to respond.\nIn some cases, the message may be dropped by the network; networks do\nlose messages, and thus either the request or the reply could be lost and\nthus the client would never receive a response.\nIt is also possible that the server has crashed, and thus is not currently\nresponding to messages. After a bit, the server will be rebooted and start\nrunning again, but in the meanwhile all requests have been lost. In all of\nthese cases, clients are left with a question: what should they do when\nthe server does not reply in a timely manner?\nIn NFSv2, a client handles all of these failures in a single, uniform, and\nelegant way: it simply retries the request. Speciﬁcally, after sending the\nrequest, the client sets a timer to go off after a speciﬁed time period. If a\nreply is received before the timer goes off, the timer is canceled and all is\nwell. If, however, the timer goes off before any reply is received, the client\nassumes the request has not been processed and resends it. If the server\nreplies, all is well and the client has neatly handled the problem.\nThe ability of the client to simply retry the request (regardless of what\ncaused the failure) is due to an important property of most NFS requests:\nthey are idempotent. An operation is called idempotent when the effect\nof performing the operation multiple times is equivalent to the effect of\nperforming the operating a single time. For example, if you store a value\nto a memory location three times, it is the same as doing so once; thus\n“store value to memory” is an idempotent operation. If, however, you in-\ncrement a counter three times, it results in a different amount than doing\nso just once; thus, “increment counter” is not idempotent. More gener-\nally, any operation that just reads data is obviously idempotent; an oper-\nation that updates data must be more carefully considered to determine\nif it has this property.\nThe heart of the design of crash recovery in NFS is the idempotency\nof most common operations. LOOKUP and READ requests are trivially\nidempotent, as they only read information from the ﬁle server and do not\nupdate it. More interestingly, WRITE requests are also idempotent. If,\nfor example, a WRITE fails, the client can simply retry it. The WRITE\nmessage contains the data, the count, and (importantly) the exact offset\nto write the data to. Thus, it can be repeated with the knowledge that the\noutcome of multiple writes is the same as the outcome of a single one.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n568\nSUN’S NETWORK FILE SYSTEM (NFS)\nCase 1: Request Lost\nClient\n[send request]\nServer\n(no mesg)\nCase 2: Server Down\nClient\n[send request]\nServer\n(down)\nCase 3: Reply lost on way back from Server\nClient\n[send request]\nServer\n[recv request]\n[handle request]\n[send reply]\nFigure 48.5: The Three Types of Loss\nIn this way, the client can handle all timeouts in a uniﬁed way. If a\nWRITE request was simply lost (Case 1 above), the client will retry it, the\nserver will perform the write, and all will be well. The same will happen\nif the server happened to be down while the request was sent, but back\nup and running when the second request is sent, and again all works\nas desired (Case 2). Finally, the server may in fact receive the WRITE\nrequest, issue the write to its disk, and send a reply. This reply may get\nlost (Case 3), again causing the client to re-send the request. When the\nserver receives the request again, it will simply do the exact same thing:\nwrite the data to disk and reply that it has done so. If the client this time\nreceives the reply, all is again well, and thus the client has handled both\nmessage loss and server failure in a uniform manner. Neat!\nA small aside: some operations are hard to make idempotent. For\nexample, when you try to make a directory that already exists, you are\ninformed that the mkdir request has failed. Thus, in NFS, if the ﬁle server\nreceives a MKDIR protocol message and executes it successfully but the\nreply is lost, the client may repeat it and encounter that failure when in\nfact the operation at ﬁrst succeeded and then only failed on the retry.\nThus, life is not perfect.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nSUN’S NETWORK FILE SYSTEM (NFS)\n569\nTIP: PERFECT IS THE ENEMY OF THE GOOD (VOLTAIRE’S LAW)\nEven when you design a beautiful system, sometimes all the corner cases\ndon’t work out exactly as you might like. Take the mkdir example above;\none could redesign mkdir to have different semantics, thus making it\nidempotent (think about how you might do so); however, why bother?\nThe NFS design philosophy covers most of the important cases, and over-\nall makes the system design clean and simple with regards to failure.\nThus, accepting that life isn’t perfect and still building the system is a sign\nof good engineering. Apparently, this wisdom is attributed to Voltaire,\nfor saying “... a wise Italian says that the best is the enemy of the good”\n[V72], and thus we call it Voltaire’s Law.\n48.8\nImproving Performance: Client-side Caching\nDistributed ﬁle systems are good for a number of reasons, but sending\nall read and write requests across the network can lead to a big perfor-\nmance problem: the network generally isn’t that fast, especially as com-\npared to local memory or disk. Thus, another problem: how can we im-\nprove the performance of a distributed ﬁle system?\nThe answer, as you might guess from reading the big bold words in\nthe sub-heading above, is client-side caching. The NFS client-side ﬁle\nsystem caches ﬁle data (and metadata) that it has read from the server in\nclient memory. Thus, while the ﬁrst access is expensive (i.e., it requires\nnetwork communication), subsequent accesses are serviced quite quickly\nout of client memory.\nThe cache also serves as a temporary buffer for writes. When a client\napplication ﬁrst writes to a ﬁle, the client buffers the data in client mem-\nory (in the same cache as the data it read from the ﬁle server) before writ-\ning the data out to the server. Such write buffering is useful because it de-\ncouples application write() latency from actual write performance, i.e.,\nthe application’s call to write() succeeds immediately (and just puts\nthe data in the client-side ﬁle system’s cache); only later does the data get\nwritten out to the ﬁle server.\nThus, NFS clients cache data and performance is usually great and\nwe are done, right? Unfortunately, not quite. Adding caching into any\nsort of system with multiple client caches introduces a big and interesting\nchallenge which we will refer to as the cache consistency problem.\n48.9\nThe Cache Consistency Problem\nThe cache consistency problem is best illustrated with two clients and\na single server. Imagine client C1 reads a ﬁle F, and keeps a copy of the\nﬁle in its local cache. Now imagine a different client, C2, overwrites the\nﬁle F, thus changing its contents; let’s call the new version of the ﬁle F\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n570\nSUN’S NETWORK FILE SYSTEM (NFS)\nC1\ncache: F[v1]\nC2\ncache: F[v2]\nC3\ncache: empty\nServer S\ndisk: F[v1] at first\n      F[v2] eventually\nFigure 48.6: The Cache Consistency Problem\n(version 2), or F[v2] and the old version F[v1] so we can keep the two\ndistinct (but of course the ﬁle has the same name, just different contents).\nFinally, there is a third client, C3, which has not yet accessed the ﬁle F.\nYou can probably see the problem that is upcoming (Figure 48.6). In\nfact, there are two subproblems. The ﬁrst subproblem is that the client C2\nmay buffer its writes in its cache for a time before propagating them to the\nserver; in this case, while F[v2] sits in C2’s memory, any access of F from\nanother client (say C3) will fetch the old version of the ﬁle (F[v1]). Thus,\nby buffering writes at the client, other clients may get stale versions of the\nﬁle, which may be undesirable; indeed, imagine the case where you log\ninto machine C2, update F, and then log into C3 and try to read the ﬁle,\nonly to get the old copy! Certainly this could be frustrating. Thus, let us\ncall this aspect of the cache consistency problem update visibility; when\ndo updates from one client become visible at other clients?\nThe second subproblem of cache consistency is a stale cache; in this\ncase, C2 has ﬁnally ﬂushed its writes to the ﬁle server, and thus the server\nhas the latest version (F[v2]). However, C1 still has F[v1] in its cache; if a\nprogram running on C1 reads ﬁle F, it will get a stale version (F[v1]) and\nnot the most recent copy (F[v2]), which is (often) undesirable.\nNFSv2 implementations solve these cache consistency problems in two\nways. First, to address update visibility, clients implement what is some-\ntimes called ﬂush-on-close (a.k.a., close-to-open) consistency semantics;\nspeciﬁcally, when a ﬁle is written to and subsequently closed by a client\napplication, the client ﬂushes all updates (i.e., dirty pages in the cache)\nto the server. With ﬂush-on-close consistency, NFS ensures that a subse-\nquent open from another node will see the latest ﬁle version.\nSecond, to address the stale-cache problem, NFSv2 clients ﬁrst check\nto see whether a ﬁle has changed before using its cached contents. Speciﬁ-\ncally, when opening a ﬁle, the client-side ﬁle system will issue a GETATTR\nrequest to the server to fetch the ﬁle’s attributes. The attributes, impor-\ntantly, include information as to when the ﬁle was last modiﬁed on the\nserver; if the time-of-modiﬁcation is more recent than the time that the\nﬁle was fetched into the client cache, the client invalidates the ﬁle, thus\nremoving it from the client cache and ensuring that subsequent reads will\ngo to the server and retrieve the latest version of the ﬁle. If, on the other\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nSUN’S NETWORK FILE SYSTEM (NFS)\n571\nhand, the client sees that it has the latest version of the ﬁle, it will go\nahead and use the cached contents, thus increasing performance.\nWhen the original team at Sun implemented this solution to the stale-\ncache problem, they realized a new problem; suddenly, the NFS server\nwas ﬂooded with GETATTR requests. A good engineering principle to\nfollow is to design for the common case, and to make it work well; here,\nalthough the common case was that a ﬁle was accessed only from a sin-\ngle client (perhaps repeatedly), the client always had to send GETATTR\nrequests to the server to make sure no one else had changed the ﬁle. A\nclient thus bombards the server, constantly asking “has anyone changed\nthis ﬁle?”, when most of the time no one had.\nTo remedy this situation (somewhat), an attribute cache was added\nto each client. A client would still validate a ﬁle before accessing it, but\nmost often would just look in the attribute cache to fetch the attributes.\nThe attributes for a particular ﬁle were placed in the cache when the ﬁle\nwas ﬁrst accessed, and then would timeout after a certain amount of time\n(say 3 seconds). Thus, during those three seconds, all ﬁle accesses would\ndetermine that it was OK to use the cached ﬁle and thus do so with no\nnetwork communication with the server.\n48.10\nAssessing NFS Cache Consistency\nA few ﬁnal words about NFS cache consistency. The ﬂush-on-close be-\nhavior was added to “make sense”, but introduced a certain performance\nproblem. Speciﬁcally, if a temporary or short-lived ﬁle was created on a\nclient and then soon deleted, it would still be forced to the server. A more\nideal implementation might keep such short-lived ﬁles in memory until\nthey are deleted and thus remove the server interaction entirely, perhaps\nincreasing performance.\nMore importantly, the addition of an attribute cache into NFS made\nit very hard to understand or reason about exactly what version of a ﬁle\none was getting. Sometimes you would get the latest version; sometimes\nyou would get an old version simply because your attribute cache hadn’t\nyet timed out and thus the client was happy to give you what was in\nclient memory. Although this was ﬁne most of the time, it would (and\nstill does!) occasionally lead to odd behavior.\nAnd thus we have described the oddity that is NFS client caching.\nIt serves as an interesting example where details of an implementation\nserve to deﬁne user-observable semantics, instead of the other way around.\n48.11\nImplications on Server-Side Write Buffering\nOur focus so far has been on client caching, and that is where most\nof the interesting issues arise. However, NFS servers tend to be well-\nequipped machines with a lot of memory too, and thus they have caching\nconcerns as well.\nWhen data (and metadata) is read from disk, NFS\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n572\nSUN’S NETWORK FILE SYSTEM (NFS)\nservers will keep it in memory, and subsequent reads of said data (and\nmetadata) will not go to disk, a potential (small) boost in performance.\nMore intriguing is the case of write buffering. NFS servers absolutely\nmay not return success on a WRITE protocol request until the write has\nbeen forced to stable storage (e.g., to disk or some other persistent device).\nWhile they can place a copy of the data in server memory, returning suc-\ncess to the client on a WRITE protocol request could result in incorrect\nbehavior; can you ﬁgure out why?\nThe answer lies in our assumptions about how clients handle server\nfailure. Imagine the following sequence of writes as issued by a client:\nwrite(fd, a_buffer, size); // fill first block with a’s\nwrite(fd, b_buffer, size); // fill second block with b’s\nwrite(fd, c_buffer, size); // fill third block with c’s\nThese writes overwrite the three blocks of a ﬁle with a block of a’s,\nthen b’s, and then c’s. Thus, if the ﬁle initially looked like this:\nxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\nyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy\nzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\nWe might expect the ﬁnal result after these writes to be like this, with the\nx’s, y’s, and z’s, would be overwritten with a’s, b’s, and c’s, respectively.\naaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\nbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb\ncccccccccccccccccccccccccccccccccccccccccccccccccccccccccccc\nNow let’s assume for the sake of the example that these three client\nwrites were issued to the server as three distinct WRITE protocol mes-\nsages. Assume the ﬁrst WRITE message is received by the server and\nissued to the disk, and the client informed of its success. Now assume\nthe second write is just buffered in memory, and the server also reports\nit success to the client before forcing it to disk; unfortunately, the server\ncrashes before writing it to disk. The server quickly restarts and receives\nthe third write request, which also succeeds.\nThus, to the client, all the requests succeeded, but we are surprised\nthat the ﬁle contents look like this:\naaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\nyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy <--- oops\ncccccccccccccccccccccccccccccccccccccccccccccccccccccccccccc\nYikes! Because the server told the client that the second write was\nsuccessful before committing it to disk, an old chunk is left in the ﬁle,\nwhich, depending on the application, might be catastrophic.\nTo avoid this problem, NFS servers must commit each write to stable\n(persistent) storage before informing the client of success; doing so en-\nables the client to detect server failure during a write, and thus retry until\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nSUN’S NETWORK FILE SYSTEM (NFS)\n573\nit ﬁnally succeeds. Doing so ensures we will never end up with ﬁle con-\ntents intermingled as in the above example.\nThe problem that this requirement gives rise to in NFS server im-\nplementation is that write performance, without great care, can be the\nmajor performance bottleneck. Indeed, some companies (e.g., Network\nAppliance) came into existence with the simple objective of building an\nNFS server that can perform writes quickly; one trick they use is to ﬁrst\nput writes in a battery-backed memory, thus enabling to quickly reply\nto WRITE requests without fear of losing the data and without the cost\nof having to write to disk right away; the second trick is to use a ﬁle sys-\ntem design speciﬁcally designed to write to disk quickly when one ﬁnally\nneeds to do so [HLM94, RO91].\n48.12\nSummary\nWe have seen the introduction of the NFS distributed ﬁle system. NFS\nis centered around the idea of simple and fast recovery in the face of\nserver failure, and achieves this end through careful protocol design. Idem-\npotency of operations is essential; because a client can safely replay a\nfailed operation, it is OK to do so whether or not the server has executed\nthe request.\nWe also have seen how the introduction of caching into a multiple-\nclient, single-server system can complicate things. In particular, the sys-\ntem must resolve the cache consistency problem in order to behave rea-\nsonably; however, NFS does so in a slightly ad hoc fashion which can\noccasionally result in observably weird behavior. Finally, we saw how\nserver caching can be tricky: writes to the server must be forced to stable\nstorage before returning success (otherwise data can be lost).\nWe haven’t talked about other issues which are certainly relevant, no-\ntably security. Security in early NFS implementations was remarkably\nlax; it was rather easy for any user on a client to masquerade as other\nusers and thus gain access to virtually any ﬁle. Subsequent integration\nwith more serious authentication services (e.g., Kerberos [NT94]) have\naddressed these obvious deﬁciencies.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n574\nSUN’S NETWORK FILE SYSTEM (NFS)\nReferences\n[S86] “The Sun Network File System: Design, Implementation and Experience”\nRussel Sandberg\nUSENIX Summer 1986\nThe original NFS paper; though a bit of a challenging read, it is worthwhile to see the source of these\nwonderful ideas.\n[NT94] “Kerberos: An Authentication Service for Computer Networks”\nB. Clifford Neuman, Theodore Ts’o\nIEEE Communications, 32(9):33-38, September 1994\nKerberos is an early and hugely inﬂuential authentication service. We probably should write a book\nchapter about it sometime...\n[P+94] “NFS Version 3: Design and Implementation”\nBrian Pawlowski, Chet Juszczak, Peter Staubach, Carl Smith, Diane Lebel, Dave Hitz\nUSENIX Summer 1994, pages 137-152\nThe small modiﬁcations that underlie NFS version 3.\n[P+00] “The NFS version 4 protocol”\nBrian Pawlowski, David Noveck, David Robinson, Robert Thurlow\n2nd International System Administration and Networking Conference (SANE 2000)\nUndoubtedly the most literary paper on NFS ever written.\n[C00] “NFS Illustrated”\nBrent Callaghan\nAddison-Wesley Professional Computing Series, 2000\nA great NFS reference; incredibly thorough and detailed per the protocol itself.\n[Sun89] “NFS: Network File System Protocol Speciﬁcation”\nSun Microsystems, Inc. Request for Comments: 1094, March 1989\nAvailable: http://www.ietf.org/rfc/rfc1094.txt\nThe dreaded speciﬁcation; read it if you must, i.e., you are getting paid to read it. Hopefully, paid a lot.\nCash money!\n[O91] “The Role of Distributed State”\nJohn K. Ousterhout\nAvailable: ftp://ftp.cs.berkeley.edu/ucb/sprite/papers/state.ps\nA rarely referenced discussion of distributed state; a broader perspective on the problems and challenges.\n[HLM94] “File System Design for an NFS File Server Appliance”\nDave Hitz, James Lau, Michael Malcolm\nUSENIX Winter 1994. San Francisco, California, 1994\nHitz et al. were greatly inﬂuenced by previous work on log-structured ﬁle systems.\n[RO91] “The Design and Implementation of the Log-structured File System”\nMendel Rosenblum, John Ousterhout\nSymposium on Operating Systems Principles (SOSP), 1991\nLFS again. No, you can never get enough LFS.\n[V72] “La Begueule”\nFrancois-Marie Arouet a.k.a. Voltaire\nPublished in 1772\nVoltaire said a number of clever things, this being but one example. For example, Voltaire also said “If\nyou have two religions in your land, the two will cut each others throats; but if you have thirty religions,\nthey will dwell in peace.” What do you say to that, Democrats and Republicans?\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "page_number": 603,
      "chapter_number": 59,
      "summary": "This chapter covers segment 59 (pages 603-610). Key topics include client, server, and write.",
      "keywords": [
        "NETWORK FILE SYSTEM",
        "NFS",
        "client",
        "Server",
        "ﬁle",
        "FILE SYSTEM",
        "WRITE",
        "NETWORK FILE",
        "NFS Cache Consistency",
        "SUN’S NETWORK FILE",
        "cache",
        "NFS clients cache",
        "NFS File Server",
        "SYSTEM",
        "NFS server"
      ],
      "concepts": [
        "client",
        "server",
        "write",
        "writing",
        "caching",
        "caches",
        "request",
        "requests",
        "version",
        "versions"
      ],
      "similar_chapters": [
        {
          "book": "A Philosophy of Software Design",
          "chapter": 24,
          "title": "Segment 24 (pages 201-208)",
          "relevance_score": 0.76,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 33,
          "title": "Segment 33 (pages 662-682)",
          "relevance_score": 0.65,
          "method": "api"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 15,
          "title": "Segment 15 (pages 115-122)",
          "relevance_score": 0.63,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 60,
          "title": "Segment 60 (pages 591-598)",
          "relevance_score": 0.59,
          "method": "api"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 8,
          "title": "Segment 8 (pages 59-66)",
          "relevance_score": 0.59,
          "method": "api"
        }
      ]
    },
    {
      "number": 60,
      "title": "Segment 60 (pages 611-622)",
      "start_page": 611,
      "end_page": 622,
      "detection_method": "topic_boundary",
      "content": "49\nThe Andrew File System (AFS)\nThe Andrew File System was introduced by researchers at Carnegie-Mellon\nUniversity (CMU) in the 1980’s [H+88]. Led by the well-known Profes-\nsor M. Satyanarayanan of Carnegie-Mellon University (“Satya” for short),\nthe main goal of this project was simple: scale. Speciﬁcally, how can one\ndesign a distributed ﬁle system such that a server can support as many\nclients as possible?\nInterestingly, there are numerous aspects of design and implementa-\ntion that affect scalability. Most important is the design of the protocol be-\ntween clients and servers. In NFS, for example, the protocol forces clients\nto check with the server periodically to determine if cached contents have\nchanged; because each check uses server resources (including CPU and\nnetwork bandwidth), frequent checks like this will limit the number of\nclients a server can respond to and thus limit scalability.\nAFS also differs from NFS in that from the beginning, reasonable user-\nvisible behavior was a ﬁrst-class concern. In NFS, cache consistency is\nhard to describe because it depends directly on low-level implementa-\ntion details, including client-side cache timeout intervals. In AFS, cache\nconsistency is simple and readily understood: when the ﬁle is opened, a\nclient will generally receive the latest consistent copy from the server.\n49.1\nAFS Version 1\nWe will discuss two versions of AFS [H+88, S+85]. The ﬁrst version\n(which we will call AFSv1, but actually the original system was called\nthe ITC distributed ﬁle system [S+85]) had some of the basic design in\nplace, but didn’t scale as desired, which led to a re-design and the ﬁnal\nprotocol (which we will call AFSv2, or just AFS) [H+88]. We now discuss\nthe ﬁrst version.\nOne of the basic tenets of all versions of AFS is whole-ﬁle caching on\nthe local disk of the client machine that is accessing a ﬁle. When you\nopen() a ﬁle, the entire ﬁle (if it exists) is fetched from the server and\nstored in a ﬁle on your local disk. Subsequent application read() and\nwrite() operations are redirected to the local ﬁle system where the ﬁle is\n575\n\n\n576\nTHE ANDREW FILE SYSTEM (AFS)\nTestAuth\nTest whether a file has changed\n(used to validate cached entries)\nGetFileStat\nGet the stat info for a file\nFetch\nFetch the contents of file\nStore\nStore this file on the server\nSetFileStat\nSet the stat info for a file\nListDir\nList the contents of a directory\nFigure 49.1: AFSv1 Protocol Highlights\nstored; thus, these operations require no network communication and are\nfast. Finally, upon close(), the ﬁle (if it has been modiﬁed) is ﬂushed\nback to the server. Note the obvious contrasts with NFS, which caches\nblocks (not whole ﬁles, although NFS could of course cache every block of\nan entire ﬁle) and does so in client memory (not local disk).\nLet’s get into the details a bit more. When a client application ﬁrst calls\nopen(), the AFS client-side code (which the AFS designers call Venus)\nwould send a Fetch protocol message to the server. The Fetch protocol\nmessage would pass the entire pathname of the desired ﬁle (for exam-\nple, /home/remzi/notes.txt) to the ﬁle server (the group of which\nthey called Vice), which would then traverse the pathname, ﬁnd the de-\nsired ﬁle, and ship the entire ﬁle back to the client. The client-side code\nwould then cache the ﬁle on the local disk of the client (by writing it to\nlocal disk). As we said above, subsequent read() and write() system\ncalls are strictly local in AFS (no communication with the server occurs);\nthey are just redirected to the local copy of the ﬁle. Because the read()\nand write() calls act just like calls to a local ﬁle system, once a block\nis accessed, it also may be cached in client memory. Thus, AFS also uses\nclient memory to cache copies of blocks that it has in its local disk. Fi-\nnally, when ﬁnished, the AFS client checks if the ﬁle has been modiﬁed\n(i.e., that it has been opened for writing); if so, it ﬂushes the new version\nback to the server with a Store protocol message, sending the entire ﬁle\nand pathname to the server for permanent storage.\nThe next time the ﬁle is accessed, AFSv1 does so much more efﬁ-\nciently. Speciﬁcally, the client-side code ﬁrst contacts the server (using\nthe TestAuth protocol message) in order to determine whether the ﬁle\nhas changed. If not, the client would use the locally-cached copy, thus\nimproving performance by avoiding a network transfer. The ﬁgure above\nshows some of the protocol messages in AFSv1. Note that this early ver-\nsion of the protocol only cached ﬁle contents; directories, for example,\nwere only kept at the server.\n49.2\nProblems with Version 1\nA few key problems with this ﬁrst version of AFS motivated the de-\nsigners to rethink their ﬁle system. To study the problems in detail, the\ndesigners of AFS spent a great deal of time measuring their existing pro-\ntotype to ﬁnd what was wrong. Such experimentation is a good thing;\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nTHE ANDREW FILE SYSTEM (AFS)\n577\nTIP: MEASURE THEN BUILD (PATTERSON’S LAW)\nOne of our advisors, David Patterson (of RISC and RAID fame), used to\nalways encourage us to measure a system and demonstrate a problem\nbefore building a new system to ﬁx said problem. By using experimen-\ntal evidence, rather than gut instinct, you can turn the process of system\nbuilding into a more scientiﬁc endeavor. Doing so also has the fringe ben-\neﬁt of making you think about how exactly to measure the system before\nyour improved version is developed. When you do ﬁnally get around to\nbuilding the new system, two things are better as a result: ﬁrst, you have\nevidence that shows you are solving a real problem; second, you now\nhave a way to measure your new system in place, to show that it actually\nimproves upon the state of the art. And thus we call this Patterson’s Law.\nmeasurement is the key to understanding how systems work and how to\nimprove them. Hard data helps take intuition and make into a concrete\nscience of deconstructing systems. In their study, the authors found two\nmain problems with AFSv1:\n• Path-traversal costs are too high: When performing a Fetch or Store\nprotocol request, the client passes the entire pathname (e.g., /home/\nremzi/notes.txt) to the server. The server, in order to access the\nﬁle, must perform a full pathname traversal, ﬁrst looking in the root\ndirectory to ﬁnd home, then in home to ﬁnd remzi, and so forth,\nall the way down the path until ﬁnally the desired ﬁle is located.\nWith many clients accessing the server at once, the designers of AFS\nfound that the server was spending much of its CPU time simply\nwalking down directory paths.\n• The client issues too many TestAuth protocol messages: Much\nlike NFS and its overabundance of GETATTR protocol messages,\nAFSv1 generated a large amount of trafﬁc to check whether a lo-\ncal ﬁle (or its stat information) was valid with the TestAuth proto-\ncol message. Thus, servers spent much of their time telling clients\nwhether it was OK to used their cached copies of a ﬁle. Most of the\ntime, the answer was that the ﬁle had not changed.\nThere were actually two other problems with AFSv1: load was not\nbalanced across servers, and the server used a single distinct process per\nclient thus inducing context switching and other overheads. The load\nimbalance problem was solved by introducing volumes, which an ad-\nministrator could move across servers to balance load; the context-switch\nproblem was solved in AFSv2 by building the server with threads instead\nof processes. However, for the sake of space, we focus here on the main\ntwo protocol problems above that limited the scale of the system.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n578\nTHE ANDREW FILE SYSTEM (AFS)\n49.3\nImproving the Protocol\nThe two problems above limited the scalability of AFS; the server CPU\nbecame the bottleneck of the system, and each server could only ser-\nvice 20 clients without becoming overloaded. Servers were receiving too\nmany TestAuth messages, and when they received Fetch or Store mes-\nsages, were spending too much time traversing the directory hierarchy.\nThus, the AFS designers were faced with a problem:\nTHE CRUX: HOW TO DESIGN A SCALABLE FILE PROTOCOL\nHow should one redesign the protocol to minimize the number of\nserver interactions, i.e., how could they reduce the number of TestAuth\nmessages? Further, how could they design the protocol to make these\nserver interactions efﬁcient? By attacking both of these issues, a new pro-\ntocol would result in a much more scalable version AFS.\n49.4\nAFS Version 2\nAFSv2 introduced the notion of a callback to reduce the number of\nclient/server interactions. A callback is simply a promise from the server\nto the client that the server will inform the client when a ﬁle that the\nclient is caching has been modiﬁed. By adding this state to the server, the\nclient no longer needs to contact the server to ﬁnd out if a cached ﬁle is\nstill valid. Rather, it assumes that the ﬁle is valid until the server tells it\notherwise; insert analogy to polling versus interrupts here.\nAFSv2 also introduced the notion of a ﬁle identiﬁer (FID) (similar to\nthe NFS ﬁle handle) instead of pathnames to specify which ﬁle a client\nwas interested in. An FID in AFS consists of a volume identiﬁer, a ﬁle\nidentiﬁer, and a “uniquiﬁer” (to enable reuse of the volume and ﬁle IDs\nwhen a ﬁle is deleted). Thus, instead of sending whole pathnames to\nthe server and letting the server walk the pathname to ﬁnd the desired\nﬁle, the client would walk the pathname, one piece at a time, caching the\nresults and thus hopefully reducing the load on the server.\nFor example, if a client accessed the ﬁle /home/remzi/notes.txt,\nand home was the AFS directory mounted onto / (i.e., / was the local root\ndirectory, but home and its children were in AFS), the client would ﬁrst\nFetch the directory contents of home, put them in the local-disk cache,\nand setup a callback on home. Then, the client would Fetch the directory\nremzi, put it in the local-disk cache, and setup a callback on the server\non remzi. Finally, the client would Fetch notes.txt, cache this regular\nﬁle in the local disk, setup a callback, and ﬁnally return a ﬁle descriptor\nto the calling application. See Table 49.1 for a summary.\nThe key difference, however, from NFS, is that with each fetch of a\ndirectory or ﬁle, the AFS client would establish a callback with the server,\nthus ensuring that the server would notify the client of a change in its\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nTHE ANDREW FILE SYSTEM (AFS)\n579\nClient (C1)\nServer\nfd = open(“/home/remzi/notes.txt”, ...);\nSend Fetch (home FID, “remzi”)\nReceive Fetch request\nlook for remzi in home dir\nestablish callback(C1) on remzi\nreturn remzi’s content and FID\nReceive Fetch reply\nwrite remzi to local disk cache\nrecord callback status of remzi\nSend Fetch (remzi FID, “notes.txt”)\nReceive Fetch request\nlook for notes.txt in remzi dir\nestablish callback(C1) on notes.txt\nreturn notes.txt’s content and FID\nReceive Fetch reply\nwrite notes.txt to local disk cache\nrecord callback status of notes.txt\nlocal open() of cached notes.txt\nreturn ﬁle descriptor to application\nread(fd, buffer, MAX);\nperform local read() on cached copy\nclose(fd);\ndo local close() on cached copy\nif ﬁle has changed, ﬂush to server\nfd = open(“/home/remzi/notes.txt”, ...);\nForeach dir (home, remzi)\nif (callback(dir) == VALID)\nuse local copy for lookup(dir)\nelse\nFetch (as above)\nif (callback(notes.txt) == VALID)\nopen local cached copy\nreturn ﬁle descriptor to it\nelse\nFetch (as above) then open and return fd\nTable 49.1: Reading A File: Client-side And File Server Actions\ncached state. The beneﬁt is obvious: although the ﬁrst access to /home/\nremzi/notes.txt generates many client-server messages (as described\nabove), it also establishes callbacks for all the directories as well as the\nﬁle notes.txt, and thus subsequent accesses are entirely local and require\nno server interaction at all. Thus, in the common case where a ﬁle is\ncached at the client, AFS behaves nearly identically to a local disk-based\nﬁle system. If one accesses a ﬁle more than once, the second access should\nbe just as fast as accessing a ﬁle locally.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n580\nTHE ANDREW FILE SYSTEM (AFS)\nASIDE: CACHE CONSISTENCY IS NOT A PANACEA\nWhen discussing distributed ﬁle systems, much is made of the cache con-\nsistency the ﬁle systems provide. However, this baseline consistency does\nnot solve all problems with regards to ﬁle access from multiple clients.\nFor example, if you are building a code repository, with multiple clients\nperforming check-ins and check-outs of code, you can’t simply rely on\nthe underlying ﬁle system to do all of the work for you; rather, you have\nto use explicit ﬁle-level locking in order to ensure that the “right” thing\nhappens when such concurrent accesses take place. Indeed, any applica-\ntion that truly cares about concurrent updates will add extra machinery\nto handle conﬂicts. The baseline consistency described in this chapter and\nthe previous one are useful primarily for casual usage, i.e., when a user\nlogs into a different client, they expect some reasonable version of their\nﬁles to show up there. Expecting more from these protocols is setting\nyourself up for failure, disappointment, and tear-ﬁlled frustration.\n49.5\nCache Consistency\nWhen we discussed NFS, there were two aspects of cache consistency\nwe considered: update visibility and cache staleness. With update visi-\nbility, the question is: when will the server be updated with a new version\nof a ﬁle? With cache staleness, the question is: once the server has a new\nversion, how long before clients see the new version instead of an older\ncached copy?\nBecause of callbacks and whole-ﬁle caching, the cache consistency pro-\nvided by AFS is easy to describe and understand. There are two im-\nportant cases to consider: consistency between processes on different ma-\nchines, and consistency between processes on the same machine.\nBetween different machines, AFS makes updates visible at the server\nand invalidates cached copies at the exact same time, which is when the\nupdated ﬁle is closed. A client opens a ﬁle, and then writes to it (perhaps\nrepeatedly). When it is ﬁnally closed, the new ﬁle is ﬂushed to the server\n(and thus visibile); the server then breaks callbacks for any clients with\ncached copies, thus ensuring that clients will no longer read stale copies\nof the ﬁle; subsequent opens on those clients will require a re-fetch of the\nnew version of the ﬁle from the server.\nAFS makes an exception to this simple model between processes on\nthe same machine. In this case, writes to a ﬁle are immediately visible to\nother local processes (i.e., a process does not have to wait until a ﬁle is\nclosed to see its latest updates). This makes using a single machine be-\nhave exactly as you would expect, as this behavior is based upon typical\nUNIX semantics. Only when switching to a different machine would you\nbe able to detect the more general AFS consistency mechanism.\nThere is one interesting cross-machine case that is worthy of further\ndiscussion. Speciﬁcally, in the rare case that processes on different ma-\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nTHE ANDREW FILE SYSTEM (AFS)\n581\nClient1\nClient2\nServer\nComments\nP1\nP2\nCache\nP3\nCache\nDisk\nopen(F)\n-\n-\n-\nFile created\nwrite(A)\nA\n-\n-\nclose()\nA\n-\nA\nopen(F)\nA\n-\nA\nread() →A\nA\n-\nA\nclose()\nA\n-\nA\nopen(F)\nA\n-\nA\nwrite(B)\nB\n-\nA\nopen(F)\nB\n-\nA\nLocal processes\nread() →B\nB\n-\nA\nsee writes immediately\nclose()\nB\n-\nA\nB\nopen(F)\nA\nA\nRemote processes\nB\nread() →A\nA\nA\ndo not see writes...\nB\nclose()\nA\nA\nclose()\nB\n\u0001A\nB\n... until close()\nB\nopen(F)\nB\nB\nhas taken place\nB\nread() →B\nB\nB\nB\nclose()\nB\nB\nB\nopen(F)\nB\nB\nopen(F)\nB\nB\nB\nwrite(D)\nD\nB\nB\nD\nwrite(C)\nC\nB\nD\nclose()\nC\nC\nclose()\nD\n\u0001C\nD\nD\nopen(F)\nD\nD\nUnfortunately for P3\nD\nread() →D\nD\nD\nthe last writer wins\nD\nclose()\nD\nD\nTable 49.2: Cache Consistency Timeline\nchines are modifying a ﬁle at the same time, AFS naturally employs what\nis known as a last writer wins approach (which perhaps should be called\nlast closer wins). Speciﬁcally, whichever client calls close() last will\nupdate the entire ﬁle on the server last and thus will be the “winning”\nﬁle, i.e., the ﬁle that remains on the server for others to see. The result is\na ﬁle that was generated in its entirety either by one client or the other.\nNote the difference from a block-based protocol like NFS: in NFS, writes\nof individual blocks may be ﬂushed out to the server as each client is up-\ndating the ﬁle, and thus the ﬁnal ﬁle on the server could end up as a mix\nof updates from both clients. In many cases, such a mixed ﬁle output\nwould not make much sense, i.e., imagine a JPEG image getting modi-\nﬁed by two clients in pieces; the resulting mix of writes would not likely\nconstitute a valid JPEG.\nA timeline showing a few of these different scenarios can be seen in\nTable 49.2. The columns of the table show the behavior of two processes\n(P1 and P2) on Client1 and its cache state, one process (P3) on Client2 and\nits cache state, and the server (Server), all operating on a single ﬁle called,\nimaginatively, F. For the server, the table just shows the contents of the\nﬁle after the operation on the left has completed. Read through it and see\nif you can understand why each read returns the results that it does. A\ncommentary ﬁeld on the right will help you if you get stuck.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n582\nTHE ANDREW FILE SYSTEM (AFS)\n49.6\nCrash Recovery\nFrom the description above, you might sense that crash recovery is\nmore involved than with NFS. You would be right. For example, imagine\nthere is a short period of time where a server (S) is not able to contact\na client (C1), for example, while the client C1 is rebooting. While C1\nis not available, S may have tried to send it one or more callback recall\nmessages; for example, imagine C1 had ﬁle F cached on its local disk, and\nthen C2 (another client) updated F, thus causing S to send messages to all\nclients caching the ﬁle to remove it from their local caches. Because C1\nmay miss those critical messages when it is rebooting, upon rejoining the\nsystem, C1 should treat all of its cache contents as suspect. Thus, upon\nthe next access to ﬁle F, C1 should ﬁrst ask the server (with a TestAuth\nprotocol message) whether its cached copy of ﬁle F is still valid; if so, C1\ncan use it; if not, C1 should fetch the newer version from the server.\nServer recovery after a crash is also more complicated. The problem\nthat arises is that callbacks are kept in memory; thus, when a server re-\nboots, it has no idea which client machine has which ﬁles. Thus, upon\nserver restart, each client of the server must realize that the server has\ncrashed and treat all of their cache contents as suspect, and (as above)\nreestablish the validity of a ﬁle before using it. Thus, a server crash is a\nbig event, as one must ensure that each client is aware of the crash in a\ntimely manner, or risk a client accessing a stale ﬁle. There are many ways\nto implement such recovery; for example, by having the server send a\nmessage (saying “don’t trust your cache contents!”) to each client when\nit is up and running again, or by having clients check that the server is\nalive periodically (with a heartbeat message, as it is called). As you can\nsee, there is a cost to building a more scalable and sensible caching model;\nwith NFS, clients hardly noticed a server crash.\n49.7\nScale And Performance Of AFSv2\nWith the new protocol in place, AFSv2 was measured and found to be\nmuch more scalable that the original version. Indeed, each server could\nsupport about 50 clients (instead of just 20). A further beneﬁt was that\nclient-side performance often came quite close to local performance, be-\ncause in the common case, all ﬁle accesses were local; ﬁle reads usually\nwent to the local disk cache (and potentially, local memory). Only when a\nclient created a new ﬁle or wrote to an existing one was there need to send\na Store message to the server and thus update the ﬁle with new contents.\nLet us also gain some perspective on AFS performance by comparing\ncommon ﬁle-system access scenarios with NFS. Table 49.3 shows the re-\nsults of our qualitative comparison.\nIn the table, we examine typical read and write patterns analytically,\nfor ﬁles of different sizes. Small ﬁles have Ns blocks in them; medium\nﬁles have Nm blocks; large ﬁles have NL blocks. We assume that small\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nTHE ANDREW FILE SYSTEM (AFS)\n583\nWorkload\nNFS\nAFS\nAFS/NFS\n1. Small ﬁle, sequential read\nNs · Lnet\nNs · Lnet\n1\n2. Small ﬁle, sequential re-read\nNs · Lmem\nNs · Lmem\n1\n3. Medium ﬁle, sequential read\nNm · Lnet\nNm · Lnet\n1\n4. Medium ﬁle, sequential re-read\nNm · Lmem\nNm · Lmem\n1\n5. Large ﬁle, sequential read\nNL · Lnet\nNL · Lnet\n1\n6. Large ﬁle, sequential re-read\nNL · Lnet\nNL · Ldisk\nLdisk\nLnet\n7. Large ﬁle, single read\nLnet\nNL · Lnet\nNL\n8. Small ﬁle, sequential write\nNs · Lnet\nNs · Lnet\n1\n9. Large ﬁle, sequential write\nNL · Lnet\nNL · Lnet\n1\n10. Large ﬁle, sequential overwrite\nNL · Lnet\n2 · NL · Lnet\n2\n11. Large ﬁle, single write\nLnet\n2 · NL · Lnet\n2 · NL\nTable 49.3: Comparison: AFS vs. NFS\nand medium ﬁles ﬁt into the memory of a client; large ﬁles ﬁt on a local\ndisk but not in client memory.\nWe also assume, for the sake of analysis, that an access across the net-\nwork to the remote server for a ﬁle block takes Lnet time units. Access\nto local memory takes Lmem, and access to local disk takes Ldisk. The\ngeneral assumption is that Lnet > Ldisk > Lmem.\nFinally, we assume that the ﬁrst access to a ﬁle does not hit in any\ncaches. Subsequent ﬁle accesses (i.e., “re-reads”) we assume will hit in\ncaches, if the relevant cache has enough capacity to hold the ﬁle.\nThe columns of the table show the time a particular operation (e.g., a\nsmall ﬁle sequential read) roughly takes on either NFS or AFS. The right-\nmost column displays the ratio of AFS to NFS.\nWe make the following observations. First, in many cases, the per-\nformance of each system is roughly equivalent. For example, when ﬁrst\nreading a ﬁle (e.g., Workloads 1, 3, 5), the time to fetch the ﬁle from the re-\nmote server dominates, and is similar on both systems. You might think\nAFS would be slower in this case, as it has to write the ﬁle to local disk;\nhowever, those writes are buffered by the local (client-side) ﬁle system\ncache and thus said costs are likely hidden. Similarly, you might think\nthat AFS reads from the local cached copy would be slower, again be-\ncause AFS stores the cached copy on disk. However, AFS again beneﬁts\nhere from local ﬁle system caching; reads on AFS would likely hit in the\nclient-side memory cache, and performance would be similar to NFS.\nSecond, an interesting difference arises during a large-ﬁle sequential\nre-read (Workload 6). Because AFS has a large local disk cache, it will\naccess the ﬁle from there when the ﬁle is accessed again. NFS, in contrast,\nonly can cache blocks in client memory; as a result, if a large ﬁle (i.e., a ﬁle\nbigger than local memory) is re-read, the NFS client will have to re-fetch\nthe entire ﬁle from the remote server. Thus, AFS is faster than NFS in this\ncase by a factor of\nLnet\nLdisk , assuming that remote access is indeed slower\nthan local disk. We also note that NFS in this case increases server load,\nwhich has an impact on scale as well.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n584\nTHE ANDREW FILE SYSTEM (AFS)\nThird, we note that sequential writes (of new ﬁles) should perform\nsimilarly on both systems (Workloads 8, 9). AFS, in this case, will write\nthe ﬁle to the local cached copy; when the ﬁle is closed, the AFS client\nwill force the writes to the server, as per the protocol. NFS will buffer\nwrites in client memory, perhaps forcing some blocks to the server due\nto client-side memory pressure, but deﬁnitely writing them to the server\nwhen the ﬁle is closed, to preserve NFS ﬂush-on-close consistency. You\nmight think AFS would be slower here, because it writes all data to local\ndisk. However, realize that it is writing to a local ﬁle system; those writes\nare ﬁrst committed to the page cache, and only later (in the background)\nto disk, and thus AFS reaps the beneﬁts of the client-side OS memory\ncaching infrastructure to improve performance.\nFourth, we note that AFS performs worse on a sequential ﬁle over-\nwrite (Workload 10). Thus far, we have assumed that the workloads that\nwrite are also creating a new ﬁle; in this case, the ﬁle exists, and is then\nover-written. Overwrite can be a particularly bad case for AFS, because\nthe client ﬁrst fetches the old ﬁle in its entirety, only to subsequently over-\nwrite it. NFS, in contrast, will simply overwrite blocks and thus avoid the\ninitial (useless) read1.\nFinally, workloads that access a small subset of data within large ﬁles\nperform much better on NFS than AFS (Workloads 7, 11). In these cases,\nthe AFS protocol fetches the entire ﬁle when the ﬁle is opened; unfortu-\nnately, only a small read or write is performed. Even worse, if the ﬁle is\nmodiﬁed, the entire ﬁle is written back to the server, doubling the per-\nformance impact. NFS, as a block-based protocol, performs I/O that is\nproportional to the size of the read or write.\nOverall, we see that NFS and AFS make different assumptions and not\nsurprisingly realize different performance outcomes as a result. Whether\nthese differences matter is, as always, a question of workload.\n49.8\nAFS: Other Improvements\nLike we saw with the introduction of Berkeley FFS (which added sym-\nbolic links and a number of other features), the designers of AFS took the\nopportunity when building their system to add a number of features that\nmade the system easier to use and manage. For example, AFS provides a\ntrue global namespace to clients, thus ensuring that all ﬁles were named\nthe same way on all client machines. NFS, in contrast, allows each client\nto mount NFS servers in any way that they please, and thus only by con-\nvention (and great administrative effort) would ﬁles be named similarly\nacross clients.\n1We assume here that NFS reads are block-sized and block-aligned; if they were not, the\nNFS client would also have to read the block ﬁrst. We also assume the ﬁle was not opened\nwith the O TRUNC ﬂag; if it had been, the initial open in AFS would not fetch the soon to be\ntruncated ﬁle’s contents.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nTHE ANDREW FILE SYSTEM (AFS)\n585\nASIDE: THE IMPORTANCE OF WORKLOAD\nOne challenge of evaluating any system is the choice of workload. Be-\ncause computer systems are used in so many different ways, there are a\nlarge variety of workloads to choose from. How should the storage sys-\ntem designer decide which workloads are important, in order to make\nreasonable design decisions?\nThe designers of AFS, given their experience in measuring how ﬁle sys-\ntems were used, made certain workload assumptions; in particular, they\nassumed that most ﬁles were not frequently shared, and accessed sequen-\ntially in their entirety. Given those assumptions, the AFS design makes\nperfect sense.\nHowever, these assumptions are not always correct. For example, imag-\nine an application that appends information, periodically, to a log. These\nlittle log writes, which add small amounts of data to an existing large ﬁle,\nare quite problematic for AFS. Many other difﬁcult workloads exist as\nwell, e.g., random updates in a transaction database.\nOne place to get some information about what types of workloads are\ncommon are through various research studies that have been performed.\nSee any of these studies for good examples of workload analysis [B+91,\nH+11, R+00, V99], including the AFS retrospective [H+88].\nAFS also takes security seriously, and incorporates mechanisms to au-\nthenticate users and ensure that a set of ﬁles could be kept private if a\nuser so desired. NFS, in contrast, had quite primitive support for security\nfor many years.\nAFS also includes facilities for ﬂexible user-managed access control.\nThus, when using AFS, a user has a great deal of control over who exactly\ncan access which ﬁles. NFS, like most UNIX ﬁle systems, has much less\nsupport for this type of sharing.\nFinally, as mentioned before, AFS adds tools to enable simpler man-\nagement of servers for the administrators of the system. In thinking about\nsystem management, AFS was light years ahead of the ﬁeld.\n49.9\nSummary\nAFS shows us how distributed ﬁle systems can be built quite differ-\nently than what we saw with NFS. The protocol design of AFS is partic-\nularly important; by minimizing server interactions (through whole-ﬁle\ncaching and callbacks), each server can support many clients and thus\nreduce the number of servers needed to manage a particular site. Many\nother features, including the single namespace, security, and access-control\nlists, make AFS quite nice to use. The consistency model provided by AFS\nis simple to understand and reason about, and does not lead to the occa-\nsional weird behavior as one sometimes observes in NFS.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n586\nTHE ANDREW FILE SYSTEM (AFS)\nPerhaps unfortunately, AFS is likely on the decline. Because NFS be-\ncame an open standard, many different vendors supported it, and, along\nwith CIFS (the Windows-based distributed ﬁle system protocol), NFS\ndominates the marketplace.\nAlthough one still sees AFS installations\nfrom time to time (such as in various educational institutions, including\nWisconsin), the only lasting inﬂuence will likely be from the ideas of AFS\nrather than the actual system itself. Indeed, NFSv4 now adds server state\n(e.g., an “open” protocol message), and thus bears an increasing similar-\nity to the basic AFS protocol.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "page_number": 611,
      "chapter_number": 60,
      "summary": "49.1\nAFS Version 1\nWe will discuss two versions of AFS [H+88, S+85] Key topics include server, cached, and clients. AFS also differs from NFS in that from the beginning, reasonable user-\nvisible behavior was a ﬁrst-class concern.",
      "keywords": [
        "AFS",
        "Andrew File System",
        "ﬁle",
        "server",
        "ﬁle system",
        "NFS",
        "local ﬁle system",
        "File System",
        "System",
        "client",
        "Andrew File",
        "local",
        "AFS client",
        "File",
        "cache"
      ],
      "concepts": [
        "server",
        "cached",
        "clients",
        "read",
        "writing",
        "writes",
        "protocol",
        "differs",
        "difference",
        "different"
      ],
      "similar_chapters": [
        {
          "book": "A Philosophy of Software Design",
          "chapter": 24,
          "title": "Segment 24 (pages 201-208)",
          "relevance_score": 0.48,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 31,
          "title": "Segment 31 (pages 291-298)",
          "relevance_score": 0.41,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 55,
          "title": "Segment 55 (pages 546-554)",
          "relevance_score": 0.41,
          "method": "api"
        },
        {
          "book": "Release It! Design and Deploy Production-Ready Software",
          "chapter": 10,
          "title": "Segment 10 (pages 75-82)",
          "relevance_score": 0.4,
          "method": "api"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 8,
          "title": "Segment 8 (pages 62-69)",
          "relevance_score": 0.4,
          "method": "api"
        }
      ]
    },
    {
      "number": 61,
      "title": "Segment 61 (pages 623-630)",
      "start_page": 623,
      "end_page": 630,
      "detection_method": "topic_boundary",
      "content": "THE ANDREW FILE SYSTEM (AFS)\n587\nReferences\n[B+91] “Measurements of a Distributed File System”\nMary Baker, John Hartman, Martin Kupfer, Ken Shirriff, John Ousterhout\nSOSP ’91, Paciﬁc Grove, CA, October 1991\nAn early paper measuring how people use distributed ﬁle systems. Matches much of the intuition found\nin AFS.\n[H+11] “A File is Not a File: Understanding the I/O Behavior of Apple Desktop Applications”\nTyler Harter, Chris Dragga, Michael Vaughn,\nAndrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau\nSOSP ’11, New York, NY, October 2011\nOur own paper studying the behavior of Apple Desktop workloads; turns out they are a bit different\nthan many of the server-based workloads the systems research community usually focuses upon. Also a\ngood recent reference which points to a lot of related work.\n[H+88] “Scale and Performance in a Distributed File System”\nJohn H. Howard, Michael L. Kazar, Sherri G. Menees, David A. Nichols, M. Satyanarayanan,\nRobert N. Sidebotham, Michael J. West\nACM Transactions on Computing Systems (ACM TOCS), page 51-81, Volume 6, Number 1,\nFebruary 1988\nThe long journal version of the famous AFS system, still in use in a number of places throughout the\nworld, and also probably the earliest clear thinking on how to build distributed ﬁle systems. A wonderful\ncombination of the science of measurement and principled engineering.\n[R+00] “A Comparison of File System Workloads”\nDrew Roselli, Jacob R. Lorch, Thomas E. Anderson\nUSENIX ’00, San Diego, CA, June 2000\nA more recent set of traces as compared to the Baker paper [B+91], with some interesting twists.\n[S+85] “The ITC Distributed File System: Principles and Design”\nM. Satyanarayanan, J.H. Howard, D.A. Nichols, R.N. Sidebotham, A. Spector, M.J. West\nSOSP ’85. pages 35-50\nThe older paper about a distributed ﬁle system. Much of the basic design of AFS is in place in this older\nsystem, but not the improvements for scale.\n[V99] “File system usage in Windows NT 4.0”\nWerner Vogels\nSOSP ’99, Kiawah Island Resort, SC, December 1999\nA cool study of Windows workloads, which are inherently different than many of the UNIX-based studies\nthat had previously been done.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n50\nSummary Dialogue on Distribution\nStudent: Well, that was quick. Too quick, in my opinion!\nProfessor: Yes, distributed systems are complicated and cool and well worth\nyour study; just not in this book (or course).\nStudent: That’s too bad; I wanted to learn more! But I did learn a few things.\nProfessor: Like what?\nStudent: Well, everything can fail.\nProfessor: Good start.\nStudent: But by having lots of these things (whether disks, machines, or what-\never), you can hide much of the failure that arises.\nProfessor: Keep going!\nStudent: Some basic techniques like retrying are really useful.\nProfessor: That’s true.\nStudent: And you have to think carefully about protocols: the exact bits that\nare exchanged between machines. Protocols can affect everything, including how\nsystems respond to failure and how scalable they are.\nProfessor: You really are getting better at this learning stuff.\nStudent: Thanks! And you’re not a bad teacher yourself!\nProfessor: Well thank you very much too.\nStudent: So is this the end of the book?\nProfessor: I’m not sure. They don’t tell me anything.\nStudent: Me neither. Let’s get out of here.\nProfessor: OK.\nStudent: Go ahead.\nProfessor: No, after you.\nStudent: Please, professors ﬁrst.\n589\n\n\n590\nSUMMARY DIALOGUE ON DISTRIBUTION\nProfessor: No, please, after you.\nStudent: (exasperated) Fine!\nProfessor: (waiting) ... so why haven’t you left?\nStudent: I don’t know how. Turns out, the only thing I can do is participate in\nthese dialogues.\nProfessor: Me too. And now you’ve learned our ﬁnal lesson...\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nGeneral Index\nabsolute pathname, 442\nabstraction, iv, 112, 395\nabstractions, 13\naccess methods, 462\naccess path, 470\naccessed bit, 174\naccounting, 77\nack, 547\nacknowledgment, 547\nacquired, 291\nadditive parity, 432\naddress, 7\naddress space, 8, 26, 108, 111, 132, 263,\n403\naddress space identiﬁer, 191\naddress translation, 130, 134, 137\naddress translations, 170\naddress-based ordering, 164\naddress-space identiﬁer, 190\naddress-translation cache, 183\nadmission control, 240\nadvice, 79, 80\nAIO control block, 378\nAIX, 17\nallocate, 472\nallocation structures, 463\nAMAT, 228\namortization, 66, 485\namortize, 65, 514\nanonymous, 125\nanticipatory disk scheduling, 415\nASID, 191\nAside, 16\nasides, iii\nasynchronous, 375\nasynchronous I/O, 377\nasynchronous read, 378\nasynchronously, 556\natomic, 274, 403, 448\natomic exchange, 296\natomically, 10, 271, 297, 429, 495\natomicity violation, 360\nattribute cache, 571\nautomatic, 119\nautomatic memory management, 122\navailable, 291\naverage memory access time, 228\naverage turnaround time, 61\navoidance, 368\navoids, 474\nB-tree, 470\nbaby prooﬁng, 55\nback pointer, 508\nbackground, 224\nbackpointer-based consistency, 508\nbase, 133, 135, 204\nbase and bounds, 133\nbash, 42\nbatch, 14, 474\nBBC, 508\nBelady’s Anomaly, 231\nBerkeley Systems Distribution, 17\nbest ﬁt, 163\nbest-ﬁt, 148\nbig endian, 554\nbig kernel lock, 322\nBill Joy, 17\nbinary buddy allocator, 166\nbinary semaphore, 344\nbitmap, 463\nBKL, 322\nblock corruption, 436, 527\nblock groups, 481\nBlocked, 29\nblocked, 67, 221\nblocks, 462\nboost, 76\nbound, 204\nbounded buffer, 329, 346\nbounded SATF, 419\nbounded-buffer, 329\nbounds, 133, 135\n591\n\n\n592\nDEPLOYMENT\nbreak, 125\nBSATF, 419\nBSD, 17\nbtrfs, 523\nbuddy algorithm, 148\nbuffer, 447\nbuffer cache, 493\nbuffer overﬂow, 123\nbugs, 561\nbus snooping, 96\nbyte ordering, 554\nC programming language, iv, 17\nC-SCAN, 413\ncache, 183, 227, 407\ncache afﬁnity, 97\ncache coherence, 96\ncache consistency problem, 569\ncache hits, 227\ncache misses, 227\ncache replacement, 192\ncached, 560\ncaches, 94\ncaching, 569\ncallback, 578\ncapability, 444\ncapacity, 423\ncapacity miss, 230\ncast, 121\ncentralized administration, 559\ncheckpoint, 498\ncheckpoint region (CR), 517\ncheckpointing, 498\nchecksum, 530, 546\nchild, 36\nchunk size, 424\ncigarette smoker’s problem, 355\ncircular log, 502\nCircular SCAN, 413\nCISC, 187, 189\nclean, 239, 519\nclient stub, 551\nclient-side ﬁle system, 560\nclient/server, 543\nclock algorithm, 238\nclock hand, 238\nclose-to-open, 570\ncluster, 223\nclustering, 240, 249\ncoalesce, 162\ncoalescing, 156, 393\ncoarse-grained, 147, 292\ncode, 111\ncode sharing, 146\ncold-start miss, 229, 230\ncollision, 532\ncommand, 391\ncommon case, 571\ncommunication, 544\ncommunication endpoint, 545\ncompact, 148, 520\ncompaction, 154\ncompare-and-exchange, 299\ncompare-and-swap, 299\nComplex Instruction Set Computing, 189\ncompulsory miss, 229, 230\ncomputed checksum, 533\nconcurrency, iii, 1, 8, 10, 13, 16, 37, 54, 261\nconcurrently, 311\ncondition, 325, 326, 344\ncondition variable, 285, 326, 344\ncondition variables, 262, 273, 362\nconﬂict miss, 230\nconsistent-update problem, 429, 495\nconsumer, 331\ncontext switch, 26, 30, 52, 63, 263\ncontinuation, 380\nconvention, 443\nconvoy effect, 61\ncooperative, 50\ncopy-on-write, 12, 251, 507, 522\ncorrectness, 299\ncorrupt, 528\ncovering condition, 338\nCOW, 251, 507\nCPU, 5\ncrash-consistency problem, 491, 495\nCRC, 532\ncritical section, 271, 272, 284\ncrux, iii\ncrux of the problem, iii\nCuller’s Law, 194\ncycle, 363\ncyclic redundancy check, 532\ncylinder groups, 481\ndangling pointer, 124\ndangling reference, 455\ndata, 391\ndata bitmap, 463, 481, 492\ndata integrity, 527\ndata journaling, 498, 503\ndata protection, 527\ndata region, 462\ndata structures, 32, 461\ndatabase management system, 194\ndatagrams, 545\nDBMS, 194\ndeadlock, 354, 359, 363\nDEC, 245\ndecay-usage, 79\ndecodes, 3\ndemand paging, 240\ndemand zeroing, 250\ndeployability, 422\ndeployment, 422\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nEXPLICIT\n593\ndescheduled, 30\ndeserialization, 552\ndeterministic, 37, 268, 270, 272\ndevice driver, 12, 395\ndialogue, iii\nDigital Equipment Corporation, 245\ndimensional analysis, 408\ndining philosopher’s problem, 352\ndirect I/O, 474\nDirect Memory Access (DMA), 394\ndirect pointers, 466\ndirectory, 442\ndirectory hierarchy, 442\ndirectory tree, 442\ndirty, 239, 447\ndirty bit, 174, 190, 239\ndisable interrupts, 55\ndisassemble, 176\ndisassembler., 269\ndisciplines, 59\ndisk, 28\ndisk address, 218\ndisk arm, 404\ndisk head, 404\nDisk Operating System, 16\ndisk scheduler, 412\ndisk scrubbing, 535\ndisks, 389\ndistributed shared memory, 550\ndistributed state, 562\nDOS, 16\ndouble free, 124\ndouble indirect pointer, 467\ndrop, 545\nDSM, 550\ndtruss, 444\ndynamic relocation, 133, 134\neagerly, 28\nease of use, 108\neasy to use, 3, 111\nECC, 528\nEdsger Dijkstra, 341\nefﬁciency, 110, 113\nefﬁcient, 113\nelevator, 413\nempty, 335\nencapsulation, 364\nend-to-end argument, 545, 555\nenergy-efﬁciency, 14\nerror correcting codes, 528\nevent handler, 374\nevent loop, 374\nevent-based concurrency, 373\nevict, 227\nexactly once, 548\nexecutable format, 28\nexecutes, 3\nexplicit, 144\nexponential back-off, 550\nextents, 467\neXternal Data Representation, 555\nexternal fragmentation, 148, 153, 154\nF-SCAN, 413\nfail-partial, 528\nfail-stop, 423, 527\nfailure, 543\nfair, 66\nfair-share, 83\nfairness, 60, 293, 299\nFast File System (FFS), 481\nFAT, 468\nFCFS, 61\nfetch-and-add, 302\nfetches, 3\nFID, 578\nFIFO, 60, 230\nﬁle, 441\nﬁle allocation table, 468\nﬁle descriptor, 444\nﬁle descriptors, 29\nﬁle handle, 563, 578\nﬁle identiﬁer, 578\nﬁle offset, 446\nﬁle server, 560\nﬁle system, 11, 12, 15\nﬁle system checker, 492\nﬁle-level locking, 580\nﬁle-system inconsistency, 494\nﬁles, 11\nﬁll, 335\nﬁnal, 31\nﬁne-grained, 147, 292\nﬁrmware, 390\nFirst Come, First Served, 60\nﬁrst ﬁt, 164\nFirst In, First Out, 60\nﬁrst-ﬁt, 148\nﬁx-sized cache, 474\nﬂash-based SSDs, 28\nFletcher checksum, 532\nﬂush, 191\nﬂush-on-close, 570\nfork(), 36\nfragmentation, 554\nfragmented, 480\nframe pointer, 27\nfree, 291\nfree list, 136, 154, 170, 463\nfree lists, 470\nfree space management, 469\nfree-space management, 153\nfrequency, 233\nfsck, 492, 495\nfull-stripe write, 432\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n594\nLINK COUNT\nfully associative, 189\nfully-associative, 189, 230\nfunction pointer, 280\nfutex, 307, 308\ngame the scheduler, 76\ngarbage, 494, 518\ngarbage collection, 519\ngarbage collector, 122\ngraphics, 18\ngreedy, 419\ngroup, 223\ngrouping, 240\nhand-over-hand locking, 318\nhard disk drive, 217, 403, 441\nhard drive, 11\nhardware caches, 94\nhardware privilege level, 15\nhardware-based address translation, 130\nhardware-managed TLB, 183\nhardware-managed TLBs, 187\nhead crash, 528\nheader, 157\nheap, 29, 111, 119, 288\nheartbeat, 582\nheld, 291\nhigh watermark, 223\nHill’s Law, 352\nhints, 80\nhit rate, 186, 192, 228\nHoare semantics, 333\nholds, 296\nholes, 520\nhomeworks, iv\nhot spare, 436\nHPUX, 17\nHUP, 379\nhybrid, 202, 205, 308, 393\nI/O, 11\nI/O bus, 389\nI/O instructions, 394\nI/O merging, 415\nIdempotency, 567\nidempotent, 567\nidle time, 224\nillusion, 130\nimmediate reporting, 407, 499\nimplicit, 145\ninconsistent, 429, 491\nindeterminate, 270, 272\nindex node, 464, 465\nindirect pointer, 466\ninitial, 31\ninode, 450, 463–465, 512\ninode bitmap, 463, 481, 492\ninode map (imap), 515\ninode number, 442\ninode table, 463\ninput/output, 11\ninput/output (I/O) device, 389\ninstruction pointer, 26\nINT, 379\ninteractivity, 110\ninterface, 390\ninternal, 202\ninternal fragmentation, 138, 154, 167, 202,\n480, 486\ninternal structure, 390\ninterposing, 129\ninterrupt, 378, 392\ninterrupt handler, 51, 392\ninterrupt service routine (ISR), 392\ninterrupts, 578\ninumber, 465\ninvalid, 173, 203\ninvalid frees, 124\ninvalidate, 96\ninvalidates, 570\ninvariant, 431\ninverted page table, 170\ninverted page tables, 212\nIP, 26\nIRIX, 17\nisolation, 13, 108, 113\nJain’s Fairness Index, 60\njobs, 60\njournal superblock, 503\njournaling, 12, 492, 497\nkernel mode, 15, 47\nkernel stack, 48\nkernel virtual memory, 213\nkill, 379\nKnuth, 322\nlast closer wins, 581\nlast writer wins, 581\nlatent sector errors, 436\nlatent-sector errors, 527\nLauer’s Law, 302\nlazily, 28\nlazy, 250\nLDE, 129\nLeast-Frequently-Used, 233\nLeast-Recently-Used, 233\nleast-recently-used, 192\nlevel of indirection, 207, 515, 516\nLFS, 512\nLFU, 233\nlimit, 133, 135, 204\nlimited direct execution, 45, 55, 105, 129\nlinear page table, 173, 183\nlink count, 454\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "page_number": 623,
      "chapter_number": 61,
      "summary": "This chapter covers segment 61 (pages 623-630). Key topics include professor, cache, and cached.",
      "keywords": [
        "ANDREW FILE SYSTEM",
        "FILE SYSTEM",
        "Distributed File System",
        "ANDREW FILE",
        "Student",
        "SYSTEM",
        "FILE",
        "ﬁle",
        "Distributed File",
        "Professor",
        "Distributed",
        "distributed ﬁle systems",
        "ﬁle system",
        "cache",
        "distributed ﬁle"
      ],
      "concepts": [
        "professor",
        "cache",
        "cached",
        "distributed",
        "distribution",
        "student",
        "pointer",
        "base",
        "disks",
        "systems"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 33,
          "title": "Segment 33 (pages 662-682)",
          "relevance_score": 0.73,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 34,
          "title": "Segment 34 (pages 683-702)",
          "relevance_score": 0.65,
          "method": "api"
        },
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 31,
          "title": "Segment 31 (pages 291-298)",
          "relevance_score": 0.65,
          "method": "api"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 2,
          "title": "Segment 2 (pages 9-16)",
          "relevance_score": 0.63,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 1,
          "title": "Segment 1 (pages 1-20)",
          "relevance_score": 0.63,
          "method": "api"
        }
      ]
    },
    {
      "number": 62,
      "title": "Segment 62 (pages 631-638)",
      "start_page": 631,
      "end_page": 638,
      "detection_method": "topic_boundary",
      "content": "MERGE\n595\nlinked list, 468\nLinus Torvalds, 18\nLinux, 18\nLinux ext2, 497\nLinux ext3, 497\nLinux ext4, 500\nlittle endian, 554\nlive, 519\nlivelock, 366, 393\nlmbench, 55\nload, 28\nload imbalance, 100\nload-linked, 300\nloader, 134\nloads, 39\nlocality, 95, 187\nlock, 291\nlock coupling, 318\nlock variable, 291\nlock-free, 96\nlocked, 291\nlocking, 55, 97, 98\nlocks, 262, 283\nlog, 522\nLog-structured File System, 512\nlogical logging, 498\nlong ﬁle names, 487\nlookaside buffer, 195\nlost write, 535\nlottery scheduling, 83\nlow watermark, 223\nlow-level name, 441, 465\nLRU, 192, 233, 474\nLSEs, 527\nMac OS, 16\nmachine state, 26\nmalicious scheduler, 297\nman pages, 42\nmanage, 4\nmanage memory, 130\nmanual pages, 42\nmanual stack management, 380\nmarshaling, 552\nmaster control program, 4\nmeasurement, 577\nmechanisms, 6, 25, 59, 105, 114\nmemory bus, 389\nmemory hierarchy, 217\nmemory hogs, 249\nmemory leak, 124\nmemory management unit (MMU), 135\nmemory overlays, 217\nmemory pressure, 227\nmemory protection, 16\nmemory-management unit, 183\nmemory-mapped I/O, 395\nMenuMeters, 42\nmerge, 162, 415\nMesa semantics, 333, 337\nmetadata, 449, 463, 466, 512\nmetadata journaling, 503\nMFU, 234\nmice, 389\nmicrokernels, 33, 113\nMicrosoft, 16\nmigrating, 99\nmigration, 101\nminicomputer, 15\nminimize the overheads, 13\nmirrored, 422\nmisdirected write, 534\nmiss rate, 192, 228\nMMU, 183\nmobility, 14\nmodiﬁed, 239\nmodiﬁed bit, 239\nmodularity, 27\nmonitors, 312\nMost-Frequently-Used, 234\nMost-Recently-Used, 234\nmount point, 456\nmount protocol, 564\nMQMS, 99\nMRU, 234\nmulti-level feedback queue, 68\nMulti-level Feedback Queue (MLFQ), 71\nmulti-level index, 467\nmulti-level page table, 187, 205\nmulti-queue multiprocessor scheduling, 99\nmulti-threaded, 9, 262, 263\nmulti-threaded programs, 37\nmulti-zoned, 407\nmulticore, 93\nMultics, 17\nmultiprocessor, 93\nmultiprocessor scheduling, 93, 94\nmultiprogramming, 15, 110\nmutex, 292\nmutual exclusion, 271, 272, 292, 293\nname, 443\nnaming, 553\nNBF, 412\nnearest-block-ﬁrst, 412\nnetworking, 18\nnew, 122\nnext ﬁt, 164\nNeXTStep, 18\nnode.js, 373\nnon-blocking data structures, 322\nnon-determinism, 37\nnon-preemptive, 63\nnon-work-conserving, 415\nnull-pointer, 248\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n596\nQUEUES\nobject caches, 165\noffset, 171\nopen protocol, 561\nopen-source software, 17\noperating system, 4\nOperating Systems in Three Easy Pieces,\n1\noptimal, 62, 228\noptimistic crash consistency, 508\norder violation, 360, 361\nordered journaling, 503\nOS, 4\nOusterhout’s Law, 79\nout-of-memory killer, 240\noverlap, 67, 68, 221, 377, 392\nowner, 292\npage, 169\npage cache, 493\npage daemon, 223\npage directory, 205, 209\npage directory entries, 206\npage fault, 219, 220, 224\npage frame, 170\npage frame number, 206\npage in, 221\npage miss, 220\npage out, 221\npage replacement, 174\npage selection, 240\npage table, 170, 176\npage table base register, 219\npage table entry (PTE), 172, 219\npage-directory index, 208\npage-fault handler, 220, 224\npage-replacement policy, 221\npage-table base register, 175, 187\npage-table index, 209\npaging, 28, 153, 169, 179, 381\npaging out, 227\nparallel, 93\nparameterization, 487\nparameterize, 78\nparent, 31, 36\nparity, 430\npartitioned, 561\npass, 88\nPatterson’s Law, 577\nPC, 16, 26\nPCB, 32\nPCI, 389\nPDE, 206\nperfect scaling, 313\nperformance, 13, 60, 293, 299, 423, 544\nperipheral bus, 389\npersist, 387, 491\npersistence, iii, 1, 11, 12, 29, 387\npersistent storage, 441\npersistently, 11, 13\npersonal computer, 16\nphysical, 4, 23, 130\nphysical address, 134\nphysical ID, 534\nphysical identiﬁer, 534\nphysical logging, 498\nphysical memory, 7\nphysically-indexed cache, 194\nPID, 36, 191\npipe, 41, 329\npipes, 17\nplatter, 404\npolicies, 26, 59, 114\npolicy, 6\npoll, 378\npolling, 391, 578\npower loss, 491\npower outage, 561\npre-allocation, 470\npreempt, 63\npreemptive, 63\npreemptive scheduler, 298\nPreemptive Shortest Job First, 64\nprefetching, 240\npremature optimization, 322\npresent, 222\npresent bit, 174, 219, 224\nprinciple of locality, 233, 234\nprinciple of SJF (shortest job ﬁrst), 412\npriority level, 72\nprivileged, 49, 137, 193, 395\nprocedure call, 15, 283\nprocess, 25, 26\nProcess Control Block, 32\nprocess control block, 137\nprocess control block (PCB), 263\nprocess identiﬁer, 36, 191\nprocess list, 30, 32\nprocess structure, 137\nproducer, 331\nproducer/consumer, 329, 346\nprogram counter, 26\nprogrammed I/O (PIO), 391\nprojects, iv\nprompt, 40\nproportional-share, 83\nprotect, 113\nprotection, 13, 108, 111, 113, 190\nprotection bits, 146, 173\nprotocol, 575\nprotocol compiler, 551\npseudocode, iv\nPSJF, 64\npurify, 125\nqueues, 72\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nROTATIONS PER MINUTE\n597\nrace condition, 270, 272\nRAID, 421\nRAID 0+1, 428\nRAID 1+0, 428\nRAID-01, 428\nRAID-10, 428\nRAID-DP, 530\nRAM, 194\nRAM isn’t always RAM, 194\nrandom, 192, 409, 426, 446\nrandom-access memory, 194\nrandomness, 84\nraw disk, 475\nread-after-write, 535\nreader-writer lock, 350\nReady, 29\nready, 304\nreal code, iv\nreassembly, 554\nreboot the machine, 51\nrecency, 233\nreconstruct, 431, 530\nrecover, 501\nrecovery, 429\nrecovery protocol, 562\nrecursive update problem, 518\nredirected, 40\nredo logging, 501\nReduced Instruction Set Computing, 189\nredundancy, 421\nRedundant Array of Inexpensive Disks,\n421\nreference bit, 174, 238, 249\nreference count, 453\nregain control, 50\nregister context, 30\nreliability, 13, 423\nrelocate, 132\nremote method invocation, 551\nremote procedure call, 551\nreplace, 192, 221\nreplacement policy, 227\nreplayed, 501\nresident set size, 249\nresource, 4\nresource manager, 4, 6\nresources, 13\nresponse time, 64\nretry, 548\nreturn-from-trap, 15, 47\nrevoke, 506\nRISC, 188, 189\nRMI, 551\nroll forward, 522\nroot directory, 442, 471\nrotates, 434\nrotation delay, 405\nrotational delay, 405\nrotations per minute, 408\nrotations per minute (RPM), 404\nround robin, 99\nRound-Robin (RR), 65\nRPC, 551\nRPM, 408\nRSS, 249\nrun-time library, 551\nrun-time stack, 29\nRunning, 29\nrunning, 304\nrunning program, 25\nSATA, 389\nSATF, 414\nscalability, 98\nscale, 575\nscaling, 167\nSCAN, 413\nscan resistance, 241\nschedule, 474\nscheduled, 30\nscheduler, 37, 52\nscheduler state, 344\nscheduling metric, 60\nscheduling policies, 59\nscheduling policy, 26\nscheduling quantum, 65\nSCSI, 389\nsecond-chance lists, 249\nsecurity, 14, 18, 544, 559\nseek, 406, 447\nsegment, 141, 512, 513\nsegment summary block, 520\nsegment table, 147\nsegmentation, 138, 141, 153, 155\nsegmentation fault, 122, 144\nsegmentation violation, 144\nsegmented FIFO, 249\nsegregated lists, 165\nSEGV, 379\nsemaphore, 341\nseparator, 442\nsequence counter, 549\nsequential, 409, 426, 446\nserialization, 552\nserver-side ﬁle system, 560\nset, 298\nset-associativity, 230\nsets, 296\nsettling time, 406\nshadow paging, 522\nshare, 11, 146\nshared state, 562\nsharing, 559\nshell, 17\nshortest access time ﬁrst, 414\nShortest Job First (SJF), 62\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n598\nTIME SLICE\nshortest positioning time ﬁrst, 414\nShortest Time-to-Completion First, 64\nshortest-seek-ﬁrst, 412\nshortest-seek-time-ﬁrst, 412\nSIG, 379\nsignal handler, 379\nsignaling, 326\nsignals, 42, 378, 379\nSIGSEGV, 379\nsilent faults, 528\nsimulations, iv\nsingle-queue multiprocessor scheduling,\n97\nsingle-threaded, 263\nslab allocator, 165\nslabs, 165\nsleeping barber problem, 355\nsloppy counter, 314\nsmall-write problem, 433, 511\nsnapshots, 523\nsockets, 545\nsoft link, 454\nsoftware RAID, 436\nsoftware-managed TLB, 188\nsolid-state drives, 11\nsolid-state storage device, 441\nspace leak, 494\nspace sharing, 26\nsparse address spaces, 143\nspatial locality, 95, 186, 187, 234\nspin lock, 298\nspin-wait, 296\nspin-waiting, 297\nspindle, 404\nsplit, 159\nsplitting, 155\nSPTF, 414\nspurious wakeups, 337\nSQMS, 97\nSSDs, 11\nSSF, 412\nSSTF, 412\nstack, 29, 111, 119\nstack pointer, 27\nstack property, 231\nstale cache, 570\nstandard library, 4, 12\nstandard output, 40\nstarvation, 76, 413\nstarve, 76\nstate, 565\nstateful, 562\nstateless, 562\nstates, 29\nstatic relocation, 134\nstatus, 391\nSTCF, 64\nstore-conditional, 300\nstored checksum, 533\nstrace, 444\nstride, 88\nstride scheduling, 88\nstripe, 424\nstriping, 424\nstub generator, 551\nsub-blocks, 486\nsub-directories, 442\nsubtractive parity, 432\nSunOS, 17\nsuper block, 481\nsuperblock, 464\nsuperpages, 214\nsupervisor, 4\nsurface, 404\nswap, 213\nswap daemon, 223\nswap space, 125, 218\nswapping, 28\nswitches contexts, 53\nsymbolic link, 454, 488\nsynchronization primitives, 271\nsynchronous, 375, 552\nsynchronously, 555\nsystem call, 15, 47\nsystem calls, 4, 12, 50, 560\nsystem crash, 491\nsystems programming, iv\nTCP, 549\nTCP/IP, 549\ntcsh, 42\ntemporal locality, 95, 186, 187, 234\ntest, 298\ntest-and-set, 297\ntest-and-set instruction, 296\ntests, 296\nthe mapping problem, 425\nthe web, 42\nthrashing, 240\nthread, 262, 263\nthread control blocks (TCBs), 263\nthread pool, 553\nthread safe, 311\nthread-local, 264\nthreads, 9, 93, 112\nThree C’s, 230\nticket, 85\nticket currency, 85\nticket inﬂation, 85\nticket lock, 302\nticket transfer, 85\ntickets, 83\nTID, 498\nTime sharing, 26\ntime sharing, 25, 45, 46, 110\ntime slice, 65\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nVALID BIT\n599\ntime-sharing, 26\ntime-slicing, 65\ntime-space trade-off, 207\ntime-space trade-offs, 207\ntimeout, 548\ntimeout/retry, 548\ntimer interrupt, 51, 52\ntips, iii\nTLB, 183\nTLB coverage, 194\nTLB hit, 184, 219\nTLB miss, 184, 219\ntorn write, 403\ntotal ordering, 365\ntrack, 404\ntrack buffer, 407, 487\ntrack skew, 406\ntrade-off, 66\ntransaction, 274\ntransaction checksum, 508\ntransaction identiﬁer, 498\ntransfer, 406\ntranslate, 174\ntranslated, 133\ntranslation lookaside buffer, 195\ntranslation-lookaside buffer, 183\ntransparency, 113, 131\ntransparent, 132, 560\ntransparently, 224, 422\ntrap, 15, 47, 51\ntrap handler, 15, 188\ntrap handlers, 48\ntrap table, 47, 48\ntraverse, 471\ntriple indirect pointer, 467\ntruss, 444\nTuring Award, 71\nturnaround time, 60\ntwo-phase lock, 307\ntwo-phased, 393\ntype, 443\nUDP/IP, 545\nunfairness metric, 87\nuniﬁed page cache, 474\nuninitialized read, 123\nunlocked, 291\nunmapped, 188\nunmarshaling, 552\nupdate, 7, 96\nupdate visibility, 570\nUSB, 389\nuse bit, 238\nuser mode, 15, 47\nutilization, 110\nvalgrind, 125\nvalid, 190, 222\nvalid bit, 173, 206\nVenus, 576\nversion number, 521\nversioning ﬁle system, 519\nVery Simple File System, 461\nVice, 576\nvirtual, 4, 23, 130\nvirtual address, 112, 114, 134\nvirtual address space, 8\nvirtual CPUs, 263\nvirtual machine, 4\nvirtual memory, 263\nvirtual page number (VPN), 171\nvirtual-to-physical address translations, 176\nvirtualization, iii, 1, 4, 8, 23\nvirtualized, 90, 269\nvirtualizes, 13\nvirtualizing, 25\nvirtualizing memory, 8, 112\nvirtualizing the CPU, 6\nvirtually-indexed cache, 194\nvoid pointer, 154, 280\nvolatile, 11\nVoltaire’s Law, 569\nvolumes, 577\nVon Neumann, 3\nvoo-doo constants, 77\nvsfs, 461\nWAFL, 523, 530\nwait-free, 367\nwait-free synchronization, 300\nwaiting, 326\nwakeup/waiting race, 306\nwhole-ﬁle caching, 575\nwired, 188\nwork stealing, 101\nwork-conserving, 415\nworking sets, 240\nworkload, 59, 492, 585\nworkloads, 234\nworst ﬁt, 163\nworst-ﬁt, 148\nwrite back, 407\nwrite barriers, 499\nwrite buffering, 474, 513, 569\nwrite through, 407\nwrite verify, 535\nwrite-ahead log, 429\nwrite-ahead logging, 492, 497\nx86, 177\nXDR, 555\nXOR, 430\nyield, 51\nZemaphores, 355\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n\n\n600\nZOMBIE\nZettabyte File System, 535\nZFS, 523, 535\nzombie, 31\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nAsides\nUNIX Signals, 379\nAdvanced Chapters, 93\nAnd Then Came Linux, 18\nBelady’s Anomaly, 231\nBlocking vs. Non-blocking Interfaces, 375\nCache Consistency Is Not A Panacea, 580\nCalling lseek() Does Not Perform A Disk Seek, 447\nComputing The “Average” Seek, 411\nData Structure – The Free List, 136\nData Structure – The Inode, 465\nData Structure – The Page Table, 176\nData Structure – The Process List, 32\nDekker’s and Peterson’s Algorithms, 295\nDimensional Analysis, 408\nEmulating Reference Bits, 250\nEvery Address You See Is Virtual, 114\nFFS File Creation, 482\nForcing Writes To Disk, 499\nFree Space Management, 470\nGreat Engineers Are Really Great, 166\nHow Long Context Switches Take, 55\nInterludes, 35\nKey Concurrency Terms, 272\n601\n\n\n602\nWHY SYSTEM CALLS LOOK LIKE PROCEDURE CALLS\nLinked-based Approaches, 468\nMeasurement Homeworks, 58\nMental Models Of File Systems, 462\nMultiple Page Sizes, 202\nOptimizing Log Writes, 500\nPreemptive Schedulers, 63\nReads Don’t Access Allocation Structures, 472\nRISC vs. CISC, 189\nRTFM – Read The Man Pages, 42\nSimulation Homeworks, 70\nSoftware-based Relocation, 134\nStorage Technologies, 218\nSwapping Terminology And Other Things, 220\nThe creat() System Call, 444\nThe End-to-End Argument, 555\nThe Importance of UNIX, 17\nThe Importance Of Workload, 585\nThe RAID Consistent-Update Problem, 429\nThe RAID Mapping Problem, 425\nThe Segmentation Fault, 144\nThread API Guidelines, 288\nTLB Valid Bit ̸= Page Table Valid Bit, 190\nTypes of Cache Misses, 230\nTypes of Locality, 234\nWhy Hardware Doesn’t Handle Page Faults, 221\nWhy Null Pointer Accesses Cause Seg Faults, 248\nWhy Servers Crash, 561\nWhy System Calls Look Like Procedure Calls, 48\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "page_number": 631,
      "chapter_number": 62,
      "summary": "This chapter covers segment 62 (pages 631-638). Key topics include pages, paging, and write.",
      "keywords": [
        "System",
        "bit",
        "memory",
        "File System",
        "system calls",
        "page table",
        "VALID BIT",
        "operating",
        "time",
        "RAID",
        "scheduling",
        "process",
        "TLB",
        "structure",
        "Linux"
      ],
      "concepts": [
        "pages",
        "paging",
        "write",
        "memory",
        "time",
        "block",
        "scheduling",
        "schedule",
        "caches",
        "caching"
      ],
      "similar_chapters": [
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 33,
          "title": "Segment 33 (pages 662-682)",
          "relevance_score": 0.77,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 42,
          "title": "Segment 42 (pages 838-857)",
          "relevance_score": 0.76,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 41,
          "title": "Segment 41 (pages 815-837)",
          "relevance_score": 0.69,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 1,
          "title": "Segment 1 (pages 1-20)",
          "relevance_score": 0.68,
          "method": "api"
        },
        {
          "book": "Game Programming Gems 8",
          "chapter": 37,
          "title": "Segment 37 (pages 343-350)",
          "relevance_score": 0.67,
          "method": "api"
        }
      ]
    },
    {
      "number": 63,
      "title": "Segment 63 (pages 639-643)",
      "start_page": 639,
      "end_page": 643,
      "detection_method": "topic_boundary",
      "content": "Tips\nAlways Hold The Lock While Signaling, 329\nAmortization Can Reduce Costs, 66\nAvoid Premature Optimization (Knuth’s Law), 322\nAvoid Voo-doo Constants (Ousterhout’s Law), 79\nBe Careful Setting The Timeout Value, 550\nBe Careful With Generalization, 356\nBe Lazy, 251\nBe Wary of Complexity, 208\nBe Wary Of Locks and Control Flow, 319\nBe Wary Of Powerful Commands, 451\nCommunication Is Inherently Unreliable, 544\nComparing Against Optimal is Useful, 229\nConsider Extent-based Approaches, 467\nDealing With Application Misbehavior, 51\nDetails Matter, 513\nDo Work In The Background, 224\nDon’t Always Do It Perfectly (Tom West’s Law), 370\nDon’t Block In Event-based Servers, 377\nGetting It Right (Lampson’s Law), 40\nHardware-based Dynamic Relocation, 135\nIdempotency Is Powerful, 567\nIf 1000 Solutions Exist, No Great One Does, 149\nInterposition Is Powerful, 131\nInterrupts Not Always Better Than PIO, 393\nIt Always Depends (Livny’s Law), 415\nIt Compiled or It Ran ̸= It Is Correct, 123\nKnow And Use Your Tools, 269\n603\n\n\n604\nWHEN IN DOUBT, TRY IT OUT\nLearn From History, 72\nLess Code Is Better Code (Lauer’s Law), 302\nMake The System Usable, 488\nMeasure Then Build (Patterson’s Law), 577\nMore Concurrency Isn’t Necessarily Faster, 319\nOverlap Enables Higher Utilization, 68\nPerfect Is The Enemy Of The Good (Voltaire’s Law), 569\nRAM Isn’t Always RAM (Culler’s Law), 194\nReboot Is Useful, 56\nSeparate Policy And Mechanism, 27\nSimple And Dumb Can Be Better (Hill’s Law), 352\nThe Principle of Isolation, 113\nThe Principle of SJF, 62\nThere’s No Free Lunch, 531\nThink About Concurrency As Malicious Scheduler, 297\nThink Carefully About Naming, 443\nTransparency Enables Deployment, 422\nTurn Flaws Into Virtues, 523\nUnderstand Time-Space Trade-offs, 207\nUse strace (And Similar Tools), 445\nUse A Level Of Indirection, 516\nUse Advice Where Possible, 80\nUse Atomic Operations, 274\nUse Caching When Possible, 187\nUse Checksums For Integrity, 547\nUse Disks Sequentially, 410\nUse Hybrids, 205\nUse Protected Control Transfer, 47\nUse Randomness, 84\nUse The Timer Interrupt To Regain Control, 52\nUse Tickets To Represent Shares, 85\nUse Time Sharing (and Space Sharing), 26\nUse While (Not If) For Conditions, 337\nWhen In Doubt, Try It Out, 121\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nCruces\nHow To Account For Disk Rotation Costs, 413\nHow To Add Locks To Data Structures, 311\nHow To Allocate And Manage Memory, 119\nHow To Avoid Spinning, 304\nHow To Avoid The Costs Of Polling, 392\nHow To Avoid The Curse Of Generality, 245\nHow To Build A Device-neutral OS, 395\nHow To Build A Distributed File System, 560\nHow To Build A Lock, 293\nHow To Build Concurrent Servers Without Threads, 373\nHow To Build Correct Concurrent Programs, 10\nHow To Build Systems That Work When Components Fail, 543\nHow To Communicate With Devices, 394\nHow To Create And Control Processes, 35\nHow To Create And Control Threads, 279\nHow To Deal With Deadlock, 363\nHow To Deal With Load Imbalance, 101\nHow To Decide Which Page To Evict, 227\nHow To Deﬁne A Stateless File Protocol, 563\nHow To Design A Scalable File Protocol, 578\nHow To Design TLB Replacement Policy, 192\nHow To Develop Scheduling Policy, 59\nHow To Efﬁciently And Flexibly Virtualize Memory, 129\nHow To Efﬁciently Virtualize The CPU With Control, 45\nHow To Ensure Data Integrity, 527\nHow To Gain Control Without Cooperation, 51\nHow To Go Beyond Physical Memory, 217\nHow To Handle Common Concurrency Bugs, 359\nHow To Handle Disk Starvation, 413\nHow To Handle Latent Sector Errors, 529\nHow To Handle Lost Writes, 535\nHow To Handle Misdirected Writes, 534\nHow To Implement A Simple File System, 461\n605\n\n\n606\nHOW TO WAIT FOR A CONDITION\nHow To Implement An LRU Replacement Policy, 238\nHow To Integrate I/O Into Systems, 389\nHow To Lower PIO Overheads, 394\nHow To Make A Large, Fast, Reliable Disk, 421\nHow To Make All Writes Sequential Writes, 512\nHow To Make Page Tables Smaller, 201\nHow To Manage A Persistent Device, 441\nHow To Manage Free Space, 154\nHow To Manage TLB Contents On A Context Switch, 191\nHow To Organize On-disk Data To Improve Performance, 480\nHow To Perform Restricted Operations, 46\nHow To Preserve Data Integrity Despite Corruption, 530\nHow To Provide Support For Synchronization, 272\nHow To Provide The Illusion Of Many CPUs, 25\nHow To Reduce File System I/O Costs, 473\nHow To Regain Control Of The CPU, 50\nHow To Schedule Jobs On Multiple CPUs, 94\nHow To Schedule Without Perfect Knowledge, 71\nHow To Share The CPU Proportionally, 83\nHow To Speed Up Address Translation, 183\nHow To Store And Access Data On Disk, 403\nHow To Store Data Persistently, 12\nHow To Support A Large Address Space, 141\nHow To Update The Disk Despite Crashes, 491\nHow To Use Semaphores, 341\nHow To Virtualize Memory, 112\nHow To Virtualize Memory Without Segments, 169\nHow To Virtualize Resources, 4\nHow To Wait For A Condition, 326\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n\n\nThis book was typeset using the amazing LATEX typesetting system and\nthe wonderful memoir book-making package. A heartfelt thank you to\nthe legions of programmers who have contributed to this powerful tool\nover the many years of its development.\nAll of the graphs and ﬁgures in the book were generated using a Python-\nbased version of zplot, a simple and useful tool developed by R. Arpaci-\nDusseau to generate graphs in PostScript. The zplot tool arose after\nmany years of frustration with existing graphing tools such as gnuplot\n(which was limited) and ploticus (which was overly complex though\nadmittedly quite awesome). As a result, R. A-D ﬁnally put his years of\nstudy of PostScript to good use and developed zplot.\n",
      "page_number": 639,
      "chapter_number": 63,
      "summary": "This chapter covers segment 63 (pages 639-643). Key topics include useful, data, and control. A heartfelt thank you to\nthe legions of programmers who have contributed to this powerful tool\nover the many years of its development.",
      "keywords": [
        "Law",
        "File System",
        "Control",
        "Build",
        "Virtualize Memory",
        "Avoid Premature Optimization",
        "Knuth ’s Law",
        "Data",
        "Avoid Voo-doo Constants",
        "Reduce File System",
        "Ousterhout ’s Law",
        "System",
        "Simple File System",
        "Disk",
        "Handle"
      ],
      "concepts": [
        "useful",
        "data",
        "control",
        "based",
        "file",
        "disks",
        "tools",
        "concurrency",
        "concurrent",
        "avoid"
      ],
      "similar_chapters": [
        {
          "book": "Designing Data-Intensive Applications",
          "chapter": 31,
          "title": "Segment 31 (pages 291-298)",
          "relevance_score": 0.62,
          "method": "api"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 2,
          "title": "Segment 2 (pages 9-16)",
          "relevance_score": 0.61,
          "method": "api"
        },
        {
          "book": "C++ Concurrency in Action",
          "chapter": 21,
          "title": "Segment 21 (pages 193-216)",
          "relevance_score": 0.61,
          "method": "api"
        },
        {
          "book": "Reliable Machine Learning",
          "chapter": 8,
          "title": "Segment 8 (pages 62-69)",
          "relevance_score": 0.61,
          "method": "api"
        },
        {
          "book": "Computer Systems A Programmer’s Perspective",
          "chapter": 34,
          "title": "Segment 34 (pages 683-702)",
          "relevance_score": 0.6,
          "method": "api"
        }
      ]
    }
  ],
  "pages": [
    {
      "page_number": 1,
      "chapter": null,
      "content": "OPERATING SYSTEMS\nTHREE EASY PIECES\nREMZI H. ARPACI-DUSSEAU\nANDREA C. ARPACI-DUSSEAU\nUNIVERSITY OF WISCONSIN–MADISON\n",
      "content_length": 117,
      "extraction_method": "Direct"
    },
    {
      "page_number": 2,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 3,
      "chapter": null,
      "content": ". .\nc⃝2014 by Arpaci-Dusseau Books, Inc.\nAll rights reserved\n",
      "content_length": 61,
      "extraction_method": "Direct"
    },
    {
      "page_number": 4,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 5,
      "chapter": null,
      "content": "i\nTo Vedat S. Arpaci, a lifelong inspiration\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 86,
      "extraction_method": "Direct"
    },
    {
      "page_number": 6,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 7,
      "chapter": null,
      "content": "Preface\nTo Everyone\nWelcome to this book! We hope you’ll enjoy reading it as much as we enjoyed\nwriting it. The book is called Operating Systems: Three Easy Pieces, and the title\nis obviously an homage to one of the greatest sets of lecture notes ever created, by\none Richard Feynman on the topic of Physics [F96]. While this book will undoubt-\nedly fall short of the high standard set by that famous physicist, perhaps it will be\ngood enough for you in your quest to understand what operating systems (and\nmore generally, systems) are all about.\nThe three easy pieces refer to the three major thematic elements the book is\norganized around: virtualization, concurrency, and persistence. In discussing\nthese concepts, we’ll end up discussing most of the important things an operating\nsystem does; hopefully, you’ll also have some fun along the way. Learning new\nthings is fun, right? At least, it should be.\nEach major concept is divided into a set of chapters, most of which present a\nparticular problem and then show how to solve it. The chapters are short, and try\n(as best as possible) to reference the source material where the ideas really came\nfrom. One of our goals in writing this book is to make the paths of history as clear\nas possible, as we think that helps a student understand what is, what was, and\nwhat will be more clearly. In this case, seeing how the sausage was made is nearly\nas important as understanding what the sausage is good for1.\nThere are a couple devices we use throughout the book which are probably\nworth introducing here. The ﬁrst is the crux of the problem. Anytime we are\ntrying to solve a problem, we ﬁrst try to state what the most important issue is;\nsuch a crux of the problem is explicitly called out in the text, and hopefully solved\nvia the techniques, algorithms, and ideas presented in the rest of the text.\nThere are also numerous asides and tips throughout the text, adding a little\ncolor to the mainline presentation. Asides tend to discuss something relevant (but\nperhaps not essential) to the main text; tips tend to be general lessons that can be\napplied to systems you build. An index at the end of the book lists all of these tips\nand asides (as well as cruces, the odd plural of crux) for your convenience.\nWe use one of the oldest didactic methods, the dialogue, throughout the book,\nas a way of presenting some of the material in a different light. These are used to\nintroduce the major thematic concepts (in a peachy way, as we will see), as well as\nto review material every now and then. They are also a chance to write in a more\n1Hint: eating! Or if you’re a vegetarian, running away from.\niii\n",
      "content_length": 2654,
      "extraction_method": "Direct"
    },
    {
      "page_number": 8,
      "chapter": null,
      "content": "iv\nhumorous style. Whether you ﬁnd them useful, or humorous, well, that’s another\nmatter entirely.\nAt the beginning of each major section, we’ll ﬁrst present an abstraction that an\noperating system provides, and then work in subsequent chapters on the mecha-\nnisms, policies, and other support needed to provide the abstraction. Abstractions\nare fundamental to all aspects of Computer Science, so it is perhaps no surprise\nthat they are also essential in operating systems.\nThroughout the chapters, we try to use real code (not pseudocode) where pos-\nsible, so for virtually all examples, you should be able to type them up yourself\nand run them. Running real code on real systems is the best way to learn about\noperating systems, so we encourage you to do so when you can.\nIn various parts of the text, we have sprinkled in a few homeworks to ensure\nthat you are understanding what is going on. Many of these homeworks are little\nsimulations of pieces of the operating system; you should download the home-\nworks, and run them to quiz yourself. The homework simulators have the follow-\ning feature: by giving them a different random seed, you can generate a virtually\ninﬁnite set of problems; the simulators can also be told to solve the problems for\nyou. Thus, you can test and re-test yourself until you have achieved a good level\nof understanding.\nThe most important addendum to this book is a set of projects in which you\nlearn about how real systems work by designing, implementing, and testing your\nown code. All projects (as well as the code examples, mentioned above) are in\nthe C programming language [KR88]; C is a simple and powerful language that\nunderlies most operating systems, and thus worth adding to your tool-chest of\nlanguages. Two types of projects are available (see the online appendix for ideas).\nThe ﬁrst are systems programming projects; these projects are great for those who\nare new to C and UNIX and want to learn how to do low-level C programming.\nThe second type are based on a real operating system kernel developed at MIT\ncalled xv6 [CK+08]; these projects are great for students that already have some C\nand want to get their hands dirty inside the OS. At Wisconsin, we’ve run the course\nin three different ways: either all systems programming, all xv6 programming, or\na mix of both.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2366,
      "extraction_method": "Direct"
    },
    {
      "page_number": 9,
      "chapter": null,
      "content": "v\nTo Educators\nIf you are an instructor or professor who wishes to use this book, please feel\nfree to do so. As you may have noticed, they are free and available on-line from\nthe following web page:\nhttp://www.ostep.org\nYou can also purchase a printed copy from lulu.com. Look for it on the web\npage above.\nThe (current) proper citation for the book is as follows:\nOperating Systems: Three Easy Pieces\nRemzi H. Arpaci-Dusseau and Andrea C. Arpaci-Dusseau\nArpaci-Dusseau Books, Inc.\nMay, 2014 (Version 0.8)\nhttp://www.ostep.org\nThe course divides fairly well across a 15-week semester, in which you can\ncover most of the topics within at a reasonable level of depth. Cramming the\ncourse into a 10-week quarter probably requires dropping some detail from each\nof the pieces. There are also a few chapters on virtual machine monitors, which we\nusually squeeze in sometime during the semester, either right at end of the large\nsection on virtualization, or near the end as an aside.\nOne slightly unusual aspect of the book is that concurrency, a topic at the front\nof many OS books, is pushed off herein until the student has built an understand-\ning of virtualization of the CPU and of memory. In our experience in teaching\nthis course for nearly 15 years, students have a hard time understanding how the\nconcurrency problem arises, or why they are trying to solve it, if they don’t yet un-\nderstand what an address space is, what a process is, or why context switches can\noccur at arbitrary points in time. Once they do understand these concepts, how-\never, introducing the notion of threads and the problems that arise due to them\nbecomes rather easy, or at least, easier.\nYou may have noticed there are no slides that go hand-in-hand with the book.\nThe major reason for this omission is that we believe in the most old-fashioned\nof teaching methods: chalk and a blackboard. Thus, when we teach the course,\nwe come to class with a few major ideas and examples in mind and use the board\nto present them; handouts and live code demos sprinkled are also useful. In our\nexperience, using too many slides encourages students to “check out” of lecture\n(and log into facebook.com), as they know the material is there for them to digest\nlater; using the blackboard makes lecture a “live” viewing experience and thus\n(hopefully) more interactive, dynamic, and enjoyable for the students in your class.\nIf you’d like a copy of the notes we use in preparation for class, please drop us\nan email. We have already shared them with many others around the world.\nOne last request: if you use the free online chapters, please just link to them,\ninstead of making a local copy. This helps us track usage (over 1 million chapters\ndownloaded in the past few years!) and also ensures students get the latest and\ngreatest version.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2850,
      "extraction_method": "Direct"
    },
    {
      "page_number": 10,
      "chapter": null,
      "content": "vi\nTo Students\nIf you are a student reading this book, thank you! It is an honor for us to\nprovide some material to help you in your pursuit of knowledge about operating\nsystems. We both think back fondly towards some textbooks of our undergraduate\ndays (e.g., Hennessy and Patterson [HP90], the classic book on computer architec-\nture) and hope this book will become one of those positive memories for you.\nYou may have noticed this book is free and available online. There is one major\nreason for this: textbooks are generally too expensive. This book, we hope, is\nthe ﬁrst of a new wave of free materials to help those in pursuit of their education,\nregardless of which part of the world they come from or how much they are willing\nto spend for a book. Failing that, it is one free book, which is better than none.\nWe also hope, where possible, to point you to the original sources of much\nof the material in the book: the great papers and persons who have shaped the\nﬁeld of operating systems over the years. Ideas are not pulled out of the air; they\ncome from smart and hard-working people (including numerous Turing-award\nwinners2), and thus we should strive to celebrate those ideas and people where\npossible. In doing so, we hopefully can better understand the revolutions that\nhave taken place, instead of writing texts as if those thoughts have always been\npresent [K62]. Further, perhaps such references will encourage you to dig deeper\non your own; reading the famous papers of our ﬁeld is certainly one of the best\nways to learn.\n2The Turing Award is the highest award in Computer Science; it is like the Nobel Prize,\nexcept that you have never heard of it.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1718,
      "extraction_method": "Direct"
    },
    {
      "page_number": 11,
      "chapter": null,
      "content": "vii\nAcknowledgments\nThis section will contain thanks to those who helped us put the book together.\nThe important thing for now: your name could go here! But, you have to help. So\nsend us some feedback and help debug this book. And you could be famous! Or,\nat least, have your name in some book.\nThe people who have helped so far include: Abhirami Senthilkumaran*, Adam\nDrescher, Adam Eggum, Ahmed Fikri*, Ajaykrishna Raghavan, Alex Wyler, Anand\nMundada, B. Brahmananda Reddy (Minnesota), Bala Subrahmanyam Kambala,\nBenita Bose, Biswajit Mazumder (Clemson), Bobby Jack, Bj¨orn Lindberg, Bren-\nnan Payne, Brian Kroth, Cara Lauritzen, Charlotte Kissinger, Chien-Chung Shen\n(Delaware)*, Cody Hanson, Dan Soendergaard (U. Aarhus), David Hanle (Grin-\nnell), Deepika Muthukumar, Dorian Arnold (New Mexico), Dustin Metzler, Dustin\nPassofaro, Emily Jacobson, Emmett Witchel (Texas), Ernst Biersack (France), Finn\nKuusisto*, Guilherme Baptista, Hamid Reza Ghasemi, Henry Abbey, Hrishikesh\nAmur, Huanchen Zhang*, Jake Gillberg, James Perry (U. Michigan-Dearborn)*, Jay\nLim, Jerod Weinman (Grinnell), Joel Sommers (Colgate), Jonathan Perry (MIT), Jun\nHe, Karl Wallinger, Kaushik Kannan, Kevin Liu*, Lei Tian (U. Nebraska-Lincoln),\nLeslie Schultz, Lihao Wang, Martha Ferris, Masashi Kishikawa (Sony), Matt Rei-\nchoff, Matty Williams, Meng Huang, Mike Griepentrog, Ming Chen (Stonybrook),\nMohammed Alali (Delaware), Murugan Kandaswamy, Natasha Eilbert, Nathan\nDipiazza, Nathan Sullivan, Neeraj Badlani (N.C. State), Nelson Gomez, Nghia\nHuynh (Texas), Patricio Jara, Radford Smith, Ripudaman Singh, Ross Aiken, Rus-\nlan Kiselev, Ryland Herrick, Samer Al-Kiswany, Sandeep Ummadi (Minnesota),\nSatish Chebrolu (NetApp), Satyanarayana Shanmugam*, Seth Pollen, Sharad Punuganti,\nShreevatsa R., Sivaraman Sivaraman*, Srinivasan Thirunarayanan*, Suriyhaprakhas\nBalaram Sankari, Sy Jin Cheah, Thomas Griebel, Tongxin Zheng, Tony Adkins,\nTorin Rudeen (Princeton), Tuo Wang, Varun Vats, Xiang Peng, Xu Di, Yue Zhuo\n(Texas A&M), Yufui Ren, Zef RosnBrick, Zuyu Zhang. Special thanks to those\nmarked with an asterisk above, who have gone above and beyond in their sugges-\ntions for improvement.\nSpecial thanks to Professor Joe Meehean (Lynchburg) for his detailed notes on\neach chapter, to Professor Jerod Weinman (Grinnell) and his entire class for their\nincredible booklets, and to Professor Chien-Chung Shen (Delaware) for his invalu-\nable and detailed reading and comments about the book. All three have helped\nthese authors immeasurably in the reﬁnement of the materials herein.\nAlso, many thanks to the hundreds of students who have taken 537 over the\nyears. In particular, the Fall ’08 class who encouraged the ﬁrst written form of\nthese notes (they were sick of not having any kind of textbook to read – pushy\nstudents!), and then praised them enough for us to keep going (including one hi-\nlarious “ZOMG! You should totally write a textbook!” comment in our course\nevaluations that year).\nA great debt of thanks is also owed to the brave few who took the xv6 project\nlab course, much of which is now incorporated into the main 537 course. From\nSpring ’09: Justin Cherniak, Patrick Deline, Matt Czech, Tony Gregerson, Michael\nGriepentrog, Tyler Harter, Ryan Kroiss, Eric Radzikowski, Wesley Reardan, Rajiv\nVaidyanathan, and Christopher Waclawik. From Fall ’09: Nick Bearson, Aaron\nBrown, Alex Bird, David Capel, Keith Gould, Tom Grim, Jeffrey Hugo, Brandon\nJohnson, John Kjell, Boyan Li, James Loethen, Will McCardell, Ryan Szaroletta, Si-\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 3561,
      "extraction_method": "Direct"
    },
    {
      "page_number": 12,
      "chapter": null,
      "content": "viii\nmon Tso, and Ben Yule. From Spring ’10: Patrick Blesi, Aidan Dennis-Oehling,\nParas Doshi, Jake Friedman, Benjamin Frisch, Evan Hanson, Pikkili Hemanth,\nMichael Jeung, Alex Langenfeld, Scott Rick, Mike Treffert, Garret Staus, Brennan\nWall, Hans Werner, Soo-Young Yang, and Carlos Grifﬁn (almost).\nAlthough they do not directly help with the book, our graduate students have\ntaught us much of what we know about systems. We talk with them regularly\nwhile they are at Wisconsin, but they do all the real work – and by telling us about\nwhat they are doing, we learn new things every week. This list includes the fol-\nlowing collection of current and former students with whom we published pa-\npers; an asterisk marks those who received a Ph.D. under our guidance: Abhishek\nRajimwale, Ao Ma, Brian Forney, Chris Dragga, Deepak Ramamurthi, Florentina\nPopovici*, Haryadi S. Gunawi*, James Nugent, John Bent*, Lanyue Lu, Lakshmi\nBairavasundaram*, Laxman Visampalli, Leo Arulraj, Meenali Rungta, Muthian Si-\nvathanu*, Nathan Burnett*, Nitin Agrawal*, Sriram Subramanian*, Stephen Todd\nJones*, Swaminathan Sundararaman*, Swetha Krishnan, Thanh Do, Thanumalayan\nS. Pillai, Timothy Denehy*, Tyler Harter, Venkat Venkataramani, Vijay Chidambaram,\nVijayan Prabhakaran*, Yiying Zhang*, Yupu Zhang*, Zev Weiss.\nA ﬁnal debt of gratitude is also owed to Aaron Brown, who ﬁrst took this course\nmany years ago (Spring ’09), then took the xv6 lab course (Fall ’09), and ﬁnally was\na graduate teaching assistant for the course for two years or so (Fall ’10 through\nSpring ’12). His tireless work has vastly improved the state of the projects (par-\nticularly those in xv6 land) and thus has helped better the learning experience for\ncountless undergraduates and graduates here at Wisconsin. As Aaron would say\n(in his usual succinct manner): “Thx.”\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1878,
      "extraction_method": "Direct"
    },
    {
      "page_number": 13,
      "chapter": null,
      "content": "ix\nFinal Words\nYeats famously said “Education is not the ﬁlling of a pail but the lighting of a\nﬁre.” He was right but wrong at the same time3. You do have to “ﬁll the pail” a bit,\nand these notes are certainly here to help with that part of your education; after all,\nwhen you go to interview at Google, and they ask you a trick question about how\nto use semaphores, it might be good to actually know what a semaphore is, right?\nBut Yeats’s larger point is obviously on the mark: the real point of education\nis to get you interested in something, to learn something more about the subject\nmatter on your own and not just what you have to digest to get a good grade in\nsome class. As one of our fathers (Remzi’s dad, Vedat Arpaci) used to say, “Learn\nbeyond the classroom”.\nWe created these notes to spark your interest in operating systems, to read more\nabout the topic on your own, to talk to your professor about all the exciting re-\nsearch that is going on in the ﬁeld, and even to get involved with that research. It\nis a great ﬁeld(!), full of exciting and wonderful ideas that have shaped computing\nhistory in profound and important ways. And while we understand this ﬁre won’t\nlight for all of you, we hope it does for many, or even a few. Because once that ﬁre\nis lit, well, that is when you truly become capable of doing something great. And\nthus the real point of the educational process: to go forth, to study many new and\nfascinating topics, to learn, to mature, and most importantly, to ﬁnd something\nthat lights a ﬁre for you.\nAndrea and Remzi\nMarried couple\nProfessors of Computer Science at the University of Wisconsin\nChief Lighters of Fires, hopefully4\n3If he actually said this; as with many famous quotes, the history of this gem is murky.\n4If this sounds like we are admitting some past history as arsonists, you are probably\nmissing the point. Probably. If this sounds cheesy, well, that’s because it is, but you’ll just have\nto forgive us for that.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2014,
      "extraction_method": "Direct"
    },
    {
      "page_number": 14,
      "chapter": null,
      "content": "x\nReferences\n[CK+08] “The xv6 Operating System”\nRuss Cox, Frans Kaashoek, Robert Morris, Nickolai Zeldovich\nFrom: http://pdos.csail.mit.edu/6.828/2008/index.html\nxv6 was developed as a port of the original UNIX version 6 and represents a beautiful, clean, and simple\nway to understand a modern operating system.\n[F96] “Six Easy Pieces: Essentials Of Physics Explained By Its Most Brilliant Teacher”\nRichard P. Feynman\nBasic Books, 1996\nThis book reprints the six easiest chapters of Feynman’s Lectures on Physics, from 1963. If you like\nPhysics, it is a fantastic read.\n[HP90] “Computer Architecture a Quantitative Approach” (1st ed.)\nDavid A. Patterson and John L. Hennessy\nMorgan-Kaufman, 1990\nA book that encouraged each of us at our undergraduate institutions to pursue graduate studies; we later\nboth had the pleasure of working with Patterson, who greatly shaped the foundations of our research\ncareers.\n[KR88] “The C Programming Language”\nBrian Kernighan and Dennis Ritchie\nPrentice-Hall, April 1988\nThe C programming reference that everyone should have, by the people who invented the language.\n[K62] “The Structure of Scientiﬁc Revolutions”\nThomas S. Kuhn\nUniversity of Chicago Press, 1962\nA great and famous read about the fundamentals of the scientiﬁc process. Mop-up work, anomaly, crisis,\nand revolution. We are mostly destined to do mop-up work, alas.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1413,
      "extraction_method": "Direct"
    },
    {
      "page_number": 15,
      "chapter": null,
      "content": "Contents\nTo Everyone\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\niii\nTo Educators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nv\nTo Students . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nvi\nAcknowledgments . . . . . . . . . . . . . . . . . . . . . . . . . . . vii\nFinal Words\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nix\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nx\n1\nA Dialogue on the Book\n1\n2\nIntroduction to Operating Systems\n3\n2.1\nVirtualizing the CPU\n. . . . . . . . . . . . . . . . . . . . . .\n5\n2.2\nVirtualizing Memory . . . . . . . . . . . . . . . . . . . . . .\n7\n2.3\nConcurrency . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n2.4\nPersistence . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n2.5\nDesign Goals . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n2.6\nSome History\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n2.7\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\nI\nVirtualization\n21\n3\nA Dialogue on Virtualization\n23\n4\nThe Abstraction: The Process\n25\n4.1\nThe Abstraction: A Process . . . . . . . . . . . . . . . . . . .\n26\n4.2\nProcess API\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\n4.3\nProcess Creation: A Little More Detail\n. . . . . . . . . . . .\n28\n4.4\nProcess States\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n29\n4.5\nData Structures\n. . . . . . . . . . . . . . . . . . . . . . . . .\n30\n4.6\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n32\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\nxi\n",
      "content_length": 1711,
      "extraction_method": "Direct"
    },
    {
      "page_number": 16,
      "chapter": null,
      "content": "xii\nCONTENTS\n5\nInterlude: Process API\n35\n5.1\nThe fork() System Call . . . . . . . . . . . . . . . . . . . .\n35\n5.2\nAdding wait() System Call . . . . . . . . . . . . . . . . . .\n37\n5.3\nFinally, the exec() System Call . . . . . . . . . . . . . . . .\n38\n5.4\nWhy? Motivating the API\n. . . . . . . . . . . . . . . . . . .\n39\n5.5\nOther Parts of the API . . . . . . . . . . . . . . . . . . . . . .\n42\n5.6\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n42\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n43\n6\nMechanism: Limited Direct Execution\n45\n6.1\nBasic Technique: Limited Direct Execution . . . . . . . . . .\n45\n6.2\nProblem #1: Restricted Operations . . . . . . . . . . . . . . .\n46\n6.3\nProblem #2: Switching Between Processes . . . . . . . . . .\n50\n6.4\nWorried About Concurrency?\n. . . . . . . . . . . . . . . . .\n54\n6.5\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n55\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n57\nHomework (Measurement) . . . . . . . . . . . . . . . . . . . . . .\n58\n7\nScheduling: Introduction\n59\n7.1\nWorkload Assumptions . . . . . . . . . . . . . . . . . . . . .\n59\n7.2\nScheduling Metrics\n. . . . . . . . . . . . . . . . . . . . . . .\n60\n7.3\nFirst In, First Out (FIFO)\n. . . . . . . . . . . . . . . . . . . .\n60\n7.4\nShortest Job First (SJF)\n. . . . . . . . . . . . . . . . . . . . .\n62\n7.5\nShortest Time-to-Completion First (STCF)\n. . . . . . . . . .\n63\n7.6\nRound Robin . . . . . . . . . . . . . . . . . . . . . . . . . . .\n65\n7.7\nIncorporating I/O . . . . . . . . . . . . . . . . . . . . . . . .\n66\n7.8\nNo More Oracle . . . . . . . . . . . . . . . . . . . . . . . . .\n68\n7.9\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n68\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n69\nHomework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n70\n8\nScheduling:The Multi-Level Feedback Queue\n71\n8.1\nMLFQ: Basic Rules\n. . . . . . . . . . . . . . . . . . . . . . .\n72\n8.2\nAttempt #1: How to Change Priority . . . . . . . . . . . . .\n73\n8.3\nAttempt #2: The Priority Boost . . . . . . . . . . . . . . . . .\n76\n8.4\nAttempt #3: Better Accounting . . . . . . . . . . . . . . . . .\n77\n8.5\nTuning MLFQ And Other Issues . . . . . . . . . . . . . . . .\n78\n8.6\nMLFQ: Summary\n. . . . . . . . . . . . . . . . . . . . . . . .\n79\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n81\nHomework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n82\n9\nScheduling: Proportional Share\n83\n9.1\nBasic Concept: Tickets Represent Your Share . . . . . . . . .\n83\n9.2\nTicket Mechanisms\n. . . . . . . . . . . . . . . . . . . . . . .\n85\n9.3\nImplementation . . . . . . . . . . . . . . . . . . . . . . . . .\n86\n9.4\nAn Example . . . . . . . . . . . . . . . . . . . . . . . . . . .\n87\n9.5\nHow To Assign Tickets? . . . . . . . . . . . . . . . . . . . . .\n88\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2962,
      "extraction_method": "Direct"
    },
    {
      "page_number": 17,
      "chapter": null,
      "content": "CONTENTS\nxiii\n9.6\nWhy Not Deterministic?\n. . . . . . . . . . . . . . . . . . . .\n88\n9.7\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n89\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n91\nHomework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n92\n10 Multiprocessor Scheduling (Advanced)\n93\n10.1 Background: Multiprocessor Architecture\n. . . . . . . . . .\n94\n10.2 Don’t Forget Synchronization\n. . . . . . . . . . . . . . . . .\n96\n10.3 One Final Issue: Cache Afﬁnity\n. . . . . . . . . . . . . . . .\n97\n10.4 Single-Queue Scheduling . . . . . . . . . . . . . . . . . . . .\n97\n10.5 Multi-Queue Scheduling . . . . . . . . . . . . . . . . . . . .\n99\n10.6 Linux Multiprocessor Schedulers\n. . . . . . . . . . . . . . . 102\n10.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103\n11 Summary Dialogue on CPU Virtualization\n105\n12 A Dialogue on Memory Virtualization\n107\n13 The Abstraction: Address Spaces\n109\n13.1 Early Systems\n. . . . . . . . . . . . . . . . . . . . . . . . . . 109\n13.2 Multiprogramming and Time Sharing . . . . . . . . . . . . . 110\n13.3 The Address Space\n. . . . . . . . . . . . . . . . . . . . . . . 111\n13.4 Goals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113\n13.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116\n14 Interlude: Memory API\n119\n14.1 Types of Memory . . . . . . . . . . . . . . . . . . . . . . . . 119\n14.2 The malloc() Call . . . . . . . . . . . . . . . . . . . . . . . 120\n14.3 The free() Call\n. . . . . . . . . . . . . . . . . . . . . . . . 122\n14.4 Common Errors . . . . . . . . . . . . . . . . . . . . . . . . . 122\n14.5 Underlying OS Support . . . . . . . . . . . . . . . . . . . . . 125\n14.6 Other Calls . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125\n14.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127\n15 Mechanism: Address Translation\n129\n15.1 Assumptions . . . . . . . . . . . . . . . . . . . . . . . . . . . 130\n15.2 An Example . . . . . . . . . . . . . . . . . . . . . . . . . . . 130\n15.3 Dynamic (Hardware-based) Relocation . . . . . . . . . . . . 133\n15.4 OS Issues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136\n15.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139\nHomework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140\n16 Segmentation\n141\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2744,
      "extraction_method": "Direct"
    },
    {
      "page_number": 18,
      "chapter": null,
      "content": "xiv\nCONTENTS\n16.1 Segmentation: Generalized Base/Bounds\n. . . . . . . . . . 141\n16.2 Which Segment Are We Referring To? . . . . . . . . . . . . . 144\n16.3 What About The Stack? . . . . . . . . . . . . . . . . . . . . . 145\n16.4 Support for Sharing . . . . . . . . . . . . . . . . . . . . . . . 146\n16.5 Fine-grained vs. Coarse-grained Segmentation\n. . . . . . . 147\n16.6 OS Support . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147\n16.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150\nHomework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152\n17 Free-Space Management\n153\n17.1 Assumptions . . . . . . . . . . . . . . . . . . . . . . . . . . . 154\n17.2 Low-level Mechanisms . . . . . . . . . . . . . . . . . . . . . 155\n17.3 Basic Strategies\n. . . . . . . . . . . . . . . . . . . . . . . . . 163\n17.4 Other Approaches . . . . . . . . . . . . . . . . . . . . . . . . 165\n17.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168\n18 Paging: Introduction\n169\n18.1 Where Are Page Tables Stored?\n. . . . . . . . . . . . . . . . 172\n18.2 What’s Actually In The Page Table? . . . . . . . . . . . . . . 173\n18.3 Paging: Also Too Slow\n. . . . . . . . . . . . . . . . . . . . . 174\n18.4 A Memory Trace . . . . . . . . . . . . . . . . . . . . . . . . . 176\n18.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180\nHomework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181\n19 Paging: Faster Translations (TLBs)\n183\n19.1 TLB Basic Algorithm\n. . . . . . . . . . . . . . . . . . . . . . 183\n19.2 Example: Accessing An Array . . . . . . . . . . . . . . . . . 185\n19.3 Who Handles The TLB Miss? . . . . . . . . . . . . . . . . . . 187\n19.4 TLB Contents: What’s In There? . . . . . . . . . . . . . . . . 189\n19.5 TLB Issue: Context Switches . . . . . . . . . . . . . . . . . . 190\n19.6 Issue: Replacement Policy\n. . . . . . . . . . . . . . . . . . . 192\n19.7 A Real TLB Entry\n. . . . . . . . . . . . . . . . . . . . . . . . 193\n19.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195\nHomework (Measurement) . . . . . . . . . . . . . . . . . . . . . . 197\n20 Paging: Smaller Tables\n201\n20.1 Simple Solution: Bigger Pages . . . . . . . . . . . . . . . . . 201\n20.2 Hybrid Approach: Paging and Segments . . . . . . . . . . . 202\n20.3 Multi-level Page Tables . . . . . . . . . . . . . . . . . . . . . 205\n20.4 Inverted Page Tables\n. . . . . . . . . . . . . . . . . . . . . . 212\n20.5 Swapping the Page Tables to Disk . . . . . . . . . . . . . . . 213\n20.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 214\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 3055,
      "extraction_method": "Direct"
    },
    {
      "page_number": 19,
      "chapter": null,
      "content": "CONTENTS\nxv\nHomework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215\n21 Beyond Physical Memory: Mechanisms\n217\n21.1 Swap Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218\n21.2 The Present Bit . . . . . . . . . . . . . . . . . . . . . . . . . . 219\n21.3 The Page Fault . . . . . . . . . . . . . . . . . . . . . . . . . . 220\n21.4 What If Memory Is Full?\n. . . . . . . . . . . . . . . . . . . . 221\n21.5 Page Fault Control Flow\n. . . . . . . . . . . . . . . . . . . . 222\n21.6 When Replacements Really Occur . . . . . . . . . . . . . . . 223\n21.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225\n22 Beyond Physical Memory: Policies\n227\n22.1 Cache Management . . . . . . . . . . . . . . . . . . . . . . . 227\n22.2 The Optimal Replacement Policy\n. . . . . . . . . . . . . . . 228\n22.3 A Simple Policy: FIFO\n. . . . . . . . . . . . . . . . . . . . . 230\n22.4 Another Simple Policy: Random . . . . . . . . . . . . . . . . 232\n22.5 Using History: LRU . . . . . . . . . . . . . . . . . . . . . . . 233\n22.6 Workload Examples . . . . . . . . . . . . . . . . . . . . . . . 234\n22.7 Implementing Historical Algorithms . . . . . . . . . . . . . 237\n22.8 Approximating LRU\n. . . . . . . . . . . . . . . . . . . . . . 238\n22.9 Considering Dirty Pages . . . . . . . . . . . . . . . . . . . . 239\n22.10Other VM Policies . . . . . . . . . . . . . . . . . . . . . . . . 240\n22.11Thrashing\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 240\n22.12Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 242\nHomework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 244\n23 The VAX/VMS Virtual Memory System\n245\n23.1 Background\n. . . . . . . . . . . . . . . . . . . . . . . . . . . 245\n23.2 Memory Management Hardware . . . . . . . . . . . . . . . 246\n23.3 A Real Address Space . . . . . . . . . . . . . . . . . . . . . . 247\n23.4 Page Replacement . . . . . . . . . . . . . . . . . . . . . . . . 249\n23.5 Other Neat VM Tricks . . . . . . . . . . . . . . . . . . . . . . 250\n23.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 252\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253\n24 Summary Dialogue on Memory Virtualization\n255\nII Concurrency\n259\n25 A Dialogue on Concurrency\n261\n26 Concurrency: An Introduction\n263\n26.1 An Example: Thread Creation . . . . . . . . . . . . . . . . . 264\n26.2 Why It Gets Worse: Shared Data . . . . . . . . . . . . . . . . 267\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2666,
      "extraction_method": "Direct"
    },
    {
      "page_number": 20,
      "chapter": null,
      "content": "xvi\nCONTENTS\n26.3 The Heart of the Problem: Uncontrolled Scheduling\n. . . . 269\n26.4 The Wish For Atomicity . . . . . . . . . . . . . . . . . . . . . 271\n26.5 One More Problem: Waiting For Another . . . . . . . . . . . 273\n26.6 Summary: Why in OS Class? . . . . . . . . . . . . . . . . . . 273\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275\nHomework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 276\n27 Interlude: Thread API\n279\n27.1 Thread Creation . . . . . . . . . . . . . . . . . . . . . . . . . 279\n27.2 Thread Completion . . . . . . . . . . . . . . . . . . . . . . . 280\n27.3 Locks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 283\n27.4 Condition Variables . . . . . . . . . . . . . . . . . . . . . . . 285\n27.5 Compiling and Running\n. . . . . . . . . . . . . . . . . . . . 287\n27.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 287\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289\n28 Locks\n291\n28.1 Locks: The Basic Idea . . . . . . . . . . . . . . . . . . . . . . 291\n28.2 Pthread Locks . . . . . . . . . . . . . . . . . . . . . . . . . . 292\n28.3 Building A Lock . . . . . . . . . . . . . . . . . . . . . . . . . 293\n28.4 Evaluating Locks\n. . . . . . . . . . . . . . . . . . . . . . . . 293\n28.5 Controlling Interrupts . . . . . . . . . . . . . . . . . . . . . . 294\n28.6 Test And Set (Atomic Exchange) . . . . . . . . . . . . . . . . 295\n28.7 Building A Working Spin Lock . . . . . . . . . . . . . . . . . 297\n28.8 Evaluating Spin Locks\n. . . . . . . . . . . . . . . . . . . . . 299\n28.9 Compare-And-Swap\n. . . . . . . . . . . . . . . . . . . . . . 299\n28.10Load-Linked and Store-Conditional . . . . . . . . . . . . . . 300\n28.11Fetch-And-Add . . . . . . . . . . . . . . . . . . . . . . . . . 302\n28.12Summary: So Much Spinning\n. . . . . . . . . . . . . . . . . 303\n28.13A Simple Approach: Just Yield, Baby . . . . . . . . . . . . . 304\n28.14Using Queues: Sleeping Instead Of Spinning . . . . . . . . . 305\n28.15Different OS, Different Support\n. . . . . . . . . . . . . . . . 307\n28.16Two-Phase Locks\n. . . . . . . . . . . . . . . . . . . . . . . . 307\n28.17Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 308\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 309\n29 Lock-based Concurrent Data Structures\n311\n29.1 Concurrent Counters . . . . . . . . . . . . . . . . . . . . . . 311\n29.2 Concurrent Linked Lists\n. . . . . . . . . . . . . . . . . . . . 316\n29.3 Concurrent Queues . . . . . . . . . . . . . . . . . . . . . . . 319\n29.4 Concurrent Hash Table . . . . . . . . . . . . . . . . . . . . . 320\n29.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 322\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 323\n30 Condition Variables\n325\n30.1 Deﬁnition and Routines . . . . . . . . . . . . . . . . . . . . . 326\n30.2 The Producer/Consumer (Bound Buffer) Problem . . . . . . 329\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 3028,
      "extraction_method": "Direct"
    },
    {
      "page_number": 21,
      "chapter": null,
      "content": "CONTENTS\nxvii\n30.3 Covering Conditions\n. . . . . . . . . . . . . . . . . . . . . . 337\n30.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 338\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 339\n31 Semaphores\n341\n31.1 Semaphores: A Deﬁnition\n. . . . . . . . . . . . . . . . . . . 341\n31.2 Binary Semaphores (Locks) . . . . . . . . . . . . . . . . . . . 343\n31.3 Semaphores As Condition Variables . . . . . . . . . . . . . . 344\n31.4 The Producer/Consumer (Bounded-Buffer) Problem . . . . 346\n31.5 Reader-Writer Locks\n. . . . . . . . . . . . . . . . . . . . . . 350\n31.6 The Dining Philosophers . . . . . . . . . . . . . . . . . . . . 352\n31.7 How To Implement Semaphores . . . . . . . . . . . . . . . . 355\n31.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 356\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 357\n32 Common Concurrency Problems\n359\n32.1 What Types Of Bugs Exist? . . . . . . . . . . . . . . . . . . . 359\n32.2 Non-Deadlock Bugs . . . . . . . . . . . . . . . . . . . . . . . 360\n32.3 Deadlock Bugs . . . . . . . . . . . . . . . . . . . . . . . . . . 363\n32.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 370\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 371\n33 Event-based Concurrency (Advanced)\n373\n33.1 The Basic Idea: An Event Loop . . . . . . . . . . . . . . . . . 373\n33.2 An Important API: select() (or poll())\n. . . . . . . . . 374\n33.3 Using select()\n. . . . . . . . . . . . . . . . . . . . . . . . 375\n33.4 Why Simpler? No Locks Needed\n. . . . . . . . . . . . . . . 376\n33.5 A Problem: Blocking System Calls . . . . . . . . . . . . . . . 377\n33.6 A Solution: Asynchronous I/O\n. . . . . . . . . . . . . . . . 377\n33.7 Another Problem: State Management . . . . . . . . . . . . . 380\n33.8 What Is Still Difﬁcult With Events . . . . . . . . . . . . . . . 381\n33.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 381\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 382\n34 Summary Dialogue on Concurrency\n383\nIII Persistence\n385\n35 A Dialogue on Persistence\n387\n36 I/O Devices\n389\n36.1 System Architecture . . . . . . . . . . . . . . . . . . . . . . . 389\n36.2 A Canonical Device . . . . . . . . . . . . . . . . . . . . . . . 390\n36.3 The Canonical Protocol . . . . . . . . . . . . . . . . . . . . . 391\n36.4 Lowering CPU Overhead With Interrupts\n. . . . . . . . . . 392\n36.5 More Efﬁcient Data Movement With DMA . . . . . . . . . . 393\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2577,
      "extraction_method": "Direct"
    },
    {
      "page_number": 22,
      "chapter": null,
      "content": "xviii\nCONTENTS\n36.6 Methods Of Device Interaction . . . . . . . . . . . . . . . . . 394\n36.7 Fitting Into The OS: The Device Driver . . . . . . . . . . . . 395\n36.8 Case Study: A Simple IDE Disk Driver . . . . . . . . . . . . 396\n36.9 Historical Notes . . . . . . . . . . . . . . . . . . . . . . . . . 399\n36.10Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 399\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 400\n37 Hard Disk Drives\n403\n37.1 The Interface . . . . . . . . . . . . . . . . . . . . . . . . . . . 403\n37.2 Basic Geometry\n. . . . . . . . . . . . . . . . . . . . . . . . . 404\n37.3 A Simple Disk Drive\n. . . . . . . . . . . . . . . . . . . . . . 404\n37.4 I/O Time: Doing The Math . . . . . . . . . . . . . . . . . . . 408\n37.5 Disk Scheduling . . . . . . . . . . . . . . . . . . . . . . . . . 412\n37.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 416\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 417\nHomework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 418\n38 Redundant Arrays of Inexpensive Disks (RAIDs)\n421\n38.1 Interface And RAID Internals\n. . . . . . . . . . . . . . . . . 422\n38.2 Fault Model\n. . . . . . . . . . . . . . . . . . . . . . . . . . . 423\n38.3 How To Evaluate A RAID\n. . . . . . . . . . . . . . . . . . . 423\n38.4 RAID Level 0: Striping . . . . . . . . . . . . . . . . . . . . . 424\n38.5 RAID Level 1: Mirroring . . . . . . . . . . . . . . . . . . . . 427\n38.6 RAID Level 4: Saving Space With Parity\n. . . . . . . . . . . 430\n38.7 RAID Level 5: Rotating Parity . . . . . . . . . . . . . . . . . 434\n38.8 RAID Comparison: A Summary . . . . . . . . . . . . . . . . 435\n38.9 Other Interesting RAID Issues . . . . . . . . . . . . . . . . . 436\n38.10Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 436\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 437\nHomework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 439\n39 Interlude: File and Directories\n441\n39.1 Files and Directories . . . . . . . . . . . . . . . . . . . . . . . 441\n39.2 The File System Interface . . . . . . . . . . . . . . . . . . . . 443\n39.3 Creating Files\n. . . . . . . . . . . . . . . . . . . . . . . . . . 443\n39.4 Reading and Writing Files\n. . . . . . . . . . . . . . . . . . . 444\n39.5 Reading And Writing, But Not Sequentially . . . . . . . . . 446\n39.6 Writing Immediately with fsync()\n. . . . . . . . . . . . . 447\n39.7 Renaming Files\n. . . . . . . . . . . . . . . . . . . . . . . . . 448\n39.8 Getting Information About Files . . . . . . . . . . . . . . . . 449\n39.9 Removing Files\n. . . . . . . . . . . . . . . . . . . . . . . . . 450\n39.10Making Directories\n. . . . . . . . . . . . . . . . . . . . . . . 450\n39.11Reading Directories . . . . . . . . . . . . . . . . . . . . . . . 451\n39.12Deleting Directories . . . . . . . . . . . . . . . . . . . . . . . 452\n39.13Hard Links . . . . . . . . . . . . . . . . . . . . . . . . . . . . 452\n39.14Symbolic Links\n. . . . . . . . . . . . . . . . . . . . . . . . . 454\n39.15Making and Mounting a File System\n. . . . . . . . . . . . . 456\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 3197,
      "extraction_method": "Direct"
    },
    {
      "page_number": 23,
      "chapter": null,
      "content": "CONTENTS\nxix\n39.16Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 457\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 458\nHomework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 459\n40 File System Implementation\n461\n40.1 The Way To Think . . . . . . . . . . . . . . . . . . . . . . . . 461\n40.2 Overall Organization . . . . . . . . . . . . . . . . . . . . . . 462\n40.3 File Organization: The Inode . . . . . . . . . . . . . . . . . . 464\n40.4 Directory Organization . . . . . . . . . . . . . . . . . . . . . 469\n40.5 Free Space Management\n. . . . . . . . . . . . . . . . . . . . 469\n40.6 Access Paths: Reading and Writing . . . . . . . . . . . . . . 470\n40.7 Caching and Buffering\n. . . . . . . . . . . . . . . . . . . . . 474\n40.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 475\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 476\nHomework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 477\n41 Locality and The Fast File System\n479\n41.1 The Problem: Poor Performance . . . . . . . . . . . . . . . . 479\n41.2 FFS: Disk Awareness Is The Solution\n. . . . . . . . . . . . . 481\n41.3 Organizing Structure: The Cylinder Group . . . . . . . . . . 481\n41.4 Policies: How To Allocate Files and Directories\n. . . . . . . 482\n41.5 Measuring File Locality . . . . . . . . . . . . . . . . . . . . . 483\n41.6 The Large-File Exception . . . . . . . . . . . . . . . . . . . . 484\n41.7 A Few Other Things About FFS . . . . . . . . . . . . . . . . 486\n41.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 488\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 489\n42 Crash Consistency: FSCK and Journaling\n491\n42.1 A Detailed Example . . . . . . . . . . . . . . . . . . . . . . . 492\n42.2 Solution #1: The File System Checker . . . . . . . . . . . . . 495\n42.3 Solution #2: Journaling (or Write-Ahead Logging) . . . . . . 497\n42.4 Solution #3: Other Approaches\n. . . . . . . . . . . . . . . . 507\n42.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 508\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 509\n43 Log-structured File Systems\n511\n43.1 Writing To Disk Sequentially . . . . . . . . . . . . . . . . . . 512\n43.2 Writing Sequentially And Effectively . . . . . . . . . . . . . 513\n43.3 How Much To Buffer? . . . . . . . . . . . . . . . . . . . . . . 514\n43.4 Problem: Finding Inodes . . . . . . . . . . . . . . . . . . . . 515\n43.5 Solution Through Indirection: The Inode Map . . . . . . . . 515\n43.6 The Checkpoint Region . . . . . . . . . . . . . . . . . . . . . 516\n43.7 Reading A File From Disk: A Recap . . . . . . . . . . . . . . 517\n43.8 What About Directories? . . . . . . . . . . . . . . . . . . . . 517\n43.9 A New Problem: Garbage Collection . . . . . . . . . . . . . 518\n43.10Determining Block Liveness . . . . . . . . . . . . . . . . . . 520\n43.11A Policy Question: Which Blocks To Clean, And When?\n. . 521\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 3052,
      "extraction_method": "Direct"
    },
    {
      "page_number": 24,
      "chapter": null,
      "content": "xx\nCONTENTS\n43.12Crash Recovery And The Log\n. . . . . . . . . . . . . . . . . 521\n43.13Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 522\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 524\n44 Data Integrity and Protection\n527\n44.1 Disk Failure Modes . . . . . . . . . . . . . . . . . . . . . . . 527\n44.2 Handling Latent Sector Errors . . . . . . . . . . . . . . . . . 529\n44.3 Detecting Corruption: The Checksum . . . . . . . . . . . . . 530\n44.4 Using Checksums . . . . . . . . . . . . . . . . . . . . . . . . 533\n44.5 A New Problem: Misdirected Writes\n. . . . . . . . . . . . . 534\n44.6 One Last Problem: Lost Writes . . . . . . . . . . . . . . . . . 535\n44.7 Scrubbing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 535\n44.8 Overheads Of Checksumming . . . . . . . . . . . . . . . . . 536\n44.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 536\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 537\n45 Summary Dialogue on Persistence\n539\n46 A Dialogue on Distribution\n541\n47 Distributed Systems\n543\n47.1 Communication Basics . . . . . . . . . . . . . . . . . . . . . 544\n47.2 Unreliable Communication Layers\n. . . . . . . . . . . . . . 545\n47.3 Reliable Communication Layers . . . . . . . . . . . . . . . . 547\n47.4 Communication Abstractions\n. . . . . . . . . . . . . . . . . 549\n47.5 Remote Procedure Call (RPC)\n. . . . . . . . . . . . . . . . . 551\n47.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 556\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 557\n48 Sun’s Network File System (NFS)\n559\n48.1 A Basic Distributed File System . . . . . . . . . . . . . . . . 560\n48.2 On To NFS . . . . . . . . . . . . . . . . . . . . . . . . . . . . 561\n48.3 Focus: Simple and Fast Server Crash Recovery . . . . . . . . 561\n48.4 Key To Fast Crash Recovery: Statelessness . . . . . . . . . . 562\n48.5 The NFSv2 Protocol . . . . . . . . . . . . . . . . . . . . . . . 563\n48.6 From Protocol to Distributed File System . . . . . . . . . . . 565\n48.7 Handling Server Failure with Idempotent Operations . . . . 567\n48.8 Improving Performance: Client-side Caching\n. . . . . . . . 569\n48.9 The Cache Consistency Problem . . . . . . . . . . . . . . . . 569\n48.10Assessing NFS Cache Consistency . . . . . . . . . . . . . . . 571\n48.11Implications on Server-Side Write Buffering . . . . . . . . . 571\n48.12Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 573\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 574\n49 The Andrew File System (AFS)\n575\n49.1 AFS Version 1\n. . . . . . . . . . . . . . . . . . . . . . . . . . 575\n49.2 Problems with Version 1\n. . . . . . . . . . . . . . . . . . . . 576\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2812,
      "extraction_method": "Direct"
    },
    {
      "page_number": 25,
      "chapter": null,
      "content": "CONTENTS\nxxi\n49.3 Improving the Protocol . . . . . . . . . . . . . . . . . . . . . 578\n49.4 AFS Version 2\n. . . . . . . . . . . . . . . . . . . . . . . . . . 578\n49.5 Cache Consistency\n. . . . . . . . . . . . . . . . . . . . . . . 580\n49.6 Crash Recovery\n. . . . . . . . . . . . . . . . . . . . . . . . . 582\n49.7 Scale And Performance Of AFSv2 . . . . . . . . . . . . . . . 582\n49.8 AFS: Other Improvements . . . . . . . . . . . . . . . . . . . 584\n49.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 585\nReferences\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 587\n50 Summary Dialogue on Distribution\n589\nGeneral Index\n591\nAsides\n601\nTips\n603\nCruces\n605\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 732,
      "extraction_method": "Direct"
    },
    {
      "page_number": 26,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 27,
      "chapter": null,
      "content": "List of Figures\n2.1\nSimple Example: Code That Loops and Prints\n. . . . . . . . .\n5\n2.2\nRunning Many Programs At Once . . . . . . . . . . . . . . . .\n6\n2.3\nA Program that Accesses Memory\n. . . . . . . . . . . . . . . .\n7\n2.4\nRunning The Memory Program Multiple Times . . . . . . . .\n8\n2.5\nA Multi-threaded Program\n. . . . . . . . . . . . . . . . . . . .\n9\n2.6\nA Program That Does I/O . . . . . . . . . . . . . . . . . . . . .\n11\n4.1\nLoading: From Program To Process . . . . . . . . . . . . . . . .\n28\n4.2\nProcess: State Transitions\n. . . . . . . . . . . . . . . . . . . . .\n30\n4.3\nThe xv6 Proc Structure . . . . . . . . . . . . . . . . . . . . . . .\n31\n5.1\np1.c: Calling fork() . . . . . . . . . . . . . . . . . . . . . . .\n36\n5.2\np2.c: Calling fork() And wait() . . . . . . . . . . . . . . .\n37\n5.3\np3.c: Calling fork(), wait(), And exec()\n. . . . . . . . .\n39\n5.4\np4.c: All Of The Above With Redirection\n. . . . . . . . . . .\n41\n6.1\nThe xv6 Context Switch Code . . . . . . . . . . . . . . . . . . .\n54\n7.1\nFIFO Simple Example\n. . . . . . . . . . . . . . . . . . . . . . .\n61\n7.2\nWhy FIFO Is Not That Great\n. . . . . . . . . . . . . . . . . . .\n61\n7.3\nSJF Simple Example\n. . . . . . . . . . . . . . . . . . . . . . . .\n62\n7.4\nSJF With Late Arrivals From B and C . . . . . . . . . . . . . . .\n63\n7.5\nSTCF Simple Example . . . . . . . . . . . . . . . . . . . . . . .\n64\n7.6\nSJF Again (Bad for Response Time)\n. . . . . . . . . . . . . . .\n65\n7.7\nRound Robin (Good for Response Time)\n. . . . . . . . . . . .\n65\n7.8\nPoor Use of Resources . . . . . . . . . . . . . . . . . . . . . . .\n67\n7.9\nOverlap Allows Better Use of Resources . . . . . . . . . . . . .\n67\n8.1\nMLFQ Example . . . . . . . . . . . . . . . . . . . . . . . . . . .\n73\n8.2\nLong-running Job Over Time . . . . . . . . . . . . . . . . . . .\n74\n8.3\nAlong Came An Interactive Job . . . . . . . . . . . . . . . . . .\n74\n8.4\nA Mixed I/O-intensive and CPU-intensive Workload . . . . .\n75\n8.5\nWithout (Left) and With (Right) Priority Boost . . . . . . . . .\n76\nxxiii\n",
      "content_length": 2017,
      "extraction_method": "Direct"
    },
    {
      "page_number": 28,
      "chapter": null,
      "content": "xxiv\nLIST OF FIGURES\n8.6\nWithout (Left) and With (Right) Gaming Tolerance\n. . . . . .\n77\n8.7\nLower Priority, Longer Quanta . . . . . . . . . . . . . . . . . .\n78\n9.1\nLottery Scheduling Decision Code . . . . . . . . . . . . . . . .\n86\n9.2\nLottery Fairness Study . . . . . . . . . . . . . . . . . . . . . . .\n87\n10.1 Single CPU With Cache\n. . . . . . . . . . . . . . . . . . . . . .\n94\n10.2 Two CPUs With Caches Sharing Memory . . . . . . . . . . . .\n95\n10.3 Simple List Delete Code . . . . . . . . . . . . . . . . . . . . . .\n97\n13.1 Operating Systems: The Early Days . . . . . . . . . . . . . . . 109\n13.2 Three Processes: Sharing Memory . . . . . . . . . . . . . . . . 110\n13.3 An Example Address Space . . . . . . . . . . . . . . . . . . . . 111\n15.1 A Process And Its Address Space . . . . . . . . . . . . . . . . . 132\n15.2 Physical Memory with a Single Relocated Process . . . . . . . 133\n16.1 An Address Space (Again) . . . . . . . . . . . . . . . . . . . . . 142\n16.2 Placing Segments In Physical Memory\n. . . . . . . . . . . . . 143\n16.3 Non-compacted and Compacted Memory . . . . . . . . . . . . 148\n17.1 An Allocated Region Plus Header\n. . . . . . . . . . . . . . . . 157\n17.2 Speciﬁc Contents Of The Header . . . . . . . . . . . . . . . . . 157\n17.3 A Heap With One Free Chunk\n. . . . . . . . . . . . . . . . . . 159\n17.4 A Heap: After One Allocation . . . . . . . . . . . . . . . . . . . 159\n17.5 Free Space With Three Chunks Allocated . . . . . . . . . . . . 160\n17.6 Free Space With Two Chunks Allocated . . . . . . . . . . . . . 161\n17.7 A Non-Coalesced Free List . . . . . . . . . . . . . . . . . . . . . 162\n18.1 A Simple 64-byte Address Space . . . . . . . . . . . . . . . . . 169\n18.2 64-Byte Address Space Placed In Physical Memory\n. . . . . . 170\n18.3 The Address Translation Process . . . . . . . . . . . . . . . . . 172\n18.4 Example: Page Table in Kernel Physical Memory\n. . . . . . . 173\n18.5 An x86 Page Table Entry (PTE)\n. . . . . . . . . . . . . . . . . . 174\n18.6 Accessing Memory With Paging\n. . . . . . . . . . . . . . . . . 175\n18.7 A Virtual (And Physical) Memory Trace . . . . . . . . . . . . . 178\n19.1 TLB Control Flow Algorithm . . . . . . . . . . . . . . . . . . . 184\n19.2 Example: An Array In A Tiny Address Space . . . . . . . . . . 185\n19.3 TLB Control Flow Algorithm (OS Handled)\n. . . . . . . . . . 188\n19.4 A MIPS TLB Entry . . . . . . . . . . . . . . . . . . . . . . . . . 193\n19.5 Discovering TLB Sizes and Miss Costs\n. . . . . . . . . . . . . 198\n20.1 A 16-KB Address Space With 1-KB Pages . . . . . . . . . . . . 203\n20.2 Linear (Left) And Multi-Level (Right) Page Tables . . . . . . . 206\n20.3 A 16-KB Address Space With 64-byte Pages . . . . . . . . . . . 207\n20.4 Multi-level Page Table Control Flow . . . . . . . . . . . . . . . 212\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2832,
      "extraction_method": "Direct"
    },
    {
      "page_number": 29,
      "chapter": null,
      "content": "LIST OF FIGURES\nxxv\n21.1 Physical Memory and Swap Space . . . . . . . . . . . . . . . . 219\n21.2 Page-Fault Control Flow Algorithm (Hardware) . . . . . . . . 222\n21.3 Page-Fault Control Flow Algorithm (Software) . . . . . . . . . 223\n22.1 Random Performance over 10,000 Trials . . . . . . . . . . . . . 232\n22.2 The No-Locality Workload . . . . . . . . . . . . . . . . . . . . . 235\n22.3 The 80-20 Workload . . . . . . . . . . . . . . . . . . . . . . . . . 236\n22.4 The Looping Workload . . . . . . . . . . . . . . . . . . . . . . . 237\n22.5 The 80-20 Workload With Clock\n. . . . . . . . . . . . . . . . . 239\n23.1 The VAX/VMS Address Space\n. . . . . . . . . . . . . . . . . . 247\n26.1 A Single-Threaded Address Space . . . . . . . . . . . . . . . . 264\n26.2 Simple Thread Creation Code (t0.c)\n. . . . . . . . . . . . . . . 265\n26.3 Sharing Data: Oh Oh (t2)\n. . . . . . . . . . . . . . . . . . . . . 267\n27.1 Creating a Thread . . . . . . . . . . . . . . . . . . . . . . . . . . 281\n27.2 Waiting for Thread Completion . . . . . . . . . . . . . . . . . . 282\n27.3 Simpler Argument Passing to a Thread . . . . . . . . . . . . . 283\n27.4 An Example Wrapper . . . . . . . . . . . . . . . . . . . . . . . . 285\n28.1 First Attempt: A Simple Flag . . . . . . . . . . . . . . . . . . . 296\n28.2 A Simple Spin Lock Using Test-and-set . . . . . . . . . . . . . 298\n28.3 Compare-and-swap . . . . . . . . . . . . . . . . . . . . . . . . . 299\n28.4 Load-linked And Store-conditional\n. . . . . . . . . . . . . . . 301\n28.5 Using LL/SC To Build A Lock . . . . . . . . . . . . . . . . . . . 301\n28.6 Ticket Locks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 303\n28.7 Lock With Test-and-set And Yield\n. . . . . . . . . . . . . . . . 304\n28.8 Lock With Queues, Test-and-set, Yield, And Wakeup . . . . . 306\n28.9 Linux-based Futex Locks . . . . . . . . . . . . . . . . . . . . . . 308\n29.1 A Counter Without Locks . . . . . . . . . . . . . . . . . . . . . 312\n29.2 A Counter With Locks . . . . . . . . . . . . . . . . . . . . . . . 312\n29.3 Performance of Traditional vs. Sloppy Counters . . . . . . . . 313\n29.4 Sloppy Counter Implementation . . . . . . . . . . . . . . . . . 315\n29.5 Scaling Sloppy Counters . . . . . . . . . . . . . . . . . . . . . . 316\n29.6 Concurrent Linked List\n. . . . . . . . . . . . . . . . . . . . . . 317\n29.7 Concurrent Linked List: Rewritten . . . . . . . . . . . . . . . . 318\n29.8 Michael and Scott Concurrent Queue\n. . . . . . . . . . . . . . 320\n29.9 A Concurrent Hash Table\n. . . . . . . . . . . . . . . . . . . . . 321\n29.10Scaling Hash Tables\n. . . . . . . . . . . . . . . . . . . . . . . . 321\n30.1 A Parent Waiting For Its Child\n. . . . . . . . . . . . . . . . . . 325\n30.2 Parent Waiting For Child: Spin-based Approach . . . . . . . . 326\n30.3 Parent Waiting For Child: Use A Condition Variable\n. . . . . 327\n30.4 The Put and Get Routines (Version 1) . . . . . . . . . . . . . . 330\n30.5 Producer/Consumer Threads (Version 1)\n. . . . . . . . . . . . 330\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 3030,
      "extraction_method": "Direct"
    },
    {
      "page_number": 30,
      "chapter": null,
      "content": "xxvi\nLIST OF FIGURES\n30.6 Producer/Consumer: Single CV and If Statement\n. . . . . . . 331\n30.7 Producer/Consumer: Single CV and While . . . . . . . . . . . 333\n30.8 Producer/Consumer: Two CVs and While . . . . . . . . . . . . 335\n30.9 The Final Put and Get Routines . . . . . . . . . . . . . . . . . . 336\n30.10The Final Working Solution . . . . . . . . . . . . . . . . . . . . 336\n30.11Covering Conditions: An Example . . . . . . . . . . . . . . . . 338\n31.1 Initializing A Semaphore . . . . . . . . . . . . . . . . . . . . . 342\n31.2 Semaphore: Deﬁnitions of Wait and Post . . . . . . . . . . . . 342\n31.3 A Binary Semaphore, a.k.a. a Lock . . . . . . . . . . . . . . . . 343\n31.4 A Parent Waiting For Its Child\n. . . . . . . . . . . . . . . . . . 345\n31.5 The Put and Get Routines . . . . . . . . . . . . . . . . . . . . . 347\n31.6 Adding the Full and Empty Conditions . . . . . . . . . . . . . 347\n31.7 Adding Mutual Exclusion (Incorrectly)\n. . . . . . . . . . . . . 349\n31.8 Adding Mutual Exclusion (Correctly) . . . . . . . . . . . . . . 350\n31.9 A Simple Reader-Writer Lock . . . . . . . . . . . . . . . . . . . 351\n31.10The Dining Philosophers\n. . . . . . . . . . . . . . . . . . . . . 353\n31.11The getforks() and putforks() Routines\n. . . . . . . . . 354\n31.12Implementing Zemaphores with Locks and CVs . . . . . . . . 355\n32.1 The Deadlock Dependency Graph . . . . . . . . . . . . . . . . 364\n33.1 Simple Code using select()\n. . . . . . . . . . . . . . . . . . 376\n36.1 Prototypical System Architecture . . . . . . . . . . . . . . . . . 390\n36.2 A Canonical Device . . . . . . . . . . . . . . . . . . . . . . . . . 391\n36.3 The File System Stack\n. . . . . . . . . . . . . . . . . . . . . . . 396\n36.4 The IDE Interface . . . . . . . . . . . . . . . . . . . . . . . . . . 397\n36.5 The xv6 IDE Disk Driver (Simpliﬁed) . . . . . . . . . . . . . . 398\n37.1 A Disk With Just A Single Track . . . . . . . . . . . . . . . . . 404\n37.2 A Single Track Plus A Head . . . . . . . . . . . . . . . . . . . . 405\n37.3 Three Tracks Plus A Head (Right: With Seek)\n. . . . . . . . . 406\n37.4 Three Tracks: Track Skew Of 2 . . . . . . . . . . . . . . . . . . 407\n37.5 SSTF: Scheduling Requests 21 And 2\n. . . . . . . . . . . . . . 412\n37.6 SSTF: Sometimes Not Good Enough . . . . . . . . . . . . . . . 414\n39.1 An Example Directory Tree\n. . . . . . . . . . . . . . . . . . . . 442\n41.1 FFS Locality For SEER Traces . . . . . . . . . . . . . . . . . . . 483\n41.2 Amortization: How Big Do Chunks Have To Be? . . . . . . . . 486\n41.3 FFS: Standard Versus Parameterized Placement\n. . . . . . . . 487\n47.1 Example UDP/IP Client/Server Code . . . . . . . . . . . . . . . 545\n47.2 A Simple UDP Library . . . . . . . . . . . . . . . . . . . . . . . 546\n47.3 Message Plus Acknowledgment\n. . . . . . . . . . . . . . . . . 547\n47.4 Message Plus Acknowledgment: Dropped Request . . . . . . 548\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2921,
      "extraction_method": "Direct"
    },
    {
      "page_number": 31,
      "chapter": null,
      "content": "LIST OF FIGURES\nxxvii\n47.5 Message Plus Acknowledgment: Dropped Reply\n. . . . . . . 549\n48.1 A Generic Client/Server System\n. . . . . . . . . . . . . . . . . 559\n48.2 Distributed File System Architecture\n. . . . . . . . . . . . . . 560\n48.3 Client Code: Reading From A File . . . . . . . . . . . . . . . . 562\n48.4 The NFS Protocol: Examples\n. . . . . . . . . . . . . . . . . . . 564\n48.5 The Three Types of Loss . . . . . . . . . . . . . . . . . . . . . . 568\n48.6 The Cache Consistency Problem . . . . . . . . . . . . . . . . . 570\n49.1 AFSv1 Protocol Highlights\n. . . . . . . . . . . . . . . . . . . . 576\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 650,
      "extraction_method": "Direct"
    },
    {
      "page_number": 32,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 33,
      "chapter": null,
      "content": "List of Tables\n6.1\nDirection Execution Protocol (Without Limits) . . . . . . . . .\n46\n6.2\nLimited Direction Execution Protocol . . . . . . . . . . . . . .\n49\n6.3\nLimited Direction Execution Protocol (Timer Interrupt) . . . .\n53\n9.1\nStride Scheduling: A Trace\n. . . . . . . . . . . . . . . . . . . .\n89\n16.1 Segment Register Values . . . . . . . . . . . . . . . . . . . . . . 143\n16.2 Segment Registers (With Negative-Growth Support) . . . . . 146\n16.3 Segment Register Values (with Protection) . . . . . . . . . . . 147\n20.1 A Page Table For 16-KB Address Space\n. . . . . . . . . . . . . 203\n20.2 A Page Directory, And Pieces Of Page Table\n. . . . . . . . . . 209\n22.1 Tracing the Optimal Policy\n. . . . . . . . . . . . . . . . . . . . 229\n22.2 Tracing the FIFO Policy\n. . . . . . . . . . . . . . . . . . . . . . 231\n22.3 Tracing the Random Policy\n. . . . . . . . . . . . . . . . . . . . 232\n22.4 Tracing the LRU Policy . . . . . . . . . . . . . . . . . . . . . . . 233\n26.1 Thread Trace (1) . . . . . . . . . . . . . . . . . . . . . . . . . . . 266\n26.2 Thread Trace (2) . . . . . . . . . . . . . . . . . . . . . . . . . . . 266\n26.3 Thread Trace (3) . . . . . . . . . . . . . . . . . . . . . . . . . . . 266\n26.4 The Problem: Up Close and Personal\n. . . . . . . . . . . . . . 270\n28.1 Trace: No Mutual Exclusion . . . . . . . . . . . . . . . . . . . . 296\n29.1 Tracing the Sloppy Counters\n. . . . . . . . . . . . . . . . . . . 314\n30.1 Thread Trace: Broken Solution (Version 1)\n. . . . . . . . . . . 332\n30.2 Thread Trace: Broken Solution (Version 2)\n. . . . . . . . . . . 334\n31.1 Thread Trace: Single Thread Using A Semaphore . . . . . . . 343\n31.2 Thread Trace: Two Threads Using A Semaphore . . . . . . . . 344\n31.3 Thread Trace: Parent Waiting For Child (Case 1) . . . . . . . . 346\nxxix\n",
      "content_length": 1795,
      "extraction_method": "Direct"
    },
    {
      "page_number": 34,
      "chapter": null,
      "content": "xxx\nLIST OF TABLES\n31.4 Thread Trace: Parent Waiting For Child (Case 2) . . . . . . . . 346\n32.1 Bugs In Modern Applications . . . . . . . . . . . . . . . . . . . 360\n37.1 Disk Drive Specs: SCSI Versus SATA . . . . . . . . . . . . . . 409\n37.2 Disk Drive Performance: SCSI Versus SATA . . . . . . . . . . 410\n38.1 RAID-0: Simple Striping\n. . . . . . . . . . . . . . . . . . . . . 424\n38.2 Striping with a Bigger Chunk Size . . . . . . . . . . . . . . . . 424\n38.3 Simple RAID-1: Mirroring\n. . . . . . . . . . . . . . . . . . . . 428\n38.4 Full-stripe Writes In RAID-4\n. . . . . . . . . . . . . . . . . . . 432\n38.5 Example: Writes To 4, 13, And Respective Parity Blocks . . . . 433\n38.6 RAID-5 With Rotated Parity . . . . . . . . . . . . . . . . . . . . 434\n38.7 RAID Capacity, Reliability, and Performance . . . . . . . . . . 435\n40.1 The ext2 inode . . . . . . . . . . . . . . . . . . . . . . . . . . . . 466\n40.2 File System Measurement Summary . . . . . . . . . . . . . . . 468\n40.3 File Read Timeline (Time Increasing Downward) . . . . . . . 471\n40.4 File Creation Timeline (Time Increasing Downward) . . . . . 473\n42.1 Data Journaling Timeline . . . . . . . . . . . . . . . . . . . . . 506\n42.2 Metadata Journaling Timeline\n. . . . . . . . . . . . . . . . . . 507\n44.1 Frequency of LSEs and Block Corruption . . . . . . . . . . . . 528\n48.1 Reading A File: Client-side And File Server Actions\n. . . . . 566\n49.1 Reading A File: Client-side And File Server Actions\n. . . . . 579\n49.2 Cache Consistency Timeline\n. . . . . . . . . . . . . . . . . . . 581\n49.3 Comparison: AFS vs. NFS . . . . . . . . . . . . . . . . . . . . . 583\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1679,
      "extraction_method": "Direct"
    },
    {
      "page_number": 35,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 36,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 37,
      "chapter": null,
      "content": "1\nA Dialogue on the Book\nProfessor: Welcome to this book! It’s called Operating Systems in Three Easy\nPieces, and I am here to teach you the things you need to know about operating\nsystems. I am called “Professor”; who are you?\nStudent: Hi Professor! I am called “Student”, as you might have guessed. And\nI am here and ready to learn!\nProfessor: Sounds good. Any questions?\nStudent: Sure! Why is it called “Three Easy Pieces”?\nProfessor: That’s an easy one. Well, you see, there are these great lectures on\nPhysics by Richard Feynman...\nStudent: Oh! The guy who wrote “Surely You’re Joking, Mr. Feynman”, right?\nGreat book! Is this going to be hilarious like that book was?\nProfessor: Um... well, no. That book was great, and I’m glad you’ve read it.\nHopefully this book is more like his notes on Physics. Some of the basics were\nsummed up in a book called “Six Easy Pieces”. He was talking about Physics;\nwe’re going to do Three Easy Pieces on the ﬁne topic of Operating Systems. This\nis appropriate, as Operating Systems are about half as hard as Physics.\nStudent: Well, I liked physics, so that is probably good. What are those pieces?\nProfessor: They are the three key ideas we’re going to learn about: virtualiza-\ntion, concurrency, and persistence. In learning about these ideas, we’ll learn\nall about how an operating system works, including how it decides what program\nto run next on a CPU, how it handles memory overload in a virtual memory sys-\ntem, how virtual machine monitors work, how to manage information on disks,\nand even a little about how to build a distributed system that works when parts\nhave failed. That sort of stuff.\nStudent: I have no idea what you’re talking about, really.\nProfessor: Good! That means you are in the right class.\nStudent: I have another question: what’s the best way to learn this stuff?\nProfessor: Excellent query! Well, each person needs to ﬁgure this out on their\n1\n",
      "content_length": 1915,
      "extraction_method": "Direct"
    },
    {
      "page_number": 38,
      "chapter": null,
      "content": "2\nA DIALOGUE ON THE BOOK\nown, of course, but here is what I would do: go to class, to hear the professor\nintroduce the material. Then, say at the end of every week, read these notes,\nto help the ideas sink into your head a bit better. Of course, some time later\n(hint: before the exam!), read the notes again to ﬁrm up your knowledge. Of\ncourse, your professor will no doubt assign some homeworks and projects, so you\nshould do those; in particular, doing projects where you write real code to solve\nreal problems is the best way to put the ideas within these notes into action. As\nConfucius said...\nStudent: Oh, I know! ’I hear and I forget. I see and I remember. I do and I\nunderstand.’ Or something like that.\nProfessor: (surprised) How did you know what I was going to say?!\nStudent: It seemed to follow. Also, I am a big fan of Confucius.\nProfessor: Well, I think we are going to get along just ﬁne! Just ﬁne indeed.\nStudent: Professor – just one more question, if I may. What are these dialogues\nfor? I mean, isn’t this just supposed to be a book? Why not present the material\ndirectly?\nProfessor: Ah, good question, good question! Well, I think it is sometimes\nuseful to pull yourself outside of a narrative and think a bit; these dialogues are\nthose times. So you and I are going to work together to make sense of all of these\npretty complex ideas. Are you up for it?\nStudent: So we have to think? Well, I’m up for that. I mean, what else do I have\nto do anyhow? It’s not like I have much of a life outside of this book.\nProfessor: Me neither, sadly. So let’s get to work!\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1628,
      "extraction_method": "Direct"
    },
    {
      "page_number": 39,
      "chapter": null,
      "content": "2\nIntroduction to Operating Systems\nIf you are taking an undergraduate operating systems course, you should\nalready have some idea of what a computer program does when it runs.\nIf not, this book (and the corresponding course) is going to be difﬁcult\n– so you should probably stop reading this book, or run to the nearest\nbookstore and quickly consume the necessary background material be-\nfore continuing (both Patt/Patel [PP03] and particularly Bryant/O’Hallaron\n[BOH10] are pretty great books).\nSo what happens when a program runs?\nWell, a running program does one very simple thing: it executes in-\nstructions. Many millions (and these days, even billions) of times ev-\nery second, the processor fetches an instruction from memory, decodes\nit (i.e., ﬁgures out which instruction this is), and executes it (i.e., it does\nthe thing that it is supposed to do, like add two numbers together, access\nmemory, check a condition, jump to a function, and so forth). After it is\ndone with this instruction, the processor moves on to the next instruction,\nand so on, and so on, until the program ﬁnally completes1.\nThus, we have just described the basics of the Von Neumann model of\ncomputing2. Sounds simple, right? But in this class, we will be learning\nthat while a program runs, a lot of other wild things are going on with\nthe primary goal of making the system easy to use.\nThere is a body of software, in fact, that is responsible for making it\neasy to run programs (even allowing you to seemingly run many at the\nsame time), allowing programs to share memory, enabling programs to\ninteract with devices, and other fun stuff like that. That body of software\n1Of course, modern processors do many bizarre and frightening things underneath the\nhood to make programs run faster, e.g., executing multiple instructions at once, and even issu-\ning and completing them out of order! But that is not our concern here; we are just concerned\nwith the simple model most programs assume: that instructions seemingly execute one at a\ntime, in an orderly and sequential fashion.\n2Von Neumann was one of the early pioneers of computing systems. He also did pioneer-\ning work on game theory and atomic bombs, and played in the NBA for six years. OK, one of\nthose things isn’t true.\n3\n",
      "content_length": 2266,
      "extraction_method": "Direct"
    },
    {
      "page_number": 40,
      "chapter": null,
      "content": "4\nINTRODUCTION TO OPERATING SYSTEMS\nTHE CRUX OF THE PROBLEM:\nHOW TO VIRTUALIZE RESOURCES\nOne central question we will answer in this book is quite simple: how\ndoes the operating system virtualize resources? This is the crux of our\nproblem. Why the OS does this is not the main question, as the answer\nshould be obvious: it makes the system easier to use. Thus, we focus on\nthe how: what mechanisms and policies are implemented by the OS to\nattain virtualization? How does the OS do so efﬁciently? What hardware\nsupport is needed?\nWe will use the “crux of the problem”, in shaded boxes such as this one,\nas a way to call out speciﬁc problems we are trying to solve in building\nan operating system. Thus, within a note on a particular topic, you may\nﬁnd one or more cruces (yes, this is the proper plural) which highlight the\nproblem. The details within the chapter, of course, present the solution,\nor at least the basic parameters of a solution.\nis called the operating system (OS)3, as it is in charge of making sure the\nsystem operates correctly and efﬁciently in an easy-to-use manner.\nThe primary way the OS does this is through a general technique that\nwe call virtualization. That is, the OS takes a physical resource (such as\nthe processor, or memory, or a disk) and transforms it into a more gen-\neral, powerful, and easy-to-use virtual form of itself. Thus, we sometimes\nrefer to the operating system as a virtual machine.\nOf course, in order to allow users to tell the OS what to do and thus\nmake use of the features of the virtual machine (such as running a pro-\ngram, or allocating memory, or accessing a ﬁle), the OS also provides\nsome interfaces (APIs) that you can call. A typical OS, in fact, exports\na few hundred system calls that are available to applications. Because\nthe OS provides these calls to run programs, access memory and devices,\nand other related actions, we also sometimes say that the OS provides a\nstandard library to applications.\nFinally, because virtualization allows many programs to run (thus shar-\ning the CPU), and many programs to concurrently access their own in-\nstructions and data (thus sharing memory), and many programs to access\ndevices (thus sharing disks and so forth), the OS is sometimes known as\na resource manager. Each of the CPU, memory, and disk is a resource\nof the system; it is thus the operating system’s role to manage those re-\nsources, doing so efﬁciently or fairly or indeed with many other possible\ngoals in mind. To understand the role of the OS a little bit better, let’s take\na look at some examples.\n3Another early name for the OS was the supervisor or even the master control program.\nApparently, the latter sounded a little overzealous (see the movie Tron for details) and thus,\nthankfully, “operating system” caught on instead.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2849,
      "extraction_method": "Direct"
    },
    {
      "page_number": 41,
      "chapter": null,
      "content": "INTRODUCTION TO OPERATING SYSTEMS\n5\n1\n#include <stdio.h>\n2\n#include <stdlib.h>\n3\n#include <sys/time.h>\n4\n#include <assert.h>\n5\n#include \"common.h\"\n6\n7\nint\n8\nmain(int argc, char *argv[])\n9\n{\n10\nif (argc != 2) {\n11\nfprintf(stderr, \"usage: cpu <string>\\n\");\n12\nexit(1);\n13\n}\n14\nchar *str = argv[1];\n15\nwhile (1) {\n16\nSpin(1);\n17\nprintf(\"%s\\n\", str);\n18\n}\n19\nreturn 0;\n20\n}\nFigure 2.1: Simple Example: Code That Loops and Prints\n2.1\nVirtualizing the CPU\nFigure 2.1 depicts our ﬁrst program. It doesn’t do much. In fact, all\nit does is call Spin(), a function that repeatedly checks the time and\nreturns once it has run for a second. Then, it prints out the string that the\nuser passed in on the command line, and repeats, forever.\nLet’s say we save this ﬁle as cpu.c and decide to compile and run it\non a system with a single processor (or CPU as we will sometimes call it).\nHere is what we will see:\nprompt> gcc -o cpu cpu.c -Wall\nprompt> ./cpu \"A\"\nA\nA\nA\nA\nˆC\nprompt>\nNot too interesting of a run – the system begins running the program,\nwhich repeatedly checks the time until a second has elapsed. Once a sec-\nond has passed, the code prints the input string passed in by the user\n(in this example, the letter “A”), and continues. Note the program will\nrun forever; only by pressing “Control-c” (which on UNIX-based systems\nwill terminate the program running in the foreground) can we halt the\nprogram.\nNow, let’s do the same thing, but this time, let’s run many different in-\nstances of this same program. Figure 2.2 shows the results of this slightly\nmore complicated example.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 1618,
      "extraction_method": "Direct"
    },
    {
      "page_number": 42,
      "chapter": null,
      "content": "6\nINTRODUCTION TO OPERATING SYSTEMS\nprompt> ./cpu A & ; ./cpu B & ; ./cpu C & ; ./cpu D &\n[1] 7353\n[2] 7354\n[3] 7355\n[4] 7356\nA\nB\nD\nC\nA\nB\nD\nC\nA\nC\nB\nD\n...\nFigure 2.2: Running Many Programs At Once\nWell, now things are getting a little more interesting. Even though we\nhave only one processor, somehow all four of these programs seem to be\nrunning at the same time! How does this magic happen?4\nIt turns out that the operating system, with some help from the hard-\nware, is in charge of this illusion, i.e., the illusion that the system has a\nvery large number of virtual CPUs. Turning a single CPU (or small set of\nthem) into a seemingly inﬁnite number of CPUs and thus allowing many\nprograms to seemingly run at once is what we call virtualizing the CPU,\nthe focus of the ﬁrst major part of this book.\nOf course, to run programs, and stop them, and otherwise tell the OS\nwhich programs to run, there need to be some interfaces (APIs) that you\ncan use to communicate your desires to the OS. We’ll talk about these\nAPIs throughout this book; indeed, they are the major way in which most\nusers interact with operating systems.\nYou might also notice that the ability to run multiple programs at once\nraises all sorts of new questions. For example, if two programs want to\nrun at a particular time, which should run? This question is answered by\na policy of the OS; policies are used in many different places within an\nOS to answer these types of questions, and thus we will study them as\nwe learn about the basic mechanisms that operating systems implement\n(such as the ability to run multiple programs at once). Hence the role of\nthe OS as a resource manager.\n4Note how we ran four processes at the same time, by using the & symbol. Doing so runs a\njob in the background in the tcsh shell, which means that the user is able to immediately issue\ntheir next command, which in this case is another program to run. The semi-colon between\ncommands allows us to run multiple programs at the same time in tcsh. If you’re using a\ndifferent shell (e.g., bash), it works slightly differently; read documentation online for details.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2166,
      "extraction_method": "Direct"
    },
    {
      "page_number": 43,
      "chapter": null,
      "content": "INTRODUCTION TO OPERATING SYSTEMS\n7\n1\n#include <unistd.h>\n2\n#include <stdio.h>\n3\n#include <stdlib.h>\n4\n#include \"common.h\"\n5\n6\nint\n7\nmain(int argc, char *argv[])\n8\n{\n9\nint *p = malloc(sizeof(int));\n// a1\n10\nassert(p != NULL);\n11\nprintf(\"(%d) address of p: %08x\\n\",\n12\ngetpid(), (unsigned) p);\n// a2\n13\n*p = 0;\n// a3\n14\nwhile (1) {\n15\nSpin(1);\n16\n*p = *p + 1;\n17\nprintf(\"(%d) p: %d\\n\", getpid(), *p); // a4\n18\n}\n19\nreturn 0;\n20\n}\nFigure 2.3: A Program that Accesses Memory\n2.2\nVirtualizing Memory\nNow let’s consider memory.\nThe model of physical memory pre-\nsented by modern machines is very simple. Memory is just an array of\nbytes; to read memory, one must specify an address to be able to access\nthe data stored there; to write (or update) memory, one must also specify\nthe data to be written to the given address.\nMemory is accessed all the time when a program is running. A pro-\ngram keeps all of its data structures in memory, and accesses them through\nvarious instructions, like loads and stores or other explicit instructions\nthat access memory in doing their work. Don’t forget that each instruc-\ntion of the program is in memory too; thus memory is accessed on each\ninstruction fetch.\nLet’s take a look at a program (in Figure 2.3) that allocates some mem-\nory by calling malloc(). The output of this program can be found here:\nprompt> ./mem\n(2134) memory address of p: 00200000\n(2134) p: 1\n(2134) p: 2\n(2134) p: 3\n(2134) p: 4\n(2134) p: 5\nˆC\nThe program does a couple of things. First, it allocates some memory\n(line a1). Then, it prints out the address of the memory (a2), and then\nputs the number zero into the ﬁrst slot of the newly allocated memory\n(a3). Finally, it loops, delaying for a second and incrementing the value\nstored at the address held in p. With every print statement, it also prints\nout what is called the process identiﬁer (the PID) of the running program.\nThis PID is unique per running process.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 1968,
      "extraction_method": "Direct"
    },
    {
      "page_number": 44,
      "chapter": null,
      "content": "8\nINTRODUCTION TO OPERATING SYSTEMS\nprompt> ./mem &; ./mem &\n[1] 24113\n[2] 24114\n(24113) memory address of p: 00200000\n(24114) memory address of p: 00200000\n(24113) p: 1\n(24114) p: 1\n(24114) p: 2\n(24113) p: 2\n(24113) p: 3\n(24114) p: 3\n(24113) p: 4\n(24114) p: 4\n...\nFigure 2.4: Running The Memory Program Multiple Times\nAgain, this ﬁrst result is not too interesting. The newly allocated mem-\nory is at address 00200000. As the program runs, it slowly updates the\nvalue and prints out the result.\nNow, we again run multiple instances of this same program to see\nwhat happens (Figure 2.4). We see from the example that each running\nprogram has allocated memory at the same address (00200000), and yet\neach seems to be updating the value at 00200000 independently! It is as\nif each running program has its own private memory, instead of sharing\nthe same physical memory with other running programs5.\nIndeed, that is exactly what is happening here as the OS is virtualiz-\ning memory. Each process accesses its own private virtual address space\n(sometimes just called its address space), which the OS somehow maps\nonto the physical memory of the machine. A memory reference within\none running program does not affect the address space of other processes\n(or the OS itself); as far as the running program is concerned, it has phys-\nical memory all to itself. The reality, however, is that physical memory is\na shared resource, managed by the operating system. Exactly how all of\nthis is accomplished is also the subject of the ﬁrst part of this book, on the\ntopic of virtualization.\n2.3\nConcurrency\nAnother main theme of this book is concurrency. We use this concep-\ntual term to refer to a host of problems that arise, and must be addressed,\nwhen working on many things at once (i.e., concurrently) in the same\nprogram. The problems of concurrency arose ﬁrst within the operating\nsystem itself; as you can see in the examples above on virtualization, the\nOS is juggling many things at once, ﬁrst running one process, then an-\nother, and so forth. As it turns out, doing so leads to some deep and\ninteresting problems.\n5For this example to work, you need to make sure address-space randomization is dis-\nabled; randomization, as it turns out, can be a good defense against certain kinds of security\nﬂaws. Read more about it on your own, especially if you want to learn how to break into\ncomputer systems via stack-smashing attacks. Not that we would recommend such a thing...\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2517,
      "extraction_method": "Direct"
    },
    {
      "page_number": 45,
      "chapter": null,
      "content": "INTRODUCTION TO OPERATING SYSTEMS\n9\n1\n#include <stdio.h>\n2\n#include <stdlib.h>\n3\n#include \"common.h\"\n4\n5\nvolatile int counter = 0;\n6\nint loops;\n7\n8\nvoid *worker(void *arg) {\n9\nint i;\n10\nfor (i = 0; i < loops; i++) {\n11\ncounter++;\n12\n}\n13\nreturn NULL;\n14\n}\n15\n16\nint\n17\nmain(int argc, char *argv[])\n18\n{\n19\nif (argc != 2) {\n20\nfprintf(stderr, \"usage: threads <value>\\n\");\n21\nexit(1);\n22\n}\n23\nloops = atoi(argv[1]);\n24\npthread_t p1, p2;\n25\nprintf(\"Initial value : %d\\n\", counter);\n26\n27\nPthread_create(&p1, NULL, worker, NULL);\n28\nPthread_create(&p2, NULL, worker, NULL);\n29\nPthread_join(p1, NULL);\n30\nPthread_join(p2, NULL);\n31\nprintf(\"Final value\n: %d\\n\", counter);\n32\nreturn 0;\n33\n}\nFigure 2.5: A Multi-threaded Program\nUnfortunately, the problems of concurrency are no longer limited just\nto the OS itself. Indeed, modern multi-threaded programs exhibit the\nsame problems. Let us demonstrate with an example of a multi-threaded\nprogram (Figure 2.5).\nAlthough you might not understand this example fully at the moment\n(and we’ll learn a lot more about it in later chapters, in the section of the\nbook on concurrency), the basic idea is simple. The main program creates\ntwo threads using Pthread create()6. You can think of a thread as a\nfunction running within the same memory space as other functions, with\nmore than one of them active at a time. In this example, each thread starts\nrunning in a routine called worker(), in which it simply increments a\ncounter in a loop for loops number of times.\nBelow is a transcript of what happens when we run this program with\nthe input value for the variable loops set to 1000. The value of loops\n6The actual call should be to lower-case pthread create(); the upper-case version is\nour own wrapper that calls pthread create() and makes sure that the return code indicates\nthat the call succeeded. See the code for details.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 1906,
      "extraction_method": "Direct"
    },
    {
      "page_number": 46,
      "chapter": null,
      "content": "10\nINTRODUCTION TO OPERATING SYSTEMS\nTHE CRUX OF THE PROBLEM:\nHOW TO BUILD CORRECT CONCURRENT PROGRAMS\nWhen there are many concurrently executing threads within the same\nmemory space, how can we build a correctly working program? What\nprimitives are needed from the OS? What mechanisms should be pro-\nvided by the hardware? How can we use them to solve the problems of\nconcurrency?\ndetermines how many times each of the two workers will increment the\nshared counter in a loop. When the program is run with the value of\nloops set to 1000, what do you expect the ﬁnal value of counter to be?\nprompt> gcc -o thread thread.c -Wall -pthread\nprompt> ./thread 1000\nInitial value : 0\nFinal value\n: 2000\nAs you probably guessed, when the two threads are ﬁnished, the ﬁnal\nvalue of the counter is 2000, as each thread incremented the counter 1000\ntimes. Indeed, when the input value of loops is set to N, we would\nexpect the ﬁnal output of the program to be 2N. But life is not so simple,\nas it turns out. Let’s run the same program, but with higher values for\nloops, and see what happens:\nprompt> ./thread 100000\nInitial value : 0\nFinal value\n: 143012\n// huh??\nprompt> ./thread 100000\nInitial value : 0\nFinal value\n: 137298\n// what the??\nIn this run, when we gave an input value of 100,000, instead of getting\na ﬁnal value of 200,000, we instead ﬁrst get 143,012. Then, when we run\nthe program a second time, we not only again get the wrong value, but\nalso a different value than the last time. In fact, if you run the program\nover and over with high values of loops, you may ﬁnd that sometimes\nyou even get the right answer! So why is this happening?\nAs it turns out, the reason for these odd and unusual outcomes relate\nto how instructions are executed, which is one at a time. Unfortunately, a\nkey part of the program above, where the shared counter is incremented,\ntakes three instructions: one to load the value of the counter from mem-\nory into a register, one to increment it, and one to store it back into mem-\nory. Because these three instructions do not execute atomically (all at\nonce), strange things can happen. It is this problem of concurrency that\nwe will address in great detail in the second part of this book.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2267,
      "extraction_method": "Direct"
    },
    {
      "page_number": 47,
      "chapter": null,
      "content": "INTRODUCTION TO OPERATING SYSTEMS\n11\n1\n#include <stdio.h>\n2\n#include <unistd.h>\n3\n#include <assert.h>\n4\n#include <fcntl.h>\n5\n#include <sys/types.h>\n6\n7\nint\n8\nmain(int argc, char *argv[])\n9\n{\n10\nint fd = open(\"/tmp/file\", O_WRONLY | O_CREAT | O_TRUNC, S_IRWXU);\n11\nassert(fd > -1);\n12\nint rc = write(fd, \"hello world\\n\", 13);\n13\nassert(rc == 13);\n14\nclose(fd);\n15\nreturn 0;\n16\n}\nFigure 2.6: A Program That Does I/O\n2.4\nPersistence\nThe third major theme of the course is persistence. In system memory,\ndata can be easily lost, as devices such as DRAM store values in a volatile\nmanner; when power goes away or the system crashes, any data in mem-\nory is lost. Thus, we need hardware and software to be able to store data\npersistently; such storage is thus critical to any system as users care a\ngreat deal about their data.\nThe hardware comes in the form of some kind of input/output or I/O\ndevice; in modern systems, a hard drive is a common repository for long-\nlived information, although solid-state drives (SSDs) are making head-\nway in this arena as well.\nThe software in the operating system that usually manages the disk is\ncalled the ﬁle system; it is thus responsible for storing any ﬁles the user\ncreates in a reliable and efﬁcient manner on the disks of the system.\nUnlike the abstractions provided by the OS for the CPU and memory,\nthe OS does not create a private, virtualized disk for each application.\nRather, it is assumed that often times, users will want to share informa-\ntion that is in ﬁles. For example, when writing a C program, you might\nﬁrst use an editor (e.g., Emacs7) to create and edit the C ﬁle (emacs -nw\nmain.c). Once done, you might use the compiler to turn the source code\ninto an executable (e.g., gcc -o main main.c). When you’re ﬁnished,\nyou might run the new executable (e.g., ./main). Thus, you can see how\nﬁles are shared across different processes. First, Emacs creates a ﬁle that\nserves as input to the compiler; the compiler uses that input ﬁle to create\na new executable ﬁle (in many steps – take a compiler course for details);\nﬁnally, the new executable is then run. And thus a new program is born!\nTo understand this better, let’s look at some code. Figure 2.6 presents\ncode to create a ﬁle (/tmp/file) that contains the string “hello world”.\n7You should be using Emacs. If you are using vi, there is probably something wrong with\nyou. If you are using something that is not a real code editor, that is even worse.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2502,
      "extraction_method": "Direct"
    },
    {
      "page_number": 48,
      "chapter": null,
      "content": "12\nINTRODUCTION TO OPERATING SYSTEMS\nTHE CRUX OF THE PROBLEM:\nHOW TO STORE DATA PERSISTENTLY\nThe ﬁle system is the part of the OS in charge of managing persistent data.\nWhat techniques are needed to do so correctly? What mechanisms and\npolicies are required to do so with high performance? How is reliability\nachieved, in the face of failures in hardware and software?\nTo accomplish this task, the program makes three calls into the oper-\nating system. The ﬁrst, a call to open(), opens the ﬁle and creates it; the\nsecond, write(), writes some data to the ﬁle; the third, close(), sim-\nply closes the ﬁle thus indicating the program won’t be writing any more\ndata to it. These system calls are routed to the part of the operating sys-\ntem called the ﬁle system, which then handles the requests and returns\nsome kind of error code to the user.\nYou might be wondering what the OS does in order to actually write\nto disk. We would show you but you’d have to promise to close your\neyes ﬁrst; it is that unpleasant. The ﬁle system has to do a fair bit of work:\nﬁrst ﬁguring out where on disk this new data will reside, and then keep-\ning track of it in various structures the ﬁle system maintains. Doing so\nrequires issuing I/O requests to the underlying storage device, to either\nread existing structures or update (write) them. As anyone who has writ-\nten a device driver8 knows, getting a device to do something on your\nbehalf is an intricate and detailed process. It requires a deep knowledge\nof the low-level device interface and its exact semantics. Fortunately, the\nOS provides a standard and simple way to access devices through its sys-\ntem calls. Thus, the OS is sometimes seen as a standard library.\nOf course, there are many more details in how devices are accessed,\nand how ﬁle systems manage data persistently atop said devices. For\nperformance reasons, most ﬁle systems ﬁrst delay such writes for a while,\nhoping to batch them into larger groups. To handle the problems of sys-\ntem crashes during writes, most ﬁle systems incorporate some kind of\nintricate write protocol, such as journaling or copy-on-write, carefully\nordering writes to disk to ensure that if a failure occurs during the write\nsequence, the system can recover to reasonable state afterwards. To make\ndifferent common operations efﬁcient, ﬁle systems employ many differ-\nent data structures and access methods, from simple lists to complex b-\ntrees. If all of this doesn’t make sense yet, good! We’ll be talking about\nall of this quite a bit more in the third part of this book on persistence,\nwhere we’ll discuss devices and I/O in general, and then disks, RAIDs,\nand ﬁle systems in great detail.\n8A device driver is some code in the operating system that knows how to deal with a\nspeciﬁc device. We will talk more about devices and device drivers later.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2881,
      "extraction_method": "Direct"
    },
    {
      "page_number": 49,
      "chapter": null,
      "content": "INTRODUCTION TO OPERATING SYSTEMS\n13\n2.5\nDesign Goals\nSo now you have some idea of what an OS actually does: it takes phys-\nical resources, such as a CPU, memory, or disk, and virtualizes them. It\nhandles tough and tricky issues related to concurrency. And it stores ﬁles\npersistently, thus making them safe over the long-term. Given that we\nwant to build such a system, we want to have some goals in mind to help\nfocus our design and implementation and make trade-offs as necessary;\nﬁnding the right set of trade-offs is a key to building systems.\nOne of the most basic goals is to build up some abstractions in order\nto make the system convenient and easy to use. Abstractions are fun-\ndamental to everything we do in computer science. Abstraction makes\nit possible to write a large program by dividing it into small and under-\nstandable pieces, to write such a program in a high-level language like\nC9 without thinking about assembly, to write code in assembly without\nthinking about logic gates, and to build a processor out of gates without\nthinking too much about transistors. Abstraction is so fundamental that\nsometimes we forget its importance, but we won’t here; thus, in each sec-\ntion, we’ll discuss some of the major abstractions that have developed\nover time, giving you a way to think about pieces of the OS.\nOne goal in designing and implementing an operating system is to\nprovide high performance; another way to say this is our goal is to mini-\nmize the overheads of the OS. Virtualization and making the system easy\nto use are well worth it, but not at any cost; thus, we must strive to pro-\nvide virtualization and other OS features without excessive overheads.\nThese overheads arise in a number of forms: extra time (more instruc-\ntions) and extra space (in memory or on disk). We’ll seek solutions that\nminimize one or the other or both, if possible. Perfection, however, is not\nalways attainable, something we will learn to notice and (where appro-\npriate) tolerate.\nAnother goal will be to provide protection between applications, as\nwell as between the OS and applications.\nBecause we wish to allow\nmany programs to run at the same time, we want to make sure that the\nmalicious or accidental bad behavior of one does not harm others; we\ncertainly don’t want an application to be able to harm the OS itself (as\nthat would affect all programs running on the system). Protection is at\nthe heart of one of the main principles underlying an operating system,\nwhich is that of isolation; isolating processes from one another is the key\nto protection and thus underlies much of what an OS must do.\nThe operating system must also run non-stop; when it fails, all appli-\ncations running on the system fail as well. Because of this dependence,\noperating systems often strive to provide a high degree of reliability. As\noperating systems grow evermore complex (sometimes containing mil-\nlions of lines of code), building a reliable operating system is quite a chal-\n9Some of you might object to calling C a high-level language. Remember this is an OS\ncourse, though, where we’re simply happy not to have to code in assembly all the time!\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 3193,
      "extraction_method": "Direct"
    },
    {
      "page_number": 50,
      "chapter": null,
      "content": "14\nINTRODUCTION TO OPERATING SYSTEMS\nlenge – and indeed, much of the on-going research in the ﬁeld (including\nsome of our own work [BS+09, SS+10]) focuses on this exact problem.\nOther goals make sense: energy-efﬁciency is important in our increas-\ningly green world; security (an extension of protection, really) against\nmalicious applications is critical, especially in these highly-networked\ntimes; mobility is increasingly important as OSes are run on smaller and\nsmaller devices. Depending in how the system is used, the OS will have\ndifferent goals and thus likely be implemented in at least slightly differ-\nent ways. However, as we will see, many of the principles we will present\non how to build operating systems are useful in the range of different de-\nvices.\n2.6\nSome History\nBefore closing this introduction, let us present a brief history of how\noperating systems developed. Like any system built by humans, good\nideas accumulated in operating systems over time, as engineers learned\nwhat was important in their design. Here, we discuss a few major devel-\nopments. For a richer treatment, see Brinch Hansen’s excellent history of\noperating systems [BH00].\nEarly Operating Systems: Just Libraries\nIn the beginning, the operating system didn’t do too much. Basically,\nit was just a set of libraries of commonly-used functions; for example,\ninstead of having each programmer of the system write low-level I/O\nhandling code, the “OS” would provide such APIs, and thus make life\neasier for the developer.\nUsually, on these old mainframe systems, one program ran at a time,\nas controlled by a human operator. Much of what you think a modern\nOS would do (e.g., deciding what order to run jobs in) was performed by\nthis operator. If you were a smart developer, you would be nice to this\noperator, so that they might move your job to the front of the queue.\nThis mode of computing was known as batch processing, as a number\nof jobs were set up and then run in a “batch” by the operator. Computers,\nas of that point, were not used in an interactive manner, because of cost:\nit was simply too costly to let a user sit in front of the computer and use it,\nas most of the time it would just sit idle then, costing the facility hundreds\nof thousands of dollars per hour [BH00].\nBeyond Libraries: Protection\nIn moving beyond being a simple library of commonly-used services, op-\nerating systems took on a more central role in managing machines. One\nimportant aspect of this was the realization that code run on behalf of the\nOS was special; it had control of devices and thus should be treated dif-\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2644,
      "extraction_method": "Direct"
    },
    {
      "page_number": 51,
      "chapter": null,
      "content": "INTRODUCTION TO OPERATING SYSTEMS\n15\nferently than normal application code. Why is this? Well, imagine if you\nallowed any application to read from anywhere on the disk; the notion of\nprivacy goes out the window, as any program could read any ﬁle. Thus,\nimplementing a ﬁle system (to manage your ﬁles) as a library makes little\nsense. Instead, something else was needed.\nThus, the idea of a system call was invented, pioneered by the Atlas\ncomputing system [K+61,L78]. Instead of providing OS routines as a li-\nbrary (where you just make a procedure call to access them), the idea here\nwas to add a special pair of hardware instructions and hardware state to\nmake the transition into the OS a more formal, controlled process.\nThe key difference between a system call and a procedure call is that\na system call transfers control (i.e., jumps) into the OS while simultane-\nously raising the hardware privilege level. User applications run in what\nis referred to as user mode which means the hardware restricts what ap-\nplications can do; for example, an application running in user mode can’t\ntypically initiate an I/O request to the disk, access any physical memory\npage, or send a packet on the network. When a system call is initiated\n(usually through a special hardware instruction called a trap), the hard-\nware transfers control to a pre-speciﬁed trap handler (that the OS set up\npreviously) and simultaneously raises the privilege level to kernel mode.\nIn kernel mode, the OS has full access to the hardware of the system and\nthus can do things like initiate an I/O request or make more memory\navailable to a program. When the OS is done servicing the request, it\npasses control back to the user via a special return-from-trap instruction,\nwhich reverts to user mode while simultaneously passing control back to\nwhere the application left off.\nThe Era of Multiprogramming\nWhere operating systems really took off was in the era of computing be-\nyond the mainframe, that of the minicomputer. Classic machines like\nthe PDP family from Digital Equipment made computers hugely more\naffordable; thus, instead of having one mainframe per large organization,\nnow a smaller collection of people within an organization could likely\nhave their own computer. Not surprisingly, one of the major impacts of\nthis drop in cost was an increase in developer activity; more smart people\ngot their hands on computers and thus made computer systems do more\ninteresting and beautiful things.\nIn particular, multiprogramming became commonplace due to the de-\nsire to make better use of machine resources. Instead of just running one\njob at a time, the OS would load a number of jobs into memory and switch\nrapidly between them, thus improving CPU utilization. This switching\nwas particularly important because I/O devices were slow; having a pro-\ngram wait on the CPU while its I/O was being serviced was a waste of\nCPU time. Instead, why not switch to another job and run it for a while?\nThe desire to support multiprogramming and overlap in the presence\nof I/O and interrupts forced innovation in the conceptual development of\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 3151,
      "extraction_method": "Direct"
    },
    {
      "page_number": 52,
      "chapter": null,
      "content": "16\nINTRODUCTION TO OPERATING SYSTEMS\noperating systems along a number of directions. Issues such as memory\nprotection became important; we wouldn’t want one program to be able\nto access the memory of another program. Understanding how to deal\nwith the concurrency issues introduced by multiprogramming was also\ncritical; making sure the OS was behaving correctly despite the presence\nof interrupts is a great challenge. We will study these issues and related\ntopics later in the book.\nOne of the major practical advances of the time was the introduction\nof the UNIX operating system, primarily thanks to Ken Thompson (and\nDennis Ritchie) at Bell Labs (yes, the phone company). UNIX took many\ngood ideas from different operating systems (particularly from Multics\n[O72], and some from systems like TENEX [B+72] and the Berkeley Time-\nSharing System [S+68]), but made them simpler and easier to use. Soon\nthis team was shipping tapes containing UNIX source code to people\naround the world, many of whom then got involved and added to the\nsystem themselves; see the Aside (next page) for more detail10.\nThe Modern Era\nBeyond the minicomputer came a new type of machine, cheaper, faster,\nand for the masses: the personal computer, or PC as we call it today. Led\nby Apple’s early machines (e.g., the Apple II) and the IBM PC, this new\nbreed of machine would soon become the dominant force in computing,\nas their low-cost enabled one machine per desktop instead of a shared\nminicomputer per workgroup.\nUnfortunately, for operating systems, the PC at ﬁrst represented a\ngreat leap backwards, as early systems forgot (or never knew of) the\nlessons learned in the era of minicomputers. For example, early operat-\ning systems such as DOS (the Disk Operating System, from Microsoft)\ndidn’t think memory protection was important; thus, a malicious (or per-\nhaps just a poorly-programmed) application could scribble all over mem-\nory. The ﬁrst generations of the Mac OS (v9 and earlier) took a coopera-\ntive approach to job scheduling; thus, a thread that accidentally got stuck\nin an inﬁnite loop could take over the entire system, forcing a reboot. The\npainful list of OS features missing in this generation of systems is long,\ntoo long for a full discussion here.\nFortunately, after some years of suffering, the old features of minicom-\nputer operating systems started to ﬁnd their way onto the desktop. For\nexample, Mac OS X has UNIX at its core, including all of the features\none would expect from such a mature system. Windows has similarly\nadopted many of the great ideas in computing history, starting in partic-\nular with Windows NT, a great leap forward in Microsoft OS technology.\nEven today’s cell phones run operating systems (such as Linux) that are\n10We’ll use asides and other related text boxes to call attention to various items that don’t\nquite ﬁt the main ﬂow of the text. Sometimes, we’ll even use them just to make a joke, because\nwhy not have a little fun along the way? Yes, many of the jokes are bad.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 3060,
      "extraction_method": "Direct"
    },
    {
      "page_number": 53,
      "chapter": null,
      "content": "INTRODUCTION TO OPERATING SYSTEMS\n17\nASIDE: THE IMPORTANCE OF UNIX\nIt is difﬁcult to overstate the importance of UNIX in the history of oper-\nating systems. Inﬂuenced by earlier systems (in particular, the famous\nMultics system from MIT), UNIX brought together many great ideas and\nmade a system that was both simple and powerful.\nUnderlying the original “Bell Labs” UNIX was the unifying principle of\nbuilding small powerful programs that could be connected together to\nform larger workﬂows. The shell, where you type commands, provided\nprimitives such as pipes to enable such meta-level programming, and\nthus it became easy to string together programs to accomplish a big-\nger task.\nFor example, to ﬁnd lines of a text ﬁle that have the word\n“foo” in them, and then to count how many such lines exist, you would\ntype: grep foo file.txt|wc -l, thus using the grep and wc (word\ncount) programs to achieve your task.\nThe UNIX environment was friendly for programmers and developers\nalike, also providing a compiler for the new C programming language.\nMaking it easy for programmers to write their own programs, as well as\nshare them, made UNIX enormously popular. And it probably helped a\nlot that the authors gave out copies for free to anyone who asked, an early\nform of open-source software.\nAlso of critical importance was the accessibility and readability of the\ncode. Having a beautiful, small kernel written in C invited others to play\nwith the kernel, adding new and cool features. For example, an enter-\nprising group at Berkeley, led by Bill Joy, made a wonderful distribution\n(the Berkeley Systems Distribution, or BSD) which had some advanced\nvirtual memory, ﬁle system, and networking subsystems. Joy later co-\nfounded Sun Microsystems.\nUnfortunately, the spread of UNIX was slowed a bit as companies tried to\nassert ownership and proﬁt from it, an unfortunate (but common) result\nof lawyers getting involved. Many companies had their own variants:\nSunOS from Sun Microsystems, AIX from IBM, HPUX (a.k.a. “H-Pucks”)\nfrom HP, and IRIX from SGI. The legal wrangling among AT&T/Bell\nLabs and these other players cast a dark cloud over UNIX, and many\nwondered if it would survive, especially as Windows was introduced and\ntook over much of the PC market...\nmuch more like what a minicomputer ran in the 1970s than what a PC\nran in the 1980s (thank goodness); it is good to see that the good ideas de-\nveloped in the heyday of OS development have found their way into the\nmodern world. Even better is that these ideas continue to develop, pro-\nviding more features and making modern systems even better for users\nand applications.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2679,
      "extraction_method": "Direct"
    },
    {
      "page_number": 54,
      "chapter": null,
      "content": "18\nINTRODUCTION TO OPERATING SYSTEMS\nASIDE: AND THEN CAME LINUX\nFortunately for UNIX, a young Finnish hacker named Linus Torvalds de-\ncided to write his own version of UNIX which borrowed heavily on the\nprinciples and ideas behind the original system, but not from the code\nbase, thus avoiding issues of legality. He enlisted help from many oth-\ners around the world, and soon Linux was born (as well as the modern\nopen-source software movement).\nAs the internet era came into place, most companies (such as Google,\nAmazon, Facebook, and others) chose to run Linux, as it was free and\ncould be readily modiﬁed to suit their needs; indeed, it is hard to imag-\nine the success of these new companies had such a system not existed.\nAs smart phones became a dominant user-facing platform, Linux found\na stronghold there too (via Android), for many of the same reasons. And\nSteve Jobs took his UNIX-based NeXTStep operating environment with\nhim to Apple, thus making UNIX popular on desktops (though many\nusers of Apple technology are probably not even aware of this fact). And\nthus UNIX lives on, more important today than ever before. The comput-\ning gods, if you believe in them, should be thanked for this wonderful\noutcome.\n2.7\nSummary\nThus, we have an introduction to the OS. Today’s operating systems\nmake systems relatively easy to use, and virtually all operating systems\nyou use today have been inﬂuenced by the developments we will discuss\nthroughout the book.\nUnfortunately, due to time constraints, there are a number of parts of\nthe OS we won’t cover in the book. For example, there is a lot of net-\nworking code in the operating system; we leave it to you to take the net-\nworking class to learn more about that. Similarly, graphics devices are\nparticularly important; take the graphics course to expand your knowl-\nedge in that direction. Finally, some operating system books talk a great\ndeal about security; we will do so in the sense that the OS must provide\nprotection between running programs and give users the ability to pro-\ntect their ﬁles, but we won’t delve into deeper security issues that one\nmight ﬁnd in a security course.\nHowever, there are many important topics that we will cover, includ-\ning the basics of virtualization of the CPU and memory, concurrency, and\npersistence via devices and ﬁle systems. Don’t worry! While there is a\nlot of ground to cover, most of it is quite cool, and at the end of the road,\nyou’ll have a new appreciation for how computer systems really work.\nNow get to work!\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2573,
      "extraction_method": "Direct"
    },
    {
      "page_number": 55,
      "chapter": null,
      "content": "INTRODUCTION TO OPERATING SYSTEMS\n19\nReferences\n[BS+09] “Tolerating File-System Mistakes with EnvyFS”\nLakshmi N. Bairavasundaram, Swaminathan Sundararaman, Andrea C. Arpaci-Dusseau, Remzi\nH. Arpaci-Dusseau\nUSENIX ’09, San Diego, CA, June 2009\nA fun paper about using multiple ﬁle systems at once to tolerate a mistake in any one of them.\n[BH00] “The Evolution of Operating Systems”\nP. Brinch Hansen\nIn Classic Operating Systems: From Batch Processing to Distributed Systems\nSpringer-Verlag, New York, 2000\nThis essay provides an intro to a wonderful collection of papers about historically signiﬁcant systems.\n[B+72] “TENEX, A Paged Time Sharing System for the PDP-10”\nDaniel G. Bobrow, Jerry D. Burchﬁel, Daniel L. Murphy, Raymond S. Tomlinson\nCACM, Volume 15, Number 3, March 1972\nTENEX has much of the machinery found in modern operating systems; read more about it to see how\nmuch innovation was already in place in the early 1970’s.\n[B75] “The Mythical Man-Month”\nFred Brooks\nAddison-Wesley, 1975\nA classic text on software engineering; well worth the read.\n[BOH10] “Computer Systems: A Programmer’s Perspective”\nRandal E. Bryant and David R. O’Hallaron\nAddison-Wesley, 2010\nAnother great intro to how computer systems work. Has a little bit of overlap with this book – so if you’d\nlike, you can skip the last few chapters of that book, or simply read them to get a different perspective\non some of the same material. After all, one good way to build up your own knowledge is to hear as\nmany other perspectives as possible, and then develop your own opinion and thoughts on the matter.\nYou know, by thinking!\n[K+61] “One-Level Storage System”\nT. Kilburn, D.B.G. Edwards, M.J. Lanigan, F.H. Sumner\nIRE Transactions on Electronic Computers, April 1962\nThe Atlas pioneered much of what you see in modern systems. However, this paper is not the best read.\nIf you were to only read one, you might try the historical perspective below [L78].\n[L78] “The Manchester Mark I and Atlas: A Historical Perspective”\nS. H. Lavington\nCommunications of the ACM archive\nVolume 21, Issue 1 (January 1978), pages 4-12\nA nice piece of history on the early development of computer systems and the pioneering efforts of the\nAtlas. Of course, one could go back and read the Atlas papers themselves, but this paper provides a great\noverview and adds some historical perspective.\n[O72] “The Multics System: An Examination of its Structure”\nElliott Organick, 1972\nA great overview of Multics. So many good ideas, and yet it was an over-designed system, shooting for\ntoo much, and thus never really worked as expected. A classic example of what Fred Brooks would call\nthe “second-system effect” [B75].\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2720,
      "extraction_method": "Direct"
    },
    {
      "page_number": 56,
      "chapter": null,
      "content": "20\nINTRODUCTION TO OPERATING SYSTEMS\n[PP03] “Introduction to Computing Systems:\nFrom Bits and Gates to C and Beyond”\nYale N. Patt and Sanjay J. Patel\nMcGraw-Hill, 2003\nOne of our favorite intro to computing systems books. Starts at transistors and gets you all the way up\nto C; the early material is particularly great.\n[RT74] “The UNIX Time-Sharing System”\nDennis M. Ritchie and Ken Thompson\nCACM, Volume 17, Number 7, July 1974, pages 365-375\nA great summary of UNIX written as it was taking over the world of computing, by the people who\nwrote it.\n[S68] “SDS 940 Time-Sharing System”\nScientiﬁc Data Systems Inc.\nTECHNICAL MANUAL, SDS 90 11168 August 1968\nAvailable: http://goo.gl/EN0Zrn\nYes, a technical manual was the best we could ﬁnd. But it is fascinating to read these old system\ndocuments, and see how much was already in place in the late 1960’s. One of the minds behind the\nBerkeley Time-Sharing System (which eventually became the SDS system) was Butler Lampson, who\nlater won a Turing award for his contributions in systems.\n[SS+10] “Membrane: Operating System Support for Restartable File Systems”\nSwaminathan Sundararaman, Sriram Subramanian, Abhishek Rajimwale,\nAndrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau, Michael M. Swift\nFAST ’10, San Jose, CA, February 2010\nThe great thing about writing your own class notes: you can advertise your own research. But this\npaper is actually pretty neat – when a ﬁle system hits a bug and crashes, Membrane auto-magically\nrestarts it, all without applications or the rest of the system being affected.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1607,
      "extraction_method": "Direct"
    },
    {
      "page_number": 57,
      "chapter": null,
      "content": "Part I\nVirtualization\n21\n",
      "content_length": 25,
      "extraction_method": "Direct"
    },
    {
      "page_number": 58,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 59,
      "chapter": null,
      "content": "3\nA Dialogue on Virtualization\nProfessor: And thus we reach the ﬁrst of our three pieces on operating systems:\nvirtualization.\nStudent: But what is virtualization, oh noble professor?\nProfessor: Imagine we have a peach.\nStudent: A peach? (incredulous)\nProfessor: Yes, a peach. Let us call that the physical peach. But we have many\neaters who would like to eat this peach. What we would like to present to each\neater is their own peach, so that they can be happy. We call the peach we give\neaters virtual peaches; we somehow create many of these virtual peaches out of\nthe one physical peach. And the important thing: in this illusion, it looks to each\neater like they have a physical peach, but in reality they don’t.\nStudent: So you are sharing the peach, but you don’t even know it?\nProfessor: Right! Exactly.\nStudent: But there’s only one peach.\nProfessor: Yes. And...?\nStudent: Well, if I was sharing a peach with somebody else, I think I would\nnotice.\nProfessor: Ah yes! Good point. But that is the thing with many eaters; most\nof the time they are napping or doing something else, and thus, you can snatch\nthat peach away and give it to someone else for a while. And thus we create the\nillusion of many virtual peaches, one peach for each person!\nStudent: Sounds like a bad campaign slogan. You are talking about computers,\nright Professor?\nProfessor: Ah, young grasshopper, you wish to have a more concrete example.\nGood idea! Let us take the most basic of resources, the CPU. Assume there is one\nphysical CPU in a system (though now there are often two or four or more). What\nvirtualization does is take that single CPU and make it look like many virtual\nCPUs to the applications running on the system. Thus, while each applications\n23\n",
      "content_length": 1744,
      "extraction_method": "Direct"
    },
    {
      "page_number": 60,
      "chapter": null,
      "content": "24\nA DIALOGUE ON VIRTUALIZATION\nthinks it has its own CPU to use, there is really only one. And thus the OS has\ncreated a beautiful illusion: it has virtualized the CPU.\nStudent: Wow! That sounds like magic. Tell me more! How does that work?\nProfessor: In time, young student, in good time. Sounds like you are ready to\nbegin.\nStudent: I am! Well, sort of. I must admit, I’m a little worried you are going to\nstart talking about peaches again.\nProfessor: Don’t worry too much; I don’t even like peaches. And thus we be-\ngin...\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 574,
      "extraction_method": "Direct"
    },
    {
      "page_number": 61,
      "chapter": null,
      "content": "4\nThe Abstraction: The Process\nIn this note, we discuss one of the most fundamental abstractions that the\nOS provides to users: the process. The deﬁnition of a process, informally,\nis quite simple: it is a running program [V+65,B70]. The program itself is\na lifeless thing: it just sits there on the disk, a bunch of instructions (and\nmaybe some static data), waiting to spring into action. It is the operating\nsystem that takes these bytes and gets them running, transforming the\nprogram into something useful.\nIt turns out that one often wants to run more than one program at\nonce; for example, consider your desktop or laptop where you might like\nto run a web browser, mail program, a game, a music player, and so forth.\nIn fact, a typical system may be seemingly running tens or even hundreds\nof processes at the same time. Doing so makes the system easy to use, as\none never need be concerned with whether a CPU is available; one simply\nruns programs. Hence our challenge:\nTHE CRUX OF THE PROBLEM:\nHOW TO PROVIDE THE ILLUSION OF MANY CPUS?\nAlthough there are only a few physical CPUs available, how can the\nOS provide the illusion of a nearly-endless supply of said CPUs?\nThe OS creates this illusion by virtualizing the CPU. By running one\nprocess, then stopping it and running another, and so forth, the OS can\npromote the illusion that many virtual CPUs exist when in fact there is\nonly one physical CPU (or a few). This basic technique, known as time\nsharing of the CPU, allows users to run as many concurrent processes as\nthey would like; the potential cost is performance, as each will run more\nslowly if the CPU(s) must be shared.\nTo implement virtualization of the CPU, and to implement it well, the\nOS will need both some low-level machinery as well as some high-level\nintelligence. We call the low-level machinery mechanisms; mechanisms\nare low-level methods or protocols that implement a needed piece of\n25\n",
      "content_length": 1923,
      "extraction_method": "Direct"
    },
    {
      "page_number": 62,
      "chapter": null,
      "content": "26\nTHE ABSTRACTION: THE PROCESS\nTIP: USE TIME SHARING (AND SPACE SHARING)\nTime sharing is one of the most basic techniques used by an OS to share\na resource. By allowing the resource to be used for a little while by one\nentity, and then a little while by another, and so forth, the resource in\nquestion (e.g., the CPU, or a network link) can be shared by many. The\nnatural counterpart of time sharing is space sharing, where a resource is\ndivided (in space) among those who wish to use it. For example, disk\nspace is naturally a space-shared resource, as once a block is assigned to\na ﬁle, it is not likely to be assigned to another ﬁle until the user deletes it.\nfunctionality. For example, we’ll learn below how to implement a con-\ntext switch, which gives the OS the ability to stop running one program\nand start running another on a given CPU; this time-sharing mechanism\nis employed by all modern OSes.\nOn top of these mechanisms resides some of the intelligence in the\nOS, in the form of policies. Policies are algorithms for making some\nkind of decision within the OS. For example, given a number of possi-\nble programs to run on a CPU, which program should the OS run? A\nscheduling policy in the OS will make this decision, likely using histori-\ncal information (e.g., which program has run more over the last minute?),\nworkload knowledge (e.g., what types of programs are run), and perfor-\nmance metrics (e.g., is the system optimizing for interactive performance,\nor throughput?) to make its decision.\n4.1\nThe Abstraction: A Process\nThe abstraction provided by the OS of a running program is something\nwe will call a process. As we said above, a process is simply a running\nprogram; at any instant in time, we can summarize a process by taking an\ninventory of the different pieces of the system it accesses or affects during\nthe course of its execution.\nTo understand what constitutes a process, we thus have to understand\nits machine state: what a program can read or update when it is running.\nAt any given time, what parts of the machine are important to the execu-\ntion of this program?\nOne obvious component of machine state that comprises a process is\nits memory. Instructions lie in memory; the data that the running pro-\ngram reads and writes sits in memory as well. Thus the memory that the\nprocess can address (called its address space) is part of the process.\nAlso part of the process’s machine state are registers; many instructions\nexplicitly read or update registers and thus clearly they are important to\nthe execution of the process.\nNote that there are some particularly special registers that form part\nof this machine state. For example, the program counter (PC) (sometimes\ncalled the instruction pointer or IP) tells us which instruction of the pro-\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2827,
      "extraction_method": "Direct"
    },
    {
      "page_number": 63,
      "chapter": null,
      "content": "THE ABSTRACTION: THE PROCESS\n27\nTIP: SEPARATE POLICY AND MECHANISM\nIn many operating systems, a common design paradigm is to separate\nhigh-level policies from their low-level mechanisms [L+75].\nYou can\nthink of the mechanism as providing the answer to a how question about\na system; for example, how does an operating system perform a context\nswitch? The policy provides the answer to a which question; for example,\nwhich process should the operating system run right now? Separating the\ntwo allows one easily to change policies without having to rethink the\nmechanism and is thus a form of modularity, a general software design\nprinciple.\ngram is currently being executed; similarly a stack pointer and associated\nframe pointer are used to manage the stack for function parameters, local\nvariables, and return addresses.\nFinally, programs often access persistent storage devices too. Such I/O\ninformation might include a list of the ﬁles the process currently has open.\n4.2\nProcess API\nThough we defer discussion of a real process API until a subsequent\nchapter, here we ﬁrst give some idea of what must be included in any\ninterface of an operating system. These APIs, in some form, are available\non any modern operating system.\n• Create: An operating system must include some method to cre-\nate new processes. When you type a command into the shell, or\ndouble-click on an application icon, the OS is invoked to create a\nnew process to run the program you have indicated.\n• Destroy: As there is an interface for process creation, systems also\nprovide an interface to destroy processes forcefully. Of course, many\nprocesses will run and just exit by themselves when complete; when\nthey don’t, however, the user may wish to kill them, and thus an in-\nterface to halt a runaway process is quite useful.\n• Wait: Sometimes it is useful to wait for a process to stop running;\nthus some kind of waiting interface is often provided.\n• Miscellaneous Control: Other than killing or waiting for a process,\nthere are sometimes other controls that are possible. For example,\nmost operating systems provide some kind of method to suspend a\nprocess (stop it from running for a while) and then resume it (con-\ntinue it running).\n• Status: There are usually interfaces to get some status information\nabout a process as well, such as how long it has run for, or what\nstate it is in.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2407,
      "extraction_method": "Direct"
    },
    {
      "page_number": 64,
      "chapter": null,
      "content": "28\nTHE ABSTRACTION: THE PROCESS\nMemory\nCPU\nDisk\ncode\nstatic data\nheap\nstack\nProcess\ncode\nstatic data\nProgram\nLoading:\nTakes on-disk program\nand reads it into the\naddress space of process\nFigure 4.1: Loading: From Program To Process\n4.3\nProcess Creation: A Little More Detail\nOne mystery that we should unmask a bit is how programs are trans-\nformed into processes. Speciﬁcally, how does the OS get a program up\nand running? How does process creation actually work?\nThe ﬁrst thing that the OS must do to run a program is to load its code\nand any static data (e.g., initialized variables) into memory, into the ad-\ndress space of the process. Programs initially reside on disk (or, in some\nmodern systems, ﬂash-based SSDs) in some kind of executable format;\nthus, the process of loading a program and static data into memory re-\nquires the OS to read those bytes from disk and place them in memory\nsomewhere (as shown in Figure 4.1).\nIn early (or simple) operating systems, the loading process is done ea-\ngerly, i.e., all at once before running the program; modern OSes perform\nthe process lazily, i.e., by loading pieces of code or data only as they are\nneeded during program execution. To truly understand how lazy loading\nof pieces of code and data works, you’ll have to understand more about\nthe machinery of paging and swapping, topics we’ll cover in the future\nwhen we discuss the virtualization of memory. For now, just remember\nthat before running anything, the OS clearly must do some work to get\nthe important program bits from disk into memory.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1602,
      "extraction_method": "Direct"
    },
    {
      "page_number": 65,
      "chapter": null,
      "content": "THE ABSTRACTION: THE PROCESS\n29\nOnce the code and static data are loaded into memory, there are a few\nother things the OS needs to do before running the process. Some mem-\nory must be allocated for the program’s run-time stack (or just stack).\nAs you should likely already know, C programs use the stack for local\nvariables, function parameters, and return addresses; the OS allocates\nthis memory and gives it to the process. The OS will also likely initial-\nize the stack with arguments; speciﬁcally, it will ﬁll in the parameters to\nthe main() function, i.e., argc and the argv array.\nThe OS may also create some initial memory for the program’s heap.\nIn C programs, the heap is used for explicitly requested dynamically-\nallocated data; programs request such space by calling malloc() and\nfree it explicitly by calling free(). The heap is needed for data struc-\ntures such as linked lists, hash tables, trees, and other interesting data\nstructures. The heap will be small at ﬁrst; as the program runs, and re-\nquests more memory via the malloc() library API, the OS may get in-\nvolved and allocate more memory to the process to help satisfy such calls.\nThe OS will also do some other initialization tasks, particularly as re-\nlated to input/output (I/O). For example, in UNIX systems, each process\nby default has three open ﬁle descriptors, for standard input, output, and\nerror; these descriptors let programs easily read input from the terminal\nas well as print output to the screen. We’ll learn more about I/O, ﬁle\ndescriptors, and the like in the third part of the book on persistence.\nBy loading the code and static data into memory, by creating and ini-\ntializing a stack, and by doing other work as related to I/O setup, the OS\nhas now (ﬁnally) set the stage for program execution. It thus has one last\ntask: to start the program running at the entry point, namely main(). By\njumping to the main() routine (through a specialized mechanism that\nwe will discuss next chapter), the OS transfers control of the CPU to the\nnewly-created process, and thus the program begins its execution.\n4.4\nProcess States\nNow that we have some idea of what a process is (though we will\ncontinue to reﬁne this notion), and (roughly) how it is created, let us talk\nabout the different states a process can be in at a given time. The notion\nthat a process can be in one of these states arose in early computer systems\n[V+65,DV66]. In a simpliﬁed view, a process can be in one of three states:\n• Running: In the running state, a process is running on a processor.\nThis means it is executing instructions.\n• Ready: In the ready state, a process is ready to run but for some\nreason the OS has chosen not to run it at this given moment.\n• Blocked: In the blocked state, a process has performed some kind\nof operation that makes it not ready to run until some other event\ntakes place. A common example: when a process initiates an I/O\nrequest to a disk, it becomes blocked and thus some other process\ncan use the processor.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 3048,
      "extraction_method": "Direct"
    },
    {
      "page_number": 66,
      "chapter": null,
      "content": "30\nTHE ABSTRACTION: THE PROCESS\nRunning\nReady\nBlocked\nDescheduled\nScheduled\nI/O: initiate\nI/O: done\nFigure 4.2: Process: State Transitions\nIf we were to map these states to a graph, we would arrive at the di-\nagram in Figure 4.2. As you can see in the diagram, a process can be\nmoved between the ready and running states at the discretion of the OS.\nBeing moved from ready to running means the process has been sched-\nuled; being moved from running to ready means the process has been\ndescheduled. Once a process has become blocked (e.g., by initiating an\nI/O operation), the OS will keep it as such until some event occurs (e.g.,\nI/O completion); at that point, the process moves to the ready state again\n(and potentially immediately to running again, if the OS so decides).\n4.5\nData Structures\nThe OS is a program, and like any program, it has some key data struc-\ntures that track various relevant pieces of information. To track the state\nof each process, for example, the OS likely will keep some kind of process\nlist for all processes that are ready, as well as some additional informa-\ntion to track which process is currently running. The OS must also track,\nin some way, blocked processes; when an I/O event completes, the OS\nshould make sure to wake the correct process and ready it to run again.\nFigure 4.3 shows what type of information an OS needs to track about\neach process in the xv6 kernel [CK+08]. Similar process structures exist\nin “real” operating systems such as Linux, Mac OS X, or Windows; look\nthem up and see how much more complex they are.\nFrom the ﬁgure, you can see a couple of important pieces of informa-\ntion the OS tracks about a process. The register context will hold, for\na stopped process, the contents of its register state. When a process is\nstopped, its register state will be saved to this memory location; by restor-\ning these registers (i.e., placing their values back into the actual physical\nregisters), the OS can resume running the process. We’ll learn more about\nthis technique known as a context switch in future chapters.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2119,
      "extraction_method": "Direct"
    },
    {
      "page_number": 67,
      "chapter": null,
      "content": "THE ABSTRACTION: THE PROCESS\n31\n// the registers xv6 will save and restore\n// to stop and subsequently restart a process\nstruct context {\nint eip;\nint esp;\nint ebx;\nint ecx;\nint edx;\nint esi;\nint edi;\nint ebp;\n};\n// the different states a process can be in\nenum proc_state { UNUSED, EMBRYO, SLEEPING,\nRUNNABLE, RUNNING, ZOMBIE };\n// the information xv6 tracks about each process\n// including its register context and state\nstruct proc {\nchar *mem;\n// Start of process memory\nuint sz;\n// Size of process memory\nchar *kstack;\n// Bottom of kernel stack\n// for this process\nenum proc_state state;\n// Process state\nint pid;\n// Process ID\nstruct proc *parent;\n// Parent process\nvoid *chan;\n// If non-zero, sleeping on chan\nint killed;\n// If non-zero, have been killed\nstruct file *ofile[NOFILE]; // Open files\nstruct inode *cwd;\n// Current directory\nstruct context context;\n// Switch here to run process\nstruct trapframe *tf;\n// Trap frame for the\n// current interrupt\n};\nFigure 4.3: The xv6 Proc Structure\nYou can also see from the ﬁgure that there are some other states a pro-\ncess can be in, beyond running, ready, and blocked. Sometimes a system\nwill have an initial state that the process is in when it is being created.\nAlso, a process could be placed in a ﬁnal state where it has exited but\nhas not yet been cleaned up (in UNIX-based systems, this is called the\nzombie state1). This ﬁnal state can be useful as it allows other processes\n(usually the parent that created the process) to examine the return code\nof the process and see if it the just-ﬁnished process executed successfully\n(usually, programs return zero in UNIX-based systems when they have\naccomplished a task successfully, and non-zero otherwise). When ﬁn-\nished, the parent will make one ﬁnal call (e.g., wait()) to wait for the\ncompletion of the child, and to also indicate to the OS that it can clean up\nany relevant data structures that referred to the now-extinct process.\n1Yes, the zombie state. Just like real zombies, these zombies are relatively easy to kill.\nHowever, different techniques are usually recommended.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2131,
      "extraction_method": "Direct"
    },
    {
      "page_number": 68,
      "chapter": null,
      "content": "32\nTHE ABSTRACTION: THE PROCESS\nASIDE: DATA STRUCTURE – THE PROCESS LIST\nOperating systems are replete with various important data structures\nthat we will discuss in these notes. The process list is the ﬁrst such struc-\nture. It is one of the simpler ones, but certainly any OS that has the ability\nto run multiple programs at once will have something akin to this struc-\nture in order to keep track of all the running programs in the system.\nSometimes people refer to the individual structure that stores informa-\ntion about a process as a Process Control Block (PCB), a fancy way of\ntalking about a C structure that contains information about each process.\n4.6\nSummary\nWe have introduced the most basic abstraction of the OS: the process.\nIt is quite simply viewed as a running program. With this conceptual\nview in mind, we will now move on to the nitty-gritty: the low-level\nmechanisms needed to implement processes, and the higher-level poli-\ncies required to schedule them in an intelligent way. By combining mech-\nanisms and policies, we will build up our understanding of how an oper-\nating system virtualizes the CPU.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1174,
      "extraction_method": "Direct"
    },
    {
      "page_number": 69,
      "chapter": null,
      "content": "THE ABSTRACTION: THE PROCESS\n33\nReferences\n[CK+08] “The xv6 Operating System”\nRuss Cox, Frans Kaashoek, Robert Morris, Nickolai Zeldovich\nFrom: http://pdos.csail.mit.edu/6.828/2008/index.html\nThe coolest real and little OS in the world. Download and play with it to learn more about the details of\nhow operating systems actually work.\n[DV66] “Programming Semantics for Multiprogrammed Computations”\nJack B. Dennis and Earl C. Van Horn\nCommunications of the ACM, Volume 9, Number 3, March 1966\nThis paper deﬁned many of the early terms and concepts around building multiprogrammed systems.\n[H70] “The Nucleus of a Multiprogramming System”\nPer Brinch Hansen\nCommunications of the ACM, Volume 13, Number 4, April 1970\nThis paper introduces one of the ﬁrst microkernels in operating systems history, called Nucleus. The\nidea of smaller, more minimal systems is a theme that rears its head repeatedly in OS history; it all began\nwith Brinch Hansen’s work described herein.\n[L+75] “Policy/mechanism separation in Hydra”\nR. Levin, E. Cohen, W. Corwin, F. Pollack, W. Wulf\nSOSP 1975\nAn early paper about how to structure operating systems in a research OS known as Hydra. While\nHydra never became a mainstream OS, some of its ideas inﬂuenced OS designers.\n[V+65] “Structure of the Multics Supervisor”\nV.A. Vyssotsky, F. J. Corbato, R. M. Graham\nFall Joint Computer Conference, 1965\nAn early paper on Multics, which described many of the basic ideas and terms that we ﬁnd in modern\nsystems. Some of the vision behind computing as a utility are ﬁnally being realized in modern cloud\nsystems.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 1623,
      "extraction_method": "Direct"
    },
    {
      "page_number": 70,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 71,
      "chapter": null,
      "content": "5\nInterlude: Process API\nASIDE: INTERLUDES\nInterludes will cover more practical aspects of systems, including a par-\nticular focus on operating system APIs and how to use them. If you don’t\nlike practical things, you could skip these interludes. But you should like\npractical things, because, well, they are generally useful in real life; com-\npanies, for example, don’t usually hire you for your non-practical skills.\nIn this interlude, we discuss process creation in UNIX systems. UNIX\npresents one of the most intriguing ways to create a new process with\na pair of system calls: fork() and exec(). A third routine, wait(),\ncan be used by a process wishing to wait for a process it has created to\ncomplete. We now present these interfaces in more detail, with a few\nsimple examples to motivate us. And thus, our problem:\nCRUX: HOW TO CREATE AND CONTROL PROCESSES\nWhat interfaces should the OS present for process creation and con-\ntrol? How should these interfaces be designed to enable ease of use as\nwell as utility?\n5.1\nThe fork() System Call\nThe fork() system call is used to create a new process [C63]. How-\never, be forewarned: it is certainly the strangest routine you will ever\ncall1. More speciﬁcally, you have a running program whose code looks\nlike what you see in Figure 5.1; examine the code, or better yet, type it in\nand run it yourself!\n1Well, OK, we admit that we don’t know that for sure; who knows what routines you\ncall when no one is looking? But fork() is pretty odd, no matter how unusual your routine-\ncalling patterns are.\n35\n",
      "content_length": 1553,
      "extraction_method": "Direct"
    },
    {
      "page_number": 72,
      "chapter": null,
      "content": "36\nINTERLUDE: PROCESS API\n1\n#include <stdio.h>\n2\n#include <stdlib.h>\n3\n#include <unistd.h>\n4\n5\nint\n6\nmain(int argc, char *argv[])\n7\n{\n8\nprintf(\"hello world (pid:%d)\\n\", (int) getpid());\n9\nint rc = fork();\n10\nif (rc < 0) {\n// fork failed; exit\n11\nfprintf(stderr, \"fork failed\\n\");\n12\nexit(1);\n13\n} else if (rc == 0) { // child (new process)\n14\nprintf(\"hello, I am child (pid:%d)\\n\", (int) getpid());\n15\n} else {\n// parent goes down this path (main)\n16\nprintf(\"hello, I am parent of %d (pid:%d)\\n\",\n17\nrc, (int) getpid());\n18\n}\n19\nreturn 0;\n20\n}\nFigure 5.1: p1.c: Calling fork()\nWhen you run this program (called p1.c), you’ll see the following:\nprompt> ./p1\nhello world (pid:29146)\nhello, I am parent of 29147 (pid:29146)\nhello, I am child (pid:29147)\nprompt>\nLet us understand what happened in more detail in p1.c. When it\nﬁrst started running, the process prints out a hello world message; in-\ncluded in that message is its process identiﬁer, also known as a PID. The\nprocess has a PID of 29146; in UNIX systems, the PID is used to name\nthe process if one wants to do something with the process, such as (for\nexample) stop it from running. So far, so good.\nNow the interesting part begins. The process calls the fork() system\ncall, which the OS provides as a way to create a new process. The odd\npart: the process that is created is an (almost) exact copy of the calling pro-\ncess. That means that to the OS, it now looks like there are two copies of\nthe program p1 running, and both are about to return from the fork()\nsystem call. The newly-created process (called the child, in contrast to the\ncreating parent) doesn’t start running at main(), like you might expect\n(note, the “hello, world” message only got printed out once); rather, it\njust comes into life as if it had called fork() itself.\nYou might have noticed: the child isn’t an exact copy. Speciﬁcally, al-\nthough it now has its own copy of the address space (i.e., its own private\nmemory), its own registers, its own PC, and so forth, the value it returns\nto the caller of fork() is different. Speciﬁcally, while the parent receives\nthe PID of the newly-created child, the child is simply returned a 0. This\ndifferentiation is useful, because it is simple then to write the code that\nhandles the two different cases (as above).\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2340,
      "extraction_method": "Direct"
    },
    {
      "page_number": 73,
      "chapter": null,
      "content": "INTERLUDE: PROCESS API\n37\n1\n#include <stdio.h>\n2\n#include <stdlib.h>\n3\n#include <unistd.h>\n4\n#include <sys/wait.h>\n5\n6\nint\n7\nmain(int argc, char *argv[])\n8\n{\n9\nprintf(\"hello world (pid:%d)\\n\", (int) getpid());\n10\nint rc = fork();\n11\nif (rc < 0) {\n// fork failed; exit\n12\nfprintf(stderr, \"fork failed\\n\");\n13\nexit(1);\n14\n} else if (rc == 0) { // child (new process)\n15\nprintf(\"hello, I am child (pid:%d)\\n\", (int) getpid());\n16\n} else {\n// parent goes down this path (main)\n17\nint wc = wait(NULL);\n18\nprintf(\"hello, I am parent of %d (wc:%d) (pid:%d)\\n\",\n19\nrc, wc, (int) getpid());\n20\n}\n21\nreturn 0;\n22\n}\nFigure 5.2: p2.c: Calling fork() And wait()\nYou might also have noticed: the output is not deterministic. When\nthe child process is created, there are now two active processes in the sys-\ntem that we care about: the parent and the child. Assuming we are run-\nning on a system with a single CPU (for simplicity), then either the child\nor the parent might run at that point. In our example (above), the parent\ndid and thus printed out its message ﬁrst. In other cases, the opposite\nmight happen, as we show in this output trace:\nprompt> ./p1\nhello world (pid:29146)\nhello, I am child (pid:29147)\nhello, I am parent of 29147 (pid:29146)\nprompt>\nThe CPU scheduler, a topic we’ll discuss in great detail soon, deter-\nmines which process runs at a given moment in time; because the sched-\nuler is complex, we cannot usually make strong assumptions about what\nit will choose to do, and hence which process will run ﬁrst. This non-\ndeterminism, as it turns out, leads to some interesting problems, par-\nticularly in multi-threaded programs; hence, we’ll see a lot more non-\ndeterminism when we study concurrency in the second part of the book.\n5.2\nAdding wait() System Call\nSo far, we haven’t done much: just created a child that prints out a\nmessage and exits. Sometimes, as it turns out, it is quite useful for a\nparent to wait for a child process to ﬁnish what it has been doing. This\ntask is accomplished with the wait() system call (or its more complete\nsibling waitpid()); see Figure 5.2 for details.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2145,
      "extraction_method": "Direct"
    },
    {
      "page_number": 74,
      "chapter": null,
      "content": "38\nINTERLUDE: PROCESS API\nIn this example (p2.c), the parent process calls wait() to delay its\nexecution until the child ﬁnishes executing.\nWhen the child is done,\nwait() returns to the parent.\nAdding a wait() call to the code above makes the output determin-\nistic. Can you see why? Go ahead, think about it.\n(waiting for you to think .... and done)\nNow that you have thought a bit, here is the output:\nprompt> ./p2\nhello world (pid:29266)\nhello, I am child (pid:29267)\nhello, I am parent of 29267 (wc:29267) (pid:29266)\nprompt>\nWith this code, we now know that the child will always print ﬁrst.\nWhy do we know that? Well, it might simply run ﬁrst, as before, and\nthus print before the parent. However, if the parent does happen to run\nﬁrst, it will immediately call wait(); this system call won’t return until\nthe child has run and exited2. Thus, even when the parent runs ﬁrst, it\npolitely waits for the child to ﬁnish running, then wait() returns, and\nthen the parent prints its message.\n5.3\nFinally, the exec() System Call\nA ﬁnal and important piece of the process creation API is the exec()\nsystem call3. This system call is useful when you want to run a program\nthat is different from the calling program. For example, calling fork()\nin p2.c is only useful if you want to keep running copies of the same\nprogram. However, often you want to run a different program; exec()\ndoes just that (Figure 5.3).\nIn this example, the child process calls execvp() in order to run the\nprogram wc, which is the word counting program. In fact, it runs wc on\nthe source ﬁle p3.c, thus telling us how many lines, words, and bytes are\nfound in the ﬁle:\nprompt> ./p3\nhello world (pid:29383)\nhello, I am child (pid:29384)\n29\n107\n1030 p3.c\nhello, I am parent of 29384 (wc:29384) (pid:29383)\nprompt>\n2There are a few cases where wait() returns before the child exits; read the man page\nfor more details, as always. And beware of any absolute and unqualiﬁed statements this book\nmakes, such as “the child will always print ﬁrst” or “UNIX is the best thing in the world, even\nbetter than ice cream.”\n3Actually, there are six variants of exec(): execl(), execle(), execlp(), execv(),\nand execvp(). Read the man pages to learn more.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2260,
      "extraction_method": "Direct"
    },
    {
      "page_number": 75,
      "chapter": null,
      "content": "INTERLUDE: PROCESS API\n39\n1\n#include <stdio.h>\n2\n#include <stdlib.h>\n3\n#include <unistd.h>\n4\n#include <string.h>\n5\n#include <sys/wait.h>\n6\n7\nint\n8\nmain(int argc, char *argv[])\n9\n{\n10\nprintf(\"hello world (pid:%d)\\n\", (int) getpid());\n11\nint rc = fork();\n12\nif (rc < 0) {\n// fork failed; exit\n13\nfprintf(stderr, \"fork failed\\n\");\n14\nexit(1);\n15\n} else if (rc == 0) { // child (new process)\n16\nprintf(\"hello, I am child (pid:%d)\\n\", (int) getpid());\n17\nchar *myargs[3];\n18\nmyargs[0] = strdup(\"wc\");\n// program: \"wc\" (word count)\n19\nmyargs[1] = strdup(\"p3.c\"); // argument: file to count\n20\nmyargs[2] = NULL;\n// marks end of array\n21\nexecvp(myargs[0], myargs);\n// runs word count\n22\nprintf(\"this shouldn’t print out\");\n23\n} else {\n// parent goes down this path (main)\n24\nint wc = wait(NULL);\n25\nprintf(\"hello, I am parent of %d (wc:%d) (pid:%d)\\n\",\n26\nrc, wc, (int) getpid());\n27\n}\n28\nreturn 0;\n29\n}\nFigure 5.3: p3.c: Calling fork(), wait(), And exec()\nIf fork() was strange, exec() is not so normal either. What it does:\ngiven the name of an executable (e.g., wc), and some arguments (e.g.,\np3.c), it loads code (and static data) from that executable and over-\nwrites its current code segment (and current static data) with it; the heap\nand stack and other parts of the memory space of the program are re-\ninitialized. Then the OS simply runs that program, passing in any argu-\nments as the argv of that process. Thus, it does not create a new process;\nrather, it transforms the currently running program (formerly p3) into a\ndifferent running program (wc). After the exec() in the child, it is al-\nmost as if p3.c never ran; a successful call to exec() never returns.\n5.4\nWhy? Motivating the API\nOf course, one big question you might have: why would we build\nsuch an odd interface to what should be the simple act of creating a new\nprocess? Well, as it turns out, the separation of fork() and exec() is\nessential in building a UNIX shell, because it lets the shell run code after\nthe call to fork() but before the call to exec(); this code can alter the\nenvironment of the about-to-be-run program, and thus enables a variety\nof interesting features to be readily built.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2209,
      "extraction_method": "Direct"
    },
    {
      "page_number": 76,
      "chapter": null,
      "content": "40\nINTERLUDE: PROCESS API\nTIP: GETTING IT RIGHT (LAMPSON’S LAW)\nAs Lampson states in his well-regarded “Hints for Computer Systems\nDesign” [L83], “Get it right. Neither abstraction nor simplicity is a substi-\ntute for getting it right.” Sometimes, you just have to do the right thing,\nand when you do, it is way better than the alternatives. There are lots\nof ways to design APIs for process creation; however, the combination\nof fork() and exec() are simple and immensely powerful. Here, the\nUNIX designers simply got it right. And because Lampson so often “got\nit right”, we name the law in his honor.\nThe shell is just a user program4. It shows you a prompt and then\nwaits for you to type something into it. You then type a command (i.e.,\nthe name of an executable program, plus any arguments) into it; in most\ncases, the shell then ﬁgures out where in the ﬁle system the executable\nresides, calls fork() to create a new child process to run the command,\ncalls some variant of exec() to run the command, and then waits for the\ncommand to complete by calling wait(). When the child completes, the\nshell returns from wait() and prints out a prompt again, ready for your\nnext command.\nThe separation of fork() and exec() allows the shell to do a whole\nbunch of useful things rather easily. For example:\nprompt> wc p3.c > newfile.txt\nIn the example above, the output of the program wc is redirected into\nthe output ﬁle newfile.txt (the greater-than sign is how said redirec-\ntion is indicated). The way the shell accomplishes this task is quite sim-\nple: when the child is created, before calling exec(), the shell closes\nstandard output and opens the ﬁle newfile.txt. By doing so, any out-\nput from the soon-to-be-running program wc are sent to the ﬁle instead\nof the screen.\nFigure 5.4 shows a program that does exactly this. The reason this redi-\nrection works is due to an assumption about how the operating system\nmanages ﬁle descriptors. Speciﬁcally, UNIX systems start looking for free\nﬁle descriptors at zero. In this case, STDOUT FILENO will be the ﬁrst\navailable one and thus get assigned when open() is called. Subsequent\nwrites by the child process to the standard output ﬁle descriptor, for ex-\nample by routines such as printf(), will then be routed transparently\nto the newly-opened ﬁle instead of the screen.\nHere is the output of running the p4.c program:\nprompt> ./p4\nprompt> cat p4.output\n32\n109\n846 p4.c\nprompt>\n4And there are lots of shells; tcsh, bash, and zsh to name a few. You should pick one,\nread its man pages, and learn more about it; all UNIX experts do.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2631,
      "extraction_method": "Direct"
    },
    {
      "page_number": 77,
      "chapter": null,
      "content": "INTERLUDE: PROCESS API\n41\n1\n#include <stdio.h>\n2\n#include <stdlib.h>\n3\n#include <unistd.h>\n4\n#include <string.h>\n5\n#include <fcntl.h>\n6\n#include <sys/wait.h>\n7\n8\nint\n9\nmain(int argc, char *argv[])\n10\n{\n11\nint rc = fork();\n12\nif (rc < 0) {\n// fork failed; exit\n13\nfprintf(stderr, \"fork failed\\n\");\n14\nexit(1);\n15\n} else if (rc == 0) { // child: redirect standard output to a file\n16\nclose(STDOUT_FILENO);\n17\nopen(\"./p4.output\", O_CREAT|O_WRONLY|O_TRUNC, S_IRWXU);\n18\n19\n// now exec \"wc\"...\n20\nchar *myargs[3];\n21\nmyargs[0] = strdup(\"wc\");\n// program: \"wc\" (word count)\n22\nmyargs[1] = strdup(\"p4.c\"); // argument: file to count\n23\nmyargs[2] = NULL;\n// marks end of array\n24\nexecvp(myargs[0], myargs);\n// runs word count\n25\n} else {\n// parent goes down this path (main)\n26\nint wc = wait(NULL);\n27\n}\n28\nreturn 0;\n29\n}\nFigure 5.4: p4.c: All Of The Above With Redirection\nYou’ll notice (at least) two interesting tidbits about this output. First,\nwhen p4 is run, it looks as if nothing has happened; the shell just prints\nthe command prompt and is immediately ready for your next command.\nHowever, that is not the case; the program p4 did indeed call fork() to\ncreate a new child, and then run the wc program via a call to execvp().\nYou don’t see any output printed to the screen because it has been redi-\nrected to the ﬁle p4.output. Second, you can see that when we cat the\noutput ﬁle, all the expected output from running wc is found. Cool, right?\nUNIX pipes are implemented in a similar way, but with the pipe()\nsystem call. In this case, the output of one process is connected to an in-\nkernel pipe (i.e., queue), and the input of another process is connected\nto that same pipe; thus, the output of one process seamlessly is used as\ninput to the next, and long and useful chains of commands can be strung\ntogether. As a simple example, consider the looking for a word in a ﬁle,\nand then counting how many times said word occurs; with pipes and the\nutilities grep and wc, it is easy – just type grep foo file | wc -l\ninto the command prompt and marvel at the result.\nFinally, while we just have sketched out the process API at a high level,\nthere is a lot more detail about these calls out there to be learned and\ndigested; we’ll learn more, for example, about ﬁle descriptors when we\ntalk about ﬁle systems in the third part of the book. For now, sufﬁce it\nto say that the fork()/exec() combination is a powerful way to create\nand manipulate processes.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2493,
      "extraction_method": "Direct"
    },
    {
      "page_number": 78,
      "chapter": null,
      "content": "42\nINTERLUDE: PROCESS API\nASIDE: RTFM – READ THE MAN PAGES\nMany times in this book, when referring to a particular system call or\nlibrary call, we’ll tell you to read the manual pages, or man pages for\nshort. Man pages are the original form of documentation that exist on\nUNIX systems; realize that they were created before the thing called the\nweb existed.\nSpending some time reading man pages is a key step in the growth of\na systems programmer; there are tons of useful tidbits hidden in those\npages.\nSome particularly useful pages to read are the man pages for\nwhichever shell you are using (e.g., tcsh, or bash), and certainly for any\nsystem calls your program makes (in order to see what return values and\nerror conditions exist).\nFinally, reading the man pages can save you some embarrassment. When\nyou ask colleagues about some intricacy of fork(), they may simply\nreply: “RTFM.” This is your colleagues’ way of gently urging you to Read\nThe Man pages. The F in RTFM just adds a little color to the phrase...\n5.5\nOther Parts of the API\nBeyond fork(), exec(), and wait(), there are a lot of other inter-\nfaces for interacting with processes in UNIX systems. For example, the\nkill() system call is used to send signals to a process, including direc-\ntives to go to sleep, die, and other useful imperatives. In fact, the entire\nsignals subsystem provides a rich infrastructure to deliver external events\nto processes, including ways to receive and process those signals.\nThere are many command-line tools that are useful as well. For exam-\nple, using the ps command allows you to see which processes are run-\nning; read the man pages for some useful ﬂags to pass to ps. The tool\ntop is also quite helpful, as it displays the processes of the system and\nhow much CPU and other resources they are eating up. Humorously,\nmany times when you run it, top claims it is the top resource hog; per-\nhaps it is a bit of an egomaniac. Finally, there are many different kinds of\nCPU meters you can use to get a quick glance understanding of the load\non your system; for example, we always keep MenuMeters (from Raging\nMenace software) running on our Macintosh toolbars, so we can see how\nmuch CPU is being utilized at any moment in time. In general, the more\ninformation about what is going on, the better.\n5.6\nSummary\nWe have introduced some of the APIs dealing with UNIX process cre-\nation: fork(), exec(), and wait(). However, we have just skimmed\nthe surface. For more detail, read Stevens and Rago [SR05], of course,\nparticularly the chapters on Process Control, Process Relationships, and\nSignals. There is much to extract from the wisdom therein.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2694,
      "extraction_method": "Direct"
    },
    {
      "page_number": 79,
      "chapter": null,
      "content": "INTERLUDE: PROCESS API\n43\nReferences\n[C63] “A Multiprocessor System Design”\nMelvin E. Conway\nAFIPS ’63 Fall Joint Computer Conference\nNew York, USA 1963\nAn early paper on how to design multiprocessing systems; may be the ﬁrst place the term fork() was\nused in the discussion of spawning new processes.\n[DV66] “Programming Semantics for Multiprogrammed Computations”\nJack B. Dennis and Earl C. Van Horn\nCommunications of the ACM, Volume 9, Number 3, March 1966\nA classic paper that outlines the basics of multiprogrammed computer systems. Undoubtedly had great\ninﬂuence on Project MAC, Multics, and eventually UNIX.\n[L83] “Hints for Computer Systems Design”\nButler Lampson\nACM Operating Systems Review, 15:5, October 1983\nLampson’s famous hints on how to design computer systems. You should read it at some point in your\nlife, and probably at many points in your life.\n[SR05] “Advanced Programming in the UNIX Environment”\nW. Richard Stevens and Stephen A. Rago\nAddison-Wesley, 2005\nAll nuances and subtleties of using UNIX APIs are found herein. Buy this book! Read it! And most\nimportantly, live it.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 1142,
      "extraction_method": "Direct"
    },
    {
      "page_number": 80,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 81,
      "chapter": null,
      "content": "6\nMechanism: Limited Direct Execution\nIn order to virtualize the CPU, the operating system needs to somehow\nshare the physical CPU among many jobs running seemingly at the same\ntime. The basic idea is simple: run one process for a little while, then\nrun another one, and so forth. By time sharing the CPU in this manner,\nvirtualization is achieved.\nThere are a few challenges, however, in building such virtualization\nmachinery. The ﬁrst is performance: how can we implement virtualiza-\ntion without adding excessive overhead to the system? The second is\ncontrol: how can we run processes efﬁciently while retaining control over\nthe CPU? Control is particularly important to the OS, as it is in charge of\nresources; without control, a process could simply run forever and take\nover the machine, or access information that it should not be allowed to\naccess. Attaining performance while maintaining control is thus one of\nthe central challenges in building an operating system.\nTHE CRUX:\nHOW TO EFFICIENTLY VIRTUALIZE THE CPU WITH CONTROL\nThe OS must virtualize the CPU in an efﬁcient manner, but while re-\ntaining control over the system. To do so, both hardware and operating\nsystems support will be required. The OS will often use a judicious bit of\nhardware support in order to accomplish its work effectively.\n6.1\nBasic Technique: Limited Direct Execution\nTo make a program run as fast as one might expect, not surprisingly\nOS developers came up with a technique, which we call limited direct\nexecution. The “direct execution” part of the idea is simple: just run the\nprogram directly on the CPU. Thus, when the OS wishes to start a pro-\ngram running, it creates a process entry for it in a process list, allocates\nsome memory pages for it, loads the program code into memory (from\n45\n",
      "content_length": 1789,
      "extraction_method": "Direct"
    },
    {
      "page_number": 82,
      "chapter": null,
      "content": "46\nMECHANISM: LIMITED DIRECT EXECUTION\nOS\nProgram\nCreate entry for process list\nAllocate memory for program\nLoad program into memory\nSet up stack with argc/argv\nClear registers\nExecute call main()\nRun main()\nExecute return from main\nFree memory of process\nRemove from process list\nTable 6.1: Direction Execution Protocol (Without Limits)\ndisk), locates its entry point (i.e., the main() routine or something simi-\nlar), jumps to it, and starts running the user’s code. Table 6.1 shows this\nbasic direct execution protocol (without any limits, yet), using a normal\ncall and return to jump to the program’s main() and later to get back\ninto the kernel.\nSounds simple, no? But this approach gives rise to a few problems\nin our quest to virtualize the CPU. The ﬁrst is simple: if we just run a\nprogram, how can the OS make sure the program doesn’t do anything\nthat we don’t want it to do, while still running it efﬁciently? The second:\nwhen we are running a process, how does the operating system stop it\nfrom running and switch to another process, thus implementing the time\nsharing we require to virtualize the CPU?\nIn answering these questions below, we’ll get a much better sense of\nwhat is needed to virtualize the CPU. In developing these techniques,\nwe’ll also see where the “limited” part of the name arises from; without\nlimits on running programs, the OS wouldn’t be in control of anything\nand thus would be “just a library” – a very sad state of affairs for an\naspiring operating system!\n6.2\nProblem #1: Restricted Operations\nDirect execution has the obvious advantage of being fast; the program\nruns natively on the hardware CPU and thus executes as quickly as one\nwould expect. But running on the CPU introduces a problem: what if\nthe process wishes to perform some kind of restricted operation, such\nas issuing an I/O request to a disk, or gaining access to more system\nresources such as CPU or memory?\nTHE CRUX: HOW TO PERFORM RESTRICTED OPERATIONS\nA process must be able to perform I/O and some other restricted oper-\nations, but without giving the process complete control over the system.\nHow can the OS and hardware work together to do so?\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2202,
      "extraction_method": "Direct"
    },
    {
      "page_number": 83,
      "chapter": null,
      "content": "MECHANISM: LIMITED DIRECT EXECUTION\n47\nTIP: USE PROTECTED CONTROL TRANSFER\nThe hardware assists the OS by providing different modes of execution.\nIn user mode, applications do not have full access to hardware resources.\nIn kernel mode, the OS has access to the full resources of the machine.\nSpecial instructions to trap into the kernel and return-from-trap back to\nuser-mode programs are also provided, as well instructions that allow the\nOS to tell the hardware where the trap table resides in memory.\nOne approach would simply be to let any process do whatever it wants\nin terms of I/O and other related operations. However, doing so would\nprevent the construction of many kinds of systems that are desirable. For\nexample, if we wish to build a ﬁle system that checks permissions before\ngranting access to a ﬁle, we can’t simply let any user process issue I/Os\nto the disk; if we did, a process could simply read or write the entire disk\nand thus all protections would be lost.\nThus, the approach we take is to introduce a new processor mode,\nknown as user mode; code that runs in user mode is restricted in what it\ncan do. For example, when running in user mode, a process can’t issue\nI/O requests; doing so would result in the processor raising an exception;\nthe OS would then likely kill the process.\nIn contrast to user mode is kernel mode, which the operating system\n(or kernel) runs in. In this mode, code that runs can do what it likes, in-\ncluding privileged operations such as issuing I/O requests and executing\nall types of restricted instructions.\nWe are still left with a challenge, however: what should a user pro-\ncess do when it wishes to perform some kind of privileged operation,\nsuch as reading from disk? To enable this, virtually all modern hard-\nware provides the ability for user programs to perform a system call.\nPioneered on ancient machines such as the Atlas [K+61,L78], system calls\nallow the kernel to carefully expose certain key pieces of functionality to\nuser programs, such as accessing the ﬁle system, creating and destroy-\ning processes, communicating with other processes, and allocating more\nmemory. Most operating systems provide a few hundred calls (see the\nPOSIX standard for details [P10]); early Unix systems exposed a more\nconcise subset of around twenty calls.\nTo execute a system call, a program must execute a special trap instruc-\ntion. This instruction simultaneously jumps into the kernel and raises the\nprivilege level to kernel mode; once in the kernel, the system can now per-\nform whatever privileged operations are needed (if allowed), and thus do\nthe required work for the calling process. When ﬁnished, the OS calls a\nspecial return-from-trap instruction, which, as you might expect, returns\ninto the calling user program while simultaneously reducing the privi-\nlege level back to user mode.\nThe hardware needs to be a bit careful when executing a trap, in that\nit must make sure to save enough of the caller’s register state in order\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 3035,
      "extraction_method": "Direct"
    },
    {
      "page_number": 84,
      "chapter": null,
      "content": "48\nMECHANISM: LIMITED DIRECT EXECUTION\nASIDE: WHY SYSTEM CALLS LOOK LIKE PROCEDURE CALLS\nYou may wonder why a call to a system call, such as open() or read(),\nlooks exactly like a typical procedure call in C; that is, if it looks just like\na procedure call, how does the system know it’s a system call, and do all\nthe right stuff? The simple reason: it is a procedure call, but hidden in-\nside that procedure call is the famous trap instruction. More speciﬁcally,\nwhen you call open() (for example), you are executing a procedure call\ninto the C library. Therein, whether for open() or any of the other sys-\ntem calls provided, the library uses an agreed-upon calling convention\nwith the kernel to put the arguments to open in well-known locations\n(e.g., on the stack, or in speciﬁc registers), puts the system-call number\ninto a well-known location as well (again, onto the stack or a register),\nand then executes the aforementioned trap instruction. The code in the\nlibrary after the trap unpacks return values and returns control to the\nprogram that issued the system call. Thus, the parts of the C library that\nmake system calls are hand-coded in assembly, as they need to carefully\nfollow convention in order to process arguments and return values cor-\nrectly, as well as execute the hardware-speciﬁc trap instruction. And now\nyou know why you personally don’t have to write assembly code to trap\ninto an OS; somebody has already written that assembly for you.\nto be able to return correctly when the OS issues the return-from-trap\ninstruction. On x86, for example, the processor will push the program\ncounter, ﬂags, and a few other registers onto a per-process kernel stack;\nthe return-from-trap will pop these values off the stack and resume exe-\ncution of the user-mode program (see the Intel systems manuals [I11] for\ndetails). Other hardware systems use different conventions, but the basic\nconcepts are similar across platforms.\nThere is one important detail left out of this discussion: how does the\ntrap know which code to run inside the OS? Clearly, the calling process\ncan’t specify an address to jump to (as you would when making a pro-\ncedure call); doing so would allow programs to jump anywhere into the\nkernel which clearly is a bad idea (imagine jumping into code to access\na ﬁle, but just after a permission check; in fact, it is likely such ability\nwould enable a wily programmer to get the kernel to run arbitrary code\nsequences [S07]). Thus the kernel must carefully control what code exe-\ncutes upon a trap.\nThe kernel does so by setting up a trap table at boot time. When the\nmachine boots up, it does so in privileged (kernel) mode, and thus is\nfree to conﬁgure machine hardware as need be. One of the ﬁrst things\nthe OS thus does is to tell the hardware what code to run when certain\nexceptional events occur. For example, what code should run when a\nhard-disk interrupt takes place, when a keyboard interrupt occurs, or\nwhen program makes a system call? The OS informs the hardware of\nthe locations of these trap handlers, usually with some kind of special\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 3134,
      "extraction_method": "Direct"
    },
    {
      "page_number": 85,
      "chapter": null,
      "content": "MECHANISM: LIMITED DIRECT EXECUTION\n49\nOS @ boot\nHardware\n(kernel mode)\ninitialize trap table\nremember address of...\nsyscall handler\nOS @ run\nHardware\nProgram\n(kernel mode)\n(user mode)\nCreate entry for process list\nAllocate memory for program\nLoad program into memory\nSetup user stack with argv\nFill kernel stack with reg/PC\nreturn-from-trap\nrestore regs from kernel stack\nmove to user mode\njump to main\nRun main()\n...\nCall system call\ntrap into OS\nsave regs to kernel stack\nmove to kernel mode\njump to trap handler\nHandle trap\nDo work of syscall\nreturn-from-trap\nrestore regs from kernel stack\nmove to user mode\njump to PC after trap\n...\nreturn from main\ntrap (via exit())\nFree memory of process\nRemove from process list\nTable 6.2: Limited Direction Execution Protocol\ninstruction. Once the hardware is informed, it remembers the location of\nthese handlers until the machine is next rebooted, and thus the hardware\nknows what to do (i.e., what code to jump to) when system calls and other\nexceptional events take place.\nOne last aside: being able to execute the instruction to tell the hard-\nware where the trap tables are is a very powerful capability. Thus, as you\nmight have guessed, it is also a privileged operation. If you try to exe-\ncute this instruction in user mode, the hardware won’t let you, and you\ncan probably guess what will happen (hint: adios, offending program).\nPoint to ponder: what horrible things could you do to a system if you\ncould install your own trap table? Could you take over the machine?\nThe timeline (with time increasing downward, in Table 6.2) summa-\nrizes the protocol. We assume each process has a kernel stack where reg-\nisters (including general purpose registers and the program counter) are\nsaved to and restored from (by the hardware) when transitioning into and\nout of the kernel.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 1867,
      "extraction_method": "Direct"
    },
    {
      "page_number": 86,
      "chapter": null,
      "content": "50\nMECHANISM: LIMITED DIRECT EXECUTION\nThere are two phases in the LDE protocol. In the ﬁrst (at boot time),\nthe kernel initializes the trap table, and the CPU remembers its location\nfor subsequent use. The kernel does so via a privileged instruction (all\nprivileged instructions are highlighted in bold).\nIn the second (when running a process), the kernel sets up a few things\n(e.g., allocating a node on the process list, allocating memory) before us-\ning a return-from-trap instruction to start the execution of the process;\nthis switches the CPU to user mode and begins running the process.\nWhen the process wishes to issue a system call, it traps back into the OS,\nwhich handles it and once again returns control via a return-from-trap\nto the process. The process then completes its work, and returns from\nmain(); this usually will return into some stub code which will properly\nexit the program (say, by calling the exit() system call, which traps into\nthe OS). At this point, the OS cleans up and we are done.\n6.3\nProblem #2: Switching Between Processes\nThe next problem with direct execution is achieving a switch between\nprocesses. Switching between processes should be simple, right? The\nOS should just decide to stop one process and start another. What’s the\nbig deal? But it actually is a little bit tricky: speciﬁcally, if a process is\nrunning on the CPU, this by deﬁnition means the OS is not running. If\nthe OS is not running, how can it do anything at all? (hint: it can’t) While\nthis sounds almost philosophical, it is a real problem: there is clearly no\nway for the OS to take an action if it is not running on the CPU. Thus we\narrive at the crux of the problem.\nTHE CRUX: HOW TO REGAIN CONTROL OF THE CPU\nHow can the operating system regain control of the CPU so that it can\nswitch between processes?\nA Cooperative Approach: Wait For System Calls\nOne approach that some systems have taken in the past (for example,\nearly versions of the Macintosh operating system [M11], or the old Xerox\nAlto system [A79]) is known as the cooperative approach. In this style,\nthe OS trusts the processes of the system to behave reasonably. Processes\nthat run for too long are assumed to periodically give up the CPU so that\nthe OS can decide to run some other task.\nThus, you might ask, how does a friendly process give up the CPU in\nthis utopian world? Most processes, as it turns out, transfer control of\nthe CPU to the OS quite frequently by making system calls, for example,\nto open a ﬁle and subsequently read it, or to send a message to another\nmachine, or to create a new process. Systems like this often include an\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2673,
      "extraction_method": "Direct"
    },
    {
      "page_number": 87,
      "chapter": null,
      "content": "MECHANISM: LIMITED DIRECT EXECUTION\n51\nTIP: DEALING WITH APPLICATION MISBEHAVIOR\nOperating systems often have to deal with misbehaving processes, those\nthat either through design (maliciousness) or accident (bugs) attempt to\ndo something that they shouldn’t. In modern systems, the way the OS\ntries to handle such malfeasance is to simply terminate the offender. One\nstrike and you’re out! Perhaps brutal, but what else should the OS do\nwhen you try to access memory illegally or execute an illegal instruction?\nexplicit yield system call, which does nothing except to transfer control\nto the OS so it can run other processes.\nApplications also transfer control to the OS when they do something\nillegal. For example, if an application divides by zero, or tries to access\nmemory that it shouldn’t be able to access, it will generate a trap to the\nOS. The OS will then have control of the CPU again (and likely terminate\nthe offending process).\nThus, in a cooperative scheduling system, the OS regains control of\nthe CPU by waiting for a system call or an illegal operation of some kind\nto take place. You might also be thinking: isn’t this passive approach less\nthan ideal? What happens, for example, if a process (whether malicious,\nor just full of bugs) ends up in an inﬁnite loop, and never makes a system\ncall? What can the OS do then?\nA Non-Cooperative Approach: The OS Takes Control\nWithout some additional help from the hardware, it turns out the OS can’t\ndo much at all when a process refuses to make system calls (or mistakes)\nand thus return control to the OS. In fact, in the cooperative approach,\nyour only recourse when a process gets stuck in an inﬁnite loop is to\nresort to the age-old solution to all problems in computer systems: reboot\nthe machine. Thus, we again arrive at a subproblem of our general quest\nto gain control of the CPU.\nTHE CRUX: HOW TO GAIN CONTROL WITHOUT COOPERATION\nHow can the OS gain control of the CPU even if processes are not being\ncooperative? What can the OS do to ensure a rogue process does not take\nover the machine?\nThe answer turns out to be simple and was discovered by a number\nof people building computer systems many years ago: a timer interrupt\n[M+63]. A timer device can be programmed to raise an interrupt every\nso many milliseconds; when the interrupt is raised, the currently running\nprocess is halted, and a pre-conﬁgured interrupt handler in the OS runs.\nAt this point, the OS has regained control of the CPU, and thus can do\nwhat it pleases: stop the current process, and start a different one.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2597,
      "extraction_method": "Direct"
    },
    {
      "page_number": 88,
      "chapter": null,
      "content": "52\nMECHANISM: LIMITED DIRECT EXECUTION\nTIP: USE THE TIMER INTERRUPT TO REGAIN CONTROL\nThe addition of a timer interrupt gives the OS the ability to run again\non a CPU even if processes act in a non-cooperative fashion. Thus, this\nhardware feature is essential in helping the OS maintain control of the\nmachine.\nAs we discussed before with system calls, the OS must inform the\nhardware of which code to run when the timer interrupt occurs; thus,\nat boot time, the OS does exactly that.\nSecond, also during the boot\nsequence, the OS must start the timer, which is of course a privileged\noperation. Once the timer has begun, the OS can thus feel safe in that\ncontrol will eventually be returned to it, and thus the OS is free to run\nuser programs. The timer can also be turned off (also a privileged opera-\ntion), something we will discuss later when we understand concurrency\nin more detail.\nNote that the hardware has some responsibility when an interrupt oc-\ncurs, in particular to save enough of the state of the program that was\nrunning when the interrupt occurred such that a subsequent return-from-\ntrap instruction will be able to resume the running program correctly.\nThis set of actions is quite similar to the behavior of the hardware during\nan explicit system-call trap into the kernel, with various registers thus\ngetting saved (e.g., onto a kernel stack) and thus easily restored by the\nreturn-from-trap instruction.\nSaving and Restoring Context\nNow that the OS has regained control, whether cooperatively via a sys-\ntem call, or more forcefully via a timer interrupt, a decision has to be\nmade: whether to continue running the currently-running process, or\nswitch to a different one. This decision is made by a part of the operating\nsystem known as the scheduler; we will discuss scheduling policies in\ngreat detail in the next few chapters.\nIf the decision is made to switch, the OS then executes a low-level\npiece of code which we refer to as a context switch. A context switch is\nconceptually simple: all the OS has to do is save a few register values\nfor the currently-executing process (onto its kernel stack, for example)\nand restore a few for the soon-to-be-executing process (from its kernel\nstack). By doing so, the OS thus ensures that when the return-from-trap\ninstruction is ﬁnally executed, instead of returning to the process that was\nrunning, the system resumes execution of another process.\nTo save the context of the currently-running process, the OS will exe-\ncute some low-level assembly code to save the general purpose registers,\nPC, as well as the kernel stack pointer of the currently-running process,\nand then restore said registers, PC, and switch to the kernel stack for the\nsoon-to-be-executing process. By switching stacks, the kernel enters the\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2833,
      "extraction_method": "Direct"
    },
    {
      "page_number": 89,
      "chapter": null,
      "content": "MECHANISM: LIMITED DIRECT EXECUTION\n53\nOS @ boot\nHardware\n(kernel mode)\ninitialize trap table\nremember addresses of...\nsyscall handler\ntimer handler\nstart interrupt timer\nstart timer\ninterrupt CPU in X ms\nOS @ run\nHardware\nProgram\n(kernel mode)\n(user mode)\nProcess A\n...\ntimer interrupt\nsave regs(A) to k-stack(A)\nmove to kernel mode\njump to trap handler\nHandle the trap\nCall switch() routine\nsave regs(A) to proc-struct(A)\nrestore regs(B) from proc-struct(B)\nswitch to k-stack(B)\nreturn-from-trap (into B)\nrestore regs(B) from k-stack(B)\nmove to user mode\njump to B’s PC\nProcess B\n...\nTable 6.3: Limited Direction Execution Protocol (Timer Interrupt)\ncall to the switch code in the context of one process (the one that was in-\nterrupted) and returns in the context of another (the soon-to-be-executing\none). When the OS then ﬁnally executes a return-from-trap instruction,\nthe soon-to-be-executing process becomes the currently-running process.\nAnd thus the context switch is complete.\nA timeline of the entire process is shown in Table 6.3. In this exam-\nple, Process A is running and then is interrupted by the timer interrupt.\nThe hardware saves its state (onto its kernel stack) and enters the kernel\n(switching to kernel mode). In the timer interrupt handler, the OS decides\nto switch from running Process A to Process B. At that point, it calls the\nswitch() routine, which carefully saves current register values (into the\nprocess structure of A), restores the registers of Process B (from its process\nstructure entry), and then switches contexts, speciﬁcally by changing the\nstack pointer to use B’s kernel stack (and not A’s). Finally, the OS returns-\nfrom-trap, which restores B’s register state and starts running it.\nNote that there are two types of register saves/restores that happen\nduring this protocol. The ﬁrst is when the timer interrupt occurs; in this\ncase, the user register state of the running process is implicitly saved by\nthe hardware, using the kernel stack of that process. The second is when\nthe OS decides to switch from A to B; in this case, the kernel register state\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2141,
      "extraction_method": "Direct"
    },
    {
      "page_number": 90,
      "chapter": null,
      "content": "54\nMECHANISM: LIMITED DIRECT EXECUTION\n1\n# void swtch(struct context **old, struct context *new);\n2\n#\n3\n# Save current register context in old\n4\n# and then load register context from new.\n5\n.globl swtch\n6\nswtch:\n7\n# Save old registers\n8\nmovl 4(%esp), %eax\n# put old ptr into eax\n9\npopl 0(%eax)\n# save the old IP\n10\nmovl %esp, 4(%eax)\n# and stack\n11\nmovl %ebx, 8(%eax)\n# and other registers\n12\nmovl %ecx, 12(%eax)\n13\nmovl %edx, 16(%eax)\n14\nmovl %esi, 20(%eax)\n15\nmovl %edi, 24(%eax)\n16\nmovl %ebp, 28(%eax)\n17\n18\n# Load new registers\n19\nmovl 4(%esp), %eax\n# put new ptr into eax\n20\nmovl 28(%eax), %ebp # restore other registers\n21\nmovl 24(%eax), %edi\n22\nmovl 20(%eax), %esi\n23\nmovl 16(%eax), %edx\n24\nmovl 12(%eax), %ecx\n25\nmovl 8(%eax), %ebx\n26\nmovl 4(%eax), %esp\n# stack is switched here\n27\npushl 0(%eax)\n# return addr put in place\n28\nret\n# finally return into new ctxt\nFigure 6.1: The xv6 Context Switch Code\nis explicitly saved by the software (i.e., the OS), but this time into memory\nin the process structure of the process. The latter action moves the system\nfrom running as if it just trapped into the kernel from A to as if it just\ntrapped into the kernel from B.\nTo give you a better sense of how such a switch is enacted, Figure 6.1\nshows the context switch code for xv6. See if you can make sense of it\n(you’ll have to know a bit of x86, as well as some xv6, to do so). The\ncontext structures old and new are found the old and new process’s\nprocess structures, respectively.\n6.4\nWorried About Concurrency?\nSome of you, as attentive and thoughtful readers, may be now think-\ning: “Hmm... what happens when, during a system call, a timer interrupt\noccurs?” or “What happens when you’re handling one interrupt and an-\nother one happens? Doesn’t that get hard to handle in the kernel?” Good\nquestions – we really have some hope for you yet!\nThe answer is yes, the OS does indeed need to be concerned as to what\nhappens if, during interrupt or trap handling, another interrupt occurs.\nThis, in fact, is the exact topic of the entire second piece of this book, on\nconcurrency; we’ll defer a detailed discussion until then.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2173,
      "extraction_method": "Direct"
    },
    {
      "page_number": 91,
      "chapter": null,
      "content": "MECHANISM: LIMITED DIRECT EXECUTION\n55\nASIDE: HOW LONG CONTEXT SWITCHES TAKE\nA natural question you might have is: how long does something like a\ncontext switch take? Or even a system call? For those of you that are cu-\nrious, there is a tool called lmbench [MS96] that measures exactly those\nthings, as well as a few other performance measures that might be rele-\nvant.\nResults have improved quite a bit over time, roughly tracking processor\nperformance. For example, in 1996 running Linux 1.3.37 on a 200-MHz\nP6 CPU, system calls took roughly 4 microseconds, and a context switch\nroughly 6 microseconds [MS96]. Modern systems perform almost an or-\nder of magnitude better, with sub-microsecond results on systems with\n2- or 3-GHz processors.\nIt should be noted that not all operating-system actions track CPU per-\nformance. As Ousterhout observed, many OS operations are memory\nintensive, and memory bandwidth has not improved as dramatically as\nprocessor speed over time [O90]. Thus, depending on your workload,\nbuying the latest and greatest processor may not speed up your OS as\nmuch as you might hope.\nTo whet your appetite, we’ll just sketch some basics of how the OS\nhandles these tricky situations. One simple thing an OS might do is dis-\nable interrupts during interrupt processing; doing so ensures that when\none interrupt is being handled, no other one will be delivered to the CPU.\nOf course, the OS has to be careful in doing so; disabling interrupts for\ntoo long could lead to lost interrupts, which is (in technical terms) bad.\nOperating systems also have developed a number of sophisticated\nlocking schemes to protect concurrent access to internal data structures.\nThis enables multiple activities to be on-going within the kernel at the\nsame time, particularly useful on multiprocessors. As we’ll see in the\nnext piece of this book on concurrency, though, such locking can be com-\nplicated and lead to a variety of interesting and hard-to-ﬁnd bugs.\n6.5\nSummary\nWe have described some key low-level mechanisms to implement CPU\nvirtualization, a set of techniques which we collectively refer to as limited\ndirect execution. The basic idea is straightforward: just run the program\nyou want to run on the CPU, but ﬁrst make sure to set up the hardware\nso as to limit what the process can do without OS assistance.\nThis general approach is taken in real life as well. For example, those\nof you who have children, or, at least, have heard of children, may be\nfamiliar with the concept of baby prooﬁng a room: locking cabinets con-\ntaining dangerous stuff and covering electrical sockets. When the room is\nthus readied, you can let your baby roam freely, secure in the knowledge\nthat the most dangerous aspects of the room have been restricted.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2797,
      "extraction_method": "Direct"
    },
    {
      "page_number": 92,
      "chapter": null,
      "content": "56\nMECHANISM: LIMITED DIRECT EXECUTION\nTIP: REBOOT IS USEFUL\nEarlier on, we noted that the only solution to inﬁnite loops (and similar\nbehaviors) under cooperative preemption is to reboot the machine. While\nyou may scoff at this hack, researchers have shown that reboot (or in gen-\neral, starting over some piece of software) can be a hugely useful tool in\nbuilding robust systems [C+04].\nSpeciﬁcally, reboot is useful because it moves software back to a known\nand likely more tested state.\nReboots also reclaim stale or leaked re-\nsources (e.g., memory) which may otherwise be hard to handle. Finally,\nreboots are easy to automate. For all of these reasons, it is not uncommon\nin large-scale cluster Internet services for system management software\nto periodically reboot sets of machines in order to reset them and thus\nobtain the advantages listed above.\nThus, next time you reboot, you are not just enacting some ugly hack.\nRather, you are using a time-tested approach to improving the behavior\nof a computer system. Well done!\nIn an analogous manner, the OS “baby proofs” the CPU, by ﬁrst (dur-\ning boot time) setting up the trap handlers and starting an interrupt timer,\nand then by only running processes in a restricted mode. By doing so, the\nOS can feel quite assured that processes can run efﬁciently, only requir-\ning OS intervention to perform privileged operations or when they have\nmonopolized the CPU for too long and thus need to be switched out.\nWe thus have the basic mechanisms for virtualizing the CPU in place.\nBut a major question is left unanswered: which process should we run at\na given time? It is this question that the scheduler must answer, and thus\nthe next topic of our study.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1755,
      "extraction_method": "Direct"
    },
    {
      "page_number": 93,
      "chapter": null,
      "content": "MECHANISM: LIMITED DIRECT EXECUTION\n57\nReferences\n[A79] “Alto User’s Handbook”\nXerox Palo Alto Research Center, September 1979\nAvailable: http://history-computer.com/Library/AltoUsersHandbook.pdf\nAn amazing system, way ahead of its time. Became famous because Steve Jobs visited, took notes, and\nbuilt Lisa and eventually Mac.\n[C+04] “Microreboot – A Technique for Cheap Recovery”\nGeorge Candea, Shinichi Kawamoto, Yuichi Fujiki, Greg Friedman, Armando Fox\nOSDI ’04, San Francisco, CA, December 2004\nAn excellent paper pointing out how far one can go with reboot in building more robust systems.\n[I11] “Intel 64 and IA-32 Architectures Software Developer’s Manual”\nVolume 3A and 3B: System Programming Guide\nIntel Corporation, January 2011\n[K+61] “One-Level Storage System”\nT. Kilburn, D.B.G. Edwards, M.J. Lanigan, F.H. Sumner\nIRE Transactions on Electronic Computers, April 1962\nThe Atlas pioneered much of what you see in modern systems. However, this paper is not the best one\nto read. If you were to only read one, you might try the historical perspective below [L78].\n[L78] “The Manchester Mark I and Atlas: A Historical Perspective”\nS. H. Lavington\nCommunications of the ACM, 21:1, January 1978\nA history of the early development of computers and the pioneering efforts of Atlas.\n[M+63] “A Time-Sharing Debugging System for a Small Computer”\nJ. McCarthy, S. Boilen, E. Fredkin, J. C. R. Licklider\nAFIPS ’63 (Spring), May, 1963, New York, USA\nAn early paper about time-sharing that refers to using a timer interrupt; the quote that discusses it:\n“The basic task of the channel 17 clock routine is to decide whether to remove the current user from core\nand if so to decide which user program to swap in as he goes out.”\n[MS96] “lmbench: Portable tools for performance analysis”\nLarry McVoy and Carl Staelin\nUSENIX Annual Technical Conference, January 1996\nA fun paper about how to measure a number of different things about your OS and its performance.\nDownload lmbench and give it a try.\n[M11] “Mac OS 9”\nJanuary 2011\nAvailable: http://en.wikipedia.org/wiki/Mac OS 9\n[O90] “Why Aren’t Operating Systems Getting Faster as Fast as Hardware?”\nJ. Ousterhout\nUSENIX Summer Conference, June 1990\nA classic paper on the nature of operating system performance.\n[P10] “The Single UNIX Speciﬁcation, Version 3”\nThe Open Group, May 2010\nAvailable: http://www.unix.org/version3/\nThis is hard and painful to read, so probably avoid it if you can.\n[S07] “The Geometry of Innocent Flesh on the Bone:\nReturn-into-libc without Function Calls (on the x86)”\nHovav Shacham\nCCS ’07, October 2007\nOne of those awesome, mind-blowing ideas that you’ll see in research from time to time. The author\nshows that if you can jump into code arbitrarily, you can essentially stitch together any code sequence\nyou like (given a large code base) – read the paper for the details. The technique makes it even harder to\ndefend against malicious attacks, alas.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2972,
      "extraction_method": "Direct"
    },
    {
      "page_number": 94,
      "chapter": null,
      "content": "58\nMECHANISM: LIMITED DIRECT EXECUTION\nHomework (Measurement)\nASIDE: MEASUREMENT HOMEWORKS\nMeasurement homeworks are small exercises where you write code to\nrun on a real machine, in order to measure some aspect of OS or hardware\nperformance. The idea behind such homeworks is to give you a little bit\nof hands-on experience with a real operating system.\nIn this homework, you’ll measure the costs of a system call and context\nswitch. Measuring the cost of a system call is relatively easy. For example,\nyou could repeatedly call a simple system call (e.g., performing a 0-byte\nread), and time how long it takes; dividing the time by the number of\niterations gives you an estimate of the cost of a system call.\nOne thing you’ll have to take into account is the precision and accu-\nracy of your timer. A typical timer that you can use is gettimeofday();\nread the man page for details. What you’ll see there is that gettimeofday()\nreturns the time in microseconds since 1970; however, this does not mean\nthat the timer is precise to the microsecond. Measure back-to-back calls\nto gettimeofday() to learn something about how precise the timer re-\nally is; this will tell you how many iterations of your null system-call\ntest you’ll have to run in order to get a good measurement result. If\ngettimeofday() is not precise enough for you, you might look into\nusing the rdtsc instruction available on x86 machines.\nMeasuring the cost of a context switch is a little trickier. The lmbench\nbenchmark does so by running two processes on a single CPU, and set-\nting up two UNIX pipes between them; a pipe is just one of many ways\nprocesses in a UNIX system can communicate with one another. The ﬁrst\nprocess then issues a write to the ﬁrst pipe, and waits for a read on the\nsecond; upon seeing the ﬁrst process waiting for something to read from\nthe second pipe, the OS puts the ﬁrst process in the blocked state, and\nswitches to the other process, which reads from the ﬁrst pipe and then\nwrites to the second. When the second process tries to read from the ﬁrst\npipe again, it blocks, and thus the back-and-forth cycle of communication\ncontinues. By measuring the cost of communicating like this repeatedly,\nlmbench can make a good estimate of the cost of a context switch. You\ncan try to re-create something similar here, using pipes, or perhaps some\nother communication mechanism such as UNIX sockets.\nOne difﬁculty in measuring context-switch cost arises in systems with\nmore than one CPU; what you need to do on such a system is ensure that\nyour context-switching processes are located on the same processor. For-\ntunately, most operating systems have calls to bind a process to a partic-\nular processor; on Linux, for example, the sched setaffinity() call\nis what you’re looking for. By ensuring both processes are on the same\nprocessor, you are making sure to measure the cost of the OS stopping\none process and restoring another on the same CPU.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2990,
      "extraction_method": "Direct"
    },
    {
      "page_number": 95,
      "chapter": null,
      "content": "7\nScheduling: Introduction\nBy now low-level mechanisms of running processes (e.g., context switch-\ning) should be clear; if they are not, go back a chapter or two, and read the\ndescription of how that stuff works again. However, we have yet to un-\nderstand the high-level policies that an OS scheduler employs. We will\nnow do just that, presenting a series of scheduling policies (sometimes\ncalled disciplines) that various smart and hard-working people have de-\nveloped over the years.\nThe origins of scheduling, in fact, predate computer systems; early\napproaches were taken from the ﬁeld of operations management and ap-\nplied to computers. This reality should be no surprise: assembly lines\nand many other human endeavors also require scheduling, and many of\nthe same concerns exist therein, including a laser-like desire for efﬁciency.\nAnd thus, our problem:\nTHE CRUX: HOW TO DEVELOP SCHEDULING POLICY\nHow should we develop a basic framework for thinking about\nscheduling policies? What are the key assumptions? What metrics are\nimportant? What basic approaches have been used in the earliest of com-\nputer systems?\n7.1\nWorkload Assumptions\nBefore getting into the range of possible policies, let us ﬁrst make a\nnumber of simplifying assumptions about the processes running in the\nsystem, sometimes collectively called the workload. Determining the\nworkload is a critical part of building policies, and the more you know\nabout workload, the more ﬁne-tuned your policy can be.\nThe workload assumptions we make here are clearly unrealistic, but\nthat is alright (for now), because we will relax them as we go, and even-\ntually develop what we will refer to as ... (dramatic pause) ...\n59\n",
      "content_length": 1690,
      "extraction_method": "Direct"
    },
    {
      "page_number": 96,
      "chapter": null,
      "content": "60\nSCHEDULING: INTRODUCTION\na fully-operational scheduling discipline1.\nWe will make the following assumptions about the processes, some-\ntimes called jobs, that are running in the system:\n1. Each job runs for the same amount of time.\n2. All jobs arrive at the same time.\n3. All jobs only use the CPU (i.e., they perform no I/O)\n4. The run-time of each job is known.\nWe said all of these assumptions were unrealistic, but just as some an-\nimals are more equal than others in Orwell’s Animal Farm [O45], some\nassumptions are more unrealistic than others in this chapter. In particu-\nlar, it might bother you that the run-time of each job is known: this would\nmake the scheduler omniscient, which, although it would be great (prob-\nably), is not likely to happen anytime soon.\n7.2\nScheduling Metrics\nBeyond making workload assumptions, we also need one more thing\nto enable us to compare different scheduling policies: a scheduling met-\nric. A metric is just something that we use to measure something, and\nthere are a number of different metrics that make sense in scheduling.\nFor now, however, let us also simplify our life by simply having a sin-\ngle metric: turnaround time. The turnaround time of a job is deﬁned\nas the time at which the job completes minus the time at which the job\narrived in the system. More formally, the turnaround time Tturnaround is:\nTturnaround = Tcompletion −Tarrival\n(7.1)\nBecause we have assumed that all jobs arrive at the same time, for now\nTarrival = 0 and hence Tturnaround = Tcompletion. This fact will change\nas we relax the aforementioned assumptions.\nYou should note that turnaround time is a performance metric, which\nwill be our primary focus this chapter. Another metric of interest is fair-\nness, as measured (for example) by Jain’s Fairness Index [J91]. Perfor-\nmance and fairness are often at odds in scheduling; a scheduler, for ex-\nample, may optimize performance but at the cost of preventing a few jobs\nfrom running, thus decreasing fairness. This conundrum shows us that\nlife isn’t always perfect.\n7.3\nFirst In, First Out (FIFO)\nThe most basic algorithm a scheduler can implement is known as First\nIn, First Out (FIFO) scheduling or sometimes First Come, First Served\n1Said in the same way you would say “A fully-operational Death Star.”\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2335,
      "extraction_method": "Direct"
    },
    {
      "page_number": 97,
      "chapter": null,
      "content": "SCHEDULING: INTRODUCTION\n61\n(FCFS). FIFO has a number of positive properties: it is clearly very simple\nand thus easy to implement. Given our assumptions, it works pretty well.\nLet’s do a quick example together. Imagine three jobs arrive in the\nsystem, A, B, and C, at roughly the same time (Tarrival = 0). Because\nFIFO has to put some job ﬁrst, let’s assume that while they all arrived\nsimultaneously, A arrived just a hair before B which arrived just a hair\nbefore C. Assume also that each job runs for 10 seconds. What will the\naverage turnaround time be for these jobs?\n0\n20\n40\n60\n80\n100\n120\nTime\nA\nB\nC\nFigure 7.1: FIFO Simple Example\nFrom Figure 7.1, you can see that A ﬁnished at 10, B at 20, and C at 30.\nThus, the average turnaround time for the three jobs is simply 10+20+30\n3\n=\n20. Computing turnaround time is as easy as that.\nNow let’s relax one of our assumptions. In particular, let’s relax as-\nsumption 1, and thus no longer assume that each job runs for the same\namount of time. How does FIFO perform now? What kind of workload\ncould you construct to make FIFO perform poorly?\n(think about this before reading on ... keep thinking ... got it?!)\nPresumably you’ve ﬁgured this out by now, but just in case, let’s do\nan example to show how jobs of different lengths can lead to trouble for\nFIFO scheduling. In particular, let’s again assume three jobs (A, B, and\nC), but this time A runs for 100 seconds while B and C run for 10 each.\n0\n20\n40\n60\n80\n100\n120\nTime\nA\nB\nC\nFigure 7.2: Why FIFO Is Not That Great\nAs you can see in Figure 7.2, Job A runs ﬁrst for the full 100 seconds\nbefore B or C even get a chance to run. Thus, the average turnaround\ntime for the system is high: a painful 110 seconds ( 100+110+120\n3\n= 110).\nThis problem is generally referredto as the convoy effect [B+79], where\na number of relatively-short potential consumers of a resource get queued\nbehind a heavyweight resource consumer. This scheduling scenario might\nremind you of a single line at a grocery store and what you feel like when\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2068,
      "extraction_method": "Direct"
    },
    {
      "page_number": 98,
      "chapter": null,
      "content": "62\nSCHEDULING: INTRODUCTION\nTIP: THE PRINCIPLE OF SJF\nShortest Job First represents a general scheduling principle that can be\napplied to any system where the perceived turnaround time per customer\n(or, in our case, a job) matters. Think of any line you have waited in: if\nthe establishment in question cares about customer satisfaction, it is likely\nthey have taken SJF into account. For example, grocery stores commonly\nhave a “ten-items-or-less” line to ensure that shoppers with only a few\nthings to purchase don’t get stuck behind the family preparing for some\nupcoming nuclear winter.\nyou see the person in front of you with three carts full of provisions and\na checkbook out; it’s going to be a while2.\nSo what should we do? How can we develop a better algorithm to\ndeal with our new reality of jobs that run for different amounts of time?\nThink about it ﬁrst; then read on.\n7.4\nShortest Job First (SJF)\nIt turns out that a very simple approach solves this problem; in fact\nit is an idea stolen from operations research [C54,PV56] and applied to\nscheduling of jobs in computer systems. This new scheduling discipline\nis known as Shortest Job First (SJF), and the name should be easy to\nremember because it describes the policy quite completely: it runs the\nshortest job ﬁrst, then the next shortest, and so on.\n0\n20\n40\n60\n80\n100\n120\nTime\nB\nC\nA\nFigure 7.3: SJF Simple Example\nLet’s take our example above but with SJF as our scheduling policy.\nFigure 7.3 shows the results of running A, B, and C. Hopefully the dia-\ngram makes it clear why SJF performs much better with regards to aver-\nage turnaround time. Simply by running B and C before A, SJF reduces\naverage turnaround from 110 seconds to 50 ( 10+20+120\n3\n= 50), more than\na factor of two improvement.\nIn fact, given our assumptions about jobs all arriving at the same time,\nwe could prove that SJF is indeed an optimal scheduling algorithm. How-\n2Recommended action in this case: either quickly switch to a different line, or take a long,\ndeep, and relaxing breath. That’s right, breathe in, breathe out. It will be OK, don’t worry.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2143,
      "extraction_method": "Direct"
    },
    {
      "page_number": 99,
      "chapter": null,
      "content": "SCHEDULING: INTRODUCTION\n63\nASIDE: PREEMPTIVE SCHEDULERS\nIn the old days of batch computing, a number of non-preemptive sched-\nulers were developed; such systems would run each job to completion\nbefore considering whether to run a new job. Virtually all modern sched-\nulers are preemptive, and quite willing to stop one process from run-\nning in order to run another. This implies that the scheduler employs the\nmechanisms we learned about previously; in particular, the scheduler can\nperform a context switch, stopping one running process temporarily and\nresuming (or starting) another.\never, you are in a systems class, not theory or operations research; no\nproofs are allowed.\nThus we arrive upon a good approach to scheduling with SJF, but our\nassumptions are still fairly unrealistic. Let’s relax another. In particular,\nwe can target assumption 2, and now assume that jobs can arrive at any\ntime instead of all at once. What problems does this lead to?\n(Another pause to think ... are you thinking? Come on, you can do it)\nHere we can illustrate the problem again with an example. This time,\nassume A arrives at t = 0 and needs to run for 100 seconds, whereas B\nand C arrive at t = 10 and each need to run for 10 seconds. With pure\nSJF, we’d get the schedule seen in Figure 7.4.\n0\n20\n40\n60\n80\n100\n120\nTime\nA\nB\nC\n[B,C arrive]\nFigure 7.4: SJF With Late Arrivals From B and C\nAs you can see from the ﬁgure, even though B and C arrived shortly\nafter A, they still are forced to wait until A has completed, and thus suffer\nthe same convoy problem. Average turnaround time for these three jobs\nis 103.33 seconds ( 100+(110−10)+(120−10)\n3\n). What can a scheduler do?\n7.5\nShortest Time-to-Completion First (STCF)\nAs you might have guessed, given our previous discussion about mech-\nanisms such as timer interrupts and context switching, the scheduler can\ncertainly do something else when B and C arrive: it can preempt job A\nand decide to run another job, perhaps continuing A later. SJF by our deﬁ-\nnition is a non-preemptive scheduler, and thus suffers from the problems\ndescribed above.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2129,
      "extraction_method": "Direct"
    },
    {
      "page_number": 100,
      "chapter": null,
      "content": "64\nSCHEDULING: INTRODUCTION\n0\n20\n40\n60\n80\n100\n120\nTime\nA\nB\nC\nA\n[B,C arrive]\nFigure 7.5: STCF Simple Example\nFortunately, there is a scheduler which does exactly that: add preemp-\ntion to SJF, known as the Shortest Time-to-Completion First (STCF) or\nPreemptive Shortest Job First (PSJF) scheduler [CK68]. Any time a new\njob enters the system, it determines of the remaining jobs and new job,\nwhich has the least time left, and then schedules that one. Thus, in our\nexample, STCF would preempt A and run B and C to completion; only\nwhen they are ﬁnished would A’s remaining time be scheduled. Figure\n7.5 shows an example.\nThe result is a much-improved average turnaround time: 50 seconds\n( (120−0)+(20−10)+(30−10)\n3\n). And as before, given our new assumptions,\nSTCF is provably optimal; given that SJF is optimal if all jobs arrive at\nthe same time, you should probably be able to see the intuition behind\nthe optimality of STCF.\nThus, if we knew that job lengths, and jobs only used the CPU, and our\nonly metric was turnaround time, STCF would be a great policy. In fact,\nfor a number of early batch computing systems, these types of scheduling\nalgorithms made some sense. However, the introduction of time-shared\nmachines changed all that. Now users would sit at a terminal and de-\nmand interactive performance from the system as well. And thus, a new\nmetric was born: response time.\nResponse time is deﬁned as the time from when the job arrives in a\nsystem to the ﬁrst time it is scheduled. More formally:\nTresponse = Tfirstrun −Tarrival\n(7.2)\nFor example, if we had the schedule above (with A arriving at time 0,\nand B and C at time 10), the response time of each job is as follows: 0 for\njob A, 0 for B, and 10 for C (average: 3.33).\nAs you might be thinking, STCF and related disciplines are not par-\nticularly good for response time. If three jobs arrive at the same time,\nfor example, the third job has to wait for the previous two jobs to run in\ntheir entirety before being scheduled just once. While great for turnaround\ntime, this approach is quite bad for response time and interactivity. In-\ndeed, imagine sitting at a terminal, typing, and having to wait 10 seconds\nto see a response from the system just because some other job got sched-\nuled in front of yours: not too pleasant.\nThus, we are left with another problem: how can we build a scheduler\nthat is sensitive to response time?\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2445,
      "extraction_method": "Direct"
    },
    {
      "page_number": 101,
      "chapter": null,
      "content": "SCHEDULING: INTRODUCTION\n65\n0\n5\n10\n15\n20\n25\n30\nTime\nA\nB\nC\nFigure 7.6: SJF Again (Bad for Response Time)\n0\n5\n10\n15\n20\n25\n30\nTime\nABCABCABCABCABC\nFigure 7.7: Round Robin (Good for Response Time)\n7.6\nRound Robin\nTo solve this problem, we will introduce a new scheduling algorithm.\nThis approach is classically known as Round-Robin (RR) scheduling [K64].\nThe basic idea is simple: instead of running jobs to completion, RR runs\na job for a time slice (sometimes called a scheduling quantum) and then\nswitches to the next job in the run queue.\nIt repeatedly does so un-\ntil the jobs are ﬁnished. For this reason, RR is sometimes called time-\nslicing. Note that the length of a time slice must be a multiple of the\ntimer-interrupt period; thus if the timer interrupts every 10 milliseconds,\nthe time slice could be 10, 20, or any other multiple of 10 ms.\nTo understand RR in more detail, let’s look at an example. Assume\nthree jobs A, B, and C arrive at the same time in the system, and that\nthey each wish to run for 5 seconds. An SJF scheduler runs each job to\ncompletion before running another (Figure 7.6). In contrast, RR with a\ntime-slice of 1 second would cycle through the jobs quickly (Figure 7.7).\nThe average response time of RR is:\n0+1+2\n3\n= 1; for SJF, average re-\nsponse time is: 0+5+10\n3\n= 5.\nAs you can see, the length of the time slice is critical for RR. The shorter\nit is, the better the performance of RR under the response-time metric.\nHowever, making the time slice too short is problematic: suddenly the\ncost of context switching will dominate overall performance. Thus, de-\nciding on the length of the time slice presents a trade-off to a system de-\nsigner, making it long enough to amortize the cost of switching without\nmaking it so long that the system is no longer responsive.\nNote that the cost of context switching does not arise solely from the\nOS actions of saving and restoring a few registers. When programs run,\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 1982,
      "extraction_method": "Direct"
    },
    {
      "page_number": 102,
      "chapter": null,
      "content": "66\nSCHEDULING: INTRODUCTION\nTIP: AMORTIZATION CAN REDUCE COSTS\nThe general technique of amortization is commonly used in systems\nwhen there is a ﬁxed cost to some operation. By incurring that cost less\noften (i.e., by performing the operation fewer times), the total cost to the\nsystem is reduced. For example, if the time slice is set to 10 ms, and the\ncontext-switch cost is 1 ms, roughly 10% of time is spent context switch-\ning and is thus wasted. If we want to amortize this cost, we can increase\nthe time slice, e.g., to 100 ms. In this case, less than 1% of time is spent\ncontext switching, and thus the cost of time-slicing has been amortized.\nthey build up a great deal of state in CPU caches, TLBs, branch predictors,\nand other on-chip hardware. Switching to another job causes this state\nto be ﬂushed and new state relevant to the currently-running job to be\nbrought in, which may exact a noticeable performance cost [MB91].\nRR, with a reasonable time slice, is thus an excellent scheduler if re-\nsponse time is our only metric. But what about our old friend turnaround\ntime? Let’s look at our example above again. A, B, and C, each with run-\nning times of 5 seconds, arrive at the same time, and RR is the scheduler\nwith a (long) 1-second time slice. We can see from the picture above that\nA ﬁnishes at 13, B at 14, and C at 15, for an average of 14. Pretty awful!\nIt is not surprising, then, that RR is indeed one of the worst policies if\nturnaround time is our metric. Intuitively, this should make sense: what\nRR is doing is stretching out each job as long as it can, by only running\neach job for a short bit before moving to the next. Because turnaround\ntime only cares about when jobs ﬁnish, RR is nearly pessimal, even worse\nthan simple FIFO in many cases.\nMore generally, any policy (such as RR) that is fair, i.e., that evenly di-\nvides the CPU among active processes on a small time scale, will perform\npoorly on metrics such as turnaround time. Indeed, this is an inherent\ntrade-off: if you are willing to be unfair, you can run shorter jobs to com-\npletion, but at the cost of response time; if you instead value fairness,\nresponse time is lowered, but at the cost of turnaround time. This type of\ntrade-off is common in systems; you can’t have your cake and eat it too.\nWe have developed two types of schedulers. The ﬁrst type (SJF, STCF)\noptimizes turnaround time, but is bad for response time. The second type\n(RR) optimizes response time but is bad for turnaround. And we still\nhave two assumptions which need to be relaxed: assumption 3 (that jobs\ndo no I/O), and assumption 4 (that the run-time of each job is known).\nLet’s tackle those assumptions next.\n7.7\nIncorporating I/O\nFirst we will relax assumption 3 – of course all programs perform I/O.\nImagine a program that didn’t take any input: it would produce the same\noutput each time. Imagine one without output: it is the proverbial tree\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2968,
      "extraction_method": "Direct"
    },
    {
      "page_number": 103,
      "chapter": null,
      "content": "SCHEDULING: INTRODUCTION\n67\nfalling in the forest, with no one to see it; it doesn’t matter that it ran.\nA scheduler clearly has a decision to make when a job initiates an I/O\nrequest, because the currently-running job won’t be using the CPU dur-\ning the I/O; it is blocked waiting for I/O completion. If the I/O is sent to\na hard disk drive, the process might be blocked for a few milliseconds or\nlonger, depending on the current I/O load of the drive. Thus, the sched-\nuler should probably schedule another job on the CPU at that time.\nThe scheduler also has to make a decision when the I/O completes.\nWhen that occurs, an interrupt is raised, and the OS runs and moves\nthe process that issued the I/O from blocked back to the ready state. Of\ncourse, it could even decide to run the job at that point. How should the\nOS treat each job?\nTo understand this issue better, let us assume we have two jobs, A and\nB, which each need 50 ms of CPU time. However, there is one obvious\ndifference: A runs for 10 ms and then issues an I/O request (assume here\nthat I/Os each take 10 ms), whereas B simply uses the CPU for 50 ms and\nperforms no I/O. The scheduler runs A ﬁrst, then B after (Figure 7.8).\n0\n20\n40\n60\n80\n100\n120\n140\nTime\nA\nA\nA\nA\nA B B B B B\nCPU\nDisk\nFigure 7.8: Poor Use of Resources\nAssume we are trying to build a STCF scheduler. How should such a\nscheduler account for the fact that A is broken up into 5 10-ms sub-jobs,\nwhereas B is just a single 50-ms CPU demand? Clearly, just running one\njob and then the other without considering how to take I/O into account\nmakes little sense.\n0\n20\n40\n60\n80\n100\n120\n140\nTime\nA\nA\nA\nA\nA\nB\nB\nB\nB\nB\nCPU\nDisk\nFigure 7.9: Overlap Allows Better Use of Resources\nA common approach is to treat each 10-ms sub-job of A as an indepen-\ndent job. Thus, when the system starts, its choice is whether to schedule\na 10-ms A or a 50-ms B. With STCF, the choice is clear: choose the shorter\none, in this case A. Then, when the ﬁrst sub-job of A has completed, only\nB is left, and it begins running. Then a new sub-job of A is submitted,\nand it preempts B and runs for 10 ms. Doing so allows for overlap, with\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2178,
      "extraction_method": "Direct"
    },
    {
      "page_number": 104,
      "chapter": null,
      "content": "68\nSCHEDULING: INTRODUCTION\nTIP: OVERLAP ENABLES HIGHER UTILIZATION\nWhen possible, overlap operations to maximize the utilization of sys-\ntems. Overlap is useful in many different domains, including when per-\nforming disk I/O or sending messages to remote machines; in either case,\nstarting the operation and then switching to other work is a good idea,\nand improved the overall utilization and efﬁciency of the system.\nthe CPU being used by one process while waiting for the I/O of another\nprocess to complete; the system is thus better utilized (see Figure 7.9).\nAnd thus we see how a scheduler might incorporate I/O. By treating\neach CPU burst as a job, the scheduler makes sure processes that are “in-\nteractive” get run frequently. While those interactive jobs are performing\nI/O, other CPU-intensive jobs run, thus better utilizing the processor.\n7.8\nNo More Oracle\nWith a basic approach to I/O in place, we come to our ﬁnal assump-\ntion: that the scheduler knows the length of each job. As we said before,\nthis is likely the worst assumption we could make. In fact, in a general-\npurpose OS (like the ones we care about), the OS usually knows very little\nabout the length of each job. Thus, how can we build an approach that be-\nhaves like SJF/STCF without such a priori knowledge? Further, how can\nwe incorporate some of the ideas we have seen with the RR scheduler so\nthat response time is also quite good?\n7.9\nSummary\nWe have introduced the basic ideas behind scheduling and developed\ntwo families of approaches. The ﬁrst runs the shortest job remaining and\nthus optimizes turnaround time; the second alternates between all jobs\nand thus optimizes response time. Both are bad where the other is good,\nalas, an inherent trade-off common in systems. We have also seen how we\nmight incorporate I/O into the picture, but have still not solved the prob-\nlem of the fundamental inability of the OS to see into the future. Shortly,\nwe will see how to overcome this problem, by building a scheduler that\nuses the recent past to predict the future. This scheduler is known as the\nmulti-level feedback queue, and it is the topic of the next chapter.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2197,
      "extraction_method": "Direct"
    },
    {
      "page_number": 105,
      "chapter": null,
      "content": "SCHEDULING: INTRODUCTION\n69\nReferences\n[B+79] “The Convoy Phenomenon”\nM. Blasgen, J. Gray, M. Mitoma, T. Price\nACM Operating Systems Review, 13:2, April 1979\nPerhaps the ﬁrst reference to convoys, which occurs in databases as well as the OS.\n[C54] “Priority Assignment in Waiting Line Problems”\nA. Cobham\nJournal of Operations Research, 2:70, pages 70–76, 1954\nThe pioneering paper on using an SJF approach in scheduling the repair of machines.\n[K64] “Analysis of a Time-Shared Processor”\nLeonard Kleinrock\nNaval Research Logistics Quarterly, 11:1, pages 59–73, March 1964\nMay be the ﬁrst reference to the round-robin scheduling algorithm; certainly one of the ﬁrst analyses of\nsaid approach to scheduling a time-shared system.\n[CK68] “Computer Scheduling Methods and their Countermeasures”\nEdward G. Coffman and Leonard Kleinrock\nAFIPS ’68 (Spring), April 1968\nAn excellent early introduction to and analysis of a number of basic scheduling disciplines.\n[J91] “The Art of Computer Systems Performance Analysis:\nTechniques for Experimental Design, Measurement, Simulation, and Modeling”\nR. Jain\nInterscience, New York, April 1991\nThe standard text on computer systems measurement. A great reference for your library, for sure.\n[O45] “Animal Farm”\nGeorge Orwell\nSecker and Warburg (London), 1945\nA great but depressing allegorical book about power and its corruptions. Some say it is a critique of\nStalin and the pre-WWII Stalin era in the U.S.S.R; we say it’s a critique of pigs.\n[PV56] “Machine Repair as a Priority Waiting-Line Problem”\nThomas E. Phipps Jr. and W. R. Van Voorhis\nOperations Research, 4:1, pages 76–86, February 1956\nFollow-on work that generalizes the SJF approach to machine repair from Cobham’s original work; also\npostulates the utility of an STCF approach in such an environment. Speciﬁcally, “There are certain\ntypes of repair work, ... involving much dismantling and covering the ﬂoor with nuts and bolts, which\ncertainly should not be interrupted once undertaken; in other cases it would be inadvisable to continue\nwork on a long job if one or more short ones became available (p.81).”\n[MB91] “The effect of context switches on cache performance”\nJeffrey C. Mogul and Anita Borg\nASPLOS, 1991\nA nice study on how cache performance can be affected by context switching; less of an issue in today’s\nsystems where processors issue billions of instructions per second but context-switches still happen in\nthe millisecond time range.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2495,
      "extraction_method": "Direct"
    },
    {
      "page_number": 106,
      "chapter": null,
      "content": "70\nSCHEDULING: INTRODUCTION\nHomework\nASIDE: SIMULATION HOMEWORKS\nSimulation homeworks come in the form of simulators you run to\nmake sure you understand some piece of the material. The simulators\nare generally python programs that enable you both to generate different\nproblems (using different random seeds) as well as to have the program\nsolve the problem for you (with the -c ﬂag) so that you can check your\nanswers. Running any simulator with a -h or --help ﬂag will provide\nwith more information as to all the options the simulator gives you.\nThis program, scheduler.py, allows you to see how different sched-\nulers perform under scheduling metrics such as response time, turnaround\ntime, and total wait time. See the README for details.\nQuestions\n1. Compute the response time and turnaround time when running\nthree jobs of length 200 with the SJF and FIFO schedulers.\n2. Now do the same but with jobs of different lengths: 100, 200, and\n300.\n3. Now do the same, but also with the RR scheduler and a time-slice\nof 1.\n4. For what types of workloads does SJF deliver the same turnaround\ntimes as FIFO?\n5. For what types of workloads and quantum lengths does SJF deliver\nthe same response times as RR?\n6. What happens to response time with SJF as job lengths increase?\nCan you use the simulator to demonstrate the trend?\n7. What happens to response time with RR as quantum lengths in-\ncrease? Can you write an equation that gives the worst-case re-\nsponse time, given N jobs?\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1525,
      "extraction_method": "Direct"
    },
    {
      "page_number": 107,
      "chapter": null,
      "content": "8\nScheduling:\nThe Multi-Level Feedback Queue\nIn this chapter, we’ll tackle the problem of developing one of the most\nwell-known approaches to scheduling, known as the Multi-level Feed-\nback Queue (MLFQ). The Multi-level Feedback Queue (MLFQ) sched-\nuler was ﬁrst described by Corbato et al. in 1962 [C+62] in a system\nknown as the Compatible Time-Sharing System (CTSS), and this work,\nalong with later work on Multics, led the ACM to award Corbato its\nhighest honor, the Turing Award. The scheduler has subsequently been\nreﬁned throughout the years to the implementations you will encounter\nin some modern systems.\nThe fundamental problem MLFQ tries to address is two-fold. First, it\nwould like to optimize turnaround time, which, as we saw in the previous\nnote, is done by running shorter jobs ﬁrst; unfortunately, the OS doesn’t\ngenerally know how long a job will run for, exactly the knowledge that\nalgorithms like SJF (or STCF) require. Second, MLFQ would like to make\na system feel responsive to interactive users (i.e., users sitting and staring\nat the screen, waiting for a process to ﬁnish), and thus minimize response\ntime; unfortunately, algorithms like Round Robin reduce response time\nbut are terrible for turnaround time. Thus, our problem: given that we\nin general do not know anything about a process, how can we build a\nscheduler to achieve these goals? How can the scheduler learn, as the\nsystem runs, the characteristics of the jobs it is running, and thus make\nbetter scheduling decisions?\nTHE CRUX:\nHOW TO SCHEDULE WITHOUT PERFECT KNOWLEDGE?\nHow can we design a scheduler that both minimizes response time for\ninteractive jobs while also minimizing turnaround time without a priori\nknowledge of job length?\n71\n",
      "content_length": 1730,
      "extraction_method": "Direct"
    },
    {
      "page_number": 108,
      "chapter": null,
      "content": "72\nSCHEDULING:\nTHE MULTI-LEVEL FEEDBACK QUEUE\nTIP: LEARN FROM HISTORY\nThe multi-level feedback queue is an excellent example of a system that\nlearns from the past to predict the future. Such approaches are com-\nmon in operating systems (and many other places in Computer Science,\nincluding hardware branch predictors and caching algorithms).\nSuch\napproaches work when jobs have phases of behavior and are thus pre-\ndictable; of course, one must be careful with such techniques, as they can\neasily be wrong and drive a system to make worse decisions than they\nwould have with no knowledge at all.\n8.1\nMLFQ: Basic Rules\nTo build such a scheduler, in this chapter we will describe the basic\nalgorithms behind a multi-level feedback queue; although the speciﬁcs of\nmany implemented MLFQs differ [E95], most approaches are similar.\nIn our treatment, the MLFQ has a number of distinct queues, each\nassigned a different priority level. At any given time, a job that is ready\nto run is on a single queue. MLFQ uses priorities to decide which job\nshould run at a given time: a job with higher priority (i.e., a job on a\nhigher queue) is chosen to run.\nOf course, more than one job may be on a given queue, and thus have\nthe same priority. In this case, we will just use round-robin scheduling\namong those jobs.\nThus, the key to MLFQ scheduling lies in how the scheduler sets pri-\norities. Rather than giving a ﬁxed priority to each job, MLFQ varies the\npriority of a job based on its observed behavior. If, for example, a job repeat-\nedly relinquishes the CPU while waiting for input from the keyboard,\nMLFQ will keep its priority high, as this is how an interactive process\nmight behave. If, instead, a job uses the CPU intensively for long periods\nof time, MLFQ will reduce its priority. In this way, MLFQ will try to learn\nabout processes as they run, and thus use the history of the job to predict\nits future behavior.\nThus, we arrive at the ﬁrst two basic rules for MLFQ:\n• Rule 1: If Priority(A) > Priority(B), A runs (B doesn’t).\n• Rule 2: If Priority(A) = Priority(B), A & B run in RR.\nIf we were to put forth a picture of what the queues might look like at\na given instant, we might see something like the following (Figure 8.1).\nIn the ﬁgure, two jobs (A and B) are at the highest priority level, while job\nC is in the middle and Job D is at the lowest priority. Given our current\nknowledge of how MLFQ works, the scheduler would just alternate time\nslices between A and B because they are the highest priority jobs in the\nsystem; poor jobs C and D would never even get to run – an outrage!\nOf course, just showing a static snapshot of some queues does not re-\nally give you an idea of how MLFQ works. What we need is to under-\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2776,
      "extraction_method": "Direct"
    },
    {
      "page_number": 109,
      "chapter": null,
      "content": "SCHEDULING:\nTHE MULTI-LEVEL FEEDBACK QUEUE\n73\nQ1\nQ2\nQ3\nQ4\nQ5\nQ6\nQ7\nQ8\n[Low Priority]\n[High Priority]\nD\nC\nA\nB\nFigure 8.1: MLFQ Example\nstand how job priority changes over time. And that, in a surprise only\nto those who are reading a chapter from this book for the ﬁrst time, is\nexactly what we will do next.\n8.2\nAttempt #1: How to Change Priority\nWe now must decide how MLFQ is going to change the priority level\nof a job (and thus which queue it is on) over the lifetime of a job. To do\nthis, we must keep in mind our workload: a mix of interactive jobs that\nare short-running (and may frequently relinquish the CPU), and some\nlonger-running “CPU-bound” jobs that need a lot of CPU time but where\nresponse time isn’t important.\nHere is our ﬁrst attempt at a priority-\nadjustment algorithm:\n• Rule 3: When a job enters the system, it is placed at the highest\npriority (the topmost queue).\n• Rule 4a: If a job uses up an entire time slice while running, its pri-\nority is reduced (i.e., it moves down one queue).\n• Rule 4b: If a job gives up the CPU before the time slice is up, it stays\nat the same priority level.\nExample 1: A Single Long-Running Job\nLet’s look at some examples. First, we’ll look at what happens when there\nhas been a long running job in the system. Figure 8.2 shows what happens\nto this job over time in a three-queue scheduler.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 1389,
      "extraction_method": "Direct"
    },
    {
      "page_number": 110,
      "chapter": null,
      "content": "74\nSCHEDULING:\nTHE MULTI-LEVEL FEEDBACK QUEUE\nQ2\nQ1\nQ0\n0\n50\n100\n150\n200\nFigure 8.2: Long-running Job Over Time\nAs you can see in the example, the job enters at the highest priority\n(Q2). After a single time-slice of 10 ms, the scheduler reduces the job’s\npriority by one, and thus the job is on Q1. After running at Q1 for a time\nslice, the job is ﬁnally lowered to the lowest priority in the system (Q0),\nwhere it remains. Pretty simple, no?\nExample 2: Along Came A Short Job\nNow let’s look at a more complicated example, and hopefully see how\nMLFQ tries to approximate SJF. In this example, there are two jobs: A,\nwhich is a long-running CPU-intensive job, and B, which is a short-running\ninteractive job. Assume A has been running for some time, and then B ar-\nrives. What will happen? Will MLFQ approximate SJF for B?\nFigure 8.3 plots the results of this scenario. A (shown in black) is run-\nning along in the lowest-priority queue (as would any long-running CPU-\nintensive jobs); B (shown in gray) arrives at time T = 100, and thus is\nQ2\nQ1\nQ0\n0\n50\n100\n150\n200\nFigure 8.3: Along Came An Interactive Job\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1155,
      "extraction_method": "Direct"
    },
    {
      "page_number": 111,
      "chapter": null,
      "content": "SCHEDULING:\nTHE MULTI-LEVEL FEEDBACK QUEUE\n75\nQ2\nQ1\nQ0\n0\n50\n100\n150\n200\nFigure 8.4: A Mixed I/O-intensive and CPU-intensive Workload\ninserted into the highest queue; as its run-time is short (only 20 ms), B\ncompletes before reaching the bottom queue, in two time slices; then A\nresumes running (at low priority).\nFrom this example, you can hopefully understand one of the major\ngoals of the algorithm: because it doesn’t know whether a job will be a\nshort job or a long-running job, it ﬁrst assumes it might be a short job, thus\ngiving the job high priority. If it actually is a short job, it will run quickly\nand complete; if it is not a short job, it will slowly move down the queues,\nand thus soon prove itself to be a long-running more batch-like process.\nIn this manner, MLFQ approximates SJF.\nExample 3: What About I/O?\nLet’s now look at an example with some I/O. As Rule 4b states above, if a\nprocess gives up the processor before using up its time slice, we keep it at\nthe same priority level. The intent of this rule is simple: if an interactive\njob, for example, is doing a lot of I/O (say by waiting for user input from\nthe keyboard or mouse), it will relinquish the CPU before its time slice is\ncomplete; in such case, we don’t wish to penalize the job and thus simply\nkeep it at the same level.\nFigure 8.4 shows an example of how this works, with an interactive job\nB (shown in gray) that needs the CPU only for 1 ms before performing an\nI/O competing for the CPU with a long-running batch job A (shown in\nblack). The MLFQ approach keeps B at the highest priority because B\nkeeps releasing the CPU; if B is an interactive job, MLFQ further achieves\nits goal of running interactive jobs quickly.\nProblems With Our Current MLFQ\nWe thus have a basic MLFQ. It seems to do a fairly good job, sharing the\nCPU fairly between long-running jobs, and letting short or I/O-intensive\ninteractive jobs run quickly. Unfortunately, the approach we have devel-\noped thus far contains serious ﬂaws. Can you think of any?\n(This is where you pause and think as deviously as you can)\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2118,
      "extraction_method": "Direct"
    },
    {
      "page_number": 112,
      "chapter": null,
      "content": "76\nSCHEDULING:\nTHE MULTI-LEVEL FEEDBACK QUEUE\nQ2\nQ1\nQ0\n0\n50\n100\n150\n200\nQ2\nQ1\nQ0\n0\n50\n100\n150\n200\nFigure 8.5: Without (Left) and With (Right) Priority Boost\nFirst, there is the problem of starvation: if there are “too many” in-\nteractive jobs in the system, they will combine to consume all CPU time,\nand thus long-running jobs will never receive any CPU time (they starve).\nWe’d like to make some progress on these jobs even in this scenario.\nSecond, a smart user could rewrite their program to game the sched-\nuler. Gaming the scheduler generally refers to the idea of doing some-\nthing sneaky to trick the scheduler into giving you more than your fair\nshare of the resource. The algorithm we have described is susceptible to\nthe following attack: before the time slice is over, issue an I/O operation\n(to some ﬁle you don’t care about) and thus relinquish the CPU; doing so\nallows you to remain in the same queue, and thus gain a higher percent-\nage of CPU time. When done right (e.g., by running for 99% of a time slice\nbefore relinquishing the CPU), a job could nearly monopolize the CPU.\nFinally, a program may change its behavior over time; what was CPU-\nbound may transition to a phase of interactivity. With our current ap-\nproach, such a job would be out of luck and not be treated like the other\ninteractive jobs in the system.\n8.3\nAttempt #2: The Priority Boost\nLet’s try to change the rules and see if we can avoid the problem of\nstarvation. What could we do in order to guarantee that CPU-bound jobs\nwill make some progress (even if it is not much?).\nThe simple idea here is to periodically boost the priority of all the jobs\nin system. There are many ways to achieve this, but let’s just do some-\nthing simple: throw them all in the topmost queue; hence, a new rule:\n• Rule 5: After some time period S, move all the jobs in the system\nto the topmost queue.\nOur new rule solves two problems at once. First, processes are guar-\nanteed not to starve: by sitting in the top queue, a job will share the CPU\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2064,
      "extraction_method": "Direct"
    },
    {
      "page_number": 113,
      "chapter": null,
      "content": "SCHEDULING:\nTHE MULTI-LEVEL FEEDBACK QUEUE\n77\nQ2\nQ1\nQ0\n0\n50\n100\n150\n200\nQ2\nQ1\nQ0\n0\n50\n100\n150\n200\nFigure 8.6: Without (Left) and With (Right) Gaming Tolerance\nwith other high-priority jobs in a round-robin fashion, and thus eventu-\nally receive service. Second, if a CPU-bound job has become interactive,\nthe scheduler treats it properly once it has received the priority boost.\nLet’s see an example. In this scenario, we just show the behavior of\na long-running job when competing for the CPU with two short-running\ninteractive jobs. Two graphs are shown in Figure 8.5. On the left, there is\nno priority boost, and thus the long-running job gets starved once the two\nshort jobs arrive; on the right, there is a priority boost every 50 ms (which\nis likely too small of a value, but used here for the example), and thus\nwe at least guarantee that the long-running job will make some progress,\ngetting boosted to the highest priority every 50 ms and thus getting to\nrun periodically.\nOf course, the addition of the time period S leads to the obvious ques-\ntion: what should S be set to? John Ousterhout, a well-regarded systems\nresearcher [O11], used to call such values in systems voo-doo constants,\nbecause they seemed to require some form of black magic to set them cor-\nrectly. Unfortunately, S has that ﬂavor. If it is set too high, long-running\njobs could starve; too low, and interactive jobs may not get a proper share\nof the CPU.\n8.4\nAttempt #3: Better Accounting\nWe now have one more problem to solve: how to prevent gaming of\nour scheduler? The real culprit here, as you might have guessed, are\nRules 4a and 4b, which let a job retain its priority by relinquishing the\nCPU before the time slice expires. So what should we do?\nThe solution here is to perform better accounting of CPU time at each\nlevel of the MLFQ. Instead of forgetting how much of a time slice a pro-\ncess used at a given level, the scheduler should keep track; once a process\nhas used its allotment, it is demoted to the next priority queue. Whether\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2069,
      "extraction_method": "Direct"
    },
    {
      "page_number": 114,
      "chapter": null,
      "content": "78\nSCHEDULING:\nTHE MULTI-LEVEL FEEDBACK QUEUE\nQ2\nQ1\nQ0\n0\n50\n100\n150\n200\nFigure 8.7: Lower Priority, Longer Quanta\nit uses the time slice in one long burst or many small ones does not matter.\nWe thus rewrite Rules 4a and 4b to the following single rule:\n• Rule 4: Once a job uses up its time allotment at a given level (re-\ngardless of how many times it has given up the CPU), its priority is\nreduced (i.e., it moves down one queue).\nLet’s look at an example. Figure 8.6 shows what happens when a\nworkload tries to game the scheduler with the old Rules 4a and 4b (on\nthe left) as well the new anti-gaming Rule 4. Without any protection from\ngaming, a process can issue an I/O just before a time slice ends and thus\ndominate CPU time. With such protections in place, regardless of the\nI/O behavior of the process, it slowly moves down the queues, and thus\ncannot gain an unfair share of the CPU.\n8.5\nTuning MLFQ And Other Issues\nA few other issues arise with MLFQ scheduling. One big question is\nhow to parameterize such a scheduler. For example, how many queues\nshould there be? How big should the time slice be per queue? How often\nshould priority be boosted in order to avoid starvation and account for\nchanges in behavior? There are no easy answers to these questions, and\nthus only some experience with workloads and subsequent tuning of the\nscheduler will lead to a satisfactory balance.\nFor example, most MLFQ variants allow for varying time-slice length\nacross different queues. The high-priority queues are usually given short\ntime slices; they are comprised of interactive jobs, after all, and thus\nquickly alternating between them makes sense (e.g., 10 or fewer millisec-\nonds). The low-priority queues, in contrast, contain long-running jobs\nthat are CPU-bound; hence, longer time slices work well (e.g., 100s of\nms). Figure 8.7 shows an example in which two long-running jobs run\nfor 10 ms at the highest queue, 20 in the middle, and 40 at the lowest.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2010,
      "extraction_method": "Direct"
    },
    {
      "page_number": 115,
      "chapter": null,
      "content": "SCHEDULING:\nTHE MULTI-LEVEL FEEDBACK QUEUE\n79\nTIP: AVOID VOO-DOO CONSTANTS (OUSTERHOUT’S LAW)\nAvoiding voo-doo constants is a good idea whenever possible. Unfor-\ntunately, as in the example above, it is often difﬁcult. One could try to\nmake the system learn a good value, but that too is not straightforward.\nThe frequent result: a conﬁguration ﬁle ﬁlled with default parameter val-\nues that a seasoned administrator can tweak when something isn’t quite\nworking correctly. As you can imagine, these are often left unmodiﬁed,\nand thus we are left to hope that the defaults work well in the ﬁeld. This\ntip brought to you by our old OS professor, John Ousterhout, and hence\nwe call it Ousterhout’s Law.\nThe Solaris MLFQ implementation – the Time-Sharing scheduling class,\nor TS – is particularly easy to conﬁgure; it provides a set of tables that\ndetermine exactly how the priority of a process is altered throughout its\nlifetime, how long each time slice is, and how often to boost the priority of\na job [AD00]; an administrator can muck with this table in order to make\nthe scheduler behave in different ways. Default values for the table are\n60 queues, with slowly increasing time-slice lengths from 20 milliseconds\n(highest priority) to a few hundred milliseconds (lowest), and priorities\nboosted around every 1 second or so.\nOther MLFQ schedulers don’t use a table or the exact rules described\nin this chapter; rather they adjust priorities using mathematical formu-\nlae. For example, the FreeBSD scheduler (version 4.3) uses a formula to\ncalculate the current priority level of a job, basing it on how much CPU\nthe process has used [LM+89]; in addition, usage is decayed over time,\nproviding the desired priority boost in a different manner than described\nherein. See [E95] for an excellent overview of such decay-usage algo-\nrithms and their properties.\nFinally, many schedulers have a few other features that you might en-\ncounter. For example, some schedulers reserve the highest priority levels\nfor operating system work; thus typical user jobs can never obtain the\nhighest levels of priority in the system. Some systems also allow some\nuser advice to help set priorities; for example, by using the command-line\nutility nice you can increase or decrease the priority of a job (somewhat)\nand thus increase or decrease its chances of running at any given time.\nSee the man page for more.\n8.6\nMLFQ: Summary\nWe have described a scheduling approach known as the Multi-Level\nFeedback Queue (MLFQ). Hopefully you can now see why it is called\nthat: it has multiple levels of queues, and uses feedback to determine the\npriority of a given job. History is its guide: pay attention to how jobs\nbehave over time and treat them accordingly.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2776,
      "extraction_method": "Direct"
    },
    {
      "page_number": 116,
      "chapter": null,
      "content": "80\nSCHEDULING:\nTHE MULTI-LEVEL FEEDBACK QUEUE\nTIP: USE ADVICE WHERE POSSIBLE\nAs the operating system rarely knows what is best for each and every\nprocess of the system, it is often useful to provide interfaces to allow users\nor administrators to provide some hints to the OS. We often call such\nhints advice, as the OS need not necessarily pay attention to it, but rather\nmight take the advice into account in order to make a better decision.\nSuch hints are useful in many parts of the OS, including the scheduler\n(e.g., with nice), memory manager (e.g., madvise), and ﬁle system (e.g.,\nTIP [P+95]).\nThe reﬁned set of MLFQ rules, spread throughout the chapter, are re-\nproduced here for your viewing pleasure:\n• Rule 1: If Priority(A) > Priority(B), A runs (B doesn’t).\n• Rule 2: If Priority(A) = Priority(B), A & B run in RR.\n• Rule 3: When a job enters the system, it is placed at the highest\npriority (the topmost queue).\n• Rule 4: Once a job uses up its time allotment at a given level (re-\ngardless of how many times it has given up the CPU), its priority is\nreduced (i.e., it moves down one queue).\n• Rule 5: After some time period S, move all the jobs in the system\nto the topmost queue.\nMLFQ is interesting because instead of demanding a priori knowledge\nof the nature of a job, it instead observes the execution of a job and pri-\noritizes it accordingly. In this way, it manages to achieve the best of both\nworlds: it can deliver excellent overall performance (similar to SJF/STCF)\nfor short-running interactive jobs, and is fair and makes progress for long-\nrunning CPU-intensive workloads. For this reason, many systems, in-\ncluding BSD UNIX derivatives [LM+89, B86], Solaris [M06], and Win-\ndows NT and subsequent Windows operating systems [CS97] use a form\nof MLFQ as their base scheduler.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1850,
      "extraction_method": "Direct"
    },
    {
      "page_number": 117,
      "chapter": null,
      "content": "SCHEDULING:\nTHE MULTI-LEVEL FEEDBACK QUEUE\n81\nReferences\n[AD00] “Multilevel Feedback Queue Scheduling in Solaris”\nAndrea Arpaci-Dusseau\nAvailable: http://www.cs.wisc.edu/˜remzi/solaris-notes.pdf\nA great short set of notes by one of the authors on the details of the Solaris scheduler. OK, we are\nprobably biased in this description, but the notes are pretty darn good.\n[B86] “The Design of the UNIX Operating System”\nM.J. Bach\nPrentice-Hall, 1986\nOne of the classic old books on how a real UNIX operating system is built; a deﬁnite must-read for kernel\nhackers.\n[C+62] “An Experimental Time-Sharing System”\nF. J. Corbato, M. M. Daggett, R. C. Daley\nIFIPS 1962\nA bit hard to read, but the source of many of the ﬁrst ideas in multi-level feedback scheduling. Much\nof this later went into Multics, which one could argue was the most inﬂuential operating system of all\ntime.\n[CS97] “Inside Windows NT”\nHelen Custer and David A. Solomon\nMicrosoft Press, 1997\nThe NT book, if you want to learn about something other than UNIX. Of course, why would you? OK,\nwe’re kidding; you might actually work for Microsoft some day you know.\n[E95] “An Analysis of Decay-Usage Scheduling in Multiprocessors”\nD.H.J. Epema\nSIGMETRICS ’95\nA nice paper on the state of the art of scheduling back in the mid 1990s, including a good overview of\nthe basic approach behind decay-usage schedulers.\n[LM+89] “The Design and Implementation of the 4.3BSD UNIX Operating System”\nS.J. Lefﬂer, M.K. McKusick, M.J. Karels, J.S. Quarterman\nAddison-Wesley, 1989\nAnother OS classic, written by four of the main people behind BSD. The later versions of this book,\nwhile more up to date, don’t quite match the beauty of this one.\n[M06] “Solaris Internals: Solaris 10 and OpenSolaris Kernel Architecture”\nRichard McDougall\nPrentice-Hall, 2006\nA good book about Solaris and how it works.\n[O11] “John Ousterhout’s Home Page”\nJohn Ousterhout\nAvailable: http://www.stanford.edu/˜ouster/\nThe home page of the famous Professor Ousterhout. The two co-authors of this book had the pleasure of\ntaking graduate operating systems from Ousterhout while in graduate school; indeed, this is where the\ntwo co-authors got to know each other, eventually leading to marriage, kids, and even this book. Thus,\nyou really can blame Ousterhout for this entire mess you’re in.\n[P+95] “Informed Prefetching and Caching”\nR.H. Patterson, G.A. Gibson, E. Ginting, D. Stodolsky, J. Zelenka\nSOSP ’95\nA fun paper about some very cool ideas in ﬁle systems, including how applications can give the OS\nadvice about what ﬁles it is accessing and how it plans to access them.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2639,
      "extraction_method": "Direct"
    },
    {
      "page_number": 118,
      "chapter": null,
      "content": "82\nSCHEDULING:\nTHE MULTI-LEVEL FEEDBACK QUEUE\nHomework\nThis program, mlfq.py, allows you to see how the MLFQ scheduler\npresented in this chapter behaves. See the README for details.\nQuestions\n1. Run a few randomly-generated problems with just two jobs and\ntwo queues; compute the MLFQ execution trace for each. Make\nyour life easier by limiting the length of each job and turning off\nI/Os.\n2. How would you run the scheduler to reproduce each of the exam-\nples in the chapter?\n3. How would you conﬁgure the scheduler parameters to behave just\nlike a round-robin scheduler?\n4. Craft a workload with two jobs and scheduler parameters so that\none job takes advantage of the older Rules 4a and 4b (turned on\nwith the -S ﬂag) to game the scheduler and obtain 99% of the CPU\nover a particular time interval.\n5. Given a system with a quantum length of 10 ms in its highest queue,\nhow often would you have to boost jobs back to the highest priority\nlevel (with the -B ﬂag) in order to guarantee that a single long-\nrunning (and potentially-starving) job gets at least 5% of the CPU?\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1122,
      "extraction_method": "Direct"
    },
    {
      "page_number": 119,
      "chapter": null,
      "content": "9\nScheduling: Proportional Share\nIn this chapter, we’ll examine a different type of scheduler known as a\nproportional-share scheduler, also sometimes referred to as a fair-share\nscheduler. Proportional-share is based around a simple concept: instead\nof optimizing for turnaround or response time, a scheduler might instead\ntry to guarantee that each job obtain a certain percentage of CPU time.\nAn excellent modern example of proportional-share scheduling is found\nin research by Waldspurger and Weihl [WW94], and is known as lottery\nscheduling; however, the idea is certainly much older [KL88]. The basic\nidea is quite simple: every so often, hold a lottery to determine which pro-\ncess should get to run next; processes that should run more often should\nbe given more chances to win the lottery. Easy, no? Now, onto the details!\nBut not before our crux:\nCRUX: HOW TO SHARE THE CPU PROPORTIONALLY\nHow can we design a scheduler to share the CPU in a proportional\nmanner? What are the key mechanisms for doing so? How effective are\nthey?\n9.1\nBasic Concept: Tickets Represent Your Share\nUnderlying lottery scheduling is one very basic concept: tickets, which\nare used to represent the share of a resource that a process (or user or\nwhatever) should receive. The percent of tickets that a process has repre-\nsents its share of the system resource in question.\nLet’s look at an example. Imagine two processes, A and B, and further\nthat A has 75 tickets while B has only 25. Thus, what we would like is for\nA to receive 75% of the CPU and B the remaining 25%.\nLottery scheduling achieves this probabilistically (but not determinis-\ntically) by holding a lottery every so often (say, every time slice). Holding\na lottery is straightforward: the scheduler must know how many total\ntickets there are (in our example, there are 100). The scheduler then picks\n83\n",
      "content_length": 1853,
      "extraction_method": "Direct"
    },
    {
      "page_number": 120,
      "chapter": null,
      "content": "84\nSCHEDULING: PROPORTIONAL SHARE\nTIP: USE RANDOMNESS\nOne of the most beautiful aspects of lottery scheduling is its use of ran-\ndomness. When you have to make a decision, using such a randomized\napproach is often a robust and simple way of doing so.\nRandom approaches has at least three advantages over more traditional\ndecisions. First, random often avoids strange corner-case behaviors that\na more traditional algorithm may have trouble handling. For example,\nconsider LRU page replacement (studied in more detail in a future chap-\nter on virtual memory); while often a good replacement algorithm, LRU\nperforms pessimally for some cyclic-sequential workloads. Random, on\nthe other hand, has no such worst case.\nSecond, random also is lightweight, requiring little state to track alter-\nnatives. In a traditional fair-share scheduling algorithm, tracking how\nmuch CPU each process has received requires per-process accounting,\nwhich must be updated after running each process. Doing so randomly\nnecessitates only the most minimal of per-process state (e.g., the number\nof tickets each has).\nFinally, random can be quite fast. As long as generating a random num-\nber is quick, making the decision is also, and thus random can be used\nin a number of places where speed is required. Of course, the faster the\nneed, the more random tends towards pseudo-random.\na winning ticket, which is a number from 0 to 991. Assuming A holds\ntickets 0 through 74 and B 75 through 99, the winning ticket simply de-\ntermines whether A or B runs. The scheduler then loads the state of that\nwinning process and runs it.\nHere is an example output of a lottery scheduler’s winning tickets:\n63 85 70 39 76 17 29 41 36 39 10 99 68 83 63 62 43\n0 49 49\nHere is the resulting schedule:\nA\nB\nA\nA\nB\nA\nA\nA\nA\nA\nA\nB\nA\nB\nA\nA\nA\nA\nA\nA\nAs you can see from the example, the use of randomness in lottery\nscheduling leads to a probabilistic correctness in meeting the desired pro-\nportion, but no guarantee. In our example above, B only gets to run 4 out\nof 20 time slices (20%), instead of the desired 25% allocation. However,\nthe longer these two jobs compete, the more likely they are to achieve the\ndesired percentages.\n1Computer Scientists always start counting at 0. It is so odd to non-computer-types that\nfamous people have felt obliged to write about why we do it this way [D82].\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2397,
      "extraction_method": "Direct"
    },
    {
      "page_number": 121,
      "chapter": null,
      "content": "SCHEDULING: PROPORTIONAL SHARE\n85\nTIP: USE TICKETS TO REPRESENT SHARES\nOne of the most powerful (and basic) mechanisms in the design of lottery\n(and stride) scheduling is that of the ticket. The ticket is used to represent\na process’s share of the CPU in these examples, but can be applied much\nmore broadly. For example, in more recent work on virtual memory man-\nagement for hypervisors, Waldspurger shows how tickets can be used to\nrepresent a guest operating system’s share of memory [W02]. Thus, if you\nare ever in need of a mechanism to represent a proportion of ownership,\nthis concept just might be ... (wait for it) ... the ticket.\n9.2\nTicket Mechanisms\nLottery scheduling also provides a number of mechanisms to manip-\nulate tickets in different and sometimes useful ways. One way is with\nthe concept of ticket currency. Currency allows a user with a set of tick-\nets to allocate tickets among their own jobs in whatever currency they\nwould like; the system then automatically converts said currency into the\ncorrect global value.\nFor example, assume users A and B have each been given 100 tickets.\nUser A is running two jobs, A1 and A2, and gives them each 500 tickets\n(out of 1000 total) in User A’s own currency. User B is running only 1 job\nand gives it 10 tickets (out of 10 total). The system will convert A1’s and\nA2’s allocation from 500 each in A’s currency to 50 each in the global cur-\nrency; similarly, B1’s 10 tickets will be converted to 100 tickets. The lottery\nwill then be held over the global ticket currency (200 total) to determine\nwhich job runs.\nUser A -> 500 (A’s currency) to A1 ->\n50 (global currency)\n-> 500 (A’s currency) to A2 ->\n50 (global currency)\nUser B ->\n10 (B’s currency) to B1 -> 100 (global currency)\nAnother useful mechanism is ticket transfer. With transfers, a process\ncan temporarily hand off its tickets to another process. This ability is\nespecially useful in a client/server setting, where a client process sends\na message to a server asking it to do some work on the client’s behalf.\nTo speed up the work, the client can pass the tickets to the server and\nthus try to maximize the performance of the server while the server is\nhandling the client’s request. When ﬁnished, the server then transfers the\ntickets back to the client and all is as before.\nFinally, ticket inﬂation can sometimes be a useful technique. With\ninﬂation, a process can temporarily raise or lower the number of tickets\nit owns. Of course, in a competitive scenario with processes that do not\ntrust one another, this makes little sense; one greedy process could give\nitself a vast number of tickets and take over the machine. Rather, inﬂation\ncan be applied in an environment where a group of processes trust one\nanother; in such a case, if any one process knows it needs more CPU time,\nit can boost its ticket value as a way to reﬂect that need to the system, all\nwithout communicating with any other processes.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2980,
      "extraction_method": "Direct"
    },
    {
      "page_number": 122,
      "chapter": null,
      "content": "86\nSCHEDULING: PROPORTIONAL SHARE\n1\n// counter: used to track if we’ve found the winner yet\n2\nint counter\n= 0;\n3\n4\n// winner: use some call to a random number generator to\n5\n//\nget a value, between 0 and the total # of tickets\n6\nint winner\n= getrandom(0, totaltickets);\n7\n8\n// current: use this to walk through the list of jobs\n9\nnode_t *current = head;\n10\n11\n// loop until the sum of ticket values is > the winner\n12\nwhile (current) {\n13\ncounter = counter + current->tickets;\n14\nif (counter > winner)\n15\nbreak; // found the winner\n16\ncurrent = current->next;\n17\n}\n18\n// ’current’ is the winner: schedule it...\nFigure 9.1: Lottery Scheduling Decision Code\n9.3\nImplementation\nProbably the most amazing thing about lottery scheduling is the sim-\nplicity of its implementation. All you need is a good random number\ngenerator to pick the winning ticket, a data structure to track the pro-\ncesses of the system (e.g., a list), and the total number of tickets.\nLet’s assume we keep the processes in a list. Here is an example com-\nprised of three processes, A, B, and C, each with some number of tickets.\nhead\nJob:A\nTix:100\nJob:B\nTix:50\nJob:C\nTix:250\nNULL\nTo make a scheduling decision, we ﬁrst have to pick a random number\n(the winner) from the total number of tickets (400)2 Let’s say we pick the\nnumber 300. Then, we simply traverse the list, with a simple counter\nused to help us ﬁnd the winner (Figure 9.1).\nThe code walks the list of processes, adding each ticket value to counter\nuntil the value exceeds winner. Once that is the case, the current list el-\nement is the winner. With our example of the winning ticket being 300,\nthe following takes place. First, counter is incremented to 100 to ac-\ncount for A’s tickets; because 100 is less than 300, the loop continues.\nThen counter would be updated to 150 (B’s tickets), still less than 300\nand thus again we continue. Finally, counter is updated to 400 (clearly\ngreater than 300), and thus we break out of the loop with current point-\ning at C (the winner).\nTo make this process most efﬁcient, it might generally be best to or-\nganize the list in sorted order, from the highest number of tickets to the\n2Surprisingly, as pointed out by Bj¨orn Lindberg, this can be challenging to do\ncorrectly; for more details, see http://stackoverflow.com/questions/2509679/\nhow-to-generate-a-random-number-from-within-a-range.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2414,
      "extraction_method": "Direct"
    },
    {
      "page_number": 123,
      "chapter": null,
      "content": "SCHEDULING: PROPORTIONAL SHARE\n87\n1\n10\n100\n1000\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nJob Length\nUnfairness (Average)\nFigure 9.2: Lottery Fairness Study\nlowest. The ordering does not affect the correctness of the algorithm;\nhowever, it does ensure in general that the fewest number of list itera-\ntions are taken, especially if there are a few processes that possess most\nof the tickets.\n9.4\nAn Example\nTo make the dynamics of lottery scheduling more understandable, we\nnow perform a brief study of the completion time of two jobs competing\nagainst one another, each with the same number of tickets (100) and same\nrun time (R, which we will vary).\nIn this scenario, we’d like for each job to ﬁnish at roughly the same\ntime, but due to the randomness of lottery scheduling, sometimes one\njob ﬁnishes before the other.\nTo quantify this difference, we deﬁne a\nsimple unfairness metric, U which is simply the time the ﬁrst job com-\npletes divided by the time that the second job completes. For example,\nif R = 10, and the ﬁrst job ﬁnishes at time 10 (and the second job at 20),\nU = 10\n20 = 0.5. When both jobs ﬁnish at nearly the same time, U will be\nquite close to 1. In this scenario, that is our goal: a perfectly fair scheduler\nwould achieve U = 1.\nFigure 9.2 plots the average unfairness as the length of the two jobs\n(R) is varied from 1 to 1000 over thirty trials (results are generated via the\nsimulator provided at the end of the chapter). As you can see from the\ngraph, when the job length is not very long, average unfairness can be\nquite severe. Only as the jobs run for a signiﬁcant number of time slices\ndoes the lottery scheduler approach the desired outcome.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 1698,
      "extraction_method": "Direct"
    },
    {
      "page_number": 124,
      "chapter": null,
      "content": "88\nSCHEDULING: PROPORTIONAL SHARE\n9.5\nHow To Assign Tickets?\nOne problem we have not addressed with lottery scheduling is: how\nto assign tickets to jobs? This problem is a tough one, because of course\nhow the system behaves is strongly dependent on how tickets are allo-\ncated. One approach is to assume that the users know best; in such a\ncase, each user is handed some number of tickets, and a user can allocate\ntickets to any jobs they run as desired. However, this solution is a non-\nsolution: it really doesn’t tell you what to do. Thus, given a set of jobs,\nthe “ticket-assignment problem” remains open.\n9.6\nWhy Not Deterministic?\nYou might also be wondering: why use randomness at all? As we saw\nabove, while randomness gets us a simple (and approximately correct)\nscheduler, it occasionally will not deliver the exact right proportions, es-\npecially over short time scales. For this reason, Waldspurger invented\nstride scheduling, a deterministic fair-share scheduler [W95].\nStride scheduling is also straightforward. Each job in the system has\na stride, which is inverse in proportion to the number of tickets it has. In\nour example above, with jobs A, B, and C, with 100, 50, and 250 tickets,\nrespectively, we can compute the stride of each by dividing some large\nnumber by the number of tickets each process has been assigned. For\nexample, if we divide 10,000 by each of those ticket values, we obtain\nthe following stride values for A, B, and C: 100, 200, and 40. We call\nthis value the stride of each process; every time a process runs, we will\nincrement a counter for it (called its pass value) by its stride to track its\nglobal progress.\nThe scheduler then uses the stride and pass to determine which pro-\ncess should run next. The basic idea is simple: at any given time, pick\nthe process to run that has the lowest pass value so far; when you run\na process, increment its pass counter by its stride. A pseudocode imple-\nmentation is provided by Waldspurger [W95]:\ncurrent = remove_min(queue);\n// pick client with minimum pass\nschedule(current);\n// use resource for quantum\ncurrent->pass += current->stride; // compute next pass using stride\ninsert(queue, current);\n// put back into the queue\nIn our example, we start with three processes (A, B, and C), with stride\nvalues of 100, 200, and 40, and all with pass values initially at 0. Thus, at\nﬁrst, any of the processes might run, as their pass values are equally low.\nAssume we pick A (arbitrarily; any of the processes with equal low pass\nvalues can be chosen). A runs; when ﬁnished with the time slice, we\nupdate its pass value to 100. Then we run B, whose pass value is then\nset to 200. Finally, we run C, whose pass value is incremented to 40. At\nthis point, the algorithm will pick the lowest pass value, which is C’s, and\nrun it, updating its pass to 80 (C’s stride is 40, as you recall). Then C will\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2922,
      "extraction_method": "Direct"
    },
    {
      "page_number": 125,
      "chapter": null,
      "content": "SCHEDULING: PROPORTIONAL SHARE\n89\nPass(A)\nPass(B)\nPass(C)\nWho Runs?\n(stride=100)\n(stride=200)\n(stride=40)\n0\n0\n0\nA\n100\n0\n0\nB\n100\n200\n0\nC\n100\n200\n40\nC\n100\n200\n80\nC\n100\n200\n120\nA\n200\n200\n120\nC\n200\n200\n160\nC\n200\n200\n200\n...\nTable 9.1: Stride Scheduling: A Trace\nrun again (still the lowest pass value), raising its pass to 120. A will run\nnow, updating its pass to 200 (now equal to B’s). Then C will run twice\nmore, updating its pass to 160 then 200. At this point, all pass values are\nequal again, and the process will repeat, ad inﬁnitum. Table 9.1 traces the\nbehavior of the scheduler over time.\nAs we can see from the table, C ran ﬁve times, A twice, and B just once,\nexactly in proportion to their ticket values of 250, 100, and 50. Lottery\nscheduling achieves the proportions probabilistically over time; stride\nscheduling gets them exactly right at the end of each scheduling cycle.\nSo you might be wondering: given the precision of stride scheduling,\nwhy use lottery scheduling at all? Well, lottery scheduling has one nice\nproperty that stride scheduling does not: no global state. Imagine a new\njob enters in the middle of our stride scheduling example above; what\nshould its pass value be? Should it be set to 0? If so, it will monopolize\nthe CPU. With lottery scheduling, there is no global state per process;\nwe simply add a new process with whatever tickets it has, update the\nsingle global variable to track how many total tickets we have, and go\nfrom there. In this way, lottery makes it much easier to incorporate new\nprocesses in a sensible manner.\n9.7\nSummary\nWe have introduced the concept of proportional-share scheduling and\nbrieﬂy discussed two implementations: lottery and stride scheduling.\nLottery uses randomness in a clever way to achieve proportional share;\nstride does so deterministically. Although both are conceptually inter-\nesting, they have not achieved wide-spread adoption as CPU schedulers\nfor a variety of reasons. One is that such approaches do not particularly\nmesh well with I/O [AC97]; another is that they leave open the hard prob-\nlem of ticket assignment, i.e., how do you know how many tickets your\nbrowser should be allocated? General-purpose schedulers (such as the\nMLFQ we discussed previously, and other similar Linux schedulers) do\nso more gracefully and thus are more widely deployed.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2377,
      "extraction_method": "Direct"
    },
    {
      "page_number": 126,
      "chapter": null,
      "content": "90\nSCHEDULING: PROPORTIONAL SHARE\nAs a result, proportional-share schedulers are more useful in domains\nwhere some of these problems (such as assignment of shares) are rela-\ntively easy to solve. For example, in a virtualized data center, where you\nmight like to assign one-quarter of your CPU cycles to the Windows VM\nand the rest to your base Linux installation, proportional sharing can be\nsimple and effective. See Waldspurger [W02] for further details on how\nsuch a scheme is used to proportionally share memory in VMWare’s ESX\nServer.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 588,
      "extraction_method": "Direct"
    },
    {
      "page_number": 127,
      "chapter": null,
      "content": "SCHEDULING: PROPORTIONAL SHARE\n91\nReferences\n[AC97] “Extending Proportional-Share Scheduling to a Network of Workstations”\nAndrea C. Arpaci-Dusseau and David E. Culler\nPDPTA’97, June 1997\nA paper by one of the authors on how to extend proportional-share scheduling to work better in a\nclustered environment.\n[D82] “Why Numbering Should Start At Zero”\nEdsger Dijkstra, August 1982\nhttp://www.cs.utexas.edu/users/EWD/ewd08xx/EWD831.PDF\nA short note from E. Dijkstra, one of the pioneers of computer science. We’ll be hearing much more\non this guy in the section on Concurrency. In the meanwhile, enjoy this note, which includes this\nmotivating quote: “One of my colleagues – not a computing scientist – accused a number of younger\ncomputing scientists of ’pedantry’ because they started numbering at zero.” The note explains why\ndoing so is logical.\n[KL88] “A Fair Share Scheduler”\nJ. Kay and P. Lauder\nCACM, Volume 31 Issue 1, January 1988\nAn early reference to a fair-share scheduler.\n[WW94] “Lottery Scheduling: Flexible Proportional-Share Resource Management”\nCarl A. Waldspurger and William E. Weihl\nOSDI ’94, November 1994\nThe landmark paper on lottery scheduling that got the systems community re-energized about schedul-\ning, fair sharing, and the power of simple randomized algorithms.\n[W95] “Lottery and Stride Scheduling: Flexible\nProportional-Share Resource Management”\nCarl A. Waldspurger\nPh.D. Thesis, MIT, 1995\nThe award-winning thesis of Waldspurger’s that outlines lottery and stride scheduling. If you’re think-\ning of writing a Ph.D. dissertation at some point, you should always have a good example around, to\ngive you something to strive for: this is such a good one.\n[W02] “Memory Resource Management in VMware ESX Server”\nCarl A. Waldspurger\nOSDI ’02, Boston, Massachusetts\nThe paper to read about memory management in VMMs (a.k.a., hypervisors). In addition to being\nrelatively easy to read, the paper contains numerous cool ideas about this new type of VMM-level\nmemory management.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2046,
      "extraction_method": "Direct"
    },
    {
      "page_number": 128,
      "chapter": null,
      "content": "92\nSCHEDULING: PROPORTIONAL SHARE\nHomework\nThis program, lottery.py, allows you to see how a lottery scheduler\nworks. See the README for details.\nQuestions\n1. Compute the solutions for simulations with 3 jobs and random seeds\nof 1, 2, and 3.\n2. Now run with two speciﬁc jobs: each of length 10, but one (job 0)\nwith just 1 ticket and the other (job 1) with 100 (e.g., -l 10:1,10:100).\nWhat happens when the number of tickets is so imbalanced? Will\njob 0 ever run before job 1 completes? How often? In general, what\ndoes such a ticket imbalance do to the behavior of lottery schedul-\ning?\n3. When running with two jobs of length 100 and equal ticket alloca-\ntions of 100 (-l 100:100,100:100), how unfair is the scheduler?\nRun with some different random seeds to determine the (probabilis-\ntic) answer; let unfairness be determined by how much earlier one\njob ﬁnishes than the other.\n4. How does your answer to the previous question change as the quan-\ntum size (-q) gets larger?\n5. Can you make a version of the graph that is found in the chapter?\nWhat else would be worth exploring? How would the graph look\nwith a stride scheduler?\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1180,
      "extraction_method": "Direct"
    },
    {
      "page_number": 129,
      "chapter": null,
      "content": "10\nMultiprocessor Scheduling (Advanced)\nThis chapter will introduce the basics of multiprocessor scheduling. As\nthis topic is relatively advanced, it may be best to cover it after you have\nstudied the topic of concurrency in some detail (i.e., the second major\n“easy piece” of the book).\nAfter years of existence only in the high-end of the computing spec-\ntrum, multiprocessor systems are increasingly commonplace, and have\nfound their way into desktop machines, laptops, and even mobile de-\nvices. The rise of the multicore processor, in which multiple CPU cores\nare packed onto a single chip, is the source of this proliferation; these\nchips have become popular as computer architects have had a difﬁcult\ntime making a single CPU much faster without using (way) too much\npower. And thus we all now have a few CPUs available to us, which is a\ngood thing, right?\nOf course, there are many difﬁculties that arise with the arrival of more\nthan a single CPU. A primary one is that a typical application (i.e., some C\nprogram you wrote) only uses a single CPU; adding more CPUs does not\nmake that single application run faster. To remedy this problem, you’ll\nhave to rewrite your application to run in parallel, perhaps using threads\n(as discussed in great detail in the second piece of this book). Multi-\nthreaded applications can spread work across multiple CPUs and thus\nrun faster when given more CPU resources.\nASIDE: ADVANCED CHAPTERS\nAdvanced chapters require material from a broad swath of the book to\ntruly understand, while logically ﬁtting into a section that is earlier than\nsaid set of prerequisite materials. For example, this chapter on multipro-\ncessor scheduling makes much more sense if you’ve ﬁrst read the middle\npiece on concurrency; however, it logically ﬁts into the part of the book\non virtualization (generally) and CPU scheduling (speciﬁcally). Thus, it\nis recommended such chapters be covered out of order; in this case, after\nthe second piece of the book.\n93\n",
      "content_length": 1984,
      "extraction_method": "Direct"
    },
    {
      "page_number": 130,
      "chapter": null,
      "content": "94\nMULTIPROCESSOR SCHEDULING (ADVANCED)\nMemory\nCPU\nCache\nFigure 10.1: Single CPU With Cache\nBeyond applications, a new problem that arises for the operating sys-\ntem is (not surprisingly!) that of multiprocessor scheduling. Thus far\nwe’ve discussed a number of principles behind single-processor schedul-\ning; how can we extend those ideas to work on multiple CPUs? What\nnew problems must we overcome? And thus, our problem:\nCRUX: HOW TO SCHEDULE JOBS ON MULTIPLE CPUS\nHow should the OS schedule jobs on multiple CPUs? What new prob-\nlems arise? Do the same old techniques work, or are new ideas required?\n10.1\nBackground: Multiprocessor Architecture\nTo understand the new issues surrounding multiprocessor schedul-\ning, we have to understand a new and fundamental difference between\nsingle-CPU hardware and multi-CPU hardware. This difference centers\naround the use of hardware caches (e.g., Figure 10.1), and exactly how\ndata is shared across multiple processors. We now discuss this issue fur-\nther, at a high level. Details are available elsewhere [CSG99], in particular\nin an upper-level or perhaps graduate computer architecture course.\nIn a system with a single CPU, there are a hierarchy of hardware\ncaches that in general help the processor run programs faster. Caches\nare small, fast memories that (in general) hold copies of popular data that\nis found in the main memory of the system. Main memory, in contrast,\nholds all of the data, but access to this larger memory is slower. By keep-\ning frequently accessed data in a cache, the system can make the large,\nslow memory appear to be a fast one.\nAs an example, consider a program that issues an explicit load instruc-\ntion to fetch a value from memory, and a simple system with only a single\nCPU; the CPU has a small cache (say 64 KB) and a large main memory.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1869,
      "extraction_method": "Direct"
    },
    {
      "page_number": 131,
      "chapter": null,
      "content": "MULTIPROCESSOR SCHEDULING (ADVANCED)\n95\nMemory\nCPU\nCPU\nCache\nCache\nBus\nFigure 10.2: Two CPUs With Caches Sharing Memory\nThe ﬁrst time a program issues this load, the data resides in main mem-\nory, and thus takes a long time to fetch (perhaps in the tens of nanosec-\nonds, or even hundreds). The processor, anticipating that the data may\nbe reused, puts a copy of the loaded data into the CPU cache. If the pro-\ngram later fetches this same data item again, the CPU ﬁrst checks for it in\nthe cache; because it ﬁnds it there, the data is fetched much more quickly\n(say, just a few nanoseconds), and thus the program runs faster.\nCaches are thus based on the notion of locality, of which there are\ntwo kinds: temporal locality and spatial locality. The idea behind tem-\nporal locality is that when a piece of data is accessed, it is likely to be\naccessed again in the near future; imagine variables or even instructions\nthemselves being accessed over and over again in a loop. The idea be-\nhind spatial locality is that if a program accesses a data item at address\nx, it is likely to access data items near x as well; here, think of a program\nstreaming through an array, or instructions being executed one after the\nother. Because locality of these types exist in many programs, hardware\nsystems can make good guesses about which data to put in a cache and\nthus work well.\nNow for the tricky part: what happens when you have multiple pro-\ncessors in a single system, with a single shared main memory, as we see\nin Figure 10.2?\nAs it turns out, caching with multiple CPUs is much more compli-\ncated. Imagine, for example, that a program running on CPU 1 reads\na data item (with value D) at address A; because the data is not in the\ncache on CPU 1, the system fetches it from main memory, and gets the\nvalue D. The program then modiﬁes the value at address A, just updat-\ning its cache with the new value D′; writing the data through all the way\nto main memory is slow, so the system will (usually) do that later. Then\nassume the OS decides to stop running the program and move it to CPU\n2. The program then re-reads the value at address A; there is no such data\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2199,
      "extraction_method": "Direct"
    },
    {
      "page_number": 132,
      "chapter": null,
      "content": "96\nMULTIPROCESSOR SCHEDULING (ADVANCED)\nCPU 2’s cache, and thus the system fetches the value from main memory,\nand gets the old value D instead of the correct value D′. Oops!\nThis general problem is called the problem of cache coherence, and\nthere is a vast research literature that describes many different subtleties\ninvolved with solving the problem [SHW11]. Here, we will skip all of the\nnuance and make some major points; take a computer architecture class\n(or three) to learn more.\nThe basic solution is provided by the hardware: by monitoring mem-\nory accesses, hardware can ensure that basically the “right thing” hap-\npens and that the view of a single shared memory is preserved. One way\nto do this on a bus-based system (as described above) is to use an old\ntechnique known as bus snooping [G83]; each cache pays attention to\nmemory updates by observing the bus that connects them to main mem-\nory. When a CPU then sees an update for a data item it holds in its cache,\nit will notice the change and either invalidate its copy (i.e., remove it\nfrom its own cache) or update it (i.e., put the new value into its cache\ntoo). Write-back caches, as hinted at above, make this more complicated\n(because the write to main memory isn’t visible until later), but you can\nimagine how the basic scheme might work.\n10.2\nDon’t Forget Synchronization\nGiven that the caches do all of this work to provide coherence, do pro-\ngrams (or the OS itself) have to worry about anything when they access\nshared data? The answer, unfortunately, is yes, and is documented in\ngreat detail in the second piece of this book on the topic of concurrency.\nWhile we won’t get into the details here, we’ll sketch/review some of the\nbasic ideas here (assuming you’re familiar with concurrency).\nWhen accessing (and in particular, updating) shared data items or\nstructures across CPUs, mutual exclusion primitives (such as locks) should\nlikely be used to guarantee correctness (other approaches, such as build-\ning lock-free data structures, are complex and only used on occasion;\nsee the chapter on deadlock in the piece on concurrency for details). For\nexample, assume we have a shared queue being accessed on multiple\nCPUs concurrently. Without locks, adding or removing elements from\nthe queue concurrently will not work as expected, even with the under-\nlying coherence protocols; one needs locks to atomically update the data\nstructure to its new state.\nTo make this more concrete, imagine this code sequence, which is used\nto remove an element from a shared linked list, as we see in Figure 10.3.\nImagine if threads on two CPUs enter this routine at the same time. If\nThread 1 executes the ﬁrst line, it will have the current value of head\nstored in its tmp variable; if Thread 2 then executes the ﬁrst line as well,\nit also will have the same value of head stored in its own private tmp\nvariable (tmp is allocated on the stack, and thus each thread will have\nits own private storage for it). Thus, instead of each thread removing\nan element from the head of the list, each thread will try to remove the\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 3133,
      "extraction_method": "Direct"
    },
    {
      "page_number": 133,
      "chapter": null,
      "content": "MULTIPROCESSOR SCHEDULING (ADVANCED)\n97\n1\ntypedef struct __Node_t {\n2\nint\nvalue;\n3\nstruct __Node_t *next;\n4\n} Node_t;\n5\n6\nint List_Pop() {\n7\nNode_t *tmp = head;\n// remember old head ...\n8\nint value\n= head->value;\n// ... and its value\n9\nhead\n= head->next;\n// advance head to next pointer\n10\nfree(tmp);\n// free old head\n11\nreturn value;\n// return value at head\n12\n}\nFigure 10.3: Simple List Delete Code\nsame head element, leading to all sorts of problems (such as an attempted\ndouble free of the head element at line 4, as well as potentially returning\nthe same data value twice).\nThe solution, of course, is to make such routines correct via lock-\ning.\nIn this case, allocating a simple mutex (e.g., pthread mutex t\nm;) and then adding a lock(&m) at the beginning of the routine and\nan unlock(&m) at the end will solve the problem, ensuring that the code\nwill execute as desired. Unfortunately, as we will see, such an approach is\nnot without problems, in particular with regards to performance. Speciﬁ-\ncally, as the number of CPUs grows, access to a synchronized shared data\nstructure becomes quite slow.\n10.3\nOne Final Issue: Cache Afﬁnity\nOne ﬁnal issue arises in building a multiprocessor cache scheduler,\nknown as cache afﬁnity. This notion is simple: a process, when run on a\nparticular CPU, builds up a fair bit of state in the caches (and TLBs) of the\nCPU. The next time the process runs, it is often advantageous to run it on\nthe same CPU, as it will run faster if some of its state is already present in\nthe caches on that CPU. If, instead, one runs a process on a different CPU\neach time, the performance of the process will be worse, as it will have to\nreload the state each time it runs (note it will run correctly on a different\nCPU thanks to the cache coherence protocols of the hardware). Thus, a\nmultiprocessor scheduler should consider cache afﬁnity when making its\nscheduling decisions, perhaps preferring to keep a process on the same\nCPU if at all possible.\n10.4\nSingle-Queue Scheduling\nWith this background in place, we now discuss how to build a sched-\nuler for a multiprocessor system. The most basic approach is to simply\nreuse the basic framework for single processor scheduling, by putting all\njobs that need to be scheduled into a single queue; we call this single-\nqueue multiprocessor scheduling or SQMS for short. This approach\nhas the advantage of simplicity; it does not require much work to take an\nexisting policy that picks the best job to run next and adapt it to work on\nmore than one CPU (where it might pick the best two jobs to run, if there\nare two CPUs, for example).\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2652,
      "extraction_method": "Direct"
    },
    {
      "page_number": 134,
      "chapter": null,
      "content": "98\nMULTIPROCESSOR SCHEDULING (ADVANCED)\nHowever, SQMS has obvious shortcomings. The ﬁrst problem is a lack\nof scalability. To ensure the scheduler works correctly on multiple CPUs,\nthe developers will have inserted some form of locking into the code, as\ndescribed above. Locks ensure that when SQMS code accesses the single\nqueue (say, to ﬁnd the next job to run), the proper outcome arises.\nLocks, unfortunately, can greatly reduce performance, particularly as\nthe number of CPUs in the systems grows [A91]. As contention for such\na single lock increases, the system spends more and more time in lock\noverhead and less time doing the work the system should be doing (note:\nit would be great to include a real measurement of this in here someday).\nThe second main problem with SQMS is cache afﬁnity. For example,\nlet us assume we have ﬁve jobs to run (A, B, C, D, E) and four processors.\nOur scheduling queue thus looks like this:\nQueue\nA\nB\nC\nD\nE\nNULL\nOver time, assuming each job runs for a time slice and then another\njob is chosen, here is a possible job schedule across CPUs:\nCPU 3\nCPU 2\nCPU 1\nCPU 0\nD\nC\nB\nA\nE\nC\nB\nA\nE\nD\nB\nA\nE\nD\nC\nA\nE\nD\nC\nB\n ... (repeat) ...\n ... (repeat) ...\n ... (repeat) ...\n ... (repeat) ...\nBecause each CPU simply picks the next job to run from the globally-\nshared queue, each job ends up bouncing around from CPU to CPU, thus\ndoing exactly the opposite of what would make sense from the stand-\npoint of cache afﬁnity.\nTo handle this problem, most SQMS schedulers include some kind of\nafﬁnity mechanism to try to make it more likely that process will continue\nto run on the same CPU if possible. Speciﬁcally, one might provide afﬁn-\nity for some jobs, but move others around to balance load. For example,\nimagine the same ﬁve jobs scheduled as follows:\nCPU 3\nCPU 2\nCPU 1\nCPU 0\nD\nD\nD\nD\nE\nC\nC\nC\nE\nC\nB\nB\nE\nB\nB\nA\nE\nA\nA\nA\n ... (repeat) ...\n ... (repeat) ...\n ... (repeat) ...\n ... (repeat) ...\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1963,
      "extraction_method": "Direct"
    },
    {
      "page_number": 135,
      "chapter": null,
      "content": "MULTIPROCESSOR SCHEDULING (ADVANCED)\n99\nIn this arrangement, jobs A through D are not moved across proces-\nsors, with only job E migrating from CPU to CPU, thus preserving afﬁn-\nity for most. You could then decide to migrate a different job the next\ntime through, thus achieving some kind of afﬁnity fairness as well. Im-\nplementing such a scheme, however, can be complex.\nThus, we can see the SQMS approach has its strengths and weak-\nnesses. It is straightforward to implement given an existing single-CPU\nscheduler, which by deﬁnition has only a single queue. However, it does\nnot scale well (due to synchronization overheads), and it does not readily\npreserve cache afﬁnity.\n10.5\nMulti-Queue Scheduling\nBecause of the problems caused in single-queue schedulers, some sys-\ntems opt for multiple queues, e.g., one per CPU. We call this approach\nmulti-queue multiprocessor scheduling (or MQMS).\nIn MQMS, our basic scheduling framework consists of multiple schedul-\ning queues. Each queue will likely follow a particular scheduling disci-\npline, such as round robin, though of course any algorithm can be used.\nWhen a job enters the system, it is placed on exactly one scheduling\nqueue, according to some heuristic (e.g., random, or picking one with\nfewer jobs than others). Then it is scheduled essentially independently,\nthus avoiding the problems of information sharing and synchronization\nfound in the single-queue approach.\nFor example, assume we have a system where there are just two CPUs\n(labeled CPU 0 and CPU 1), and some number of jobs enter the system:\nA, B, C, and D for example. Given that each CPU has a scheduling queue\nnow, the OS has to decide into which queue to place each job. It might do\nsomething like this:\nQ0\nA\nC\nQ1\nB\nD\nDepending on the queue scheduling policy, each CPU now has two\njobs to choose from when deciding what should run. For example, with\nround robin, the system might produce a schedule that looks like this:\nCPU 1\nCPU 0\nA\nA\nC\nC\nA\nA\nC\nC\nA\nA\nC\nC\nB\nB\nD\nD\nB\nB\nD\nD\nB\nB\nD\nD\n ... \n ... \nMQMS has a distinct advantage of SQMS in that it should be inher-\nently more scalable. As the number of CPUs grows, so too does the num-\nber of queues, and thus lock and cache contention should not become a\ncentral problem. In addition, MQMS intrinsically provides cache afﬁnity;\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2341,
      "extraction_method": "Direct"
    },
    {
      "page_number": 136,
      "chapter": null,
      "content": "100\nMULTIPROCESSOR SCHEDULING (ADVANCED)\njobs stay on the same CPU and thus reap the advantage of reusing cached\ncontents therein.\nBut, if you’ve been paying attention, you might see that we have a new\nproblem, which is fundamental in the multi-queue based approach: load\nimbalance. Let’s assume we have the same set up as above (four jobs,\ntwo CPUs), but then one of the jobs (say C) ﬁnishes. We now have the\nfollowing scheduling queues:\nQ0\nA\nQ1\nB\nD\nIf we then run our round-robin policy on each queue of the system, we\nwill see this resulting schedule:\nCPU 1\nCPU 0\nA\nA\nA\nA\nA\nA\nA\nA\nA\nA\nA\nA\nB\nB\nD\nD\nB\nB\nD\nD\nB\nB\nD\nD\n ... \n ... \nAs you can see from this diagram, A gets twice as much CPU as B and\nD, which is not the desired outcome. Even worse, let’s imagine that both\nA and C ﬁnish, leaving just jobs B and D in the system. The scheduling\nqueues will look like this:\nQ0\nQ1\nB\nD\nAs a result, CPU 0 will be left idle! (insert dramatic and sinister music here)\nAnd hence our CPU usage timeline looks sad:\nCPU 0\nCPU 1\nB\nB\nD\nD\nB\nB\nD\nD\nB\nB\nD\nD\n ... \nSo what should a poor multi-queue multiprocessor scheduler do? How\ncan we overcome the insidious problem of load imbalance and defeat the\nevil forces of ... the Decepticons1? How do we stop asking questions that\nare hardly relevant to this otherwise wonderful book?\n1Little known fact is that the home planet of Cybertron was destroyed by bad CPU\nscheduling decisions. And now let that be the ﬁrst and last reference to Transformers in this\nbook, for which we sincerely apologize.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1571,
      "extraction_method": "Direct"
    },
    {
      "page_number": 137,
      "chapter": null,
      "content": "MULTIPROCESSOR SCHEDULING (ADVANCED)\n101\nCRUX: HOW TO DEAL WITH LOAD IMBALANCE\nHow should a multi-queue multiprocessor scheduler handle load im-\nbalance, so as to better achieve its desired scheduling goals?\nThe obvious answer to this query is to move jobs around, a technique\nwhich we (once again) refer to as migration. By migrating a job from one\nCPU to another, true load balance can be achieved.\nLet’s look at a couple of examples to add some clarity. Once again, we\nhave a situation where one CPU is idle and the other has some jobs.\nQ0\nQ1\nB\nD\nIn this case, the desired migration is easy to understand: the OS should\nsimply move one of B or D to CPU 0. The result of this single job migra-\ntion is evenly balanced load and everyone is happy.\nA more tricky case arises in our earlier example, where A was left\nalone on CPU 0 and B and D were alternating on CPU 1:\nQ0\nA\nQ1\nB\nD\nIn this case, a single migration does not solve the problem. What\nwould you do in this case? The answer, alas, is continuous migration\nof one or more jobs. One possible solution is to keep switching jobs, as\nwe see in the following timeline. In the ﬁgure, ﬁrst A is alone on CPU 0,\nand B and D alternate on CPU 1. After a few time slices, B is moved to\ncompete with A on CPU 0, while D enjoys a few time slices alone on CPU\n1. And thus load is balanced:\nCPU 0\nCPU 1\nA\nA\nA\nA\nB\nA\nB\nA\nB\nB\nB\nB\nB\nD\nB\nD\nD\nD\nD\nD\nA\nD\nA\nD\n ... \n ... \nOf course, many other possible migration patterns exist. But now for\nthe tricky part: how should the system decide to enact such a migration?\nOne basic approach is to use a technique known as work stealing\n[FLR98]. With a work-stealing approach, a (source) queue that is low\non jobs will occasionally peek at another (target) queue, to see how full\nit is. If the target queue is (notably) more full than the source queue, the\nsource will “steal” one or more jobs from the target to help balance load.\nOf course, there is a natural tension in such an approach. If you look\naround at other queues too often, you will suffer from high overhead and\nhave trouble scaling, which was the entire purpose of implementing the\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2164,
      "extraction_method": "Direct"
    },
    {
      "page_number": 138,
      "chapter": null,
      "content": "102\nMULTIPROCESSOR SCHEDULING (ADVANCED)\nmultiple queue scheduling in the ﬁrst place! If, on the other hand, you\ndon’t look at other queues very often, you are in danger of suffering from\nsevere load balances. Finding the right threshold remains, as is common\nin system policy design, a black art.\n10.6\nLinux Multiprocessor Schedulers\nInterestingly, in the Linux community, no common solution has ap-\nproached to building a multiprocessor scheduler. Over time, three dif-\nferent schedulers arose: the O(1) scheduler, the Completely Fair Sched-\nuler (CFS), and the BF Scheduler (BFS)2. See Meehean’s dissertation for\nan excellent overview of the strengths and weaknesses of said schedulers\n[M11]; here we just summarize a few of the basics.\nBoth O(1) and CFS uses multiple queues, whereas BFS uses a single\nqueue, showing that both approaches can be successful. Of course, there\nare many other details which separate these schedulers. For example, the\nO(1) scheduler is a priority-based scheduler (similar to the MLFQ dis-\ncussed before), changing a process’s priority over time and then schedul-\ning those with highest priority in order to meet various scheduling objec-\ntives; interactivity is a particular focus. CFS, in contrast, is a deterministic\nproportional-share approach (more like Stride scheduling, as discussed\nearlier). BFS, the only single-queue approach among the three, is also\nproportional-share, but based on a more complicated scheme known as\nEarliest Eligible Virtual Deadline First (EEVDF) [SA96]. Read more about\nthese modern algorithms on your own; you should be able to understand\nhow they work now!\n10.7\nSummary\nWe have seen various approaches to multiprocessor scheduling. The\nsingle-queue approach (SQMS) is rather straightforward to build and bal-\nances load well but inherently has difﬁculty with scaling to many pro-\ncessors and cache afﬁnity. The multiple-queue approach (MQMS) scales\nbetter and handles cache afﬁnity well, but has trouble with load imbal-\nance and is more complicated. Whichever approach you take, there is no\nsimple answer: building a general purpose scheduler remains a daunting\ntask, as small code changes can lead to large behavioral differences. Only\nundertake such an exercise if you know exactly what you are doing, or,\nat least, are getting paid a large amount of money to do so.\n2Look up what BF stands for on your own; be forewarned, it is not for the faint of heart.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2474,
      "extraction_method": "Direct"
    },
    {
      "page_number": 139,
      "chapter": null,
      "content": "MULTIPROCESSOR SCHEDULING (ADVANCED)\n103\nReferences\n[A90] “The Performance of Spin Lock Alternatives for Shared-Memory Multiprocessors”\nThomas E. Anderson\nIEEE TPDS Volume 1:1, January 1990\nA classic paper on how different locking alternatives do and don’t scale. By Tom Anderson, very well\nknown researcher in both systems and networking. And author of a very ﬁne OS textbook, we must say.\n[B+10] “An Analysis of Linux Scalability to Many Cores Abstract”\nSilas Boyd-Wickizer, Austin T. Clements, Yandong Mao, Aleksey Pesterev, M. Frans Kaashoek,\nRobert Morris, Nickolai Zeldovich\nOSDI ’10, Vancouver, Canada, October 2010\nA terriﬁc modern paper on the difﬁculties of scaling Linux to many cores.\n[CSG99] “Parallel Computer Architecture: A Hardware/Software Approach”\nDavid E. Culler, Jaswinder Pal Singh, and Anoop Gupta\nMorgan Kaufmann, 1999\nA treasure ﬁlled with details about parallel machines and algorithms. As Mark Hill humorously ob-\nserves on the jacket, the book contains more information than most research papers.\n[FLR98] “The Implementation of the Cilk-5 Multithreaded Language”\nMatteo Frigo, Charles E. Leiserson, Keith Randall\nPLDI ’98, Montreal, Canada, June 1998\nCilk is a lightweight language and runtime for writing parallel programs, and an excellent example of\nthe work-stealing paradigm.\n[G83] “Using Cache Memory To Reduce Processor-Memory Trafﬁc”\nJames R. Goodman\nISCA ’83, Stockholm, Sweden, June 1983\nThe pioneering paper on how to use bus snooping, i.e., paying attention to requests you see on the bus, to\nbuild a cache coherence protocol. Goodman’s research over many years at Wisconsin is full of cleverness,\nthis being but one example.\n[M11] “Towards Transparent CPU Scheduling”\nJoseph T. Meehean\nDoctoral Dissertation at University of Wisconsin–Madison, 2011\nA dissertation that covers a lot of the details of how modern Linux multiprocessor scheduling works.\nPretty awesome! But, as co-advisors of Joe’s, we may be a bit biased here.\n[SHW11] “A Primer on Memory Consistency and Cache Coherence”\nDaniel J. Sorin, Mark D. Hill, and David A. Wood\nSynthesis Lectures in Computer Architecture\nMorgan and Claypool Publishers, May 2011\nA deﬁnitive overview of memory consistency and multiprocessor caching. Required reading for anyone\nwho likes to know way too much about a given topic.\n[SA96] “Earliest Eligible Virtual Deadline First: A Flexible and Accurate Mechanism for Pro-\nportional Share Resource Allocation”\nIon Stoica and Hussein Abdel-Wahab\nTechnical Report TR-95-22, Old Dominion University, 1996\nA tech report on this cool scheduling idea, from Ion Stoica, now a professor at U.C. Berkeley and world\nexpert in networking, distributed systems, and many other things.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2746,
      "extraction_method": "Direct"
    },
    {
      "page_number": 140,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 141,
      "chapter": null,
      "content": "11\nSummary Dialogue on CPU Virtualization\nProfessor: So, Student, did you learn anything?\nStudent: Well, Professor, that seems like a loaded question. I think you only\nwant me to say “yes.”\nProfessor: That’s true. But it’s also still an honest question. Come on, give a\nprofessor a break, will you?\nStudent: OK, OK. I think I did learn a few things. First, I learned a little about\nhow the OS virtualizes the CPU. There are a bunch of important mechanisms\nthat I had to understand to make sense of this: traps and trap handlers, timer\ninterrupts, and how the OS and the hardware have to carefully save and restore\nstate when switching between processes.\nProfessor: Good, good!\nStudent: All those interactions do seem a little complicated though; how can I\nlearn more?\nProfessor: Well, that’s a good question. I think there is no substitute for doing;\njust reading about these things doesn’t quite give you the proper sense. Do the\nclass projects and I bet by the end it will all kind of make sense.\nStudent: Sounds good. What else can I tell you?\nProfessor: Well, did you get some sense of the philosophy of the OS in your\nquest to understand its basic machinery?\nStudent: Hmm... I think so. It seems like the OS is fairly paranoid. It wants\nto make sure it stays in charge of the machine. While it wants a program to run\nas efﬁciently as possible (and hence the whole reasoning behind limited direct\nexecution), the OS also wants to be able to say “Ah! Not so fast my friend”\nin case of an errant or malicious process. Paranoia rules the day, and certainly\nkeeps the OS in charge of the machine. Perhaps that is why we think of the OS\nas a resource manager.\nProfessor: Yes indeed – sounds like you are starting to put it together! Nice.\nStudent: Thanks.\n105\n",
      "content_length": 1759,
      "extraction_method": "Direct"
    },
    {
      "page_number": 142,
      "chapter": null,
      "content": "106\nSUMMARY DIALOGUE ON CPU VIRTUALIZATION\nProfessor: And what about the policies on top of those mechanisms – any inter-\nesting lessons there?\nStudent: Some lessons to be learned there for sure. Perhaps a little obvious, but\nobvious can be good. Like the notion of bumping short jobs to the front of the\nqueue – I knew that was a good idea ever since the one time I was buying some\ngum at the store, and the guy in front of me had a credit card that wouldn’t work.\nHe was no short job, let me tell you.\nProfessor: That sounds oddly rude to that poor fellow. What else?\nStudent: Well, that you can build a smart scheduler that tries to be like SJF and\nRR all at once – that MLFQ was pretty neat. Building up a real scheduler seems\ndifﬁcult.\nProfessor: Indeed it is. That’s why there is still controversy to this day over\nwhich scheduler to use; see the Linux battles between CFS, BFS, and the O(1)\nscheduler, for example. And no, I will not spell out the full name of BFS.\nStudent: And I won’t ask you to! These policy battles seem like they could rage\nforever; is there really a right answer?\nProfessor: Probably not. After all, even our own metrics are at odds: if your\nscheduler is good at turnaround time, it’s bad at response time, and vice versa.\nAs Lampson said, perhaps the goal isn’t to ﬁnd the best solution, but rather to\navoid disaster.\nStudent: That’s a little depressing.\nProfessor: Good engineering can be that way. And it can also be uplifting!\nIt’s just your perspective on it, really. I personally think being pragmatic is a\ngood thing, and pragmatists realize that not all problems have clean and easy\nsolutions. Anything else that caught your fancy?\nStudent: I really liked the notion of gaming the scheduler; it seems like that\nmight be something to look into when I’m next running a job on Amazon’s EC2\nservice. Maybe I can steal some cycles from some other unsuspecting (and more\nimportantly, OS-ignorant) customer!\nProfessor: It looks like I might have created a monster! Professor Frankenstein\nis not what I’d like to be called, you know.\nStudent: But isn’t that the idea? To get us excited about something, so much so\nthat we look into it on our own? Lighting ﬁres and all that?\nProfessor: I guess so. But I didn’t think it would work!\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2309,
      "extraction_method": "Direct"
    },
    {
      "page_number": 143,
      "chapter": null,
      "content": "12\nA Dialogue on Memory Virtualization\nStudent: So, are we done with virtualization?\nProfessor: No!\nStudent: Hey, no reason to get so excited; I was just asking a question. Students\nare supposed to do that, right?\nProfessor: Well, professors do always say that, but really they mean this: ask\nquestions, if they are good questions, and you have actually put a little thought\ninto them.\nStudent: Well, that sure takes the wind out of my sails.\nProfessor: Mission accomplished. In any case, we are not nearly done with\nvirtualization! Rather, you have just seen how to virtualize the CPU, but really\nthere is a big monster waiting in the closet: memory. Virtualizing memory is\ncomplicated and requires us to understand many more intricate details about\nhow the hardware and OS interact.\nStudent: That sounds cool. Why is it so hard?\nProfessor: Well, there are a lot of details, and you have to keep them straight\nin your head to really develop a mental model of what is going on. We’ll start\nsimple, with very basic techniques like base/bounds, and slowly add complexity\nto tackle new challenges, including fun topics like TLBs and multi-level page\ntables. Eventually, we’ll be able to describe the workings of a fully-functional\nmodern virtual memory manager.\nStudent: Neat! Any tips for the poor student, inundated with all of this infor-\nmation and generally sleep-deprived?\nProfessor: For the sleep deprivation, that’s easy: sleep more (and party less).\nFor understanding virtual memory, start with this: every address generated\nby a user program is a virtual address. The OS is just providing an illusion\nto each process, speciﬁcally that it has its own large and private memory; with\nsome hardware help, the OS will turn these pretend virtual addresses into real\nphysical addresses, and thus be able to locate the desired information.\n107\n",
      "content_length": 1843,
      "extraction_method": "Direct"
    },
    {
      "page_number": 144,
      "chapter": null,
      "content": "108\nA DIALOGUE ON MEMORY VIRTUALIZATION\nStudent: OK, I think I can remember that... (to self) every address from a user\nprogram is virtual, every address from a user program is virtual, every ...\nProfessor: What are you mumbling about?\nStudent: Oh nothing.... (awkward pause) ... Anyway, why does the OS want\nto provide this illusion again?\nProfessor: Mostly ease of use: the OS will give each program the view that it\nhas a large contiguous address space to put its code and data into; thus, as a\nprogrammer, you never have to worry about things like “where should I store this\nvariable?” because the virtual address space of the program is large and has lots\nof room for that sort of thing. Life, for a programmer, becomes much more tricky\nif you have to worry about ﬁtting all of your code data into a small, crowded\nmemory.\nStudent: Why else?\nProfessor: Well, isolation and protection are big deals, too. We don’t want\none errant program to be able to read, or worse, overwrite, some other program’s\nmemory, do we?\nStudent: Probably not. Unless it’s a program written by someone you don’t\nlike.\nProfessor: Hmmm.... I think we might need to add a class on morals and ethics\nto your schedule for next semester. Perhaps OS class isn’t getting the right mes-\nsage across.\nStudent: Maybe we should. But remember, it’s not me who taught us that the\nproper OS response to errant process behavior is to kill the offending process!\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1474,
      "extraction_method": "Direct"
    },
    {
      "page_number": 145,
      "chapter": null,
      "content": "13\nThe Abstraction: Address Spaces\nIn the early days, building computer systems was easy. Why, you ask?\nBecause users didn’t expect much. It is those darned users with their\nexpectations of “ease of use”, “high performance”, “reliability”, etc., that\nreally have led to all these headaches. Next time you meet one of those\ncomputer users, thank them for all the problems they have caused.\n13.1\nEarly Systems\nFrom the perspective of memory, early machines didn’t provide much\nof an abstraction to users. Basically, the physical memory of the machine\nlooked something like what you see in Figure 13.1.\nThe OS was a set of routines (a library, really) that sat in memory (start-\ning at physical address 0 in this example), and there would be one run-\nning program (a process) that currently sat in physical memory (starting\nat physical address 64k in this example) and used the rest of memory.\nThere were few illusions here, and the user didn’t expect much from the\nOS. Life was sure easy for OS developers in those days, wasn’t it?\nmax\n64KB\n0KB\nCurrent Program\n(code, data, etc.)\nOperating System\n(code, data, etc.)\nFigure 13.1: Operating Systems: The Early Days\n109\n",
      "content_length": 1165,
      "extraction_method": "Direct"
    },
    {
      "page_number": 146,
      "chapter": null,
      "content": "110\nTHE ABSTRACTION: ADDRESS SPACES\n512KB\n448KB\n384KB\n320KB\n256KB\n192KB\n128KB\n64KB\n0KB\n(free)\n(free)\n(free)\n(free)\nOperating System\n(code, data, etc.)\nProcess A\n(code, data, etc.)\nProcess B\n(code, data, etc.)\nProcess C\n(code, data, etc.)\nFigure 13.2: Three Processes: Sharing Memory\n13.2\nMultiprogramming and Time Sharing\nAfter a time, because machines were expensive, people began to share\nmachines more effectively. Thus the era of multiprogramming was born\n[DV66], in which multiple processes were ready to run at a given time,\nand the OS would switch between them, for example when one decided\nto perform an I/O. Doing so increased the effective utilization of the\nCPU. Such increases in efﬁciency were particularly important in those\ndays where each machine cost hundreds of thousands or even millions of\ndollars (and you thought your Mac was expensive!).\nSoon enough, however, people began demanding more of machines,\nand the era of time sharing was born [S59, L60, M62, M83]. Speciﬁcally,\nmany realized the limitations of batch computing, particularly on pro-\ngrammers themselves [CV65], who were tired of long (and hence ineffec-\ntive) program-debug cycles. The notion of interactivity became impor-\ntant, as many users might be concurrently using a machine, each waiting\nfor (or hoping for) a timely response from their currently-executing tasks.\nOne way to implement time sharing would be to run one process for\na short while, giving it full access to all memory (as in Figure 13.1), then\nstop it, save all of its state to some kind of disk (including all of physical\nmemory), load some other process’s state, run it for a while, and thus\nimplement some kind of crude sharing of the machine [M+63].\nUnfortunately, this approach has a big problem: it is way too slow, par-\nticularly as memory grew. While saving and restoring register-level state\n(e.g., the PC, general-purpose registers, etc.) is relatively fast, saving the\nentire contents of memory to disk is brutally non-performant. Thus, what\nwe’d rather do is leave processes in memory while switching between\nthem, allowing the OS to implement time sharing efﬁciently (Figure 13.2).\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2197,
      "extraction_method": "Direct"
    },
    {
      "page_number": 147,
      "chapter": null,
      "content": "THE ABSTRACTION: ADDRESS SPACES\n111\n16KB\n15KB\n2KB\n1KB\n0KB\nStack\n(free)\nHeap\nProgram Code\nthe code segment:\nwhere instructions live\nthe heap segment:\ncontains malloc’d data\ndynamic data structures\n(it grows downward)\n(it grows upward)\nthe stack segment:\ncontains local variables\narguments to routines, \nreturn values, etc.\nFigure 13.3: An Example Address Space\nIn the diagram, there are three processes (A, B, and C) and each of\nthem have a small part of the 512-KB physical memory carved out for\nthem. Assuming a single CPU, the OS chooses to run one of the processes\n(say A), while the others (B and C) sit in the ready queue waiting to run.\nAs time sharing became more popular, you can probably guess that\nnew demands were placed on the operating system. In particular, allow-\ning multiple programs to reside concurrently in memory makes protec-\ntion an important issue; you don’t want a process to be able to read, or\nworse, write some other process’s memory.\n13.3\nThe Address Space\nHowever, we have to keep those pesky users in mind, and doing so\nrequires the OS to create an easy to use abstraction of physical memory.\nWe call this abstraction the address space, and it is the running program’s\nview of memory in the system. Understanding this fundamental OS ab-\nstraction of memory is key to understanding how memory is virtualized.\nThe address space of a process contains all of the memory state of the\nrunning program. For example, the code of the program (the instruc-\ntions) have to live in memory somewhere, and thus they are in the ad-\ndress space. The program, while it is running, uses a stack to keep track\nof where it is in the function call chain as well as to allocate local variables\nand pass parameters and return values to and from routines. Finally, the\nheap is used for dynamically-allocated, user-managed memory, such as\nthat you might receive from a call to malloc() in C or new in an object-\noriented language such as C++ or Java. Of course, there are other things\nin there too (e.g., statically-initialized variables), but for now let us just\nassume those three components: code, stack, and heap.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2165,
      "extraction_method": "Direct"
    },
    {
      "page_number": 148,
      "chapter": null,
      "content": "112\nTHE ABSTRACTION: ADDRESS SPACES\nIn the example in Figure 13.3, we have a tiny address space (only 16\nKB)1. The program code lives at the top of the address space (starting at\n0 in this example, and is packed into the ﬁrst 1K of the address space).\nCode is static (and thus easy to place in memory), so we can place it at\nthe top of the address space and know that it won’t need any more space\nas the program runs.\nNext, we have the two regions of the address space that may grow\n(and shrink) while the program runs. Those are the heap (at the top) and\nthe stack (at the bottom). We place them like this because each wishes to\nbe able to grow, and by putting them at opposite ends of the address\nspace, we can allow such growth: they just have to grow in opposite\ndirections. The heap thus starts just after the code (at 1KB) and grows\ndownward (say when a user requests more memory via malloc()); the\nstack starts at 16KB and grows upward (say when a user makes a proce-\ndure call). However, this placement of stack and heap is just a convention;\nyou could arrange the address space in a different way if you’d like (as\nwe’ll see later, when multiple threads co-exist in an address space, no\nnice way to divide the address space like this works anymore, alas).\nOf course, when we describe the address space, what we are describ-\ning is the abstraction that the OS is providing to the running program.\nThe program really isn’t in memory at physical addresses 0 through 16KB;\nrather it is loaded at some arbitrary physical address(es). Examine pro-\ncesses A, B, and C in Figure 13.2; there you can see how each process is\nloaded into memory at a different address. And hence the problem:\nTHE CRUX: HOW TO VIRTUALIZE MEMORY\nHow can the OS build this abstraction of a private, potentially large\naddress space for multiple running processes (all sharing memory) on\ntop of a single, physical memory?\nWhen the OS does this, we say the OS is virtualizing memory, because\nthe running program thinks it is loaded into memory at a particular ad-\ndress (say 0) and has a potentially very large address space (say 32-bits or\n64-bits); the reality is quite different.\nWhen, for example, process A in Figure 13.2 tries to perform a load\nat address 0 (which we will call a virtual address), somehow the OS, in\ntandem with some hardware support, will have to make sure the load\ndoesn’t actually go to physical address 0 but rather to physical address\n320KB (where A is loaded into memory). This is the key to virtualization\nof memory, which underlies every modern computer system in the world.\n1We will often use small examples like this because it is a pain to represent a 32-bit address\nspace and the numbers start to become hard to handle.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2777,
      "extraction_method": "Direct"
    },
    {
      "page_number": 149,
      "chapter": null,
      "content": "THE ABSTRACTION: ADDRESS SPACES\n113\nTIP: THE PRINCIPLE OF ISOLATION\nIsolation is a key principle in building reliable systems. If two entities are\nproperly isolated from one another, this implies that one can fail with-\nout affecting the other. Operating systems strive to isolate processes from\neach other and in this way prevent one from harming the other. By using\nmemory isolation, the OS further ensures that running programs cannot\naffect the operation of the underlying OS. Some modern OS’s take iso-\nlation even further, by walling off pieces of the OS from other pieces of\nthe OS. Such microkernels [BH70, R+89, S+03] thus may provide greater\nreliability than typical monolithic kernel designs.\n13.4\nGoals\nThus we arrive at the job of the OS in this set of notes: to virtualize\nmemory. The OS will not only virtualize memory, though; it will do so\nwith style. To make sure the OS does so, we need some goals to guide us.\nWe have seen these goals before (think of the Introduction), and we’ll see\nthem again, but they are certainly worth repeating.\nOne major goal of a virtual memory (VM) system is transparency2.\nThe OS should implement virtual memory in a way that is invisible to\nthe running program. Thus, the program shouldn’t be aware of the fact\nthat memory is virtualized; rather, the program behaves as if it has its\nown private physical memory. Behind the scenes, the OS (and hardware)\ndoes all the work to multiplex memory among many different jobs, and\nhence implements the illusion.\nAnother goal of VM is efﬁciency. The OS should strive to make the\nvirtualization as efﬁcient as possible, both in terms of time (i.e., not mak-\ning programs run much more slowly) and space (i.e., not using too much\nmemory for structures needed to support virtualization). In implement-\ning time-efﬁcient virtualization, the OS will have to rely on hardware\nsupport, including hardware features such as TLBs (which we will learn\nabout in due course).\nFinally, a third VM goal is protection. The OS should make sure to\nprotect processes from one another as well as the OS itself from pro-\ncesses. When one process performs a load, a store, or an instruction fetch,\nit should not be able to access or affect in any way the memory contents\nof any other process or the OS itself (that is, anything outside its address\nspace). Protection thus enables us to deliver the property of isolation\namong processes; each process should be running in its own isolated co-\ncoon, safe from the ravages of other faulty or even malicious processes.\n2This usage of transparency is sometimes confusing; some students think that “being\ntransparent” means keeping everything out in the open, i.e., what government should be like.\nHere, it means the opposite: that the illusion provided by the OS should not be visible to ap-\nplications. Thus, in common usage, a transparent system is one that is hard to notice, not one\nthat responds to requests as stipulated by the Freedom of Information Act.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 3017,
      "extraction_method": "Direct"
    },
    {
      "page_number": 150,
      "chapter": null,
      "content": "114\nTHE ABSTRACTION: ADDRESS SPACES\nASIDE: EVERY ADDRESS YOU SEE IS VIRTUAL\nEver write a C program that prints out a pointer? The value you see\n(some large number, often printed in hexadecimal), is a virtual address.\nEver wonder where the code of your program is found? You can print\nthat out too, and yes, if you can print it, it also is a virtual address. In\nfact, any address you can see as a programmer of a user-level program\nis a virtual address. It’s only the OS, through its tricky techniques of\nvirtualizing memory, that knows where in the physical memory of the\nmachine these instructions and data values lie. So never forget: if you\nprint out an address in a program, it’s a virtual one, an illusion of how\nthings are laid out in memory; only the OS (and the hardware) knows the\nreal truth.\nHere’s a little program that prints out the locations of the main() rou-\ntine (where code lives), the value of a heap-allocated value returned from\nmalloc(), and the location of an integer on the stack:\n1\n#include <stdio.h>\n2\n#include <stdlib.h>\n3\nint main(int argc, char *argv[]) {\n4\nprintf(\"location of code\n: %p\\n\", (void *) main);\n5\nprintf(\"location of heap\n: %p\\n\", (void *) malloc(1));\n6\nint x = 3;\n7\nprintf(\"location of stack : %p\\n\", (void *) &x);\n8\nreturn x;\n9\n}\nWhen run on a 64-bit Mac OS X machine, we get the following output:\nlocation of code\n: 0x1095afe50\nlocation of heap\n: 0x1096008c0\nlocation of stack : 0x7fff691aea64\nFrom this, you can see that code comes ﬁrst in the address space, then\nthe heap, and the stack is all the way at the other end of this large virtual\nspace. All of these addresses are virtual, and will be translated by the OS\nand hardware in order to fetch values from their true physical locations.\nIn the next chapters, we’ll focus our exploration on the basic mecha-\nnisms needed to virtualize memory, including hardware and operating\nsystems support. We’ll also investigate some of the more relevant poli-\ncies that you’ll encounter in operating systems, including how to manage\nfree space and which pages to kick out of memory when you run low on\nspace. In doing so, we’ll build up your understanding of how a modern\nvirtual memory system really works3.\n3Or, we’ll convince you to drop the course. But hold on; if you make it through VM, you’ll\nlikely make it all the way!\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2363,
      "extraction_method": "Direct"
    },
    {
      "page_number": 151,
      "chapter": null,
      "content": "THE ABSTRACTION: ADDRESS SPACES\n115\n13.5\nSummary\nWe have seen the introduction of a major OS subsystem: virtual mem-\nory. The VM system is responsible for providing the illusion of a large,\nsparse, private address space to programs, which hold all of their instruc-\ntions and data therein. The OS, with some serious hardware help, will\ntake each of these virtual memory references, and turn them into physi-\ncal addresses, which can be presented to the physical memory in order to\nfetch the desired information. The OS will do this for many processes at\nonce, making sure to protect programs from one another, as well as pro-\ntect the OS. The entire approach requires a great deal of mechanism (lots\nof low-level machinery) as well as some critical policies to work; we’ll\nstart from the bottom up, describing the critical mechanisms ﬁrst. And\nthus we proceed!\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 902,
      "extraction_method": "Direct"
    },
    {
      "page_number": 152,
      "chapter": null,
      "content": "116\nTHE ABSTRACTION: ADDRESS SPACES\nReferences\n[BH70] “The Nucleus of a Multiprogramming System”\nPer Brinch Hansen\nCommunications of the ACM, 13:4, April 1970\nThe ﬁrst paper to suggest that the OS, or kernel, should be a minimal and ﬂexible substrate for building\ncustomized operating systems; this theme is revisited throughout OS research history.\n[CV65] “Introduction and Overview of the Multics System”\nF. J. Corbato and V. A. Vyssotsky\nFall Joint Computer Conference, 1965\nA great early Multics paper. Here is the great quote about time sharing: “The impetus for time-sharing\nﬁrst arose from professional programmers because of their constant frustration in debugging programs\nat batch processing installations. Thus, the original goal was to time-share computers to allow simulta-\nneous access by several persons while giving to each of them the illusion of having the whole machine at\nhis disposal.”\n[DV66] “Programming Semantics for Multiprogrammed Computations”\nJack B. Dennis and Earl C. Van Horn\nCommunications of the ACM, Volume 9, Number 3, March 1966\nAn early paper (but not the ﬁrst) on multiprogramming.\n[L60] “Man-Computer Symbiosis”\nJ. C. R. Licklider\nIRE Transactions on Human Factors in Electronics, HFE-1:1, March 1960\nA funky paper about how computers and people are going to enter into a symbiotic age; clearly well\nahead of its time but a fascinating read nonetheless.\n[M62] “Time-Sharing Computer Systems”\nJ. McCarthy\nManagement and the Computer of the Future, MIT Press, Cambridge, Mass, 1962\nProbably McCarthy’s earliest recorded paper on time sharing. However, in another paper [M83], he\nclaims to have been thinking of the idea since 1957. McCarthy left the systems area and went on to be-\ncome a giant in Artiﬁcial Intelligence at Stanford, including the creation of the LISP programming lan-\nguage. See McCarthy’s home page for more info: http://www-formal.stanford.edu/jmc/\n[M+63] “A Time-Sharing Debugging System for a Small Computer”\nJ. McCarthy, S. Boilen, E. Fredkin, J. C. R. Licklider\nAFIPS ’63 (Spring), May, 1963, New York, USA\nA great early example of a system that swapped program memory to the “drum” when the program\nwasn’t running, and then back into “core” memory when it was about to be run.\n[M83] “Reminiscences on the History of Time Sharing”\nJohn McCarthy\nWinter or Spring of 1983\nAvailable: http://www-formal.stanford.edu/jmc/history/timesharing/timesharing.html\nA terriﬁc historical note on where the idea of time-sharing might have come from, including some doubts\ntowards those who cite Strachey’s work [S59] as the pioneering work in this area.\n[R+89] “Mach: A System Software kernel”\nRichard Rashid, Daniel Julin, Douglas Orr, Richard Sanzi, Robert Baron, Alessandro Forin,\nDavid Golub, Michael Jones\nCOMPCON 89, February 1989\nAlthough not the ﬁrst project on microkernels per se, the Mach project at CMU was well-known and\ninﬂuential; it still lives today deep in the bowels of Mac OS X.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2992,
      "extraction_method": "Direct"
    },
    {
      "page_number": 153,
      "chapter": null,
      "content": "THE ABSTRACTION: ADDRESS SPACES\n117\n[S59] “Time Sharing in Large Fast Computers”\nC. Strachey\nProceedings of the International Conference on Information Processing, UNESCO, June 1959\nOne of the earliest references on time sharing.\n[S+03] “Improving the Reliability of Commodity Operating Systems”\nMichael M. Swift, Brian N. Bershad, Henry M. Levy\nSOSP 2003\nThe ﬁrst paper to show how microkernel-like thinking can improve operating system reliability.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 492,
      "extraction_method": "Direct"
    },
    {
      "page_number": 154,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 155,
      "chapter": null,
      "content": "14\nInterlude: Memory API\nIn this interlude, we discuss the memory allocation interfaces in UNIX\nsystems. The interfaces provided are quite simple, and hence the chapter\nis short and to the point1. The main problem we address is this:\nCRUX: HOW TO ALLOCATE AND MANAGE MEMORY\nIn UNIX/C programs, understanding how to allocate and manage\nmemory is critical in building robust and reliable software. What inter-\nfaces are commonly used? What mistakes should be avoided?\n14.1\nTypes of Memory\nIn running a C program, there are two types of memory that are allo-\ncated. The ﬁrst is called stack memory, and allocations and deallocations\nof it are managed implicitly by the compiler for you, the programmer; for\nthis reason it is sometimes called automatic memory.\nDeclaring memory on the stack in C is easy. For example, let’s say you\nneed some space in a function func() for an integer, called x. To declare\nsuch a piece of memory, you just do something like this:\nvoid func() {\nint x; // declares an integer on the stack\n...\n}\nThe compiler does the rest, making sure to make space on the stack\nwhen you call into func(). When your return from the function, the\ncompiler deallocates the memory for you; thus, if you want some infor-\nmation to live beyond the call invocation, you had better not leave that\ninformation on the stack.\nIt is this need for long-lived memory that gets us to the second type\nof memory, called heap memory, where all allocations and deallocations\n1Indeed, we hope all chapters are! But this one is shorter and pointier, we think.\n119\n",
      "content_length": 1554,
      "extraction_method": "Direct"
    },
    {
      "page_number": 156,
      "chapter": null,
      "content": "120\nINTERLUDE: MEMORY API\nare explicitly handled by you, the programmer. A heavy responsibility,\nno doubt! And certainly the cause of many bugs. But if you are careful\nand pay attention, you will use such interfaces correctly and without too\nmuch trouble. Here is an example of how one might allocate a pointer to\nan integer on the heap:\nvoid func() {\nint *x = (int *) malloc(sizeof(int));\n...\n}\nA couple of notes about this small code snippet. First, you might no-\ntice that both stack and heap allocation occur on this line: ﬁrst the com-\npiler knows to make room for a pointer to an integer when it sees your\ndeclaration of said pointer (int *x); subsequently, when the program\ncalls malloc(), it requests space for an integer on the heap; the routine\nreturns the address of such an integer (upon success, or NULL on failure),\nwhich is then stored on the stack for use by the program.\nBecause of its explicit nature, and because of its more varied usage,\nheap memory presents more challenges to both users and systems. Thus,\nit is the focus of the remainder of our discussion.\n14.2\nThe malloc() Call\nThe malloc() call is quite simple: you pass it a size asking for some\nroom on the heap, and it either succeeds and gives you back a pointer to\nthe newly-allocated space, or fails and returns NULL2.\nThe manual page shows what you need to do to use malloc; type man\nmalloc at the command line and you will see:\n#include <stdlib.h>\n...\nvoid *malloc(size_t size);\nFrom this information, you can see that all you need to do is include\nthe header ﬁle stdlib.h to use malloc. In fact, you don’t really need to\neven do this, as the C library, which all C programs link with by default,\nhas the code for malloc() inside of it; adding the header just lets the\ncompiler check whether you are calling malloc() correctly (e.g., passing\nthe right number of arguments to it, of the right type).\nThe single parameter malloc() takes is of type size t which sim-\nply describes how many bytes you need. However, most programmers\ndo not type in a number here directly (such as 10); indeed, it would be\nconsidered poor form to do so. Instead, various routines and macros are\nutilized. For example, to allocate space for a double-precision ﬂoating\npoint value, you simply do this:\ndouble *d = (double *) malloc(sizeof(double));\n2Note that NULL in C isn’t really anything special at all, just a macro for the value zero.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2448,
      "extraction_method": "Direct"
    },
    {
      "page_number": 157,
      "chapter": null,
      "content": "INTERLUDE: MEMORY API\n121\nTIP: WHEN IN DOUBT, TRY IT OUT\nIf you aren’t sure how some routine or operator you are using behaves,\nthere is no substitute for simply trying it out and making sure it behaves\nas you expect. While reading the manual pages or other documentation\nis useful, how it works in practice is what matters. Write some code and\ntest it! That is no doubt the best way to make sure your code behaves as\nyou desire. Indeed, that is what we did to double-check the things we\nwere saying about sizeof() were actually true!\nWow, that’s lot of double-ing! This invocation of malloc() uses the\nsizeof() operator to request the right amount of space; in C, this is\ngenerally thought of as a compile-time operator, meaning that the actual\nsize is known at compile time and thus a number (in this case, 8, for a\ndouble) is substituted as the argument to malloc(). For this reason,\nsizeof() is correctly thought of as an operator and not a function call\n(a function call would take place at run time).\nYou can also pass in the name of a variable (and not just a type) to\nsizeof(), but in some cases you may not get the desired results, so be\ncareful. For example, let’s look at the following code snippet:\nint *x = malloc(10 * sizeof(int));\nprintf(\"%d\\n\", sizeof(x));\nIn the ﬁrst line, we’ve declared space for an array of 10 integers, which\nis ﬁne and dandy. However, when we use sizeof() in the next line,\nit returns a small value, such as 4 (on 32-bit machines) or 8 (on 64-bit\nmachines). The reason is that in this case, sizeof() thinks we are sim-\nply asking how big a pointer to an integer is, not how much memory we\nhave dynamically allocated. However, sometimes sizeof() does work\nas you might expect:\nint x[10];\nprintf(\"%d\\n\", sizeof(x));\nIn this case, there is enough static information for the compiler to\nknow that 40 bytes have been allocated.\nAnother place to be careful is with strings. When declaring space for a\nstring, use the following idiom: malloc(strlen(s) + 1), which gets\nthe length of the string using the function strlen(), and adds 1 to it\nin order to make room for the end-of-string character. Using sizeof()\nmay lead to trouble here.\nYou might also notice that malloc() returns a pointer to type void.\nDoing so is just the way in C to pass back an address and let the pro-\ngrammer decide what to do with it. The programmer further helps out\nby using what is called a cast; in our example above, the programmer\ncasts the return type of malloc() to a pointer to a double. Casting\ndoesn’t really accomplish anything, other than tell the compiler and other\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2628,
      "extraction_method": "Direct"
    },
    {
      "page_number": 158,
      "chapter": null,
      "content": "122\nINTERLUDE: MEMORY API\nprogrammers who might be reading your code: “yeah, I know what I’m\ndoing.” By casting the result of malloc(), the programmer is just giving\nsome reassurance; the cast is not needed for the correctness.\n14.3\nThe free() Call\nAs it turns out, allocating memory is the easy part of the equation;\nknowing when, how, and even if to free memory is the hard part. To free\nheap memory that is no longer in use, programmers simply call free():\nint *x = malloc(10 * sizeof(int));\n...\nfree(x);\nThe routine takes one argument, a pointer that was returned by malloc().\nThus, you might notice, the size of the allocated region is not passed in\nby the user, and must be tracked by the memory-allocation library itself.\n14.4\nCommon Errors\nThere are a number of common errors that arise in the use of malloc()\nand free(). Here are some we’ve seen over and over again in teaching\nthe undergraduate operating systems course. All of these examples com-\npile and run with nary a peep from the compiler; while compiling a C\nprogram is necessary to build a correct C program, it is far from sufﬁ-\ncient, as you will learn (often in the hard way).\nCorrect memory management has been such a problem, in fact, that\nmany newer languages have support for automatic memory manage-\nment. In such languages, while you call something akin to malloc()\nto allocate memory (usually new or something similar to allocate a new\nobject), you never have to call something to free space; rather, a garbage\ncollector runs and ﬁgures out what memory you no longer have refer-\nences to and frees it for you.\nForgetting To Allocate Memory\nMany routines expect memory to be allocated before you call them. For\nexample, the routine strcpy(dst, src) copies a string from a source\npointer to a destination pointer. However, if you are not careful, you\nmight do this:\nchar *src = \"hello\";\nchar *dst;\n// oops! unallocated\nstrcpy(dst, src); // segfault and die\nWhen you run this code, it will likely lead to a segmentation fault3,\nwhich is a fancy term for YOU DID SOMETHING WRONG WITH\nMEMORY YOU FOOLISH PROGRAMMER AND I AM ANGRY.\n3Although it sounds arcane, you will soon learn why such an illegal memory access is\ncalled a segmentation fault; if that isn’t incentive to read on, what is?\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2311,
      "extraction_method": "Direct"
    },
    {
      "page_number": 159,
      "chapter": null,
      "content": "INTERLUDE: MEMORY API\n123\nTIP: IT COMPILED OR IT RAN ̸= IT IS CORRECT\nJust because a program compiled(!) or even ran once or many times cor-\nrectly does not mean the program is correct. Many events may have con-\nspired to get you to a point where you believe it works, but then some-\nthing changes and it stops. A common student reaction is to say (or yell)\n“But it worked before!” and then blame the compiler, operating system,\nhardware, or even (dare we say it) the professor. But the problem is usu-\nally right where you think it would be, in your code. Get to work and\ndebug it before you blame those other components.\nIn this case, the proper code might instead look like this:\nchar *src = \"hello\";\nchar *dst = (char *) malloc(strlen(src) + 1);\nstrcpy(dst, src); // work properly\nAlternately, you could use strdup() and make your life even easier.\nRead the strdup man page for more information.\nNot Allocating Enough Memory\nA related error is not allocating enough memory, sometimes called a buffer\noverﬂow. In the example above, a common error is to make almost enough\nroom for the destination buffer.\nchar *src = \"hello\";\nchar *dst = (char *) malloc(strlen(src)); // too small!\nstrcpy(dst, src); // work properly\nOddly enough, depending on how malloc is implemented and many\nother details, this program will often run seemingly correctly. In some\ncases, when the string copy executes, it writes one byte too far past the\nend of the allocated space, but in some cases this is harmless, perhaps\noverwriting a variable that isn’t used anymore. In some cases, these over-\nﬂows can be incredibly harmful, and in fact are the source of many secu-\nrity vulnerabilities in systems [W06]. In other cases, the malloc library\nallocated a little extra space anyhow, and thus your program actually\ndoesn’t scribble on some other variable’s value and works quite ﬁne. In\neven other cases, the program will indeed fault and crash. And thus we\nlearn another valuable lesson: even though it ran correctly once, doesn’t\nmean it’s correct.\nForgetting to Initialize Allocated Memory\nWith this error, you call malloc() properly, but forget to ﬁll in some val-\nues into your newly-allocated data type. Don’t do this! If you do forget,\nyour program will eventually encounter an uninitialized read, where it\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2332,
      "extraction_method": "Direct"
    },
    {
      "page_number": 160,
      "chapter": null,
      "content": "124\nINTERLUDE: MEMORY API\nreads from the heap some data of unknown value. Who knows what\nmight be in there? If you’re lucky, some value such that the program still\nworks (e.g., zero). If you’re not lucky, something random and harmful.\nForgetting To Free Memory\nAnother common error is known as a memory leak, and it occurs when\nyou forget to free memory. In long-running applications or systems (such\nas the OS itself), this is a huge problem, as slowly leaking memory even-\ntually leads one to run out of memory, at which point a restart is required.\nThus, in general, when you are done with a chunk of memory, you should\nmake sure to free it. Note that using a garbage-collected language doesn’t\nhelp here: if you still have a reference to some chunk of memory, no\ngarbage collector will ever free it, and thus memory leaks remain a prob-\nlem even in more modern languages.\nNote that not all memory need be freed, at least, in certain cases. For\nexample, when you write a short-lived program, you might allocate some\nspace using malloc(). The program runs and is about to complete: is\nthere need to call free() a bunch of times just before exiting? While\nit seems wrong not to, it is in this case quite ﬁne to simply exit. After\nall, when your program exits, the OS will clean up everything about this\nprocess, including any memory it has allocated. Calling free() a bunch\nof times and then exiting is thus pointless, and, if you do so incorrectly,\nwill cause the program to crash. Just call exit and be happy instead.\nFreeing Memory Before You Are Done With It\nSometimes a program will free memory before it is ﬁnished using it; such\na mistake is called a dangling pointer, and it, as you can guess, is also a\nbad thing. The subsequent use can crash the program, or overwrite valid\nmemory (e.g., you called free(), but then called malloc() again to\nallocate something else, which then recycles the errantly-freed memory).\nFreeing Memory Repeatedly\nPrograms also sometimes free memory more than once; this is known as\nthe double free. The result of doing so is undeﬁned. As you can imag-\nine, the memory-allocation library might get confused and do all sorts of\nweird things; crashes are a common outcome.\nCalling free() Incorrectly\nOne last problem we discuss is the call of free() incorrectly. After all,\nfree() expects you only to pass to it one of the pointers you received\nfrom malloc() earlier. When you pass in some other value, bad things\ncan (and do) happen. Thus, such invalid frees are dangerous and of\ncourse should also be avoided.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2594,
      "extraction_method": "Direct"
    },
    {
      "page_number": 161,
      "chapter": null,
      "content": "INTERLUDE: MEMORY API\n125\nSummary\nAs you can see, there are lots of ways to abuse memory. Because of fre-\nquent errors with memory, a whole ecosphere of tools have developed to\nhelp ﬁnd such problems in your code. Check out both purify [HJ92] and\nvalgrind [SN05]; both are excellent at helping you locate the source of\nyour memory-related problems. Once you become accustomed to using\nthese powerful tools, you will wonder how you survived without them.\n14.5\nUnderlying OS Support\nYou might have noticed that we haven’t been talking about system\ncalls when discussing malloc() and free(). The reason for this is sim-\nple: they are not system calls, but rather library calls. Thus the malloc li-\nbrary manages space within your virtual address space, but itself is built\non top of some system calls which call into the OS to ask for more mem-\nory or release some back to the system.\nOne such system call is called brk, which is used to change the loca-\ntion of the program’s break: the location of the end of the heap. It takes\none argument (the address of the new break), and thus either increases or\ndecreases the size of the heap based on whether the new break is larger\nor smaller than the current break. An additional call sbrk is passed an\nincrement but otherwise serves a similar purpose.\nNote that you should never directly call either brk or sbrk. They\nare used by the memory-allocation library; if you try to use them, you\nwill likely make something go (horribly) wrong. Stick to malloc() and\nfree() instead.\nFinally, you can also obtain memory from the operating system via the\nmmap() call. By passing in the correct arguments, mmap() can create an\nanonymous memory region within your program – a region which is not\nassociated with any particular ﬁle but rather with swap space, something\nwe’ll discuss in detail later on in virtual memory. This memory can then\nalso be treated like a heap and managed as such. Read the manual page\nof mmap() for more details.\n14.6\nOther Calls\nThere are a few other calls that the memory-allocation library sup-\nports. For example, calloc() allocates memory and also zeroes it be-\nfore returning; this prevents some errors where you assume that memory\nis zeroed and forget to initialize it yourself (see the paragraph on “unini-\ntialized reads” above). The routine realloc() can also be useful, when\nyou’ve allocated space for something (say, an array), and then need to\nadd something to it: realloc() makes a new larger region of memory,\ncopies the old region into it, and returns the pointer to the new region.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2598,
      "extraction_method": "Direct"
    },
    {
      "page_number": 162,
      "chapter": null,
      "content": "126\nINTERLUDE: MEMORY API\n14.7\nSummary\nWe have introduced some of the APIs dealing with memory allocation.\nAs always, we have just covered the basics; more details are available\nelsewhere. Read the C book [KR88] and Stevens [SR05] (Chapter 7) for\nmore information. For a cool modern paper on how to detect and correct\nmany of these problems automatically, see Novark et al. [N+07]; this\npaper also contains a nice summary of common problems and some neat\nideas on how to ﬁnd and ﬁx them.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 535,
      "extraction_method": "Direct"
    },
    {
      "page_number": 163,
      "chapter": null,
      "content": "INTERLUDE: MEMORY API\n127\nReferences\n[HJ92] Purify: Fast Detection of Memory Leaks and Access Errors\nR. Hastings and B. Joyce\nUSENIX Winter ’92\nThe paper behind the cool Purify tool, now a commercial product.\n[KR88] “The C Programming Language”\nBrian Kernighan and Dennis Ritchie\nPrentice-Hall 1988\nThe C book, by the developers of C. Read it once, do some programming, then read it again, and then\nkeep it near your desk or wherever you program.\n[N+07] “Exterminator: Automatically Correcting Memory Errors with High Probability”\nGene Novark, Emery D. Berger, and Benjamin G. Zorn\nPLDI 2007\nA cool paper on ﬁnding and correcting memory errors automatically, and a great overview of many\ncommon errors in C and C++ programs.\n[SN05] “Using Valgrind to Detect Undeﬁned Value Errors with Bit-precision”\nJ. Seward and N. Nethercote\nUSENIX ’05\nHow to use valgrind to ﬁnd certain types of errors.\n[SR05] “Advanced Programming in the UNIX Environment”\nW. Richard Stevens and Stephen A. Rago\nAddison-Wesley, 2005\nWe’ve said it before, we’ll say it again: read this book many times and use it as a reference whenever you\nare in doubt. The authors are always surprised at how each time they read something in this book, they\nlearn something new, even after many years of C programming.\n[W06] “Survey on Buffer Overﬂow Attacks and Countermeasures”\nTim Werthman\nAvailable: www.nds.rub.de/lehre/seminar/SS06/Werthmann BufferOverﬂow.pdf\nA nice survey of buffer overﬂows and some of the security problems they cause. Refers to many of the\nfamous exploits.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 1582,
      "extraction_method": "Direct"
    },
    {
      "page_number": 164,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 165,
      "chapter": null,
      "content": "15\nMechanism: Address Translation\nIn developing the virtualization of the CPU, we focused on a general\nmechanism known as limited direct execution (or LDE). The idea be-\nhind LDE is simple: for the most part, let the program run directly on the\nhardware; however, at certain key points in time (such as when a process\nissues a system call, or a timer interrupt occurs), arrange so that the OS\ngets involved and makes sure the “right” thing happens. Thus, the OS,\nwith a little hardware support, tries its best to get out of the way of the\nrunning program, to deliver an efﬁcient virtualization; however, by inter-\nposing at those critical points in time, the OS ensures that it maintains\ncontrol over the hardware. Efﬁciency and control together are two of the\nmain goals of any modern operating system.\nIn virtualizing memory, we will pursue a similar strategy, attaining\nboth efﬁciency and control while providing the desired virtualization. Ef-\nﬁciency dictates that we make use of hardware support, which at ﬁrst\nwill be quite rudimentary (e.g., just a few registers) but will grow to be\nfairly complex (e.g., TLBs, page-table support, and so forth, as you will\nsee). Control implies that the OS ensures that no application is allowed\nto access any memory but its own; thus, to protect applications from one\nanother, and the OS from applications, we will need help from the hard-\nware here too. Finally, we will need a little more from the VM system, in\nterms of ﬂexibility; speciﬁcally, we’d like for programs to be able to use\ntheir address spaces in whatever way they would like, thus making the\nsystem easier to program. And thus arrive at the reﬁned crux:\nTHE CRUX:\nHOW TO EFFICIENTLY AND FLEXIBLY VIRTUALIZE MEMORY\nHow can we build an efﬁcient virtualization of memory? How do\nwe provide the ﬂexibility needed by applications? How do we maintain\ncontrol over which memory locations an application can access, and thus\nensure that application memory accesses are properly restricted? How\ndo we do all of this efﬁciently?\n129\n",
      "content_length": 2034,
      "extraction_method": "Direct"
    },
    {
      "page_number": 166,
      "chapter": null,
      "content": "130\nMECHANISM: ADDRESS TRANSLATION\nThe generic technique we will use, which you can consider an addition\nto our general approach of limited direct execution, is something that is\nreferred to as hardware-based address translation, or just address trans-\nlation for short. With address translation, the hardware transforms each\nmemory access (e.g., an instruction fetch, load, or store), changing the vir-\ntual address provided by the instruction to a physical address where the\ndesired information is actually located. Thus, on each and every memory\nreference, an address translation is performed by the hardware to redirect\napplication memory references to their actual locations in memory.\nOf course, the hardware alone cannot virtualize memory, as it just pro-\nvides the low-level mechanism for doing so efﬁciently. The OS must get\ninvolved at key points to set up the hardware so that the correct trans-\nlations take place; it must thus manage memory, keeping track of which\nlocations are free and which are in use, and judiciously intervening to\nmaintain control over how memory is used.\nOnce again the goal of all of this work is to create a beautiful illu-\nsion: that the program has its own private memory, where its own code\nand data reside. Behind that virtual reality lies the ugly physical truth:\nthat many programs are actually sharing memory at the same time, as\nthe CPU (or CPUs) switches between running one program and the next.\nThrough virtualization, the OS (with the hardware’s help) turns the ugly\nmachine reality into something that is a useful, powerful, and easy to use\nabstraction.\n15.1\nAssumptions\nOur ﬁrst attempts at virtualizing memory will be very simple, almost\nlaughably so. Go ahead, laugh all you want; pretty soon it will be the OS\nlaughing at you, when you try to understand the ins and outs of TLBs,\nmulti-level page tables, and other technical wonders. Don’t like the idea\nof the OS laughing at you? Well, you may be out of luck then; that’s just\nhow the OS rolls.\nSpeciﬁcally, we will assume for now that the user’s address space must\nbe placed contiguously in physical memory. We will also assume, for sim-\nplicity, that the size of the address space is not too big; speciﬁcally, that\nit is less than the size of physical memory. Finally, we will also assume that\neach address space is exactly the same size. Don’t worry if these assump-\ntions sound unrealistic; we will relax them as we go, thus achieving a\nrealistic virtualization of memory.\n15.2\nAn Example\nTo understand better what we need to do to implement address trans-\nlation, and why we need such a mechanism, let’s look at a simple exam-\nple. Imagine there is a process whose address space as indicated in Figure\n15.1. What we are going to examine here is a short code sequence that\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2831,
      "extraction_method": "Direct"
    },
    {
      "page_number": 167,
      "chapter": null,
      "content": "MECHANISM: ADDRESS TRANSLATION\n131\nTIP: INTERPOSITION IS POWERFUL\nInterposition is a generic and powerful technique that is often used to\ngreat effect in computer systems. In virtualizing memory, the hardware\nwill interpose on each memory access, and translate each virtual address\nissued by the process to a physical address where the desired informa-\ntion is actually stored. However, the general technique of interposition is\nmuch more broadly applicable; indeed, almost any well-deﬁned interface\ncan be interposed upon, to add new functionality or improve some other\naspect of the system. One of the usual beneﬁts of such an approach is\ntransparency; the interposition often is done without changing the client\nof the interface, thus requiring no changes to said client.\nloads a value from memory, increments it by three, and then stores the\nvalue back into memory. You can imagine the C-language representation\nof this code might look like this:\nvoid func()\nint x;\n...\nx = x + 3; // this is the line of code we are interested in\nThe compiler turns this line of code into assembly, which might look\nsomething like this (in x86 assembly). Use objdump on Linux or otool\non Mac OS X to disassemble it:\n128: movl 0x0(%ebx), %eax\n;load 0+ebx into eax\n132: addl $0x03, %eax\n;add 3 to eax register\n135: movl %eax, 0x0(%ebx)\n;store eax back to mem\nThis code snippet is relatively straightforward; it presumes that the\naddress of x has been placed in the register ebx, and then loads the value\nat that address into the general-purpose register eax using the movl in-\nstruction (for “longword” move). The next instruction adds 3 to eax,\nand the ﬁnal instruction stores the value in eax back into memory at that\nsame location.\nIn Figure 15.1, you can see how both the code and data are laid out in\nthe process’s address space; the three-instruction code sequence is located\nat address 128 (in the code section near the top), and the value of the\nvariable x at address 15 KB (in the stack near the bottom). In the ﬁgure,\nthe initial value of x is 3000, as shown in its location on the stack.\nWhen these instructions run, from the perspective of the process, the\nfollowing memory accesses take place.\n• Fetch instruction at address 128\n• Execute this instruction (load from address 15 KB)\n• Fetch instruction at address 132\n• Execute this instruction (no memory reference)\n• Fetch the instruction at address 135\n• Execute this instruction (store to address 15 KB)\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2496,
      "extraction_method": "Direct"
    },
    {
      "page_number": 168,
      "chapter": null,
      "content": "132\nMECHANISM: ADDRESS TRANSLATION\n16KB\n15KB\n14KB\n4KB\n3KB\n2KB\n1KB\n0KB\nStack\n(free)\nHeap\nProgram Code\n128\n132\n135\nmovl 0x0(%ebx),%eax\naddl 0x03, %eax\nmovl %eax,0x0(%ebx)\n3000\nFigure 15.1: A Process And Its Address Space\nFrom the program’s perspective, its address space starts at address 0\nand grows to a maximum of 16 KB; all memory references it generates\nshould be within these bounds. However, to virtualize memory, the OS\nwants to place the process somewhere else in physical memory, not nec-\nessarily at address 0. Thus, we have the problem: how can we relocate\nthis process in memory in a way that is transparent to the process? How\ncan provide the illusion of a virtual address space starting at 0, when in\nreality the address space is located at some other physical address?\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 830,
      "extraction_method": "Direct"
    },
    {
      "page_number": 169,
      "chapter": null,
      "content": "MECHANISM: ADDRESS TRANSLATION\n133\n64KB\n48KB\n32KB\n16KB\n0KB\n(not in use)\n(not in use)\nOperating System\nStack\nCode\nHeap\n(allocated but not in use)\nRelocated Process\nFigure 15.2: Physical Memory with a Single Relocated Process\nAn example of what physical memory might look like once this pro-\ncess’s address space has been placed in memory is found in Figure 15.2.\nIn the ﬁgure, you can see the OS using the ﬁrst slot of physical memory\nfor itself, and that it has relocated the process from the example above\ninto the slot starting at physical memory address 32 KB. The other two\nslots are free (16 KB-32 KB and 48 KB-64 KB).\n15.3\nDynamic (Hardware-based) Relocation\nTo gain some understanding of hardware-based address translation,\nwe’ll ﬁrst discuss its ﬁrst incarnation. Introduced in the ﬁrst time-sharing\nmachines of the late 1950’s is a simple idea referred to as base and bounds\n(the technique is also referred to as dynamic relocation; we’ll use both\nterms interchangeably) [SS74].\nSpeciﬁcally, we’ll need two hardware registers within each CPU: one\nis called the base register, and the other the bounds (sometimes called a\nlimit register). This base-and-bounds pair is going to allow us to place the\naddress space anywhere we’d like in physical memory, and do so while\nensuring that the process can only access its own address space.\nIn this setup, each program is written and compiled as if it is loaded at\naddress zero. However, when a program starts running, the OS decides\nwhere in physical memory it should be loaded and sets the base register\nto that value. In the example above, the OS decides to load the process at\nphysical address 32 KB and thus sets the base register to this value.\nInteresting things start to happen when the process is running. Now,\nwhen any memory reference is generated by the process, it is translated\nby the processor in the following manner:\nphysical address = virtual address + base\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 1967,
      "extraction_method": "Direct"
    },
    {
      "page_number": 170,
      "chapter": null,
      "content": "134\nMECHANISM: ADDRESS TRANSLATION\nASIDE: SOFTWARE-BASED RELOCATION\nIn the early days, before hardware support arose, some systems per-\nformed a crude form of relocation purely via software methods.\nThe\nbasic technique is referred to as static relocation, in which a piece of soft-\nware known as the loader takes an executable that is about to be run and\nrewrites its addresses to the desired offset in physical memory.\nFor example, if an instruction was a load from address 1000 into a reg-\nister (e.g., movl 1000, %eax), and the address space of the program\nwas loaded starting at address 3000 (and not 0, as the program thinks),\nthe loader would rewrite the instruction to offset each address by 3000\n(e.g., movl 4000, %eax). In this way, a simple static relocation of the\nprocess’s address space is achieved.\nHowever, static relocation has numerous problems. First and most im-\nportantly, it does not provide protection, as processes can generate bad\naddresses and thus illegally access other process’s or even OS memory; in\ngeneral, hardware support is likely needed for true protection [WL+93].\nA smaller negative is that once placed, it is difﬁcult to later relocate an\naddress space to another location [M65].\nEach memory reference generated by the process is a virtual address;\nthe hardware in turn adds the contents of the base register to this address\nand the result is a physical address that can be issued to the memory\nsystem.\nTo understand this better, let’s trace through what happens when a\nsingle instruction is executed. Speciﬁcally, let’s look at one instruction\nfrom our earlier sequence:\n128: movl 0x0(%ebx), %eax\nThe program counter (PC) is set to 128; when the hardware needs to\nfetch this instruction, it ﬁrst adds the value to the the base register value\nof 32 KB (32768) to get a physical address of 32896; the hardware then\nfetches the instruction from that physical address. Next, the processor\nbegins executing the instruction. At some point, the process then issues\nthe load from virtual address 15 KB, which the processor takes and again\nadds to the base register (32 KB), getting the ﬁnal physical address of\n47 KB and thus the desired contents.\nTransforming a virtual address into a physical address is exactly the\ntechnique we refer to as address translation; that is, the hardware takes a\nvirtual address the process thinks it is referencing and transforms it into\na physical address which is where the data actually resides. Because this\nrelocation of the address happens at runtime, and because we can move\naddress spaces even after the process has started running, the technique\nis often referred to as dynamic relocation [M65].\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2715,
      "extraction_method": "Direct"
    },
    {
      "page_number": 171,
      "chapter": null,
      "content": "MECHANISM: ADDRESS TRANSLATION\n135\nTIP: HARDWARE-BASED DYNAMIC RELOCATION\nWith dynamic relocation, we can see how a little hardware goes a long\nway. Namely, a base register is used to transform virtual addresses (gen-\nerated by the program) into physical addresses. A bounds (or limit) reg-\nister ensures that such addresses are within the conﬁnes of the address\nspace. Together, they combine to provide a simple and efﬁcient virtual-\nization of memory.\nNow you might be asking: what happened to that bounds (limit) reg-\nister? After all, isn’t this supposed to be the base-and-bounds approach?\nIndeed, it is. And as you might have guessed, the bounds register is there\nto help with protection. Speciﬁcally, the processor will ﬁrst check that\nthe memory reference is within bounds to make sure it is legal; in the sim-\nple example above, the bounds register would always be set to 16 KB. If\na process generates a virtual address that is greater than the bounds, or\none that is negative, the CPU will raise an exception, and the process will\nlikely be terminated. The point of the bounds is thus to make sure that all\naddresses generated by the process are legal and within the “bounds” of\nthe process.\nWe should note that the base and bounds registers are hardware struc-\ntures kept on the chip (one pair per CPU). Sometimes people call the\npart of the processor that helps with address translation the memory\nmanagement unit (MMU); as we develop more sophisticated memory-\nmanagement techniques, we will be adding more circuitry to the MMU.\nA small aside about bound registers, which can be deﬁned in one of\ntwo ways. In one way (as above), it holds the size of the address space,\nand thus the hardware checks the virtual address against it ﬁrst before\nadding the base. In the second way, it holds the physical address of the\nend of the address space, and thus the hardware ﬁrst adds the base and\nthen makes sure the address is within bounds. Both methods are logically\nequivalent; for simplicity, we’ll usually assume that the bounds register\nholds the size of the address space.\nExample Translations\nTo understand address translation via base-and-bounds in more detail,\nlet’s take a look at an example. Imagine a process with an address space of\nsize 4 KB (yes, unrealistically small) has been loaded at physical address\n16 KB. Here are the results of a number of address translations:\n• Virtual Address 0 →Physical Address 16 KB\n• VA 1 KB →PA 17 KB\n• VA 3000 →PA 19384\n• VA 4400 →Fault (out of bounds)\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2547,
      "extraction_method": "Direct"
    },
    {
      "page_number": 172,
      "chapter": null,
      "content": "136\nMECHANISM: ADDRESS TRANSLATION\nASIDE: DATA STRUCTURE – THE FREE LIST\nThe OS must track which parts of free memory are not in use, so as to\nbe able to allocate memory to processes. Many different data structures\ncan of course be used for such a task; the simplest (which we will assume\nhere) is a free list, which simply is a list of the ranges of the physical\nmemory which are not currently in use.\nAs you can see from the example, it is easy for you to simply add the\nbase address to the virtual address (which can rightly be viewed as an\noffset into the address space) to get the resulting physical address. Only if\nthe virtual address is “too big” or negative will the result be a fault (e.g.,\n4400 is greater than the 4 KB bounds), causing an exception to be raised\nand the process to be terminated.\n15.4\nOS Issues\nThere are a number of new OS issues that arise when using base and\nbounds to implement a simple virtual memory. Speciﬁcally, there are\nthree critical junctures where the OS must take action to implement this\nbase-and-bounds approach to virtualizing memory.\nFirst, The OS must take action when a process is created, ﬁnding space\nfor its address space in memory. Fortunately, given our assumptions that\neach address space is (a) smaller than the size of physical memory and\n(b) the same size, this is quite easy for the OS; it can simply view physical\nmemory as an array of slots, and track whether each one is free or in use.\nWhen a new process is created, the OS will have to search a data structure\n(often called a free list) to ﬁnd room for the new address space and then\nmark it used.\nAn example of what physical memory might look like can be found\nin Figure 15.2. In the ﬁgure, you can see the OS using the ﬁrst slot of\nphysical memory for itself, and that it has relocated the process from the\nexample above into the slot starting at physical memory address 32 KB.\nThe other two slots are free (16 KB-32 KB and 48 KB-64 KB); thus, the free\nlist should consist of these two entries.\nSecond, the OS must take action when a process is terminated, reclaim-\ning all of its memory for use in other processes or the OS. Upon termina-\ntion of a process, the OS thus puts its memory back on the free list, and\ncleans up any associated data structures as need be.\nThird, the OS must also take action when a context switch occurs.\nThere is only one base and bounds register on each CPU, after all, and\ntheir values differ for each running program, as each program is loaded at\na different physical address in memory. Thus, the OS must save and restore\nthe base-and-bounds pair when it switches between processes. Speciﬁ-\ncally, when the OS decides to stop running a process, it must save the\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2756,
      "extraction_method": "Direct"
    },
    {
      "page_number": 173,
      "chapter": null,
      "content": "MECHANISM: ADDRESS TRANSLATION\n137\nvalues of the base and bounds registers to memory, in some per-process\nstructure such as the process structure or process control block (PCB).\nSimilarly, when the OS resumes a running process (or runs it the ﬁrst\ntime), it must set the values of the base and bounds on the CPU to the\ncorrect values for this process.\nWe should note that when a process is stopped (i.e., not running), it is\npossible for the OS to move an address space from one location in mem-\nory to another rather easily. To move a process’s address space, the OS\nﬁrst deschedules the process; then, the OS copies the address space from\nthe current location to the new location; ﬁnally, the OS updates the saved\nbase register (in the process structure) to point to the new location. When\nthe process is resumed, its (new) base register is restored, and it begins\nrunning again, oblivious that its instructions and data are now in a com-\npletely new spot in memory!\nWe should also note that access to the base and bounds registers is ob-\nviously privileged. Special hardware instructions are required to access\nbase-and-bounds registers; if a process, running in user mode, attempts\nto do so, the CPU will raise an exception and the OS will likely termi-\nnate the process. Only in kernel (or privileged) mode can such registers\nbe modiﬁed. Imagine the havoc a user process could wreak1 if it could\narbitrarily change the base register while running. Imagine it! And then\nquickly ﬂush such dark thoughts from your mind, as they are the ghastly\nstuff of which nightmares are made.\n15.5\nSummary\nIn this chapter, we have extended the concept of limited direct exe-\ncution with a speciﬁc mechanism used in virtual memory, known as ad-\ndress translation. With address translation, the OS can control each and\nevery memory access from a process, ensuring the accesses stay within\nthe bounds of the address space. Key to the efﬁciency of this technique\nis hardware support, which performs the translation quickly for each ac-\ncess, turning virtual addresses (the process’s view of memory) into phys-\nical ones (the actual view). All of this is performed in a way that is trans-\nparent to the process that has been relocated; the process has no idea its\nmemory references are being translated, making for a wonderful illusion.\nWe have also seen one particular form of virtualization, known as base\nand bounds or dynamic relocation. Base-and-bounds virtualization is\nquite efﬁcient, as only a little more hardware logic is required to add a\nbase register to the virtual address and check that the address generated\nby the process is in bounds. Base-and-bounds also offers protection; the\nOS and hardware combine to ensure no process can generate memory\nreferences outside its own address space. Protection is certainly one of\nthe most important goals of the OS; without it, the OS could not control\n1Is there anything other than “havoc” that can be “wreaked”?\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2994,
      "extraction_method": "Direct"
    },
    {
      "page_number": 174,
      "chapter": null,
      "content": "138\nMECHANISM: ADDRESS TRANSLATION\nthe machine (if processes were free to overwrite memory, they could eas-\nily do nasty things like overwrite the trap table and take over the system).\nUnfortunately, this simple technique of dynamic relocation does have\nits inefﬁciencies. For example, as you can see in Figure 15.2 (back a few\npages), the relocated process is using physical memory from 32 KB to\n48 KB; however, because the process stack and heap are not too big, all of\nthe space between the two is simply wasted. This type of waste is usually\ncalled internal fragmentation, as the space inside the allocated unit is not\nall used (i.e., is fragmented) and thus wasted. In our current approach, al-\nthough there might be enough physical memory for more processes, we\nare currently restricted to placing an address space in a ﬁxed-sized slot\nand thus internal fragmentation can arise2. Thus, we are going to need\nmore sophisticated machinery, to try to better utilize physical memory\nand avoid internal fragmentation. Our ﬁrst attempt will be a slight gen-\neralization of base and bounds known as segmentation, which we will\ndiscuss next.\n2A different solution might instead place a ﬁxed-sized stack within the address space,\njust below the code region, and a growing heap below that. However, this limits ﬂexibility\nby making recursion and deeply-nested function calls challenging, and thus is something we\nhope to avoid.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1470,
      "extraction_method": "Direct"
    },
    {
      "page_number": 175,
      "chapter": null,
      "content": "MECHANISM: ADDRESS TRANSLATION\n139\nReferences\n[M65] “On Dynamic Program Relocation”\nW.C. McGee\nIBM Systems Journal\nVolume 4, Number 3, 1965, pages 184–199\nThis paper is a nice summary of early work on dynamic relocation, as well as some basics on static\nrelocation.\n[P90] “Relocating loader for MS-DOS .EXE executable ﬁles”\nKenneth D. A. Pillay\nMicroprocessors & Microsystems archive\nVolume 14, Issue 7 (September 1990)\nAn example of a relocating loader for MS-DOS. Not the ﬁrst one, but just a relatively modern example\nof how such a system works.\n[SS74] “The Protection of Information in Computer Systems”\nJ. Saltzer and M. Schroeder\nCACM, July 1974\nFrom this paper: “The concepts of base-and-bound register and hardware-interpreted descriptors ap-\npeared, apparently independently, between 1957 and 1959 on three projects with diverse goals. At\nM.I.T., McCarthy suggested the base-and-bound idea as part of the memory protection system nec-\nessary to make time-sharing feasible. IBM independently developed the base-and-bound register as a\nmechanism to permit reliable multiprogramming of the Stretch (7030) computer system. At Burroughs,\nR. Barton suggested that hardware-interpreted descriptors would provide direct support for the naming\nscope rules of higher level languages in the B5000 computer system.” We found this quote on Mark\nSmotherman’s cool history pages [S04]; see them for more information.\n[S04] “System Call Support”\nMark Smotherman, May 2004\nhttp://people.cs.clemson.edu/˜mark/syscall.html\nA neat history of system call support. Smotherman has also collected some early history on items like\ninterrupts and other fun aspects of computing history. See his web pages for more details.\n[WL+93] “Efﬁcient Software-based Fault Isolation”\nRobert Wahbe, Steven Lucco, Thomas E. Anderson, Susan L. Graham\nSOSP ’93\nA terriﬁc paper about how you can use compiler support to bound memory references from a program,\nwithout hardware support. The paper sparked renewed interest in software techniques for isolation of\nmemory references.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2088,
      "extraction_method": "Direct"
    },
    {
      "page_number": 176,
      "chapter": null,
      "content": "140\nMECHANISM: ADDRESS TRANSLATION\nHomework\nThe program relocation.py allows you to see how address trans-\nlations are performed in a system with base and bounds registers. See the\nREADME for details.\nQuestions\n• Run with seeds 1, 2, and 3, and compute whether each virtual ad-\ndress generated by the process is in or out of bounds. If in bounds,\ncompute the translation.\n• Run with these ﬂags: -s 0 -n 10. What value do you have set\n-l (the bounds register) to in order to ensure that all the generated\nvirtual addresses are within bounds?\n• Run with these ﬂags: -s 1 -n 10 -l 100. What is the maxi-\nmum value that bounds can be set to, such that the address space\nstill ﬁts into physical memory in its entirety?\n• Run some of the same problems above, but with larger address\nspaces (-a) and physical memories (-p).\n• What fraction of randomly-generated virtual addresses are valid,\nas a function of the value of the bounds register? Make a graph\nfrom running with different random seeds, with limit values rang-\ning from 0 up to the maximum size of the address space.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1117,
      "extraction_method": "Direct"
    },
    {
      "page_number": 177,
      "chapter": null,
      "content": "16\nSegmentation\nSo far we have been putting the entire address space of each process in\nmemory. With the base and bounds registers, the OS can easily relocate\nprocesses to different parts of physical memory. However, you might\nhave noticed something interesting about these address spaces of ours:\nthere is a big chunk of “free” space right in the middle, between the stack\nand the heap.\nAs you can imagine from Figure 16.1, although the space between the\nstack and heap is not being used by the process, it is still taking up phys-\nical memory when we relocate the entire address space somewhere in\nphysical memory; thus, the simple approach of using a base and bounds\nregister pair to virtualize memory is wasteful. It also makes it quite hard\nto run a program when the entire address space doesn’t ﬁt into memory;\nthus, base and bounds is not as ﬂexible as we would like. And thus:\nTHE CRUX: HOW TO SUPPORT A LARGE ADDRESS SPACE\nHow do we support a large address space with (potentially) a lot of\nfree space between the stack and the heap? Note that in our examples,\nwith tiny (pretend) address spaces, the waste doesn’t seem too bad. Imag-\nine, however, a 32-bit address space (4 GB in size); a typical program will\nonly use megabytes of memory, but still would demand that the entire\naddress space be resident in memory.\n16.1\nSegmentation: Generalized Base/Bounds\nTo solve this problem, an idea was born, and it is called segmenta-\ntion. It is quite an old idea, going at least as far back as the very early\n1960’s [H61, G62]. The idea is simple: instead of having just one base\nand bounds pair in our MMU, why not have a base and bounds pair per\nlogical segment of the address space? A segment is just a contiguous\nportion of the address space of a particular length, and in our canonical\n141\n",
      "content_length": 1799,
      "extraction_method": "Direct"
    },
    {
      "page_number": 178,
      "chapter": null,
      "content": "142\nSEGMENTATION\n16KB\n15KB\n14KB\n6KB\n5KB\n4KB\n3KB\n2KB\n1KB\n0KB\nProgram Code\nHeap\n(free)\nStack\nFigure 16.1: An Address Space (Again)\naddress space, we have three logically-different segments: code, stack,\nand heap. What segmentation allows the OS to do is to place each one\nof those segments in different parts of physical memory, and thus avoid\nﬁlling physical memory with unused virtual address space.\nLet’s look at an example. Assume we want to place the address space\nfrom Figure 16.1 into physical memory. With a base and bounds pair per\nsegment, we can place each segment independently in physical memory.\nFor example, see Figure 16.2; there you see a 64-KB physical memory\nwith those three segments within it (and 16KB reserved for the OS).\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 791,
      "extraction_method": "Direct"
    },
    {
      "page_number": 179,
      "chapter": null,
      "content": "SEGMENTATION\n143\n64KB\n48KB\n32KB\n16KB\n0KB\n(not in use)\n(not in use)\n(not in use)\nOperating System\nStack\nCode\nHeap\nFigure 16.2: Placing Segments In Physical Memory\nAs you can see in the diagram, only used memory is allocated space\nin physical memory, and thus large address spaces with large amounts of\nunused address space (which we sometimes call sparse address spaces)\ncan be accommodated.\nThe hardware structure in our MMU required to support segmenta-\ntion is just what you’d expect: in this case, a set of three base and bounds\nregister pairs. Table 16.1 below shows the register values for the example\nabove; each bounds register holds the size of a segment.\nSegment\nBase\nSize\nCode\n32K\n2K\nHeap\n34K\n2K\nStack\n28K\n2K\nTable 16.1: Segment Register Values\nYou can see from the table that the code segment is placed at physical\naddress 32KB and has a size of 2KB and the heap segment is placed at\n34KB and also has a size of 2KB.\nLet’s do an example translation, using the address space in Figure 16.1.\nAssume a reference is made to virtual address 100 (which is in the code\nsegment). When the reference takes place (say, on an instruction fetch),\nthe hardware will add the base value to the offset into this segment (100 in\nthis case) to arrive at the desired physical address: 100 + 32KB, or 32868.\nIt will then check that the address is within bounds (100 is less than 2KB),\nﬁnd that it is, and issue the reference to physical memory address 32868.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 1491,
      "extraction_method": "Direct"
    },
    {
      "page_number": 180,
      "chapter": null,
      "content": "144\nSEGMENTATION\nASIDE: THE SEGMENTATION FAULT\nThe term segmentation fault or violation arises from a memory access\non a segmented machine to an illegal address. Humorously, the term\npersists, even on machines with no support for segmentation at all. Or\nnot so humorously, if you can’t ﬁgure why your code keeps faulting.\nNow let’s look at an address in the heap, virtual address 4200 (again\nrefer to Figure 16.1). If we just add the virtual address 4200 to the base\nof the heap (34KB), we get a physical address of 39016, which is not the\ncorrect physical address. What we need to ﬁrst do is extract the offset into\nthe heap, i.e., which byte(s) in this segment the address refers to. Because\nthe heap starts at virtual address 4KB (4096), the offset of 4200 is actually\n4200 – 4096 or 104. We then take this offset (104) and add it to the base\nregister physical address (34K or 34816) to get the desired result: 34920.\nWhat if we tried to refer to an illegal address, such as 7KB which is be-\nyond the end of the heap? You can imagine what will happen: the hard-\nware detects that the address is out of bounds, traps into the OS, likely\nleading to the termination of the offending process. And now you know\nthe origin of the famous term that all C programmers learn to dread: the\nsegmentation violation or segmentation fault.\n16.2\nWhich Segment Are We Referring To?\nThe hardware uses segment registers during translation. How does it\nknow the offset into a segment, and to which segment an address refers?\nOne common approach, sometimes referred to as an explicit approach,\nis to chop up the address space into segments based on the top few bits\nof the virtual address; this technique was used in the VAX/VMS system\n[LL82]. In our example above, we have three segments; thus we need two\nbits to accomplish our task. If we use the top two bits of our 14-bit virtual\naddress to select the segment, our virtual address looks like this:\n13 12 11 10\n9\n8\n7\n6\n5\n4\n3\n2\n1\n0\nSegment\nOffset\nIn our example, then, if the top two bits are 00, the hardware knows\nthe virtual address is in the code segment, and thus uses the code base\nand bounds pair to relocate the address to the correct physical location.\nIf the top two bits are 01, the hardware knows the address is in the heap,\nand thus uses the heap base and bounds. Let’s take our example heap\nvirtual address from above (4200) and translate it, just to make sure this\nis clear. The virtual address 4200, in binary form, can be seen here:\n13\n0\n12\n1\n11\n0\n10\n0\n9\n0\n8\n0\n7\n0\n6\n1\n5\n1\n4\n0\n3\n1\n2\n0\n1\n0\n0\n0\nSegment\nOffset\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2607,
      "extraction_method": "Direct"
    },
    {
      "page_number": 181,
      "chapter": null,
      "content": "SEGMENTATION\n145\nAs you can see from the picture, the top two bits (01) tell the hardware\nwhich segment we are referring to. The bottom 12 bits are the offset into\nthe segment: 0000 0110 1000, or hex 0x068, or 104 in decimal. Thus, the\nhardware simply takes the ﬁrst two bits to determine which segment reg-\nister to use, and then takes the next 12 bits as the offset into the segment.\nBy adding the base register to the offset, the hardware arrives at the ﬁ-\nnal physical address. Note the offset eases the bounds check too: we can\nsimply check if the offset is less than the bounds; if not, the address is ille-\ngal. Thus, if base and bounds were arrays (with one entry per segment),\nthe hardware would be doing something like this to obtain the desired\nphysical address:\n1\n// get top 2 bits of 14-bit VA\n2\nSegment = (VirtualAddress & SEG_MASK) >> SEG_SHIFT\n3\n// now get offset\n4\nOffset\n= VirtualAddress & OFFSET_MASK\n5\nif (Offset >= Bounds[Segment])\n6\nRaiseException(PROTECTION_FAULT)\n7\nelse\n8\nPhysAddr = Base[Segment] + Offset\n9\nRegister = AccessMemory(PhysAddr)\nIn our running example, we can ﬁll in values for the constants above.\nSpeciﬁcally, SEG MASK would be set to 0x3000, SEG SHIFT to 12, and\nOFFSET MASK to 0xFFF.\nYou may also have noticed that when we use the top two bits, and we\nonly have three segments (code, heap, stack), one segment of the address\nspace goes unused. Thus, some systems put code in the same segment as\nthe heap and thus use only one bit to select which segment to use [LL82].\nThere are other ways for the hardware to determine which segment\na particular address is in. In the implicit approach, the hardware deter-\nmines the segment by noticing how the address was formed. If, for ex-\nample, the address was generated from the program counter (i.e., it was\nan instruction fetch), then the address is within the code segment; if the\naddress is based off of the stack or base pointer, it must be in the stack\nsegment; any other address must be in the heap.\n16.3\nWhat About The Stack?\nThus far, we’ve left out one important component of the address space:\nthe stack. The stack has been relocated to physical address 28KB in the di-\nagram above, but with one critical difference: it grows backwards – in phys-\nical memory, it starts at 28KB and grows back to 26KB, corresponding to\nvirtual addresses 16KB to 14KB; translation has to proceed differently.\nThe ﬁrst thing we need is a little extra hardware support. Instead of\njust base and bounds values, the hardware also needs to know which way\nthe segment grows (a bit, for example, that is set to 1 when the segment\ngrows in the positive direction, and 0 for negative). Our updated view of\nwhat the hardware tracks is seen in Table 16.2.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2762,
      "extraction_method": "Direct"
    },
    {
      "page_number": 182,
      "chapter": null,
      "content": "146\nSEGMENTATION\nSegment\nBase\nSize\nGrows Positive?\nCode\n32K\n2K\n1\nHeap\n34K\n2K\n1\nStack\n28K\n2K\n0\nTable 16.2: Segment Registers (With Negative-Growth Support)\nWith the hardware understanding that segments can grow in the neg-\native direction, the hardware must now translate such virtual addresses\nslightly differently. Let’s take an example stack virtual address and trans-\nlate it to understand the process.\nIn this example, assume we wish to access virtual address 15KB, which\nshould map to physical address 27KB. Our virtual address, in binary\nform, thus looks like this: 11 1100 0000 0000 (hex 0x3C00). The hard-\nware uses the top two bits (11) to designate the segment, but then we\nare left with an offset of 3KB. To obtain the correct negative offset, we\nmust subtract the maximum segment size from 3KB: in this example, a\nsegment can be 4KB, and thus the correct negative offset is 3KB - 4KB\nwhich equals -1KB. We simply add the negative offset (-1KB) to the base\n(28KB) to arrive at the correct physical address: 27KB. The bounds check\ncan be calculated by ensuring the absolute value of the negative offset is\nless than the segment’s size.\n16.4\nSupport for Sharing\nAs support for segmentation grew, system designers soon realized that\nthey could realize new types of efﬁciencies with a little more hardware\nsupport. Speciﬁcally, to save memory, sometimes it is useful to share\ncertain memory segments between address spaces. In particular, code\nsharing is common and still in use in systems today.\nTo support sharing, we need a little extra support from the hardware,\nin the form of protection bits. Basic support adds a few bits per segment,\nindicating whether or not a program can read or write a segment, or per-\nhaps execute code that lies within the segment. By setting a code segment\nto read-only, the same code can be shared across multiple processes, with-\nout worry of harming isolation; while each process still thinks that it is ac-\ncessing its own private memory, the OS is secretly sharing memory which\ncannot be modiﬁed by the process, and thus the illusion is preserved.\nAn example of the additional information tracked by the hardware\n(and OS) is shown in Figure 16.3. As you can see, the code segment is\nset to read and execute, and thus the same physical segment in memory\ncould be mapped into multiple virtual address spaces.\nWith protection bits, the hardware algorithm described earlier would\nalso have to change. In addition to checking whether a virtual address is\nwithin bounds, the hardware also has to check whether a particular ac-\ncess is permissible. If a user process tries to write to a read-only page, or\nexecute from a non-executable page, the hardware should raise an excep-\ntion, and thus let the OS deal with the offending process.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2821,
      "extraction_method": "Direct"
    },
    {
      "page_number": 183,
      "chapter": null,
      "content": "SEGMENTATION\n147\nSegment\nBase\nSize\nGrows Positive?\nProtection\nCode\n32K\n2K\n1\nRead-Execute\nHeap\n34K\n2K\n1\nRead-Write\nStack\n28K\n2K\n0\nRead-Write\nTable 16.3: Segment Register Values (with Protection)\n16.5\nFine-grained vs. Coarse-grained Segmentation\nMost of our examples thus far have focused on systems with just a\nfew segments (i.e., code, stack, heap); we can think of this segmentation\nas coarse-grained, as it chops up the address space into relatively large,\ncoarse chunks. However, some early systems (e.g., Multics [CV65,DD68])\nwere more ﬂexible and allowed for address spaces to consist of a large\nnumber smaller segments, referred to as ﬁne-grained segmentation.\nSupporting many segments requires even further hardware support,\nwith a segment table of some kind stored in memory. Such segment ta-\nbles usually support the creation of a very large number of segments, and\nthus enable a system to use segments in more ﬂexible ways than we have\nthus far discussed. For example, early machines like the Burroughs B5000\nhad support for thousands of segments, and expected a compiler to chop\ncode and data into separate segments which the OS and hardware would\nthen support [RK68]. The thinking at the time was that by having ﬁne-\ngrained segments, the OS could better learn about which segments are in\nuse and which are not and thus utilize main memory more effectively.\n16.6\nOS Support\nYou now should have a basic idea as to how segmentation works.\nPieces of the address space are relocated into physical memory as the\nsystem runs, and thus a huge savings of physical memory is achieved\nrelative to our simpler approach with just a single base/bounds pair for\nthe entire address space. Speciﬁcally, all the unused space between the\nstack and the heap need not be allocated in physical memory, allowing\nus to ﬁt more address spaces into physical memory.\nHowever, segmentation raises a number of new issues. We’ll ﬁrst de-\nscribe the new OS issues that must be addressed. The ﬁrst is an old one:\nwhat should the OS do on a context switch? You should have a good\nguess by now: the segment registers must be saved and restored. Clearly,\neach process has its own virtual address space, and the OS must make\nsure to set up these registers correctly before letting the process run again.\nThe second, and more important, issue is managing free space in phys-\nical memory. When a new address space is created, the OS has to be\nable to ﬁnd space in physical memory for its segments. Previously, we\nassumed that each address space was the same size, and thus physical\nmemory could be thought of as a bunch of slots where processes would\nﬁt in. Now, we have a number of segments per process, and each segment\nmight be a different size.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2765,
      "extraction_method": "Direct"
    },
    {
      "page_number": 184,
      "chapter": null,
      "content": "148\nSEGMENTATION\n64KB\n56KB\n48KB\n40KB\n32KB\n24KB\n16KB\n8KB\n0KB\nOperating System\nNot Compacted\n(not in use)\n(not in use)\n(not in use)\nAllocated\nAllocated\nAllocated\n64KB\n56KB\n48KB\n40KB\n32KB\n24KB\n16KB\n8KB\n0KB\n(not in use)\nAllocated\nOperating System\nCompacted\nFigure 16.3: Non-compacted and Compacted Memory\nThe general problem that arises is that physical memory quickly be-\ncomes full of little holes of free space, making it difﬁcult to allocate new\nsegments, or to grow existing ones. We call this problem external frag-\nmentation [R69]; see Figure 16.3 (left).\nIn the example, a process comes along and wishes to allocate a 20KB\nsegment. In that example, there is 24KB free, but not in one contiguous\nsegment (rather, in three non-contiguous chunks). Thus, the OS cannot\nsatisfy the 20KB request.\nOne solution to this problem would be to compact physical memory\nby rearranging the existing segments. For example, the OS could stop\nwhichever processes are running, copy their data to one contiguous re-\ngion of memory, change their segment register values to point to the\nnew physical locations, and thus have a large free extent of memory with\nwhich to work. By doing so, the OS enables the new allocation request\nto succeed. However, compaction is expensive, as copying segments is\nmemory-intensive and thus would use a fair amount of processor time.\nSee Figure 16.3 (right) for a diagram of compacted physical memory.\nA simpler approach is to use a free-list management algorithm that\ntries to keep large extents of memory available for allocation. There are\nliterally hundreds of approaches that people have taken, including clas-\nsic algorithms like best-ﬁt (which keeps a list of free spaces and returns\nthe one closest in size that satisﬁes the desired allocation to the requester),\nworst-ﬁt, ﬁrst-ﬁt, and more complex schemes like buddy algorithm [K68].\nAn excellent survey by Wilson et al. is a good place to start if you want to\nlearn more about such algorithms [W+95], or you can wait until we cover\nsome of the basics ourselves in a later chapter. Unfortunately, though, no\nmatter how smart the algorithm, external fragmentation will still exist;\nthus, a good algorithm simply attempts to minimize it.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2258,
      "extraction_method": "Direct"
    },
    {
      "page_number": 185,
      "chapter": null,
      "content": "SEGMENTATION\n149\nTIP: IF 1000 SOLUTIONS EXIST, NO GREAT ONE DOES\nThe fact that so many different algorithms exist to try to minimize exter-\nnal fragmentation is indicative of a stronger underlying truth: there is no\none “best” way to solve the problem. Thus, we settle for something rea-\nsonable and hope it is good enough. The only real solution (as we will\nsee in forthcoming chapters) is to avoid the problem altogether, by never\nallocating memory in variable-sized chunks.\n16.7\nSummary\nSegmentation solves a number of problems, and helps us build a more\neffective virtualization of memory. Beyond just dynamic relocation, seg-\nmentation can better support sparse address spaces, by avoiding the huge\npotential waste of memory between logical segments of the address space.\nIt is also fast, as doing the arithmetic segmentation requires in hardware\nis easy and well-suited to hardware; the overheads of translation are min-\nimal. A fringe beneﬁt arises too: code sharing. If code is placed within\na separate segment, such a segment could potentially be shared across\nmultiple running programs.\nHowever, as we learned, allocating variable-sized segments in mem-\nory leads to some problems that we’d like to overcome. The ﬁrst, as dis-\ncussed above, is external fragmentation. Because segments are variable-\nsized, free memory gets chopped up into odd-sized pieces, and thus sat-\nisfying a memory-allocation request can be difﬁcult. One can try to use\nsmart algorithms [W+95] or periodically compact memory, but the prob-\nlem is fundamental and hard to avoid.\nThe second and perhaps more important problem is that segmentation\nstill isn’t ﬂexible enough to support our fully generalized, sparse address\nspace. For example, if we have a large but sparsely-used heap all in one\nlogical segment, the entire heap must still reside in memory in order to be\naccessed. In other words, if our model of how the address space is being\nused doesn’t exactly match how the underlying segmentation has been\ndesigned to support it, segmentation doesn’t work very well. We thus\nneed to ﬁnd some new solutions. Ready to ﬁnd them?\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2155,
      "extraction_method": "Direct"
    },
    {
      "page_number": 186,
      "chapter": null,
      "content": "150\nSEGMENTATION\nReferences\n[CV65] “Introduction and Overview of the Multics System”\nF. J. Corbato and V. A. Vyssotsky\nFall Joint Computer Conference, 1965\nOne of ﬁve papers presented on Multics at the Fall Joint Computer Conference; oh to be a ﬂy on the wall\nin that room that day!\n[DD68] “Virtual Memory, Processes, and Sharing in Multics”\nRobert C. Daley and Jack B. Dennis\nCommunications of the ACM, Volume 11, Issue 5, May 1968\nAn early paper on how to perform dynamic linking in Multics, which was way ahead of its time. Dy-\nnamic linking ﬁnally found its way back into systems about 20 years later, as the large X-windows\nlibraries demanded it. Some say that these large X11 libraries were MIT’s revenge for removing support\nfor dynamic linking in early versions of UNIX!\n[G62] “Fact Segmentation”\nM. N. Greenﬁeld\nProceedings of the SJCC, Volume 21, May 1962\nAnother early paper on segmentation; so early that it has no references to other work.\n[H61] “Program Organization and Record Keeping for Dynamic Storage”\nA. W. Holt\nCommunications of the ACM, Volume 4, Issue 10, October 1961\nAn incredibly early and difﬁcult to read paper about segmentation and some of its uses.\n[I09] “Intel 64 and IA-32 Architectures Software Developer’s Manuals”\nIntel, 2009\nAvailable: http://www.intel.com/products/processor/manuals\nTry reading about segmentation in here (Chapter 3 in Volume 3a); it’ll hurt your head, at least a little\nbit.\n[K68] “The Art of Computer Programming: Volume I”\nDonald Knuth\nAddison-Wesley, 1968\nKnuth is famous not only for his early books on the Art of Computer Programming but for his typeset-\nting system TeX which is still a powerhouse typesetting tool used by professionals today, and indeed to\ntypeset this very book. His tomes on algorithms are a great early reference to many of the algorithms\nthat underly computing systems today.\n[L83] “Hints for Computer Systems Design”\nButler Lampson\nACM Operating Systems Review, 15:5, October 1983\nA treasure-trove of sage advice on how to build systems. Hard to read in one sitting; take it in a little at\na time, like a ﬁne wine, or a reference manual.\n[LL82] “Virtual Memory Management in the VAX/VMS Operating System”\nHenry M. Levy and Peter H. Lipman\nIEEE Computer, Volume 15, Number 3 (March 1982)\nA classic memory management system, with lots of common sense in its design. We’ll study it in more\ndetail in a later chapter.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2446,
      "extraction_method": "Direct"
    },
    {
      "page_number": 187,
      "chapter": null,
      "content": "SEGMENTATION\n151\n[RK68] “Dynamic Storage Allocation Systems”\nB. Randell and C.J. Kuehner\nCommunications of the ACM\nVolume 11(5), pages 297-306, May 1968\nA nice overview of the differences between paging and segmentation, with some historical discussion of\nvarious machines.\n[R69] “A note on storage fragmentation and program segmentation”\nBrian Randell\nCommunications of the ACM\nVolume 12(7), pages 365-372, July 1969\nOne of the earliest papers to discuss fragmentation.\n[W+95] “Dynamic Storage Allocation: A Survey and Critical Review”\nPaul R. Wilson, Mark S. Johnstone, Michael Neely, and David Boles\nIn International Workshop on Memory Management\nScotland, United Kingdom, September 1995\nA great survey paper on memory allocators.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 775,
      "extraction_method": "Direct"
    },
    {
      "page_number": 188,
      "chapter": null,
      "content": "152\nSEGMENTATION\nHomework\nThis program allows you to see how address translations are performed\nin a system with segmentation. See the README for details.\nQuestions\n• First let’s use a tiny address space to translate some addresses. Here’s\na simple set of parameters with a few different random seeds; can\nyou translate the addresses?\nsegmentation.py -a 128 -p 512 -b 0 -l 20 -B 512 -L 20 -s 0\nsegmentation.py -a 128 -p 512 -b 0 -l 20 -B 512 -L 20 -s 1\nsegmentation.py -a 128 -p 512 -b 0 -l 20 -B 512 -L 20 -s 2\n• Now, let’s see if we understand this tiny address space we’ve con-\nstructed (using the parameters from the question above). What is\nthe highest legal virtual address in segment 0? What about the low-\nest legal virtual address in segment 1? What are the lowest and\nhighest illegal addresses in this entire address space? Finally, how\nwould you run segmentation.py with the -A ﬂag to test if you\nare right?\n• Let’s say we have a tiny 16-byte address space in a 128-byte physical\nmemory. What base and bounds would you set up so as to get\nthe simulator to generate the following translation results for the\nspeciﬁed address stream: valid, valid, violation, ..., violation, valid,\nvalid? Assume the following parameters:\nsegmentation.py -a 16 -p 128\n-A 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15\n--b0 ? --l0 ? --b1 ? --l1 ?\n• Assuming we want to generate a problem where roughly 90% of the\nrandomly-generated virtual addresses are valid (i.e., not segmenta-\ntion violations). How should you conﬁgure the simulator to do so?\nWhich parameters are important?\n• Can you run the simulator such that no virtual addresses are valid?\nHow?\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1683,
      "extraction_method": "Direct"
    },
    {
      "page_number": 189,
      "chapter": null,
      "content": "17\nFree-Space Management\nIn this chapter, we take a small detour from our discussion of virtual-\nizing memory to discuss a fundamental aspect of any memory manage-\nment system, whether it be a malloc library (managing pages of a pro-\ncess’s heap) or the OS itself (managing portions of the address space of a\nprocess). Speciﬁcally, we will discuss the issues surrounding free-space\nmanagement.\nLet us make the problem more speciﬁc. Managing free space can cer-\ntainly be easy, as we will see when we discuss the concept of paging. It is\neasy when the space you are managing is divided into ﬁxed-sized units;\nin such a case, you just keep a list of these ﬁxed-sized units; when a client\nrequests one of them, return the ﬁrst entry.\nWhere free-space management becomes more difﬁcult (and interest-\ning) is when the free space you are managing consists of variable-sized\nunits; this arises in a user-level memory-allocation library (as in malloc()\nand free()) and in an OS managing physical memory when using seg-\nmentation to implement virtual memory. In either case, the problem that\nexists is known as external fragmentation: the free space gets chopped\ninto little pieces of different sizes and is thus fragmented; subsequent re-\nquests may fail because there is no single contiguous space that can sat-\nisfy the request, even though the total amount of free space exceeds the\nsize of the request.\nfree\nused\nfree\n0\n10\n20\n30\nThe ﬁgure shows an example of this problem. In this case, the total\nfree space available is 20 bytes; unfortunately, it is fragmented into two\nchunks of size 10 each. As a result, a request for 15 bytes will fail even\nthough there are 20 bytes free. And thus we arrive at the problem ad-\ndressed in this chapter.\n153\n",
      "content_length": 1742,
      "extraction_method": "Direct"
    },
    {
      "page_number": 190,
      "chapter": null,
      "content": "154\nFREE-SPACE MANAGEMENT\nCRUX: HOW TO MANAGE FREE SPACE\nHow should free space be managed, when satisfying variable-sized re-\nquests? What strategies can be used to minimize fragmentation? What\nare the time and space overheads of alternate approaches?\n17.1\nAssumptions\nMost of this discussion will focus on the great history of allocators\nfound in user-level memory-allocation libraries. We draw on Wilson’s\nexcellent survey [W+95] but encourage interested readers to go to the\nsource document itself for more details1.\nWe assume a basic interface such as that provided by malloc() and\nfree(). Speciﬁcally, void *malloc(size t size) takes a single pa-\nrameter, size, which is the number of bytes requested by the applica-\ntion; it hands back a pointer (of no particular type, or a void pointer in\nC lingo) to a region of that size (or greater). The complementary routine\nvoid free(void *ptr) takes a pointer and frees the corresponding\nchunk. Note the implication of the interface: the user, when freeing the\nspace, does not inform the library of its size; thus, the library must be able\nto ﬁgure out how big a chunk of memory is when handed just a pointer\nto it. We’ll discuss how to do this a bit later on in the chapter.\nThe space that this library manages is known historically as the heap,\nand the generic data structure used to manage free space in the heap is\nsome kind of free list. This structure contains references to all of the free\nchunks of space in the managed region of memory. Of course, this data\nstructure need not be a list per se, but just some kind of data structure to\ntrack free space.\nWe further assume that primarily we are concerned with external frag-\nmentation, as described above. Allocators could of course also have the\nproblem of internal fragmentation; if an allocator hands out chunks of\nmemory bigger than that requested, any unasked for (and thus unused)\nspace in such a chunk is considered internal fragmentation (because the\nwaste occurs inside the allocated unit) and is another example of space\nwaste. However, for the sake of simplicity, and because it is the more in-\nteresting of the two types of fragmentation, we’ll mostly focus on external\nfragmentation.\nWe’ll also assume that once memory is handed out to a client, it cannot\nbe relocated to another location in memory. For example, if a program\ncalls malloc() and is given a pointer to some space within the heap,\nthat memory region is essentially “owned” by the program (and cannot\nbe moved by the library) until the program returns it via a correspond-\ning call to free(). Thus, no compaction of free space is possible, which\n1It is nearly 80 pages long; thus, you really have to be interested!\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2743,
      "extraction_method": "Direct"
    },
    {
      "page_number": 191,
      "chapter": null,
      "content": "FREE-SPACE MANAGEMENT\n155\nwould be useful to combat fragmentation2. Compaction could, however,\nbe used in the OS to deal with fragmentation when implementing seg-\nmentation; see the chapter on segmentation for details.\nFinally, we’ll assume that the allocator manages a contiguous region\nof bytes. In some cases, an allocator could ask for that region to grow;\nfor example, a user-level memory-allocation library might call into the\nkernel to grow the heap (via a system call such as sbrk) when it runs out\nof space. However, for simplicity, we’ll just assume that the region is a\nsingle ﬁxed size throughout its life.\n17.2\nLow-level Mechanisms\nBefore delving into some policy details, we’ll ﬁrst cover some com-\nmon mechanisms used in most allocators. First, we’ll discuss the basics of\nsplitting and coalescing, common techniques in most any allocator. Sec-\nond, we’ll show how one can track the size of allocated regions quickly\nand with relative ease. Finally, we’ll discuss how to build a simple list\ninside the free space to keep track of what is free and what isn’t.\nSplitting and Coalescing\nA free list contains a set of elements that describe the free space still re-\nmaining in the heap. Thus, assume the following 30-byte heap:\nfree\nused\nfree\n0\n10\n20\n30\nThe free list for this heap would have two elements on it. One entry de-\nscribes the ﬁrst 10-byte free segment (bytes 0-9), and one entry describes\nthe other free segment (bytes 20-29):\nhead\naddr:0\nlen:10\naddr:20\nlen:10\nNULL\nAs described above, a request for anything greater than 10 bytes will\nfail (returning NULL); there just isn’t a single contiguous chunk of mem-\nory of that size available. A request for exactly that size (10 bytes) could\nbe satisﬁed easily by either of the free chunks. But what happens if the\nrequest is for something smaller than 10 bytes?\nAssume we have a request for just a single byte of memory. In this\ncase, the allocator will perform an action known as splitting: it will ﬁnd\n2Once you hand a pointer to a chunk of memory to a C program, it is generally difﬁcult\nto determine all references (pointers) to that region, which may be stored in other variables\nor even in registers at a given point in execution. This may not be the case in more strongly-\ntyped, garbage-collected languages, which would thus enable compaction as a technique to\ncombat fragmentation.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2402,
      "extraction_method": "Direct"
    },
    {
      "page_number": 192,
      "chapter": null,
      "content": "156\nFREE-SPACE MANAGEMENT\na free chunk of memory that can satisfy the request and split it into two.\nThe ﬁrst chunk it will return to the caller; the second chunk will remain\non the list. Thus, in our example above, if a request for 1 byte were made,\nand the allocator decided to use the second of the two elements on the list\nto satisfy the request, the call to malloc() would return 20 (the address of\nthe 1-byte allocated region) and the list would end up looking like this:\nhead\naddr:0\nlen:10\naddr:21\nlen:9\nNULL\nIn the picture, you can see the list basically stays intact; the only change\nis that the free region now starts at 21 instead of 20, and the length of that\nfree region is now just 93. Thus, the split is commonly used in allocators\nwhen requests are smaller than the size of any particular free chunk.\nA corollary mechanism found in many allocators is known as coalesc-\ning of free space. Take our example from above once more (free 10 bytes,\nused 10 bytes, and another free 10 bytes).\nGiven this (tiny) heap, what happens when an application calls free(10),\nthus returning the space in the middle of the heap? If we simply add this\nfree space back into our list without too much thinking, we might end up\nwith a list that looks like this:\nhead\naddr:10\nlen:10\naddr:0\nlen:10\naddr:20\nlen:10\nNULL\nNote the problem: while the entire heap is now free, it is seemingly\ndivided into three chunks of 10 bytes each. Thus, if a user requests 20\nbytes, a simple list traversal will not ﬁnd such a free chunk, and return\nfailure.\nWhat allocators do in order to avoid this problem is coalesce free space\nwhen a chunk of memory is freed. The idea is simple: when returning a\nfree chunk in memory, look carefully at the addresses of the chunk you\nare returning as well as the nearby chunks of free space; if the newly-\nfreed space sits right next to one (or two, as in this example) existing free\nchunks, merge them into a single larger free chunk. Thus, with coalesc-\ning, our ﬁnal list should look like this:\nhead\naddr:0\nlen:30\nNULL\nIndeed, this is what the heap list looked like at ﬁrst, before any allo-\ncations were made. With coalescing, an allocator can better ensure that\nlarge free extents are available for the application.\n3This discussion assumes that there are no headers, an unrealistic but simplifying assump-\ntion we make for now.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2394,
      "extraction_method": "Direct"
    },
    {
      "page_number": 193,
      "chapter": null,
      "content": "FREE-SPACE MANAGEMENT\n157\nptr\nThe header used by malloc library\nThe 20 bytes returned to caller\nFigure 17.1: An Allocated Region Plus Header\nsize:\n20\nmagic: 1234567\nhptr\nptr\nThe 20 bytes returned to caller\nFigure 17.2: Speciﬁc Contents Of The Header\nTracking The Size Of Allocated Regions\nYou might have noticed that the interface to free(void *ptr) does\nnot take a size parameter; thus it is assumed that given a pointer, the\nmalloc library can quickly determine the size of the region of memory\nbeing freed and thus incorporate the space back into the free list.\nTo accomplish this task, most allocators store a little bit of extra infor-\nmation in a header block which is kept in memory, usually just before\nthe handed-out chunk of memory. Let’s look at an example again (Fig-\nure 17.1). In this example, we are examining an allocated block of size 20\nbytes, pointed to by ptr; imagine the user called malloc() and stored\nthe results in ptr, e.g., ptr = malloc(20);.\nThe header minimally contains the size of the allocated region (in this\ncase, 20); it may also contain additional pointers to speed up dealloca-\ntion, a magic number to provide additional integrity checking, and other\ninformation. Let’s assume a simple header which contains the size of the\nregion and a magic number, like this:\ntypedef struct __header_t {\nint size;\nint magic;\n} header_t;\nThe example above would look like what you see in Figure 17.2. When\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 1469,
      "extraction_method": "Direct"
    },
    {
      "page_number": 194,
      "chapter": null,
      "content": "158\nFREE-SPACE MANAGEMENT\nthe user calls free(ptr), the library then uses simple pointer arithmetic\nto ﬁgure out where the header begins:\nvoid free(void *ptr) {\nheader_t *hptr = (void *)ptr - sizeof(header_t);\n}\nAfter obtaining such a pointer to the header, the library can easily de-\ntermine whether the magic number matches the expected value as a san-\nity check (assert(hptr->magic == 1234567)) and calculate the to-\ntal size of the newly-freed region via simple math (i.e., adding the size of\nthe header to size of the region). Note the small but critical detail in the\nlast sentence: the size of the free region is the size of the header plus the\nsize of the space allocated to the user. Thus, when a user requests N bytes\nof memory, the library does not search for a free chunk of size N; rather,\nit searches for a free chunk of size N plus the size of the header.\nEmbedding A Free List\nThus far we have treated our simple free list as a conceptual entity; it is\njust a list describing the free chunks of memory in the heap. But how do\nwe build such a list inside the free space itself?\nIn a more typical list, when allocating a new node, you would just call\nmalloc() when you need space for the node. Unfortunately, within the\nmemory-allocation library, you can’t do this! Instead, you need to build\nthe list inside the free space itself. Don’t worry if this sounds a little weird;\nit is, but not so weird that you can’t do it!\nAssume we have a 4096-byte chunk of memory to manage (i.e., the\nheap is 4KB). To manage this as a free list, we ﬁrst have to initialize said\nlist; initially, the list should have one entry, of size 4096 (minus the header\nsize). Here is the description of a node of the list:\ntypedef struct __node_t {\nint\nsize;\nstruct __node_t *next;\n} node_t;\nNow let’s look at some code that initializes the heap and puts the ﬁrst\nelement of the free list inside that space. We are assuming that the heap is\nbuilt within some free space acquired via a call to the system call mmap();\nthis is not the only way to build such a heap but serves us well in this\nexample. Here is the code:\n// mmap() returns a pointer to a chunk of free space\nnode_t *head = mmap(NULL, 4096, PROT_READ|PROT_WRITE,\nMAP_ANON|MAP_PRIVATE, -1, 0);\nhead->size\n= 4096 - sizeof(node_t);\nhead->next\n= NULL;\nAfter running this code, the status of the list is that it has a single entry,\nof size 4088. Yes, this is a tiny heap, but it serves as a ﬁne example for us\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2500,
      "extraction_method": "Direct"
    },
    {
      "page_number": 195,
      "chapter": null,
      "content": "FREE-SPACE MANAGEMENT\n159\nsize:\n4088\nnext:\n0\n...\nhead\n[virtual address: 16KB]\nheader: size field\nheader: next field (NULL is 0)\nthe rest of the 4KB chunk\nFigure 17.3: A Heap With One Free Chunk\nsize:\n100\nmagic: 1234567\n. . .\nsize:\n3980\nnext:\n0\n. . .\nptr\n[virtual address: 16KB]\nhead\nThe 100 bytes now allocated\nThe free 3980 byte chunk\nFigure 17.4: A Heap: After One Allocation\nhere. The head pointer contains the beginning address of this range; let’s\nassume it is 16KB (though any virtual address would be ﬁne). Visually,\nthe heap thus looks like what you see in Figure 17.3.\nNow, let’s imagine that a chunk of memory is requested, say of size\n100 bytes. To service this request, the library will ﬁrst ﬁnd a chunk that is\nlarge enough to accommodate the request; because there is only one free\nchunk (size: 4088), this chunk will be chosen. Then, the chunk will be\nsplit into two: one chunk big enough to service the request (and header,\nas described above), and the remaining free chunk. Assuming an 8-byte\nheader (an integer size and an integer magic number), the space in the\nheap now looks like what you see in Figure 17.4.\nThus, upon the request for 100 bytes, the library allocated 108 bytes\nout of the existing one free chunk, returns a pointer (marked ptr in the\nﬁgure above) to it, stashes the header information immediately before the\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 1388,
      "extraction_method": "Direct"
    },
    {
      "page_number": 196,
      "chapter": null,
      "content": "160\nFREE-SPACE MANAGEMENT\nsize:\n100\nmagic: 1234567\n. . .\nsize:\n100\nmagic: 1234567\n. . .\nsize:\n100\nmagic: 1234567\n. . .\nsize:\n3764\nnext:\n0\n. . .\nsptr\n[virtual address: 16KB]\nhead\n100 bytes still allocated\n100 bytes still allocated\n (but about to be freed)\n100-bytes still allocated\nThe free 3764-byte chunk\nFigure 17.5: Free Space With Three Chunks Allocated\nallocated space for later use upon free(), and shrinks the one free node\nin the list to 3980 bytes (4088 minus 108).\nNow let’s look at the heap when there are three allocated regions, each\nof 100 bytes (or 108 including the header). A visualization of this heap is\nshown in Figure 17.5.\nAs you can see therein, the ﬁrst 324 bytes of the heap are now allo-\ncated, and thus we see three headers in that space as well as three 100-\nbyte regions being used by the calling program. The free list remains\nuninteresting: just a single node (pointed to by head), but now only 3764\nbytes in size after the three splits. But what happens when the calling\nprogram returns some memory via free()?\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1090,
      "extraction_method": "Direct"
    },
    {
      "page_number": 197,
      "chapter": null,
      "content": "FREE-SPACE MANAGEMENT\n161\nsize:\n100\nmagic: 1234567\n. . .\nsize:\n100\nnext:\n16708\n. . .\nsize:\n100\nmagic: 1234567\n. . .\nsize:\n3764\nnext:\n0\n. . .\n[virtual address: 16KB]\nhead\nsptr\n100 bytes still allocated\n(now a free chunk of memory)\n100-bytes still allocated\nThe free 3764-byte chunk\nFigure 17.6: Free Space With Two Chunks Allocated\nIn this example, the application returns the middle chunk of allocated\nmemory, by calling free(16500) (the value 16500 is arrived upon by\nadding the start of the memory region, 16384, to the 108 of the previous\nchunk and the 8 bytes of the header for this chunk). This value is shown\nin the previous diagram by the pointer sptr.\nThe library immediately ﬁgures out the size of the free region, and\nthen adds the free chunk back onto the free list. Assuming we insert at\nthe head of the free list, the space now looks like this (Figure 17.6).\nAnd now we have a list that starts with a small free chunk (100 bytes,\npointed to by the head of the list) and a large free chunk (3764 bytes).\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 1057,
      "extraction_method": "Direct"
    },
    {
      "page_number": 198,
      "chapter": null,
      "content": "162\nFREE-SPACE MANAGEMENT\nsize:\n100\nnext:\n16492\n. . .\nsize:\n100\nnext:\n16708\n. . .\nsize:\n100\nnext:\n16384\n. . .\nsize:\n3764\nnext:\n0\n. . .\n[virtual address: 16KB]\nhead\n(now free)\n(now free)\n(now free)\nThe free 3764-byte chunk\nFigure 17.7: A Non-Coalesced Free List\nOur list ﬁnally has more than one element on it! And yes, the free space\nis fragmented, an unfortunate but common occurrence.\nOne last example: let’s assume now that the last two in-use chunks are\nfreed. Without coalescing, you might end up with a free list that is highly\nfragmented (see Figure 17.7).\nAs you can see from the ﬁgure, we now have a big mess! Why? Simple,\nwe forgot to coalesce the list. Although all of the memory is free, it is\nchopped up into pieces, thus appearing as a fragmented memory despite\nnot being one. The solution is simple: go through the list and merge\nneighboring chunks; when ﬁnished, the heap will be whole again.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 956,
      "extraction_method": "Direct"
    },
    {
      "page_number": 199,
      "chapter": null,
      "content": "FREE-SPACE MANAGEMENT\n163\nGrowing The Heap\nWe should discuss one last mechanism found within many allocation li-\nbraries. Speciﬁcally, what should you do if the heap runs out of space?\nThe simplest approach is just to fail. In some cases this is the only option,\nand thus returning NULL is an honorable approach. Don’t feel bad! You\ntried, and though you failed, you fought the good ﬁght.\nMost traditional allocators start with a small-sized heap and then re-\nquest more memory from the OS when they run out. Typically, this means\nthey make some kind of system call (e.g., sbrk in most UNIX systems) to\ngrow the heap, and then allocate the new chunks from there. To service\nthe sbrk request, the OS ﬁnds free physical pages, maps them into the\naddress space of the requesting process, and then returns the value of\nthe end of the new heap; at that point, a larger heap is available, and the\nrequest can be successfully serviced.\n17.3\nBasic Strategies\nNow that we have some machinery under our belt, let’s go over some\nbasic strategies for managing free space. These approaches are mostly\nbased on pretty simple policies that you could think up yourself; try it\nbefore reading and see if you come up with all of the alternatives (or\nmaybe some new ones!).\nThe ideal allocator is both fast and minimizes fragmentation. Unfortu-\nnately, because the stream of allocation and free requests can be arbitrary\n(after all, they are determined by the programmer), any particular strat-\negy can do quite badly given the wrong set of inputs. Thus, we will not\ndescribe a “best” approach, but rather talk about some basics and discuss\ntheir pros and cons.\nBest Fit\nThe best ﬁt strategy is quite simple: ﬁrst, search through the free list and\nﬁnd chunks of free memory that are as big or bigger than the requested\nsize. Then, return the one that is the smallest in that group of candidates;\nthis is the so called best-ﬁt chunk (it could be called smallest ﬁt too). One\npass through the free list is enough to ﬁnd the correct block to return.\nThe intuition behind best ﬁt is simple: by returning a block that is close\nto what the user asks, best ﬁt tries to reduce wasted space. However, there\nis a cost; naive implementations pay a heavy performance penalty when\nperforming an exhaustive search for the correct free block.\nWorst Fit\nThe worst ﬁt approach is the opposite of best ﬁt; ﬁnd the largest chunk\nand return the requested amount; keep the remaining (large) chunk on\nthe free list. Worst ﬁt tries to thus leave big chunks free instead of lots of\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2580,
      "extraction_method": "Direct"
    },
    {
      "page_number": 200,
      "chapter": null,
      "content": "164\nFREE-SPACE MANAGEMENT\nsmall chunks that can arise from a best-ﬁt approach. Once again, how-\never, a full search of free space is required, and thus this approach can be\ncostly. Worse, most studies show that it performs badly, leading to excess\nfragmentation while still having high overheads.\nFirst Fit\nThe ﬁrst ﬁt method simply ﬁnds the ﬁrst block that is big enough and\nreturns the requested amount to the user. As before, the remaining free\nspace is kept free for subsequent requests.\nFirst ﬁt has the advantage of speed – no exhaustive search of all the\nfree spaces are necessary – but sometimes pollutes the beginning of the\nfree list with a small objects. Thus, how the allocator manages the free\nlist’s order becomes an issue. One approach is to use address-based or-\ndering; by keeping the list ordered by the address of the free space, coa-\nlescing becomes easier, and fragmentation tends to be reduced.\nNext Fit\nInstead of always beginning the ﬁrst-ﬁt search at the beginning of the list,\nthe next ﬁt algorithm keeps an extra pointer to the location within the\nlist where one was looking last. The idea is to spread the searches for\nfree space throughout the list more uniformly, thus avoiding splintering\nof the beginning of the list. The performance of such an approach is quite\nsimilar to ﬁrst ﬁt, as an exhaustive search is once again avoided.\nExamples\nHere are a few examples of the above strategies. Envision a free list with\nthree elements on it, of sizes 10, 30, and 20 (we’ll ignore headers and other\ndetails here, instead just focusing on how strategies operate):\nhead\n10\n30\n20\nNULL\nAssume an allocation request of size 15. A best-ﬁt approach would\nsearch the entire list and ﬁnd that 20 was the best ﬁt, as it is the smallest\nfree space that can accommodate the request. The resulting free list:\nhead\n10\n30\n5\nNULL\nAs happens in this example, and often happens with a best-ﬁt ap-\nproach, a small free chunk is now left over. A worst-ﬁt approach is similar\nbut instead ﬁnds the largest chunk, in this example 30. The resulting list:\nhead\n10\n15\n20\nNULL\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2122,
      "extraction_method": "Direct"
    },
    {
      "page_number": 201,
      "chapter": null,
      "content": "FREE-SPACE MANAGEMENT\n165\nThe ﬁrst-ﬁt strategy, in this example, does the same thing as worst-ﬁt,\nalso ﬁnding the ﬁrst free block that can satisfy the request. The difference\nis in the search cost; both best-ﬁt and worst-ﬁt look through the entire list;\nﬁrst-ﬁt only examines free chunks until it ﬁnds one that ﬁts, thus reducing\nsearch cost.\nThese examples just scratch the surface of allocation policies. More\ndetailed analysis with real workloads and more complex allocator behav-\niors (e.g., coalescing) are required for a deeper understanding. Perhaps\nsomething for a homework section, you say?\n17.4\nOther Approaches\nBeyond the basic approaches described above, there have been a host\nof suggested techniques and algorithms to improve memory allocation in\nsome way. We list a few of them here for your consideration (i.e., to make\nyou think about a little more than just best-ﬁt allocation).\nSegregated Lists\nOne interesting approach that has been around for some time is the use\nof segregated lists. The basic idea is simple: if a particular application\nhas one (or a few) popular-sized request that it makes, keep a separate\nlist just to manage objects of that size; all other requests are forwarded to\na more general memory allocator.\nThe beneﬁts of such an approach are obvious. By having a chunk of\nmemory dedicated for one particular size of requests, fragmentation is\nmuch less of a concern; moreover, allocation and free requests can be\nserved quite quickly when they are of the right size, as no complicated\nsearch of a list is required.\nJust like any good idea, this approach introduces new complications\ninto a system as well. For example, how much memory should one ded-\nicate to the pool of memory that serves specialized requests of a given\nsize, as opposed to the general pool? One particular allocator, the slab\nallocator by uber-engineer Jeff Bonwick (which was designed for use in\nthe Solaris kernel), handles this issue in a rather nice way [B94].\nSpeciﬁcally, when the kernel boots up, it allocates a number of object\ncaches for kernel objects that are likely to be requested frequently (such as\nlocks, ﬁle-system inodes, etc.); the object caches thus are each segregated\nfree lists of a given size and serve memory allocation and free requests\nquickly. When a given cache is running low on free space, it requests\nsome slabs of memory from a more general memory allocator (the to-\ntal amount requested being a multiple of the page size and the object in\nquestion). Conversely, when the reference counts of the objects within\na given slab all go to zero, the general allocator can reclaim them from\nthe specialized allocator, which is often done when the VM system needs\nmore memory.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2750,
      "extraction_method": "Direct"
    },
    {
      "page_number": 202,
      "chapter": null,
      "content": "166\nFREE-SPACE MANAGEMENT\nASIDE: GREAT ENGINEERS ARE REALLY GREAT\nEngineers like Jeff Bonwick (who not only wrote the slab allocator men-\ntioned herein but also was the lead of an amazing ﬁle system, ZFS) are\nthe heart of Silicon Valley. Behind almost any great product or technol-\nogy is a human (or small group of humans) who are way above average\nin their talents, abilities, and dedication. As Mark Zuckerberg (of Face-\nbook) says: “Someone who is exceptional in their role is not just a little\nbetter than someone who is pretty good. They are 100 times better.” This\nis why, still today, one or two people can start a company that changes\nthe face of the world forever (think Google, Apple, or Facebook). Work\nhard and you might become such a “100x” person as well. Failing that,\nwork with such a person; you’ll learn more in day than most learn in a\nmonth. Failing that, feel sad.\nThe slab allocator also goes beyond most segregated list approaches\nby keeping free objects on the lists in a pre-initialized state. Bonwick\nshows that initialization and destruction of data structures is costly [B94];\nby keeping freed objects in a particular list in their initialized state, the\nslab allocator thus avoids frequent initialization and destruction cycles\nper object and thus lowers overheads noticeably.\nBuddy Allocation\nBecause coalescing is critical for an allocator, some approaches have been\ndesigned around making coalescing simple. One good example is found\nin the binary buddy allocator [K65].\nIn such a system, free memory is ﬁrst conceptually thought of as one\nbig space of size 2N. When a request for memory is made, the search for\nfree space recursively divides free space by two until a block that is big\nenough to accommodate the request is found (and a further split into two\nwould result in a space that is too small). At this point, the requested\nblock is returned to the user. Here is an example of a 64KB free space\ngetting divided in the search for a 7KB block:\n64 KB\n32 KB\n32 KB\n16 KB\n16 KB\n8 KB 8 KB\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2071,
      "extraction_method": "Direct"
    },
    {
      "page_number": 203,
      "chapter": null,
      "content": "FREE-SPACE MANAGEMENT\n167\nIn the example, the leftmost 8KB block is allocated (as indicated by the\ndarker shade of gray) and returned to the user; note that this scheme can\nsuffer from internal fragmentation, as you are only allowed to give out\npower-of-two-sized blocks.\nThe beauty of buddy allocation is found in what happens when that\nblock is freed. When returning the 8KB block to the free list, the allocator\nchecks whether the “buddy” 8KB is free; if so, it coalesces the two blocks\ninto a 16KB block. The allocator then checks if the buddy of the 16KB\nblock is still free; if so, it coalesces those two blocks. This recursive coa-\nlescing process continues up the tree, either restoring the entire free space\nor stopping when a buddy is found to be in use.\nThe reason buddy allocation works so well is that it is simple to de-\ntermine the buddy of a particular block. How, you ask? Think about the\naddresses of the blocks in the free space above. If you think carefully\nenough, you’ll see that the address of each buddy pair only differs by\na single bit; which bit is determined by the level in the buddy tree. And\nthus you have a basic idea of how binary buddy allocation schemes work.\nFor more detail, as always, see the Wilson survey [W+95].\nOther Ideas\nOne major problem with many of the approaches described above is their\nlack of scaling.\nSpeciﬁcally, searching lists can be quite slow.\nThus,\nadvanced allocators use more complex data structures to address these\ncosts, trading simplicity for performance. Examples include balanced bi-\nnary trees, splay trees, or partially-ordered trees [W+95].\nGiven that modern systems often have multiple processors and run\nmulti-threaded workloads (something you’ll learn about in great detail\nin the section of the book on Concurrency), it is not surprising that a lot\nof effort has been spent making allocators work well on multiprocessor-\nbased systems. Two wonderful examples are found in Berger et al. [B+00]\nand Evans [E06]; check them out for the details.\nThese are but two of the thousands of ideas people have had over time\nabout memory allocators. Read on your own if you are curious.\n17.5\nSummary\nIn this chapter, we’ve discussed the most rudimentary forms of mem-\nory allocators. Such allocators exist everywhere, linked into every C pro-\ngram you write, as well as in the underlying OS which is managing mem-\nory for its own data structures. As with many systems, there are many\ntrade-offs to be made in building such a system, and the more you know\nabout the exact workload presented to an allocator, the more you could do\nto tune it to work better for that workload. Making a fast, space-efﬁcient,\nscalable allocator that works well for a broad range of workloads remains\nan on-going challenge in modern computer systems.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2830,
      "extraction_method": "Direct"
    },
    {
      "page_number": 204,
      "chapter": null,
      "content": "168\nFREE-SPACE MANAGEMENT\nReferences\n[B+00] “Hoard: A Scalable Memory Allocator for Multithreaded Applications”\nEmery D. Berger, Kathryn S. McKinley, Robert D. Blumofe, and Paul R. Wilson\nASPLOS-IX, November 2000\nBerger and company’s excellent allocator for multiprocessor systems. Beyond just being a fun paper,\nalso used in practice!\n[B94] “The Slab Allocator: An Object-Caching Kernel Memory Allocator”\nJeff Bonwick\nUSENIX ’94\nA cool paper about how to build an allocator for an operating system kernel, and a great example of how\nto specialize for particular common object sizes.\n[E06] “A Scalable Concurrent malloc(3) Implementation for FreeBSD”\nJason Evans\nhttp://people.freebsd.org/˜jasone/jemalloc/bsdcan2006/jemalloc.pdf\nApril 2006\nA detailed look at how to build a real modern allocator for use in multiprocessors. The “jemalloc”\nallocator is in widespread use today, within FreeBSD, NetBSD, Mozilla Firefox, and within Facebook.\n[K65] “A Fast Storage Allocator”\nKenneth C. Knowlton\nCommunications of the ACM, Volume 8, Number 10, October 1965\nThe common reference for buddy allocation. Random strange fact: Knuth gives credit for the idea to not\nto Knowlton but to Harry Markowitz, a Nobel-prize winning economist. Another strange fact: Knuth\ncommunicates all of his emails via a secretary; he doesn’t send email himself, rather he tells his secretary\nwhat email to send and then the secretary does the work of emailing. Last Knuth fact: he created TeX,\nthe tool used to typeset this book. It is an amazing piece of software4.\n[W+95] “Dynamic Storage Allocation: A Survey and Critical Review”\nPaul R. Wilson, Mark S. Johnstone, Michael Neely, David Boles\nInternational Workshop on Memory Management\nKinross, Scotland, September 1995\nAn excellent and far-reaching survey of many facets of memory allocation. Far too much detail to go\ninto in this tiny chapter!\n4Actually we use LaTeX, which is based on Lamport’s additions to TeX, but close enough.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2006,
      "extraction_method": "Direct"
    },
    {
      "page_number": 205,
      "chapter": null,
      "content": "18\nPaging: Introduction\nRemember our goal: to virtualize memory. Segmentation (a generaliza-\ntion of dynamic relocation) helped us do this, but has some problems; in\nparticular, managing free space becomes quite a pain as memory becomes\nfragmented and segmentation is not as ﬂexible as we might like. Is there\na better solution?\nTHE CRUX:\nHOW TO VIRTUALIZE MEMORY WITHOUT SEGMENTS\nHow can we virtualize memory in a way as to avoid the problems of\nsegmentation? What are the basic techniques? How do we make those\ntechniques work well?\nThus comes along the idea of paging, which goes back to the earliest\nof computer systems, namely the Atlas [KE+62,L78]. Instead of splitting\nup our address space into three logical segments (each of variable size),\nwe split up our address space into ﬁxed-sized units we call a page. Here\nin Figure 18.1 an example of a tiny address space, only 64 bytes total in\nsize, with 16 byte pages (real address spaces are much bigger, of course,\ncommonly 32 bits and thus 4-GB of address space, or even 64 bits). We’ll\nuse tiny examples to make them easier to digest (at ﬁrst).\n64\n48\n32\n16\n0\n(page 3)\n(page 2)\n(page 1)\n(page 0 of the address space)\nFigure 18.1: A Simple 64-byte Address Space\n169\n",
      "content_length": 1222,
      "extraction_method": "Direct"
    },
    {
      "page_number": 206,
      "chapter": null,
      "content": "170\nPAGING: INTRODUCTION\n128\n112\n96\n80\n64\n48\n32\n16\n0\npage frame 7\npage frame 6\npage frame 5\npage frame 4\npage frame 3\npage frame 2\npage frame 1\npage frame 0 of physical memory\nreserved for OS\n(unused)\npage 3 of AS\npage 0 of AS\n(unused)\npage 2 of AS\n(unused)\npage 1 of AS\nFigure 18.2: 64-Byte Address Space Placed In Physical Memory\nThus, we have an address space that is split into four pages (0 through\n3). With paging, physical memory is also split into some number of pages\nas well; we sometimes will call each page of physical memory a page\nframe. For an example, let’s examine Figure 18.2.\nPaging, as we will see, has a number of advantages over our previous\napproaches. Probably the most important improvement will be ﬂexibility:\nwith a fully-developed paging approach, the system will be able to sup-\nport the abstraction of an address space effectively, regardless of how the\nprocesses uses the address space; we won’t, for example, have to make\nassumptions about how the heap and stack grow and how they are used.\nAnother advantage is the simplicity of free-space management that pag-\ning affords. For example, when the OS wishes to place our tiny 64-byte\naddress space from above into our 8-page physical memory, it simply\nﬁnds four free pages; perhaps the OS keeps a free list of all free pages for\nthis, and just grabs the ﬁrst four free pages off of this list. In the exam-\nple above, the OS has placed virtual page 0 of the address space (AS) in\nphysical page 3, virtual page 1 of the AS on physical page 7, page 2 on\npage 5, and page 3 on page 2.\nTo record where each virtual page of the address space is placed in\nphysical memory, the operating system keeps a per-process data structure\nknown as a page table. The major role of the page table is to store address\ntranslations for each of the virtual pages of the address space, thus letting\nus know where in physical memory they live. For our simple example\nabove (Figure 18.2), the page table would thus have the following entries:\n(Virtual Page 0 →Physical Frame 3), (VP 1 →PF 7), (VP 2 →PF 5), and\n(VP 3 →PF 2).\nIt is important to remember that this page table is a per-process data\nstructure (most page table structures we discuss are per-process struc-\ntures; an exception we’ll touch on is the inverted page table). If another\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2346,
      "extraction_method": "Direct"
    },
    {
      "page_number": 207,
      "chapter": null,
      "content": "PAGING: INTRODUCTION\n171\nprocess were to run in our example above, the OS would have to manage\na different page table for it, as its virtual pages obviously map to different\nphysical pages (modulo any sharing going on).\nNow, we know enough to perform an address-translation example.\nLet’s imagine the process with that tiny address space (64 bytes) is per-\nforming a memory access:\nmovl <virtual address>, %eax\nSpeciﬁcally, let’s pay attention to the explicit load of the data at <virtual\naddress> into the register eax (and thus ignore the instruction fetch that\nmust have happened prior).\nTo translate this virtual address that the process generated, we have to\nﬁrst split it into two components: the virtual page number (VPN), and\nthe offset within the page. For this example, because the virtual address\nspace of the process is 64 bytes, we need 6 bits total for our virtual address\n(26 = 64). Thus, our virtual address:\nVa5 Va4 Va3 Va2 Va1 Va0\nwhere Va5 is the highest-order bit of the virtual address, and Va0 the\nlowest order bit. Because we know the page size (16 bytes), we can further\ndivide the virtual address as follows:\nVa5 Va4 Va3 Va2 Va1 Va0\nVPN\noffset\nThe page size is 16 bytes in a 64-byte address space; thus we need to\nbe able to select 4 pages, and the top 2 bits of the address do just that.\nThus, we have a 2-bit virtual page number (VPN). The remaining bits tell\nus which byte of the page we are interested in, 4 bits in this case; we call\nthis the offset.\nWhen a process generates a virtual address, the OS and hardware\nmust combine to translate it into a meaningful physical address. For ex-\nample, let us assume the load above was to virtual address 21:\nmovl 21, %eax\nTurning “21” into binary form, we get “010101”, and thus we can ex-\namine this virtual address and see how it breaks down into a virtual page\nnumber (VPN) and offset:\n0\n1\n0\n1\n0\n1\nVPN\noffset\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 1926,
      "extraction_method": "Direct"
    },
    {
      "page_number": 208,
      "chapter": null,
      "content": "172\nPAGING: INTRODUCTION\n0\n1\n0\n1\n0\n1\nVPN\noffset\n1\n1\n1\n0\n1\n0\n1\nAddress\nTranslation\nPFN\noffset\nVirtual\nAddress\nPhysical\nAddress\nFigure 18.3: The Address Translation Process\nThus, the virtual address “21” is on the 5th (“0101”th) byte of vir-\ntual page “01” (or 1). With our virtual page number, we can now index\nour page table and ﬁnd which physical page that virtual page 1 resides\nwithin. In the page table above the physical page number (PPN) (a.k.a.\nphysical frame number or PFN) is 7 (binary 111). Thus, we can translate\nthis virtual address by replacing the VPN with the PFN and then issue\nthe load to physical memory (Figure 18.3).\nNote the offset stays the same (i.e., it is not translated), because the\noffset just tells us which byte within the page we want. Our ﬁnal physical\naddress is 1110101 (117 in decimal), and is exactly where we want our\nload to fetch data from (Figure 18.2).\n18.1\nWhere Are Page Tables Stored?\nPage tables can get awfully large, much bigger than the small segment\ntable or base/bounds pair we have discussed previously. For example,\nimagine a typical 32-bit address space, with 4-KB pages. This virtual ad-\ndress splits into a 20-bit VPN and 12-bit offset (recall that 10 bits would\nbe needed for a 1-KB page size, and just add two more to get to 4 KB).\nA 20-bit VPN implies that there are 220 translations that the OS would\nhave to manage for each process (that’s roughly a million); assuming we\nneed 4 bytes per page table entry (PTE) to hold the physical translation\nplus any other useful stuff, we get an immense 4MB of memory needed\nfor each page table! That is pretty big. Now imagine there are 100 pro-\ncesses running: this means the OS would need 400MB of memory just for\nall those address translations! Even in the modern era, where machines\nhave gigabytes of memory, it seems a little crazy to use a large chunk of\nif just for translations, no?\nBecause page tables are so big, we don’t keep any special on-chip hard-\nware in the MMU to store the page table of the currently-running process.\nInstead, we store the page table for each process in memory somewhere.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2154,
      "extraction_method": "Direct"
    },
    {
      "page_number": 209,
      "chapter": null,
      "content": "PAGING: INTRODUCTION\n173\n128\n112\n96\n80\n64\n48\n32\n16\n0\npage frame 7\npage frame 6\npage frame 5\npage frame 4\npage frame 3\npage frame 2\npage frame 1\npage frame 0 of physical memory\n(unused)\npage 3 of AS\npage 0 of AS\n(unused)\npage 2 of AS\n(unused)\npage 1 of AS\npage table:\n3 7 5 2\nFigure 18.4: Example: Page Table in Kernel Physical Memory\nLet’s assume for now that the page tables live in physical memory that\nthe OS manages. In Figure 18.4 is a picture of what that might look like.\n18.2\nWhat’s Actually In The Page Table?\nLet’s talk a little about page table organization. The page table is just a\ndata structure that is used to map virtual addresses (or really, virtual page\nnumbers) to physical addresses (physical page numbers). Thus, any data\nstructure could work. The simplest form is called a linear page table,\nwhich is just an array. The OS indexes the array by the VPN, and looks up\nthe page-table entry (PTE) at that index in order to ﬁnd the desired PFN.\nFor now, we will assume this simple linear structure; in later chapters,\nwe will make use of more advanced data structures to help solve some\nproblems with paging.\nAs for the contents of each PTE, we have a number of different bits\nin there worth understanding at some level. A valid bit is common to\nindicate whether the particular translation is valid; for example, when\na program starts running, it will have code and heap at one end of its\naddress space, and the stack at the other. All the unused space in-between\nwill be marked invalid, and if the process tries to access such memory, it\nwill generate a trap to the OS which will likely terminate the process.\nThus, the valid bit is crucial for supporting a sparse address space; by\nsimply marking all the unused pages in the address space invalid, we\nremove the need to allocate physical frames for those pages and thus save\na great deal of memory.\nWe also might have protection bits, indicating whether the page could\nbe read from, written to, or executed from. Again, accessing a page in a\nway not allowed by these bits will generate a trap to the OS.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2115,
      "extraction_method": "Direct"
    },
    {
      "page_number": 210,
      "chapter": null,
      "content": "174\nPAGING: INTRODUCTION\n31 30 29 28 27 26 25 24 23 22 21 20 19 18 17 16 15 14 13 12 11 10\n9\n8\n7\n6\n5\n4\n3\n2\n1\n0\nPFN\nG\nPAT\nD\nA\nPCD\nPWT\nU/S\nR/W\nP\nFigure 18.5: An x86 Page Table Entry (PTE)\nThere are a couple of other bits that are important but we won’t talk\nabout much for now. A present bit indicates whether this page is in phys-\nical memory or on disk (swapped out); we will understand this in more\ndetail when we study how to move parts of the address space to disk\nand back in order to support address spaces that are larger than physical\nmemory and allow for the pages of processes that aren’t actively being\nrun to be swapped out. A dirty bit is also common, indicating whether\nthe page has been modiﬁed since it was brought into memory.\nA reference bit (a.k.a. accessed bit) is sometimes used to track whether\na page has been accessed, and is useful in determining which pages are\npopular and thus should be kept in memory; such knowledge is critical\nduring page replacement, a topic we will study in great detail in subse-\nquent chapters.\nFigure 18.5 shows an example page table entry from the x86 architec-\nture [I09]. It contains a present bit (P); a read/write bit (R/W) which\ndetermines if writes are allowed to this page; a user/supervisor bit (U/S)\nwhich determines if user-mode processes can access the page; a few bits\n(PWT, PCD, PAT, and G) that determine how hardware caching works for\nthese pages; an accessed bit (A) and a dirty bit (D); and ﬁnally, the page\nframe number (PFN) itself.\nRead the Intel Architecture Manuals [I09] for more details on x86 pag-\ning support. Be forewarned, however; reading manuals such as these,\nwhile quite informative (and certainly necessary for those who write code\nto use such page tables in the OS), can be challenging at ﬁrst. A little pa-\ntience, and a lot of desire, is required.\n18.3\nPaging: Also Too Slow\nWith page tables in memory, we already know that they might be too\nbig. Turns out they can slow things down too. For example, take our\nsimple instruction:\nmovl 21, %eax\nAgain, let’s just examine the explicit reference to address 21 and not\nworry about the instruction fetch. In this example, we will assume the\nhardware performs the translation for us. To fetch the desired data, the\nsystem must ﬁrst translate the virtual address (21) into the correct physi-\ncal address (117). Thus, before issuing the load to address 117, the system\nmust ﬁrst fetch the proper page table entry from the process’s page ta-\nble, perform the translation, and then ﬁnally get the desired data from\nphysical memory.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2608,
      "extraction_method": "Direct"
    },
    {
      "page_number": 211,
      "chapter": null,
      "content": "PAGING: INTRODUCTION\n175\nTo do so, the hardware must know where the page table is for the\ncurrently-running process. Let’s assume for now that a single page-table\nbase register contains the physical address of the starting location of the\npage table. To ﬁnd the location of the desired PTE, the hardware will thus\nperform the following functions:\nVPN\n= (VirtualAddress & VPN_MASK) >> SHIFT\nPTEAddr = PageTableBaseRegister + (VPN * sizeof(PTE))\nIn our example, VPN MASK would be set to 0x30 (hex 30, or binary\n110000) which picks out the VPN bits from the full virtual address; SHIFT\nis set to 4 (the number of bits in the offset), such that we move the VPN\nbits down to form the correct integer virtual page number. For exam-\nple, with virtual address 21 (010101), and masking turns this value into\n010000; the shift turns it into 01, or virtual page 1, as desired. We then use\nthis value as an index into the array of PTEs pointed to by the page table\nbase register.\nOnce this physical address is known, the hardware can fetch the PTE\nfrom memory, extract the PFN, and concatenate it with the offset from\nthe virtual address to form the desired physical address. Speciﬁcally, you\ncan think of the PFN being left-shifted by SHIFT, and then logically OR’d\nwith the offset to form the ﬁnal address as follows:\noffset\n= VirtualAddress & OFFSET_MASK\nPhysAddr = (PFN << SHIFT) | offset\n1\n// Extract the VPN from the virtual address\n2\nVPN = (VirtualAddress & VPN_MASK) >> SHIFT\n3\n4\n// Form the address of the page-table entry (PTE)\n5\nPTEAddr = PTBR + (VPN * sizeof(PTE))\n6\n7\n// Fetch the PTE\n8\nPTE = AccessMemory(PTEAddr)\n9\n10\n// Check if process can access the page\n11\nif (PTE.Valid == False)\n12\nRaiseException(SEGMENTATION_FAULT)\n13\nelse if (CanAccess(PTE.ProtectBits) == False)\n14\nRaiseException(PROTECTION_FAULT)\n15\nelse\n16\n// Access is OK: form physical address and fetch it\n17\noffset\n= VirtualAddress & OFFSET_MASK\n18\nPhysAddr = (PTE.PFN << PFN_SHIFT) | offset\n19\nRegister = AccessMemory(PhysAddr)\nFigure 18.6: Accessing Memory With Paging\nFinally, the hardware can fetch the desired data from memory and put\nit into register eax. The program has now succeeded at loading a value\nfrom memory!\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2234,
      "extraction_method": "Direct"
    },
    {
      "page_number": 212,
      "chapter": null,
      "content": "176\nPAGING: INTRODUCTION\nASIDE: DATA STRUCTURE – THE PAGE TABLE\nOne of the most important data structures in the memory management\nsubsystem of a modern OS is the page table. In general, a page table\nstores virtual-to-physical address translations, thus letting the system\nknow where each page of an address space actually resides in physical\nmemory. Because each address space requires such translations, in gen-\neral there is one page table per process in the system. The exact structure\nof the page table is either determined by the hardware (older systems) or\ncan be more ﬂexibly managed by the OS (modern systems).\nTo summarize, we now describe the initial protocol for what happens\non each memory reference. Figure 18.6 shows the basic approach. For\nevery memory reference (whether an instruction fetch or an explicit load\nor store), paging requires us to perform one extra memory reference in\norder to ﬁrst fetch the translation from the page table. That is a lot of\nwork! Extra memory references are costly, and in this case will likely\nslow down the process by a factor of two or more.\nAnd now you can hopefully see that there are two real problems that\nwe must solve. Without careful design of both hardware and software,\npage tables will cause the system to run too slowly, as well as take up\ntoo much memory. While seemingly a great solution for our memory\nvirtualization needs, these two crucial problems must ﬁrst be overcome.\n18.4\nA Memory Trace\nBefore closing, we now trace through a simple memory access exam-\nple to demonstrate all of the resulting memory accesses that occur when\nusing paging. The code snippet (in C, in a ﬁle called array.c) that are\ninterested in is as follows:\nint array[1000];\n...\nfor (i = 0; i < 1000; i++)\narray[i] = 0;\nWe could then compile array.c and run it with the following com-\nmands:\nprompt> gcc -o array array.c -Wall -O\nprompt> ./array\nOf course, to truly understand what memory accesses this code snip-\npet (which simply initializes an array) will make, we’ll have to know (or\nassume) a few more things. First, we’ll have to disassemble the result-\ning binary (using objdump on Linux, or otool on a Mac) to see what\nassembly instructions are used to initialize the array in a loop. Here it the\nresulting assembly code:\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2319,
      "extraction_method": "Direct"
    },
    {
      "page_number": 213,
      "chapter": null,
      "content": "PAGING: INTRODUCTION\n177\n0x1024 movl $0x0,(%edi,%eax,4)\n0x1028 incl %eax\n0x102c cmpl $0x03e8,%eax\n0x1030 jne\n0x1024\nThe code, if you know a little x86, is actually quite easy to understand.\nThe ﬁrst instruction moves the value zero (shown as $0x0) into the vir-\ntual memory address of the location of the array; this address is computed\nby taking the contents of %edi and adding %eax multiplied by four to it.\nThus, %edi holds the base address of the array, whereas %eax holds the\narray index (i); we multiply by four because the array is an array of inte-\ngers, each size four bytes (note we are cheating a little bit here, assuming\neach instruction is four bytes in size for simplicity; in actuality, x86 in-\nstructions are variable-sized).\nThe second instruction increments the array index held in %eax, and\nthe third instruction compares the contents of that register to the hex\nvalue 0x03e8, or decimal 1000. If the comparison shows that that two\nvalues are not yet equal (which is what the jne instruction tests), the\nfourth instruction jumps back to the top of the loop.\nTo understand which memory accesses this instruction sequence makes\n(at both the virtual and physical levels), we’ll have assume something\nabout where in virtual memory the code snippet and array are found, as\nwell as the contents and location of the page table.\nFor this example, we assume a virtual address space of size 64 KB\n(unrealistically small). We also assume a page size of 1 KB.\nAll we need to know now are the contents of the page table, and its\nlocation in physical memory. Let’s assume we have a linear (array-based)\npage table and that it is located at physical address 1 KB (1024).\nAs for its contents, there are just a few virtual pages we need to worry\nabout having mapped for this example. First, there is the virtual page the\ncode lives on. Because the page size is 1 KB, virtual address 1024 resides\non the the second page of the virtual address space (VPN=1, as VPN=0 is\nthe ﬁrst page). Let’s assume this virtual page maps to physical frame 4\n(VPN 1 →PFN 4).\nNext, there is the array itself. Its size is 4000 bytes (1000 integers), and\nit lives at virtual addresses 40000 through 44000 (not including the last\nbyte). The virtual pages for this decimal range is VPN=39 ... VPN=42.\nThus, we need mappings for these pages. Let’s assume these virtual-to-\nphysical mappings for the example: (VPN 39 →PFN 7), (VPN 40 →PFN 8),\n(VPN 41 →PFN 9), (VPN 42 →PFN 10).\nWe are now ready to trace the memory references of the program.\nWhen it runs, each instruction fetch will generate two memory references:\none to the page table to ﬁnd the physical frame that the instruction resides\nwithin, and one to the instruction itself to fetch it to the CPU for process-\ning. In addition, there is one explicit memory reference in the form of\nthe mov instruction; this adds another page table access ﬁrst (to translate\nthe array virtual address to the correct physical one) and then the array\naccess itself.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 3026,
      "extraction_method": "Direct"
    },
    {
      "page_number": 214,
      "chapter": null,
      "content": "178\nPAGING: INTRODUCTION\n0\n10\n20\n30\n40\n50\n1024\n1074\n1124\nMemory Access\nCode (VA)\n40000\n40050\n40100\nArray (VA)\n1024\n1074\n1124\n1174\n1224\nPage Table (PA)\n4096\n4146\n4196\nCode (PA)\n7232\n7282\n7132\nArray (PA)\nmov\ninc\ncmp\njne\nmov\nPageTable[1]\nPageTable[39]\nFigure 18.7: A Virtual (And Physical) Memory Trace\nThe entire process, for the ﬁrst ﬁve loop iterations, is depicted in Fig-\nure 18.7. The bottom most graph shows the instruction memory refer-\nences on the y-axis in black (with virtual addresses on the left, and the\nactual physical addresses on the right); the middle graph shows array\naccesses in dark gray (again with virtual on left and physical on right); ﬁ-\nnally, the topmost graph shows page table memory accesses in light gray\n(just physical, as the page table in this example resides in physical mem-\nory). The x-axis, for the entire trace, shows memory accesses across the\nﬁrst ﬁve iterations of the loop (there are 10 memory accesses per loop,\nwhich includes four instruction fetches, one explicit update of memory,\nand ﬁve page table accesses to translate those four fetches and one explicit\nupdate).\nSee if you can make sense of the patterns that show up in this visu-\nalization. In particular, what will change as the loop continues to run\nbeyond these ﬁrst ﬁve iterations? Which new memory locations will be\naccessed? Can you ﬁgure it out?\nThis has just been the simplest of examples (only a few lines of C code),\nand yet you might already be able to sense the complexity of understand-\ning the actual memory behavior of real applications. Don’t worry: it deﬁ-\nnitely gets worse, because the mechanisms we are about to introduce only\ncomplicate this already complex machinery. Sorry!\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1746,
      "extraction_method": "Direct"
    },
    {
      "page_number": 215,
      "chapter": null,
      "content": "PAGING: INTRODUCTION\n179\n18.5\nSummary\nWe have introduced the concept of paging as a solution to our chal-\nlenge of virtualizing memory. Paging has many advantages over previ-\nous approaches (such as segmentation). First, it does not lead to external\nfragmentation, as paging (by design) divides memory into ﬁxed-sized\nunits. Second, it is quite ﬂexible, enabling the sparse use of virtual ad-\ndress spaces.\nHowever, implementing paging support without care will lead to a\nslower machine (with many extra memory accesses to access the page\ntable) as well as memory waste (with memory ﬁlled with page tables in-\nstead of useful application data). We’ll thus have to think a little harder\nto come up with a paging system that not only works, but works well.\nThe next two chapters, fortunately, will show us how to do so.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 859,
      "extraction_method": "Direct"
    },
    {
      "page_number": 216,
      "chapter": null,
      "content": "180\nPAGING: INTRODUCTION\nReferences\n[KE+62] “One-level Storage System”\nT. Kilburn, and D.B.G. Edwards and M.J. Lanigan and F.H. Sumner\nIRE Trans. EC-11, 2 (1962), pp. 223-235\n(Reprinted in Bell and Newell, “Computer Structures: Readings and Examples” McGraw-Hill,\nNew York, 1971).\nThe Atlas pioneered the idea of dividing memory into ﬁxed-sized pages and in many senses was an early\nform of the memory-management ideas we see in modern computer systems.\n[I09] “Intel 64 and IA-32 Architectures Software Developer’s Manuals”\nIntel, 2009\nAvailable: http://www.intel.com/products/processor/manuals\nIn particular, pay attention to “Volume 3A: System Programming Guide Part 1” and “Volume 3B:\nSystem Programming Guide Part 2”\n[L78] “The Manchester Mark I and atlas: a historical perspective”\nS. H. Lavington\nCommunications of the ACM archive\nVolume 21, Issue 1 (January 1978), pp. 4-12\nSpecial issue on computer architecture\nThis paper is a great retrospective of some of the history of the development of some important computer\nsystems. As we sometimes forget in the US, many of these new ideas came from overseas.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1159,
      "extraction_method": "Direct"
    },
    {
      "page_number": 217,
      "chapter": null,
      "content": "PAGING: INTRODUCTION\n181\nHomework\nIn this homework, you will use a simple program, which is known as\npaging-linear-translate.py, to see if you understand how simple\nvirtual-to-physical address translation works with linear page tables. See\nthe README for details.\nQuestions\n• Before doing any translations, let’s use the simulator to study how\nlinear page tables change size given different parameters. Compute\nthe size of linear page tables as different parameters change. Some\nsuggested inputs are below; by using the -v flag, you can see\nhow many page-table entries are ﬁlled.\nFirst, to understand how linear page table size changes as the ad-\ndress space grows:\npaging-linear-translate.py -P 1k -a 1m -p 512m -v -n 0\npaging-linear-translate.py -P 1k -a 2m -p 512m -v -n 0\npaging-linear-translate.py -P 1k -a 4m -p 512m -v -n 0\nThen, to understand how linear page table size changes as page size\ngrows:\npaging-linear-translate.py -P 1k -a 1m -p 512m -v -n 0\npaging-linear-translate.py -P 2k -a 1m -p 512m -v -n 0\npaging-linear-translate.py -P 4k -a 1m -p 512m -v -n 0\nBefore running any of these, try to think about the expected trends.\nHow should page-table size change as the address space grows? As\nthe page size grows? Why shouldn’t we just use really big pages in\ngeneral?\n• Now let’s do some translations. Start with some small examples,\nand change the number of pages that are allocated to the address\nspace with the -u flag. For example:\npaging-linear-translate.py -P 1k -a 16k -p 32k -v -u 0\npaging-linear-translate.py -P 1k -a 16k -p 32k -v -u 25\npaging-linear-translate.py -P 1k -a 16k -p 32k -v -u 50\npaging-linear-translate.py -P 1k -a 16k -p 32k -v -u 75\npaging-linear-translate.py -P 1k -a 16k -p 32k -v -u 100\nWhat happens as you increase the percentage of pages that are al-\nlocated in each address space?\n• Now let’s try some different random seeds, and some different (and\nsometimes quite crazy) address-space parameters, for variety:\npaging-linear-translate.py -P 8\n-a 32\n-p 1024 -v -s 1\npaging-linear-translate.py -P 8k -a 32k\n-p 1m\n-v -s 2\npaging-linear-translate.py -P 1m -a 256m -p 512m -v -s 3\nWhich of these parameter combinations are unrealistic? Why?\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2223,
      "extraction_method": "Direct"
    },
    {
      "page_number": 218,
      "chapter": null,
      "content": "182\nPAGING: INTRODUCTION\n• Use the program to try out some other problems. Can you ﬁnd the\nlimits of where the program doesn’t work anymore? For example,\nwhat happens if the address-space size is bigger than physical mem-\nory?\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 274,
      "extraction_method": "Direct"
    },
    {
      "page_number": 219,
      "chapter": null,
      "content": "19\nPaging: Faster Translations (TLBs)\nUsing paging as the core mechanism to support virtual memory can lead\nto high performance overheads. By chopping the address space into small,\nﬁxed-sized units (i.e., pages), paging requires a large amount of mapping\ninformation. Because that mapping information is generally stored in\nphysical memory, paging logically requires an extra memory lookup for\neach virtual address generated by the program. Going to memory for\ntranslation information before every instruction fetch or explicit load or\nstore is prohibitively slow. And thus our problem:\nTHE CRUX:\nHOW TO SPEED UP ADDRESS TRANSLATION\nHow can we speed up address translation, and generally avoid the\nextra memory reference that paging seems to require? What hardware\nsupport is required? What OS involvement is needed?\nWhen we want to make things fast, the OS usually needs some help.\nAnd help often comes from the OS’s old friend: the hardware. To speed\naddress translation, we are going to add what is called (for historical rea-\nsons [CP78]) a translation-lookaside buffer, or TLB [C68, C95]. A TLB\nis part of the chip’s memory-management unit (MMU), and is simply a\nhardware cache of popular virtual-to-physical address translations; thus,\na better name would be an address-translation cache. Upon each virtual\nmemory reference, the hardware ﬁrst checks the TLB to see if the desired\ntranslation is held therein; if so, the translation is performed (quickly)\nwithout having to consult the page table (which has all translations). Be-\ncause of their tremendous performance impact, TLBs in a real sense make\nvirtual memory possible [C95].\n19.1\nTLB Basic Algorithm\nFigure 19.1 shows a rough sketch of how hardware might handle a\nvirtual address translation, assuming a simple linear page table (i.e., the\npage table is an array) and a hardware-managed TLB (i.e., the hardware\nhandles much of the responsibility of page table accesses; we’ll explain\nmore about this below).\n183\n",
      "content_length": 1976,
      "extraction_method": "Direct"
    },
    {
      "page_number": 220,
      "chapter": null,
      "content": "184\nPAGING: FASTER TRANSLATIONS (TLBS)\n1\nVPN = (VirtualAddress & VPN_MASK) >> SHIFT\n2\n(Success, TlbEntry) = TLB_Lookup(VPN)\n3\nif (Success == True)\n// TLB Hit\n4\nif (CanAccess(TlbEntry.ProtectBits) == True)\n5\nOffset\n= VirtualAddress & OFFSET_MASK\n6\nPhysAddr = (TlbEntry.PFN << SHIFT) | Offset\n7\nAccessMemory(PhysAddr)\n8\nelse\n9\nRaiseException(PROTECTION_FAULT)\n10\nelse\n// TLB Miss\n11\nPTEAddr = PTBR + (VPN * sizeof(PTE))\n12\nPTE = AccessMemory(PTEAddr)\n13\nif (PTE.Valid == False)\n14\nRaiseException(SEGMENTATION_FAULT)\n15\nelse if (CanAccess(PTE.ProtectBits) == False)\n16\nRaiseException(PROTECTION_FAULT)\n17\nelse\n18\nTLB_Insert(VPN, PTE.PFN, PTE.ProtectBits)\n19\nRetryInstruction()\nFigure 19.1: TLB Control Flow Algorithm\nThe algorithm the hardware follows works like this: ﬁrst, extract the\nvirtual page number (VPN) from the virtual address (Line 1 in Figure 19.1),\nand check if the TLB holds the translation for this VPN (Line 2). If it does,\nwe have a TLB hit, which means the TLB holds the translation. Success!\nWe can now extract the page frame number (PFN) from the relevant TLB\nentry, concatenate that onto the offset from the original virtual address,\nand form the desired physical address (PA), and access memory (Lines\n5–7), assuming protection checks do not fail (Line 4).\nIf the CPU does not ﬁnd the translation in the TLB (a TLB miss), we\nhave some more work to do. In this example, the hardware accesses the\npage table to ﬁnd the translation (Lines 11–12), and, assuming that the\nvirtual memory reference generated by the process is valid and accessi-\nble (Lines 13, 15), updates the TLB with the translation (Line 18). These\nset of actions are costly, primarily because of the extra memory reference\nneeded to access the page table (Line 12). Finally, once the TLB is up-\ndated, the hardware retries the instruction; this time, the translation is\nfound in the TLB, and the memory reference is processed quickly.\nThe TLB, like all caches, is built on the premise that in the common\ncase, translations are found in the cache (i.e., are hits). If so, little over-\nhead is added, as the TLB is found near the processing core and is de-\nsigned to be quite fast. When a miss occurs, the high cost of paging is\nincurred; the page table must be accessed to ﬁnd the translation, and an\nextra memory reference (or more, with more complex page tables) results.\nIf this happens often, the program will likely run noticeably more slowly;\nmemory accesses, relative to most CPU instructions, are quite costly, and\nTLB misses lead to more memory accesses. Thus, it is our hope to avoid\nTLB misses as much as we can.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2655,
      "extraction_method": "Direct"
    },
    {
      "page_number": 221,
      "chapter": null,
      "content": "PAGING: FASTER TRANSLATIONS (TLBS)\n185\nVPN = 15\nVPN = 14\nVPN = 13\nVPN = 12\nVPN = 11\nVPN = 10\nVPN = 09\nVPN = 08\nVPN = 07\nVPN = 06\nVPN = 05\nVPN = 04\nVPN = 03\nVPN = 02\nVPN = 01\nVPN = 00\n00\n04\n08\n12\n16\nOffset\na[0]\na[1]\na[2]\na[3]\na[4]\na[5]\na[6]\na[7]\na[8]\na[9]\nFigure 19.2: Example: An Array In A Tiny Address Space\n19.2\nExample: Accessing An Array\nTo make clear the operation of a TLB, let’s examine a simple virtual\naddress trace and see how a TLB can improve its performance. In this\nexample, let’s assume we have an array of 10 4-byte integers in memory,\nstarting at virtual address 100. Assume further that we have a small 8-bit\nvirtual address space, with 16-byte pages; thus, a virtual address breaks\ndown into a 4-bit VPN (there are 16 virtual pages) and a 4-bit offset (there\nare 16 bytes on each of those pages).\nFigure 19.2 shows the array laid out on the 16 16-byte pages of the sys-\ntem. As you can see, the array’s ﬁrst entry (a[0]) begins on (VPN=06, off-\nset=04); only three 4-byte integers ﬁt onto that page. The array continues\nonto the next page (VPN=07), where the next four entries (a[3] ... a[6])\nare found. Finally, the last three entries of the 10-entry array (a[7] ... a[9])\nare located on the next page of the address space (VPN=08).\nNow let’s consider a simple loop that accesses each array element,\nsomething that would look like this in C:\nint sum = 0;\nfor (i = 0; i < 10; i++) {\nsum += a[i];\n}\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 1459,
      "extraction_method": "Direct"
    },
    {
      "page_number": 222,
      "chapter": null,
      "content": "186\nPAGING: FASTER TRANSLATIONS (TLBS)\nFor the sake of simplicity, we will pretend that the only memory ac-\ncesses the loop generates are to the array (ignoring the variables i and\nsum, as well as the instructions themselves). When the ﬁrst array element\n(a[0]) is accessed, the CPU will see a load to virtual address 100. The\nhardware extracts the VPN from this (VPN=06), and uses that to check\nthe TLB for a valid translation. Assuming this is the ﬁrst time the pro-\ngram accesses the array, the result will be a TLB miss.\nThe next access is to a[1], and there is some good news here: a TLB\nhit! Because the second element of the array is packed next to the ﬁrst, it\nlives on the same page; because we’ve already accessed this page when\naccessing the ﬁrst element of the array, the translation is already loaded\ninto the TLB. And hence the reason for our success. Access to a[2] en-\ncounters similar success (another hit), because it too lives on the same\npage as a[0] and a[1].\nUnfortunately, when the program accesses a[3], we encounter an-\nother TLB miss. However, once again, the next entries (a[4] ... a[6])\nwill hit in the TLB, as they all reside on the same page in memory.\nFinally, access to a[7] causes one last TLB miss. The hardware once\nagain consults the page table to ﬁgure out the location of this virtual page\nin physical memory, and updates the TLB accordingly. The ﬁnal two ac-\ncesses (a[8] and a[9]) receive the beneﬁts of this TLB update; when the\nhardware looks in the TLB for their translations, two more hits result.\nLet us summarize TLB activity during our ten accesses to the array:\nmiss, hit, hit, miss, hit, hit, hit, miss, hit, hit. Thus, our TLB hit rate,\nwhich is the number of hits divided by the total number of accesses, is\n70%. Although this is not too high (indeed, we desire hit rates that ap-\nproach 100%), it is non-zero, which may be a surprise. Even though this\nis the ﬁrst time the program accesses the array, TLB performance gains\nbeneﬁt from spatial locality. The elements of the array are packed tightly\ninto pages (i.e., they are close to one another in space), and thus only the\nﬁrst access to an element on a page yields a TLB miss.\nAlso note the role that page size plays in this example. If the page size\nhad simply been twice as big (32 bytes, not 16), the array access would\nsuffer even fewer misses. As typical page sizes are more like 4KB, these\ntypes of dense, array-based accesses achieve excellent TLB performance,\nencountering only a single miss per page of accesses.\nOne last point about TLB performance: if the program, soon after this\nloop completes, accesses the array again, we’d likely see an even bet-\nter result, assuming that we have a big enough TLB to cache the needed\ntranslations: hit, hit, hit, hit, hit, hit, hit, hit, hit, hit. In this case, the\nTLB hit rate would be high because of temporal locality, i.e., the quick\nre-referencing of memory items in time. Like any cache, TLBs rely upon\nboth spatial and temporal locality for success, which are program proper-\nties. If the program of interest exhibits such locality (and many programs\ndo), the TLB hit rate will likely be high.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 3204,
      "extraction_method": "Direct"
    },
    {
      "page_number": 223,
      "chapter": null,
      "content": "PAGING: FASTER TRANSLATIONS (TLBS)\n187\nTIP: USE CACHING WHEN POSSIBLE\nCaching is one of the most fundamental performance techniques in com-\nputer systems, one that is used again and again to make the “common-\ncase fast” [HP06]. The idea behind hardware caches is to take advantage\nof locality in instruction and data references. There are usually two types\nof locality: temporal locality and spatial locality. With temporal locality,\nthe idea is that an instruction or data item that has been recently accessed\nwill likely be re-accessed soon in the future. Think of loop variables or in-\nstructions in a loop; they are accessed repeatedly over time. With spatial\nlocality, the idea is that if a program accesses memory at address x, it will\nlikely soon access memory near x. Imagine here streaming through an\narray of some kind, accessing one element and then the next. Of course,\nthese properties depend on the exact nature of the program, and thus are\nnot hard-and-fast laws but more like rules of thumb.\nHardware caches, whether for instructions, data, or address translations\n(as in our TLB) take advantage of locality by keeping copies of memory in\nsmall, fast on-chip memory. Instead of having to go to a (slow) memory\nto satisfy a request, the processor can ﬁrst check if a nearby copy exists\nin a cache; if it does, the processor can access it quickly (i.e., in a few cy-\ncles) and avoid spending the costly time it takes to access memory (many\nnanoseconds).\nYou might be wondering: if caches (like the TLB) are so great, why don’t\nwe just make bigger caches and keep all of our data in them? Unfor-\ntunately, this is where we run into more fundamental laws like those of\nphysics. If you want a fast cache, it has to be small, as issues like the\nspeed-of-light and other physical constraints become relevant. Any large\ncache by deﬁnition is slow, and thus defeats the purpose. Thus, we are\nstuck with small, fast caches; the question that remains is how to best use\nthem to improve performance.\n19.3\nWho Handles The TLB Miss?\nOne question that we must answer: who handles a TLB miss? Two an-\nswers are possible: the hardware, or the software (OS). In the olden days,\nthe hardware had complex instruction sets (sometimes called CISC, for\ncomplex-instruction set computers) and the people who built the hard-\nware didn’t much trust those sneaky OS people. Thus, the hardware\nwould handle the TLB miss entirely. To do this, the hardware has to\nknow exactly where the page tables are located in memory (via a page-\ntable base register, used in Line 11 in Figure 19.1), as well as their exact\nformat; on a miss, the hardware would “walk” the page table, ﬁnd the cor-\nrect page-table entry and extract the desired translation, update the TLB\nwith the translation, and retry the instruction. An example of an “older”\narchitecture that has hardware-managed TLBs is the Intel x86 architec-\nture, which uses a ﬁxed multi-level page table (see the next chapter for\ndetails); the current page table is pointed to by the CR3 register [I09].\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 3078,
      "extraction_method": "Direct"
    },
    {
      "page_number": 224,
      "chapter": null,
      "content": "188\nPAGING: FASTER TRANSLATIONS (TLBS)\n1\nVPN = (VirtualAddress & VPN_MASK) >> SHIFT\n2\n(Success, TlbEntry) = TLB_Lookup(VPN)\n3\nif (Success == True)\n// TLB Hit\n4\nif (CanAccess(TlbEntry.ProtectBits) == True)\n5\nOffset\n= VirtualAddress & OFFSET_MASK\n6\nPhysAddr = (TlbEntry.PFN << SHIFT) | Offset\n7\nRegister = AccessMemory(PhysAddr)\n8\nelse\n9\nRaiseException(PROTECTION_FAULT)\n10\nelse\n// TLB Miss\n11\nRaiseException(TLB_MISS)\nFigure 19.3: TLB Control Flow Algorithm (OS Handled)\nMore modern architectures (e.g., MIPS R10k [H93] or Sun’s SPARC v9\n[WG00], both RISC or reduced-instruction set computers) have what is\nknown as a software-managed TLB. On a TLB miss, the hardware sim-\nply raises an exception (line 11 in Figure 19.3), which pauses the current\ninstruction stream, raises the privilege level to kernel mode, and jumps\nto a trap handler. As you might guess, this trap handler is code within\nthe OS that is written with the express purpose of handling TLB misses.\nWhen run, the code will lookup the translation in the page table, use spe-\ncial “privileged” instructions to update the TLB, and return from the trap;\nat this point, the hardware retries the instruction (resulting in a TLB hit).\nLet’s discuss a couple of important details. First, the return-from-trap\ninstruction needs to be a little different than the return-from-trap we saw\nbefore when servicing a system call. In the latter case, the return-from-\ntrap should resume execution at the instruction after the trap into the OS,\njust as a return from a procedure call returns to the instruction imme-\ndiately following the call into the procedure. In the former case, when\nreturning from a TLB miss-handling trap, the hardware must resume ex-\necution at the instruction that caused the trap; this retry thus lets the in-\nstruction run again, this time resulting in a TLB hit. Thus, depending on\nhow a trap or exception was caused, the hardware must save a different\nPC when trapping into the OS, in order to resume properly when the time\nto do so arrives.\nSecond, when running the TLB miss-handling code, the OS needs to be\nextra careful not to cause an inﬁnite chain of TLB misses to occur. Many\nsolutions exist; for example, you could keep TLB miss handlers in physi-\ncal memory (where they are unmapped and not subject to address trans-\nlation), or reserve some entries in the TLB for permanently-valid transla-\ntions and use some of those permanent translation slots for the handler\ncode itself; these wired translations always hit in the TLB.\nThe primary advantage of the software-managed approach is ﬂexibil-\nity: the OS can use any data structure it wants to implement the page\ntable, without necessitating hardware change. Another advantage is sim-\nplicity; as you can see in the TLB control ﬂow (line 11 in Figure 19.3, in\ncontrast to lines 11–19 in Figure 19.1), the hardware doesn’t have to do\nmuch on a miss; it raises an exception, and the OS TLB miss handler does\nthe rest.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2998,
      "extraction_method": "Direct"
    },
    {
      "page_number": 225,
      "chapter": null,
      "content": "PAGING: FASTER TRANSLATIONS (TLBS)\n189\nASIDE: RISC VS. CISC\nIn the 1980’s, a great battle took place in the computer architecture com-\nmunity.\nOn one side was the CISC camp, which stood for Complex\nInstruction Set Computing; on the other side was RISC, for Reduced\nInstruction Set Computing [PS81]. The RISC side was spear-headed by\nDavid Patterson at Berkeley and John Hennessy at Stanford (who are also\nco-authors of some famous books [HP06]), although later John Cocke was\nrecognized with a Turing award for his earliest work on RISC [CM00].\nCISC instruction sets tend to have a lot of instructions in them, and each\ninstruction is relatively powerful. For example, you might see a string\ncopy, which takes two pointers and a length and copies bytes from source\nto destination. The idea behind CISC was that instructions should be\nhigh-level primitives, to make the assembly language itself easier to use,\nand to make code more compact.\nRISC instruction sets are exactly the opposite. A key observation behind\nRISC is that instruction sets are really compiler targets, and all compil-\ners really want are a few simple primitives that they can use to gener-\nate high-performance code. Thus, RISC proponents argued, let’s rip out\nas much from the hardware as possible (especially the microcode), and\nmake what’s left simple, uniform, and fast.\nIn the early days, RISC chips made a huge impact, as they were noticeably\nfaster [BC91]; many papers were written; a few companies were formed\n(e.g., MIPS and Sun). However, as time progressed, CISC manufacturers\nsuch as Intel incorporated many RISC techniques into the core of their\nprocessors, for example by adding early pipeline stages that transformed\ncomplex instructions into micro-instructions which could then be pro-\ncessed in a RISC-like manner. These innovations, plus a growing number\nof transistors on each chip, allowed CISC to remain competitive. The end\nresult is that the debate died down, and today both types of processors\ncan be made to run fast.\n19.4\nTLB Contents: What’s In There?\nLet’s look at the contents of the hardware TLB in more detail. A typical\nTLB might have 32, 64, or 128 entries and be what is called fully associa-\ntive. Basically, this just means that any given translation can be anywhere\nin the TLB, and that the hardware will search the entire TLB in parallel to\nﬁnd the desired translation. A typical TLB entry might look like this:\nVPN\nPFN\nother bits\nNote that both the VPN and PFN are present in each entry, as a trans-\nlation could end up in any of these locations (in hardware terms, the TLB\nis known as a fully-associative cache). The hardware searches the entries\nin parallel to see if there is a match.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2738,
      "extraction_method": "Direct"
    },
    {
      "page_number": 226,
      "chapter": null,
      "content": "190\nPAGING: FASTER TRANSLATIONS (TLBS)\nASIDE: TLB VALID BIT ̸= PAGE TABLE VALID BIT\nA common mistake is to confuse the valid bits found in a TLB with\nthose found in a page table. In a page table, when a page-table entry\n(PTE) is marked invalid, it means that the page has not been allocated by\nthe process, and should not be accessed by a correctly-working program.\nThe usual response when an invalid page is accessed is to trap to the OS,\nwhich will respond by killing the process.\nA TLB valid bit, in contrast, simply refers to whether a TLB entry has a\nvalid translation within it. When a system boots, for example, a common\ninitial state for each TLB entry is to be set to invalid, because no address\ntranslations are yet cached there. Once virtual memory is enabled, and\nonce programs start running and accessing their virtual address spaces,\nthe TLB is slowly populated, and thus valid entries soon ﬁll the TLB.\nThe TLB valid bit is quite useful when performing a context switch too,\nas we’ll discuss further below. By setting all TLB entries to invalid, the\nsystem can ensure that the about-to-be-run process does not accidentally\nuse a virtual-to-physical translation from a previous process.\nMore interesting are the “other bits”. For example, the TLB commonly\nhas a valid bit, which says whether the entry has a valid translation or\nnot. Also common are protection bits, which determine how a page can\nbe accessed (as in the page table). For example, code pages might be\nmarked read and execute, whereas heap pages might be marked read and\nwrite. There may also be a few other ﬁelds, including an address-space\nidentiﬁer, a dirty bit, and so forth; see below for more information.\n19.5\nTLB Issue: Context Switches\nWith TLBs, some new issues arise when switching between processes\n(and hence address spaces). Speciﬁcally, the TLB contains virtual-to-physical\ntranslations that are only valid for the currently running process; these\ntranslations are not meaningful for other processes. As a result, when\nswitching from one process to another, the hardware or OS (or both) must\nbe careful to ensure that the about-to-be-run process does not accidentally\nuse translations from some previously run process.\nTo understand this situation better, let’s look at an example. When one\nprocess (P1) is running, it assumes the TLB might be caching translations\nthat are valid for it, i.e., that come from P1’s page table. Assume, for this\nexample, that the 10th virtual page of P1 is mapped to physical frame 100.\nIn this example, assume another process (P2) exists, and the OS soon\nmight decide to perform a context switch and run it. Assume here that\nthe 10th virtual page of P2 is mapped to physical frame 170. If entries for\nboth processes were in the TLB, the contents of the TLB would be:\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2840,
      "extraction_method": "Direct"
    },
    {
      "page_number": 227,
      "chapter": null,
      "content": "PAGING: FASTER TRANSLATIONS (TLBS)\n191\nVPN\nPFN\nvalid\nprot\n10\n100\n1\nrwx\n—\n—\n0\n—\n10\n170\n1\nrwx\n—\n—\n0\n—\nIn the TLB above, we clearly have a problem: VPN 10 translates to\neither PFN 100 (P1) or PFN 170 (P2), but the hardware can’t distinguish\nwhich entry is meant for which process. Thus, we need to do some more\nwork in order for the TLB to correctly and efﬁciently support virtualiza-\ntion across multiple processes. And thus, a crux:\nTHE CRUX:\nHOW TO MANAGE TLB CONTENTS ON A CONTEXT SWITCH\nWhen context-switching between processes, the translations in the TLB\nfor the last process are not meaningful to the about-to-be-run process.\nWhat should the hardware or OS do in order to solve this problem?\nThere are a number of possible solutions to this problem. One ap-\nproach is to simply ﬂush the TLB on context switches, thus emptying\nit before running the next process.\nOn a software-based system, this\ncan be accomplished with an explicit (and privileged) hardware instruc-\ntion; with a hardware-managed TLB, the ﬂush could be enacted when the\npage-table base register is changed (note the OS must change the PTBR\non a context switch anyhow). In either case, the ﬂush operation simply\nsets all valid bits to 0, essentially clearing the contents of the TLB.\nBy ﬂushing the TLB on each context switch, we now have a working\nsolution, as a process will never accidentally encounter the wrong trans-\nlations in the TLB. However, there is a cost: each time a process runs, it\nmust incur TLB misses as it touches its data and code pages. If the OS\nswitches between processes frequently, this cost may be high.\nTo reduce this overhead, some systems add hardware support to en-\nable sharing of the TLB across context switches. In particular, some hard-\nware systems provide an address space identiﬁer (ASID) ﬁeld in the\nTLB. You can think of the ASID as a process identiﬁer (PID), but usu-\nally it has fewer bits (e.g., 8 bits for the ASID versus 32 bits for a PID).\nIf we take our example TLB from above and add ASIDs, it is clear\nprocesses can readily share the TLB: only the ASID ﬁeld is needed to dif-\nferentiate otherwise identical translations. Here is a depiction of a TLB\nwith the added ASID ﬁeld:\nVPN\nPFN\nvalid\nprot\nASID\n10\n100\n1\nrwx\n1\n—\n—\n0\n—\n—\n10\n170\n1\nrwx\n2\n—\n—\n0\n—\n—\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2311,
      "extraction_method": "Direct"
    },
    {
      "page_number": 228,
      "chapter": null,
      "content": "192\nPAGING: FASTER TRANSLATIONS (TLBS)\nThus, with address-space identiﬁers, the TLB can hold translations\nfrom different processes at the same time without any confusion.\nOf\ncourse, the hardware also needs to know which process is currently run-\nning in order to perform translations, and thus the OS must, on a context\nswitch, set some privileged register to the ASID of the current process.\nAs an aside, you may also have thought of another case where two\nentries of the TLB are remarkably similar. In this example, there are two\nentries for two different processes with two different VPNs that point to\nthe same physical page:\nVPN\nPFN\nvalid\nprot\nASID\n10\n101\n1\nr-x\n1\n—\n—\n0\n—\n—\n50\n101\n1\nr-x\n2\n—\n—\n0\n—\n—\nThis situation might arise, for example, when two processes share a\npage (a code page, for example). In the example above, Process 1 is shar-\ning physical page 101 with Process 2; P1 maps this page into the 10th\npage of its address space, whereas P2 maps it to the 50th page of its ad-\ndress space. Sharing of code pages (in binaries, or shared libraries) is\nuseful as it reduces the number of physical pages in use, thus reducing\nmemory overheads.\n19.6\nIssue: Replacement Policy\nAs with any cache, and thus also with the TLB, one more issue that we\nmust consider is cache replacement. Speciﬁcally, when we are installing\na new entry in the TLB, we have to replace an old one, and thus the\nquestion: which one to replace?\nTHE CRUX: HOW TO DESIGN TLB REPLACEMENT POLICY\nWhich TLB entry should be replaced when we add a new TLB entry?\nThe goal, of course, being to minimize the miss rate (or increase hit rate)\nand thus improve performance.\nWe will study such policies in some detail when we tackle the problem\nof swapping pages to disk in a virtual memory system; here we’ll just\nhighlight a few of typical policies. One common approach is to evict the\nleast-recently-used or LRU entry. The idea here is to take advantage of\nlocality in the memory-reference stream; thus, it is likely that an entry that\nhas not recently been used is a good candidate for eviction as (perhaps)\nit won’t soon be referenced again. Another typical approach is to use a\nrandom policy. Randomness sometimes makes a bad decision but has the\nnice property that there are not any weird corner case behaviors that can\ncause pessimal behavior, e.g., think of a loop accessing n+1 pages, a TLB\nof size n, and an LRU replacement policy.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2458,
      "extraction_method": "Direct"
    },
    {
      "page_number": 229,
      "chapter": null,
      "content": "PAGING: FASTER TRANSLATIONS (TLBS)\n193\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31\nVPN\nG\nASID\nPFN\nC\nD V\nFigure 19.4: A MIPS TLB Entry\n19.7\nA Real TLB Entry\nFinally, let’s brieﬂy look at a real TLB. This example is from the MIPS\nR4000 [H93], a modern system that uses software-managed TLBs. All 64\nbits of this TLB entry can be seen in Figure 19.4.\nThe MIPS R4000 supports a 32-bit address space with 4KB pages. Thus,\nwe would expect a 20-bit VPN and 12-bit offset in our typical virtual ad-\ndress. However, as you can see in the TLB, there are only 19 bits for the\nVPN; as it turns out, user addresses will only come from half the address\nspace (the rest reserved for the kernel) and hence only 19 bits of VPN\nare needed. The VPN translates to up to a 24-bit physical frame number\n(PFN), and hence can support systems with up to 64GB of (physical) main\nmemory (224 4KB pages).\nThere are a few other interesting bits in the MIPS TLB. We see a global\nbit (G), which is used for pages that are globally-shared among processes.\nThus, if the global bit is set, the ASID is ignored. We also see the 8-bit\nASID, which the OS can use to distinguish between address spaces (as\ndescribed above). One question for you: what should the OS do if there\nare more than 256 (28) processes running at a time? Finally, we see 3\nCoherence (C) bits, which determine how a page is cached by the hardware\n(a bit beyond the scope of these notes); a dirty bit which is marked when\nthe page has been written to (we’ll see the use of this later); a valid bit\nwhich tells the hardware if there is a valid translation present in the entry.\nThere is also a page mask ﬁeld (not shown), which supports multiple page\nsizes; we’ll see later why having larger pages might be useful. Finally,\nsome of the 64 bits are unused (shaded gray in the diagram).\nMIPS TLBs usually have 32 or 64 of these entries, most of which are\nused by user processes as they run. However, a few are reserved for the\nOS. A wired register can be set by the OS to tell the hardware how many\nslots of the TLB to reserve for the OS; the OS uses these reserved map-\npings for code and data that it wants to access during critical times, where\na TLB miss would be problematic (e.g., in the TLB miss handler).\nBecause the MIPS TLB is software managed, there needs to be instruc-\ntions to update the TLB. The MIPS provides four such instructions: TLBP,\nwhich probes the TLB to see if a particular translation is in there; TLBR,\nwhich reads the contents of a TLB entry into registers; TLBWI, which re-\nplaces a speciﬁc TLB entry; and TLBWR, which replaces a random TLB\nentry. The OS uses these instructions to manage the TLB’s contents. It is\nof course critical that these instructions are privileged; imagine what a\nuser process could do if it could modify the contents of the TLB (hint: just\nabout anything, including take over the machine, run its own malicious\n“OS”, or even make the Sun disappear).\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 3019,
      "extraction_method": "Direct"
    },
    {
      "page_number": 230,
      "chapter": null,
      "content": "194\nPAGING: FASTER TRANSLATIONS (TLBS)\nTIP: RAM ISN’T ALWAYS RAM (CULLER’S LAW)\nThe term random-access memory, or RAM, implies that you can access\nany part of RAM just as quickly as another. While it is generally good to\nthink of RAM in this way, because of hardware/OS features such as the\nTLB, accessing a particular page of memory may be costly, particularly if\nthat page isn’t currently mapped by your TLB. Thus, it is always good to\nremember the implementation tip: RAM isn’t always RAM. Sometimes\nrandomly accessing your address space, particular if the number of pages\naccessed exceeds the TLB coverage, can lead to severe performance penal-\nties. Because one of our advisors, David Culler, used to always point to\nthe TLB as the source of many performance problems, we name this law\nin his honor: Culler’s Law.\n19.8\nSummary\nWe have seen how hardware can help us make address translation\nfaster. By providing a small, dedicated on-chip TLB as an address-translation\ncache, most memory references will hopefully be handled without having\nto access the page table in main memory. Thus, in the common case,\nthe performance of the program will be almost as if memory isn’t being\nvirtualized at all, an excellent achievement for an operating system, and\ncertainly essential to the use of paging in modern systems.\nHowever, TLBs do not make the world rosy for every program that\nexists. In particular, if the number of pages a program accesses in a short\nperiod of time exceeds the number of pages that ﬁt into the TLB, the pro-\ngram will generate a large number of TLB misses, and thus run quite a\nbit more slowly. We refer to this phenomenon as exceeding the TLB cov-\nerage, and it can be quite a problem for certain programs. One solution,\nas we’ll discuss in the next chapter, is to include support for larger page\nsizes; by mapping key data structures into regions of the program’s ad-\ndress space that are mapped by larger pages, the effective coverage of the\nTLB can be increased. Support for large pages is often exploited by pro-\ngrams such as a database management system (a DBMS), which have\ncertain data structures that are both large and randomly-accessed.\nOne other TLB issue worth mentioning: TLB access can easily be-\ncome a bottleneck in the CPU pipeline, in particular with what is called a\nphysically-indexed cache. With such a cache, address translation has to\ntake place before the cache is accessed, which can slow things down quite\na bit. Because of this potential problem, people have looked into all sorts\nof clever ways to access caches with virtual addresses, thus avoiding the\nexpensive step of translation in the case of a cache hit. Such a virtually-\nindexed cache solves some performance problems, but introduces new\nissues into hardware design as well. See Wiggins’s ﬁne survey for more\ndetails [W03].\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2882,
      "extraction_method": "Direct"
    },
    {
      "page_number": 231,
      "chapter": null,
      "content": "PAGING: FASTER TRANSLATIONS (TLBS)\n195\nReferences\n[BC91] “Performance from Architecture: Comparing a RISC and a CISC\nwith Similar Hardware Organization”\nD. Bhandarkar and Douglas W. Clark\nCommunications of the ACM, September 1991\nA great and fair comparison between RISC and CISC. The bottom line: on similar hardware, RISC was\nabout a factor of three better in performance.\n[CM00] “The evolution of RISC technology at IBM”\nJohn Cocke and V. Markstein\nIBM Journal of Research and Development, 44:1/2\nA summary of the ideas and work behind the IBM 801, which many consider the ﬁrst true RISC micro-\nprocessor.\n[C95] “The Core of the Black Canyon Computer Corporation”\nJohn Couleur\nIEEE Annals of History of Computing, 17:4, 1995\nIn this fascinating historical note, Couleur talks about how he invented the TLB in 1964 while working\nfor GE, and the fortuitous collaboration that thus ensued with the Project MAC folks at MIT.\n[CG68] “Shared-access Data Processing System”\nJohn F. Couleur and Edward L. Glaser\nPatent 3412382, November 1968\nThe patent that contains the idea for an associative memory to store address translations. The idea,\naccording to Couleur, came in 1964.\n[CP78] “The architecture of the IBM System/370”\nR.P. Case and A. Padegs\nCommunications of the ACM. 21:1, 73-96, January 1978\nPerhaps the ﬁrst paper to use the term translation lookaside buffer. The name arises from the his-\ntorical name for a cache, which was a lookaside buffer as called by those developing the Atlas system\nat the University of Manchester; a cache of address translations thus became a translation lookaside\nbuffer. Even though the term lookaside buffer fell out of favor, TLB seems to have stuck, for whatever\nreason.\n[H93] “MIPS R4000 Microprocessor User’s Manual”.\nJoe Heinrich, Prentice-Hall, June 1993\nAvailable: http://cag.csail.mit.edu/raw/\ndocuments/R4400 Uman book Ed2.pdf\n[HP06] “Computer Architecture: A Quantitative Approach”\nJohn Hennessy and David Patterson\nMorgan-Kaufmann, 2006\nA great book about computer architecture. We have a particular attachment to the classic ﬁrst edition.\n[I09] “Intel 64 and IA-32 Architectures Software Developer’s Manuals”\nIntel, 2009\nAvailable: http://www.intel.com/products/processor/manuals\nIn particular, pay attention to “Volume 3A: System Programming Guide Part 1” and “Volume 3B:\nSystem Programming Guide Part 2”\n[PS81] “RISC-I: A Reduced Instruction Set VLSI Computer”\nD.A. Patterson and C.H. Sequin\nISCA ’81, Minneapolis, May 1981\nThe paper that introduced the term RISC, and started the avalanche of research into simplifying com-\nputer chips for performance.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2648,
      "extraction_method": "Direct"
    },
    {
      "page_number": 232,
      "chapter": null,
      "content": "196\nPAGING: FASTER TRANSLATIONS (TLBS)\n[SB92] “CPU Performance Evaluation and Execution Time Prediction\nUsing Narrow Spectrum Benchmarking”\nRafael H. Saavedra-Barrera\nEECS Department, University of California, Berkeley\nTechnical Report No. UCB/CSD-92-684, February 1992\nwww.eecs.berkeley.edu/Pubs/TechRpts/1992/CSD-92-684.pdf\nA great dissertation about how to predict execution time of applications by breaking them down into\nconstituent pieces and knowing the cost of each piece. Probably the most interesting part that comes out\nof this work is the tool to measure details of the cache hierarchy (described in Chapter 5). Make sure to\ncheck out the wonderful diagrams therein.\n[W03] “A Survey on the Interaction Between Caching, Translation and Protection”\nAdam Wiggins\nUniversity of New South Wales TR UNSW-CSE-TR-0321, August, 2003\nAn excellent survey of how TLBs interact with other parts of the CPU pipeline, namely hardware caches.\n[WG00] “The SPARC Architecture Manual: Version 9”\nDavid L. Weaver and Tom Germond, September 2000\nSPARC International, San Jose, California\nAvailable: http://www.sparc.org/standards/SPARCV9.pdf\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1180,
      "extraction_method": "Direct"
    },
    {
      "page_number": 233,
      "chapter": null,
      "content": "PAGING: FASTER TRANSLATIONS (TLBS)\n197\nHomework (Measurement)\nIn this homework, you are to measure the size and cost of accessing\na TLB. The idea is based on work by Saavedra-Barrera [SB92], who de-\nveloped a simple but beautiful method to measure numerous aspects of\ncache hierarchies, all with a very simple user-level program. Read his\nwork for more details.\nThe basic idea is to access some number of pages within large data\nstructure (e.g., an array) and to time those accesses. For example, let’s say\nthe TLB size of a machine happens to be 4 (which would be very small,\nbut useful for the purposes of this discussion). If you write a program\nthat touches 4 or fewer pages, each access should be a TLB hit, and thus\nrelatively fast. However, once you touch 5 pages or more, repeatedly in a\nloop, each access will suddenly jump in cost, to that of a TLB miss.\nThe basic code to loop through an array once should look like this:\nint jump = PAGESIZE / sizeof(int);\nfor (i = 0; i < NUMPAGES * jump; i += jump) {\na[i] += 1;\n}\nIn this loop, one integer per page of the the array a is updated, up\nto the number of pages speciﬁed by NUMPAGES. By timing such a loop\nrepeatedly (say, a few hundred million times in another loop around this\none, or however many loops are needed to run for a few seconds), you\ncan time how long each access takes (on average). By looking for jumps\nin cost as NUMPAGES increases, you can roughly determine how big the\nﬁrst-level TLB is, determine whether a second-level TLB exists (and how\nbig it is if it does), and in general get a good sense of how TLB hits and\nmisses can affect performance.\nHere is an example graph:\nAs you can see in the graph, when just a few pages are accessed (8\nor fewer), the average access time is roughly 5 nanoseconds. When 16\nor more pages are accessed, there is a sudden jump to about 20 nanosec-\nonds per access. A ﬁnal jump in cost occurs at around 1024 pages, at\nwhich point each access takes around 70 nanoseconds. From this data,\nwe can conclude that there is a two-level TLB hierarchy; the ﬁrst is quite\nsmall (probably holding between 8 and 16 entries); the second is larger\nbut slower (holding roughly 512 entries). The overall difference between\nhits in the ﬁrst-level TLB and misses is quite large, roughly a factor of\nfourteen. TLB performance matters!\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2365,
      "extraction_method": "Direct"
    },
    {
      "page_number": 234,
      "chapter": null,
      "content": "198\nPAGING: FASTER TRANSLATIONS (TLBS)\n1\n4\n16\n64\n256 1024\n0\n20\n40\n60\n80\nTLB Size Measurement\nNumber Of Pages\nTime Per Access (ns)\nFigure 19.5: Discovering TLB Sizes and Miss Costs\nQuestions\n• For timing, you’ll need to use a timer such as that made available\nby gettimeofday(). How precise is such a timer? How long\ndoes an operation have to take in order for you to time it precisely?\n(this will help determine how many times, in a loop, you’ll have to\nrepeat a page access in order to time it successfully)\n• Write the program, called tlb.c, that can roughly measure the cost\nof accessing each page. Inputs to the program should be: the num-\nber of pages to touch and the number of trials.\n• Now write a script in your favorite scripting language (csh, python,\netc.) to run this program, while varying the number of pages ac-\ncessed from 1 up to a few thousand, perhaps incrementing by a\nfactor of two per iteration. Run the script on different machines\nand gather some data. How many trials are needed to get reliable\nmeasurements?\n• Next, graph the results, making a graph that looks similar to the\none above. Use a good tool like ploticus. Visualization usually\nmakes the data much easier to digest; why do you think that is?\n• One thing to watch out for is compiler optimzation. Compilers do\nall sorts of clever things, including removing loops which incre-\nment values that no other part of the program subsequently uses.\nHow can you ensure the compiler does not remove the main loop\nabove from your TLB size estimator?\n• Another thing to watch out for is the fact that most systems today\nship with multiple CPUs, and each CPU, of course, has its own TLB\nhierarchy. To really get good measurements, you have to run your\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1774,
      "extraction_method": "Direct"
    },
    {
      "page_number": 235,
      "chapter": null,
      "content": "PAGING: FASTER TRANSLATIONS (TLBS)\n199\ncode on just one CPU, instead of letting the scheduler bounce it\nfrom one CPU to the next. How can you do that? (hint: look up\n“pinning a thread” on Google for some clues) What will happen if\nyou don’t do this, and the code moves from one CPU to the other?\n• Another issue that might arise relates to initialization. If you don’t\ninitialize the array a above before accessing it, the ﬁrst time you\naccess it will be very expensive, due to initial access costs such as\ndemand zeroing. Will this affect your code and its timing? What\ncan you do to counterbalance these potential costs?\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 664,
      "extraction_method": "Direct"
    },
    {
      "page_number": 236,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 237,
      "chapter": null,
      "content": "20\nPaging: Smaller Tables\nWe now tackle the second problem that paging introduces: page tables\nare too big and thus consume too much memory. Let’s start out with\na linear page table. As you might recall1, linear page tables get pretty\nbig. Assume again a 32-bit address space (232 bytes), with 4KB (212 byte)\npages and a 4-byte page-table entry. An address space thus has roughly\none million virtual pages in it ( 232\n212 ); multiply by the page-table size and\nyou see that our page table is 4MB in size. Recall also: we usually have\none page table for every process in the system! With a hundred active pro-\ncesses (not uncommon on a modern system), we will be allocating hun-\ndreds of megabytes of memory just for page tables! As a result, we are in\nsearch of some techniques to reduce this heavy burden. There are a lot of\nthem, so let’s get going. But not before our crux:\nCRUX: HOW TO MAKE PAGE TABLES SMALLER?\nSimple array-based page tables (usually called linear page tables) are\ntoo big, taking up far too much memory on typical systems. How can we\nmake page tables smaller? What are the key ideas? What inefﬁciencies\narise as a result of these new data structures?\n20.1\nSimple Solution: Bigger Pages\nWe could reduce the size of the page table in one simple way: use\nbigger pages. Take our 32-bit address space again, but this time assume\n16KB pages. We would thus have an 18-bit VPN plus a 14-bit offset. As-\nsuming the same size for each PTE (4 bytes), we now have 218 entries in\nour linear page table and thus a total size of 1MB per page table, a factor\n1Or indeed, you might not; this paging thing is getting out of control, no? That said,\nalways make sure you understand the problem you are solving before moving onto the solution;\nindeed, if you understand the problem, you can often derive the solution yourself. Here, the\nproblem should be clear: simple linear (array-based) page tables are too big.\n201\n",
      "content_length": 1921,
      "extraction_method": "Direct"
    },
    {
      "page_number": 238,
      "chapter": null,
      "content": "202\nPAGING: SMALLER TABLES\nASIDE: MULTIPLE PAGE SIZES\nAs an aside, do note that many architectures (e.g., MIPS, SPARC, x86-64)\nnow support multiple page sizes. Usually, a small (4KB or 8KB) page\nsize is used. However, if a “smart” application requests it, a single large\npage (e.g., of size 4MB) can be used for a speciﬁc portion of the address\nspace, enabling such applications to place a frequently-used (and large)\ndata structure in such a space while consuming only a single TLB en-\ntry. This type of large page usage is common in database management\nsystems and other high-end commercial applications. The main reason\nfor multiple page sizes is not to save page table space, however; it is to\nreduce pressure on the TLB, enabling a program to access more of its ad-\ndress space without suffering from too many TLB misses. However, as\nresearchers have shown [N+02], using multiple page sizes makes the OS\nvirtual memory manager notably more complex, and thus large pages\nare sometimes most easily used simply by exporting a new interface to\napplications to request large pages directly.\nof four reduction in size of the page table (not surprisingly, the reduction\nexactly mirrors the factor of four increase in page size).\nThe major problem with this approach, however, is that big pages lead\nto waste within each page, a problem known as internal fragmentation\n(as the waste is internal to the unit of allocation). Applications thus end\nup allocating pages but only using little bits and pieces of each, and mem-\nory quickly ﬁlls up with these overly-large pages. Thus, most systems use\nrelatively small page sizes in the common case: 4KB (as in x86) or 8KB (as\nin SPARCv9). Our problem will not be solved so simply, alas.\n20.2\nHybrid Approach: Paging and Segments\nWhenever you have two reasonable but different approaches to some-\nthing in life, you should always examine the combination of the two to\nsee if you can obtain the best of both worlds. We call such a combination a\nhybrid. For example, why eat just chocolate or plain peanut butter when\nyou can instead combine the two in a lovely hybrid known as the Reese’s\nPeanut Butter Cup [M28]?\nYears ago, the creators of Multics (in particular Jack Dennis) chanced\nupon such an idea in the construction of the Multics virtual memory sys-\ntem [M07]. Speciﬁcally, Dennis had the idea of combining paging and\nsegmentation in order to reduce the memory overhead of page tables.\nWe can see why this might work by examining a typical linear page ta-\nble in more detail. Assume we have an address space in which the used\nportions of the heap and stack are small. For the example, we use a tiny\n16KB address space with 1KB pages (Figure 20.1); the page table for this\naddress space is in Table 20.1.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2798,
      "extraction_method": "Direct"
    },
    {
      "page_number": 239,
      "chapter": null,
      "content": "PAGING: SMALLER TABLES\n203\ncode\nheap\nstack\nVirtual Address Space\nPhysical Memory\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\nFigure 20.1: A 16-KB Address Space With 1-KB Pages\nThis example assumes the single code page (VPN 0) is mapped to\nphysical page 10, the single heap page (VPN 4) to physical page 23, and\nthe two stack pages at the other end of the address space (VPNs 14 and\n15) are mapped to physical pages 28 and 4, respectively. As you can see\nfrom the picture, most of the page table is unused, full of invalid entries.\nWhat a waste! And this is for a tiny 16KB address space. Imagine the\npage table of a 32-bit address space and all the potential wasted space in\nthere! Actually, don’t imagine such a thing; it’s far too gruesome.\nPFN\nvalid\nprot\npresent\ndirty\n10\n1\nr-x\n1\n0\n-\n0\n—\n-\n-\n-\n0\n—\n-\n-\n-\n0\n—\n-\n-\n23\n1\nrw-\n1\n1\n-\n0\n—\n-\n-\n-\n0\n—\n-\n-\n-\n0\n—\n-\n-\n-\n0\n—\n-\n-\n-\n0\n—\n-\n-\n-\n0\n—\n-\n-\n-\n0\n—\n-\n-\n-\n0\n—\n-\n-\n-\n0\n—\n-\n-\n28\n1\nrw-\n1\n1\n4\n1\nrw-\n1\n1\nTable 20.1: A Page Table For 16-KB Address Space\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 1113,
      "extraction_method": "Direct"
    },
    {
      "page_number": 240,
      "chapter": null,
      "content": "204\nPAGING: SMALLER TABLES\nThus, our hybrid approach: instead of having a single page table for\nthe entire address space of the process, why not have one per logical seg-\nment? In this example, we might thus have three page tables, one for the\ncode, heap, and stack parts of the address space.\nNow, remember with segmentation, we had a base register that told\nus where each segment lived in physical memory, and a bound or limit\nregister that told us the size of said segment. In our hybrid, we still have\nthose structures in the MMU; here, we use the base not to point to the\nsegment itself but rather to hold the physical address of the page table of that\nsegment. The bounds register is used to indicate the end of the page table\n(i.e., how many valid pages it has).\nLet’s do a simple example to clarify. Assume a 32-bit virtual address\nspace with 4KB pages, and an address space split into four segments.\nWe’ll only use three segments for this example: one for code, one for\nheap, and one for stack.\nTo determine which segment an address refers to, we’ll use the top\ntwo bits of the address space. Let’s assume 00 is the unused segment,\nwith 01 for code, 10 for the heap, and 11 for the stack. Thus, a virtual\naddress looks like this:\n31 30 29 28 27 26 25 24 23 22 21 20 19 18 17 16 15 14 13 12 11 10\n9\n8\n7\n6\n5\n4\n3\n2\n1\n0\nSeg\nVPN\nOffset\nIn the hardware, assume that there are thus three base/bounds pairs,\none each for code, heap, and stack. When a process is running, the base\nregister for each of these segments contains the physical address of a lin-\near page table for that segment; thus, each process in the system now has\nthree page tables associated with it. On a context switch, these registers\nmust be changed to reﬂect the location of the page tables of the newly-\nrunning process.\nOn a TLB miss (assuming a hardware-managed TLB, i.e., where the\nhardware is responsible for handling TLB misses), the hardware uses the\nsegment bits (SN) to determine which base and bounds pair to use. The\nhardware then takes the physical address therein and combines it with\nthe VPN as follows to form the address of the page table entry (PTE):\nSN\n= (VirtualAddress & SEG_MASK) >> SN_SHIFT\nVPN\n= (VirtualAddress & VPN_MASK) >> VPN_SHIFT\nAddressOfPTE = Base[SN] + (VPN * sizeof(PTE))\nThis sequence should look familiar; it is virtually identical to what we\nsaw before with linear page tables. The only difference, of course, is the\nuse of one of three segment base registers instead of the single page table\nbase register.\nThe critical difference in our hybrid scheme is the presence of a bounds\nregister per segment; each bounds register holds the value of the maxi-\nmum valid page in the segment. For example, if the code segment is\nusing its ﬁrst three pages (0, 1, and 2), the code segment page table will\nonly have three entries allocated to it and the bounds register will be set\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2928,
      "extraction_method": "Direct"
    },
    {
      "page_number": 241,
      "chapter": null,
      "content": "PAGING: SMALLER TABLES\n205\nTIP: USE HYBRIDS\nWhen you have two good and seemingly opposing ideas, you should\nalways see if you can combine them into a hybrid that manages to achieve\nthe best of both worlds. Hybrid corn species, for example, are known to\nbe more robust than any naturally-occurring species. Of course, not all\nhybrids are a good idea; see the Zeedonk (or Zonkey), which is a cross of\na Zebra and a Donkey. If you don’t believe such a creature exists, look it\nup, and prepare to be amazed.\nto 3; memory accesses beyond the end of the segment will generate an ex-\nception and likely lead to the termination of the process. In this manner,\nour hybrid approach realizes a signiﬁcant memory savings compared to\nthe linear page table; unallocated pages between the stack and the heap\nno longer take up space in a page table (just to mark them as not valid).\nHowever, as you might notice, this approach is not without problems.\nFirst, it still requires us to use segmentation; as we discussed before, seg-\nmentation is not quite as ﬂexible as we would like, as it assumes a certain\nusage pattern of the address space; if we have a large but sparsely-used\nheap, for example, we can still end up with a lot of page table waste.\nSecond, this hybrid causes external fragmentation to arise again. While\nmost of memory is managed in page-sized units, page tables now can be\nof arbitrary size (in multiples of PTEs). Thus, ﬁnding free space for them\nin memory is more complicated. For these reasons, people continued to\nlook for better approaches to implementing smaller page tables.\n20.3\nMulti-level Page Tables\nA different approach doesn’t rely on segmentation but attacks the same\nproblem: how to get rid of all those invalid regions in the page table in-\nstead of keeping them all in memory? We call this approach a multi-level\npage table, as it turns the linear page table into something like a tree. This\napproach is so effective that many modern systems employ it (e.g., x86\n[BOH10]). We now describe this approach in detail.\nThe basic idea behind a multi-level page table is simple. First, chop up\nthe page table into page-sized units; then, if an entire page of page-table\nentries (PTEs) is invalid, don’t allocate that page of the page table at all.\nTo track whether a page of the page table is valid (and if valid, where it\nis in memory), use a new structure, called the page directory. The page\ndirectory thus either can be used to tell you where a page of the page\ntable is, or that the entire page of the page table contains no valid pages.\nFigure 20.2 shows an example. On the left of the ﬁgure is the classic\nlinear page table; even though most of the middle regions of the address\nspace are not valid, we still have to have page-table space allocated for\nthose regions (i.e., the middle two pages of the page table). On the right\nis a multi-level page table. The page directory marks just two pages of\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2961,
      "extraction_method": "Direct"
    },
    {
      "page_number": 242,
      "chapter": null,
      "content": "206\nPAGING: SMALLER TABLES\nvalid\nprot\nPFN\n1\nrx\n12\n1\nrx\n13\n0\n-\n-\n1\nrw\n100\n0\n-\n-\n0\n-\n-\n0\n-\n-\n0\n-\n-\n0\n-\n-\n0\n-\n-\n0\n-\n-\n0\n-\n-\n0\n-\n-\n0\n-\n-\n1\nrw\n86\n1\nrw\n15\nLinear Page Table\nPTBR\n201\nPFN 201\nPFN 202\nPFN 203\nPFN 204\nvalid\nprot\nPFN\n1\nrx\n12\n1\nrx\n13\n0\n-\n-\n1\nrw\n100\n0\n-\n-\n0\n-\n-\n1\nrw\n86\n1\nrw\n15\n[Page 1 of PT: Not Allocated]\n[Page 2 of PT: Not Allocated]\nPFN 201\nPFN 204\nMulti-level Page Table\nPDBR\n200\nvalid\nPFN\n1\n201\n0\n-\n0\n-\n1\n204\nPFN 200\nThe Page Directory\nFigure 20.2: Linear (Left) And Multi-Level (Right) Page Tables\nthe page table as valid (the ﬁrst and last); thus, just those two pages of the\npage table reside in memory. And thus you can see one way to visualize\nwhat a multi-level table is doing: it just makes parts of the linear page\ntable disappear (freeing those frames for other uses), and tracks which\npages of the page table are allocated with the page directory.\nThe page directory, in a simple two-level table, contains one entry per\npage of the page table. It consists of a number of page directory entries\n(PDE). A PDE (minimally) has a valid bit and a page frame number\n(PFN), similar to a PTE. However, as hinted at above, the meaning of\nthis valid bit is slightly different: if the PDE entry is valid, it means that\nat least one of the pages of the page table that the entry points to (via the\nPFN) is valid, i.e., in at least one PTE on that page pointed to by this PDE,\nthe valid bit in that PTE is set to one. If the PDE entry is not valid (i.e.,\nequal to zero), the rest of the PDE is not deﬁned.\nMulti-level page tables have some obvious advantages over approaches\nwe’ve seen thus far. First, and perhaps most obviously, the multi-level ta-\nble only allocates page-table space in proportion to the amount of address\nspace you are using; thus it is generally compact and supports sparse ad-\ndress spaces.\nSecond, if carefully constructed, each portion of the page table ﬁts\nneatly within a page, making it easier to manage memory; the OS can\nsimply grab the next free page when it needs to allocate or grow a page\ntable. Contrast this to a simple (non-paged) linear page table2, which\nis just an array of PTEs indexed by VPN; with such a structure, the en-\ntire linear page table must reside contiguously in physical memory. For\na large page table (say 4MB), ﬁnding such a large chunk of unused con-\ntiguous free physical memory can be quite a challenge. With a multi-level\n2We are making some assumptions here, i.e., that all page tables reside in their entirety in\nphysical memory (i.e., they are not swapped to disk); we’ll soon relax this assumption.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2618,
      "extraction_method": "Direct"
    },
    {
      "page_number": 243,
      "chapter": null,
      "content": "PAGING: SMALLER TABLES\n207\nTIP: UNDERSTAND TIME-SPACE TRADE-OFFS\nWhen building a data structure, one should always consider time-space\ntrade-offs in its construction. Usually, if you wish to make access to a par-\nticular data structure faster, you will have to pay a space-usage penalty\nfor the structure.\nstructure, we add a level of indirection through use of the page directory,\nwhich points to pieces of the page table; that indirection allows us to place\npage-table pages wherever we would like in physical memory.\nIt should be noted that there is a cost to multi-level tables; on a TLB\nmiss, two loads from memory will be required to get the right translation\ninformation from the page table (one for the page directory, and one for\nthe PTE itself), in contrast to just one load with a linear page table. Thus,\nthe multi-level table is a small example of a time-space trade-off. We\nwanted smaller tables (and got them), but not for free; although in the\ncommon case (TLB hit), performance is obviously identical, a TLB miss\nsuffers from a higher cost with this smaller table.\nAnother obvious negative is complexity. Whether it is the hardware or\nOS handling the page-table lookup (on a TLB miss), doing so is undoubt-\nedly more involved than a simple linear page-table lookup. Often we are\nwilling to increase complexity in order to improve performance or reduce\noverheads; in the case of a multi-level table, we make page-table lookups\nmore complicated in order to save valuable memory.\nA Detailed Multi-Level Example\nTo understand the idea behind multi-level page tables better, let’s do an\nexample. Imagine a small address space of size 16 KB, with 64-byte pages.\nThus, we have a 14-bit virtual address space, with 8 bits for the VPN and\n6 bits for the offset. A linear page table would have 28 (256) entries, even\nif only a small portion of the address space is in use. Figure 20.3 presents\none example of such an address space.\nstack\nstack\n(free)\n(free)\n... all free ...\n(free)\n(free)\nheap\nheap\n(free)\n(free)\ncode\ncode\n1111 1111\n1111 1110\n1111 1101\n1111 1100\n0000 0111\n0000 0110\n0000 0101\n0000 0100\n0000 0011\n0000 0010\n0000 0001\n0000 0000\n................\nFigure 20.3: A 16-KB Address Space With 64-byte Pages\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2262,
      "extraction_method": "Direct"
    },
    {
      "page_number": 244,
      "chapter": null,
      "content": "208\nPAGING: SMALLER TABLES\nTIP: BE WARY OF COMPLEXITY\nSystem designers should be wary of adding complexity into their sys-\ntem. What a good systems builder does is implement the least complex\nsystem that achieves the task at hand. For example, if disk space is abun-\ndant, you shouldn’t design a ﬁle system that works hard to use as few\nbytes as possible; similarly, if processors are fast, it is better to write a\nclean and understandable module within the OS than perhaps the most\nCPU-optimized, hand-assembled code for the task at hand. Be wary of\nneedless complexity, in prematurely-optimized code or other forms; such\napproaches make systems harder to understand, maintain, and debug.\nAs Antoine de Saint-Exupery famously wrote: “Perfection is ﬁnally at-\ntained not when there is no longer anything to add, but when there is no\nlonger anything to take away.” What he didn’t write: “It’s a lot easier to\nsay something about perfection than to actually achieve it.”\nIn this example, virtual pages 0 and 1 are for code, virtual pages 4 and\n5 for the heap, and virtual pages 254 and 255 for the stack; the rest of the\npages of the address space are unused.\nTo build a two-level page table for this address space, we start with\nour full linear page table and break it up into page-sized units. Recall our\nfull table (in this example) has 256 entries; assume each PTE is 4 bytes in\nsize. Thus, our page table is 1KB (256 × 4 bytes) in size. Given that we\nhave 64-byte pages, the 1-KB page table can be divided into 16 64-byte\npages; each page can hold 16 PTEs.\nWhat we need to understand now is how to take a VPN and use it to\nindex ﬁrst into the page directory and then into the page of the page table.\nRemember that each is an array of entries; thus, all we need to ﬁgure out\nis how to construct the index for each from pieces of the VPN.\nLet’s ﬁrst index into the page directory. Our page table in this example\nis small: 256 entries, spread across 16 pages. The page directory needs one\nentry per page of the page table; thus, it has 16 entries. As a result, we\nneed four bits of the VPN to index into the directory; we use the top four\nbits of the VPN, as follows:\n13\n12\n11\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\n0\nVPN\noffset\nPage Directory Index\nOnce we extract the page-directory index (PDIndex for short) from\nthe VPN, we can use it to ﬁnd the address of the page-directory entry\n(PDE) with a simple calculation: PDEAddr = PageDirBase + (PDIndex\n* sizeof(PDE)). This results in our page directory, which we now ex-\namine to make further progress in our translation.\nIf the page-directory entry is marked invalid, we know that the access\nis invalid, and thus raise an exception. If, however, the PDE is valid,\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2744,
      "extraction_method": "Direct"
    },
    {
      "page_number": 245,
      "chapter": null,
      "content": "PAGING: SMALLER TABLES\n209\nwe have more work to do. Speciﬁcally, we now have to fetch the page-\ntable entry (PTE) from the page of the page table pointed to by this page-\ndirectory entry. To ﬁnd this PTE, we have to index into the portion of the\npage table using the remaining bits of the VPN:\n13\n12\n11\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\n0\nVPN\noffset\nPage Directory Index\nPage Table Index\nThis page-table index (PTIndex for short) can then be used to index\ninto the page table itself, giving us the address of our PTE:\nPTEAddr = (PDE.PFN << SHIFT) + (PTIndex * sizeof(PTE))\nNote that the page-frame number (PFN) obtained from the page-directory\nentry must be left-shifted into place before combining it with the page-\ntable index to form the address of the PTE.\nTo see if this all makes sense, we’ll now ﬁll in a multi-level page ta-\nble with some actual values, and translate a single virtual address. Let’s\nbegin with the page directory for this example (left side of Table 20.2).\nIn the ﬁgure, you can see that each page directory entry (PDE) de-\nscribes something about a page of the page table for the address space.\nIn this example, we have two valid regions in the address space (at the\nbeginning and end), and a number of invalid mappings in-between.\nIn physical page 100 (the physical frame number of the 0th page of the\npage table), we have the ﬁrst page of 16 page table entries for the ﬁrst 16\nVPNs in the address space. See Table 20.2 (middle part) for the contents\nof this portion of the page table.\nPage Directory\nPage of PT (@PFN:100)\nPage of PT (@PFN:101)\nPFN\nvalid?\nPFN\nvalid\nprot\nPFN\nvalid\nprot\n100\n1\n10\n1\nr-x\n–\n0\n—\n——\n0\n23\n1\nr-x\n–\n0\n—\n——\n0\n–\n0\n—\n–\n0\n—\n——\n0\n–\n0\n—\n–\n0\n—\n——\n0\n80\n1\nrw-\n–\n0\n—\n——\n0\n59\n1\nrw-\n–\n0\n—\n——\n0\n–\n0\n—\n–\n0\n—\n——\n0\n–\n0\n—\n–\n0\n—\n——\n0\n–\n0\n—\n–\n0\n—\n——\n0\n–\n0\n—\n–\n0\n—\n——\n0\n–\n0\n—\n–\n0\n—\n——\n0\n–\n0\n—\n–\n0\n—\n——\n0\n–\n0\n—\n–\n0\n—\n——\n0\n–\n0\n—\n–\n0\n—\n——\n0\n–\n0\n—\n55\n1\nrw-\n101\n1\n–\n0\n—\n45\n1\nrw-\nTable 20.2: A Page Directory, And Pieces Of Page Table\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 1988,
      "extraction_method": "Direct"
    },
    {
      "page_number": 246,
      "chapter": null,
      "content": "210\nPAGING: SMALLER TABLES\nThis page of the page table contains the mappings for the ﬁrst 16\nVPNs; in our example, VPNs 0 and 1 are valid (the code segment), as\nare 4 and 5 (the heap). Thus, the table has mapping information for each\nof those pages. The rest of the entries are marked invalid.\nThe other valid page of page table is found inside PFN 101. This page\ncontains mappings for the last 16 VPNs of the address space; see Table\n20.2 (right) for details.\nIn the example, VPNs 254 and 255 (the stack) have valid mappings.\nHopefully, what we can see from this example is how much space savings\nare possible with a multi-level indexed structure. In this example, instead\nof allocating the full sixteen pages for a linear page table, we allocate only\nthree: one for the page directory, and two for the chunks of the page table\nthat have valid mappings. The savings for large (32-bit or 64-bit) address\nspaces could obviously be much greater.\nFinally, let’s use this information in order to perform a translation.\nHere is an address that refers to the 0th byte of VPN 254: 0x3F80, or\n11 1111 1000 0000 in binary.\nRecall that we will use the top 4 bits of the VPN to index into the\npage directory. Thus, 1111 will choose the last (15th, if you start at the\n0th) entry of the page directory above. This points us to a valid page\nof the page table located at address 101. We then use the next 4 bits\nof the VPN (1110) to index into that page of the page table and ﬁnd\nthe desired PTE. 1110 is the next-to-last (14th) entry on the page, and\ntells us that page 254 of our virtual address space is mapped at physi-\ncal page 55. By concatenating PFN=55 (or hex 0x37) with offset=000000,\nwe can thus form our desired physical address and issue the request to\nthe memory system: PhysAddr = (PTE.PFN << SHIFT) + offset\n= 00 1101 1100 0000 = 0x0DC0.\nYou should now have some idea of how to construct a two-level page\ntable, using a page directory which points to pages of the page table. Un-\nfortunately, however, our work is not done. As we’ll now discuss, some-\ntimes two levels of page table is not enough!\nMore Than Two Levels\nIn our example thus far, we’ve assumed that multi-level page tables only\nhave two levels: a page directory and then pieces of the page table. In\nsome cases, a deeper tree is possible (and indeed, needed).\nLet’s take a simple example and use it to show why a deeper multi-\nlevel table can be useful. In this example, assume we have a 30-bit virtual\naddress space, and a small (512 byte) page. Thus our virtual address has\na 21-bit virtual page number component and a 9-bit offset.\nRemember our goal in constructing a multi-level page table: to make\neach piece of the page table ﬁt within a single page. Thus far, we’ve only\nconsidered the page table itself; however, what if the page directory gets\ntoo big?\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2875,
      "extraction_method": "Direct"
    },
    {
      "page_number": 247,
      "chapter": null,
      "content": "PAGING: SMALLER TABLES\n211\nTo determine how many levels are needed in a multi-level table to\nmake all pieces of the page table ﬁt within a page, we start by determining\nhow many page-table entries ﬁt within a page. Given our page size of 512\nbytes, and assuming a PTE size of 4 bytes, you should see that you can ﬁt\n128 PTEs on a single page. When we index into a page of the page table,\nwe can thus conclude we’ll need the least signiﬁcant 7 bits (log2128) of\nthe VPN as an index:\n29 28 27 26 25 24 23 22 21 20 19 18 17 16 15 14 13 12 11 10 9\n8\n7\n6\n5\n4\n3\n2\n1\n0\nVPN\noffset\nPage Directory Index\nPage Table Index\nWhat you also might notice from the diagram above is how many bits\nare left into the (large) page directory: 14. If our page directory has 214\nentries, it spans not one page but 128, and thus our goal of making every\npiece of the multi-level page table ﬁt into a page vanishes.\nTo remedy this problem, we build a further level of the tree, by split-\nting the page directory itself into multiple pages, and then adding another\npage directory on top of that, to point to the pages of the page directory.\nWe can thus split up our virtual address as follows:\n29 28 27 26 25 24 23 22 21 20 19 18 17 16 15 14 13 12 11 10 9\n8\n7\n6\n5\n4\n3\n2\n1\n0\nVPN\noffset\nPD Index 0\nPD Index 1\nPage Table Index\nNow, when indexing the upper-level page directory, we use the very\ntop bits of the virtual address (PD Index 0 in the diagram); this index\ncan be used to fetch the page-directory entry from the top-level page di-\nrectory. If valid, the second level of the page directory is consulted by\ncombining the physical frame number from the top-level PDE and the\nnext part of the VPN (PD Index 1). Finally, if valid, the PTE address\ncan be formed by using the page-table index combined with the address\nfrom the second-level PDE. Whew! That’s a lot of work. And all just to\nlook something up in a multi-level table.\nThe Translation Process: Remember the TLB\nTo summarize the entire process of address translation using a two-level\npage table, we once again present the control ﬂow in algorithmic form\n(Figure 20.4). The ﬁgure shows what happens in hardware (assuming a\nhardware-managed TLB) upon every memory reference.\nAs you can see from the ﬁgure, before any of the complicated multi-\nlevel page table access occurs, the hardware ﬁrst checks the TLB; upon\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2387,
      "extraction_method": "Direct"
    },
    {
      "page_number": 248,
      "chapter": null,
      "content": "212\nPAGING: SMALLER TABLES\n1\nVPN = (VirtualAddress & VPN_MASK) >> SHIFT\n2\n(Success, TlbEntry) = TLB_Lookup(VPN)\n3\nif (Success == True)\n// TLB Hit\n4\nif (CanAccess(TlbEntry.ProtectBits) == True)\n5\nOffset\n= VirtualAddress & OFFSET_MASK\n6\nPhysAddr = (TlbEntry.PFN << SHIFT) | Offset\n7\nRegister = AccessMemory(PhysAddr)\n8\nelse\n9\nRaiseException(PROTECTION_FAULT)\n10\nelse\n// TLB Miss\n11\n// first, get page directory entry\n12\nPDIndex = (VPN & PD_MASK) >> PD_SHIFT\n13\nPDEAddr = PDBR + (PDIndex * sizeof(PDE))\n14\nPDE\n= AccessMemory(PDEAddr)\n15\nif (PDE.Valid == False)\n16\nRaiseException(SEGMENTATION_FAULT)\n17\nelse\n18\n// PDE is valid: now fetch PTE from page table\n19\nPTIndex = (VPN & PT_MASK) >> PT_SHIFT\n20\nPTEAddr = (PDE.PFN << SHIFT) + (PTIndex * sizeof(PTE))\n21\nPTE\n= AccessMemory(PTEAddr)\n22\nif (PTE.Valid == False)\n23\nRaiseException(SEGMENTATION_FAULT)\n24\nelse if (CanAccess(PTE.ProtectBits) == False)\n25\nRaiseException(PROTECTION_FAULT)\n26\nelse\n27\nTLB_Insert(VPN, PTE.PFN, PTE.ProtectBits)\n28\nRetryInstruction()\nFigure 20.4: Multi-level Page Table Control Flow\na hit, the physical address is formed directly without accessing the page\ntable at all, as before. Only upon a TLB miss does the hardware need to\nperform the full multi-level lookup. On this path, you can see the cost of\nour traditional two-level page table: two additional memory accesses to\nlook up a valid translation.\n20.4\nInverted Page Tables\nAn even more extreme space savings in the world of page tables is\nfound with inverted page tables. Here, instead of having many page\ntables (one per process of the system), we keep a single page table that\nhas an entry for each physical page of the system. The entry tells us which\nprocess is using this page, and which virtual page of that process maps to\nthis physical page.\nFinding the correct entry is now a matter of searching through this\ndata structure. A linear scan would be expensive, and thus a hash table is\noften built over the base structure to speed lookups. The PowerPC is one\nexample of such an architecture [JM98].\nMore generally, inverted page tables illustrate what we’ve said from\nthe beginning: page tables are just data structures. You can do lots of\ncrazy things with data structures, making them smaller or bigger, making\nthem slower or faster. Multi-level and inverted page tables are just two\nexamples of the many things one could do.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2415,
      "extraction_method": "Direct"
    },
    {
      "page_number": 249,
      "chapter": null,
      "content": "PAGING: SMALLER TABLES\n213\n20.5\nSwapping the Page Tables to Disk\nFinally, we discuss the relaxation of one ﬁnal assumption. Thus far,\nwe have assumed that page tables reside in kernel-owned physical mem-\nory. Even with our many tricks to reduce the size of page tables, it is still\npossible, however, that they may be too big to ﬁt into memory all at once.\nThus, some systems place such page tables in kernel virtual memory,\nthereby allowing the system to swap some of these page tables to disk\nwhen memory pressure gets a little tight. We’ll talk more about this in\na future chapter (namely, the case study on VAX/VMS), once we under-\nstand how to move pages in and out of memory in more detail.\n20.6\nSummary\nWe have now seen how real page tables are built; not necessarily just\nas linear arrays but as more complex data structures. The trade-offs such\ntables present are in time and space – the bigger the table, the faster a TLB\nmiss can be serviced, as well as the converse – and thus the right choice of\nstructure depends strongly on the constraints of the given environment.\nIn a memory-constrained system (like many older systems), small struc-\ntures make sense; in a system with a reasonable amount of memory and\nwith workloads that actively use a large number of pages, a bigger ta-\nble that speeds up TLB misses might be the right choice. With software-\nmanaged TLBs, the entire space of data structures opens up to the delight\nof the operating system innovator (hint: that’s you). What new struc-\ntures can you come up with? What problems do they solve? Think of\nthese questions as you fall asleep, and dream the big dreams that only\noperating-system developers can dream.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 1725,
      "extraction_method": "Direct"
    },
    {
      "page_number": 250,
      "chapter": null,
      "content": "214\nPAGING: SMALLER TABLES\nReferences\n[BOH10] “Computer Systems: A Programmer’s Perspective”\nRandal E. Bryant and David R. O’Hallaron\nAddison-Wesley, 2010\nWe have yet to ﬁnd a good ﬁrst reference to the multi-level page table. However, this great textbook by\nBryant and O’Hallaron dives into the details of x86, which at least is an early system that used such\nstructures. It’s also just a great book to have.\n[JM98] “Virtual Memory: Issues of Implementation”\nBruce Jacob and Trevor Mudge\nIEEE Computer, June 1998\nAn excellent survey of a number of different systems and their approach to virtualizing memory. Plenty\nof details on x86, PowerPC, MIPS, and other architectures.\n[LL82] “Virtual Memory Management in the VAX/VMS Operating System”\nHank Levy and P. Lipman\nIEEE Computer, Vol. 15, No. 3, March 1982\nA terriﬁc paper about a real virtual memory manager in a classic operating system, VMS. So terriﬁc, in\nfact, that we’ll use it to review everything we’ve learned about virtual memory thus far a few chapters\nfrom now.\n[M28] “Reese’s Peanut Butter Cups”\nMars Candy Corporation.\nApparently these ﬁne confections were invented in 1928 by Harry Burnett Reese, a former dairy farmer\nand shipping foreman for one Milton S. Hershey. At least, that is what it says on Wikipedia. If true,\nHershey and Reese probably hated each other’s guts, as any two chocolate barons should.\n[N+02] “Practical, Transparent Operating System Support for Superpages”\nJuan Navarro, Sitaram Iyer, Peter Druschel, Alan Cox\nOSDI ’02, Boston, Massachusetts, October 2002\nA nice paper showing all the details you have to get right to incorporate large pages, or superpages,\ninto a modern OS. Not as easy as you might think, alas.\n[M07] “Multics: History”\nAvailable: http://www.multicians.org/history.html\nThis amazing web site provides a huge amount of history on the Multics system, certainly one of the\nmost inﬂuential systems in OS history. The quote from therein: “Jack Dennis of MIT contributed\ninﬂuential architectural ideas to the beginning of Multics, especially the idea of combining paging and\nsegmentation.” (from Section 1.2.1)\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2162,
      "extraction_method": "Direct"
    },
    {
      "page_number": 251,
      "chapter": null,
      "content": "PAGING: SMALLER TABLES\n215\nHomework\nThis fun little homework tests if you understand how a multi-level\npage table works. And yes, there is some debate over the use of the term\n“fun” in the previous sentence. The program is called, perhaps unsur-\nprisingly: paging-multilevel-translate.py; see the README for\ndetails.\nQuestions\n• With a linear page table, you need a single register to locate the\npage table, assuming that hardware does the lookup upon a TLB\nmiss. How many registers do you need to locate a two-level page\ntable? A three-level table?\n• Use the simulator to perform translations given random seeds 0,\n1, and 2, and check your answers using the -c ﬂag. How many\nmemory references are needed to perform each lookup?\n• Given your understanding of how cache memory works, how do\nyou think memory references to the page table will behave in the\ncache? Will they lead to lots of cache hits (and thus fast accesses?)\nOr lots of misses (and thus slow accesses)?\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 1010,
      "extraction_method": "Direct"
    },
    {
      "page_number": 252,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 253,
      "chapter": null,
      "content": "21\nBeyond Physical Memory: Mechanisms\nThus far, we’ve assumed that an address space is unrealistically small\nand ﬁts into physical memory. In fact, we’ve been assuming that every\naddress space of every running process ﬁts into memory. We will now\nrelax these big assumptions, and assume that we wish to support many\nconcurrently-running large address spaces.\nTo do so, we require an additional level in the memory hierarchy.\nThus far, we have assumed that all pages reside in physical memory.\nHowever, to support large address spaces, the OS will need a place to\nstash away portions of address spaces that currently aren’t in great de-\nmand. In general, the characteristics of such a location are that it should\nhave more capacity than memory; as a result, it is generally slower (if it\nwere faster, we would just use it as memory, no?). In modern systems,\nthis role is usually served by a hard disk drive. Thus, in our memory\nhierarchy, big and slow hard drives sit at the bottom, with memory just\nabove. And thus we arrive at the crux of the problem:\nTHE CRUX: HOW TO GO BEYOND PHYSICAL MEMORY\nHow can the OS make use of a larger, slower device to transparently pro-\nvide the illusion of a large virtual address space?\nOne question you might have: why do we want to support a single\nlarge address space for a process? Once again, the answer is convenience\nand ease of use. With a large address space, you don’t have to worry\nabout if there is room enough in memory for your program’s data struc-\ntures; rather, you just write the program naturally, allocating memory as\nneeded. It is a powerful illusion that the OS provides, and makes your\nlife vastly simpler. You’re welcome! A contrast is found in older systems\nthat used memory overlays, which required programmers to manually\nmove pieces of code or data in and out of memory as they were needed\n[D97]. Try imagining what this would be like: before calling a function or\naccessing some data, you need to ﬁrst arrange for the code or data to be\nin memory; yuck!\n217\n",
      "content_length": 2021,
      "extraction_method": "Direct"
    },
    {
      "page_number": 254,
      "chapter": null,
      "content": "218\nBEYOND PHYSICAL MEMORY: MECHANISMS\nASIDE: STORAGE TECHNOLOGIES\nWe’ll delve much more deeply into how I/O devices actually work later\n(see the chapter on I/O devices). So be patient! And of course the slower\ndevice need not be a hard disk, but could be something more modern\nsuch as a Flash-based SSD. We’ll talk about those things too. For now,\njust assume we have a big and relatively-slow device which we can use\nto help us build the illusion of a very large virtual memory, even bigger\nthan physical memory itself.\nBeyond just a single process, the addition of swap space allows the OS\nto support the illusion of a large virtual memory for multiple concurrently-\nrunning processes. The invention of multiprogramming (running multi-\nple programs “at once”, to better utilize the machine) almost demanded\nthe ability to swap out some pages, as early machines clearly could not\nhold all the pages needed by all processes at once. Thus, the combina-\ntion of multiprogramming and ease-of-use leads us to want to support\nusing more memory than is physically available. It is something that all\nmodern VM systems do; it is now something we will learn more about.\n21.1\nSwap Space\nThe ﬁrst thing we will need to do is to reserve some space on the disk\nfor moving pages back and forth. In operating systems, we generally refer\nto such space as swap space, because we swap pages out of memory to it\nand swap pages into memory from it. Thus, we will simply assume that\nthe OS can read from and write to the swap space, in page-sized units. To\ndo so, the OS will need to remember the disk address of a given page.\nThe size of the swap space is important, as ultimately it determines\nthe maximum number of memory pages that can be in use by a system at\na given time. Let us assume for simplicity that it is very large for now.\nIn the tiny example (Figure 21.1), you can see a little example of a 4-\npage physical memory and an 8-page swap space. In the example, three\nprocesses (Proc 0, Proc 1, and Proc 2) are actively sharing physical mem-\nory; each of the three, however, only have some of their valid pages in\nmemory, with the rest located in swap space on disk. A fourth process\n(Proc 3) has all of its pages swapped out to disk, and thus clearly isn’t\ncurrently running. One block of swap remains free. Even from this tiny\nexample, hopefully you can see how using swap space allows the system\nto pretend that memory is larger than it actually is.\nWe should note that swap space is not the only on-disk location for\nswapping trafﬁc. For example, assume you are running a program binary\n(e.g., ls, or your own compiled main program). The code pages from this\nbinary are initially found on disk, and when the program runs, they are\nloaded into memory (either all at once when the program starts execution,\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2849,
      "extraction_method": "Direct"
    },
    {
      "page_number": 255,
      "chapter": null,
      "content": "BEYOND PHYSICAL MEMORY: MECHANISMS\n219\nPhysical\nMemory\nPFN 0\nProc 0\n[VPN 0]\nPFN 1\nProc 1\n[VPN 2]\nPFN 2\nProc 1\n[VPN 3]\nPFN 3\nProc 2\n[VPN 0]\nSwap\nSpace\nProc 0\n[VPN 1]\nBlock 0\nProc 0\n[VPN 2]\nBlock 1\n[Free]\nBlock 2\nProc 1\n[VPN 0]\nBlock 3\nProc 1\n[VPN 1]\nBlock 4\nProc 3\n[VPN 0]\nBlock 5\nProc 2\n[VPN 1]\nBlock 6\nProc 3\n[VPN 1]\nBlock 7\nFigure 21.1: Physical Memory and Swap Space\nor, as in modern systems, one page at a time when needed). However, if\nthe system needs to make room in physical memory for other needs, it\ncan safely re-use the memory space for these code pages, knowing that it\ncan later swap them in again from the on-disk binary in the ﬁle system.\n21.2\nThe Present Bit\nNow that we have some space on the disk, we need to add some ma-\nchinery higher up in the system in order to support swapping pages to\nand from the disk. Let us assume, for simplicity, that we have a system\nwith a hardware-managed TLB.\nRecall ﬁrst what happens on a memory reference. The running pro-\ncess generates virtual memory references (for instruction fetches, or data\naccesses), and, in this case, the hardware translates them into physical\naddresses before fetching the desired data from memory.\nRemember that the hardware ﬁrst extracts the VPN from the virtual\naddress, checks the TLB for a match (a TLB hit), and if a hit, produces the\nresulting physical address and fetches it from memory. This is hopefully\nthe common case, as it is fast (requiring no additional memory accesses).\nIf the VPN is not found in the TLB (i.e., a TLB miss), the hardware\nlocates the page table in memory (using the page table base register)\nand looks up the page table entry (PTE) for this page using the VPN\nas an index. If the page is valid and present in physical memory, the\nhardware extracts the PFN from the PTE, installs it in the TLB, and retries\nthe instruction, this time generating a TLB hit; so far, so good.\nIf we wish to allow pages to be swapped to disk, however, we must\nadd even more machinery. Speciﬁcally, when the hardware looks in the\nPTE, it may ﬁnd that the page is not present in physical memory. The way\nthe hardware (or the OS, in a software-managed TLB approach) deter-\nmines this is through a new piece of information in each page-table entry,\nknown as the present bit. If the present bit is set to one, it means the\npage is present in physical memory and everything proceeds as above; if\nit is set to zero, the page is not in memory but rather on disk somewhere.\nThe act of accessing a page that is not in physical memory is commonly\nreferred to as a page fault.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2600,
      "extraction_method": "Direct"
    },
    {
      "page_number": 256,
      "chapter": null,
      "content": "220\nBEYOND PHYSICAL MEMORY: MECHANISMS\nASIDE: SWAPPING TERMINOLOGY AND OTHER THINGS\nTerminology in virtual memory systems can be a little confusing and vari-\nable across machines and operating systems. For example, a page fault\nmore generally could refer to any reference to a page table that generates\na fault of some kind: this could include the type of fault we are discussing\nhere, i.e., a page-not-present fault, but sometimes can refer to illegal mem-\nory accesses. Indeed, it is odd that we call what is deﬁnitely a legal access\n(to a page mapped into the virtual address space of a process, but simply\nnot in physical memory at the time) a “fault” at all; really, it should be\ncalled a page miss. But often, when people say a program is “page fault-\ning”, they mean that it is accessing parts of its virtual address space that\nthe OS has swapped out to disk.\nWe suspect the reason that this behavior became known as a “fault” re-\nlates to the machinery in the operating system to handle it. When some-\nthing unusual happens, i.e., when something the hardware doesn’t know\nhow to handle occurs, the hardware simply transfers control to the OS,\nhoping it can make things better. In this case, a page that a process wants\nto access is missing from memory; the hardware does the only thing it\ncan, which is raise an exception, and the OS takes over from there. As\nthis is identical to what happens when a process does something illegal,\nit is perhaps not surprising that we term the activity a “fault.”\nUpon a page fault, the OS is invoked to service the page fault. A partic-\nular piece of code, known as a page-fault handler, runs, and must service\nthe page fault, as we now describe.\n21.3\nThe Page Fault\nRecall that with TLB misses, we have two types of systems: hardware-\nmanaged TLBs (where the hardware looks in the page table to ﬁnd the\ndesired translation) and software-managed TLBs (where the OS does). In\neither type of system, if a page is not present, the OS is put in charge to\nhandle the page fault. The appropriately-named OS page-fault handler\nruns to determine what to do. Virtually all systems handle page faults in\nsoftware; even with a hardware-managed TLB, the hardware trusts the\nOS to manage this important duty.\nIf a page is not present and has been swapped to disk, the OS will need\nto swap the page into memory in order to service the page fault. Thus, a\nquestion arises: how will the OS know where to ﬁnd the desired page? In\nmany systems, the page table is a natural place to store such information.\nThus, the OS could use the bits in the PTE normally used for data such as\nthe PFN of the page for a disk address. When the OS receives a page fault\nfor a page, it looks in the PTE to ﬁnd the address, and issues the request\nto disk to fetch the page into memory.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2841,
      "extraction_method": "Direct"
    },
    {
      "page_number": 257,
      "chapter": null,
      "content": "BEYOND PHYSICAL MEMORY: MECHANISMS\n221\nASIDE: WHY HARDWARE DOESN’T HANDLE PAGE FAULTS\nWe know from our experience with the TLB that hardware designers are\nloathe to trust the OS to do much of anything. So why do they trust the\nOS to handle a page fault? There are a few main reasons. First, page\nfaults to disk are slow; even if the OS takes a long time to handle a fault,\nexecuting tons of instructions, the disk operation itself is traditionally so\nslow that the extra overheads of running software are minimal. Second,\nto be able to handle a page fault, the hardware would have to understand\nswap space, how to issue I/Os to the disk, and a lot of other details which\nit currently doesn’t know much about. Thus, for both reasons of perfor-\nmance and simplicity, the OS handles page faults, and even hardware\ntypes can be happy.\nWhen the disk I/O completes, the OS will then update the page table\nto mark the page as present, update the PFN ﬁeld of the page-table entry\n(PTE) to record the in-memory location of the newly-fetched page, and\nretry the instruction. This next attempt may generate a TLB miss, which\nwould then be serviced and update the TLB with the translation (one\ncould alternately update the TLB upon when servicing the page fault,\nto avoid this step). Finally, a last restart would ﬁnd the translation in\nthe TLB and thus proceed to fetch the desired data or instruction from\nmemory at the translated physical address.\nNote that while the I/O is in ﬂight, the process will be in the blocked\nstate. Thus, the OS will be free to run other ready processes while the\npage fault is being serviced. Because I/O is expensive, this overlap of\nthe I/O (page fault) of one process and the execution of another is yet\nanother way a multiprogrammed system can make the most effective use\nof its hardware.\n21.4\nWhat If Memory Is Full?\nIn the process described above, you may notice that we assumed there\nis plenty of free memory in which to page in a page from swap space.\nOf course, this may not be the case; memory may be full (or close to it).\nThus, the OS might like to ﬁrst page out one or more pages to make room\nfor the new page(s) the OS is about to bring in. The process of picking a\npage to kick out, or replace is known as the page-replacement policy.\nAs it turns out, a lot of thought has been put into creating a good page-\nreplacement policy, as kicking out the wrong page can exact a great cost\non program performance. Making the wrong decision can cause a pro-\ngram to run at disk-like speeds instead of memory-like speeds; in cur-\nrent technology that means a program could run 10,000 or 100,000 times\nslower. Thus, such a policy is something we should study in some detail;\nindeed, that is exactly what we will do in the next chapter. For now, it is\ngood enough to understand that such a policy exists, built on top of the\nmechanisms described here.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2916,
      "extraction_method": "Direct"
    },
    {
      "page_number": 258,
      "chapter": null,
      "content": "222\nBEYOND PHYSICAL MEMORY: MECHANISMS\n1\nVPN = (VirtualAddress & VPN_MASK) >> SHIFT\n2\n(Success, TlbEntry) = TLB_Lookup(VPN)\n3\nif (Success == True)\n// TLB Hit\n4\nif (CanAccess(TlbEntry.ProtectBits) == True)\n5\nOffset\n= VirtualAddress & OFFSET_MASK\n6\nPhysAddr = (TlbEntry.PFN << SHIFT) | Offset\n7\nRegister = AccessMemory(PhysAddr)\n8\nelse\n9\nRaiseException(PROTECTION_FAULT)\n10\nelse\n// TLB Miss\n11\nPTEAddr = PTBR + (VPN * sizeof(PTE))\n12\nPTE = AccessMemory(PTEAddr)\n13\nif (PTE.Valid == False)\n14\nRaiseException(SEGMENTATION_FAULT)\n15\nelse\n16\nif (CanAccess(PTE.ProtectBits) == False)\n17\nRaiseException(PROTECTION_FAULT)\n18\nelse if (PTE.Present == True)\n19\n// assuming hardware-managed TLB\n20\nTLB_Insert(VPN, PTE.PFN, PTE.ProtectBits)\n21\nRetryInstruction()\n22\nelse if (PTE.Present == False)\n23\nRaiseException(PAGE_FAULT)\nFigure 21.2: Page-Fault Control Flow Algorithm (Hardware)\n21.5\nPage Fault Control Flow\nWith all of this knowledge in place, we can now roughly sketch the\ncomplete control ﬂow of memory access. In other words, when some-\nbody asks you “what happens when a program fetches some data from\nmemory?”, you should have a pretty good idea of all the different pos-\nsibilities. See the control ﬂow in Figures 21.2 and 21.3 for more details;\nthe ﬁrst ﬁgure shows what the hardware does during translation, and the\nsecond what the OS does upon a page fault.\nFrom the hardware control ﬂow diagram in Figure 21.2, notice that\nthere are now three important cases to understand when a TLB miss oc-\ncurs. First, that the page was both present and valid (Lines 18–21); in\nthis case, the TLB miss handler can simply grab the PFN from the PTE,\nretry the instruction (this time resulting in a TLB hit), and thus continue\nas described (many times) before. In the second case (Lines 22–23), the\npage fault handler must be run; although this was a legitimate page for\nthe process to access (it is valid, after all), it is not present in physical\nmemory. Third (and ﬁnally), the access could be to an invalid page, due\nfor example to a bug in the program (Lines 13–14). In this case, no other\nbits in the PTE really matter; the hardware traps this invalid access, and\nthe OS trap handler runs, likely terminating the offending process.\nFrom the software control ﬂow in Figure 21.3, we can see what the OS\nroughly must do in order to service the page fault. First, the OS must ﬁnd\na physical frame for the soon-to-be-faulted-in page to reside within; if\nthere is no such page, we’ll have to wait for the replacement algorithm to\nrun and kick some pages out of memory, thus freeing them for use here.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2635,
      "extraction_method": "Direct"
    },
    {
      "page_number": 259,
      "chapter": null,
      "content": "BEYOND PHYSICAL MEMORY: MECHANISMS\n223\n1\nPFN = FindFreePhysicalPage()\n2\nif (PFN == -1)\n// no free page found\n3\nPFN = EvictPage()\n// run replacement algorithm\n4\nDiskRead(PTE.DiskAddr, pfn) // sleep (waiting for I/O)\n5\nPTE.present = True\n// update page table with present\n6\nPTE.PFN\n= PFN\n// bit and translation (PFN)\n7\nRetryInstruction()\n// retry instruction\nFigure 21.3: Page-Fault Control Flow Algorithm (Software)\nWith a physical frame in hand, the handler then issues the I/O request\nto read in the page from swap space. Finally, when that slow operation\ncompletes, the OS updates the page table and retries the instruction. The\nretry will result in a TLB miss, and then, upon another retry, a TLB hit, at\nwhich point the hardware will be able to access the desired item.\n21.6\nWhen Replacements Really Occur\nThus far, the way we’ve described how replacements occur assumes\nthat the OS waits until memory is entirely full, and only then replaces\n(evicts) a page to make room for some other page. As you can imagine,\nthis is a little bit unrealistic, and there are many reasons for the OS to keep\na small portion of memory free more proactively.\nTo keep a small amount of memory free, most operating systems thus\nhave some kind of high watermark (HW ) and low watermark (LW ) to\nhelp decide when to start evicting pages from memory. How this works is\nas follows: when the OS notices that there are fewer than LW pages avail-\nable, a background thread that is responsible for freeing memory runs.\nThe thread evicts pages until there are HW pages available. The back-\nground thread, sometimes called the swap daemon or page daemon1,\nthen goes to sleep, happy that is has freed some memory for running pro-\ncesses and the OS to use.\nBy performing a number of replacements at once, new performance\noptimizations become possible. For example, many systems will cluster\nor group a number of pages and write them out at once to the swap parti-\ntion, thus increasing the efﬁciency of the disk [LL82]; as we will see later\nwhen we discuss disks in more detail, such clustering reduces seek and\nrotational overheads of a disk and thus increases performance noticeably.\nTo work with the background paging thread, the control ﬂow in Figure\n21.3 should be modiﬁed slightly; instead of performing a replacement\ndirectly, the algorithm would instead simply check if there are any free\npages available. If not, it would signal that the background paging thread\nthat free pages are needed; when the thread frees up some pages, it would\nre-awaken the original thread, which could then page in the desired page\nand go about its work.\n1The word “daemon”, usually pronounced “demon”, is an old term for a background\nthread or process that does something useful. Turns out (once again!) that the source of the\nterm is Multics [CS94].\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2853,
      "extraction_method": "Direct"
    },
    {
      "page_number": 260,
      "chapter": null,
      "content": "224\nBEYOND PHYSICAL MEMORY: MECHANISMS\nTIP: DO WORK IN THE BACKGROUND\nWhen you have some work to do, it is often a good idea to do it in the\nbackground to increase efﬁciency and to allow for grouping of opera-\ntions. Operating systems often do work in the background; for example,\nmany systems buffer ﬁle writes in memory before actually writing the\ndata to disk. Doing so has many possible beneﬁts: increased disk efﬁ-\nciency, as the disk may now receive many writes at once and thus better\nbe able to schedule them; improved latency of writes, as the application\nthinks the writes completed quite quickly; the possibility of work reduc-\ntion, as the writes may need never to go to disk (i.e., if the ﬁle is deleted);\nand better use of idle time, as the background work may possibly be\ndone when the system is otherwise idle, thus better utilizing the hard-\nware [G+95].\n21.7\nSummary\nIn this brief chapter, we have introduced the notion of accessing more\nmemory than is physically present within a system. To do so requires\nmore complexity in page-table structures, as a present bit (of some kind)\nmust be included to tell us whether the page is present in memory or not.\nWhen not, the operating system page-fault handler runs to service the\npage fault, and thus arranges for the transfer of the desired page from\ndisk to memory, perhaps ﬁrst replacing some pages in memory to make\nroom for those soon to be swapped in.\nRecall, importantly (and amazingly!), that these actions all take place\ntransparently to the process. As far as the process is concerned, it is just\naccessing its own private, contiguous virtual memory. Behind the scenes,\npages are placed in arbitrary (non-contiguous) locations in physical mem-\nory, and sometimes they are not even present in memory, requiring a fetch\nfrom disk. While we hope that in the common case a memory access is\nfast, in some cases it will take multiple disk operations to service it; some-\nthing as simple as performing a single instruction can, in the worst case,\ntake many milliseconds to complete.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2096,
      "extraction_method": "Direct"
    },
    {
      "page_number": 261,
      "chapter": null,
      "content": "BEYOND PHYSICAL MEMORY: MECHANISMS\n225\nReferences\n[CS94] “Take Our Word For It”\nF. Corbato and R. Steinberg\nAvailable: http://www.takeourword.com/TOW146/page4.html\nRichard Steinberg writes: “Someone has asked me the origin of the word daemon as it applies to comput-\ning. Best I can tell based on my research, the word was ﬁrst used by people on your team at Project MAC\nusing the IBM 7094 in 1963.” Professor Corbato replies: “Our use of the word daemon was inspired\nby the Maxwell’s daemon of physics and thermodynamics (my background is in physics). Maxwell’s\ndaemon was an imaginary agent which helped sort molecules of different speeds and worked tirelessly\nin the background. We fancifully began to use the word daemon to describe background processes which\nworked tirelessly to perform system chores.”\n[D97] “Before Memory Was Virtual”\nPeter Denning\nFrom In the Beginning: Recollections of Software Pioneers, Wiley, November 1997\nAn excellent historical piece by one of the pioneers of virtual memory and working sets.\n[G+95] “Idleness is not sloth”\nRichard Golding, Peter Bosch, Carl Staelin, Tim Sullivan, John Wilkes\nUSENIX ATC ’95, New Orleans, Louisiana\nA fun and easy-to-read discussion of how idle time can be better used in systems, with lots of good\nexamples.\n[LL82] “Virtual Memory Management in the VAX/VMS Operating System”\nHank Levy and P. Lipman\nIEEE Computer, Vol. 15, No. 3, March 1982\nNot the ﬁrst place where such clustering was used, but a clear and simple explanation of how such a\nmechanism works.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 1567,
      "extraction_method": "Direct"
    },
    {
      "page_number": 262,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 263,
      "chapter": null,
      "content": "22\nBeyond Physical Memory: Policies\nIn a virtual memory manager, life is easy when you have a lot of free\nmemory. A page fault occurs, you ﬁnd a free page on the free-page list,\nand assign it to the faulting page. Hey, Operating System, congratula-\ntions! You did it again.\nUnfortunately, things get a little more interesting when little memory\nis free. In such a case, this memory pressure forces the OS to start paging\nout pages to make room for actively-used pages. Deciding which page\n(or pages) to evict is encapsulated within the replacement policy of the\nOS; historically, it was one of the most important decisions the early vir-\ntual memory systems made, as older systems had little physical memory.\nMinimally, it is an interesting set of policies worth knowing a little more\nabout. And thus our problem:\nTHE CRUX: HOW TO DECIDE WHICH PAGE TO EVICT\nHow can the OS decide which page (or pages) to evict from memory?\nThis decision is made by the replacement policy of the system, which usu-\nally follows some general principles (discussed below) but also includes\ncertain tweaks to avoid corner-case behaviors.\n22.1\nCache Management\nBefore diving into policies, we ﬁrst describe the problem we are trying\nto solve in more detail. Given that main memory holds some subset of\nall the pages in the system, it can rightly be viewed as a cache for virtual\nmemory pages in the system. Thus, our goal in picking a replacement\npolicy for this cache is to minimize the number of cache misses; that is,\nto minimize the number of times that we have to go to disk to fetch the\ndesired page. Alternately, one can view our goal as maximizing the num-\nber of cache hits, i.e., the number of times a page that is read or written\nis found in memory.\n227\n",
      "content_length": 1744,
      "extraction_method": "Direct"
    },
    {
      "page_number": 264,
      "chapter": null,
      "content": "228\nBEYOND PHYSICAL MEMORY: POLICIES\nKnowing the number of cache hits and misses let us calculate the av-\nerage memory access time (AMAT) for a program (a metric computer\narchitects compute for hardware caches [HP06]). Speciﬁcally, given these\nvalues, we can compute the AMAT of a program as follows:\nAMAT = (Hit% · TM) + (Miss% · TD)\n(22.1)\nwhere TM represents the cost of accessing memory, and represents TD the\ncost of accessing disk.\nFor example, let us imagine a machine with a (tiny) address space:\n4KB, with 256-byte pages. Thus, a virtual address has two components: a\n4-bit VPN (the most-signiﬁcant bits) and an 8-bit offset (the least-signiﬁcant\nbits). Thus, a process in this example can access 24 or 16 total virtual\npages. In this example, the process generates the following memory ref-\nerences (i.e., virtual addresses): 0x000, 0x100, 0x200, 0x300, 0x400, 0x500,\n0x600, 0x700, 0x800, 0x900. These virtual addresses refer to the ﬁrst byte\nof each of the ﬁrst ten pages of the address space (the page number being\nthe ﬁrst hex digit of each virtual address).\nLet us further assume that every page except virtual page 3 are already\nin memory. Thus, our sequence of memory references will encounter the\nfollowing behavior: hit, hit, hit, miss, hit, hit, hit, hit, hit, hit. We can\ncompute the hit rate (the percent of references found in memory): 90%,\nas 9 out of 10 references are in memory. The miss rate is obviously 10%.\nTo calculate AMAT, we simply need to know the cost of accessing\nmemory and the cost of accessing disk. Assuming the cost of access-\ning memory (TM) is around 100 nanoseconds, and the cost of access-\ning disk (TD) is about 10 milliseconds, we have the following AMAT:\n0.9 · 100ns + 0.1 · 10ms, which is 90ns + 1ms, or 1.00009 ms, or about\n1 millisecond. If our hit rate had instead been 99.9%, the result is quite\ndifferent: AMAT is 10.1 microseconds, or roughly 100 times faster. As the\nhit rate approaches 100%, AMAT approaches 100 nanoseconds.\nUnfortunately, as you can see in this example, the cost of disk access\nis so high in modern systems that even a tiny miss rate will quickly dom-\ninate the overall AMAT of running programs. Clearly, we need to avoid\nas many misses as possible or run slowly, at the rate of the disk. One way\nto help with this is to carefully develop a smart policy, as we now do.\n22.2\nThe Optimal Replacement Policy\nTo better understand how a particular replacement policy works, it\nwould be nice to compare it to the best possible replacement policy. As it\nturns out, such an optimal policy was developed by Belady many years\nago [B66] (he originally called it MIN). The optimal replacement policy\nleads to the fewest number of misses overall. Belady showed that a sim-\nple (but, unfortunately, difﬁcult to implement!) approach that replaces\nthe page that will be accessed furthest in the future is the optimal policy,\nresulting in the fewest-possible cache misses.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2978,
      "extraction_method": "Direct"
    },
    {
      "page_number": 265,
      "chapter": null,
      "content": "BEYOND PHYSICAL MEMORY: POLICIES\n229\nTIP: COMPARING AGAINST OPTIMAL IS USEFUL\nAlthough optimal is not very practical as a real policy, it is incredibly\nuseful as a comparison point in simulation or other studies. Saying that\nyour fancy new algorithm has a 80% hit rate isn’t meaningful in isolation;\nsaying that optimal achieves an 82% hit rate (and thus your new approach\nis quite close to optimal) makes the result more meaningful and gives it\ncontext. Thus, in any study you perform, knowing what the optimal is\nlets you perform a better comparison, showing how much improvement\nis still possible, and also when you can stop making your policy better,\nbecause it is close enough to the ideal [AD03].\nHopefully, the intuition behind the optimal policy makes sense. Think\nabout it like this: if you have to throw out some page, why not throw\nout the one that is needed the furthest from now? By doing so, you are\nessentially saying that all the other pages in the cache are more important\nthan the one furthest out. The reason this is true is simple: you will refer\nto the other pages before you refer to the one furthest out.\nLet’s trace through a simple example to understand the decisions the\noptimal policy makes. Assume a program accesses the following stream\nof virtual pages: 0, 1, 2, 0, 1, 3, 0, 3, 1, 2, 1. Table 22.1 shows the behavior\nof optimal, assuming a cache that ﬁts three pages.\nIn the table, you can see the following actions. Not surprisingly, the\nﬁrst three accesses are misses, as the cache begins in an empty state; such\na miss is sometimes referred to as a cold-start miss (or compulsory miss).\nThen we refer again to pages 0 and 1, which both hit in the cache. Finally,\nwe reach another miss (to page 3), but this time the cache is full; a re-\nplacement must take place! Which begs the question: which page should\nwe replace? With the optimal policy, we examine the future for each page\ncurrently in the cache (0, 1, and 2), and see that 0 is accessed almost imme-\ndiately, 1 is accessed a little later, and 2 is accessed furthest in the future.\nThus the optimal policy has an easy choice: evict page 2, resulting in\npages 0, 1, and 3 in the cache. The next three references are hits, but then\nResulting\nAccess\nHit/Miss?\nEvict\nCache State\n0\nMiss\n0\n1\nMiss\n0, 1\n2\nMiss\n0, 1, 2\n0\nHit\n0, 1, 2\n1\nHit\n0, 1, 2\n3\nMiss\n2\n0, 1, 3\n0\nHit\n0, 1, 3\n3\nHit\n0, 1, 3\n1\nHit\n0, 1, 3\n2\nMiss\n3\n0, 1, 2\n1\nHit\n0, 1, 2\nTable 22.1: Tracing the Optimal Policy\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2499,
      "extraction_method": "Direct"
    },
    {
      "page_number": 266,
      "chapter": null,
      "content": "230\nBEYOND PHYSICAL MEMORY: POLICIES\nASIDE: TYPES OF CACHE MISSES\nIn the computer architecture world, architects sometimes ﬁnd it useful\nto characterize misses by type, into one of three categories: compulsory,\ncapacity, and conﬂict misses, sometimes called the Three C’s [H87]. A\ncompulsory miss (or cold-start miss [EF78]) occurs because the cache is\nempty to begin with and this is the ﬁrst reference to the item; in con-\ntrast, a capacity miss occurs because the cache ran out of space and had\nto evict an item to bring a new item into the cache. The third type of\nmiss (a conﬂict miss) arises in hardware because of limits on where an\nitem can be placed in a hardware cache, due to something known as set-\nassociativity; it does not arise in the OS page cache because such caches\nare always fully-associative, i.e., there are no restrictions on where in\nmemory a page can be placed. See H&P for details [HP06].\nwe get to page 2, which we evicted long ago, and suffer another miss.\nHere the optimal policy again examines the future for each page in the\ncache (0, 1, and 3), and sees that as long as it doesn’t evict page 1 (which\nis about to be accessed), we’ll be OK. The example shows page 3 getting\nevicted, although 0 would have been a ﬁne choice too. Finally, we hit on\npage 1 and the trace completes.\nWe can also calculate the hit rate for the cache: with 6 hits and 5 misses,\nthe hit rate is\nHits\nHits+Misses which is\n6\n6+5 or 54.6%. You can also compute\nthe hit rate modulo compulsory misses (i.e., ignore the ﬁrst miss to a given\npage), resulting in a 85.7% hit rate.\nUnfortunately, as we saw before in the development of scheduling\npolicies, the future is not generally known; you can’t build the optimal\npolicy for a general-purpose operating system1. Thus, in developing a\nreal, deployable policy, we will focus on approaches that ﬁnd some other\nway to decide which page to evict. The optimal policy will thus serve\nonly as a comparison point, to know how close we are to “perfect”.\n22.3\nA Simple Policy: FIFO\nMany early systems avoided the complexity of trying to approach\noptimal and employed very simple replacement policies. For example,\nsome systems used FIFO (ﬁrst-in, ﬁrst-out) replacement, where pages\nwere simply placed in a queue when they enter the system; when a re-\nplacement occurs, the page on the tail of the queue (the “ﬁrst-in” page) is\nevicted. FIFO has one great strength: it is quite simple to implement.\nLet’s examine how FIFO does on our example reference stream (Table\n22.2). We again begin our trace with three compulsory misses to pages 0,\n1, and 2, and then hit on both 0 and 1. Next, page 3 is referenced, causing\na miss; the replacement decision is easy with FIFO: pick the page that\n1If you can, let us know! We can become rich together. Or, like the scientists who “discov-\nered” cold fusion, widely scorned and mocked.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2914,
      "extraction_method": "Direct"
    },
    {
      "page_number": 267,
      "chapter": null,
      "content": "BEYOND PHYSICAL MEMORY: POLICIES\n231\nResulting\nAccess\nHit/Miss?\nEvict\nCache State\n0\nMiss\nFirst-in→\n0\n1\nMiss\nFirst-in→\n0, 1\n2\nMiss\nFirst-in→\n0, 1, 2\n0\nHit\nFirst-in→\n0, 1, 2\n1\nHit\nFirst-in→\n0, 1, 2\n3\nMiss\n0\nFirst-in→\n1, 2, 3\n0\nMiss\n1\nFirst-in→\n2, 3, 0\n3\nHit\nFirst-in→\n2, 3, 0\n1\nMiss\n2\nFirst-in→\n3, 0, 1\n2\nMiss\n3\nFirst-in→\n0, 1, 2\n1\nHit\nFirst-in→\n0, 1, 2\nTable 22.2: Tracing the FIFO Policy\nwas the “ﬁrst one” in (the cache state in the table is kept in FIFO order,\nwith the ﬁrst-in page on the left), which is page 0. Unfortunately, our next\naccess is to page 0, causing another miss and replacement (of page 1). We\nthen hit on page 3, but miss on 1 and 2, and ﬁnally hit on 3.\nComparing FIFO to optimal, FIFO does notably worse: a 36.4% hit\nrate (or 57.1% excluding compulsory misses). FIFO simply can’t deter-\nmine the importance of blocks: even though page 0 had been accessed\na number of times, FIFO still kicks it out, simply because it was the ﬁrst\none brought into memory.\nASIDE: BELADY’S ANOMALY\nBelady (of the optimal policy) and colleagues found an interesting refer-\nence stream that behaved a little unexpectedly [BNS69]. The memory-\nreference stream: 1, 2, 3, 4, 1, 2, 5, 1, 2, 3, 4, 5. The replacement policy\nthey were studying was FIFO. The interesting part: how the cache hit\nrate changed when moving from a cache size of 3 to 4 pages.\nIn general, you would expect the cache hit rate to increase (get better)\nwhen the cache gets larger. But in this case, with FIFO, it gets worse! Cal-\nculate the hits and misses yourself and see. This odd behavior is generally\nreferred to as Belady’s Anomaly (to the chagrin of his co-authors).\nSome other policies, such as LRU, don’t suffer from this problem. Can\nyou guess why? As it turns out, LRU has what is known as a stack prop-\nerty [M+70]. For algorithms with this property, a cache of size N + 1\nnaturally includes the contents of a cache of size N. Thus, when increas-\ning the cache size, hit rate will either stay the same or improve. FIFO and\nRandom (among others) clearly do not obey the stack property, and thus\nare susceptible to anomalous behavior.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2156,
      "extraction_method": "Direct"
    },
    {
      "page_number": 268,
      "chapter": null,
      "content": "232\nBEYOND PHYSICAL MEMORY: POLICIES\nResulting\nAccess\nHit/Miss?\nEvict\nCache State\n0\nMiss\n0\n1\nMiss\n0, 1\n2\nMiss\n0, 1, 2\n0\nHit\n0, 1, 2\n1\nHit\n0, 1, 2\n3\nMiss\n0\n1, 2, 3\n0\nMiss\n1\n2, 3, 0\n3\nHit\n2, 3, 0\n1\nMiss\n3\n2, 0, 1\n2\nHit\n2, 0, 1\n1\nHit\n2, 0, 1\nTable 22.3: Tracing the Random Policy\n22.4\nAnother Simple Policy: Random\nAnother similar replacement policy is Random, which simply picks a\nrandom page to replace under memory pressure. Random has properties\nsimilar to FIFO; it is simple to implement, but it doesn’t really try to be\ntoo intelligent in picking which blocks to evict. Let’s look at how Random\ndoes on our famous example reference stream (see Table 22.3).\nOf course, how Random does depends entirely upon how lucky (or\nunlucky) Random gets in its choices. In the example above, Random does\na little better than FIFO, and a little worse than optimal. In fact, we can\nrun the Random experiment thousands of times and determine how it\ndoes in general. Figure 22.1 shows how many hits Random achieves over\n10,000 trials, each with a different random seed. As you can see, some-\ntimes (just over 40% of the time), Random is as good as optimal, achieving\n6 hits on the example trace; sometimes it does much worse, achieving 2\nhits or fewer. How Random does depends on the luck of the draw.\n0\n1\n2\n3\n4\n5\n6\n7\n0\n10\n20\n30\n40\n50\nNumber of Hits\nFrequency\nFigure 22.1: Random Performance over 10,000 Trials\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1444,
      "extraction_method": "Direct"
    },
    {
      "page_number": 269,
      "chapter": null,
      "content": "BEYOND PHYSICAL MEMORY: POLICIES\n233\nResulting\nAccess\nHit/Miss?\nEvict\nCache State\n0\nMiss\nLRU→\n0\n1\nMiss\nLRU→\n0, 1\n2\nMiss\nLRU→\n0, 1, 2\n0\nHit\nLRU→\n1, 2, 0\n1\nHit\nLRU→\n2, 0, 1\n3\nMiss\n2\nLRU→\n0, 1, 3\n0\nHit\nLRU→\n1, 3, 0\n3\nHit\nLRU→\n1, 0, 3\n1\nHit\nLRU→\n0, 3, 1\n2\nMiss\n0\nLRU→\n3, 1, 2\n1\nHit\nLRU→\n3, 2, 1\nTable 22.4: Tracing the LRU Policy\n22.5\nUsing History: LRU\nUnfortunately, any policy as simple as FIFO or Random is likely to\nhave a common problem: it might kick out an important page, one that\nis about to be referenced again. FIFO kicks out the page that was ﬁrst\nbrought in; if this happens to be a page with important code or data\nstructures upon it, it gets thrown out anyhow, even though it will soon be\npaged back in. Thus, FIFO, Random, and similar policies are not likely to\napproach optimal; something smarter is needed.\nAs we did with scheduling policy, to improve our guess at the future,\nwe once again lean on the past and use history as our guide. For example,\nif a program has accessed a page in the near past, it is likely to access it\nagain in the near future.\nOne type of historical information a page-replacement policy could\nuse is frequency; if a page has been accessed many times, perhaps it\nshould not be replaced as it clearly has some value. A more commonly-\nused property of a page is its recency of access; the more recently a page\nhas been accessed, perhaps the more likely it will be accessed again.\nThis family of policies is based on what people refer to as the prin-\nciple of locality [D70], which basically is just an observation about pro-\ngrams and their behavior. What this principle says, quite simply, is that\nprograms tend to access certain code sequences (e.g., in a loop) and data\nstructures (e.g., an array accessed by the loop) quite frequently; we should\nthus try to use history to ﬁgure out which pages are important, and keep\nthose pages in memory when it comes to eviction time.\nAnd thus, a family of simple historically-based algorithms are born.\nThe Least-Frequently-Used (LFU) policy replaces the least-frequently-\nused page when an eviction must take place. Similarly, the Least-Recently-\nUsed (LRU) policy replaces the least-recently-used page.\nThese algo-\nrithms are easy to remember: once you know the name, you know exactly\nwhat it does, which is an excellent property for a name.\nTo better understand LRU, let’s examine how LRU does on our exam-\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2434,
      "extraction_method": "Direct"
    },
    {
      "page_number": 270,
      "chapter": null,
      "content": "234\nBEYOND PHYSICAL MEMORY: POLICIES\nASIDE: TYPES OF LOCALITY\nThere are two types of locality that programs tend to exhibit. The ﬁrst\nis known as spatial locality, which states that if a page P is accessed,\nit is likely the pages around it (say P −1 or P + 1) will also likely be\naccessed. The second is temporal locality, which states that pages that\nhave been accessed in the near past are likely to be accessed again in the\nnear future. The assumption of the presence of these types of locality\nplays a large role in the caching hierarchies of hardware systems, which\ndeploy many levels of instruction, data, and address-translation caching\nto help programs run fast when such locality exists.\nOf course, the principle of locality, as it is often called, is no hard-and-\nfast rule that all programs must obey. Indeed, some programs access\nmemory (or disk) in rather random fashion and don’t exhibit much or\nany locality in their access streams. Thus, while locality is a good thing to\nkeep in mind while designing caches of any kind (hardware or software),\nit does not guarantee success. Rather, it is a heuristic that often proves\nuseful in the design of computer systems.\nple reference stream. Table 22.4 shows the results. From the table, you\ncan see how LRU can use history to do better than stateless policies such\nas Random or FIFO. In the example, LRU evicts page 2 when it ﬁrst has\nto replace a page, because 0 and 1 have been accessed more recently. It\nthen replaces page 0 because 1 and 3 have been accessed more recently.\nIn both cases, LRU’s decision, based on history, turns out to be correct,\nand the next references are thus hits. Thus, in our simple example, LRU\ndoes as well as possible, matching optimal in its performance.\nWe should also note that the opposites of these algorithms exist: Most-\nFrequently-Used (MFU) and Most-Recently-Used (MRU). In most cases\n(not all!), these policies do not work well, as they ignore the locality most\nprograms exhibit instead of embracing it.\n22.6\nWorkload Examples\nLet’s look at a few more examples in order to better understand how\nsome of these policies behave. We’ll look at more complex workloads\ninstead just a small trace of references. However, even these workloads\nare greatly simpliﬁed; a real study would include application traces.\nOur ﬁrst workload has no locality, which means that each reference\nis to a random page within the set of accessed pages. In this simple ex-\nample, the workload accesses 100 unique pages over time, choosing the\nnext page to refer to at random; overall, 10,000 pages are accessed. In the\nexperiment, we vary the cache size from very small (1 page) to enough\nto hold all the unique pages (100 page), in order to see how each policy\nbehaves over the range of cache sizes.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2819,
      "extraction_method": "Direct"
    },
    {
      "page_number": 271,
      "chapter": null,
      "content": "BEYOND PHYSICAL MEMORY: POLICIES\n235\n0\n20\n40\n60\n80\n100\n0%\n20%\n40%\n60%\n80%\n100%\nThe No-Locality Workload\nCache Size (Blocks)\nHit Rate\nOPT\nLRU\nFIFO\nRAND\nFigure 22.2: The No-Locality Workload\nFigure 22.2 plots the results of the experiment for optimal, LRU, Ran-\ndom, and FIFO. The y-axis of the ﬁgure shows the hit rate that each policy\nachieves; the x-axis varies the cache size as described above.\nWe can draw a number of conclusions from the graph. First, when\nthere is no locality in the workload, it doesn’t matter much which realistic\npolicy you are using; LRU, FIFO, and Random all perform the same, with\nthe hit rate exactly determined by the size of the cache. Second, when\nthe cache is large enough to ﬁt the entire workload, it also doesn’t matter\nwhich policy you use; all policies (even optimal) converge to a 100% hit\nrate when all the referenced blocks ﬁt in cache. Finally, you can see that\noptimal performs noticeably better than the realistic policies; peeking into\nthe future, if it were possible, does a much better job of replacement.\nThe next workload we examine is called the “80-20” workload, which\nexhibits locality: 80% of the references are made to 20% of the pages (the\n“hot” pages); the remaining 20% of the references are made to the re-\nmaining 80% of the pages (the “cold” pages). In our workload, there are\na total 100 unique pages again; thus, “hot” pages are referred to most of\nthe time, and “cold” pages the remainder. Figure 22.3 shows how the\npolicies perform with this workload.\nAs you can see from the ﬁgure, while both random and FIFO do rea-\nsonably well, LRU does better, as it is more likely to hold onto the hot\npages; as those pages have been referred to frequently in the past, they\nare likely to be referred to again in the near future. Optimal once again\ndoes better, showing that LRU’s historical information is not perfect.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 1915,
      "extraction_method": "Direct"
    },
    {
      "page_number": 272,
      "chapter": null,
      "content": "236\nBEYOND PHYSICAL MEMORY: POLICIES\n0\n20\n40\n60\n80\n100\n0%\n20%\n40%\n60%\n80%\n100%\nThe 80-20 Workload\nCache Size (Blocks)\nHit Rate\nOPT\nLRU\nFIFO\nRAND\nFigure 22.3: The 80-20 Workload\nYou might now be wondering: is LRU’s improvement over Random\nand FIFO really that big of a deal? The answer, as usual, is “it depends.” If\neach miss is very costly (not uncommon), then even a small increase in hit\nrate (reduction in miss rate) can make a huge difference on performance.\nIf misses are not so costly, then of course the beneﬁts possible with LRU\nare not nearly as important.\nLet’s look at one ﬁnal workload. We call this one the “looping sequen-\ntial” workload, as in it, we refer to 50 pages in sequence, starting at 0,\nthen 1, ..., up to page 49, and then we loop, repeating those accesses, for a\ntotal of 10,000 accesses to 50 unique pages. The last graph in Figure 22.4\nshows the behavior of the policies under this workload.\nThis workload, common in many applications (including important\ncommercial applications such as databases [CD85]), represents a worst-\ncase for both LRU and FIFO. These algorithms, under a looping-sequential\nworkload, kick out older pages; unfortunately, due to the looping nature\nof the workload, these older pages are going to be accessed sooner than\nthe pages that the policies prefer to keep in cache. Indeed, even with\na cache of size 49, a looping-sequential workload of 50 pages results in\na 0% hit rate. Interestingly, Random fares notably better, not quite ap-\nproaching optimal, but at least achieving a non-zero hit rate. Turns out\nthat random has some nice properties; one such property is not having\nweird corner-case behaviors.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1711,
      "extraction_method": "Direct"
    },
    {
      "page_number": 273,
      "chapter": null,
      "content": "BEYOND PHYSICAL MEMORY: POLICIES\n237\n0\n20\n40\n60\n80\n100\n0%\n20%\n40%\n60%\n80%\n100%\nThe Looping-Sequential Workload\nCache Size (Blocks)\nHit Rate\nOPT\nLRU\nFIFO\nRAND\nFigure 22.4: The Looping Workload\n22.7\nImplementing Historical Algorithms\nAs you can see, an algorithm such as LRU can generally do a better\njob than simpler policies like FIFO or Random, which may throw out\nimportant pages. Unfortunately, historical policies present us with a new\nchallenge: how do we implement them?\nLet’s take, for example, LRU. To implement it perfectly, we need to\ndo a lot of work. Speciﬁcally, upon each page access (i.e., each memory\naccess, whether an instruction fetch or a load or store), we must update\nsome data structure to move this page to the front of the list (i.e., the\nMRU side). Contrast this to FIFO, where the FIFO list of pages is only\naccessed when a page is evicted (by removing the ﬁrst-in page) or when\na new page is added to the list (to the last-in side). To keep track of which\npages have been least- and most-recently used, the system has to do some\naccounting work on every memory reference. Clearly, without great care,\nsuch accounting could greatly reduce performance.\nOne method that could help speed this up is to add a little bit of hard-\nware support. For example, a machine could update, on each page access,\na time ﬁeld in memory (for example, this could be in the per-process page\ntable, or just in some separate array in memory, with one entry per phys-\nical page of the system). Thus, when a page is accessed, the time ﬁeld\nwould be set, by hardware, to the current time. Then, when replacing a\npage, the OS could simply scan all the time ﬁelds in the system to ﬁnd the\nleast-recently-used page.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 1756,
      "extraction_method": "Direct"
    },
    {
      "page_number": 274,
      "chapter": null,
      "content": "238\nBEYOND PHYSICAL MEMORY: POLICIES\nUnfortunately, as the number of pages in a system grows, scanning a\nhuge array of times just to ﬁnd the absolute least-recently-used page is\nprohibitively expensive. Imagine a modern machine with 4GB of mem-\nory, chopped into 4KB pages. This machine has 1 million pages, and thus\nﬁnding the LRU page will take a long time, even at modern CPU speeds.\nWhich begs the question: do we really need to ﬁnd the absolute oldest\npage to replace? Can we instead survive with an approximation?\nCRUX: HOW TO IMPLEMENT AN LRU REPLACEMENT POLICY\nGiven that it will be expensive to implement perfect LRU, can we ap-\nproximate it in some way, and still obtain the desired behavior?\n22.8\nApproximating LRU\nAs it turns out, the answer is yes: approximating LRU is more fea-\nsible from a computational-overhead standpoint, and indeed it is what\nmany modern systems do. The idea requires some hardware support,\nin the form of a use bit (sometimes called the reference bit), the ﬁrst of\nwhich was implemented in the ﬁrst system with paging, the Atlas one-\nlevel store [KE+62]. There is one use bit per page of the system, and the\nuse bits live in memory somewhere (they could be in the per-process page\ntables, for example, or just in an array somewhere). Whenever a page is\nreferenced (i.e., read or written), the use bit is set by hardware to 1. The\nhardware never clears the bit, though (i.e., sets it to 0); that is the respon-\nsibility of the OS.\nHow does the OS employ the use bit to approximate LRU? Well, there\ncould be a lot of ways, but with the clock algorithm [C69], one simple\napproach was suggested. Imagine all the pages of the system arranged in\na circular list. A clock hand points to some particular page to begin with\n(it doesn’t really matter which). When a replacement must occur, the OS\nchecks if the currently-pointed to page P has a use bit of 1 or 0. If 1, this\nimplies that page P was recently used and thus is not a good candidate\nfor replacement. Thus, the clock hand is incremented to the next page\nP + 1, and the use bit for P set to 0 (cleared). The algorithm continues\nuntil it ﬁnds a use bit that is set to 0, implying this page has not been\nrecently used (or, in the worst case, that all pages have been and that we\nhave now searched through the entire set of pages, clearing all the bits).\nNote that this approach is not the only way to employ a use bit to\napproximate LRU. Indeed, any approach which periodically clears the\nuse bits and then differentiates between which pages have use bits of 1\nversus 0 to decide which to replace would be ﬁne. The clock algorithm of\nCorbato’s was just one early approach which met with some success, and\nhad the nice property of not repeatedly scanning through all of memory\nlooking for an unused page.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2838,
      "extraction_method": "Direct"
    },
    {
      "page_number": 275,
      "chapter": null,
      "content": "BEYOND PHYSICAL MEMORY: POLICIES\n239\n0\n20\n40\n60\n80\n100\n0%\n20%\n40%\n60%\n80%\n100%\nThe 80-20 Workload\nCache Size (Blocks)\nHit Rate\nOPT\nLRU\nFIFO\nRAND\nClock\nFigure 22.5: The 80-20 Workload With Clock\nThe behavior of a clock algorithm variant is shown in Figure 22.5. This\nvariant randomly scans pages when doing a replacement; when it en-\ncounters a page with a reference bit set to 1, it clears the bit (i.e., sets it\nto 0); when it ﬁnds a page with the reference bit set to 0, it chooses it as\nits victim. As you can see, although it doesn’t do quite as well as perfect\nLRU, it does better than approaches that don’t consider history at all.\n22.9\nConsidering Dirty Pages\nOne small modiﬁcation to the clock algorithm (also originally sug-\ngested by Corbato [C69]) that is commonly made is the additional con-\nsideration of whether a page has been modiﬁed or not while in memory.\nThe reason for this: if a page has been modiﬁed and is thus dirty, it must\nbe written back to disk to evict it, which is expensive. If it has not been\nmodiﬁed (and is thus clean), the eviction is free; the physical frame can\nsimply be reused for other purposes without additional I/O. Thus, some\nVM systems prefer to evict clean pages over dirty pages.\nTo support this behavior, the hardware should include a modiﬁed bit\n(a.k.a. dirty bit). This bit is set any time a page is written, and thus can be\nincorporated into the page-replacement algorithm. The clock algorithm,\nfor example, could be changed to scan for pages that are both unused\nand clean to evict ﬁrst; failing to ﬁnd those, then for unused pages that\nare dirty; etc.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 1646,
      "extraction_method": "Direct"
    },
    {
      "page_number": 276,
      "chapter": null,
      "content": "240\nBEYOND PHYSICAL MEMORY: POLICIES\n22.10\nOther VM Policies\nPage replacement is not the only policy the VM subsystem employs\n(though it may be the most important). For example, the OS also has to\ndecide when to bring a page into memory. This policy, sometimes called\nthe page selection policy (as it was called by Denning [D70]), presents\nthe OS with some different options.\nFor most pages, the OS simply uses demand paging, which means the\nOS brings the page into memory when it is accessed, “on demand” as\nit were. Of course, the OS could guess that a page is about to be used,\nand thus bring it in ahead of time; this behavior is known as prefetching\nand should only be done when there is reasonable chance of success. For\nexample, some systems will assume that if a code page P is brought into\nmemory, that code page P +1 will likely soon be accessed and thus should\nbe brought into memory too.\nAnother policy determines how the OS writes pages out to disk. Of\ncourse, they could simply be written out one at a time; however, many\nsystems instead collect a number of pending writes together in memory\nand write them to disk in one (more efﬁcient) write. This behavior is\nusually called clustering or simply grouping of writes, and is effective\nbecause of the nature of disk drives, which perform a single large write\nmore efﬁciently than many small ones.\n22.11\nThrashing\nBefore closing, we address one ﬁnal question: what should the OS do\nwhen memory is simply oversubscribed, and the memory demands of the\nset of running processes simply exceeds the available physical memory?\nIn this case, the system will constantly be paging, a condition sometimes\nreferred to as thrashing [D70].\nSome earlier operating systems had a fairly sophisticated set of mech-\nanisms to both detect and cope with thrashing when it took place. For\nexample, given a set of processes, a system could decide not to run a sub-\nset of processes, with the hope that the reduced set of processes working\nsets (the pages that they are using actively) ﬁt in memory and thus can\nmake progress. This approach, generally known as admission control,\nstates that it is sometimes better to do less work well than to try to do\neverything at once poorly, a situation we often encounter in real life as\nwell as in modern computer systems (sadly).\nSome current systems take more a draconian approach to memory\noverload. For example, some versions of Linux run an out-of-memory\nkiller when memory is oversubscribed; this daemon chooses a memory-\nintensive process and kills it, thus reducing memory in a none-too-subtle\nmanner. While successful at reducing memory pressure, this approach\ncan have problems, if, for example, it kills the X server and thus renders\nany applications requiring the display unusable.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2821,
      "extraction_method": "Direct"
    },
    {
      "page_number": 277,
      "chapter": null,
      "content": "BEYOND PHYSICAL MEMORY: POLICIES\n241\n22.12\nSummary\nWe have seen the introduction of a number of page-replacement (and\nother) policies, which are part of the VM subsystem of all modern operat-\ning systems. Modern systems add some tweaks to straightforward LRU\napproximations like clock; for example, scan resistance is an important\npart of many modern algorithms, such as ARC [MM03]. Scan-resistant al-\ngorithms are usually LRU-like but also try to avoid the worst-case behav-\nior of LRU, which we saw with the looping-sequential workload. Thus,\nthe evolution of page-replacement algorithms continues.\nHowever, in many cases the importance of said algorithms has de-\ncreased, as the discrepancy between memory-access and disk-access times\nhas increased. Because paging to disk is so expensive, the cost of frequent\npaging is prohibitive. Thus, the best solution to excessive paging is often\na simple (if intellectually dissatisfying) one: buy more memory.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 996,
      "extraction_method": "Direct"
    },
    {
      "page_number": 278,
      "chapter": null,
      "content": "242\nBEYOND PHYSICAL MEMORY: POLICIES\nReferences\n[AD03] “Run-Time Adaptation in River”\nRemzi H. Arpaci-Dusseau\nACM TOCS, 21:1, February 2003\nA summary of one of the authors’ dissertation work on a system named River. Certainly one place where\nhe learned that comparison against the ideal is an important technique for system designers.\n[B66] “A Study of Replacement Algorithms for Virtual-Storage Computer”\nLaszlo A. Belady\nIBM Systems Journal 5(2): 78-101, 1966\nThe paper that introduces the simple way to compute the optimal behavior of a policy (the MIN algo-\nrithm).\n[BNS69] “An Anomaly in Space-time Characteristics of Certain Programs Running in a Paging\nMachine”\nL. A. Belady and R. A. Nelson and G. S. Shedler\nCommunications of the ACM, 12:6, June 1969\nIntroduction of the little sequence of memory references known as Belady’s Anomaly. How do Nelson\nand Shedler feel about this name, we wonder?\n[CD85] “An Evaluation of Buffer Management Strategies for Relational Database Systems”\nHong-Tai Chou and David J. DeWitt\nVLDB ’85, Stockholm, Sweden, August 1985\nA famous database paper on the different buffering strategies you should use under a number of common\ndatabase access patterns. The more general lesson: if you know something about a workload, you can\ntailor policies to do better than the general-purpose ones usually found in the OS.\n[C69] “A Paging Experiment with the Multics System”\nF.J. Corbato\nIncluded in a Festschrift published in honor of Prof. P.M. Morse\nMIT Press, Cambridge, MA, 1969\nThe original (and hard to ﬁnd!) reference to the clock algorithm, though not the ﬁrst usage of a use bit.\nThanks to H. Balakrishnan of MIT for digging up this paper for us.\n[D70] “Virtual Memory”\nPeter J. Denning\nComputing Surveys, Vol. 2, No. 3, September 1970\nDenning’s early and famous survey on virtual memory systems.\n[EF78] “Cold-start vs. Warm-start Miss Ratios”\nMalcolm C. Easton and Ronald Fagin\nCommunications of the ACM, 21:10, October 1978\nA good discussion of cold-start vs. warm-start misses.\n[HP06] “Computer Architecture: A Quantitative Approach”\nJohn Hennessy and David Patterson\nMorgan-Kaufmann, 2006\nA great and marvelous book about computer architecture. Read it!\n[H87] “Aspects of Cache Memory and Instruction Buffer Performance”\nMark D. Hill\nPh.D. Dissertation, U.C. Berkeley, 1987\nMark Hill, in his dissertation work, introduced the Three C’s, which later gained wide popularity with\nits inclusion in H&P [HP06]. The quote from therein: “I have found it useful to partition misses ... into\nthree components intuitively based on the cause of the misses (page 49).”\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2645,
      "extraction_method": "Direct"
    },
    {
      "page_number": 279,
      "chapter": null,
      "content": "BEYOND PHYSICAL MEMORY: POLICIES\n243\n[KE+62] “One-level Storage System”\nT. Kilburn, and D.B.G. Edwards and M.J. Lanigan and F.H. Sumner\nIRE Trans. EC-11:2, 1962\nAlthough Atlas had a use bit, it only had a very small number of pages, and thus the scanning of the\nuse bits in large memories was not a problem the authors solved.\n[M+70] “Evaluation Techniques for Storage Hierarchies”\nR. L. Mattson, J. Gecsei, D. R. Slutz, I. L. Traiger\nIBM Systems Journal, Volume 9:2, 1970\nA paper that is mostly about how to simulate cache hierarchies efﬁciently; certainly a classic in that\nregard, as well for its excellent discussion of some of the properties of various replacement algorithms.\nCan you ﬁgure out why the stack property might be useful for simulating a lot of different-sized caches\nat once?\n[MM03] “ARC: A Self-Tuning, Low Overhead Replacement Cache”\nNimrod Megiddo and Dharmendra S. Modha\nFAST 2003, February 2003, San Jose, California\nAn excellent modern paper about replacement algorithms, which includes a new policy, ARC, that is\nnow used in some systems. Recognized in 2014 as a “Test of Time” award winner by the storage systems\ncommunity at the FAST ’14 conference.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 1219,
      "extraction_method": "Direct"
    },
    {
      "page_number": 280,
      "chapter": null,
      "content": "244\nBEYOND PHYSICAL MEMORY: POLICIES\nHomework\nThis simulator, paging-policy.py, allows you to play around with\ndifferent page-replacement policies. See the README for details.\nQuestions\n• Generate random addresses with the following arguments: -s 0\n-n 10, -s 1 -n 10, and -s 2 -n 10. Change the policy from\nFIFO, to LRU, to OPT. Compute whether each access in said address\ntraces are hits or misses.\n• For a cache of size 5, generate worst-case address reference streams\nfor each of the following policies: FIFO, LRU, and MRU (worst-case\nreference streams cause the most misses possible. For the worst case\nreference streams, how much bigger of a cache is needed to improve\nperformance dramatically and approach OPT?\n• Generate a random trace (use python or perl). How would you\nexpect the different policies to perform on such a trace?\n• Now generate a trace with some locality. How can you generate\nsuch a trace? How does LRU perform on it? How much better than\nRAND is LRU? How does CLOCK do? How about CLOCK with\ndifferent numbers of clock bits?\n• Use a program like valgrind to instrument a real application and\ngenerate a virtual page reference stream.\nFor example, running\nvalgrind --tool=lackey --trace-mem=yes ls will output\na nearly-complete reference trace of every instruction and data ref-\nerence made by the program ls. To make this useful for the sim-\nulator above, you’ll have to ﬁrst transform each virtual memory\nreference into a virtual page-number reference (done by masking\noff the offset and shifting the resulting bits downward). How big\nof a cache is needed for your application trace in order to satisfy a\nlarge fraction of requests? Plot a graph of its working set as the size\nof the cache increases.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1774,
      "extraction_method": "Direct"
    },
    {
      "page_number": 281,
      "chapter": null,
      "content": "23\nThe VAX/VMS Virtual Memory System\nBefore we end our study of virtual memory, let us take a closer look at one\nparticularly clean and well done virtual memory manager, that found in\nthe VAX/VMS operating system [LL82]. In this note, we will discuss the\nsystem to illustrate how some of the concepts brought forth in earlier\nchapters together in a complete memory manager.\n23.1\nBackground\nThe VAX-11 minicomputer architecture was introduced in the late 1970’s\nby Digital Equipment Corporation (DEC). DEC was a massive player\nin the computer industry during the era of the mini-computer; unfortu-\nnately, a series of bad decisions and the advent of the PC slowly (but\nsurely) led to their demise [C03]. The architecture was realized in a num-\nber of implementations, including the VAX-11/780 and the less powerful\nVAX-11/750.\nThe OS for the system was known as VAX/VMS (or just plain VMS),\none of whose primary architects was Dave Cutler, who later led the effort\nto develop Microsoft’s Windows NT [C93]. VMS had the general prob-\nlem that it would be run on a broad range of machines, including very\ninexpensive VAXen (yes, that is the proper plural) to extremely high-end\nand powerful machines in the same architecture family. Thus, the OS had\nto have mechanisms and policies that worked (and worked well) across\nthis huge range of systems.\nTHE CRUX: HOW TO AVOID THE CURSE OF GENERALITY\nOperating systems often have a problem known as “the curse of gen-\nerality”, where they are tasked with general support for a broad class of\napplications and systems. The fundamental result of the curse is that the\nOS is not likely to support any one installation very well. In the case of\nVMS, the curse was very real, as the VAX-11 architecture was realized in\na number of different implementations. Thus, how can an OS be built so\nas to run effectively on a wide range of systems?\n245\n",
      "content_length": 1878,
      "extraction_method": "Direct"
    },
    {
      "page_number": 282,
      "chapter": null,
      "content": "246\nTHE VAX/VMS VIRTUAL MEMORY SYSTEM\nAs an additional issue, VMS is an excellent example of software inno-\nvations used to hide some of the inherent ﬂaws of the architecture. Al-\nthough the OS often relies on the hardware to build efﬁcient abstractions\nand illusions, sometimes the hardware designers don’t quite get every-\nthing right; in the VAX hardware, we’ll see a few examples of this, and\nwhat the VMS operating system does to build an effective, working sys-\ntem despite these hardware ﬂaws.\n23.2\nMemory Management Hardware\nThe VAX-11 provided a 32-bit virtual address space per process, di-\nvided into 512-byte pages. Thus, a virtual address consisted of a 23-bit\nVPN and a 9-bit offset. Further, the upper two bits of the VPN were used\nto differentiate which segment the page resided within; thus, the system\nwas a hybrid of paging and segmentation, as we saw previously.\nThe lower-half of the address space was known as “process space” and\nis unique to each process. In the ﬁrst half of process space (known as P0),\nthe user program is found, as well as a heap which grows downward.\nIn the second half of process space (P1), we ﬁnd the stack, which grows\nupwards. The upper-half of the address space is known as system space\n(S), although only half of it is used. Protected OS code and data reside\nhere, and the OS is in this way shared across processes.\nOne major concern of the VMS designers was the incredibly small size\nof pages in the VAX hardware (512 bytes). This size, chosen for historical\nreasons, has the fundamental problem of making simple linear page ta-\nbles excessively large. Thus, one of the ﬁrst goals of the VMS designers\nwas to make sure that VMS would not overwhelm memory with page\ntables.\nThe system reduced the pressure page tables place on memory in two\nways. First, by segmenting the user address space into two, the VAX-11\nprovides a page table for each of these regions (P0 and P1) per process;\nthus, no page-table space is needed for the unused portion of the address\nspace between the stack and the heap. The base and bounds registers\nare used as you would expect; a base register holds the address of the\npage table for that segment, and the bounds holds its size (i.e., number of\npage-table entries).\nSecond, the OS reduces memory pressure even further by placing user\npage tables (for P0 and P1, thus two per process) in kernel virtual mem-\nory. Thus, when allocating or growing a page table, the kernel allocates\nspace out of its own virtual memory, in segment S. If memory comes un-\nder severe pressure, the kernel can swap pages of these page tables out to\ndisk, thus making physical memory available for other uses.\nPutting page tables in kernel virtual memory means that address trans-\nlation is even further complicated. For example, to translate a virtual ad-\ndress in P0 or P1, the hardware has to ﬁrst try to look up the page-table\nentry for that page in its page table (the P0 or P1 page table for that pro-\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 3011,
      "extraction_method": "Direct"
    },
    {
      "page_number": 283,
      "chapter": null,
      "content": "THE VAX/VMS VIRTUAL MEMORY SYSTEM\n247\nPage 0: Invalid\nUser Code\nUser Heap\nUser Stack\nTrap Tables\nKernel Data\nKernel Code\nKernel Heap\nUnused\nSystem (S)\nUser (P1)\nUser (P0)\n0\n230\n231\n232\nFigure 23.1: The VAX/VMS Address Space\ncess); in doing so, however, the hardware may ﬁrst have to consult the\nsystem page table (which lives in physical memory); with that transla-\ntion complete, the hardware can learn the address of the page of the page\ntable, and then ﬁnally learn the address of the desired memory access.\nAll of this, fortunately, is made faster by the VAX’s hardware-managed\nTLBs, which usually (hopefully) circumvent this laborious lookup.\n23.3\nA Real Address Space\nOne neat aspect of studying VMS is that we can see how a real address\nspace is constructed (Figure 23.1. Thus far, we have assumed a simple\naddress space of just user code, user data, and user heap, but as we can\nsee above, a real address space is notably more complex.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 985,
      "extraction_method": "Direct"
    },
    {
      "page_number": 284,
      "chapter": null,
      "content": "248\nTHE VAX/VMS VIRTUAL MEMORY SYSTEM\nASIDE: WHY NULL POINTER ACCESSES CAUSE SEG FAULTS\nYou should now have a good understanding of exactly what happens on\na null-pointer dereference. A process generates a virtual address of 0, by\ndoing something like this:\nint *p = NULL; // set p = 0\n*p = 10;\n// try to store value 10 to virtual address 0\nThe hardware tries to look up the VPN (also 0 here) in the TLB, and suf-\nfers a TLB miss. The page table is consulted, and the entry for VPN 0\nis found to be marked invalid. Thus, we have an invalid access, which\ntransfers control to the OS, which likely terminates the process (on UNIX\nsystems, processes are sent a signal which allows them to react to such a\nfault; if uncaught, however, the process is killed).\nFor example, the code segment never begins at page 0. This page,\ninstead, is marked inaccessible, in order to provide some support for de-\ntecting null-pointer accesses. Thus, one concern when designing an ad-\ndress space is support for debugging, which the inaccessible zero page\nprovides here in some form.\nPerhaps more importantly, the kernel virtual address space (i.e., its\ndata structures and code) is a part of each user address space. On a con-\ntext switch, the OS changes the P0 and P1 registers to point to the ap-\npropriate page tables of the soon-to-be-run process; however, it does not\nchange the S base and bound registers, and as a result the “same” kernel\nstructures are mapped into each user address space.\nThe kernel is mapped into each address space for a number of reasons.\nThis construction makes life easier for the kernel; when, for example, the\nOS is handed a pointer from a user program (e.g., on a write() system\ncall), it is easy to copy data from that pointer to its own structures. The\nOS is naturally written and compiled, without worry of where the data\nit is accessing comes from. If in contrast the kernel were located entirely\nin physical memory, it would be quite hard to do things like swap pages\nof the page table to disk; if the kernel were given its own address space,\nmoving data between user applications and the kernel would again be\ncomplicated and painful. With this construction (now used widely), the\nkernel appears almost as a library to applications, albeit a protected one.\nOne last point about this address space relates to protection. Clearly,\nthe OS does not want user applications reading or writing OS data or\ncode. Thus, the hardware must support different protection levels for\npages to enable this. The VAX did so by specifying, in protection bits\nin the page table, what privilege level the CPU must be at in order to\naccess a particular page. Thus, system data and code are set to a higher\nlevel of protection than user data and code; an attempted access to such\ninformation from user code will generate a trap into the OS, and (you\nguessed it) the likely termination of the offending process.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2955,
      "extraction_method": "Direct"
    },
    {
      "page_number": 285,
      "chapter": null,
      "content": "THE VAX/VMS VIRTUAL MEMORY SYSTEM\n249\n23.4\nPage Replacement\nThe page table entry (PTE) in VAX contains the following bits: a valid\nbit, a protection ﬁeld (4 bits), a modify (or dirty) bit, a ﬁeld reserved for\nOS use (5 bits), and ﬁnally a physical frame number (PFN) to store the\nlocation of the page in physical memory. The astute reader might note:\nno reference bit! Thus, the VMS replacement algorithm must make do\nwithout hardware support for determining which pages are active.\nThe developers were also concerned about memory hogs, programs\nthat use a lot of memory and make it hard for other programs to run.\nMost of the policies we have looked at thus far are susceptible to such\nhogging; for example, LRU is a global policy that doesn’t share memory\nfairly among processes.\nSegmented FIFO\nTo address these two problems, the developers came up with the seg-\nmented FIFO replacement policy [RL81]. The idea is simple: each pro-\ncess has a maximum number of pages it can keep in memory, known as\nits resident set size (RSS). Each of these pages is kept on a FIFO list; when\na process exceeds its RSS, the “ﬁrst-in” page is evicted. FIFO clearly does\nnot need any support from the hardware, and is thus easy to implement.\nOf course, pure FIFO does not perform particularly well, as we saw\nearlier. To improve FIFO’s performance, VMS introduced two second-\nchance lists where pages are placed before getting evicted from memory,\nspeciﬁcally a global clean-page free list and dirty-page list. When a process\nP exceeds its RSS, a page is removed from its per-process FIFO; if clean\n(not modiﬁed), it is placed on the end of the clean-page list; if dirty (mod-\niﬁed), it is placed on the end of the dirty-page list.\nIf another process Q needs a free page, it takes the ﬁrst free page off\nof the global clean list. However, if the original process P faults on that\npage before it is reclaimed, P reclaims it from the free (or dirty) list, thus\navoiding a costly disk access. The bigger these global second-chance lists\nare, the closer the segmented FIFO algorithm performs to LRU [RL81].\nPage Clustering\nAnother optimization used in VMS also helps overcome the small page\nsize in VMS. Speciﬁcally, with such small pages, disk I/O during swap-\nping could be highly inefﬁcient, as disks do better with large transfers.\nTo make swapping I/O more efﬁcient, VMS adds a number of optimiza-\ntions, but most important is clustering. With clustering, VMS groups\nlarge batches of pages together from the global dirty list, and writes them\nto disk in one fell swoop (thus making them clean). Clustering is used\nin most modern systems, as the freedom to place pages anywhere within\nswap space lets the OS group pages, perform fewer and bigger writes,\nand thus improve performance.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2809,
      "extraction_method": "Direct"
    },
    {
      "page_number": 286,
      "chapter": null,
      "content": "250\nTHE VAX/VMS VIRTUAL MEMORY SYSTEM\nASIDE: EMULATING REFERENCE BITS\nAs it turns out, you don’t need a hardware reference bit in order to get\nsome notion of which pages are in use in a system. In fact, in the early\n1980’s, Babaoglu and Joy showed that protection bits on the VAX can be\nused to emulate reference bits [BJ81]. The basic idea: if you want to gain\nsome understanding of which pages are actively being used in a system,\nmark all of the pages in the page table as inaccessible (but keep around\nthe information as to which pages are really accessible by the process,\nperhaps in the “reserved OS ﬁeld” portion of the page table entry). When\na process accesses a page, it will generate a trap into the OS; the OS will\nthen check if the page really should be accessible, and if so, revert the\npage to its normal protections (e.g., read-only, or read-write). At the time\nof a replacement, the OS can check which pages remain marked inacces-\nsible, and thus get an idea of which pages have not been recently used.\nThe key to this “emulation” of reference bits is reducing overhead while\nstill obtaining a good idea of page usage. The OS must not be too aggres-\nsive in marking pages inaccessible, or overhead would be too high. The\nOS also must not be too passive in such marking, or all pages will end up\nreferenced; the OS will again have no good idea which page to evict.\n23.5\nOther Neat VM Tricks\nVMS had two other now-standard tricks: demand zeroing and copy-\non-write. We now describe these lazy optimizations.\nOne form of laziness in VMS (and most modern systems) is demand\nzeroing of pages. To understand this better, let’s consider the example\nof adding a page to your address space, say in your heap. In a naive\nimplementation, the OS responds to a request to add a page to your heap\nby ﬁnding a page in physical memory, zeroing it (required for security;\notherwise you’d be able to see what was on the page from when some\nother process used it!), and then mapping it into your address space (i.e.,\nsetting up the page table to refer to that physical page as desired). But the\nnaive implementation can be costly, particularly if the page does not get\nused by the process.\nWith demand zeroing, the OS instead does very little work when the\npage is added to your address space; it puts an entry in the page table\nthat marks the page inaccessible. If the process then reads or writes the\npage, a trap into the OS takes place. When handling the trap, the OS no-\ntices (usually through some bits marked in the “reserved for OS” portion\nof the page table entry) that this is actually a demand-zero page; at this\npoint, the OS then does the needed work of ﬁnding a physical page, ze-\nroing it, and mapping it into the process’s address space. If the process\nnever accesses the page, all of this work is avoided, and thus the virtue of\ndemand zeroing.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2907,
      "extraction_method": "Direct"
    },
    {
      "page_number": 287,
      "chapter": null,
      "content": "THE VAX/VMS VIRTUAL MEMORY SYSTEM\n251\nTIP: BE LAZY\nBeing lazy can be a virtue in both life as well as in operating systems.\nLaziness can put off work until later, which is beneﬁcial within an OS for\na number of reasons. First, putting off work might reduce the latency of\nthe current operation, thus improving responsiveness; for example, op-\nerating systems often report that writes to a ﬁle succeeded immediately,\nand only write them to disk later in the background. Second, and more\nimportantly, laziness sometimes obviates the need to do the work at all;\nfor example, delaying a write until the ﬁle is deleted removes the need to\ndo the write at all. Laziness is also good in life: for example, by putting\noff your OS project, you may ﬁnd that the project speciﬁcation bugs are\nworked out by your fellow classmates; however, the class project is un-\nlikely to get canceled, so being too lazy may be problematic, leading to a\nlate project, bad grade, and a sad professor. Don’t make professors sad!\nAnother cool optimization found in VMS (and again, in virtually every\nmodern OS) is copy-on-write (COW for short). The idea, which goes at\nleast back to the TENEX operating system [BB+72], is simple: when the\nOS needs to copy a page from one address space to another, instead of\ncopying it, it can map it into the target address space and mark it read-\nonly in both address spaces. If both address spaces only read the page, no\nfurther action is taken, and thus the OS has affected a fast copy without\nactually moving any data.\nIf, however, one of the address spaces does indeed try to write to the\npage, it will trap into the OS. The OS will then notice that the page is a\nCOW page, and thus (lazily) allocate a new page, ﬁll it with the data, and\nmap this new page into the address space of the faulting process. The\nprocess then continues and now has its own private copy of the page.\nCOW is useful for a number of reasons. Certainly any sort of shared\nlibrary can be mapped copy-on-write into the address spaces of many\nprocesses, saving valuable memory space. In UNIX systems, COW is\neven more critical, due to the semantics of fork() and exec(). As\nyou might recall, fork() creates an exact copy of the address space of\nthe caller; with a large address space, making such a copy is slow and\ndata intensive. Even worse, most of the address space is immediately\nover-written by a subsequent call to exec(), which overlays the calling\nprocess’s address space with that of the soon-to-be-exec’d program. By\ninstead performing a copy-on-write fork(), the OS avoids much of the\nneedless copying and thus retains the correct semantics while improving\nperformance.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2706,
      "extraction_method": "Direct"
    },
    {
      "page_number": 288,
      "chapter": null,
      "content": "252\nTHE VAX/VMS VIRTUAL MEMORY SYSTEM\n23.6\nSummary\nYou have now seen a top-to-bottom review of an entire virtual mem-\nory system. Hopefully, most of the details were easy to follow, as you\nshould have already had a good understanding of most of the basic mech-\nanisms and policies. More detail is available in the excellent (and short)\npaper by Levy and Lipman [LL82]; we encourage you to read it, a great\nway to see what the source material behind these chapters is like.\nYou should also learn more about the state of the art by reading about\nLinux and other modern systems when possible. There is a lot of source\nmaterial out there, including some reasonable books [BC05]. One thing\nthat will amaze you: how classic ideas, found in old papers such as\nthis one on VAX/VMS, still inﬂuence how modern operating systems are\nbuilt.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 876,
      "extraction_method": "Direct"
    },
    {
      "page_number": 289,
      "chapter": null,
      "content": "THE VAX/VMS VIRTUAL MEMORY SYSTEM\n253\nReferences\n[BB+72] “TENEX, A Paged Time Sharing System for the PDP-10”\nDaniel G. Bobrow, Jerry D. Burchﬁel, Daniel L. Murphy, Raymond S. Tomlinson\nCommunications of the ACM, Volume 15, March 1972\nAn early time-sharing OS where a number of good ideas came from. Copy-on-write was just one of\nthose; inspiration for many other aspects of modern systems, including process management, virtual\nmemory, and ﬁle systems are found herein.\n[BJ81] “Converting a Swap-Based System to do Paging\nin an Architecture Lacking Page-Reference Bits”\nOzalp Babaoglu and William N. Joy\nSOSP ’81, December 1981, Paciﬁc Grove, California\nA clever idea paper on how to exploit existing protection machinery within a machine in order to emulate\nreference bits. The idea came from the group at Berkeley working on their own version of UNIX, known\nas the Berkeley Systems Distribution, or BSD. The group was heavily inﬂuential in the development of\nUNIX, in virtual memory, ﬁle systems, and networking.\n[BC05] “Understanding the Linux Kernel (Third Edition)”\nDaniel P. Bovet and Marco Cesati\nO’Reilly Media, November 2005\nOne of the many books you can ﬁnd on Linux. They go out of date quickly, but many of the basics\nremain and are worth reading about.\n[C03] “The Innovator’s Dilemma”\nClayton M. Christenson\nHarper Paperbacks, January 2003\nA fantastic book about the disk-drive industry and how new innovations disrupt existing ones. A good\nread for business majors and computer scientists alike. Provides insight on how large and successful\ncompanies completely fail.\n[C93] “Inside Windows NT”\nHelen Custer and David Solomon\nMicrosoft Press, 1993\nThe book about Windows NT that explains the system top to bottom, in more detail than you might like.\nBut seriously, a pretty good book.\n[LL82] “Virtual Memory Management in the VAX/VMS Operating System”\nHenry M. Levy, Peter H. Lipman\nIEEE Computer, Volume 15, Number 3 (March 1982) Read the original source of most of this ma-\nterial; tt is a concise and easy read. Particularly important if you wish to go to graduate school, where\nall you do is read papers, work, read some more papers, work more, eventually write a paper, and then\nwork some more. But it is fun!\n[RL81] “Segmented FIFO Page Replacement”\nRollins Turner and Henry Levy\nSIGMETRICS ’81\nA short paper that shows for some workloads, segmented FIFO can approach the performance of LRU.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2452,
      "extraction_method": "Direct"
    },
    {
      "page_number": 290,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 291,
      "chapter": null,
      "content": "24\nSummary Dialogue on Memory Virtualization\nStudent: (Gulps) Wow, that was a lot of material.\nProfessor: Yes, and?\nStudent: Well, how am I supposed to remember it all? You know, for the exam?\nProfessor: Goodness, I hope that’s not why you are trying to remember it.\nStudent: Why should I then?\nProfessor: Come on, I thought you knew better. You’re trying to learn some-\nthing here, so that when you go off into the world, you’ll understand how systems\nactually work.\nStudent: Hmm... can you give an example?\nProfessor: Sure! One time back in graduate school, my friends and I were\nmeasuring how long memory accesses took, and once in a while the numbers\nwere way higher than we expected; we thought all the data was ﬁtting nicely into\nthe second-level hardware cache, you see, and thus should have been really fast\nto access.\nStudent: (nods)\nProfessor: We couldn’t ﬁgure out what was going on. So what do you do in such\na case? Easy, ask a professor! So we went and asked one of our professors, who\nlooked at the graph we had produced, and simply said “TLB”. Aha! Of course,\nTLB misses! Why didn’t we think of that? Having a good model of how virtual\nmemory works helps diagnose all sorts of interesting performance problems.\nStudent: I think I see. I’m trying to build these mental models of how things\nwork, so that when I’m out there working on my own, I won’t be surprised when\na system doesn’t quite behave as expected. I should even be able to anticipate how\nthe system will work just by thinking about it.\nProfessor: Exactly. So what have you learned? What’s in your mental model of\nhow virtual memory works?\nStudent: Well, I think I now have a pretty good idea of what happens when\nmemory is referenced by a process, which, as you’ve said many times, happens\n255\n",
      "content_length": 1772,
      "extraction_method": "Direct"
    },
    {
      "page_number": 292,
      "chapter": null,
      "content": "256\nSUMMARY DIALOGUE ON MEMORY VIRTUALIZATION\non each instruction fetch as well as explicit loads and stores.\nProfessor: Sounds good – tell me more.\nStudent: Well, one thing I’ll always remember is that the addresses we see in a\nuser program, written in C for example...\nProfessor: What other language is there?\nStudent: (continuing) ... Yes, I know you like C. So do I! Anyhow, as I was\nsaying, I now really know that all addresses that we can observe within a program\nare virtual addresses; that I, as a programmer, am just given this illusion of where\ndata and code are in memory. I used to think it was cool that I could print the\naddress of a pointer, but now I ﬁnd it frustrating – it’s just a virtual address! I\ncan’t see the real physical address where the data lives.\nProfessor: Nope, the OS deﬁnitely hides that from you. What else?\nStudent: Well, I think the TLB is a really key piece, providing the system with\na small hardware cache of address translations. Page tables are usually quite\nlarge and hence live in big and slow memories. Without that TLB, programs\nwould certainly run a great deal more slowly. Seems like the TLB truly makes\nvirtualizing memory possible. I couldn’t imagine building a system without one!\nAnd I shudder at the thought of a program with a working set that exceeds the\ncoverage of the TLB: with all those TLB misses, it would be hard to watch.\nProfessor: Yes, cover the eyes of the children! Beyond the TLB, what did you\nlearn?\nStudent: I also now understand that the page table is one of those data structures\nyou need to know about; it’s just a data structure, though, and that means almost\nany structure could be used. We started with simple structures, like arrays (a.k.a.\nlinear page tables), and advanced all the way up to multi-level tables (which look\nlike trees), and even crazier things like pageable page tables in kernel virtual\nmemory. All to save a little space in memory!\nProfessor: Indeed.\nStudent: And here’s one more important thing: I learned that the address trans-\nlation structures need to be ﬂexible enough to support what programmers want\nto do with their address spaces. Structures like the multi-level table are perfect\nin this sense; they only create table space when the user needs a portion of the\naddress space, and thus there is little waste. Earlier attempts, like the simple base\nand bounds register, just weren’t ﬂexible enough; the structures need to match\nwhat users expect and want out of their virtual memory system.\nProfessor: That’s a nice perspective. What about all of the stuff we learned\nabout swapping to disk?\nStudent: Well, it’s certainly fun to study, and good to know how page replace-\nment works. Some of the basic policies are kind of obvious (like LRU, for ex-\nample), but building a real virtual memory system seems more interesting, like\nwe saw in the VMS case study. But somehow, I found the mechanisms more\ninteresting, and the policies less so.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2990,
      "extraction_method": "Direct"
    },
    {
      "page_number": 293,
      "chapter": null,
      "content": "SUMMARY DIALOGUE ON MEMORY VIRTUALIZATION\n257\nProfessor: Oh, why is that?\nStudent: Well, as you said, in the end the best solution to policy problems is\nsimple: buy more memory. But the mechanisms you need to understand to know\nhow stuff really works. Speaking of which...\nProfessor: Yes?\nStudent: Well, my machine is running a little slowly these days... and memory\ncertainly doesn’t cost that much...\nProfessor: Oh ﬁne, ﬁne! Here’s a few bucks. Go and get yourself some DRAM,\ncheapskate.\nStudent: Thanks professor! I’ll never swap to disk again – or, if I do, at least I’ll\nknow what’s actually going on!\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 648,
      "extraction_method": "Direct"
    },
    {
      "page_number": 294,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 295,
      "chapter": null,
      "content": "Part II\nConcurrency\n259\n",
      "content_length": 24,
      "extraction_method": "Direct"
    },
    {
      "page_number": 296,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 297,
      "chapter": null,
      "content": "25\nA Dialogue on Concurrency\nProfessor: And thus we reach the second of our three pillars of operating sys-\ntems: concurrency.\nStudent: I thought there were four pillars...?\nProfessor: Nope, that was in an older version of the book.\nStudent: Umm... OK. So what is concurrency, oh wonderful professor?\nProfessor: Well, imagine we have a peach –\nStudent: (interrupting) Peaches again! What is it with you and peaches?\nProfessor: Ever read T.S. Eliot? The Love Song of J. Alfred Prufrock, “Do I dare\nto eat a peach”, and all that fun stuff?\nStudent: Oh yes! In English class in high school. Great stuff! I really liked the\npart where –\nProfessor: (interrupting) This has nothing to do with that – I just like peaches.\nAnyhow, imagine there are a lot of peaches on a table, and a lot of people who\nwish to eat them. Let’s say we did it this way: each eater ﬁrst identiﬁes a peach\nvisually, and then tries to grab it and eat it. What is wrong with this approach?\nStudent: Hmmm... seems like you might see a peach that somebody else also\nsees. If they get there ﬁrst, when you reach out, no peach for you!\nProfessor: Exactly! So what should we do about it?\nStudent: Well, probably develop a better way of going about this. Maybe form a\nline, and when you get to the front, grab a peach and get on with it.\nProfessor: Good! But what’s wrong with your approach?\nStudent: Sheesh, do I have to do all the work?\nProfessor: Yes.\nStudent: OK, let me think. Well, we used to have many people grabbing for\npeaches all at once, which is faster. But in my way, we just go one at a time,\nwhich is correct, but quite a bit slower. The best kind of approach would be fast\nand correct, probably.\n261\n",
      "content_length": 1679,
      "extraction_method": "Direct"
    },
    {
      "page_number": 298,
      "chapter": null,
      "content": "262\nA DIALOGUE ON CONCURRENCY\nProfessor: You are really starting to impress. In fact, you just told us everything\nwe need to know about concurrency! Well done.\nStudent: I did? I thought we were just talking about peaches. Remember, this\nis usually a part where you make it about computers again.\nProfessor: Indeed. My apologies! One must never forget the concrete. Well,\nas it turns out, there are certain types of programs that we call multi-threaded\napplications; each thread is kind of like an independent agent running around\nin this program, doing things on the program’s behalf. But these threads access\nmemory, and for them, each spot of memory is kind of like one of those peaches. If\nwe don’t coordinate access to memory between threads, the program won’t work\nas expected. Make sense?\nStudent: Kind of. But why do we talk about this in an OS class? Isn’t that just\napplication programming?\nProfessor: Good question! A few reasons, actually. First, the OS must support\nmulti-threaded applications with primitives such as locks and condition vari-\nables, which we’ll talk about soon. Second, the OS itself was the ﬁrst concurrent\nprogram – it must access its own memory very carefully or many strange and\nterrible things will happen. Really, it can get quite grisly.\nStudent: I see. Sounds interesting. There are more details, I imagine?\nProfessor: Indeed there are...\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1424,
      "extraction_method": "Direct"
    },
    {
      "page_number": 299,
      "chapter": null,
      "content": "26\nConcurrency: An Introduction\nThus far, we have seen the development of the basic abstractions that the\nOS performs. We have seen how to take a single physical CPU and turn\nit into multiple virtual CPUs, thus enabling the illusion of multiple pro-\ngrams running at the same time. We have also seen how to create the\nillusion of a large, private virtual memory for each process; this abstrac-\ntion of the address space enables each program to behave as if it has its\nown memory when indeed the OS is secretly multiplexing address spaces\nacross physical memory (and sometimes, disk).\nIn this note, we introduce a new abstraction for a single running pro-\ncess: that of a thread. Instead of our classic view of a single point of\nexecution within a program (i.e., a single PC where instructions are be-\ning fetched from and executed), a multi-threaded program has more than\none point of execution (i.e., multiple PCs, each of which is being fetched\nand executed from). Perhaps another way to think of this is that each\nthread is very much like a separate process, except for one difference:\nthey share the same address space and thus can access the same data.\nThe state of a single thread is thus very similar to that of a process.\nIt has a program counter (PC) that tracks where the program is fetch-\ning instructions from. Each thread has its own private set of registers it\nuses for computation; thus, if there are two threads that are running on\na single processor, when switching from running one (T1) to running the\nother (T2), a context switch must take place. The context switch between\nthreads is quite similar to the context switch between processes, as the\nregister state of T1 must be saved and the register state of T2 restored\nbefore running T2. With processes, we saved state to a process control\nblock (PCB); now, we’ll need one or more thread control blocks (TCBs)\nto store the state of each thread of a process. There is one major difference,\nthough, in the context switch we perform between threads as compared\nto processes: the address space remains the same (i.e., there is no need to\nswitch which page table we are using).\nOne other major difference between threads and processes concerns\nthe stack. In our simple model of the address space of a classic process\n(which we can now call a single-threaded process), there is a single stack,\nusually residing at the bottom of the address space (Figure 26.1, left).\n263\n",
      "content_length": 2435,
      "extraction_method": "Direct"
    },
    {
      "page_number": 300,
      "chapter": null,
      "content": "264\nCONCURRENCY: AN INTRODUCTION\n16KB\n15KB\n2KB\n1KB\n0KB\nStack\n(free)\nHeap\nProgram Code\nthe code segment:\nwhere instructions live\nthe heap segment:\ncontains malloc’d data\ndynamic data structures\n(it grows downward)\n(it grows upward)\nthe stack segment:\ncontains local variables\narguments to routines, \nreturn values, etc.\n16KB\n15KB\n2KB\n1KB\n0KB\nStack (1)\nStack (2)\n(free)\n(free)\nHeap\nProgram Code\nFigure 26.1: A Single-Threaded Address Space\nHowever, in a multi-threaded process, each thread runs independently\nand of course may call into various routines to do whatever work it is do-\ning. Instead of a single stack in the address space, there will be one per\nthread. Let’s say we have a multi-threaded process that has two threads\nin it; the resulting address space looks different (Figure 26.1, right).\nIn this ﬁgure, you can see two stacks spread throughout the address\nspace of the process. Thus, any stack-allocated variables, parameters, re-\nturn values, and other things that we put on the stack will be placed in\nwhat is sometimes called thread-local storage, i.e., the stack of the rele-\nvant thread.\nYou might also notice how this ruins our beautiful address space lay-\nout. Before, the stack and heap could grow independently and trouble\nonly arose when you ran out of room in the address space. Here, we\nno longer have such a nice situation. Fortunately, this is usually OK, as\nstacks do not generally have to be very large (the exception being in pro-\ngrams that make heavy use of recursion).\n26.1\nAn Example: Thread Creation\nLet’s say we wanted to run a program that created two threads, each\nof which was doing some independent work, in this case printing “A” or\n“B”. The code is shown in Figure 26.2.\nThe main program creates two threads, each of which will run the\nfunction mythread(), though with different arguments (the string A or\nB). Once a thread is created, it may start running right away (depending\non the whims of the scheduler); alternately, it may be put in a “ready” but\nnot “running” state and thus not run yet. After creating the two threads\n(T1 and T2), the main thread calls pthread join(), which waits for a\nparticular thread to complete.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2218,
      "extraction_method": "Direct"
    },
    {
      "page_number": 301,
      "chapter": null,
      "content": "CONCURRENCY: AN INTRODUCTION\n265\n1\n#include <stdio.h>\n2\n#include <assert.h>\n3\n#include <pthread.h>\n4\n5\nvoid *mythread(void *arg) {\n6\nprintf(\"%s\\n\", (char *) arg);\n7\nreturn NULL;\n8\n}\n9\n10\nint\n11\nmain(int argc, char *argv[]) {\n12\npthread_t p1, p2;\n13\nbr int rc;\n14\nprintf(\"main: begin\\n\");\n15\nrc = pthread_create(&p1, NULL, mythread, \"A\"); assert(rc == 0);\n16\nrc = pthread_create(&p2, NULL, mythread, \"B\"); assert(rc == 0);\n17\n// join waits for the threads to finish\n18\nrc = pthread_join(p1, NULL); assert(rc == 0);\n19\nrc = pthread_join(p2, NULL); assert(rc == 0);\n20\nprintf(\"main: end\\n\");\n21\nreturn 0;\n22\n}\nFigure 26.2: Simple Thread Creation Code (t0.c)\nLet us examine the possible execution ordering of this little program.\nIn the execution diagram (Table 26.1), time increases in the downwards\ndirection, and each column shows when a different thread (the main one,\nor Thread 1, or Thread 2) is running.\nNote, however, that this ordering is not the only possible ordering. In\nfact, given a sequence of instructions, there are quite a few, depending on\nwhich thread the scheduler decides to run at a given point. For example,\nonce a thread is created, it may run immediately, which would lead to the\nexecution shown in Table 26.2.\nWe also could even see “B” printed before “A”, if, say, the scheduler\ndecided to run Thread 2 ﬁrst even though Thread 1 was created earlier;\nthere is no reason to assume that a thread that is created ﬁrst will run ﬁrst.\nTable 26.3 shows this ﬁnal execution ordering, with Thread 2 getting to\nstrut its stuff before Thread 1.\nAs you might be able to see, one way to think about thread creation\nis that it is a bit like making a function call; however, instead of ﬁrst ex-\necuting the function and then returning to the caller, the system instead\ncreates a new thread of execution for the routine that is being called, and\nit runs independently of the caller, perhaps before returning from the cre-\nate, but perhaps much later.\nAs you also might be able to tell from this example, threads make life\ncomplicated: it is already hard to tell what will run when! Computers are\nhard enough to understand without concurrency. Unfortunately, with\nconcurrency, it gets worse. Much worse.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2252,
      "extraction_method": "Direct"
    },
    {
      "page_number": 302,
      "chapter": null,
      "content": "266\nCONCURRENCY: AN INTRODUCTION\nmain\nThread 1\nThread2\nstarts running\nprints “main: begin”\ncreates Thread 1\ncreates Thread 2\nwaits for T1\nruns\nprints “A”\nreturns\nwaits for T2\nruns\nprints “B”\nreturns\nprints “main: end”\nTable 26.1: Thread Trace (1)\nmain\nThread 1\nThread2\nstarts running\nprints “main: begin”\ncreates Thread 1\nruns\nprints “A”\nreturns\ncreates Thread 2\nruns\nprints “B”\nreturns\nwaits for T1\nreturns immediately; T1 is done\nwaits for T2\nreturns immediately; T2 is done\nprints “main: end”\nTable 26.2: Thread Trace (2)\nmain\nThread 1\nThread2\nstarts running\nprints “main: begin”\ncreates Thread 1\ncreates Thread 2\nruns\nprints “B”\nreturns\nwaits for T1\nruns\nprints “A”\nreturns\nwaits for T2\nreturns immediately; T2 is done\nprints “main: end”\nTable 26.3: Thread Trace (3)\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 818,
      "extraction_method": "Direct"
    },
    {
      "page_number": 303,
      "chapter": null,
      "content": "CONCURRENCY: AN INTRODUCTION\n267\n1\n#include <stdio.h>\n2\n#include <pthread.h>\n3\n#include \"mythreads.h\"\n4\n5\nstatic volatile int counter = 0;\n6\n7\n//\n8\n// mythread()\n9\n//\n10\n// Simply adds 1 to counter repeatedly, in a loop\n11\n// No, this is not how you would add 10,000,000 to\n12\n// a counter, but it shows the problem nicely.\n13\n//\n14\nvoid *\n15\nmythread(void *arg)\n16\n{\n17\nprintf(\"%s: begin\\n\", (char *) arg);\n18\nint i;\n19\nfor (i = 0; i < 1e7; i++) {\n20\ncounter = counter + 1;\n21\n}\n22\nprintf(\"%s: done\\n\", (char *) arg);\n23\nreturn NULL;\n24\n}\n25\n26\n//\n27\n// main()\n28\n//\n29\n// Just launches two threads (pthread_create)\n30\n// and then waits for them (pthread_join)\n31\n//\n32\nint\n33\nmain(int argc, char *argv[])\n34\n{\n35\npthread_t p1, p2;\n36\nprintf(\"main: begin (counter = %d)\\n\", counter);\n37\nPthread_create(&p1, NULL, mythread, \"A\");\n38\nPthread_create(&p2, NULL, mythread, \"B\");\n39\n40\n// join waits for the threads to finish\n41\nPthread_join(p1, NULL);\n42\nPthread_join(p2, NULL);\n43\nprintf(\"main: done with both (counter = %d)\\n\", counter);\n44\nreturn 0;\n45\n}\nFigure 26.3: Sharing Data: Oh Oh (t2)\n26.2\nWhy It Gets Worse: Shared Data\nThe simple thread example we showed above was useful in showing\nhow threads are created and how they can run in different orders depend-\ning on how the scheduler decides to run them. What it doesn’t show you,\nthough, is how threads interact when they access shared data.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 1440,
      "extraction_method": "Direct"
    },
    {
      "page_number": 304,
      "chapter": null,
      "content": "268\nCONCURRENCY: AN INTRODUCTION\nLet us imagine a simple example where two threads wish to update a\nglobal shared variable. The code we’ll study is in Figure 26.3.\nHere are a few notes about the code. First, as Stevens suggests [SR05],\nwe wrap the thread creation and join routines to simply exit on failure;\nfor a program as simple as this one, we want to at least notice an error\noccurred (if it did), but not do anything very smart about it (e.g., just\nexit). Thus, Pthread create() simply calls pthread create() and\nmakes sure the return code is 0; if it isn’t, Pthread create() just prints\na message and exits.\nSecond, instead of using two separate function bodies for the worker\nthreads, we just use a single piece of code, and pass the thread an argu-\nment (in this case, a string) so we can have each thread print a different\nletter before its messages.\nFinally, and most importantly, we can now look at what each worker is\ntrying to do: add a number to the shared variable counter, and do so 10\nmillion times (1e7) in a loop. Thus, the desired ﬁnal result is: 20,000,000.\nWe now compile and run the program, to see how it behaves. Some-\ntimes, everything works how we might expect:\nprompt> gcc -o main main.c -Wall -pthread\nprompt> ./main\nmain: begin (counter = 0)\nA: begin\nB: begin\nA: done\nB: done\nmain: done with both (counter = 20000000)\nUnfortunately, when we run this code, even on a single processor, we\ndon’t necessarily get the desired result. Sometimes, we get:\nprompt> ./main\nmain: begin (counter = 0)\nA: begin\nB: begin\nA: done\nB: done\nmain: done with both (counter = 19345221)\nLet’s try it one more time, just to see if we’ve gone crazy. After all,\naren’t computers supposed to produce deterministic results, as you have\nbeen taught?! Perhaps your professors have been lying to you? (gasp)\nprompt> ./main\nmain: begin (counter = 0)\nA: begin\nB: begin\nA: done\nB: done\nmain: done with both (counter = 19221041)\nNot only is each run wrong, but also yields a different result! A big\nquestion remains: why does this happen?\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2084,
      "extraction_method": "Direct"
    },
    {
      "page_number": 305,
      "chapter": null,
      "content": "CONCURRENCY: AN INTRODUCTION\n269\nTIP: KNOW AND USE YOUR TOOLS\nYou should always learn new tools that help you write, debug, and un-\nderstand computer systems. Here, we use a neat tool called a disassem-\nbler. When you run a disassembler on an executable, it shows you what\nassembly instructions make up the program. For example, if we wish to\nunderstand the low-level code to update a counter (as in our example),\nwe run objdump (Linux) to see the assembly code:\nprompt> objdump -d main\nDoing so produces a long listing of all the instructions in the program,\nneatly labeled (particularly if you compiled with the -g ﬂag), which in-\ncludes symbol information in the program. The objdump program is just\none of many tools you should learn how to use; a debugger like gdb,\nmemory proﬁlers like valgrind or purify, and of course the compiler\nitself are others that you should spend time to learn more about; the better\nyou are at using your tools, the better systems you’ll be able to build.\n26.3\nThe Heart of the Problem: Uncontrolled Scheduling\nTo understand why this happens, we must understand the code se-\nquence that the compiler generates for the update to counter. In this\ncase, we wish to simply add a number (1) to counter. Thus, the code\nsequence for doing so might look something like this (in x86);\nmov 0x8049a1c, %eax\nadd $0x1, %eax\nmov %eax, 0x8049a1c\nThis example assumes that the variable counter is located at address\n0x8049a1c. In this three-instruction sequence, the x86 mov instruction is\nused ﬁrst to get the memory value at the address and put it into register\neax. Then, the add is performed, adding 1 (0x1) to the contents of the\neax register, and ﬁnally, the contents of eax are stored back into memory\nat the same address.\nLet us imagine one of our two threads (Thread 1) enters this region of\ncode, and is thus about to increment counter by one. It loads the value\nof counter (let’s say it’s 50 to begin with) into its register eax. Thus,\neax=50 for Thread 1. Then it adds one to the register; thus eax=51.\nNow, something unfortunate happens: a timer interrupt goes off; thus,\nthe OS saves the state of the currently running thread (its PC, its registers\nincluding eax, etc.) to the thread’s TCB.\nNow something worse happens: Thread 2 is chosen to run, and it en-\nters this same piece of code. It also executes the ﬁrst instruction, getting\nthe value of counter and putting it into its eax (remember: each thread\nwhen running has its own private registers; the registers are virtualized\nby the context-switch code that saves and restores them). The value of\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2624,
      "extraction_method": "Direct"
    },
    {
      "page_number": 306,
      "chapter": null,
      "content": "270\nCONCURRENCY: AN INTRODUCTION\n(after instruction)\nOS\nThread 1\nThread 2\nPC\n%eax counter\nbefore critical section\n100\n0\n50\nmov 0x8049a1c, %eax\n105\n50\n50\nadd $0x1, %eax\n108\n51\n50\ninterrupt\nsave T1’s state\nrestore T2’s state\n100\n0\n50\nmov 0x8049a1c, %eax\n105\n50\n50\nadd $0x1, %eax\n108\n51\n50\nmov %eax, 0x8049a1c\n113\n51\n51\ninterrupt\nsave T2’s state\nrestore T1’s state\n108\n51\n50\nmov %eax, 0x8049a1c\n113\n51\n51\nTable 26.4: The Problem: Up Close and Personal\ncounter is still 50 at this point, and thus Thread 2 has eax=50. Let’s\nthen assume that Thread 2 executes the next two instructions, increment-\ning eax by 1 (thus eax=51), and then saving the contents of eax into\ncounter (address 0x8049a1c). Thus, the global variable counter now\nhas the value 51.\nFinally, another context switch occurs, and Thread 1 resumes running.\nRecall that it had just executed the mov and add, and is now about to\nperform the ﬁnal mov instruction. Recall also that eax=51. Thus, the ﬁnal\nmov instruction executes, and saves the value to memory; the counter is\nset to 51 again.\nPut simply, what has happened is this: the code to increment counter\nhas been run twice, but counter, which started at 50, is now only equal\nto 51. A “correct” version of this program should have resulted in counter\nequal to 52.\nHere is a pictorial depiction of what happened and when in the ex-\nample above. Assume, for this depiction, that the above code is loaded at\naddress 100 in memory, like the following sequence (note for those of you\nused to nice, RISC-like instruction sets: x86 has variable-length instruc-\ntions; the mov instructions here take up 5 bytes of memory, whereas the\nadd takes only 3):\n100 mov\n0x8049a1c, %eax\n105 add\n$0x1, %eax\n108 mov\n%eax, 0x8049a1c\nWith these assumptions, what happens is seen in Table 26.4. Assume\nthe counter starts at value 50, and trace through this example to make\nsure you understand what is going on.\nWhat we have demonstrated here is called a race condition: the results\ndepend on the timing execution of the code. With some bad luck (i.e.,\ncontext switches that occur at untimely points in the execution), we get\nthe wrong result. In fact, we may get a different result each time; thus,\ninstead of a nice deterministic computation (which we are used to from\ncomputers), we call this result indeterminate, where it is not known what\nthe output will be and it is indeed likely to be different across runs.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2455,
      "extraction_method": "Direct"
    },
    {
      "page_number": 307,
      "chapter": null,
      "content": "CONCURRENCY: AN INTRODUCTION\n271\nBecause multiple threads executing this code can result in a race con-\ndition, we call this code a critical section. A critical section is a piece of\ncode that accesses a shared variable (or more generally, a shared resource)\nand must not be concurrently executed by more than one thread.\nWhat we really want for this code is what we call mutual exclusion.\nThis property guarantees that if one thread is executing within the critical\nsection, the others will be prevented from doing so.\nVirtually all of these terms, by the way, were coined by Edsger Dijk-\nstra, who was a pioneer in the ﬁeld and indeed won the Turing Award\nbecause of this and other work; see his 1968 paper on “Cooperating Se-\nquential Processes” [D68] for an amazingly clear description of the prob-\nlem. We’ll be hearing more about Dijkstra in this section of the book.\n26.4\nThe Wish For Atomicity\nOne way to solve this problem would be to have more powerful in-\nstructions that, in a single step, did exactly whatever we needed done\nand thus removed the possibility of an untimely interrupt. For example,\nwhat if we had a super instruction that looked like this?\nmemory-add 0x8049a1c, $0x1\nAssume this instruction adds a value to a memory location, and the\nhardware guarantees that it executes atomically; when the instruction\nexecuted, it would perform the update as desired. It could not be inter-\nrupted mid-instruction, because that is precisely the guarantee we receive\nfrom the hardware: when an interrupt occurs, either the instruction has\nnot run at all, or it has run to completion; there is no in-between state.\nHardware can be a beautiful thing, no?\nAtomically, in this context, means “as a unit”, which sometimes we\ntake as “all or none.” What we’d like is to execute the three instruction\nsequence atomically:\nmov 0x8049a1c, %eax\nadd $0x1, %eax\nmov %eax, 0x8049a1c\nAs we said, if we had a single instruction to do this, we could just\nissue that instruction and be done. But in the general case, we won’t have\nsuch an instruction. Imagine we were building a concurrent B-tree, and\nwished to update it; would we really want the hardware to support an\n“atomic update of B-tree” instruction? Probably not, at least in a sane\ninstruction set.\nThus, what we will instead do is ask the hardware for a few useful\ninstructions upon which we can build a general set of what we call syn-\nchronization primitives. By using these hardware synchronization prim-\nitives, in combination with some help from the operating system, we will\nbe able to build multi-threaded code that accesses critical sections in a\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2654,
      "extraction_method": "Direct"
    },
    {
      "page_number": 308,
      "chapter": null,
      "content": "272\nCONCURRENCY: AN INTRODUCTION\nASIDE: KEY CONCURRENCY TERMS\nCRITICAL SECTION, RACE CONDITION,\nINDETERMINATE, MUTUAL EXCLUSION\nThese four terms are so central to concurrent code that we thought it\nworth while to call them out explicitly. See some of Dijkstra’s early work\n[D65,D68] for more details.\n• A critical section is a piece of code that accesses a shared resource,\nusually a variable or data structure.\n• A race condition arises if multiple threads of execution enter the\ncritical section at roughly the same time; both attempt to update\nthe shared data structure, leading to a surprising (and perhaps un-\ndesirable) outcome.\n• An indeterminate program consists of one or more race conditions;\nthe output of the program varies from run to run, depending on\nwhich threads ran when. The outcome is thus not deterministic,\nsomething we usually expect from computer systems.\n• To avoid these problems, threads should use some kind of mutual\nexclusion primitives; doing so guarantees that only a single thread\never enters a critical section, thus avoiding races, and resulting in\ndeterministic program outputs.\nsynchronized and controlled manner, and thus reliably produces the cor-\nrect result despite the challenging nature of concurrent execution. Pretty\nawesome, right?\nThis is the problem we will study in this section of the book. It is a\nwonderful and hard problem, and should make your mind hurt (a bit).\nIf it doesn’t, then you don’t understand! Keep working until your head\nhurts; you then know you’re headed in the right direction. At that point,\ntake a break; we don’t want your head hurting too much.\nTHE CRUX:\nHOW TO PROVIDE SUPPORT FOR SYNCHRONIZATION\nWhat support do we need from the hardware in order to build use-\nful synchronization primitives? What support do we need from the OS?\nHow can we build these primitives correctly and efﬁciently? How can\nprograms use them to get the desired results?\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1966,
      "extraction_method": "Direct"
    },
    {
      "page_number": 309,
      "chapter": null,
      "content": "CONCURRENCY: AN INTRODUCTION\n273\n26.5\nOne More Problem: Waiting For Another\nThis chapter has set up the problem of concurrency as if only one type\nof interaction occurs between threads, that of accessing shared variables\nand the need to support atomicity for critical sections. As it turns out,\nthere is another common interaction that arises, where one thread must\nwait for another to complete some action before it continues. This inter-\naction arises, for example, when a process performs a disk I/O and is put\nto sleep; when the I/O completes, the process needs to be roused from its\nslumber so it can continue.\nThus, in the coming chapters, we’ll be not only studying how to build\nsupport for synchronization primitives to support atomicity but also for\nmechanisms to support this type of sleeping/waking interaction that is\ncommon in multi-threaded programs. If this doesn’t make sense right\nnow, that is OK! It will soon enough, when you read the chapter on con-\ndition variables. If it doesn’t by then, well, then it is less OK, and you\nshould read that chapter again (and again) until it does make sense.\n26.6\nSummary: Why in OS Class?\nBefore wrapping up, one question that you might have is: why are we\nstudying this in OS class? “History” is the one-word answer; the OS was\nthe ﬁrst concurrent program, and many techniques were created for use\nwithin the OS. Later, with multi-threaded processes, application program-\nmers also had to consider such things.\nFor example, imagine the case where there are two processes running.\nAssume they both call write() to write to the ﬁle, and both wish to\nappend the data to the ﬁle (i.e., add the data to the end of the ﬁle, thus in-\ncreasing its length). To do so, both must allocate a new block, record in the\ninode of the ﬁle where this block lives, and change the size of the ﬁle to re-\nﬂect the new larger size (among other things; we’ll learn more about ﬁles\nin the third part of the book). Because an interrupt may occur at any time,\nthe code that updates to these shared structures (e.g., a bitmap for alloca-\ntion, or the ﬁle’s inode) are critical sections; thus, OS designers, from the\nvery beginning of the introduction of the interrupt, had to worry about\nhow the OS updates internal structures. An untimely interrupt causes all\nof the problems described above. Not surprisingly, page tables, process\nlists, ﬁle system structures, and virtually every kernel data structure has\nto be carefully accessed, with the proper synchronization primitives, to\nwork correctly.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2569,
      "extraction_method": "Direct"
    },
    {
      "page_number": 310,
      "chapter": null,
      "content": "274\nCONCURRENCY: AN INTRODUCTION\nTIP: USE ATOMIC OPERATIONS\nAtomic operations are one of the most powerful underlying techniques\nin building computer systems, from the computer architecture, to concur-\nrent code (what we are studying here), to ﬁle systems (which we’ll study\nsoon enough), database management systems, and even distributed sys-\ntems [L+93].\nThe idea behind making a series of actions atomic is simply expressed\nwith the phrase “all or nothing”; it should either appear as if all of the ac-\ntions you wish to group together occurred, or that none of them occurred,\nwith no in-between state visible. Sometimes, the grouping of many ac-\ntions into a single atomic action is called a transaction, an idea devel-\noped in great detail in the world of databases and transaction processing\n[GR92].\nIn our theme of exploring concurrency, we’ll be using synchronization\nprimitives to turn short sequences of instructions into atomic blocks of\nexecution, but the idea of atomicity is much bigger than that, as we will\nsee. For example, ﬁle systems use techniques such as journaling or copy-\non-write in order to atomically transition their on-disk state, critical for\noperating correctly in the face of system failures. If that doesn’t make\nsense, don’t worry – it will, in some future chapter.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1347,
      "extraction_method": "Direct"
    },
    {
      "page_number": 311,
      "chapter": null,
      "content": "CONCURRENCY: AN INTRODUCTION\n275\nReferences\n[D65] “Solution of a problem in concurrent programming control”\nE. W. Dijkstra\nCommunications of the ACM, 8(9):569, September 1965\nPointed to as the ﬁrst paper of Dijkstra’s where he outlines the mutual exclusion problem and a solution.\nThe solution, however, is not widely used; advanced hardware and OS support is needed, as we will see\nin the coming chapters.\n[D68] “Cooperating sequential processes”\nEdsger W. Dijkstra, 1968\nAvailable: http://www.cs.utexas.edu/users/EWD/ewd01xx/EWD123.PDF\nDijkstra has an amazing number of his old papers, notes, and thoughts recorded (for posterity) on this\nwebsite at the last place he worked, the University of Texas. Much of his foundational work, however,\nwas done years earlier while he was at the Technische Hochshule of Eindhoven (THE), including this\nfamous paper on “cooperating sequential processes”, which basically outlines all of the thinking that\nhas to go into writing multi-threaded programs. Dijkstra discovered much of this while working on an\noperating system named after his school: the “THE” operating system (said “T”, “H”, “E”, and not\nlike the word “the”).\n[GR92] “Transaction Processing: Concepts and Techniques”\nJim Gray and Andreas Reuter\nMorgan Kaufmann, September 1992\nThis book is the bible of transaction processing, written by one of the legends of the ﬁeld, Jim Gray. It is,\nfor this reason, also considered Jim Gray’s “brain dump”, in which he wrote down everything he knows\nabout how database management systems work. Sadly, Gray passed away tragically a few years back,\nand many of us lost a friend and great mentor, including the co-authors of said book, who were lucky\nenough to interact with Gray during their graduate school years.\n[L+93] “Atomic Transactions”\nNancy Lynch, Michael Merritt, William Weihl, Alan Fekete\nMorgan Kaufmann, August 1993\nA nice text on some of the theory and practice of atomic transactions for distributed systems. Perhaps a\nbit formal for some, but lots of good material is found herein.\n[SR05] “Advanced Programming in the UNIX Environment”\nW. Richard Stevens and Stephen A. Rago\nAddison-Wesley, 2005\nAs we’ve said many times, buy this book, and read it, in little chunks, preferably before going to bed.\nThis way, you will actually fall asleep more quickly; more importantly, you learn a little more about\nhow to become a serious UNIX programmer.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2441,
      "extraction_method": "Direct"
    },
    {
      "page_number": 312,
      "chapter": null,
      "content": "276\nCONCURRENCY: AN INTRODUCTION\nHomework\nThis program, x86.py, allows you to see how different thread inter-\nleavings either cause or avoid race conditions. See the README for de-\ntails on how the program works and its basic inputs, then answer the\nquestions below.\nQuestions\n1. To start, let’s examine a simple program, “loop.s”. First, just look at\nthe program, and see if you can understand it: cat loop.s. Then,\nrun it with these arguments:\n./x86.py -p loop.s -t 1 -i 100 -R dx\nTthis speciﬁes a single thread, an interrupt every 100 instructions,\nand tracing of register %dx. Can you ﬁgure out what the value of\n%dx will be during the run? Once you have, run the same above\nand use the -c ﬂag to check your answers; note the answers, on\nthe left, show the value of the register (or memory value) after the\ninstruction on the right has run.\n2. Now run the same code but with these ﬂags:\n./x86.py -p loop.s -t 2 -i 100 -a dx=3,dx=3 -R dx\nTthis speciﬁes two threads, and initializes each %dx register to 3.\nWhat values will %dx see? Run with the -c ﬂag to see the answers.\nDoes the presence of multiple threads affect anything about your\ncalculations? Is there a race condition in this code?\n3. Now run the following:\n./x86.py -p loop.s -t 2 -i 3 -r -a dx=3,dx=3 -R dx\nThis makes the interrupt interval quite small and random; use dif-\nferent seeds with -s to see different interleavings. Does the fre-\nquency of interruption change anything about this program?\n4. Next we’ll examine a different program (looping-race-nolock.s).\nThis program accesses a shared variable located at memory address\n2000; we’ll call this variable x for simplicity. Run it with a single\nthread and make sure you understand what it does, like this:\n./x86.py -p looping-race-nolock.s -t 1 -M 2000\nWhat value is found in x (i.e., at memory address 2000) throughout\nthe run? Use -c to check your answer.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1927,
      "extraction_method": "Direct"
    },
    {
      "page_number": 313,
      "chapter": null,
      "content": "CONCURRENCY: AN INTRODUCTION\n277\n5. Now run with multiple iterations and threads:\n./x86.py -p looping-race-nolock.s -t 2 -a bx=3 -M 2000\nDo you understand why the code in each thread loops three times?\nWhat will the ﬁnal value of x be?\n6. Now run with random interrupt intervals:\n./x86.py -p looping-race-nolock.s -t 2 -M 2000 -i 4 -r -s 0\nThen change the random seed, setting -s 1, then -s 2, etc. Can\nyou tell, just by looking at the thread interleaving, what the ﬁnal\nvalue of x will be? Does the exact location of the interrupt matter?\nWhere can it safely occur? Where does an interrupt cause trouble?\nIn other words, where is the critical section exactly?\n7. Now use a ﬁxed interrupt interval to explore the program further.\nRun:\n./x86.py -p looping-race-nolock.s -a bx=1 -t 2 -M 2000 -i 1\nSee if you can guess what the ﬁnal value of the shared variable\nx will be. What about when you change -i 2, -i 3, etc.? For\nwhich interrupt intervals does the program give the “correct” ﬁnal\nanswer?\n8. Now run the same code for more loops (e.g., set -a bx=100). What\ninterrupt intervals, set with the -i ﬂag, lead to a “correct” outcome?\nWhich intervals lead to surprising results?\n9. We’ll examine one last program in this homework (wait-for-me.s).\nRun the code like this:\n./x86.py -p wait-for-me.s -a ax=1,ax=0 -R ax -M 2000\nThis sets the %ax register to 1 for thread 0, and 0 for thread 1, and\nwatches the value of %ax and memory location 2000 throughout\nthe run. How should the code behave? How is the value at location\n2000 being used by the threads? What will its ﬁnal value be?\n10. Now switch the inputs:\n./x86.py -p wait-for-me.s -a ax=0,ax=1 -R ax -M 2000\nHow do the threads behave? What is thread 0 doing? How would\nchanging the interrupt interval (e.g., -i 1000, or perhaps to use\nrandom intervals) change the trace outcome? Is the program efﬁ-\nciently using the CPU?\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 1915,
      "extraction_method": "Direct"
    },
    {
      "page_number": 314,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 315,
      "chapter": null,
      "content": "27\nInterlude: Thread API\nThis chapter brieﬂy covers the main portions of the thread API. Each part\nwill be explained further in the subsequent chapters, as we show how\nto use the API. More details can be found in various books and online\nsources [B97, B+96, K+96]. We should note that the subsequent chapters\nintroduce the concepts of locks and condition variables more slowly, with\nmany examples; this chapter is thus better used as a reference.\nCRUX: HOW TO CREATE AND CONTROL THREADS\nWhat interfaces should the OS present for thread creation and control?\nHow should these interfaces be designed to enable ease of use as well as\nutility?\n27.1\nThread Creation\nThe ﬁrst thing you have to be able to do to write a multi-threaded\nprogram is to create new threads, and thus some kind of thread creation\ninterface must exist. In POSIX, it is easy:\n#include <pthread.h>\nint\npthread_create(\npthread_t *\nthread,\nconst pthread_attr_t *\nattr,\nvoid *\n(*start_routine)(void*),\nvoid *\narg);\nThis declaration might look a little complex (particularly if you haven’t\nused function pointers in C), but actually it’s not too bad.\nThere are\nfour arguments: thread, attr, start routine, and arg. The ﬁrst,\nthread, is a pointer to a structure of type pthread t; we’ll use this\nstructure to interact with this thread, and thus we need to pass it to\npthread create() in order to initialize it.\n279\n",
      "content_length": 1377,
      "extraction_method": "Direct"
    },
    {
      "page_number": 316,
      "chapter": null,
      "content": "280\nINTERLUDE: THREAD API\nThe second argument, attr, is used to specify any attributes this thread\nmight have. Some examples include setting the stack size or perhaps in-\nformation about the scheduling priority of the thread. An attribute is\ninitialized with a separate call to pthread attr init(); see the man-\nual page for details. However, in most cases, the defaults will be ﬁne; in\nthis case, we will simply pass the value NULL in.\nThe third argument is the most complex, but is really just asking: which\nfunction should this thread start running in? In C, we call this a function\npointer, and this one tells us the following is expected: a function name\n(start routine), which is passed a single argument of type void * (as\nindicated in the parentheses after start routine), and which returns a\nvalue of type void * (i.e., a void pointer).\nIf this routine instead required an integer argument, instead of a void\npointer, the declaration would look like this:\nint pthread_create(..., // first two args are the same\nvoid *\n(*start_routine)(int),\nint\narg);\nIf instead the routine took a void pointer as an argument, but returned\nan integer, it would look like this:\nint pthread_create(..., // first two args are the same\nint\n(*start_routine)(void *),\nvoid *\narg);\nFinally, the fourth argument, arg, is exactly the argument to be passed\nto the function where the thread begins execution. You might ask: why\ndo we need these void pointers? Well, the answer is quite simple: having\na void pointer as an argument to the function start routine allows us\nto pass in any type of argument; having it as a return value allows the\nthread to return any type of result.\nLet’s look at an example in Figure 27.1. Here we just create a thread\nthat is passed two arguments, packaged into a single type we deﬁne our-\nselves (myarg t). The thread, once created, can simply cast its argument\nto the type it expects and thus unpack the arguments as desired.\nAnd there it is! Once you create a thread, you really have another\nlive executing entity, complete with its own call stack, running within the\nsame address space as all the currently existing threads in the program.\nThe fun thus begins!\n27.2\nThread Completion\nThe example above shows how to create a thread. However, what\nhappens if you want to wait for a thread to complete? You need to do\nsomething special in order to wait for completion; in particular, you must\ncall the routine pthread join().\nint pthread_join(pthread_t thread, void **value_ptr);\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2541,
      "extraction_method": "Direct"
    },
    {
      "page_number": 317,
      "chapter": null,
      "content": "INTERLUDE: THREAD API\n281\n1\n#include <pthread.h>\n2\n3\ntypedef struct __myarg_t {\n4\nint a;\n5\nint b;\n6\n} myarg_t;\n7\n8\nvoid *mythread(void *arg) {\n9\nmyarg_t *m = (myarg_t *) arg;\n10\nprintf(\"%d %d\\n\", m->a, m->b);\n11\nreturn NULL;\n12\n}\n13\n14\nint\n15\nmain(int argc, char *argv[]) {\n16\npthread_t p;\n17\nint rc;\n18\n19\nmyarg_t args;\n20\nargs.a = 10;\n21\nargs.b = 20;\n22\nrc = pthread_create(&p, NULL, mythread, &args);\n23\n...\n24\n}\nFigure 27.1: Creating a Thread\nThis routine takes only two arguments. The ﬁrst is of type pthread t,\nand is used to specify which thread to wait for. This value is exactly what\nyou passed into the thread library during creation; if you held onto it,\nyou can now use it to wait for the thread to stop running.\nThe second argument is a pointer to the return value you expect to get\nback. Because the routine can return anything, it is deﬁned to return a\npointer to void; because the pthread join() routine changes the value\nof the passed in argument, you need to pass in a pointer to that value, not\njust the value itself.\nLet’s look at another example (Figure 27.2). In the code, a single thread\nis again created, and passed a couple of arguments via the myarg t struc-\nture. To return values, the myret t type is used. Once the thread is\nﬁnished running, the main thread, which has been waiting inside of the\npthread join() routine1, then returns, and we can access the values\nreturned from the thread, namely whatever is in myret t.\nA few things to note about this example. First, often times we don’t\nhave to do all of this painful packing and unpacking of arguments. For\nexample, if we just create a thread with no arguments, we can pass NULL\nin as an argument when the thread is created. Similarly, we can pass NULL\ninto pthread join() if we don’t care about the return value.\nSecond, if we are just passing in a single value (e.g., an int), we don’t\nhave to package it up as an argument. Figure 27.3 shows an example. In\n1Note we use wrapper functions here; speciﬁcally, we call Malloc(), Pthread join(), and\nPthread create(), which just call their similarly-named lower-case versions and make sure the\nroutines did not return anything unexpected.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2210,
      "extraction_method": "Direct"
    },
    {
      "page_number": 318,
      "chapter": null,
      "content": "282\nINTERLUDE: THREAD API\n1\n#include <stdio.h>\n2\n#include <pthread.h>\n3\n#include <assert.h>\n4\n#include <stdlib.h>\n5\n6\ntypedef struct __myarg_t {\n7\nint a;\n8\nint b;\n9\n} myarg_t;\n10\n11\ntypedef struct __myret_t {\n12\nint x;\n13\nint y;\n14\n} myret_t;\n15\n16\nvoid *mythread(void *arg) {\n17\nmyarg_t *m = (myarg_t *) arg;\n18\nprintf(\"%d %d\\n\", m->a, m->b);\n19\nmyret_t *r = Malloc(sizeof(myret_t));\n20\nr->x = 1;\n21\nr->y = 2;\n22\nreturn (void *) r;\n23\n}\n24\n25\nint\n26\nmain(int argc, char *argv[]) {\n27\nint rc;\n28\npthread_t p;\n29\nmyret_t *m;\n30\n31\nmyarg_t args;\n32\nargs.a = 10;\n33\nargs.b = 20;\n34\nPthread_create(&p, NULL, mythread, &args);\n35\nPthread_join(p, (void **) &m);\n36\nprintf(\"returned %d %d\\n\", m->x, m->y);\n37\nreturn 0;\n38\n}\nFigure 27.2: Waiting for Thread Completion\nthis case, life is a bit simpler, as we don’t have to package arguments and\nreturn values inside of structures.\nThird, we should note that one has to be extremely careful with how\nvalues are returned from a thread. In particular, never return a pointer\nwhich refers to something allocated on the thread’s call stack. If you do,\nwhat do you think will happen? (think about it!) Here is an example of a\ndangerous piece of code, modiﬁed from the example in Figure 27.2.\n1\nvoid *mythread(void *arg) {\n2\nmyarg_t *m = (myarg_t *) arg;\n3\nprintf(\"%d %d\\n\", m->a, m->b);\n4\nmyret_t r; // ALLOCATED ON STACK: BAD!\n5\nr.x = 1;\n6\nr.y = 2;\n7\nreturn (void *) &r;\n8\n}\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1458,
      "extraction_method": "Direct"
    },
    {
      "page_number": 319,
      "chapter": null,
      "content": "INTERLUDE: THREAD API\n283\nvoid *mythread(void *arg) {\nint m = (int) arg;\nprintf(\"%d\\n\", m);\nreturn (void *) (arg + 1);\n}\nint main(int argc, char *argv[]) {\npthread_t p;\nint rc, m;\nPthread_create(&p, NULL, mythread, (void *) 100);\nPthread_join(p, (void **) &m);\nprintf(\"returned %d\\n\", m);\nreturn 0;\n}\nFigure 27.3: Simpler Argument Passing to a Thread\nIn this case, the variable r is allocated on the stack of mythread. How-\never, when it returns, the value is automatically deallocated (that’s why\nthe stack is so easy to use, after all!), and thus, passing back a pointer to\na now deallocated variable will lead to all sorts of bad results. Certainly,\nwhen you print out the values you think you returned, you’ll probably\n(but not necessarily!) be surprised. Try it and ﬁnd out for yourself2!\nFinally, you might notice that the use of pthread create() to create\na thread, followed by an immediate call to pthread join(), is a pretty\nstrange way to create a thread. In fact, there is an easier way to accom-\nplish this exact task; it’s called a procedure call. Clearly, we’ll usually be\ncreating more than just one thread and waiting for it to complete, other-\nwise there is not much purpose to using threads at all.\nWe should note that not all code that is multi-threaded uses the join\nroutine. For example, a multi-threaded web server might create a number\nof worker threads, and then use the main thread to accept requests and\npass them to the workers, indeﬁnitely. Such long-lived programs thus\nmay not need to join. However, a parallel program that creates threads\nto execute a particular task (in parallel) will likely use join to make sure\nall such work completes before exiting or moving onto the next stage of\ncomputation.\n27.3\nLocks\nBeyond thread creation and join, probably the next most useful set of\nfunctions provided by the POSIX threads library are those for providing\nmutual exclusion to a critical section via locks. The most basic pair of\nroutines to use for this purpose is provided by this pair of routines:\nint pthread_mutex_lock(pthread_mutex_t *mutex);\nint pthread_mutex_unlock(pthread_mutex_t *mutex);\n2Fortunately the compiler gcc will likely complain when you write code like this, which\nis yet another reason to pay attention to compiler warnings.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2317,
      "extraction_method": "Direct"
    },
    {
      "page_number": 320,
      "chapter": null,
      "content": "284\nINTERLUDE: THREAD API\nThe routines should be easy to understand and use. When you have a\nregion of code you realize is a critical section, and thus needs to be pro-\ntected by locks in order to operate as desired. You can probably imagine\nwhat the code looks like:\npthread_mutex_t lock;\npthread_mutex_lock(&lock);\nx = x + 1; // or whatever your critical section is\npthread_mutex_unlock(&lock);\nThe intent of the code is as follows: if no other thread holds the lock\nwhen pthread mutex lock() is called, the thread will acquire the lock\nand enter the critical section. If another thread does indeed hold the lock,\nthe thread trying to grab the lock will not return from the call until it has\nacquired the lock (implying that the thread holding the lock has released\nit via the unlock call). Of course, many threads may be stuck waiting\ninside the lock acquisition function at a given time; only the thread with\nthe lock acquired, however, should call unlock.\nUnfortunately, this code is broken, in two important ways. The ﬁrst\nproblem is a lack of proper initialization. All locks must be properly\ninitialized in order to guarantee that they have the correct values to begin\nwith and thus work as desired when lock and unlock are called.\nWith POSIX threads, there are two ways to initialize locks. One way\nto do this is to use PTHREAD MUTEX INITIALIZER, as follows:\npthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;\nDoing so sets the lock to the default values and thus makes the lock\nusable. The dynamic way to do it (i.e., at run time) is to make a call to\npthread mutex init(), as follows:\nint rc = pthread_mutex_init(&lock, NULL);\nassert(rc == 0); // always check success!\nThe ﬁrst argument to this routine is the address of the lock itself, whereas\nthe second is an optional set of attributes. Read more about the attributes\nyourself; passing NULL in simply uses the defaults. Either way works, but\nwe usually use the dynamic (latter) method. Note that a corresponding\ncall to pthread cond destroy() should also be made, when you are\ndone with the lock; see the manual page for all of details.\nThe second problem with the code above is that it fails to check errors\ncode when calling lock and unlock. Just like virtually any library rou-\ntine you call in a UNIX system, these routines can also fail! If your code\ndoesn’t properly check error codes, the failure will happen silently, which\nin this case could allow multiple threads into a critical section. Minimally,\nuse wrappers, which assert that the routine succeeded (e.g., as in Fig-\nure 27.4); more sophisticated (non-toy) programs, which can’t simply exit\nwhen something goes wrong, should check for failure and do something\nappropriate when the lock or unlock does not succeed.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2790,
      "extraction_method": "Direct"
    },
    {
      "page_number": 321,
      "chapter": null,
      "content": "INTERLUDE: THREAD API\n285\n// Use this to keep your code clean but check for failures\n// Only use if exiting program is OK upon failure\nvoid Pthread_mutex_lock(pthread_mutex_t *mutex) {\nint rc = pthread_mutex_lock(mutex);\nassert(rc == 0);\n}\nFigure 27.4: An Example Wrapper\nThe lock and unlock routines are not the only routines that pthreads\nhas to interact with locks. In particular, here are two more routines which\nmay be of interest:\nint pthread_mutex_trylock(pthread_mutex_t *mutex);\nint pthread_mutex_timedlock(pthread_mutex_t *mutex,\nstruct timespec *abs_timeout);\nThese two calls are used in lock acquisition. The trylock version re-\nturns failure if the lock is already held; the timedlock version of acquir-\ning a lock returns after a timeout or after acquiring the lock, whichever\nhappens ﬁrst. Thus, the timedlock with a timeout of zero degenerates\nto the trylock case. Both of these versions should generally be avoided;\nhowever, there are a few cases where avoiding getting stuck (perhaps in-\ndeﬁnitely) in a lock acquisition routine can be useful, as we’ll see in future\nchapters (e.g., when we study deadlock).\n27.4\nCondition Variables\nThe other major component of any threads library, and certainly the\ncase with POSIX threads, is the presence of a condition variable. Con-\ndition variables are useful when some kind of signaling must take place\nbetween threads, if one thread is waiting for another to do something be-\nfore it can continue. Two primary routines are used by programs wishing\nto interact in this way:\nint pthread_cond_wait(pthread_cond_t *cond, pthread_mutex_t *mutex);\nint pthread_cond_signal(pthread_cond_t *cond);\nTo use a condition variable, one has to in addition have a lock that is\nassociated with this condition. When calling either of the above routines,\nthis lock should be held.\nThe ﬁrst routine, pthread cond wait(), puts the calling thread to\nsleep, and thus waits for some other thread to signal it, usually when\nsomething in the program has changed that the now-sleeping thread might\ncare about. For example, a typical usage looks like this:\npthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;\npthread_cond_t\ninit = PTHREAD_COND_INITIALIZER;\nPthread_mutex_lock(&lock);\nwhile (initialized == 0)\nPthread_cond_wait(&init, &lock);\nPthread_mutex_unlock(&lock);\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2342,
      "extraction_method": "Direct"
    },
    {
      "page_number": 322,
      "chapter": null,
      "content": "286\nINTERLUDE: THREAD API\nIn this code, after initialization of the relevant lock and condition3,\na thread checks to see if the variable initialized has yet been set to\nsomething other than zero. If not, the thread simply calls the wait routine\nin order to sleep until some other thread wakes it.\nThe code to wake a thread, which would run in some other thread,\nlooks like this:\nPthread_mutex_lock(&lock);\ninitialized = 1;\nPthread_cond_signal(&init);\nPthread_mutex_unlock(&lock);\nA few things to note about this code sequence. First, when signal-\ning (as well as when modifying the global variable initialized), we\nalways make sure to have the lock held. This ensures that we don’t acci-\ndentally introduce a race condition into our code.\nSecond, you might notice that the wait call takes a lock as its second\nparameter, whereas the signal call only takes a condition. The reason\nfor this difference is that the wait call, in addition to putting the call-\ning thread to sleep, releases the lock when putting said caller to sleep.\nImagine if it did not: how could the other thread acquire the lock and\nsignal it to wake up? However, before returning after being woken, the\npthread cond wait() re-acquires the lock, thus ensuring that any time\nthe waiting thread is running between the lock acquire at the beginning\nof the wait sequence, and the lock release at the end, it holds the lock.\nOne last oddity: the waiting thread re-checks the condition in a while\nloop, instead of a simple if statement. We’ll discuss this issue in detail\nwhen we study condition variables in a future chapter, but in general,\nusing a while loop is the simple and safe thing to do. Although it rechecks\nthe condition (perhaps adding a little overhead), there are some pthread\nimplementations that could spuriously wake up a waiting thread; in such\na case, without rechecking, the waiting thread will continue thinking that\nthe condition has changed even though it has not. It is safer thus to view\nwaking up as a hint that something might have changed, rather than an\nabsolute fact.\nNote that sometimes it is tempting to use a simple ﬂag to signal be-\ntween two threads, instead of a condition variable and associated lock.\nFor example, we could rewrite the waiting code above to look more like\nthis in the waiting code:\nwhile (initialized == 0)\n; // spin\nThe associated signaling code would look like this:\ninitialized = 1;\n3Note\nthat\none\ncould\nuse\npthread cond init()\n(and\ncorrespond-\ning\nthe\npthread cond destroy()\ncall)\ninstead\nof\nthe\nstatic\ninitializer\nPTHREAD COND INITIALIZER. Sound like more work? It is.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2638,
      "extraction_method": "Direct"
    },
    {
      "page_number": 323,
      "chapter": null,
      "content": "INTERLUDE: THREAD API\n287\nDon’t ever do this, for the following reasons. First, it performs poorly\nin many cases (spinning for a long time just wastes CPU cycles). Sec-\nond, it is error prone. As recent research shows [X+10], it is surprisingly\neasy to make mistakes when using ﬂags (as above) to synchronize be-\ntween threads; roughly half the uses of these ad hoc synchronizations were\nbuggy! Don’t be lazy; use condition variables even when you think you\ncan get away without doing so.\n27.5\nCompiling and Running\nAll of the code examples in this chapter are relatively easy to get up\nand running. To compile them, you must include the header pthread.h\nin your code. On the link line, you must also explicitly link with the\npthreads library, by adding the -pthread ﬂag.\nFor example, to compile a simple multi-threaded program, all you\nhave to do is the following:\nprompt> gcc -o main main.c -Wall -pthread\nAs long as main.c includes the pthreads header, you have now suc-\ncessfully compiled a concurrent program. Whether it works or not, as\nusual, is a different matter entirely.\n27.6\nSummary\nWe have introduced the basics of the pthread library, including thread\ncreation, building mutual exclusion via locks, and signaling and waiting\nvia condition variables. You don’t need much else to write robust and\nefﬁcient multi-threaded code, except patience and a great deal of care!\nWe now end the chapter with a set of tips that might be useful to you\nwhen you write multi-threaded code (see the aside on the following page\nfor details). There are other aspects of the API that are interesting; if you\nwant more information, type man -k pthread on a Linux system to\nsee over one hundred APIs that make up the entire interface. However,\nthe basics discussed herein should enable you to build sophisticated (and\nhopefully, correct and performant) multi-threaded programs. The hard\npart with threads is not the APIs, but rather the tricky logic of how you\nbuild concurrent programs. Read on to learn more.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2043,
      "extraction_method": "Direct"
    },
    {
      "page_number": 324,
      "chapter": null,
      "content": "288\nINTERLUDE: THREAD API\nASIDE: THREAD API GUIDELINES\nThere are a number of small but important things to remember when\nyou use the POSIX thread library (or really, any thread library) to build a\nmulti-threaded program. They are:\n• Keep it simple. Above all else, any code to lock or signal between\nthreads should be as simple as possible. Tricky thread interactions\nlead to bugs.\n• Minimize thread interactions. Try to keep the number of ways\nin which threads interact to a minimum. Each interaction should\nbe carefully thought out and constructed with tried and true ap-\nproaches (many of which we will learn about in the coming chap-\nters).\n• Initialize locks and condition variables. Failure to do so will lead\nto code that sometimes works and sometimes fails in very strange\nways.\n• Check your return codes. Of course, in any C and UNIX program-\nming you do, you should be checking each and every return code,\nand it’s true here as well. Failure to do so will lead to bizarre and\nhard to understand behavior, making you likely to (a) scream, (b)\npull some of your hair out, or (c) both.\n• Be careful with how you pass arguments to, and return values\nfrom, threads. In particular, any time you are passing a reference to\na variable allocated on the stack, you are probably doing something\nwrong.\n• Each thread has its own stack. As related to the point above, please\nremember that each thread has its own stack. Thus, if you have a\nlocally-allocated variable inside of some function a thread is exe-\ncuting, it is essentially private to that thread; no other thread can\n(easily) access it. To share data between threads, the values must be\nin the heap or otherwise some locale that is globally accessible.\n• Always use condition variables to signal between threads. While\nit is often tempting to use a simple ﬂag, don’t do it.\n• Use the manual pages. On Linux, in particular, the pthread man\npages are highly informative and discuss much of the nuances pre-\nsented here, often in even more detail. Read them carefully!\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2071,
      "extraction_method": "Direct"
    },
    {
      "page_number": 325,
      "chapter": null,
      "content": "INTERLUDE: THREAD API\n289\nReferences\n[B97] “Programming with POSIX Threads”\nDavid R. Butenhof\nAddison-Wesley, May 1997\nAnother one of these books on threads.\n[B+96] “PThreads Programming:\nA POSIX Standard for Better Multiprocessing”\nDick Buttlar, Jacqueline Farrell, Bradford Nichols\nO’Reilly, September 1996\nA reasonable book from the excellent, practical publishing house O’Reilly. Our bookshelves certainly\ncontain a great deal of books from this company, including some excellent offerings on Perl, Python, and\nJavascript (particularly Crockford’s “Javascript: The Good Parts”.)\n[K+96] “Programming With Threads”\nSteve Kleiman, Devang Shah, Bart Smaalders\nPrentice Hall, January 1996\nProbably one of the better books in this space. Get it at your local library.\n[X+10] “Ad Hoc Synchronization Considered Harmful”\nWeiwei Xiong, Soyeon Park, Jiaqi Zhang, Yuanyuan Zhou, Zhiqiang Ma\nOSDI 2010, Vancouver, Canada\nThis paper shows how seemingly simple synchronization code can lead to a surprising number of bugs.\nUse condition variables and do the signaling correctly!\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 1110,
      "extraction_method": "Direct"
    },
    {
      "page_number": 326,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 327,
      "chapter": null,
      "content": "28\nLocks\nFrom the introduction to concurrency, we saw one of the fundamental\nproblems in concurrent programming: we would like to execute a series\nof instructions atomically, but due to the presence of interrupts on a single\nprocessor (or multiple threads executing on multiple processors concur-\nrently), we couldn’t. In this chapter, we thus attack this problem directly,\nwith the introduction of something referred to as a lock. Programmers\nannotate source code with locks, putting them around critical sections,\nand thus ensure that any such critical section executes as if it were a sin-\ngle atomic instruction.\n28.1\nLocks: The Basic Idea\nAs an example, assume our critical section looks like this, the canonical\nupdate of a shared variable:\nbalance = balance + 1;\nOf course, other critical sections are possible, such as adding an ele-\nment to a linked list or other more complex updates to shared structures,\nbut we’ll just keep to this simple example for now. To use a lock, we add\nsome code around the critical section like this:\n1\nlock_t mutex; // some globally-allocated lock ’mutex’\n2\n...\n3\nlock(&mutex);\n4\nbalance = balance + 1;\n5\nunlock(&mutex);\nA lock is just a variable, and thus to use one, you must declare a lock\nvariable of some kind (such as mutex above). This lock variable (or just\n“lock” for short) holds the state of the lock at any instant in time. It is ei-\nther available (or unlocked or free) and thus no thread holds the lock, or\nacquired (or locked or held), and thus exactly one thread holds the lock\nand presumably is in a critical section. We could store other information\n291\n",
      "content_length": 1611,
      "extraction_method": "Direct"
    },
    {
      "page_number": 328,
      "chapter": null,
      "content": "292\nLOCKS\nin the data type as well, such as which thread holds the lock, or a queue\nfor ordering lock acquisition, but information like that is hidden from the\nuser of the lock.\nThe semantics of the lock() and unlock() routines are simple. Call-\ning the routine lock() tries to acquire the lock; if no other thread holds\nthe lock (i.e., it is free), the thread will acquire the lock and enter the crit-\nical section; this thread is sometimes said to be the owner of the lock. If\nanother thread then calls lock() on that same lock variable (mutex in\nthis example), it will not return while the lock is held by another thread;\nin this way, other threads are prevented from entering the critical section\nwhile the ﬁrst thread that holds the lock is in there.\nOnce the owner of the lock calls unlock(), the lock is now available\n(free) again. If no other threads are waiting for the lock (i.e., no other\nthread has called lock() and is stuck therein), the state of the lock is\nsimply changed to free. If there are waiting threads (stuck in lock()),\none of them will (eventually) notice (or be informed of) this change of the\nlock’s state, acquire the lock, and enter the critical section.\nLocks provide some minimal amount of control over scheduling to\nprogrammers. In general, we view threads as entities created by the pro-\ngrammer but scheduled by the OS, in any fashion that the OS chooses.\nLocks yield some of that control back to the programmer; by putting\na lock around a section of code, the programmer can guarantee that no\nmore than a single thread can ever be active within that code. Thus locks\nhelp transform the chaos that is traditional OS scheduling into a more\ncontrolled activity.\n28.2\nPthread Locks\nThe name that the POSIX library uses for a lock is a mutex, as it is used\nto provide mutual exclusion between threads, i.e., if one thread is in the\ncritical section, it excludes the others from entering until it has completed\nthe section. Thus, when you see the following POSIX threads code, you\nshould understand that it is doing the same thing as above (we again use\nour wrappers that check for errors upon lock and unlock):\n1\npthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;\n2\n3\nPthread_mutex_lock(&lock);\n// wrapper for pthread_mutex_lock()\n4\nbalance = balance + 1;\n5\nPthread_mutex_unlock(&lock);\nYou might also notice here that the POSIX version passes a variable\nto lock and unlock, as we may be using different locks to protect different\nvariables. Doing so can increase concurrency: instead of one big lock that\nis used any time any critical section is accessed (a coarse-grained locking\nstrategy), one will often protect different data and data structures with\ndifferent locks, thus allowing more threads to be in locked code at once\n(a more ﬁne-grained approach).\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2837,
      "extraction_method": "Direct"
    },
    {
      "page_number": 329,
      "chapter": null,
      "content": "LOCKS\n293\n28.3\nBuilding A Lock\nBy now, you should have some understanding of how a lock works,\nfrom the perspective of a programmer. But how should we build a lock?\nWhat hardware support is needed? What OS support? It is this set of\nquestions we address in the rest of this chapter.\nThe Crux: HOW TO BUILD A LOCK\nHow can we build an efﬁcient lock? Efﬁcient locks provided mutual\nexclusion at low cost, and also might attain a few other properties we\ndiscuss below. What hardware support is needed? What OS support?\nTo build a working lock, we will need some help from our old friend,\nthe hardware, as well as our good pal, the OS. Over the years, a num-\nber of different hardware primitives have been added to the instruction\nsets of various computer architectures; while we won’t study how these\ninstructions are implemented (that, after all, is the topic of a computer\narchitecture class), we will study how to use them in order to build a mu-\ntual exclusion primitive like a lock. We will also study how the OS gets\ninvolved to complete the picture and enable us to build a sophisticated\nlocking library.\n28.4\nEvaluating Locks\nBefore building any locks, we should ﬁrst understand what our goals\nare, and thus we ask how to evaluate the efﬁcacy of a particular lock\nimplementation. To evaluate whether a lock works (and works well), we\nshould ﬁrst establish some basic criteria. The ﬁrst is whether the lock does\nits basic task, which is to provide mutual exclusion. Basically, does the\nlock work, preventing multiple threads from entering a critical section?\nThe second is fairness. Does each thread contending for the lock get\na fair shot at acquiring it once it is free? Another way to look at this is\nby examining the more extreme case: does any thread contending for the\nlock starve while doing so, thus never obtaining it?\nThe ﬁnal criterion is performance, speciﬁcally the time overheads added\nby using the lock. There are a few different cases that are worth con-\nsidering here. One is the case of no contention; when a single thread\nis running and grabs and releases the lock, what is the overhead of do-\ning so? Another is the case where multiple threads are contending for\nthe lock on a single CPU; in this case, are there performance concerns? Fi-\nnally, how does the lock perform when there are multiple CPUs involved,\nand threads on each contending for the lock? By comparing these differ-\nent scenarios, we can better understand the performance impact of using\nvarious locking techniques, as described below.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2567,
      "extraction_method": "Direct"
    },
    {
      "page_number": 330,
      "chapter": null,
      "content": "294\nLOCKS\n28.5\nControlling Interrupts\nOne of the earliest solutions used to provide mutual exclusion was\nto disable interrupts for critical sections; this solution was invented for\nsingle-processor systems. The code would look like this:\n1\nvoid lock() {\n2\nDisableInterrupts();\n3\n}\n4\nvoid unlock() {\n5\nEnableInterrupts();\n6\n}\nAssume we are running on such a single-processor system. By turn-\ning off interrupts (using some kind of special hardware instruction) be-\nfore entering a critical section, we ensure that the code inside the critical\nsection will not be interrupted, and thus will execute as if it were atomic.\nWhen we are ﬁnished, we re-enable interrupts (again, via a hardware in-\nstruction) and thus the program proceeds as usual.\nThe main positive of this approach is its simplicity. You certainly don’t\nhave to scratch your head too hard to ﬁgure out why this works. Without\ninterruption, a thread can be sure that the code it executes will execute\nand that no other thread will interfere with it.\nThe negatives, unfortunately, are many. First, this approach requires\nus to allow any calling thread to perform a privileged operation (turning\ninterrupts on and off), and thus trust that this facility is not abused. As\nyou already know, any time we are required to trust an arbitrary pro-\ngram, we are probably in trouble. Here, the trouble manifests in numer-\nous ways: a greedy program could call lock() at the beginning of its\nexecution and thus monopolize the processor; worse, an errant or mali-\ncious program could call lock() and go into an endless loop. In this\nlatter case, the OS never regains control of the system, and there is only\none recourse: restart the system. Using interrupt disabling as a general-\npurpose synchronization solution requires too much trust in applications.\nSecond, the approach does not work on multiprocessors. If multiple\nthreads are running on different CPUs, and each try to enter the same\ncritical section, it does not matter whether interrupts are disabled; threads\nwill be able to run on other processors, and thus could enter the critical\nsection. As multiprocessors are now commonplace, our general solution\nwill have to do better than this.\nThird, and probably least important, this approach can be inefﬁcient.\nCompared to normal instruction execution, code that masks or unmasks\ninterrupts tends to be executed slowly by modern CPUs.\nFor these reasons, turning off interrupts is only used in limited con-\ntexts as a mutual-exclusion primitive. For example, in some cases an\noperating system itself will sometimes use interrupt masking to guaran-\ntee atomicity when accessing its own data structures, or at least to pre-\nvent certain messy interrupt handling situations from arising. This usage\nmakes sense, as the trust issue disappears inside the OS, which always\ntrusts itself to perform privileged operations anyhow.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2926,
      "extraction_method": "Direct"
    },
    {
      "page_number": 331,
      "chapter": null,
      "content": "LOCKS\n295\nASIDE: DEKKER’S AND PETERSON’S ALGORITHMS\nIn the 1960’s, Dijkstra posed the concurrency problem to his friends,\nand one of them, a mathematician named Theodorus Jozef Dekker, came\nup with a solution [D68]. Unlike the solutions we discuss here, which use\nspecial hardware instructions and even OS support, Dekker’s approach\nuses just loads and stores (assuming they are atomic with respect to each\nother, which was true on early hardware).\nDekker’s approach was later reﬁned by Peterson [P81] (and thus “Pe-\nterson’s algorithm”), shown here. Once again, just loads and stores are\nused, and the idea is to ensure that two threads never enter a critical sec-\ntion at the same time. Here is Peterson’s algorithm (for two threads); see\nif you can understand it.\nint flag[2];\nint turn;\nvoid init() {\nflag[0] = flag[1] = 0;\n// 1->thread wants to grab lock\nturn = 0;\n// whose turn? (thread 0 or 1?)\n}\nvoid lock() {\nflag[self] = 1;\n// self: thread ID of caller\nturn = 1 - self;\n// make it other thread’s turn\nwhile ((flag[1-self] == 1) && (turn == 1 - self))\n; // spin-wait\n}\nvoid unlock() {\nflag[self] = 0;\n// simply undo your intent\n}\nFor some reason, developing locks that work without special hard-\nware support became all the rage for a while, giving theory-types a lot\nof problems to work on. Of course, this all became quite useless when\npeople realized it is much easier to assume a little hardware support (and\nindeed that support had been around from the very earliest days of multi-\nprocessing). Further, algorithms like the ones above don’t work on mod-\nern hardware (due to relaxed memory consistency models), thus making\nthem even less useful than they were before. Yet more research relegated\nto the dustbin of history...\n28.6\nTest And Set (Atomic Exchange)\nBecause disabling interrupts does not work on multiple processors,\nsystem designers started to invent hardware support for locking. The\nearliest multiprocessor systems, such as the Burroughs B5000 in the early\n1960’s [M82], had such support; today all systems provide this type of\nsupport, even for single CPU systems.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2134,
      "extraction_method": "Direct"
    },
    {
      "page_number": 332,
      "chapter": null,
      "content": "296\nLOCKS\n1\ntypedef struct __lock_t { int flag; } lock_t;\n2\n3\nvoid init(lock_t *mutex) {\n4\n// 0 -> lock is available, 1 -> held\n5\nmutex->flag = 0;\n6\n}\n7\n8\nvoid lock(lock_t *mutex) {\n9\nwhile (mutex->flag == 1)\n// TEST the flag\n10\n; // spin-wait (do nothing)\n11\nmutex->flag = 1;\n// now SET it!\n12\n}\n13\n14\nvoid unlock(lock_t *mutex) {\n15\nmutex->flag = 0;\n16\n}\nFigure 28.1: First Attempt: A Simple Flag\nThe simplest bit of hardware support to understand is what is known\nas a test-and-set instruction, also known as atomic exchange. To under-\nstand how test-and-set works, let’s ﬁrst try to build a simple lock without\nit. In this failed attempt, we use a simple ﬂag variable to denote whether\nthe lock is held or not.\nIn this ﬁrst attempt (Figure 28.1), the idea is quite simple: use a simple\nvariable to indicate whether some thread has possession of a lock. The\nﬁrst thread that enters the critical section will call lock(), which tests\nwhether the ﬂag is equal to 1 (in this case, it is not), and then sets the ﬂag\nto 1 to indicate that the thread now holds the lock. When ﬁnished with\nthe critical section, the thread calls unlock() and clears the ﬂag, thus\nindicating that the lock is no longer held.\nIf another thread happens to call lock() while that ﬁrst thread is in\nthe critical section, it will simply spin-wait in the while loop for that\nthread to call unlock() and clear the ﬂag. Once that ﬁrst thread does\nso, the waiting thread will fall out of the while loop, set the ﬂag to 1 for\nitself, and proceed into the critical section.\nUnfortunately, the code has two problems: one of correctness, and an-\nother of performance. The correctness problem is simple to see once you\nget used to thinking about concurrent programming. Imagine the code\ninterleaving in Table 28.1 (assume flag=0 to begin).\nThread 1\nThread 2\ncall lock()\nwhile (ﬂag == 1)\ninterrupt: switch to Thread 2\ncall lock()\nwhile (ﬂag == 1)\nﬂag = 1;\ninterrupt: switch to Thread 1\nﬂag = 1; // set ﬂag to 1 (too!)\nTable 28.1: Trace: No Mutual Exclusion\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2067,
      "extraction_method": "Direct"
    },
    {
      "page_number": 333,
      "chapter": null,
      "content": "LOCKS\n297\nTIP: THINK ABOUT CONCURRENCY AS MALICIOUS SCHEDULER\nWhat we also get from this example is a sense of the approach we\nneed to take when trying to understand concurrent execution. What you\nare really trying to do is to pretend you are a malicious scheduler, one\nthat interrupts threads at the most inopportune of times in order to foil\ntheir feeble attempts at building synchronization primitives. Although\nthe exact sequence of interrupts may be improbable, it is possible, and that\nis all we need to show to demonstrate that a particular approach does not\nwork.\nAs you can see from this interleaving, with timely (untimely?) inter-\nrupts, we can easily produce a case where both threads set their ﬂags to 1\nand both threads are thus able to enter the critical section. This is bad! We\nhave obviously failed to provide the most basic requirement: providing\nmutual exclusion.\nThe performance problem, which we will address more later on, is the\nfact that the way a thread waits to acquire a lock that is already held:\nit endlessly checks the value of ﬂag, a technique known as spin-waiting.\nSpin-waiting wastes time waiting for another thread to release a lock. The\nwaste is exceptionally high on a uniprocessor, where the thread that the\nwaiter is waiting for cannot even run (at least, until a context switch oc-\ncurs)! Thus, as we move forward and develop more sophisticated solu-\ntions, we should also consider ways to avoid this kind of waste.\n28.7\nBuilding A Working Spin Lock\nWhile the idea behind the example above is a good one, it is not possi-\nble to implement without some support from the hardware. Fortunately,\nsome systems provide an instruction to support the creation of simple\nlocks based on this concept. This more powerful instruction has differ-\nent names – on SPARC, it is the load/store unsigned byte instruction\n(ldstub), whereas on x86, it is the atomic exchange instruction (xchg)\n– but basically does the same thing across platforms, and is usually gen-\nerally referred to as test-and-set. We deﬁne what the test-and-set instruc-\ntion does with the following C code snippet:\n1\nint TestAndSet(int *ptr, int new) {\n2\nint old = *ptr; // fetch old value at ptr\n3\n*ptr = new;\n// store ’new’ into ptr\n4\nreturn old;\n// return the old value\n5\n}\nWhat the test-and-set instruction does is as follows. It returns the old\nvalue pointed to by the ptr, and simultaneously updates said value to\nnew. The key, of course, is that this sequence of operations is performed\natomically. The reason it is called “test and set” is that it enables you\nto “test” the old value (which is what is returned) while simultaneously\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2677,
      "extraction_method": "Direct"
    },
    {
      "page_number": 334,
      "chapter": null,
      "content": "298\nLOCKS\n1\ntypedef struct __lock_t {\n2\nint flag;\n3\n} lock_t;\n4\n5\nvoid init(lock_t *lock) {\n6\n// 0 indicates that lock is available, 1 that it is held\n7\nlock->flag = 0;\n8\n}\n9\n10\nvoid lock(lock_t *lock) {\n11\nwhile (TestAndSet(&lock->flag, 1) == 1)\n12\n; // spin-wait (do nothing)\n13\n}\n14\n15\nvoid unlock(lock_t *lock) {\n16\nlock->flag = 0;\n17\n}\nFigure 28.2: A Simple Spin Lock Using Test-and-set\n“setting” the memory location to a new value; as it turns out, this slightly\nmore powerful instruction is enough to build a simple spin lock, as we\nnow examine in Figure 28.2.\nLet’s make sure we understand why this works. Imagine ﬁrst the case\nwhere a thread calls lock() and no other thread currently holds the lock;\nthus, flag should be 0. When the thread then calls TestAndSet(flag,\n1), the routine will return the old value of flag, which is 0; thus, the call-\ning thread, which is testing the value of ﬂag, will not get caught spinning\nin the while loop and will acquire the lock. The thread will also atomi-\ncally set the value to 1, thus indicating that the lock is now held. When\nthe thread is ﬁnished with its critical section, it calls unlock() to set the\nﬂag back to zero.\nThe second case we can imagine arises when one thread already has\nthe lock held (i.e., flag is 1). In this case, this thread will call lock() and\nthen call TestAndSet(flag, 1) as well. This time, TestAndSet()\nwill return the old value at ﬂag, which is 1 (because the lock is held),\nwhile simultaneously setting it to 1 again. As long as the lock is held by\nanother thread, TestAndSet() will repeatedly return 1, and thus this\nthread will spin and spin until the lock is ﬁnally released. When the ﬂag is\nﬁnally set to 0 by some other thread, this thread will call TestAndSet()\nagain, which will now return 0 while atomically setting the value to 1 and\nthus acquire the lock and enter the critical section.\nBy making both the test (of the old lock value) and set (of the new\nvalue) a single atomic operation, we ensure that only one thread acquires\nthe lock. And that’s how to build a working mutual exclusion primitive!\nYou may also now understand why this type of lock is usually referred\nto as a spin lock. It is the simplest type of lock to build, and simply spins,\nusing CPU cycles, until the lock becomes available. To work correctly\non a single processor, it requires a preemptive scheduler (i.e., one that\nwill interrupt a thread via a timer, in order to run a different thread, from\ntime to time). Without preemption, spin locks don’t make much sense on\na single CPU, as a thread spinning on a CPU will never relinquish it.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2654,
      "extraction_method": "Direct"
    },
    {
      "page_number": 335,
      "chapter": null,
      "content": "LOCKS\n299\n28.8\nEvaluating Spin Locks\nGiven our basic spin lock, we can now evaluate how effective it is\nalong our previously described axes. The most important aspect of a lock\nis correctness: does it provide mutual exclusion? The answer here is ob-\nviously yes: the spin lock only allows a single thread to enter the critical\nsection at a time. Thus, we have a correct lock.\nThe next axis is fairness. How fair is a spin lock to a waiting thread?\nCan you guarantee that a waiting thread will ever enter the critical sec-\ntion? The answer here, unfortunately, is bad news: spin locks don’t pro-\nvide any fairness guarantees. Indeed, a thread spinning may spin forever,\nunder contention. Spin locks are not fair and may lead to starvation.\nThe ﬁnal axis is performance. What are the costs of using a spin lock?\nTo analyze this more carefully, we suggest thinking about a few different\ncases. In the ﬁrst, imagine threads competing for the lock on a single\nprocessor; in the second, consider the threads as spread out across many\nprocessors.\nFor spin locks, in the single CPU case, performance overheads can\nbe quite painful; imagine the case where the thread holding the lock is\npre-empted within a critical section. The scheduler might then run every\nother thread (imagine there are N −1 others), each of which tries to ac-\nquire the lock. In this case, each of those threads will spin for the duration\nof a time slice before giving up the CPU, a waste of CPU cycles.\nHowever, on multiple CPUs, spin locks work reasonably well (if the\nnumber of threads roughly equals the number of CPUs). The thinking\ngoes as follows: imagine Thread A on CPU 1 and Thread B on CPU 2,\nboth contending for a lock. If Thread A (CPU 1) grabs the lock, and then\nThread B tries to, B will spin (on CPU 2). However, presumably the crit-\nical section is short, and thus soon the lock becomes available, and is ac-\nquired by Thread B. Spinning to wait for a lock held on another processor\ndoesn’t waste many cycles in this case, and thus can be quite effective.\n28.9\nCompare-And-Swap\nAnother hardware primitive that some systems provide is known as\nthe compare-and-swap instruction (as it is called on SPARC, for exam-\nple), or compare-and-exchange (as it called on x86). The C pseudocode\nfor this single instruction is found in Figure 28.3.\nThe basic idea is for compare-and-swap to test whether the value at the\n1\nint CompareAndSwap(int *ptr, int expected, int new) {\n2\nint actual = *ptr;\n3\nif (actual == expected)\n4\n*ptr = new;\n5\nreturn actual;\n6\n}\nFigure 28.3: Compare-and-swap\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2598,
      "extraction_method": "Direct"
    },
    {
      "page_number": 336,
      "chapter": null,
      "content": "300\nLOCKS\naddress speciﬁed by ptr is equal to expected; if so, update the memory\nlocation pointed to by ptr with the new value. If not, do nothing. In\neither case, return the actual value at that memory location, thus allowing\nthe code calling compare-and-swap to know whether it succeeded or not.\nWith the compare-and-swap instruction, we can build a lock in a man-\nner quite similar to that with test-and-set. For example, we could just\nreplace the lock() routine above with the following:\n1\nvoid lock(lock_t *lock) {\n2\nwhile (CompareAndSwap(&lock->flag, 0, 1) == 1)\n3\n; // spin\n4\n}\nThe rest of the code is the same as the test-and-set example above.\nThis code works quite similarly; it simply checks if the ﬂag is 0 and if\nso, atomically swaps in a 1 thus acquiring the lock. Threads that try to\nacquire the lock while it is held will get stuck spinning until the lock is\nﬁnally released.\nIf you want to see how to really make a C-callable x86-version of\ncompare-and-swap, this code sequence might be useful (from [S05]):\n1\nchar CompareAndSwap(int *ptr, int old, int new) {\n2\nunsigned char ret;\n3\n4\n// Note that sete sets a ’byte’ not the word\n5\n__asm__ __volatile__ (\n6\n\"\nlock\\n\"\n7\n\"\ncmpxchgl %2,%1\\n\"\n8\n\"\nsete %0\\n\"\n9\n: \"=q\" (ret), \"=m\" (*ptr)\n10\n: \"r\" (new), \"m\" (*ptr), \"a\" (old)\n11\n: \"memory\");\n12\nreturn ret;\n13\n}\nFinally, as you may have sensed, compare-and-swap is a more power-\nful instruction than test-and-set. We will make some use of this power in\nthe future when we brieﬂy delve into wait-free synchronization [H91].\nHowever, if we just build a simple spin lock with it, its behavior is iden-\ntical to the spin lock we analyzed above.\n28.10\nLoad-Linked and Store-Conditional\nSome platforms provide a pair of instructions that work in concert to\nhelp build critical sections. On the MIPS architecture [H93], for example,\nthe load-linked and store-conditional instructions can be used in tandem\nto build locks and other concurrent structures. The C pseudocode for\nthese instructions is as found in Figure 28.4. Alpha, PowerPC, and ARM\nprovide similar instructions [W09].\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2133,
      "extraction_method": "Direct"
    },
    {
      "page_number": 337,
      "chapter": null,
      "content": "LOCKS\n301\n1\nint LoadLinked(int *ptr) {\n2\nreturn *ptr;\n3\n}\n4\n5\nint StoreConditional(int *ptr, int value) {\n6\nif (no one has updated *ptr since the LoadLinked to this address) {\n7\n*ptr = value;\n8\nreturn 1; // success!\n9\n} else {\n10\nreturn 0; // failed to update\n11\n}\n12\n}\nFigure 28.4: Load-linked And Store-conditional\n1\nvoid lock(lock_t *lock) {\n2\nwhile (1) {\n3\nwhile (LoadLinked(&lock->flag) == 1)\n4\n; // spin until it’s zero\n5\nif (StoreConditional(&lock->flag, 1) == 1)\n6\nreturn; // if set-it-to-1 was a success: all done\n7\n// otherwise: try it all over again\n8\n}\n9\n}\n10\n11\nvoid unlock(lock_t *lock) {\n12\nlock->flag = 0;\n13\n}\nFigure 28.5: Using LL/SC To Build A Lock\nThe load-linked operates much like a typical load instruction, and sim-\nply fetches a value from memory and places it in a register. The key differ-\nence comes with the store-conditional, which only succeeds (and updates\nthe value stored at the address just load-linked from) if no intermittent\nstore to the address has taken place. In the case of success, the store-\nconditional returns 1 and updates the value at ptr to value; if it fails,\nthe value at ptr is not updated and 0 is returned.\nAs a challenge to yourself, try thinking about how to build a lock using\nload-linked and store-conditional. Then, when you are ﬁnished, look at\nthe code below which provides one simple solution. Do it! The solution\nis in Figure 28.5.\nThe lock() code is the only interesting piece. First, a thread spins\nwaiting for the ﬂag to be set to 0 (and thus indicate the lock is not held).\nOnce so, the thread tries to acquire the lock via the store-conditional; if it\nsucceeds, the thread has atomically changed the ﬂag’s value to 1 and thus\ncan proceed into the critical section.\nNote how failure of the store-conditional might arise. One thread calls\nlock() and executes the load-linked, returning 0 as the lock is not held.\nBefore it can attempt the store-conditional, it is interrupted and another\nthread enters the lock code, also executing the load-linked instruction,\nand also getting a 0 and continuing. At this point, two threads have\neach executed the load-linked and each are about to attempt the store-\nconditional. The key feature of these instructions is that only one of these\nthreads will succeed in updating the ﬂag to 1 and thus acquire the lock;\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2358,
      "extraction_method": "Direct"
    },
    {
      "page_number": 338,
      "chapter": null,
      "content": "302\nLOCKS\nTIP: LESS CODE IS BETTER CODE (LAUER’S LAW)\nProgrammers tend to brag about how much code they wrote to do some-\nthing. Doing so is fundamentally broken. What one should brag about,\nrather, is how little code one wrote to accomplish a given task. Short,\nconcise code is always preferred; it is likely easier to understand and has\nfewer bugs. As Hugh Lauer said, when discussing the construction of\nthe Pilot operating system: “If the same people had twice as much time,\nthey could produce as good of a system in half the code.” [L81] We’ll call\nthis Lauer’s Law, and it is well worth remembering. So next time you’re\nbragging about how much code you wrote to ﬁnish the assignment, think\nagain, or better yet, go back, rewrite, and make the code as clear and con-\ncise as possible.\nthe second thread to attempt the store-conditional will fail (because the\nother thread updated the value of ﬂag between its load-linked and store-\nconditional) and thus have to try to acquire the lock again.\nIn class a few years ago, undergraduate student David Capel sug-\ngested a more concise form of the above, for those of you who enjoy\nshort-circuiting boolean conditionals. See if you can ﬁgure out why it\nis equivalent. It certainly is shorter!\n1\nvoid lock(lock_t *lock) {\n2\nwhile (LoadLinked(&lock->flag)||!StoreConditional(&lock->flag, 1))\n3\n; // spin\n4\n}\n28.11\nFetch-And-Add\nOne ﬁnal hardware primitive is the fetch-and-add instruction, which\natomically increments a value while returning the old value at a partic-\nular address. The C pseudocode for the fetch-and-add instruction looks\nlike this:\n1\nint FetchAndAdd(int *ptr) {\n2\nint old = *ptr;\n3\n*ptr = old + 1;\n4\nreturn old;\n5\n}\nIn this example, we’ll use fetch-and-add to build a more interesting\nticket lock, as introduced by Mellor-Crummey and Scott [MS91]. The\nlock and unlock code looks like what you see in Figure 28.6.\nInstead of a single value, this solution uses a ticket and turn variable in\ncombination to build a lock. The basic operation is pretty simple: when\na thread wishes to acquire a lock, it ﬁrst does an atomic fetch-and-add\non the ticket value; that value is now considered this thread’s “turn”\n(myturn). The globally shared lock->turn is then used to determine\nwhich thread’s turn it is; when (myturn == turn) for a given thread,\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2353,
      "extraction_method": "Direct"
    },
    {
      "page_number": 339,
      "chapter": null,
      "content": "LOCKS\n303\n1\ntypedef struct __lock_t {\n2\nint ticket;\n3\nint turn;\n4\n} lock_t;\n5\n6\nvoid lock_init(lock_t *lock) {\n7\nlock->ticket = 0;\n8\nlock->turn\n= 0;\n9\n}\n10\n11\nvoid lock(lock_t *lock) {\n12\nint myturn = FetchAndAdd(&lock->ticket);\n13\nwhile (lock->turn != myturn)\n14\n; // spin\n15\n}\n16\n17\nvoid unlock(lock_t *lock) {\n18\nFetchAndAdd(&lock->turn);\n19\n}\nFigure 28.6: Ticket Locks\nit is that thread’s turn to enter the critical section. Unlock is accomplished\nsimply by incrementing the turn such that the next waiting thread (if\nthere is one) can now enter the critical section.\nNote one important difference with this solution versus our previous\nattempts: it ensures progress for all threads. Once a thread is assigned its\nticket value, it will be scheduled at some point in the future (once those in\nfront of it have passed through the critical section and released the lock).\nIn our previous attempts, no such guarantee existed; a thread spinning\non test-and-set (for example) could spin forever even as other threads\nacquire and release the lock.\n28.12\nSummary: So Much Spinning\nOur simple hardware-based locks are simple (only a few lines of code)\nand they work (you could even prove that if you’d like to, by writing\nsome code), which are two excellent properties of any system or code.\nHowever, in some cases, these solutions can be quite inefﬁcient. Imagine\nyou are running two threads on a single processor. Now imagine that\none thread (thread 0) is in a critical section and thus has a lock held, and\nunfortunately gets interrupted. The second thread (thread 1) now tries to\nacquire the lock, but ﬁnds that it is held. Thus, it begins to spin. And spin.\nThen it spins some more. And ﬁnally, a timer interrupt goes off, thread\n0 is run again, which releases the lock, and ﬁnally (the next time it runs,\nsay), thread 1 won’t have to spin so much and will be able to acquire the\nlock. Thus, any time a thread gets caught spinning in a situation like this,\nit wastes an entire time slice doing nothing but checking a value that isn’t\ngoing to change! The problem gets worse with N threads contending\nfor a lock; N −1 time slices may be wasted in a similar manner, simply\nspinning and waiting for a single thread to release the lock. And thus,\nour next problem:\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2302,
      "extraction_method": "Direct"
    },
    {
      "page_number": 340,
      "chapter": null,
      "content": "304\nLOCKS\nTHE CRUX: HOW TO AVOID SPINNING\nHow can we develop a lock that doesn’t needlessly waste time spin-\nning on the CPU?\nHardware support alone cannot solve the problem. We’ll need OS sup-\nport too! Let’s now ﬁgure out just how that might work.\n28.13\nA Simple Approach: Just Yield, Baby\nHardware support got us pretty far: working locks, and even (as with\nthe case of the ticket lock) fairness in lock acquisition. However, we still\nhave a problem: what to do when a context switch occurs in a critical\nsection, and threads start to spin endlessly, waiting for the interrupt (lock-\nholding) thread to be run again?\nOur ﬁrst try is a simple and friendly approach: when you are going to\nspin, instead give up the CPU to another thread. Or, as Al Davis might\nsay, “just yield, baby!” [D91]. Figure 28.7 presents the approach.\nIn this approach, we assume an operating system primitive yield()\nwhich a thread can call when it wants to give up the CPU and let an-\nother thread run. Because a thread can be in one of three states (running,\nready, or blocked), you can think of this as an OS system call that moves\nthe caller from the running state to the ready state, and thus promotes\nanother thread to running.\nThink about the example with two threads on one CPU; in this case,\nour yield-based approach works quite well. If a thread happens to call\nlock() and ﬁnd a lock held, it will simply yield the CPU, and thus the\nother thread will run and ﬁnish its critical section. In this simple case, the\nyielding approach works well.\nLet us now consider the case where there are many threads (say 100)\ncontending for a lock repeatedly.\nIn this case, if one thread acquires\nthe lock and is preempted before releasing it, the other 99 will each call\n1\nvoid init() {\n2\nflag = 0;\n3\n}\n4\n5\nvoid lock() {\n6\nwhile (TestAndSet(&flag, 1) == 1)\n7\nyield(); // give up the CPU\n8\n}\n9\n10\nvoid unlock() {\n11\nflag = 0;\n12\n}\nFigure 28.7: Lock With Test-and-set And Yield\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1995,
      "extraction_method": "Direct"
    },
    {
      "page_number": 341,
      "chapter": null,
      "content": "LOCKS\n305\nlock(), ﬁnd the lock held, and yield the CPU. Assuming some kind\nof round-robin scheduler, each of the 99 will execute this run-and-yield\npattern before the thread holding the lock gets to run again. While better\nthan our spinning approach (which would waste 99 time slices spinning),\nthis approach is still costly; the cost of a context switch can be substantial,\nand there is thus plenty of waste.\nWorse, we have not tackled the starvation problem at all. A thread\nmay get caught in an endless yield loop while other threads repeatedly\nenter and exit the critical section. We clearly will need an approach that\naddresses this problem directly.\n28.14\nUsing Queues: Sleeping Instead Of Spinning\nThe real problem with our previous approaches is that they leave too\nmuch to chance. The scheduler determines which thread runs next; if\nthe scheduler makes a bad choice, a thread runs that must either spin\nwaiting for the lock (our ﬁrst approach), or yield the CPU immediately\n(our second approach). Either way, there is potential for waste and no\nprevention of starvation.\nThus, we must explicitly exert some control over who gets to acquire\nthe lock next after the current holder releases it. To do this, we will need a\nlittle more OS support, as well as a queue to keep track of which threads\nare waiting to enter the lock.\nFor simplicity, we will use the support provided by Solaris, in terms of\ntwo calls: park() to put a calling thread to sleep, and unpark(threadID)\nto wake a particular thread as designated by threadID. These two rou-\ntines can be used in tandem to build a lock that puts a caller to sleep if it\ntries to acquire a held lock and wakes it when the lock is free. Let’s look at\nthe code in Figure 28.8 to understand one possible use of such primitives.\nWe do a couple of interesting things in this example. First, we combine\nthe old test-and-set idea with an explicit queue of lock waiters to make a\nmore efﬁcient lock. Second, we use a queue to help control who gets the\nlock next and thus avoid starvation.\nYou might notice how the guard is used, basically as a spin-lock around\nthe ﬂag and queue manipulations the lock is using. This approach thus\ndoesn’t avoid spin-waiting entirely; a thread might be interrupted while\nacquiring or releasing the lock, and thus cause other threads to spin-wait\nfor this one to run again. However, the time spent spinning is quite lim-\nited (just a few instructions inside the lock and unlock code, instead of the\nuser-deﬁned critical section), and thus this approach may be reasonable.\nSecond, you might notice that in lock(), when a thread can not ac-\nquire the lock (it is already held), we are careful to add ourselves to a\nqueue (by calling the gettid() call to get the thread ID of the current\nthread), set guard to 0, and yield the CPU. A question for the reader:\nWhat would happen if the release of the guard lock came after the park(),\nand not before? Hint: something bad.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2988,
      "extraction_method": "Direct"
    },
    {
      "page_number": 342,
      "chapter": null,
      "content": "306\nLOCKS\n1\ntypedef struct __lock_t {\n2\nint flag;\n3\nint guard;\n4\nqueue_t *q;\n5\n} lock_t;\n6\n7\nvoid lock_init(lock_t *m) {\n8\nm->flag\n= 0;\n9\nm->guard = 0;\n10\nqueue_init(m->q);\n11\n}\n12\n13\nvoid lock(lock_t *m) {\n14\nwhile (TestAndSet(&m->guard, 1) == 1)\n15\n; //acquire guard lock by spinning\n16\nif (m->flag == 0) {\n17\nm->flag = 1; // lock is acquired\n18\nm->guard = 0;\n19\n} else {\n20\nqueue_add(m->q, gettid());\n21\nm->guard = 0;\n22\npark();\n23\n}\n24\n}\n25\n26\nvoid unlock(lock_t *m) {\n27\nwhile (TestAndSet(&m->guard, 1) == 1)\n28\n; //acquire guard lock by spinning\n29\nif (queue_empty(m->q))\n30\nm->flag = 0; // let go of lock; no one wants it\n31\nelse\n32\nunpark(queue_remove(m->q)); // hold lock (for next thread!)\n33\nm->guard = 0;\n34\n}\nFigure 28.8: Lock With Queues, Test-and-set, Yield, And Wakeup\nYou might also notice the interesting fact that the ﬂag does not get set\nback to 0 when another thread gets woken up. Why is this? Well, it is not\nan error, but rather a necessity! When a thread is woken up, it will be as\nif it is returning from park(); however, it does not hold the guard at that\npoint in the code and thus cannot even try to set the ﬂag to 1. Thus, we\njust pass the lock directly from the thread releasing the lock to the next\nthread acquiring it; ﬂag is not set to 0 in-between.\nFinally, you might notice the perceived race condition in the solution,\njust before the call to park(). With just the wrong timing, a thread will\nbe about to park, assuming that it should sleep until the lock is no longer\nheld. A switch at that time to another thread (say, a thread holding the\nlock) could lead to trouble, for example, if that thread then released the\nlock. The subsequent park by the ﬁrst thread would then sleep forever\n(potentially). This problem is sometimes called the wakeup/waiting race;\nto avoid it, we need to do some extra work.\nSolaris solves this problem by adding a third system call: setpark().\nBy calling this routine, a thread can indicate it is about to park. If it then\nhappens to be interrupted and another thread calls unpark before park is\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2110,
      "extraction_method": "Direct"
    },
    {
      "page_number": 343,
      "chapter": null,
      "content": "LOCKS\n307\nactually called, the subsequent park returns immediately instead of sleep-\ning. The code modiﬁcation, inside of lock(), is quite small:\n1\nqueue_add(m->q, gettid());\n2\nsetpark(); // new code\n3\nm->guard = 0;\nA different solution could pass the guard into the kernel. In that case,\nthe kernel could take precautions to atomically release the lock and de-\nqueue the running thread.\n28.15\nDifferent OS, Different Support\nWe have thus far seen one type of support that an OS can provide in\norder to build a more efﬁcient lock in a thread library. Other OS’s provide\nsimilar support; the details vary.\nFor example, Linux provides something called a futex which is simi-\nlar to the Solaris interface but provides a bit more in-kernel functionality.\nSpeciﬁcally, each futex has associated with it a speciﬁc physical mem-\nory location; associated with each such memory location is an in-kernel\nqueue. Callers can use futex calls (described below) to sleep and wake as\nneed be.\nSpeciﬁcally, two calls are available. The call to futex wait(address,\nexpected) puts the calling thread to sleep, assuming the value at address\nis equal to expected. If it is not equal, the call returns immediately. The\ncall to the routine futex wake(address) wakes one thread that is wait-\ning on the queue. The usage of these in Linux is as found in 28.9.\nThis code snippet from lowlevellock.h in the nptl library (part of\nthe gnu libc library) [L09] is pretty interesting. Basically, it uses a single\ninteger to track both whether the lock is held or not (the high bit of the\ninteger) and the number of waiters on the lock (all the other bits). Thus,\nif the lock is negative, it is held (because the high bit is set and that bit\ndetermines the sign of the integer). The code is also interesting because it\nshows how to optimize for the common case where there is no contention:\nwith only one thread acquiring and releasing a lock, very little work is\ndone (the atomic bit test-and-set to lock and an atomic add to release the\nlock). See if you can puzzle through the rest of this “real-world” lock to\nsee how it works.\n28.16\nTwo-Phase Locks\nOne ﬁnal note: the Linux approach has the ﬂavor of an old approach\nthat has been used on and off for years, going at least as far back to Dahm\nLocks in the early 1960’s [M82], and is now referred to as a two-phase\nlock. A two-phase lock realizes that spinning can be useful, particularly\nif the lock is about to be released. So in the ﬁrst phase, the lock spins for\na while, hoping that it can acquire the lock.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2574,
      "extraction_method": "Direct"
    },
    {
      "page_number": 344,
      "chapter": null,
      "content": "308\nLOCKS\n1\nvoid mutex_lock (int *mutex) {\n2\nint v;\n3\n/* Bit 31 was clear, we got the mutex (this is the fastpath)\n*/\n4\nif (atomic_bit_test_set (mutex, 31) == 0)\n5\nreturn;\n6\natomic_increment (mutex);\n7\nwhile (1) {\n8\nif (atomic_bit_test_set (mutex, 31) == 0) {\n9\natomic_decrement (mutex);\n10\nreturn;\n11\n}\n12\n/* We have to wait now. First make sure the futex value\n13\nwe are monitoring is truly negative (i.e. locked). */\n14\nv = *mutex;\n15\nif (v >= 0)\n16\ncontinue;\n17\nfutex_wait (mutex, v);\n18\n}\n19\n}\n20\n21\nvoid mutex_unlock (int *mutex) {\n22\n/* Adding 0x80000000 to the counter results in 0 if and only if\n23\nthere are not other interested threads */\n24\nif (atomic_add_zero (mutex, 0x80000000))\n25\nreturn;\n26\n27\n/* There are other threads waiting for this mutex,\n28\nwake one of them up.\n*/\n29\nfutex_wake (mutex);\nFigure 28.9: Linux-based Futex Locks\nHowever, if the lock is not acquired during the ﬁrst spin phase, a sec-\nond phase is entered, where the caller is put to sleep, and only woken up\nwhen the lock becomes free later. The Linux lock above is a form of such\na lock, but it only spins once; a generalization of this could spin in a loop\nfor a ﬁxed amount of time before using futex support to sleep.\nTwo-phase locks are yet another instance of a hybrid approach, where\ncombining two good ideas may indeed yield a better one. Of course,\nwhether it does depends strongly on many things, including the hard-\nware environment, number of threads, and other workload details. As\nalways, making a single general-purpose lock, good for all possible use\ncases, is quite a challenge.\n28.17\nSummary\nThe above approach shows how real locks are built these days: some\nhardware support (in the form of a more powerful instruction) plus some\noperating system support (e.g., in the form of park() and unpark()\nprimitives on Solaris, or futex on Linux). Of course, the details differ, and\nthe exact code to perform such locking is usually highly tuned. Check\nout the Solaris or Linux open source code bases if you want to see more\ndetails; they are a fascinating read [L09, S09].\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2119,
      "extraction_method": "Direct"
    },
    {
      "page_number": 345,
      "chapter": null,
      "content": "LOCKS\n309\nReferences\n[D91] “Just Win, Baby: Al Davis and His Raiders”\nGlenn Dickey, Harcourt 1991\nThere is even an undoubtedly bad book about Al Davis and his famous “just win” quote. Or, we suppose,\nthe book is more about Al Davis and the Raiders, and maybe not just the quote. Read the book to ﬁnd\nout?\n[D68] “Cooperating sequential processes”\nEdsger W. Dijkstra, 1968\nAvailable: http://www.cs.utexas.edu/users/EWD/ewd01xx/EWD123.PDF\nOne of the early seminal papers in the area. Discusses how Dijkstra posed the original concurrency\nproblem, and Dekker’s solution.\n[H93] “MIPS R4000 Microprocessor User’s Manual”.\nJoe Heinrich, Prentice-Hall, June 1993\nAvailable: http://cag.csail.mit.edu/raw/\ndocuments/R4400 Uman book Ed2.pdf\n[H91] “Wait-free Synchronization”\nMaurice Herlihy\nACM Transactions on Programming Languages and Systems (TOPLAS)\nVolume 13, Issue 1, January 1991\nA landmark paper introducing a different approach to building concurrent data structures. However,\nbecause of the complexity involved, many of these ideas have been slow to gain acceptance in deployed\nsystems.\n[L81] “Observations on the Development of an Operating System”\nHugh Lauer\nSOSP ’81\nA must-read retrospective about the development of the Pilot OS, an early PC operating system. Fun\nand full of insights.\n[L09] “glibc 2.9 (include Linux pthreads implementation)”\nAvailable: http://ftp.gnu.org/gnu/glibc/\nIn particular, take a look at the nptl subdirectory where you will ﬁnd most of the pthread support in\nLinux today.\n[M82] “The Architecture of the Burroughs B5000\n20 Years Later and Still Ahead of the Times?”\nAlastair J.W. Mayer, 1982\nwww.ajwm.net/amayer/papers/B5000.html\nFrom the paper: “One particularly useful instruction is the RDLK (read-lock). It is an indivisible\noperation which reads from and writes into a memory location.” RDLK is thus an early test-and-set\nprimitive, if not the earliest. Some credit here goes to an engineer named Dave Dahm, who apparently\ninvented a number of these things for the Burroughs systems, including a form of spin locks (called\n“Buzz Locks” as well as a two-phase lock eponymously called “Dahm Locks.”)\n[MS91] “Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors”\nJohn M. Mellor-Crummey and M. L. Scott\nACM TOCS, February 1991\nAn excellent survey on different locking algorithms. However, no OS support is used, just fancy hard-\nware instructions.\n[P81] “Myths About the Mutual Exclusion Problem”\nG.L. Peterson\nInformation Processing Letters. 12(3) 1981, 115–116\nPeterson’s algorithm introduced here.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2594,
      "extraction_method": "Direct"
    },
    {
      "page_number": 346,
      "chapter": null,
      "content": "310\nLOCKS\n[S05] “Guide to porting from Solaris to Linux on x86”\nAjay Sood, April 29, 2005\nAvailable: http://www.ibm.com/developerworks/linux/library/l-solar/\n[S09] “OpenSolaris Thread Library”\nAvailable: http://src.opensolaris.org/source/xref/onnv/onnv-gate/\nusr/src/lib/libc/port/threads/synch.c\nThis is also pretty interesting to look at, though who knows what will happen to it now that Oracle owns\nSun. Thanks to Mike Swift for the pointer to the code.\n[W09] “Load-Link, Store-Conditional”\nWikipedia entry on said topic, as of October 22, 2009\nhttp://en.wikipedia.org/wiki/Load-Link/Store-Conditional\nCan you believe we referenced wikipedia? Pretty shabby. But, we found the information there ﬁrst,\nand it felt wrong not to cite it. Further, they even listed the instructions for the different architec-\ntures: ldl l/stl c and ldq l/stq c (Alpha), lwarx/stwcx (PowerPC), ll/sc (MIPS), and\nldrex/strex (ARM version 6 and above).\n[WG00] “The SPARC Architecture Manual: Version 9”\nDavid L. Weaver and Tom Germond, September 2000\nSPARC International, San Jose, California\nAvailable: http://www.sparc.org/standards/SPARCV9.pdf\nAlso see: http://developers.sun.com/solaris/articles/atomic sparc/ for some\nmore details on Sparc atomic operations.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1290,
      "extraction_method": "Direct"
    },
    {
      "page_number": 347,
      "chapter": null,
      "content": "29\nLock-based Concurrent Data Structures\nBefore moving beyond locks, we’ll ﬁrst describe how to use locks in some\ncommon data structures. Adding locks to a data structure to make it us-\nable by threads makes the structure thread safe. Of course, exactly how\nsuch locks are added determines both the correctness and performance of\nthe data structure. And thus, our challenge:\nCRUX: HOW TO ADD LOCKS TO DATA STRUCTURES\nWhen given a particular data structure, how should we add locks to\nit, in order to make it work correctly? Further, how do we add locks such\nthat the data structure yields high performance, enabling many threads\nto access the structure at once, i.e., concurrently?\nOf course, we will be hard pressed to cover all data structures or all\nmethods for adding concurrency, as this is a topic that has been studied\nfor years, with (literally) thousands of research papers published about\nit. Thus, we hope to provide a sufﬁcient introduction to the type of think-\ning required, and refer you to some good sources of material for further\ninquiry on your own. We found Moir and Shavit’s survey to be a great\nsource of information [MS04].\n29.1\nConcurrent Counters\nOne of the simplest data structures is a counter. It is a structure that\nis commonly used and has a simple interface. We deﬁne a simple non-\nconcurrent counter in Figure 29.1.\nSimple But Not Scalable\nAs you can see, the non-synchronized counter is a trivial data structure,\nrequiring a tiny amount of code to implement. We now have our next\nchallenge: how can we make this code thread safe? Figure 29.2 shows\nhow we do so.\n311\n",
      "content_length": 1599,
      "extraction_method": "Direct"
    },
    {
      "page_number": 348,
      "chapter": null,
      "content": "312\nLOCK-BASED CONCURRENT DATA STRUCTURES\n1\ntypedef struct __counter_t {\n2\nint value;\n3\n} counter_t;\n4\n5\nvoid init(counter_t *c) {\n6\nc->value = 0;\n7\n}\n8\n9\nvoid increment(counter_t *c) {\n10\nc->value++;\n11\n}\n12\n13\nvoid decrement(counter_t *c) {\n14\nc->value--;\n15\n}\n16\n17\nint get(counter_t *c) {\n18\nreturn c->value;\n19\n}\nFigure 29.1: A Counter Without Locks\n1\ntypedef struct __counter_t {\n2\nint\nvalue;\n3\npthread_lock_t lock;\n4\n} counter_t;\n5\n6\nvoid init(counter_t *c) {\n7\nc->value = 0;\n8\nPthread_mutex_init(&c->lock, NULL);\n9\n}\n10\n11\nvoid increment(counter_t *c) {\n12\nPthread_mutex_lock(&c->lock);\n13\nc->value++;\n14\nPthread_mutex_unlock(&c->lock);\n15\n}\n16\n17\nvoid decrement(counter_t *c) {\n18\nPthread_mutex_lock(&c->lock);\n19\nc->value--;\n20\nPthread_mutex_unlock(&c->lock);\n21\n}\n22\n23\nint get(counter_t *c) {\n24\nPthread_mutex_lock(&c->lock);\n25\nint rc = c->value;\n26\nPthread_mutex_unlock(&c->lock);\n27\nreturn rc;\n28\n}\nFigure 29.2: A Counter With Locks\nThis concurrent counter is simple and works correctly. In fact, it fol-\nlows a design pattern common to the simplest and most basic concurrent\ndata structures: it simply adds a single lock, which is acquired when call-\ning a routine that manipulates the data structure, and is released when\nreturning from the call. In this manner, it is similar to a data structure\nbuilt with monitors [BH73], where locks are acquired and released auto-\nmatically as you call and return from object methods.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1487,
      "extraction_method": "Direct"
    },
    {
      "page_number": 349,
      "chapter": null,
      "content": "LOCK-BASED CONCURRENT DATA STRUCTURES\n313\n1\n2\n3\n4\n0\n5\n10\n15\nThreads\nTime (seconds)\nPrecise\nSloppy\nFigure 29.3: Performance of Traditional vs. Sloppy Counters\nAt this point, you have a working concurrent data structure. The prob-\nlem you might have is performance. If your data structure is too slow,\nyou’ll have to do more than just add a single lock; such optimizations, if\nneeded, are thus the topic of the rest of the chapter. Note that if the data\nstructure is not too slow, you are done! No need to do something fancy if\nsomething simple will work.\nTo understand the performance costs of the simple approach, we run a\nbenchmark in which each thread updates a single shared counter a ﬁxed\nnumber of times; we then vary the number of threads. Figure 29.3 shows\nthe total time taken, with one to four threads active; each thread updates\nthe counter one million times. This experiment was run upon an iMac\nwith four Intel 2.7 GHz i5 CPUs; with more CPUs active, we hope to get\nmore total work done per unit time.\nFrom the top line in the ﬁgure (labeled precise), you can see that the\nperformance of the synchronized counter scales poorly. Whereas a single\nthread can complete the million counter updates in a tiny amount of time\n(roughly 0.03 seconds), having two threads each update the counter one\nmillion times concurrently leads to a massive slowdown (taking over 5\nseconds!). It only gets worse with more threads.\nIdeally, you’d like to see the threads complete just as quickly on mul-\ntiple processors as the single thread does on one. Achieving this end is\ncalled perfect scaling; even though more work is done, it is done in par-\nallel, and hence the time taken to complete the task is not increased.\nScalable Counting\nAmazingly, researchers have studied how to build more scalable coun-\nters for years [MS04]. Even more amazing is the fact that scalable coun-\nters matter, as recent work in operating system performance analysis has\nshown [B+10]; without scalable counting, some workloads running on\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2051,
      "extraction_method": "Direct"
    },
    {
      "page_number": 350,
      "chapter": null,
      "content": "314\nLOCK-BASED CONCURRENT DATA STRUCTURES\nTime\nL1\nL2\nL3\nL4\nG\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n1\n0\n2\n1\n0\n2\n1\n0\n3\n2\n0\n3\n1\n0\n4\n3\n0\n3\n2\n0\n5\n4\n1\n3\n3\n0\n6\n5 →0\n1\n3\n4\n5 (from L1)\n7\n0\n2\n4\n5 →0\n10 (from L4)\nTable 29.1: Tracing the Sloppy Counters\nLinux suffer from serious scalability problems on multicore machines.\nThough many techniques have been developed to attack this problem,\nwe’ll now describe one particular approach. The idea, introduced in re-\ncent research [B+10], is known as a sloppy counter.\nThe sloppy counter works by representing a single logical counter via\nnumerous local physical counters, one per CPU core, as well as a single\nglobal counter. Speciﬁcally, on a machine with four CPUs, there are four\nlocal counters and one global one. In addition to these counters, there are\nalso locks: one for each local counter, and one for the global counter.\nThe basic idea of sloppy counting is as follows. When a thread running\non a given core wishes to increment the counter, it increments its local\ncounter; access to this local counter is synchronized via the corresponding\nlocal lock. Because each CPU has its own local counter, threads across\nCPUs can update local counters without contention, and thus counter\nupdates are scalable.\nHowever, to keep the global counter up to date (in case a thread wishes\nto read its value), the local values are periodically transferred to the global\ncounter, by acquiring the global lock and incrementing it by the local\ncounter’s value; the local counter is then reset to zero.\nHow often this local-to-global transfer occurs is determined by a thresh-\nold, which we call S here (for sloppiness). The smaller S is, the more the\ncounter behaves like the non-scalable counter above; the bigger S is, the\nmore scalable the counter, but the further off the global value might be\nfrom the actual count. One could simply acquire all the local locks and\nthe global lock (in a speciﬁed order, to avoid deadlock) to get an exact\nvalue, but that is not scalable.\nTo make this clear, let’s look at an example (Table 29.1). In this exam-\nple, the threshold S is set to 5, and there are threads on each of four CPUs\nupdating their local counters L1 ... L4. The global counter value (G) is\nalso shown in the trace, with time increasing downward. At each time\nstep, a local counter may be incremented; if the local value reaches the\nthreshold S, the local value is transferred to the global counter and the\nlocal counter is reset.\nThe lower line in Figure 29.3 (labeled sloppy) shows the performance of\nsloppy counters with a threshold S of 1024. Performance is excellent; the\ntime taken to update the counter four million times on four processors is\nhardly higher than the time taken to update it one million times on one\nprocessor.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2797,
      "extraction_method": "Direct"
    },
    {
      "page_number": 351,
      "chapter": null,
      "content": "LOCK-BASED CONCURRENT DATA STRUCTURES\n315\n1\ntypedef struct __counter_t {\n2\nint\nglobal;\n// global count\n3\npthread_mutex_t glock;\n// global lock\n4\nint\nlocal[NUMCPUS];\n// local count (per cpu)\n5\npthread_mutex_t llock[NUMCPUS];\n// ... and locks\n6\nint\nthreshold;\n// update frequency\n7\n} counter_t;\n8\n9\n// init: record threshold, init locks, init values\n10\n//\nof all local counts and global count\n11\nvoid init(counter_t *c, int threshold) {\n12\nc->threshold = threshold;\n13\n14\nc->global = 0;\n15\npthread_mutex_init(&c->glock, NULL);\n16\n17\nint i;\n18\nfor (i = 0; i < NUMCPUS; i++) {\n19\nc->local[i] = 0;\n20\npthread_mutex_init(&c->llock[i], NULL);\n21\n}\n22\n}\n23\n24\n// update: usually, just grab local lock and update local amount\n25\n//\nonce local count has risen by ’threshold’, grab global\n26\n//\nlock and transfer local values to it\n27\nvoid update(counter_t *c, int threadID, int amt) {\n28\npthread_mutex_lock(&c->llock[threadID]);\n29\nc->local[threadID] += amt;\n// assumes amt > 0\n30\nif (c->local[threadID] >= c->threshold) { // transfer to global\n31\npthread_mutex_lock(&c->glock);\n32\nc->global += c->local[threadID];\n33\npthread_mutex_unlock(&c->glock);\n34\nc->local[threadID] = 0;\n35\n}\n36\npthread_mutex_unlock(&c->llock[threadID]);\n37\n}\n38\n39\n// get: just return global amount (which may not be perfect)\n40\nint get(counter_t *c) {\n41\npthread_mutex_lock(&c->glock);\n42\nint val = c->global;\n43\npthread_mutex_unlock(&c->glock);\n44\nreturn val; // only approximate!\n45\n}\nFigure 29.4: Sloppy Counter Implementation\nFigure 29.5 shows the importance of the threshold value S, with four\nthreads each incrementing the counter 1 million times on four CPUs. If S\nis low, performance is poor (but the global count is always quite accurate);\nif S is high, performance is excellent, but the global count lags (by the\nnumber of CPUs multiplied by S). This accuracy/performance trade-off\nis what sloppy counters enables.\nA rough version of such a sloppy counter is found in Figure 29.4. Read\nit, or better yet, run it yourself in some experiments to better understand\nhow it works.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2093,
      "extraction_method": "Direct"
    },
    {
      "page_number": 352,
      "chapter": null,
      "content": "316\nLOCK-BASED CONCURRENT DATA STRUCTURES\n1\n2\n4\n8\n16\n32\n64 128 256\n1024\n512\n0\n5\n10\n15\nSloppiness\nTime (seconds)\nFigure 29.5: Scaling Sloppy Counters\n29.2\nConcurrent Linked Lists\nWe next examine a more complicated structure, the linked list. Let’s\nstart with a basic approach once again. For simplicity, we’ll omit some of\nthe obvious routines that such a list would have and just focus on concur-\nrent insert; we’ll leave it to the reader to think about lookup, delete, and\nso forth. Figure 29.6 shows the code for this rudimentary data structure.\nAs you can see in the code, the code simply acquires a lock in the insert\nroutine upon entry, and releases it upon exit. One small tricky issue arises\nif malloc() happens to fail (a rare case); in this case, the code must also\nrelease the lock before failing the insert.\nThis kind of exceptional control ﬂow has been shown to be quite error\nprone; a recent study of Linux kernel patches found that a huge fraction of\nbugs (nearly 40%) are found on such rarely-taken code paths (indeed, this\nobservation sparked some of our own research, in which we removed all\nmemory-failing paths from a Linux ﬁle system, resulting in a more robust\nsystem [S+11]).\nThus, a challenge: can we rewrite the insert and lookup routines to re-\nmain correct under concurrent insert but avoid the case where the failure\npath also requires us to add the call to unlock?\nThe answer, in this case, is yes. Speciﬁcally, we can rearrange the code\na bit so that the lock and release only surround the actual critical section\nin the insert code, and that a common exit path is used in the lookup code.\nThe former works because part of the lookup actually need not be locked;\nassuming that malloc() itself is thread-safe, each thread can call into it\nwithout worry of race conditions or other concurrency bugs. Only when\nupdating the shared list does a lock need to be held. See Figure 29.7 for\nthe details of these modiﬁcations.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1993,
      "extraction_method": "Direct"
    },
    {
      "page_number": 353,
      "chapter": null,
      "content": "LOCK-BASED CONCURRENT DATA STRUCTURES\n317\n1\n// basic node structure\n2\ntypedef struct __node_t {\n3\nint\nkey;\n4\nstruct __node_t\n*next;\n5\n} node_t;\n6\n7\n// basic list structure (one used per list)\n8\ntypedef struct __list_t {\n9\nnode_t\n*head;\n10\npthread_mutex_t\nlock;\n11\n} list_t;\n12\n13\nvoid List_Init(list_t *L) {\n14\nL->head = NULL;\n15\npthread_mutex_init(&L->lock, NULL);\n16\n}\n17\n18\nint List_Insert(list_t *L, int key) {\n19\npthread_mutex_lock(&L->lock);\n20\nnode_t *new = malloc(sizeof(node_t));\n21\nif (new == NULL) {\n22\nperror(\"malloc\");\n23\npthread_mutex_unlock(&L->lock);\n24\nreturn -1; // fail\n25\n}\n26\nnew->key\n= key;\n27\nnew->next = L->head;\n28\nL->head\n= new;\n29\npthread_mutex_unlock(&L->lock);\n30\nreturn 0; // success\n31\n}\n32\n33\nint List_Lookup(list_t *L, int key) {\n34\npthread_mutex_lock(&L->lock);\n35\nnode_t *curr = L->head;\n36\nwhile (curr) {\n37\nif (curr->key == key) {\n38\npthread_mutex_unlock(&L->lock);\n39\nreturn 0; // success\n40\n}\n41\ncurr = curr->next;\n42\n}\n43\npthread_mutex_unlock(&L->lock);\n44\nreturn -1; // failure\n45\n}\nFigure 29.6: Concurrent Linked List\nAs for the lookup routine, it is a simple code transformation to jump\nout of the main search loop to a single return path. Doing so again re-\nduces the number of lock acquire/release points in the code, and thus\ndecreases the chances of accidentally introducing bugs (such as forget-\nting to unlock before returning) into the code.\nScaling Linked Lists\nThough we again have a basic concurrent linked list, once again we\nare in a situation where it does not scale particularly well. One technique\nthat researchers have explored to enable more concurrency within a list is\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 1672,
      "extraction_method": "Direct"
    },
    {
      "page_number": 354,
      "chapter": null,
      "content": "318\nLOCK-BASED CONCURRENT DATA STRUCTURES\n1\nvoid List_Init(list_t *L) {\n2\nL->head = NULL;\n3\npthread_mutex_init(&L->lock, NULL);\n4\n}\n5\n6\nvoid List_Insert(list_t *L, int key) {\n7\n// synchronization not needed\n8\nnode_t *new = malloc(sizeof(node_t));\n9\nif (new == NULL) {\n10\nperror(\"malloc\");\n11\nreturn;\n12\n}\n13\nnew->key = key;\n14\n15\n// just lock critical section\n16\npthread_mutex_lock(&L->lock);\n17\nnew->next = L->head;\n18\nL->head\n= new;\n19\npthread_mutex_unlock(&L->lock);\n20\n}\n21\n22\nint List_Lookup(list_t *L, int key) {\n23\nint rv = -1;\n24\npthread_mutex_lock(&L->lock);\n25\nnode_t *curr = L->head;\n26\nwhile (curr) {\n27\nif (curr->key == key) {\n28\nrv = 0;\n29\nbreak;\n30\n}\n31\ncurr = curr->next;\n32\n}\n33\npthread_mutex_unlock(&L->lock);\n34\nreturn rv; // now both success and failure\n35\n}\nFigure 29.7: Concurrent Linked List: Rewritten\nsomething called hand-over-hand locking (a.k.a. lock coupling) [MS04].\nThe idea is pretty simple. Instead of having a single lock for the entire\nlist, you instead add a lock per node of the list. When traversing the\nlist, the code ﬁrst grabs the next node’s lock and then releases the current\nnode’s lock (which inspires the name hand-over-hand).\nConceptually, a hand-over-hand linked list makes some sense; it en-\nables a high degree of concurrency in list operations. However, in prac-\ntice, it is hard to make such a structure faster than the simple single lock\napproach, as the overheads of acquiring and releasing locks for each node\nof a list traversal is prohibitive. Even with very large lists, and a large\nnumber of threads, the concurrency enabled by allowing multiple on-\ngoing traversals is unlikely to be faster than simply grabbing a single\nlock, performing an operation, and releasing it. Perhaps some kind of hy-\nbrid (where you grab a new lock every so many nodes) would be worth\ninvestigating.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1885,
      "extraction_method": "Direct"
    },
    {
      "page_number": 355,
      "chapter": null,
      "content": "LOCK-BASED CONCURRENT DATA STRUCTURES\n319\nTIP: MORE CONCURRENCY ISN’T NECESSARILY FASTER\nIf the scheme you design adds a lot of overhead (for example, by acquir-\ning and releasing locks frequently, instead of once), the fact that it is more\nconcurrent may not be important. Simple schemes tend to work well,\nespecially if they use costly routines rarely. Adding more locks and com-\nplexity can be your downfall. All of that said, there is one way to really\nknow: build both alternatives (simple but less concurrent, and complex\nbut more concurrent) and measure how they do. In the end, you can’t\ncheat on performance; your idea is either faster, or it isn’t.\nTIP: BE WARY OF LOCKS AND CONTROL FLOW\nA general design tip, which is useful in concurrent code as well as\nelsewhere, is to be wary of control ﬂow changes that lead to function re-\nturns, exits, or other similar error conditions that halt the execution of\na function. Because many functions will begin by acquiring a lock, al-\nlocating some memory, or doing other similar stateful operations, when\nerrors arise, the code has to undo all of the state before returning, which\nis error-prone. Thus, it is best to structure code to minimize this pattern.\n29.3\nConcurrent Queues\nAs you know by now, there is always a standard method to make a\nconcurrent data structure: add a big lock. For a queue, we’ll skip that\napproach, assuming you can ﬁgure it out.\nInstead, we’ll take a look at a slightly more concurrent queue designed\nby Michael and Scott [MS98]. The data structures and code used for this\nqueue are found in Figure 29.8 on the following page.\nIf you study this code carefully, you’ll notice that there are two locks,\none for the head of the queue, and one for the tail. The goal of these two\nlocks is to enable concurrency of enqueue and dequeue operations. In\nthe common case, the enqueue routine will only access the tail lock, and\ndequeue only the head lock.\nOne trick used by the Michael and Scott is to add a dummy node (allo-\ncated in the queue initialization code); this dummy enables the separation\nof head and tail operations. Study the code, or better yet, type it in, run\nit, and measure it, to understand how it works deeply.\nQueues are commonly used in multi-threaded applications. However,\nthe type of queue used here (with just locks) often does not completely\nmeet the needs of such programs.\nA more fully developed bounded\nqueue, that enables a thread to wait if the queue is either empty or overly\nfull, is the subject of our intense study in the next chapter on condition\nvariables. Watch for it!\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2622,
      "extraction_method": "Direct"
    },
    {
      "page_number": 356,
      "chapter": null,
      "content": "320\nLOCK-BASED CONCURRENT DATA STRUCTURES\n1\ntypedef struct __node_t {\n2\nint\nvalue;\n3\nstruct __node_t\n*next;\n4\n} node_t;\n5\n6\ntypedef struct __queue_t {\n7\nnode_t\n*head;\n8\nnode_t\n*tail;\n9\npthread_mutex_t\nheadLock;\n10\npthread_mutex_t\ntailLock;\n11\n} queue_t;\n12\n13\nvoid Queue_Init(queue_t *q) {\n14\nnode_t *tmp = malloc(sizeof(node_t));\n15\ntmp->next = NULL;\n16\nq->head = q->tail = tmp;\n17\npthread_mutex_init(&q->headLock, NULL);\n18\npthread_mutex_init(&q->tailLock, NULL);\n19\n}\n20\n21\nvoid Queue_Enqueue(queue_t *q, int value) {\n22\nnode_t *tmp = malloc(sizeof(node_t));\n23\nassert(tmp != NULL);\n24\ntmp->value = value;\n25\ntmp->next\n= NULL;\n26\n27\npthread_mutex_lock(&q->tailLock);\n28\nq->tail->next = tmp;\n29\nq->tail = tmp;\n30\npthread_mutex_unlock(&q->tailLock);\n31\n}\n32\n33\nint Queue_Dequeue(queue_t *q, int *value) {\n34\npthread_mutex_lock(&q->headLock);\n35\nnode_t *tmp = q->head;\n36\nnode_t *newHead = tmp->next;\n37\nif (newHead == NULL) {\n38\npthread_mutex_unlock(&q->headLock);\n39\nreturn -1; // queue was empty\n40\n}\n41\n*value = newHead->value;\n42\nq->head = newHead;\n43\npthread_mutex_unlock(&q->headLock);\n44\nfree(tmp);\n45\nreturn 0;\n46\n}\nFigure 29.8: Michael and Scott Concurrent Queue\n29.4\nConcurrent Hash Table\nWe end our discussion with a simple and widely applicable concurrent\ndata structure, the hash table. We’ll focus on a simple hash table that does\nnot resize; a little more work is required to handle resizing, which we\nleave as an exercise for the reader (sorry!).\nThis concurrent hash table is straightforward, is built using the con-\ncurrent lists we developed earlier, and works incredibly well. The reason\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1656,
      "extraction_method": "Direct"
    },
    {
      "page_number": 357,
      "chapter": null,
      "content": "LOCK-BASED CONCURRENT DATA STRUCTURES\n321\n1\n#define BUCKETS (101)\n2\n3\ntypedef struct __hash_t {\n4\nlist_t lists[BUCKETS];\n5\n} hash_t;\n6\n7\nvoid Hash_Init(hash_t *H) {\n8\nint i;\n9\nfor (i = 0; i < BUCKETS; i++) {\n10\nList_Init(&H->lists[i]);\n11\n}\n12\n}\n13\n14\nint Hash_Insert(hash_t *H, int key) {\n15\nint bucket = key % BUCKETS;\n16\nreturn List_Insert(&H->lists[bucket], key);\n17\n}\n18\n19\nint Hash_Lookup(hash_t *H, int key) {\n20\nint bucket = key % BUCKETS;\n21\nreturn List_Lookup(&H->lists[bucket], key);\n22\n}\nFigure 29.9: A Concurrent Hash Table\nfor its good performance is that instead of having a single lock for the en-\ntire structure, it uses a lock per hash bucket (each of which is represented\nby a list). Doing so enables many concurrent operations to take place.\nFigure 29.10 shows the performance of the hash table under concur-\nrent updates (from 10,000 to 50,000 concurrent updates from each of four\nthreads, on the same iMac with four CPUs). Also shown, for the sake\nof comparison, is the performance of a linked list (with a single lock).\nAs you can see from the graph, this simple concurrent hash table scales\nmagniﬁcently; the linked list, in contrast, does not.\n0\n10\n20\n30\n40\n0\n5\n10\n15\nInserts (Thousands)\nTime (seconds)\nSimple Concurrent List\nConcurrent Hash Table\nFigure 29.10: Scaling Hash Tables\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 1348,
      "extraction_method": "Direct"
    },
    {
      "page_number": 358,
      "chapter": null,
      "content": "322\nLOCK-BASED CONCURRENT DATA STRUCTURES\nTIP: AVOID PREMATURE OPTIMIZATION (KNUTH’S LAW)\nWhen building a concurrent data structure, start with the most basic ap-\nproach, which is to add a single big lock to provide synchronized access.\nBy doing so, you are likely to build a correct lock; if you then ﬁnd that it\nsuffers from performance problems, you can reﬁne it, thus only making\nit fast if need be. As Knuth famously stated, “Premature optimization is\nthe root of all evil.”\nMany operating systems added a single lock when transitioning to multi-\nprocessors, including Sun OS and Linux. In the latter, it even had a name,\nthe big kernel lock (BKL), and was the source of performance problems\nfor many years until it was ﬁnally removed in 2011. In SunOS (which\nwas a BSD variant), the notion of removing the single lock protecting\nthe kernel was so painful that the Sun engineers decided on a different\nroute: building the entirely new Solaris operating system, which was\nmulti-threaded from day one. Read the Linux and Solaris kernel books\nfor more information [BC05, MM00].\n29.5\nSummary\nWe have introduced a sampling of concurrent data structures, from\ncounters, to lists and queues, and ﬁnally to the ubiquitous and heavily-\nused hash table. We have learned a few important lessons along the way:\nto be careful with acquisition and release of locks around control ﬂow\nchanges; that enabling more concurrency does not necessarily increase\nperformance; that performance problems should only be remedied once\nthey exist. This last point, of avoiding premature optimization, is cen-\ntral to any performance-minded developer; there is no value in making\nsomething faster if doing so will not improve the overall performance of\nthe application.\nOf course, we have just scratched the surface of high performance\nstructures. See Moir and Shavit’s excellent survey for more information,\nas well as links to other sources [MS04]. In particular, you might be inter-\nested in other structures (such as B-trees); for this knowledge, a database\nclass is your best bet. You also might be interested in techniques that don’t\nuse traditional locks at all; such non-blocking data structures are some-\nthing we’ll get a taste of in the chapter on common concurrency bugs,\nbut frankly this topic is an entire area of knowledge requiring more study\nthan is possible in this humble book. Find out more on your own if you\nare interested (as always!).\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2482,
      "extraction_method": "Direct"
    },
    {
      "page_number": 359,
      "chapter": null,
      "content": "LOCK-BASED CONCURRENT DATA STRUCTURES\n323\nReferences\n[B+10] “An Analysis of Linux Scalability to Many Cores”\nSilas Boyd-Wickizer, Austin T. Clements, Yandong Mao, Aleksey Pesterev, M. Frans Kaashoek,\nRobert Morris, Nickolai Zeldovich\nOSDI ’10, Vancouver, Canada, October 2010\nA great study of how Linux performs on multicore machines, as well as some simple solutions.\n[BH73] “Operating System Principles”\nPer Brinch Hansen, Prentice-Hall, 1973\nAvailable: http://portal.acm.org/citation.cfm?id=540365\nOne of the ﬁrst books on operating systems; certainly ahead of its time. Introduced monitors as a\nconcurrency primitive.\n[BC05] “Understanding the Linux Kernel (Third Edition)”\nDaniel P. Bovet and Marco Cesati\nO’Reilly Media, November 2005\nThe classic book on the Linux kernel. You should read it.\n[L+13] “A Study of Linux File System Evolution”\nLanyue Lu, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau, Shan Lu\nFAST ’13, San Jose, CA, February 2013\nOur paper that studies every patch to Linux ﬁle systems over nearly a decade. Lots of fun ﬁndings in\nthere; read it to see! The work was painful to do though; the poor graduate student, Lanyue Lu, had to\nlook through every single patch by hand in order to understand what they did.\n[MS98] “Nonblocking Algorithms and Preemption-safe Locking on Multiprogrammed Shared-\nmemory Multiprocessors”\nM. Michael and M. Scott\nJournal of Parallel and Distributed Computing, Vol. 51, No. 1, 1998\nProfessor Scott and his students have been at the forefront of concurrent algorithms and data structures\nfor many years; check out his web page, numerous papers, or books to ﬁnd out more.\n[MS04] “Concurrent Data Structures”\nMark Moir and Nir Shavit\nIn Handbook of Data Structures and Applications\n(Editors D. Metha and S.Sahni)\nChapman and Hall/CRC Press, 2004\nAvailable: www.cs.tau.ac.il/˜shanir/concurrent-data-structures.pdf\nA short but relatively comprehensive reference on concurrent data structures. Though it is missing\nsome of the latest works in the area (due to its age), it remains an incredibly useful reference.\n[MM00] “Solaris Internals: Core Kernel Architecture”\nJim Mauro and Richard McDougall\nPrentice Hall, October 2000\nThe Solaris book. You should also read this, if you want to learn in great detail about something other\nthan Linux.\n[S+11] “Making the Common Case the Only Case with Anticipatory Memory Allocation”\nSwaminathan Sundararaman, Yupu Zhang, Sriram Subramanian,\nAndrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau\nFAST ’11, San Jose, CA, February 2011\nOur work on removing possibly-failing calls to malloc from kernel code paths. The idea is to allocate all\npotentially needed memory before doing any of the work, thus avoiding failure deep down in the storage\nstack.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2778,
      "extraction_method": "Direct"
    },
    {
      "page_number": 360,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 361,
      "chapter": null,
      "content": "30\nCondition Variables\nThus far we have developed the notion of a lock and seen how one can be\nproperly built with the right combination of hardware and OS support.\nUnfortunately, locks are not the only primitives that are needed to build\nconcurrent programs.\nIn particular, there are many cases where a thread wishes to check\nwhether a condition is true before continuing its execution. For example,\na parent thread might wish to check whether a child thread has completed\nbefore continuing (this is often called a join()); how should such a wait\nbe implemented? Let’s look at Figure 30.1.\n1\nvoid *child(void *arg) {\n2\nprintf(\"child\\n\");\n3\n// XXX how to indicate we are done?\n4\nreturn NULL;\n5\n}\n6\n7\nint main(int argc, char *argv[]) {\n8\nprintf(\"parent: begin\\n\");\n9\npthread_t c;\n10\nPthread_create(&c, NULL, child, NULL); // create child\n11\n// XXX how to wait for child?\n12\nprintf(\"parent: end\\n\");\n13\nreturn 0;\n14\n}\nFigure 30.1: A Parent Waiting For Its Child\nWhat we would like to see here is the following output:\nparent: begin\nchild\nparent: end\nWe could try using a shared variable, as you see in Figure 30.2. This\nsolution will generally work, but it is hugely inefﬁcient as the parent spins\nand wastes CPU time. What we would like here instead is some way to\nput the parent to sleep until the condition we are waiting for (e.g., the\nchild is done executing) comes true.\n325\n",
      "content_length": 1379,
      "extraction_method": "Direct"
    },
    {
      "page_number": 362,
      "chapter": null,
      "content": "326\nCONDITION VARIABLES\n1\nvolatile int done = 0;\n2\n3\nvoid *child(void *arg) {\n4\nprintf(\"child\\n\");\n5\ndone = 1;\n6\nreturn NULL;\n7\n}\n8\n9\nint main(int argc, char *argv[]) {\n10\nprintf(\"parent: begin\\n\");\n11\npthread_t c;\n12\nPthread_create(&c, NULL, child, NULL); // create child\n13\nwhile (done == 0)\n14\n; // spin\n15\nprintf(\"parent: end\\n\");\n16\nreturn 0;\n17\n}\nFigure 30.2: Parent Waiting For Child: Spin-based Approach\nTHE CRUX: HOW TO WAIT FOR A CONDITION\nIn multi-threaded programs, it is often useful for a thread to wait for\nsome condition to become true before proceeding. The simple approach,\nof just spinning until the condition becomes true, is grossly inefﬁcient\nand wastes CPU cycles, and in some cases, can be incorrect. Thus, how\nshould a thread wait for a condition?\n30.1\nDeﬁnition and Routines\nTo wait for a condition to become true, a thread can make use of what\nis known as a condition variable. A condition variable is an explicit\nqueue that threads can put themselves on when some state of execution\n(i.e., some condition) is not as desired (by waiting on the condition);\nsome other thread, when it changes said state, can then wake one (or\nmore) of those waiting threads and thus allow them to continue (by sig-\nnaling on the condition). The idea goes back to Dijkstra’s use of “private\nsemaphores” [D68]; a similar idea was later named a “condition variable”\nby Hoare in his work on monitors [H74].\nTo declare such a condition variable, one simply writes something\nlike this: pthread cond t c;, which declares c as a condition variable\n(note: proper initialization is also required). A condition variable has two\noperations associated with it: wait() and signal(). The wait() call\nis executed when a thread wishes to put itself to sleep; the signal() call\nis executed when a thread has changed something in the program and\nthus wants to wake a sleeping thread waiting on this condition. Speciﬁ-\ncally, the POSIX calls look like this:\npthread_cond_wait(pthread_cond_t *c, pthread_mutex_t *m);\npthread_cond_signal(pthread_cond_t *c);\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2092,
      "extraction_method": "Direct"
    },
    {
      "page_number": 363,
      "chapter": null,
      "content": "CONDITION VARIABLES\n327\n1\nint done\n= 0;\n2\npthread_mutex_t m = PTHREAD_MUTEX_INITIALIZER;\n3\npthread_cond_t c\n= PTHREAD_COND_INITIALIZER;\n4\n5\nvoid thr_exit() {\n6\nPthread_mutex_lock(&m);\n7\ndone = 1;\n8\nPthread_cond_signal(&c);\n9\nPthread_mutex_unlock(&m);\n10\n}\n11\n12\nvoid *child(void *arg) {\n13\nprintf(\"child\\n\");\n14\nthr_exit();\n15\nreturn NULL;\n16\n}\n17\n18\nvoid thr_join() {\n19\nPthread_mutex_lock(&m);\n20\nwhile (done == 0)\n21\nPthread_cond_wait(&c, &m);\n22\nPthread_mutex_unlock(&m);\n23\n}\n24\n25\nint main(int argc, char *argv[]) {\n26\nprintf(\"parent: begin\\n\");\n27\npthread_t p;\n28\nPthread_create(&p, NULL, child, NULL);\n29\nthr_join();\n30\nprintf(\"parent: end\\n\");\n31\nreturn 0;\n32\n}\nFigure 30.3: Parent Waiting For Child: Use A Condition Variable\nWe will often refer to these as wait() and signal() for simplicity.\nOne thing you might notice about the wait() call is that it also takes a\nmutex as a parameter; it assumes that this mutex is locked when wait()\nis called. The responsibility of wait() is to release the lock and put the\ncalling thread to sleep (atomically); when the thread wakes up (after some\nother thread has signaled it), it must re-acquire the lock before returning\nto the caller. This complexity stems from the desire to prevent certain\nrace conditions from occurring when a thread is trying to put itself to\nsleep. Let’s take a look at the solution to the join problem (Figure 30.3) to\nunderstand this better.\nThere are two cases to consider. In the ﬁrst, the parent creates the child\nthread but continues running itself (assume we have only a single pro-\ncessor) and thus immediately calls into thr join() to wait for the child\nthread to complete. In this case, it will acquire the lock, check if the child\nis done (it is not), and put itself to sleep by calling wait() (hence releas-\ning the lock). The child will eventually run, print the message “child”,\nand call thr exit() to wake the parent thread; this code just grabs the\nlock, sets the state variable done, and signals the parent thus waking it.\nFinally, the parent will run (returning from wait() with the lock held),\nunlock the lock, and print the ﬁnal message “parent: end”.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2188,
      "extraction_method": "Direct"
    },
    {
      "page_number": 364,
      "chapter": null,
      "content": "328\nCONDITION VARIABLES\nIn the second case, the child runs immediately upon creation, sets\ndone to 1, calls signal to wake a sleeping thread (but there is none, so\nit just returns), and is done. The parent then runs, calls thr join(), sees\nthat done is 1, and thus does not wait and returns.\nOne last note: you might observe the parent uses a while loop instead\nof just an if statement when deciding whether to wait on the condition.\nWhile this does not seem strictly necessary per the logic of the program,\nit is always a good idea, as we will see below.\nTo make sure you understand the importance of each piece of the\nthr exit() and thr join() code, let’s try a few alternate implemen-\ntations. First, you might be wondering if we need the state variable done.\nWhat if the code looked like the example below? Would this work?\n1\nvoid thr_exit() {\n2\nPthread_mutex_lock(&m);\n3\nPthread_cond_signal(&c);\n4\nPthread_mutex_unlock(&m);\n5\n}\n6\n7\nvoid thr_join() {\n8\nPthread_mutex_lock(&m);\n9\nPthread_cond_wait(&c, &m);\n10\nPthread_mutex_unlock(&m);\n11\n}\nUnfortunately this approach is broken. Imagine the case where the\nchild runs immediately and calls thr exit() immediately; in this case,\nthe child will signal, but there is no thread asleep on the condition. When\nthe parent runs, it will simply call wait and be stuck; no thread will ever\nwake it. From this example, you should appreciate the importance of\nthe state variable done; it records the value the threads are interested in\nknowing. The sleeping, waking, and locking all are built around it.\nHere is another poor implementation. In this example, we imagine\nthat one does not need to hold a lock in order to signal and wait. What\nproblem could occur here? Think about it!\n1\nvoid thr_exit() {\n2\ndone = 1;\n3\nPthread_cond_signal(&c);\n4\n}\n5\n6\nvoid thr_join() {\n7\nif (done == 0)\n8\nPthread_cond_wait(&c);\n9\n}\nThe issue here is a subtle race condition. Speciﬁcally, if the parent calls\nthr join() and then checks the value of done, it will see that it is 0 and\nthus try to go to sleep. But just before it calls wait to go to sleep, the parent\nis interrupted, and the child runs. The child changes the state variable\ndone to 1 and signals, but no thread is waiting and thus no thread is\nwoken. When the parent runs again, it sleeps forever, which is sad.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2346,
      "extraction_method": "Direct"
    },
    {
      "page_number": 365,
      "chapter": null,
      "content": "CONDITION VARIABLES\n329\nTIP: ALWAYS HOLD THE LOCK WHILE SIGNALING\nAlthough it is strictly not necessary in all cases, it is likely simplest and\nbest to hold the lock while signaling when using condition variables. The\nexample above shows a case where you must hold the lock for correct-\nness; however, there are some other cases where it is likely OK not to, but\nprobably is something you should avoid. Thus, for simplicity, hold the\nlock when calling signal.\nThe converse of this tip, i.e., hold the lock when calling wait, is not just\na tip, but rather mandated by the semantics of wait, because wait always\n(a) assumes the lock is held when you call it, (b) releases said lock when\nputting the caller to sleep, and (c) re-acquires the lock just before return-\ning. Thus, the generalization of this tip is correct: hold the lock when\ncalling signal or wait, and you will always be in good shape.\nHopefully, from this simple join example, you can see some of the ba-\nsic requirements of using condition variables properly. To make sure you\nunderstand, we now go through a more complicated example: the pro-\nducer/consumer or bounded-buffer problem.\n30.2\nThe Producer/Consumer (Bound Buffer) Problem\nThe next synchronization problem we will confront in this chapter is\nknown as the producer/consumer problem, or sometimes as the bounded\nbuffer problem, which was ﬁrst posed by Dijkstra [D72]. Indeed, it was\nthis very producer/consumer problem that led Dijkstra and his co-workers\nto invent the generalized semaphore (which can be used as either a lock\nor a condition variable) [D01]; we will learn more about semaphores later.\nImagine one or more producer threads and one or more consumer\nthreads. Producers produce data items and wish to place them in a buffer;\nconsumers grab data items out of the buffer consume them in some way.\nThis arrangement occurs in many real systems.\nFor example, in a\nmulti-threaded web server, a producer puts HTTP requests into a work\nqueue (i.e., the bounded buffer); consumer threads take requests out of\nthis queue and process them.\nA bounded buffer is also used when you pipe the output of one pro-\ngram into another, e.g., grep foo file.txt | wc -l. This example\nruns two processes concurrently; grep writes lines from file.txt with\nthe string foo in them to what it thinks is standard output; the UNIX\nshell redirects the output to what is called a UNIX pipe (created by the\npipe system call). The other end of this pipe is connected to the stan-\ndard input of the process wc, which simply counts the number of lines in\nthe input stream and prints out the result. Thus, the grep process is the\nproducer; the wc process is the consumer; between them is an in-kernel\nbounded buffer; you, in this example, are just the happy user.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2807,
      "extraction_method": "Direct"
    },
    {
      "page_number": 366,
      "chapter": null,
      "content": "330\nCONDITION VARIABLES\n1\nint buffer;\n2\nint count = 0; // initially, empty\n3\n4\nvoid put(int value) {\n5\nassert(count == 0);\n6\ncount = 1;\n7\nbuffer = value;\n8\n}\n9\n10\nint get() {\n11\nassert(count == 1);\n12\ncount = 0;\n13\nreturn buffer;\n14\n}\nFigure 30.4: The Put and Get Routines (Version 1)\n1\nvoid *producer(void *arg) {\n2\nint i;\n3\nint loops = (int) arg;\n4\nfor (i = 0; i < loops; i++) {\n5\nput(i);\n6\n}\n7\n}\n8\n9\nvoid *consumer(void *arg) {\n10\nint i;\n11\nwhile (1) {\n12\nint tmp = get();\n13\nprintf(\"%d\\n\", tmp);\n14\n}\n15\n}\nFigure 30.5: Producer/Consumer Threads (Version 1)\nBecause the bounded buffer is a shared resource, we must of course\nrequire synchronized access to it, lest1 a race condition arise. To begin to\nunderstand this problem better, let us examine some actual code.\nThe ﬁrst thing we need is a shared buffer, into which a producer puts\ndata, and out of which a consumer takes data. Let’s just use a single\ninteger for simplicity (you can certainly imagine placing a pointer to a\ndata structure into this slot instead), and the two inner routines to put\na value into the shared buffer, and to get a value out of the buffer. See\nFigure 30.4 for details.\nPretty simple, no? The put() routine assumes the buffer is empty\n(and checks this with an assertion), and then simply puts a value into the\nshared buffer and marks it full by setting count to 1. The get() routine\ndoes the opposite, setting the buffer to empty (i.e., setting count to 0)\nand returning the value. Don’t worry that this shared buffer has just a\nsingle entry; later, we’ll generalize it to a queue that can hold multiple\nentries, which will be even more fun than it sounds.\nNow we need to write some routines that know when it is OK to access\nthe buffer to either put data into it or get data out of it. The conditions for\n1This is where we drop some serious Old English on you, and the subjunctive form.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1921,
      "extraction_method": "Direct"
    },
    {
      "page_number": 367,
      "chapter": null,
      "content": "CONDITION VARIABLES\n331\n1\ncond_t\ncond;\n2\nmutex_t mutex;\n3\n4\nvoid *producer(void *arg) {\n5\nint i;\n6\nfor (i = 0; i < loops; i++) {\n7\nPthread_mutex_lock(&mutex);\n// p1\n8\nif (count == 1)\n// p2\n9\nPthread_cond_wait(&cond, &mutex); // p3\n10\nput(i);\n// p4\n11\nPthread_cond_signal(&cond);\n// p5\n12\nPthread_mutex_unlock(&mutex);\n// p6\n13\n}\n14\n}\n15\n16\nvoid *consumer(void *arg) {\n17\nint i;\n18\nfor (i = 0; i < loops; i++) {\n19\nPthread_mutex_lock(&mutex);\n// c1\n20\nif (count == 0)\n// c2\n21\nPthread_cond_wait(&cond, &mutex); // c3\n22\nint tmp = get();\n// c4\n23\nPthread_cond_signal(&cond);\n// c5\n24\nPthread_mutex_unlock(&mutex);\n// c6\n25\nprintf(\"%d\\n\", tmp);\n26\n}\n27\n}\nFigure 30.6: Producer/Consumer: Single CV and If Statement\nthis should be obvious: only put data into the buffer when count is zero\n(i.e., when the buffer is empty), and only get data from the buffer when\ncount is one (i.e., when the buffer is full). If we write the synchronization\ncode such that a producer puts data into a full buffer, or a consumer gets\ndata from an empty one, we have done something wrong (and in this\ncode, an assertion will ﬁre).\nThis work is going to be done by two types of threads, one set of which\nwe’ll call the producer threads, and the other set which we’ll call con-\nsumer threads. Figure 30.5 shows the code for a producer that puts an\ninteger into the shared buffer loops number of times, and a consumer\nthat gets the data out of that shared buffer (forever), each time printing\nout the data item it pulled from the shared buffer.\nA Broken Solution\nNow imagine that we have just a single producer and a single consumer.\nObviously the put() and get() routines have critical sections within\nthem, as put() updates the buffer, and get() reads from it. However,\nputting a lock around the code doesn’t work; we need something more.\nNot surprisingly, that something more is some condition variables. In this\n(broken) ﬁrst try (Figure 30.6), we have a single condition variable cond\nand associated lock mutex.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2030,
      "extraction_method": "Direct"
    },
    {
      "page_number": 368,
      "chapter": null,
      "content": "332\nCONDITION VARIABLES\nTc1\nState\nTc2\nState\nTp\nState\nCount\nComment\nc1\nRunning\nReady\nReady\n0\nc2\nRunning\nReady\nReady\n0\nc3\nSleep\nReady\nReady\n0\nNothing to get\nSleep\nReady\np1\nRunning\n0\nSleep\nReady\np2\nRunning\n0\nSleep\nReady\np4\nRunning\n1\nBuffer now full\nReady\nReady\np5\nRunning\n1\nTc1 awoken\nReady\nReady\np6\nRunning\n1\nReady\nReady\np1\nRunning\n1\nReady\nReady\np2\nRunning\n1\nReady\nReady\np3\nSleep\n1\nBuffer full; sleep\nReady\nc1\nRunning\nSleep\n1\nTc2 sneaks in ...\nReady\nc2\nRunning\nSleep\n1\nReady\nc4\nRunning\nSleep\n0\n... and grabs data\nReady\nc5\nRunning\nReady\n0\nTp awoken\nReady\nc6\nRunning\nReady\n0\nc4\nRunning\nReady\nReady\n0\nOh oh! No data\nTable 30.1: Thread Trace: Broken Solution (Version 1)\nLet’s examine the signaling logic between producers and consumers.\nWhen a producer wants to ﬁll the buffer, it waits for it to be empty (p1–\np3). The consumer has the exact same logic, but waits for a different\ncondition: fullness (c1–c3).\nWith just a single producer and a single consumer, the code in Figure\n30.6 works. However, if we have more than one of these threads (e.g.,\ntwo consumers), the solution has two critical problems. What are they?\n... (pause here to think) ...\nLet’s understand the ﬁrst problem, which has to do with the if state-\nment before the wait. Assume there are two consumers (Tc1 and Tc2) and\none producer (Tp). First, a consumer (Tc1) runs; it acquires the lock (c1),\nchecks if any buffers are ready for consumption (c2), and ﬁnding that\nnone are, waits (c3) (which releases the lock).\nThen the producer (Tp) runs. It acquires the lock (p1), checks if all\nbuffers are full (p2), and ﬁnding that not to be the case, goes ahead and\nﬁlls the buffer (p4). The producer then signals that a buffer has been\nﬁlled (p5). Critically, this moves the ﬁrst consumer (Tc1) from sleeping\non a condition variable to the ready queue; Tc1 is now able to run (but\nnot yet running). The producer then continues until realizing the buffer\nis full, at which point it sleeps (p6, p1–p3).\nHere is where the problem occurs: another consumer (Tc2) sneaks in\nand consumes the one existing value in the buffer (c1, c2, c4, c5, c6, skip-\nping the wait at c3 because the buffer is full). Now assume Tc1 runs; just\nbefore returning from the wait, it re-acquires the lock and then returns. It\nthen calls get() (c4), but there are no buffers to consume! An assertion\ntriggers, and the code has not functioned as desired. Clearly, we should\nhave somehow prevented Tc1 from trying to consume because Tc2 snuck\nin and consumed the one value in the buffer that had been produced. Ta-\nble 30.1 shows the action each thread takes, as well as its scheduler state\n(Ready, Running, or Sleeping) over time.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2707,
      "extraction_method": "Direct"
    },
    {
      "page_number": 369,
      "chapter": null,
      "content": "CONDITION VARIABLES\n333\n1\ncond_t\ncond;\n2\nmutex_t mutex;\n3\n4\nvoid *producer(void *arg) {\n5\nint i;\n6\nfor (i = 0; i < loops; i++) {\n7\nPthread_mutex_lock(&mutex);\n// p1\n8\nwhile (count == 1)\n// p2\n9\nPthread_cond_wait(&cond, &mutex); // p3\n10\nput(i);\n// p4\n11\nPthread_cond_signal(&cond);\n// p5\n12\nPthread_mutex_unlock(&mutex);\n// p6\n13\n}\n14\n}\n15\n16\nvoid *consumer(void *arg) {\n17\nint i;\n18\nfor (i = 0; i < loops; i++) {\n19\nPthread_mutex_lock(&mutex);\n// c1\n20\nwhile (count == 0)\n// c2\n21\nPthread_cond_wait(&cond, &mutex); // c3\n22\nint tmp = get();\n// c4\n23\nPthread_cond_signal(&cond);\n// c5\n24\nPthread_mutex_unlock(&mutex);\n// c6\n25\nprintf(\"%d\\n\", tmp);\n26\n}\n27\n}\nFigure 30.7: Producer/Consumer: Single CV and While\nThe problem arises for a simple reason: after the producer woke Tc1,\nbut before Tc1 ever ran, the state of the bounded buffer changed (thanks to\nTc2). Signaling a thread only wakes them up; it is thus a hint that the state\nof the world has changed (in this case, that a value has been placed in the\nbuffer), but there is no guarantee that when the woken thread runs, the\nstate will still be as desired. This interpretation of what a signal means\nis often referred to as Mesa semantics, after the ﬁrst research that built\na condition variable in such a manner [LR80]; the contrast, referred to as\nHoare semantics, is harder to build but provides a stronger guarantee\nthat the woken thread will run immediately upon being woken [H74].\nVirtually every system ever built employs Mesa semantics.\nBetter, But Still Broken: While, Not If\nFortunately, this ﬁx is easy (Figure 30.7): change the if to a while. Think\nabout why this works; now consumer Tc1 wakes up and (with the lock\nheld) immediately re-checks the state of the shared variable (c2). If the\nbuffer is empty at that point, the consumer simply goes back to sleep\n(c3). The corollary if is also changed to a while in the producer (p2).\nThanks to Mesa semantics, a simple rule to remember with condition\nvariables is to always use while loops. Sometimes you don’t have to re-\ncheck the condition, but it is always safe to do so; just do it and be happy.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2158,
      "extraction_method": "Direct"
    },
    {
      "page_number": 370,
      "chapter": null,
      "content": "334\nCONDITION VARIABLES\nTc1\nState\nTc2\nState\nTp\nState\nCount\nComment\nc1\nRunning\nReady\nReady\n0\nc2\nRunning\nReady\nReady\n0\nc3\nSleep\nReady\nReady\n0\nNothing to get\nSleep\nc1\nRunning\nReady\n0\nSleep\nc2\nRunning\nReady\n0\nSleep\nc3\nSleep\nReady\n0\nNothing to get\nSleep\nSleep\np1\nRunning\n0\nSleep\nSleep\np2\nRunning\n0\nSleep\nSleep\np4\nRunning\n1\nBuffer now full\nReady\nSleep\np5\nRunning\n1\nTc1 awoken\nReady\nSleep\np6\nRunning\n1\nReady\nSleep\np1\nRunning\n1\nReady\nSleep\np2\nRunning\n1\nReady\nSleep\np3\nSleep\n1\nMust sleep (full)\nc2\nRunning\nSleep\nSleep\n1\nRecheck condition\nc4\nRunning\nSleep\nSleep\n0\nTc1 grabs data\nc5\nRunning\nReady\nSleep\n0\nOops! Woke Tc2\nc6\nRunning\nReady\nSleep\n0\nc1\nRunning\nReady\nSleep\n0\nc2\nRunning\nReady\nSleep\n0\nc3\nSleep\nReady\nSleep\n0\nNothing to get\nSleep\nc2\nRunning\nSleep\n0\nSleep\nc3\nSleep\nSleep\n0\nEveryone asleep...\nTable 30.2: Thread Trace: Broken Solution (Version 2)\nHowever, this code still has a bug, the second of two problems men-\ntioned above. Can you see it? It has something to do with the fact that\nthere is only one condition variable. Try to ﬁgure out what the problem\nis, before reading ahead. DO IT!\n... (another pause for you to think, or close your eyes for a bit) ...\nLet’s conﬁrm you ﬁgured it out correctly, or perhaps let’s conﬁrm that\nyou are now awake and reading this part of the book. The problem oc-\ncurs when two consumers run ﬁrst (Tc1 and Tc2), and both go to sleep\n(c3). Then, a producer runs, put a value in the buffer, wakes one of the\nconsumers (say Tc1), and goes back to sleep. Now we have one consumer\nready to run (Tc1), and two threads sleeping on a condition (Tc2 and Tp).\nAnd we are about to cause a problem to occur: things are getting exciting!\nThe consumer Tc1 then wakes by returning from wait() (c3), re-checks\nthe condition (c2), and ﬁnding the buffer full, consumes the value (c4).\nThis consumer then, critically, signals on the condition (c5), waking one\nthread that is sleeping. However, which thread should it wake?\nBecause the consumer has emptied the buffer, it clearly should wake\nthe producer. However, if it wakes the consumer Tc2 (which is deﬁnitely\npossible, depending on how the wait queue is managed), we have a prob-\nlem.\nSpeciﬁcally, the consumer Tc2 will wake up and ﬁnd the buffer\nempty (c2), and go back to sleep (c3). The producer Tp, which has a value\nto put into the buffer, is left sleeping. The other consumer thread, Tc1,\nalso goes back to sleep. All three threads are left sleeping, a clear bug; see\nTable 30.2 for the brutal step-by-step of this terrible calamity.\nSignaling is clearly needed, but must be more directed. A consumer\nshould not wake other consumers, only producers, and vice-versa.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2689,
      "extraction_method": "Direct"
    },
    {
      "page_number": 371,
      "chapter": null,
      "content": "CONDITION VARIABLES\n335\n1\ncond_t\nempty, fill;\n2\nmutex_t mutex;\n3\n4\nvoid *producer(void *arg) {\n5\nint i;\n6\nfor (i = 0; i < loops; i++) {\n7\nPthread_mutex_lock(&mutex);\n8\nwhile (count == 1)\n9\nPthread_cond_wait(&empty, &mutex);\n10\nput(i);\n11\nPthread_cond_signal(&fill);\n12\nPthread_mutex_unlock(&mutex);\n13\n}\n14\n}\n15\n16\nvoid *consumer(void *arg) {\n17\nint i;\n18\nfor (i = 0; i < loops; i++) {\n19\nPthread_mutex_lock(&mutex);\n20\nwhile (count == 0)\n21\nPthread_cond_wait(&fill, &mutex);\n22\nint tmp = get();\n23\nPthread_cond_signal(&empty);\n24\nPthread_mutex_unlock(&mutex);\n25\nprintf(\"%d\\n\", tmp);\n26\n}\n27\n}\nFigure 30.8: Producer/Consumer: Two CVs and While\nThe Single Buffer Producer/Consumer Solution\nThe solution here is once again a small one: use two condition variables,\ninstead of one, in order to properly signal which type of thread should\nwake up when the state of the system changes. Figure 30.8 shows the\nresulting code.\nIn the code above, producer threads wait on the condition empty, and\nsignals ﬁll. Conversely, consumer threads wait on ﬁll and signal empty.\nBy doing so, the second problem above is avoided by design: a consumer\ncan never accidentally wake a consumer, and a producer can never acci-\ndentally wake a producer.\nThe Final Producer/Consumer Solution\nWe now have a working producer/consumer solution, albeit not a fully\ngeneral one. The last change we make is to enable more concurrency and\nefﬁciency; speciﬁcally, we add more buffer slots, so that multiple values\ncan be produced before sleeping, and similarly multiple values can be\nconsumed before sleeping. With just a single producer and consumer, this\napproach is more efﬁcient as it reduces context switches; with multiple\nproducers or consumers (or both), it even allows concurrent producing\nor consuming to take place, thus increasing concurrency. Fortunately, it\nis a small change from our current solution.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 1924,
      "extraction_method": "Direct"
    },
    {
      "page_number": 372,
      "chapter": null,
      "content": "336\nCONDITION VARIABLES\n1\nint buffer[MAX];\n2\nint fill\n= 0;\n3\nint use\n= 0;\n4\nint count = 0;\n5\n6\nvoid put(int value) {\n7\nbuffer[fill] = value;\n8\nfill = (fill + 1) % MAX;\n9\ncount++;\n10\n}\n11\n12\nint get() {\n13\nint tmp = buffer[use];\n14\nuse = (use + 1) % MAX;\n15\ncount--;\n16\nreturn tmp;\n17\n}\nFigure 30.9: The Final Put and Get Routines\n1\ncond_t empty, fill;\n2\nmutex_t mutex;\n3\n4\nvoid *producer(void *arg) {\n5\nint i;\n6\nfor (i = 0; i < loops; i++) {\n7\nPthread_mutex_lock(&mutex);\n// p1\n8\nwhile (count == MAX)\n// p2\n9\nPthread_cond_wait(&empty, &mutex); // p3\n10\nput(i);\n// p4\n11\nPthread_cond_signal(&fill);\n// p5\n12\nPthread_mutex_unlock(&mutex);\n// p6\n13\n}\n14\n}\n15\n16\nvoid *consumer(void *arg) {\n17\nint i;\n18\nfor (i = 0; i < loops; i++) {\n19\nPthread_mutex_lock(&mutex);\n// c1\n20\nwhile (count == 0)\n// c2\n21\nPthread_cond_wait(&fill, &mutex);\n// c3\n22\nint tmp = get();\n// c4\n23\nPthread_cond_signal(&empty);\n// c5\n24\nPthread_mutex_unlock(&mutex);\n// c6\n25\nprintf(\"%d\\n\", tmp);\n26\n}\n27\n}\nFigure 30.10: The Final Working Solution\nThe ﬁrst change for this ﬁnal solution is within the buffer structure\nitself and the corresponding put() and get() (Figure 30.9). We also\nslightly change the conditions that producers and consumers check in or-\nder to determine whether to sleep or not. Figure 30.10 shows the ﬁnal\nwaiting and signaling logic. A producer only sleeps if all buffers are cur-\nrently ﬁlled (p2); similarly, a consumer only sleeps if all buffers are cur-\nrently empty (c2). And thus we solve the producer/consumer problem.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1565,
      "extraction_method": "Direct"
    },
    {
      "page_number": 373,
      "chapter": null,
      "content": "CONDITION VARIABLES\n337\nTIP: USE WHILE (NOT IF) FOR CONDITIONS\nWhen checking for a condition in a multi-threaded program, using\na while loop is always correct; using an if statement only might be,\ndepending on the semantics of signaling. Thus, always use while and\nyour code will behave as expected.\nUsing while loops around conditional checks also handles the case\nwhere spurious wakeups occur. In some thread packages, due to de-\ntails of the implementation, it is possible that two threads get woken up\nthough just a single signal has taken place [L11]. Spurious wakeups are\nfurther reason to re-check the condition a thread is waiting on.\n30.3\nCovering Conditions\nWe’ll now look at one more example of how condition variables can\nbe used. This code study is drawn from Lampson and Redell’s paper on\nPilot [LR80], the same group who ﬁrst implemented the Mesa semantics\ndescribed above (the language they used was Mesa, hence the name).\nThe problem they ran into is best shown via simple example, in this\ncase in a simple multi-threaded memory allocation library. Figure 30.11\nshows a code snippet which demonstrates the issue.\nAs you might see in the code, when a thread calls into the memory\nallocation code, it might have to wait in order for more memory to be-\ncome free. Conversely, when a thread frees memory, it signals that more\nmemory is free. However, our code above has a problem: which waiting\nthread (there can be more than one) should be woken up?\nConsider the following scenario. Assume there are zero bytes free;\nthread Ta calls allocate(100), followed by thread Tb which asks for\nless memory by calling allocate(10). Both Ta and Tb thus wait on the\ncondition and go to sleep; there aren’t enough free bytes to satisfy either\nof these requests.\nAt that point, assume a third thread, Tc, calls free(50). Unfortu-\nnately, when it calls signal to wake a waiting thread, it might not wake\nthe correct waiting thread, Tb, which is waiting for only 10 bytes to be\nfreed; Ta should remain waiting, as not enough memory is yet free. Thus,\nthe code in the ﬁgure does not work, as the thread waking other threads\ndoes not know which thread (or threads) to wake up.\nThe solution suggested by Lampson and Redell is straightforward: re-\nplace the pthread cond signal() call in the code above with a call to\npthread cond broadcast(), which wakes up all waiting threads. By\ndoing so, we guarantee that any threads that should be woken are. The\ndownside, of course, can be a negative performance impact, as we might\nneedlessly wake up many other waiting threads that shouldn’t (yet) be\nawake. Those threads will simply wake up, re-check the condition, and\nthen go immediately back to sleep.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2734,
      "extraction_method": "Direct"
    },
    {
      "page_number": 374,
      "chapter": null,
      "content": "338\nCONDITION VARIABLES\n1\n// how many bytes of the heap are free?\n2\nint bytesLeft = MAX_HEAP_SIZE;\n3\n4\n// need lock and condition too\n5\ncond_t\nc;\n6\nmutex_t m;\n7\n8\nvoid *\n9\nallocate(int size) {\n10\nPthread_mutex_lock(&m);\n11\nwhile (bytesLeft < size)\n12\nPthread_cond_wait(&c, &m);\n13\nvoid *ptr = ...; // get mem from heap\n14\nbytesLeft -= size;\n15\nPthread_mutex_unlock(&m);\n16\nreturn ptr;\n17\n}\n18\n19\nvoid free(void *ptr, int size) {\n20\nPthread_mutex_lock(&m);\n21\nbytesLeft += size;\n22\nPthread_cond_signal(&c); // whom to signal??\n23\nPthread_mutex_unlock(&m);\n24\n}\nFigure 30.11: Covering Conditions: An Example\nLampson and Redell call such a condition a covering condition, as it\ncovers all the cases where a thread needs to wake up (conservatively);\nthe cost, as we’ve discussed, is that too many threads might be woken.\nThe astute reader might also have noticed we could have used this ap-\nproach earlier (see the producer/consumer problem with only a single\ncondition variable). However, in that case, a better solution was avail-\nable to us, and thus we used it. In general, if you ﬁnd that your program\nonly works when you change your signals to broadcasts (but you don’t\nthink it should need to), you probably have a bug; ﬁx it! But in cases like\nthe memory allocator above, broadcast may be the most straightforward\nsolution available.\n30.4\nSummary\nWe have seen the introduction of another important synchronization\nprimitive beyond locks: condition variables. By allowing threads to sleep\nwhen some program state is not as desired, CVs enable us to neatly solve\na number of important synchronization problems, including the famous\n(and still important) producer/consumer problem, as well as covering\nconditions. A more dramatic concluding sentence would go here, such as\n“He loved Big Brother” [O49].\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1851,
      "extraction_method": "Direct"
    },
    {
      "page_number": 375,
      "chapter": null,
      "content": "CONDITION VARIABLES\n339\nReferences\n[D72] “Information Streams Sharing a Finite Buffer”\nE.W. Dijkstra\nInformation Processing Letters 1: 179180, 1972\nAvailable: http://www.cs.utexas.edu/users/EWD/ewd03xx/EWD329.PDF\nThe famous paper that introduced the producer/consumer problem.\n[D01] “My recollections of operating system design”\nE.W. Dijkstra\nApril, 2001\nAvailable: http://www.cs.utexas.edu/users/EWD/ewd13xx/EWD1303.PDF\nA fascinating read for those of you interested in how the pioneers of our ﬁeld came up with some very\nbasic and fundamental concepts, including ideas like “interrupts” and even “a stack”!\n[H74] “Monitors: An Operating System Structuring Concept”\nC.A.R. Hoare\nCommunications of the ACM, 17:10, pages 549–557, October 1974\nHoare did a fair amount of theoretical work in concurrency. However, he is still probably most known\nfor his work on Quicksort, the coolest sorting algorithm in the world, at least according to these authors.\n[L11] “Pthread cond signal Man Page”\nAvailable: http://linux.die.net/man/3/pthread cond signal\nMarch, 2011\nThe Linux man page shows a nice simple example of why a thread might get a spurious wakeup, due to\nrace conditions within the signal/wakeup code.\n[LR80] “Experience with Processes and Monitors in Mesa”\nB.W. Lampson, D.R. Redell\nCommunications of the ACM. 23:2, pages 105-117, February 1980\nA terriﬁc paper about how to actually implement signaling and condition variables in a real system,\nleading to the term “Mesa” semantics for what it means to be woken up; the older semantics, developed\nby Tony Hoare [H74], then became known as “Hoare” semantics, which is hard to say out loud in class\nwith a straight face.\n[O49] “1984”\nGeorge Orwell, 1949, Secker and Warburg\nA little heavy-handed, but of course a must read. That said, we kind of gave away the ending by quoting\nthe last sentence. Sorry! And if the government is reading this, let us just say that we think that the\ngovernment is “double plus good”. Hear that, our pals at the NSA?\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2040,
      "extraction_method": "Direct"
    },
    {
      "page_number": 376,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 377,
      "chapter": null,
      "content": "31\nSemaphores\nAs we know now, one needs both locks and condition variables to solve\na broad range of relevant and interesting concurrency problems. One of\nthe ﬁrst people to realize this years ago was Edsger Dijkstra (though it\nis hard to know the exact history [GR92]), known among other things for\nhis famous “shortest paths” algorithm in graph theory [D59], an early\npolemic on structured programming entitled “Goto Statements Consid-\nered Harmful” [D68a] (what a great title!), and, in the case we will study\nhere, the introduction of a synchronization primitive called the semaphore\n[D68b,D72]. Indeed, Dijkstra and colleagues invented the semaphore as a\nsingle primitive for all things related to synchronization; as you will see,\none can use semaphores as both locks and condition variables.\nTHE CRUX: HOW TO USE SEMAPHORES\nHow can we use semaphores instead of locks and condition variables?\nWhat is the deﬁnition of a semaphore? What is a binary semaphore?\nIs it straightforward to build a semaphore out of locks and condition\nvariables?\nWhat about building locks and condition variables out of\nsemaphores?\n31.1\nSemaphores: A Deﬁnition\nA semaphore is as an object with an integer value that we can ma-\nnipulate with two routines; in the POSIX standard, these routines are\nsem wait() and sem post()1. Because the initial value of the semaphore\ndetermines its behavior, before calling any other routine to interact with\nthe semaphore, we must ﬁrst initialize it to some value, as the code in\nFigure 31.1 does.\n1Historically, sem wait() was ﬁrst called P() by Dijkstra (for the Dutch word “to probe”)\nand sem post() was called V() (for the Dutch word “to test”). Sometimes, people call them\ndown and up, too. Use the Dutch versions to impress your friends.\n341\n",
      "content_length": 1766,
      "extraction_method": "Direct"
    },
    {
      "page_number": 378,
      "chapter": null,
      "content": "342\nSEMAPHORES\n1\n#include <semaphore.h>\n2\nsem_t s;\n3\nsem_init(&s, 0, 1);\nFigure 31.1: Initializing A Semaphore\nIn the ﬁgure, we declare a semaphore s and initialize it to the value 1\nby passing 1 in as the third argument. The second argument to sem init()\nwill be set to 0 in all of the examples we’ll see; this indicates that the\nsemaphore is shared between threads in the same process. See the man\npage for details on other usages of semaphores (namely, how they can\nbe used to synchronize access across different processes), which require a\ndifferent value for that second argument.\nAfter a semaphore is initialized, we can call one of two functions to\ninteract with it, sem wait() or sem post(). The behavior of these two\nfunctions is seen in Figure 31.2.\nFor now, we are not concerned with the implementation of these rou-\ntines, which clearly requires some care; with multiple threads calling into\nsem wait() and sem post(), there is the obvious need for managing\nthese critical sections. We will now focus on how to use these primitives;\nlater we may discuss how they are built.\nWe should discuss a few salient aspects of the interfaces here. First, we\ncan see that sem wait() will either return right away (because the value\nof the semaphore was one or higher when we called sem wait()), or it\nwill cause the caller to suspend execution waiting for a subsequent post.\nOf course, multiple calling threads may call into sem wait(), and thus\nall be queued waiting to be woken.\nSecond, we can see that sem post() does not wait for some particular\ncondition to hold like sem wait() does. Rather, it simply increments the\nvalue of the semaphore and then, if there is a thread waiting to be woken,\nwakes one of them up.\nThird, the value of the semaphore, when negative, is equal to the num-\nber of waiting threads [D68b]. Though the value generally isn’t seen by\nusers of the semaphores, this invariant is worth knowing and perhaps\ncan help you remember how a semaphore functions.\nDon’t worry (yet) about the seeming race conditions possible within\nthe semaphore; assume that the actions they make are performed atomi-\ncally. We will soon use locks and condition variables to do just this.\n1\nint sem_wait(sem_t *s) {\n2\ndecrement the value of semaphore s by one\n3\nwait if value of semaphore s is negative\n4\n}\n5\n6\nint sem_post(sem_t *s) {\n7\nincrement the value of semaphore s by one\n8\nif there are one or more threads waiting, wake one\n9\n}\nFigure 31.2: Semaphore: Deﬁnitions of Wait and Post\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2538,
      "extraction_method": "Direct"
    },
    {
      "page_number": 379,
      "chapter": null,
      "content": "SEMAPHORES\n343\n1\nsem_t m;\n2\nsem_init(&m, 0, X); // initialize semaphore to X; what should X be?\n3\n4\nsem_wait(&m);\n5\n// critical section here\n6\nsem_post(&m);\nFigure 31.3: A Binary Semaphore, a.k.a. a Lock\n31.2\nBinary Semaphores (Locks)\nWe are now ready to use a semaphore. Our ﬁrst use will be one with\nwhich we are already familiar: using a semaphore as a lock. See Figure\n31.3 for a code snippet; therein, you’ll see that we simply surround the\ncritical section of interest with a sem wait()/sem post() pair. Criti-\ncal to making this work, though, is the initial value of the semaphore m\n(initialized to X in the ﬁgure). What should X be?\n... (Try thinking about it before going on) ...\nLooking back at deﬁnition of the sem wait() and sem post() rou-\ntines above, we can see that the initial value should be 1.\nTo make this clear, let’s imagine a scenario with two threads. The ﬁrst\nthread (Thread 0) calls sem wait(); it will ﬁrst decrement the value of\nthe semaphore, changing it to 0. Then, it will wait only if the value is\nnot greater than or equal to 0; because the value is 0, the calling thread\nwill simply return and continue; Thread 0 is now free to enter the critical\nsection. If no other thread tries to acquire the lock while Thread 0 is inside\nthe critical section, when it calls sem post(), it will simply restore the\nvalue of the semaphore to 1 (and not wake any waiting thread, because\nthere are none). Table 31.1 shows a trace of this scenario.\nA more interesting case arises when Thread 0 “holds the lock” (i.e.,\nit has called sem wait() but not yet called sem post()), and another\nthread (Thread 1) tries to enter the critical section by calling sem wait().\nIn this case, Thread 1 will decrement the value of the semaphore to -1, and\nthus wait (putting itself to sleep and relinquishing the processor). When\nThread 0 runs again, it will eventually call sem post(), incrementing the\nvalue of the semaphore back to zero, and then wake the waiting thread\n(Thread 1), which will then be able to acquire the lock for itself. When\nThread 1 ﬁnishes, it will again increment the value of the semaphore,\nrestoring it to 1 again.\nValue of Semaphore\nThread 0\nThread 1\n1\n1\ncall sem wait()\n0\nsem wait() returns\n0\n(crit sect)\n0\ncall sem post()\n1\nsem post() returns\nTable 31.1: Thread Trace: Single Thread Using A Semaphore\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2372,
      "extraction_method": "Direct"
    },
    {
      "page_number": 380,
      "chapter": null,
      "content": "344\nSEMAPHORES\nValue\nThread 0\nState\nThread 1\nState\n1\nRunning\nReady\n1\ncall sem wait()\nRunning\nReady\n0\nsem wait() returns\nRunning\nReady\n0\n(crit sect:\nbegin)\nRunning\nReady\n0\nInterrupt; Switch→T1\nReady\nRunning\n0\nReady\ncall sem wait()\nRunning\n-1\nReady\ndecrement sem\nRunning\n-1\nReady\n(sem<0)→sleep\nSleeping\n-1\nRunning\nSwitch→T0\nSleeping\n-1\n(crit sect:\nend)\nRunning\nSleeping\n-1\ncall sem post()\nRunning\nSleeping\n0\nincrement sem\nRunning\nSleeping\n0\nwake(T1)\nRunning\nReady\n0\nsem post() returns\nRunning\nReady\n0\nInterrupt; Switch→T1\nReady\nRunning\n0\nReady\nsem wait() returns\nRunning\n0\nReady\n(crit sect)\nRunning\n0\nReady\ncall sem post()\nRunning\n1\nReady\nsem post() returns\nRunning\nTable 31.2: Thread Trace: Two Threads Using A Semaphore\nTable 31.2 shows a trace of this example. In addition to thread actions,\nthe table shows the scheduler state of each thread: Running, Ready (i.e.,\nrunnable but not running), and Sleeping. Note in particular that Thread 1\ngoes into the sleeping state when it tries to acquire the already-held lock;\nonly when Thread 0 runs again can Thread 1 be awoken and potentially\nrun again.\nIf you want to work through your own example, try a scenario where\nmultiple threads queue up waiting for a lock. What would the value of\nthe semaphore be during such a trace?\nThus we are able to use semaphores as locks. Because locks only have\ntwo states (held and not held), this usage is sometimes known as a binary\nsemaphore and in fact can be implemented in a more simpliﬁed manner\nthan discussed here; we instead use the generalized semaphore as a lock.\n31.3\nSemaphores As Condition Variables\nSemaphores are also useful when a thread wants to halt its progress\nwaiting for a condition to become true. For example, a thread may wish\nto wait for a list to become non-empty, so it can delete an element from it.\nIn this pattern of usage, we often ﬁnd a thread waiting for something to\nhappen, and a different thread making that something happen and then\nsignaling that it has happened, thus waking the waiting thread. Because\nthe waiting thread (or threads) is waiting for some condition in the pro-\ngram to change, we are using the semaphore as a condition variable.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2215,
      "extraction_method": "Direct"
    },
    {
      "page_number": 381,
      "chapter": null,
      "content": "SEMAPHORES\n345\n1\nsem_t s;\n2\n3\nvoid *\n4\nchild(void *arg) {\n5\nprintf(\"child\\n\");\n6\nsem_post(&s); // signal here: child is done\n7\nreturn NULL;\n8\n}\n9\n10\nint\n11\nmain(int argc, char *argv[]) {\n12\nsem_init(&s, 0, X); // what should X be?\n13\nprintf(\"parent: begin\\n\");\n14\npthread_t c;\n15\nPthread_create(c, NULL, child, NULL);\n16\nsem_wait(&s); // wait here for child\n17\nprintf(\"parent: end\\n\");\n18\nreturn 0;\n19\n}\nFigure 31.4: A Parent Waiting For Its Child\nA simple example is as follows.\nImagine a thread creates another\nthread and then wants to wait for it to complete its execution (Figure\n31.4). When this program runs, we would like to see the following:\nparent: begin\nchild\nparent: end\nThe question, then, is how to use a semaphore to achieve this effect,\nand is it turns out, it is relatively easy to understand. As you can see in\nthe code, the parent simply calls sem wait() and the child sem post()\nto wait for the condition of the child ﬁnishing its execution to become\ntrue. However, this raises the question: what should the initial value of\nthis semaphore be?\n(Again, think about it here, instead of reading ahead)\nThe answer, of course, is that the value of the semaphore should be\nset to is 0. There are two cases to consider. First, let us assume that the\nparent creates the child but the child has not run yet (i.e., it is sitting in\na ready queue but not running). In this case (Table 31.3), the parent will\ncall sem wait() before the child has called sem post(); we’d like the\nparent to wait for the child to run. The only way this will happen is if the\nvalue of the semaphore is not greater than 0; hence, 0 is the initial value.\nThe parent runs, decrements the semaphore (to -1), then waits (sleeping).\nWhen the child ﬁnally runs, it will call sem post(), increment the value\nof the semaphore to 0, and wake the parent, which will then return from\nsem wait() and ﬁnish the program.\nThe second case (Table 31.4) occurs when the child runs to comple-\ntion before the parent gets a chance to call sem wait(). In this case,\nthe child will ﬁrst call sem post(), thus incrementing the value of the\nsemaphore from 0 to 1. When the parent then gets a chance to run, it\nwill call sem wait() and ﬁnd the value of the semaphore to be 1; the\nparent will thus decrement the value (to 0) and return from sem wait()\nwithout waiting, also achieving the desired effect.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2406,
      "extraction_method": "Direct"
    },
    {
      "page_number": 382,
      "chapter": null,
      "content": "346\nSEMAPHORES\nValue\nParent\nState\nChild\nState\n0\ncreate(Child)\nRunning\n(Child exists; is runnable)\nReady\n0\ncall sem wait()\nRunning\nReady\n-1\ndecrement sem\nRunning\nReady\n-1\n(sem<0)→sleep\nSleeping\nReady\n-1\nSwitch→Child\nSleeping\nchild runs\nRunning\n-1\nSleeping\ncall sem post()\nRunning\n0\nSleeping\nincrement sem\nRunning\n0\nReady\nwake(Parent)\nRunning\n0\nReady\nsem post() returns\nRunning\n0\nReady\nInterrupt; Switch→Parent\nReady\n0\nsem wait() returns\nReady\nReady\nTable 31.3: Thread Trace: Parent Waiting For Child (Case 1)\nValue\nParent\nState\nChild\nState\n0\ncreate(Child)\nRunning\n(Child exists; is runnable)\nReady\n0\nInterrupt; Switch→Child\nReady\nchild runs\nRunning\n0\nReady\ncall sem post()\nRunning\n1\nReady\nincrement sem\nRunning\n1\nReady\nwake(nobody)\nRunning\n1\nReady\nsem post() returns\nRunning\n1\nparent runs\nRunning\nInterrupt; Switch→Parent\nReady\n1\ncall sem wait()\nRunning\nReady\n0\ndecrement sem\nRunning\nReady\n0\n(sem≥0)→awake\nRunning\nReady\n0\nsem wait() returns\nRunning\nReady\nTable 31.4: Thread Trace: Parent Waiting For Child (Case 2)\n31.4\nThe Producer/Consumer (Bounded-Buffer) Problem\nThe next problem we will confront in this chapter is known as the pro-\nducer/consumer problem, or sometimes as the bounded buffer problem\n[D72]. This problem is described in detail in the previous chapter on con-\ndition variables; see there for details.\nFirst Attempt\nOur ﬁrst attempt at solving the problem introduces two semaphores, empty\nand full, which the threads will use to indicate when a buffer entry has\nbeen emptied or ﬁlled, respectively. The code for the put and get routines\nis in Figure 31.5, and our attempt at solving the producer and consumer\nproblem is in Figure 31.6.\nIn this example, the producer ﬁrst waits for a buffer to become empty\nin order to put data into it, and the consumer similarly waits for a buffer\nto become ﬁlled before using it. Let us ﬁrst imagine that MAX=1 (there is\nonly one buffer in the array), and see if this works.\nImagine again there are two threads, a producer and a consumer. Let\nus examine a speciﬁc scenario on a single CPU. Assume the consumer\ngets to run ﬁrst. Thus, the consumer will hit line c1 in the ﬁgure above,\ncalling sem wait(&full). Because full was initialized to the value 0,\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2254,
      "extraction_method": "Direct"
    },
    {
      "page_number": 383,
      "chapter": null,
      "content": "SEMAPHORES\n347\n1\nint buffer[MAX];\n2\nint fill = 0;\n3\nint use\n= 0;\n4\n5\nvoid put(int value) {\n6\nbuffer[fill] = value;\n// line f1\n7\nfill = (fill + 1) % MAX; // line f2\n8\n}\n9\n10\nint get() {\n11\nint tmp = buffer[use];\n// line g1\n12\nuse = (use + 1) % MAX;\n// line g2\n13\nreturn tmp;\n14\n}\nFigure 31.5: The Put and Get Routines\n1\nsem_t empty;\n2\nsem_t full;\n3\n4\nvoid *producer(void *arg) {\n5\nint i;\n6\nfor (i = 0; i < loops; i++) {\n7\nsem_wait(&empty);\n// line P1\n8\nput(i);\n// line P2\n9\nsem_post(&full);\n// line P3\n10\n}\n11\n}\n12\n13\nvoid *consumer(void *arg) {\n14\nint i, tmp = 0;\n15\nwhile (tmp != -1) {\n16\nsem_wait(&full);\n// line C1\n17\ntmp = get();\n// line C2\n18\nsem_post(&empty);\n// line C3\n19\nprintf(\"%d\\n\", tmp);\n20\n}\n21\n}\n22\n23\nint main(int argc, char *argv[]) {\n24\n// ...\n25\nsem_init(&empty, 0, MAX); // MAX buffers are empty to begin with...\n26\nsem_init(&full, 0, 0);\n// ... and 0 are full\n27\n// ...\n28\n}\nFigure 31.6: Adding the Full and Empty Conditions\nthe call will decrement full (to -1), block the consumer, and wait for\nanother thread to call sem post() on full, as desired.\nAssume the producer then runs. It will hit line P1, thus calling the\nsem wait(&empty) routine. Unlike the consumer, the producer will\ncontinue through this line, because empty was initialized to the value\nMAX (in this case, 1). Thus, empty will be decremented to 0 and the\nproducer will put a data value into the ﬁrst entry of buffer (line P2). The\nproducer will then continue on to P3 and call sem post(&full), chang-\ning the value of the full semaphore from -1 to 0 and waking the consumer\n(e.g., move it from blocked to ready).\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 1644,
      "extraction_method": "Direct"
    },
    {
      "page_number": 384,
      "chapter": null,
      "content": "348\nSEMAPHORES\nIn this case, one of two things could happen. If the producer contin-\nues to run, it will loop around and hit line P1 again. This time, how-\never, it would block, as the empty semaphore’s value is 0. If the producer\ninstead was interrupted and the consumer began to run, it would call\nsem wait(&full) (line c1) and ﬁnd that the buffer was indeed full and\nthus consume it. In either case, we achieve the desired behavior.\nYou can try this same example with more threads (e.g., multiple pro-\nducers, and multiple consumers). It should still work.\nLet us now imagine that MAX is greater than 1 (say MAX = 10). For this\nexample, let us assume that there are multiple producers and multiple\nconsumers. We now have a problem: a race condition. Do you see where\nit occurs? (take some time and look for it) If you can’t see it, here’s a hint:\nlook more closely at the put() and get() code.\nOK, let’s understand the issue. Imagine two producers (Pa and Pb)\nboth calling into put() at roughly the same time. Assume producer Pa gets\nto run ﬁrst, and just starts to ﬁll the ﬁrst buffer entry (ﬁll = 0 at line f1).\nBefore Pa gets a chance to increment the ﬁll counter to 1, it is interrupted.\nProducer Pb starts to run, and at line f1 it also puts its data into the 0th\nelement of buffer, which means that the old data there is overwritten!\nThis is a no-no; we don’t want any data from the producer to be lost.\nA Solution: Adding Mutual Exclusion\nAs you can see, what we’ve forgotten here is mutual exclusion. The\nﬁlling of a buffer and incrementing of the index into the buffer is a critical\nsection, and thus must be guarded carefully. So let’s use our friend the\nbinary semaphore and add some locks. Figure 31.7 shows our attempt.\nNow we’ve added some locks around the entire put()/get() parts of\nthe code, as indicated by the NEW LINE comments. That seems like the\nright idea, but it also doesn’t work. Why? Deadlock. Why does deadlock\noccur? Take a moment to consider it; try to ﬁnd a case where deadlock\narises. What sequence of steps must happen for the program to deadlock?\nAvoiding Deadlock\nOK, now that you ﬁgured it out, here is the answer. Imagine two threads,\none producer and one consumer. The consumer gets to run ﬁrst. It ac-\nquires the mutex (line c0), and then calls sem wait() on the full semaphore\n(line c1); because there is no data yet, this call causes the consumer to\nblock and thus yield the CPU; importantly, though, the consumer still\nholds the lock.\nA producer then runs. It has data to produce and if it were able to run,\nit would be able to wake the consumer thread and all would be good.\nUnfortunately, the ﬁrst thing it does is call sem wait() on the binary\nmutex semaphore (line p0). The lock is already held. Hence, the producer\nis now stuck waiting too.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2838,
      "extraction_method": "Direct"
    },
    {
      "page_number": 385,
      "chapter": null,
      "content": "SEMAPHORES\n349\n1\nsem_t empty;\n2\nsem_t full;\n3\nsem_t mutex;\n4\n5\nvoid *producer(void *arg) {\n6\nint i;\n7\nfor (i = 0; i < loops; i++) {\n8\nsem_wait(&mutex);\n// line p0 (NEW LINE)\n9\nsem_wait(&empty);\n// line p1\n10\nput(i);\n// line p2\n11\nsem_post(&full);\n// line p3\n12\nsem_post(&mutex);\n// line p4 (NEW LINE)\n13\n}\n14\n}\n15\n16\nvoid *consumer(void *arg) {\n17\nint i;\n18\nfor (i = 0; i < loops; i++) {\n19\nsem_wait(&mutex);\n// line c0 (NEW LINE)\n20\nsem_wait(&full);\n// line c1\n21\nint tmp = get();\n// line c2\n22\nsem_post(&empty);\n// line c3\n23\nsem_post(&mutex);\n// line c4 (NEW LINE)\n24\nprintf(\"%d\\n\", tmp);\n25\n}\n26\n}\n27\n28\nint main(int argc, char *argv[]) {\n29\n// ...\n30\nsem_init(&empty, 0, MAX); // MAX buffers are empty to begin with...\n31\nsem_init(&full, 0, 0);\n// ... and 0 are full\n32\nsem_init(&mutex, 0, 1);\n// mutex=1 because it is a lock (NEW LINE)\n33\n// ...\n34\n}\nFigure 31.7: Adding Mutual Exclusion (Incorrectly)\nThere is a simple cycle here. The consumer holds the mutex and is\nwaiting for the someone to signal full. The producer could signal full but\nis waiting for the mutex. Thus, the producer and consumer are each stuck\nwaiting for each other: a classic deadlock.\nFinally, A Working Solution\nTo solve this problem, we simply must reduce the scope of the lock. Fig-\nure 31.8 shows the ﬁnal working solution. As you can see, we simply\nmove the mutex acquire and release to be just around the critical section;\nthe full and empty wait and signal code is left outside. The result is a\nsimple and working bounded buffer, a commonly-used pattern in multi-\nthreaded programs. Understand it now; use it later. You will thank us for\nyears to come. Or at least, you will thank us when the same question is\nasked on the ﬁnal exam.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 1763,
      "extraction_method": "Direct"
    },
    {
      "page_number": 386,
      "chapter": null,
      "content": "350\nSEMAPHORES\n1\nsem_t empty;\n2\nsem_t full;\n3\nsem_t mutex;\n4\n5\nvoid *producer(void *arg) {\n6\nint i;\n7\nfor (i = 0; i < loops; i++) {\n8\nsem_wait(&empty);\n// line p1\n9\nsem_wait(&mutex);\n// line p1.5 (MOVED MUTEX HERE...)\n10\nput(i);\n// line p2\n11\nsem_post(&mutex);\n// line p2.5 (... AND HERE)\n12\nsem_post(&full);\n// line p3\n13\n}\n14\n}\n15\n16\nvoid *consumer(void *arg) {\n17\nint i;\n18\nfor (i = 0; i < loops; i++) {\n19\nsem_wait(&full);\n// line c1\n20\nsem_wait(&mutex);\n// line c1.5 (MOVED MUTEX HERE...)\n21\nint tmp = get();\n// line c2\n22\nsem_post(&mutex);\n// line c2.5 (... AND HERE)\n23\nsem_post(&empty);\n// line c3\n24\nprintf(\"%d\\n\", tmp);\n25\n}\n26\n}\n27\n28\nint main(int argc, char *argv[]) {\n29\n// ...\n30\nsem_init(&empty, 0, MAX); // MAX buffers are empty to begin with...\n31\nsem_init(&full, 0, 0);\n// ... and 0 are full\n32\nsem_init(&mutex, 0, 1);\n// mutex=1 because it is a lock\n33\n// ...\n34\n}\nFigure 31.8: Adding Mutual Exclusion (Correctly)\n31.5\nReader-Writer Locks\nAnother classic problem stems from the desire for a more ﬂexible lock-\ning primitive that admits that different data structure accesses might re-\nquire different kinds of locking. For example, imagine a number of con-\ncurrent list operations, including inserts and simple lookups. While in-\nserts change the state of the list (and thus a traditional critical section\nmakes sense), lookups simply read the data structure; as long as we can\nguarantee that no insert is on-going, we can allow many lookups to pro-\nceed concurrently. The special type of lock we will now develop to sup-\nport this type of operation is known as a reader-writer lock [CHP71]. The\ncode for such a lock is available in Figure 31.9.\nThe code is pretty simple. If some thread wants to update the data\nstructure in question, it should call the new pair of synchronization op-\nerations: rwlock acquire writelock(), to acquire a write lock, and\nrwlock release writelock(), to release it. Internally, these simply\nuse the writelock semaphore to ensure that only a single writer can ac-\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2060,
      "extraction_method": "Direct"
    },
    {
      "page_number": 387,
      "chapter": null,
      "content": "SEMAPHORES\n351\n1\ntypedef struct _rwlock_t {\n2\nsem_t lock;\n// binary semaphore (basic lock)\n3\nsem_t writelock; // used to allow ONE writer or MANY readers\n4\nint\nreaders;\n// count of readers reading in critical section\n5\n} rwlock_t;\n6\n7\nvoid rwlock_init(rwlock_t *rw) {\n8\nrw->readers = 0;\n9\nsem_init(&rw->lock, 0, 1);\n10\nsem_init(&rw->writelock, 0, 1);\n11\n}\n12\n13\nvoid rwlock_acquire_readlock(rwlock_t *rw) {\n14\nsem_wait(&rw->lock);\n15\nrw->readers++;\n16\nif (rw->readers == 1)\n17\nsem_wait(&rw->writelock); // first reader acquires writelock\n18\nsem_post(&rw->lock);\n19\n}\n20\n21\nvoid rwlock_release_readlock(rwlock_t *rw) {\n22\nsem_wait(&rw->lock);\n23\nrw->readers--;\n24\nif (rw->readers == 0)\n25\nsem_post(&rw->writelock); // last reader releases writelock\n26\nsem_post(&rw->lock);\n27\n}\n28\n29\nvoid rwlock_acquire_writelock(rwlock_t *rw) {\n30\nsem_wait(&rw->writelock);\n31\n}\n32\n33\nvoid rwlock_release_writelock(rwlock_t *rw) {\n34\nsem_post(&rw->writelock);\n35\n}\nFigure 31.9: A Simple Reader-Writer Lock\nquire the lock and thus enter the critical section to update the data struc-\nture in question.\nMore interesting is the pair of routines to acquire and release read\nlocks. When acquiring a read lock, the reader ﬁrst acquires lock and\nthen increments the readers variable to track how many readers are\ncurrently inside the data structure. The important step then taken within\nrwlock acquire readlock() occurs when the ﬁrst reader acquires\nthe lock; in that case, the reader also acquires the write lock by calling\nsem wait() on the writelock semaphore, and then ﬁnally releasing\nthe lock by calling sem post().\nThus, once a reader has acquired a read lock, more readers will be\nallowed to acquire the read lock too; however, any thread that wishes to\nacquire the write lock will have to wait until all readers are ﬁnished; the\nlast one to exit the critical section calls sem post() on “writelock” and\nthus enables a waiting writer to acquire the lock.\nThis approach works (as desired), but does have some negatives, espe-\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2051,
      "extraction_method": "Direct"
    },
    {
      "page_number": 388,
      "chapter": null,
      "content": "352\nSEMAPHORES\nTIP: SIMPLE AND DUMB CAN BE BETTER (HILL’S LAW)\nYou should never underestimate the notion that the simple and dumb\napproach can be the best one. With locking, sometimes a simple spin lock\nworks best, because it is easy to implement and fast. Although something\nlike reader/writer locks sounds cool, they are complex, and complex can\nmean slow. Thus, always try the simple and dumb approach ﬁrst.\nThis idea, of appealing to simplicity, is found in many places. One early\nsource is Mark Hill’s dissertation [H87], which studied how to design\ncaches for CPUs. Hill found that simple direct-mapped caches worked\nbetter than fancy set-associative designs (one reason is that in caching,\nsimpler designs enable faster lookups). As Hill succinctly summarized\nhis work: “Big and dumb is better.” And thus we call this similar advice\nHill’s Law.\ncially when it comes to fairness. In particular, it would be relatively easy\nfor readers to starve writers. More sophisticated solutions to this prob-\nlem exist; perhaps you can think of a better implementation? Hint: think\nabout what you would need to do to prevent more readers from entering\nthe lock once a writer is waiting.\nFinally, it should be noted that reader-writer locks should be used\nwith some caution. They often add more overhead (especially with more\nsophisticated implementations), and thus do not end up speeding up\nperformance as compared to just using simple and fast locking primi-\ntives [CB08].\nEither way, they showcase once again how we can use\nsemaphores in an interesting and useful way.\n31.6\nThe Dining Philosophers\nOne of the most famous concurrency problems posed, and solved, by\nDijkstra, is known as the dining philosopher’s problem [DHO71]. The\nproblem is famous because it is fun and somewhat intellectually inter-\nesting; however, its practical utility is low. However, its fame forces its\ninclusion here; indeed, you might be asked about it on some interview,\nand you’d really hate your OS professor if you miss that question and\ndon’t get the job. Conversely, if you get the job, please feel free to send\nyour OS professor a nice note, or some stock options.\nThe basic setup for the problem is this (as shown in Figure 31.10): as-\nsume there are ﬁve “philosophers” sitting around a table. Between each\npair of philosophers is a single fork (and thus, ﬁve total). The philoso-\nphers each have times where they think, and don’t need any forks, and\ntimes where they eat. In order to eat, a philosopher needs two forks, both\nthe one on their left and the one on their right. The contention for these\nforks, and the synchronization problems that ensue, are what makes this\na problem we study in concurrent programming.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2749,
      "extraction_method": "Direct"
    },
    {
      "page_number": 389,
      "chapter": null,
      "content": "SEMAPHORES\n353\nP0\nP1\nP2\nP3\nP4\nf0\nf1\nf2\nf3\nf4\nFigure 31.10: The Dining Philosophers\nHere is the basic loop of each philosopher:\nwhile (1) {\nthink();\ngetforks();\neat();\nputforks();\n}\nThe key challenge, then, is to write the routines getforks() and\nputforks() such that there is no deadlock, no philosopher starves and\nnever gets to eat, and concurrency is high (i.e., as many philosophers can\neat at the same time as possible).\nFollowing Downey’s solutions [D08], we’ll use a few helper functions\nto get us towards a solution. They are:\nint left(int p)\n{ return p; }\nint right(int p) { return (p + 1) % 5; }\nWhen philosopher p wishes to refer to the fork on their left, they sim-\nply call left(p). Similarly, the fork on the right of a philosopher p is\nreferred to by calling right(p); the modulo operator therein handles\nthe one case where the last philosopher (p=4) tries to grab the fork on\ntheir right, which is fork 0.\nWe’ll also need some semaphores to solve this problem. Let us assume\nwe have ﬁve, one for each fork: sem t forks[5].\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 1080,
      "extraction_method": "Direct"
    },
    {
      "page_number": 390,
      "chapter": null,
      "content": "354\nSEMAPHORES\n1\nvoid getforks() {\n2\nsem_wait(forks[left(p)]);\n3\nsem_wait(forks[right(p)]);\n4\n}\n5\n6\nvoid putforks() {\n7\nsem_post(forks[left(p)]);\n8\nsem_post(forks[right(p)]);\n9\n}\nFigure 31.11: The getforks() and putforks() Routines\nBroken Solution\nWe attempt our ﬁrst solution to the problem. Assume we initialize each\nsemaphore (in the forks array) to a value of 1. Assume also that each\nphilosopher knows its own number (p). We can thus write the getforks()\nand putforks() routine as shown in Figure 31.11.\nThe intuition behind this (broken) solution is as follows. To acquire\nthe forks, we simply grab a “lock” on each one: ﬁrst the one on the left,\nand then the one on the right. When we are done eating, we release them.\nSimple, no? Unfortunately, in this case, simple means broken. Can you\nsee the problem that arises? Think about it.\nThe problem is deadlock. If each philosopher happens to grab the fork\non their left before any philosopher can grab the fork on their right, each\nwill be stuck holding one fork and waiting for another, forever. Speciﬁ-\ncally, philosopher 0 grabs fork 0, philosopher 1 grabs fork 1, philosopher\n2 grabs fork 2, philosopher 3 grabs fork 3, and philosopher 4 grabs fork 4;\nall the forks are acquired, and all the philosophers are stuck waiting for\na fork that another philosopher possesses. We’ll study deadlock in more\ndetail soon; for now, it is safe to say that this is not a working solution.\nA Solution: Breaking The Dependency\nThe simplest way to attack this problem is to change how forks are ac-\nquired by at least one of the philosophers; indeed, this is how Dijkstra\nhimself solved the problem. Speciﬁcally, let’s assume that philosopher\n4 (the highest numbered one) acquires the forks in a different order. The\ncode to do so is as follows:\n1\nvoid getforks() {\n2\nif (p == 4) {\n3\nsem_wait(forks[right(p)]);\n4\nsem_wait(forks[left(p)]);\n5\n} else {\n6\nsem_wait(forks[left(p)]);\n7\nsem_wait(forks[right(p)]);\n8\n}\n9\n}\nBecause the last philosopher tries to grab right before left, there is no\nsituation where each philosopher grabs one fork and is stuck waiting for\nanother; the cycle of waiting is broken. Think through the ramiﬁcations\nof this solution, and convince yourself that it works.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2279,
      "extraction_method": "Direct"
    },
    {
      "page_number": 391,
      "chapter": null,
      "content": "SEMAPHORES\n355\n1\ntypedef struct __Zem_t {\n2\nint value;\n3\npthread_cond_t cond;\n4\npthread_mutex_t lock;\n5\n} Zem_t;\n6\n7\n// only one thread can call this\n8\nvoid Zem_init(Zem_t *s, int value) {\n9\ns->value = value;\n10\nCond_init(&s->cond);\n11\nMutex_init(&s->lock);\n12\n}\n13\n14\nvoid Zem_wait(Zem_t *s) {\n15\nMutex_lock(&s->lock);\n16\nwhile (s->value <= 0)\n17\nCond_wait(&s->cond, &s->lock);\n18\ns->value--;\n19\nMutex_unlock(&s->lock);\n20\n}\n21\n22\nvoid Zem_post(Zem_t *s) {\n23\nMutex_lock(&s->lock);\n24\ns->value++;\n25\nCond_signal(&s->cond);\n26\nMutex_unlock(&s->lock);\n27\n}\nFigure 31.12: Implementing Zemaphores with Locks and CVs\nThere are other “famous” problems like this one, e.g., the cigarette\nsmoker’s problem or the sleeping barber problem. Most of them are\njust excuses to think about concurrency; some of them have fascinating\nnames. Look them up if you are interested in learning more, or just get-\nting more practice thinking in a concurrent manner [D08].\n31.7\nHow To Implement Semaphores\nFinally, let’s use our low-level synchronization primitives, locks and\ncondition variables, to build our own version of semaphores called ...\n(drum roll here) ... Zemaphores. This task is fairly straightforward, as\nyou can see in Figure 31.12.\nAs you can see from the ﬁgure, we use just one lock and one condition\nvariable, plus a state variable to track the value of the semaphore. Study\nthe code for yourself until you really understand it. Do it!\nOne subtle difference between our Zemaphore and pure semaphores\nas deﬁned by Dijkstra is that we don’t maintain the invariant that the\nvalue of the semaphore, when negative, reﬂects the number of waiting\nthreads; indeed, the value will never be lower than zero. This behavior is\neasier to implement and matches the current Linux implementation.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 1819,
      "extraction_method": "Direct"
    },
    {
      "page_number": 392,
      "chapter": null,
      "content": "356\nSEMAPHORES\nTIP: BE CAREFUL WITH GENERALIZATION\nThe abstract technique of generalization can thus be quite useful in sys-\ntems design, where one good idea can be made slightly broader and thus\nsolve a larger class of problems. However, be careful when generalizing;\nas Lampson warns us “Don’t generalize; generalizations are generally\nwrong” [L83].\nOne could view semaphores as a generalization of locks and condition\nvariables; however, is such a generalization needed? And, given the dif-\nﬁculty of realizing a condition variable on top of a semaphore, perhaps\nthis generalization is not as general as you might think.\nCuriously, building locks and condition variables out of semaphores\nis a much trickier proposition. Some highly experienced concurrent pro-\ngrammers tried to do this in the Windows environment, and many differ-\nent bugs ensued [B04]. Try it yourself, and see if you can ﬁgure out why\nbuilding condition variables out of semaphores is more challenging than\nit might appear.\n31.8\nSummary\nSemaphores are a powerful and ﬂexible primitive for writing concur-\nrent programs. Some programmers use them exclusively, shunning locks\nand condition variables, due to their simplicity and utility.\nIn this chapter, we have presented just a few classic problems and solu-\ntions. If you are interested in ﬁnding out more, there are many other ma-\nterials you can reference. One great (and free reference) is Allen Downey’s\nbook on concurrency and programming with semaphores [D08]. This\nbook has lots of puzzles you can work on to improve your understand-\ning of both semaphores in speciﬁc and concurrency in general. Becoming\na real concurrency expert takes years of effort; going beyond what you\nlearn in this class is undoubtedly the key to mastering such a topic.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1824,
      "extraction_method": "Direct"
    },
    {
      "page_number": 393,
      "chapter": null,
      "content": "SEMAPHORES\n357\nReferences\n[B04] “Implementing Condition Variables with Semaphores”\nAndrew Birrell\nDecember 2004\nAn interesting read on how difﬁcult implementing CVs on top of semaphores really is, and the mistakes\nthe author and co-workers made along the way. Particularly relevant because the group had done a ton\nof concurrent programming; Birrell, for example, is known for (among other things) writing various\nthread-programming guides.\n[CB08] “Real-world Concurrency”\nBryan Cantrill and Jeff Bonwick\nACM Queue. Volume 6, No. 5. September 2008\nA nice article by some kernel hackers from a company formerly known as Sun on the real problems faced\nin concurrent code.\n[CHP71] “Concurrent Control with Readers and Writers”\nP.J. Courtois, F. Heymans, D.L. Parnas\nCommunications of the ACM, 14:10, October 1971\nThe introduction of the reader-writer problem, and a simple solution. Later work introduced more\ncomplex solutions, skipped here because, well, they are pretty complex.\n[D59] “A Note on Two Problems in Connexion with Graphs”\nE. W. Dijkstra\nNumerische Mathematik 1, 269271, 1959\nAvailable: http://www-m3.ma.tum.de/twiki/pub/MN0506/WebHome/dijkstra.pdf\nCan you believe people worked on algorithms in 1959? We can’t. Even before computers were any fun\nto use, these people had a sense that they would transform the world...\n[D68a] “Go-to Statement Considered Harmful”\nE.W. Dijkstra\nCommunications of the ACM, volume 11(3): pages 147148, March 1968\nAvailable: http://www.cs.utexas.edu/users/EWD/ewd02xx/EWD215.PDF\nSometimes thought as the beginning of the ﬁeld of software engineering.\n[D68b] “The Structure of the THE Multiprogramming System”\nE.W. Dijkstra\nCommunications of the ACM, volume 11(5), pages 341346, 1968\nOne of the earliest papers to point out that systems work in computer science is an engaging intellectual\nendeavor. Also argues strongly for modularity in the form of layered systems.\n[D72] “Information Streams Sharing a Finite Buffer”\nE.W. Dijkstra\nInformation Processing Letters 1: 179180, 1972\nAvailable: http://www.cs.utexas.edu/users/EWD/ewd03xx/EWD329.PDF\nDid Dijkstra invent everything? No, but maybe close. He certainly was the ﬁrst to clearly write down\nwhat the problems were in concurrent code. However, it is true that practitioners in operating system\ndesign knew of many of the problems described by Dijkstra, so perhaps giving him too much credit\nwould be a misrepresentation of history.\n[D08] “The Little Book of Semaphores”\nA.B. Downey\nAvailable: http://greenteapress.com/semaphores/\nA nice (and free!) book about semaphores. Lots of fun problems to solve, if you like that sort of thing.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2671,
      "extraction_method": "Direct"
    },
    {
      "page_number": 394,
      "chapter": null,
      "content": "358\nSEMAPHORES\n[DHO71] “Hierarchical ordering of sequential processes”\nE.W. Dijkstra\nAvailable: http://www.cs.utexas.edu/users/EWD/ewd03xx/EWD310.PDF\nPresents numerous concurrency problems, including the Dining Philosophers. The wikipedia page\nabout this problem is also quite informative.\n[GR92] “Transaction Processing: Concepts and Techniques”\nJim Gray and Andreas Reuter\nMorgan Kaufmann, September 1992\nThe exact quote that we ﬁnd particularly humorous is found on page 485, at the top of Section 8.8:\n“The ﬁrst multiprocessors, circa 1960, had test and set instructions ... presumably the OS imple-\nmentors worked out the appropriate algorithms, although Dijkstra is generally credited with inventing\nsemaphores many years later.”\n[H87] “Aspects of Cache Memory and Instruction Buffer Performance”\nMark D. Hill\nPh.D. Dissertation, U.C. Berkeley, 1987\nHill’s dissertation work, for those obsessed with caching in early systems. A great example of a quanti-\ntative dissertation.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1029,
      "extraction_method": "Direct"
    },
    {
      "page_number": 395,
      "chapter": null,
      "content": "32\nCommon Concurrency Problems\nResearchers have spent a great deal of time and effort looking into con-\ncurrency bugs over many years.\nMuch of the early work focused on\ndeadlock, a topic which we’ve touched on in the past chapters but will\nnow dive into deeply [C+71].\nMore recent work focuses on studying\nother types of common concurrency bugs (i.e., non-deadlock bugs). In\nthis chapter, we take a brief look at some example concurrency problems\nfound in real code bases, to better understand what problems to look out\nfor. And thus our problem:\nCRUX: HOW TO HANDLE COMMON CONCURRENCY BUGS\nConcurrency bugs tend to come in a variety of common patterns.\nKnowing which ones to look out for is the ﬁrst step to writing more ro-\nbust, correct concurrent code.\n32.1\nWhat Types Of Bugs Exist?\nThe ﬁrst, and most obvious, question is this: what types of concur-\nrency bugs manifest in complex, concurrent programs? This question is\ndifﬁcult to answer in general, but fortunately, some others have done the\nwork for us. Speciﬁcally, we rely upon a study by Lu et al. [L+08], which\nanalyzes a number of popular concurrent applications in great detail to\nunderstand what types of bugs arise in practice.\nThe study focuses on four major and important open-source applica-\ntions: MySQL (a popular database management system), Apache (a well-\nknown web server), Mozilla (the famous web browser), and OpenOfﬁce\n(a free version of the MS Ofﬁce suite, which some people actually use).\nIn the study, the authors examine concurrency bugs that have been found\nand ﬁxed in each of these code bases, turning the developers’ work into a\nquantitative bug analysis; understanding these results can help you un-\nderstand what types of problems actually occur in mature code bases.\n359\n",
      "content_length": 1761,
      "extraction_method": "Direct"
    },
    {
      "page_number": 396,
      "chapter": null,
      "content": "360\nCOMMON CONCURRENCY PROBLEMS\nApplication\nWhat it does\nNon-Deadlock\nDeadlock\nMySQL\nDatabase Server\n14\n9\nApache\nWeb Server\n13\n4\nMozilla\nWeb Browser\n41\n16\nOpenOfﬁce\nOfﬁce Suite\n6\n2\nTotal\n74\n31\nTable 32.1: Bugs In Modern Applications\nTable 32.1 shows a summary of the bugs Lu and colleagues studied.\nFrom the table, you can see that there were 105 total bugs, most of which\nwere not deadlock (74); the remaining 31 were deadlock bugs. Further,\nyou can see that the number of bugs studied from each application; while\nOpenOfﬁce only had 8 total concurrency bugs, Mozilla had nearly 60.\nWe now dive into these different classes of bugs (non-deadlock, dead-\nlock) a bit more deeply. For the ﬁrst class of non-deadlock bugs, we use\nexamples from the study to drive our discussion. For the second class of\ndeadlock bugs, we discuss the long line of work that has been done in\neither preventing, avoiding, or handling deadlock.\n32.2\nNon-Deadlock Bugs\nNon-deadlock bugs make up a majority of concurrency bugs, accord-\ning to Lu’s study. But what types of bugs are these? How do they arise?\nHow can we ﬁx them? We now discuss the two major types of non-\ndeadlock bugs found by Lu et al.: atomicity violation bugs and order\nviolation bugs.\nAtomicity-Violation Bugs\nThe ﬁrst type of problem encountered is referred to as an atomicity vi-\nolation. Here is a simple example, found in MySQL. Before reading the\nexplanation, try ﬁguring out what the bug is. Do it!\n1\nThread 1::\n2\nif (thd->proc_info) {\n3\n...\n4\nfputs(thd->proc_info, ...);\n5\n...\n6\n}\n7\n8\nThread 2::\n9\nthd->proc_info = NULL;\nIn the example, two different threads access the ﬁeld proc info in\nthe structure thd. The ﬁrst thread checks if the value is non-NULL and\nthen prints its value; the second thread sets it to NULL. Clearly, if the\nﬁrst thread performs the check but then is interrupted before the call to\nfputs, the second thread could run in-between, thus setting the pointer\nto NULL; when the ﬁrst thread resumes, it will crash, as a NULL pointer\nwill be dereferenced by fputs.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2081,
      "extraction_method": "Direct"
    },
    {
      "page_number": 397,
      "chapter": null,
      "content": "COMMON CONCURRENCY PROBLEMS\n361\nThe more formal deﬁnition of an atomicity violation, according to Lu\net al, is this: “The desired serializability among multiple memory accesses\nis violated (i.e. a code region is intended to be atomic, but the atomicity\nis not enforced during execution).” In our example above, the code has\nan atomicity assumption (in Lu’s words) about the check for non-NULL\nof proc info and the usage of proc info in the fputs() call; when\nassumption is broken, the code will not work as desired.\nFinding a ﬁx for this type of problem is often (but not always) straight-\nforward. Can you think of how to ﬁx the code above?\nIn this solution, we simply add locks around the shared-variable ref-\nerences, ensuring that when either thread accesses the proc info ﬁeld,\nit has a lock held. Of course (not shown), any other code that accesses the\nstructure should also acquire this lock before doing so.\n1\npthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;\n2\n3\nThread 1::\n4\npthread_mutex_lock(&lock);\n5\nif (thd->proc_info) {\n6\n...\n7\nfputs(thd->proc_info, ...);\n8\n...\n9\n}\n10\npthread_mutex_unlock(&lock);\n11\n12\nThread 2::\n13\npthread_mutex_lock(&lock);\n14\nthd->proc_info = NULL;\n15\npthread_mutex_unlock(&lock);\nOrder-Violation Bugs\nAnother common type of non-deadlock bug found by Lu et al. is known\nas an order violation. Here is another simple example; once again, see if\nyou can ﬁgure out why the code below has a bug in it.\n1\nThread 1::\n2\nvoid init() {\n3\n...\n4\nmThread = PR_CreateThread(mMain, ...);\n5\n...\n6\n}\n7\n8\nThread 2::\n9\nvoid mMain(...) {\n10\n...\n11\nmState = mThread->State;\n12\n...\n13\n}\nAs you probably ﬁgured out, the code in Thread 2 seems to assume\nthat the variable mThread has already been initialized (and is not NULL);\nhowever, if Thread 1 does not happen to run ﬁrst, we are out of luck, and\nThread 2 will likely crash with a NULL pointer dereference (assuming\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 1927,
      "extraction_method": "Direct"
    },
    {
      "page_number": 398,
      "chapter": null,
      "content": "362\nCOMMON CONCURRENCY PROBLEMS\nthat the value of mThread is initially NULL; if not, even stranger things\ncould happen as arbitrary memory locations are read through the deref-\nerence in Thread 2).\nThe more formal deﬁnition of an order violation is this: “The desired\norder between two (groups of) memory accesses is ﬂipped (i.e., A should\nalways be executed before B, but the order is not enforced during execu-\ntion).” [L+08]\nThe ﬁx to this type of bug is generally to enforce ordering. As we\ndiscussed in detail previously, using condition variables is an easy and\nrobust way to add this style of synchronization into modern code bases.\nIn the example above, we could thus rewrite the code as follows:\n1\npthread_mutex_t mtLock = PTHREAD_MUTEX_INITIALIZER;\n2\npthread_cond_t\nmtCond = PTHREAD_COND_INITIALIZER;\n3\nint mtInit\n= 0;\n4\n5\nThread 1::\n6\nvoid init() {\n7\n...\n8\nmThread = PR_CreateThread(mMain, ...);\n9\n10\n// signal that the thread has been created...\n11\npthread_mutex_lock(&mtLock);\n12\nmtInit = 1;\n13\npthread_cond_signal(&mtCond);\n14\npthread_mutex_unlock(&mtLock);\n15\n...\n16\n}\n17\n18\nThread 2::\n19\nvoid mMain(...) {\n20\n...\n21\n// wait for the thread to be initialized...\n22\npthread_mutex_lock(&mtLock);\n23\nwhile (mtInit == 0)\n24\npthread_cond_wait(&mtCond, &mtLock);\n25\npthread_mutex_unlock(&mtLock);\n26\n27\nmState = mThread->State;\n28\n...\n29\n}\nIn this ﬁxed-up code sequence, we have added a lock (mtLock) and\ncorresponding condition variable (mtCond), as well as a state variable\n(mtInit). When the initialization code runs, it sets the state of mtInit\nto 1 and signals that it has done so. If Thread 2 had run before this point,\nit will be waiting for this signal and corresponding state change; if it runs\nlater, it will check the state and see that the initialization has already oc-\ncurred (i.e., mtInit is set to 1), and thus continue as is proper. Note that\nwe could likely use mThread as the state variable itself, but do not do so\nfor the sake of simplicity here. When ordering matters between threads,\ncondition variables (or semaphores) can come to the rescue.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2122,
      "extraction_method": "Direct"
    },
    {
      "page_number": 399,
      "chapter": null,
      "content": "COMMON CONCURRENCY PROBLEMS\n363\nNon-Deadlock Bugs: Summary\nA large fraction (97%) of non-deadlock bugs studied by Lu et al. are either\natomicity or order violations. Thus, by carefully thinking about these\ntypes of bug patterns, programmers can likely do a better job of avoiding\nthem. Moreover, as more automated code-checking tools develop, they\nshould likely focus on these two types of bugs as they constitute such a\nlarge fraction of non-deadlock bugs found in deployment.\nUnfortunately, not all bugs are as easily ﬁxable as the examples we\nlooked at above. Some require a deeper understanding of what the pro-\ngram is doing, or a larger amount of code or data structure reorganization\nto ﬁx. Read Lu et al.’s excellent (and readable) paper for more details.\n32.3\nDeadlock Bugs\nBeyond the concurrency bugs mentioned above, a classic problem that\narises in many concurrent systems with complex locking protocols is known\nas deadlock. Deadlock occurs, for example, when a thread (say Thread\n1) is holding a lock (L1) and waiting for another one (L2); unfortunately,\nthe thread (Thread 2) that holds lock L2 is waiting for L1 to be released.\nHere is a code snippet that demonstrates such a potential deadlock:\nThread 1:\nThread 2:\nlock(L1);\nlock(L2);\nlock(L2);\nlock(L1);\nNote that if this code runs, deadlock does not necessarily occur; rather,\nit may occur, if, for example, Thread 1 grabs lock L1 and then a context\nswitch occurs to Thread 2. At that point, Thread 2 grabs L2, and tries to\nacquire L1. Thus we have a deadlock, as each thread is waiting for the\nother and neither can run. See Figure 32.1 for details; the presence of a\ncycle in the graph is indicative of the deadlock.\nThe ﬁgure should make clear the problem. How should programmers\nwrite code so as to handle deadlock in some way?\nCRUX: HOW TO DEAL WITH DEADLOCK\nHow should we build systems to prevent, avoid, or at least detect and\nrecover from deadlock? Is this a real problem in systems today?\nWhy Do Deadlocks Occur?\nAs you may be thinking, simple deadlocks such as the one above seem\nreadily avoidable. For example, if Thread 1 and 2 both made sure to grab\nlocks in the same order, the deadlock would never arise. So why do dead-\nlocks happen?\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2260,
      "extraction_method": "Direct"
    },
    {
      "page_number": 400,
      "chapter": null,
      "content": "364\nCOMMON CONCURRENCY PROBLEMS\nThread 1\nThread 2\nLock L1\nLock L2\nHolds\nHolds\nWanted by\nWanted by\nFigure 32.1: The Deadlock Dependency Graph\nOne reason is that in large code bases, complex dependencies arise\nbetween components. Take the operating system, for example. The vir-\ntual memory system might need to access the ﬁle system in order to page\nin a block from disk; the ﬁle system might subsequently require a page\nof memory to read the block into and thus contact the virtual memory\nsystem. Thus, the design of locking strategies in large systems must be\ncarefully done to avoid deadlock in the case of circular dependencies that\nmay occur naturally in the code.\nAnother reason is due to the nature of encapsulation. As software de-\nvelopers, we are taught to hide details of implementations and thus make\nsoftware easier to build in a modular way. Unfortunately, such modular-\nity does not mesh well with locking. As Jula et al. point out [J+08], some\nseemingly innocuous interfaces almost invite you to deadlock. For exam-\nple, take the Java Vector class and the method AddAll(). This routine\nwould be called as follows:\nVector v1, v2;\nv1.AddAll(v2);\nInternally, because the method needs to be multi-thread safe, locks for\nboth the vector being added to (v1) and the parameter (v2) need to be\nacquired. The routine acquires said locks in some arbitrary order (say v1\nthen v2) in order to add the contents of v2 to v1. If some other thread\ncalls v2.AddAll(v1) at nearly the same time, we have the potential for\ndeadlock, all in a way that is quite hidden from the calling application.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1639,
      "extraction_method": "Direct"
    },
    {
      "page_number": 401,
      "chapter": null,
      "content": "COMMON CONCURRENCY PROBLEMS\n365\nConditions for Deadlock\nFour conditions need to hold for a deadlock to occur [C+71]:\n• Mutual exclusion: Threads claim exclusive control of resources that\nthey require (e.g., a thread grabs a lock).\n• Hold-and-wait: Threads hold resources allocated to them (e.g., locks\nthat they have already acquired) while waiting for additional re-\nsources (e.g., locks that they wish to acquire).\n• No preemption: Resources (e.g., locks) cannot be forcibly removed\nfrom threads that are holding them.\n• Circular wait: There exists a circular chain of threads such that\neach thread holds one more resources (e.g., locks) that are being\nrequested by the next thread in the chain.\nIf any of these four conditions are not met, deadlock cannot occur.\nThus, we ﬁrst explore techniques to prevent deadlock; each of these strate-\ngies seeks to prevent one of the above conditions from arising and thus is\none approach to handling the deadlock problem.\nPrevention\nCircular Wait\nProbably the most practical prevention technique (and certainly one that\nis used frequently) is to write your locking code such that you never in-\nduce a circular wait. The way to do that is to provide a total ordering on\nlock acquisition. For example, if there are only two locks in the system (L1\nand L2), we can prevent deadlock by always acquiring L1 before L2. Such\nstrict ordering ensures that no cyclical wait arises; hence, no deadlock.\nAs you can imagine, this approach requires careful design of global\nlocking strategies and must be done with great care. Further, it is just a\nconvention, and a sloppy programmer can easily ignore the locking pro-\ntocol and potentially cause deadlock. Finally, it requires a deep under-\nstanding of the code base, and how various routines are called; just one\nmistake could result in the wrong ordering of lock acquisition, and hence\ndeadlock.\nHold-and-wait\nThe hold-and-wait requirement for deadlock can be avoided by acquiring\nall locks at once, atomically. In practice, this could be achieved as follows:\n1\nlock(prevention);\n2\nlock(L1);\n3\nlock(L2);\n4\n...\n5\nunlock(prevention);\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2155,
      "extraction_method": "Direct"
    },
    {
      "page_number": 402,
      "chapter": null,
      "content": "366\nCOMMON CONCURRENCY PROBLEMS\nBy ﬁrst grabbing the lock prevention, this code guarantees that no\nuntimely thread switch can occur in the midst of lock acquisition and thus\ndeadlock can once again be avoided. Of course, it requires that any time\nany thread grabs a lock, it ﬁrst acquires the global prevention lock. For\nexample, if another thread was trying to grab locks L1 and L2 in a dif-\nferent order, it would be OK, because it would be holding the prevention\nlock while doing so.\nNote that the solution is problematic for a number of reasons. As be-\nfore, encapsulation works against us: this approach requires us to know\nwhen calling a routine exactly which locks must be held and to acquire\nthem ahead of time. Further, the approach likely decreases concurrency\nas all locks must be acquired early on (at once) instead of when they are\ntruly needed.\nNo Preemption\nBecause we generally view locks as held until unlock is called, multiple\nlock acquisition often gets us into trouble because when waiting for one\nlock we are holding another. Many thread libraries provide a more ﬂexi-\nble set of interfaces to help avoid this situation. Speciﬁcally, a trylock()\nroutine will grab the lock (if it is available) or return -1 indicating that the\nlock is held right now and that you should try again later if you want to\ngrab that lock.\nSuch an interface could be used as follows to build a deadlock-free,\nordering-robust lock acquisition protocol:\n1\ntop:\n2\nlock(L1);\n3\nif (trylock(L2) == -1) {\n4\nunlock(L1);\n5\ngoto top;\n6\n}\nNote that another thread could follow the same protocol but grab the\nlocks in the other order (L2 then L1) and the program would still be dead-\nlock free. One new problem does arise, however: livelock. It is possible\n(though perhaps unlikely) that two threads could both be repeatedly at-\ntempting this sequence and repeatedly failing to acquire both locks. In\nthis case, both systems are running through this code sequence over and\nover again (and thus it is not a deadlock), but progress is not being made,\nhence the name livelock. There are solutions to the livelock problem, too:\nfor example, one could add a random delay before looping back and try-\ning the entire thing over again, thus decreasing the odds of repeated in-\nterference among competing threads.\nOne ﬁnal point about this solution: it skirts around the hard parts of\nusing a trylock approach. The ﬁrst problem that would likely exist again\narises due to encapsulation: if one of these locks is buried in some routine\nthat is getting called, the jump back to the beginning becomes more com-\nplex to implement. If the code had acquired some resources (other than\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2704,
      "extraction_method": "Direct"
    },
    {
      "page_number": 403,
      "chapter": null,
      "content": "COMMON CONCURRENCY PROBLEMS\n367\nL1) along the way, it must make sure to carefully release them as well;\nfor example, if after acquiring L1, the code had allocated some memory,\nit would have to release that memory upon failure to acquire L2, before\njumping back to the top to try the entire sequence again. However, in\nlimited circumstances (e.g., the Java vector method above), this type of\napproach could work well.\nMutual Exclusion\nThe ﬁnal prevention technique would be to avoid the need for mutual\nexclusion at all. In general, we know this is difﬁcult, because the code we\nwish to run does indeed have critical sections. So what can we do?\nHerlihy had the idea that one could design various data structures to\nbe wait-free [H91]. The idea here is simple: using powerful hardware in-\nstructions, you can build data structures in a manner that does not require\nexplicit locking.\nAs a simple example, let us assume we have a compare-and-swap in-\nstruction, which as you may recall is an atomic instruction provided by\nthe hardware that does the following:\n1\nint CompareAndSwap(int *address, int expected, int new) {\n2\nif (*address == expected) {\n3\n*address = new;\n4\nreturn 1; // success\n5\n}\n6\nreturn 0; // failure\n7\n}\nImagine we now wanted to atomically increment a value by a certain\namount. We could do it as follows:\n1\nvoid AtomicIncrement(int *value, int amount) {\n2\ndo {\n3\nint old = *value;\n4\n} while (CompareAndSwap(value, old, old + amount) == 0);\n5\n}\nInstead of acquiring a lock, doing the update, and then releasing it, we\nhave instead built an approach that repeatedly tries to update the value to\nthe new amount and uses the compare-and-swap to do so. In this manner,\nno lock is acquired, and no deadlock can arise (though livelock is still a\npossibility).\nLet us consider a slightly more complex example: list insertion. Here\nis code that inserts at the head of a list:\n1\nvoid insert(int value) {\n2\nnode_t *n = malloc(sizeof(node_t));\n3\nassert(n != NULL);\n4\nn->value = value;\n5\nn->next\n= head;\n6\nhead\n= n;\n7\n}\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2065,
      "extraction_method": "Direct"
    },
    {
      "page_number": 404,
      "chapter": null,
      "content": "368\nCOMMON CONCURRENCY PROBLEMS\nThis code performs a simple insertion, but if called by multiple threads\nat the “same time”, has a race condition (see if you can ﬁgure out why). Of\ncourse, we could solve this by surrounding this code with a lock acquire\nand release:\n1\nvoid insert(int value) {\n2\nnode_t *n = malloc(sizeof(node_t));\n3\nassert(n != NULL);\n4\nn->value = value;\n5\nlock(listlock);\n// begin critical section\n6\nn->next\n= head;\n7\nhead\n= n;\n8\nunlock(listlock); // end of critical section\n9\n}\nIn this solution, we are using locks in the traditional manner1. Instead,\nlet us try to perform this insertion in a wait-free manner simply using the\ncompare-and-swap instruction. Here is one possible approach:\n1\nvoid insert(int value) {\n2\nnode_t *n = malloc(sizeof(node_t));\n3\nassert(n != NULL);\n4\nn->value = value;\n5\ndo {\n6\nn->next = head;\n7\n} while (CompareAndSwap(&head, n->next, n));\n8\n}\nThe code here updates the next pointer to point to the current head,\nand then tries to swap the newly-created node into position as the new\nhead of the list. However, this will fail if some other thread successfully\nswapped in a new head in the meanwhile, causing this thread to retry\nagain with the new head.\nOf course, building a useful list requires more than just a list insert,\nand not surprisingly building a list that you can insert into, delete from,\nand perform lookups on in a wait-free manner is non-trivial. Read the\nrich literature on wait-free synchronization if you ﬁnd this interesting.\nDeadlock Avoidance via Scheduling\nInstead of deadlock prevention, in some scenarios deadlock avoidance\nis preferable. Avoidance requires some global knowledge of which locks\nvarious threads might grab during their execution, and subsequently sched-\nules said threads in a way as to guarantee no deadlock can occur.\nFor example, assume we have two processors and four threads which\nmust be scheduled upon them. Assume further we know that Thread\n1 (T1) grabs locks L1 and L2 (in some order, at some point during its\nexecution), T2 grabs L1 and L2 as well, T3 grabs just L2, and T4 grabs no\n1The astute reader might be asking why we grabbed the lock so late, instead of right when\nentering the insert() routine; can you, astute reader, ﬁgure out why that is likely OK?\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2308,
      "extraction_method": "Direct"
    },
    {
      "page_number": 405,
      "chapter": null,
      "content": "COMMON CONCURRENCY PROBLEMS\n369\nlocks at all. We can show these lock acquisition demands of the threads\nin tabular form:\nT1\nT2\nT3\nT4\nL1\nyes\nyes\nno\nno\nL2\nyes\nyes\nyes\nno\nA smart scheduler could thus compute that as long as T1 and T2 are\nnot run at the same time, no deadlock could ever arise. Here is one such\nschedule:\nCPU 1\nCPU 2\nT1\nT2\nT3\nT4\nNote that it is OK for (T3 and T1) or (T3 and T2) to overlap. Even\nthough T3 grabs lock L2, it can never cause a deadlock by running con-\ncurrently with other threads because it only grabs one lock.\nLet’s look at one more example. In this one, there is more contention\nfor the same resources (again, locks L1 and L2), as indicated by the fol-\nlowing contention table:\nT1\nT2\nT3\nT4\nL1\nyes\nyes\nyes\nno\nL2\nyes\nyes\nyes\nno\nIn particular, threads T1, T2, and T3 all need to grab both locks L1 and\nL2 at some point during their execution. Here is a possible schedule that\nguarantees that no deadlock could ever occur:\nCPU 1\nCPU 2\nT1\nT2\nT3\nT4\nAs you can see, static scheduling leads to a conservative approach\nwhere T1, T2, and T3 are all run on the same processor, and thus the\ntotal time to complete the jobs is lengthened considerably. Though it may\nhave been possible to run these tasks concurrently, the fear of deadlock\nprevents us from doing so, and the cost is performance.\nOne famous example of an approach like this is Dijkstra’s Banker’s Al-\ngorithm [D64], and many similar approaches have been described in the\nliterature. Unfortunately, they are only useful in very limited environ-\nments, for example, in an embedded system where one has full knowl-\nedge of the entire set of tasks that must be run and the locks that they\nneed. Further, such approaches can limit concurrency, as we saw in the\nsecond example above. Thus, avoidance of deadlock via scheduling is\nnot a widely-used general-purpose solution.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 1893,
      "extraction_method": "Direct"
    },
    {
      "page_number": 406,
      "chapter": null,
      "content": "370\nCOMMON CONCURRENCY PROBLEMS\nTIP: DON’T ALWAYS DO IT PERFECTLY (TOM WEST’S LAW)\nTom West, famous as the subject of the classic computer-industry book\n“Soul of a New Machine” [K81], says famously: “Not everything worth\ndoing is worth doing well”, which is a terriﬁc engineering maxim. If a\nbad thing happens rarely, certainly one should not spend a great deal of\neffort to prevent it, particularly if the cost of the bad thing occurring is\nsmall.\nDetect and Recover\nOne ﬁnal general strategy is to allow deadlocks to occasionally occur, and\nthen take some action once such a deadlock has been detected. For exam-\nple, if an OS froze once a year, you would just reboot it and get happily (or\ngrumpily) on with your work. If deadlocks are rare, such a non-solution\nis indeed quite pragmatic.\nMany database systems employ deadlock detection and recovery tech-\nniques. A deadlock detector runs periodically, building a resource graph\nand checking it for cycles. In the event of a cycle (deadlock), the system\nneeds to be restarted. If more intricate repair of data structures is ﬁrst\nrequired, a human being may be involved to ease the process.\n32.4\nSummary\nIn this chapter, we have studied the types of bugs that occur in con-\ncurrent programs. The ﬁrst type, non-deadlock bugs, are surprisingly\ncommon, but often are easier to ﬁx. They include atomicity violations,\nin which a sequence of instructions that should have been executed to-\ngether was not, and order violations, in which the needed order between\ntwo threads was not enforced.\nWe have also brieﬂy discussed deadlock: why it occurs, and what can\nbe done about it. The problem is as old as concurrency itself, and many\nhundreds of papers have been written about the topic. The best solu-\ntion in practice is to be careful, develop a lock acquisition total order,\nand thus prevent deadlock from occurring in the ﬁrst place. Wait-free\napproaches also have promise, as some wait-free data structures are now\nﬁnding their way into commonly-used libraries and critical systems, in-\ncluding Linux. However, their lack of generality and the complexity to\ndevelop a new wait-free data structure will likely limit the overall util-\nity of this approach. Perhaps the best solution is to develop new concur-\nrent programming models: in systems such as MapReduce (from Google)\n[GD02], programmers can describe certain types of parallel computations\nwithout any locks whatsoever. Locks are problematic by their very na-\nture; perhaps we should seek to avoid using them unless we truly must.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2585,
      "extraction_method": "Direct"
    },
    {
      "page_number": 407,
      "chapter": null,
      "content": "COMMON CONCURRENCY PROBLEMS\n371\nReferences\n[C+71] “System Deadlocks”\nE.G. Coffman, M.J. Elphick, A. Shoshani\nACM Computing Surveys, 3:2, June 1971\nThe classic paper outlining the conditions for deadlock and how you might go about dealing with it.\nThere are certainly some earlier papers on this topic; see the references within this paper for details.\n[D64] “Een algorithme ter voorkoming van de dodelijke omarming”\nCirculated privately, around 1964\nAvailable: http://www.cs.utexas.edu/users/EWD/ewd01xx/EWD108.PDF\nIndeed, not only did Dijkstra come up with a number of solutions to the deadlock problem, he was the\nﬁrst to note its existence, at least in written form. However, he called it the “deadly embrace”, which\n(thankfully) did not catch on.\n[GD02] “MapReduce: Simpliﬁed Data Processing on Large Clusters”\nSanjay Ghemawhat and Jeff Dean\nOSDI ’04, San Francisco, CA, October 2004\nThe MapReduce paper ushered in the era of large-scale data processing, and proposes a framework for\nperforming such computations on clusters of generally unreliable machines.\n[H91] “Wait-free Synchronization”\nMaurice Herlihy\nACM TOPLAS, 13(1), pages 124-149, January 1991\nHerlihy’s work pioneers the ideas behind wait-free approaches to writing concurrent programs. These\napproaches tend to be complex and hard, often more difﬁcult than using locks correctly, probably limiting\ntheir success in the real world.\n[J+08] “Deadlock Immunity: Enabling Systems To Defend Against Deadlocks”\nHoratiu Jula, Daniel Tralamazza, Cristian Zamﬁr, George Candea\nOSDI ’08, San Diego, CA, December 2008\nAn excellent recent paper on deadlocks and how to avoid getting caught in the same ones over and over\nagain in a particular system.\n[K81] “Soul of a New Machine”\nTracy Kidder, 1980\nA must-read for any systems builder or engineer, detailing the early days of how a team inside Data\nGeneral (DG), led by Tom West, worked to produce a “new machine.” Kidder’s other book are also\nexcellent, in particular, “Mountains beyond Mountains”. Or maybe you don’t agree with me, comma?\n[L+08] “Learning from Mistakes – A Comprehensive Study on\nReal World Concurrency Bug Characteristics”\nShan Lu, Soyeon Park, Eunsoo Seo, Yuanyuan Zhou\nASPLOS ’08, March 2008, Seattle, Washington\nThe ﬁrst in-depth study of concurrency bugs in real software, and the basis for this chapter. Look at Y.Y.\nZhou’s or Shan Lu’s web pages for many more interesting papers on bugs.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2461,
      "extraction_method": "Direct"
    },
    {
      "page_number": 408,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 409,
      "chapter": null,
      "content": "33\nEvent-based Concurrency (Advanced)\nThus far, we’ve written about concurrency as if the only way to build\nconcurrent applications is to use threads. Like many things in life, this\nis not completely true. Speciﬁcally, a different style of concurrent pro-\ngramming is often used in both GUI-based applications [O96] as well as\nsome types of internet servers [PDZ99]. This style, known as event-based\nconcurrency, has become popular in some modern systems, including\nserver-side frameworks such as node.js [N13], but its roots are found in\nC/UNIX systems that we’ll discuss below.\nThe problem that event-based concurrency addresses is two-fold. The\nﬁrst is that managing concurrency correctly in multi-threaded applica-\ntions can be challenging; as we’ve discussed, missing locks, deadlock,\nand other nasty problems can arise. The second is that in a multi-threaded\napplication, the developer has little or no control over what is scheduled\nat a given moment in time; rather, the programmer simply creates threads\nand then hopes that the underlying OS schedules them in a reasonable\nmanner across available CPUs. Given the difﬁculty of building a general-\npurpose scheduler that works well in all cases for all workloads, some-\ntimes the OS will schedule work in a manner that is less than optimal.\nThe crux:\nTHE CRUX:\nHOW TO BUILD CONCURRENT SERVERS WITHOUT THREADS\nHow can we build a concurrent server without using threads, and thus\nretain control over concurrency as well as avoid some of the problems\nthat seem to plague multi-threaded applications?\n33.1\nThe Basic Idea: An Event Loop\nThe basic approach we’ll use, as stated above, is called event-based\nconcurrency. The approach is quite simple: you simply wait for some-\nthing (i.e., an “event”) to occur; when it does, you check what type of\n373\n",
      "content_length": 1803,
      "extraction_method": "Direct"
    },
    {
      "page_number": 410,
      "chapter": null,
      "content": "374\nEVENT-BASED CONCURRENCY (ADVANCED)\nevent it is and do the small amount of work it requires (which may in-\nclude issuing I/O requests, or scheduling other events for future han-\ndling, etc.). That’s it!\nBefore getting into the details, let’s ﬁrst examine what a canonical\nevent-based server looks like. Such applications are based around a sim-\nple construct known as the event loop. Pseudocode for an event loop\nlooks like this:\nwhile (1) {\nevents = getEvents();\nfor (e in events)\nprocessEvent(e);\n}\nIt’s really that simple. The main loop simply waits for something to do\n(by calling getEvents() in the code above) and then, for each event re-\nturned, processes them, one at a time; the code that processes each event\nis known as an event handler. Importantly, when a handler processes\nan event, it is the only activity taking place in the system; thus, deciding\nwhich event to handle next is equivalent to scheduling. This explicit con-\ntrol over scheduling is one of the fundamental advantages of the event-\nbased approach.\nBut this discussion leaves us with a bigger question: how exactly does\nan event-based server determine which events are taking place, in par-\nticular with regards to network and disk I/O? Speciﬁcally, how can an\nevent server tell if a message has arrived for it?\n33.2\nAn Important API: select() (or poll())\nWith that basic event loop in mind, we next must address the question\nof how to receive events. In most systems, a basic API is available, via\neither the select() or poll() system calls.\nWhat these interfaces enable a program to do is simple: check whether\nthere is any incoming I/O that should be attended to. For example, imag-\nine that a network application (such as a web server) wishes to check\nwhether any network packets have arrived, in order to service them.\nThese system calls let you do exactly that.\nTake select() for example. The manual page (on Mac OS X) de-\nscribes the API in this manner:\nint select(int nfds,\nfd_set *restrict readfds,\nfd_set *restrict writefds,\nfd_set *restrict errorfds,\nstruct timeval *restrict timeout);\nThe actual description from the man page: select() examines the I/O de-\nscriptor sets whose addresses are passed in readfds, writefds, and errorfds to see\nif some of their descriptors are ready for reading, are ready for writing, or have\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2363,
      "extraction_method": "Direct"
    },
    {
      "page_number": 411,
      "chapter": null,
      "content": "EVENT-BASED CONCURRENCY (ADVANCED)\n375\nASIDE: BLOCKING VS. NON-BLOCKING INTERFACES\nBlocking (or synchronous) interfaces do all of their work before returning\nto the caller; non-blocking (or asynchronous) interfaces begin some work\nbut return immediately, thus letting whatever work that needs to be done\nget done in the background.\nThe usual culprit in blocking calls is I/O of some kind. For example, if a\ncall must read from disk in order to complete, it might block, waiting for\nthe I/O request that has been sent to the disk to return.\nNon-blocking interfaces can be used in any style of programming (e.g.,\nwith threads), but are essential in the event-based approach, as a call that\nblocks will halt all progress.\nan exceptional condition pending, respectively. The ﬁrst nfds descriptors are\nchecked in each set, i.e., the descriptors from 0 through nfds-1 in the descriptor\nsets are examined. On return, select() replaces the given descriptor sets with\nsubsets consisting of those descriptors that are ready for the requested operation.\nselect() returns the total number of ready descriptors in all the sets.\nA couple of points about select(). First, note that it lets you check\nwhether descriptors can be read from as well as written to; the former\nlets a server determine that a new packet has arrived and is in need of\nprocessing, whereas the latter lets the service know when it is OK to reply\n(i.e., the outbound queue is not full).\nSecond, note the timeout argument. One common usage here is to\nset the timeout to NULL, which causes select() to block indeﬁnitely,\nuntil some descriptor is ready. However, more robust servers will usually\nspecify some kind of timeout; one common technique is to set the timeout\nto zero, and thus use the call to select() to return immediately.\nThe poll() system call is quite similar. See its manual page, or Stevens\nand Rago [SR05], for details.\nEither way, these basic primitives give us a way to build a non-blocking\nevent loop, which simply checks for incoming packets, reads from sockets\nwith messages upon them, and replies as needed.\n33.3\nUsing select()\nTo make this more concrete, let’s examine how to use select() to see\nwhich network descriptors have incoming messages upon them. Figure\n33.1 shows a simple example.\nThis code is actually fairly simple to understand. After some initial-\nization, the server enters an inﬁnite loop. Inside the loop, it uses the\nFD ZERO() macro to ﬁrst clear the set of ﬁle descriptors, and then uses\nFD SET() to include all of the ﬁle descriptors from minFD to maxFD in\nthe set. This set of descriptors might represent, for example, all of the net-\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2677,
      "extraction_method": "Direct"
    },
    {
      "page_number": 412,
      "chapter": null,
      "content": "376\nEVENT-BASED CONCURRENCY (ADVANCED)\n1\n#include <stdio.h>\n2\n#include <stdlib.h>\n3\n#include <sys/time.h>\n4\n#include <sys/types.h>\n5\n#include <unistd.h>\n6\n7\nint main(void) {\n8\n// open and set up a bunch of sockets (not shown)\n9\n// main loop\n10\nwhile (1) {\n11\n// initialize the fd_set to all zero\n12\nfd_set readFDs;\n13\nFD_ZERO(&readFDs);\n14\n15\n// now set the bits for the descriptors\n16\n// this server is interested in\n17\n// (for simplicity, all of them from min to max)\n18\nint fd;\n19\nfor (fd = minFD; fd < maxFD; fd++)\n20\nFD_SET(fd, &readFDs);\n21\n22\n// do the select\n23\nint rc = select(maxFD+1, &readFDs, NULL, NULL, NULL);\n24\n25\n// check which actually have data using FD_ISSET()\n26\nint fd;\n27\nfor (fd = minFD; fd < maxFD; fd++)\n28\nif (FD_ISSET(fd, &readFDs))\n29\nprocessFD(fd);\n30\n}\n31\n}\nFigure 33.1: Simple Code using select()\nwork sockets to which the server is paying attention. Finally, the server\ncalls select() to see which of the connections have data available upon\nthem. By then using FD ISSET() in a loop, the event server can see\nwhich of the descriptors have data ready and process the incoming data.\nOf course, a real server would be more complicated than this, and\nrequire logic to use when sending messages, issuing disk I/O, and many\nother details. For further information, see Stevens and Rago [SR05] for\nAPI information, or Pai et. al or Welsh et al. for a good overview of the\ngeneral ﬂow of event-based servers [PDZ99, WCB01].\n33.4\nWhy Simpler? No Locks Needed\nWith a single CPU and an event-based application, the problems found\nin concurrent programs are no longer present. Speciﬁcally, because only\none event is being handled at a time, there is no need to acquire or release\nlocks; the event-based server cannot be interrupted by another thread be-\ncause it is decidedly single threaded. Thus, concurrency bugs common in\nthreaded programs do not manifest in the basic event-based approach.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1962,
      "extraction_method": "Direct"
    },
    {
      "page_number": 413,
      "chapter": null,
      "content": "EVENT-BASED CONCURRENCY (ADVANCED)\n377\nTIP: DON’T BLOCK IN EVENT-BASED SERVERS\nEvent-based servers enable ﬁne-grained control over scheduling of tasks.\nHowever, to maintain such control, no call that blocks the execution the\ncaller can ever be made; failing to obey this design tip will result in a\nblocked event-based server, frustrated clients, and serious questions as to\nwhether you ever read this part of the book.\n33.5\nA Problem: Blocking System Calls\nThus far, event-based programming sounds great, right? You program\na simple loop, and handle events as they arise. You don’t even need to\nthink about locking! But there is an issue: what if an event requires that\nyou issue a system call that might block?\nFor example, imagine a request comes from a client into a server to\nread a ﬁle from disk and return its contents to the requesting client (much\nlike a simple HTTP request). To service such a request, some event han-\ndler will eventually have to issue an open() system call to open the ﬁle,\nfollowed by a series of read() calls to read the ﬁle. When the ﬁle is read\ninto memory, the server will likely start sending the results to the client.\nBoth the open() and read() calls may issue I/O requests to the stor-\nage system (when the needed metadata or data is not in memory already),\nand thus may take a long time to service. With a thread-based server, this\nis no issue: while the thread issuing the I/O request suspends (waiting\nfor the I/O to complete), other threads can run, thus enabling the server\nto make progress. Indeed, this natural overlap of I/O and other computa-\ntion is what makes thread-based programming quite natural and straight-\nforward.\nWith an event-based approach, however, there are no other threads to\nrun: just the main event loop. And this implies that if an event handler\nissues a call that blocks, the entire server will do just that: block until the\ncall completes. When the event loop blocks, the system sits idle, and thus\nis a huge potential waste of resources. We thus have a rule that must be\nobeyed in event-based systems: no blocking calls are allowed.\n33.6\nA Solution: Asynchronous I/O\nTo overcome this limit, many modern operating systems have intro-\nduced new ways to issue I/O requests to the disk system, referred to\ngenerically as asynchronous I/O. These interfaces enable an application\nto issue an I/O request and return control immediately to the caller, be-\nfore the I/O has completed; additional interfaces enable an application to\ndetermine whether various I/Os have completed.\nFor example, let us examine the interface provided on Mac OS X (other\nsystems have similar APIs). The APIs revolve around a basic structure,\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2722,
      "extraction_method": "Direct"
    },
    {
      "page_number": 414,
      "chapter": null,
      "content": "378\nEVENT-BASED CONCURRENCY (ADVANCED)\nthe struct aiocb or AIO control block in common terminology. A\nsimpliﬁed version of the structure looks like this (see the manual pages\nfor more information):\nstruct aiocb {\nint\naio_fildes;\n/* File descriptor */\noff_t\naio_offset;\n/* File offset */\nvolatile void\n*aio_buf;\n/* Location of buffer */\nsize_t\naio_nbytes;\n/* Length of transfer */\n};\nTo issue an asynchronous read to a ﬁle, an application should ﬁrst\nﬁll in this structure with the relevant information: the ﬁle descriptor of\nthe ﬁle to be read (aio fildes), the offset within the ﬁle (aio offset)\nas well as the length of the request (aio nbytes), and ﬁnally the tar-\nget memory location into which the results of the read should be copied\n(aio buf).\nAfter this structure is ﬁlled in, the application must issue the asyn-\nchronous call to read the ﬁle; on Mac OS X, this API is simply the asyn-\nchronous read API:\nint aio_read(struct aiocb *aiocbp);\nThis call tries to issue the I/O; if successful, it simply returns right\naway and the application (i.e., the event-based server) can continue with\nits work.\nThere is one last piece of the puzzle we must solve, however. How can\nwe tell when an I/O is complete, and thus that the buffer (pointed to by\naio buf) now has the requested data within it?\nOne last API is needed. On Mac OS X, it is referred to (somewhat\nconfusingly) as aio error(). The API looks like this:\nint aio_error(const struct aiocb *aiocbp);\nThis system call checks whether the request referred to by aiocbp has\ncompleted. If it has, the routine returns success (indicated by a zero);\nif not, EINPROGRESS is returned. Thus, for every outstanding asyn-\nchronous I/O, an application can periodically poll the system via a call\nto aio error() to determine whether said I/O has yet completed.\nOne thing you might have noticed is that it is painful to check whether\nan I/O has completed; if a program has tens or hundreds of I/Os issued\nat a given point in time, should it simply keep checking each of them\nrepeatedly, or wait a little while ﬁrst, or ... ?\nTo remedy this issue, some systems provide an approach based on the\ninterrupt. This method uses UNIX signals to inform applications when\nan asynchronous I/O completes, thus removing the need to repeatedly\nask the system. This polling vs. interrupts issue is seen in devices too, as\nyou will see (or already have seen) in the chapter on I/O devices.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2465,
      "extraction_method": "Direct"
    },
    {
      "page_number": 415,
      "chapter": null,
      "content": "EVENT-BASED CONCURRENCY (ADVANCED)\n379\nASIDE: UNIX SIGNALS\nA huge and fascinating infrastructure known as signals is present in all mod-\nern UNIX variants. At its simplest, signals provide a way to communicate with a\nprocess. Speciﬁcally, a signal can be delivered to an application; doing so stops the\napplication from whatever it is doing to run a signal handler, i.e., some code in\nthe application to handle that signal. When ﬁnished, the process just resumes its\nprevious behavior.\nEach signal has a name, such as HUP (hang up), INT (interrupt), SEGV (seg-\nmentation violation), etc; see the manual page for details. Interestingly, sometimes\nit is the kernel itself that does the signaling. For example, when your program en-\ncounters a segmentation violation, the OS sends it a SIGSEGV (prepending SIG\nto signal names is common); if your program is conﬁgured to catch that signal,\nyou can actually run some code in response to this erroneous program behavior\n(which can be useful for debugging). When a signal is sent to a process not conﬁg-\nured to handle that signal, some default behavior is enacted; for SEGV, the process\nis killed.\nHere is a simple program that goes into an inﬁnite loop, but has ﬁrst set up a\nsignal handler to catch SIGHUP:\n#include <stdio.h>\n#include <signal.h>\nvoid handle(int arg) {\nprintf(\"stop wakin’ me up...\\n\");\n}\nint main(int argc, char *argv[]) {\nsignal(SIGHUP, handle);\nwhile (1)\n; // doin’ nothin’ except catchin’ some sigs\nreturn 0;\n}\nYou can send signals to it with the kill command line tool (yes, this is an odd\nand aggressive name). Doing so will interrupt the main while loop in the program\nand run the handler code handle():\nprompt> ./main &\n[3] 36705\nprompt> kill -HUP 36705\nstop wakin’ me up...\nprompt> kill -HUP 36705\nstop wakin’ me up...\nprompt> kill -HUP 36705\nstop wakin’ me up...\nThere is a lot more to learn about signals, so much that a single page, much\nless a single chapter, does not nearly sufﬁce. As always, there is one great source:\nStevens and Rago [SR05]. Read more if interested.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2088,
      "extraction_method": "Direct"
    },
    {
      "page_number": 416,
      "chapter": null,
      "content": "380\nEVENT-BASED CONCURRENCY (ADVANCED)\nIn systems without asynchronous I/O, the pure event-based approach\ncannot be implemented. However, clever researchers have derived meth-\nods that work fairly well in their place. For example, Pai et al. [PDZ99]\ndescribe a hybrid approach in which events are used to process network\npackets, and a thread pool is used to manage outstanding I/Os. Read\ntheir paper for details.\n33.7\nAnother Problem: State Management\nAnother issue with the event-based approach is that such code is gen-\nerally more complicated to write than traditional thread-based code. The\nreason is as follows: when an event handler issues an asynchronous I/O,\nit must package up some program state for the next event handler to use\nwhen the I/O ﬁnally completes; this additional work is not needed in\nthread-based programs, as the state the program needs is on the stack of\nthe thread. Adya et al. call this work manual stack management, and it\nis fundamental to event-based programming [A+02].\nTo make this point more concrete, let’s look at a simple example in\nwhich a thread-based server needs to read from a ﬁle descriptor (fd) and,\nonce complete, write the data that it read from the ﬁle to a network socket\ndescriptor (sd). The code (ignoring error checking) looks like this:\nint rc = read(fd, buffer, size);\nrc = write(sd, buffer, size);\nAs you can see, in a multi-threaded program, doing this kind of work\nis trivial; when the read() ﬁnally returns, the code immediately knows\nwhich socket to write to because that information is on the stack of the\nthread (in the variable sd).\nIn an event-based system, life is not so easy. To perform the same task,\nwe’d ﬁrst issue the read asynchronously, using the AIO calls described\nabove. Let’s say we then periodically check for completion of the read\nusing the aio error() call; when that call informs us that the read is\ncomplete, how does the event-based server know what to do?\nThe solution, as described by Adya et al. [A+02], is to use an old pro-\ngramming language construct known as a continuation [FHK84]. Though\nit sounds complicated, the idea is rather simple: basically, record the\nneeded information to ﬁnish processing this event in some data struc-\nture; when the event happens (i.e., when the disk I/O completes), look\nup the needed information and process the event.\nIn this speciﬁc case, the solution would be to record the socket de-\nscriptor (sd) in some kind of data structure (e.g., a hash table), indexed\nby the ﬁle descriptor (fd). When the disk I/O completes, the event han-\ndler would use the ﬁle descriptor to look up the continuation, which will\nreturn the value of the socket descriptor to the caller. At this point (ﬁ-\nnally), the server can then do the last bit of work to write the data to the\nsocket.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2839,
      "extraction_method": "Direct"
    },
    {
      "page_number": 417,
      "chapter": null,
      "content": "EVENT-BASED CONCURRENCY (ADVANCED)\n381\n33.8\nWhat Is Still Difﬁcult With Events\nThere are a few other difﬁculties with the event-based approach that\nwe should mention. For example, when systems moved from a single\nCPU to multiple CPUs, some of the simplicity of the event-based ap-\nproach disappeared. Speciﬁcally, in order to utilize more than one CPU,\nthe event server has to run multiple event handlers in parallel; when do-\ning so, the usual synchronization problems (e.g., critical sections) arise,\nand the usual solutions (e.g., locks) must be employed. Thus, on mod-\nern multicore systems, simple event handling without locks is no longer\npossible.\nAnother problem with the event-based approach is that it does not\nintegrate well with certain kinds of systems activity, such as paging. For\nexample, if an event-handler page faults, it will block, and thus the server\nwill not make progress until the page fault completes. Even though the\nserver has been structured to avoid explicit blocking, this type of implicit\nblocking due to page faults is hard to avoid and thus can lead to large\nperformance problems when prevalent.\nA third issue is that event-based code can be hard to manage over time,\nas the exact semantics of various routines changes [A+02]. For example,\nif a routine changes from non-blocking to blocking, the event handler\nthat calls that routine must also change to accommodate its new nature,\nby ripping itself into two pieces. Because blocking is so disastrous for\nevent-based servers, a programmer must always be on the lookout for\nsuch changes in the semantics of the APIs each event uses.\nFinally, though asynchronous disk I/O is now possible on most plat-\nforms, it has taken a long time to get there [PDZ99], and it never quite\nintegrates with asynchronous network I/O in as simple and uniform a\nmanner as you might think. For example, while one would simply like\nto use the select() interface to manage all outstanding I/Os, usually\nsome combination of select() for networking and the AIO calls for\ndisk I/O are required.\n33.9\nSummary\nWe’ve presented a bare bones introduction to a different style of con-\ncurrency based on events. Event-based servers give control of schedul-\ning to the application itself, but do so at some cost in complexity and\ndifﬁculty of integration with other aspects of modern systems (e.g., pag-\ning). Because of these challenges, no single approach has emerged as\nbest; thus, both threads and events are likely to persist as two different\napproaches to the same concurrency problem for many years to come.\nRead some research papers (e.g., [A+02, PDZ99, vB+03, WCB01]) or bet-\nter yet, write some event-based code, to learn more.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2728,
      "extraction_method": "Direct"
    },
    {
      "page_number": 418,
      "chapter": null,
      "content": "382\nEVENT-BASED CONCURRENCY (ADVANCED)\nReferences\n[A+02] “Cooperative Task Management Without Manual Stack Management”\nAtul Adya, Jon Howell, Marvin Theimer, William J. Bolosky, John R. Douceur\nUSENIX ATC ’02, Monterey, CA, June 2002\nThis gem of a paper is the ﬁrst to clearly articulate some of the difﬁculties of event-based concurrency,\nand suggests some simple solutions, as well explores the even crazier idea of combining the two types of\nconcurrency management into a single application!\n[FHK84] “Programming With Continuations”\nDaniel P. Friedman, Christopher T. Haynes, Eugene E. Kohlbecker\nIn Program Transformation and Programming Environments, Springer Verlag, 1984\nThe classic reference to this old idea from the world of programming languages. Now increasingly\npopular in some modern languages.\n[N13] “Node.js Documentation”\nBy the folks who build node.js\nAvailable: http://nodejs.org/api/\nOne of the many cool new frameworks that help you readily build web services and applications. Every\nmodern systems hacker should be proﬁcient in frameworks such as this one (and likely, more than one).\nSpend the time and do some development in one of these worlds and become an expert.\n[O96] “Why Threads Are A Bad Idea (for most purposes)”\nJohn Ousterhout\nInvited Talk at USENIX ’96, San Diego, CA, January 1996\nA great talk about how threads aren’t a great match for GUI-based applications (but the ideas are more\ngeneral). Ousterhout formed many of these opinions while he was developing Tcl/Tk, a cool scripting\nlanguage and toolkit that made it 100x easier to develop GUI-based applications than the state of the\nart at the time. While the Tk GUI toolkit lives on (in Python for example), Tcl seems to be slowly dying\n(unfortunately).\n[PDZ99] “Flash: An Efﬁcient and Portable Web Server”\nVivek S. Pai, Peter Druschel, Willy Zwaenepoel\nUSENIX ’99, Monterey, CA, June 1999\nA pioneering paper on how to structure web servers in the then-burgeoning Internet era. Read it to\nunderstand the basics as well as to see the authors’ ideas on how to build hybrids when support for\nasynchronous I/O is lacking.\n[SR05] “Advanced Programming in the UNIX Environment”\nW. Richard Stevens and Stephen A. Rago\nAddison-Wesley, 2005\nOnce again, we refer to the classic must-have-on-your-bookshelf book of UNIX systems programming.\nIf there is some detail you need to know, it is in here.\n[vB+03] “Capriccio: Scalable Threads for Internet Services”\nRob von Behren, Jeremy Condit, Feng Zhou, George C. Necula, Eric Brewer\nSOSP ’03, Lake George, New York, October 2003\nA paper about how to make threads work at extreme scale; a counter to all the event-based work ongoing\nat the time.\n[WCB01] “SEDA: An Architecture for Well-Conditioned, Scalable Internet Services”\nMatt Welsh, David Culler, and Eric Brewer\nSOSP ’01, Banff, Canada, October 2001\nA nice twist on event-based serving that combines threads, queues, and event-based hanlding into one\nstreamlined whole. Some of these ideas have found their way into the infrastructures of companies such\nas Google, Amazon, and elsewhere.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 3118,
      "extraction_method": "Direct"
    },
    {
      "page_number": 419,
      "chapter": null,
      "content": "34\nSummary Dialogue on Concurrency\nProfessor: So, does your head hurt now?\nStudent: (taking two Motrin tablets) Well, some. It’s hard to think about all the\nways threads can interleave.\nProfessor: Indeed it is. I am always amazed at how so few line of code, when\nconcurrent execution is involved, can become nearly impossible to understand.\nStudent: Me too! It’s kind of embarrassing, as a Computer Scientist, not to be\nable to make sense of ﬁve lines of code.\nProfessor: Oh, don’t feel too badly. If you look through the ﬁrst papers on con-\ncurrent algorithms, they are sometimes wrong! And the authors often professors!\nStudent: (gasps) Professors can be ... umm... wrong?\nProfessor: Yes, it is true. Though don’t tell anybody – it’s one of our trade\nsecrets.\nStudent: I am sworn to secrecy. But if concurrent code is so hard to think about,\nand so hard to get right, how are we supposed to write correct concurrent code?\nProfessor: Well that is the real question, isn’t it? I think it starts with a few\nsimple things. First, keep it simple! Avoid complex interactions between threads,\nand use well-known and tried-and-true ways to manage thread interactions.\nStudent: Like simple locking, and maybe a producer-consumer queue?\nProfessor: Exactly! Those are common paradigms, and you should be able to\nproduce the working solutions given what you’ve learned. Second, only use con-\ncurrency when absolutely needed; avoid it if at all possible. There is nothing\nworse than premature optimization of a program.\nStudent: I see – why add threads if you don’t need them?\nProfessor: Exactly. Third, if you really need parallelism, seek it in other sim-\npliﬁed forms. For example, the Map-Reduce method for writing parallel data\nanalysis code is an excellent example of achieving parallelism without having to\nhandle any of the horriﬁc complexities of locks, condition variables, and the other\nnasty things we’ve talked about.\n383\n",
      "content_length": 1924,
      "extraction_method": "Direct"
    },
    {
      "page_number": 420,
      "chapter": null,
      "content": "384\nSUMMARY DIALOGUE ON CONCURRENCY\nStudent: Map-Reduce, huh? Sounds interesting – I’ll have to read more about\nit on my own.\nProfessor: Good! You should. In the end, you’ll have to do a lot of that, as\nwhat we learn together can only serve as the barest introduction to the wealth of\nknowledge that is out there. Read, read, and read some more! And then try things\nout, write some code, and then write some more too. As Gladwell talks about in\nhis book “Outliers”, you need to put roughly 10,000 hours into something in\norder to become a real expert. You can’t do that all inside of class time!\nStudent: Wow, I’m not sure if that is depressing, or uplifting. But I’ll assume\nthe latter, and get to work! Time to write some more concurrent code...\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 795,
      "extraction_method": "Direct"
    },
    {
      "page_number": 421,
      "chapter": null,
      "content": "Part III\nPersistence\n385\n",
      "content_length": 25,
      "extraction_method": "Direct"
    },
    {
      "page_number": 422,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 423,
      "chapter": null,
      "content": "35\nA Dialogue on Persistence\nProfessor: And thus we reach the third of our four ... err... three pillars of\noperating systems: persistence.\nStudent: Did you say there were three pillars, or four? What is the fourth?\nProfessor: No. Just three, young student, just three. Trying to keep it simple\nhere.\nStudent: OK, ﬁne. But what is persistence, oh ﬁne and noble professor?\nProfessor: Actually, you probably know what it means in the traditional sense,\nright? As the dictionary would say: “a ﬁrm or obstinate continuance in a course\nof action in spite of difﬁculty or opposition.”\nStudent: It’s kind of like taking your class: some obstinance required.\nProfessor: Ha! Yes. But persistence here means something else. Let me explain.\nImagine you are outside, in a ﬁeld, and you pick a –\nStudent: (interrupting) I know! A peach! From a peach tree!\nProfessor: I was going to say apple, from an apple tree. Oh well; we’ll do it your\nway, I guess.\nStudent: (stares blankly)\nProfessor: Anyhow, you pick a peach; in fact, you pick many many peaches,\nbut you want to make them last for a long time. Winter is hard and cruel in\nWisconsin, after all. What do you do?\nStudent: Well, I think there are some different things you can do. You can pickle\nit! Or bake a pie. Or make a jam of some kind. Lots of fun!\nProfessor: Fun? Well, maybe. Certainly, you have to do a lot more work to make\nthe peach persist. And so it is with information as well; making information\npersist, despite computer crashes, disk failures, or power outages is a tough and\ninteresting challenge.\nStudent: Nice segue; you’re getting quite good at that.\nProfessor: Thanks! A professor can always use a few kind words, you know.\n387\n",
      "content_length": 1691,
      "extraction_method": "Direct"
    },
    {
      "page_number": 424,
      "chapter": null,
      "content": "388\nA DIALOGUE ON PERSISTENCE\nStudent: I’ll try to remember that. I guess it’s time to stop talking peaches, and\nstart talking computers?\nProfessor: Yes, it is that time...\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 220,
      "extraction_method": "Direct"
    },
    {
      "page_number": 425,
      "chapter": null,
      "content": "36\nI/O Devices\nBefore delving into the main content of this part of the book (on persis-\ntence), we ﬁrst introduce the concept of an input/output (I/O) device and\nshow how the operating system might interact with such an entity. I/O is\nquite critical to computer systems, of course; imagine a program without\nany input (it produces the same result each time); now imagine a pro-\ngram with no output (what was the purpose of it running?). Clearly, for\ncomputer systems to be interesting, both input and output are required.\nAnd thus, our general problem:\nCRUX: HOW TO INTEGRATE I/O INTO SYSTEMS\nHow should I/O be integrated into systems? What are the general\nmechanisms? How can we make them efﬁcient?\n36.1\nSystem Architecture\nTo begin our discussion, let’s look at the structure of a typical system\n(Figure 36.1). The picture shows a single CPU attached to the main mem-\nory of the system via some kind of memory bus or interconnect. Some\ndevices are connected to the system via a general I/O bus, which in many\nmodern systems would be PCI (or one if its many derivatives); graph-\nics and some other higher-performance I/O devices might be found here.\nFinally, even lower down are one or more of what we call a peripheral\nbus, such as SCSI, SATA, or USB. These connect the slowest devices to\nthe system, including disks, mice, and other similar components.\nOne question you might ask is: why do we need a hierarchical struc-\nture like this? Put simply: physics, and cost. The faster a bus is, the\nshorter it must be; thus, a high-performance memory bus does not have\nmuch room to plug devices and such into it. In addition, engineering\na bus for high performance is quite costly. Thus, system designers have\nadopted this hierarchical approach, where components that demands high\nperformance (such as the graphics card) are nearer the CPU. Lower per-\n389\n",
      "content_length": 1854,
      "extraction_method": "Direct"
    },
    {
      "page_number": 426,
      "chapter": null,
      "content": "390\nI/O DEVICES\nGraphics\nMemory\nCPU\nMemory Bus\n(proprietary)\nGeneral I/O Bus\n(e.g., PCI)\nPeripheral I/O Bus\n(e.g., SCSI, SATA, USB)\nFigure 36.1: Prototypical System Architecture\nformance components are further away. The beneﬁts of placing disks and\nother slow devices on a peripheral bus are manifold; in particular, you\ncan place a large number of devices on it.\n36.2\nA Canonical Device\nLet us now look at a canonical device (not a real one), and use this\ndevice to drive our understanding of some of the machinery required\nto make device interaction efﬁcient. From Figure 36.2, we can see that a\ndevice has two important components. The ﬁrst is the hardware interface\nit presents to the rest of the system. Just like a piece of software, hardware\nmust also present some kind of interface that allows the system software\nto control its operation. Thus, all devices have some speciﬁed interface\nand protocol for typical interaction.\nThe second part of any device is its internal structure. This part of\nthe device is implementation speciﬁc and is responsible for implement-\ning the abstraction the device presents to the system. Very simple devices\nwill have one or a few hardware chips to implement their functionality;\nmore complex devices will include a simple CPU, some general purpose\nmemory, and other device-speciﬁc chips to get their job done. For exam-\nple, modern RAID controllers might consist of hundreds of thousands of\nlines of ﬁrmware (i.e., software within a hardware device) to implement\nits functionality.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1571,
      "extraction_method": "Direct"
    },
    {
      "page_number": 427,
      "chapter": null,
      "content": "I/O DEVICES\n391\nOther Hardware-specific Chips\nMemory (DRAM or SRAM or both)\nMicro-controller (CPU)\nRegisters\nStatus\nCommand\nData\nInterface\nInternals\nFigure 36.2: A Canonical Device\n36.3\nThe Canonical Protocol\nIn the picture above, the (simpliﬁed) device interface is comprised of\nthree registers: a status register, which can be read to see the current sta-\ntus of the device; a command register, to tell the device to perform a cer-\ntain task; and a data register to pass data to the device, or get data from\nthe device. By reading and writing these registers, the operating system\ncan control device behavior.\nLet us now describe a typical interaction that the OS might have with\nthe device in order to get the device to do something on its behalf. The\nprotocol is as follows:\nWhile (STATUS == BUSY)\n; // wait until device is not busy\nWrite data to DATA register\nWrite command to COMMAND register\n(Doing so starts the device and executes the command)\nWhile (STATUS == BUSY)\n; // wait until device is done with your request\nThe protocol has four steps. In the ﬁrst, the OS waits until the device is\nready to receive a command by repeatedly reading the status register; we\ncall this polling the device (basically, just asking it what is going on). Sec-\nond, the OS sends some data down to the data register; one can imagine\nthat if this were a disk, for example, that multiple writes would need to\ntake place to transfer a disk block (say 4KB) to the device. When the main\nCPU is involved with the data movement (as in this example protocol),\nwe refer to it as programmed I/O (PIO). Third, the OS writes a command\nto the command register; doing so implicitly lets the device know that\nboth the data is present and that it should begin working on the com-\nmand. Finally, the OS waits for the device to ﬁnish by again polling it\nin a loop, waiting to see if it is ﬁnished (it may then get an error code to\nindicate success or failure).\nThis basic protocol has the positive aspect of being simple and work-\ning. However, there are some inefﬁciencies and inconveniences involved.\nThe ﬁrst problem you might notice in the protocol is that polling seems\ninefﬁcient; speciﬁcally, it wastes a great deal of CPU time just waiting for\nthe (potentially slow) device to complete its activity, instead of switching\nto another ready process and thus better utilizing the CPU.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2403,
      "extraction_method": "Direct"
    },
    {
      "page_number": 428,
      "chapter": null,
      "content": "392\nI/O DEVICES\nTHE CRUX: HOW TO AVOID THE COSTS OF POLLING\nHow can the OS check device status without frequent polling, and\nthus lower the CPU overhead required to manage the device?\n36.4\nLowering CPU Overhead With Interrupts\nThe invention that many engineers came upon years ago to improve\nthis interaction is something we’ve seen already: the interrupt. Instead\nof polling the device repeatedly, the OS can issue a request, put the call-\ning process to sleep, and context switch to another task. When the device\nis ﬁnally ﬁnished with the operation, it will raise a hardware interrupt,\ncausing the CPU to jump into the OS at a pre-determined interrupt ser-\nvice routine (ISR) or more simply an interrupt handler. The handler is\njust a piece of operating system code that will ﬁnish the request (for ex-\nample, by reading data and perhaps an error code from the device) and\nwake the process waiting for the I/O, which can then proceed as desired.\nInterrupts thus allow for overlap of computation and I/O, which is\nkey for improved utilization. This timeline shows the problem:\nCPU\nDisk\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\np\np\np\np\np\n1\n1\n1\n1\n1\nIn the diagram, Process 1 runs on the CPU for some time (indicated by\na repeated 1 on the CPU line), and then issues an I/O request to the disk\nto read some data. Without interrupts, the system simply spins, polling\nthe status of the device repeatedly until the I/O is complete (indicated by\na p). The disk services the request and ﬁnally Process 1 can run again.\nIf instead we utilize interrupts and allow for overlap, the OS can do\nsomething else while waiting for the disk:\nCPU\nDisk\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n2\n2\n2\n2\n2\n1\n1\n1\n1\n1\nIn this example, the OS runs Process 2 on the CPU while the disk ser-\nvices Process 1’s request. When the disk request is ﬁnished, an interrupt\noccurs, and the OS wakes up Process 1 and runs it again. Thus, both the\nCPU and the disk are properly utilized during the middle stretch of time.\nNote that using interrupts is not always the best solution. For example,\nimagine a device that performs its tasks very quickly: the ﬁrst poll usually\nﬁnds the device to be done with task. Using an interrupt in this case will\nactually slow down the system: switching to another process, handling the\ninterrupt, and switching back to the issuing process is expensive. Thus, if\na device is fast, it may be best to poll; if it is slow, interrupts, which allow\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2447,
      "extraction_method": "Direct"
    },
    {
      "page_number": 429,
      "chapter": null,
      "content": "I/O DEVICES\n393\nTIP: INTERRUPTS NOT ALWAYS BETTER THAN PIO\nAlthough interrupts allow for overlap of computation and I/O, they only\nreally make sense for slow devices. Otherwise, the cost of interrupt han-\ndling and context switching may outweigh the beneﬁts interrupts pro-\nvide. There are also cases where a ﬂood of interrupts may overload a sys-\ntem and lead it to livelock [MR96]; in such cases, polling provides more\ncontrol to the OS in its scheduling and thus is again useful.\noverlap, are best. If the speed of the device is not known, or sometimes\nfast and sometimes slow, it may be best to use a hybrid that polls for a\nlittle while and then, if the device is not yet ﬁnished, uses interrupts. This\ntwo-phased approach may achieve the best of both worlds.\nAnother reason not to use interrupts arises in networks [MR96]. When\na huge stream of incoming packets each generate an interrupt, it is pos-\nsible for the OS to livelock, that is, ﬁnd itself only processing interrupts\nand never allowing a user-level process to run and actually service the\nrequests. For example, imagine a web server that suddenly experiences\na high load due to the “slashdot effect”. In this case, it is better to occa-\nsionally use polling to better control what is happening in the system and\nallow the web server to service some requests before going back to the\ndevice to check for more packet arrivals.\nAnother interrupt-based optimization is coalescing. In such a setup, a\ndevice which needs to raise an interrupt ﬁrst waits for a bit before deliv-\nering the interrupt to the CPU. While waiting, other requests may soon\ncomplete, and thus multiple interrupts can be coalesced into a single in-\nterrupt delivery, thus lowering the overhead of interrupt processing. Of\ncourse, waiting too long will increase the latency of a request, a common\ntrade-off in systems. See Ahmad et al. [A+11] for an excellent summary.\n36.5\nMore Efﬁcient Data Movement With DMA\nUnfortunately, there is one other aspect of our canonical protocol that\nrequires our attention. In particular, when using programmed I/O (PIO)\nto transfer a large chunk of data to a device, the CPU is once again over-\nburdened with a rather trivial task, and thus wastes a lot of time and\neffort that could better be spent running other processes. This timeline\nillustrates the problem:\nCPU\nDisk\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\nc\nc\nc\n2\n2\n2\n2\n2\n1\n1\nIn the timeline, Process 1 is running and then wishes to write some data to\nthe disk. It then initiates the I/O, which must copy the data from memory\nto the device explicitly, one word at a time (marked c in the diagram).\nWhen the copy is complete, the I/O begins on the disk and the CPU can\nﬁnally be used for something else.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2749,
      "extraction_method": "Direct"
    },
    {
      "page_number": 430,
      "chapter": null,
      "content": "394\nI/O DEVICES\nTHE CRUX: HOW TO LOWER PIO OVERHEADS\nWith PIO, the CPU spends too much time moving data to and from\ndevices by hand. How can we ofﬂoad this work and thus allow the CPU\nto be more effectively utilized?\nThe solution to this problem is something we refer to as Direct Mem-\nory Access (DMA). A DMA engine is essentially a very speciﬁc device\nwithin a system that can orchestrate transfers between devices and main\nmemory without much CPU intervention.\nDMA works as follows. To transfer data to the device, for example, the\nOS would program the DMA engine by telling it where the data lives in\nmemory, how much data to copy, and which device to send it to. At that\npoint, the OS is done with the transfer and can proceed with other work.\nWhen the DMA is complete, the DMA controller raises an interrupt, and\nthe OS thus knows the transfer is complete. The revised timeline:\nCPU\nDMA\nDisk\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n2\n2\n2\n2\n2\n2\n2\n2\n1\n1\nc\nc\nc\nFrom the timeline, you can see that the copying of data is now handled\nby the DMA controller. Because the CPU is free during that time, the OS\ncan do something else, here choosing to run Process 2. Process 2 thus gets\nto use more CPU before Process 1 runs again.\n36.6\nMethods Of Device Interaction\nNow that we have some sense of the efﬁciency issues involved with\nperforming I/O, there are a few other problems we need to handle to\nincorporate devices into modern systems. One problem you may have\nnoticed thus far: we have not really said anything about how the OS ac-\ntually communicates with the device! Thus, the problem:\nTHE CRUX: HOW TO COMMUNICATE WITH DEVICES\nHow should the hardware communicate with a device? Should there\nbe explicit instructions? Or are there other ways to do it?\nOver time, two primary methods of device communication have de-\nveloped. The ﬁrst, oldest method (used by IBM mainframes for many\nyears) is to have explicit I/O instructions. These instructions specify a\nway for the OS to send data to speciﬁc device registers and thus allow the\nconstruction of the protocols described above.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2109,
      "extraction_method": "Direct"
    },
    {
      "page_number": 431,
      "chapter": null,
      "content": "I/O DEVICES\n395\nFor example, on x86, the in and out instructions can be used to com-\nmunicate with devices. For example, to send data to a device, the caller\nspeciﬁes a register with the data in it, and a speciﬁc port which names the\ndevice. Executing the instruction leads to the desired behavior.\nSuch instructions are usually privileged. The OS controls devices, and\nthe OS thus is the only entity allowed to directly communicate with them.\nImagine if any program could read or write the disk, for example: total\nchaos (as always), as any user program could use such a loophole to gain\ncomplete control over the machine.\nThe second method to interact with devices is known as memory-\nmapped I/O. With this approach, the hardware makes device registers\navailable as if they were memory locations. To access a particular register,\nthe OS issues a load (to read) or store (to write) the address; the hardware\nthen routes the load/store to the device instead of main memory.\nThere is not some great advantage to one approach or the other. The\nmemory-mapped approach is nice in that no new instructions are needed\nto support it, but both approaches are still in use today.\n36.7\nFitting Into The OS: The Device Driver\nOne ﬁnal problem we will discuss: how to ﬁt devices, each of which\nhave very speciﬁc interfaces, into the OS, which we would like to keep\nas general as possible. For example, consider a ﬁle system. We’d like\nto build a ﬁle system that worked on top of SCSI disks, IDE disks, USB\nkeychain drives, and so forth, and we’d like the ﬁle system to be relatively\noblivious to all of the details of how to issue a read or write request to\nthese difference types of drives. Thus, our problem:\nTHE CRUX: HOW TO BUILD A DEVICE-NEUTRAL OS\nHow can we keep most of the OS device-neutral, thus hiding the de-\ntails of device interactions from major OS subsystems?\nThe problem is solved through the age-old technique of abstraction.\nAt the lowest level, a piece of software in the OS must know in detail\nhow a device works. We call this piece of software a device driver, and\nany speciﬁcs of device interaction are encapsulated within.\nLet us see how this abstraction might help OS design and implemen-\ntation by examining the Linux ﬁle system software stack. Figure 36.3 is\na rough and approximate depiction of the Linux software organization.\nAs you can see from the diagram, a ﬁle system (and certainly, an appli-\ncation above) is completely oblivious to the speciﬁcs of which disk class\nit is using; it simply issues block read and write requests to the generic\nblock layer, which routes them to the appropriate device driver, which\nhandles the details of issuing the speciﬁc request. Although simpliﬁed,\nthe diagram shows how such detail can be hidden from most of the OS.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2818,
      "extraction_method": "Direct"
    },
    {
      "page_number": 432,
      "chapter": null,
      "content": "396\nI/O DEVICES\nApplication\nFile System\nGeneric Block Layer\nDevice Driver [SCSI, ATA, etc.]\nPOSIX API [open, read, write, close, etc.]\nGeneric Block Interface [block read/write]\nSpecific Block Interface [protocol-specific read/write]\nuser\nkernel mode\nFigure 36.3: The File System Stack\nNote that such encapsulation can have its downside as well. For ex-\nample, if there is a device that has many special capabilities, but has to\npresent a generic interface to the rest of the kernel, those special capabili-\nties will go unused. This situation arises, for example, in Linux with SCSI\ndevices, which have very rich error reporting; because other block de-\nvices (e.g., ATA/IDE) have much simpler error handling, all that higher\nlevels of software ever receive is a generic EIO (generic IO error) error\ncode; any extra detail that SCSI may have provided is thus lost to the ﬁle\nsystem [G08].\nInterestingly, because device drivers are needed for any device you\nmight plug into your system, over time they have come to represent a\nhuge percentage of kernel code. Studies of the Linux kernel reveal that\nover 70% of OS code is found in device drivers [C01]; for Windows-based\nsystems, it is likely quite high as well. Thus, when people tell you that the\nOS has millions of lines of code, what they are really saying is that the OS\nhas millions of lines of device-driver code. Of course, for any given in-\nstallation, most of that code may not be active (i.e., only a few devices are\nconnected to the system at a time). Perhaps more depressingly, as drivers\nare often written by “amateurs” (instead of full-time kernel developers),\nthey tend to have many more bugs and thus are a primary contributor to\nkernel crashes [S03].\n36.8\nCase Study: A Simple IDE Disk Driver\nTo dig a little deeper here, let’s take a quick look at an actual device: an\nIDE disk drive [L94]. We summarize the protocol as described in this ref-\nerence [W10]; we’ll also peek at the xv6 source code for a simple example\nof a working IDE driver [CK+08].\nAn IDE disk presents a simple interface to the system, consisting of\nfour types of register: control, command block, status, and error. These\nregisters are available by reading or writing to speciﬁc “I/O addresses”\n(such as 0x3F6 below) using (on x86) the in and out I/O instructions.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2351,
      "extraction_method": "Direct"
    },
    {
      "page_number": 433,
      "chapter": null,
      "content": "I/O DEVICES\n397\nControl Register:\nAddress 0x3F6 = 0x80 (0000 1RE0): R=reset, E=0 means \"enable interrupt\"\nCommand Block Registers:\nAddress 0x1F0 = Data Port\nAddress 0x1F1 = Error\nAddress 0x1F2 = Sector Count\nAddress 0x1F3 = LBA low byte\nAddress 0x1F4 = LBA mid byte\nAddress 0x1F5 = LBA hi\nbyte\nAddress 0x1F6 = 1B1D TOP4LBA: B=LBA, D=drive\nAddress 0x1F7 = Command/status\nStatus Register (Address 0x1F7):\n7\n6\n5\n4\n3\n2\n1\n0\nBUSY\nREADY FAULT SEEK\nDRQ\nCORR IDDEX ERROR\nError Register (Address 0x1F1): (check when Status ERROR==1)\n7\n6\n5\n4\n3\n2\n1\n0\nBBK\nUNC\nMC\nIDNF\nMCR\nABRT T0NF AMNF\nBBK\n= Bad Block\nUNC\n= Uncorrectable data error\nMC\n= Media Changed\nIDNF = ID mark Not Found\nMCR\n= Media Change Requested\nABRT = Command aborted\nT0NF = Track 0 Not Found\nAMNF = Address Mark Not Found\nFigure 36.4: The IDE Interface\nThe basic protocol to interact with the device is as follows, assuming\nit has already been initialized.\n• Wait for drive to be ready. Read Status Register (0x1F7) until drive\nis not busy and READY.\n• Write parameters to command registers. Write the sector count,\nlogical block address (LBA) of the sectors to be accessed, and drive\nnumber (master=0x00 or slave=0x10, as IDE permits just two drives)\nto command registers (0x1F2-0x1F6).\n• Start the I/O. by issuing read/write to command register. Write\nREAD—WRITE command to command register (0x1F7).\n• Data transfer (for writes): Wait until drive status is READY and\nDRQ (drive request for data); write data to data port.\n• Handle interrupts. In the simplest case, handle an interrupt for\neach sector transferred; more complex approaches allow batching\nand thus one ﬁnal interrupt when the entire transfer is complete.\n• Error handling. After each operation, read the status register. If the\nERROR bit is on, read the error register for details.\nMost of this protocol is found in the xv6 IDE driver (Figure 36.5),\nwhich (after initialization) works through four primary functions. The\nﬁrst is ide rw(), which queues a request (if there are others pending),\nor issues it directly to the disk (via ide start request()); in either\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2121,
      "extraction_method": "Direct"
    },
    {
      "page_number": 434,
      "chapter": null,
      "content": "398\nI/O DEVICES\nstatic int ide_wait_ready() {\nwhile (((int r = inb(0x1f7)) & IDE_BSY) || !(r & IDE_DRDY))\n;\n// loop until drive isn’t busy\n}\nstatic void ide_start_request(struct buf *b) {\nide_wait_ready();\noutb(0x3f6, 0);\n// generate interrupt\noutb(0x1f2, 1);\n// how many sectors?\noutb(0x1f3, b->sector & 0xff);\n// LBA goes here ...\noutb(0x1f4, (b->sector >> 8) & 0xff);\n// ... and here\noutb(0x1f5, (b->sector >> 16) & 0xff);\n// ... and here!\noutb(0x1f6, 0xe0 | ((b->dev&1)<<4) | ((b->sector>>24)&0x0f));\nif(b->flags & B_DIRTY){\noutb(0x1f7, IDE_CMD_WRITE);\n// this is a WRITE\noutsl(0x1f0, b->data, 512/4);\n// transfer data too!\n} else {\noutb(0x1f7, IDE_CMD_READ);\n// this is a READ (no data)\n}\n}\nvoid ide_rw(struct buf *b) {\nacquire(&ide_lock);\nfor (struct buf **pp = &ide_queue; *pp; pp=&(*pp)->qnext)\n;\n// walk queue\n*pp = b;\n// add request to end\nif (ide_queue == b)\n// if q is empty\nide_start_request(b);\n// send req to disk\nwhile ((b->flags & (B_VALID|B_DIRTY)) != B_VALID)\nsleep(b, &ide_lock);\n// wait for completion\nrelease(&ide_lock);\n}\nvoid ide_intr() {\nstruct buf *b;\nacquire(&ide_lock);\nif (!(b->flags & B_DIRTY) && ide_wait_ready(1) >= 0)\ninsl(0x1f0, b->data, 512/4);\n// if READ: get data\nb->flags |= B_VALID;\nb->flags &= ˜B_DIRTY;\nwakeup(b);\n// wake waiting process\nif ((ide_queue = b->qnext) != 0) // start next request\nide_start_request(ide_queue);\n// (if one exists)\nrelease(&ide_lock);\n}\nFigure 36.5: The xv6 IDE Disk Driver (Simpliﬁed)\ncase, the routine waits for the request to complete and the calling pro-\ncess is put to sleep. The second is ide start request(), which is\nused to send a request (and perhaps data, in the case of a write) to the\ndisk; the in and out x86 instructions are called to read and write device\nregisters, respectively. The start request routine uses the third function,\nide wait ready(), to ensure the drive is ready before issuing a request\nto it. Finally, ide intr() is invoked when an interrupt takes place; it\nreads data from the device (if the request is a read, not a write), wakes the\nprocess waiting for the I/O to complete, and (if there are more requests\nin the I/O queue), launches the next I/O via ide start request().\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2224,
      "extraction_method": "Direct"
    },
    {
      "page_number": 435,
      "chapter": null,
      "content": "I/O DEVICES\n399\n36.9\nHistorical Notes\nBefore ending, we include a brief historical note on the origin of some\nof these fundamental ideas. If you are interested in learning more, read\nSmotherman’s excellent summary [S08].\nInterrupts are an ancient idea, existing on the earliest of machines. For\nexample, the UNIVAC in the early 1950’s had some form of interrupt vec-\ntoring, although it is unclear in exactly which year this feature was avail-\nable [S08]. Sadly, even in its infancy, we are beginning to lose the origins\nof computing history.\nThere is also some debate as to which machine ﬁrst introduced the idea\nof DMA. For example, Knuth and others point to the DYSEAC (a “mo-\nbile” machine, which at the time meant it could be hauled in a trailer),\nwhereas others think the IBM SAGE may have been the ﬁrst [S08]. Ei-\nther way, by the mid 50’s, systems with I/O devices that communicated\ndirectly with memory and interrupted the CPU when ﬁnished existed.\nThe history here is difﬁcult to trace because the inventions are tied to\nreal, and sometimes obscure, machines. For example, some think that the\nLincoln Labs TX-2 machine was ﬁrst with vectored interrupts [S08], but\nthis is hardly clear.\nBecause the ideas are relatively obvious – no Einsteinian leap is re-\nquired to come up with the idea of letting the CPU do something else\nwhile a slow I/O is pending – perhaps our focus on “who ﬁrst?” is mis-\nguided. What is certainly clear: as people built these early machines, it\nbecame obvious that I/O support was needed. Interrupts, DMA, and re-\nlated ideas are all direct outcomes of the nature of fast CPUs and slow\ndevices; if you were there at the time, you might have had similar ideas.\n36.10\nSummary\nYou should now have a very basic understanding of how an OS inter-\nacts with a device. Two techniques, the interrupt and DMA, have been\nintroduced to help with device efﬁciency, and two approaches to access-\ning device registers, explicit I/O instructions and memory-mapped I/O,\nhave been described. Finally, the notion of a device driver has been pre-\nsented, showing how the OS itself can encapsulate low-level details and\nthus make it easier to build the rest of the OS in a device-neutral fashion.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2252,
      "extraction_method": "Direct"
    },
    {
      "page_number": 436,
      "chapter": null,
      "content": "400\nI/O DEVICES\nReferences\n[A+11] “vIC: Interrupt Coalescing for Virtual Machine Storage Device IO”\nIrfan Ahmad, Ajay Gulati, Ali Mashtizadeh\nUSENIX ’11\nA terriﬁc survey of interrupt coalescing in traditional and virtualized environments.\n[C01] “An Empirical Study of Operating System Errors”\nAndy Chou, Junfeng Yang, Benjamin Chelf, Seth Hallem, Dawson Engler\nSOSP ’01\nOne of the ﬁrst papers to systematically explore how many bugs are in modern operating systems.\nAmong other neat ﬁndings, the authors show that device drivers have something like seven times more\nbugs than mainline kernel code.\n[CK+08] “The xv6 Operating System”\nRuss Cox, Frans Kaashoek, Robert Morris, Nickolai Zeldovich\nFrom: http://pdos.csail.mit.edu/6.828/2008/index.html\nSee ide.c for the IDE device driver, with a few more details therein.\n[D07] “What Every Programmer Should Know About Memory”\nUlrich Drepper\nNovember, 2007\nAvailable: http://www.akkadia.org/drepper/cpumemory.pdf\nA fantastic read about modern memory systems, starting at DRAM and going all the way up to virtu-\nalization and cache-optimized algorithms.\n[G08] “EIO: Error-handling is Occasionally Correct”\nHaryadi Gunawi, Cindy Rubio-Gonzalez, Andrea Arpaci-Dusseau, Remzi Arpaci-Dusseau,\nBen Liblit\nFAST ’08, San Jose, CA, February 2008\nOur own work on building a tool to ﬁnd code in Linux ﬁle systems that does not handle error return\nproperly. We found hundreds and hundreds of bugs, many of which have now been ﬁxed.\n[L94] “AT Attachment Interface for Disk Drives”\nLawrence J. Lamers, X3T10 Technical Editor\nAvailable: ftp://ftp.t10.org/t13/project/d0791r4c-ATA-1.pdf\nReference number: ANSI X3.221 - 1994 A rather dry document about device interfaces. Read it at\nyour own peril.\n[MR96] “Eliminating Receive Livelock in an Interrupt-driven Kernel”\nJeffrey Mogul and K. K. Ramakrishnan\nUSENIX ’96, San Diego, CA, January 1996\nMogul and colleagues did a great deal of pioneering work on web server network performance. This\npaper is but one example.\n[S08] “Interrupts”\nMark Smotherman, as of July ’08\nAvailable: http://people.cs.clemson.edu/˜mark/interrupts.html\nA treasure trove of information on the history of interrupts, DMA, and related early ideas in computing.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2260,
      "extraction_method": "Direct"
    },
    {
      "page_number": 437,
      "chapter": null,
      "content": "I/O DEVICES\n401\n[S03] “Improving the Reliability of Commodity Operating Systems”\nMichael M. Swift, Brian N. Bershad, and Henry M. Levy\nSOSP ’03\nSwift’s work revived interest in a more microkernel-like approach to operating systems; minimally, it\nﬁnally gave some good reasons why address-space based protection could be useful in a modern OS.\n[W10] “Hard Disk Driver”\nWashington State Course Homepage\nAvailable: http://eecs.wsu.edu/˜cs460/cs560/HDdriver.html\nA nice summary of a simple IDE disk drive’s interface and how to build a device driver for it.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 595,
      "extraction_method": "Direct"
    },
    {
      "page_number": 438,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 439,
      "chapter": null,
      "content": "37\nHard Disk Drives\nThe last chapter introduced the general concept of an I/O device and\nshowed you how the OS might interact with such a beast. In this chapter,\nwe dive into more detail about one device in particular: the hard disk\ndrive. These drives have been the main form of persistent data storage in\ncomputer systems for decades and much of the development of ﬁle sys-\ntem technology (coming soon) is predicated on their behavior. Thus, it\nis worth understanding the details of a disk’s operation before building\nthe ﬁle system software that manages it. Many of these details are avail-\nable in excellent papers by Ruemmler and Wilkes [RW92] and Anderson,\nDykes, and Riedel [ADR03].\nCRUX: HOW TO STORE AND ACCESS DATA ON DISK\nHow do modern hard-disk drives store data? What is the interface?\nHow is the data actually laid out and accessed? How does disk schedul-\ning improve performance?\n37.1\nThe Interface\nLet’s start by understanding the interface to a modern disk drive. The\nbasic interface for all modern drives is straightforward. The drive consists\nof a large number of sectors (512-byte blocks), each of which can be read\nor written. The sectors are numbered from 0 to n −1 on a disk with n\nsectors. Thus, we can view the disk as an array of sectors; 0 to n −1 is\nthus the address space of the drive.\nMulti-sector operations are possible; indeed, many ﬁle systems will\nread or write 4KB at a time (or more). However, when updating the\ndisk, the only guarantee drive manufactures make is that a single 512-\nbyte write is atomic (i.e., it will either complete in its entirety or it won’t\ncomplete at all); thus, if an untimely power loss occurs, only a portion of\na larger write may complete (sometimes called a torn write).\n403\n",
      "content_length": 1741,
      "extraction_method": "Direct"
    },
    {
      "page_number": 440,
      "chapter": null,
      "content": "404\nHARD DISK DRIVES\n0\n11\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\nSpindle\nFigure 37.1: A Disk With Just A Single Track\nThere are some assumptions most clients of disk drives make, but\nthat are not speciﬁed directly in the interface; Schlosser and Ganger have\ncalled this the “unwritten contract” of disk drives [SG04]. Speciﬁcally,\none can usually assume that accessing two blocks that are near one-another\nwithin the drive’s address space will be faster than accessing two blocks\nthat are far apart. One can also usually assume that accessing blocks in\na contiguous chunk (i.e., a sequential read or write) is the fastest access\nmode, and usually much faster than any more random access pattern.\n37.2\nBasic Geometry\nLet’s start to understand some of the components of a modern disk.\nWe start with a platter, a circular hard surface on which data is stored\npersistently by inducing magnetic changes to it. A disk may have one\nor more platters; each platter has 2 sides, each of which is called a sur-\nface. These platters are usually made of some hard material (such as\naluminum), and then coated with a thin magnetic layer that enables the\ndrive to persistently store bits even when the drive is powered off.\nThe platters are all bound together around the spindle, which is con-\nnected to a motor that spins the platters around (while the drive is pow-\nered on) at a constant (ﬁxed) rate. The rate of rotation is often measured in\nrotations per minute (RPM), and typical modern values are in the 7,200\nRPM to 15,000 RPM range. Note that we will often be interested in the\ntime of a single rotation, e.g., a drive that rotates at 10,000 RPM means\nthat a single rotation takes about 6 milliseconds (6 ms).\nData is encoded on each surface in concentric circles of sectors; we call\none such concentric circle a track. A single surface contains many thou-\nsands and thousands of tracks, tightly packed together, with hundreds of\ntracks ﬁtting into the width of a human hair.\nTo read and write from the surface, we need a mechanism that allows\nus to either sense (i.e., read) the magnetic patterns on the disk or to in-\nduce a change in (i.e., write) them. This process of reading and writing is\naccomplished by the disk head; there is one such head per surface of the\ndrive. The disk head is attached to a single disk arm, which moves across\nthe surface to position the head over the desired track.\n37.3\nA Simple Disk Drive\nLet’s understand how disks work by building up a model one track at\na time. Assume we have a simple disk with a single track (Figure 37.1).\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2588,
      "extraction_method": "Direct"
    },
    {
      "page_number": 441,
      "chapter": null,
      "content": "HARD DISK DRIVES\n405\nHead\nArm\n0\n11\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\nSpindle\nRotates this way\nFigure 37.2: A Single Track Plus A Head\nThis track has just 12 sectors, each of which is 512 bytes in size (our\ntypical sector size, recall) and addressed therefore by the numbers 0 through\n11. The single platter we have here rotates around the spindle, to which\na motor is attached. Of course, the track by itself isn’t too interesting; we\nwant to be able to read or write those sectors, and thus we need a disk\nhead, attached to a disk arm, as we now see (Figure 37.2).\nIn the ﬁgure, the disk head, attached to the end of the arm, is posi-\ntioned over sector 6, and the surface is rotating counter-clockwise.\nSingle-track Latency: The Rotational Delay\nTo understand how a request would be processed on our simple, one-\ntrack disk, imagine we now receive a request to read block 0. How should\nthe disk service this request?\nIn our simple disk, the disk doesn’t have to do much. In particular, it\nmust just wait for the desired sector to rotate under the disk head. This\nwait happens often enough in modern drives, and is an important enough\ncomponent of I/O service time, that it has a special name: rotational de-\nlay (sometimes rotation delay, though that sounds weird). In the exam-\nple, if the full rotational delay is R, the disk has to incur a rotational delay\nof about R\n2 to wait for 0 to come under the read/write head (if we start at\n6). A worst-case request on this single track would be to sector 5, causing\nnearly a full rotational delay in order to service such a request.\nMultiple Tracks: Seek Time\nSo far our disk just has a single track, which is not too realistic; modern\ndisks of course have many millions. Let’s thus look at ever-so-slightly\nmore realistic disk surface, this one with three tracks (Figure 37.3, left).\nIn the ﬁgure, the head is currently positioned over the innermost track\n(which contains sectors 24 through 35); the next track over contains the\nnext set of sectors (12 through 23), and the outermost track contains the\nﬁrst sectors (0 through 11).\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2110,
      "extraction_method": "Direct"
    },
    {
      "page_number": 442,
      "chapter": null,
      "content": "406\nHARD DISK DRIVES\n0\n11\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\n12\n23\n22\n21\n20\n19\n18\n17\n16\n15\n14\n13\n24\n35\n34\n33\n32\n31\n30\n29\n28\n27\n26\n25\nSpindle\nRotates this way\nSeek\nRemaining rotation\n3\n2\n1\n0\n11\n10\n9\n8\n7\n6\n5\n4\n15\n14\n13\n12\n23\n22\n21\n20\n19\n18\n17\n16\n27\n26\n25\n24\n35\n34\n33\n32\n31\n30\n29\n28\nSpindle\nRotates this way\nFigure 37.3: Three Tracks Plus A Head (Right: With Seek)\nTo understand how the drive might access a given sector, we now trace\nwhat would happen on a request to a distant sector, e.g., a read to sector\n11. To service this read, the drive has to ﬁrst move the disk arm to the cor-\nrect track (in this case, the outermost one), in a process known as a seek.\nSeeks, along with rotations, are one of the most costly disk operations.\nThe seek, it should be noted, has many phases: ﬁrst an acceleration\nphase as the disk arm gets moving; then coasting as the arm is moving\nat full speed, then deceleration as the arm slows down; ﬁnally settling as\nthe head is carefully positioned over the correct track. The settling time\nis often quite signiﬁcant, e.g., 0.5 to 2 ms, as the drive must be certain to\nﬁnd the right track (imagine if it just got close instead!).\nAfter the seek, the disk arm has positioned the head over the right\ntrack. A depiction of the seek is found in Figure 37.3 (right).\nAs we can see, during the seek, the arm has been moved to the desired\ntrack, and the platter of course has rotated, in this case about 3 sectors.\nThus, sector 9 is just about to pass under the disk head, and we must\nonly endure a short rotational delay to complete the transfer.\nWhen sector 11 passes under the disk head, the ﬁnal phase of I/O\nwill take place, known as the transfer, where data is either read from or\nwritten to the surface. And thus, we have a complete picture of I/O time:\nﬁrst a seek, then waiting for the rotational delay, and ﬁnally the transfer.\nSome Other Details\nThough we won’t spend too much time on it, there are some other inter-\nesting details about how hard drives operate. Many drives employ some\nkind of track skew to make sure that sequential reads can be properly\nserviced even when crossing track boundaries. In our simple example\ndisk, this might appear as seen in Figure 37.4.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2241,
      "extraction_method": "Direct"
    },
    {
      "page_number": 443,
      "chapter": null,
      "content": "HARD DISK DRIVES\n407\nTrack skew: 2 blocks\n0\n11\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\n22\n21\n20\n19\n18\n17\n16\n15\n14\n13\n12\n23\n32\n31\n30\n29\n28\n27\n26\n25\n24\n35\n34\n33\nSpindle\nRotates this way\nFigure 37.4: Three Tracks: Track Skew Of 2\nSectors are often skewed like this because when switching from one\ntrack to another, the disk needs time to reposition the head (even to neigh-\nboring tracks). Without such skew, the head would be moved to the next\ntrack but the desired next block would have already rotated under the\nhead, and thus the drive would have to wait almost the entire rotational\ndelay to access the next block.\nAnother reality is that outer tracks tend to have more sectors than\ninner tracks, which is a result of geometry; there is simply more room\nout there. These tracks are often referred to as multi-zoned disk drives,\nwhere the disk is organized into multiple zones, and where a zone is con-\nsecutive set of tracks on a surface. Each zone has the same number of\nsectors per track, and outer zones have more sectors than inner zones.\nFinally, an important part of any modern disk drive is its cache, for\nhistorical reasons sometimes called a track buffer. This cache is just some\nsmall amount of memory (usually around 8 or 16 MB) which the drive\ncan use to hold data read from or written to the disk. For example, when\nreading a sector from the disk, the drive might decide to read in all of the\nsectors on that track and cache them in its memory; doing so allows the\ndrive to quickly respond to any subsequent requests to the same track.\nOn writes, the drive has a choice: should it acknowledge the write has\ncompleted when it has put the data in its memory, or after the write has\nactually been written to disk? The former is called write back caching\n(or sometimes immediate reporting), and the latter write through. Write\nback caching sometimes makes the drive appear “faster”, but can be dan-\ngerous; if the ﬁle system or applications require that data be written to\ndisk in a certain order for correctness, write-back caching can lead to\nproblems (read the chapter on ﬁle-system journaling for details).\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2144,
      "extraction_method": "Direct"
    },
    {
      "page_number": 444,
      "chapter": null,
      "content": "408\nHARD DISK DRIVES\nASIDE: DIMENSIONAL ANALYSIS\nRemember in Chemistry class, how you solved virtually every prob-\nlem by simply setting up the units such that they canceled out, and some-\nhow the answers popped out as a result? That chemical magic is known\nby the highfalutin name of dimensional analysis and it turns out it is\nuseful in computer systems analysis too.\nLet’s do an example to see how dimensional analysis works and why\nit is useful. In this case, assume you have to ﬁgure out how long, in mil-\nliseconds, a single rotation of a disk takes. Unfortunately, you are given\nonly the RPM of the disk, or rotations per minute. Let’s assume we’re\ntalking about a 10K RPM disk (i.e., it rotates 10,000 times per minute).\nHow do we set up the dimensional analysis so that we get time per rota-\ntion in milliseconds?\nTo do so, we start by putting the desired units on the left; in this case,\nwe wish to obtain the time (in milliseconds) per rotation, so that is ex-\nactly what we write down:\nT ime (ms)\n1 Rotation. We then write down everything\nwe know, making sure to cancel units where possible. First, we obtain\n1 minute\n10,000 Rotations (keeping rotation on the bottom, as that’s where it is on\nthe left), then transform minutes into seconds with 60 seconds\n1 minute , and then\nﬁnally transform seconds in milliseconds with 1000 ms\n1 second. The ﬁnal result is\nthis equation, with units nicely canceled, is:\nT ime (ms)\n1 Rot.\n=\n1\u0018\u0018\u0018\nminute\n10,000 Rot. · 60\u0018\u0018\u0018\nseconds\n1\u0018\u0018\u0018\nminute · 1000 ms\n1\u0018\u0018\u0018\nsecond =\n60,000 ms\n10,000 Rot. =\n6 ms\nRotation\nAs you can see from this example, dimensional analysis makes what\nseems obvious into a simple and repeatable process. Beyond the RPM\ncalculation above, it comes in handy with I/O analysis regularly. For\nexample, you will often be given the transfer rate of a disk, e.g.,\n100 MB/second, and then asked: how long does it take to transfer a\n512 KB block (in milliseconds)? With dimensional analysis, it’s easy:\nT ime (ms)\n1 Request =\n512\b\b\nKB\n1 Request ·\n1\b\b\nMB\n1024\b\b\nKB · 1\u0018\u0018\u0018\nsecond\n100\b\b\nMB · 1000 ms\n1\u0018\u0018\u0018\nsecond =\n5 ms\nRequest\n37.4\nI/O Time: Doing The Math\nNow that we have an abstract model of the disk, we can use a little\nanalysis to better understand disk performance. In particular, we can\nnow represent I/O time as the sum of three major components:\nTI/O = Tseek + Trotation + Ttransfer\n(37.1)\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2401,
      "extraction_method": "Direct"
    },
    {
      "page_number": 445,
      "chapter": null,
      "content": "HARD DISK DRIVES\n409\nCheetah 15K.5\nBarracuda\nCapacity\n300 GB\n1 TB\nRPM\n15,000\n7,200\nAverage Seek\n4 ms\n9 ms\nMax Transfer\n125 MB/s\n105 MB/s\nPlatters\n4\n4\nCache\n16 MB\n16/32 MB\nConnects via\nSCSI\nSATA\nTable 37.1: Disk Drive Specs: SCSI Versus SATA\nNote that the rate of I/O (RI/O), which is often more easily used for\ncomparison between drives (as we will do below), is easily computed\nfrom the time. Simply divide the size of the transfer by the time it took:\nRI/O = SizeT ransfer\nTI/O\n(37.2)\nTo get a better feel for I/O time, let us perform the following calcu-\nlation. Assume there are two workloads we are interested in. The ﬁrst,\nknown as the random workload, issues small (e.g., 4KB) reads to random\nlocations on the disk. Random workloads are common in many impor-\ntant applications, including database management systems. The second,\nknown as the sequential workload, simply reads a large number of sec-\ntors consecutively from the disk, without jumping around. Sequential\naccess patterns are quite common and thus important as well.\nTo understand the difference in performance between random and se-\nquential workloads, we need to make a few assumptions about the disk\ndrive ﬁrst. Let’s look at a couple of modern disks from Seagate. The ﬁrst,\nknown as the Cheetah 15K.5 [S09b], is a high-performance SCSI drive.\nThe second, the Barracuda [S09a], is a drive built for capacity. Details on\nboth are found in Table 37.1.\nAs you can see, the drives have quite different characteristics, and\nin many ways nicely summarize two important components of the disk\ndrive market. The ﬁrst is the “high performance” drive market, where\ndrives are engineered to spin as fast as possible, deliver low seek times,\nand transfer data quickly. The second is the “capacity” market, where\ncost per byte is the most important aspect; thus, the drives are slower but\npack as many bits as possible into the space available.\nFrom these numbers, we can start to calculate how well the drives\nwould do under our two workloads outlined above. Let’s start by looking\nat the random workload. Assuming each 4 KB read occurs at a random\nlocation on disk, we can calculate how long each such read would take.\nOn the Cheetah:\nTseek = 4 ms, Trotation = 2 ms, Ttransfer = 30 microsecs\n(37.3)\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2301,
      "extraction_method": "Direct"
    },
    {
      "page_number": 446,
      "chapter": null,
      "content": "410\nHARD DISK DRIVES\nTIP: USE DISKS SEQUENTIALLY\nWhen at all possible, transfer data to and from disks in a sequential man-\nner. If sequential is not possible, at least think about transferring data\nin large chunks: the bigger, the better. If I/O is done in little random\npieces, I/O performance will suffer dramatically. Also, users will suffer.\nAlso, you will suffer, knowing what suffering you have wrought with\nyour careless random I/Os.\nThe average seek time (4 milliseconds) is just taken as the average time\nreported by the manufacturer; note that a full seek (from one end of the\nsurface to the other) would likely take two or three times longer. The\naverage rotational delay is calculated from the RPM directly. 15000 RPM\nis equal to 250 RPS (rotations per second); thus, each rotation takes 4 ms.\nOn average, the disk will encounter a half rotation and thus 2 ms is the\naverage time. Finally, the transfer time is just the size of the transfer over\nthe peak transfer rate; here it is vanishingly small (30 microseconds; note\nthat we need 1000 microseconds just to get 1 millisecond!).\nThus, from our equation above, TI/O for the Cheetah roughly equals\n6 ms. To compute the rate of I/O, we just divide the size of the transfer\nby the average time, and thus arrive at RI/O for the Cheetah under the\nrandom workload of about 0.66 MB/s. The same calculation for the Bar-\nracuda yields a TI/O of about 13.2 ms, more than twice as slow, and thus\na rate of about 0.31 MB/s.\nNow let’s look at the sequential workload. Here we can assume there\nis a single seek and rotation before a very long transfer. For simplicity,\nassume the size of the transfer is 100 MB. Thus, TI/O for the Barracuda\nand Cheetah is about 800 ms and 950 ms, respectively. The rates of I/O\nare thus very nearly the peak transfer rates of 125 MB/s and 105 MB/s,\nrespectively. Table 37.2 summarizes these numbers.\nThe table shows us a number of important things. First, and most\nimportantly, there is a huge gap in drive performance between random\nand sequential workloads, almost a factor of 200 or so for the Cheetah\nand more than a factor 300 difference for the Barracuda. And thus we\narrive at the most obvious design tip in the history of computing.\nA second, more subtle point: there is a large difference in performance\nbetween high-end “performance” drives and low-end “capacity” drives.\nFor this reason (and others), people are often willing to pay top dollar for\nthe former while trying to get the latter as cheaply as possible.\nCheetah\nBarracuda\nRI/O Random\n0.66 MB/s\n0.31 MB/s\nRI/O Sequential\n125 MB/s\n105 MB/s\nTable 37.2: Disk Drive Performance: SCSI Versus SATA\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2694,
      "extraction_method": "Direct"
    },
    {
      "page_number": 447,
      "chapter": null,
      "content": "HARD DISK DRIVES\n411\nASIDE: COMPUTING THE “AVERAGE” SEEK\nIn many books and papers, you will see average disk-seek time cited\nas being roughly one-third of the full seek time. Where does this come\nfrom?\nTurns out it arises from a simple calculation based on average seek\ndistance, not time. Imagine the disk as a set of tracks, from 0 to N. The\nseek distance between any two tracks x and y is thus computed as the\nabsolute value of the difference between them: |x −y|.\nTo compute the average seek distance, all you need to do is to ﬁrst add\nup all possible seek distances:\nN\nX\nx=0\nN\nX\ny=0\n|x −y|.\n(37.4)\nThen, divide this by the number of different possible seeks: N 2. To\ncompute the sum, we’ll just use the integral form:\nZ N\nx=0\nZ N\ny=0\n|x −y| dy dx.\n(37.5)\nTo compute the inner integral, let’s break out the absolute value:\nZ x\ny=0\n(x −y) dy +\nZ N\ny=x\n(y −x) dy.\n(37.6)\nSolving this leads to (xy −1\n2y2)\n\f\fx\n0 + ( 1\n2y2 −xy)\n\f\fN\nx which can be sim-\npliﬁed to (x2 −Nx + 1\n2N 2). Now we have to compute the outer integral:\nZ N\nx=0\n(x2 −Nx + 1\n2N 2) dx,\n(37.7)\nwhich results in:\n(1\n3x3 −N\n2 x2 + N 2\n2 x)\n\f\f\f\f\nN\n0\n= N 3\n3 .\n(37.8)\nRemember that we still have to divide by the total number of seeks\n(N 2) to compute the average seek distance: ( N3\n3 )/(N 2) =\n1\n3N. Thus the\naverage seek distance on a disk, over all possible seeks, is one-third the\nfull distance. And now when you hear that an average seek is one-third\nof a full seek, you’ll know where it came from.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 1509,
      "extraction_method": "Direct"
    },
    {
      "page_number": 448,
      "chapter": null,
      "content": "412\nHARD DISK DRIVES\n0\n11\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\n12\n23\n22\n21\n20\n19\n18\n17\n16\n15\n14\n13\n24\n35\n34\n33\n32\n31\n30\n29\n28\n27\n26\n25\nSpindle\nRotates this way\nFigure 37.5: SSTF: Scheduling Requests 21 And 2\n37.5\nDisk Scheduling\nBecause of the high cost of I/O, the OS has historically played a role in\ndeciding the order of I/Os issued to the disk. More speciﬁcally, given a\nset of I/O requests, the disk scheduler examines the requests and decides\nwhich one to schedule next [SCO90, JW91].\nUnlike job scheduling, where the length of each job is usually un-\nknown, with disk scheduling, we can make a good guess at how long\na “job” (i.e., disk request) will take. By estimating the seek and possible\nthe rotational delay of a request, the disk scheduler can know how long\neach request will take, and thus (greedily) pick the one that will take the\nleast time to service ﬁrst. Thus, the disk scheduler will try to follow the\nprinciple of SJF (shortest job ﬁrst) in its operation.\nSSTF: Shortest Seek Time First\nOne early disk scheduling approach is known as shortest-seek-time-ﬁrst\n(SSTF) (also called shortest-seek-ﬁrst or SSF). SSTF orders the queue of\nI/O requests by track, picking requests on the nearest track to complete\nﬁrst. For example, assuming the current position of the head is over the\ninner track, and we have requests for sectors 21 (middle track) and 2\n(outer track), we would then issue the request to 21 ﬁrst, wait for it to\ncomplete, and then issue the request to 2 (Figure 37.5).\nSSTF works well in this example, seeking to the middle track ﬁrst and\nthen the outer track. However, SSTF is not a panacea, for the following\nreasons. First, the drive geometry is not available to the host OS; rather,\nit sees an array of blocks. Fortunately, this problem is rather easily ﬁxed.\nInstead of SSTF, an OS can simply implement nearest-block-ﬁrst (NBF),\nwhich schedules the request with the nearest block address next.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1963,
      "extraction_method": "Direct"
    },
    {
      "page_number": 449,
      "chapter": null,
      "content": "HARD DISK DRIVES\n413\nThe second problem is more fundamental: starvation.\nImagine in\nour example above if there were a steady stream of requests to the in-\nner track, where the head currently is positioned. Requests to any other\ntracks would then be ignored completely by a pure SSTF approach. And\nthus the crux of the problem:\nCRUX: HOW TO HANDLE DISK STARVATION\nHow can we implement SSTF-like scheduling but avoid starvation?\nElevator (a.k.a. SCAN or C-SCAN)\nThe answer to this query was developed some time ago (see [CKR72]\nfor example), and is relatively straightforward. The algorithm, originally\ncalled SCAN, simply moves across the disk servicing requests in order\nacross the tracks. Let us call a single pass across the disk a sweep. Thus, if\na request comes for a block on a track that has already been serviced on\nthis sweep of the disk, it is not handled immediately, but rather queued\nuntil the next sweep.\nSCAN has a number of variants, all of which do about the same thing.\nFor example, Coffman et al. introduced F-SCAN, which freezes the queue\nto be serviced when it is doing a sweep [CKR72]; this action places re-\nquests that come in during the sweep into a queue to be serviced later.\nDoing so avoids starvation of far-away requests, by delaying the servic-\ning of late-arriving (but nearer by) requests.\nC-SCAN is another common variant, short for Circular SCAN. In-\nstead of sweeping in one direction across the disk, the algorithm sweeps\nfrom outer-to-inner, and then inner-to-outer, etc.\nFor reasons that should now be obvious, this algorithm (and its vari-\nants) is sometimes referred to as the elevator algorithm, because it be-\nhaves like an elevator which is either going up or down and not just ser-\nvicing requests to ﬂoors based on which ﬂoor is closer. Imagine how an-\nnoying it would be if you were going down from ﬂoor 10 to 1, and some-\nbody got on at 3 and pressed 4, and the elevator went up to 4 because it\nwas “closer” than 1! As you can see, the elevator algorithm, when used\nin real life, prevents ﬁghts from taking place on elevators. In disks, it just\nprevents starvation.\nUnfortunately, SCAN and its cousins do not represent the best schedul-\ning technology. In particular, SCAN (or SSTF even) do not actually adhere\nas closely to the principle of SJF as they could. In particular, they ignore\nrotation. And thus, another crux:\nCRUX: HOW TO ACCOUNT FOR DISK ROTATION COSTS\nHow can we implement an algorithm that more closely approximates SJF\nby taking both seek and rotation into account?\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2571,
      "extraction_method": "Direct"
    },
    {
      "page_number": 450,
      "chapter": null,
      "content": "414\nHARD DISK DRIVES\n0\n11\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\n12\n23\n22\n21\n20\n19\n18\n17\n16\n15\n14\n13\n24\n35\n34\n33\n32\n31\n30\n29\n28\n27\n26\n25\nSpindle\nRotates this way\nFigure 37.6: SSTF: Sometimes Not Good Enough\nSPTF: Shortest Positioning Time First\nBefore discussing shortest positioning time ﬁrst or SPTF scheduling (some-\ntimes also called shortest access time ﬁrst or SATF), which is the solution\nto our problem, let us make sure we understand the problem in more de-\ntail. Figure 37.6 presents an example.\nIn the example, the head is currently positioned over sector 30 on the\ninner track. The scheduler thus has to decide: should it schedule sector 16\n(on the middle track) or sector 8 (on the outer track) for its next request.\nSo which should it service next?\nThe answer, of course, is “it depends”. In engineering, it turns out\n“it depends” is almost always the answer, reﬂecting that trade-offs are\npart of the life of the engineer; such maxims are also good in a pinch,\ne.g., when you don’t know an answer to your boss’s question, you might\nwant to try this gem. However, it is almost always better to know why it\ndepends, which is what we discuss here.\nWhat it depends on here is the relative time of seeking as compared\nto rotation. If, in our example, seek time is much higher than rotational\ndelay, then SSTF (and variants) are just ﬁne. However, imagine if seek is\nquite a bit faster than rotation. Then, in our example, it would make more\nsense to seek further to service request 8 on the outer track than it would\nto perform the shorter seek to the middle track to service 16, which has to\nrotate all the way around before passing under the disk head.\nOn modern drives, as we saw above, both seek and rotation are roughly\nequivalent (depending, of course, on the exact requests), and thus SPTF\nis useful and improves performance. However, it is even more difﬁcult\nto implement in an OS, which generally does not have a good idea where\ntrack boundaries are or where the disk head currently is (in a rotational\nsense). Thus, SPTF is usually performed inside a drive, described below.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2124,
      "extraction_method": "Direct"
    },
    {
      "page_number": 451,
      "chapter": null,
      "content": "HARD DISK DRIVES\n415\nTIP: IT ALWAYS DEPENDS (LIVNY’S LAW)\nAlmost any question can be answered with “it depends”, as our colleague\nMiron Livny always says. However, use with caution, as if you answer\ntoo many questions this way, people will stop asking you questions alto-\ngether. For example, somebody asks: “want to go to lunch?” You reply:\n“it depends, are you coming along?”\nOther Scheduling Issues\nThere are many other issues we do not discuss in this brief description\nof basic disk operation, scheduling, and related topics.\nOne such is-\nsue is this: where is disk scheduling performed on modern systems? In\nolder systems, the operating system did all the scheduling; after looking\nthrough the set of pending requests, the OS would pick the best one, and\nissue it to the disk. When that request completed, the next one would be\nchosen, and so forth. Disks were simpler then, and so was life.\nIn modern systems, disks can accommodate multiple outstanding re-\nquests, and have sophisticated internal schedulers themselves (which can\nimplement SPTF accurately; inside the disk controller, all relevant details\nare available, including exact head position). Thus, the OS scheduler usu-\nally picks what it thinks the best few requests are (say 16) and issues them\nall to disk; the disk then uses its internal knowledge of head position and\ndetailed track layout information to service said requests in the best pos-\nsible (SPTF) order.\nAnother important related task performed by disk schedulers is I/O\nmerging. For example, imagine a series of requests to read blocks 33,\nthen 8, then 34, as in Figure 37.6. In this case, the scheduler should merge\nthe requests for blocks 33 and 34 into a single two-block request; any re-\nordering that the scheduler does is performed upon the merged requests.\nMerging is particularly important at the OS level, as it reduces the num-\nber of requests sent to the disk and thus lowers overheads.\nOne ﬁnal problem that modern schedulers address is this: how long\nshould the system wait before issuing an I/O to disk? One might naively\nthink that the disk, once it has even a single I/O, should immediately\nissue the request to the drive; this approach is called work-conserving, as\nthe disk will never be idle if there are requests to serve. However, research\non anticipatory disk scheduling has shown that sometimes it is better to\nwait for a bit [ID01], in what is called a non-work-conserving approach.\nBy waiting, a new and “better” request may arrive at the disk, and thus\noverall efﬁciency is increased. Of course, deciding when to wait, and for\nhow long, can be tricky; see the research paper for details, or check out\nthe Linux kernel implementation to see how such ideas are transitioned\ninto practice (if you are the ambitious sort).\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2820,
      "extraction_method": "Direct"
    },
    {
      "page_number": 452,
      "chapter": null,
      "content": "416\nHARD DISK DRIVES\n37.6\nSummary\nWe have presented a summary of how disks work. The summary is\nactually a detailed functional model; it does not describe the amazing\nphysics, electronics, and material science that goes into actual drive de-\nsign. For those interested in even more details of that nature, we suggest\na different major (or perhaps minor); for those that are happy with this\nmodel, good! We can now proceed to using the model to build more in-\nteresting systems on top of these incredible devices.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 560,
      "extraction_method": "Direct"
    },
    {
      "page_number": 453,
      "chapter": null,
      "content": "HARD DISK DRIVES\n417\nReferences\n[ADR03] “More Than an Interface: SCSI vs. ATA”\nDave Anderson, Jim Dykes, Erik Riedel\nFAST ’03, 2003\nOne of the best recent-ish references on how modern disk drives really work; a must read for anyone\ninterested in knowing more.\n[CKR72] “Analysis of Scanning Policies for Reducing Disk Seek Times”\nE.G. Coffman, L.A. Klimko, B. Ryan\nSIAM Journal of Computing, September 1972, Vol 1. No 3.\nSome of the early work in the ﬁeld of disk scheduling.\n[ID01] “Anticipatory Scheduling: A Disk-scheduling Framework\nTo Overcome Deceptive Idleness In Synchronous I/O”\nSitaram Iyer, Peter Druschel\nSOSP ’01, October 2001\nA cool paper showing how waiting can improve disk scheduling: better requests may be on their way!\n[JW91] “Disk Scheduling Algorithms Based On Rotational Position”\nD. Jacobson, J. Wilkes\nTechnical Report HPL-CSP-91-7rev1, Hewlett-Packard (February 1991)\nA more modern take on disk scheduling. It remains a technical report (and not a published paper)\nbecause the authors were scooped by Seltzer et al. [SCO90].\n[RW92] “An Introduction to Disk Drive Modeling”\nC. Ruemmler, J. Wilkes\nIEEE Computer, 27:3, pp. 17-28, March 1994\nA terriﬁc introduction to the basics of disk operation. Some pieces are out of date, but most of the basics\nremain.\n[SCO90] “Disk Scheduling Revisited”\nMargo Seltzer, Peter Chen, John Ousterhout\nUSENIX 1990\nA paper that talks about how rotation matters too in the world of disk scheduling.\n[SG04] “MEMS-based storage devices and standard disk interfaces: A square peg in a round\nhole?”\nSteven W. Schlosser, Gregory R. Ganger\nFAST ’04, pp. 87-100, 2004\nWhile the MEMS aspect of this paper hasn’t yet made an impact, the discussion of the contract between\nﬁle systems and disks is wonderful and a lasting contribution.\n[S09a] “Barracuda ES.2 data sheet”\nhttp://www.seagate.com/docs/pdf/datasheet/disc/ds cheetah 15k 5.pdf A data\nsheet; read at your own risk. Risk of what? Boredom.\n[S09b] “Cheetah 15K.5”\nhttp://www.seagate.com/docs/pdf/datasheet/disc/ds barracuda es.pdf See above\ncommentary on data sheets.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2112,
      "extraction_method": "Direct"
    },
    {
      "page_number": 454,
      "chapter": null,
      "content": "418\nHARD DISK DRIVES\nHomework\nThis homework uses disk.py to familiarize you with how a modern\nhard drive works. It has a lot of different options, and unlike most of\nthe other simulations, has a graphical animator to show you exactly what\nhappens when the disk is in action. See the README for details.\n1. Compute the seek, rotation, and transfer times for the following\nsets of requests: -a 0, -a 6, -a 30, -a 7,30,8, and ﬁnally -a\n10,11,12,13.\n2. Do the same requests above, but change the seek rate to different\nvalues: -S 2, -S 4, -S 8, -S 10, -S 40, -S 0.1. How do the\ntimes change?\n3. Do the same requests above, but change the rotation rate: -R 0.1,\n-R 0.5, -R 0.01. How do the times change?\n4. You might have noticed that some request streams would be bet-\nter served with a policy better than FIFO. For example, with the\nrequest stream -a 7,30,8, what order should the requests be pro-\ncessed in? Now run the shortest seek-time ﬁrst (SSTF) scheduler\n(-p SSTF) on the same workload; how long should it take (seek,\nrotation, transfer) for each request to be served?\n5. Now do the same thing, but using the shortest access-time ﬁrst\n(SATF) scheduler (-p SATF). Does it make any difference for the\nset of requests as speciﬁed by -a 7,30,8? Find a set of requests\nwhere SATF does noticeably better than SSTF; what are the condi-\ntions for a noticeable difference to arise?\n6. You might have noticed that the request stream -a 10,11,12,13\nwasn’t particularly well handled by the disk. Why is that? Can you\nintroduce a track skew to address this problem (-o skew, where\nskew is a non-negative integer)? Given the default seek rate, what\nshould the skew be to minimize the total time for this set of re-\nquests? What about for different seek rates (e.g., -S 2, -S 4)? In\ngeneral, could you write a formula to ﬁgure out the skew, given the\nseek rate and sector layout information?\n7. Multi-zone disks pack more sectors into the outer tracks. To conﬁg-\nure this disk in such a way, run with the -z ﬂag. Speciﬁcally, try\nrunning some requests against a disk run with -z 10,20,30 (the\nnumbers specify the angular space occupied by a sector, per track;\nin this example, the outer track will be packed with a sector every\n10 degrees, the middle track every 20 degrees, and the inner track\nwith a sector every 30 degrees). Run some random requests (e.g.,\n-a -1 -A 5,-1,0, which speciﬁes that random requests should\nbe used via the -a -1 ﬂag and that ﬁve requests ranging from 0 to\nthe max be generated), and see if you can compute the seek, rota-\ntion, and transfer times. Use different random seeds (-s 1, -s 2,\netc.). What is the bandwidth (in sectors per unit time) on the outer,\nmiddle, and inner tracks?\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2750,
      "extraction_method": "Direct"
    },
    {
      "page_number": 455,
      "chapter": null,
      "content": "HARD DISK DRIVES\n419\n8. Scheduling windows determine how many sector requests a disk\ncan examine at once in order to determine which sector to serve\nnext. Generate some random workloads of a lot of requests (e.g.,\n-A 1000,-1,0, with different seeds perhaps) and see how long\nthe SATF scheduler takes when the scheduling window is changed\nfrom 1 up to the number of requests (e.g., -w 1 up to -w 1000,\nand some values in between). How big of scheduling window is\nneeded to approach the best possible performance? Make a graph\nand see. Hint: use the -c ﬂag and don’t turn on graphics with -G\nto run these more quickly. When the scheduling window is set to 1,\ndoes it matter which policy you are using?\n9. Avoiding starvation is important in a scheduler. Can you think of a\nseries of requests such that a particular sector is delayed for a very\nlong time given a policy such as SATF? Given that sequence, how\ndoes it perform if you use a bounded SATF or BSATF scheduling\napproach? In this approach, you specify the scheduling window\n(e.g., -w 4) as well as the BSATF policy (-p BSATF); the scheduler\nthen will only move onto the next window of requests when all of\nthe requests in the current window have been serviced. Does this\nsolve the starvation problem? How does it perform, as compared\nto SATF? In general, how should a disk make this trade-off between\nperformance and starvation avoidance?\n10. All the scheduling policies we have looked at thus far are greedy,\nin that they simply pick the next best option instead of looking for\nthe optimal schedule over a set of requests. Can you ﬁnd a set of\nrequests in which this greedy approach is not optimal?\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 1697,
      "extraction_method": "Direct"
    },
    {
      "page_number": 456,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 457,
      "chapter": null,
      "content": "38\nRedundant Arrays of Inexpensive Disks\n(RAIDs)\nWhen we use a disk, we sometimes wish it to be faster; I/O operations\nare slow and thus can be the bottleneck for the entire system. When we\nuse a disk, we sometimes wish it to be larger; more and more data is being\nput online and thus our disks are getting fuller and fuller. When we use\na disk, we sometimes wish for it to be more reliable; when a disk fails, if\nour data isn’t backed up, all that valuable data is gone.\nCRUX: HOW TO MAKE A LARGE, FAST, RELIABLE DISK\nHow can we make a large, fast, and reliable storage system? What are\nthe key techniques? What are trade-offs between different approaches?\nIn this chapter, we introduce the Redundant Array of Inexpensive\nDisks better known as RAID [P+88], a technique to use multiple disks in\nconcert to build a faster, bigger, and more reliable disk system. The term\nwas introduced in the late 1980s by a group of researchers at U.C. Berke-\nley (led by Professors David Patterson and Randy Katz and then student\nGarth Gibson); it was around this time that many different researchers si-\nmultaneously arrived upon the basic idea of using multiple disks to build\na better storage system [BG88, K86,K88,PB86,SG86].\nExternally, a RAID looks like a disk: a group of blocks one can read\nor write. Internally, the RAID is a complex beast, consisting of multiple\ndisks, memory (both volatile and non-), and one or more processors to\nmanage the system. A hardware RAID is very much like a computer\nsystem, specialized for the task of managing a group of disks.\nRAIDs offer a number of advantages over a single disk. One advan-\ntage is performance. Using multiple disks in parallel can greatly speed\nup I/O times. Another beneﬁt is capacity. Large data sets demand large\ndisks. Finally, RAIDs can improve reliability; spreading data across mul-\ntiple disks (without RAID techniques) makes the data vulnerable to the\nloss of a single disk; with some form of redundancy, RAIDs can tolerate\nthe loss of a disk and keep operating as if nothing were wrong.\n421\n",
      "content_length": 2049,
      "extraction_method": "Direct"
    },
    {
      "page_number": 458,
      "chapter": null,
      "content": "422\nREDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAIDS)\nTIP: TRANSPARENCY ENABLES DEPLOYMENT\nWhen considering how to add new functionality to a system, one should\nalways consider whether such functionality can be added transparently,\nin a way that demands no changes to the rest of the system. Requiring a\ncomplete rewrite of the existing software (or radical hardware changes)\nlessens the chance of impact of an idea. RAID is a perfect example, and\ncertainly its transparency contributed to its success; administrators could\ninstall a SCSI-based RAID storage array instead of a SCSI disk, and the\nrest of the system (host computer, OS, etc.) did not have to change one bit\nto start using it. By solving this problem of deployment, RAID was made\nmore successful from day one.\nAmazingly, RAIDs provide these advantages transparently to systems\nthat use them, i.e., a RAID just looks like a big disk to the host system. The\nbeauty of transparency, of course, is that it enables one to simply replace\na disk with a RAID and not change a single line of software; the operat-\ning system and client applications continue to operate without modiﬁca-\ntion. In this manner, transparency greatly improves the deployability of\nRAID, enabling users and administrators to put a RAID to use without\nworries of software compatibility.\nWe now discuss some of the important aspects of RAIDs. We begin\nwith the interface, fault model, and then discuss how one can evaluate a\nRAID design along three important axes: capacity, reliability, and perfor-\nmance. We then discuss a number of other issues that are important to\nRAID design and implementation.\n38.1\nInterface And RAID Internals\nTo a ﬁle system above, a RAID looks like a big, (hopefully) fast, and\n(hopefully) reliable disk. Just as with a single disk, it presents itself as\na linear array of blocks, each of which can be read or written by the ﬁle\nsystem (or other client).\nWhen a ﬁle system issues a logical I/O request to the RAID, the RAID\ninternally must calculate which disk (or disks) to access in order to com-\nplete the request, and then issue one or more physical I/Os to do so. The\nexact nature of these physical I/Os depends on the RAID level, as we will\ndiscuss in detail below. However, as a simple example, consider a RAID\nthat keeps two copies of each block (each one on a separate disk); when\nwriting to such a mirrored RAID system, the RAID will have to perform\ntwo physical I/Os for every one logical I/O it is issued.\nA RAID system is often built as a separate hardware box, with a stan-\ndard connection (e.g., SCSI, or SATA) to a host.\nInternally, however,\nRAIDs are fairly complex, consisting of a microcontroller that runs ﬁrmware\nto direct the operation of the RAID, volatile memory such as DRAM\nto buffer data blocks as they are read and written, and in some cases,\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2872,
      "extraction_method": "Direct"
    },
    {
      "page_number": 459,
      "chapter": null,
      "content": "REDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAIDS)\n423\nnon-volatile memory to buffer writes safely and perhaps even special-\nized logic to perform parity calculations (useful in some RAID levels, as\nwe will also see below). At a high level, a RAID is very much a special-\nized computer system: it has a processor, memory, and disks; however,\ninstead of running applications, it runs specialized software designed to\noperate the RAID.\n38.2\nFault Model\nTo understand RAID and compare different approaches, we must have\na fault model in mind. RAIDs are designed to detect and recover from\ncertain kinds of disk faults; thus, knowing exactly which faults to expect\nis critical in arriving upon a working design.\nThe ﬁrst fault model we will assume is quite simple, and has been\ncalled the fail-stop fault model [S84]. In this model, a disk can be in\nexactly one of two states: working or failed. With a working disk, all\nblocks can be read or written. In contrast, when a disk has failed, we\nassume it is permanently lost.\nOne critical aspect of the fail-stop model is what it assumes about fault\ndetection. Speciﬁcally, when a disk has failed, we assume that this is\neasily detected. For example, in a RAID array, we would assume that the\nRAID controller hardware (or software) can immediately observe when a\ndisk has failed.\nThus, for now, we do not have to worry about more complex “silent”\nfailures such as disk corruption. We also do not have to worry about a sin-\ngle block becoming inaccessible upon an otherwise working disk (some-\ntimes called a latent sector error). We will consider these more complex\n(and unfortunately, more realistic) disk faults later.\n38.3\nHow To Evaluate A RAID\nAs we will soon see, there are a number of different approaches to\nbuilding a RAID. Each of these approaches has different characteristics\nwhich are worth evaluating, in order to understand their strengths and\nweaknesses.\nSpeciﬁcally, we will evaluate each RAID design along three axes. The\nﬁrst axis is capacity; given a set of N disks, how much useful capacity is\navailable to systems that use the RAID? Without redundancy, the answer\nis obviously N; however, if we have a system that keeps a two copies of\neach block, we will obtain a useful capacity of N/2. Different schemes\n(e.g., parity-based ones) tend to fall in between.\nThe second axis of evaluation is reliability. How many disk faults can\nthe given design tolerate? In alignment with our fault model, we assume\nonly that an entire disk can fail; in later chapters (i.e., on data integrity),\nwe’ll think about how to handle more complex failure modes.\nFinally, the third axis is performance. Performance is somewhat chal-\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2712,
      "extraction_method": "Direct"
    },
    {
      "page_number": 460,
      "chapter": null,
      "content": "424\nREDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAIDS)\nlenging to evaluate, because it depends heavily on the workload pre-\nsented to the disk array. Thus, before evaluating performance, we will\nﬁrst present a set of typical workloads that one should consider.\nWe now consider three important RAID designs: RAID Level 0 (strip-\ning), RAID Level 1 (mirroring), and RAID Levels 4/5 (parity-based re-\ndundancy). The naming of each of these designs as a “level” stems from\nthe pioneering work of Patterson, Gibson, and Katz at Berkeley [P+88].\n38.4\nRAID Level 0: Striping\nThe ﬁrst RAID level is actually not a RAID level at all, in that there is\nno redundancy. However, RAID level 0, or striping as it is better known,\nserves as an excellent upper-bound on performance and capacity and\nthus is worth understanding.\nThe simplest form of striping will stripe blocks across the disks of the\nsystem as follows (assume here a 4-disk array):\nDisk 0\nDisk 1\nDisk 2\nDisk 3\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\nTable 38.1: RAID-0: Simple Striping\nFrom Table 38.1, you get the basic idea: spread the blocks of the array\nacross the disks in a round-robin fashion. This approach is designed to\nextract the most parallelism from the array when requests are made for\ncontiguous chunks of the array (as in a large, sequential read, for exam-\nple). We call the blocks in the same row a stripe; thus, blocks 0, 1, 2, and\n3 are in the same stripe above.\nIn the example, we have made the simplifying assumption that only 1\nblock (each of say size 4KB) is placed on each disk before moving on to\nthe next. However, this arrangement need not be the case. For example,\nwe could arrange the blocks across disks as in Table 38.2:\nDisk 0\nDisk 1\nDisk 2\nDisk 3\n0\n2\n4\n6\nchunk size:\n1\n3\n5\n7\n2 blocks\n8\n10\n12\n14\n9\n11\n13\n15\nTable 38.2: Striping with a Bigger Chunk Size\nIn this example, we place two 4KB blocks on each disk before moving\non to the next disk. Thus, the chunk size of this RAID array is 8KB, and\na stripe thus consists of 4 chunks or 32KB of data.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2070,
      "extraction_method": "Direct"
    },
    {
      "page_number": 461,
      "chapter": null,
      "content": "REDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAIDS)\n425\nASIDE: THE RAID MAPPING PROBLEM\nBefore studying the capacity, reliability, and performance characteristics\nof the RAID, we ﬁrst present an aside on what we call the mapping prob-\nlem. This problem arises in all RAID arrays; simply put, given a logical\nblock to read or write, how does the RAID know exactly which physical\ndisk and offset to access?\nFor these simple RAID levels, we do not need much sophistication in\norder to correctly map logical blocks onto their physical locations. Take\nthe ﬁrst striping example above (chunk size = 1 block = 4KB). In this case,\ngiven a logical block address A, the RAID can easily compute the desired\ndisk and offset with two simple equations:\nDisk\n= A % number_of_disks\nOffset = A / number_of_disks\nNote that these are all integer operations (e.g., 4 / 3 = 1 not 1.33333...).\nLet’s see how these equations work for a simple example. Imagine in the\nﬁrst RAID above that a request arrives for block 14. Given that there are\n4 disks, this would mean that the disk we are interested in is (14 % 4 = 2):\ndisk 2. The exact block is calculated as (14 / 4 = 3): block 3. Thus, block\n14 should be found on the fourth block (block 3, starting at 0) of the third\ndisk (disk 2, starting at 0), which is exactly where it is.\nYou can think about how these equations would be modiﬁed to support\ndifferent chunk sizes. Try it! It’s not too hard.\nChunk Sizes\nChunk size mostly affects performance of the array. For example, a small\nchunk size implies that many ﬁles will get striped across many disks, thus\nincreasing the parallelism of reads and writes to a single ﬁle; however, the\npositioning time to access blocks across multiple disks increases, because\nthe positioning time for the entire request is determined by the maximum\nof the positioning times of the requests across all drives.\nA big chunk size, on the other hand, reduces such intra-ﬁle paral-\nlelism, and thus relies on multiple concurrent requests to achieve high\nthroughput. However, large chunk sizes reduce positioning time; if, for\nexample, a single ﬁle ﬁts within a chunk and thus is placed on a single\ndisk, the positioning time incurred while accessing it will just be the po-\nsitioning time of a single disk.\nThus, determining the “best” chunk size is hard to do, as it requires a\ngreat deal of knowledge about the workload presented to the disk system\n[CL95]. For the rest of this discussion, we will assume that the array uses\na chunk size of a single block (4KB). Most arrays use larger chunk sizes\n(e.g., 64 KB), but for the issues we discuss below, the exact chunk size\ndoes not matter; thus we use a single block for the sake of simplicity.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2736,
      "extraction_method": "Direct"
    },
    {
      "page_number": 462,
      "chapter": null,
      "content": "426\nREDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAIDS)\nBack To RAID-0 Analysis\nLet us now evaluate the capacity, reliability, and performance of striping.\nFrom the perspective of capacity, it is perfect: given N disks, striping de-\nlivers N disks worth of useful capacity. From the standpoint of reliability,\nstriping is also perfect, but in the bad way: any disk failure will lead to\ndata loss. Finally, performance is excellent: all disks are utilized, often in\nparallel, to service user I/O requests.\nEvaluating RAID Performance\nIn analyzing RAID performance, one can consider two different perfor-\nmance metrics. The ﬁrst is single-request latency. Understanding the la-\ntency of a single I/O request to a RAID is useful as it reveals how much\nparallelism can exist during a single logical I/O operation. The second\nis steady-state throughput of the RAID, i.e., the total bandwidth of many\nconcurrent requests. Because RAIDs are often used in high-performance\nenvironments, the steady-state bandwidth is critical, and thus will be the\nmain focus of our analyses.\nTo understand throughput in more detail, we need to put forth some\nworkloads of interest. We will assume, for this discussion, that there\nare two types of workloads: sequential and random. With a sequential\nworkload, we assume that requests to the array come in large contiguous\nchunks; for example, a request (or series of requests) that accesses 1 MB\nof data, starting at block (B) and ending at block (B + 1 MB), would be\ndeemed sequential. Sequential workloads are common in many environ-\nments (think of searching through a large ﬁle for a keyword), and thus\nare considered important.\nFor random workloads, we assume that each request is rather small,\nand that each request is to a different random location on disk. For exam-\nple, a random stream of requests may ﬁrst access 4KB at logical address\n10, then at logical address 550,000, then at 20,100, and so forth. Some im-\nportant workloads, such as transactional workloads on a database man-\nagement system (DBMS), exhibit this type of access pattern, and thus it is\nconsidered an important workload.\nOf course, real workloads are not so simple, and often have a mix\nof sequential and random-seeming components as well as behaviors in-\nbetween the two. For simplicity, we just consider these two possibilities.\nAs you can tell, sequential and random workloads will result in widely\ndifferent performance characteristics from a disk. With sequential access,\na disk operates in its most efﬁcient mode, spending little time seeking and\nwaiting for rotation and most of its time transferring data. With random\naccess, just the opposite is true: most time is spent seeking and waiting\nfor rotation and relatively little time is spent transferring data. To capture\nthis difference in our analysis, we will assume that a disk can transfer\ndata at S MB/s under a sequential workload, and R MB/s when under a\nrandom workload. In general, S is much greater than R.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 3022,
      "extraction_method": "Direct"
    },
    {
      "page_number": 463,
      "chapter": null,
      "content": "REDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAIDS)\n427\nTo make sure we understand this difference, let’s do a simple exer-\ncise. Speciﬁcally, lets calculate S and R given the following disk charac-\nteristics. Assume a sequential transfer of size 10 MB on average, and a\nrandom transfer of 10 KB on average. Also, assume the following disk\ncharacteristics:\nAverage seek time\n7 ms\nAverage rotational delay\n3 ms\nTransfer rate of disk\n50 MB/s\nTo compute S, we need to ﬁrst ﬁgure out how time is spent in a typical\n10 MB transfer. First, we spend 7 ms seeking, and then 3 ms rotating.\nFinally, transfer begins; 10 MB @ 50 MB/s leads to 1/5th of a second, or\n200 ms, spent in transfer. Thus, for each 10 MB request, we spend 210 ms\ncompleting the request. To compute S, we just need to divide:\nS = Amount of Data\nT ime to access\n= 10 MB\n210 ms = 47.62 MB/s\nAs we can see, because of the large time spent transferring data, S is\nvery near the peak bandwidth of the disk (the seek and rotational costs\nhave been amortized).\nWe can compute R similarly. Seek and rotation are the same; we then\ncompute the time spent in transfer, which is 10 KB @ 50 MB/s, or 0.195\nms.\nR = Amount of Data\nT ime to access =\n10 KB\n10.195 ms = 0.981 MB/s\nAs we can see, R is less than 1 MB/s, and S/R is almost 50.\nBack To RAID-0 Analysis, Again\nLet’s now evaluate the performance of striping. As we said above, it is\ngenerally good. From a latency perspective, for example, the latency of a\nsingle-block request should be just about identical to that of a single disk;\nafter all, RAID-0 will simply redirect that request to one of its disks.\nFrom the perspective of steady-state throughput, we’d expect to get\nthe full bandwidth of the system. Thus, throughput equals N (the number\nof disks) multiplied by S (the sequential bandwidth of a single disk). For\na large number of random I/Os, we can again use all of the disks, and\nthus obtain N · R MB/s. As we will see below, these values are both\nthe simplest to calculate and will serve as an upper bound in comparison\nwith other RAID levels.\n38.5\nRAID Level 1: Mirroring\nOur ﬁrst RAID level beyond striping is known as RAID level 1, or\nmirroring. With a mirrored system, we simply make more than one copy\nof each block in the system; each copy should be placed on a separate\ndisk, of course. By doing so, we can tolerate disk failures.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2393,
      "extraction_method": "Direct"
    },
    {
      "page_number": 464,
      "chapter": null,
      "content": "428\nREDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAIDS)\nIn a typical mirrored system, we will assume that for each logical\nblock, the RAID keeps two physical copies of it. Here is an example:\nDisk 0\nDisk 1\nDisk 2\nDisk 3\n0\n0\n1\n1\n2\n2\n3\n3\n4\n4\n5\n5\n6\n6\n7\n7\nTable 38.3: Simple RAID-1: Mirroring\nIn the example, disk 0 and disk 1 have identical contents, and disk 2\nand disk 3 do as well; the data is striped across these mirror pairs. In fact,\nyou may have noticed that there are a number of different ways to place\nblock copies across the disks. The arrangement above is a common one\nand is sometimes called RAID-10 or (RAID 1+0) because it uses mirrored\npairs (RAID-1) and then stripes (RAID-0) on top of them; another com-\nmon arrangement is RAID-01 (or RAID 0+1), which contains two large\nstriping (RAID-0) arrays, and then mirrors (RAID-1) on top of them. For\nnow, we will just talk about mirroring assuming the above layout.\nWhen reading a block from a mirrored array, the RAID has a choice: it\ncan read either copy. For example, if a read to logical block 5 is issued to\nthe RAID, it is free to read it from either disk 2 or disk 3. When writing\na block, though, no such choice exists: the RAID must update both copies\nof the data, in order to preserve reliability. Do note, though, that these\nwrites can take place in parallel; for example, a write to logical block 5\ncould proceed to disks 2 and 3 at the same time.\nRAID-1 Analysis\nLet us assess RAID-1. From a capacity standpoint, RAID-1 is expensive;\nwith the mirroring level = 2, we only obtain half of our peak useful ca-\npacity. Thus, with N disks, the useful capacity of mirroring is N/2.\nFrom a reliability standpoint, RAID-1 does well. It can tolerate the fail-\nure of any one disk. You may also notice RAID-1 can actually do better\nthan this, with a little luck. Imagine, in the ﬁgure above, that disk 0 and\ndisk 2 both failed. In such a situation, there is no data loss! More gen-\nerally, a mirrored system (with mirroring level of 2) can tolerate 1 disk\nfailure for certain, and up to N/2 failures depending on which disks fail.\nIn practice, we generally don’t like to leave things like this to chance; thus\nmost people consider mirroring to be good for handling a single failure.\nFinally, we analyze performance. From the perspective of the latency\nof a single read request, we can see it is the same as the latency on a single\ndisk; all the RAID-1 does is direct the read to one of its copies. A write\nis a little different: it requires two physical writes to complete before it\nis done. These two writes happen in parallel, and thus the time will be\nroughly equivalent to the time of a single write; however, because the\nlogical write must wait for both physical writes to complete, it suffers the\nworst-case seek and rotational delay of the two requests, and thus (on\naverage) will be slightly higher than a write to a single disk.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2941,
      "extraction_method": "Direct"
    },
    {
      "page_number": 465,
      "chapter": null,
      "content": "REDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAIDS)\n429\nASIDE: THE RAID CONSISTENT-UPDATE PROBLEM\nBefore analyzing RAID-1, let us ﬁrst discuss a problem that arises in\nany multi-disk RAID system, known as the consistent-update problem\n[DAA05]. The problem occurs on a write to any RAID that has to up-\ndate multiple disks during a single logical operation. In this case, let us\nassume we are considering a mirrored disk array.\nImagine the write is issued to the RAID, and then the RAID decides that\nit must be written to two disks, disk 0 and disk 1. The RAID then issues\nthe write to disk 0, but just before the RAID can issue the request to disk\n1, a power loss (or system crash) occurs. In this unfortunate case, let us\nassume that the request to disk 0 completed (but clearly the request to\ndisk 1 did not, as it was never issued).\nThe result of this untimely power loss is that the two copies of the block\nare now inconsistent; the copy on disk 0 is the new version, and the copy\non disk 1 is the old. What we would like to happen is for the state of both\ndisks to change atomically, i.e., either both should end up as the new\nversion or neither.\nThe general way to solve this problem is to use a write-ahead log of some\nkind to ﬁrst record what the RAID is about to do (i.e., update two disks\nwith a certain piece of data) before doing it. By taking this approach, we\ncan ensure that in the presence of a crash, the right thing will happen; by\nrunning a recovery procedure that replays all pending transactions to the\nRAID, we can ensure that no two mirrored copies (in the RAID-1 case)\nare out of sync.\nOne last note: because logging to disk on every write is prohibitively\nexpensive, most RAID hardware includes a small amount of non-volatile\nRAM (e.g., battery-backed) where it performs this type of logging. Thus,\nconsistent update is provided without the high cost of logging to disk.\nTo analyze steady-state throughput, let us start with the sequential\nworkload. When writing out to disk sequentially, each logical write must\nresult in two physical writes; for example, when we write logical block\n0 (in the ﬁgure above), the RAID internally would write it to both disk\n0 and disk 1. Thus, we can conclude that the maximum bandwidth ob-\ntained during sequential writing to a mirrored array is ( N\n2 · S), or half the\npeak bandwidth.\nUnfortunately, we obtain the exact same performance during a se-\nquential read. One might think that a sequential read could do better,\nbecause it only needs to read one copy of the data, not both. However,\nlet’s use an example to illustrate why this doesn’t help much. Imagine we\nneed to read blocks 0, 1, 2, 3, 4, 5, 6, and 7. Let’s say we issue the read of\n0 to disk 0, the read of 1 to disk 2, the read of 2 to disk 1, and the read of\n3 to disk 3. We continue by issuing reads to 4, 5, 6, and 7 to disks 0, 2, 1,\nand 3, respectively. One might naively think that because we are utilizing\nall disks, we are achieving the full bandwidth of the array.\nTo see that this is not the case, however, consider the requests a single\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 3107,
      "extraction_method": "Direct"
    },
    {
      "page_number": 466,
      "chapter": null,
      "content": "430\nREDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAIDS)\ndisk receives (say disk 0). First, it gets a request for block 0; then, it gets a\nrequest for block 4 (skipping block 2). In fact, each disk receives a request\nfor every other block. While it is rotating over the skipped block, it is\nnot delivering useful bandwidth to the client. Thus, each disk will only\ndeliver half its peak bandwidth. And thus, the sequential read will only\nobtain a bandwidth of ( N\n2 · S) MB/s.\nRandom reads are the best case for a mirrored RAID. In this case, we\ncan distribute the reads across all the disks, and thus obtain the full pos-\nsible bandwidth. Thus, for random reads, RAID-1 delivers N · R MB/s.\nFinally, random writes perform as you might expect: N\n2 ·R MB/s. Each\nlogical write must turn into two physical writes, and thus while all the\ndisks will be in use, the client will only perceive this as half the available\nbandwidth. Even though a write to logical block X turns into two parallel\nwrites to two different physical disks, the bandwidth of many small re-\nquests only achieves half of what we saw with striping. As we will soon\nsee, getting half the available bandwidth is actually pretty good!\n38.6\nRAID Level 4: Saving Space With Parity\nWe now present a different method of adding redundancy to a disk ar-\nray known as parity. Parity-based approaches attempt to use less capac-\nity and thus overcome the huge space penalty paid by mirrored systems.\nThey do so at a cost, however: performance.\nIn a ﬁve-disk RAID-4 system, we might observe the following layout:\nDisk 0\nDisk 1\nDisk 2\nDisk 3\nDisk 4\n0\n1\n2\n3\nP0\n4\n5\n6\n7\nP1\n8\n9\n10\n11\nP2\n12\n13\n14\n15\nP3\nAs you can see, for each stripe of data, we have added a single par-\nity block that stores the redundant information for that stripe of blocks.\nFor example, parity block P1 has redundant information that it calculated\nfrom blocks 4, 5, 6, and 7.\nTo compute parity, we need to use a mathematical function that en-\nables us to withstand the loss of any one block from our stripe. It turns\nout the simple function XOR does the trick quite nicely. For a given set of\nbits, the XOR of all of those bits returns a 0 if there are an even number of\n1’s in the bits, and a 1 if there are an odd number of 1’s. For example:\nC0\nC1\nC2\nC3\nP\n0\n0\n1\n1\nXOR(0,0,1,1) = 0\n0\n1\n0\n0\nXOR(0,1,0,0) = 1\nIn the ﬁrst row (0,0,1,1), there are two 1’s (C2, C3), and thus XOR of\nall of those values will be 0 (P); similarly, in the second row there is only\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2518,
      "extraction_method": "Direct"
    },
    {
      "page_number": 467,
      "chapter": null,
      "content": "REDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAIDS)\n431\none 1 (C1), and thus the XOR must be 1 (P). You can remember this in a\nvery simple way: that the number of 1’s in any row must be an even (not\nodd) number; that is the invariant that the RAID must maintain in order\nfor parity to be correct.\nFrom the example above, you might also be able to guess how parity\ninformation can be used to recover from a failure. Imagine the column la-\nbeled C2 is lost. To ﬁgure out what values must have been in the column,\nwe simply have to read in all the other values in that row (including the\nXOR’d parity bit) and reconstruct the right answer. Speciﬁcally, assume\nthe ﬁrst row’s value in column C2 is lost (it is a 1); by reading the other\nvalues in that row (0 from C0, 0 from C1, 1 from C3, and 0 from the parity\ncolumn P), we get the values 0, 0, 1, and 0. Because we know that XOR\nkeeps an even number of 1’s in each row, we know what the missing data\nmust be: a 1. And that is how reconstruction works in a XOR-based par-\nity scheme! Note also how we compute the reconstructed value: we just\nXOR the data bits and the parity bits together, in the same way that we\ncalculated the parity in the ﬁrst place.\nNow you might be wondering: we are talking about XORing all of\nthese bits, and yet above we know that the RAID places 4KB (or larger)\nblocks on each disk; how do we apply XOR to a bunch of blocks to com-\npute the parity? It turns out this is easy as well. Simply perform a bitwise\nXOR across each bit of the data blocks; put the result of each bitwise XOR\ninto the corresponding bit slot in the parity block. For example, if we had\nblocks of size 4 bits (yes, this is still quite a bit smaller than a 4KB block,\nbut you get the picture), they might look something like this:\nBlock0\nBlock1\nBlock2\nBlock3\nParity\n00\n10\n11\n10\n11\n10\n01\n00\n01\n10\nAs you can see from the ﬁgure, the parity is computed for each bit of\neach block and the result placed in the parity block.\nRAID-4 Analysis\nLet us now analyze RAID-4. From a capacity standpoint, RAID-4 uses 1\ndisk for parity information for every group of disks it is protecting. Thus,\nour useful capacity for a RAID group is (N-1).\nReliability is also quite easy to understand: RAID-4 tolerates 1 disk\nfailure and no more. If more than one disk is lost, there is simply no way\nto reconstruct the lost data.\nFinally, there is performance. This time, let us start by analyzing steady-\nstate throughput. Sequential read performance can utilize all of the disks\nexcept for the parity disk, and thus deliver a peak effective bandwidth of\n(N −1) · S MB/s (an easy case).\nTo understand the performance of sequential writes, we must ﬁrst un-\nderstand how they are done. When writing a big chunk of data to disk,\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2783,
      "extraction_method": "Direct"
    },
    {
      "page_number": 468,
      "chapter": null,
      "content": "432\nREDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAIDS)\nRAID-4 can perform a simple optimization known as a full-stripe write.\nFor example, imagine the case where the blocks 0, 1, 2, and 3 have been\nsent to the RAID as part of a write request (Table 38.4).\nDisk 0\nDisk 1\nDisk 2\nDisk 3\nDisk 4\n0\n1\n2\n3\nP0\n4\n5\n6\n7\nP1\n8\n9\n10\n11\nP2\n12\n13\n14\n15\nP3\nTable 38.4: Full-stripe Writes In RAID-4\nIn this case, the RAID can simply calculate the new value of P0 (by\nperforming an XOR across the blocks 0, 1, 2, and 3) and then write all of\nthe blocks (including the parity block) to the ﬁve disks above in parallel\n(highlighted in gray in the ﬁgure). Thus, full-stripe writes are the most\nefﬁcient way for RAID-4 to write to disk.\nOnce we understand the full-stripe write, calculating the performance\nof sequential writes on RAID-4 is easy; the effective bandwidth is also\n(N −1)·S MB/s. Even though the parity disk is constantly in use during\nthe operation, the client does not gain performance advantage from it.\nNow let us analyze the performance of random reads. As you can also\nsee from the ﬁgure above, a set of 1-block random reads will be spread\nacross the data disks of the system but not the parity disk. Thus, the\neffective performance is: (N −1) · R MB/s.\nRandom writes, which we have saved for last, present the most in-\nteresting case for RAID-4. Imagine we wish to overwrite block 1 in the\nexample above. We could just go ahead and overwrite it, but that would\nleave us with a problem: the parity block P0 would no longer accurately\nreﬂect the correct parity value for the stripe. Thus, in this example, P0\nmust also be updated. But how can we update it both correctly and efﬁ-\nciently?\nIt turns out there are two methods. The ﬁrst, known as additive parity,\nrequires us to do the following. To compute the value of the new parity\nblock, read in all of the other data blocks in the stripe in parallel (in the\nexample, blocks 0, 2, and 3) and XOR those with the new block (1). The\nresult is your new parity block. To complete the write, you can then write\nthe new data and new parity to their respective disks, also in parallel.\nThe problem with this technique is that it scales with the number of\ndisks, and thus in larger RAIDs requires a high number of reads to com-\npute parity. Thus, the subtractive parity method.\nFor example, imagine this string of bits (4 data bits, one parity):\nC0\nC1\nC2\nC3\nP\n0\n0\n1\n1\nXOR(0,0,1,1) = 0\nLet’s imagine that we wish to overwrite bit C2 with a new value which\nwe will call C2(new). The subtractive method works in three steps. First,\nwe read in the old data at C2 (C2(old) = 1) and the old parity (P(old) =\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2684,
      "extraction_method": "Direct"
    },
    {
      "page_number": 469,
      "chapter": null,
      "content": "REDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAIDS)\n433\n0). Then, we compare the old data and the new data; if they are the same\n(e.g., C2(new) = C2(old)), then we know the parity bit will also remain\nthe same (i.e., P(new) = P(old)). If, however, they are different, then we\nmust ﬂip the old parity bit to the opposite of its current state, that is, if\n(P(old) == 1), P(new) will be set to 0; if (P(old) == 0), P(new) will be set to\n1. We can express this whole mess neatly with XOR as it turns out (if you\nunderstand XOR, this will now make sense to you):\nP(new) = (C(old) XOR C(new)) XOR P(old)\nBecause we are dealing with blocks, not bits, we perform this calcula-\ntion over all the bits in the block (e.g., 4096 bytes in each block multiplied\nby 8 bits per byte). Thus, in most cases, the new block will be different\nthan the old block and thus the new parity block will too.\nYou should now be able to ﬁgure out when we would use the additive\nparity calculation and when we would use the subtractive method. Think\nabout how many disks would need to be in the system so that the additive\nmethod performs fewer I/Os than the subtractive method; what is the\ncross-over point?\nFor this performance analysis, let us assume we are using the subtrac-\ntive method. Thus, for each write, the RAID has to perform 4 physical\nI/Os (two reads and two writes). Now imagine there are lots of writes\nsubmitted to the RAID; how many can RAID-4 perform in parallel? To\nunderstand, let us again look at the RAID-4 layout (Figure 38.5).\nDisk 0\nDisk 1\nDisk 2\nDisk 3\nDisk 4\n0\n1\n2\n3\nP0\n∗4\n5\n6\n7\n+P1\n8\n9\n10\n11\nP2\n12\n∗13\n14\n15\n+P3\nTable 38.5: Example: Writes To 4, 13, And Respective Parity Blocks\nNow imagine there were 2 small writes submitted to the RAID-4 at\nabout the same time, to blocks 4 and 13 (marked with ∗in the diagram).\nThe data for those disks is on disks 0 and 1, and thus the read and write\nto data could happen in parallel, which is good. The problem that arises\nis with the parity disk; both the requests have to read the related parity\nblocks for 4 and 13, parity blocks 1 and 3 (marked with +). Hopefully, the\nissue is now clear: the parity disk is a bottleneck under this type of work-\nload; we sometimes thus call this the small-write problem for parity-\nbased RAIDs. Thus, even though the data disks could be accessed in\nparallel, the parity disk prevents any parallelism from materializing; all\nwrites to the system will be serialized because of the parity disk. Because\nthe parity disk has to perform two I/Os (one read, one write) per logical\nI/O, we can compute the performance of small random writes in RAID-4\nby computing the parity disk’s performance on those two I/Os, and thus\nwe achieve (R/2) MB/s. RAID-4 throughput under random small writes\nis terrible; it does not improve as you add disks to the system.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2857,
      "extraction_method": "Direct"
    },
    {
      "page_number": 470,
      "chapter": null,
      "content": "434\nREDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAIDS)\nWe conclude by analyzing I/O latency in RAID-4. As you now know,\na single read (assuming no failure) is just mapped to a single disk, and\nthus its latency is equivalent to the latency of a single disk request. The\nlatency of a single write requires two reads and then two writes; the reads\ncan happen in parallel, as can the writes, and thus total latency is about\ntwice that of a single disk (with some differences because we have to wait\nfor both reads to complete and thus get the worst-case positioning time,\nbut then the updates don’t incur seek cost and thus may be a better-than-\naverage positioning cost).\n38.7\nRAID Level 5: Rotating Parity\nTo address the small-write problem (at least, partially), Patterson, Gib-\nson, and Katz introduced RAID-5. RAID-5 works almost identically to\nRAID-4, except that it rotates the parity block across drives (Figure 38.6).\nDisk 0\nDisk 1\nDisk 2\nDisk 3\nDisk 4\n0\n1\n2\n3\nP0\n5\n6\n7\nP1\n4\n10\n11\nP2\n8\n9\n15\nP3\n12\n13\n14\nP4\n16\n17\n18\n19\nTable 38.6: RAID-5 With Rotated Parity\nAs you can see, the parity block for each stripe is now rotated across\nthe disks, in order to remove the parity-disk bottleneck for RAID-4.\nRAID-5 Analysis\nMuch of the analysis for RAID-5 is identical to RAID-4. For example, the\neffective capacity and failure tolerance of the two levels are identical. So\nare sequential read and write performance. The latency of a single request\n(whether a read or a write) is also the same as RAID-4.\nRandom read performance is a little better, because we can utilize all of\nthe disks. Finally, random write performance improves noticeably over\nRAID-4, as it allows for parallelism across requests. Imagine a write to\nblock 1 and a write to block 10; this will turn into requests to disk 1 and\ndisk 4 (for block 1 and its parity) and requests to disk 0 and disk 2 (for\nblock 10 and its parity). Thus, they can proceed in parallel. In fact, we\ncan generally assume that that given a large number of random requests,\nwe will be able to keep all the disks about evenly busy. If that is the case,\nthen our total bandwidth for small writes will be N\n4 · R MB/s. The factor\nof four loss is due to the fact that each RAID-5 write still generates 4 total\nI/O operations, which is simply the cost of using parity-based RAID.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2356,
      "extraction_method": "Direct"
    },
    {
      "page_number": 471,
      "chapter": null,
      "content": "REDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAIDS)\n435\nRAID-0\nRAID-1\nRAID-4\nRAID-5\nCapacity\nN\nN/2\nN −1\nN −1\nReliability\n0\n1 (for sure)\n1\n1\nN\n2 (if lucky)\nThroughput\nSequential Read\nN · S\n(N/2) · S\n(N −1) · S\n(N −1) · S\nSequential Write\nN · S\n(N/2) · S\n(N −1) · S\n(N −1) · S\nRandom Read\nN · R\nN · R\n(N −1) · R\nN · R\nRandom Write\nN · R\n(N/2) · R\n1\n2 · R\nN\n4 R\nLatency\nRead\nD\nD\nD\nD\nWrite\nD\nD\n2D\n2D\nTable 38.7: RAID Capacity, Reliability, and Performance\nBecause RAID-5 is basically identical to RAID-4 except in the few cases\nwhere it is better, it has almost completely replaced RAID-4 in the market-\nplace. The only place where it has not is in systems that know they will\nnever perform anything other than a large write, thus avoiding the small-\nwrite problem altogether [HLM94]; in those cases, RAID-4 is sometimes\nused as it is slightly simpler to build.\n38.8\nRAID Comparison: A Summary\nWe now summarize our simpliﬁed comparison of RAID levels in Ta-\nble 38.7. Note that we have omitted a number of details to simplify our\nanalysis. For example, when writing in a mirrored system, the average\nseek time is a little higher than when writing to just a single disk, because\nthe seek time is the max of two seeks (one on each disk). Thus, random\nwrite performance to two disks will generally be a little less than random\nwrite performance of a single disk. Also, when updating the parity disk\nin RAID-4/5, the ﬁrst read of the old parity will likely cause a full seek\nand rotation, but the second write of the parity will only result in rotation.\nHowever, our comparison does capture the essential differences, and\nis useful for understanding tradeoffs across RAID levels. We present a\nsummary in the table below; for the latency analysis, we simply use D to\nrepresent the time that a request to a single disk would take.\nTo conclude, if you strictly want performance and do not care about\nreliability, striping is obviously best. If, however, you want random I/O\nperformance and reliability, mirroring is the best; the cost you pay is in\nlost capacity. If capacity and reliability are your main goals, then RAID-\n5 is the winner; the cost you pay is in small-write performance. Finally,\nif you are always doing sequential I/O and want to maximize capacity,\nRAID-5 also makes the most sense.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2326,
      "extraction_method": "Direct"
    },
    {
      "page_number": 472,
      "chapter": null,
      "content": "436\nREDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAIDS)\n38.9\nOther Interesting RAID Issues\nThere are a number of other interesting ideas that one could (and per-\nhaps should) discuss when thinking about RAID. Here are some things\nwe might eventually write about.\nFor example, there are many other RAID designs, including Levels 2\nand 3 from the original taxonomy, and Level 6 to tolerate multiple disk\nfaults [C+04]. There is also what the RAID does when a disk fails; some-\ntimes it has a hot spare sitting around to ﬁll in for the failed disk. What\nhappens to performance under failure, and performance during recon-\nstruction of the failed disk? There are also more realistic fault models,\nto take into account latent sector errors or block corruption [B+08], and\nlots of techniques to handle such faults (see the data integrity chapter for\ndetails). Finally, you can even build raid as a software layer: such soft-\nware RAID systems are cheaper but have other problems, including the\nconsistent-update problem [DAA05].\n38.10\nSummary\nWe have discussed RAID. RAID transforms a number of independent\ndisks into a large, more capacious, and more reliable single entity; impor-\ntantly, it does so transparently, and thus hardware and software above is\nrelatively oblivious to the change.\nThere are many possible RAID levels to choose from, and the exact\nRAID level to use depends heavily on what is important to the end-user.\nFor example, mirrored RAID is simple, reliable, and generally provides\ngood performance but at a high capacity cost. RAID-5, in contrast, is\nreliable and better from a capacity standpoint, but performs quite poorly\nwhen there are small writes in the workload. Picking a RAID and setting\nits parameters (chunk size, number of disks, etc.) properly for a particular\nworkload is challenging, and remains more of an art than a science.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1898,
      "extraction_method": "Direct"
    },
    {
      "page_number": 473,
      "chapter": null,
      "content": "REDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAIDS)\n437\nReferences\n[B+08] “An Analysis of Data Corruption in the Storage Stack”\nLakshmi N. Bairavasundaram, Garth R. Goodson, Bianca Schroeder, Andrea C. Arpaci-Dusseau,\nRemzi H. Arpaci-Dusseau\nFAST ’08, San Jose, CA, February 2008\nOur own work analyzing how often disks actually corrupt your data. Not often, but sometimes! And\nthus something a reliable storage system must consider.\n[BJ88] “Disk Shadowing”\nD. Bitton and J. Gray\nVLDB 1988\nOne of the ﬁrst papers to discuss mirroring, herein called “shadowing”.\n[CL95] “Striping in a RAID level 5 disk array”\nPeter M. Chen, Edward K. Lee\nSIGMETRICS 1995\nA nice analysis of some of the important parameters in a RAID-5 disk array.\n[C+04] “Row-Diagonal Parity for Double Disk Failure Correction”\nP. Corbett, B. English, A. Goel, T. Grcanac, S. Kleiman, J. Leong, S. Sankar\nFAST ’04, February 2004\nThough not the ﬁrst paper on a RAID system with two disks for parity, it is a recent and highly-\nunderstandable version of said idea. Read it to learn more.\n[DAA05] “Journal-guided Resynchronization for Software RAID”\nTimothy E. Denehy, A. Arpaci-Dusseau, R. Arpaci-Dusseau\nFAST 2005\nOur own work on the consistent-update problem. Here we solve it for Software RAID by integrating\nthe journaling machinery of the ﬁle system above with the software RAID beneath it.\n[HLM94] “File System Design for an NFS File Server Appliance”\nDave Hitz, James Lau, Michael Malcolm\nUSENIX Winter 1994, San Francisco, California, 1994\nThe sparse paper introducing a landmark product in storage, the write-anywhere ﬁle layout or WAFL\nﬁle system that underlies the NetApp ﬁle server.\n[K86] “Synchronized Disk Interleaving”\nM.Y. Kim.\nIEEE Transactions on Computers, Volume C-35: 11, November 1986\nSome of the earliest work on RAID is found here.\n[K88] “Small Disk Arrays - The Emerging Approach to High Performance”\nF. Kurzweil.\nPresentation at Spring COMPCON ’88, March 1, 1988, San Francisco, California\nAnother early RAID reference.\n[P+88] “Redundant Arrays of Inexpensive Disks”\nD. Patterson, G. Gibson, R. Katz.\nSIGMOD 1988\nThis is considered the RAID paper, written by famous authors Patterson, Gibson, and Katz. The paper\nhas since won many test-of-time awards and ushered in the RAID era, including the name RAID itself!\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2337,
      "extraction_method": "Direct"
    },
    {
      "page_number": 474,
      "chapter": null,
      "content": "438\nREDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAIDS)\n[PB86] “Providing Fault Tolerance in Parallel Secondary Storage Systems”\nA. Park and K. Balasubramaniam\nDepartment of Computer Science, Princeton, CS-TR-O57-86, November 1986\nAnother early work on RAID.\n[SG86] “Disk Striping”\nK. Salem and H. Garcia-Molina.\nIEEE International Conference on Data Engineering, 1986\nAnd yes, another early RAID work. There are a lot of these, which kind of came out of the woodwork\nwhen the RAID paper was published in SIGMOD.\n[S84] “Byzantine Generals in Action: Implementing Fail-Stop Processors”\nF.B. Schneider.\nACM Transactions on Computer Systems, 2(2):145154, May 1984\nFinally, a paper that is not about RAID! This paper is actually about how systems fail, and how to make\nsomething behave in a fail-stop manner.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 846,
      "extraction_method": "Direct"
    },
    {
      "page_number": 475,
      "chapter": null,
      "content": "REDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAIDS)\n439\nHomework\nThis section introduces raid.py, a simple RAID simulator you can\nuse to shore up your knowledge of how RAID systems work. See the\nREADME for details.\nQuestions\n1. Use the simulator to perform some basic RAID mapping tests. Run\nwith different levels (0, 1, 4, 5) and see if you can ﬁgure out the\nmappings of a set of requests. For RAID-5, see if you can ﬁgure out\nthe difference between left-symmetric and left-asymmetric layouts.\nUse some different random seeds to generate different problems\nthan above.\n2. Do the same as the ﬁrst problem, but this time vary the chunk size\nwith -C. How does chunk size change the mappings?\n3. Do the same as above, but use the -r ﬂag to reverse the nature of\neach problem.\n4. Now use the reverse ﬂag but increase the size of each request with\nthe -S ﬂag. Try specifying sizes of 8k, 12k, and 16k, while varying\nthe RAID level. What happens to the underlying I/O pattern when\nthe size of the request increases? Make sure to try this with the\nsequential workload too (-W sequential); for what request sizes\nare RAID-4 and RAID-5 much more I/O efﬁcient?\n5. Use the timing mode of the simulator (-t) to estimate the perfor-\nmance of 100 random reads to the RAID, while varying the RAID\nlevels, using 4 disks.\n6. Do the same as above, but increase the number of disks. How does\nthe performance of each RAID level scale as the number of disks\nincreases?\n7. Do the same as above, but use all writes (-w 100) instead of reads.\nHow does the performance of each RAID level scale now? Can you\ndo a rough estimate of the time it will take to complete the workload\nof 100 random writes?\n8. Run the timing mode one last time, but this time with a sequen-\ntial workload (-W sequential). How does the performance vary\nwith RAID level, and when doing reads versus writes? How about\nwhen varying the size of each request? What size should you write\nto a RAID when using RAID-4 or RAID-5?\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2005,
      "extraction_method": "Direct"
    },
    {
      "page_number": 476,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 477,
      "chapter": null,
      "content": "39\nInterlude: File and Directories\nThus far we have seen the development of two key operating system ab-\nstractions: the process, which is a virtualization of the CPU, and the ad-\ndress space, which is a virtualization of memory. In tandem, these two\nabstractions allow a program to run as if it is in its own private, isolated\nworld; as if it has its own processor (or processors); as if it has its own\nmemory. This illusion makes programming the system much easier and\nthus is prevalent today not only on desktops and servers but increasingly\non all programmable platforms including mobile phones and the like.\nIn this section, we add one more critical piece to the virtualization puz-\nzle: persistent storage. A persistent-storage device, such as a classic hard\ndisk drive or a more modern solid-state storage device, stores informa-\ntion permanently (or at least, for a long time). Unlike memory, whose\ncontents are lost when there is a power loss, a persistent-storage device\nkeeps such data intact. Thus, the OS must take extra care with such a\ndevice: this is where users keep data that they really care about.\nCRUX: HOW TO MANAGE A PERSISTENT DEVICE\nHow should the OS manage a persistent device? What are the APIs?\nWhat are the important aspects of the implementation?\nThus, in the next few chapters, we will explore critical techniques for\nmanaging persistent data, focusing on methods to improve performance\nand reliability. We begin, however, with an overview of the API: the in-\nterfaces you’ll expect to see when interacting with a UNIX ﬁle system.\n39.1\nFiles and Directories\nTwo key abstractions have developed over time in the virtualization\nof storage. The ﬁrst is the ﬁle. A ﬁle is simply a linear array of bytes,\neach of which you can read or write. Each ﬁle has some kind of low-level\n441\n",
      "content_length": 1808,
      "extraction_method": "Direct"
    },
    {
      "page_number": 478,
      "chapter": null,
      "content": "442\nINTERLUDE: FILE AND DIRECTORIES\nname, usually a number of some kind; often, the user is not aware of\nthis name (as we will see). For historical reasons, the low-level name of a\nﬁle is often referred to as its inode number. We’ll be learning a lot more\nabout inodes in future chapters; for now, just assume that each ﬁle has an\ninode number associated with it.\nIn most systems, the OS does not know much about the structure of\nthe ﬁle (e.g., whether it is a picture, or a text ﬁle, or C code); rather, the\nresponsibility of the ﬁle system is simply to store such data persistently\non disk and make sure that when you request the data again, you get\nwhat you put there in the ﬁrst place. Doing so is not as simple as it seems!\nThe second abstraction is that of a directory. A directory, like a ﬁle,\nalso has a low-level name (i.e., an inode number), but its contents are\nquite speciﬁc: it contains a list of (user-readable name, low-level name)\npairs. For example, let’s say there is a ﬁle with the low-level name “10”,\nand it is referred to by the user-readable name of “foo”. The directory\n“foo” resides in thus would have an entry (“foo”, “10”) that maps the\nuser-readable name to the low-level name. Each entry in a directory refers\nto either ﬁles or other directories. By placing directories within other di-\nrectories, users are able to build an arbitrary directory tree (or directory\nhierarchy), under which all ﬁles and directories are stored.\n/\nfoo\nbar.txt\nbar\nfoo\nbar\nbar.txt\nFigure 39.1: An Example Directory Tree\nThe directory hierarchy starts at a root directory (in UNIX-based sys-\ntems, the root directory is simply referred to as /) and uses some kind\nof separator to name subsequent sub-directories until the desired ﬁle or\ndirectory is named. For example, if a user created a directory foo in the\nroot directory /, and then created a ﬁle bar.txt in the directory foo,\nwe could refer to the ﬁle by its absolute pathname, which in this case\nwould be /foo/bar.txt. See Figure 39.1 for a more complex directory\ntree; valid directories in the example are /, /foo, /bar, /bar/bar,\n/bar/foo and valid ﬁles are /foo/bar.txt and /bar/foo/bar.txt.\nDirectories and ﬁles can have the same name as long as they are in dif-\nferent locations in the ﬁle-system tree (e.g., there are two ﬁles named\nbar.txt in the ﬁgure, /foo/bar.txt and /bar/foo/bar.txt).\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2407,
      "extraction_method": "Direct"
    },
    {
      "page_number": 479,
      "chapter": null,
      "content": "INTERLUDE: FILE AND DIRECTORIES\n443\nTIP: THINK CAREFULLY ABOUT NAMING\nNaming is an important aspect of computer systems [SK09]. In UNIX\nsystems, virtually everything that you can think of is named through the\nﬁle system. Beyond just ﬁles, devices, pipes, and even processes [K84]\ncan be found in what looks like a plain old ﬁle system. This uniformity\nof naming eases your conceptual model of the system, and makes the\nsystem simpler and more modular. Thus, whenever creating a system or\ninterface, think carefully about what names you are using.\nYou may also notice that the ﬁle name in this example often has two\nparts: bar and txt, separated by a period. The ﬁrst part is an arbitrary\nname, whereas the second part of the ﬁle name is usually used to indi-\ncate the type of the ﬁle, e.g., whether it is C code (e.g., .c), or an image\n(e.g., .jpg), or a music ﬁle (e.g., .mp3). However, this is usually just a\nconvention: there is usually no enforcement that the data contained in a\nﬁle named main.c is indeed C source code.\nThus, we can see one great thing provided by the ﬁle system: a conve-\nnient way to name all the ﬁles we are interested in. Names are important\nin systems as the ﬁrst step to accessing any resource is being able to name\nit. In UNIX systems, the ﬁle system thus provides a uniﬁed way to access\nﬁles on disk, USB stick, CD-ROM, many other devices, and in fact many\nother things, all located under the single directory tree.\n39.2\nThe File System Interface\nLet’s now discuss the ﬁle system interface in more detail. We’ll start\nwith the basics of creating, accessing, and deleting ﬁles. You may think\nthis straightforward, but along the way we’ll discover the mysterious call\nthat is used to remove ﬁles, known as unlink(). Hopefully, by the end\nof this chapter, this mystery won’t be so mysterious to you!\n39.3\nCreating Files\nWe’ll start with the most basic of operations: creating a ﬁle. This can be\naccomplished with the open system call; by calling open() and passing\nit the O CREAT ﬂag, a program can create a new ﬁle. Here is some exam-\nple code to create a ﬁle called “foo” in the current working directory.\nint fd = open(\"foo\", O_CREAT | O_WRONLY | O_TRUNC);\nThe routine open() takes a number of different ﬂags. In this exam-\nple, the program creates the ﬁle (O CREAT), can only write to that ﬁle\nwhile opened in this manner (O WRONLY), and, if the ﬁle already exists,\nﬁrst truncate it to a size of zero bytes thus removing any existing content\n(O TRUNC).\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2526,
      "extraction_method": "Direct"
    },
    {
      "page_number": 480,
      "chapter": null,
      "content": "444\nINTERLUDE: FILE AND DIRECTORIES\nASIDE: THE CREAT() SYSTEM CALL\nThe older way of creating a ﬁle is to call creat(), as follows:\nint fd = creat(\"foo\");\nYou can think of creat() as open() with the following ﬂags:\nO CREAT | O WRONLY | O TRUNC. Because open() can create a ﬁle,\nthe usage of creat() has somewhat fallen out of favor (indeed, it could\njust be implemented as a library call to open()); however, it does hold a\nspecial place in UNIX lore. Speciﬁcally, when Ken Thompson was asked\nwhat he would do differently if he were redesigning UNIX, he replied:\n“I’d spell creat with an e.”\nOne important aspect of open() is what it returns: a ﬁle descriptor. A\nﬁle descriptor is just an integer, private per process, and is used in UNIX\nsystems to access ﬁles; thus, once a ﬁle is opened, you use the ﬁle de-\nscriptor to read or write the ﬁle, assuming you have permission to do so.\nIn this way, a ﬁle descriptor is a capability [L84], i.e., an opaque handle\nthat gives you the power to perform certain operations. Another way to\nthink of a ﬁle descriptor is as a pointer to an object of type ﬁle; once you\nhave such an object, you can call other “methods” to access the ﬁle, like\nread() and write(). We’ll see just how a ﬁle descriptor is used below.\n39.4\nReading and Writing Files\nOnce we have some ﬁles, of course we might like to read or write them.\nLet’s start by reading an existing ﬁle. If we were typing at a command\nline, we might just use the program cat to dump the contents of the ﬁle\nto the screen.\nprompt> echo hello > foo\nprompt> cat foo\nhello\nprompt>\nIn this code snippet, we redirect the output of the program echo to\nthe ﬁle foo, which then contains the word “hello” in it. We then use cat\nto see the contents of the ﬁle. But how does the cat program access the\nﬁle foo?\nTo ﬁnd this out, we’ll use an incredibly useful tool to trace the system\ncalls made by a program. On Linux, the tool is called strace; other sys-\ntems have similar tools (see dtruss on Mac OS X, or truss on some older\nUNIX variants). What strace does is trace every system call made by a\nprogram while it runs, and dump the trace to the screen for you to see.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2197,
      "extraction_method": "Direct"
    },
    {
      "page_number": 481,
      "chapter": null,
      "content": "INTERLUDE: FILE AND DIRECTORIES\n445\nTIP: USE STRACE (AND SIMILAR TOOLS)\nThe strace tool provides an awesome way to see what programs are up\nto. By running it, you can trace which system calls a program makes, see\nthe arguments and return codes, and generally get a very good idea of\nwhat is going on.\nThe tool also takes some arguments which can be quite useful. For ex-\nample, -f follows any fork’d children too; -t reports the time of day\nat each call; -e trace=open,close,read,write only traces calls to\nthose system calls and ignores all others. There are many more powerful\nﬂags – read the man pages and ﬁnd out how to harness this wonderful\ntool.\nHere is an example of using strace to ﬁgure out what cat is doing\n(some calls removed for readability):\nprompt> strace cat foo\n...\nopen(\"foo\", O_RDONLY|O_LARGEFILE)\n= 3\nread(3, \"hello\\n\", 4096)\n= 6\nwrite(1, \"hello\\n\", 6)\n= 6\nhello\nread(3, \"\", 4096)\n= 0\nclose(3)\n= 0\n...\nprompt>\nThe ﬁrst thing that cat does is open the ﬁle for reading. A couple\nof things we should note about this; ﬁrst, that the ﬁle is only opened for\nreading (not writing), as indicated by the O RDONLY ﬂag; second, that\nthe 64-bit offset be used (O LARGEFILE); third, that the call to open()\nsucceeds and returns a ﬁle descriptor, which has the value of 3.\nWhy does the ﬁrst call to open() return 3, not 0 or perhaps 1 as you\nmight expect? As it turns out, each running process already has three\nﬁles open, standard input (which the process can read to receive input),\nstandard output (which the process can write to in order to dump infor-\nmation to the screen), and standard error (which the process can write\nerror messages to). These are represented by ﬁle descriptors 0, 1, and 2,\nrespectively. Thus, when you ﬁrst open another ﬁle (as cat does above),\nit will almost certainly be ﬁle descriptor 3.\nAfter the open succeeds, cat uses the read() system call to repeat-\nedly read some bytes from a ﬁle. The ﬁrst argument to read() is the ﬁle\ndescriptor, thus telling the ﬁle system which ﬁle to read; a process can of\ncourse have multiple ﬁles open at once, and thus the descriptor enables\nthe operating system to know which ﬁle a particular read refers to. The\nsecond argument points to a buffer where the result of the read() will be\nplaced; in the system-call trace above, strace shows the results of the read\nin this spot (“hello”). The third argument is the size of the buffer, which\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2455,
      "extraction_method": "Direct"
    },
    {
      "page_number": 482,
      "chapter": null,
      "content": "446\nINTERLUDE: FILE AND DIRECTORIES\nin this case is 4 KB. The call to read() returns successfully as well, here\nreturning the number of bytes it read (6, which includes 5 for the letters\nin the word “hello” and one for an end-of-line marker).\nAt this point, you see another interesting result of the strace: a single\ncall to the write() system call, to the ﬁle descriptor 1. As we mentioned\nabove, this descriptor is known as the standard output, and thus is used\nto write the word “hello” to the screen as the program cat is meant to\ndo. But does it call write() directly? Maybe (if it is highly optimized).\nBut if not, what cat might do is call the library routine printf(); in-\nternally, printf() ﬁgures out all the formatting details passed to it, and\neventually calls write on the standard output to print the results to the\nscreen.\nThe cat program then tries to read more from the ﬁle, but since there\nare no bytes left in the ﬁle, the read() returns 0 and the program knows\nthat this means it has read the entire ﬁle. Thus, the program calls close()\nto indicate that it is done with the ﬁle “foo”, passing in the corresponding\nﬁle descriptor. The ﬁle is thus closed, and the reading of it thus complete.\nWriting a ﬁle is accomplished via a similar set of steps. First, a ﬁle\nis opened for writing, then the write() system call is called, perhaps\nrepeatedly for larger ﬁles, and then close(). Use strace to trace writes\nto a ﬁle, perhaps of a program you wrote yourself, or by tracing the dd\nutility, e.g., dd if=foo of=bar.\n39.5\nReading And Writing, But Not Sequentially\nThus far, we’ve discussed how to read and write ﬁles, but all access\nhas been sequential; that is, we have either read a ﬁle from the beginning\nto the end, or written a ﬁle out from beginning to end.\nSometimes, however, it is useful to be able to read or write to a spe-\nciﬁc offset within a ﬁle; for example, if you build an index over a text\ndocument, and use it to look up a speciﬁc word, you may end up reading\nfrom some random offsets within the document. To do so, we will use\nthe lseek() system call. Here is the function prototype:\noff_t lseek(int fildes, off_t offset, int whence);\nThe ﬁrst argument is familiar (a ﬁle descriptor). The second argu-\nment is the offset, which positions the ﬁle offset to a particular location\nwithin the ﬁle. The third argument, called whence for historical reasons,\ndetermines exactly how the seek is performed. From the man page:\nIf whence is SEEK_SET, the offset is set to offset bytes.\nIf whence is SEEK_CUR, the offset is set to its current\nlocation plus offset bytes.\nIf whence is SEEK_END, the offset is set to the size of\nthe file plus offset bytes.\nAs you can tell from this description, for each ﬁle a process opens, the\nOS tracks a “current” offset, which determines where the next read or\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2867,
      "extraction_method": "Direct"
    },
    {
      "page_number": 483,
      "chapter": null,
      "content": "INTERLUDE: FILE AND DIRECTORIES\n447\nASIDE: CALLING LSEEK() DOES NOT PERFORM A DISK SEEK\nThe poorly-named system call lseek() confuses many a student try-\ning to understand disks and how the ﬁle systems atop them work. Do\nnot confuse the two! The lseek() call simply changes a variable in OS\nmemory that tracks, for a particular process, at which offset to which its\nnext read or write will start. A disk seek occurs when a read or write\nissued to the disk is not on the same track as the last read or write, and\nthus necessitates a head movement. Making this even more confusing is\nthe fact that calling lseek() to read or write from/to random parts of a\nﬁle, and then reading/writing to those random parts, will indeed lead to\nmore disk seeks. Thus, calling lseek() can certainly lead to a seek in an\nupcoming read or write, but absolutely does not cause any disk I/O to\noccur itself.\nwrite will begin reading from or writing to within the ﬁle. Thus, part\nof the abstraction of an open ﬁle is that it has a current offset, which\nis updated in one of two ways. The ﬁrst is when a read or write of N\nbytes takes place, N is added to the current offset; thus each read or write\nimplicitly updates the offset. The second is explicitly with lseek, which\nchanges the offset as speciﬁed above.\nNote that this call lseek() has nothing to do with the seek operation\nof a disk, which moves the disk arm. The call to lseek() simply changes\nthe value of a variable within the kernel; when the I/O is performed,\ndepending on where the disk head is, the disk may or may not perform\nan actual seek to fulﬁll the request.\n39.6\nWriting Immediately with fsync()\nMost times when a program calls write(), it is just telling the ﬁle\nsystem: please write this data to persistent storage, at some point in the\nfuture. The ﬁle system, for performance reasons, will buffer such writes\nin memory for some time (say 5 seconds, or 30); at that later point in\ntime, the write(s) will actually be issued to the storage device. From the\nperspective of the calling application, writes seem to complete quickly,\nand only in rare cases (e.g., the machine crashes after the write() call\nbut before the write to disk) will data be lost.\nHowever, some applications require something more than this even-\ntual guarantee. For example, in a database management system (DBMS),\ndevelopment of a correct recovery protocol requires the ability to force\nwrites to disk from time to time.\nTo support these types of applications, most ﬁle systems provide some\nadditional control APIs. In the UNIX world, the interface provided to ap-\nplications is known as fsync(int fd). When a process calls fsync()\nfor a particular ﬁle descriptor, the ﬁle system responds by forcing all dirty\n(i.e., not yet written) data to disk, for the ﬁle referred to by the speciﬁed\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2852,
      "extraction_method": "Direct"
    },
    {
      "page_number": 484,
      "chapter": null,
      "content": "448\nINTERLUDE: FILE AND DIRECTORIES\nﬁle descriptor. The fsync() routine returns once all of these writes are\ncomplete.\nHere is a simple example of how to use fsync(). The code opens\nthe ﬁle foo, writes a single chunk of data to it, and then calls fsync()\nto ensure the writes are forced immediately to disk. Once the fsync()\nreturns, the application can safely move on, knowing that the data has\nbeen persisted (if fsync() is correctly implemented, that is).\nint fd = open(\"foo\", O_CREAT | O_WRONLY | O_TRUNC);\nassert(fd > -1);\nint rc = write(fd, buffer, size);\nassert(rc == size);\nrc = fsync(fd);\nassert(rc == 0);\nInterestingly, this sequence does not guarantee everything that you\nmight expect; in some cases, you also need to fsync() the directory that\ncontains the ﬁle foo. Adding this step ensures not only that the ﬁle itself\nis on disk, but that the ﬁle, if newly created, also is durably a part of the\ndirectory. Not surprisingly, this type of detail is often overlooked, leading\nto many application-level bugs [P+13].\n39.7\nRenaming Files\nOnce we have a ﬁle, it is sometimes useful to be able to give a ﬁle a\ndifferent name. When typing at the command line, this is accomplished\nwith mv command; in this example, the ﬁle foo is renamed bar:\nprompt> mv foo bar\nUsing strace, we can see that mv uses the system call rename(char\n*old, char *new), which takes precisely two arguments: the original\nname of the ﬁle (old) and the new name (new).\nOne interesting guarantee provided by the rename() call is that it is\n(usually) implemented as an atomic call with respect to system crashes;\nif the system crashes during the renaming, the ﬁle will either be named\nthe old name or the new name, and no odd in-between state can arise.\nThus, rename() is critical for supporting certain kinds of applications\nthat require an atomic update to ﬁle state.\nLet’s be a little more speciﬁc here. Imagine that you are using a ﬁle ed-\nitor (e.g., emacs), and you insert a line into the middle of a ﬁle. The ﬁle’s\nname, for the example, is foo.txt. The way the editor might update the\nﬁle to guarantee that the new ﬁle has the original contents plus the line\ninserted is as follows (ignoring error-checking for simplicity):\nint fd = open(\"foo.txt.tmp\", O_WRONLY|O_CREAT|O_TRUNC);\nwrite(fd, buffer, size); // write out new version of file\nfsync(fd);\nclose(fd);\nrename(\"foo.txt.tmp\", \"foo.txt\");\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2426,
      "extraction_method": "Direct"
    },
    {
      "page_number": 485,
      "chapter": null,
      "content": "INTERLUDE: FILE AND DIRECTORIES\n449\nWhat the editor does in this example is simple: write out the new\nversion of the ﬁle under temporary name (foot.txt.tmp), force it to\ndisk with fsync(), and then, when the application is certain the new\nﬁle metadata and contents are on the disk, rename the temporary ﬁle to\nthe original ﬁle’s name. This last step atomically swaps the new ﬁle into\nplace, while concurrently deleting the old version of the ﬁle, and thus an\natomic ﬁle update is achieved.\n39.8\nGetting Information About Files\nBeyond ﬁle access, we expect the ﬁle system to keep a fair amount of\ninformation about each ﬁle it is storing. We generally call such data about\nﬁles metadata. To see the metadata for a certain ﬁle, we can use stat()\nor fstat() system call – read their man pages for details on how to call\nthem. These calls take a pathname (or ﬁle descriptor) to a ﬁle and ﬁll in a\nstat structure as seen here:\nstruct stat {\ndev_t\nst_dev;\n/* ID of device containing file */\nino_t\nst_ino;\n/* inode number */\nmode_t\nst_mode;\n/* protection */\nnlink_t\nst_nlink;\n/* number of hard links */\nuid_t\nst_uid;\n/* user ID of owner */\ngid_t\nst_gid;\n/* group ID of owner */\ndev_t\nst_rdev;\n/* device ID (if special file) */\noff_t\nst_size;\n/* total size, in bytes */\nblksize_t st_blksize; /* blocksize for filesystem I/O */\nblkcnt_t\nst_blocks;\n/* number of blocks allocated */\ntime_t\nst_atime;\n/* time of last access */\ntime_t\nst_mtime;\n/* time of last modification */\ntime_t\nst_ctime;\n/* time of last status change */\n};\nYou can see that there is a lot of information kept about each ﬁle, in-\ncluding its size (in bytes), its low-level name (i.e., inode number), some\nownership information, and some information about when the ﬁle was\naccessed or modiﬁed, among other things. To see this information, you\ncan use the command line tool stat:\nprompt> echo hello > file\nprompt> stat file\nFile: ‘file’\nSize: 6 Blocks: 8\nIO Block: 4096\nregular file\nDevice: 811h/2065d Inode: 67158084\nLinks: 1\nAccess: (0640/-rw-r-----) Uid: (30686/ remzi) Gid: (30686/ remzi)\nAccess: 2011-05-03 15:50:20.157594748 -0500\nModify: 2011-05-03 15:50:20.157594748 -0500\nChange: 2011-05-03 15:50:20.157594748 -0500\nAs it turns out, each ﬁle system usually keeps this type of information\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2295,
      "extraction_method": "Direct"
    },
    {
      "page_number": 486,
      "chapter": null,
      "content": "450\nINTERLUDE: FILE AND DIRECTORIES\nin a structure called an inode1. We’ll be learning a lot more about inodes\nwhen we talk about ﬁle system implementation. For now, you should just\nthink of an inode as a persistent data structure kept by the ﬁle system that\nhas information like we see above inside of it.\n39.9\nRemoving Files\nAt this point, we know how to create ﬁles and access them, either se-\nquentially or not. But how do you delete ﬁles? If you’ve used UNIX, you\nprobably think you know: just run the program rm. But what system call\ndoes rm use to remove a ﬁle?\nLet’s use our old friend strace again to ﬁnd out. Here we remove\nthat pesky ﬁle “foo”:\nprompt> strace rm foo\n...\nunlink(\"foo\")\n= 0\n...\nWe’ve removed a bunch of unrelated cruft from the traced output,\nleaving just a single call to the mysteriously-named system call unlink().\nAs you can see, unlink() just takes the name of the ﬁle to be removed,\nand returns zero upon success. But this leads us to a great puzzle: why\nis this system call named “unlink”? Why not just “remove” or “delete”.\nTo understand the answer to this puzzle, we must ﬁrst understand more\nthan just ﬁles, but also directories.\n39.10\nMaking Directories\nBeyond ﬁles, a set of directory-related system calls enable you to make,\nread, and delete directories. Note you can never write to a directory di-\nrectly; because the format of the directory is considered ﬁle system meta-\ndata, you can only update a directory indirectly by, for example, creating\nﬁles, directories, or other object types within it. In this way, the ﬁle system\nmakes sure that the contents of the directory always are as expected.\nTo create a directory, a single system call, mkdir(), is available. The\neponymous mkdir program can be used to create such a directory. Let’s\ntake a look at what happens when we run the mkdir program to make a\nsimple directory called foo:\nprompt> strace mkdir foo\n...\nmkdir(\"foo\", 0777)\n= 0\n...\nprompt>\n1Some ﬁle systems call these structures similar, but slightly different, names, such as\ndnodes; the basic idea is similar however.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2119,
      "extraction_method": "Direct"
    },
    {
      "page_number": 487,
      "chapter": null,
      "content": "INTERLUDE: FILE AND DIRECTORIES\n451\nTIP: BE WARY OF POWERFUL COMMANDS\nThe program rm provides us with a great example of powerful com-\nmands, and how sometimes too much power can be a bad thing. For\nexample, to remove a bunch of ﬁles at once, you can type something like:\nprompt> rm *\nwhere the * will match all ﬁles in the current directory. But sometimes\nyou want to also delete the directories too, and in fact all of their contents.\nYou can do this by telling rm to recursively descend into each directory,\nand remove its contents too:\nprompt> rm -rf *\nWhere you get into trouble with this small string of characters is when\nyou issue the command, accidentally, from the root directory of a ﬁle sys-\ntem, thus removing every ﬁle and directory from it. Oops!\nThus, remember the double-edged sword of powerful commands; while\nthey give you the ability to do a lot of work with a small number of\nkeystrokes, they also can quickly and readily do a great deal of harm.\nWhen such a directory is created, it is considered “empty”, although it\ndoes have a bare minimum of contents. Speciﬁcally, an empty directory\nhas two entries: one entry that refers to itself, and one entry that refers\nto its parent. The former is referred to as the “.” (dot) directory, and the\nlatter as “..” (dot-dot). You can see these directories by passing a ﬂag (-a)\nto the program ls:\nprompt> ls -a\n./\n../\nprompt> ls -al\ntotal 8\ndrwxr-x---\n2 remzi remzi\n6 Apr 30 16:17 ./\ndrwxr-x--- 26 remzi remzi 4096 Apr 30 16:17 ../\n39.11\nReading Directories\nNow that we’ve created a directory, we might wish to read one too.\nIndeed, that is exactly what the program ls does. Let’s write our own\nlittle tool like ls and see how it is done.\nInstead of just opening a directory as if it were a ﬁle, we instead use\na new set of calls. Below is an example program that prints the contents\nof a directory. The program uses three calls, opendir(), readdir(),\nand closedir(), to get the job done, and you can see how simple the\ninterface is; we just use a simple loop to read one directory entry at a time,\nand print out the name and inode number of each ﬁle in the directory.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2173,
      "extraction_method": "Direct"
    },
    {
      "page_number": 488,
      "chapter": null,
      "content": "452\nINTERLUDE: FILE AND DIRECTORIES\nint main(int argc, char *argv[]) {\nDIR *dp = opendir(\".\");\nassert(dp != NULL);\nstruct dirent *d;\nwhile ((d = readdir(dp)) != NULL) {\nprintf(\"%d %s\\n\", (int) d->d_ino, d->d_name);\n}\nclosedir(dp);\nreturn 0;\n}\nThe declaration below shows the information available within each\ndirectory entry in the struct dirent data structure:\nstruct dirent {\nchar\nd_name[256]; /* filename */\nino_t\nd_ino;\n/* inode number */\noff_t\nd_off;\n/* offset to the next dirent */\nunsigned short d_reclen;\n/* length of this record */\nunsigned char\nd_type;\n/* type of file */\n};\nBecause directories are light on information (basically, just mapping\nthe name to the inode number, along with a few other details), a program\nmay want to call stat() on each ﬁle to get more information on each,\nsuch as its length or other detailed information. Indeed, this is exactly\nwhat ls does when you pass it the -l ﬂag; try strace on ls with and\nwithout that ﬂag to see for yourself.\n39.12\nDeleting Directories\nFinally, you can delete a directory with a call to rmdir() (which is\nused by the program of the same name, rmdir). Unlike ﬁle deletion,\nhowever, removing directories is more dangerous, as you could poten-\ntially delete a large amount of data with a single command. Thus, rmdir()\nhas the requirement that the directory be empty (i.e., only has “.” and “..”\nentries) before it is deleted. If you try to delete a non-empty directory, the\ncall to rmdir() simply will fail.\n39.13\nHard Links\nWe now come back to the mystery of why removing a ﬁle is performed\nvia unlink(), by understanding a new way to make an entry in the\nﬁle system tree, through a system call known as link(). The link()\nsystem call takes two arguments, an old pathname and a new one; when\nyou “link” a new ﬁle name to an old one, you essentially create another\nway to refer to the same ﬁle. The command-line program ln is used to\ndo this, as we see in this example:\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1982,
      "extraction_method": "Direct"
    },
    {
      "page_number": 489,
      "chapter": null,
      "content": "INTERLUDE: FILE AND DIRECTORIES\n453\nprompt> echo hello > file\nprompt> cat file\nhello\nprompt> ln file file2\nprompt> cat file2\nhello\nHere we created a ﬁle with the word “hello” in it, and called the ﬁle\nfile2. We then create a hard link to that ﬁle using the ln program. After\nthis, we can examine the ﬁle by either opening file or file2.\nThe way link works is that it simply creates another name in the di-\nrectory you are creating the link to, and refers it to the same inode number\n(i.e., low-level name) of the original ﬁle. The ﬁle is not copied in any way;\nrather, you now just have two human names (file and file2) that both\nrefer to the same ﬁle. We can even see this in the directory itself, by print-\ning out the inode number of each ﬁle:\nprompt> ls -i file file2\n67158084 file\n67158084 file2\nprompt>\nBy passing the -i ﬂag to ls, it prints out the inode number of each ﬁle\n(as well as the ﬁle name). And thus you can see what link really has done:\njust make a new reference to the same exact inode number (67158084 in\nthis example).\nBy now you might be starting to see why unlink() is called unlink().\nWhen you create a ﬁle, you are really doing two things. First, you are\nmaking a structure (the inode) that will track virtually all relevant infor-\nmation about the ﬁle, including its size, where its blocks are on disk, and\nso forth. Second, you are linking a human-readable name to that ﬁle, and\nputting that link into a directory.\nAfter creating a hard link to a ﬁle, to the ﬁle system, there is no dif-\nference between the original ﬁle name (file) and the newly created ﬁle\nname (file2); indeed, they are both just links to the underlying meta-\ndata about the ﬁle, which is found in inode number 67158084.\nThus, to remove a ﬁle from the ﬁle system, we call unlink(). In the\nexample above, we could for example remove the ﬁle named file, and\nstill access the ﬁle without difﬁculty:\nprompt> rm file\nremoved ‘file’\nprompt> cat file2\nhello\nThe reason this works is because when the ﬁle system unlinks ﬁle, it\nchecks a reference count within the inode number. This reference count\n2Note how creative the authors of this book are. We also used to have a cat named “Cat”\n(true story). However, she died, and we now have a hamster named “Hammy.”\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2292,
      "extraction_method": "Direct"
    },
    {
      "page_number": 490,
      "chapter": null,
      "content": "454\nINTERLUDE: FILE AND DIRECTORIES\n(sometimes called the link count) allows the ﬁle system to track how\nmany different ﬁle names have been linked to this particular inode. When\nunlink() is called, it removes the “link” between the human-readable\nname (the ﬁle that is being deleted) to the given inode number, and decre-\nments the reference count; only when the reference count reaches zero\ndoes the ﬁle system also free the inode and related data blocks, and thus\ntruly “delete” the ﬁle.\nYou can see the reference count of a ﬁle using stat() of course. Let’s\nsee what it is when we create and delete hard links to a ﬁle. In this exam-\nple, we’ll create three links to the same ﬁle, and then delete them. Watch\nthe link count!\nprompt> echo hello > file\nprompt> stat file\n... Inode: 67158084\nLinks: 1 ...\nprompt> ln file file2\nprompt> stat file\n... Inode: 67158084\nLinks: 2 ...\nprompt> stat file2\n... Inode: 67158084\nLinks: 2 ...\nprompt> ln file2 file3\nprompt> stat file\n... Inode: 67158084\nLinks: 3 ...\nprompt> rm file\nprompt> stat file2\n... Inode: 67158084\nLinks: 2 ...\nprompt> rm file2\nprompt> stat file3\n... Inode: 67158084\nLinks: 1 ...\nprompt> rm file3\n39.14\nSymbolic Links\nThere is one other type of link that is really useful, and it is called a\nsymbolic link or sometimes a soft link. As it turns out, hard links are\nsomewhat limited: you can’t create one to a directory (for fear that you\nwill create a cycle in the directory tree); you can’t hard link to ﬁles in\nother disk partitions (because inode numbers are only unique within a\nparticular ﬁle system, not across ﬁle systems); etc. Thus, a new type of\nlink called the symbolic link was created.\nTo create such a link, you can use the same program ln, but with the\n-s ﬂag. Here is an example:\nprompt> echo hello > file\nprompt> ln -s file file2\nprompt> cat file2\nhello\nAs you can see, creating a soft link looks much the same, and the orig-\ninal ﬁle can now be accessed through the ﬁle name file as well as the\nsymbolic link name file2.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2046,
      "extraction_method": "Direct"
    },
    {
      "page_number": 491,
      "chapter": null,
      "content": "INTERLUDE: FILE AND DIRECTORIES\n455\nHowever, beyond this surface similarity, symbolic links are actually\nquite different from hard links. The ﬁrst difference is that a symbolic\nlink is actually a ﬁle itself, of a different type. We’ve already talked about\nregular ﬁles and directories; symbolic links are a third type the ﬁle system\nknows about. A stat on the symlink reveals all:\nprompt> stat file\n... regular file ...\nprompt> stat file2\n... symbolic link ...\nRunning ls also reveals this fact. If you look closely at the ﬁrst char-\nacter of the long-form of the output from ls, you can see that the ﬁrst\ncharacter in the left-most column is a - for regular ﬁles, a d for directo-\nries, and an l for soft links. You can also see the size of the symbolic link\n(4 bytes in this case), as well as what the link points to (the ﬁle named\nfile).\nprompt> ls -al\ndrwxr-x---\n2 remzi remzi\n29 May\n3 19:10 ./\ndrwxr-x--- 27 remzi remzi 4096 May\n3 15:14 ../\n-rw-r-----\n1 remzi remzi\n6 May\n3 19:10 file\nlrwxrwxrwx\n1 remzi remzi\n4 May\n3 19:10 file2 -> file\nThe reason that file2 is 4 bytes is because the way a symbolic link is\nformed is by holding the pathname of the linked-to ﬁle as the data of the\nlink ﬁle. Because we’ve linked to a ﬁle named file, our link ﬁle file2\nis small (4 bytes). If we link to a longer pathname, our link ﬁle would be\nbigger:\nprompt> echo hello > alongerfilename\nprompt> ln -s alongerfilename file3\nprompt> ls -al alongerfilename file3\n-rw-r----- 1 remzi remzi\n6 May\n3 19:17 alongerfilename\nlrwxrwxrwx 1 remzi remzi 15 May\n3 19:17 file3 -> alongerfilename\nFinally, because of the way symbolic links are created, they leave the\npossibility for what is known as a dangling reference:\nprompt> echo hello > file\nprompt> ln -s file file2\nprompt> cat file2\nhello\nprompt> rm file\nprompt> cat file2\ncat: file2: No such file or directory\nAs you can see in this example, quite unlike hard links, removing the\noriginal ﬁle named file causes the link to point to a pathname that no\nlonger exists.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2042,
      "extraction_method": "Direct"
    },
    {
      "page_number": 492,
      "chapter": null,
      "content": "456\nINTERLUDE: FILE AND DIRECTORIES\n39.15\nMaking and Mounting a File System\nWe’ve now toured the basic interfaces to access ﬁles, directories, and\ncertain types of special types of links. But there is one more topic we\nshould discuss: how to assemble a full directory tree from many under-\nlying ﬁle systems. This task is accomplished via ﬁrst making ﬁle systems,\nand then mounting them to make their contents accessible.\nTo make a ﬁle system, most ﬁle systems provide a tool, usually re-\nferred to as mkfs (pronounced “make fs”), that performs exactly this task.\nThe idea is as follows: give the tool, as input, a device (such as a disk\npartition, e.g., /dev/sda1) a ﬁle system type (e.g., ext3), and it simply\nwrites an empty ﬁle system, starting with a root directory, onto that disk\npartition. And mkfs said, let there be a ﬁle system!\nHowever, once such a ﬁle system is created, it needs to be made ac-\ncessible within the uniform ﬁle-system tree. This task is achieved via the\nmount program (which makes the underlying system call mount() to do\nthe real work). What mount does, quite simply is take an existing direc-\ntory as a target mount point and essentially paste a new ﬁle system onto\nthe directory tree at that point.\nAn example here might be useful. Imagine we have an unmounted\next3 ﬁle system, stored in device partition /dev/sda1, that has the fol-\nlowing contents: a root directory which contains two sub-directories, a\nand b, each of which in turn holds a single ﬁle named foo. Let’s say we\nwish to mount this ﬁle system at the mount point /home/users. We\nwould type something like this:\nprompt> mount -t ext3 /dev/sda1 /home/users\nIf successful, the mount would thus make this new ﬁle system avail-\nable. However, note how the new ﬁle system is now accessed. To look at\nthe contents of the root directory, we would use ls like this:\nprompt> ls /home/users/\na b\nAs you can see, the pathname /home/users/ now refers to the root\nof the newly-mounted directory. Similarly, we could access ﬁles a and\nb with the pathnames /home/users/a and /home/users/b. Finally,\nthe ﬁles named foo could be accessed via /home/users/a/foo and\n/home/users/b/foo. And thus the beauty of mount: instead of having\na number of separate ﬁle systems, mount uniﬁes all ﬁle systems into one\ntree, making naming uniform and convenient.\nTo see what is mounted on your system, and at which points, simply\nrun the mount program. You’ll see something like this:\n/dev/sda1 on / type ext3 (rw)\nproc on /proc type proc (rw)\nsysfs on /sys type sysfs (rw)\n/dev/sda5 on /tmp type ext3 (rw)\n/dev/sda7 on /var/vice/cache type ext3 (rw)\ntmpfs on /dev/shm type tmpfs (rw)\nAFS on /afs type afs (rw)\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2720,
      "extraction_method": "Direct"
    },
    {
      "page_number": 493,
      "chapter": null,
      "content": "INTERLUDE: FILE AND DIRECTORIES\n457\nThis crazy mix shows that a whole number of different ﬁle systems,\nincluding ext3 (a standard disk-based ﬁle system), the proc ﬁle system (a\nﬁle system for accessing information about current processes), tmpfs (a\nﬁle system just for temporary ﬁles), and AFS (a distributed ﬁle system)\nare all glued together onto this one machine’s ﬁle-system tree.\n39.16\nSummary\nThe ﬁle system interface in UNIX systems (and indeed, in any system)\nis seemingly quite rudimentary, but there is a lot to understand if you\nwish to master it. Nothing is better, of course, than simply using it (a lot).\nSo please do so! Of course, read more; as always, Stevens [SR05] is the\nplace to begin.\nWe’ve toured the basic interfaces, and hopefully understood a little bit\nabout how they work. Even more interesting is how to implement a ﬁle\nsystem that meets the needs of the API, a topic we will delve into in great\ndetail next.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 979,
      "extraction_method": "Direct"
    },
    {
      "page_number": 494,
      "chapter": null,
      "content": "458\nINTERLUDE: FILE AND DIRECTORIES\nReferences\n[K84] “Processes as Files”\nTom J. Killian\nUSENIX, June 1984\nThe paper that introduced the /proc ﬁle system, where each process can be treated as a ﬁle within a\npseudo ﬁle system. A clever idea that you can still see in modern UNIX systems.\n[L84] “Capability-Based Computer Systems”\nHenry M. Levy\nDigital Press, 1984\nAvailable: http://homes.cs.washington.edu/ levy/capabook\nAn excellent overview of early capability-based systems.\n[P+13] “Towards Efﬁcient, Portable Application-Level Consistency”\nThanumalayan S. Pillai, Vijay Chidambaram, Joo-Young Hwang, Andrea C. Arpaci-Dusseau,\nand Remzi H. Arpaci-Dusseau\nHotDep ’13, November 2013\nOur own work that shows how readily applications can make mistakes in committing data to disk; in\nparticular, assumptions about the ﬁle system creep into applications and thus make the applications\nwork correctly only if they are running on a speciﬁc ﬁle system.\n[SK09] “Principles of Computer System Design”\nJerome H. Saltzer and M. Frans Kaashoek\nMorgan-Kaufmann, 2009\nThis tour de force of systems is a must-read for anybody interested in the ﬁeld. It’s how they teach\nsystems at MIT. Read it once, and then read it a few more times to let it all soak in.\n[SR05] “Advanced Programming in the UNIX Environment”\nW. Richard Stevens and Stephen A. Rago\nAddison-Wesley, 2005\nWe have probably referenced this book a few hundred thousand times. It is that useful to you, if you care\nto become an awesome systems programmer.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1550,
      "extraction_method": "Direct"
    },
    {
      "page_number": 495,
      "chapter": null,
      "content": "INTERLUDE: FILE AND DIRECTORIES\n459\nHomework\nIn this homework, we’ll just familiarize ourselves with how the APIs\ndescribed in the chapter work. To do so, you’ll just write a few different\nprograms, mostly based on various UNIX utilities.\nQuestions\n1. Stat: Write your own version of the command line program stat,\nwhich simply calls the stat() system call on a given ﬁle or di-\nrectory. Print out ﬁle size, number of blocks allocated, reference\n(link) count, and so forth. What is the link count of a directory, as\nthe number of entries in the directory changes? Useful interfaces:\nstat()\n2. List Files: Write a program that lists ﬁles in the given directory.\nWhen called without any arguments, the program should just print\nthe ﬁle names. When invoked with the -l ﬂag, the program should\nprint out information about each ﬁle, such as the owner, group, per-\nmissions, and other information obtained from the stat() system\ncall. The program should take one additional argument, which is\nthe directory to read, e.g., myls -l directory. If no directory is\ngiven, the program should just use the current working directory.\nUseful interfaces: stat(), opendir(), readdir(), getcwd().\n3. Tail: Write a program that prints out the last few lines of a ﬁle. The\nprogram should be efﬁcient, in that it seeks to near the end of the\nﬁle, reads in a block of data, and then goes backwards until it ﬁnds\nthe requested number of lines; at this point, it should print out those\nlines from beginning to the end of the ﬁle. To invoke the program,\none should type: mytail -n file, where n is the number of lines\nat the end of the ﬁle to print. Useful interfaces: stat(), lseek(),\nopen(), read(), close().\n4. Recursive Search: Write a program that prints out the names of\neach ﬁle and directory in the ﬁle system tree, starting at a given\npoint in the tree. For example, when run without arguments, the\nprogram should start with the current working directory and print\nits contents, as well as the contents of any sub-directories, etc., until\nthe entire tree, root at the CWD, is printed. If given a single argu-\nment (of a directory name), use that as the root of the tree instead.\nReﬁne your recursive search with more fun options, similar to the\npowerful find command line tool. Useful interfaces: you ﬁgure it\nout.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2340,
      "extraction_method": "Direct"
    },
    {
      "page_number": 496,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 497,
      "chapter": null,
      "content": "40\nFile System Implementation\nIn this chapter, we introduce a simple ﬁle system implementation, known\nas vsfs (the Very Simple File System). This ﬁle system is a simpliﬁed\nversion of a typical UNIX ﬁle system and thus serves to introduce some\nof the basic on-disk structures, access methods, and various policies that\nyou will ﬁnd in many ﬁle systems today.\nThe ﬁle system is pure software; unlike our development of CPU and\nmemory virtualization, we will not be adding hardware features to make\nsome aspect of the ﬁle system work better (though we will want to pay at-\ntention to device characteristics to make sure the ﬁle system works well).\nBecause of the great ﬂexibility we have in building a ﬁle system, many\ndifferent ones have been built, literally from AFS (the Andrew File Sys-\ntem) [H+88] to ZFS (Sun’s Zettabyte File System) [B07]. All of these ﬁle\nsystems have different data structures and do some things better or worse\nthan their peers. Thus, the way we will be learning about ﬁle systems is\nthrough case studies: ﬁrst, a simple ﬁle system (vsfs) in this chapter to\nintroduce most concepts, and then a series of studies of real ﬁle systems\nto understand how they can differ in practice.\nTHE CRUX: HOW TO IMPLEMENT A SIMPLE FILE SYSTEM\nHow can we build a simple ﬁle system? What structures are needed\non the disk? What do they need to track? How are they accessed?\n40.1\nThe Way To Think\nTo think about ﬁle systems, we usually suggest thinking about two\ndifferent aspects of them; if you understand both of these aspects, you\nprobably understand how the ﬁle system basically works.\nThe ﬁrst is the data structures of the ﬁle system. In other words, what\ntypes of on-disk structures are utilized by the ﬁle system to organize its\ndata and metadata? The ﬁrst ﬁle systems we’ll see (including vsfs below)\nemploy simple structures, like arrays of blocks or other objects, whereas\n461\n",
      "content_length": 1895,
      "extraction_method": "Direct"
    },
    {
      "page_number": 498,
      "chapter": null,
      "content": "462\nFILE SYSTEM IMPLEMENTATION\nASIDE: MENTAL MODELS OF FILE SYSTEMS\nAs we’ve discussed before, mental models are what you are really trying\nto develop when learning about systems. For ﬁle systems, your mental\nmodel should eventually include answers to questions like: what on-disk\nstructures store the ﬁle system’s data and metadata? What happens when\na process opens a ﬁle? Which on-disk structures are accessed during a\nread or write? By working on and improving your mental model, you\ndevelop an abstract understanding of what is going on, instead of just\ntrying to understand the speciﬁcs of some ﬁle-system code (though that\nis also useful, of course!).\nmore sophisticated ﬁle systems, like SGI’s XFS, use more complicated\ntree-based structures [S+96].\nThe second aspect of a ﬁle system is its access methods. How does\nit map the calls made by a process, such as open(), read(), write(),\netc., onto its structures? Which structures are read during the execution\nof a particular system call? Which are written? How efﬁciently are all of\nthese steps performed?\nIf you understand the data structures and access methods of a ﬁle sys-\ntem, you have developed a good mental model of how it truly works, a\nkey part of the systems mindset. Try to work on developing your mental\nmodel as we delve into our ﬁrst implementation.\n40.2\nOverall Organization\nWe now develop the overall on-disk organization of the data struc-\ntures of the vsfs ﬁle system. The ﬁrst thing we’ll need to do is divide the\ndisk into blocks; simple ﬁle systems use just one block size, and that’s\nexactly what we’ll do here. Let’s choose a commonly-used size of 4 KB.\nThus, our view of the disk partition where we’re building our ﬁle sys-\ntem is simple: a series of blocks, each of size 4 KB. The blocks are ad-\ndressed from 0 to N −1, in a partition of size N 4-KB blocks. Assume we\nhave a really small disk, with just 64 blocks:\n0\n7\n8\n15 16\n23 24\n31\n32\n39 40\n47 48\n55 56\n63\nLet’s now think about what we need to store in these blocks to build\na ﬁle system. Of course, the ﬁrst thing that comes to mind is user data.\nIn fact, most of the space in any ﬁle system is (and should be) user data.\nLet’s call the region of the disk we use for user data the data region, and,\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2285,
      "extraction_method": "Direct"
    },
    {
      "page_number": 499,
      "chapter": null,
      "content": "FILE SYSTEM IMPLEMENTATION\n463\nagain for simplicity, reserve a ﬁxed portion of the disk for these blocks,\nsay the last 56 of 64 blocks on the disk:\n0\n7\nD\n8\nD D D D D D D\n15\nD\n16\nD D D D D D D\n23\nD\n24\nD D D D D D D\n31\nD\n32\nD D D D D D D\n39\nD\n40\nD D D D D D D\n47\nD\n48\nD D D D D D D\n55\nD\n56\nD D D D D D D\n63\nData Region\nData Region\nAs we learned about (a little) last chapter, the ﬁle system has to track\ninformation about each ﬁle. This information is a key piece of metadata,\nand tracks things like which data blocks (in the data region) comprise\na ﬁle, the size of the ﬁle, its owner and access rights, access and mod-\nify times, and other similar kinds of information. To store this informa-\ntion, ﬁle system usually have a structure called an inode (we’ll read more\nabout inodes below).\nTo accommodate inodes, we’ll need to reserve some space on the disk\nfor them as well. Let’s call this portion of the disk the inode table, which\nsimply holds an array of on-disk inodes. Thus, our on-disk image now\nlooks like this picture, assuming that we use 5 of our 64 blocks for inodes\n(denoted by I’s in the diagram):\n0\nI\nI\nI\nI\nI\n7\nD\n8\nD D D D D D D\n15\nD\n16\nD D D D D D D\n23\nD\n24\nD D D D D D D\n31\nD\n32\nD D D D D D D\n39\nD\n40\nD D D D D D D\n47\nD\n48\nD D D D D D D\n55\nD\n56\nD D D D D D D\n63\nData Region\nData Region\nInodes\nWe should note here that inodes are typically not that big, for example\n128 or 256 bytes. Assuming 256 bytes per inode, a 4-KB block can hold 16\ninodes, and our ﬁle system above contains 80 total inodes. In our simple\nﬁle system, built on a tiny 64-block partition, this number represents the\nmaximum number of ﬁles we can have in our ﬁle system; however, do\nnote that the same ﬁle system, built on a larger disk, could simply allocate\na larger inode table and thus accommodate more ﬁles.\nOur ﬁle system thus far has data blocks (D), and inodes (I), but a few\nthings are still missing. One primary component that is still needed, as\nyou might have guessed, is some way to track whether inodes or data\nblocks are free or allocated. Such allocation structures are thus a requisite\nelement in any ﬁle system.\nMany allocation-tracking methods are possible, of course. For exam-\nple, we could use a free list that points to the ﬁrst free block, which then\npoints to the next free block, and so forth. We instead choose a simple and\npopular structure known as a bitmap, one for the data region (the data\nbitmap), and one for the inode table (the inode bitmap). A bitmap is a\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2520,
      "extraction_method": "Direct"
    },
    {
      "page_number": 500,
      "chapter": null,
      "content": "464\nFILE SYSTEM IMPLEMENTATION\nsimple structure: each bit is used to indicate whether the corresponding\nobject/block is free (0) or in-use (1). And thus our new on-disk layout,\nwith an inode bitmap (i) and a data bitmap (d):\n0\ni d\nI\nI\nI\nI\nI\n7\nD\n8\nD D D D D D D\n15\nD\n16\nD D D D D D D\n23\nD\n24\nD D D D D D D\n31\nD\n32\nD D D D D D D\n39\nD\n40\nD D D D D D D\n47\nD\n48\nD D D D D D D\n55\nD\n56\nD D D D D D D\n63\nData Region\nData Region\nInodes\nYou may notice that it is a bit of overkill to use an entire 4-KB block for\nthese bitmaps; such a bitmap can track whether 32K objects are allocated,\nand yet we only have 80 inodes and 56 data blocks. However, we just use\nan entire 4-KB block for each of these bitmaps for simplicity.\nThe careful reader (i.e., the reader who is still awake) may have no-\nticed there is one block left in the design of the on-disk structure of our\nvery simple ﬁle system. We reserve this for the superblock, denoted by\nan S in the diagram below. The superblock contains information about\nthis particular ﬁle system, including, for example, how many inodes and\ndata blocks are in the ﬁle system (80 and 56, respectively in this instance),\nwhere the inode table begins (block 3), and so forth. It will likely also\ninclude a magic number of some kind to identify the ﬁle system type (in\nthis case, vsfs).\nS\n0\ni d\nI\nI\nI\nI\nI\n7\nD\n8\nD D D D D D D\n15\nD\n16\nD D D D D D D\n23\nD\n24\nD D D D D D D\n31\nD\n32\nD D D D D D D\n39\nD\n40\nD D D D D D D\n47\nD\n48\nD D D D D D D\n55\nD\n56\nD D D D D D D\n63\nData Region\nData Region\nInodes\nThus, when mounting a ﬁle system, the operating system will read\nthe superblock ﬁrst, to initialize various parameters, and then attach the\nvolume to the ﬁle-system tree. When ﬁles within the volume are accessed,\nthe system will thus know exactly where to look for the needed on-disk\nstructures.\n40.3\nFile Organization: The Inode\nOne of the most important on-disk structures of a ﬁle system is the\ninode; virtually all ﬁle systems have a structure similar to this. The name\ninode is short for index node, the historical name given to it by UNIX in-\nventor Ken Thompson [RT74], used because these nodes were originally\narranged in an array, and the array indexed into when accessing a partic-\nular inode.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2267,
      "extraction_method": "Direct"
    },
    {
      "page_number": 501,
      "chapter": null,
      "content": "FILE SYSTEM IMPLEMENTATION\n465\nASIDE: DATA STRUCTURE – THE INODE\nThe inode is the generic name that is used in many ﬁle systems to de-\nscribe the structure that holds the metadata for a given ﬁle, such as its\nlength, permissions, and the location of its constituent blocks. The name\ngoes back at least as far as UNIX (and probably further back to Multics\nif not earlier systems); it is short for index node, as the inode number is\nused to index into an array of on-disk inodes in order to ﬁnd the inode\nof that number. As we’ll see, design of the inode is one key part of ﬁle\nsystem design. Most modern systems have some kind of structure like\nthis for every ﬁle they track, but perhaps call them different things (such\nas dnodes, fnodes, etc.).\nEach inode is implicitly referred to by a number (called the inumber),\nwhich we’ve earlier called the low-level name of the ﬁle. In vsfs (and\nother simple ﬁle systems), given an i-number, you should directly be able\nto calculate where on the disk the corresponding inode is located. For ex-\nample, take the inode table of vsfs as above: 20-KB in size (5 4-KB blocks)\nand thus consisting of 80 inodes (assuming each inode is 256 bytes); fur-\nther assume that the inode region starts at 12KB (i.e, the superblock starts\nat 0KB, the inode bitmap is at address 4KB, the data bitmap at 8KB, and\nthus the inode table comes right after). In vsfs, we thus have the following\nlayout for the beginning of the ﬁle system partition (in closeup view):\nSuper\ni-bmap d-bmap\n0KB\n4KB\n8KB\n12KB\n16KB\n20KB\n24KB\n28KB\n32KB\nThe Inode Table (Closeup)\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10 11\n12 13 14 15\n16 17 18 19\n20 21 22 23\n24 25 26 27\n28 29 30 31\n32 33 34 35\n36 37 38 39\n40 41 42 43\n44 45 46 47\n48 49 50 51\n52 53 54 55\n56 57 58 59\n60 61 62 63\n64 65 66 67\n68 69 70 71\n72 73 74 75\n76 77 78 79\niblock 0\niblock 1\niblock 2\niblock 3\niblock 4\nTo read inode number 32, the ﬁle system would ﬁrst calculate the offset\ninto the inode region (32·sizeof(inode) or 8192, add it to the start address\nof the inode table on disk (inodeStartAddr = 12KB), and thus arrive\nupon the correct byte address of the desired block of inodes: 20KB. Re-\ncall that disks are not byte addressable, but rather consist of a large num-\nber of addressable sectors, usually 512 bytes. Thus, to fetch the block of\ninodes that contains inode 32, the ﬁle system would issue a read to sector\n20×1024\n512\n, or 40, to fetch the desired inode block. More generally, the sector\naddress iaddr of the inode block can be calculated as follows:\nblk\n= (inumber * sizeof(inode_t)) / blockSize;\nsector = ((blk * blockSize) + inodeStartAddr) / sectorSize;\nInside each inode is virtually all of the information you need about a\nﬁle: its type (e.g., regular ﬁle, directory, etc.), its size, the number of blocks\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2811,
      "extraction_method": "Direct"
    },
    {
      "page_number": 502,
      "chapter": null,
      "content": "466\nFILE SYSTEM IMPLEMENTATION\nSize\nName\nWhat is this inode ﬁeld for?\n2\nmode\ncan this ﬁle be read/written/executed?\n2\nuid\nwho owns this ﬁle?\n4\nsize\nhow many bytes are in this ﬁle?\n4\ntime\nwhat time was this ﬁle last accessed?\n4\nctime\nwhat time was this ﬁle created?\n4\nmtime\nwhat time was this ﬁle last modiﬁed?\n4\ndtime\nwhat time was this inode deleted?\n2\ngid\nwhich group does this ﬁle belong to?\n2\nlinks count\nhow many hard links are there to this ﬁle?\n4\nblocks\nhow many blocks have been allocated to this ﬁle?\n4\nﬂags\nhow should ext2 use this inode?\n4\nosd1\nan OS-dependent ﬁeld\n60\nblock\na set of disk pointers (15 total)\n4\ngeneration\nﬁle version (used by NFS)\n4\nﬁle acl\na new permissions model beyond mode bits\n4\ndir acl\ncalled access control lists\n4\nfaddr\nan unsupported ﬁeld\n12\ni osd2\nanother OS-dependent ﬁeld\nTable 40.1: The ext2 inode\nallocated to it, protection information (such as who owns the ﬁle, as well\nas who can access it), some time information, including when the ﬁle was\ncreated, modiﬁed, or last accessed, as well as information about where its\ndata blocks reside on disk (e.g., pointers of some kind). We refer to all\nsuch information about a ﬁle as metadata; in fact, any information inside\nthe ﬁle system that isn’t pure user data is often referred to as such. An\nexample inode from ext2 [P09] is shown below in Table 40.1.\nOne of the most important decisions in the design of the inode is how\nit refers to where data blocks are. One simple approach would be to\nhave one or more direct pointers (disk addresses) inside the inode; each\npointer refers to one disk block that belongs to the ﬁle. Such an approach\nis limited: for example, if you want to have a ﬁle that is really big (e.g.,\nbigger than the size of a block multiplied by the number of direct point-\ners), you are out of luck.\nThe Multi-Level Index\nTo support bigger ﬁles, ﬁle system designers have had to introduce dif-\nferent structures within inodes. One common idea is to have a special\npointer known as an indirect pointer. Instead of pointing to a block that\ncontains user data, it points to a block that contains more pointers, each\nof which point to user data. Thus, an inode may have some ﬁxed number\nof direct pointers (e.g., 12), and a single indirect pointer. If a ﬁle grows\nlarge enough, an indirect block is allocated (from the data-block region\nof the disk), and the inode’s slot for an indirect pointer is set to point to\nit. Assuming that a block is 4KB and 4-byte disk addresses, that adds\nanother 1024 pointers; the ﬁle can grow to be (12 + 1024) · 4K or 4144KB.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2610,
      "extraction_method": "Direct"
    },
    {
      "page_number": 503,
      "chapter": null,
      "content": "FILE SYSTEM IMPLEMENTATION\n467\nTIP: CONSIDER EXTENT-BASED APPROACHES\nA different approach is to use extents instead of pointers. An extent is\nsimply a disk pointer plus a length (in blocks); thus, instead of requiring\na pointer for every block of a ﬁle, all one needs is a pointer and a length\nto specify the on-disk location of a ﬁle. Just a single extent is limiting, as\none may have trouble ﬁnding a contiguous chunk of on-disk free space\nwhen allocating a ﬁle. Thus, extent-based ﬁle systems often allow for\nmore than one extent, thus giving more freedom to the ﬁle system during\nﬁle allocation.\nIn comparing the two approaches, pointer-based approaches are the most\nﬂexible but use a large amount of metadata per ﬁle (particularly for large\nﬁles). Extent-based approaches are less ﬂexible but more compact; in par-\nticular, they work well when there is enough free space on the disk and\nﬁles can be laid out contiguously (which is the goal for virtually any ﬁle\nallocation policy anyhow).\nNot surprisingly, in such an approach, you might want to support\neven larger ﬁles. To do so, just add another pointer to the inode: the dou-\nble indirect pointer. This pointer refers to a block that contains pointers\nto indirect blocks, each of which contain pointers to data blocks. A dou-\nble indirect block thus adds the possibility to grow ﬁles with an additional\n1024 · 1024 or 1-million 4KB blocks, in other words supporting ﬁles that\nare over 4GB in size. You may want even more, though, and we bet you\nknow where this is headed: the triple indirect pointer.\nOverall, this imbalanced tree is referred to as the multi-level index ap-\nproach to pointing to ﬁle blocks. Let’s examine an example with twelve\ndirect pointers, as well as both a single and a double indirect block. As-\nsuming a block size of 4 KB, and 4-byte pointers, this structure can accom-\nmodate a ﬁle of just over 4 GB in size (i.e., (12 + 1024 + 10242) × 4 KB).\nCan you ﬁgure out how big of a ﬁle can be handled with the addition of\na triple-indirect block? (hint: pretty big)\nMany ﬁle systems use a multi-level index, including commonly-used\nﬁle systems such as Linux ext2 [P09] and ext3, NetApp’s WAFL, as well as\nthe original UNIX ﬁle system. Other ﬁle systems, including SGI XFS and\nLinux ext4, use extents instead of simple pointers; see the earlier aside for\ndetails on how extent-based schemes work (they are akin to segments in\nthe discussion of virtual memory).\nYou might be wondering: why use an imbalanced tree like this? Why\nnot a different approach? Well, as it turns out, many researchers have\nstudied ﬁle systems and how they are used, and virtually every time they\nﬁnd certain “truths” that hold across the decades. One such ﬁnding is\nthat most ﬁles are small. This imbalanced design reﬂects such a reality; if\nmost ﬁles are indeed small, it makes sense to optimize for this case. Thus,\nwith a small number of direct pointers (12 is a typical number), an inode\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2987,
      "extraction_method": "Direct"
    },
    {
      "page_number": 504,
      "chapter": null,
      "content": "468\nFILE SYSTEM IMPLEMENTATION\nASIDE: LINKED-BASED APPROACHES\nAnother simpler approach in designing inodes is to use a linked list.\nThus, inside an inode, instead of having multiple pointers, you just need\none, to point to the ﬁrst block of the ﬁle. To handle larger ﬁles, add an-\nother pointer at the end of that data block, and so on, and thus you can\nsupport large ﬁles.\nAs you might have guessed, linked ﬁle allocation performs poorly for\nsome workloads; think about reading the last block of a ﬁle, for example,\nor just doing random access. Thus, to make linked allocation work better,\nsome systems will keep an in-memory table of link information, instead\nof storing the next pointers with the data blocks themselves. The table\nis indexed by the address of a data block D; the content of an entry is\nsimply D’s next pointer, i.e., the address of the next block in a ﬁle which\nfollows D. A null-value could be there too (indicating an end-of-ﬁle), or\nsome other marker to indicate that a particular block is free. Having such\na table of next pointers makes it so that a linked allocation scheme can\neffectively do random ﬁle accesses, simply by ﬁrst scanning through the\n(in memory) table to ﬁnd the desired block, and then accessing (on disk)\nit directly.\nDoes such a table sound familiar? What we have described is the basic\nstructure of what is known as the ﬁle allocation table, or FAT ﬁle system.\nYes, this classic old Windows ﬁle system, before NTFS [C94], is based on a\nsimple linked-based allocation scheme. There are other differences from\na standard UNIX ﬁle system too; for example, there are no inodes per se,\nbut rather directory entries which store metadata about a ﬁle and refer\ndirectly to the ﬁrst block of said ﬁle, which makes creating hard links\nimpossible. See Brouwer [B02] for more of the inelegant details.\ncan directly point to 48 KB of data, needing one (or more) indirect blocks\nfor larger ﬁles. See Agrawal et. al [A+07] for a recent study; Table 40.2\nsummarizes those results.\nOf course, in the space of inode design, many other possibilities ex-\nist; after all, the inode is just a data structure, and any data structure that\nstores the relevant information, and can query it effectively, is sufﬁcient.\nAs ﬁle system software is readily changed, you should be willing to ex-\nplore different designs should workloads or technologies change.\nMost ﬁles are small\nRoughly 2K is the most common size\nAverage ﬁle size is growing\nAlmost 200K is the average\nMost bytes are stored in large ﬁles\nA few big ﬁles use most of the space\nFile systems contains lots of ﬁles\nAlmost 100K on average\nFile systems are roughly half full\nEven as disks grow, ﬁle systems remain ˜50% full\nDirectories are typically small\nMany have few entries; most have 20 or fewer\nTable 40.2: File System Measurement Summary\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2868,
      "extraction_method": "Direct"
    },
    {
      "page_number": 505,
      "chapter": null,
      "content": "FILE SYSTEM IMPLEMENTATION\n469\n40.4\nDirectory Organization\nIn vsfs (as in many ﬁle systems), directories have a simple organiza-\ntion; a directory basically just contains a list of (entry name, inode num-\nber) pairs. For each ﬁle or directory in a given directory, there is a string\nand a number in the data block(s) of the directory. For each string, there\nmay also be a length (assuming variable-sized names).\nFor example, assume a directory dir (inode number 5) has three ﬁles\nin it (foo, bar, and foobar), and their inode numbers are 12, 13, and 24\nrespectively. The on-disk data for dir might look like this:\ninum | reclen | strlen | name\n5\n4\n2\n.\n2\n4\n3\n..\n12\n4\n4\nfoo\n13\n4\n4\nbar\n24\n8\n7\nfoobar\nIn this example, each entry has an inode number, record length (the\ntotal bytes for the name plus any left over space), string length (the actual\nlength of the name), and ﬁnally the name of the entry. Note that each di-\nrectory has two extra entries, . “dot” and .. “dot-dot”; the dot directory\nis just the current directory (in this example, dir), whereas dot-dot is the\nparent directory (in this case, the root).\nDeleting a ﬁle (e.g., calling unlink()) can leave an empty space in\nthe middle of the directory, and hence there should be some way to mark\nthat as well (e.g., with a reserved inode number such as zero). Such a\ndelete is one reason the record length is used: a new entry may reuse an\nold, bigger entry and thus have extra space within.\nYou might be wondering where exactly directories are stored. Often,\nﬁle systems treat directories as a special type of ﬁle. Thus, a directory has\nan inode, somewhere in the inode table (with the type ﬁeld of the inode\nmarked as “directory” instead of “regular ﬁle”). The directory has data\nblocks pointed to by the inode (and perhaps, indirect blocks); these data\nblocks live in the data block region of our simple ﬁle system. Our on-disk\nstructure thus remains unchanged.\nWe should also note again that this simple linear list of directory en-\ntries is not the only way to store such information. As before, any data\nstructure is possible. For example, XFS [S+96] stores directories in B-tree\nform, making ﬁle create operations (which have to ensure that a ﬁle name\nhas not been used before creating it) faster than systems with simple lists\nthat must be scanned in their entirety.\n40.5\nFree Space Management\nA ﬁle system must track which inodes and data blocks are free, and\nwhich are not, so that when a new ﬁle or directory is allocated, it can ﬁnd\nspace for it. Thus free space management is important for all ﬁle systems.\nIn vsfs, we have two simple bitmaps for this task.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2668,
      "extraction_method": "Direct"
    },
    {
      "page_number": 506,
      "chapter": null,
      "content": "470\nFILE SYSTEM IMPLEMENTATION\nASIDE: FREE SPACE MANAGEMENT\nThere are many ways to manage free space; bitmaps are just one way.\nSome early ﬁle systems used free lists, where a single pointer in the super\nblock was kept to point to the ﬁrst free block; inside that block the next\nfree pointer was kept, thus forming a list through the free blocks of the\nsystem. When a block was needed, the head block was used and the list\nupdated accordingly.\nModern ﬁle systems use more sophisticated data structures. For example,\nSGI’s XFS [S+96] uses some form of a B-tree to compactly represent which\nchunks of the disk are free. As with any data structure, different time-\nspace trade-offs are possible.\nFor example, when we create a ﬁle, we will have to allocate an inode\nfor that ﬁle. The ﬁle system will thus search through the bitmap for an in-\node that is free, and allocate it to the ﬁle; the ﬁle system will have to mark\nthe inode as used (with a 1) and eventually update the on-disk bitmap\nwith the correct information. A similar set of activities take place when a\ndata block is allocated.\nSome other considerations might also come into play when allocating\ndata blocks for a new ﬁle. For example, some Linux ﬁle systems, such\nas ext2 and ext3, will look for a sequence of blocks (say 8) that are free\nwhen a new ﬁle is created and needs data blocks; by ﬁnding such a se-\nquence of free blocks, and then allocating them to the newly-created ﬁle,\nthe ﬁle system guarantees that a portion of the ﬁle will be on the disk and\ncontiguous, thus improving performance. Such a pre-allocation policy is\nthus a commonly-used heuristic when allocating space for data blocks.\n40.6\nAccess Paths: Reading and Writing\nNow that we have some idea of how ﬁles and directories are stored on\ndisk, we should be able to follow the ﬂow of operation during the activity\nof reading or writing a ﬁle. Understanding what happens on this access\npath is thus the second key in developing an understanding of how a ﬁle\nsystem works; pay attention!\nFor the following examples, let us assume that the ﬁle system has been\nmounted and thus that the superblock is already in memory. Everything\nelse (i.e., inodes, directories) is still on the disk.\nReading A File From Disk\nIn this simple example, let us ﬁrst assume that you want to simply open\na ﬁle (e.g., /foo/bar, read it, and then close it. For this simple example,\nlet’s assume the ﬁle is just 4KB in size (i.e., 1 block).\nWhen you issue an open(\"/foo/bar\", O RDONLY) call, the ﬁle sys-\ntem ﬁrst needs to ﬁnd the inode for the ﬁle bar, to obtain some basic in-\nformation about the ﬁle (permissions information, ﬁle size, etc.). To do so,\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2706,
      "extraction_method": "Direct"
    },
    {
      "page_number": 507,
      "chapter": null,
      "content": "FILE SYSTEM IMPLEMENTATION\n471\ndata\ninode\nroot\nfoo\nbar\nroot\nfoo\nbar\nbar\nbar\nbitmap bitmap inode inode inode data data data[0] data[1] data[1]\nread\nread\nopen(bar)\nread\nread\nread\nread\nread()\nread\nwrite\nread\nread()\nread\nwrite\nread\nread()\nread\nwrite\nTable 40.3: File Read Timeline (Time Increasing Downward)\nthe ﬁle system must be able to ﬁnd the inode, but all it has right now is\nthe full pathname. The ﬁle system must traverse the pathname and thus\nlocate the desired inode.\nAll traversals begin at the root of the ﬁle system, in the root directory\nwhich is simply called /. Thus, the ﬁrst thing the FS will read from disk\nis the inode of the root directory. But where is this inode? To ﬁnd an\ninode, we must know its i-number. Usually, we ﬁnd the i-number of a ﬁle\nor directory in its parent directory; the root has no parent (by deﬁnition).\nThus, the root inode number must be “well known”; the FS must know\nwhat it is when the ﬁle system is mounted. In most UNIX ﬁle systems,\nthe root inode number is 2. Thus, to begin the process, the FS reads in the\nblock that contains inode number 2 (the ﬁrst inode block).\nOnce the inode is read in, the FS can look inside of it to ﬁnd pointers to\ndata blocks, which contain the contents of the root directory. The FS will\nthus use these on-disk pointers to read through the directory, in this case\nlooking for an entry for foo. By reading in one or more directory data\nblocks, it will ﬁnd the entry for foo; once found, the FS will also have\nfound the inode number of foo (say it is 44) which it will need next.\nThe next step is to recursively traverse the pathname until the desired\ninode is found. In this example, the FS would next read the block contain-\ning the inode of foo and then read in its directory data, ﬁnally ﬁnding the\ninode number of bar. The ﬁnal step of open(), then, is to read its inode\ninto memory; the FS can then do a ﬁnal permissions check, allocate a ﬁle\ndescriptor for this process in the per-process open-ﬁle table, and return it\nto the user.\nOnce open, the program can then issue a read() system call to read\nfrom the ﬁle. The ﬁrst read (at offset 0 unless lseek() has been called)\nwill thus read in the ﬁrst block of the ﬁle, consulting the inode to ﬁnd\nthe location of such a block; it may also update the inode with a new last-\naccessed time. The read will further update the in-memory open ﬁle table\nfor this ﬁle descriptor, updating the ﬁle offset such that the next read will\nread the second ﬁle block, etc.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2525,
      "extraction_method": "Direct"
    },
    {
      "page_number": 508,
      "chapter": null,
      "content": "472\nFILE SYSTEM IMPLEMENTATION\nASIDE: READS DON’T ACCESS ALLOCATION STRUCTURES\nWe’ve seen many students get confused by allocation structures such\nas bitmaps. In particular, many often think that when you are simply\nreading a ﬁle, and not allocating any new blocks, that the bitmap will still\nbe consulted. This is not true! Allocation structures, such as bitmaps,\nare only accessed when allocation is needed. The inodes, directories, and\nindirect blocks have all the information they need to complete a read re-\nquest; there is no need to make sure a block is allocated when the inode\nalready points to it.\nAt some point, the ﬁle will be closed. There is much less work to be\ndone here; clearly, the ﬁle descriptor should be deallocated, but for now,\nthat is all the FS really needs to do. No disk I/Os take place.\nA depiction of this entire process is found in Figure 40.3 (time increases\ndownward). In the ﬁgure, the open causes numerous reads to take place\nin order to ﬁnally locate the inode of the ﬁle. Afterwards, reading each\nblock requires the ﬁle system to ﬁrst consult the inode, then read the\nblock, and then update the inode’s last-accessed-time ﬁeld with a write.\nSpend some time and try to understand what is going on.\nAlso note that the amount of I/O generated by the open is propor-\ntional to the length of the pathname. For each additional directory in the\npath, we have to read its inode as well as its data. Making this worse\nwould be the presence of large directories; here, we only have to read one\nblock to get the contents of a directory, whereas with a large directory, we\nmight have to read many data blocks to ﬁnd the desired entry. Yes, life\ncan get pretty bad when reading a ﬁle; as you’re about to ﬁnd out, writing\nout a ﬁle (and especially, creating a new one) is even worse.\nWriting to Disk\nWriting to a ﬁle is a similar process. First, the ﬁle must be opened (as\nabove). Then, the application can issue write() calls to update the ﬁle\nwith new contents. Finally, the ﬁle is closed.\nUnlike reading, writing to the ﬁle may also allocate a block (unless\nthe block is being overwritten, for example). When writing out a new\nﬁle, each write not only has to write data to disk but has to ﬁrst decide\nwhich block to allocate to the ﬁle and thus update other structures of the\ndisk accordingly (e.g., the data bitmap). Thus, each write to a ﬁle logically\ngenerates three I/Os: one to read the data bitmap, which is then updated\nto mark the newly-allocated block as used, one to write the bitmap (to\nreﬂect its new state to disk), and one to write the actual block itself.\nThe amount of write trafﬁc is even worse when one considers a sim-\nple and common operation such as ﬁle creation. To create a ﬁle, the ﬁle\nsystem must not only allocate an inode, but also allocate space within\nthe directory containing the new ﬁle. The total amount of I/O trafﬁc to\ndo so is quite high: one read to the inode bitmap (to ﬁnd a free inode),\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2999,
      "extraction_method": "Direct"
    },
    {
      "page_number": 509,
      "chapter": null,
      "content": "FILE SYSTEM IMPLEMENTATION\n473\ndata\ninode\nroot\nfoo\nbar\nroot\nfoo\nbar\nbar\nbar\nbitmap bitmap inode inode inode data\ndata\ndata[0] data[1] data[1]\nread\nread\nread\nread\ncreate\nread\n(/foo/bar)\nwrite\nwrite\nread\nwrite\nwrite\nread\nread\nwrite()\nwrite\nwrite\nwrite\nread\nread\nwrite()\nwrite\nwrite\nwrite\nread\nread\nwrite()\nwrite\nwrite\nwrite\nTable 40.4: File Creation Timeline (Time Increasing Downward)\none write to the inode bitmap (to mark it allocated), one write to the new\ninode itself (to initialize it), one to the data of the directory (to link the\nhigh-level name of the ﬁle to its inode number), and one read and write\nto the directory inode to update it. If the directory needs to grow to ac-\ncommodate the new entry, additional I/Os (i.e., to the data bitmap, and\nthe new directory block) will be needed too. All that just to create a ﬁle!\nLet’s look at a speciﬁc example, where the ﬁle /foo/bar is created,\nand three blocks are written to it. Figure 40.4 shows what happens during\nthe open() (which creates the ﬁle) and during each of three 4KB writes.\nIn the ﬁgure, reads and writes to the disk are grouped under which\nsystem call caused them to occur, and the rough ordering they might take\nplace in goes from top to bottom of the ﬁgure. You can see how much\nwork it is to create the ﬁle: 10 I/Os in this case, to walk the pathname\nand then ﬁnally create the ﬁle. You can also see that each allocating write\ncosts 5 I/Os: a pair to read and update the inode, another pair to read\nand update the data bitmap, and then ﬁnally the write of the data itself.\nHow can a ﬁle system accomplish any of this with reasonable efﬁciency?\nTHE CRUX: HOW TO REDUCE FILE SYSTEM I/O COSTS\nEven the simplest of operations like opening, reading, or writing a ﬁle\nincurs a huge number of I/O operations, scattered over the disk. What\ncan a ﬁle system do to reduce the high costs of doing so many I/Os?\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 1918,
      "extraction_method": "Direct"
    },
    {
      "page_number": 510,
      "chapter": null,
      "content": "474\nFILE SYSTEM IMPLEMENTATION\n40.7\nCaching and Buffering\nAs the examples above show, reading and writing ﬁles can be expen-\nsive, incurring many I/Os to the (slow) disk. To remedy what would\nclearly be a huge performance problem, most ﬁle systems aggressively\nuse system memory (DRAM) to cache important blocks.\nImagine the open example above: without caching, every ﬁle open\nwould require at least two reads for every level in the directory hierarchy\n(one to read the inode of the directory in question, and at least one to read\nits data). With a long pathname (e.g., /1/2/3/ ... /100/ﬁle.txt), the ﬁle\nsystem would literally perform hundreds of reads just to open the ﬁle!\nEarly ﬁle systems thus introduced a ﬁx-sized cache to hold popular\nblocks. As in our discussion of virtual memory, strategies such as LRU\nand different variants would decide which blocks to keep in cache. This\nﬁx-sized cache would usually be allocated at boot time to be roughly 10%\nof total memory. Modern systems integrate virtual memory pages and ﬁle\nsystem pages into a uniﬁed page cache [S00]. In this way, memory can be\nallocated more ﬂexibly across virtual memory and ﬁle system, depending\non which needs more memory at a given time.\nNow imagine the ﬁle open example with caching. The ﬁrst open may\ngenerate a lot of I/O trafﬁc to read in directory inode and data, but sub-\nsequent ﬁle opens of that same ﬁle (or ﬁles in the same directory) will\nmostly hit in the cache and thus no I/O is needed.\nLet us also consider the effect of caching on writes. Whereas read I/O\ncan be avoided altogether with a sufﬁciently large cache, write trafﬁc has\nto go to disk in order to become persistent. Thus, a cache does not serve\nas the same kind of ﬁlter on write trafﬁc that it does for reads. That said,\nwrite buffering (as it is sometimes called) certainly has a number of per-\nformance beneﬁts. First, by delaying writes, the ﬁle system can batch\nsome updates into a smaller set of I/Os; for example, if an inode bitmap\nis updated when one ﬁle is created and then updated moments later as\nanother ﬁle is created, the ﬁle system saves an I/O by delaying the write\nafter the ﬁrst update. Second, by buffering a number of writes in memory,\nthe system can then schedule the subsequent I/Os and thus increase per-\nformance. Finally, some writes are avoided altogether by delaying them;\nfor example, if an application creates a ﬁle and then deletes it, delaying\nthe writes to reﬂect the ﬁle creation to disk avoids them entirely. In this\ncase, laziness (in writing blocks to disk) is a virtue.\nFor the reasons above, most modern ﬁle systems buffer writes in mem-\nory for anywhere between ﬁve and thirty seconds, representing yet an-\nother trade-off: if the system crashes before the updates have been prop-\nagated to disk, the updates are lost; however, by keeping writes in mem-\nory longer, performance can be improved by batching, scheduling, and\neven avoiding writes.\nSome applications (such as databases) don’t enjoy this trade-off. Thus,\nto avoid unexpected data loss due to write buffering, they simply force\nwrites to disk, by calling fsync(), by using direct I/O interfaces that\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 3203,
      "extraction_method": "Direct"
    },
    {
      "page_number": 511,
      "chapter": null,
      "content": "FILE SYSTEM IMPLEMENTATION\n475\nwork around the cache, or by using the raw disk interface and avoiding\nthe ﬁle system altogether1. While most applications live with the trade-\noffs made by the ﬁle system, there are enough controls in place to get the\nsystem to do what you want it to, should the default not be satisfying.\n40.8\nSummary\nWe have seen the basic machinery required in building a ﬁle system.\nThere needs to be some information about each ﬁle (metadata), usually\nstored in a structure called an inode. Directories are just a speciﬁc type\nof ﬁle that store name→inode-number mappings. And other structures\nare needed too; for example, ﬁle systems often use a structure such as a\nbitmap to track which inodes or data blocks are free or allocated.\nThe terriﬁc aspect of ﬁle system design is its freedom; the ﬁle systems\nwe explore in the coming chapters each take advantage of this freedom\nto optimize some aspect of the ﬁle system. There are also clearly many\npolicy decisions we have left unexplored. For example, when a new ﬁle\nis created, where should it be placed on disk? This policy and others will\nalso be the subject of future chapters. Or will they?\n1Take a database class to learn more about old-school databases and their former insis-\ntence on avoiding the OS and controlling everything themselves.\nBut watch out!\nThose\ndatabase types are always trying to bad mouth the OS. Shame on you, database people. Shame.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 1473,
      "extraction_method": "Direct"
    },
    {
      "page_number": 512,
      "chapter": null,
      "content": "476\nFILE SYSTEM IMPLEMENTATION\nReferences\n[A+07] Nitin Agrawal, William J. Bolosky, John R. Douceur, Jacob R. Lorch\nA Five-Year Study of File-System Metadata\nFAST ’07, pages 31–45, February 2007, San Jose, CA\nAn excellent recent analysis of how ﬁle systems are actually used. Use the bibliography within to follow\nthe trail of ﬁle-system analysis papers back to the early 1980s.\n[B07] “ZFS: The Last Word in File Systems”\nJeff Bonwick and Bill Moore\nAvailable: http://opensolaris.org/os/community/zfs/docs/zfs last.pdf\nOne of the most recent important ﬁle systems, full of features and awesomeness. We should have a\nchapter on it, and perhaps soon will.\n[B02] “The FAT File System”\nAndries Brouwer\nSeptember, 2002\nAvailable: http://www.win.tue.nl/˜aeb/linux/fs/fat/fat.html\nA nice clean description of FAT. The ﬁle system kind, not the bacon kind. Though you have to admit,\nbacon fat probably tastes better.\n[C94] “Inside the Windows NT File System”\nHelen Custer\nMicrosoft Press, 1994\nA short book about NTFS; there are probably ones with more technical details elsewhere.\n[H+88] “Scale and Performance in a Distributed File System”\nJohn H. Howard, Michael L. Kazar, Sherri G. Menees, David A. Nichols, M. Satyanarayanan,\nRobert N. Sidebotham, Michael J. West.\nACM Transactions on Computing Systems (ACM TOCS), page 51-81, Volume 6, Number 1,\nFebruary 1988\nA classic distributed ﬁle system; we’ll be learning more about it later, don’t worry.\n[P09] “The Second Extended File System: Internal Layout”\nDave Poirier, 2009\nAvailable: http://www.nongnu.org/ext2-doc/ext2.html\nSome details on ext2, a very simple Linux ﬁle system based on FFS, the Berkeley Fast File System. We’ll\nbe reading about it in the next chapter.\n[RT74] “The UNIX Time-Sharing System”\nM. Ritchie and K. Thompson\nCACM, Volume 17:7, pages 365-375, 1974\nThe original paper about UNIX. Read it to see the underpinnings of much of modern operating systems.\n[S00] “UBC: An Efﬁcient Uniﬁed I/O and Memory Caching Subsystem for NetBSD”\nChuck Silvers\nFREENIX, 2000\nA nice paper about NetBSD’s integration of ﬁle-system buffer caching and the virtual-memory page\ncache. Many other systems do the same type of thing.\n[S+96] “Scalability in the XFS File System”\nAdan Sweeney, Doug Doucette, Wei Hu, Curtis Anderson,\nMike Nishimoto, Geoff Peck\nUSENIX ’96, January 1996, San Diego, CA\nThe ﬁrst attempt to make scalability of operations, including things like having millions of ﬁles in a\ndirectory, a central focus. A great example of pushing an idea to the extreme. The key idea behind this\nﬁle system: everything is a tree. We should have a chapter on this ﬁle system too.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2677,
      "extraction_method": "Direct"
    },
    {
      "page_number": 513,
      "chapter": null,
      "content": "FILE SYSTEM IMPLEMENTATION\n477\nHomework\nUse this tool, vsfs.py, to study how ﬁle system state changes as var-\nious operations take place. The ﬁle system begins in an empty state, with\njust a root directory. As the simulation takes place, various operations are\nperformed, thus slowly changing the on-disk state of the ﬁle system. See\nthe README for details.\nQuestions\n1. Run the simulator with some different random seeds (say 17, 18, 19,\n20), and see if you can ﬁgure out which operations must have taken\nplace between each state change.\n2. Now do the same, using different random seeds (say 21, 22, 23,\n24), except run with the -r ﬂag, thus making you guess the state\nchange while being shown the operation. What can you conclude\nabout the inode and data-block allocation algorithms, in terms of\nwhich blocks they prefer to allocate?\n3. Now reduce the number of data blocks in the ﬁle system, to very\nlow numbers (say two), and run the simulator for a hundred or so\nrequests. What types of ﬁles end up in the ﬁle system in this highly-\nconstrained layout? What types of operations would fail?\n4. Now do the same, but with inodes. With very few inodes, what\ntypes of operations can succeed? Which will usually fail? What is\nthe ﬁnal state of the ﬁle system likely to be?\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 1313,
      "extraction_method": "Direct"
    },
    {
      "page_number": 514,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 515,
      "chapter": null,
      "content": "41\nLocality and The Fast File System\nWhen the UNIX operating system was ﬁrst introduced, the UNIX wizard\nhimself Ken Thompson wrote the ﬁrst ﬁle system. We will call that the\n“old UNIX ﬁle system”, and it was really simple. Basically, its data struc-\ntures looked like this on the disk:\nS\nInodes\nData\nThe super block (S) contained information about the entire ﬁle system:\nhow big the volume is, how many inodes there are, a pointer to the head\nof a free list of blocks, and so forth. The inode region of the disk contained\nall the inodes for the ﬁle system. Finally, most of the disk was taken up\nby data blocks.\nThe good thing about the old ﬁle system was that it was simple, and\nsupported the basic abstractions the ﬁle system was trying to deliver:\nﬁles and the directory hierarchy. This easy-to-use system was a real step\nforward from the clumsy, record-based storage systems of the past, and\nthe directory hierarchy a true advance over simpler, one-level hierarchies\nprovided by earlier systems.\n41.1\nThe Problem: Poor Performance\nThe problem: performance was terrible. As measured by Kirk McKu-\nsick and his colleagues at Berkeley [MJLF84], performance started off bad\nand got worse over time, to the point where the ﬁle system was delivering\nonly 2% of overall disk bandwidth!\nThe main issue was that the old UNIX ﬁle system treated the disk like it\nwas a random-access memory; data was spread all over the place without\nregard to the fact that the medium holding the data was a disk, and thus\nhad real and expensive positioning costs. For example, the data blocks of\na ﬁle were often very far away from its inode, thus inducing an expensive\nseek whenever one ﬁrst read the inode and then the data blocks of a ﬁle\n(a pretty common operation).\n479\n",
      "content_length": 1754,
      "extraction_method": "Direct"
    },
    {
      "page_number": 516,
      "chapter": null,
      "content": "480\nLOCALITY AND THE FAST FILE SYSTEM\nWorse, the ﬁle system would end up getting quite fragmented, as the\nfree space was not carefully managed. The free list would end up point-\ning to a bunch of blocks spread across the disk, and as ﬁles got allocated,\nthey would simply take the next free block. The result was that a logi-\ncally contiguous ﬁle would be accessed by going back and forth across\nthe disk, thus reducing performance dramatically.\nFor example, imagine the following data block region, which contains\nfour ﬁles (A, B, C, and D), each of size 2 blocks:\nA1\nA2\nB1\nB2\nC1\nC2\nD1\nD2\nIf B and D are deleted, the resulting layout is:\nA1\nA2\nC1\nC2\nAs you can see, the free space is fragmented into two chunks of two\nblocks, instead of one nice contiguous chunk of four. Let’s say we now\nwish to allocate a ﬁle E, of size four blocks:\nA1\nA2\nE1\nE2\nC1\nC2\nE3\nE4\nYou can see what happens: E gets spread across the disk, and as a\nresult, when accessing E, you don’t get peak (sequential) performance\nfrom the disk. Rather, you ﬁrst read E1 and E2, then seek, then read E3\nand E4. This fragmentation problem happened all the time in the old\nUNIX ﬁle system, and it hurt performance. (A side note: this problem is\nexactly what disk defragmentation tools help with; they will reorganize\non-disk data to place ﬁles contiguously and make free space one or a few\ncontiguous regions, moving data around and then rewriting inodes and\nsuch to reﬂect the changes)\nOne other problem: the original block size was too small (512 bytes).\nThus, transferring data from the disk was inherently inefﬁcient. Smaller\nblocks were good because they minimized internal fragmentation (waste\nwithin the block), but bad for transfer as each block might require a posi-\ntioning overhead to reach it. We can summarize the problem as follows:\nTHE CRUX:\nHOW TO ORGANIZE ON-DISK DATA TO IMPROVE PERFORMANCE\nHow can we organize ﬁle system data structures so as to improve per-\nformance? What types of allocation policies do we need on top of those\ndata structures? How do we make the ﬁle system “disk aware”?\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2121,
      "extraction_method": "Direct"
    },
    {
      "page_number": 517,
      "chapter": null,
      "content": "LOCALITY AND THE FAST FILE SYSTEM\n481\n41.2\nFFS: Disk Awareness Is The Solution\nA group at Berkeley decided to build a better, faster ﬁle system, which\nthey cleverly called the Fast File System (FFS). The idea was to design\nthe ﬁle system structures and allocation policies to be “disk aware” and\nthus improve performance, which is exactly what they did. FFS thus ush-\nered in a new era of ﬁle system research; by keeping the same interface\nto the ﬁle system (the same APIs, including open(), read(), write(),\nclose(), and other ﬁle system calls) but changing the internal implemen-\ntation, the authors paved the path for new ﬁle system construction, work\nthat continues today. Virtually all modern ﬁle systems adhere to the ex-\nisting interface (and thus preserve compatibility with applications) while\nchanging their internals for performance, reliability, or other reasons.\n41.3\nOrganizing Structure: The Cylinder Group\nThe ﬁrst step was to change the on-disk structures. FFS divides the\ndisk into a bunch of groups known as cylinder groups (some modern ﬁle\nsystems like Linux ext2 and ext3 just call them block groups). We can\nthus imagine a disk with ten cylinder groups:\nG0\nG1\nG2\nG3\nG4\nG5\nG6\nG7\nG8\nG9\nThese groups are the central mechanism that FFS uses to improve per-\nformance; by placing two ﬁles within the same group, FFS can ensure that\naccessing one after the other will not result in long seeks across the disk.\nThus, FFS needs to have the ability to allocate ﬁles and directories\nwithin each of these groups. Each group looks like this:\nS ib db\nInodes\nData\nWe now describe the components of a cylinder group. A copy of the\nsuper block (S) is found in each group for reliability reasons (e.g., if one\ngets corrupted or scratched, you can still mount and access the ﬁle system\nby using one of the others).\nThe inode bitmap (ib) and data bitmap (db) track whether each inode\nor data block is free, respectively. Bitmaps are an excellent way to manage\nfree space in a ﬁle system because it is easy to ﬁnd a large chunk of free\nspace and allocate it to a ﬁle, perhaps avoiding some of the fragmentation\nproblems of the free list in the old ﬁle system.\nFinally, the inode and data block regions are just like in the previous\nvery simple ﬁle system. Most of each cylinder group, as usual, is com-\nprised of data blocks.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2368,
      "extraction_method": "Direct"
    },
    {
      "page_number": 518,
      "chapter": null,
      "content": "482\nLOCALITY AND THE FAST FILE SYSTEM\nASIDE: FFS FILE CREATION\nAs an example, think about what data structures must be updated when\na ﬁle is created; assume, for this example, that the user creates a new ﬁle\n/foo/bar.txt and that the ﬁle is one block long (4KB). The ﬁle is new,\nand thus needs a new inode; thus, both the inode bitmap and the newly-\nallocated inode will be written to disk. The ﬁle also has data in it and\nthus it too must be allocated; the data bitmap and a data block will thus\n(eventually) be written to disk. Hence, at least four writes to the current\ncylinder group will take place (recall that these writes may be buffered\nin memory for a while before the write takes place). But this is not all!\nIn particular, when creating a new ﬁle, we must also place the ﬁle in the\nﬁle-system hierarchy; thus, the directory must be updated. Speciﬁcally,\nthe parent directory foo must be updated to add the entry for bar.txt;\nthis update may ﬁt in an existing data block of foo or require a new block\nto be allocated (with associated data bitmap). The inode of foo must also\nbe updated, both to reﬂect the new length of the directory as well as to\nupdate time ﬁelds (such as last-modiﬁed-time). Overall, it is a lot of work\njust to create a new ﬁle! Perhaps next time you do so, you should be more\nthankful, or at least surprised that it all works so well.\n41.4\nPolicies: How To Allocate Files and Directories\nWith this group structure in place, FFS now has to decide how to place\nﬁles and directories and associated metadata on disk to improve perfor-\nmance. The basic mantra is simple: keep related stuff together (and its corol-\nlary, keep unrelated stuff far apart).\nThus, to obey the mantra, FFS has to decide what is “related” and\nplace it within the same block group; conversely, unrelated items should\nbe placed into different block groups. To achieve this end, FFS makes use\nof a few simple placement heuristics.\nThe ﬁrst is the placement of directories. FFS employs a simple ap-\nproach: ﬁnd the cylinder group with a low number of allocated directo-\nries (because we want to balance directories across groups) and a high\nnumber of free inodes (because we want to subsequently be able to allo-\ncate a bunch of ﬁles), and put the directory data and inode in that group.\nOf course, other heuristics could be used here (e.g., taking into account\nthe number of free data blocks).\nFor ﬁles, FFS does two things. First, it makes sure (in the general case)\nto allocate the data blocks of a ﬁle in the same group as its inode, thus\npreventing long seeks between inode and data (as in the old ﬁle sys-\ntem). Second, it places all ﬁles that are in the same directory in the cylin-\nder group of the directory they are in. Thus, if a user creates four ﬁles,\n/dir1/1.txt, /dir1/2.txt, /dir1/3.txt, and /dir99/4.txt, FFS\nwould try to place the ﬁrst three near one another (same group) and the\nfourth far away (in some other group).\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2985,
      "extraction_method": "Direct"
    },
    {
      "page_number": 519,
      "chapter": null,
      "content": "LOCALITY AND THE FAST FILE SYSTEM\n483\n0\n2\n4\n6\n8\n10\n0%\n20%\n40%\n60%\n80%\n100%\nFFS Locality\nPath Difference\nCumulative Frequency\nTrace\nRandom\nFigure 41.1: FFS Locality For SEER Traces\nIt should be noted that these heuristics are not based on extensive\nstudies of ﬁle-system trafﬁc or anything particularly nuanced; rather, they\nare based on good old-fashioned common sense (isn’t that what CS stands\nfor after all?). Files in a directory are often accessed together (imagine\ncompiling a bunch of ﬁles and then linking them into a single executable).\nBecause they are, FFS will often improve performance, making sure that\nseeks between related ﬁles are short.\n41.5\nMeasuring File Locality\nTo understand better whether these heuristics make sense, we decided\nto analyze some traces of ﬁle system access and see if indeed there is\nnamespace locality; for some reason, there doesn’t seem to be a good\nstudy of this topic in the literature.\nSpeciﬁcally, we took the SEER traces [K94] and analyzed how “far\naway” ﬁle accesses were from one another in the directory tree. For ex-\nample, if ﬁle f is opened, and then re-opened next in the trace (before\nany other ﬁles are opened), the distance between these two opens in the\ndirectory tree is zero (as they are the same ﬁle). If a ﬁle f in directory\ndir (i.e., dir/f) is opened, and followed by an open of ﬁle g in the same\ndirectory (i.e., dir/g), the distance between the two ﬁle accesses is one,\nas they share the same directory but are not the same ﬁle. Our distance\nmetric, in other words, measures how far up the directory tree you have\nto travel to ﬁnd the common ancestor of two ﬁles; the closer they are in the\ntree, the lower the metric.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 1727,
      "extraction_method": "Direct"
    },
    {
      "page_number": 520,
      "chapter": null,
      "content": "484\nLOCALITY AND THE FAST FILE SYSTEM\nFigure 41.1 shows the locality observed in the SEER traces over all\nworkstations in the SEER cluster over the entirety of all traces. The graph\nplots the difference metric along the x-axis, and shows the cumulative\npercentage of ﬁle opens that were of that difference along the y-axis.\nSpeciﬁcally, for the SEER traces (marked “Trace” in the graph), you can\nsee that about 7% of ﬁle accesses were to the ﬁle that was opened previ-\nously, and that nearly 40% of ﬁle accesses were to either the same ﬁle or\nto one in the same directory (i.e., a difference of zero or one). Thus, the\nFFS locality assumption seems to make sense (at least for these traces).\nInterestingly, another 25% or so of ﬁle accesses were to ﬁles that had a\ndistance of two. This type of locality occurs when the user has structured\na set of related directories in a multi-level fashion and consistently jumps\nbetween them. For example, if a user has a src directory and builds\nobject ﬁles (.o ﬁles) into a obj directory, and both of these directories\nare sub-directories of a main proj directory, a common access pattern\nwill be proj/src/foo.c followed by proj/obj/foo.o. The distance\nbetween these two accesses is two, as proj is the common ancestor. FFS\ndoes not capture this type of locality in its policies, and thus more seeking\nwill occur between such accesses.\nWe also show what locality would be for a “Random” trace for the\nsake of comparison. We generated the random trace by selecting ﬁles\nfrom within an existing SEER trace in random order, and calculating the\ndistance metric between these randomly-ordered accesses. As you can\nsee, there is less namespace locality in the random traces, as expected.\nHowever, because eventually every ﬁle shares a common ancestor (e.g.,\nthe root), there is some locality eventually, and thus random trace is use-\nful as a comparison point.\n41.6\nThe Large-File Exception\nIn FFS, there is one important exception to the general policy of ﬁle\nplacement, and it arises for large ﬁles. Without a different rule, a large\nﬁle would entirely ﬁll the block group it is ﬁrst placed within (and maybe\nothers). Filling a block group in this manner is undesirable, as it prevents\nsubsequent “related” ﬁles from being placed within this block group, and\nthus may hurt ﬁle-access locality.\nThus, for large ﬁles, FFS does the following. After some number of\nblocks are allocated into the ﬁrst block group (e.g., 12 blocks, or the num-\nber of direct pointers available within an inode), FFS places the next “large”\nchunk of the ﬁle (e.g., those pointed to by the ﬁrst indirect block) in an-\nother block group (perhaps chosen for its low utilization). Then, the next\nchunk of the ﬁle is placed in yet another different block group, and so on.\nLet’s look at some pictures to understand this policy better. Without\nthe large-ﬁle exception, a single large ﬁle would place all of its blocks into\none part of the disk. We use a small example of a ﬁle with 10 blocks to\nillustrate the behavior visually.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 3082,
      "extraction_method": "Direct"
    },
    {
      "page_number": 521,
      "chapter": null,
      "content": "LOCALITY AND THE FAST FILE SYSTEM\n485\nHere is the depiction of FFS without the large-ﬁle exception:\nG0\nG1\nG2\nG3\nG4\nG5\nG6\nG7\nG8\nG9\n0 1 2 3 4\n5 6 7 8 9\nWith the large-ﬁle exception, we might see something more like this, with\nthe ﬁle spread across the disk in chunks:\nG0\nG1\nG2\nG3\nG4\nG5\nG6\nG7\nG8\nG9\n0 1\n2 3\n4 5\n6 7\n8 9\nThe astute reader will note that spreading blocks of a ﬁle across the\ndisk will hurt performance, particularly in the relatively common case\nof sequential ﬁle access (e.g., when a user or application reads chunks 0\nthrough 9 in order). And you are right! It will. We can help this a little,\nby choosing our chunk size carefully.\nSpeciﬁcally, if the chunk size is large enough, we will still spend most\nof our time transferring data from disk and just a relatively little time\nseeking between chunks of the block. This process of reducing an over-\nhead by doing more work per overhead paid is called amortization and\nis a common technique in computer systems.\nLet’s do an example: assume that the average positioning time (i.e.,\nseek and rotation) for a disk is 10 ms. Assume further that the disk trans-\nfers data at 40 MB/s. If our goal was to spend half our time seeking be-\ntween chunks and half our time transferring data (and thus achieve 50%\nof peak disk performance), we would thus need to spend 10 ms transfer-\nring data for every 10 ms positioning. So the question becomes: how big\ndoes a chunk have to be in order to spend 10 ms in transfer? Easy, just\nuse our old friend, math, in particular the dimensional analysis we spoke\nof in the chapter on disks:\n40 \u0018\u0018\nMB\n\b\b\nsec\n· 1024 KB\n1 \u0018\u0018\nMB\n·\n1 \b\b\nsec\n1000 \b\b\nms · 10 \b\b\nms = 409.6 KB\n(41.1)\nBasically, what this equation says is this: if you transfer data at 40\nMB/s, you need to transfer only 409.6 KB every time you seek in order to\nspend half your time seeking and half your time transferring. Similarly,\nyou can compute the size of the chunk you would need to achieve 90%\nof peak bandwidth (turns out it is about 3.69 MB), or even 99% of peak\nbandwidth (40.6 MB!). As you can see, the closer you want to get to peak,\nthe bigger these chunks get (see Figure 41.2 for a plot of these values).\nFFS did not use this type of calculation in order to spread large ﬁles\nacross groups, however. Instead, it took a simple approach, based on the\nstructure of the inode itself. The ﬁrst twelve direct blocks were placed\nin the same group as the inode; each subsequent indirect block, and all\nthe blocks it pointed to, was placed in a different group. With a block\nsize of 4-KB, and 32-bit disk addresses, this strategy implies that every\n1024 blocks of the ﬁle (4 MB) were placed in separate groups, the lone\nexception being the ﬁrst 48-KB of the ﬁle as pointed to by direct pointers.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2793,
      "extraction_method": "Direct"
    },
    {
      "page_number": 522,
      "chapter": null,
      "content": "486\nLOCALITY AND THE FAST FILE SYSTEM\n0%\n25%\n50%\n75%\n100%\n1K\n32K\n1M\n10M\nThe Challenges of Amortization\nPercent Bandwidth (Desired)\nLog(Chunk Size Needed)\n50%, 409.6K\n90%, 3.69M\nFigure 41.2: Amortization: How Big Do Chunks Have To Be?\nWe should note that the trend in disk drives is that transfer rate im-\nproves fairly rapidly, as disk manufacturers are good at cramming more\nbits into the same surface, but the mechanical aspects of drives related\nto seeks (disk arm speed and the rate of rotation) improve rather slowly\n[P98]. The implication is that over time, mechanical costs become rel-\natively more expensive, and thus, to amortize said costs, you have to\ntransfer more data between seeks.\n41.7\nA Few Other Things About FFS\nFFS introduced a few other innovations too. In particular, the design-\ners were extremely worried about accommodating small ﬁles; as it turned\nout, many ﬁles were 2 KB or so in size back then, and using 4-KB blocks,\nwhile good for transferring data, was not so good for space efﬁciency.\nThis internal fragmentation could thus lead to roughly half the disk be-\ning wasted for a typical ﬁle system.\nThe solution the FFS designers hit upon was simple and solved the\nproblem. They decided to introduce sub-blocks, which were 512-byte lit-\ntle blocks that the ﬁle system could allocate to ﬁles. Thus, if you created a\nsmall ﬁle (say 1 KB in size), it would occupy two sub-blocks and thus not\nwaste an entire 4-KB block. As the ﬁle grew, the ﬁle system will continue\nallocating 512-byte blocks to it until it acquires a full 4-KB of data. At that\npoint, FFS will ﬁnd a 4-KB block, copy the sub-blocks into it, and free the\nsub-blocks for future use.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1722,
      "extraction_method": "Direct"
    },
    {
      "page_number": 523,
      "chapter": null,
      "content": "LOCALITY AND THE FAST FILE SYSTEM\n487\n0\n11\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\nSpindle\n0\n11\n5\n10\n4\n9\n3\n8\n2\n7\n1\n6\nSpindle\nFigure 41.3: FFS: Standard Versus Parameterized Placement\nYou might observe that this process is inefﬁcient, requiring a lot of ex-\ntra work for the ﬁle system (in particular, a lot of extra I/O to perform the\ncopy). And you’d be right again! Thus, FFS generally avoided this pes-\nsimal behavior by modifying the libc library; the library would buffer\nwrites and then issue them in 4-KB chunks to the ﬁle system, thus avoid-\ning the sub-block specialization entirely in most cases.\nA second neat thing that FFS introduced was a disk layout that was\noptimized for performance. In those times (before SCSI and other more\nmodern device interfaces), disks were much less sophisticated and re-\nquired the host CPU to control their operation in a more hands-on way.\nA problem arose in FFS when a ﬁle was placed on consecutive sectors of\nthe disk, as on the left in Figure 41.3.\nIn particular, the problem arose during sequential reads. FFS would\nﬁrst issue a read to block 0; by the time the read was complete, and FFS\nissued a read to block 1, it was too late: block 1 had rotated under the\nhead and now the read to block 1 would incur a full rotation.\nFFS solved this problem with a different layout, as you can see on the\nright in Figure 41.3. By skipping over every other block (in the example),\nFFS has enough time to request the next block before it went past the\ndisk head. In fact, FFS was smart enough to ﬁgure out for a particular\ndisk how many blocks it should skip in doing layout in order to avoid the\nextra rotations; this technique was called parameterization, as FFS would\nﬁgure out the speciﬁc performance parameters of the disk and use those\nto decide on the exact staggered layout scheme.\nYou might be thinking: this scheme isn’t so great after all. In fact, you\nwill only get 50% of peak bandwidth with this type of layout, because\nyou have to go around each track twice just to read each block once. For-\ntunately, modern disks are much smarter: they internally read the entire\ntrack in and buffer it in an internal disk cache (often called a track buffer\nfor this very reason). Then, on subsequent reads to the track, the disk will\njust return the desired data from its cache. File systems thus no longer\nhave to worry about these incredibly low-level details. Abstraction and\nhigher-level interfaces can be a good thing, when designed properly.\nSome other usability improvements were added as well. FFS was one\nof the ﬁrst ﬁle systems to allow for long ﬁle names, thus enabling more\nexpressive names in the ﬁle system instead of a the traditional ﬁxed-size\napproach (e.g., 8 characters).\nFurther, a new concept was introduced\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2790,
      "extraction_method": "Direct"
    },
    {
      "page_number": 524,
      "chapter": null,
      "content": "488\nLOCALITY AND THE FAST FILE SYSTEM\nTIP: MAKE THE SYSTEM USABLE\nProbably the most basic lesson from FFS is that not only did it intro-\nduce the conceptually good idea of disk-aware layout, but it also added\na number of features that simply made the system more usable. Long ﬁle\nnames, symbolic links, and a rename operation that worked atomically\nall improved the utility of a system; while hard to write a research pa-\nper about (imagine trying to read a 14-pager about “The Symbolic Link:\nHard Link’s Long Lost Cousin”), such small features made FFS more use-\nful and thus likely increased its chances for adoption. Making a system\nusable is often as or more important than its deep technical innovations.\ncalled a symbolic link. As discussed in a previous chapter, hard links are\nlimited in that they both could not point to directories (for fear of intro-\nducing loops in the ﬁle system hierarchy) and that they can only point to\nﬁles within the same volume (i.e., the inode number must still be mean-\ningful). Symbolic links allow the user to create an “alias” to any other\nﬁle or directory on a system and thus are much more ﬂexible. FFS also\nintroduced an atomic rename() operation for renaming ﬁles. Usabil-\nity improvements, beyond the basic technology, also likely gained FFS a\nstronger user base.\n41.8\nSummary\nThe introduction of FFS was a watershed moment in ﬁle system his-\ntory, as it made clear that the problem of ﬁle management was one of the\nmost interesting issues within an operating system, and showed how one\nmight begin to deal with that most important of devices, the hard disk.\nSince that time, hundreds of new ﬁle systems have developed, but still\ntoday many ﬁle systems take cues from FFS (e.g., Linux ext2 and ext3 are\nobvious intellectual descendants). Certainly all modern systems account\nfor the main lesson of FFS: treat the disk like it’s a disk.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1929,
      "extraction_method": "Direct"
    },
    {
      "page_number": 525,
      "chapter": null,
      "content": "LOCALITY AND THE FAST FILE SYSTEM\n489\nReferences\n[MJLF84] “A Fast File System for UNIX”\nMarshall K. McKusick, William N. Joy, Sam J. Lefﬂer, Robert S. Fabry\nACM Transactions on Computing Systems.\nAugust, 1984. Volume 2, Number 3.\npages 181-197.\nMcKusick was recently honored with the IEEE Reynold B. Johnson award for his contributions to ﬁle\nsystems, much of which was based on his work building FFS. In his acceptance speech, he discussed the\noriginal FFS software: only 1200 lines of code! Modern versions are a little more complex, e.g., the BSD\nFFS descendant now is in the 50-thousand lines-of-code range.\n[P98] “Hardware Technology Trends and Database Opportunities”\nDavid A. Patterson\nKeynote Lecture at the ACM SIGMOD Conference (SIGMOD ’98)\nJune, 1998\nA great and simple overview of disk technology trends and how they change over time.\n[K94] “The Design of the SEER Predictive Caching System”\nG. H. Kuenning\nMOBICOMM ’94, Santa Cruz, California, December 1994\nAccording to Kuenning, this is the best overview of the SEER project, which led to (among other things)\nthe collection of these traces.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 1148,
      "extraction_method": "Direct"
    },
    {
      "page_number": 526,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 527,
      "chapter": null,
      "content": "42\nCrash Consistency: FSCK and Journaling\nAs we’ve seen thus far, the ﬁle system manages a set of data structures to\nimplement the expected abstractions: ﬁles, directories, and all of the other\nmetadata needed to support the basic abstraction that we expect from a\nﬁle system. Unlike most data structures (for example, those found in\nmemory of a running program), ﬁle system data structures must persist,\ni.e., they must survive over the long haul, stored on devices that retain\ndata despite power loss (such as hard disks or ﬂash-based SSDs).\nOne major challenge faced by a ﬁle system is how to update persis-\ntent data structures despite the presence of a power loss or system crash.\nSpeciﬁcally, what happens if, right in the middle of updating on-disk\nstructures, someone trips over the power cord and the machine loses\npower? Or the operating system encounters a bug and crashes? Because\nof power losses and crashes, updating a persistent data structure can be\nquite tricky, and leads to a new and interesting problem in ﬁle system\nimplementation, known as the crash-consistency problem.\nThis problem is quite simple to understand. Imagine you have to up-\ndate two on-disk structures, A and B, in order to complete a particular\noperation. Because the disk only services a single request at a time, one\nof these requests will reach the disk ﬁrst (either A or B). If the system\ncrashes or loses power after one write completes, the on-disk structure\nwill be left in an inconsistent state. And thus, we have a problem that all\nﬁle systems need to solve:\nTHE CRUX: HOW TO UPDATE THE DISK DESPITE CRASHES\nThe system may crash or lose power between any two writes, and\nthus the on-disk state may only partially get updated. After the crash,\nthe system boots and wishes to mount the ﬁle system again (in order to\naccess ﬁles and such). Given that crashes can occur at arbitrary points\nin time, how do we ensure the ﬁle system keeps the on-disk image in a\nreasonable state?\n491\n",
      "content_length": 1975,
      "extraction_method": "Direct"
    },
    {
      "page_number": 528,
      "chapter": null,
      "content": "492\nCRASH CONSISTENCY: FSCK AND JOURNALING\nIn this chapter, we’ll describe this problem in more detail, and look\nat some methods ﬁle systems have used to overcome it. We’ll begin by\nexamining the approach taken by older ﬁle systems, known as fsck or the\nﬁle system checker. We’ll then turn our attention to another approach,\nknown as journaling (also known as write-ahead logging), a technique\nwhich adds a little bit of overhead to each write but recovers more quickly\nfrom crashes or power losses. We will discuss the basic machinery of\njournaling, including a few different ﬂavors of journaling that Linux ext3\n[T98,PAA05] (a relatively modern journaling ﬁle system) implements.\n42.1\nA Detailed Example\nTo kick off our investigation of journaling, let’s look at an example.\nWe’ll need to use a workload that updates on-disk structures in some\nway. Assume here that the workload is simple: the append of a single\ndata block to an existing ﬁle. The append is accomplished by opening the\nﬁle, calling lseek() to move the ﬁle offset to the end of the ﬁle, and then\nissuing a single 4KB write to the ﬁle before closing it.\nLet’s also assume we are using standard simple ﬁle system structures\non the disk, similar to ﬁle systems we have seen before. This tiny example\nincludes an inode bitmap (with just 8 bits, one per inode), a data bitmap\n(also 8 bits, one per data block), inodes (8 total, numbered 0 to 7, and\nspread across four blocks), and data blocks (8 total, numbered 0 to 7).\nHere is a diagram of this ﬁle system:\nInode\nBmap\nData\nBmap\nInodes\nData Blocks\nI[v1]\nDa\nIf you look at the structures in the picture, you can see that a single inode\nis allocated (inode number 2), which is marked in the inode bitmap, and a\nsingle allocated data block (data block 4), also marked in the data bitmap.\nThe inode is denoted I[v1], as it is the ﬁrst version of this inode; it will\nsoon be updated (due to the workload described above).\nLet’s peek inside this simpliﬁed inode too. Inside of I[v1], we see:\nowner\n: remzi\npermissions : read-only\nsize\n: 1\npointer\n: 4\npointer\n: null\npointer\n: null\npointer\n: null\nIn this simpliﬁed inode, the size of the ﬁle is 1 (it has one block al-\nlocated), the ﬁrst direct pointer points to block 4 (the ﬁrst data block of\nthe ﬁle, Da), and all three other direct pointers are set to null (indicating\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2377,
      "extraction_method": "Direct"
    },
    {
      "page_number": 529,
      "chapter": null,
      "content": "CRASH CONSISTENCY: FSCK AND JOURNALING\n493\nthat they are not used). Of course, real inodes have many more ﬁelds; see\nprevious chapters for more information.\nWhen we append to the ﬁle, we are adding a new data block to it, and\nthus must update three on-disk structures: the inode (which must point\nto the new block as well as have a bigger size due to the append), the\nnew data block Db, and a new version of the data bitmap (call it B[v2]) to\nindicate that the new data block has been allocated.\nThus, in the memory of the system, we have three blocks which we\nmust write to disk. The updated inode (inode version 2, or I[v2] for short)\nnow looks like this:\nowner\n: remzi\npermissions : read-only\nsize\n: 2\npointer\n: 4\npointer\n: 5\npointer\n: null\npointer\n: null\nThe updated data bitmap (B[v2]) now looks like this: 00001100. Finally,\nthere is the data block (Db), which is just ﬁlled with whatever it is users\nput into ﬁles. Stolen music perhaps?\nWhat we would like is for the ﬁnal on-disk image of the ﬁle system to\nlook like this:\nInode\nBmap\nData\nBmap\nInodes\nData Blocks\nI[v2]\nDa\nDb\nTo achieve this transition, the ﬁle system must perform three sepa-\nrate writes to the disk, one each for the inode (I[v2]), bitmap (B[v2]), and\ndata block (Db). Note that these writes usually don’t happen immedi-\nately when the user issues a write() system call; rather, the dirty in-\node, bitmap, and new data will sit in main memory (in the page cache\nor buffer cache) for some time ﬁrst; then, when the ﬁle system ﬁnally\ndecides to write them to disk (after say 5 seconds or 30 seconds), the ﬁle\nsystem will issue the requisite write requests to the disk. Unfortunately,\na crash may occur and thus interfere with these updates to the disk. In\nparticular, if a crash happens after one or two of these writes have taken\nplace, but not all three, the ﬁle system could be left in a funny state.\nCrash Scenarios\nTo understand the problem better, let’s look at some example crash sce-\nnarios. Imagine only a single write succeeds; there are thus three possible\noutcomes, which we list here:\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2112,
      "extraction_method": "Direct"
    },
    {
      "page_number": 530,
      "chapter": null,
      "content": "494\nCRASH CONSISTENCY: FSCK AND JOURNALING\n• Just the data block (Db) is written to disk. In this case, the data is\non disk, but there is no inode that points to it and no bitmap that\neven says the block is allocated. Thus, it is as if the write never\noccurred. This case is not a problem at all, from the perspective of\nﬁle-system crash consistency1.\n• Just the updated inode (I[v2]) is written to disk. In this case, the\ninode points to the disk address (5) where Db was about to be writ-\nten, but Db has not yet been written there. Thus, if we trust that\npointer, we will read garbage data from the disk (the old contents\nof disk address 5).\nFurther, we have a new problem, which we call a ﬁle-system incon-\nsistency. The on-disk bitmap is telling us that data block 5 has not\nbeen allocated, but the inode is saying that it has. This disagree-\nment in the ﬁle system data structures is an inconsistency in the\ndata structures of the ﬁle system; to use the ﬁle system, we must\nsomehow resolve this problem (more on that below).\n• Just the updated bitmap (B[v2]) is written to disk. In this case, the\nbitmap indicates that block 5 is allocated, but there is no inode that\npoints to it. Thus the ﬁle system is inconsistent again; if left unre-\nsolved, this write would result in a space leak, as block 5 would\nnever be used by the ﬁle system.\nThere are also three more crash scenarios in this attempt to write three\nblocks to disk. In these cases, two writes succeed and the last one fails:\n• The inode (I[v2]) and bitmap (B[v2]) are written to disk, but not\ndata (Db). In this case, the ﬁle system metadata is completely con-\nsistent: the inode has a pointer to block 5, the bitmap indicates that\n5 is in use, and thus everything looks OK from the perspective of\nthe ﬁle system’s metadata. But there is one problem: 5 has garbage\nin it again.\n• The inode (I[v2]) and the data block (Db) are written, but not the\nbitmap (B[v2]). In this case, we have the inode pointing to the cor-\nrect data on disk, but again have an inconsistency between the in-\node and the old version of the bitmap (B1). Thus, we once again\nneed to resolve the problem before using the ﬁle system.\n• The bitmap (B[v2]) and data block (Db) are written, but not the\ninode (I[v2]). In this case, we again have an inconsistency between\nthe inode and the data bitmap. However, even though the block\nwas written and the bitmap indicates its usage, we have no idea\nwhich ﬁle it belongs to, as no inode points to the ﬁle.\n1However, it might be a problem for the user, who just lost some data!\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2605,
      "extraction_method": "Direct"
    },
    {
      "page_number": 531,
      "chapter": null,
      "content": "CRASH CONSISTENCY: FSCK AND JOURNALING\n495\nThe Crash Consistency Problem\nHopefully, from these crash scenarios, you can see the many problems\nthat can occur to our on-disk ﬁle system image because of crashes: we can\nhave inconsistency in ﬁle system data structures; we can have space leaks;\nwe can return garbage data to a user; and so forth. What we’d like to do\nideally is move the ﬁle system from one consistent state (e.g., before the\nﬁle got appended to) to another atomically (e.g., after the inode, bitmap,\nand new data block have been written to disk). Unfortunately, we can’t\ndo this easily because the disk only commits one write at a time, and\ncrashes or power loss may occur between these updates. We call this\ngeneral problem the crash-consistency problem (we could also call it the\nconsistent-update problem).\n42.2\nSolution #1: The File System Checker\nEarly ﬁle systems took a simple approach to crash consistency. Basi-\ncally, they decided to let inconsistencies happen and then ﬁx them later\n(when rebooting). A classic example of this lazy approach is found in a\ntool that does this: fsck2. fsck is a UNIX tool for ﬁnding such inconsis-\ntencies and repairing them [M86]; similar tools to check and repair a disk\npartition exist on different systems. Note that such an approach can’t ﬁx\nall problems; consider, for example, the case above where the ﬁle system\nlooks consistent but the inode points to garbage data. The only real goal\nis to make sure the ﬁle system metadata is internally consistent.\nThe tool fsck operates in a number of phases, as summarized in\nMcKusick and Kowalski’s paper [MK96]. It is run before the ﬁle system\nis mounted and made available (fsck assumes that no other ﬁle-system\nactivity is on-going while it runs); once ﬁnished, the on-disk ﬁle system\nshould be consistent and thus can be made accessible to users.\nHere is a basic summary of what fsck does:\n• Superblock: fsck ﬁrst checks if the superblock looks reasonable,\nmostly doing sanity checks such as making sure the ﬁle system size\nis greater than the number of blocks allocated. Usually the goal of\nthese sanity checks is to ﬁnd a suspect (corrupt) superblock; in this\ncase, the system (or administrator) may decide to use an alternate\ncopy of the superblock.\n• Free blocks: Next, fsck scans the inodes, indirect blocks, double\nindirect blocks, etc., to build an understanding of which blocks are\ncurrently allocated within the ﬁle system. It uses this knowledge\nto produce a correct version of the allocation bitmaps; thus, if there\nis any inconsistency between bitmaps and inodes, it is resolved by\ntrusting the information within the inodes. The same type of check\nis performed for all the inodes, making sure that all inodes that look\nlike they are in use are marked as such in the inode bitmaps.\n2Pronounced either “eff-ess-see-kay”, “eff-ess-check”, or, if you don’t like the tool, “eff-\nsuck”. Yes, serious professional people use this term.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2991,
      "extraction_method": "Direct"
    },
    {
      "page_number": 532,
      "chapter": null,
      "content": "496\nCRASH CONSISTENCY: FSCK AND JOURNALING\n• Inode state: Each inode is checked for corruption or other prob-\nlems. For example, fsck makes sure that each allocated inode has\na valid type ﬁeld (e.g., regular ﬁle, directory, symbolic link, etc.). If\nthere are problems with the inode ﬁelds that are not easily ﬁxed, the\ninode is considered suspect and cleared by fsck; the inode bitmap\nis correspondingly updated.\n• Inode links: fsck also veriﬁes the link count of each allocated in-\node. As you may recall, the link count indicates the number of dif-\nferent directories that contain a reference (i.e., a link) to this par-\nticular ﬁle. To verify the link count, fsck scans through the en-\ntire directory tree, starting at the root directory, and builds its own\nlink counts for every ﬁle and directory in the ﬁle system. If there\nis a mismatch between the newly-calculated count and that found\nwithin an inode, corrective action must be taken, usually by ﬁxing\nthe count within the inode. If an allocated inode is discovered but\nno directory refers to it, it is moved to the lost+found directory.\n• Duplicates: fsck also checks for duplicate pointers, i.e., cases where\ntwo different inodes refer to the same block. If one inode is obvi-\nously bad, it may be cleared. Alternately, the pointed-to block could\nbe copied, thus giving each inode its own copy as desired.\n• Bad blocks: A check for bad block pointers is also performed while\nscanning through the list of all pointers. A pointer is considered\n“bad” if it obviously points to something outside its valid range,\ne.g., it has an address that refers to a block greater than the parti-\ntion size. In this case, fsck can’t do anything too intelligent; it just\nremoves (clears) the pointer from the inode or indirect block.\n• Directory checks: fsck does not understand the contents of user\nﬁles; however, directories hold speciﬁcally formatted information\ncreated by the ﬁle system itself. Thus, fsck performs additional\nintegrity checks on the contents of each directory, making sure that\n“.” and “..” are the ﬁrst entries, that each inode referred to in a\ndirectory entry is allocated, and ensuring that no directory is linked\nto more than once in the entire hierarchy.\nAs you can see, building a working fsck requires intricate knowledge\nof the ﬁle system; making sure such a piece of code works correctly in all\ncases can be challenging [G+08]. However, fsck (and similar approaches)\nhave a bigger and perhaps more fundamental problem: they are too slow.\nWith a very large disk volume, scanning the entire disk to ﬁnd all the\nallocated blocks and read the entire directory tree may take many minutes\nor hours. Performance of fsck, as disks grew in capacity and RAIDs\ngrew in popularity, became prohibitive (despite recent advances [M+13]).\nAt a higher level, the basic premise of fsck seems just a tad irra-\ntional. Consider our example above, where just three blocks are written\nto the disk; it is incredibly expensive to scan the entire disk to ﬁx prob-\nlems that occurred during an update of just three blocks. This situation is\nakin to dropping your keys on the ﬂoor in your bedroom, and then com-\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 3205,
      "extraction_method": "Direct"
    },
    {
      "page_number": 533,
      "chapter": null,
      "content": "CRASH CONSISTENCY: FSCK AND JOURNALING\n497\nmencing a search-the-entire-house-for-keys recovery algorithm, starting in\nthe basement and working your way through every room. It works but is\nwasteful. Thus, as disks (and RAIDs) grew, researchers and practitioners\nstarted to look for other solutions.\n42.3\nSolution #2: Journaling (or Write-Ahead Logging)\nProbably the most popular solution to the consistent update problem\nis to steal an idea from the world of database management systems. That\nidea, known as write-ahead logging, was invented to address exactly this\ntype of problem. In ﬁle systems, we usually call write-ahead logging jour-\nnaling for historical reasons. The ﬁrst ﬁle system to do this was Cedar\n[H87], though many modern ﬁle systems use the idea, including Linux\next3 and ext4, reiserfs, IBM’s JFS, SGI’s XFS, and Windows NTFS.\nThe basic idea is as follows. When updating the disk, before over-\nwriting the structures in place, ﬁrst write down a little note (somewhere\nelse on the disk, in a well-known location) describing what you are about\nto do. Writing this note is the “write ahead” part, and we write it to a\nstructure that we organize as a “log”; hence, write-ahead logging.\nBy writing the note to disk, you are guaranteeing that if a crash takes\nplaces during the update (overwrite) of the structures you are updating,\nyou can go back and look at the note you made and try again; thus, you\nwill know exactly what to ﬁx (and how to ﬁx it) after a crash, instead\nof having to scan the entire disk. By design, journaling thus adds a bit\nof work during updates to greatly reduce the amount of work required\nduring recovery.\nWe’ll now describe how Linux ext3, a popular journaling ﬁle system,\nincorporates journaling into the ﬁle system. Most of the on-disk struc-\ntures are identical to Linux ext2, e.g., the disk is divided into block groups,\nand each block group has an inode and data bitmap as well as inodes and\ndata blocks. The new key structure is the journal itself, which occupies\nsome small amount of space within the partition or on another device.\nThus, an ext2 ﬁle system (without journaling) looks like this:\nSuper\nGroup 0\nGroup 1\n. . .\nGroup N\nAssuming the journal is placed within the same ﬁle system image\n(though sometimes it is placed on a separate device, or as a ﬁle within\nthe ﬁle system), an ext3 ﬁle system with a journal looks like this:\nSuper\nJournal\nGroup 0\nGroup 1\n. . .\nGroup N\nThe real difference is just the presence of the journal, and of course,\nhow it is used.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2557,
      "extraction_method": "Direct"
    },
    {
      "page_number": 534,
      "chapter": null,
      "content": "498\nCRASH CONSISTENCY: FSCK AND JOURNALING\nData Journaling\nLet’s look at a simple example to understand how data journaling works.\nData journaling is available as a mode with the Linux ext3 ﬁle system,\nfrom which much of this discussion is based.\nSay we have our canonical update again, where we wish to write the\n‘inode (I[v2]), bitmap (B[v2]), and data block (Db) to disk again. Before\nwriting them to their ﬁnal disk locations, we are now ﬁrst going to write\nthem to the log (a.k.a. journal). This is what this will look like in the log:\nJournal\nTxB\nI[v2]\nB[v2]\nDb\nTxE\nYou can see we have written ﬁve blocks here. The transaction begin\n(TxB) tells us about this update, including information about the pend-\ning update to the ﬁle system (e.g., the ﬁnal addresses of the blocks I[v2],\nB[v2], and Db), as well as some kind of transaction identiﬁer (TID). The\nmiddle three blocks just contain the exact contents of the blocks them-\nselves; this is known as physical logging as we are putting the exact\nphysical contents of the update in the journal (an alternate idea, logi-\ncal logging, puts a more compact logical representation of the update in\nthe journal, e.g., “this update wishes to append data block Db to ﬁle X”,\nwhich is a little more complex but can save space in the log and perhaps\nimprove performance). The ﬁnal block (TxE) is a marker of the end of this\ntransaction, and will also contain the TID.\nOnce this transaction is safely on disk, we are ready to overwrite the\nold structures in the ﬁle system; this process is called checkpointing.\nThus, to checkpoint the ﬁle system (i.e., bring it up to date with the pend-\ning update in the journal), we issue the writes I[v2], B[v2], and Db to\ntheir disk locations as seen above; if these writes complete successfully,\nwe have successfully checkpointed the the ﬁle system and are basically\ndone. Thus, our initial sequence of operations:\n1. Journal write: Write the transaction, including a transaction-begin\nblock, all pending data and metadata updates, and a transaction-\nend block, to the log; wait for these writes to complete.\n2. Checkpoint: Write the pending metadata and data updates to their\nﬁnal locations in the ﬁle system.\nIn our example, we would write TxB, I[v2], B[v2], Db, and TxE to the\njournal ﬁrst. When these writes complete, we would complete the update\nby checkpointing I[v2], B[v2], and Db, to their ﬁnal locations on disk.\nThings get a little trickier when a crash occurs during the writes to\nthe journal. Here, we are trying to write the set of blocks in the transac-\ntion (e.g., TxB, I[v2], B[v2], Db, TxE) to disk. One simple way to do this\nwould be to issue each one at a time, waiting for each to complete, and\nthen issuing the next. However, this is slow. Ideally, we’d like to issue\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2821,
      "extraction_method": "Direct"
    },
    {
      "page_number": 535,
      "chapter": null,
      "content": "CRASH CONSISTENCY: FSCK AND JOURNALING\n499\nASIDE: FORCING WRITES TO DISK\nTo enforce ordering between two disk writes, modern ﬁle systems have\nto take a few extra precautions. In olden times, forcing ordering between\ntwo writes, A and B, was easy: just issue the write of A to the disk, wait\nfor the disk to interrupt the OS when the write is complete, and then issue\nthe write of B.\nThings got slightly more complex due to the increased use of write caches\nwithin disks. With write buffering enabled (sometimes called immediate\nreporting), a disk will inform the OS the write is complete when it simply\nhas been placed in the disk’s memory cache, and has not yet reached\ndisk. If the OS then issues a subsequent write, it is not guaranteed to\nreach the disk after previous writes; thus ordering between writes is not\npreserved. One solution is to disable write buffering. However, more\nmodern systems take extra precautions and issue explicit write barriers;\nsuch a barrier, when it completes, guarantees that all writes issued before\nthe barrier will reach disk before any writes issued after the barrier.\nAll of this machinery requires a great deal of trust in the correct oper-\nation of the disk. Unfortunately, recent research shows that some disk\nmanufacturers, in an effort to deliver “higher performing” disks, explic-\nitly ignore write-barrier requests, thus making the disks seemingly run\nfaster but at the risk of incorrect operation [C+13, R+11]. As Kahan said,\nthe fast almost always beats out the slow, even if the fast is wrong.\nall ﬁve block writes at once, as this would turn ﬁve writes into a single\nsequential write and thus be faster. However, this is unsafe, for the fol-\nlowing reason: given such a big write, the disk internally may perform\nscheduling and complete small pieces of the big write in any order. Thus,\nthe disk internally may (1) write TxB, I[v2], B[v2], and TxE and only later\n(2) write Db. Unfortunately, if the disk loses power between (1) and (2),\nthis is what ends up on disk:\nJournal\nTxB\nid=1\nI[v2]\nB[v2]\n??\nTxE\nid=1\nWhy is this a problem? Well, the transaction looks like a valid trans-\naction (it has a begin and an end with matching sequence numbers). Fur-\nther, the ﬁle system can’t look at that fourth block and know it is wrong;\nafter all, it is arbitrary user data. Thus, if the system now reboots and\nruns recovery, it will replay this transaction, and ignorantly copy the con-\ntents of the garbage block ’??’ to the location where Db is supposed to\nlive. This is bad for arbitrary user data in a ﬁle; it is much worse if it hap-\npens to a critical piece of ﬁle system, such as the superblock, which could\nrender the ﬁle system unmountable.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2731,
      "extraction_method": "Direct"
    },
    {
      "page_number": 536,
      "chapter": null,
      "content": "500\nCRASH CONSISTENCY: FSCK AND JOURNALING\nASIDE: OPTIMIZING LOG WRITES\nYou may have noticed a particular inefﬁciency of writing to the log.\nNamely, the ﬁle system ﬁrst has to write out the transaction-begin block\nand contents of the transaction; only after these writes complete can the\nﬁle system send the transaction-end block to disk. The performance im-\npact is clear, if you think about how a disk works: usually an extra rota-\ntion is incurred (think about why).\nOne of our former graduate students, Vijayan Prabhakaran, had a simple\nidea to ﬁx this problem [P+05]. When writing a transaction to the journal,\ninclude a checksum of the contents of the journal in the begin and end\nblocks. Doing so enables the ﬁle system to write the entire transaction at\nonce, without incurring a wait; if, during recovery, the ﬁle system sees\na mismatch in the computed checksum versus the stored checksum in\nthe transaction, it can conclude that a crash occurred during the write\nof the transaction and thus discard the ﬁle-system update. Thus, with a\nsmall tweak in the write protocol and recovery system, a ﬁle system can\nachieve faster common-case performance; on top of that, the system is\nslightly more reliable, as any reads from the journal are now protected by\na checksum.\nThis simple ﬁx was attractive enough to gain the notice of Linux ﬁle sys-\ntem developers, who then incorporated it into the next generation Linux\nﬁle system, called (you guessed it!) Linux ext4. It now ships on mil-\nlions of machines worldwide, including the Android handheld platform.\nThus, every time you write to disk on many Linux-based systems, a little\ncode developed at Wisconsin makes your system a little faster and more\nreliable.\nTo avoid this problem, the ﬁle system issues the transactional write in\ntwo steps. First, it writes all blocks except the TxE block to the journal,\nissuing these writes all at once. When these writes complete, the journal\nwill look something like this (assuming our append workload again):\nJournal\nTxB\nid=1\nI[v2]\nB[v2]\nDb\nWhen those writes complete, the ﬁle system issues the write of the TxE\nblock, thus leaving the journal in this ﬁnal, safe state:\nJournal\nTxB\nid=1\nI[v2]\nB[v2]\nDb\nTxE\nid=1\nAn important aspect of this process is the atomicity guarantee pro-\nvided by the disk. It turns out that the disk guarantees that any 512-byte\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2396,
      "extraction_method": "Direct"
    },
    {
      "page_number": 537,
      "chapter": null,
      "content": "CRASH CONSISTENCY: FSCK AND JOURNALING\n501\nwrite will either happen or not (and never be half-written); thus, to make\nsure the write of TxE is atomic, one should make it a single 512-byte block.\nThus, our current protocol to update the ﬁle system, with each of its three\nphases labeled:\n1. Journal write: Write the contents of the transaction (including TxB,\nmetadata, and data) to the log; wait for these writes to complete.\n2. Journal commit: Write the transaction commit block (containing\nTxE) to the log; wait for write to complete; transaction is said to be\ncommitted.\n3. Checkpoint: Write the contents of the update (metadata and data)\nto their ﬁnal on-disk locations.\nRecovery\nLet’s now understand how a ﬁle system can use the contents of the jour-\nnal to recover from a crash. A crash may happen at any time during this\nsequence of updates. If the crash happens before the transaction is writ-\nten safely to the log (i.e., before Step 2 above completes), then our job\nis easy: the pending update is simply skipped. If the crash happens af-\nter the transaction has committed to the log, but before the checkpoint is\ncomplete, the ﬁle system can recover the update as follows. When the\nsystem boots, the ﬁle system recovery process will scan the log and look\nfor transactions that have committed to the disk; these transactions are\nthus replayed (in order), with the ﬁle system again attempting to write\nout the blocks in the transaction to their ﬁnal on-disk locations. This form\nof logging is one of the simplest forms there is, and is called redo logging.\nBy recovering the committed transactions in the journal, the ﬁle system\nensures that the on-disk structures are consistent, and thus can proceed\nby mounting the ﬁle system and readying itself for new requests.\nNote that it is ﬁne for a crash to happen at any point during check-\npointing, even after some of the updates to the ﬁnal locations of the blocks\nhave completed. In the worst case, some of these updates are simply per-\nformed again during recovery. Because recovery is a rare operation (only\ntaking place after an unexpected system crash), a few redundant writes\nare nothing to worry about3.\nBatching Log Updates\nYou might have noticed that the basic protocol could add a lot of extra\ndisk trafﬁc. For example, imagine we create two ﬁles in a row, called\nfile1 and file2, in the same directory. To create one ﬁle, one has to\nupdate a number of on-disk structures, minimally including: the inode\nbitmap (to allocated a new inode), the newly-created inode of the ﬁle, the\n3Unless you worry about everything, in which case we can’t help you. Stop worrying so\nmuch, it is unhealthy! But now you’re probably worried about over-worrying.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2748,
      "extraction_method": "Direct"
    },
    {
      "page_number": 538,
      "chapter": null,
      "content": "502\nCRASH CONSISTENCY: FSCK AND JOURNALING\ndata block of the parent directory containing the new directory entry, as\nwell as the parent directory inode (which now has a new modiﬁcation\ntime). With journaling, we logically commit all of this information to\nthe journal for each of our two ﬁle creations; because the ﬁles are in the\nsame directory, and let’s assume even have inodes within the same inode\nblock, this means that if we’re not careful, we’ll end up writing these same\nblocks over and over.\nTo remedy this problem, some ﬁle systems do not commit each update\nto disk one at a time (e.g., Linux ext3); rather, one can buffer all updates\ninto a global transaction. In our example above, when the two ﬁles are\ncreated, the ﬁle system just marks the in-memory inode bitmap, inodes\nof the ﬁles, directory data, and directory inode as dirty, and adds them to\nthe list of blocks that form the current transaction. When it is ﬁnally time\nto write these blocks to disk (say, after a timeout of 5 seconds), this single\nglobal transaction is committed containing all of the updates described\nabove. Thus, by buffering updates, a ﬁle system can avoid excessive write\ntrafﬁc to disk in many cases.\nMaking The Log Finite\nWe thus have arrived at a basic protocol for updating ﬁle-system on-disk\nstructures. The ﬁle system buffers updates in memory for some time;\nwhen it is ﬁnally time to write to disk, the ﬁle system ﬁrst carefully writes\nout the details of the transaction to the journal (a.k.a. write-ahead log);\nafter the transaction is complete, the ﬁle system checkpoints those blocks\nto their ﬁnal locations on disk.\nHowever, the log is of a ﬁnite size. If we keep adding transactions to\nit (as in this ﬁgure), it will soon ﬁll. What do you think happens then?\nJournal\nTx1\nTx2\nTx3\nTx4\nTx5\n...\nTwo problems arise when the log becomes full. The ﬁrst is simpler,\nbut less critical: the larger the log, the longer recovery will take, as the\nrecovery process must replay all the transactions within the log (in order)\nto recover. The second is more of an issue: when the log is full (or nearly\nfull), no further transactions can be committed to the disk, thus making\nthe ﬁle system “less than useful” (i.e., useless).\nTo address these problems, journaling ﬁle systems treat the log as a\ncircular data structure, re-using it over and over; this is why the journal is\nsometimes referred to as a circular log. To do so, the ﬁle system must take\naction some time after a checkpoint. Speciﬁcally, once a transaction has\nbeen checkpointed, the ﬁle system should free the space it was occupying\nwithin the journal, allowing the log space to be reused. There are many\nways to achieve this end; for example, you could simply mark the oldest\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2777,
      "extraction_method": "Direct"
    },
    {
      "page_number": 539,
      "chapter": null,
      "content": "CRASH CONSISTENCY: FSCK AND JOURNALING\n503\nand newest transactions in the log in a journal superblock; all other space\nis free. Here is a graphical depiction of such a mechanism:\nJournal\nJournal\nSuper\nTx1\nTx2\nTx3\nTx4\nTx5\n...\nIn the journal superblock (not to be confused with the main ﬁle system\nsuperblock), the journaling system records enough information to know\nwhich transactions have not yet been checkpointed, and thus reduces re-\ncovery time as well as enables re-use of the log in a circular fashion. And\nthus we add another step to our basic protocol:\n1. Journal write: Write the contents of the transaction (containing TxB\nand the contents of the update) to the log; wait for these writes to\ncomplete.\n2. Journal commit: Write the transaction commit block (containing\nTxE) to the log; wait for the write to complete; the transaction is\nnow committed.\n3. Checkpoint: Write the contents of the update to their ﬁnal locations\nwithin the ﬁle system.\n4. Free: Some time later, mark the transaction free in the journal by\nupdating the journal superblock.\nThus we have our ﬁnal data journaling protocol. But there is still a\nproblem: we are writing each data block to the disk twice, which is a\nheavy cost to pay, especially for something as rare as a system crash. Can\nyou ﬁgure out a way to retain consistency without writing data twice?\nMetadata Journaling\nAlthough recovery is now fast (scanning the journal and replaying a few\ntransactions as opposed to scanning the entire disk), normal operation\nof the ﬁle system is slower than we might desire. In particular, for each\nwrite to disk, we are now also writing to the journal ﬁrst, thus doubling\nwrite trafﬁc; this doubling is especially painful during sequential write\nworkloads, which now will proceed at half the peak write bandwidth of\nthe drive. Further, between writes to the journal and writes to the main\nﬁle system, there is a costly seek, which adds noticeable overhead for\nsome workloads.\nBecause of the high cost of writing every data block to disk twice, peo-\nple have tried a few different things in order to speed up performance.\nFor example, the mode of journaling we described above is often called\ndata journaling (as in Linux ext3), as it journals all user data (in addition\nto the metadata of the ﬁle system). A simpler (and more common) form\nof journaling is sometimes called ordered journaling (or just metadata\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2434,
      "extraction_method": "Direct"
    },
    {
      "page_number": 540,
      "chapter": null,
      "content": "504\nCRASH CONSISTENCY: FSCK AND JOURNALING\njournaling), and it is nearly the same, except that user data is not writ-\nten to the journal. Thus, when performing the same update as above, the\nfollowing information would be written to the journal:\nJournal\nTxB\nI[v2]\nB[v2]\nTxE\nThe data block Db, previously written to the log, would instead be\nwritten to the ﬁle system proper, avoiding the extra write; given that most\nI/O trafﬁc to the disk is data, not writing data twice substantially reduces\nthe I/O load of journaling. The modiﬁcation does raise an interesting\nquestion, though: when should we write data blocks to disk?\nLet’s again consider our example append of a ﬁle to understand the\nproblem better. The update consists of three blocks: I[v2], B[v2], and\nDb. The ﬁrst two are both metadata and will be logged and then check-\npointed; the latter will only be written once to the ﬁle system. When\nshould we write Db to disk? Does it matter?\nAs it turns out, the ordering of the data write does matter for metadata-\nonly journaling. For example, what if we write Db to disk after the trans-\naction (containing I[v2] and B[v2]) completes? Unfortunately, this ap-\nproach has a problem: the ﬁle system is consistent but I[v2] may end up\npointing to garbage data. Speciﬁcally, consider the case where I[v2] and\nB[v2] are written but Db did not make it to disk. The ﬁle system will then\ntry to recover. Because Db is not in the log, the ﬁle system will replay\nwrites to I[v2] and B[v2], and produce a consistent ﬁle system (from the\nperspective of ﬁle-system metadata). However, I[v2] will be pointing to\ngarbage data, i.e., at whatever was in the the slot where Db was headed.\nTo ensure this situation does not arise, some ﬁle systems (e.g., Linux\next3) write data blocks (of regular ﬁles) to the disk ﬁrst, before related\nmetadata is written to disk. Speciﬁcally, the protocol is as follows:\n1. Data write: Write data to ﬁnal location; wait for completion\n(the wait is optional; see below for details).\n2. Journal metadata write: Write the begin block and metadata to the\nlog; wait for writes to complete.\n3. Journal commit: Write the transaction commit block (containing\nTxE) to the log; wait for the write to complete; the transaction (in-\ncluding data) is now committed.\n4. Checkpoint metadata: Write the contents of the metadata update\nto their ﬁnal locations within the ﬁle system.\n5. Free: Later, mark the transaction free in journal superblock.\nBy forcing the data write ﬁrst, a ﬁle system can guarantee that a pointer\nwill never point to garbage. Indeed, this rule of “write the pointed to ob-\nject before the object with the pointer to it” is at the core of crash consis-\ntency, and is exploited even further by other crash consistency schemes\n[GP94] (see below for details).\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2832,
      "extraction_method": "Direct"
    },
    {
      "page_number": 541,
      "chapter": null,
      "content": "CRASH CONSISTENCY: FSCK AND JOURNALING\n505\nIn most systems, metadata journaling (akin to ordered journaling of\next3) is more popular than full data journaling. For example, Windows\nNTFS and SGI’s XFS both use non-ordered metadata journaling. Linux\next3 gives you the option of choosing either data, ordered, or unordered\nmodes (in unordered mode, data can be written at any time). All of these\nmodes keep metadata consistent; they vary in their semantics for data.\nFinally, note that forcing the data write to complete (Step 1) before\nissuing writes to the journal (Step 2) is not required for correctness, as\nindicated in the protocol above. Speciﬁcally, it would be ﬁne to issue data\nwrites as well as the transaction-begin block and metadata to the journal;\nthe only real requirement is that Steps 1 and 2 complete before the issuing\nof the journal commit block (Step 3).\nTricky Case: Block Reuse\nThere are some interesting corner cases that make journaling more tricky,\nand thus are worth discussing. A number of them revolve around block\nreuse; as Stephen Tweedie (one of the main forces behind ext3) said:\n“What’s the hideous part of the entire system? ... It’s deleting ﬁles.\nEverything to do with delete is hairy. Everything to do with delete...\nyou have nightmares around what happens if blocks get deleted and\nthen reallocated.” [T00]\nThe particular example Tweedie gives is as follows. Suppose you are\nusing some form of metadata journaling (and thus data blocks for ﬁles\nare not journaled). Let’s say you have a directory called foo. The user\nadds an entry to foo (say by creating a ﬁle), and thus the contents of\nfoo (because directories are considered metadata) are written to the log;\nassume the location of the foo directory data is block 1000. The log thus\ncontains something like this:\nJournal\nTxB\nid=1\nI[foo]\nptr:1000\nD[foo]\n[final addr:1000]\nTxE\nid=1\nAt this point, the user deletes everything in the directory as well as the\ndirectory itself, freeing up block 1000 for reuse. Finally, the user creates a\nnew ﬁle (say foobar), which ends up reusing the same block (1000) that\nused to belong to foo. The inode of foobar is committed to disk, as is\nits data; note, however, because metadata journaling is in use, only the\ninode of foobar is committed to the journal; the newly-written data in\nblock 1000 in the ﬁle foobar is not journaled.\nJournal\nTxB\nid=1\nI[foo]\nptr:1000\nD[foo]\n[final addr:1000]\nTxE\nid=1\nTxB\nid=2\nI[foobar]\nptr:1000\nTxE\nid=2\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2503,
      "extraction_method": "Direct"
    },
    {
      "page_number": 542,
      "chapter": null,
      "content": "506\nCRASH CONSISTENCY: FSCK AND JOURNALING\nJournal\nFile System\nTxB\nContents\nTxE\nMetadata\nData\n(metadata)\n(data)\nissue\nissue\nissue\ncomplete\ncomplete\ncomplete\nissue\ncomplete\nissue\nissue\ncomplete\ncomplete\nTable 42.1: Data Journaling Timeline\nNow assume a crash occurs and all of this information is still in the\nlog. During replay, the recovery process simply replays everything in\nthe log, including the write of directory data in block 1000; the replay\nthus overwrites the user data of current ﬁle foobar with old directory\ncontents! Clearly this is not a correct recovery action, and certainly it will\nbe a surprise to the user when reading the ﬁle foobar.\nThere are a number of solutions to this problem. One could, for ex-\nample, never reuse blocks until the delete of said blocks is checkpointed\nout of the journal. What Linux ext3 does instead is to add a new type\nof record to the journal, known as a revoke record. In the case above,\ndeleting the directory would cause a revoke record to be written to the\njournal. When replaying the journal, the system ﬁrst scans for such re-\nvoke records; any such revoked data is never replayed, thus avoiding the\nproblem mentioned above.\nWrapping Up Journaling: A Timeline\nBefore ending our discussion of journaling, we summarize the protocols\nwe have discussed with timelines depicting each of them.\nTable 42.1\nshows the protocol when journaling data as well as metadata, whereas\nTable 42.2 shows the protocol when journaling only metadata.\nIn each table, time increases in the downward direction, and each row\nin the table shows the logical time that a write can be issued or might\ncomplete. For example, in the data journaling protocol (42.1), the writes\nof the transaction begin block (TxB) and the contents of the transaction\ncan logically be issued at the same time, and thus can be completed in\nany order; however, the write to the transaction end block (TxE) must not\nbe issued until said previous writes complete. Similarly, the checkpoint-\ning writes to data and metadata blocks cannot begin until the transaction\nend block has committed. Horizontal dashed lines show where write-\nordering requirements must be obeyed.\nA similar timeline is shown for the metadata journaling protocol. Note\nthat the data write can logically be issued at the same time as the writes\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2366,
      "extraction_method": "Direct"
    },
    {
      "page_number": 543,
      "chapter": null,
      "content": "CRASH CONSISTENCY: FSCK AND JOURNALING\n507\nJournal\nFile System\nTxB\nContents\nTxE\nMetadata\nData\n(metadata)\nissue\nissue\nissue\ncomplete\ncomplete\ncomplete\nissue\ncomplete\nissue\ncomplete\nTable 42.2: Metadata Journaling Timeline\nto the transaction begin and the contents of the journal; however, it must\nbe issued and complete before the transaction end has been issued.\nFinally, note that the time of completion marked for each write in the\ntimelines is arbitrary. In a real system, completion time is determined by\nthe I/O subsystem, which may reorder writes to improve performance.\nThe only guarantees about ordering that we have are those that must\nbe enforced for protocol correctness (and are shown via the horizontal\ndashed lines in the tables).\n42.4\nSolution #3: Other Approaches\nWe’ve thus far described two options in keeping ﬁle system metadata\nconsistent: a lazy approach based on fsck, and a more active approach\nknown as journaling. However, these are not the only two approaches.\nOne such approach, known as Soft Updates [GP94], was introduced by\nGanger and Patt. This approach carefully orders all writes to the ﬁle sys-\ntem to ensure that the on-disk structures are never left in an inconsis-\ntent state. For example, by writing a pointed-to data block to disk before\nthe inode that points to it, we can ensure that the inode never points to\ngarbage; similar rules can be derived for all the structures of the ﬁle sys-\ntem. Implementing Soft Updates can be a challenge, however; whereas\nthe journaling layer described above can be implemented with relatively\nlittle knowledge of the exact ﬁle system structures, Soft Updates requires\nintricate knowledge of each ﬁle system data structure and thus adds a fair\namount of complexity to the system.\nAnother approach is known as copy-on-write (yes, COW), and is used\nin a number of popular ﬁle systems, including Sun’s ZFS [B07]. This tech-\nnique never overwrites ﬁles or directories in place; rather, it places new\nupdates to previously unused locations on disk. After a number of up-\ndates are completed, COW ﬁle systems ﬂip the root structure of the ﬁle\nsystem to include pointers to the newly updated structures. Doing so\nmakes keeping the ﬁle system consistent straightforward. We’ll be learn-\ning more about this technique when we discuss the log-structured ﬁle\nsystem (LFS) in a future chapter; LFS is an early example of a COW.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2431,
      "extraction_method": "Direct"
    },
    {
      "page_number": 544,
      "chapter": null,
      "content": "508\nCRASH CONSISTENCY: FSCK AND JOURNALING\nAnother approach is one we just developed here at Wisconsin. In this\ntechnique, entitled backpointer-based consistency (or BBC), no ordering\nis enforced between writes. To achieve consistency, an additional back\npointer is added to every block in the system; for example, each data\nblock has a reference to the inode to which it belongs. When accessing\na ﬁle, the ﬁle system can determine if the ﬁle is consistent by checking if\nthe forward pointer (e.g., the address in the inode or direct block) points\nto a block that refers back to it. If so, everything must have safely reached\ndisk and thus the ﬁle is consistent; if not, the ﬁle is inconsistent, and an\nerror is returned. By adding back pointers to the ﬁle system, a new form\nof lazy crash consistency can be attained [C+12].\nFinally, we also have explored techniques to reduce the number of\ntimes a journal protocol has to wait for disk writes to complete. Entitled\noptimistic crash consistency [C+13], this new approach issues as many\nwrites to disk as possible and uses a generalized form of the transaction\nchecksum [P+05], as well as a few other techniques, to detect inconsisten-\ncies should they arise. For some workloads, these optimistic techniques\ncan improve performance by an order of magnitude. However, to truly\nfunction well, a slightly different disk interface is required [C+13].\n42.5\nSummary\nWe have introduced the problem of crash consistency, and discussed\nvarious approaches to attacking this problem. The older approach of\nbuilding a ﬁle system checker works but is likely too slow to recover on\nmodern systems. Thus, many ﬁle systems now use journaling. Journaling\nreduces recovery time from O(size-of-the-disk-volume) to O(size-of-the-\nlog), thus speeding recovery substantially after a crash and restart. For\nthis reason, many modern ﬁle systems use journaling. We have also seen\nthat journaling can come in many different forms; the most commonly\nused is ordered metadata journaling, which reduces the amount of trafﬁc\nto the journal while still preserving reasonable consistency guarantees for\nboth ﬁle system metadata as well as user data.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2215,
      "extraction_method": "Direct"
    },
    {
      "page_number": 545,
      "chapter": null,
      "content": "CRASH CONSISTENCY: FSCK AND JOURNALING\n509\nReferences\n[B07] “ZFS: The Last Word in File Systems”\nJeff Bonwick and Bill Moore\nAvailable: http://opensolaris.org/os/community/zfs/docs/zfs last.pdf\nZFS uses copy-on-write and journaling, actually, as in some cases, logging writes to disk will perform\nbetter.\n[C+12] “Consistency Without Ordering”\nVijay Chidambaram, Tushar Sharma, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau\nFAST ’12, San Jose, California\nA recent paper of ours about a new form of crash consistency based on back pointers. Read it for the\nexciting details!\n[C+13] “Optimistic Crash Consistency”\nVijay Chidambaram, Thanu S. Pillai, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau\nSOSP ’13, Nemacolin Woodlands Resort, PA, November 2013\nOur work on a more optimistic and higher performance journaling protocol. For workloads that call\nfsync() a lot, performance can be greatly improved.\n[GP94] “Metadata Update Performance in File Systems”\nGregory R. Ganger and Yale N. Patt\nOSDI ’94\nA clever paper about using careful ordering of writes as the main way to achieve consistency. Imple-\nmented later in BSD-based systems.\n[G+08] “SQCK: A Declarative File System Checker”\nHaryadi S. Gunawi, Abhishek Rajimwale, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau\nOSDI ’08, San Diego, California\nOur own paper on a new and better way to build a ﬁle system checker using SQL queries. We also show\nsome problems with the existing checker, ﬁnding numerous bugs and odd behaviors, a direct result of\nthe complexity of fsck.\n[H87] “Reimplementing the Cedar File System Using Logging and Group Commit”\nRobert Hagmann\nSOSP ’87, Austin, Texas, November 1987\nThe ﬁrst work (that we know of) that applied write-ahead logging (a.k.a. journaling) to a ﬁle system.\n[M+13] “ffsck: The Fast File System Checker”\nAo Ma, Chris Dragga, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau\nFAST ’13, San Jose, California, February 2013\nA recent paper of ours detailing how to make fsck an order of magnitude faster. Some of the ideas have\nalready been incorporated into the BSD ﬁle system checker [MK96] and are deployed today.\n[MK96] “Fsck - The UNIX File System Check Program”\nMarshall Kirk McKusick and T. J. Kowalski\nRevised in 1996\nDescribes the ﬁrst comprehensive ﬁle-system checking tool, the eponymous fsck. Written by some of\nthe same people who brought you FFS.\n[MJLF84] “A Fast File System for UNIX”\nMarshall K. McKusick, William N. Joy, Sam J. Lefﬂer, Robert S. Fabry\nACM Transactions on Computing Systems.\nAugust 1984, Volume 2:3\nYou already know enough about FFS, right? But yeah, it is OK to reference papers like this more than\nonce in a book.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2693,
      "extraction_method": "Direct"
    },
    {
      "page_number": 546,
      "chapter": null,
      "content": "510\nCRASH CONSISTENCY: FSCK AND JOURNALING\n[P+05] “IRON File Systems”\nVijayan Prabhakaran, Lakshmi N. Bairavasundaram, Nitin Agrawal, Haryadi S. Gunawi, An-\ndrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau\nSOSP ’05, Brighton, England, October 2005\nA paper mostly focused on studying how ﬁle systems react to disk failures. Towards the end, we intro-\nduce a transaction checksum to speed up logging, which was eventually adopted into Linux ext4.\n[PAA05] “Analysis and Evolution of Journaling File Systems”\nVijayan Prabhakaran, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau\nUSENIX ’05, Anaheim, California, April 2005\nAn early paper we wrote analyzing how journaling ﬁle systems work.\n[R+11] “Coerced Cache Eviction and Discreet-Mode Journaling”\nAbhishek Rajimwale, Vijay Chidambaram, Deepak Ramamurthi,\nAndrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau\nDSN ’11, Hong Kong, China, June 2011\nOur own paper on the problem of disks that buffer writes in a memory cache instead of forcing them to\ndisk, even when explicitly told not to do that! Our solution to overcome this problem: if you want A to\nbe written to disk before B, ﬁrst write A, then send a lot of “dummy” writes to disk, hopefully causing\nA to be forced to disk to make room for them in the cache. A neat if impractical solution.\n[T98] “Journaling the Linux ext2fs File System”\nStephen C. Tweedie\nThe Fourth Annual Linux Expo, May 1998\nTweedie did much of the heavy lifting in adding journaling to the Linux ext2 ﬁle system; the result,\nnot surprisingly, is called ext3. Some nice design decisions include the strong focus on backwards\ncompatibility, e.g., you can just add a journaling ﬁle to an existing ext2 ﬁle system and then mount it\nas an ext3 ﬁle system.\n[T00] “EXT3, Journaling Filesystem”\nStephen Tweedie\nTalk at the Ottawa Linux Symposium, July 2000\nolstrans.sourceforge.net/release/OLS2000-ext3/OLS2000-ext3.html\nA transcript of a talk given by Tweedie on ext3.\n[T01] “The Linux ext2 File System”\nTheodore Ts’o, June, 2001.\nAvailable: http://e2fsprogs.sourceforge.net/ext2.html\nA simple Linux ﬁle system based on the ideas found in FFS. For a while it was quite heavily used; now\nit is really just in the kernel as an example of a simple ﬁle system.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2272,
      "extraction_method": "Direct"
    },
    {
      "page_number": 547,
      "chapter": null,
      "content": "43\nLog-structured File Systems\nIn the early 90’s, a group at Berkeley led by Professor John Ousterhout\nand graduate student Mendel Rosenblum developed a new ﬁle system\nknown as the log-structured ﬁle system [RO91]. Their motivation to do\nso was based on the following observations:\n• Memory sizes were growing: As memory got bigger, more data\ncould be cached in memory. As more data is cached, disk trafﬁc\nwould increasingly consist of writes, as reads would be serviced in\nthe cache. Thus, ﬁle system performance would largely be deter-\nmined by its performance for writes.\n• There was a large and growing gap between random I/O perfor-\nmance and sequential I/O performance: Transfer bandwidth in-\ncreases roughly 50%-100% every year; seek and rotational delay\ncosts decrease much more slowly, maybe at 5%-10% per year [P98].\nThus, if one is able to use disks in a sequential manner, one gets a\nhuge performance advantage, which grows over time.\n• Existing ﬁle systems perform poorly on many common workloads:\nFor example, FFS [MJLF84] would perform a large number of writes\nto create a new ﬁle of size one block: one for a new inode, one to\nupdate the inode bitmap, one to the directory data block that the\nﬁle is in, one to the directory inode to update it, one to the new data\nblock that is apart of the new ﬁle, and one to the data bitmap to\nmark the data block as allocated. Thus, although FFS would place\nall of these blocks within the same block group, FFS would incur\nmany short seeks and subsequent rotational delays and thus per-\nformance would fall far short of peak sequential bandwidth.\n• File systems were not RAID-aware: For example, RAID-4 and RAID-\n5 have the small-write problem where a logical write to a single\nblock causes 4 physical I/Os to take place. Existing ﬁle systems do\nnot try to avoid this worst-case RAID writing behavior.\nAn ideal ﬁle system would thus focus on write performance, and try\nto make use of the sequential bandwidth of the disk. Further, it would\nperform well on common workloads that not only write out data but also\n511\n",
      "content_length": 2069,
      "extraction_method": "Direct"
    },
    {
      "page_number": 548,
      "chapter": null,
      "content": "512\nLOG-STRUCTURED FILE SYSTEMS\nupdate on-disk metadata structures frequently. Finally, it would work\nwell on RAIDs as well as single disks.\nThe new type of ﬁle system Rosenblum and Ousterhout introduced\nwas called LFS, short for the Log-structured File System. When writ-\ning to disk, LFS ﬁrst buffers all updates (including metadata!) in an in-\nmemory segment; when the segment is full, it is written to disk in one\nlong, sequential transfer to an unused part of the disk, i.e., LFS never\noverwrites existing data, but rather always writes segments to free loca-\ntions. Because segments are large, the disk is used efﬁciently, and perfor-\nmance of the ﬁle system approaches its zenith.\nTHE CRUX:\nHOW TO MAKE ALL WRITES SEQUENTIAL WRITES?\nHow can a ﬁle system turns all writes into sequential writes?\nFor\nreads, this task is impossible, as the desired block to be read may be any-\nwhere on disk. For writes, however, the ﬁle system always has a choice,\nand it is exactly this choice we hope to exploit.\n43.1\nWriting To Disk Sequentially\nWe thus have our ﬁrst challenge: how do we transform all updates to\nﬁle-system state into a series of sequential writes to disk? To understand\nthis better, let’s use a simple example. Imagine we are writing a data block\nD to a ﬁle. Writing the data block to disk might result in the following\non-disk layout, with D written at disk address A0:\nD\nA0\nHowever, when a user writes a data block, it is not only data that gets\nwritten to disk; there is also other metadata that needs to be updated.\nIn this case, let’s also write the inode (I) of the ﬁle to disk, and have it\npoint to the data block D. When written to disk, the data block and inode\nwould look something like this (note that the inode looks as big as the\ndata block, which generally isn’t the case; in most systems, data blocks\nare 4 KB in size, whereas an inode is much smaller, around 128 bytes):\nD\nA0\nI\nblk[0]:A0\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1962,
      "extraction_method": "Direct"
    },
    {
      "page_number": 549,
      "chapter": null,
      "content": "LOG-STRUCTURED FILE SYSTEMS\n513\nTIP: DETAILS MATTER\nAll interesting systems are comprised of a few general ideas and a\nnumber of details. Sometimes, when you are learning about these sys-\ntems, you think to yourself “Oh, I get the general idea; the rest is just de-\ntails,” and you use this to only half-learn how things really work. Don’t\ndo this! Many times, the details are critical. As we’ll see with LFS, the\ngeneral idea is easy to understand, but to really build a working system,\nyou have to think through all of the tricky cases.\nThis basic idea, of simply writing all updates (such as data blocks,\ninodes, etc.) to the disk sequentially, sits at the heart of LFS. If you un-\nderstand this, you get the basic idea. But as with all complicated systems,\nthe devil is in the details.\n43.2\nWriting Sequentially And Effectively\nUnfortunately, writing to disk sequentially is not (alone) enough to\nguarantee efﬁcient writes. For example, imagine if we wrote a single\nblock to address A, at time T. We then wait a little while, and write to\nthe disk at address A + 1 (the next block address in sequential order),\nbut at time T + δ. In-between the ﬁrst and second writes, unfortunately,\nthe disk has rotated; when you issue the second write, it will thus wait\nfor most of a rotation before being committed (speciﬁcally, if the rotation\ntakes time Trotation, the disk will wait Trotation −δ before it can commit\nthe second write to the disk surface). And thus you can hopefully see\nthat simply writing to disk in sequential order is not enough to achieve\npeak performance; rather, you must issue a large number of contiguous\nwrites (or one large write) to the drive in order to achieve good write\nperformance.\nTo achieve this end, LFS uses an ancient technique known as write\nbuffering1. Before writing to the disk, LFS keeps track of updates in\nmemory; when it has received a sufﬁcient number of updates, it writes\nthem to disk all at once, thus ensuring efﬁcient use of the disk.\nThe large chunk of updates LFS writes at one time is referred to by\nthe name of a segment. Although this term is over-used in computer\nsystems, here it just means a large-ish chunk which LFS uses to group\nwrites. Thus, when writing to disk, LFS buffers updates in an in-memory\nsegment, and then writes the segment all at once to the disk. As long as\nthe segment is large enough, these writes will be efﬁcient.\nHere is an example, in which LFS buffers two sets updates into a small\nsegment; actual segments are larger (a few MB). The ﬁrst update is of\n1Indeed, it is hard to ﬁnd a good citation for this idea, since it was likely invented by many\nand very early on in the history of computing. For a study of the beneﬁts of write buffering,\nsee Solworth and Orji [SO90]; to learn about its potential harms, see Mogul [M94].\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2846,
      "extraction_method": "Direct"
    },
    {
      "page_number": 550,
      "chapter": null,
      "content": "514\nLOG-STRUCTURED FILE SYSTEMS\nfour block writes to ﬁle j; the second is one block being added to ﬁle k.\nLFS then commits the entire segment of seven blocks to disk at once. The\nresulting on-disk layout of these blocks is as follows:\nD[j,0]\nA0\nD[j,1]\nA1\nD[j,2]\nA2\nD[j,3]\nA3\nblk[0]:A0\nblk[1]:A1\nblk[2]:A2\nblk[3]:A3\nInode[j]\nD[k,0]\nA5\nblk[0]:A5\nInode[k]\n43.3\nHow Much To Buffer?\nThis raises the following question: how many updates LFS should\nbuffer before writing to disk? The answer, of course, depends on the disk\nitself, speciﬁcally how high the positioning overhead is in comparison to\nthe transfer rate; see the FFS chapter for a similar analysis.\nFor example, assume that positioning (i.e., rotation and seek over-\nheads) before each write takes roughly Tposition seconds. Assume further\nthat the disk transfer rate is Rpeak MB/s. How much should LFS buffer\nbefore writing when running on such a disk?\nThe way to think about this is that every time you write, you pay a\nﬁxed overhead of the positioning cost. Thus, how much do you have\nto write in order to amortize that cost? The more you write, the better\n(obviously), and the closer you get to achieving peak bandwidth.\nTo obtain a concrete answer, let’s assume we are writing out D MB.\nThe time to write out this chunk of data (Twrite) is the positioning time\nTposition plus the time to transfer D (\nD\nRpeak ), or:\nTwrite = Tposition +\nD\nRpeak\n(43.1)\nAnd thus the effective rate of writing (Reffective), which is just the\namount of data written divided by the total time to write it, is:\nReffective =\nD\nTwrite =\nD\nTposition +\nD\nRpeak\n.\n(43.2)\nWhat we’re interested in is getting the effective rate (Reffective) close\nto the peak rate. Speciﬁcally, we want the effective rate to be some fraction\nF of the peak rate, where 0 < F < 1 (a typical F might be 0.9, or 90% of\nthe peak rate). In mathematical form, this means we want Reffective =\nF × Rpeak.\nAt this point, we can solve for D:\nReffective =\nD\nTposition +\nD\nRpeak\n= F × Rpeak\n(43.3)\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2045,
      "extraction_method": "Direct"
    },
    {
      "page_number": 551,
      "chapter": null,
      "content": "LOG-STRUCTURED FILE SYSTEMS\n515\nD = F × Rpeak × (Tposition +\nD\nRpeak\n)\n(43.4)\nD = (F × Rpeak × Tposition) + (F × Rpeak ×\nD\nRpeak\n)\n(43.5)\nD =\nF\n1 −F × Rpeak × Tposition\n(43.6)\nLet’s do an example, with a disk with a positioning time of 10 mil-\nliseconds and peak transfer rate of 100 MB/s; assume we want an ef-\nfective bandwidth of 90% of peak (F = 0.9). In this case, D =\n0.9\n0.1 ×\n100 MB/s × 0.01 seconds = 9 MB. Try some different values to see\nhow much we need to buffer in order to approach peak bandwidth. How\nmuch is needed to reach 95% of peak? 99%?\n43.4\nProblem: Finding Inodes\nTo understand how we ﬁnd an inode in LFS, let us brieﬂy review how\nto ﬁnd an inode in a typical UNIX ﬁle system. In a typical ﬁle system such\nas FFS, or even the old UNIX ﬁle system, ﬁnding inodes is easy, because\nthey are organized in an array and placed on disk at ﬁxed locations.\nFor example, the old UNIX ﬁle system keeps all inodes at a ﬁxed por-\ntion of the disk. Thus, given an inode number and the start address, to\nﬁnd a particular inode, you can calculate its exact disk address simply by\nmultiplying the inode number by the size of an inode, and adding that\nto the start address of the on-disk array; array-based indexing, given an\ninode number, is fast and straightforward.\nFinding an inode given an inode number in FFS is only slightly more\ncomplicated, because FFS splits up the inode table into chunks and places\na group of inodes within each cylinder group. Thus, one must know how\nbig each chunk of inodes is and the start addresses of each. After that, the\ncalculations are similar and also easy.\nIn LFS, life is more difﬁcult. Why? Well, we’ve managed to scatter the\ninodes all throughout the disk! Worse, we never overwrite in place, and\nthus the latest version of an inode (i.e., the one we want) keeps moving.\n43.5\nSolution Through Indirection: The Inode Map\nTo remedy this, the designers of LFS introduced a level of indirection\nbetween inode numbers and the inodes through a data structure called\nthe inode map (imap). The imap is a structure that takes an inode number\nas input and produces the disk address of the most recent version of the\ninode. Thus, you can imagine it would often be implemented as a simple\narray, with 4 bytes (a disk pointer) per entry. Any time an inode is written\nto disk, the imap is updated with its new location.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2396,
      "extraction_method": "Direct"
    },
    {
      "page_number": 552,
      "chapter": null,
      "content": "516\nLOG-STRUCTURED FILE SYSTEMS\nTIP: USE A LEVEL OF INDIRECTION\nPeople often say that the solution to all problems in Computer Science\nis simply a level of indirection. This is clearly not true; it is just the\nsolution to most problems. You certainly can think of every virtualization\nwe have studied, e.g., virtual memory, as simply a level of indirection.\nAnd certainly the inode map in LFS is a virtualization of inode numbers.\nHopefully you can see the great power of indirection in these examples,\nallowing us to freely move structures around (such as pages in the VM\nexample, or inodes in LFS) without having to change every reference to\nthem. Of course, indirection can have a downside too: extra overhead. So\nnext time you have a problem, try solving it with indirection. But make\nsure to think about the overheads of doing so ﬁrst.\nThe imap, unfortunately, needs to be kept persistent (i.e., written to\ndisk); doing so allows LFS to keep track of the locations of inodes across\ncrashes, and thus operate as desired. Thus, a question: where should the\nimap reside on disk?\nIt could live on a ﬁxed part of the disk, of course. Unfortunately, as it\ngets updated frequently, this would then require updates to ﬁle structures\nto be followed by writes to the imap, and hence performance would suffer\n(i.e., there would be more disk seeks, between each update and the ﬁxed\nlocation of the imap).\nInstead, LFS places chunks of the inode map right next to where it is\nwriting all of the other new information. Thus, when appending a data\nblock to a ﬁle k, LFS actually writes the new data block, its inode, and a\npiece of the inode map all together onto the disk, as follows:\nD\nA0\nI[k]\nblk[0]:A0\nA1\nimap\nmap[k]:A1\nIn this picture, the piece of the imap array stored in the block marked\nimap tells LFS that the inode k is at disk address A1; this inode, in turn,\ntells LFS that its data block D is at address A0.\n43.6\nThe Checkpoint Region\nThe clever reader (that’s you, right?) might have noticed a problem\nhere. How do we ﬁnd the inode map, now that pieces of it are also now\nspread across the disk? In the end, there is no magic: the ﬁle system must\nhave some ﬁxed and known location on disk to begin a ﬁle lookup.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2264,
      "extraction_method": "Direct"
    },
    {
      "page_number": 553,
      "chapter": null,
      "content": "LOG-STRUCTURED FILE SYSTEMS\n517\nLFS has just such a ﬁxed place on disk for this, known as the check-\npoint region (CR). The checkpoint region contains pointers to (i.e., ad-\ndresses of) the latest pieces of the inode map, and thus the inode map\npieces can be found by reading the CR ﬁrst. Note the checkpoint region\nis only updated periodically (say every 30 seconds or so), and thus perfor-\nmance is not ill-affected. Thus, the overall structure of the on-disk layout\ncontains a checkpoint region (which points to the latest pieces of the in-\node map); the inode map pieces each contain addresses of the inodes; the\ninodes point to ﬁles (and directories) just like typical UNIX ﬁle systems.\nHere is an example of the checkpoint region (note it is all the way at\nthe beginning of the disk, at address 0), and a single imap chunk, inode,\nand data block. A real ﬁle system would of course have a much bigger\nCR (indeed, it would have two, as we’ll come to understand later), many\nimap chunks, and of course many more inodes, data blocks, etc.\nimap\n[k...k+N]:\nA2\nCR\n0\nD\nA0\nI[k]\nblk[0]:A0\nA1\nimap\nmap[k]:A1\nA2\n43.7\nReading A File From Disk: A Recap\nTo make sure you understand how LFS works, let us now walk through\nwhat must happen to read a ﬁle from disk. Assume we have nothing in\nmemory to begin. The ﬁrst on-disk data structure we must read is the\ncheckpoint region. The checkpoint region contains pointers (i.e., disk ad-\ndresses) to the entire inode map, and thus LFS then reads in the entire in-\node map and caches it in memory. After this point, when given an inode\nnumber of a ﬁle, LFS simply looks up the inode-number to inode-disk-\naddress mapping in the imap, and reads in the most recent version of the\ninode. To read a block from the ﬁle, at this point, LFS proceeds exactly\nas a typical UNIX ﬁle system, by using direct pointers or indirect pointers\nor doubly-indirect pointers as need be. In the common case, LFS should\nperform the same number of I/Os as a typical ﬁle system when reading a\nﬁle from disk; the entire imap is cached and thus the extra work LFS does\nduring a read is to look up the inode’s address in the imap.\n43.8\nWhat About Directories?\nThus far, we’ve simpliﬁed our discussion a bit by only considering in-\nodes and data blocks. However, to access a ﬁle in a ﬁle system (such as\n/home/remzi/foo, one of our favorite fake ﬁle names), some directo-\nries must be accessed too. So how does LFS store directory data?\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2485,
      "extraction_method": "Direct"
    },
    {
      "page_number": 554,
      "chapter": null,
      "content": "518\nLOG-STRUCTURED FILE SYSTEMS\nFortunately, directory structure is basically identical to classic UNIX\nﬁle systems, in that a directory is just a collection of (name, inode number)\nmappings. For example, when creating a ﬁle on disk, LFS must both write\na new inode, some data, as well as the directory data and its inode that\nrefer to this ﬁle. Remember that LFS will do so sequentially on the disk\n(after buffering the updates for some time). Thus, creating a ﬁle foo in a\ndirectory would lead to the following new structures on disk:\nD[k]\nA0\nI[k]\nblk[0]:A0\nA1\n(foo, k)\nD[dir]\nA2\nI[dir]\nblk[0]:A2\nA3\nmap[k]:A1\nmap[dir]:A3\nimap\nThe piece of the inode map contains the information for the location of\nboth the directory ﬁle dir as well as the newly-created ﬁle f. Thus, when\naccessing ﬁle foo (with inode number f), you would ﬁrst look in the\ninode map (usually cached in memory) to ﬁnd the location of the inode\nof directory dir (A3); you then read the directory inode, which gives you\nthe location of the directory data (A2); reading this data block gives you\nthe name-to-inode-number mapping of (foo, k). You then consult the\ninode map again to ﬁnd the location of inode number k (A1), and ﬁnally\nread the desired data block at address A0.\nThere is one other serious problem in LFS that the inode map solves,\nknown as the recursive update problem [Z+12].\nThe problem arises\nin any ﬁle system that never updates in place (such as LFS), but rather\nmoves updates to new locations on the disk.\nSpeciﬁcally, whenever an inode is updated, its location on disk changes.\nIf we hadn’t been careful, this would have also entailed an update to\nthe directory that points to this ﬁle, which then would have mandated\na change to the parent of that directory, and so on, all the way up the ﬁle\nsystem tree.\nLFS cleverly avoids this problem with the inode map. Even though\nthe location of an inode may change, the change is never reﬂected in the\ndirectory itself; rather, the imap structure is updated while the directory\nholds the same name-to-inumber mapping. Thus, through indirection,\nLFS avoids the recursive update problem.\n43.9\nA New Problem: Garbage Collection\nYou may have noticed another problem with LFS; it keeps writing\nnewer version of a ﬁle, its inode, and in fact all data to new parts of the\ndisk. This process, while keeping writes efﬁcient, implies that LFS leaves\nolder versions of ﬁle structures all over the disk, scattered throughout the\ndisk. We call such old stuff garbage.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2533,
      "extraction_method": "Direct"
    },
    {
      "page_number": 555,
      "chapter": null,
      "content": "LOG-STRUCTURED FILE SYSTEMS\n519\nFor example, let’s imagine the case where we have an existing ﬁle re-\nferred to by inode number k, which points to a single data block D0. We\nnow overwrite that block, generating both a new inode and a new data\nblock. The resulting on-disk layout of LFS would look something like this\n(note we omit the imap and other structures for simplicity; a new chunk\nof imap would also have to be written to disk to point to the new inode):\nD0\nA0\nI[k]\nblk[0]:A0\n(both garbage)\nD0\nA4\nI[k]\nblk[0]:A4\nIn the diagram, you can see that both the inode and data block have\ntwo versions on disk, one old (the one on the left) and one current and\nthus live (the one on the right). By the simple act of overwriting a data\nblock, a number of new structures must be persisted by LFS, thus leaving\nold versions of said blocks on the disk.\nAs another example, imagine we instead append a block to that orig-\ninal ﬁle k. In this case, a new version of the inode is generated, but the\nold data block is still pointed to by the inode. Thus, it is still live and very\nmuch apart of the current ﬁle system:\nD0\nA0\nI[k]\nblk[0]:A0\n(garbage)\nD1\nA4\nI[k]\nblk[0]:A0\nblk[1]:A4\nSo what should we do with these older versions of inodes, data blocks,\nand so forth? One could keep those older versions around and allow\nusers to restore old ﬁle versions (for example, when they accidentally\noverwrite or delete a ﬁle, it could be quite handy to do so); such a ﬁle\nsystem is known as a versioning ﬁle system because it keeps track of the\ndifferent versions of a ﬁle.\nHowever, LFS instead keeps only the latest live version of a ﬁle; thus\n(in the background), LFS must periodically ﬁnd these old dead versions\nof ﬁle data, inodes, and other structures, and clean them; cleaning should\nthus make blocks on disk free again for use in a subsequent writes. Note\nthat the process of cleaning is a form of garbage collection, a technique\nthat arises in programming languages that automatically free unused mem-\nory for programs.\nEarlier we discussed segments as important as they are the mechanism\nthat enables large writes to disk in LFS. As it turns out, they are also quite\nintegral to effective cleaning. Imagine what would happen if the LFS\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2269,
      "extraction_method": "Direct"
    },
    {
      "page_number": 556,
      "chapter": null,
      "content": "520\nLOG-STRUCTURED FILE SYSTEMS\ncleaner simply went through and freed single data blocks, inodes, etc.,\nduring cleaning. The result: a ﬁle system with some number of free holes\nmixed between allocated space on disk. Write performance would drop\nconsiderably, as LFS would not be able to ﬁnd a large contiguous region\nto write to disk sequentially and with high performance.\nInstead, the LFS cleaner works on a segment-by-segment basis, thus\nclearing up large chunks of space for subsequent writing. The basic clean-\ning process works as follows. Periodically, the LFS cleaner reads in a\nnumber of old (partially-used) segments, determines which blocks are\nlive within these segments, and then write out a new set of segments\nwith just the live blocks within them, freeing up the old ones for writing.\nSpeciﬁcally, we expect the cleaner to read in M existing segments, com-\npact their contents into N new segments (where N < M), and then write\nthe N segments to disk in new locations. The old M segments are then\nfreed and can be used by the ﬁle system for subsequent writes.\nWe are now left with two problems, however. The ﬁrst is mechanism:\nhow can LFS tell which blocks within a segment are live, and which are\ndead? The second is policy: how often should the cleaner run, and which\nsegments should it pick to clean?\n43.10\nDetermining Block Liveness\nWe address the mechanism ﬁrst. Given a data block D within an on-\ndisk segment S, LFS must be able to determine whether D is live. To do\nso, LFS adds a little extra information to each segment that describes each\nblock. Speciﬁcally, LFS includes, for each data block D, its inode number\n(which ﬁle it belongs to) and its offset (which block of the ﬁle this is). This\ninformation is recorded in a structure at the head of the segment known\nas the segment summary block.\nGiven this information, it is straightforward to determine whether a\nblock is live or dead. For a block D located on disk at address A, look\nin the segment summary block and ﬁnd its inode number N and offset\nT. Next, look in the imap to ﬁnd where N lives and read N from disk\n(perhaps it is already in memory, which is even better). Finally, using\nthe offset T, look in the inode (or some indirect block) to see where the\ninode thinks the Tth block of this ﬁle is on disk. If it points exactly to disk\naddress A, LFS can conclude that the block D is live. If it points anywhere\nelse, LFS can conclude that D is not in use (i.e., it is dead) and thus know\nthat this version is no longer needed. A pseudocode summary of this\nprocess is shown here:\n(N, T) = SegmentSummary[A];\ninode\n= Read(imap[N]);\nif (inode[T] == A)\n// block D is alive\nelse\n// block D is garbage\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2731,
      "extraction_method": "Direct"
    },
    {
      "page_number": 557,
      "chapter": null,
      "content": "LOG-STRUCTURED FILE SYSTEMS\n521\nHere is a diagram depicting the mechanism, in which the segment\nsummary block (marked SS) records that the data block at address A0\nis actually a part of ﬁle k at offset 0. By checking the imap for k, you can\nﬁnd the inode, and see that it does indeed point to that location.\nD\nA0\nI[k]\nblk[0]:A0\nA1\nimap\nmap[k]:A1\nss\nA0:\n(k,0)\nThere are some shortcuts LFS takes to make the process of determining\nliveness more efﬁcient. For example, when a ﬁle is truncated or deleted,\nLFS increases its version number and records the new version number in\nthe imap. By also recording the version number in the on-disk segment,\nLFS can short circuit the longer check described above simply by compar-\ning the on-disk version number with a version number in the imap, thus\navoiding extra reads.\n43.11\nA Policy Question: Which Blocks To Clean, And When?\nOn top of the mechanism described above, LFS must include a set of\npolicies to determine both when to clean and which blocks are worth\ncleaning. Determining when to clean is easier; either periodically, dur-\ning idle time, or when you have to because the disk is full.\nDetermining which blocks to clean is more challenging, and has been\nthe subject of many research papers. In the original LFS paper [RO91],\nthe authors describe an approach which tries to segregate hot and cold\nsegment. A hot segment is one in which the contents are being frequently\nover-written; thus, for such a segment, the best policy is to wait a long\ntime before cleaning it, as more and more blocks are getting over-written\n(in new segments) and thus being freed for use. A cold segment, in con-\ntrast, may have a few dead blocks but the rest of its contents are relatively\nstable. Thus, the authors conclude that one should clean cold segments\nsooner and hot segments later, and develop a heuristic that does exactly\nthat. However, as with most policies, this is just one approach, and by\ndeﬁnition is not “the best” approach; later approaches show how to do\nbetter [MR+97].\n43.12\nCrash Recovery And The Log\nOne ﬁnal problem: what happens if the system crashes while LFS is\nwriting to disk? As you may recall in the previous chapter about jour-\nnaling, crashes during updates are tricky for ﬁle systems, and thus some-\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2305,
      "extraction_method": "Direct"
    },
    {
      "page_number": 558,
      "chapter": null,
      "content": "522\nLOG-STRUCTURED FILE SYSTEMS\nthing LFS must consider as well.\nDuring normal operation, LFS buffers writes in a segment, and then\n(when the segment is full, or when some amount of time has elapsed),\nwrites the segment to disk. LFS organizes these writes in a log, i.e., the\ncheckpoint region points to a head and tail segment, and each segment\npoints to the next segment to be written. LFS also periodically updates the\ncheckpoint region. Crashes could clearly happen during either of these\noperations (write to a segment, write to the CR). So how does LFS handle\ncrashes during writes to these structures?\nLet’s cover the second case ﬁrst. To ensure that the CR update happens\natomically, LFS actually keeps two CRs, one at either end of the disk, and\nwrites to them alternately. LFS also implements a careful protocol when\nupdating the CR with the latest pointers to the inode map and other infor-\nmation; speciﬁcally, it ﬁrst writes out a header (with timestamp), then the\nbody of the CR, and then ﬁnally one last block (also with a timestamp). If\nthe system crashes during a CR update, LFS can detect this by seeing an\ninconsistent pair of timestamps. LFS will always choose to use the most\nrecent CR that has consistent timestamps, and thus consistent update of\nthe CR is achieved.\nLet’s now address the ﬁrst case. Because LFS writes the CR every 30\nseconds or so, the last consistent snapshot of the ﬁle system may be quite\nold. Thus, upon reboot, LFS can easily recover by simply reading in the\ncheckpoint region, the imap pieces it points to, and subsequent ﬁles and\ndirectories; however, the last many seconds of updates would be lost.\nTo improve upon this, LFS tries to rebuild many of those segments\nthrough a technique known as roll forward in the database community.\nThe basic idea is to start with the last checkpoint region, ﬁnd the end of\nthe log (which is included in the CR), and then use that to read through\nthe next segments and see if there are any valid updates within it. If there\nare, LFS updates the ﬁle system accordingly and thus recovers much of\nthe data and metadata written since the last checkpoint. See Rosenblum’s\naward-winning dissertation for details [R92].\n43.13\nSummary\nLFS introduces a new approach to updating the disk. Instead of over-\nwriting ﬁles in places, LFS always writes to an unused portion of the\ndisk, and then later reclaims that old space through cleaning. This ap-\nproach, which in database systems is called shadow paging [L77] and in\nﬁle-system-speak is sometimes called copy-on-write, enables highly efﬁ-\ncient writing, as LFS can gather all updates into an in-memory segment\nand then write them out together sequentially.\nThe downside to this approach is that it generates garbage; old copies\nof the data are scattered throughout the disk, and if one wants to reclaim\nsuch space for subsequent usage, one must clean old segments periodi-\ncally. Cleaning became the focus of much controversy in LFS, and con-\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 3015,
      "extraction_method": "Direct"
    },
    {
      "page_number": 559,
      "chapter": null,
      "content": "LOG-STRUCTURED FILE SYSTEMS\n523\nTIP: TURN FLAWS INTO VIRTUES\nWhenever your system has a fundamental ﬂaw, see if you can turn it\naround into a feature or something useful. NetApp’s WAFL does this\nwith old ﬁle contents; by making old versions available, WAFL no longer\nhas to worry about cleaning, and thus provides a cool feature and re-\nmoves the LFS cleaning problem all in one wonderful twist. Are there\nother examples of this in systems? Undoubtedly, but you’ll have to think\nof them yourself, because this chapter is over with a capital “O”. Over.\nDone. Kaput. We’re out. Peace!\ncerns over cleaning costs [SS+95] perhaps limited LFS’s initial impact on\nthe ﬁeld. However, some modern commercial ﬁle systems, including Ne-\ntApp’s WAFL [HLM94], Sun’s ZFS [B07], and Linux btrfs [M07] adopt\na similar copy-on-write approach to writing to disk, and thus the intel-\nlectual legacy of LFS lives on in these modern ﬁle systems. In particular,\nWAFL got around cleaning problems by turning them into a feature; by\nproviding old versions of the ﬁle system via snapshots, users could ac-\ncess old ﬁles whenever they deleted current ones accidentally.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 1185,
      "extraction_method": "Direct"
    },
    {
      "page_number": 560,
      "chapter": null,
      "content": "524\nLOG-STRUCTURED FILE SYSTEMS\nReferences\n[B07] “ZFS: The Last Word in File Systems”\nJeff Bonwick and Bill Moore\nAvailable: http://opensolaris.org/os/community/zfs/docs/zfs last.pdf\nSlides on ZFS; unfortunately, there is no great ZFS paper.\n[HLM94] “File System Design for an NFS File Server Appliance”\nDave Hitz, James Lau, Michael Malcolm\nUSENIX Spring ’94\nWAFL takes many ideas from LFS and RAID and puts it into a high-speed NFS appliance for the\nmulti-billion dollar storage company NetApp.\n[L77] “Physical Integrity in a Large Segmented Database”\nR. Lorie\nACM Transactions on Databases, 1977, Volume 2:1, pages 91-104\nThe original idea of shadow paging is presented here.\n[M07] “The Btrfs Filesystem”\nChris Mason\nSeptember 2007\nAvailable: oss.oracle.com/projects/btrfs/dist/documentation/btrfs-ukuug.pdf\nA recent copy-on-write Linux ﬁle system, slowly gaining in importance and usage.\n[MJLF84] “A Fast File System for UNIX”\nMarshall K. McKusick, William N. Joy, Sam J. Lefﬂer, Robert S. Fabry\nACM TOCS, August, 1984, Volume 2, Number 3\nThe original FFS paper; see the chapter on FFS for more details.\n[MR+97] “Improving the Performance of Log-structured File Systems with Adaptive Meth-\nods” Jeanna Neefe Matthews, Drew Roselli, Adam M. Costello, Randolph Y. Wang, Thomas E.\nAnderson\nSOSP 1997, pages 238-251, October, Saint Malo, France\nA more recent paper detailing better policies for cleaning in LFS.\n[M94] “A Better Update Policy”\nJeffrey C. Mogul\nUSENIX ATC ’94, June 1994\nIn this paper, Mogul ﬁnds that read workloads can be harmed by buffering writes for too long and then\nsending them to the disk in a big burst. Thus, he recommends sending writes more frequently and in\nsmaller batches.\n[P98] “Hardware Technology Trends and Database Opportunities”\nDavid A. Patterson\nACM SIGMOD ’98 Keynote Address, Presented June 3, 1998, Seattle, Washington\nAvailable: http://www.cs.berkeley.edu/˜pattrsn/talks/keynote.html\nA great set of slides on technology trends in computer systems. Hopefully, Patterson will create another\nof these sometime soon.\n[RO91] “Design and Implementation of the Log-structured File System”\nMendel Rosenblum and John Ousterhout\nSOSP ’91, Paciﬁc Grove, CA, October 1991\nThe original SOSP paper about LFS, which has been cited by hundreds of other papers and inspired\nmany real systems.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2366,
      "extraction_method": "Direct"
    },
    {
      "page_number": 561,
      "chapter": null,
      "content": "LOG-STRUCTURED FILE SYSTEMS\n525\n[R92] “Design and Implementation of the Log-structured File System”\nMendel Rosenblum\nhttp://www.eecs.berkeley.edu/Pubs/TechRpts/1992/CSD-92-696.pdf\nThe award-winning dissertation about LFS, with many of the details missing from the paper.\n[SS+95] “File system logging versus clustering: a performance comparison”\nMargo Seltzer, Keith A. Smith, Hari Balakrishnan, Jacqueline Chang, Sara McMains, Venkata\nPadmanabhan\nUSENIX 1995 Technical Conference, New Orleans, Louisiana, 1995\nA paper that showed the LFS performance sometimes has problems, particularly for workloads with\nmany calls to fsync() (such as database workloads). The paper was controversial at the time.\n[SO90] “Write-Only Disk Caches”\nJon A. Solworth, Cyril U. Orji\nSIGMOD ’90, Atlantic City, New Jersey, May 1990\nAn early study of write buffering and its beneﬁts. However, buffering for too long can be harmful: see\nMogul [M94] for details.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 979,
      "extraction_method": "Direct"
    },
    {
      "page_number": 562,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 563,
      "chapter": null,
      "content": "44\nData Integrity and Protection\nBeyond the basic advances found in the ﬁle systems we have studied thus\nfar, a number of features are worth studying. In this chapter, we focus on\nreliability once again (having previously studied storage system reliabil-\nity in the RAID chapter). Speciﬁcally, how should a ﬁle system or storage\nsystem ensure that data is safe, given the unreliable nature of modern\nstorage devices?\nThis general area is referred to as data integrity or data protection.\nThus, we will now investigate techniques used to ensure that the data\nyou put into your storage system is the same when the storage system\nreturns it to you.\nCRUX: HOW TO ENSURE DATA INTEGRITY\nHow should systems ensure that the data written to storage is pro-\ntected? What techniques are required? How can such techniques be made\nefﬁcient, with both low space and time overheads?\n44.1\nDisk Failure Modes\nAs you learned in the chapter about RAID, disks are not perfect, and\ncan fail (on occasion). In early RAID systems, the model of failure was\nquite simple: either the entire disk is working, or it fails completely, and\nthe detection of such a failure is straightforward. This fail-stop model of\ndisk failure makes building RAID relatively simple [S90].\nWhat you didn’t learn is about all of the other types of failure modes\nmodern disks exhibit. Speciﬁcally, as Bairavasundaram et al. studied\nin great detail [B+07, B+08], modern disks will occasionally seem to be\nmostly working but have trouble successfully accessing one or more blocks.\nSpeciﬁcally, two types of single-block failures are common and worthy of\nconsideration: latent-sector errors (LSEs) and block corruption. We’ll\nnow discuss each in more detail.\n527\n",
      "content_length": 1712,
      "extraction_method": "Direct"
    },
    {
      "page_number": 564,
      "chapter": null,
      "content": "528\nDATA INTEGRITY AND PROTECTION\nCheap\nCostly\nLSEs\n9.40%\n1.40%\nCorruption\n0.50%\n0.05%\nTable 44.1: Frequency of LSEs and Block Corruption\nLSEs arise when a disk sector (or group of sectors) has been damaged\nin some way. For example, if the disk head touches the surface for some\nreason (a head crash, something which shouldn’t happen during nor-\nmal operation), it may damage the surface, making the bits unreadable.\nCosmic rays can also ﬂip bits, leading to incorrect contents. Fortunately,\nin-disk error correcting codes (ECC) are used by the drive to determine\nwhether the on-disk bits in a block are good, and in some cases, to ﬁx\nthem; if they are not good, and the drive does not have enough informa-\ntion to ﬁx the error, the disk will return an error when a request is issued\nto read them.\nThere are also cases where a disk block becomes corrupt in a way not\ndetectable by the disk itself. For example, buggy disk ﬁrmware may write\na block to the wrong location; in such a case, the disk ECC indicates the\nblock contents are ﬁne, but from the client’s perspective the wrong block\nis returned when subsequently accessed. Similarly, a block may get cor-\nrupted when it is transferred from the host to the disk across a faulty\nbus; the resulting corrupt data is stored by the disk, but it is not what\nthe client desires. These types of faults are particularly insidious because\nthe are silent faults; the disk gives no indication of the problem when\nreturning the faulty data.\nPrabhakaran et al. describes this more modern view of disk failure as\nthe fail-partial disk failure model [P+05]. In this view, disks can still fail\nin their entirety (as was the case in the traditional fail-stop model); how-\never, disks can also seemingly be working and have one or more blocks\nbecome inaccessible (i.e., LSEs) or hold the wrong contents (i.e., corrup-\ntion). Thus, when accessing a seemingly-working disk, once in a while\nit may either return an error when trying to read or write a given block\n(a non-silent partial fault), and once in a while it may simply return the\nwrong data (a silent partial fault).\nBoth of these types of faults are somewhat rare, but just how rare? Ta-\nble 44.1 summarizes some of the ﬁndings from the two Bairavasundaram\nstudies [B+07,B+08].\nThe table shows the percent of drives that exhibited at least one LSE\nor block corruption over the course of the study (about 3 years, over\n1.5 million disk drives). The table further sub-divides the results into\n“cheap” drives (usually SATA drives) and “costly” drives (usually SCSI\nor FibreChannel). As you can see from the table, while buying better\ndrives reduces the frequency of both types of problem (by about an or-\nder of magnitude), they still happen often enough that you need to think\ncarefully about them.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2836,
      "extraction_method": "Direct"
    },
    {
      "page_number": 565,
      "chapter": null,
      "content": "DATA INTEGRITY AND PROTECTION\n529\nSome additional ﬁndings about LSEs are:\n• Costly drives with more than one LSE are as likely to develop ad-\nditional errors as cheaper drives\n• For most drives, annual error rate increases in year two\n• LSEs increase with disk size\n• Most disks with LSEs have less than 50\n• Disks with LSEs are more likely to develop additional LSEs\n• There exists a signiﬁcant amount of spatial and temporal locality\n• Disk scrubbing is useful (most LSEs were found this way)\nSome ﬁndings about corruption:\n• Chance of corruption varies greatly across different drive models\nwithin the same drive class\n• Age affects are different across models\n• Workload and disk size have little impact on corruption\n• Most disks with corruption only have a few corruptions\n• Corruption is not independent with a disk or across disks in RAID\n• There exists spatial locality, and some temporal locality\n• There is a weak correlation with LSEs\nTo learn more about these failures, you should likely read the original\npapers [B+07,B+08]. But hopefully the main point should be clear: if you\nreally wish to build a reliable storage system, you must include machin-\nery to detect and recovery from both LSEs and block corruption.\n44.2\nHandling Latent Sector Errors\nGiven these two new modes of partial disk failure, we should now try\nto see what we can do about them. Let’s ﬁrst tackle the easier of the two,\nnamely latent sector errors.\nCRUX: HOW TO HANDLE LATENT SECTOR ERRORS\nHow should a storage system handle latent sector errors? How much\nextra machinery is needed to handle this form of partial failure?\nAs it turns out, latent sector errors are rather straightforward to han-\ndle, as they are (by deﬁnition) easily detected. When a storage system\ntries to access a block, and the disk returns an error, the storage system\nshould simply use whatever redundancy mechanism it has to return the\ncorrect data. In a mirrored RAID, for example, the system should access\nthe alternate copy; in a RAID-4 or RAID-5 system based on parity, the\nsystem should reconstruct the block from the other blocks in the parity\ngroup. Thus, easily detected problems such as LSEs are readily recovered\nthrough standard redundancy mechanisms.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2266,
      "extraction_method": "Direct"
    },
    {
      "page_number": 566,
      "chapter": null,
      "content": "530\nDATA INTEGRITY AND PROTECTION\nThe growing prevalence of LSEs has inﬂuenced RAID designs over the\nyears. One particularly interesting problem arises in RAID-4/5 systems\nwhen both full-disk faults and LSEs occur in tandem. Speciﬁcally, when\nan entire disk fails, the RAID tries to reconstruct the disk (say, onto a\nhot spare) by reading through all of the other disks in the parity group\nand recomputing the missing values. If, during reconstruction, an LSE\nis encountered on any one of the other disks, we have a problem: the\nreconstruction cannot successfully complete.\nTo combat this issue, some systems add an extra degree of redundancy.\nFor example, NetApp’s RAID-DP has the equivalent of two parity disks\ninstead of one [C+04]. When an LSE is discovered during reconstruction,\nthe extra parity helps to reconstruct the missing block. As always, there is\na cost, in that maintaining two parity blocks for each stripe is more costly;\nhowever, the log-structured nature of the NetApp WAFL ﬁle system mit-\nigates that cost in many cases [HLM94]. The remaining cost is space, in\nthe form of an extra disk for the second parity block.\n44.3\nDetecting Corruption: The Checksum\nLet’s now tackle the more challenging problem, that of silent failures\nvia data corruption. How can we prevent users from getting bad data\nwhen corruption arises, and thus leads to disks returning bad data?\nCRUX: HOW TO PRESERVE DATA INTEGRITY DESPITE CORRUPTION\nGiven the silent nature of such failures, what can a storage system do\nto detect when corruption arises? What techniques are needed? How can\none implement them efﬁciently?\nUnlike latent sector errors, detection of corruption is a key problem.\nHow can a client tell that a block has gone bad? Once it is known that a\nparticular block is bad, recovery is the same as before: you need to have\nsome other copy of the block around (and hopefully, one that is not cor-\nrupt!). Thus, we focus here on detection techniques.\nThe primary mechanism used by modern storage systems to preserve\ndata integrity is called the checksum. A checksum is simply the result\nof a function that takes a chunk of data (say a 4KB block) as input and\ncomputes a function over said data, producing a small summary of the\ncontents of the data (say 4 or 8 bytes). This summary is referred to as the\nchecksum. The goal of such a computation is to enable a system to detect\nif data has somehow been corrupted or altered by storing the checksum\nwith the data and then conﬁrming upon later access that the data’s cur-\nrent checksum matches the original storage value.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2621,
      "extraction_method": "Direct"
    },
    {
      "page_number": 567,
      "chapter": null,
      "content": "DATA INTEGRITY AND PROTECTION\n531\nTIP: THERE’S NO FREE LUNCH\nThere’s No Such Thing As A Free Lunch, or TNSTAAFL for short, is\nan old American idiom that implies that when you are seemingly get-\nting something for free, in actuality you are likely paying some cost for\nit. It comes from the old days when diners would advertise a free lunch\nfor customers, hoping to draw them in; only when you went in, did you\nrealize that to acquire the “free” lunch, you had to purchase one or more\nalcoholic beverages. Of course, this may not actually be a problem, partic-\nularly if you are an aspiring alcoholic (or typical undergraduate student).\nCommon Checksum Functions\nA number of different functions are used to compute checksums, and\nvary in strength (i.e., how good they are at protecting data integrity) and\nspeed (i.e., how quickly can they be computed). A trade-off that is com-\nmon in systems arises here: usually, the more protection you get, the\ncostlier it is. There is no such thing as a free lunch.\nOne simple checksum function that some use is based on exclusive\nor (XOR). With XOR-based checksums, the checksum is computed sim-\nply by XOR’ing each chunk of the data block being checksummed, thus\nproducing a single value that represents the XOR of the entire block.\nTo make this more concrete, imagine we are computing a 4-byte check-\nsum over a block of 16 bytes (this block is of course too small to really be a\ndisk sector or block, but it will serve for the example). The 16 data bytes,\nin hex, look like this:\n365e c4cd ba14 8a92 ecef 2c3a 40be f666\nIf we view them in binary, we get the following:\n0011 0110 0101 1110\n1100 0100 1100 1101\n1011 1010 0001 0100\n1000 1010 1001 0010\n1110 1100 1110 1111\n0010 1100 0011 1010\n0100 0000 1011 1110\n1111 0110 0110 0110\nBecause we’ve lined up the data in groups of 4 bytes per row, it is easy\nto see what the resulting checksum will be: simply perform an XOR over\neach column to get the ﬁnal checksum value:\n0010 0000 0001 1011\n1001 0100 0000 0011\nThe result, in hex, is 0x201b9403.\nXOR is a reasonable checksum but has its limitations. If, for example,\ntwo bits in the same position within each checksummed unit change, the\nchecksum will not detect the corruption. For this reason, people have\ninvestigated other checksum functions.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2326,
      "extraction_method": "Direct"
    },
    {
      "page_number": 568,
      "chapter": null,
      "content": "532\nDATA INTEGRITY AND PROTECTION\nAnother simple checksum function is addition. This approach has\nthe advantage of being fast; computing it just requires performing 2’s-\ncomplement addition over each chunk of the data, ignoring overﬂow. It\ncan detect many changes in data, but is not good if the data, for example,\nis shifted.\nA slightly more complex algorithm is known as the Fletcher check-\nsum, named (as you might guess) for the inventor, John G. Fletcher [F82].\nIt is quite simple and involves the computation of two check bytes, s1\nand s2. Speciﬁcally, assume a block D consists of bytes d1 ... dn; s1 is\nsimply deﬁned as follows: s1 = s1 + di mod 255 (computed over all di);\ns2 in turn is: s2 = s2 + s1 mod 255 (again over all di) [F04]. The ﬂetcher\nchecksum is known to be almost as strong as the CRC (described next),\ndetecting all single-bit errors, all double-bit errors, and a large percentage\nof burst errors [F04].\nOne ﬁnal commonly-used checksum is known as a cyclic redundancy\ncheck (CRC). While this sounds fancy, the basic idea is quite simple. As-\nsume you wish to compute the checksum over a data block D. All you do\nis treat D as if it is a large binary number (it is just a string of bits after all)\nand divide it by an agreed upon value (k). The remainder of this division\nis the value of the CRC. As it turns out, one can implement this binary\nmodulo operation rather efﬁciently, and hence the popularity of the CRC\nin networking as well. See elsewhere for more details [M13].\nWhatever the method used, it should be obvious that there is no per-\nfect checksum: it is possible two data blocks with non-identical contents\nwill have identical checksums, something referred to as a collision. This\nfact should be intuitive: after all, computing a checksum is taking some-\nthing large (e.g., 4KB) and producing a summary that is much smaller\n(e.g., 4 or 8 bytes). In choosing a good checksum function, we are thus\ntrying to ﬁnd one that minimizes the chance of collisions while remain-\ning easy to compute.\nChecksum Layout\nNow that you understand a bit about how to compute a checksum, let’s\nnext analyze how to use checksums in a storage system. The ﬁrst question\nwe must address is the layout of the checksum, i.e., how should check-\nsums be stored on disk?\nThe most basic approach simply stores a checksum with each disk sec-\ntor (or block). Given a data block D, let us call the checksum over that\ndata C(D). Thus, without checksums, the disk layout looks like this:\nD0\nD1\nD2\nD3\nD4\nD5\nD6\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2558,
      "extraction_method": "Direct"
    },
    {
      "page_number": 569,
      "chapter": null,
      "content": "DATA INTEGRITY AND PROTECTION\n533\nWith checksums, the layout adds a single checksum for every block:\nC[D0]\nD0\nC[D1]\nD1\nC[D2]\nD2\nC[D3]\nD3\nC[D4]\nD4\nBecause checksums are usually small (e.g., 8 bytes), and disks only can\nwrite in sector-sized chunks (512 bytes) or multiples thereof, one problem\nthat arises is how to achieve the above layout. One solution employed by\ndrive manufacturers is to format the drive with 520-byte sectors; an extra\n8 bytes per sector can be used to store the checksum.\nIn disks that don’t have such functionality, the ﬁle system must ﬁgure\nout a way to store the checksums packed into 512-byte blocks. One such\npossibility is as follows:\nC[D0]\nC[D1]\nC[D2]\nC[D3]\nC[D4]\nD0\nD1\nD2\nD3\nD4\nIn this scheme, the n checksums are stored together in a sector, fol-\nlowed by n data blocks, followed by another checksum sector for the next\nn blocks, and so forth. This scheme has the beneﬁt of working on all disks,\nbut can be less efﬁcient; if the ﬁle system, for example, wants to overwrite\nblock D1, it has to read in the checksum sector containing C(D1), update\nC(D1) in it, and then write out the checksum sector as well as the new\ndata block D1 (thus, one read and two writes). The earlier approach (of\none checksum per sector) just performs a single write.\n44.4\nUsing Checksums\nWith a checksum layout decided upon, we can now proceed to actu-\nally understand how to use the checksums. When reading a block D, the\nclient (i.e., ﬁle system or storage controller) also reads its checksum from\ndisk Cs(D), which we call the stored checksum (hence the subscript Cs).\nThe client then computes the checksum over the retrieved block D, which\nwe call the computed checksum Cc(D). At this point, the client com-\npares the stored and computed checksums; if they are equal (i.e., Cs(D)\n== Cc(D), the data has likely not been corrupted, and thus can be safely\nreturned to the user. If they do not match (i.e., Cs(D) != Cc(D)), this im-\nplies the data has changed since the time it was stored (since the stored\nchecksum reﬂects the value of the data at that time). In this case, we have\na corruption, which our checksum has helped us to detect.\nGiven a corruption, the natural question is what should we do about\nit? If the storage system has a redundant copy, the answer is easy: try to\nuse it instead. If the storage system has no such copy, the likely answer is\nto return an error. In either case, realize that corruption detection is not a\nmagic bullet; if there is no other way to get the non-corrupted data, you\nare simply out of luck.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2588,
      "extraction_method": "Direct"
    },
    {
      "page_number": 570,
      "chapter": null,
      "content": "534\nDATA INTEGRITY AND PROTECTION\n44.5\nA New Problem: Misdirected Writes\nThe basic scheme described above works well in the general case of\ncorrupted blocks. However, modern disks have a couple of unusual fail-\nure modes that require different solutions.\nThe ﬁrst failure mode of interest is called a misdirected write. This\narises in disk and RAID controllers which write the data to disk correctly,\nexcept in the wrong location. In a single-disk system, this means that the\ndisk wrote block Dx not to address x (as desired) but rather to address\ny (thus “corrupting” Dy); in addition, within a multi-disk system, the\ncontroller may also write Di,x not to address x of disk i but rather to\nsome other disk j. Thus our question:\nCRUX: HOW TO HANDLE MISDIRECTED WRITES\nHow should a storage system or disk controller detect misdirected\nwrites? What additional features are required from the checksum?\nThe answer, not surprisingly, is simple: add a little more information\nto each checksum. In this case, adding a physical identiﬁer (physical\nID) is quite helpful. For example, if the stored information now contains\nthe checksum C(D) as well as the disk and sector number of the block,\nit is easy for the client to determine whether the correct information re-\nsides within the block. Speciﬁcally, if the client is reading block 4 on disk\n10 (D10,4), the stored information should include that disk number and\nsector offset, as shown below. If the information does not match, a misdi-\nrected write has taken place, and a corruption is now detected. Here is an\nexample of what this added information would look like on a two-disk\nsystem. Note that this ﬁgure, like the others before it, is not to scale, as the\nchecksums are usually small (e.g., 8 bytes) whereas the blocks are much\nlarger (e.g., 4 KB or bigger):\nDisk 0\nDisk 1\nC[D0]\ndisk=0\nblock=0\nD0\nC[D1]\ndisk=0\nblock=1\nD1\nC[D2]\ndisk=0\nblock=2\nD2\nC[D0]\ndisk=1\nblock=0\nD0\nC[D1]\ndisk=1\nblock=1\nD1\nC[D2]\ndisk=1\nblock=2\nD2\nYou can see from the on-disk format that there is now a fair amount of\nredundancy on disk: for each block, the disk number is repeated within\neach block, and the offset of the block in question is also kept next to the\nblock itself. The presence of redundant information should be no sur-\nprise, though; redundancy is the key to error detection (in this case) and\nrecovery (in others). A little extra information, while not strictly needed\nwith perfect disks, can go a long ways in helping detect problematic situ-\nations should they arise.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2557,
      "extraction_method": "Direct"
    },
    {
      "page_number": 571,
      "chapter": null,
      "content": "DATA INTEGRITY AND PROTECTION\n535\n44.6\nOne Last Problem: Lost Writes\nUnfortunately, misdirected writes are not the last problem we will\naddress. Speciﬁcally, some modern storage devices also have an issue\nknown as a lost write, which occurs when the device informs the upper\nlayer that a write has completed but in fact it never is persisted; thus,\nwhat remains is left is the old contents of the block rather than the up-\ndated new contents.\nThe obvious question here is: do any of our checksumming strategies\nfrom above (e.g., basic checksums, or physical identity) help to detect\nlost writes? Unfortunately, the answer is no: the old block likely has a\nmatching checksum, and the physical ID used above (disk number and\nblock offset) will also be correct. Thus our ﬁnal problem:\nCRUX: HOW TO HANDLE LOST WRITES\nHow should a storage system or disk controller detect lost writes?\nWhat additional features are required from the checksum?\nThere are a number of possible solutions that can help [K+08]. One\nclassic approach [BS04] is to perform a write verify or read-after-write;\nby immediately reading back the data after a write, a system can ensure\nthat the data indeed reached the disk surface. This approach, however, is\nquite slow, doubling the number of I/Os needed to complete a write.\nSome systems add a checksum elsewhere in the system to detect lost\nwrites. For example, Sun’s Zettabyte File System (ZFS) includes a check-\nsum in each ﬁle system inode and indirect block for every block included\nwithin a ﬁle. Thus, even if the write to a data block itself is lost, the check-\nsum within the inode will not match the old data. Only if the writes to\nboth the inode and the data are lost simultaneously will such a scheme\nfail, an unlikely (but unfortunately, possible!) situation.\n44.7\nScrubbing\nGiven all of this discussion, you might be wondering: when do these\nchecksums actually get checked? Of course, some amount of checking\noccurs when data is accessed by applications, but most data is rarely\naccessed, and thus would remain unchecked. Unchecked data is prob-\nlematic for a reliable storage system, as bit rot could eventually affect all\ncopies of a particular piece of data.\nTo remedy this problem, many systems utilize disk scrubbing of var-\nious forms [K+08]. By periodically reading through every block of the\nsystem, and checking whether checksums are still valid, the disk system\ncan reduce the chances that all copies of a certain data item become cor-\nrupted. Typical systems schedule scans on a nightly or weekly basis.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2587,
      "extraction_method": "Direct"
    },
    {
      "page_number": 572,
      "chapter": null,
      "content": "536\nDATA INTEGRITY AND PROTECTION\n44.8\nOverheads Of Checksumming\nBefore closing, we now discuss some of the overheads of using check-\nsums for data protection. There are two distinct kinds of overheads, as is\ncommon in computer systems: space and time.\nSpace overheads come in two forms. The ﬁrst is on the disk (or other\nstorage medium) itself; each stored checksum takes up room on the disk,\nwhich can no longer be used for user data. A typical ratio might be an 8-\nbyte checksum per 4 KB data block, for a 0.19% on-disk space overhead.\nThe second type of space overhead comes in the memory of the sys-\ntem. When accessing data, there must now be room in memory for the\nchecksums as well as the data itself. However, if the system simply checks\nthe checksum and then discards it once done, this overhead is short-lived\nand not much of a concern. Only if checksums are kept in memory (for\nan added level of protection against memory corruption [Z+13]) will this\nsmall overhead be observable.\nWhile space overheads are small, the time overheads induced by check-\nsumming can be quite noticeable. Minimally, the CPU must compute the\nchecksum over each block, both when the data is stored (to determine\nthe value of the stored checksum) as well as when it is accessed (to com-\npute the checksum again and compare it against the stored checksum).\nOne approach to reducing CPU overheads, employed by many systems\nthat use checksums (including network stacks), is to combine data copy-\ning and checksumming into one streamlined activity; because the copy is\nneeded anyhow (e.g., to copy the data from the kernel page cache into a\nuser buffer), combined copying/checksumming can be quite effective.\nBeyond CPU overheads, some checksumming schemes can induce ex-\ntra I/O overheads, particularly when checksums are stored distinctly from\nthe data (thus requiring extra I/Os to access them), and for any extra I/O\nneeded for background scrubbing. The former can be reduced by design;\nthe latter can be tuned and thus its impact limited, perhaps by control-\nling when such scrubbing activity takes place. The middle of the night,\nwhen most (not all!) productive workers have gone to bed, may be a\ngood time to perform such scrubbing activity and increase the robustness\nof the storage system.\n44.9\nSummary\nWe have discussed data protection in modern storage systems, focus-\ning on checksum implementation and usage. Different checksums protect\nagainst different types of faults; as storage devices evolve, new failure\nmodes will undoubtedly arise. Perhaps such change will force the re-\nsearch community and industry to revisit some of these basic approaches,\nor invent entirely new approaches altogether. Time will tell. Or it won’t.\nTime is funny that way.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2796,
      "extraction_method": "Direct"
    },
    {
      "page_number": 573,
      "chapter": null,
      "content": "DATA INTEGRITY AND PROTECTION\n537\nReferences\n[B+08] “An Analysis of Data Corruption in the Storage Stack”\nLakshmi N. Bairavasundaram, Garth R. Goodson, Bianca Schroeder,\nAndrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau\nFAST ’08, San Jose, CA, February 2008\nThe ﬁrst paper to truly study disk corruption in great detail, focusing on how often such corruption\noccurs over three years for over 1.5 million drives. Lakshmi did this work while a graduate student at\nWisconsin under our supervision, but also in collaboration with his colleagues at NetApp where he was\nan intern for multiple summers. A great example of how working with industry can make for much\nmore interesting and relevant research.\n[BS04] “Commercial Fault Tolerance: A Tale of Two Systems”\nWendy Bartlett, Lisa Spainhower\nIEEE Transactions on Dependable and Secure Computing, Vol. 1, No. 1, January 2004\nThis classic in building fault tolerant systems is an excellent overview of the state of the art from both\nIBM and Tandem. Another must read for those interested in the area.\n[C+04] “Row-Diagonal Parity for Double Disk Failure Correction”\nP. Corbett, B. English, A. Goel, T. Grcanac, S. Kleiman, J. Leong, S. Sankar\nFAST ’04, San Jose, CA, February 2004\nAn early paper on how extra redundancy helps to solve the combined full-disk-failure/partial-disk-failure\nproblem. Also a nice example of how to mix more theoretical work with practical.\n[F04] “Checksums and Error Control”\nPeter M. Fenwick\nAvailable: www.cs.auckland.ac.nz/compsci314s2c/resources/Checksums.pdf\nA great simple tutorial on checksums, available to you for the amazing cost of free.\n[F82] “An Arithmetic Checksum for Serial Transmissions”\nJohn G. Fletcher\nIEEE Transactions on Communication, Vol. 30, No. 1, January 1982\nFletcher’s original work on his eponymous checksum. Of course, he didn’t call it the Fletcher checksum,\nrather he just didn’t call it anything, and thus it became natural to name it after the inventor. So don’t\nblame old Fletch for this seeming act of braggadocio.\n[HLM94] “File System Design for an NFS File Server Appliance”\nDave Hitz, James Lau, Michael Malcolm\nUSENIX Spring ’94\nThe pioneering paper that describes the ideas and product at the heart of NetApp’s core. Based on this\nsystem, NetApp has grown into a multi-billion dollar storage company. If you’re interested in learning\nmore about its founding, read Hitz’s autobiography “How to Castrate a Bull: Unexpected Lessons on\nRisk, Growth, and Success in Business” (which is the actual title, no joking). And you thought you\ncould avoid bull castration by going into Computer Science.\n[K+08] “Parity Lost and Parity Regained”\nAndrew Krioukov, Lakshmi N. Bairavasundaram, Garth R. Goodson, Kiran Srinivasan,\nRandy Thelen, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau\nFAST ’08, San Jose, CA, February 2008\nThis work of ours, joint with colleagues at NetApp, explores how different checksum schemes work (or\ndon’t work) in protecting data. We reveal a number of interesting ﬂaws in current protection strategies,\nsome of which have led to ﬁxes in commercial products.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 3132,
      "extraction_method": "Direct"
    },
    {
      "page_number": 574,
      "chapter": null,
      "content": "538\nDATA INTEGRITY AND PROTECTION\n[M13] “Cyclic Redundancy Checks”\nAuthor Unknown\nAvailable: http://www.mathpages.com/home/kmath458.htm\nNot sure who wrote this, but a super clear and concise description of CRCs is available here. The internet\nis full of information, as it turns out.\n[P+05] “IRON File Systems”\nVijayan Prabhakaran, Lakshmi N. Bairavasundaram, Nitin Agrawal, Haryadi S. Gunawi, An-\ndrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau\nSOSP ’05, Brighton, England, October 2005\nOur paper on how disks have partial failure modes, which includes a detailed study of how ﬁle systems\nsuch as Linux ext3 and Windows NTFS react to such failures. As it turns out, rather poorly! We found\nnumerous bugs, design ﬂaws, and other oddities in this work. Some of this has fed back into the Linux\ncommunity, thus helping to yield a new more robust group of ﬁle systems to store your data.\n[RO91] “Design and Implementation of the Log-structured File System”\nMendel Rosenblum and John Ousterhout\nSOSP ’91, Paciﬁc Grove, CA, October 1991\nAnother reference to this ground-breaking paper on how to improve write performance in ﬁle systems.\n[S90] “Implementing Fault-Tolerant Services Using The State Machine Approach: A Tutorial”\nFred B. Schneider\nACM Surveys, Vol. 22, No. 4, December 1990\nThis classic paper talks generally about how to build fault tolerant services, and includes many basic\ndeﬁnitions of terms. A must read for those building distributed systems.\n[Z+13] “Zettabyte Reliability with Flexible End-to-end Data Integrity”\nYupu Zhang, Daniel S. Myers, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau\nMSST ’13, Long Beach, California, May 2013\nOur own work on adding data protection to the page cache of a system, which protects against memory\ncorruption as well as on-disk corruption.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1841,
      "extraction_method": "Direct"
    },
    {
      "page_number": 575,
      "chapter": null,
      "content": "45\nSummary Dialogue on Persistence\nStudent: Wow, ﬁle systems seem interesting(!), and yet complicated.\nProfessor: That’s why me and my spouse do our research in this space.\nStudent: Hold on. Are you one of the professors who wrote this book? I thought\nwe were both just fake constructs, used to summarize some main points, and\nperhaps add a little levity in the study of operating systems.\nProfessor: Uh... er... maybe. And none of your business! And who did you\nthink was writing these things? (sighs) Anyhow, let’s get on with it: what did\nyou learn?\nStudent: Well, I think I got one of the main points, which is that it is much\nharder to manage data for a long time (persistently) than it is to manage data\nthat isn’t persistent (like the stuff in memory). After all, if your machines crashes,\nmemory contents disappear! But the stuff in the ﬁle system needs to live forever.\nProfessor: Well, as my friend Kevin Hultquist used to say, “Forever is a long\ntime”; while he was talking about plastic golf tees, it’s especially true for the\ngarbage that is found in most ﬁle systems.\nStudent: Well, you know what I mean! For a long time at least. And even simple\nthings, such as updating a persistent storage device, are complicated, because you\nhave to care what happens if you crash. Recovery, something I had never even\nthought of when we were virtualizing memory, is now a big deal!\nProfessor: Too true. Updates to persistent storage have always been, and re-\nmain, a fun and challenging problem.\nStudent: I also learned about cool things like disk scheduling, and about data\nprotection techniques like RAID and even checksums. That stuff is cool.\nProfessor: I like those topics too. Though, if you really get into it, they can get a\nlittle mathematical. Check out some the latest on erasure codes if you want your\nbrain to hurt.\nStudent: I’ll get right on that.\n539\n",
      "content_length": 1869,
      "extraction_method": "Direct"
    },
    {
      "page_number": 576,
      "chapter": null,
      "content": "540\nSUMMARY DIALOGUE ON PERSISTENCE\nProfessor: (frowns) I think you’re being sarcastic. Well, what else did you like?\nStudent: And I also liked all the thought that has gone into building technology-\naware systems, like FFS and LFS. Neat stuff! Being disk aware seems cool. But\nwill it matter anymore, with Flash and all the newest, latest technologies?\nProfessor: Good question! And a reminder to get working on that Flash chap-\nter... (scribbles note down to self) ... But yes, even with Flash, all of this stuff\nis still relevant, amazingly. For example, Flash Translation Layers (FTLs) use\nlog-structuring internally, to improve performance and reliability of Flash-based\nSSDs. And thinking about locality is always useful. So while the technology\nmay be changing, many of the ideas we have studied will continue to be useful,\nfor a while at least.\nStudent: That’s good. I just spent all this time learning it, and I didn’t want it\nto all be for no reason!\nProfessor: Professors wouldn’t do that to you, would they?\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1067,
      "extraction_method": "Direct"
    },
    {
      "page_number": 577,
      "chapter": null,
      "content": "46\nA Dialogue on Distribution\nProfessor: And thus we reach our ﬁnal little piece in the world of operating\nsystems: distributed systems. Since we can’t cover much here, we’ll sneak in a\nlittle intro here in the section on persistence, and focus mostly on distributed ﬁle\nsystems. Hope that is OK!\nStudent: Sounds OK. But what is a distributed system exactly, oh glorious and\nall-knowing professor?\nProfessor: Well, I bet you know how this is going to go...\nStudent: There’s a peach?\nProfessor: Exactly! But this time, it’s far away from you, and may take some\ntime to get the peach. And there are a lot of them! Even worse, sometimes a\npeach becomes rotten. But you want to make sure that when anybody bites into\na peach, they will get a mouthful of deliciousness.\nStudent: This peach analogy is working less and less for me.\nProfessor: Come on! It’s the last one, just go with it.\nStudent: Fine.\nProfessor: So anyhow, forget about the peaches. Building distributed systems\nis hard, because things fail all the time. Messages get lost, machines go down,\ndisks corrupt data. It’s like the whole world is working against you!\nStudent: But I use distributed systems all the time, right?\nProfessor: Yes! You do. And... ?\nStudent: Well, it seems like they mostly work. After all, when I send a search\nrequest to google, it usually comes back in a snap, with some great results! Same\nthing when I use facebook, or Amazon, and so forth.\n541\n",
      "content_length": 1434,
      "extraction_method": "Direct"
    },
    {
      "page_number": 578,
      "chapter": null,
      "content": "542\nA DIALOGUE ON DISTRIBUTION\nProfessor: Yes, it is amazing. And that’s despite all of those failures taking\nplace! Those companies build a huge amount of machinery into their systems so\nas to ensure that even though some machines have failed, the entire system stays\nup and running. They use a lot of techniques to do this: replication, retry, and\nvarious other tricks people have developed over time to detect and recover from\nfailures.\nStudent: Sounds interesting. Time to learn something for real?\nProfessor: It does seem so. Let’s get to work! But ﬁrst things ﬁrst ...\n(bites into peach he has been holding, which unfortunately is rotten)\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 692,
      "extraction_method": "Direct"
    },
    {
      "page_number": 579,
      "chapter": null,
      "content": "47\nDistributed Systems\nDistributed systems have changed the face of the world. When your web\nbrowser connects to a web server somewhere else on the planet, it is par-\nticipating in what seems to be a simple form of a client/server distributed\nsystem. When you contact a modern web service such as Google or face-\nbook, you are not just interacting with a single machine, however; be-\nhind the scenes, these complex services are built from a large collection\n(i.e., thousands) of machines, each of which cooperate to provide the par-\nticular service of the site. Thus, it should be clear what makes studying\ndistributed systems interesting. Indeed, it is worthy of an entire class;\nhere, we just introduce a few of the major topics.\nA number of new challenges arise when building a distributed system.\nThe major one we focus on is failure; machines, disks, networks, and\nsoftware all fail from time to time, as we do not (and likely, will never)\nknow how to build “perfect” components and systems. However, when\nwe build a modern web service, we’d like it to appear to clients as if it\nnever fails; how can we accomplish this task?\nTHE CRUX:\nHOW TO BUILD SYSTEMS THAT WORK WHEN COMPONENTS FAIL\nHow can we build a working system out of parts that don’t work correctly\nall the time? The basic question should remind you of some of the topics\nwe discussed in RAID storage arrays; however, the problems here tend\nto be more complex, as are the solutions.\nInterestingly, while failure is a central challenge in constructing dis-\ntributed systems, it also represents an opportunity. Yes, machines fail;\nbut the mere fact that a machine fails does not imply the entire system\nmust fail. By collecting together a set of machines, we can build a sys-\ntem that appears to rarely fail, despite the fact that its components fail\nregularly. This reality is the central beauty and value of distributed sys-\ntems, and why they underly virtually every modern web service you use,\nincluding Google, Facebook, etc.\n543\n",
      "content_length": 2000,
      "extraction_method": "Direct"
    },
    {
      "page_number": 580,
      "chapter": null,
      "content": "544\nDISTRIBUTED SYSTEMS\nTIP: COMMUNICATION IS INHERENTLY UNRELIABLE\nIn virtually all circumstances, it is good to view communication as a\nfundamentally unreliable activity. Bit corruption, down or non-working\nlinks and machines, and lack of buffer space for incoming packets all lead\nto the same result: packets sometimes do not reach their destination. To\nbuild reliable services atop such unreliable networks, we must consider\ntechniques that can cope with packet loss.\nOther important issues exist as well. System performance is often crit-\nical; with a network connecting our distributed system together, system\ndesigners must often think carefully about how to accomplish their given\ntasks, trying to reduce the number of messages sent and further make\ncommunication as efﬁcient (low latency, high bandwidth) as possible.\nFinally, security is also a necessary consideration. When connecting\nto a remote site, having some assurance that the remote party is who\nthey say they are becomes a central problem. Further, ensuring that third\nparties cannot monitor or alter an on-going communication between two\nothers is also a challenge.\nIn this introduction, we’ll cover the most basic new aspect that is new\nin a distributed system: communication. Namely, how should machines\nwithin a distributed system communicate with one another? We’ll start\nwith the most basic primitives available, messages, and build a few higher-\nlevel primitives on top of them. As we said above, failure will be a central\nfocus: how should communication layers handle failures?\n47.1\nCommunication Basics\nThe central tenet of modern networking is that communication is fun-\ndamentally unreliable. Whether in the wide-area Internet, or a local-area\nhigh-speed network such as Inﬁniband, packets are regularly lost, cor-\nrupted, or otherwise do not reach their destination.\nThere are a multitude of causes for packet loss or corruption. Some-\ntimes, during transmission, some bits get ﬂipped due to electrical or other\nsimilar problems. Sometimes, an element in the system, such as a net-\nwork link or packet router or even the remote host, are somehow dam-\naged or otherwise not working correctly; network cables do accidentally\nget severed, at least sometimes.\nMore fundamental however is packet loss due to lack of buffering\nwithin a network switch, router, or endpoint. Speciﬁcally, even if we\ncould guarantee that all links worked correctly, and that all the compo-\nnents in the system (switches, routers, end hosts) were up and running as\nexpected, loss is still possible, for the following reason. Imagine a packet\narrives at a router; for the packet to be processed, it must be placed in\nmemory somewhere within the router. If many such packets arrive at\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2785,
      "extraction_method": "Direct"
    },
    {
      "page_number": 581,
      "chapter": null,
      "content": "DISTRIBUTED SYSTEMS\n545\n// client code\nint main(int argc, char *argv[]) {\nint sd = UDP_Open(20000);\nstruct sockaddr_in addr, addr2;\nint rc = UDP_FillSockAddr(&addr, \"machine.cs.wisc.edu\", 10000);\nchar message[BUFFER_SIZE];\nsprintf(message, \"hello world\");\nrc = UDP_Write(sd, &addr, message, BUFFER_SIZE);\nif (rc > 0) {\nint rc = UDP_Read(sd, &addr2, buffer, BUFFER_SIZE);\n}\nreturn 0;\n}\n// server code\nint main(int argc, char *argv[]) {\nint sd = UDP_Open(10000);\nassert(sd > -1);\nwhile (1) {\nstruct sockaddr_in s;\nchar buffer[BUFFER_SIZE];\nint rc = UDP_Read(sd, &s, buffer, BUFFER_SIZE);\nif (rc > 0) {\nchar reply[BUFFER_SIZE];\nsprintf(reply, \"reply\");\nrc = UDP_Write(sd, &s, reply, BUFFER_SIZE);\n}\n}\nreturn 0;\n}\nFigure 47.1: Example UDP/IP Client/Server Code\nonce, it is possible that the memory within the router cannot accommo-\ndate all of the packets. The only choice the router has at that point is\nto drop one or more of the packets. This same behavior occurs at end\nhosts as well; when you send a large number of messages to a single ma-\nchine, the machine’s resources can easily become overwhelmed, and thus\npacket loss again arises.\nThus, packet loss is fundamental in networking. The question thus\nbecomes: how should we deal with it?\n47.2\nUnreliable Communication Layers\nOne simple way is this: we don’t deal with it. Because some appli-\ncations know how to deal with packet loss, it is sometimes useful to let\nthem communicate with a basic unreliable messaging layer, an example\nof the end-to-end argument one often hears about (see the Aside at end\nof chapter). One excellent example of such an unreliable layer is found\nin the UDP/IP networking stack available today on virtually all modern\nsystems. To use UDP, a process uses the sockets API in order to create a\ncommunication endpoint; processes on other machines (or on the same\nmachine) send UDP datagrams to the original process (a datagram is a\nﬁxed-sized message up to some max size).\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 1994,
      "extraction_method": "Direct"
    },
    {
      "page_number": 582,
      "chapter": null,
      "content": "546\nDISTRIBUTED SYSTEMS\nint UDP_Open(int port) {\nint sd;\nif ((sd = socket(AF_INET, SOCK_DGRAM, 0)) == -1) { return -1; }\nstruct sockaddr_in myaddr;\nbzero(&myaddr, sizeof(myaddr));\nmyaddr.sin_family\n= AF_INET;\nmyaddr.sin_port\n= htons(port);\nmyaddr.sin_addr.s_addr = INADDR_ANY;\nif (bind(sd, (struct sockaddr *) &myaddr, sizeof(myaddr)) == -1) {\nclose(sd);\nreturn -1;\n}\nreturn sd;\n}\nint UDP_FillSockAddr(struct sockaddr_in *addr, char *hostName, int port) {\nbzero(addr, sizeof(struct sockaddr_in));\naddr->sin_family = AF_INET;\n// host byte order\naddr->sin_port\n= htons(port);\n// short, network byte order\nstruct in_addr *inAddr;\nstruct hostent *hostEntry;\nif ((hostEntry = gethostbyname(hostName)) == NULL) { return -1; }\ninAddr = (struct in_addr *) hostEntry->h_addr;\naddr->sin_addr = *inAddr;\nreturn 0;\n}\nint UDP_Write(int sd, struct sockaddr_in *addr, char *buffer, int n) {\nint addrLen = sizeof(struct sockaddr_in);\nreturn sendto(sd, buffer, n, 0, (struct sockaddr *) addr, addrLen);\n}\nint UDP_Read(int sd, struct sockaddr_in *addr, char *buffer, int n) {\nint len = sizeof(struct sockaddr_in);\nreturn recvfrom(sd, buffer, n, 0, (struct sockaddr *) addr,\n(socklen_t *) &len);\nreturn rc;\n}\nFigure 47.2: A Simple UDP Library\nFigures 47.1 and 47.2 show a simple client and server built on top of\nUDP/IP. The client can send a message to the server, which then responds\nwith a reply. With this small amount of code, you have all you need to\nbegin building distributed systems!\nUDP is a great example of an unreliable communication layer. If you\nuse it, you will encounter situations where packets get lost (dropped) and\nthus do not reach their destination; the sender is never thus informed of\nthe loss. However, that does not mean that UDP does not guard against\nany failures at all. For example, UDP includes a checksum to detect some\nforms of packet corruption.\nHowever, because many applications simply want to send data to a\ndestination and not worry about packet loss, we need more. Speciﬁcally,\nwe need reliable communication on top of an unreliable network.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2110,
      "extraction_method": "Direct"
    },
    {
      "page_number": 583,
      "chapter": null,
      "content": "DISTRIBUTED SYSTEMS\n547\nTIP: USE CHECKSUMS FOR INTEGRITY\nChecksums are a commonly-used method to detect corruption quickly\nand effectively in modern systems. A simple checksum is addition: just\nsum up the bytes of a chunk of data; of course, many other more sophis-\nticated checksums have been created, including basic cyclic redundancy\ncodes (CRCs), the Fletcher checksum, and many others [MK09].\nIn networking, checksums are used as follows. Before sending a message\nfrom one machine to another, compute a checksum over the bytes of the\nmessage. Then send both the message and the checksum to the desti-\nnation. At the destination, the receiver computes a checksum over the\nincoming message as well; if this computed checksum matches the sent\nchecksum, the receiver can feel some assurance that the data likely did\nnot get corrupted during transmission.\nChecksums can be evaluated along a number of different axes. Effective-\nness is one primary consideration: does a change in the data lead to a\nchange in the checksum? The stronger the checksum, the harder it is for\nchanges in the data to go unnoticed. Performance is the other important\ncriterion: how costly is the checksum to compute? Unfortunately, effec-\ntiveness and performance are often at odds, meaning that checksums of\nhigh quality are often expensive to compute. Life, again, isn’t perfect.\n47.3\nReliable Communication Layers\nTo build a reliable communication layer, we need some new mech-\nanisms and techniques to handle packet loss. Let us consider a simple\nexample in which a client is sending a message to a server over an unreli-\nable connection. The ﬁrst question we must answer: how does the sender\nknow that the receiver has actually received the message?\nThe technique that we will use is known as an acknowledgment, or\nack for short. The idea is simple: the sender sends a message to the re-\nceiver; the receiver then sends a short message back to acknowledge its\nreceipt. Figure 47.3 depicts the process.\nSender\n[send message]\nReceiver\n[receive message]\n[send ack]\n[receive ack]\nFigure 47.3: Message Plus Acknowledgment\nWhen the sender receives an acknowledgment of the message, it can\nthen rest assured that the message did indeed receive the original mes-\nsage. However, what should the sender do if it does not receive an ac-\nknowledgment?\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2362,
      "extraction_method": "Direct"
    },
    {
      "page_number": 584,
      "chapter": null,
      "content": "548\nDISTRIBUTED SYSTEMS\nSender\n[send message;\n keep copy;\n set timer]\nReceiver\n...\n (waiting for ack)\n...\n[timer goes off;\n set timer/retry]\n[receive message]\n[send ack]\n[receive ack;\n delete copy/timer off]\nFigure 47.4: Message Plus Acknowledgment: Dropped Request\nTo handle this case, we need an additional mechanism, known as a\ntimeout. When the sender sends a message, the sender now sets a timer\nto go off after some period of time. If, in that time, no acknowledgment\nhas been received, the sender concludes that the message has been lost.\nThe sender then simply performs a retry of the send, sending the same\nmessage again with hopes that this time, it will get through. For this\napproach to work, the sender must keep a copy of the message around,\nin case it needs to send it again. The combination of the timeout and\nthe retry have led some to call the approach timeout/retry; pretty clever\ncrowd, those networking types, no? Figure 47.4 shows an example.\nUnfortunately, timeout/retry in this form is not quite enough. Figure\n47.5 shows an example of packet loss which could lead to trouble. In this\nexample, it is not the original message that gets lost, but the acknowledg-\nment. From the perspective of the sender, the situation seems the same:\nno ack was received, and thus a timeout and retry are in order. But from\nthe perspective of the receiver, it is quite different: now the same message\nhas been received twice! While there may be cases where this is OK, in\ngeneral it is not; imagine what would happen when you are downloading\na ﬁle and extra packets are repeated inside the download. Thus, when we\nare aiming for a reliable message layer, we also usually want to guarantee\nthat each message is received exactly once by the receiver.\nTo enable the receiver to detect duplicate message transmission, the\nsender has to identify each message in some unique way, and the receiver\nneeds some way to track whether it has already seen each message be-\nfore. When the receiver sees a duplicate transmission, it simply acks the\nmessage, but (critically) does not pass the message to the application that\nreceives the data. Thus, the sender receives the ack but the message is not\nreceived twice, preserving the exactly-once semantics mentioned above.\nThere are myriad ways to detect duplicate messages. For example, the\nsender could generate a unique ID for each message; the receiver could\ntrack every ID it has ever seen. This approach could work, but it is pro-\nhibitively costly, requiring unbounded memory to track all IDs.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2588,
      "extraction_method": "Direct"
    },
    {
      "page_number": 585,
      "chapter": null,
      "content": "DISTRIBUTED SYSTEMS\n549\nSender\n[send message;\n keep copy;\n set timer]\nReceiver\n[receive message]\n[send ack]\n...\n (waiting for ack)\n...\n[timer goes off;\n set timer/retry]\n[receive message]\n[send ack]\n[receive ack;\n delete copy/timer off]\nFigure 47.5: Message Plus Acknowledgment: Dropped Reply\nA simpler approach, requiring little memory, solves this problem, and\nthe mechanism is known as a sequence counter. With a sequence counter,\nthe sender and receiver agree upon a start value (e.g., 1) for a counter\nthat each side will maintain. Whenever a message is sent, the current\nvalue of the counter is sent along with the message; this counter value\n(N) serves as an ID for the message. After the message is sent, the sender\nthen increments the value (to N + 1).\nThe receiver uses its counter value as the expected value for the ID\nof the incoming message from that sender. If the ID of a received mes-\nsage (N) matches the receiver’s counter (also N), it acks the message and\npasses it up to the application; in this case, the receiver concludes this\nis the ﬁrst time this message has been received. The receiver then incre-\nments its counter (to N + 1), and waits for the next message.\nIf the ack is lost, the sender will timeout and re-send message N. This\ntime, the receiver’s counter is higher (N +1), and thus the receiver knows\nit has already received this message. Thus it acks the message but does\nnot pass it up to the application. In this simple manner, sequence counters\ncan be used to avoid duplicates.\nThe most commonly used reliable communication layer is known as\nTCP/IP, or just TCP for short. TCP has a great deal more sophistication\nthan we describe above, including machinery to handle congestion in the\nnetwork [VJ90], multiple outstanding requests, and hundreds of other\nsmall tweaks and optimizations. Read more about it if you’re curious;\nbetter yet, take a networking course and learn that material well.\n47.4\nCommunication Abstractions\nGiven a basic messaging layer, we now approach the next question\nin this chapter: what abstraction of communication should we use when\nbuilding a distributed system?\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2168,
      "extraction_method": "Direct"
    },
    {
      "page_number": 586,
      "chapter": null,
      "content": "550\nDISTRIBUTED SYSTEMS\nTIP: BE CAREFUL SETTING THE TIMEOUT VALUE\nAs you can probably guess from the discussion, setting the timeout value\ncorrectly is an important aspect of using timeouts to retry message sends.\nIf the timeout is too small, the sender will re-send messages needlessly,\nthus wasting CPU time on the sender and network resources. If the time-\nout is too large, the sender waits too long to re-send and thus perceived\nperformance at the sender is reduced. The “right” value, from the per-\nspective of a single client and server, is thus to wait just long enough to\ndetect packet loss but no longer.\nHowever, there are often more than just a single client and server in a\ndistributed system, as we will see in future chapters. In a scenario with\nmany clients sending to a single server, packet loss at the server may be\nan indicator that the server is overloaded. If true, clients might retry in\na different adaptive manner; for example, after the ﬁrst timeout, a client\nmight increase its timeout value to a higher amount, perhaps twice as\nhigh as the original value. Such an exponential back-off scheme, pio-\nneered in the early Aloha network and adopted in early Ethernet [A70],\navoid situations where resources are being overloaded by an excess of\nre-sends. Robust systems strive to avoid overload of this nature.\nThe systems community developed a number of approaches over the\nyears. One body of work took OS abstractions and extended them to\noperate in a distributed environment. For example, distributed shared\nmemory (DSM) systems enable processes on different machines to share\na large, virtual address space [LH89]. This abstraction turns a distributed\ncomputation into something that looks like a multi-threaded application;\nthe only difference is that these threads run on different machines instead\nof different processors within the same machine.\nThe way most DSM systems work is through the virtual memory sys-\ntem of the OS. When a page is accessed on one machine, two things can\nhappen. In the ﬁrst (best) case, the page is already local on the machine,\nand thus the data is fetched quickly. In the second case, the page is cur-\nrently on some other machine. A page fault occurs, and the page fault\nhandler sends a message to some other machine to fetch the page, install\nit in the page table of the requesting process, and continue execution.\nThis approach is not widely in use today for a number of reasons. The\nlargest problem for DSM is how it handles failure. Imagine, for example,\nif a machine fails; what happens to the pages on that machine? What if\nthe data structures of the distributed computation are spread across the\nentire address space? In this case, parts of these data structures would\nsuddenly become unavailable. Dealing with failure when parts of your\naddress space go missing is hard; imagine a linked list that where a next\npointer points into a portion of the address space that is gone. Yikes!\nA further problem is performance. One usually assumes, when writ-\ning code, that access to memory is cheap. In DSM systems, some accesses\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 3136,
      "extraction_method": "Direct"
    },
    {
      "page_number": 587,
      "chapter": null,
      "content": "DISTRIBUTED SYSTEMS\n551\nare inexpensive, but others cause page faults and expensive fetches from\nremote machines. Thus, programmers of such DSM systems had to be\nvery careful to organize computations such that almost no communica-\ntion occurred at all, defeating much of the point of such an approach.\nThough much research was performed in this space, there was little prac-\ntical impact; nobody builds reliable distributed systems using DSM today.\n47.5\nRemote Procedure Call (RPC)\nWhile OS abstractions turned out to be a poor choice for building dis-\ntributed systems, programming language (PL) abstractions make much\nmore sense. The most dominant abstraction is based on the idea of a re-\nmote procedure call, or RPC for short [BN84]1.\nRemote procedure call packages all have a simple goal: to make the\nprocess of executing code on a remote machine as simple and straight-\nforward as calling a local function. Thus, to a client, a procedure call is\nmade, and some time later, the results are returned. The server simply\ndeﬁnes some routines that it wishes to export. The rest of the magic is\nhandled by the RPC system, which in general has two pieces: a stub gen-\nerator (sometimes called a protocol compiler), and the run-time library.\nWe’ll now take a look at each of these pieces in more detail.\nStub Generator\nThe stub generator’s job is simple: to remove some of the pain of packing\nfunction arguments and results into messages by automating it. Numer-\nous beneﬁts arise: one avoids, by design, the simple mistakes that occur\nin writing such code by hand; further, a stub compiler can perhaps opti-\nmize such code and thus improve performance.\nThe input to such a compiler is simply the set of calls a server wishes\nto export to clients. Conceptually, it could be something as simple as this:\ninterface {\nint func1(int arg1);\nint func2(int arg1, int arg2);\n};\nThe stub generator takes an interface like this and generates a few dif-\nferent pieces of code. For the client, a client stub is generated, which\ncontains each of the functions speciﬁed in the interface; a client program\nwishing to use this RPC service would link with this client stub and call\ninto it in order to make RPCs.\nInternally, each of these functions in the client stub do all of the work\nneeded to perform the remote procedure call. To the client, the code just\n1In modern programming languages, we might instead say remote method invocation\n(RMI), but who likes these languages anyhow, with all of their fancy objects?\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2541,
      "extraction_method": "Direct"
    },
    {
      "page_number": 588,
      "chapter": null,
      "content": "552\nDISTRIBUTED SYSTEMS\nappears as a function call (e.g., the client calls func1(x)); internally, the\ncode in the client stub for func1() does this:\n• Create a message buffer. A message buffer is usually just a con-\ntiguous array of bytes of some size.\n• Pack the needed information into the message buffer. This infor-\nmation includes some kind of identiﬁer for the function to be called,\nas well as all of the arguments that the function needs (e.g., in our\nexample above, one integer for func1). The process of putting all\nof this information into a single contiguous buffer is sometimes re-\nferred to as the marshaling of arguments or the serialization of the\nmessage.\n• Send the message to the destination RPC server. The communi-\ncation with the RPC server, and all of the details required to make\nit operate correctly, are handled by the RPC run-time library, de-\nscribed further below.\n• Wait for the reply. Because function calls are usually synchronous,\nthe call will wait for its completion.\n• Unpack return code and other arguments. If the function just re-\nturns a single return code, this process is straightforward; however,\nmore complex functions might return more complex results (e.g., a\nlist), and thus the stub might need to unpack those as well. This\nstep is also known as unmarshaling or deserialization.\n• Return to the caller. Finally, just return from the client stub back\ninto the client code.\nFor the server, code is also generated. The steps taken on the server\nare as follows:\n• Unpack the message. This step, called unmarshaling or deserial-\nization, takes the information out of the incoming message. The\nfunction identiﬁer and arguments are extracted.\n• Call into the actual function. Finally! We have reached the point\nwhere the remote function is actually executed. The RPC runtime\ncalls into the function speciﬁed by the ID and passes in the desired\narguments.\n• Package the results. The return argument(s) are marshaled back\ninto a single reply buffer.\n• Send the reply. The reply is ﬁnally sent to the caller.\nThere are a few other important issues to consider in a stub compiler.\nThe ﬁrst is complex arguments, i.e., how does one package and send\na complex data structure? For example, when one calls the write()\nsystem call, one passes in three arguments: an integer ﬁle descriptor, a\npointer to a buffer, and a size indicating how many bytes (starting at the\npointer) are to be written. If an RPC package is passed a pointer, it needs\nto be able to ﬁgure out how to interpret that pointer, and perform the\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2593,
      "extraction_method": "Direct"
    },
    {
      "page_number": 589,
      "chapter": null,
      "content": "DISTRIBUTED SYSTEMS\n553\ncorrect action. Usually this is accomplished through either well-known\ntypes (e.g., a buffer t that is used to pass chunks of data given a size,\nwhich the RPC compiler understands), or by annotating the data struc-\ntures with more information, enabling the compiler to know which bytes\nneed to be serialized.\nAnother important issue is the organization of the server with regards\nto concurrency. A simple server just waits for requests in a simple loop,\nand handles each request one at a time. However, as you might have\nguessed, this can be grossly inefﬁcient; if one RPC call blocks (e.g., on\nI/O), server resources are wasted. Thus, most servers are constructed in\nsome sort of concurrent fashion. A common organization is a thread pool.\nIn this organization, a ﬁnite set of threads are created when the server\nstarts; when a message arrives, it is dispatched to one of these worker\nthreads, which then does the work of the RPC call, eventually replying;\nduring this time, a main thread keeps receiving other requests, and per-\nhaps dispatching them to other workers. Such an organization enables\nconcurrent execution within the server, thus increasing its utilization; the\nstandard costs arise as well, mostly in programming complexity, as the\nRPC calls may now need to use locks and other synchronization primi-\ntives in order to ensure their correct operation.\nRun-Time Library\nThe run-time library handles much of the heavy lifting in an RPC system;\nmost performance and reliability issues are handled herein. We’ll now\ndiscuss some of the major challenges in building such a run-time layer.\nOne of the ﬁrst challenges we must overcome is how to locate a re-\nmote service. This problem, of naming, is a common one in distributed\nsystems, and in some sense goes beyond the scope of our current discus-\nsion. The simplest of approaches build on existing naming systems, e.g.,\nhostnames and port numbers provided by current internet protocols. In\nsuch a system, the client must know the hostname or IP address of the\nmachine running the desired RPC service, as well as the port number it is\nusing (a port number is just a way of identifying a particular communica-\ntion activity taking place on a machine, allowing multiple communication\nchannels at once). The protocol suite must then provide a mechanism to\nroute packets to a particular address from any other machine in the sys-\ntem. For a good discussion of naming, read either the Grapevine paper\nor about DNS and name resolution on the Internet, or better yet just read\nthe excellent chapter in Saltzer and Kaashoek’s book [SK09].\nOnce a client knows which server it should talk to for a particular re-\nmote service, the next question is which transport-level protocol should\nRPC be built upon. Speciﬁcally, should the RPC system use a reliable pro-\ntocol such as TCP/IP, or be built upon an unreliable communication layer\nsuch as UDP/IP?\nNaively the choice would seem easy: clearly we would like for a re-\nquest to be reliably delivered to the remote server, and clearly we would\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 3106,
      "extraction_method": "Direct"
    },
    {
      "page_number": 590,
      "chapter": null,
      "content": "554\nDISTRIBUTED SYSTEMS\nlike to reliably receive a reply. Thus we should choose the reliable trans-\nport protocol such as TCP, right?\nUnfortunately, building RPC on top of a reliable communication layer\ncan lead to a major inefﬁciency in performance. Recall from the discus-\nsion above how reliable communication layers work: with acknowledg-\nments plus timeout/retry. Thus, when the client sends an RPC request\nto the server, the server responds with an acknowledgment so that the\ncaller knows the request was received. Similarly, when the server sends\nthe reply to the client, the client acks it so that the server knows it was\nreceived. By building a request/response protocol (such as RPC) on top\nof a reliable communication layer, two “extra” messages are sent.\nFor this reason, many RPC packages are built on top of unreliable com-\nmunication layers, such as UDP. Doing so enables a more efﬁcient RPC\nlayer, but does add the responsibility of providing reliability to the RPC\nsystem. The RPC layer achieves the desired level of responsibility by us-\ning timeout/retry and acknowledgments much like we described above.\nBy using some form of sequence numbering, the communication layer\ncan guarantee that each RPC takes place exactly once (in the case of no\nfailure), or at most once (in the case where failure arises).\nOther Issues\nThere are some other issues an RPC run-time must handle as well. For\nexample, what happens when a remote call takes a long time to com-\nplete? Given our timeout machinery, a long-running remote call might\nappear as a failure to a client, thus triggering a retry, and thus the need\nfor some care here. One solution is to use an explicit acknowledgment\n(from the receiver to sender) when the reply isn’t immediately generated;\nthis lets the client know the server received the request. Then, after some\ntime has passed, the client can periodically ask whether the server is still\nworking on the request; if the server keeps saying “yes”, the client should\nbe happy and continue to wait (after all, sometimes a procedure call can\ntake a long time to ﬁnish executing).\nThe run-time must also handle procedure calls with large arguments,\nlarger than what can ﬁt into a single packet. Some lower-level network\nprotocols provide such sender-side fragmentation (of larger packets into\na set of smaller ones) and receiver-side reassembly (of smaller parts into\none larger logical whole); if not, the RPC run-time may have to implement\nsuch functionality itself. See Birrell and Nelson’s excellent RPC paper for\ndetails [BN84].\nOne issue that many systems handle is that of byte ordering. As you\nmay know, some machines store values in what is known as big endian\nordering, whereas others use little endian ordering. Big endian stores\nbytes (say, of an integer) from most signiﬁcant to least signiﬁcant bits,\nmuch like Arabic numerals; little endian does the opposite. Both are\nequally valid ways of storing numeric information; the question here is\nhow to communicate between machines of different endianness.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 3085,
      "extraction_method": "Direct"
    },
    {
      "page_number": 591,
      "chapter": null,
      "content": "DISTRIBUTED SYSTEMS\n555\nAside: The End-to-End Argument\nThe end-to-end argument makes the case that the highest level in a\nsystem, i.e., usually the application at “the end”, is ultimately the only\nlocale within a layered system where certain functionality can truly be\nimplemented. In their landmark paper, Saltzer et al. argue this through\nan excellent example: reliable ﬁle transfer between two machines. If you\nwant to transfer a ﬁle from machine A to machine B, and make sure that\nthe bytes that end up on B are exactly the same as those that began on\nA, you must have an “end-to-end” check of this; lower-level reliable ma-\nchinery, e.g., in the network or disk, provides no such guarantee.\nThe contrast is an approach which tries to solve the reliable-ﬁle-\ntransfer problem by adding reliability to lower layers of the system. For\nexample, say we build a reliable communication protocol and use it to\nbuild our reliable ﬁle transfer. The communication protocol guarantees\nthat every byte sent by a sender will be received in order by the receiver,\nsay using timeout/retry, acknowledgments, and sequence numbers. Un-\nfortunately, using such a protocol does not a reliable ﬁle transfer make;\nimagine the bytes getting corrupted in sender memory before the com-\nmunication even takes place, or something bad happening when the re-\nceiver writes the data to disk. In those cases, even though the bytes were\ndelivered reliably across the network, our ﬁle transfer was ultimately\nnot reliable. To build a reliable ﬁle transfer, one must include end-to-\nend checks of reliability, e.g., after the entire transfer is complete, read\nback the ﬁle on the receiver disk, compute a checksum, and compare that\nchecksum to that of the ﬁle on the sender.\nThe corollary to this maxim is that sometimes having lower layers pro-\nvide extra functionality can indeed improve system performance or oth-\nerwise optimize a system. Thus, you should not rule out having such\nmachinery at a lower-level in a system; rather, you should carefully con-\nsider the utility of such machinery, given its eventual usage in an overall\nsystem or application.\nRPC packages often handle this by providing a well-deﬁned endian-\nness within their message formats. In Sun’s RPC package, the XDR (eX-\nternal Data Representation) layer provides this functionality. If the ma-\nchine sending or receiving a message matches the endianness of XDR,\nmessages are just sent and received as expected. If, however, the machine\ncommunicating has a different endianness, each piece of information in\nthe message must be converted. Thus, the difference in endianness can\nhave a small performance cost.\nA ﬁnal issue is whether to expose the asynchronous nature of com-\nmunication to clients, thus enabling some performance optimizations.\nSpeciﬁcally, typical RPCs are made synchronously, i.e., when a client\nissues the procedure call, it must wait for the procedure call to return\nbefore continuing. Because this wait can be long, and because the client\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 3044,
      "extraction_method": "Direct"
    },
    {
      "page_number": 592,
      "chapter": null,
      "content": "556\nDISTRIBUTED SYSTEMS\nmay have other work it could be doing, some RPC packages enable you\nto invoke an RPC asynchronously. When an asynchronous RPC is is-\nsued, the RPC package sends the request and returns immediately; the\nclient is then free to do other work, such as call other RPCs or other use-\nful computation. The client at some point will want to see the results of\nthe asynchronous RPC; it thus calls back into the RPC layer, telling it to\nwait for outstanding RPCs to complete, at which point return arguments\ncan be accessed.\n47.6\nSummary\nWe have seen the introduction of a new topic, distributed systems, and\nits major issue: how to handle failure which is now a commonplace event.\nAs they say inside of Google, when you have just your desktop machine,\nfailure is rare; when you’re in a data center with thousands of machines,\nfailure is happening all the time. The key to any distributed system is\nhow you deal with that failure.\nWe have also seen that communication forms the heart of any dis-\ntributed system. A common abstraction of that communication is found\nin remote procedure call (RPC), which enables clients to make remote\ncalls on servers; the RPC package handles all of the gory details, includ-\ning timeout/retry and acknowledgment, in order to deliver a service that\nclosely mirrors a local procedure call.\nThe best way to really understand an RPC package is of course to use\none yourself. Sun’s RPC system, using the stub compiler rpcgen, is a\ncommon one, and is widely available on systems today, including Linux.\nTry it out, and see what all the fuss is about.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1640,
      "extraction_method": "Direct"
    },
    {
      "page_number": 593,
      "chapter": null,
      "content": "DISTRIBUTED SYSTEMS\n557\nReferences\n[A70] “The ALOHA System – Another Alternative for Computer Communications”\nNorman Abramson\nThe 1970 Fall Joint Computer Conference\nThe ALOHA network pioneered some basic concepts in networking, including exponential back-off and\nretransmit, which formed the basis for communication in shared-bus Ethernet networks for years.\n[BN84] “Implementing Remote Procedure Calls”\nAndrew D. Birrell, Bruce Jay Nelson\nACM TOCS, Volume 2:1, February 1984\nThe foundational RPC system upon which all others build. Yes, another pioneering effort from our\nfriends at Xerox PARC.\n[MK09] “The Effectiveness of Checksums for Embedded Control Networks”\nTheresa C. Maxino and Philip J. Koopman\nIEEE Transactions on Dependable and Secure Computing, 6:1, January ’09\nA nice overview of basic checksum machinery and some performance and robustness comparisons be-\ntween them.\n[LH89] “Memory Coherence in Shared Virtual Memory Systems”\nKai Li and Paul Hudak\nACM TOCS, 7:4, November 1989\nThe introduction of software-based shared memory via virtual memory. An intriguing idea for sure, but\nnot a lasting or good one in the end.\n[SK09] “Principles of Computer System Design”\nJerome H. Saltzer and M. Frans Kaashoek\nMorgan-Kaufmann, 2009\nAn excellent book on systems, and a must for every bookshelf. One of the few terriﬁc discussions on\nnaming we’ve seen.\n[SRC84] “End-To-End Arguments in System Design”\nJerome H. Saltzer, David P. Reed, David D. Clark\nACM TOCS, 2:4, November 1984\nA beautiful discussion of layering, abstraction, and where functionality must ultimately reside in com-\nputer systems.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 1649,
      "extraction_method": "Direct"
    },
    {
      "page_number": 594,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 595,
      "chapter": null,
      "content": "48\nSun’s Network File System (NFS)\nOne of the ﬁrst uses of distributed client/server computing was in the\nrealm of distributed ﬁle systems. In such an environment, there are a\nnumber of client machines and one server (or a few); the server stores the\ndata on its disks, and clients request data through well-formed protocol\nmessages. Figure 48.1 depicts the basic setup.\nClient 0\nClient 1\nClient 2\nClient 3\nServer\nNetwork\nFigure 48.1: A Generic Client/Server System\nAs you can see from the picture, the server has the disks, and clients\nsend messages network to access their directories and ﬁles on those disks.\nWhy do we bother with this arrangement? (i.e., why don’t we just let\nclients use their local disks?) Well, primarily this setup allows for easy\nsharing of data across clients. Thus, if you access a ﬁle on one machine\n(Client 0) and then later use another (Client 2), you will have the same\nview of the ﬁle system. Your data is naturally shared across these dif-\nferent machines. A secondary beneﬁt is centralized administration; for\nexample, backing up ﬁles can be done from the few server machines in-\nstead of from the multitude of clients. Another advantage could be secu-\nrity; having all servers in a locked machine room prevents certain types\nof problems from arising.\n559\n",
      "content_length": 1291,
      "extraction_method": "Direct"
    },
    {
      "page_number": 596,
      "chapter": null,
      "content": "560\nSUN’S NETWORK FILE SYSTEM (NFS)\nCRUX: HOW TO BUILD A DISTRIBUTED FILE SYSTEM\nHow do you build a distributed ﬁle system? What are the key aspects\nto think about? What is easy to get wrong? What can we learn from\nexisting systems?\n48.1\nA Basic Distributed File System\nWe now will study the architecture of a simpliﬁed distributed ﬁle sys-\ntem. A simple client/server distributed ﬁle system has more components\nthan the ﬁle systems we have studied so far. On the client side, there are\nclient applications which access ﬁles and directories through the client-\nside ﬁle system. A client application issues system calls to the client-side\nﬁle system (such as open(), read(), write(), close(), mkdir(),\netc.) in order to access ﬁles which are stored on the server. Thus, to client\napplications, the ﬁle system does not appear to be any different than a lo-\ncal (disk-based) ﬁle system, except perhaps for performance; in this way,\ndistributed ﬁle systems provide transparent access to ﬁles, an obvious\ngoal; after all, who would want to use a ﬁle system that required a differ-\nent set of APIs or otherwise was a pain to use?\nThe role of the client-side ﬁle system is to execute the actions needed\nto service those system calls. For example, if the client issues a read()\nrequest, the client-side ﬁle system may send a message to the server-side\nﬁle system (or, more commonly, the ﬁle server) to read a particular block;\nthe ﬁle server will then read the block from disk (or its own in-memory\ncache), and send a message back to the client with the requested data.\nThe client-side ﬁle system will then copy the data into the user buffer\nsupplied to the read() system call and thus the request will complete.\nNote that a subsequent read() of the same block on the client may be\ncached in client memory or on the client’s disk even; in the best such case,\nno network trafﬁc need be generated.\nClient Application\nClient-side File System\nNetworking Layer\nFile Server\nNetworking Layer\nDisks\nFigure 48.2: Distributed File System Architecture\nFrom this simple overview, you should get a sense that there are two\nimportant pieces of software in a client/server distributed ﬁle system: the\nclient-side ﬁle system and the ﬁle server. Together their behavior deter-\nmines the behavior of the distributed ﬁle system. Now it’s time to study\none particular system: Sun’s Network File System (NFS).\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2428,
      "extraction_method": "Direct"
    },
    {
      "page_number": 597,
      "chapter": null,
      "content": "SUN’S NETWORK FILE SYSTEM (NFS)\n561\nASIDE: WHY SERVERS CRASH\nBefore getting into the details of the NFSv2 protocol, you might be\nwondering: why do servers crash? Well, as you might guess, there are\nplenty of reasons. Servers may simply suffer from a power outage (tem-\nporarily); only when power is restored can the machines be restarted.\nServers are often comprised of hundreds of thousands or even millions\nof lines of code; thus, they have bugs (even good software has a few\nbugs per hundred or thousand lines of code), and thus they eventually\nwill trigger a bug that will cause them to crash. They also have memory\nleaks; even a small memory leak will cause a system to run out of mem-\nory and crash. And, ﬁnally, in distributed systems, there is a network\nbetween the client and the server; if the network acts strangely (for ex-\nample, if it becomes partitioned and clients and servers are working but\ncannot communicate), it may appear as if a remote machine has crashed,\nbut in reality it is just not currently reachable through the network.\n48.2\nOn To NFS\nOne of the earliest and quite successful distributed systems was devel-\noped by Sun Microsystems, and is known as the Sun Network File Sys-\ntem (or NFS) [S86]. In deﬁning NFS, Sun took an unusual approach: in-\nstead of building a proprietary and closed system, Sun instead developed\nan open protocol which simply speciﬁed the exact message formats that\nclients and servers would use to communicate. Different groups could\ndevelop their own NFS servers and thus compete in an NFS marketplace\nwhile preserving interoperability. It worked: today there are many com-\npanies that sell NFS servers (including Oracle/Sun, NetApp [HLM94],\nEMC, IBM, and others), and the widespread success of NFS is likely at-\ntributed to this “open market” approach.\n48.3\nFocus: Simple and Fast Server Crash Recovery\nIn this chapter, we will discuss the classic NFS protocol (version 2,\na.k.a. NFSv2), which was the standard for many years; small changes\nwere made in moving to NFSv3, and larger-scale protocol changes were\nmade in moving to NFSv4. However, NFSv2 is both wonderful and frus-\ntrating and thus serves as our focus.\nIn NFSv2, the main goal in the design of the protocol was simple and\nfast server crash recovery. In a multiple-client, single-server environment,\nthis goal makes a great deal of sense; any minute that the server is down\n(or unavailable) makes all the client machines (and their users) unhappy\nand unproductive. Thus, as the server goes, so goes the entire system.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2577,
      "extraction_method": "Direct"
    },
    {
      "page_number": 598,
      "chapter": null,
      "content": "562\nSUN’S NETWORK FILE SYSTEM (NFS)\n48.4\nKey To Fast Crash Recovery: Statelessness\nThis simple goal is realized in NFSv2 by designing what we refer to\nas a stateless protocol. The server, by design, does not keep track of any-\nthing about what is happening at each client. For example, the server\ndoes not know which clients are caching which blocks, or which ﬁles are\ncurrently open at each client, or the current ﬁle pointer position for a ﬁle,\netc. Simply put, the server does not track anything about what clients are\ndoing; rather, the protocol is designed to deliver in each protocol request\nall the information that is needed in order to complete the request. If it\ndoesn’t now, this stateless approach will make more sense as we discuss\nthe protocol in more detail below.\nFor an example of a stateful (not stateless) protocol, consider the open()\nsystem call. Given a pathname, open() returns a ﬁle descriptor (an inte-\nger). This descriptor is used on subsequent read() or write() requests\nto access various ﬁle blocks, as in this application code (note that proper\nerror checking of the system calls is omitted for space reasons):\nchar buffer[MAX];\nint fd = open(\"foo\", O_RDONLY); // get descriptor \"fd\"\nread(fd, buffer, MAX);\n// read MAX bytes from foo (via fd)\nread(fd, buffer, MAX);\n// read MAX bytes from foo\n...\nread(fd, buffer, MAX);\n// read MAX bytes from foo\nclose(fd);\n// close file\nFigure 48.3: Client Code: Reading From A File\nNow imagine that the client-side ﬁle system opens the ﬁle by sending\na protocol message to the server saying “open the ﬁle ’foo’ and give me\nback a descriptor”. The ﬁle server then opens the ﬁle locally on its side\nand sends the descriptor back to the client. On subsequent reads, the\nclient application uses that descriptor to call the read() system call; the\nclient-side ﬁle system then passes the descriptor in a message to the ﬁle\nserver, saying “read some bytes from the ﬁle that is referred to by the\ndescriptor I am passing you here”.\nIn this example, the ﬁle descriptor is a piece of shared state between\nthe client and the server (Ousterhout calls this distributed state [O91]).\nShared state, as we hinted above, complicates crash recovery. Imagine\nthe server crashes after the ﬁrst read completes, but before the client\nhas issued the second one. After the server is up and running again,\nthe client then issues the second read. Unfortunately, the server has no\nidea to which ﬁle fd is referring; that information was ephemeral (i.e.,\nin memory) and thus lost when the server crashed. To handle this situa-\ntion, the client and server would have to engage in some kind of recovery\nprotocol, where the client would make sure to keep enough information\naround in its memory to be able to tell the server what it needs to know\n(in this case, that ﬁle descriptor fd refers to ﬁle foo).\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2887,
      "extraction_method": "Direct"
    },
    {
      "page_number": 599,
      "chapter": null,
      "content": "SUN’S NETWORK FILE SYSTEM (NFS)\n563\nIt gets even worse when you consider the fact that a stateful server has\nto deal with client crashes. Imagine, for example, a client that opens a ﬁle\nand then crashes. The open() uses up a ﬁle descriptor on the server; how\ncan the server know it is OK to close a given ﬁle? In normal operation, a\nclient would eventually call close() and thus inform the server that the\nﬁle should be closed. However, when a client crashes, the server never\nreceives a close(), and thus has to notice the client has crashed in order\nto close the ﬁle.\nFor these reasons, the designers of NFS decided to pursue a stateless\napproach: each client operation contains all the information needed to\ncomplete the request. No fancy crash recovery is needed; the server just\nstarts running again, and a client, at worst, might have to retry a request.\n48.5\nThe NFSv2 Protocol\nWe thus arrive at the NFSv2 protocol deﬁnition. Our problem state-\nment is simple:\nTHE CRUX: HOW TO DEFINE A STATELESS FILE PROTOCOL\nHow can we deﬁne the network protocol to enable stateless operation?\nClearly, stateful calls like open() can’t be a part of the discussion (as it\nwould require the server to track open ﬁles); however, the client appli-\ncation will want to call open(), read(), write(), close() and other\nstandard API calls to access ﬁles and directories. Thus, as a reﬁned ques-\ntion, how do we deﬁne the protocol to both be stateless and support the\nPOSIX ﬁle system API?\nOne key to understanding the design of the NFS protocol is under-\nstanding the ﬁle handle. File handles are used to uniquely describe the\nﬁle or directory a particular operation is going to operate upon; thus,\nmany of the protocol requests include a ﬁle handle.\nYou can think of a ﬁle handle as having three important components: a\nvolume identiﬁer, an inode number, and a generation number; together, these\nthree items comprise a unique identiﬁer for a ﬁle or directory that a client\nwishes to access. The volume identiﬁer informs the server which ﬁle sys-\ntem the request refers to (an NFS server can export more than one ﬁle\nsystem); the inode number tells the server which ﬁle within that partition\nthe request is accessing. Finally, the generation number is needed when\nreusing an inode number; by incrementing it whenever an inode num-\nber is reused, the server ensures that a client with an old ﬁle handle can’t\naccidentally access the newly-allocated ﬁle.\nHere is a summary of some of the important pieces of the protocol; the\nfull protocol is available elsewhere (see Callaghan’s book for an excellent\nand detailed overview of NFS [C00]).\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2663,
      "extraction_method": "Direct"
    },
    {
      "page_number": 600,
      "chapter": null,
      "content": "564\nSUN’S NETWORK FILE SYSTEM (NFS)\nNFSPROC_GETATTR\nexpects: file handle\nreturns: attributes\nNFSPROC_SETATTR\nexpects: file handle, attributes\nreturns: nothing\nNFSPROC_LOOKUP\nexpects: directory file handle, name of file/directory to look up\nreturns: file handle\nNFSPROC_READ\nexpects: file handle, offset, count\nreturns: data, attributes\nNFSPROC_WRITE\nexpects: file handle, offset, count, data\nreturns: attributes\nNFSPROC_CREATE\nexpects: directory file handle, name of file, attributes\nreturns: nothing\nNFSPROC_REMOVE\nexpects: directory file handle, name of file to be removed\nreturns: nothing\nNFSPROC_MKDIR\nexpects: directory file handle, name of directory, attributes\nreturns: file handle\nNFSPROC_RMDIR\nexpects: directory file handle, name of directory to be removed\nreturns: nothing\nNFSPROC_READDIR\nexpects: directory handle, count of bytes to read, cookie\nreturns: directory entries, cookie (to get more entries)\nFigure 48.4: The NFS Protocol: Examples\nWe brieﬂy highlight the important components of the protocol. First,\nthe LOOKUP protocol message is used to obtain a ﬁle handle, which is\nthen subsequently used to access ﬁle data. The client passes a directory\nﬁle handle and name of a ﬁle to look up, and the handle to that ﬁle (or\ndirectory) plus its attributes are passed back to the client from the server.\nFor example, assume the client already has a directory ﬁle handle for\nthe root directory of a ﬁle system (/) (indeed, this would be obtained\nthrough the NFS mount protocol, which is how clients and servers ﬁrst\nare connected together; we do not discuss the mount protocol here for\nsake of brevity). If an application running on the client opens the ﬁle\n/foo.txt, the client-side ﬁle system sends a lookup request to the server,\npassing it the root ﬁle handle and the name foo.txt; if successful, the\nﬁle handle (and attributes) for foo.txt will be returned.\nIn case you are wondering, attributes are just the metadata that the ﬁle\nsystem tracks about each ﬁle, including ﬁelds such as ﬁle creation time,\nlast modiﬁcation time, size, ownership and permissions information, and\nso forth, i.e., the same type of information that you would get back if you\ncalled stat() on a ﬁle.\nOnce a ﬁle handle is available, the client can issue READ and WRITE\nprotocol messages on a ﬁle to read or write the ﬁle, respectively. The\nREAD protocol message requires the protocol to pass along the ﬁle handle\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2451,
      "extraction_method": "Direct"
    },
    {
      "page_number": 601,
      "chapter": null,
      "content": "SUN’S NETWORK FILE SYSTEM (NFS)\n565\nof the ﬁle along with the offset within the ﬁle and number of bytes to read.\nThe server then will be able to issue the read (after all, the handle tells the\nserver which volume and which inode to read from, and the offset and\ncount tells it which bytes of the ﬁle to read) and return the data to the\nclient (or an error if there was a failure). WRITE is handled similarly,\nexcept the data is passed from the client to the server, and just a success\ncode is returned.\nOne last interesting protocol message is the GETATTR request; given a\nﬁle handle, it simply fetches the attributes for that ﬁle, including the last\nmodiﬁed time of the ﬁle. We will see why this protocol request is impor-\ntant in NFSv2 below when we discuss caching (can you guess why?).\n48.6\nFrom Protocol to Distributed File System\nHopefully you are now getting some sense of how this protocol is\nturned into a ﬁle system across the client-side ﬁle system and the ﬁle\nserver. The client-side ﬁle system tracks open ﬁles, and generally trans-\nlates application requests into the relevant set of protocol messages. The\nserver simply responds to each protocol message, each of which has all\nthe information needed to complete request.\nFor example, let us consider a simple application which reads a ﬁle.\nIn the diagram (Figure 48.1), we show what system calls the application\nmakes, and what the client-side ﬁle system and ﬁle server do in respond-\ning to such calls.\nA few comments about the ﬁgure. First, notice how the client tracks all\nrelevant state for the ﬁle access, including the mapping of the integer ﬁle\ndescriptor to an NFS ﬁle handle as well as the current ﬁle pointer. This\nenables the client to turn each read request (which you may have noticed\ndo not specify the offset to read from explicitly) into a properly-formatted\nread protocol message which tells the server exactly which bytes from\nthe ﬁle to read. Upon a successful read, the client updates the current\nﬁle position; subsequent reads are issued with the same ﬁle handle but a\ndifferent offset.\nSecond, you may notice where server interactions occur. When the ﬁle\nis opened for the ﬁrst time, the client-side ﬁle system sends a LOOKUP\nrequest message. Indeed, if a long pathname must be traversed (e.g.,\n/home/remzi/foo.txt), the client would send three LOOKUPs: one\nto look up home in the directory /, one to look up remzi in home, and\nﬁnally one to look up foo.txt in remzi.\nThird, you may notice how each server request has all the information\nneeded to complete the request in its entirety. This design point is critical\nto be able to gracefully recover from server failure, as we will now discuss\nin more detail; it ensures that the server does not need state to be able to\nrespond to the request.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2822,
      "extraction_method": "Direct"
    },
    {
      "page_number": 602,
      "chapter": null,
      "content": "566\nSUN’S NETWORK FILE SYSTEM (NFS)\nClient\nServer\nfd = open(”/foo”, ...);\nSend LOOKUP (rootdir FH, ”foo”)\nReceive LOOKUP request\nlook for ”foo” in root dir\nreturn foo’s FH + attributes\nReceive LOOKUP reply\nallocate ﬁle desc in open ﬁle table\nstore foo’s FH in table\nstore current ﬁle position (0)\nreturn ﬁle descriptor to application\nread(fd, buffer, MAX);\nIndex into open ﬁle table with fd\nget NFS ﬁle handle (FH)\nuse current ﬁle position as offset\nSend READ (FH, offset=0, count=MAX)\nReceive READ request\nuse FH to get volume/inode num\nread inode from disk (or cache)\ncompute block location (using offset)\nread data from disk (or cache)\nreturn data to client\nReceive READ reply\nupdate ﬁle position (+bytes read)\nset current ﬁle position = MAX\nreturn data/error code to app\nread(fd, buffer, MAX);\nSame except offset=MAX and set current ﬁle position = 2*MAX\nread(fd, buffer, MAX);\nSame except offset=2*MAX and set current ﬁle position = 3*MAX\nclose(fd);\nJust need to clean up local structures\nFree descriptor ”fd” in open ﬁle table\n(No need to talk to server)\nTable 48.1: Reading A File: Client-side And File Server Actions\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1171,
      "extraction_method": "Direct"
    },
    {
      "page_number": 603,
      "chapter": null,
      "content": "SUN’S NETWORK FILE SYSTEM (NFS)\n567\nTIP: IDEMPOTENCY IS POWERFUL\nIdempotency is a useful property when building reliable systems. When\nan operation can be issued more than once, it is much easier to handle\nfailure of the operation; you can just retry it. If an operation is not idem-\npotent, life becomes more difﬁcult.\n48.7\nHandling Server Failure with Idempotent Operations\nWhen a client sends a message to the server, it sometimes does not re-\nceive a reply. There are many possible reasons for this failure to respond.\nIn some cases, the message may be dropped by the network; networks do\nlose messages, and thus either the request or the reply could be lost and\nthus the client would never receive a response.\nIt is also possible that the server has crashed, and thus is not currently\nresponding to messages. After a bit, the server will be rebooted and start\nrunning again, but in the meanwhile all requests have been lost. In all of\nthese cases, clients are left with a question: what should they do when\nthe server does not reply in a timely manner?\nIn NFSv2, a client handles all of these failures in a single, uniform, and\nelegant way: it simply retries the request. Speciﬁcally, after sending the\nrequest, the client sets a timer to go off after a speciﬁed time period. If a\nreply is received before the timer goes off, the timer is canceled and all is\nwell. If, however, the timer goes off before any reply is received, the client\nassumes the request has not been processed and resends it. If the server\nreplies, all is well and the client has neatly handled the problem.\nThe ability of the client to simply retry the request (regardless of what\ncaused the failure) is due to an important property of most NFS requests:\nthey are idempotent. An operation is called idempotent when the effect\nof performing the operation multiple times is equivalent to the effect of\nperforming the operating a single time. For example, if you store a value\nto a memory location three times, it is the same as doing so once; thus\n“store value to memory” is an idempotent operation. If, however, you in-\ncrement a counter three times, it results in a different amount than doing\nso just once; thus, “increment counter” is not idempotent. More gener-\nally, any operation that just reads data is obviously idempotent; an oper-\nation that updates data must be more carefully considered to determine\nif it has this property.\nThe heart of the design of crash recovery in NFS is the idempotency\nof most common operations. LOOKUP and READ requests are trivially\nidempotent, as they only read information from the ﬁle server and do not\nupdate it. More interestingly, WRITE requests are also idempotent. If,\nfor example, a WRITE fails, the client can simply retry it. The WRITE\nmessage contains the data, the count, and (importantly) the exact offset\nto write the data to. Thus, it can be repeated with the knowledge that the\noutcome of multiple writes is the same as the outcome of a single one.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 3021,
      "extraction_method": "Direct"
    },
    {
      "page_number": 604,
      "chapter": null,
      "content": "568\nSUN’S NETWORK FILE SYSTEM (NFS)\nCase 1: Request Lost\nClient\n[send request]\nServer\n(no mesg)\nCase 2: Server Down\nClient\n[send request]\nServer\n(down)\nCase 3: Reply lost on way back from Server\nClient\n[send request]\nServer\n[recv request]\n[handle request]\n[send reply]\nFigure 48.5: The Three Types of Loss\nIn this way, the client can handle all timeouts in a uniﬁed way. If a\nWRITE request was simply lost (Case 1 above), the client will retry it, the\nserver will perform the write, and all will be well. The same will happen\nif the server happened to be down while the request was sent, but back\nup and running when the second request is sent, and again all works\nas desired (Case 2). Finally, the server may in fact receive the WRITE\nrequest, issue the write to its disk, and send a reply. This reply may get\nlost (Case 3), again causing the client to re-send the request. When the\nserver receives the request again, it will simply do the exact same thing:\nwrite the data to disk and reply that it has done so. If the client this time\nreceives the reply, all is again well, and thus the client has handled both\nmessage loss and server failure in a uniform manner. Neat!\nA small aside: some operations are hard to make idempotent. For\nexample, when you try to make a directory that already exists, you are\ninformed that the mkdir request has failed. Thus, in NFS, if the ﬁle server\nreceives a MKDIR protocol message and executes it successfully but the\nreply is lost, the client may repeat it and encounter that failure when in\nfact the operation at ﬁrst succeeded and then only failed on the retry.\nThus, life is not perfect.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1675,
      "extraction_method": "Direct"
    },
    {
      "page_number": 605,
      "chapter": null,
      "content": "SUN’S NETWORK FILE SYSTEM (NFS)\n569\nTIP: PERFECT IS THE ENEMY OF THE GOOD (VOLTAIRE’S LAW)\nEven when you design a beautiful system, sometimes all the corner cases\ndon’t work out exactly as you might like. Take the mkdir example above;\none could redesign mkdir to have different semantics, thus making it\nidempotent (think about how you might do so); however, why bother?\nThe NFS design philosophy covers most of the important cases, and over-\nall makes the system design clean and simple with regards to failure.\nThus, accepting that life isn’t perfect and still building the system is a sign\nof good engineering. Apparently, this wisdom is attributed to Voltaire,\nfor saying “... a wise Italian says that the best is the enemy of the good”\n[V72], and thus we call it Voltaire’s Law.\n48.8\nImproving Performance: Client-side Caching\nDistributed ﬁle systems are good for a number of reasons, but sending\nall read and write requests across the network can lead to a big perfor-\nmance problem: the network generally isn’t that fast, especially as com-\npared to local memory or disk. Thus, another problem: how can we im-\nprove the performance of a distributed ﬁle system?\nThe answer, as you might guess from reading the big bold words in\nthe sub-heading above, is client-side caching. The NFS client-side ﬁle\nsystem caches ﬁle data (and metadata) that it has read from the server in\nclient memory. Thus, while the ﬁrst access is expensive (i.e., it requires\nnetwork communication), subsequent accesses are serviced quite quickly\nout of client memory.\nThe cache also serves as a temporary buffer for writes. When a client\napplication ﬁrst writes to a ﬁle, the client buffers the data in client mem-\nory (in the same cache as the data it read from the ﬁle server) before writ-\ning the data out to the server. Such write buffering is useful because it de-\ncouples application write() latency from actual write performance, i.e.,\nthe application’s call to write() succeeds immediately (and just puts\nthe data in the client-side ﬁle system’s cache); only later does the data get\nwritten out to the ﬁle server.\nThus, NFS clients cache data and performance is usually great and\nwe are done, right? Unfortunately, not quite. Adding caching into any\nsort of system with multiple client caches introduces a big and interesting\nchallenge which we will refer to as the cache consistency problem.\n48.9\nThe Cache Consistency Problem\nThe cache consistency problem is best illustrated with two clients and\na single server. Imagine client C1 reads a ﬁle F, and keeps a copy of the\nﬁle in its local cache. Now imagine a different client, C2, overwrites the\nﬁle F, thus changing its contents; let’s call the new version of the ﬁle F\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2751,
      "extraction_method": "Direct"
    },
    {
      "page_number": 606,
      "chapter": null,
      "content": "570\nSUN’S NETWORK FILE SYSTEM (NFS)\nC1\ncache: F[v1]\nC2\ncache: F[v2]\nC3\ncache: empty\nServer S\ndisk: F[v1] at first\n      F[v2] eventually\nFigure 48.6: The Cache Consistency Problem\n(version 2), or F[v2] and the old version F[v1] so we can keep the two\ndistinct (but of course the ﬁle has the same name, just different contents).\nFinally, there is a third client, C3, which has not yet accessed the ﬁle F.\nYou can probably see the problem that is upcoming (Figure 48.6). In\nfact, there are two subproblems. The ﬁrst subproblem is that the client C2\nmay buffer its writes in its cache for a time before propagating them to the\nserver; in this case, while F[v2] sits in C2’s memory, any access of F from\nanother client (say C3) will fetch the old version of the ﬁle (F[v1]). Thus,\nby buffering writes at the client, other clients may get stale versions of the\nﬁle, which may be undesirable; indeed, imagine the case where you log\ninto machine C2, update F, and then log into C3 and try to read the ﬁle,\nonly to get the old copy! Certainly this could be frustrating. Thus, let us\ncall this aspect of the cache consistency problem update visibility; when\ndo updates from one client become visible at other clients?\nThe second subproblem of cache consistency is a stale cache; in this\ncase, C2 has ﬁnally ﬂushed its writes to the ﬁle server, and thus the server\nhas the latest version (F[v2]). However, C1 still has F[v1] in its cache; if a\nprogram running on C1 reads ﬁle F, it will get a stale version (F[v1]) and\nnot the most recent copy (F[v2]), which is (often) undesirable.\nNFSv2 implementations solve these cache consistency problems in two\nways. First, to address update visibility, clients implement what is some-\ntimes called ﬂush-on-close (a.k.a., close-to-open) consistency semantics;\nspeciﬁcally, when a ﬁle is written to and subsequently closed by a client\napplication, the client ﬂushes all updates (i.e., dirty pages in the cache)\nto the server. With ﬂush-on-close consistency, NFS ensures that a subse-\nquent open from another node will see the latest ﬁle version.\nSecond, to address the stale-cache problem, NFSv2 clients ﬁrst check\nto see whether a ﬁle has changed before using its cached contents. Speciﬁ-\ncally, when opening a ﬁle, the client-side ﬁle system will issue a GETATTR\nrequest to the server to fetch the ﬁle’s attributes. The attributes, impor-\ntantly, include information as to when the ﬁle was last modiﬁed on the\nserver; if the time-of-modiﬁcation is more recent than the time that the\nﬁle was fetched into the client cache, the client invalidates the ﬁle, thus\nremoving it from the client cache and ensuring that subsequent reads will\ngo to the server and retrieve the latest version of the ﬁle. If, on the other\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2789,
      "extraction_method": "Direct"
    },
    {
      "page_number": 607,
      "chapter": null,
      "content": "SUN’S NETWORK FILE SYSTEM (NFS)\n571\nhand, the client sees that it has the latest version of the ﬁle, it will go\nahead and use the cached contents, thus increasing performance.\nWhen the original team at Sun implemented this solution to the stale-\ncache problem, they realized a new problem; suddenly, the NFS server\nwas ﬂooded with GETATTR requests. A good engineering principle to\nfollow is to design for the common case, and to make it work well; here,\nalthough the common case was that a ﬁle was accessed only from a sin-\ngle client (perhaps repeatedly), the client always had to send GETATTR\nrequests to the server to make sure no one else had changed the ﬁle. A\nclient thus bombards the server, constantly asking “has anyone changed\nthis ﬁle?”, when most of the time no one had.\nTo remedy this situation (somewhat), an attribute cache was added\nto each client. A client would still validate a ﬁle before accessing it, but\nmost often would just look in the attribute cache to fetch the attributes.\nThe attributes for a particular ﬁle were placed in the cache when the ﬁle\nwas ﬁrst accessed, and then would timeout after a certain amount of time\n(say 3 seconds). Thus, during those three seconds, all ﬁle accesses would\ndetermine that it was OK to use the cached ﬁle and thus do so with no\nnetwork communication with the server.\n48.10\nAssessing NFS Cache Consistency\nA few ﬁnal words about NFS cache consistency. The ﬂush-on-close be-\nhavior was added to “make sense”, but introduced a certain performance\nproblem. Speciﬁcally, if a temporary or short-lived ﬁle was created on a\nclient and then soon deleted, it would still be forced to the server. A more\nideal implementation might keep such short-lived ﬁles in memory until\nthey are deleted and thus remove the server interaction entirely, perhaps\nincreasing performance.\nMore importantly, the addition of an attribute cache into NFS made\nit very hard to understand or reason about exactly what version of a ﬁle\none was getting. Sometimes you would get the latest version; sometimes\nyou would get an old version simply because your attribute cache hadn’t\nyet timed out and thus the client was happy to give you what was in\nclient memory. Although this was ﬁne most of the time, it would (and\nstill does!) occasionally lead to odd behavior.\nAnd thus we have described the oddity that is NFS client caching.\nIt serves as an interesting example where details of an implementation\nserve to deﬁne user-observable semantics, instead of the other way around.\n48.11\nImplications on Server-Side Write Buffering\nOur focus so far has been on client caching, and that is where most\nof the interesting issues arise. However, NFS servers tend to be well-\nequipped machines with a lot of memory too, and thus they have caching\nconcerns as well.\nWhen data (and metadata) is read from disk, NFS\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2873,
      "extraction_method": "Direct"
    },
    {
      "page_number": 608,
      "chapter": null,
      "content": "572\nSUN’S NETWORK FILE SYSTEM (NFS)\nservers will keep it in memory, and subsequent reads of said data (and\nmetadata) will not go to disk, a potential (small) boost in performance.\nMore intriguing is the case of write buffering. NFS servers absolutely\nmay not return success on a WRITE protocol request until the write has\nbeen forced to stable storage (e.g., to disk or some other persistent device).\nWhile they can place a copy of the data in server memory, returning suc-\ncess to the client on a WRITE protocol request could result in incorrect\nbehavior; can you ﬁgure out why?\nThe answer lies in our assumptions about how clients handle server\nfailure. Imagine the following sequence of writes as issued by a client:\nwrite(fd, a_buffer, size); // fill first block with a’s\nwrite(fd, b_buffer, size); // fill second block with b’s\nwrite(fd, c_buffer, size); // fill third block with c’s\nThese writes overwrite the three blocks of a ﬁle with a block of a’s,\nthen b’s, and then c’s. Thus, if the ﬁle initially looked like this:\nxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\nyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy\nzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\nWe might expect the ﬁnal result after these writes to be like this, with the\nx’s, y’s, and z’s, would be overwritten with a’s, b’s, and c’s, respectively.\naaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\nbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb\ncccccccccccccccccccccccccccccccccccccccccccccccccccccccccccc\nNow let’s assume for the sake of the example that these three client\nwrites were issued to the server as three distinct WRITE protocol mes-\nsages. Assume the ﬁrst WRITE message is received by the server and\nissued to the disk, and the client informed of its success. Now assume\nthe second write is just buffered in memory, and the server also reports\nit success to the client before forcing it to disk; unfortunately, the server\ncrashes before writing it to disk. The server quickly restarts and receives\nthe third write request, which also succeeds.\nThus, to the client, all the requests succeeded, but we are surprised\nthat the ﬁle contents look like this:\naaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\nyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy <--- oops\ncccccccccccccccccccccccccccccccccccccccccccccccccccccccccccc\nYikes! Because the server told the client that the second write was\nsuccessful before committing it to disk, an old chunk is left in the ﬁle,\nwhich, depending on the application, might be catastrophic.\nTo avoid this problem, NFS servers must commit each write to stable\n(persistent) storage before informing the client of success; doing so en-\nables the client to detect server failure during a write, and thus retry until\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2871,
      "extraction_method": "Direct"
    },
    {
      "page_number": 609,
      "chapter": null,
      "content": "SUN’S NETWORK FILE SYSTEM (NFS)\n573\nit ﬁnally succeeds. Doing so ensures we will never end up with ﬁle con-\ntents intermingled as in the above example.\nThe problem that this requirement gives rise to in NFS server im-\nplementation is that write performance, without great care, can be the\nmajor performance bottleneck. Indeed, some companies (e.g., Network\nAppliance) came into existence with the simple objective of building an\nNFS server that can perform writes quickly; one trick they use is to ﬁrst\nput writes in a battery-backed memory, thus enabling to quickly reply\nto WRITE requests without fear of losing the data and without the cost\nof having to write to disk right away; the second trick is to use a ﬁle sys-\ntem design speciﬁcally designed to write to disk quickly when one ﬁnally\nneeds to do so [HLM94, RO91].\n48.12\nSummary\nWe have seen the introduction of the NFS distributed ﬁle system. NFS\nis centered around the idea of simple and fast recovery in the face of\nserver failure, and achieves this end through careful protocol design. Idem-\npotency of operations is essential; because a client can safely replay a\nfailed operation, it is OK to do so whether or not the server has executed\nthe request.\nWe also have seen how the introduction of caching into a multiple-\nclient, single-server system can complicate things. In particular, the sys-\ntem must resolve the cache consistency problem in order to behave rea-\nsonably; however, NFS does so in a slightly ad hoc fashion which can\noccasionally result in observably weird behavior. Finally, we saw how\nserver caching can be tricky: writes to the server must be forced to stable\nstorage before returning success (otherwise data can be lost).\nWe haven’t talked about other issues which are certainly relevant, no-\ntably security. Security in early NFS implementations was remarkably\nlax; it was rather easy for any user on a client to masquerade as other\nusers and thus gain access to virtually any ﬁle. Subsequent integration\nwith more serious authentication services (e.g., Kerberos [NT94]) have\naddressed these obvious deﬁciencies.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2141,
      "extraction_method": "Direct"
    },
    {
      "page_number": 610,
      "chapter": null,
      "content": "574\nSUN’S NETWORK FILE SYSTEM (NFS)\nReferences\n[S86] “The Sun Network File System: Design, Implementation and Experience”\nRussel Sandberg\nUSENIX Summer 1986\nThe original NFS paper; though a bit of a challenging read, it is worthwhile to see the source of these\nwonderful ideas.\n[NT94] “Kerberos: An Authentication Service for Computer Networks”\nB. Clifford Neuman, Theodore Ts’o\nIEEE Communications, 32(9):33-38, September 1994\nKerberos is an early and hugely inﬂuential authentication service. We probably should write a book\nchapter about it sometime...\n[P+94] “NFS Version 3: Design and Implementation”\nBrian Pawlowski, Chet Juszczak, Peter Staubach, Carl Smith, Diane Lebel, Dave Hitz\nUSENIX Summer 1994, pages 137-152\nThe small modiﬁcations that underlie NFS version 3.\n[P+00] “The NFS version 4 protocol”\nBrian Pawlowski, David Noveck, David Robinson, Robert Thurlow\n2nd International System Administration and Networking Conference (SANE 2000)\nUndoubtedly the most literary paper on NFS ever written.\n[C00] “NFS Illustrated”\nBrent Callaghan\nAddison-Wesley Professional Computing Series, 2000\nA great NFS reference; incredibly thorough and detailed per the protocol itself.\n[Sun89] “NFS: Network File System Protocol Speciﬁcation”\nSun Microsystems, Inc. Request for Comments: 1094, March 1989\nAvailable: http://www.ietf.org/rfc/rfc1094.txt\nThe dreaded speciﬁcation; read it if you must, i.e., you are getting paid to read it. Hopefully, paid a lot.\nCash money!\n[O91] “The Role of Distributed State”\nJohn K. Ousterhout\nAvailable: ftp://ftp.cs.berkeley.edu/ucb/sprite/papers/state.ps\nA rarely referenced discussion of distributed state; a broader perspective on the problems and challenges.\n[HLM94] “File System Design for an NFS File Server Appliance”\nDave Hitz, James Lau, Michael Malcolm\nUSENIX Winter 1994. San Francisco, California, 1994\nHitz et al. were greatly inﬂuenced by previous work on log-structured ﬁle systems.\n[RO91] “The Design and Implementation of the Log-structured File System”\nMendel Rosenblum, John Ousterhout\nSymposium on Operating Systems Principles (SOSP), 1991\nLFS again. No, you can never get enough LFS.\n[V72] “La Begueule”\nFrancois-Marie Arouet a.k.a. Voltaire\nPublished in 1772\nVoltaire said a number of clever things, this being but one example. For example, Voltaire also said “If\nyou have two religions in your land, the two will cut each others throats; but if you have thirty religions,\nthey will dwell in peace.” What do you say to that, Democrats and Republicans?\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2553,
      "extraction_method": "Direct"
    },
    {
      "page_number": 611,
      "chapter": null,
      "content": "49\nThe Andrew File System (AFS)\nThe Andrew File System was introduced by researchers at Carnegie-Mellon\nUniversity (CMU) in the 1980’s [H+88]. Led by the well-known Profes-\nsor M. Satyanarayanan of Carnegie-Mellon University (“Satya” for short),\nthe main goal of this project was simple: scale. Speciﬁcally, how can one\ndesign a distributed ﬁle system such that a server can support as many\nclients as possible?\nInterestingly, there are numerous aspects of design and implementa-\ntion that affect scalability. Most important is the design of the protocol be-\ntween clients and servers. In NFS, for example, the protocol forces clients\nto check with the server periodically to determine if cached contents have\nchanged; because each check uses server resources (including CPU and\nnetwork bandwidth), frequent checks like this will limit the number of\nclients a server can respond to and thus limit scalability.\nAFS also differs from NFS in that from the beginning, reasonable user-\nvisible behavior was a ﬁrst-class concern. In NFS, cache consistency is\nhard to describe because it depends directly on low-level implementa-\ntion details, including client-side cache timeout intervals. In AFS, cache\nconsistency is simple and readily understood: when the ﬁle is opened, a\nclient will generally receive the latest consistent copy from the server.\n49.1\nAFS Version 1\nWe will discuss two versions of AFS [H+88, S+85]. The ﬁrst version\n(which we will call AFSv1, but actually the original system was called\nthe ITC distributed ﬁle system [S+85]) had some of the basic design in\nplace, but didn’t scale as desired, which led to a re-design and the ﬁnal\nprotocol (which we will call AFSv2, or just AFS) [H+88]. We now discuss\nthe ﬁrst version.\nOne of the basic tenets of all versions of AFS is whole-ﬁle caching on\nthe local disk of the client machine that is accessing a ﬁle. When you\nopen() a ﬁle, the entire ﬁle (if it exists) is fetched from the server and\nstored in a ﬁle on your local disk. Subsequent application read() and\nwrite() operations are redirected to the local ﬁle system where the ﬁle is\n575\n",
      "content_length": 2102,
      "extraction_method": "Direct"
    },
    {
      "page_number": 612,
      "chapter": null,
      "content": "576\nTHE ANDREW FILE SYSTEM (AFS)\nTestAuth\nTest whether a file has changed\n(used to validate cached entries)\nGetFileStat\nGet the stat info for a file\nFetch\nFetch the contents of file\nStore\nStore this file on the server\nSetFileStat\nSet the stat info for a file\nListDir\nList the contents of a directory\nFigure 49.1: AFSv1 Protocol Highlights\nstored; thus, these operations require no network communication and are\nfast. Finally, upon close(), the ﬁle (if it has been modiﬁed) is ﬂushed\nback to the server. Note the obvious contrasts with NFS, which caches\nblocks (not whole ﬁles, although NFS could of course cache every block of\nan entire ﬁle) and does so in client memory (not local disk).\nLet’s get into the details a bit more. When a client application ﬁrst calls\nopen(), the AFS client-side code (which the AFS designers call Venus)\nwould send a Fetch protocol message to the server. The Fetch protocol\nmessage would pass the entire pathname of the desired ﬁle (for exam-\nple, /home/remzi/notes.txt) to the ﬁle server (the group of which\nthey called Vice), which would then traverse the pathname, ﬁnd the de-\nsired ﬁle, and ship the entire ﬁle back to the client. The client-side code\nwould then cache the ﬁle on the local disk of the client (by writing it to\nlocal disk). As we said above, subsequent read() and write() system\ncalls are strictly local in AFS (no communication with the server occurs);\nthey are just redirected to the local copy of the ﬁle. Because the read()\nand write() calls act just like calls to a local ﬁle system, once a block\nis accessed, it also may be cached in client memory. Thus, AFS also uses\nclient memory to cache copies of blocks that it has in its local disk. Fi-\nnally, when ﬁnished, the AFS client checks if the ﬁle has been modiﬁed\n(i.e., that it has been opened for writing); if so, it ﬂushes the new version\nback to the server with a Store protocol message, sending the entire ﬁle\nand pathname to the server for permanent storage.\nThe next time the ﬁle is accessed, AFSv1 does so much more efﬁ-\nciently. Speciﬁcally, the client-side code ﬁrst contacts the server (using\nthe TestAuth protocol message) in order to determine whether the ﬁle\nhas changed. If not, the client would use the locally-cached copy, thus\nimproving performance by avoiding a network transfer. The ﬁgure above\nshows some of the protocol messages in AFSv1. Note that this early ver-\nsion of the protocol only cached ﬁle contents; directories, for example,\nwere only kept at the server.\n49.2\nProblems with Version 1\nA few key problems with this ﬁrst version of AFS motivated the de-\nsigners to rethink their ﬁle system. To study the problems in detail, the\ndesigners of AFS spent a great deal of time measuring their existing pro-\ntotype to ﬁnd what was wrong. Such experimentation is a good thing;\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2857,
      "extraction_method": "Direct"
    },
    {
      "page_number": 613,
      "chapter": null,
      "content": "THE ANDREW FILE SYSTEM (AFS)\n577\nTIP: MEASURE THEN BUILD (PATTERSON’S LAW)\nOne of our advisors, David Patterson (of RISC and RAID fame), used to\nalways encourage us to measure a system and demonstrate a problem\nbefore building a new system to ﬁx said problem. By using experimen-\ntal evidence, rather than gut instinct, you can turn the process of system\nbuilding into a more scientiﬁc endeavor. Doing so also has the fringe ben-\neﬁt of making you think about how exactly to measure the system before\nyour improved version is developed. When you do ﬁnally get around to\nbuilding the new system, two things are better as a result: ﬁrst, you have\nevidence that shows you are solving a real problem; second, you now\nhave a way to measure your new system in place, to show that it actually\nimproves upon the state of the art. And thus we call this Patterson’s Law.\nmeasurement is the key to understanding how systems work and how to\nimprove them. Hard data helps take intuition and make into a concrete\nscience of deconstructing systems. In their study, the authors found two\nmain problems with AFSv1:\n• Path-traversal costs are too high: When performing a Fetch or Store\nprotocol request, the client passes the entire pathname (e.g., /home/\nremzi/notes.txt) to the server. The server, in order to access the\nﬁle, must perform a full pathname traversal, ﬁrst looking in the root\ndirectory to ﬁnd home, then in home to ﬁnd remzi, and so forth,\nall the way down the path until ﬁnally the desired ﬁle is located.\nWith many clients accessing the server at once, the designers of AFS\nfound that the server was spending much of its CPU time simply\nwalking down directory paths.\n• The client issues too many TestAuth protocol messages: Much\nlike NFS and its overabundance of GETATTR protocol messages,\nAFSv1 generated a large amount of trafﬁc to check whether a lo-\ncal ﬁle (or its stat information) was valid with the TestAuth proto-\ncol message. Thus, servers spent much of their time telling clients\nwhether it was OK to used their cached copies of a ﬁle. Most of the\ntime, the answer was that the ﬁle had not changed.\nThere were actually two other problems with AFSv1: load was not\nbalanced across servers, and the server used a single distinct process per\nclient thus inducing context switching and other overheads. The load\nimbalance problem was solved by introducing volumes, which an ad-\nministrator could move across servers to balance load; the context-switch\nproblem was solved in AFSv2 by building the server with threads instead\nof processes. However, for the sake of space, we focus here on the main\ntwo protocol problems above that limited the scale of the system.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2710,
      "extraction_method": "Direct"
    },
    {
      "page_number": 614,
      "chapter": null,
      "content": "578\nTHE ANDREW FILE SYSTEM (AFS)\n49.3\nImproving the Protocol\nThe two problems above limited the scalability of AFS; the server CPU\nbecame the bottleneck of the system, and each server could only ser-\nvice 20 clients without becoming overloaded. Servers were receiving too\nmany TestAuth messages, and when they received Fetch or Store mes-\nsages, were spending too much time traversing the directory hierarchy.\nThus, the AFS designers were faced with a problem:\nTHE CRUX: HOW TO DESIGN A SCALABLE FILE PROTOCOL\nHow should one redesign the protocol to minimize the number of\nserver interactions, i.e., how could they reduce the number of TestAuth\nmessages? Further, how could they design the protocol to make these\nserver interactions efﬁcient? By attacking both of these issues, a new pro-\ntocol would result in a much more scalable version AFS.\n49.4\nAFS Version 2\nAFSv2 introduced the notion of a callback to reduce the number of\nclient/server interactions. A callback is simply a promise from the server\nto the client that the server will inform the client when a ﬁle that the\nclient is caching has been modiﬁed. By adding this state to the server, the\nclient no longer needs to contact the server to ﬁnd out if a cached ﬁle is\nstill valid. Rather, it assumes that the ﬁle is valid until the server tells it\notherwise; insert analogy to polling versus interrupts here.\nAFSv2 also introduced the notion of a ﬁle identiﬁer (FID) (similar to\nthe NFS ﬁle handle) instead of pathnames to specify which ﬁle a client\nwas interested in. An FID in AFS consists of a volume identiﬁer, a ﬁle\nidentiﬁer, and a “uniquiﬁer” (to enable reuse of the volume and ﬁle IDs\nwhen a ﬁle is deleted). Thus, instead of sending whole pathnames to\nthe server and letting the server walk the pathname to ﬁnd the desired\nﬁle, the client would walk the pathname, one piece at a time, caching the\nresults and thus hopefully reducing the load on the server.\nFor example, if a client accessed the ﬁle /home/remzi/notes.txt,\nand home was the AFS directory mounted onto / (i.e., / was the local root\ndirectory, but home and its children were in AFS), the client would ﬁrst\nFetch the directory contents of home, put them in the local-disk cache,\nand setup a callback on home. Then, the client would Fetch the directory\nremzi, put it in the local-disk cache, and setup a callback on the server\non remzi. Finally, the client would Fetch notes.txt, cache this regular\nﬁle in the local disk, setup a callback, and ﬁnally return a ﬁle descriptor\nto the calling application. See Table 49.1 for a summary.\nThe key difference, however, from NFS, is that with each fetch of a\ndirectory or ﬁle, the AFS client would establish a callback with the server,\nthus ensuring that the server would notify the client of a change in its\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2829,
      "extraction_method": "Direct"
    },
    {
      "page_number": 615,
      "chapter": null,
      "content": "THE ANDREW FILE SYSTEM (AFS)\n579\nClient (C1)\nServer\nfd = open(“/home/remzi/notes.txt”, ...);\nSend Fetch (home FID, “remzi”)\nReceive Fetch request\nlook for remzi in home dir\nestablish callback(C1) on remzi\nreturn remzi’s content and FID\nReceive Fetch reply\nwrite remzi to local disk cache\nrecord callback status of remzi\nSend Fetch (remzi FID, “notes.txt”)\nReceive Fetch request\nlook for notes.txt in remzi dir\nestablish callback(C1) on notes.txt\nreturn notes.txt’s content and FID\nReceive Fetch reply\nwrite notes.txt to local disk cache\nrecord callback status of notes.txt\nlocal open() of cached notes.txt\nreturn ﬁle descriptor to application\nread(fd, buffer, MAX);\nperform local read() on cached copy\nclose(fd);\ndo local close() on cached copy\nif ﬁle has changed, ﬂush to server\nfd = open(“/home/remzi/notes.txt”, ...);\nForeach dir (home, remzi)\nif (callback(dir) == VALID)\nuse local copy for lookup(dir)\nelse\nFetch (as above)\nif (callback(notes.txt) == VALID)\nopen local cached copy\nreturn ﬁle descriptor to it\nelse\nFetch (as above) then open and return fd\nTable 49.1: Reading A File: Client-side And File Server Actions\ncached state. The beneﬁt is obvious: although the ﬁrst access to /home/\nremzi/notes.txt generates many client-server messages (as described\nabove), it also establishes callbacks for all the directories as well as the\nﬁle notes.txt, and thus subsequent accesses are entirely local and require\nno server interaction at all. Thus, in the common case where a ﬁle is\ncached at the client, AFS behaves nearly identically to a local disk-based\nﬁle system. If one accesses a ﬁle more than once, the second access should\nbe just as fast as accessing a ﬁle locally.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 1720,
      "extraction_method": "Direct"
    },
    {
      "page_number": 616,
      "chapter": null,
      "content": "580\nTHE ANDREW FILE SYSTEM (AFS)\nASIDE: CACHE CONSISTENCY IS NOT A PANACEA\nWhen discussing distributed ﬁle systems, much is made of the cache con-\nsistency the ﬁle systems provide. However, this baseline consistency does\nnot solve all problems with regards to ﬁle access from multiple clients.\nFor example, if you are building a code repository, with multiple clients\nperforming check-ins and check-outs of code, you can’t simply rely on\nthe underlying ﬁle system to do all of the work for you; rather, you have\nto use explicit ﬁle-level locking in order to ensure that the “right” thing\nhappens when such concurrent accesses take place. Indeed, any applica-\ntion that truly cares about concurrent updates will add extra machinery\nto handle conﬂicts. The baseline consistency described in this chapter and\nthe previous one are useful primarily for casual usage, i.e., when a user\nlogs into a different client, they expect some reasonable version of their\nﬁles to show up there. Expecting more from these protocols is setting\nyourself up for failure, disappointment, and tear-ﬁlled frustration.\n49.5\nCache Consistency\nWhen we discussed NFS, there were two aspects of cache consistency\nwe considered: update visibility and cache staleness. With update visi-\nbility, the question is: when will the server be updated with a new version\nof a ﬁle? With cache staleness, the question is: once the server has a new\nversion, how long before clients see the new version instead of an older\ncached copy?\nBecause of callbacks and whole-ﬁle caching, the cache consistency pro-\nvided by AFS is easy to describe and understand. There are two im-\nportant cases to consider: consistency between processes on different ma-\nchines, and consistency between processes on the same machine.\nBetween different machines, AFS makes updates visible at the server\nand invalidates cached copies at the exact same time, which is when the\nupdated ﬁle is closed. A client opens a ﬁle, and then writes to it (perhaps\nrepeatedly). When it is ﬁnally closed, the new ﬁle is ﬂushed to the server\n(and thus visibile); the server then breaks callbacks for any clients with\ncached copies, thus ensuring that clients will no longer read stale copies\nof the ﬁle; subsequent opens on those clients will require a re-fetch of the\nnew version of the ﬁle from the server.\nAFS makes an exception to this simple model between processes on\nthe same machine. In this case, writes to a ﬁle are immediately visible to\nother local processes (i.e., a process does not have to wait until a ﬁle is\nclosed to see its latest updates). This makes using a single machine be-\nhave exactly as you would expect, as this behavior is based upon typical\nUNIX semantics. Only when switching to a different machine would you\nbe able to detect the more general AFS consistency mechanism.\nThere is one interesting cross-machine case that is worthy of further\ndiscussion. Speciﬁcally, in the rare case that processes on different ma-\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 3010,
      "extraction_method": "Direct"
    },
    {
      "page_number": 617,
      "chapter": null,
      "content": "THE ANDREW FILE SYSTEM (AFS)\n581\nClient1\nClient2\nServer\nComments\nP1\nP2\nCache\nP3\nCache\nDisk\nopen(F)\n-\n-\n-\nFile created\nwrite(A)\nA\n-\n-\nclose()\nA\n-\nA\nopen(F)\nA\n-\nA\nread() →A\nA\n-\nA\nclose()\nA\n-\nA\nopen(F)\nA\n-\nA\nwrite(B)\nB\n-\nA\nopen(F)\nB\n-\nA\nLocal processes\nread() →B\nB\n-\nA\nsee writes immediately\nclose()\nB\n-\nA\nB\nopen(F)\nA\nA\nRemote processes\nB\nread() →A\nA\nA\ndo not see writes...\nB\nclose()\nA\nA\nclose()\nB\n\u0001A\nB\n... until close()\nB\nopen(F)\nB\nB\nhas taken place\nB\nread() →B\nB\nB\nB\nclose()\nB\nB\nB\nopen(F)\nB\nB\nopen(F)\nB\nB\nB\nwrite(D)\nD\nB\nB\nD\nwrite(C)\nC\nB\nD\nclose()\nC\nC\nclose()\nD\n\u0001C\nD\nD\nopen(F)\nD\nD\nUnfortunately for P3\nD\nread() →D\nD\nD\nthe last writer wins\nD\nclose()\nD\nD\nTable 49.2: Cache Consistency Timeline\nchines are modifying a ﬁle at the same time, AFS naturally employs what\nis known as a last writer wins approach (which perhaps should be called\nlast closer wins). Speciﬁcally, whichever client calls close() last will\nupdate the entire ﬁle on the server last and thus will be the “winning”\nﬁle, i.e., the ﬁle that remains on the server for others to see. The result is\na ﬁle that was generated in its entirety either by one client or the other.\nNote the difference from a block-based protocol like NFS: in NFS, writes\nof individual blocks may be ﬂushed out to the server as each client is up-\ndating the ﬁle, and thus the ﬁnal ﬁle on the server could end up as a mix\nof updates from both clients. In many cases, such a mixed ﬁle output\nwould not make much sense, i.e., imagine a JPEG image getting modi-\nﬁed by two clients in pieces; the resulting mix of writes would not likely\nconstitute a valid JPEG.\nA timeline showing a few of these different scenarios can be seen in\nTable 49.2. The columns of the table show the behavior of two processes\n(P1 and P2) on Client1 and its cache state, one process (P3) on Client2 and\nits cache state, and the server (Server), all operating on a single ﬁle called,\nimaginatively, F. For the server, the table just shows the contents of the\nﬁle after the operation on the left has completed. Read through it and see\nif you can understand why each read returns the results that it does. A\ncommentary ﬁeld on the right will help you if you get stuck.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2214,
      "extraction_method": "Direct"
    },
    {
      "page_number": 618,
      "chapter": null,
      "content": "582\nTHE ANDREW FILE SYSTEM (AFS)\n49.6\nCrash Recovery\nFrom the description above, you might sense that crash recovery is\nmore involved than with NFS. You would be right. For example, imagine\nthere is a short period of time where a server (S) is not able to contact\na client (C1), for example, while the client C1 is rebooting. While C1\nis not available, S may have tried to send it one or more callback recall\nmessages; for example, imagine C1 had ﬁle F cached on its local disk, and\nthen C2 (another client) updated F, thus causing S to send messages to all\nclients caching the ﬁle to remove it from their local caches. Because C1\nmay miss those critical messages when it is rebooting, upon rejoining the\nsystem, C1 should treat all of its cache contents as suspect. Thus, upon\nthe next access to ﬁle F, C1 should ﬁrst ask the server (with a TestAuth\nprotocol message) whether its cached copy of ﬁle F is still valid; if so, C1\ncan use it; if not, C1 should fetch the newer version from the server.\nServer recovery after a crash is also more complicated. The problem\nthat arises is that callbacks are kept in memory; thus, when a server re-\nboots, it has no idea which client machine has which ﬁles. Thus, upon\nserver restart, each client of the server must realize that the server has\ncrashed and treat all of their cache contents as suspect, and (as above)\nreestablish the validity of a ﬁle before using it. Thus, a server crash is a\nbig event, as one must ensure that each client is aware of the crash in a\ntimely manner, or risk a client accessing a stale ﬁle. There are many ways\nto implement such recovery; for example, by having the server send a\nmessage (saying “don’t trust your cache contents!”) to each client when\nit is up and running again, or by having clients check that the server is\nalive periodically (with a heartbeat message, as it is called). As you can\nsee, there is a cost to building a more scalable and sensible caching model;\nwith NFS, clients hardly noticed a server crash.\n49.7\nScale And Performance Of AFSv2\nWith the new protocol in place, AFSv2 was measured and found to be\nmuch more scalable that the original version. Indeed, each server could\nsupport about 50 clients (instead of just 20). A further beneﬁt was that\nclient-side performance often came quite close to local performance, be-\ncause in the common case, all ﬁle accesses were local; ﬁle reads usually\nwent to the local disk cache (and potentially, local memory). Only when a\nclient created a new ﬁle or wrote to an existing one was there need to send\na Store message to the server and thus update the ﬁle with new contents.\nLet us also gain some perspective on AFS performance by comparing\ncommon ﬁle-system access scenarios with NFS. Table 49.3 shows the re-\nsults of our qualitative comparison.\nIn the table, we examine typical read and write patterns analytically,\nfor ﬁles of different sizes. Small ﬁles have Ns blocks in them; medium\nﬁles have Nm blocks; large ﬁles have NL blocks. We assume that small\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 3049,
      "extraction_method": "Direct"
    },
    {
      "page_number": 619,
      "chapter": null,
      "content": "THE ANDREW FILE SYSTEM (AFS)\n583\nWorkload\nNFS\nAFS\nAFS/NFS\n1. Small ﬁle, sequential read\nNs · Lnet\nNs · Lnet\n1\n2. Small ﬁle, sequential re-read\nNs · Lmem\nNs · Lmem\n1\n3. Medium ﬁle, sequential read\nNm · Lnet\nNm · Lnet\n1\n4. Medium ﬁle, sequential re-read\nNm · Lmem\nNm · Lmem\n1\n5. Large ﬁle, sequential read\nNL · Lnet\nNL · Lnet\n1\n6. Large ﬁle, sequential re-read\nNL · Lnet\nNL · Ldisk\nLdisk\nLnet\n7. Large ﬁle, single read\nLnet\nNL · Lnet\nNL\n8. Small ﬁle, sequential write\nNs · Lnet\nNs · Lnet\n1\n9. Large ﬁle, sequential write\nNL · Lnet\nNL · Lnet\n1\n10. Large ﬁle, sequential overwrite\nNL · Lnet\n2 · NL · Lnet\n2\n11. Large ﬁle, single write\nLnet\n2 · NL · Lnet\n2 · NL\nTable 49.3: Comparison: AFS vs. NFS\nand medium ﬁles ﬁt into the memory of a client; large ﬁles ﬁt on a local\ndisk but not in client memory.\nWe also assume, for the sake of analysis, that an access across the net-\nwork to the remote server for a ﬁle block takes Lnet time units. Access\nto local memory takes Lmem, and access to local disk takes Ldisk. The\ngeneral assumption is that Lnet > Ldisk > Lmem.\nFinally, we assume that the ﬁrst access to a ﬁle does not hit in any\ncaches. Subsequent ﬁle accesses (i.e., “re-reads”) we assume will hit in\ncaches, if the relevant cache has enough capacity to hold the ﬁle.\nThe columns of the table show the time a particular operation (e.g., a\nsmall ﬁle sequential read) roughly takes on either NFS or AFS. The right-\nmost column displays the ratio of AFS to NFS.\nWe make the following observations. First, in many cases, the per-\nformance of each system is roughly equivalent. For example, when ﬁrst\nreading a ﬁle (e.g., Workloads 1, 3, 5), the time to fetch the ﬁle from the re-\nmote server dominates, and is similar on both systems. You might think\nAFS would be slower in this case, as it has to write the ﬁle to local disk;\nhowever, those writes are buffered by the local (client-side) ﬁle system\ncache and thus said costs are likely hidden. Similarly, you might think\nthat AFS reads from the local cached copy would be slower, again be-\ncause AFS stores the cached copy on disk. However, AFS again beneﬁts\nhere from local ﬁle system caching; reads on AFS would likely hit in the\nclient-side memory cache, and performance would be similar to NFS.\nSecond, an interesting difference arises during a large-ﬁle sequential\nre-read (Workload 6). Because AFS has a large local disk cache, it will\naccess the ﬁle from there when the ﬁle is accessed again. NFS, in contrast,\nonly can cache blocks in client memory; as a result, if a large ﬁle (i.e., a ﬁle\nbigger than local memory) is re-read, the NFS client will have to re-fetch\nthe entire ﬁle from the remote server. Thus, AFS is faster than NFS in this\ncase by a factor of\nLnet\nLdisk , assuming that remote access is indeed slower\nthan local disk. We also note that NFS in this case increases server load,\nwhich has an impact on scale as well.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2930,
      "extraction_method": "Direct"
    },
    {
      "page_number": 620,
      "chapter": null,
      "content": "584\nTHE ANDREW FILE SYSTEM (AFS)\nThird, we note that sequential writes (of new ﬁles) should perform\nsimilarly on both systems (Workloads 8, 9). AFS, in this case, will write\nthe ﬁle to the local cached copy; when the ﬁle is closed, the AFS client\nwill force the writes to the server, as per the protocol. NFS will buffer\nwrites in client memory, perhaps forcing some blocks to the server due\nto client-side memory pressure, but deﬁnitely writing them to the server\nwhen the ﬁle is closed, to preserve NFS ﬂush-on-close consistency. You\nmight think AFS would be slower here, because it writes all data to local\ndisk. However, realize that it is writing to a local ﬁle system; those writes\nare ﬁrst committed to the page cache, and only later (in the background)\nto disk, and thus AFS reaps the beneﬁts of the client-side OS memory\ncaching infrastructure to improve performance.\nFourth, we note that AFS performs worse on a sequential ﬁle over-\nwrite (Workload 10). Thus far, we have assumed that the workloads that\nwrite are also creating a new ﬁle; in this case, the ﬁle exists, and is then\nover-written. Overwrite can be a particularly bad case for AFS, because\nthe client ﬁrst fetches the old ﬁle in its entirety, only to subsequently over-\nwrite it. NFS, in contrast, will simply overwrite blocks and thus avoid the\ninitial (useless) read1.\nFinally, workloads that access a small subset of data within large ﬁles\nperform much better on NFS than AFS (Workloads 7, 11). In these cases,\nthe AFS protocol fetches the entire ﬁle when the ﬁle is opened; unfortu-\nnately, only a small read or write is performed. Even worse, if the ﬁle is\nmodiﬁed, the entire ﬁle is written back to the server, doubling the per-\nformance impact. NFS, as a block-based protocol, performs I/O that is\nproportional to the size of the read or write.\nOverall, we see that NFS and AFS make different assumptions and not\nsurprisingly realize different performance outcomes as a result. Whether\nthese differences matter is, as always, a question of workload.\n49.8\nAFS: Other Improvements\nLike we saw with the introduction of Berkeley FFS (which added sym-\nbolic links and a number of other features), the designers of AFS took the\nopportunity when building their system to add a number of features that\nmade the system easier to use and manage. For example, AFS provides a\ntrue global namespace to clients, thus ensuring that all ﬁles were named\nthe same way on all client machines. NFS, in contrast, allows each client\nto mount NFS servers in any way that they please, and thus only by con-\nvention (and great administrative effort) would ﬁles be named similarly\nacross clients.\n1We assume here that NFS reads are block-sized and block-aligned; if they were not, the\nNFS client would also have to read the block ﬁrst. We also assume the ﬁle was not opened\nwith the O TRUNC ﬂag; if it had been, the initial open in AFS would not fetch the soon to be\ntruncated ﬁle’s contents.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2994,
      "extraction_method": "Direct"
    },
    {
      "page_number": 621,
      "chapter": null,
      "content": "THE ANDREW FILE SYSTEM (AFS)\n585\nASIDE: THE IMPORTANCE OF WORKLOAD\nOne challenge of evaluating any system is the choice of workload. Be-\ncause computer systems are used in so many different ways, there are a\nlarge variety of workloads to choose from. How should the storage sys-\ntem designer decide which workloads are important, in order to make\nreasonable design decisions?\nThe designers of AFS, given their experience in measuring how ﬁle sys-\ntems were used, made certain workload assumptions; in particular, they\nassumed that most ﬁles were not frequently shared, and accessed sequen-\ntially in their entirety. Given those assumptions, the AFS design makes\nperfect sense.\nHowever, these assumptions are not always correct. For example, imag-\nine an application that appends information, periodically, to a log. These\nlittle log writes, which add small amounts of data to an existing large ﬁle,\nare quite problematic for AFS. Many other difﬁcult workloads exist as\nwell, e.g., random updates in a transaction database.\nOne place to get some information about what types of workloads are\ncommon are through various research studies that have been performed.\nSee any of these studies for good examples of workload analysis [B+91,\nH+11, R+00, V99], including the AFS retrospective [H+88].\nAFS also takes security seriously, and incorporates mechanisms to au-\nthenticate users and ensure that a set of ﬁles could be kept private if a\nuser so desired. NFS, in contrast, had quite primitive support for security\nfor many years.\nAFS also includes facilities for ﬂexible user-managed access control.\nThus, when using AFS, a user has a great deal of control over who exactly\ncan access which ﬁles. NFS, like most UNIX ﬁle systems, has much less\nsupport for this type of sharing.\nFinally, as mentioned before, AFS adds tools to enable simpler man-\nagement of servers for the administrators of the system. In thinking about\nsystem management, AFS was light years ahead of the ﬁeld.\n49.9\nSummary\nAFS shows us how distributed ﬁle systems can be built quite differ-\nently than what we saw with NFS. The protocol design of AFS is partic-\nularly important; by minimizing server interactions (through whole-ﬁle\ncaching and callbacks), each server can support many clients and thus\nreduce the number of servers needed to manage a particular site. Many\nother features, including the single namespace, security, and access-control\nlists, make AFS quite nice to use. The consistency model provided by AFS\nis simple to understand and reason about, and does not lead to the occa-\nsional weird behavior as one sometimes observes in NFS.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2658,
      "extraction_method": "Direct"
    },
    {
      "page_number": 622,
      "chapter": null,
      "content": "586\nTHE ANDREW FILE SYSTEM (AFS)\nPerhaps unfortunately, AFS is likely on the decline. Because NFS be-\ncame an open standard, many different vendors supported it, and, along\nwith CIFS (the Windows-based distributed ﬁle system protocol), NFS\ndominates the marketplace.\nAlthough one still sees AFS installations\nfrom time to time (such as in various educational institutions, including\nWisconsin), the only lasting inﬂuence will likely be from the ideas of AFS\nrather than the actual system itself. Indeed, NFSv4 now adds server state\n(e.g., an “open” protocol message), and thus bears an increasing similar-\nity to the basic AFS protocol.\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 684,
      "extraction_method": "Direct"
    },
    {
      "page_number": 623,
      "chapter": null,
      "content": "THE ANDREW FILE SYSTEM (AFS)\n587\nReferences\n[B+91] “Measurements of a Distributed File System”\nMary Baker, John Hartman, Martin Kupfer, Ken Shirriff, John Ousterhout\nSOSP ’91, Paciﬁc Grove, CA, October 1991\nAn early paper measuring how people use distributed ﬁle systems. Matches much of the intuition found\nin AFS.\n[H+11] “A File is Not a File: Understanding the I/O Behavior of Apple Desktop Applications”\nTyler Harter, Chris Dragga, Michael Vaughn,\nAndrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau\nSOSP ’11, New York, NY, October 2011\nOur own paper studying the behavior of Apple Desktop workloads; turns out they are a bit different\nthan many of the server-based workloads the systems research community usually focuses upon. Also a\ngood recent reference which points to a lot of related work.\n[H+88] “Scale and Performance in a Distributed File System”\nJohn H. Howard, Michael L. Kazar, Sherri G. Menees, David A. Nichols, M. Satyanarayanan,\nRobert N. Sidebotham, Michael J. West\nACM Transactions on Computing Systems (ACM TOCS), page 51-81, Volume 6, Number 1,\nFebruary 1988\nThe long journal version of the famous AFS system, still in use in a number of places throughout the\nworld, and also probably the earliest clear thinking on how to build distributed ﬁle systems. A wonderful\ncombination of the science of measurement and principled engineering.\n[R+00] “A Comparison of File System Workloads”\nDrew Roselli, Jacob R. Lorch, Thomas E. Anderson\nUSENIX ’00, San Diego, CA, June 2000\nA more recent set of traces as compared to the Baker paper [B+91], with some interesting twists.\n[S+85] “The ITC Distributed File System: Principles and Design”\nM. Satyanarayanan, J.H. Howard, D.A. Nichols, R.N. Sidebotham, A. Spector, M.J. West\nSOSP ’85. pages 35-50\nThe older paper about a distributed ﬁle system. Much of the basic design of AFS is in place in this older\nsystem, but not the improvements for scale.\n[V99] “File system usage in Windows NT 4.0”\nWerner Vogels\nSOSP ’99, Kiawah Island Resort, SC, December 1999\nA cool study of Windows workloads, which are inherently different than many of the UNIX-based studies\nthat had previously been done.\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2192,
      "extraction_method": "Direct"
    },
    {
      "page_number": 624,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 625,
      "chapter": null,
      "content": "50\nSummary Dialogue on Distribution\nStudent: Well, that was quick. Too quick, in my opinion!\nProfessor: Yes, distributed systems are complicated and cool and well worth\nyour study; just not in this book (or course).\nStudent: That’s too bad; I wanted to learn more! But I did learn a few things.\nProfessor: Like what?\nStudent: Well, everything can fail.\nProfessor: Good start.\nStudent: But by having lots of these things (whether disks, machines, or what-\never), you can hide much of the failure that arises.\nProfessor: Keep going!\nStudent: Some basic techniques like retrying are really useful.\nProfessor: That’s true.\nStudent: And you have to think carefully about protocols: the exact bits that\nare exchanged between machines. Protocols can affect everything, including how\nsystems respond to failure and how scalable they are.\nProfessor: You really are getting better at this learning stuff.\nStudent: Thanks! And you’re not a bad teacher yourself!\nProfessor: Well thank you very much too.\nStudent: So is this the end of the book?\nProfessor: I’m not sure. They don’t tell me anything.\nStudent: Me neither. Let’s get out of here.\nProfessor: OK.\nStudent: Go ahead.\nProfessor: No, after you.\nStudent: Please, professors ﬁrst.\n589\n",
      "content_length": 1229,
      "extraction_method": "Direct"
    },
    {
      "page_number": 626,
      "chapter": null,
      "content": "590\nSUMMARY DIALOGUE ON DISTRIBUTION\nProfessor: No, please, after you.\nStudent: (exasperated) Fine!\nProfessor: (waiting) ... so why haven’t you left?\nStudent: I don’t know how. Turns out, the only thing I can do is participate in\nthese dialogues.\nProfessor: Me too. And now you’ve learned our ﬁnal lesson...\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 355,
      "extraction_method": "Direct"
    },
    {
      "page_number": 627,
      "chapter": null,
      "content": "General Index\nabsolute pathname, 442\nabstraction, iv, 112, 395\nabstractions, 13\naccess methods, 462\naccess path, 470\naccessed bit, 174\naccounting, 77\nack, 547\nacknowledgment, 547\nacquired, 291\nadditive parity, 432\naddress, 7\naddress space, 8, 26, 108, 111, 132, 263,\n403\naddress space identiﬁer, 191\naddress translation, 130, 134, 137\naddress translations, 170\naddress-based ordering, 164\naddress-space identiﬁer, 190\naddress-translation cache, 183\nadmission control, 240\nadvice, 79, 80\nAIO control block, 378\nAIX, 17\nallocate, 472\nallocation structures, 463\nAMAT, 228\namortization, 66, 485\namortize, 65, 514\nanonymous, 125\nanticipatory disk scheduling, 415\nASID, 191\nAside, 16\nasides, iii\nasynchronous, 375\nasynchronous I/O, 377\nasynchronous read, 378\nasynchronously, 556\natomic, 274, 403, 448\natomic exchange, 296\natomically, 10, 271, 297, 429, 495\natomicity violation, 360\nattribute cache, 571\nautomatic, 119\nautomatic memory management, 122\navailable, 291\naverage memory access time, 228\naverage turnaround time, 61\navoidance, 368\navoids, 474\nB-tree, 470\nbaby prooﬁng, 55\nback pointer, 508\nbackground, 224\nbackpointer-based consistency, 508\nbase, 133, 135, 204\nbase and bounds, 133\nbash, 42\nbatch, 14, 474\nBBC, 508\nBelady’s Anomaly, 231\nBerkeley Systems Distribution, 17\nbest ﬁt, 163\nbest-ﬁt, 148\nbig endian, 554\nbig kernel lock, 322\nBill Joy, 17\nbinary buddy allocator, 166\nbinary semaphore, 344\nbitmap, 463\nBKL, 322\nblock corruption, 436, 527\nblock groups, 481\nBlocked, 29\nblocked, 67, 221\nblocks, 462\nboost, 76\nbound, 204\nbounded buffer, 329, 346\nbounded SATF, 419\nbounded-buffer, 329\nbounds, 133, 135\n591\n",
      "content_length": 1613,
      "extraction_method": "Direct"
    },
    {
      "page_number": 628,
      "chapter": null,
      "content": "592\nDEPLOYMENT\nbreak, 125\nBSATF, 419\nBSD, 17\nbtrfs, 523\nbuddy algorithm, 148\nbuffer, 447\nbuffer cache, 493\nbuffer overﬂow, 123\nbugs, 561\nbus snooping, 96\nbyte ordering, 554\nC programming language, iv, 17\nC-SCAN, 413\ncache, 183, 227, 407\ncache afﬁnity, 97\ncache coherence, 96\ncache consistency problem, 569\ncache hits, 227\ncache misses, 227\ncache replacement, 192\ncached, 560\ncaches, 94\ncaching, 569\ncallback, 578\ncapability, 444\ncapacity, 423\ncapacity miss, 230\ncast, 121\ncentralized administration, 559\ncheckpoint, 498\ncheckpoint region (CR), 517\ncheckpointing, 498\nchecksum, 530, 546\nchild, 36\nchunk size, 424\ncigarette smoker’s problem, 355\ncircular log, 502\nCircular SCAN, 413\nCISC, 187, 189\nclean, 239, 519\nclient stub, 551\nclient-side ﬁle system, 560\nclient/server, 543\nclock algorithm, 238\nclock hand, 238\nclose-to-open, 570\ncluster, 223\nclustering, 240, 249\ncoalesce, 162\ncoalescing, 156, 393\ncoarse-grained, 147, 292\ncode, 111\ncode sharing, 146\ncold-start miss, 229, 230\ncollision, 532\ncommand, 391\ncommon case, 571\ncommunication, 544\ncommunication endpoint, 545\ncompact, 148, 520\ncompaction, 154\ncompare-and-exchange, 299\ncompare-and-swap, 299\nComplex Instruction Set Computing, 189\ncompulsory miss, 229, 230\ncomputed checksum, 533\nconcurrency, iii, 1, 8, 10, 13, 16, 37, 54, 261\nconcurrently, 311\ncondition, 325, 326, 344\ncondition variable, 285, 326, 344\ncondition variables, 262, 273, 362\nconﬂict miss, 230\nconsistent-update problem, 429, 495\nconsumer, 331\ncontext switch, 26, 30, 52, 63, 263\ncontinuation, 380\nconvention, 443\nconvoy effect, 61\ncooperative, 50\ncopy-on-write, 12, 251, 507, 522\ncorrectness, 299\ncorrupt, 528\ncovering condition, 338\nCOW, 251, 507\nCPU, 5\ncrash-consistency problem, 491, 495\nCRC, 532\ncritical section, 271, 272, 284\ncrux, iii\ncrux of the problem, iii\nCuller’s Law, 194\ncycle, 363\ncyclic redundancy check, 532\ncylinder groups, 481\ndangling pointer, 124\ndangling reference, 455\ndata, 391\ndata bitmap, 463, 481, 492\ndata integrity, 527\ndata journaling, 498, 503\ndata protection, 527\ndata region, 462\ndata structures, 32, 461\ndatabase management system, 194\ndatagrams, 545\nDBMS, 194\ndeadlock, 354, 359, 363\nDEC, 245\ndecay-usage, 79\ndecodes, 3\ndemand paging, 240\ndemand zeroing, 250\ndeployability, 422\ndeployment, 422\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2303,
      "extraction_method": "Direct"
    },
    {
      "page_number": 629,
      "chapter": null,
      "content": "EXPLICIT\n593\ndescheduled, 30\ndeserialization, 552\ndeterministic, 37, 268, 270, 272\ndevice driver, 12, 395\ndialogue, iii\nDigital Equipment Corporation, 245\ndimensional analysis, 408\ndining philosopher’s problem, 352\ndirect I/O, 474\nDirect Memory Access (DMA), 394\ndirect pointers, 466\ndirectory, 442\ndirectory hierarchy, 442\ndirectory tree, 442\ndirty, 239, 447\ndirty bit, 174, 190, 239\ndisable interrupts, 55\ndisassemble, 176\ndisassembler., 269\ndisciplines, 59\ndisk, 28\ndisk address, 218\ndisk arm, 404\ndisk head, 404\nDisk Operating System, 16\ndisk scheduler, 412\ndisk scrubbing, 535\ndisks, 389\ndistributed shared memory, 550\ndistributed state, 562\nDOS, 16\ndouble free, 124\ndouble indirect pointer, 467\ndrop, 545\nDSM, 550\ndtruss, 444\ndynamic relocation, 133, 134\neagerly, 28\nease of use, 108\neasy to use, 3, 111\nECC, 528\nEdsger Dijkstra, 341\nefﬁciency, 110, 113\nefﬁcient, 113\nelevator, 413\nempty, 335\nencapsulation, 364\nend-to-end argument, 545, 555\nenergy-efﬁciency, 14\nerror correcting codes, 528\nevent handler, 374\nevent loop, 374\nevent-based concurrency, 373\nevict, 227\nexactly once, 548\nexecutable format, 28\nexecutes, 3\nexplicit, 144\nexponential back-off, 550\nextents, 467\neXternal Data Representation, 555\nexternal fragmentation, 148, 153, 154\nF-SCAN, 413\nfail-partial, 528\nfail-stop, 423, 527\nfailure, 543\nfair, 66\nfair-share, 83\nfairness, 60, 293, 299\nFast File System (FFS), 481\nFAT, 468\nFCFS, 61\nfetch-and-add, 302\nfetches, 3\nFID, 578\nFIFO, 60, 230\nﬁle, 441\nﬁle allocation table, 468\nﬁle descriptor, 444\nﬁle descriptors, 29\nﬁle handle, 563, 578\nﬁle identiﬁer, 578\nﬁle offset, 446\nﬁle server, 560\nﬁle system, 11, 12, 15\nﬁle system checker, 492\nﬁle-level locking, 580\nﬁle-system inconsistency, 494\nﬁles, 11\nﬁll, 335\nﬁnal, 31\nﬁne-grained, 147, 292\nﬁrmware, 390\nFirst Come, First Served, 60\nﬁrst ﬁt, 164\nFirst In, First Out, 60\nﬁrst-ﬁt, 148\nﬁx-sized cache, 474\nﬂash-based SSDs, 28\nFletcher checksum, 532\nﬂush, 191\nﬂush-on-close, 570\nfork(), 36\nfragmentation, 554\nfragmented, 480\nframe pointer, 27\nfree, 291\nfree list, 136, 154, 170, 463\nfree lists, 470\nfree space management, 469\nfree-space management, 153\nfrequency, 233\nfsck, 492, 495\nfull-stripe write, 432\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2206,
      "extraction_method": "Direct"
    },
    {
      "page_number": 630,
      "chapter": null,
      "content": "594\nLINK COUNT\nfully associative, 189\nfully-associative, 189, 230\nfunction pointer, 280\nfutex, 307, 308\ngame the scheduler, 76\ngarbage, 494, 518\ngarbage collection, 519\ngarbage collector, 122\ngraphics, 18\ngreedy, 419\ngroup, 223\ngrouping, 240\nhand-over-hand locking, 318\nhard disk drive, 217, 403, 441\nhard drive, 11\nhardware caches, 94\nhardware privilege level, 15\nhardware-based address translation, 130\nhardware-managed TLB, 183\nhardware-managed TLBs, 187\nhead crash, 528\nheader, 157\nheap, 29, 111, 119, 288\nheartbeat, 582\nheld, 291\nhigh watermark, 223\nHill’s Law, 352\nhints, 80\nhit rate, 186, 192, 228\nHoare semantics, 333\nholds, 296\nholes, 520\nhomeworks, iv\nhot spare, 436\nHPUX, 17\nHUP, 379\nhybrid, 202, 205, 308, 393\nI/O, 11\nI/O bus, 389\nI/O instructions, 394\nI/O merging, 415\nIdempotency, 567\nidempotent, 567\nidle time, 224\nillusion, 130\nimmediate reporting, 407, 499\nimplicit, 145\ninconsistent, 429, 491\nindeterminate, 270, 272\nindex node, 464, 465\nindirect pointer, 466\ninitial, 31\ninode, 450, 463–465, 512\ninode bitmap, 463, 481, 492\ninode map (imap), 515\ninode number, 442\ninode table, 463\ninput/output, 11\ninput/output (I/O) device, 389\ninstruction pointer, 26\nINT, 379\ninteractivity, 110\ninterface, 390\ninternal, 202\ninternal fragmentation, 138, 154, 167, 202,\n480, 486\ninternal structure, 390\ninterposing, 129\ninterrupt, 378, 392\ninterrupt handler, 51, 392\ninterrupt service routine (ISR), 392\ninterrupts, 578\ninumber, 465\ninvalid, 173, 203\ninvalid frees, 124\ninvalidate, 96\ninvalidates, 570\ninvariant, 431\ninverted page table, 170\ninverted page tables, 212\nIP, 26\nIRIX, 17\nisolation, 13, 108, 113\nJain’s Fairness Index, 60\njobs, 60\njournal superblock, 503\njournaling, 12, 492, 497\nkernel mode, 15, 47\nkernel stack, 48\nkernel virtual memory, 213\nkill, 379\nKnuth, 322\nlast closer wins, 581\nlast writer wins, 581\nlatent sector errors, 436\nlatent-sector errors, 527\nLauer’s Law, 302\nlazily, 28\nlazy, 250\nLDE, 129\nLeast-Frequently-Used, 233\nLeast-Recently-Used, 233\nleast-recently-used, 192\nlevel of indirection, 207, 515, 516\nLFS, 512\nLFU, 233\nlimit, 133, 135, 204\nlimited direct execution, 45, 55, 105, 129\nlinear page table, 173, 183\nlink count, 454\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2209,
      "extraction_method": "Direct"
    },
    {
      "page_number": 631,
      "chapter": null,
      "content": "MERGE\n595\nlinked list, 468\nLinus Torvalds, 18\nLinux, 18\nLinux ext2, 497\nLinux ext3, 497\nLinux ext4, 500\nlittle endian, 554\nlive, 519\nlivelock, 366, 393\nlmbench, 55\nload, 28\nload imbalance, 100\nload-linked, 300\nloader, 134\nloads, 39\nlocality, 95, 187\nlock, 291\nlock coupling, 318\nlock variable, 291\nlock-free, 96\nlocked, 291\nlocking, 55, 97, 98\nlocks, 262, 283\nlog, 522\nLog-structured File System, 512\nlogical logging, 498\nlong ﬁle names, 487\nlookaside buffer, 195\nlost write, 535\nlottery scheduling, 83\nlow watermark, 223\nlow-level name, 441, 465\nLRU, 192, 233, 474\nLSEs, 527\nMac OS, 16\nmachine state, 26\nmalicious scheduler, 297\nman pages, 42\nmanage, 4\nmanage memory, 130\nmanual pages, 42\nmanual stack management, 380\nmarshaling, 552\nmaster control program, 4\nmeasurement, 577\nmechanisms, 6, 25, 59, 105, 114\nmemory bus, 389\nmemory hierarchy, 217\nmemory hogs, 249\nmemory leak, 124\nmemory management unit (MMU), 135\nmemory overlays, 217\nmemory pressure, 227\nmemory protection, 16\nmemory-management unit, 183\nmemory-mapped I/O, 395\nMenuMeters, 42\nmerge, 162, 415\nMesa semantics, 333, 337\nmetadata, 449, 463, 466, 512\nmetadata journaling, 503\nMFU, 234\nmice, 389\nmicrokernels, 33, 113\nMicrosoft, 16\nmigrating, 99\nmigration, 101\nminicomputer, 15\nminimize the overheads, 13\nmirrored, 422\nmisdirected write, 534\nmiss rate, 192, 228\nMMU, 183\nmobility, 14\nmodiﬁed, 239\nmodiﬁed bit, 239\nmodularity, 27\nmonitors, 312\nMost-Frequently-Used, 234\nMost-Recently-Used, 234\nmount point, 456\nmount protocol, 564\nMQMS, 99\nMRU, 234\nmulti-level feedback queue, 68\nMulti-level Feedback Queue (MLFQ), 71\nmulti-level index, 467\nmulti-level page table, 187, 205\nmulti-queue multiprocessor scheduling, 99\nmulti-threaded, 9, 262, 263\nmulti-threaded programs, 37\nmulti-zoned, 407\nmulticore, 93\nMultics, 17\nmultiprocessor, 93\nmultiprocessor scheduling, 93, 94\nmultiprogramming, 15, 110\nmutex, 292\nmutual exclusion, 271, 272, 292, 293\nname, 443\nnaming, 553\nNBF, 412\nnearest-block-ﬁrst, 412\nnetworking, 18\nnew, 122\nnext ﬁt, 164\nNeXTStep, 18\nnode.js, 373\nnon-blocking data structures, 322\nnon-determinism, 37\nnon-preemptive, 63\nnon-work-conserving, 415\nnull-pointer, 248\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2180,
      "extraction_method": "Direct"
    },
    {
      "page_number": 632,
      "chapter": null,
      "content": "596\nQUEUES\nobject caches, 165\noffset, 171\nopen protocol, 561\nopen-source software, 17\noperating system, 4\nOperating Systems in Three Easy Pieces,\n1\noptimal, 62, 228\noptimistic crash consistency, 508\norder violation, 360, 361\nordered journaling, 503\nOS, 4\nOusterhout’s Law, 79\nout-of-memory killer, 240\noverlap, 67, 68, 221, 377, 392\nowner, 292\npage, 169\npage cache, 493\npage daemon, 223\npage directory, 205, 209\npage directory entries, 206\npage fault, 219, 220, 224\npage frame, 170\npage frame number, 206\npage in, 221\npage miss, 220\npage out, 221\npage replacement, 174\npage selection, 240\npage table, 170, 176\npage table base register, 219\npage table entry (PTE), 172, 219\npage-directory index, 208\npage-fault handler, 220, 224\npage-replacement policy, 221\npage-table base register, 175, 187\npage-table index, 209\npaging, 28, 153, 169, 179, 381\npaging out, 227\nparallel, 93\nparameterization, 487\nparameterize, 78\nparent, 31, 36\nparity, 430\npartitioned, 561\npass, 88\nPatterson’s Law, 577\nPC, 16, 26\nPCB, 32\nPCI, 389\nPDE, 206\nperfect scaling, 313\nperformance, 13, 60, 293, 299, 423, 544\nperipheral bus, 389\npersist, 387, 491\npersistence, iii, 1, 11, 12, 29, 387\npersistent storage, 441\npersistently, 11, 13\npersonal computer, 16\nphysical, 4, 23, 130\nphysical address, 134\nphysical ID, 534\nphysical identiﬁer, 534\nphysical logging, 498\nphysical memory, 7\nphysically-indexed cache, 194\nPID, 36, 191\npipe, 41, 329\npipes, 17\nplatter, 404\npolicies, 26, 59, 114\npolicy, 6\npoll, 378\npolling, 391, 578\npower loss, 491\npower outage, 561\npre-allocation, 470\npreempt, 63\npreemptive, 63\npreemptive scheduler, 298\nPreemptive Shortest Job First, 64\nprefetching, 240\npremature optimization, 322\npresent, 222\npresent bit, 174, 219, 224\nprinciple of locality, 233, 234\nprinciple of SJF (shortest job ﬁrst), 412\npriority level, 72\nprivileged, 49, 137, 193, 395\nprocedure call, 15, 283\nprocess, 25, 26\nProcess Control Block, 32\nprocess control block, 137\nprocess control block (PCB), 263\nprocess identiﬁer, 36, 191\nprocess list, 30, 32\nprocess structure, 137\nproducer, 331\nproducer/consumer, 329, 346\nprogram counter, 26\nprogrammed I/O (PIO), 391\nprojects, iv\nprompt, 40\nproportional-share, 83\nprotect, 113\nprotection, 13, 108, 111, 113, 190\nprotection bits, 146, 173\nprotocol, 575\nprotocol compiler, 551\npseudocode, iv\nPSJF, 64\npurify, 125\nqueues, 72\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2378,
      "extraction_method": "Direct"
    },
    {
      "page_number": 633,
      "chapter": null,
      "content": "ROTATIONS PER MINUTE\n597\nrace condition, 270, 272\nRAID, 421\nRAID 0+1, 428\nRAID 1+0, 428\nRAID-01, 428\nRAID-10, 428\nRAID-DP, 530\nRAM, 194\nRAM isn’t always RAM, 194\nrandom, 192, 409, 426, 446\nrandom-access memory, 194\nrandomness, 84\nraw disk, 475\nread-after-write, 535\nreader-writer lock, 350\nReady, 29\nready, 304\nreal code, iv\nreassembly, 554\nreboot the machine, 51\nrecency, 233\nreconstruct, 431, 530\nrecover, 501\nrecovery, 429\nrecovery protocol, 562\nrecursive update problem, 518\nredirected, 40\nredo logging, 501\nReduced Instruction Set Computing, 189\nredundancy, 421\nRedundant Array of Inexpensive Disks,\n421\nreference bit, 174, 238, 249\nreference count, 453\nregain control, 50\nregister context, 30\nreliability, 13, 423\nrelocate, 132\nremote method invocation, 551\nremote procedure call, 551\nreplace, 192, 221\nreplacement policy, 227\nreplayed, 501\nresident set size, 249\nresource, 4\nresource manager, 4, 6\nresources, 13\nresponse time, 64\nretry, 548\nreturn-from-trap, 15, 47\nrevoke, 506\nRISC, 188, 189\nRMI, 551\nroll forward, 522\nroot directory, 442, 471\nrotates, 434\nrotation delay, 405\nrotational delay, 405\nrotations per minute, 408\nrotations per minute (RPM), 404\nround robin, 99\nRound-Robin (RR), 65\nRPC, 551\nRPM, 408\nRSS, 249\nrun-time library, 551\nrun-time stack, 29\nRunning, 29\nrunning, 304\nrunning program, 25\nSATA, 389\nSATF, 414\nscalability, 98\nscale, 575\nscaling, 167\nSCAN, 413\nscan resistance, 241\nschedule, 474\nscheduled, 30\nscheduler, 37, 52\nscheduler state, 344\nscheduling metric, 60\nscheduling policies, 59\nscheduling policy, 26\nscheduling quantum, 65\nSCSI, 389\nsecond-chance lists, 249\nsecurity, 14, 18, 544, 559\nseek, 406, 447\nsegment, 141, 512, 513\nsegment summary block, 520\nsegment table, 147\nsegmentation, 138, 141, 153, 155\nsegmentation fault, 122, 144\nsegmentation violation, 144\nsegmented FIFO, 249\nsegregated lists, 165\nSEGV, 379\nsemaphore, 341\nseparator, 442\nsequence counter, 549\nsequential, 409, 426, 446\nserialization, 552\nserver-side ﬁle system, 560\nset, 298\nset-associativity, 230\nsets, 296\nsettling time, 406\nshadow paging, 522\nshare, 11, 146\nshared state, 562\nsharing, 559\nshell, 17\nshortest access time ﬁrst, 414\nShortest Job First (SJF), 62\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2214,
      "extraction_method": "Direct"
    },
    {
      "page_number": 634,
      "chapter": null,
      "content": "598\nTIME SLICE\nshortest positioning time ﬁrst, 414\nShortest Time-to-Completion First, 64\nshortest-seek-ﬁrst, 412\nshortest-seek-time-ﬁrst, 412\nSIG, 379\nsignal handler, 379\nsignaling, 326\nsignals, 42, 378, 379\nSIGSEGV, 379\nsilent faults, 528\nsimulations, iv\nsingle-queue multiprocessor scheduling,\n97\nsingle-threaded, 263\nslab allocator, 165\nslabs, 165\nsleeping barber problem, 355\nsloppy counter, 314\nsmall-write problem, 433, 511\nsnapshots, 523\nsockets, 545\nsoft link, 454\nsoftware RAID, 436\nsoftware-managed TLB, 188\nsolid-state drives, 11\nsolid-state storage device, 441\nspace leak, 494\nspace sharing, 26\nsparse address spaces, 143\nspatial locality, 95, 186, 187, 234\nspin lock, 298\nspin-wait, 296\nspin-waiting, 297\nspindle, 404\nsplit, 159\nsplitting, 155\nSPTF, 414\nspurious wakeups, 337\nSQMS, 97\nSSDs, 11\nSSF, 412\nSSTF, 412\nstack, 29, 111, 119\nstack pointer, 27\nstack property, 231\nstale cache, 570\nstandard library, 4, 12\nstandard output, 40\nstarvation, 76, 413\nstarve, 76\nstate, 565\nstateful, 562\nstateless, 562\nstates, 29\nstatic relocation, 134\nstatus, 391\nSTCF, 64\nstore-conditional, 300\nstored checksum, 533\nstrace, 444\nstride, 88\nstride scheduling, 88\nstripe, 424\nstriping, 424\nstub generator, 551\nsub-blocks, 486\nsub-directories, 442\nsubtractive parity, 432\nSunOS, 17\nsuper block, 481\nsuperblock, 464\nsuperpages, 214\nsupervisor, 4\nsurface, 404\nswap, 213\nswap daemon, 223\nswap space, 125, 218\nswapping, 28\nswitches contexts, 53\nsymbolic link, 454, 488\nsynchronization primitives, 271\nsynchronous, 375, 552\nsynchronously, 555\nsystem call, 15, 47\nsystem calls, 4, 12, 50, 560\nsystem crash, 491\nsystems programming, iv\nTCP, 549\nTCP/IP, 549\ntcsh, 42\ntemporal locality, 95, 186, 187, 234\ntest, 298\ntest-and-set, 297\ntest-and-set instruction, 296\ntests, 296\nthe mapping problem, 425\nthe web, 42\nthrashing, 240\nthread, 262, 263\nthread control blocks (TCBs), 263\nthread pool, 553\nthread safe, 311\nthread-local, 264\nthreads, 9, 93, 112\nThree C’s, 230\nticket, 85\nticket currency, 85\nticket inﬂation, 85\nticket lock, 302\nticket transfer, 85\ntickets, 83\nTID, 498\nTime sharing, 26\ntime sharing, 25, 45, 46, 110\ntime slice, 65\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 2168,
      "extraction_method": "Direct"
    },
    {
      "page_number": 635,
      "chapter": null,
      "content": "VALID BIT\n599\ntime-sharing, 26\ntime-slicing, 65\ntime-space trade-off, 207\ntime-space trade-offs, 207\ntimeout, 548\ntimeout/retry, 548\ntimer interrupt, 51, 52\ntips, iii\nTLB, 183\nTLB coverage, 194\nTLB hit, 184, 219\nTLB miss, 184, 219\ntorn write, 403\ntotal ordering, 365\ntrack, 404\ntrack buffer, 407, 487\ntrack skew, 406\ntrade-off, 66\ntransaction, 274\ntransaction checksum, 508\ntransaction identiﬁer, 498\ntransfer, 406\ntranslate, 174\ntranslated, 133\ntranslation lookaside buffer, 195\ntranslation-lookaside buffer, 183\ntransparency, 113, 131\ntransparent, 132, 560\ntransparently, 224, 422\ntrap, 15, 47, 51\ntrap handler, 15, 188\ntrap handlers, 48\ntrap table, 47, 48\ntraverse, 471\ntriple indirect pointer, 467\ntruss, 444\nTuring Award, 71\nturnaround time, 60\ntwo-phase lock, 307\ntwo-phased, 393\ntype, 443\nUDP/IP, 545\nunfairness metric, 87\nuniﬁed page cache, 474\nuninitialized read, 123\nunlocked, 291\nunmapped, 188\nunmarshaling, 552\nupdate, 7, 96\nupdate visibility, 570\nUSB, 389\nuse bit, 238\nuser mode, 15, 47\nutilization, 110\nvalgrind, 125\nvalid, 190, 222\nvalid bit, 173, 206\nVenus, 576\nversion number, 521\nversioning ﬁle system, 519\nVery Simple File System, 461\nVice, 576\nvirtual, 4, 23, 130\nvirtual address, 112, 114, 134\nvirtual address space, 8\nvirtual CPUs, 263\nvirtual machine, 4\nvirtual memory, 263\nvirtual page number (VPN), 171\nvirtual-to-physical address translations, 176\nvirtualization, iii, 1, 4, 8, 23\nvirtualized, 90, 269\nvirtualizes, 13\nvirtualizing, 25\nvirtualizing memory, 8, 112\nvirtualizing the CPU, 6\nvirtually-indexed cache, 194\nvoid pointer, 154, 280\nvolatile, 11\nVoltaire’s Law, 569\nvolumes, 577\nVon Neumann, 3\nvoo-doo constants, 77\nvsfs, 461\nWAFL, 523, 530\nwait-free, 367\nwait-free synchronization, 300\nwaiting, 326\nwakeup/waiting race, 306\nwhole-ﬁle caching, 575\nwired, 188\nwork stealing, 101\nwork-conserving, 415\nworking sets, 240\nworkload, 59, 492, 585\nworkloads, 234\nworst ﬁt, 163\nworst-ﬁt, 148\nwrite back, 407\nwrite barriers, 499\nwrite buffering, 474, 513, 569\nwrite through, 407\nwrite verify, 535\nwrite-ahead log, 429\nwrite-ahead logging, 492, 497\nx86, 177\nXDR, 555\nXOR, 430\nyield, 51\nZemaphores, 355\nc⃝2014, ARPACI-DUSSEAU\nTHREE\nEASY\nPIECES\n",
      "content_length": 2164,
      "extraction_method": "Direct"
    },
    {
      "page_number": 636,
      "chapter": null,
      "content": "600\nZOMBIE\nZettabyte File System, 535\nZFS, 523, 535\nzombie, 31\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 110,
      "extraction_method": "Direct"
    },
    {
      "page_number": 637,
      "chapter": null,
      "content": "Asides\nUNIX Signals, 379\nAdvanced Chapters, 93\nAnd Then Came Linux, 18\nBelady’s Anomaly, 231\nBlocking vs. Non-blocking Interfaces, 375\nCache Consistency Is Not A Panacea, 580\nCalling lseek() Does Not Perform A Disk Seek, 447\nComputing The “Average” Seek, 411\nData Structure – The Free List, 136\nData Structure – The Inode, 465\nData Structure – The Page Table, 176\nData Structure – The Process List, 32\nDekker’s and Peterson’s Algorithms, 295\nDimensional Analysis, 408\nEmulating Reference Bits, 250\nEvery Address You See Is Virtual, 114\nFFS File Creation, 482\nForcing Writes To Disk, 499\nFree Space Management, 470\nGreat Engineers Are Really Great, 166\nHow Long Context Switches Take, 55\nInterludes, 35\nKey Concurrency Terms, 272\n601\n",
      "content_length": 733,
      "extraction_method": "Direct"
    },
    {
      "page_number": 638,
      "chapter": null,
      "content": "602\nWHY SYSTEM CALLS LOOK LIKE PROCEDURE CALLS\nLinked-based Approaches, 468\nMeasurement Homeworks, 58\nMental Models Of File Systems, 462\nMultiple Page Sizes, 202\nOptimizing Log Writes, 500\nPreemptive Schedulers, 63\nReads Don’t Access Allocation Structures, 472\nRISC vs. CISC, 189\nRTFM – Read The Man Pages, 42\nSimulation Homeworks, 70\nSoftware-based Relocation, 134\nStorage Technologies, 218\nSwapping Terminology And Other Things, 220\nThe creat() System Call, 444\nThe End-to-End Argument, 555\nThe Importance of UNIX, 17\nThe Importance Of Workload, 585\nThe RAID Consistent-Update Problem, 429\nThe RAID Mapping Problem, 425\nThe Segmentation Fault, 144\nThread API Guidelines, 288\nTLB Valid Bit ̸= Page Table Valid Bit, 190\nTypes of Cache Misses, 230\nTypes of Locality, 234\nWhy Hardware Doesn’t Handle Page Faults, 221\nWhy Null Pointer Accesses Cause Seg Faults, 248\nWhy Servers Crash, 561\nWhy System Calls Look Like Procedure Calls, 48\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 980,
      "extraction_method": "Direct"
    },
    {
      "page_number": 639,
      "chapter": null,
      "content": "Tips\nAlways Hold The Lock While Signaling, 329\nAmortization Can Reduce Costs, 66\nAvoid Premature Optimization (Knuth’s Law), 322\nAvoid Voo-doo Constants (Ousterhout’s Law), 79\nBe Careful Setting The Timeout Value, 550\nBe Careful With Generalization, 356\nBe Lazy, 251\nBe Wary of Complexity, 208\nBe Wary Of Locks and Control Flow, 319\nBe Wary Of Powerful Commands, 451\nCommunication Is Inherently Unreliable, 544\nComparing Against Optimal is Useful, 229\nConsider Extent-based Approaches, 467\nDealing With Application Misbehavior, 51\nDetails Matter, 513\nDo Work In The Background, 224\nDon’t Always Do It Perfectly (Tom West’s Law), 370\nDon’t Block In Event-based Servers, 377\nGetting It Right (Lampson’s Law), 40\nHardware-based Dynamic Relocation, 135\nIdempotency Is Powerful, 567\nIf 1000 Solutions Exist, No Great One Does, 149\nInterposition Is Powerful, 131\nInterrupts Not Always Better Than PIO, 393\nIt Always Depends (Livny’s Law), 415\nIt Compiled or It Ran ̸= It Is Correct, 123\nKnow And Use Your Tools, 269\n603\n",
      "content_length": 1014,
      "extraction_method": "Direct"
    },
    {
      "page_number": 640,
      "chapter": null,
      "content": "604\nWHEN IN DOUBT, TRY IT OUT\nLearn From History, 72\nLess Code Is Better Code (Lauer’s Law), 302\nMake The System Usable, 488\nMeasure Then Build (Patterson’s Law), 577\nMore Concurrency Isn’t Necessarily Faster, 319\nOverlap Enables Higher Utilization, 68\nPerfect Is The Enemy Of The Good (Voltaire’s Law), 569\nRAM Isn’t Always RAM (Culler’s Law), 194\nReboot Is Useful, 56\nSeparate Policy And Mechanism, 27\nSimple And Dumb Can Be Better (Hill’s Law), 352\nThe Principle of Isolation, 113\nThe Principle of SJF, 62\nThere’s No Free Lunch, 531\nThink About Concurrency As Malicious Scheduler, 297\nThink Carefully About Naming, 443\nTransparency Enables Deployment, 422\nTurn Flaws Into Virtues, 523\nUnderstand Time-Space Trade-offs, 207\nUse strace (And Similar Tools), 445\nUse A Level Of Indirection, 516\nUse Advice Where Possible, 80\nUse Atomic Operations, 274\nUse Caching When Possible, 187\nUse Checksums For Integrity, 547\nUse Disks Sequentially, 410\nUse Hybrids, 205\nUse Protected Control Transfer, 47\nUse Randomness, 84\nUse The Timer Interrupt To Regain Control, 52\nUse Tickets To Represent Shares, 85\nUse Time Sharing (and Space Sharing), 26\nUse While (Not If) For Conditions, 337\nWhen In Doubt, Try It Out, 121\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1254,
      "extraction_method": "Direct"
    },
    {
      "page_number": 641,
      "chapter": null,
      "content": "Cruces\nHow To Account For Disk Rotation Costs, 413\nHow To Add Locks To Data Structures, 311\nHow To Allocate And Manage Memory, 119\nHow To Avoid Spinning, 304\nHow To Avoid The Costs Of Polling, 392\nHow To Avoid The Curse Of Generality, 245\nHow To Build A Device-neutral OS, 395\nHow To Build A Distributed File System, 560\nHow To Build A Lock, 293\nHow To Build Concurrent Servers Without Threads, 373\nHow To Build Correct Concurrent Programs, 10\nHow To Build Systems That Work When Components Fail, 543\nHow To Communicate With Devices, 394\nHow To Create And Control Processes, 35\nHow To Create And Control Threads, 279\nHow To Deal With Deadlock, 363\nHow To Deal With Load Imbalance, 101\nHow To Decide Which Page To Evict, 227\nHow To Deﬁne A Stateless File Protocol, 563\nHow To Design A Scalable File Protocol, 578\nHow To Design TLB Replacement Policy, 192\nHow To Develop Scheduling Policy, 59\nHow To Efﬁciently And Flexibly Virtualize Memory, 129\nHow To Efﬁciently Virtualize The CPU With Control, 45\nHow To Ensure Data Integrity, 527\nHow To Gain Control Without Cooperation, 51\nHow To Go Beyond Physical Memory, 217\nHow To Handle Common Concurrency Bugs, 359\nHow To Handle Disk Starvation, 413\nHow To Handle Latent Sector Errors, 529\nHow To Handle Lost Writes, 535\nHow To Handle Misdirected Writes, 534\nHow To Implement A Simple File System, 461\n605\n",
      "content_length": 1349,
      "extraction_method": "Direct"
    },
    {
      "page_number": 642,
      "chapter": null,
      "content": "606\nHOW TO WAIT FOR A CONDITION\nHow To Implement An LRU Replacement Policy, 238\nHow To Integrate I/O Into Systems, 389\nHow To Lower PIO Overheads, 394\nHow To Make A Large, Fast, Reliable Disk, 421\nHow To Make All Writes Sequential Writes, 512\nHow To Make Page Tables Smaller, 201\nHow To Manage A Persistent Device, 441\nHow To Manage Free Space, 154\nHow To Manage TLB Contents On A Context Switch, 191\nHow To Organize On-disk Data To Improve Performance, 480\nHow To Perform Restricted Operations, 46\nHow To Preserve Data Integrity Despite Corruption, 530\nHow To Provide Support For Synchronization, 272\nHow To Provide The Illusion Of Many CPUs, 25\nHow To Reduce File System I/O Costs, 473\nHow To Regain Control Of The CPU, 50\nHow To Schedule Jobs On Multiple CPUs, 94\nHow To Schedule Without Perfect Knowledge, 71\nHow To Share The CPU Proportionally, 83\nHow To Speed Up Address Translation, 183\nHow To Store And Access Data On Disk, 403\nHow To Store Data Persistently, 12\nHow To Support A Large Address Space, 141\nHow To Update The Disk Despite Crashes, 491\nHow To Use Semaphores, 341\nHow To Virtualize Memory, 112\nHow To Virtualize Memory Without Segments, 169\nHow To Virtualize Resources, 4\nHow To Wait For A Condition, 326\nOPERATING\nSYSTEMS\n[VERSION 0.80]\nWWW.OSTEP.ORG\n",
      "content_length": 1272,
      "extraction_method": "Direct"
    },
    {
      "page_number": 643,
      "chapter": null,
      "content": "This book was typeset using the amazing LATEX typesetting system and\nthe wonderful memoir book-making package. A heartfelt thank you to\nthe legions of programmers who have contributed to this powerful tool\nover the many years of its development.\nAll of the graphs and ﬁgures in the book were generated using a Python-\nbased version of zplot, a simple and useful tool developed by R. Arpaci-\nDusseau to generate graphs in PostScript. The zplot tool arose after\nmany years of frustration with existing graphing tools such as gnuplot\n(which was limited) and ploticus (which was overly complex though\nadmittedly quite awesome). As a result, R. A-D ﬁnally put his years of\nstudy of PostScript to good use and developed zplot.\n",
      "content_length": 721,
      "extraction_method": "Direct"
    }
  ],
  "enrichment": {
    "version": "1.0.0",
    "generated_by": "generate_chapter_metadata.py",
    "contains": [
      "keywords",
      "concepts",
      "summary"
    ]
  }
}